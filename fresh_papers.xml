<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 25 Mar 2024 06:00:23 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data</title><link>http://arxiv.org/abs/2403.15389v1</link><description>Recently, there has been an increased interest in the practical problem oflearning multiple dense scene understanding tasks from partially annotateddata, where each training sample is only labeled for a subset of the tasks. Themissing of task labels in training leads to low-quality and noisy predictions,as can be observed from state-of-the-art methods. To tackle this issue, wereformulate the partially-labeled multi-task dense prediction as a pixel-leveldenoising problem, and propose a novel multi-task denoising diffusion frameworkcoined as DiffusionMTL. It designs a joint diffusion and denoising paradigm tomodel a potential noisy distribution in the task prediction or feature maps andgenerate rectified outputs for different tasks. To exploit multi-taskconsistency in denoising, we further introduce a Multi-Task Conditioningstrategy, which can implicitly utilize the complementary nature of the tasks tohelp learn the unlabeled tasks, leading to an improvement in the denoisingperformance of the different tasks. Extensive quantitative and qualitativeexperiments demonstrate that the proposed multi-task denoising diffusion modelcan significantly improve multi-task prediction maps, and outperform thestate-of-the-art methods on three challenging multi-task benchmarks, under twodifferent partial-labeling evaluation settings. The code is available athttps://prismformore.github.io/diffusionmtl/.</description><author>Hanrong Ye, Dan Xu</author><pubDate>Fri, 22 Mar 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15389v1</guid></item><item><title>LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models</title><link>http://arxiv.org/abs/2403.15388v1</link><description>Large Multimodal Models (LMMs) have shown significant reasoning capabilitiesby connecting a visual encoder and a large language model. LMMs typically use afixed amount of visual tokens, such as the penultimate layer features in theCLIP visual encoder, as the prefix content. Recent LMMs incorporate morecomplex visual inputs, such as high-resolution images and videos, whichincrease the number of visual tokens significantly. However, due to the designof the Transformer architecture, computational costs associated with thesemodels tend to increase quadratically with the number of input tokens. Totackle this problem, we explore a token reduction mechanism and find, similarto prior work, that many visual tokens are spatially redundant. Based on this,we propose PruMerge, a novel adaptive visual token reduction approach, whichlargely reduces the number of visual tokens while maintaining comparable modelperformance. We first select the unpruned visual tokens based on theirsimilarity to class tokens and spatial tokens. We then cluster the prunedtokens based on key similarity and merge the clustered tokens with the unprunedtokens to supplement their information. Empirically, when applied to LLaVA-1.5,our approach can compress the visual tokens by 14.4 times on average, andachieve comparable performance across diverse visual question-answering andreasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.</description><author>Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan</author><pubDate>Fri, 22 Mar 2024 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15388v1</guid></item><item><title>LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis</title><link>http://arxiv.org/abs/2403.15385v1</link><description>Recent text-to-3D generation approaches produce impressive 3D results butrequire time-consuming optimization that can take up to an hour per prompt.Amortized methods like ATT3D optimize multiple prompts simultaneously toimprove efficiency, enabling fast text-to-3D synthesis. However, they cannotcapture high-frequency geometry and texture details and struggle to scale tolarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressingthese limitations to achieve fast, high-quality generation on a significantlylarger prompt set. Key to our method is 1) building a scalable architecture and2) leveraging 3D data during optimization through 3D-aware diffusion priors,shape regularization, and model initialization to achieve robustness to diverseand complex training prompts. LATTE3D amortizes both neural field and texturedsurface generation to produce highly detailed textured meshes in a singleforward pass. LATTE3D generates 3D objects in 400ms, and can be furtherenhanced with fast test-time optimization.</description><author>Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas, Antonio Torralba, Sanja Fidler, Xiaohui Zeng</author><pubDate>Fri, 22 Mar 2024 18:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15385v1</guid></item><item><title>Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting</title><link>http://arxiv.org/abs/2312.10070v2</link><description>We present a dense simultaneous localization and mapping (SLAM) method thatuses 3D Gaussians as a scene representation. Our approach enablesinteractive-time reconstruction and photo-realistic rendering from real-worldsingle-camera RGBD videos. To this end, we propose a novel effective strategyfor seeding new Gaussians for newly explored areas and their effective onlineoptimization that is independent of the scene size and thus scalable to largerscenes. This is achieved by organizing the scene into sub-maps which areindependently optimized and do not need to be kept in memory. We furtheraccomplish frame-to-model camera tracking by minimizing photometric andgeometric losses between the input and rendered frames. The Gaussianrepresentation allows for high-quality photo-realistic real-time rendering ofreal-world scenes. Evaluation on synthetic and real-world datasets demonstratescompetitive or superior performance in mapping, tracking, and renderingcompared to existing neural dense SLAM methods.</description><author>Vladimir Yugay, Yue Li, Theo Gevers, Martin R. Oswald</author><pubDate>Fri, 22 Mar 2024 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10070v2</guid></item><item><title>ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars</title><link>http://arxiv.org/abs/2403.15383v1</link><description>Real-world applications often require a large gallery of 3D assets that sharea consistent theme. While remarkable advances have been made in general 3Dcontent creation from text or image, synthesizing customized 3D assetsfollowing the shared theme of input 3D exemplars remains an open andchallenging problem. In this work, we present ThemeStation, a novel approachfor theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3Dassets based on given few exemplars with two goals: 1) unity for generating 3Dassets that thematically align with the given exemplars and 2) diversity forgenerating 3D assets with a high degree of variations. To this end, we design atwo-stage framework that draws a concept image first, followed by areference-informed 3D modeling stage. We propose a novel dual scoredistillation (DSD) loss to jointly leverage priors from both the inputexemplars and the synthesized concept image. Extensive experiments and userstudies confirm that ThemeStation surpasses prior works in producing diversetheme-aware 3D models with impressive quality. ThemeStation also enablesvarious applications such as controllable 3D-to-3D generation.</description><author>Zhenwei Wang, Tengfei Wang, Gerhard Hancke, Ziwei Liu, Rynson W. H. Lau</author><pubDate>Fri, 22 Mar 2024 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15383v1</guid></item><item><title>DragAPart: Learning a Part-Level Motion Prior for Articulated Objects</title><link>http://arxiv.org/abs/2403.15382v1</link><description>We introduce DragAPart, a method that, given an image and a set of drags asinput, can generate a new image of the same object in a new state, compatiblewith the action of the drags. Differently from prior works that focused onrepositioning objects, DragAPart predicts part-level interactions, such asopening and closing a drawer. We study this problem as a proxy for learning ageneralist motion model, not restricted to a specific kinematic structure orobject category. To this end, we start from a pre-trained image generator andfine-tune it on a new synthetic dataset, Drag-a-Move, which we introduce.Combined with a new encoding for the drags and dataset randomization, the newmodel generalizes well to real images and different categories. Compared toprior motion-controlled generators, we demonstrate much better part-levelmotion understanding.</description><author>Ruining Li, Chuanxia Zheng, Christian Rupprecht, Andrea Vedaldi</author><pubDate>Fri, 22 Mar 2024 18:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15382v1</guid></item><item><title>Long-CLIP: Unlocking the Long-Text Capability of CLIP</title><link>http://arxiv.org/abs/2403.15378v1</link><description>Contrastive Language-Image Pre-training (CLIP) has been the cornerstone forzero-shot classification, text-image retrieval, and text-image generation byaligning image and text modalities. Despite its widespread adoption, asignificant limitation of CLIP lies in the inadequate length of text input. Thelength of the text token is restricted to 77, and an empirical study shows theactual effective length is even less than 20. This prevents CLIP from handlingdetailed descriptions, limiting its applications for image retrieval andtext-to-image generation with extensive prerequisites. To this end, we proposeLong-CLIP as a plug-and-play alternative to CLIP that supports long-text input,retains or even surpasses its zero-shot generalizability, and aligns the CLIPlatent space, making it readily replace CLIP without any further adaptation indownstream frameworks. Nevertheless, achieving this goal is far fromstraightforward, as simplistic fine-tuning can result in a significantdegradation of CLIP's performance. Moreover, substituting the text encoder witha language model supporting longer contexts necessitates pretraining with vastamounts of data, incurring significant expenses. Accordingly, Long-CLIPintroduces an efficient fine-tuning solution on CLIP with two novel strategiesdesigned to maintain the original capabilities, including (1) aknowledge-preserved stretching of positional embedding and (2) a primarycomponent matching of CLIP features. With leveraging just one million extralong text-image pairs, Long-CLIP has shown the superiority to CLIP for about20% in long caption text-image retrieval and 6% in traditional text-imageretrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offersenhanced capabilities for generating images from detailed text descriptions byreplacing CLIP in a plug-and-play manner.</description><author>Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Jiaqi Wang</author><pubDate>Fri, 22 Mar 2024 18:58:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15378v1</guid></item><item><title>InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding</title><link>http://arxiv.org/abs/2403.15377v1</link><description>We introduce InternVideo2, a new video foundation model (ViFM) that achievesthe state-of-the-art performance in action recognition, video-text tasks, andvideo-centric dialogue. Our approach employs a progressive training paradigmthat unifies the different self- or weakly-supervised learning frameworks ofmasked video token reconstruction, cross-modal contrastive learning, and nexttoken prediction. Different training stages would guide our model to capturedifferent levels of structure and semantic information through differentpretext tasks. At the data level, we prioritize the spatiotemporal consistencyby semantically segmenting videos and generating video-audio-speech captions.This improves the alignment between video and text. We scale both data andmodel size for our InternVideo2. Through extensive experiments, we validate ourdesigns and demonstrate the state-of-the-art performance on over 60 video andaudio tasks. Notably, our model outperforms others on various video-relatedcaptioning, dialogue, and long video understanding benchmarks, highlighting itsability to reason and comprehend long temporal contexts. Code and models areavailable at https://github.com/OpenGVLab/InternVideo2/.</description><author>Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang</author><pubDate>Fri, 22 Mar 2024 18:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15377v1</guid></item><item><title>Simple and Scalable Strategies to Continually Pre-train Large Language Models</title><link>http://arxiv.org/abs/2403.08763v2</link><description>Large language models (LLMs) are routinely pre-trained on billions of tokens,only to start the process over again once new data becomes available. A muchmore efficient solution is to continually pre-train these models, savingsignificant compute compared to re-training. However, the distribution shiftinduced by new data typically results in degraded performance on previous dataor poor adaptation to the new data. In this work, we show that a simple andscalable combination of learning rate (LR) re-warming, LR re-decaying, andreplay of previous data is sufficient to match the performance of fullyre-training from scratch on all available data, as measured by final loss andlanguage model (LM) evaluation benchmarks. Specifically, we show this for aweak but realistic distribution shift between two commonly used LLMpre-training datasets (English$\rightarrow$English) and a stronger distributionshift (English$\rightarrow$German) at the $405$M parameter model scale withlarge dataset sizes (hundreds of billions of tokens). Selecting the weak butrealistic shift for larger-scale experiments, we also find that our continuallearning strategies match the re-training baseline for a 10B parameter LLM. Ourresults demonstrate that LLMs can be successfully updated via simple andscalable continual learning strategies, matching the re-training baseline usingonly a fraction of the compute. Finally, inspired by previous work, we proposealternatives to the cosine learning rate schedule that help circumventforgetting induced by LR re-warming and that are not bound to a fixed tokenbudget.</description><author>Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish</author><pubDate>Fri, 22 Mar 2024 18:56:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08763v2</guid></item><item><title>Finding the right XAI method -- A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science</title><link>http://arxiv.org/abs/2303.00652v2</link><description>Explainable artificial intelligence (XAI) methods shed light on thepredictions of machine learning algorithms. Several different approaches existand have already been applied in climate science. However, usually missingground truth explanations complicate their evaluation and comparison,subsequently impeding the choice of the XAI method. Therefore, in this work, weintroduce XAI evaluation in the climate context and discuss different desiredexplanation properties, namely robustness, faithfulness, randomization,complexity, and localization. To this end, we chose previous work as a casestudy where the decade of annual-mean temperature maps is predicted. Aftertraining both a multi-layer perceptron (MLP) and a convolutional neural network(CNN), multiple XAI methods are applied and their skill scores in reference toa random uniform explanation are calculated for each property. Independent ofthe network, we find that XAI methods Integrated Gradients, layer-wiserelevance propagation, and input times gradients exhibit considerablerobustness, faithfulness, and complexity while sacrificing randomizationperformance. Sensitivity methods -- gradient, SmoothGrad, NoiseGrad, andFusionGrad, match the robustness skill but sacrifice faithfulness andcomplexity for randomization skill. We find architecture-dependent performancedifferences regarding robustness, complexity and localization skills ofdifferent XAI methods, highlighting the necessity for research task-specificevaluation. Overall, our work offers an overview of different evaluationproperties in the climate science context and shows how to compare andbenchmark different explanation methods, assessing their suitability based onstrengths and weaknesses, for the specific research problem at hand. By that,we aim to support climate researchers in the selection of a suitable XAImethod.</description><author>Philine Bommer, Marlene Kretschmer, Anna Hedström, Dilyara Bareeva, Marina M. -C. Höhne</author><pubDate>Fri, 22 Mar 2024 18:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00652v2</guid></item><item><title>Can large language models explore in-context?</title><link>http://arxiv.org/abs/2403.15371v1</link><description>We investigate the extent to which contemporary Large Language Models (LLMs)can engage in exploration, a core capability in reinforcement learning anddecision making. We focus on native performance of existing LLMs, withouttraining interventions. We deploy LLMs as agents in simple multi-armed banditenvironments, specifying the environment description and interaction historyentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,GPT-4, and Llama2, using a variety of prompt designs, and find that the modelsdo not robustly engage in exploration without substantial interventions: i)Across all of our experiments, only one configuration resulted in satisfactoryexploratory behavior: GPT-4 with chain-of-thought reasoning and an externallysummarized interaction history, presented as sufficient statistics; ii) Allother configurations did not result in robust exploratory behavior, includingthose with chain-of-thought reasoning but unsummarized history. Although thesefindings can be interpreted positively, they suggest that externalsummarization -- which may not be possible in more complex settings -- isimportant for obtaining desirable behavior from LLM agents. We conclude thatnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,may be required to empower LLM-based decision making agents in complexsettings.</description><author>Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, Aleksandrs Slivkins</author><pubDate>Fri, 22 Mar 2024 18:50:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15371v1</guid></item><item><title>Augmented Reality based Simulated Data (ARSim) with multi-view consistency for AV perception networks</title><link>http://arxiv.org/abs/2403.15370v1</link><description>Detecting a diverse range of objects under various driving scenarios isessential for the effectiveness of autonomous driving systems. However, thereal-world data collected often lacks the necessary diversity presenting along-tail distribution. Although synthetic data has been utilized to overcomethis issue by generating virtual scenes, it faces hurdles such as a significantdomain gap and the substantial efforts required from 3D artists to createrealistic environments. To overcome these challenges, we present ARSim, a fullyautomated, comprehensive, modular framework designed to enhance real multi-viewimage data with 3D synthetic objects of interest. The proposed methodintegrates domain adaptation and randomization strategies to address covariateshift between real and simulated data by inferring essential domain attributesfrom real data and employing simulation-based randomization for otherattributes. We construct a simplified virtual scene using real data andstrategically place 3D synthetic assets within it. Illumination is achieved byestimating light distribution from multiple images capturing the surroundingsof the vehicle. Camera parameters from real data are employed to rendersynthetic assets in each frame. The resulting augmented multi-view consistentdataset is used to train a multi-camera perception network for autonomousvehicles. Experimental results on various AV perception tasks demonstrate thesuperior performance of networks trained on the augmented dataset.</description><author>Aqeel Anwar, Tae Eun Choe, Zian Wang, Sanja Fidler, Minwoo Park</author><pubDate>Fri, 22 Mar 2024 18:49:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15370v1</guid></item><item><title>Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion</title><link>http://arxiv.org/abs/2403.14617v2</link><description>We introduce Videoshop, a training-free video editing algorithm for localizedsemantic edits. Videoshop allows users to use any editing software, includingPhotoshop and generative inpainting, to modify the first frame; itautomatically propagates those changes, with semantic, spatial, and temporallyconsistent motion, to the remaining frames. Unlike existing methods that enableedits only through imprecise textual instructions, Videoshop allows users toadd or remove objects, semantically change objects, insert stock photos intovideos, etc. with fine-grained control over locations and appearance. Weachieve this through image-based video editing by inverting latents with noiseextrapolation, from which we generate videos conditioned on the edited image.Videoshop produces higher quality edits against 6 baselines on 2 editingbenchmarks using 10 evaluation metrics.</description><author>Xiang Fan, Anand Bhattad, Ranjay Krishna</author><pubDate>Fri, 22 Mar 2024 18:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14617v2</guid></item><item><title>A Transfer Attack to Image Watermarks</title><link>http://arxiv.org/abs/2403.15365v1</link><description>Watermark has been widely deployed by industry to detect AI-generated images.The robustness of such watermark-based detector against evasion attacks in thewhite-box and black-box settings is well understood in the literature. However,the robustness in the no-box setting is much less understood. In particular,multiple studies claimed that image watermark is robust in such setting. Inthis work, we propose a new transfer evasion attack to image watermark in theno-box setting. Our transfer attack adds a perturbation to a watermarked imageto evade multiple surrogate watermarking models trained by the attacker itself,and the perturbed watermarked image also evades the target watermarking model.Our major contribution is to show that, both theoretically and empirically,watermark-based AI-generated image detector is not robust to evasion attackseven if the attacker does not have access to the watermarking model nor thedetection API.</description><author>Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong</author><pubDate>Fri, 22 Mar 2024 18:33:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15365v1</guid></item><item><title>Towards Knowledge-Grounded Natural Language Understanding and Generation</title><link>http://arxiv.org/abs/2403.15364v1</link><description>This thesis investigates how natural language understanding and generationwith transformer models can benefit from grounding the models with knowledgerepresentations and addresses the following key research questions: (i) Canknowledge of entities extend its benefits beyond entity-centric tasks, such asentity linking? (ii) How can we faithfully and effectively extract suchstructured knowledge from raw text, especially noisy web text? (iii) How doother types of knowledge, beyond structured knowledge, contribute to improvingNLP tasks? Studies in this thesis find that incorporating relevant and up-to-dateknowledge of entities benefits fake news detection, and entity-focusedcode-switching significantly enhances zero-shot cross-lingual transfer onentity-centric tasks. In terms of effective and faithful approaches toextracting structured knowledge, it is observed that integrating negativeexamples and training with entity planning significantly improves performance.Additionally, it is established that other general forms of knowledge, such asparametric and distilled knowledge, enhance multimodal and multilingualknowledge-intensive tasks. This research shows the tangible benefits of diverseknowledge integration and motivates further exploration in this direction.</description><author>Chenxi Whitehouse</author><pubDate>Fri, 22 Mar 2024 18:32:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15364v1</guid></item><item><title>Cascading Blackout Severity Prediction with Statistically-Augmented Graph Neural Networks</title><link>http://arxiv.org/abs/2403.15363v1</link><description>Higher variability in grid conditions, resulting from growing renewablepenetration and increased incidence of extreme weather events, has increasedthe difficulty of screening for scenarios that may lead to catastrophiccascading failures. Traditional power-flow-based tools for assessing cascadingblackout risk are too slow to properly explore the space of possible failuresand load/generation patterns. We add to the growing literature of fastergraph-neural-network (GNN)-based techniques, developing two novel techniquesfor the estimation of blackout magnitude from initial grid conditions. First wepropose several methods for employing an initial classification step to filterout safe "non blackout" scenarios prior to magnitude estimation. Second, usinginsights from the statistical properties of cascading blackouts, we propose amethod for facilitating non-local message passing in our GNN models. Wevalidate these two approaches on a large simulated dataset, and show thepotential of both to increase blackout size estimation performance.</description><author>Joe Gorka, Tim Hsu, Wenting Li, Yury Maximov, Line Roald</author><pubDate>Fri, 22 Mar 2024 18:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15363v1</guid></item><item><title>Empowering Autonomous Driving with Large Language Models: A Safety Perspective</title><link>http://arxiv.org/abs/2312.00812v4</link><description>Autonomous Driving (AD) encounters significant safety hurdles in long-tailunforeseen driving scenarios, largely stemming from the non-interpretabilityand poor generalization of the deep neural networks within the AD system,particularly in out-of-distribution and uncertain data. To this end, this paperexplores the integration of Large Language Models (LLMs) into AD systems,leveraging their robust common-sense knowledge and reasoning abilities. Theproposed methodologies employ LLMs as intelligent decision-makers in behavioralplanning, augmented with a safety verifier shield for contextual safetylearning, for enhancing driving performance and safety. We present two keystudies in a simulated environment: an adaptive LLM-conditioned ModelPredictive Control (MPC) and an LLM-enabled interactive behavior planningscheme with a state machine. Demonstrating superior performance and safetymetrics compared to state-of-the-art approaches, our approach shows thepromising potential for using LLMs for autonomous vehicles.</description><author>Yixuan Wang, Ruochen Jiao, Sinong Simon Zhan, Chengtian Lang, Chao Huang, Zhaoran Wang, Zhuoran Yang, Qi Zhu</author><pubDate>Fri, 22 Mar 2024 18:29:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00812v4</guid></item><item><title>MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis</title><link>http://arxiv.org/abs/2403.04639v2</link><description>The present paper introduces new sentiment data, MaCMS, forMagahi-Hindi-English (MHE) code-mixed language, where Magahi is aless-resourced minority language. This dataset is the firstMagahi-Hindi-English code-mixed dataset for sentiment analysis tasks. Further,we also provide a linguistics analysis of the dataset to understand thestructure of code-mixing and a statistical study to understand the languagepreferences of speakers with different polarities. With these analyses, we alsotrain baseline models to evaluate the dataset's quality.</description><author>Priya Rani, Gaurav Negi, Theodorus Fransen, John P. McCrae</author><pubDate>Fri, 22 Mar 2024 18:28:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04639v2</guid></item><item><title>LLMR: Real-time Prompting of Interactive Worlds using Large Language Models</title><link>http://arxiv.org/abs/2309.12276v3</link><description>We present Large Language Model for Mixed Reality (LLMR), a framework for thereal-time creation and modification of interactive Mixed Reality experiencesusing LLMs. LLMR leverages novel strategies to tackle difficult cases whereideal training data is scarce, or where the design goal requires the synthesisof internal dynamics, intuitive analysis, or advanced interactivity. Ourframework relies on text interaction and the Unity game engine. Byincorporating techniques for scene understanding, task planning,self-debugging, and memory management, LLMR outperforms the standard GPT-4 by4x in average error rate. We demonstrate LLMR's cross-platform interoperabilitywith several example worlds, and evaluate it on a variety of creation andmodification tasks to show that it can produce and edit diverse objects, tools,and scenes. Finally, we conducted a usability study (N=11) with a diverse setthat revealed participants had positive experiences with the system and woulduse it again.</description><author>Fernanda De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-Fahey, Judith Amores Fernandez, Jaron Lanier</author><pubDate>Fri, 22 Mar 2024 18:28:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12276v3</guid></item><item><title>From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity</title><link>http://arxiv.org/abs/2309.16512v4</link><description>In this paper, we introduce a novel analysis of neural networks based ongeometric (Clifford) algebra and convex optimization. We show that optimalweights of deep ReLU neural networks are given by the wedge product of trainingsamples when trained with standard regularized loss. Furthermore, the trainingproblem reduces to convex optimization over wedge product features, whichencode the geometric structure of the training dataset. This structure is givenin terms of signed volumes of triangles and parallelotopes generated by datavectors. The convex problem finds a small subset of samples via $\ell_1$regularization to discover only relevant wedge product features. Our analysisprovides a novel perspective on the inner workings of deep neural networks andsheds light on the role of the hidden layers.</description><author>Mert Pilanci</author><pubDate>Fri, 22 Mar 2024 18:26:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16512v4</guid></item><item><title>CoLLEGe: Concept Embedding Generation for Large Language Models</title><link>http://arxiv.org/abs/2403.15362v1</link><description>Current language models are unable to quickly learn new concepts on the fly,often requiring a more involved finetuning process to learn robustly. Promptingin-context is not robust to context distractions, and often fails to confermuch information about the new concepts. Classic methods for few-shot wordlearning in NLP, relying on global word vectors, are less applicable to largelanguage models. In this paper, we introduce a novel approach named CoLLEGe(Concept Learning with Language Embedding Generation) to modernize few-shotconcept learning. CoLLEGe is a meta-learning framework capable of generatingflexible embeddings for new concepts using a small number of example sentencesor definitions. Our primary meta-learning objective is simply to facilitate alanguage model to make next word predictions in forthcoming sentences, makingit compatible with language model pretraining. We design a series of tasks totest new concept learning in challenging real-world scenarios, including newword acquisition, definition inference, and verbal reasoning, and demonstratethat our method succeeds in each setting without task-specific training.</description><author>Ryan Teehan, Brenden Lake, Mengye Ren</author><pubDate>Fri, 22 Mar 2024 18:26:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15362v1</guid></item><item><title>Learning Topological Representations for Deep Image Understanding</title><link>http://arxiv.org/abs/2403.15361v1</link><description>In many scenarios, especially biomedical applications, the correctdelineation of complex fine-scaled structures such as neurons, tissues, andvessels is critical for downstream analysis. Despite the strong predictivepower of deep learning methods, they do not provide a satisfactoryrepresentation of these structures, thus creating significant barriers inscalable annotation and downstream analysis. In this dissertation, we tacklesuch challenges by proposing novel representations of these topologicalstructures in a deep learning framework. We leverage the mathematical toolsfrom topological data analysis, i.e., persistent homology and discrete Morsetheory, to develop principled methods for better segmentation and uncertaintyestimation, which will become powerful tools for scalable annotation.</description><author>Xiaoling Hu</author><pubDate>Fri, 22 Mar 2024 18:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15361v1</guid></item><item><title>SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series</title><link>http://arxiv.org/abs/2403.15360v1</link><description>Transformers have widely adopted attention networks for sequence mixing andMLPs for channel mixing, playing a pivotal role in achieving breakthroughsacross domains. However, recent literature highlights issues with attentionnetworks, including low inductive bias and quadratic complexity concerninginput sequence length. State Space Models (SSMs) like S4 and others (Hippo,Global Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to addressthe above issues to help handle longer sequence lengths. Mamba, while being thestate-of-the-art SSM, has a stability issue when scaled to large networks forcomputer vision datasets. We propose SiMBA, a new architecture that introducesEinstein FFT (EinFFT) for channel modeling by specific eigenvalue computationsand uses the Mamba block for sequence modeling. Extensive performance studiesacross image and time-series benchmarks demonstrate that SiMBA outperformsexisting SSMs, bridging the performance gap with state-of-the-art transformers.Notably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNetand transfer learning benchmarks such as Stanford Car and Flower as well astask learning benchmarks as well as seven time series benchmark datasets. Theproject page is available on this website~\url{https://github.com/badripatro/Simba}.</description><author>Badri N. Patro, Vijay S. Agneeswaran</author><pubDate>Fri, 22 Mar 2024 18:22:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15360v1</guid></item><item><title>Building Efficient Universal Classifiers with Natural Language Inference</title><link>http://arxiv.org/abs/2312.17543v2</link><description>Generative Large Language Models (LLMs) have become the mainstream choice forfewshot and zeroshot learning thanks to the universality of text generation.Many users, however, do not need the broad capabilities of generative LLMs whenthey only want to automate a classification task. Smaller BERT-like models canalso learn universal tasks, which allow them to do any text classification taskwithout requiring fine-tuning (zeroshot classification) or to learn new taskswith only a few examples (fewshot), while being significantly more efficientthan generative LLMs. This paper (1) explains how Natural Language Inference(NLI) can be used as a universal classification task that follows similarprinciples as instruction fine-tuning of generative LLMs, (2) provides astep-by-step guide with reusable Jupyter notebooks for building a universalclassifier, and (3) shares the resulting universal classifier that is trainedon 33 datasets with 389 diverse classes. Parts of the code we share has beenused to train our older zeroshot classifiers that have been downloaded morethan 55 million times via the Hugging Face Hub as of December 2023. Our newclassifier improves zeroshot performance by 9.4%.</description><author>Moritz Laurer, Wouter van Atteveldt, Andreu Casas, Kasper Welbers</author><pubDate>Fri, 22 Mar 2024 18:12:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17543v2</guid></item><item><title>Neural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities</title><link>http://arxiv.org/abs/2403.15356v1</link><description>The development of foundation models has revolutionized our ability tointerpret the Earth's surface using satellite observational data. Traditionalmodels have been siloed, tailored to specific sensors or data types likeoptical, radar, and hyperspectral, each with its own unique characteristics.This specialization hinders the potential for a holistic analysis that couldbenefit from the combined strengths of these diverse data sources. Our novelapproach introduces the Dynamic One-For-All (DOFA) model, leveraging theconcept of neural plasticity in brain science to integrate various datamodalities into a single framework adaptively. This dynamic hypernetwork,adjusting to different wavelengths, enables a single versatile Transformerjointly trained on data from five sensors to excel across 12 distinct Earthobservation tasks, including sensors never seen during pretraining. DOFA'sinnovative design offers a promising leap towards more accurate, efficient, andunified Earth observation analysis, showcasing remarkable adaptability andperformance in harnessing the potential of multimodal Earth observation data.</description><author>Zhitong Xiong, Yi Wang, Fahong Zhang, Adam J. Stewart, Joëlle Hanna, Damian Borth, Ioannis Papoutsis, Bertrand Le Saux, Gustau Camps-Valls, Xiao Xiang Zhu</author><pubDate>Fri, 22 Mar 2024 18:11:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15356v1</guid></item><item><title>Fully automated workflow for the design of patient-specific orthopaedic implants: application to total knee arthroplasty</title><link>http://arxiv.org/abs/2403.15353v1</link><description>Arthroplasty is commonly performed to treat joint osteoarthritis, reducingpain and improving mobility. While arthroplasty has known several technicalimprovements, a significant share of patients are still unsatisfied with theirsurgery. Personalised arthroplasty improves surgical outcomes however currentsolutions require delays, making it difficult to integrate in clinical routine.We propose a fully automated workflow to design patient-specific implants,presented for total knee arthroplasty, the most widely performed arthroplastyin the world nowadays. The proposed pipeline first uses artificial neural networks to segment theproximal and distal extremities of the femur and tibia. Then the full bones arereconstructed using augmented statistical shape models, combining shape andlandmarks information. Finally, 77 morphological parameters are computed todesign patient-specific implants. The developed workflow has been trained using91 CT scans of lower limb and evaluated on 41 CT scans manually segmented, interms of accuracy and execution time. The workflow accuracy was $0.4\pm0.2mm$ for the segmentation, $1.2\pm0.4mm$for the full bones reconstruction, and $2.8\pm2.2mm$ for the anatomicallandmarks determination. The custom implants fitted the patients' anatomy with$0.6\pm0.2mm$ accuracy. The whole process from segmentation to implants' designlasted about 5 minutes. The proposed workflow allows for a fast and reliable personalisation of kneeimplants, directly from the patient CT image without requiring any manualintervention. It establishes a patient-specific pre-operative planning for TKAin a very short time making it easily available for all patients. Combined withefficient implant manufacturing techniques, this solution could help answer thegrowing number of arthroplasties while reducing complications and improving thepatients' satisfaction.</description><author>Aziliz Guezou-Philippe, Arnaud Clavé, Ehouarn Maguet, Ludivine Maintier, Charles Garraud, Jean-Rassaire Fouefack, Valérie Burdin, Eric Stindel, Guillaume Dardenne</author><pubDate>Fri, 22 Mar 2024 18:08:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15353v1</guid></item><item><title>VideoPoet: A Large Language Model for Zero-Shot Video Generation</title><link>http://arxiv.org/abs/2312.14125v3</link><description>We present VideoPoet, a language model capable of synthesizing high-qualityvideo, with matching audio, from a large variety of conditioning signals.VideoPoet employs a decoder-only transformer architecture that processesmultimodal inputs -- including images, videos, text, and audio. The trainingprotocol follows that of Large Language Models (LLMs), consisting of twostages: pretraining and task-specific adaptation. During pretraining, VideoPoetincorporates a mixture of multimodal generative objectives within anautoregressive Transformer framework. The pretrained LLM serves as a foundationthat can be adapted for a range of video generation tasks. We present empiricalresults demonstrating the model's state-of-the-art capabilities in zero-shotvideo generation, specifically highlighting VideoPoet's ability to generatehigh-fidelity motions. Project page: http://sites.research.google/videopoet/</description><author>Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Josh Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, Lu Jiang</author><pubDate>Fri, 22 Mar 2024 18:06:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14125v3</guid></item><item><title>Multi-Review Fusion-in-Context</title><link>http://arxiv.org/abs/2403.15351v1</link><description>Grounded text generation, encompassing tasks such as long-formquestion-answering and summarization, necessitates both content selection andcontent consolidation. Current end-to-end methods are difficult to control andinterpret due to their opaqueness. Accordingly, recent works have proposed amodular approach, with separate components for each step. Specifically, wefocus on the second subtask, of generating coherent text given pre-selectedcontent in a multi-document setting. Concretely, we formalize\textit{Fusion-in-Context} (FiC) as a standalone task, whose input consists ofsource texts with highlighted spans of targeted content. A model then needs togenerate a coherent passage that includes all and only the target information.Our work includes the development of a curated dataset of 1000 instances in thereviews domain, alongside a novel evaluation framework for assessing thefaithfulness and coverage of highlights, which strongly correlate to humanjudgment. Several baseline models exhibit promising outcomes and provideinsightful analyses. This study lays the groundwork for further exploration ofmodular text generation in the multi-document setting, offering potentialimprovements in the quality and reliability of generated content. \footnote{Ourbenchmark, FuseReviews, including the dataset, evaluation framework anddesignated leaderboard, can be found at \url{https://fusereviews.github.io/}.}</description><author>Aviv Slobodkin, Ori Shapira, Ran Levy, Ido Dagan</author><pubDate>Fri, 22 Mar 2024 18:06:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15351v1</guid></item><item><title>MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training</title><link>http://arxiv.org/abs/2403.09611v3</link><description>In this work, we discuss building performant Multimodal Large Language Models(MLLMs). In particular, we study the importance of various architecturecomponents and data choices. Through careful and comprehensive ablations of theimage encoder, the vision language connector, and various pre-training datachoices, we identified several crucial design lessons. For example, wedemonstrate that for large-scale multimodal pre-training using a careful mix ofimage-caption, interleaved image-text, and text-only data is crucial forachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,compared to other published pre-training results. Further, we show that theimage encoder together with image resolution and the image token count hassubstantial impact, while the vision-language connector design is ofcomparatively negligible importance. By scaling up the presented recipe, webuild MM1, a family of multimodal models up to 30B parameters, including bothdense models and mixture-of-experts (MoE) variants, that are SOTA inpre-training metrics and achieve competitive performance after supervisedfine-tuning on a range of established multimodal benchmarks. Thanks tolarge-scale pre-training, MM1 enjoys appealing properties such as enhancedin-context learning, and multi-image reasoning, enabling few-shotchain-of-thought prompting.</description><author>Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang</author><pubDate>Fri, 22 Mar 2024 18:03:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09611v3</guid></item><item><title>Attractor reconstruction with reservoir computers: The effect of the reservoir's conditional Lyapunov exponents on faithful attractor reconstruction</title><link>http://arxiv.org/abs/2401.00885v2</link><description>Reservoir computing is a machine learning framework that has been shown to beable to replicate the chaotic attractor, including the fractal dimension andthe entire Lyapunov spectrum, of the dynamical system on which it is trained.We quantitatively relate the generalized synchronization dynamics of a drivenreservoir during the training stage to the performance of the trained reservoircomputer at the attractor reconstruction task. We show that, in order to obtainsuccessful attractor reconstruction and Lyapunov spectrum estimation, thelargest conditional Lyapunov exponent of the driven reservoir must besignificantly more negative than the most negative Lyapunov exponent of thetarget system. We also find that the maximal conditional Lyapunov exponent ofthe reservoir depends strongly on the spectral radius of the reservoiradjacency matrix, and therefore, for attractor reconstruction and Lyapunovspectrum estimation, small spectral radius reservoir computers perform betterin general. Our arguments are supported by numerical examples on well-knownchaotic systems.</description><author>Joseph D. Hart</author><pubDate>Fri, 22 Mar 2024 17:57:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00885v2</guid></item><item><title>Collaborative AI Teaming in Unknown Environments via Active Goal Deduction</title><link>http://arxiv.org/abs/2403.15341v1</link><description>With the advancements of artificial intelligence (AI), we're seeing morescenarios that require AI to work closely with other agents, whose goals andstrategies might not be known beforehand. However, existing approaches fortraining collaborative agents often require defined and known reward signalsand cannot address the problem of teaming with unknown agents that often havelatent objectives/rewards. In response to this challenge, we propose teamingwith unknown agents framework, which leverages kernel density Bayesian inverselearning method for active goal deduction and utilizes pre-trained,goal-conditioned policies to enable zero-shot policy adaptation. We prove thatunbiased reward estimates in our framework are sufficient for optimal teamingwith unknown agents. We further evaluate the framework of redesignedmulti-agent particle and StarCraft II micromanagement environments with diverseunknown agents of different behaviors/rewards. Empirical results demonstratethat our framework significantly advances the teaming performance of AI andunknown agents in a wide range of collaborative scenarios.</description><author>Zuyuan Zhang, Hanhan Zhou, Mahdi Imani, Taeyoung Lee, Tian Lan</author><pubDate>Fri, 22 Mar 2024 17:50:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15341v1</guid></item><item><title>Learning to Predict Structural Vibrations</title><link>http://arxiv.org/abs/2310.05469v3</link><description>In mechanical structures like airplanes, cars and houses, noise is generatedand transmitted through vibrations. To take measures to reduce this noise,vibrations need to be simulated with expensive numerical computations.Surrogate deep learning models present a promising alternative to classicalnumerical simulations as they can be evaluated magnitudes faster, whiletrading-off accuracy. To quantify such trade-offs systematically and foster thedevelopment of methods, we present a benchmark on the task of predicting thevibration of harmonically excited plates. The benchmark features a total of12000 plate geometries with varying forms of beadings, material and sizes withassociated numerical solutions. To address the benchmark task, we propose a newnetwork architecture, named Frequency-Query Operator, which is trained to mapplate geometries to their vibration pattern given a specific excitationfrequency. Applying principles from operator learning and implicit models forshape encoding, our approach effectively addresses the prediction of highlyvariable frequency response functions occurring in dynamic systems. To quantifythe prediction quality, we introduce a set of evaluation metrics and evaluatethe method on our vibrating-plates benchmark. Our method outperforms DeepONets,Fourier Neural Operators and more traditional neural network architectures.Code, dataset and visualizations: https://eckerlab.org/code/delden2023_plate</description><author>Jan van Delden, Julius Schultz, Christopher Blech, Sabine C. Langer, Timo Lüddecke</author><pubDate>Fri, 22 Mar 2024 17:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05469v3</guid></item><item><title>SkySense: A Multi-Modal Remote Sensing Foundation Model Towards Universal Interpretation for Earth Observation Imagery</title><link>http://arxiv.org/abs/2312.10115v2</link><description>Prior studies on Remote Sensing Foundation Model (RSFM) reveal immensepotential towards a generic model for Earth Observation. Nevertheless, theseworks primarily focus on a single modality without temporal and geo-contextmodeling, hampering their capabilities for diverse tasks. In this study, wepresent SkySense, a generic billion-scale model, pre-trained on a curatedmulti-modal Remote Sensing Imagery (RSI) dataset with 21.5 million temporalsequences. SkySense incorporates a factorized multi-modal spatiotemporalencoder taking temporal sequences of optical and Synthetic Aperture Radar (SAR)data as input. This encoder is pre-trained by our proposed Multi-GranularityContrastive Learning to learn representations across different modal andspatial granularities. To further enhance the RSI representations by thegeo-context clue, we introduce Geo-Context Prototype Learning to learnregion-aware prototypes upon RSI's multi-modal spatiotemporal features. To ourbest knowledge, SkySense is the largest Multi-Modal RSFM to date, whose modulescan be flexibly combined or used individually to accommodate various tasks. Itdemonstrates remarkable generalization capabilities on a thorough evaluationencompassing 16 datasets over 7 tasks, from single- to multi-modal, static totemporal, and classification to localization. SkySense surpasses 18 recentRSFMs in all test scenarios. Specifically, it outperforms the latest modelssuch as GFM, SatLas and Scale-MAE by a large margin, i.e., 2.76%, 3.67% and3.61% on average respectively. We will release the pre-trained weights tofacilitate future research and Earth Observation applications.</description><author>Xin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang, Kang Wu, Dingxiang Hu, Huimei He, Jian Wang, Jingdong Chen, Ming Yang, Yongjun Zhang, Yansheng Li</author><pubDate>Fri, 22 Mar 2024 17:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10115v2</guid></item><item><title>Fast ODE-based Sampling for Diffusion Models in Around 5 Steps</title><link>http://arxiv.org/abs/2312.00094v2</link><description>Sampling from diffusion models can be treated as solving the correspondingordinary differential equations (ODEs), with the aim of obtaining an accuratesolution with as few number of function evaluations (NFE) as possible.Recently, various fast samplers utilizing higher-order ODE solvers have emergedand achieved better performance than the initial first-order one. However,these numerical methods inherently result in certain approximation errors,which significantly degrades sample quality with extremely small NFE (e.g.,around 5). In contrast, based on the geometric observation that each samplingtrajectory almost lies in a two-dimensional subspace embedded in the ambientspace, we propose Approximate MEan-Direction Solver (AMED-Solver) thateliminates truncation errors by directly learning the mean direction for fastdiffusion sampling. Besides, our method can be easily used as a plugin tofurther improve existing ODE-based samplers. Extensive experiments on imagesynthesis with the resolution ranging from 32 to 512 demonstrate theeffectiveness of our method. With only 5 NFE, we achieve 6.61 FID on CIFAR-10,10.74 FID on ImageNet 64$\times$64, and 13.20 FID on LSUN Bedroom. Our code isavailable at https://github.com/zju-pi/diff-sampler.</description><author>Zhenyu Zhou, Defang Chen, Can Wang, Chun Chen</author><pubDate>Fri, 22 Mar 2024 17:38:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00094v2</guid></item><item><title>Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference</title><link>http://arxiv.org/abs/2403.14520v2</link><description>In recent years, the application of multimodal large language models (MLLM)in various fields has achieved remarkable success. However, as the foundationmodel for many downstream tasks, current MLLMs are composed of the well-knownTransformer network, which has a less efficient quadratic computationcomplexity. To improve the efficiency of such basic models, we propose Cobra, alinear computational complexity MLLM. Specifically, Cobra integrates theefficient Mamba language model into the visual modality. Moreover, we exploreand study various modal fusion schemes to create an effective multi-modalMamba. Extensive experiments demonstrate that (1) Cobra achieves extremelycompetitive performance with current computationally efficient state-of-the-artmethods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed dueto Cobra's linear sequential modeling. (2) Interestingly, the results ofclosed-set challenging prediction benchmarks show that Cobra performs well inovercoming visual illusions and spatial relationship judgments. (3) Notably,Cobra even achieves comparable performance to LLaVA with about 43% of thenumber of parameters. We will make all codes of Cobra open-source and hope thatthe proposed method can facilitate future research on complexity problems inMLLM. Our project page is available at: https://sites.google.com/view/cobravlm.</description><author>Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, Donglin Wang</author><pubDate>Fri, 22 Mar 2024 17:35:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14520v2</guid></item><item><title>Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization</title><link>http://arxiv.org/abs/2403.15330v1</link><description>In text-to-image personalization, a timely and crucial challenge is thetendency of generated images overfitting to the biases present in the referenceimages. We initiate our study with a comprehensive categorization of the biasesinto background, nearby-object, tied-object, substance (in stylere-contextualization), and pose biases. These biases manifest in the generatedimages due to their entanglement into the subject embedding. This undesiredembedding entanglement not only results in the reflection of biases from thereference images into the generated images but also notably diminishes thealignment of the generated images with the given generation prompt. To addressthis challenge, we propose SID~(Selectively Informative Description), a textdescription strategy that deviates from the prevalent approach of onlycharacterizing the subject's class identification. SID is generated utilizingmultimodal GPT-4 and can be seamlessly integrated into optimization-basedmodels. We present comprehensive experimental results along with analyses ofcross-attention maps, subject-alignment, non-subject-disentanglement, andtext-alignment.</description><author>Jimyeong Kim, Jungwon Park, Wonjong Rhee</author><pubDate>Fri, 22 Mar 2024 17:35:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15330v1</guid></item><item><title>Learning High-level Semantic-Relational Concepts for SLAM</title><link>http://arxiv.org/abs/2310.00401v2</link><description>Recent works on SLAM extend their pose graphs with higher-level semanticconcepts like Rooms exploiting relationships between them, to provide, not onlya richer representation of the situation/environment but also to improve theaccuracy of its estimation. Concretely, our previous work, Situational Graphs(S-Graphs+), a pioneer in jointly leveraging semantic relationships in thefactor optimization process, relies on semantic entities such as Planes andRooms, whose relationship is mathematically defined. Nevertheless, there is nounique approach to finding all the hidden patterns in lower-level factor-graphsthat correspond to high-level concepts of different natures. It is currentlytackled with ad-hoc algorithms, which limits its graph expressiveness. To overcome this limitation, in this work, we propose an algorithm based onGraph Neural Networks for learning high-level semantic-relational concepts thatcan be inferred from the low-level factor graph. Given a set of mapped Planesour algorithm is capable of inferring Room entities relating to the Planes.Additionally, to demonstrate the versatility of our method, our algorithm caninfer an additional semantic-relational concept, i.e. Wall, and itsrelationship with its Planes. We validate our method in both simulated and realdatasets demonstrating improved performance over two baseline approaches.Furthermore, we integrate our method into the S-Graphs+ algorithm providingimproved pose and map accuracy compared to the baseline while further enhancingthe scene representation.</description><author>Jose Andres Millan-Romera, Hriday Bavle, Muhammad Shaheer, Martin R. Oswald, Holger Voos, Jose Luis Sanchez-Lopez</author><pubDate>Fri, 22 Mar 2024 17:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00401v2</guid></item><item><title>A Technological Perspective on Misuse of Available AI</title><link>http://arxiv.org/abs/2403.15325v1</link><description>Potential malicious misuse of civilian artificial intelligence (AI) posesserious threats to security on a national and international level. Besidesdefining autonomous systems from a technological viewpoint and explaining howAI development is characterized, we show how already existing and openlyavailable AI technology could be misused. To underline this, we developed threeexemplary use cases of potentially misused AI that threaten political, digitaland physical security. The use cases can be built from existing AI technologiesand components from academia, the private sector and the developer-community.This shows how freely available AI can be combined into autonomous weaponsystems. Based on the use cases, we deduce points of control and furthermeasures to prevent the potential threat through misused AI. Further, wepromote the consideration of malicious misuse of civilian AI systems in thediscussion on autonomous weapon systems (AWS).</description><author>Lukas Pöhler, Valentin Schrader, Alexander Ladwein, Florian von Keller</author><pubDate>Fri, 22 Mar 2024 17:30:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15325v1</guid></item><item><title>Novelty Detection in Reinforcement Learning with World Models</title><link>http://arxiv.org/abs/2310.08731v2</link><description>Reinforcement learning (RL) using world models has found significant recentsuccesses. However, when a sudden change to world mechanics or propertiesoccurs then agent performance and reliability can dramatically decline. Werefer to the sudden change in visual properties or state transitions asnovelties. Implementing novelty detection within generated world modelframeworks is a crucial task for protecting the agent when deployed. In thispaper, we propose straightforward bounding approaches to incorporate noveltydetection into world model RL agents, by utilizing the misalignment of theworld model's hallucinated states and the true observed states as an anomalyscore. We provide effective approaches to detecting novelties in a distributionof transitions learned by an agent in a world model. Finally, we show theadvantage of our work in a novel environment compared to traditional machinelearning novelty detection methods as well as currently accepted RL focusednovelty detection algorithms.</description><author>Geigh Zollicoffer, Kenneth Eaton, Jonathan Balloch, Julia Kim, Mark O. Riedl, Robert Wright</author><pubDate>Fri, 22 Mar 2024 17:30:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08731v2</guid></item><item><title>Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level</title><link>http://arxiv.org/abs/2403.04690v2</link><description>Neighborhood attention reduces the cost of self attention by restricting eachtoken's attention span to its nearest neighbors. This restriction,parameterized by a window size and dilation factor, draws a spectrum ofpossible attention patterns between linear projection and self attention.Neighborhood attention, and more generally sliding window attention patterns,have long been bounded by infrastructure, particularly in higher-rank spaces(2-D and 3-D), calling for the development of custom kernels, which have beenlimited in either functionality, or performance, if not both. In this work, wefirst show that neighborhood attention can be represented as a batched GEMMproblem, similar to standard attention, and implement it for 1-D and 2-Dneighborhood attention. These kernels on average provide 895% and 272%improvement in full precision latency compared to existing naive kernels for1-D and 2-D neighborhood attention respectively. We find certain inherentinefficiencies in all unfused neighborhood attention kernels that bound theirperformance and lower-precision scalability. We also developed fusedneighborhood attention; an adaptation of fused dot-product attention kernelsthat allow fine-grained control over attention across different spatial axes.Known for reducing the quadratic time complexity of self attention to a linearcomplexity, neighborhood attention can now enjoy a reduced and constant memoryfootprint, and record-breaking half precision latency. We observe that ourfused kernels successfully circumvent some of the unavoidable inefficiencies inunfused implementations. While our unfused GEMM-based kernels only improve halfprecision performance compared to naive kernels by an average of 496% and 113%in 1-D and 2-D problems respectively, our fused kernels improve naive kernelsby an average of 1607% and 581% in 1-D and 2-D problems respectively.</description><author>Ali Hassani, Wen-Mei Hwu, Humphrey Shi</author><pubDate>Fri, 22 Mar 2024 17:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04690v2</guid></item><item><title>Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning</title><link>http://arxiv.org/abs/2402.07586v2</link><description>In the evolving field of machine learning, ensuring fairness has become acritical concern, prompting the development of algorithms designed to mitigatediscriminatory outcomes in decision-making processes. However, achievingfairness in the presence of group-specific concept drift remains an unexploredfrontier, and our research represents pioneering efforts in this regard.Group-specific concept drift refers to situations where one group experiencesconcept drift over time while another does not, leading to a decrease infairness even if accuracy remains fairly stable. Within the framework offederated learning, where clients collaboratively train models, its distributednature further amplifies these challenges since each client can experiencegroup-specific concept drift independently while still sharing the sameunderlying concept, creating a complex and dynamic environment for maintainingfairness. One of the significant contributions of our research is theformalization and introduction of the problem of group-specific concept driftand its distributed counterpart, shedding light on its critical importance inthe realm of fairness. In addition, leveraging insights from prior research, weadapt an existing distributed concept drift adaptation algorithm to tacklegroup-specific distributed concept drift which utilizes a multi-model approach,a local group-specific drift detection mechanism, and continuous clustering ofmodels over time. The findings from our experiments highlight the importance ofaddressing group-specific concept drift and its distributed counterpart toadvance fairness in machine learning.</description><author>Teresa Salazar, João Gama, Helder Araújo, Pedro Henriques Abreu</author><pubDate>Fri, 22 Mar 2024 17:25:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07586v2</guid></item><item><title>CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for Named Entity Recognition and Relation Extraction</title><link>http://arxiv.org/abs/2403.15322v1</link><description>The process of cyber mapping gives insights in relationships among financialentities and service providers. Centered around the outsourcing practices ofcompanies within fund prospectuses in Germany, we introduce a datasetspecifically designed for named entity recognition and relation extractiontasks. The labeling process on 948 sentences was carried out by three expertswhich yields to 5,969 annotations for four entity types (Outsourcing, Company,Location and Software) and 4,102 relation annotations (Outsourcing-Company,Company-Location). State-of-the-art deep learning models were trained torecognize entities and extract relations showing first promising results. Ananonymized version of the dataset, along with guidelines and the code used formodel training, are publicly available athttps://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip.</description><author>Neda Foroutan, Markus Schröder, Andreas Dengel</author><pubDate>Fri, 22 Mar 2024 17:17:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15322v1</guid></item><item><title>Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection</title><link>http://arxiv.org/abs/2403.15317v1</link><description>Training high-accuracy 3D detectors necessitates massive labeled 3Dannotations with 7 degree-of-freedom, which is laborious and time-consuming.Therefore, the form of point annotations is proposed to offer significantprospects for practical applications in 3D detection, which is not only moreaccessible and less expensive but also provides strong spatial information forobject localization.In this paper, we empirically discover that it isnon-trivial to merely adapt Point-DETR to its 3D form, encountering two mainbottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) itgenerates low-quality pseudo labels in distant regions due to the extremesparsity of LiDAR points. To overcome these challenges, we introducePoint-DETR3D, a teacher-student framework for weakly semi-supervised 3Ddetection, designed to fully capitalize on point-wise supervision within aconstrained instance-wise annotation budget.Different from Point-DETR whichencodes 3D positional information solely through a point encoder, we propose anexplicit positional query initialization strategy to enhance the positionalprior. Considering the low quality of pseudo labels at distant regions producedby the teacher model, we enhance the detector's perception by incorporatingdense imagery data through a novel Cross-Modal Deformable RoI Fusion(D-RoI).Moreover, an innovative point-guided self-supervised learning techniqueis proposed to allow for fully exploiting point priors, even in studentmodels.Extensive experiments on representative nuScenes dataset demonstrate ourPoint-DETR3D obtains significant improvements compared to previous works.Notably, with only 5% of labeled data, Point-DETR3D achieves over 90%performance of its fully supervised counterpart.</description><author>Hongzhi Gao, Zheng Chen, Zehui Chen, Lin Chen, Jiaming Liu, Shanghang Zhang, Feng Zhao</author><pubDate>Fri, 22 Mar 2024 17:11:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15317v1</guid></item><item><title>Ultrasound Imaging based on the Variance of a Diffusion Restoration Model</title><link>http://arxiv.org/abs/2403.15316v1</link><description>Despite today's prevalence of ultrasound imaging in medicine, ultrasoundsignal-to-noise ratio is still affected by several sources of noise andartefacts. Moreover, enhancing ultrasound image quality involves balancingconcurrent factors like contrast, resolution, and speckle preservation.Recently, there has been progress in both model-based and learning-basedapproaches addressing the problem of ultrasound image reconstruction. Bringingthe best from both worlds, we propose a hybrid reconstruction method combiningan ultrasound linear direct model with a learning-based prior coming from agenerative Denoising Diffusion model. More specifically, we rely on theunsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model(DDRM). Given the nature of multiplicative noise inherent to ultrasound, thispaper proposes an empirical model to characterize the stochasticity ofdiffusion reconstruction of ultrasound images, and shows the interest of itsvariance as an echogenicity map estimator. We conduct experiments on synthetic,in-vitro, and in-vivo data, demonstrating the efficacy of our variance imagingapproach in achieving high-quality image reconstructions from single plane-waveacquisitions and in comparison to state-of-the-art methods.</description><author>Yuxin Zhang, Clément Huneau, Jérôme Idier, Diana Mateus</author><pubDate>Fri, 22 Mar 2024 17:10:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15316v1</guid></item><item><title>The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun</title><link>http://arxiv.org/abs/2402.10311v5</link><description>The word order of a sentence is shaped by multiple principles. The principleof syntactic dependency distance minimization is in conflict with the principleof surprisal minimization (or predictability maximization) in single headsyntactic dependency structures: while the former predicts that the head shouldbe placed at the center of the linear arrangement, the latter predicts that thehead should be placed at one of the ends (either first or last). A criticalquestion is when surprisal minimization (or predictability maximization) shouldsurpass syntactic dependency distance minimization. In the context of singlehead structures, it has been predicted that this is more likely to happen whentwo conditions are met, i.e. (a) fewer words are involved and (b) words areshorter. Here we test the prediction on the noun phrase when it is composed ofa demonstrative, a numeral, an adjective and a noun. We find that, acrosspreferred orders in languages, the noun tends to be placed at one of the ends,confirming the theoretical prediction. We also show evidence of anti localityeffects: syntactic dependency distances in preferred orders are longer thanexpected by chance.</description><author>Ramon Ferrer-i-Cancho</author><pubDate>Fri, 22 Mar 2024 17:10:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10311v5</guid></item><item><title>Global Control for Local SO(3)-Equivariant Scale-Invariant Vessel Segmentation</title><link>http://arxiv.org/abs/2403.15314v1</link><description>Personalized 3D vascular models can aid in a range of diagnostic, prognostic,and treatment-planning tasks relevant to cardiovascular disease management.Deep learning provides a means to automatically obtain such models. Ideally, auser should have control over the exact region of interest (ROI) to be includedin a vascular model, and the model should be watertight and highly accurate. Tothis end, we propose a combination of a global controller leveraging voxel masksegmentations to provide boundary conditions for vessels of interest to alocal, iterative vessel segmentation model. We introduce the preservation ofscale- and rotational symmetries in the local segmentation model, leading togeneralisation to vessels of unseen sizes and orientations. Combined with theglobal controller, this enables flexible 3D vascular model building, withoutadditional retraining. We demonstrate the potential of our method on a datasetcontaining abdominal aortic aneurysms (AAAs). Our method performs on par with astate-of-the-art segmentation model in the segmentation of AAAs, iliac arteriesand renal arteries, while providing a watertight, smooth surface segmentation.Moreover, we demonstrate that by adapting the global controller, we can easilyextend vessel sections in the 3D model.</description><author>Patryk Rygiel, Dieuwertje Alblas, Christoph Brune, Kak Khee Yeung, Jelmer M. Wolterink</author><pubDate>Fri, 22 Mar 2024 17:06:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15314v1</guid></item><item><title>Recurrent Drafter for Fast Speculative Decoding in Large Language Models</title><link>http://arxiv.org/abs/2403.09919v2</link><description>In this paper, we introduce an improved approach of speculative decodingaimed at enhancing the efficiency of serving large language models. Our methodcapitalizes on the strengths of two established techniques: the classictwo-model speculative decoding approach, and the more recent single-modelapproach, Medusa. Drawing inspiration from Medusa, our approach adopts asingle-model strategy for speculative decoding. However, our methoddistinguishes itself by employing a single, lightweight draft head with arecurrent dependency design, akin in essence to the small, draft model uses inclassic speculative decoding, but without the complexities of the fulltransformer architecture. And because of the recurrent dependency, we can usebeam search to swiftly filter out undesired candidates with the draft head. Theoutcome is a method that combines the simplicity of single-model design andavoids the need to create a data-dependent tree attention structure only forinference in Medusa. We empirically demonstrate the effectiveness of theproposed method on several popular open source language models, along with acomprehensive analysis of the trade-offs involved in adopting this approach.</description><author>Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng</author><pubDate>Fri, 22 Mar 2024 17:06:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09919v2</guid></item><item><title>CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking</title><link>http://arxiv.org/abs/2403.15313v1</link><description>Accurate detection and tracking of surrounding objects is essential to enableself-driving vehicles. While Light Detection and Ranging (LiDAR) sensors haveset the benchmark for high performance, the appeal of camera-only solutionslies in their cost-effectiveness. Notably, despite the prevalent use of RadioDetection and Ranging (RADAR) sensors in automotive systems, their potential in3D detection and tracking has been largely disregarded due to data sparsity andmeasurement noise. As a recent development, the combination of RADARs andcameras is emerging as a promising solution. This paper presents Camera-RADAR3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D objectdetection, and Multi-Object Tracking (MOT). Building upon the foundations ofthe State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstratessubstantial improvements in both detection and tracking capabilities, byincorporating the spatial and velocity information of the RADAR sensor.Experimental results demonstrate an absolute improvement in detectionperformance of 5.3% in mean Average Precision (mAP) and a 14.9% increase inAverage Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset whenleveraging both modalities. CR3DT bridges the gap between high-performance andcost-effective perception systems in autonomous driving, by capitalizing on theubiquitous presence of RADAR in automotive applications.</description><author>Nicolas Baumann, Michael Baumgartner, Edoardo Ghignone, Jonas Kühne, Tobias Fischer, Yung-Hsu Yang, Marc Pollefeys, Michele Magno</author><pubDate>Fri, 22 Mar 2024 17:06:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15313v1</guid></item><item><title>A Wasserstein perspective of Vanilla GANs</title><link>http://arxiv.org/abs/2403.15312v1</link><description>The empirical success of Generative Adversarial Networks (GANs) caused anincreasing interest in theoretical research. The statistical literature ismainly focused on Wasserstein GANs and generalizations thereof, whichespecially allow for good dimension reduction properties. Statistical resultsfor Vanilla GANs, the original optimization problem, are still rather limitedand require assumptions such as smooth activation functions and equaldimensions of the latent space and the ambient space. To bridge this gap, wedraw a connection from Vanilla GANs to the Wasserstein distance. By doing so,existing results for Wasserstein GANs can be extended to Vanilla GANs. Inparticular, we obtain an oracle inequality for Vanilla GANs in Wassersteindistance. The assumptions of this oracle inequality are designed to besatisfied by network architectures commonly used in practice, such asfeedforward ReLU networks. By providing a quantitative result for theapproximation of a Lipschitz function by a feedforward ReLU network withbounded H\"older norm, we conclude a rate of convergence for Vanilla GANs aswell as Wasserstein GANs as estimators of the unknown probability distribution.</description><author>Lea Kunkel, Mathias Trabs</author><pubDate>Fri, 22 Mar 2024 17:04:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15312v1</guid></item><item><title>Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction</title><link>http://arxiv.org/abs/2403.10581v2</link><description>Heart failure (HF) poses a significant public health challenge, with a risingglobal mortality rate. Early detection and prevention of HF could significantlyreduce its impact. We introduce a novel methodology for predicting HF riskusing 12-lead electrocardiograms (ECGs). We present a novel, lightweightdual-attention ECG network designed to capture complex ECG features essentialfor early HF risk prediction, despite the notable imbalance between low andhigh-risk groups. This network incorporates a cross-lead attention module andtwelve lead-specific temporal attention modules, focusing on cross-leadinteractions and each lead's local dynamics. To further alleviate modeloverfitting, we leverage a large language model (LLM) with a public ECG-Reportdataset for pretraining on an ECG-report alignment task. The network is thenfine-tuned for HF risk prediction using two specific cohorts from the UKBiobank study, focusing on patients with hypertension (UKB-HYP) and those whohave had a myocardial infarction (UKB-MI).The results reveal that LLM-informedpre-training substantially enhances HF risk prediction in these cohorts. Thedual-attention design not only improves interpretability but also predictiveaccuracy, outperforming existing competitive methods with C-index scores of0.6349 for UKB-HYP and 0.5805 for UKB-MI. This demonstrates our method'spotential in advancing HF risk assessment with clinical complex ECG data.</description><author>Chen Chen, Lei Li, Marcel Beetz, Abhirup Banerjee, Ramneek Gupta, Vicente Grau</author><pubDate>Fri, 22 Mar 2024 17:00:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10581v2</guid></item><item><title>Controlled Training Data Generation with Diffusion Models</title><link>http://arxiv.org/abs/2403.15309v1</link><description>In this work, we present a method to control a text-to-image generative modelto produce training data specifically "useful" for supervised learning. Unlikeprevious works that employ an open-loop approach and pre-define prompts togenerate new data using either a language model or human expertise, we developan automated closed-loop system which involves two feedback mechanisms. Thefirst mechanism uses feedback from a given supervised model and findsadversarial prompts that result in image generations that maximize the modelloss. While these adversarial prompts result in diverse data informed by themodel, they are not informed of the target distribution, which can beinefficient. Therefore, we introduce the second feedback mechanism that guidesthe generation process towards a certain target distribution. We call themethod combining these two mechanisms Guided Adversarial Prompts. We performour evaluations on different tasks, datasets and architectures, with differenttypes of distribution shifts (spuriously correlated data, unseen domains) anddemonstrate the efficiency of the proposed feedback mechanisms compared toopen-loop approaches.</description><author>Teresa Yeo, Andrei Atanov, Harold Benoit, Aleksandr Alekseev, Ruchira Ray, Pooya Esmaeil Akhoondi, Amir Zamir</author><pubDate>Fri, 22 Mar 2024 16:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15309v1</guid></item><item><title>"This is not a data problem": Algorithms and Power in Public Higher Education in Canada</title><link>http://arxiv.org/abs/2403.13969v2</link><description>Algorithmic decision-making is increasingly being adopted across publichigher education. The expansion of data-driven practices by post-secondaryinstitutions has occurred in parallel with the adoption of New PublicManagement approaches by neoliberal administrations. In this study, we conducta qualitative analysis of an in-depth ethnographic case study of data andalgorithms in use at a public college in Ontario, Canada. We identify the data,algorithms, and outcomes in use at the college. We assess how the college'sprocesses and relationships support those outcomes and the differentstakeholders' perceptions of the college's data-driven systems. In addition, wefind that the growing reliance on algorithmic decisions leads to increasedstudent surveillance, exacerbation of existing inequities, and the automationof the faculty-student relationship. Finally, we identify a cycle of increasedinstitutional power perpetuated by algorithmic decision-making, and driven by apush towards financial sustainability.</description><author>Kelly McConvey, Shion Guha</author><pubDate>Fri, 22 Mar 2024 16:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13969v2</guid></item><item><title>KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing</title><link>http://arxiv.org/abs/2403.15304v1</link><description>Knowledge Tracing (KT) is concerned with predicting students' futureperformance on learning items in intelligent tutoring systems. Learning itemsare tagged with skill labels called knowledge concepts (KCs). Many KT modelsexpand the sequence of item-student interactions into KC-student interactionsby replacing learning items with their constituting KCs. This often results ina longer sequence length. This approach addresses the issue of sparseitem-student interactions and minimises model parameters. However, two problemshave been identified with such models. The first problem is the model's ability to learn correlations between KCsbelonging to the same item, which can result in the leakage of ground truthlabels and hinder performance. This problem can lead to a significant decreasein performance on datasets with a higher number of KCs per item. The secondproblem is that the available benchmark implementations ignore accounting forchanges in sequence length when expanding KCs, leading to different modelsbeing tested with varying sequence lengths but still compared against the samebenchmark. To address these problems, we introduce a general masking framework thatmitigates the first problem and enhances the performance of such KT modelswhile preserving the original model architecture without significantalterations. Additionally, we introduce KTbench, an open-source benchmarklibrary designed to ensure the reproducibility of this work while mitigatingthe second problem.</description><author>Yahya Badran, Christine Preisach</author><pubDate>Fri, 22 Mar 2024 16:54:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15304v1</guid></item><item><title>Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish</title><link>http://arxiv.org/abs/2403.00411v2</link><description>The rapid spread of misinformation through social media platforms has raisedconcerns regarding its impact on public opinion. While misinformation isprevalent in other languages, the majority of research in this field hasconcentrated on the English language. Hence, there is a scarcity of datasetsfor other languages, including Turkish. To address this concern, we haveintroduced the FCTR dataset, consisting of 3238 real-world claims. This datasetspans multiple domains and incorporates evidence collected from three Turkishfact-checking organizations. Additionally, we aim to assess the effectivenessof cross-lingual transfer learning for low-resource languages, with aparticular focus on Turkish. We demonstrate in-context learning (zero-shot andfew-shot) performance of large language models in this context. Theexperimental results indicate that the dataset has the potential to advanceresearch in the Turkish language.</description><author>Recep Firat Cekinel, Pinar Karagoz, Cagri Coltekin</author><pubDate>Fri, 22 Mar 2024 16:54:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00411v2</guid></item><item><title>LogPrécis: Unleashing Language Models for Automated Malicious Log Analysis</title><link>http://arxiv.org/abs/2307.08309v3</link><description>The collection of security-related logs holds the key to understanding attackbehaviors and diagnosing vulnerabilities. Still, their analysis remains adaunting challenge. Recently, Language Models (LMs) have demonstrated unmatchedpotential in understanding natural and programming languages. The questionarises whether and how LMs could be also useful for security experts sincetheir logs contain intrinsically confused and obfuscated information. In thispaper, we systematically study how to benefit from the state-of-the-art in LMto automatically analyze text-like Unix shell attack logs. We present athorough design methodology that leads to LogPr\'ecis. It receives as input rawshell sessions and automatically identifies and assigns the attacker tactic toeach portion of the session, i.e., unveiling the sequence of the attacker'sgoals. We demonstrate LogPr\'ecis capability to support the analysis of twolarge datasets containing about 400,000 unique Unix shell attacks. LogPr\'ecisreduces them into about 3,000 fingerprints, each grouping sessions with thesame sequence of tactics. The abstraction it provides lets the analyst betterunderstand attacks, identify fingerprints, detect novelty, link similarattacks, and track families and mutations. Overall, LogPr\'ecis, released asopen source, paves the way for better and more responsive defense againstcyberattacks.</description><author>Matteo Boffa, Rodolfo Vieira Valentim, Luca Vassio, Danilo Giordano, Idilio Drago, Marco Mellia, Zied Ben Houidi</author><pubDate>Fri, 22 Mar 2024 16:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08309v3</guid></item><item><title>Planning with a Learned Policy Basis to Optimally Solve Complex Tasks</title><link>http://arxiv.org/abs/2403.15301v1</link><description>Conventional reinforcement learning (RL) methods can successfully solve awide range of sequential decision problems. However, learning policies that cangeneralize predictably across multiple tasks in a setting with non-Markovianreward specifications is a challenging problem. We propose to use successorfeatures to learn a policy basis so that each (sub)policy in it solves awell-defined subproblem. In a task described by a finite state automaton (FSA)that involves the same set of subproblems, the combination of these(sub)policies can then be used to generate an optimal solution withoutadditional learning. In contrast to other methods that combine (sub)policiesvia planning, our method asymptotically attains global optimality, even instochastic environments.</description><author>Guillermo Infante, David Kuric, Anders Jonsson, Vicenç Gómez, Herke van Hoof</author><pubDate>Fri, 22 Mar 2024 16:51:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15301v1</guid></item><item><title>Sphere Neural-Networks for Rational Reasoning</title><link>http://arxiv.org/abs/2403.15297v1</link><description>The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed bytheir planetary popularity, their capability of human-like question-answering,and also by their steadily improved reasoning performance. However, it remainsunclear whether LLMs reason. It is an open problem how traditional neuralnetworks can be qualitatively extended to go beyond the statistic paradigm andachieve high-level cognition. Here, we present a minimalist qualitativeextension by generalising computational building blocks from vectors tospheres. We propose Sphere Neural Networks (SphNNs) for human-like reasoningthrough model construction and inspection, and develop SphNN for syllogisticreasoning, a microcosm of human rationality. Instead of training data, SphNNuses a neuro-symbolic transition map of neighbourhood spatial relations toguide transformations from the current sphere configuration towards the target.SphNN is the first neural model that can determine the validity of long-chainedsyllogistic reasoning in one epoch by constructing sphere configurations asEuler diagrams, with the worst computational complexity of O(N^2). SphNN canevolve into various types of reasoning, such as spatio-temporal reasoning,logical reasoning with negation and disjunction, event reasoning,neuro-symbolic reasoning, and humour understanding (the highest level ofcognition). All these suggest a new kind of Herbert A. Simon's scissors withtwo neural blades. SphNNs will tremendously enhance interdisciplinarycollaborations to develop the two neural blades and realise deterministicneural reasoning and human-bounded rationality and elevate LLMs to reliablepsychological AI. This work suggests that the non-zero radii of spheres are themissing components that prevent traditional deep-learning systems from reachingthe realm of rational reasoning and cause LLMs to be trapped in the swamp ofhallucination.</description><author>Tiansi Dong, Mateja Jamnik, Pietro Liò</author><pubDate>Fri, 22 Mar 2024 16:44:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15297v1</guid></item><item><title>Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation</title><link>http://arxiv.org/abs/2403.12505v2</link><description>This paper addresses an interesting yet challenging problem -- source-freeunsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semanticsegmentation -- given only a pinhole image-trained model (i.e., source) andunlabeled panoramic images (i.e., target). Tackling this problem is nontrivialdue to the semantic mismatches, style discrepancies, and inevitable distortionof panoramic images. To this end, we propose a novel method that utilizesTangent Projection (TP) as it has less distortion and meanwhile slits theequirectangular projection (ERP) with a fixed FoV to mimic the pinhole images.Both projections are shown effective in extracting knowledge from the sourcemodel. However, the distinct projection discrepancies between source and targetdomains impede the direct knowledge transfer; thus, we propose a panoramicprototype adaptation module (PPAM) to integrate panoramic prototypes from theextracted knowledge for adaptation. We then impose the loss constraints on bothpredictions and prototypes and propose a cross-dual attention module (CDAM) atthe feature level to better align the spatial and channel characteristicsacross the domains and projections. Both knowledge extraction and transferprocesses are synchronously updated to reach the best performance. Extensiveexperiments on the synthetic and real-world benchmarks, including outdoor andindoor scenarios, demonstrate that our method achieves significantly betterperformance than prior SFUDA methods for pinhole-to-panoramic adaptation.</description><author>Xu Zheng, Pengyuan Zhou, Athanasios V. Vasilakos, Lin Wang</author><pubDate>Fri, 22 Mar 2024 16:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12505v2</guid></item><item><title>Human behaviour through a LENS: How Linguistic content triggers Emotions and Norms and determines Strategy choices</title><link>http://arxiv.org/abs/2403.15293v1</link><description>Over the last two decades, a growing body of experimental research hasprovided evidence that linguistic frames influence human behaviour in economicgames, beyond the economic consequences of the available actions. This articleproposes a novel framework that transcends the traditional confines ofoutcome-based preference models. According to the LENS model, the Linguisticdescription of the decision problem triggers Emotional responses and suggestspotential Norms of behaviour, which then interact to shape an individual'sStrategic choice. The article reviews experimental evidence that supports eachpath of the LENS model. Furthermore, it identifies and discusses severalcritical research questions that arise from this model, pointing towardsavenues for future inquiry.</description><author>Valerio Capraro</author><pubDate>Fri, 22 Mar 2024 16:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15293v1</guid></item><item><title>Robustness of the Random Language Model</title><link>http://arxiv.org/abs/2309.14913v2</link><description>The Random Language Model (De Giuli 2019) is an ensemble of stochasticcontext-free grammars, quantifying the syntax of human and computer languages.The model suggests a simple picture of first language learning as a type ofannealing in the vast space of potential languages. In its simplestformulation, it implies a single continuous transition to grammatical syntax,at which the symmetry among potential words and categories is spontaneouslybroken. Here this picture is scrutinized by considering its robustness againstextensions of the original model, and trajectories through parameter spacedifferent from those originally considered. It is shown here that (i) thescenario is robust to explicit symmetry breaking, an inevitable component oflearning in the real world; and (ii) the transition to grammatical syntax canbe encountered by fixing the deep (hidden) structure while varying the surface(observable) properties. It is also argued that the transition becomes a sharpthermodynamic transition in an idealized limit. Moreover, comparison with humandata on the clustering coefficient of syntax networks suggests that theobserved transition is equivalent to that normally experienced by children atage 24 months. The results are discussed in light of theory of first-languageacquisition in linguistics, and recent successes in machine learning.</description><author>Fatemeh Lalegani, Eric De Giuli</author><pubDate>Fri, 22 Mar 2024 16:39:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14913v2</guid></item><item><title>Win-Win: Training High-Resolution Vision Transformers from Two Windows</title><link>http://arxiv.org/abs/2310.00632v2</link><description>Transformers have become the standard in state-of-the-art visionarchitectures, achieving impressive performance on both image-level and densepixelwise tasks. However, training vision transformers for high-resolutionpixelwise tasks has a prohibitive cost. Typical solutions boil down tohierarchical architectures, fast and approximate attention, or training onlow-resolution crops. This latter solution does not constrain architecturalchoices, but it leads to a clear performance drop when testing at resolutionssignificantly higher than that used for training, thus requiring ad-hoc andslow post-processing schemes. In this paper, we propose a novel strategy forefficient training and inference of high-resolution vision transformers. Thekey principle is to mask out most of the high-resolution inputs duringtraining, keeping only N random windows. This allows the model to learn localinteractions between tokens inside each window, and global interactions betweentokens from different windows. As a result, the model can directly process thehigh-resolution input at test time without any special trick. We show that thisstrategy is effective when using relative positional embedding such as rotaryembeddings. It is 4 times faster to train than a full-resolution network, andit is straightforward to use at test time compared to existing approaches. Weapply this strategy to three dense prediction tasks with high-resolution data.First, we show on the task of semantic segmentation that a simple setting with2 windows performs best, hence the name of our method: Win-Win. Second, weconfirm this result on the task of monocular depth prediction. Third, wefurther extend it to the binocular task of optical flow, reachingstate-of-the-art performance on the Spring benchmark that contains Full-HDimages with an order of magnitude faster inference than the best competitor.</description><author>Vincent Leroy, Jerome Revaud, Thomas Lucas, Philippe Weinzaepfel</author><pubDate>Fri, 22 Mar 2024 16:38:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00632v2</guid></item><item><title>Multi-resolution Time-Series Transformer for Long-term Forecasting</title><link>http://arxiv.org/abs/2311.04147v2</link><description>The performance of transformers for time-series forecasting has improvedsignificantly. Recent architectures learn complex temporal patterns bysegmenting a time-series into patches and using the patches as tokens. Thepatch size controls the ability of transformers to learn the temporal patternsat different frequencies: shorter patches are effective for learning localized,high-frequency patterns, whereas mining long-term seasonalities and trendsrequires longer patches. Inspired by this observation, we propose a novelframework, Multi-resolution Time-Series Transformer (MTST), which consists of amulti-branch architecture for simultaneous modeling of diverse temporalpatterns at different resolutions. In contrast to many existing time-seriestransformers, we employ relative positional encoding, which is better suitedfor extracting periodic components at different scales. Extensive experimentson several real-world datasets demonstrate the effectiveness of MTST incomparison to state-of-the-art forecasting techniques.</description><author>Yitian Zhang, Liheng Ma, Soumyasundar Pal, Yingxue Zhang, Mark Coates</author><pubDate>Fri, 22 Mar 2024 16:37:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04147v2</guid></item><item><title>Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images</title><link>http://arxiv.org/abs/2401.11170v2</link><description>Large vision-language models (VLMs) such as GPT-4 have achieved exceptionalperformance across various multi-modal tasks. However, the deployment of VLMsnecessitates substantial energy consumption and computational resources. Onceattackers maliciously induce high energy consumption and latency time(energy-latency cost) during inference of VLMs, it will exhaust computationalresources. In this paper, we explore this attack surface about availability ofVLMs and aim to induce high energy-latency cost during inference of VLMs. Wefind that high energy-latency cost during inference of VLMs can be manipulatedby maximizing the length of generated sequences. To this end, we proposeverbose images, with the goal of crafting an imperceptible perturbation toinduce VLMs to generate long sentences during inference. Concretely, we designthree loss objectives. First, a loss is proposed to delay the occurrence ofend-of-sequence (EOS) token, where EOS token is a signal for VLMs to stopgenerating further tokens. Moreover, an uncertainty loss and a token diversityloss are proposed to increase the uncertainty over each generated token and thediversity among all tokens of the whole generated sequence, respectively, whichcan break output dependency at token-level and sequence-level. Furthermore, atemporal weight adjustment algorithm is proposed, which can effectively balancethese losses. Extensive experiments demonstrate that our verbose images canincrease the length of generated sequences by 7.87 times and 8.56 timescompared to original images on MS-COCO and ImageNet datasets, which presentspotential challenges for various applications. Our code is available athttps://github.com/KuofengGao/Verbose_Images.</description><author>Kuofeng Gao, Yang Bai, Jindong Gu, Shu-Tao Xia, Philip Torr, Zhifeng Li, Wei Liu</author><pubDate>Fri, 22 Mar 2024 16:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11170v2</guid></item><item><title>Blockchain-based Pseudonym Management for Vehicle Twin Migrations in Vehicular Edge Metaverse</title><link>http://arxiv.org/abs/2403.15285v1</link><description>Driven by the great advances in metaverse and edge computing technologies,vehicular edge metaverses are expected to disrupt the current paradigm ofintelligent transportation systems. As highly computerized avatars of VehicularMetaverse Users (VMUs), the Vehicle Twins (VTs) deployed in edge servers canprovide valuable metaverse services to improve driving safety and on-boardsatisfaction for their VMUs throughout journeys. To maintain uninterruptedmetaverse experiences, VTs must be migrated among edge servers following themovements of vehicles. This can raise concerns about privacy breaches duringthe dynamic communications among vehicular edge metaverses. To address theseconcerns and safeguard location privacy, pseudonyms as temporary identifierscan be leveraged by both VMUs and VTs to realize anonymous communications inthe physical space and virtual spaces. However, existing pseudonym managementmethods fall short in meeting the extensive pseudonym demands in vehicular edgemetaverses, thus dramatically diminishing the performance of privacypreservation. To this end, we present a cross-metaverse empowered dualpseudonym management framework. We utilize cross-chain technology to enhancemanagement efficiency and data security for pseudonyms. Furthermore, we proposea metric to assess the privacy level and employ a Multi-Agent DeepReinforcement Learning (MADRL) approach to obtain an optimal pseudonymgenerating strategy. Numerical results demonstrate that our proposed schemesare high-efficiency and cost-effective, showcasing their promising applicationsin vehicular edge metaverses.</description><author>Jiawen Kang, Xiaofeng Luo, Jiangtian Nie, Tianhao Wu, Haibo Zhou, Yonghua Wang, Dusit Niyato, Shiwen Mao, Shengli Xie</author><pubDate>Fri, 22 Mar 2024 16:31:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15285v1</guid></item><item><title>Residual Denoising Diffusion Models</title><link>http://arxiv.org/abs/2308.13712v3</link><description>We propose residual denoising diffusion models (RDDM), a novel dual diffusionprocess that decouples the traditional single denoising diffusion process intoresidual diffusion and noise diffusion. This dual diffusion framework expandsthe denoising-based diffusion models, initially uninterpretable for imagerestoration, into a unified and interpretable model for both image generationand restoration by introducing residuals. Specifically, our residual diffusionrepresents directional diffusion from the target image to the degraded inputimage and explicitly guides the reverse generation process for imagerestoration, while noise diffusion represents random perturbations in thediffusion process. The residual prioritizes certainty, while the noiseemphasizes diversity, enabling RDDM to effectively unify tasks with varyingcertainty or diversity requirements, such as image generation and restoration.We demonstrate that our sampling process is consistent with that of DDPM andDDIM through coefficient transformation, and propose a partiallypath-independent generation process to better understand the reverse process.Notably, our RDDM enables a generic UNet, trained with only an L1 loss and abatch size of 1, to compete with state-of-the-art image restoration methods. Weprovide code and pre-trained models to encourage further exploration,application, and development of our innovative framework(https://github.com/nachifur/RDDM).</description><author>Jiawei Liu, Qiang Wang, Huijie Fan, Yinong Wang, Yandong Tang, Liangqiong Qu</author><pubDate>Fri, 22 Mar 2024 16:30:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13712v3</guid></item><item><title>VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding</title><link>http://arxiv.org/abs/2403.09530v2</link><description>The evolution of text to visual components facilitates people's daily lives,such as generating image, videos from text and identifying the desired elementswithin the images. Computer vision models involving the multimodal abilities inthe previous days are focused on image detection, classification based onwell-defined objects. Large language models (LLMs) introduces thetransformation from nature language to visual objects, which present the visuallayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,while the computer vision (CV) domain boasts a plethora of state-of-the-art(SOTA) models and algorithms to convert 2D images to their 3D representations.However, the mismatching between the algorithms with the problem could lead toundesired results. In response to this challenge, we propose an unifiedVisionGPT-3D framework to consolidate the state-of-the-art vision models,thereby facilitating the development of vision-oriented AI. VisionGPT-3Dprovides a versatile multimodal framework building upon the strengths ofmultimodal foundation models. It seamlessly integrates various SOTA visionmodels and brings the automation in the selection of SOTA vision models,identifies the suitable 3D mesh creation algorithms corresponding to 2D depthmaps analysis, generates optimal results based on diverse multimodal inputssuch as text prompts. Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent</description><author>Chris Kelly, Luhui Hu, Jiayin Hu, Yu Tian, Deshun Yang, Bang Yang, Cindy Yang, Zihao Li, Zaoshan Huang, Yuexian Zou</author><pubDate>Fri, 22 Mar 2024 16:26:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09530v2</guid></item><item><title>Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech</title><link>http://arxiv.org/abs/2309.09510v2</link><description>Text language models have shown remarkable zero-shot capability ingeneralizing to unseen tasks when provided with well-formulated instructions.However, existing studies in speech processing primarily focus on limited orspecific tasks. Moreover, the lack of standardized benchmarks hinders a faircomparison across different approaches. Thus, we present Dynamic-SUPERB, abenchmark designed for building universal speech models capable of leveraginginstruction tuning to perform multiple tasks in a zero-shot fashion. To achievecomprehensive coverage of diverse speech tasks and harness instruction tuning,we invite the community to collaborate and contribute, facilitating the dynamicgrowth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluationinstances by combining 33 tasks and 22 datasets. This spans a broad spectrum ofdimensions, providing a comprehensive platform for evaluation. Additionally, wepropose several approaches to establish benchmark baselines. These include theutilization of speech models, text language models, and the multimodal encoder.Evaluation results indicate that while these baselines perform reasonably onseen tasks, they struggle with unseen ones. We release all materials to thepublic and welcome researchers to collaborate on the project, advancingtechnologies in the field together.</description><author>Chien-yu Huang, Ke-Han Lu, Shih-Heng Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee</author><pubDate>Fri, 22 Mar 2024 16:25:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09510v2</guid></item><item><title>Fundus: A Simple-to-Use News Scraper Optimized for High Quality Extractions</title><link>http://arxiv.org/abs/2403.15279v1</link><description>This paper introduces Fundus, a user-friendly news scraper that enables usersto obtain millions of high-quality news articles with just a few lines of code.Unlike existing news scrapers, we use manually crafted, bespoke contentextractors that are specifically tailored to the formatting guidelines of eachsupported online newspaper. This allows us to optimize our scraping for qualitysuch that retrieved news articles are textually complete and without HTMLartifacts. Further, our framework combines both crawling (retrieving HTML fromthe web or large web archives) and content extraction into a single pipeline.By providing a unified interface for a predefined collection of newspapers, weaim to make Fundus broadly usable even for non-technical users. This papergives an overview of the framework, discusses our design choices, and presentsa comparative evaluation against other popular news scrapers. Our evaluationshows that Fundus yields significantly higher quality extractions (complete andartifact-free news articles) than prior work. The framework is available onGitHub under https://github.com/flairNLP/fundus and can be simply installedusing pip.</description><author>Max Dallabetta, Conrad Dobberstein, Adrian Breiding, Alan Akbik</author><pubDate>Fri, 22 Mar 2024 16:22:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15279v1</guid></item><item><title>Specifying Genericity through Inclusiveness and Abstractness Continuous Scales</title><link>http://arxiv.org/abs/2403.15278v1</link><description>This paper introduces a novel annotation framework for the fine-grainedmodeling of Noun Phrases' (NPs) genericity in natural language. The frameworkis designed to be simple and intuitive, making it accessible to non-expertannotators and suitable for crowd-sourced tasks. Drawing from theoretical andcognitive literature on genericity, this framework is grounded in establishedlinguistic theory. Through a pilot study, we created a small but crucialannotated dataset of 324 sentences, serving as a foundation for futureresearch. To validate our approach, we conducted an evaluation comparing ourcontinuous annotations with existing binary annotations on the same dataset,demonstrating the framework's effectiveness in capturing nuanced aspects ofgenericity. Our work offers a practical resource for linguists, providing afirst annotated dataset and an annotation scheme designed to buildreal-language datasets that can be used in studies on the semantics ofgenericity, and NLP practitioners, contributing to the development ofcommonsense knowledge repositories valuable in enhancing various NLPapplications.</description><author>Claudia Collacciani, Andrea Amelio Ravelli, Marianna Marcella Bolognesi</author><pubDate>Fri, 22 Mar 2024 16:21:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15278v1</guid></item><item><title>Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review</title><link>http://arxiv.org/abs/2403.15274v1</link><description>The year 2023 marked a significant surge in the exploration of applying largelanguage model (LLM) chatbots, notably ChatGPT, across various disciplines. Wesurveyed the applications of ChatGPT in various sectors of bioinformatics andbiomedical informatics throughout the year, covering omics, genetics,biomedical text mining, drug discovery, biomedical image understanding,bioinformatics programming, and bioinformatics education. Our survey delineatesthe current strengths and limitations of this chatbot in bioinformatics andoffers insights into potential avenues for future development.</description><author>Jinge Wang, Zien Cheng, Qiuming Yao, Li Liu, Dong Xu, Gangqing Hu</author><pubDate>Fri, 22 Mar 2024 16:16:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15274v1</guid></item><item><title>Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs</title><link>http://arxiv.org/abs/2403.15273v1</link><description>Event temporal relation (TempRel) is a primary subject of the event relationextraction task. However, the inherent ambiguity of TempRel increases thedifficulty of the task. With the rise of prompt engineering, it is important todesign effective prompt templates and verbalizers to extract relevantknowledge. The traditional manually designed templates struggle to extractprecise temporal knowledge. This paper introduces a novel retrieval-augmentedTempRel extraction approach, leveraging knowledge retrieved from large languagemodels (LLMs) to enhance prompt templates and verbalizers. Our methodcapitalizes on the diverse capabilities of various LLMs to generate a widearray of ideas for template and verbalizer design. Our proposed method fullyexploits the potential of LLMs for generation tasks and contributes moreknowledge to our design. Empirical evaluations across three widely recognizeddatasets demonstrate the efficacy of our method in improving the performance ofevent temporal relation extraction tasks.</description><author>Xiaobin Zhang, Liangjun Zang, Qianwen Liu, Shuchong Wei, Songlin Hu</author><pubDate>Fri, 22 Mar 2024 16:16:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15273v1</guid></item><item><title>WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization</title><link>http://arxiv.org/abs/2403.15272v1</link><description>Despite the advancements in deep learning for camera relocalization tasks,obtaining ground truth pose labels required for the training process remains acostly endeavor. While current weakly supervised methods excel in lightweightlabel generation, their performance notably declines in scenarios with sparseviews. In response to this challenge, we introduce WSCLoc, a system capable ofbeing customized to various deep learning-based relocalization models toenhance their performance under weakly-supervised and sparse view conditions.This is realized with two stages. In the initial stage, WSCLoc employs amultilayer perceptron-based structure called WFT-NeRF to co-optimize imagereconstruction quality and initial pose information. To ensure a stablelearning process, we incorporate temporal information as input. Furthermore,instead of optimizing SE(3), we opt for $\mathfrak{sim}(3)$ optimization toexplicitly enforce a scale constraint. In the second stage, we co-optimize thepre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced byTime-Encoding based Random View Synthesis and supervised by inter-framegeometric constraints that consider pose, depth, and RGB information. Wevalidate our approaches on two publicly available datasets, one outdoor and oneindoor. Our experimental results demonstrate that our weakly-supervisedrelocalization solutions achieve superior pose estimation accuracy insparse-view scenarios, comparable to state-of-the-art camera relocalizationmethods. We will make our code publicly available.</description><author>Jialu Wang, Kaichen Zhou, Andrew Markham, Niki Trigoni</author><pubDate>Fri, 22 Mar 2024 16:15:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15272v1</guid></item><item><title>Combining the Strengths of Dutch Survey and Register Data in a Data Challenge to Predict Fertility (PreFer)</title><link>http://arxiv.org/abs/2402.00705v2</link><description>The social sciences have produced an impressive body of research ondeterminants of fertility outcomes, or whether and when people have children.However, the strength of these determinants and underlying theories are rarelyevaluated on their predictive ability on new data. This prevents us fromsystematically comparing studies, hindering the evaluation and accumulation ofknowledge. In this paper, we present two datasets which can be used to studythe predictability of fertility outcomes in the Netherlands. One dataset isbased on the LISS panel, a longitudinal survey which includes thousands ofvariables on a wide range of topics, including individual preferences andvalues. The other is based on the Dutch register data which lacks attitudinaldata but includes detailed information about the life courses of millions ofDutch residents. We provide information about the datasets and the samples, anddescribe the fertility outcome of interest. We also introduce the fertilityprediction data challenge PreFer which is based on these datasets and willstart in Spring 2024. We outline the ways in which measuring the predictabilityof fertility outcomes using these datasets and combining their strengths in thedata challenge can advance our understanding of fertility behaviour andcomputational social science. We further provide details for participants onhow to take part in the data challenge.</description><author>Elizaveta Sivak, Paulina Pankowska, Adrienne Mendrik, Tom Emery, Javier Garcia-Bernardo, Seyit Hocuk, Kasia Karpinska, Angelica Maineri, Joris Mulder, Malvina Nissim, Gert Stulp</author><pubDate>Fri, 22 Mar 2024 16:13:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00705v2</guid></item><item><title>Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models</title><link>http://arxiv.org/abs/2403.15268v1</link><description>Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have beenproposed to enhance the knowledge required for question answering over LargeLanguage Models (LLMs). However, the former depends on external resources, andboth require incorporating the explicit documents into the context, whichresults in longer contexts that lead to more resource consumption. Recent worksindicate that LLMs have modeled rich knowledge, albeit not effectivelytriggered or activated. Inspired by this, we propose a novelknowledge-augmented framework, Imagination-Augmented-Generation (IAG), whichsimulates the human capacity to compensate for knowledge deficits whileanswering questions solely through imagination, without relying on externalresources. Guided by IAG, we propose an imagine richer context method forquestion answering (IMcQA), which obtains richer context through the followingtwo modules: explicit imagination by generating a short dummy document withlong context compress and implicit imagination with HyperNetwork for generatingadapter weights. Experimental results on three datasets demonstrate that IMcQAexhibits significant advantages in both open-domain and closed-book settings,as well as in both in-distribution performance and out-of-distributiongeneralizations. Our code will be available athttps://github.com/Xnhyacinth/IAG.</description><author>Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping Liu, Jun Zhao</author><pubDate>Fri, 22 Mar 2024 16:06:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15268v1</guid></item><item><title>Parametric PDE Control with Deep Reinforcement Learning and Differentiable L0-Sparse Polynomial Policies</title><link>http://arxiv.org/abs/2403.15267v1</link><description>Optimal control of parametric partial differential equations (PDEs) iscrucial in many applications in engineering and science. In recent years, theprogress in scientific machine learning has opened up new frontiers for thecontrol of parametric PDEs. In particular, deep reinforcement learning (DRL)has the potential to solve high-dimensional and complex control problems in alarge variety of applications. Most DRL methods rely on deep neural network(DNN) control policies. However, for many dynamical systems, DNN-based controlpolicies tend to be over-parametrized, which means they need large amounts oftraining data, show limited robustness, and lack interpretability. In thiswork, we leverage dictionary learning and differentiable L$_0$ regularizationto learn sparse, robust, and interpretable control policies for parametricPDEs. Our sparse policy architecture is agnostic to the DRL method and can beused in different policy-gradient and actor-critic DRL algorithms withoutchanging their policy-optimization procedure. We test our approach on thechallenging tasks of controlling parametric Kuramoto-Sivashinsky andconvection-diffusion-reaction PDEs. We show that our method (1) outperformsbaseline DNN-based DRL policies, (2) allows for the derivation of interpretableequations of the learned optimal control laws, and (3) generalizes to unseenparameters of the PDE without retraining the policies.</description><author>Nicolò Botteghi, Urban Fasel</author><pubDate>Fri, 22 Mar 2024 16:06:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15267v1</guid></item><item><title>Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models</title><link>http://arxiv.org/abs/2403.15263v1</link><description>Federated learning (FL) is an approach to training machine learning modelsthat takes advantage of multiple distributed datasets while maintaining dataprivacy and reducing communication costs associated with sharing localdatasets. Aggregation strategies have been developed to pool or fuse theweights and biases of distributed deterministic models; however, moderndeterministic deep learning (DL) models are often poorly calibrated and lackthe ability to communicate a measure of epistemic uncertainty in prediction,which is desirable for remote sensing platforms and safety-criticalapplications. Conversely, Bayesian DL models are often well calibrated andcapable of quantifying and communicating a measure of epistemic uncertaintyalong with a competitive prediction accuracy. Unfortunately, because theweights and biases in Bayesian DL models are defined by a probabilitydistribution, simple application of the aggregation methods associated with FLschemes for deterministic models is either impossible or results in sub-optimalperformance. In this work, we use independent and identically distributed (IID)and non-IID partitions of the CIFAR-10 dataset and a fully variationalResNet-20 architecture to analyze six different aggregation strategies forBayesian DL models. Additionally, we analyze the traditional federatedaveraging approach applied to an approximate Bayesian Monte Carlo dropout modelas a lightweight alternative to more complex variational inference methods inFL. We show that aggregation strategy is a key hyperparameter in the design ofa Bayesian FL system with downstream effects on accuracy, calibration,uncertainty quantification, training stability, and client computerequirements.</description><author>John Fischer, Marko Orescanin, Justin Loomis, Patrick McClure</author><pubDate>Fri, 22 Mar 2024 16:02:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15263v1</guid></item><item><title>Sparse Mean Field Load Balancing in Large Localized Queueing Systems</title><link>http://arxiv.org/abs/2312.12973v2</link><description>Scalable load balancing algorithms are of great interest in cloud networksand data centers, necessitating the use of tractable techniques to computeoptimal load balancing policies for good performance. However, most existingscalable techniques, especially asymptotically scaling methods based on meanfield theory, have not been able to model large queueing networks with stronglocality. Meanwhile, general multi-agent reinforcement learning techniques canbe hard to scale and usually lack a theoretical foundation. In this work, weaddress this challenge by leveraging recent advances in sparse mean fieldtheory to learn a near-optimal load balancing policy in sparsely connectedqueueing networks in a tractable manner, which may be preferable to globalapproaches in terms of wireless communication overhead. Importantly, we obtaina general load balancing framework for a large class of sparse bounded-degreewireless topologies. By formulating a novel mean field control problem in thecontext of graphs with bounded degree, we reduce the otherwise difficultmulti-agent problem to a single-agent problem. Theoretically, the approach isjustified by approximation guarantees. Empirically, the proposed methodologyperforms well on several realistic and scalable wireless network topologies ascompared to a number of well-known load balancing heuristics and existingscalable multi-agent reinforcement learning methods.</description><author>Anam Tahir, Kai Cui, Heinz Koeppl</author><pubDate>Fri, 22 Mar 2024 16:00:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12973v2</guid></item><item><title>Hyperbolic Metric Learning for Visual Outlier Detection</title><link>http://arxiv.org/abs/2403.15260v1</link><description>Out-Of-Distribution (OOD) detection is critical to deploy deep learningmodels in safety-critical applications. However, the inherent hierarchicalconcept structure of visual data, which is instrumental to OOD detection, isoften poorly captured by conventional methods based on Euclidean geometry. Thiswork proposes a metric framework that leverages the strengths of Hyperbolicgeometry for OOD detection. Inspired by previous works that refine the decisionboundary for OOD data with synthetic outliers, we extend this method toHyperbolic space. Interestingly, we find that synthetic outliers do not benefitOOD detection in Hyperbolic space as they do in Euclidean space. Furthermore weexplore the relationship between OOD detection performance and Hyperbolicembedding dimension, addressing practical concerns in resource-constrainedenvironments. Extensive experiments show that our framework improves the FPR95for OOD detection from 22\% to 15\% and from 49% to 28% on CIFAR-10 andCIFAR-100 respectively compared to Euclidean methods.</description><author>Alvaro Gonzalez-Jimenez, Simone Lionetti, Dena Bazazian, Philippe Gottfrois, Fabian Gröger, Marc Pouly, Alexander Navarini</author><pubDate>Fri, 22 Mar 2024 16:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15260v1</guid></item><item><title>Hierarchical Information Enhancement Network for Cascade Prediction in Social Networks</title><link>http://arxiv.org/abs/2403.15257v1</link><description>Understanding information cascades in networks is a fundamental issue innumerous applications. Current researches often sample cascade information intoseveral independent paths or subgraphs to learn a simple cascaderepresentation. However, these approaches fail to exploit the hierarchicalsemantic associations between different modalities, limiting their predictiveperformance. In this work, we propose a novel Hierarchical InformationEnhancement Network (HIENet) for cascade prediction. Our approach integratesfundamental cascade sequence, user social graphs, and sub-cascade graph into aunified framework. Specifically, HIENet utilizes DeepWalk to sample cascadesinformation into a series of sequences. It then gathers path informationbetween users to extract the social relationships of propagators. Additionally,we employ a time-stamped graph convolutional network to aggregate sub-cascadegraph information effectively. Ultimately, we introduce a Multi-modal CascadeTransformer to powerfully fuse these clues, providing a comprehensiveunderstanding of cascading process. Extensive experiments have demonstrated theeffectiveness of the proposed method.</description><author>Fanrui Zhang, Jiawei Liu, Qiang Zhang, Xiaoling Zhu, Zheng-Jun Zha</author><pubDate>Fri, 22 Mar 2024 15:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15257v1</guid></item><item><title>Safe Learning of PDDL Domains with Conditional Effects -- Extended Version</title><link>http://arxiv.org/abs/2403.15251v1</link><description>Powerful domain-independent planners have been developed to solve varioustypes of planning problems. These planners often require a model of the actingagent's actions, given in some planning domain description language. Manuallydesigning such an action model is a notoriously challenging task. Analternative is to automatically learn action models from observation. Such anaction model is called safe if every plan created with it is consistent withthe real, unknown action model. Algorithms for learning such safe action modelsexist, yet they cannot handle domains with conditional or universal effects,which are common constructs in many planning problems. We prove that learningnon-trivial safe action models with conditional effects may require anexponential number of samples. Then, we identify reasonable assumptions underwhich such learning is tractable and propose SAM Learning of ConditionalEffects (Conditional-SAM), the first algorithm capable of doing so. We analyzeConditional-SAM theoretically and evaluate it experimentally. Our results showthat the action models learned by Conditional-SAM can be used to solveperfectly most of the test set problems in most of the experimented domains.</description><author>Argaman Mordoch, Enrico Scala, Roni Stern, Brendan Juba</author><pubDate>Fri, 22 Mar 2024 15:49:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15251v1</guid></item><item><title>MSAC: Multiple Speech Attribute Control Method for Reliable Speech Emotion Recognition</title><link>http://arxiv.org/abs/2308.04025v3</link><description>Despite notable progress, speech emotion recognition (SER) remainschallenging due to the intricate and ambiguous nature of speech emotion,particularly in wild world. While current studies primarily focus onrecognition and generalization abilities, our research pioneers aninvestigation into the reliability of SER methods in the presence of semanticdata shifts and explores how to exert fine-grained control over variousattributes inherent in speech signals to enhance speech emotion modeling. Inthis paper, we first introduce MSAC-SERNet, a novel unified SER frameworkcapable of simultaneously handling both single-corpus and cross-corpus SER.Specifically, concentrating exclusively on the speech emotion attribute, anovel CNN-based SER model is presented to extract discriminative emotionalrepresentations, guided by additive margin softmax loss. Consideringinformation overlap between various speech attributes, we propose a novellearning paradigm based on correlations of different speech attributes, termedMultiple Speech Attribute Control (MSAC), which empowers the proposed SER modelto simultaneously capture fine-grained emotion-related features whilemitigating the negative impact of emotion-agnostic representations.Furthermore, we make a first attempt to examine the reliability of theMSAC-SERNet framework using out-of-distribution detection methods. Experimentson both single-corpus and cross-corpus SER scenarios indicate that MSAC-SERNetnot only consistently outperforms the baseline in all aspects, but achievessuperior performance compared to state-of-the-art SER approaches.</description><author>Yu Pan, Yuguang Yang, Yuheng Huang, Jixun Yao, Jingjing Yin, Yanni Hu, Heng Lu, Lei Ma, Jianjun Zhao</author><pubDate>Fri, 22 Mar 2024 15:49:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04025v3</guid></item><item><title>Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach</title><link>http://arxiv.org/abs/2403.15250v1</link><description>Amidst the rapid evolution of LLMs, the significance of evaluation incomprehending and propelling these models forward is increasingly paramount.Evaluations have revealed that factors such as scaling, training types,architectures and other factors profoundly impact the performance of LLMs.However, the extent and nature of these impacts continue to be subjects ofdebate because most assessments have been restricted to a limited number ofmodels and data points. Clarifying the effects of these factors on performancescores can be more effectively achieved through a statistical lens. Our studyembarks on a thorough re-examination of these LLMs, targeting the inadequaciesin current evaluation methods. With the advent of a uniform evaluationframework, our research leverages an expansive dataset of evaluation results,introducing a comprehensive statistical methodology. This includes theapplication of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offeringa robust and transparent approach to deciphering LLM performance data. Contraryto prevailing findings, our results challenge assumptions about emergentabilities and the influence of given training types and architectures in LLMs.These findings furnish new perspectives on the characteristics, intrinsicnature, and developmental trajectories of LLMs. By providing straightforwardand reliable methods to scrutinize and reassess LLM performance data, thisstudy contributes a nuanced perspective on LLM efficiency and potentials.</description><author>Kun Sun, Rong Wang, Haitao Liu, Anders Søgaard</author><pubDate>Fri, 22 Mar 2024 15:47:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15250v1</guid></item><item><title>Spectral Motion Alignment for Video Motion Transfer using Diffusion Models</title><link>http://arxiv.org/abs/2403.15249v1</link><description>The evolution of diffusion models has greatly impacted video generation andunderstanding. Particularly, text-to-video diffusion models (VDMs) havesignificantly facilitated the customization of input video with targetappearance, motion, etc. Despite these advances, challenges persist inaccurately distilling motion information from video frames. While existingworks leverage the consecutive frame residual as the target motion vector, theyinherently lack global motion context and are vulnerable to frame-wisedistortions. To address this, we present Spectral Motion Alignment (SMA), anovel framework that refines and aligns motion vectors using Fourier andwavelet transforms. SMA learns motion patterns by incorporatingfrequency-domain regularization, facilitating the learning of whole-frameglobal motion dynamics, and mitigating spatial artifacts. Extensive experimentsdemonstrate SMA's efficacy in improving motion transfer while maintainingcomputational efficiency and compatibility across various video customizationframeworks.</description><author>Geon Yeong Park, Hyeonho Jeong, Sang Wan Lee, Jong Chul Ye</author><pubDate>Fri, 22 Mar 2024 15:47:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15249v1</guid></item><item><title>Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks</title><link>http://arxiv.org/abs/2403.15248v1</link><description>Computer vision in agriculture is game-changing with its ability to transformfarming into a data-driven, precise, and sustainable industry. Deep learninghas empowered agriculture vision to analyze vast, complex visual data, butheavily rely on the availability of large annotated datasets. This remains abottleneck as manual labeling is error-prone, time-consuming, and expensive.The lack of efficient labeling approaches inspired us to considerself-supervised learning as a paradigm shift, learning meaningful featurerepresentations from raw agricultural image data. In this work, we explore howself-supervised representation learning unlocks the potential applicability todiverse agriculture vision tasks by eliminating the need for large-scaleannotated datasets. We propose a lightweight framework utilizing SimCLR, acontrastive learning approach, to pre-train a ResNet-50 backbone on a large,unannotated dataset of real-world agriculture field images. Our experimentalanalysis and results indicate that the model learns robust features applicableto a broad range of downstream agriculture tasks discussed in the paper.Additionally, the reduced reliance on annotated data makes our approach morecost-effective and accessible, paving the way for broader adoption of computervision in agriculture.</description><author>Sudhir Sornapudi, Rajhans Singh</author><pubDate>Fri, 22 Mar 2024 15:46:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15248v1</guid></item><item><title>Toulouse Hyperspectral Data Set: a benchmark data set to assess semi-supervised spectral representation learning and pixel-wise classification techniques</title><link>http://arxiv.org/abs/2311.08863v2</link><description>Airborne hyperspectral images can be used to map the land cover in largeurban areas, thanks to their very high spatial and spectral resolutions on awide spectral domain. While the spectral dimension of hyperspectral images ishighly informative of the chemical composition of the land surface, the use ofstate-of-the-art machine learning algorithms to map the land cover has beendramatically limited by the availability of training data. To cope with thescarcity of annotations, semi-supervised and self-supervised techniques havelately raised a lot of interest in the community. Yet, the publicly availablehyperspectral data sets commonly used to benchmark machine learning models arenot totally suited to evaluate their generalization performances due to one orseveral of the following properties: a limited geographical coverage (whichdoes not reflect the spectral diversity in metropolitan areas), a small numberof land cover classes and a lack of appropriate standard train / test splitsfor semi-supervised and self-supervised learning. Therefore, we release in thispaper the Toulouse Hyperspectral Data Set that stands out from other data setsin the above-mentioned respects in order to meet key issues in spectralrepresentation learning and classification over large-scale hyperspectralimages with very few labeled pixels. Besides, we discuss and experimentself-supervised techniques for spectral representation learning, including theMasked Autoencoder, and establish a baseline for pixel-wise classificationachieving 85% overall accuracy and 77% F1 score. The Toulouse HyperspectralData Set and our code are publicly available athttps://www.toulouse-hyperspectral-data-set.com andhttps://www.github.com/Romain3Ch216/tlse-experiments, respectively.</description><author>Romain Thoreau, Laurent Risser, Véronique Achard, Béatrice Berthelot, Xavier Briottet</author><pubDate>Fri, 22 Mar 2024 15:46:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08863v2</guid></item><item><title>Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling</title><link>http://arxiv.org/abs/2403.05752v2</link><description>A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse rangeof node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popularfor training machine learning tasks like node classification and linkprediction on KGs. However, HGNN methods exhibit excessive complexityinfluenced by the KG's size, density, and the number of node and edge types. AIpractitioners handcraft a subgraph of a KG G relevant to a specific task. Werefer to this subgraph as a task-oriented subgraph (TOSG), which contains asubset of task-related node and edge types in G. Training the task using TOSGinstead of G alleviates the excessive computation required for a large KG.Crafting the TOSG demands a deep understanding of the KG's structure and thetask's objectives. Hence, it is challenging and time-consuming. This paperproposes KG-TOSA, an approach to automate the TOSG extraction for task-orientedHGNN training on a large KG. In KG-TOSA, we define a generic graph pattern thatcaptures the KG's local and global structure relevant to a specific task. Weexplore different techniques to extract subgraphs matching our graph pattern:namely (i) two techniques sampling around targeted nodes using biased randomwalk or influence scores, and (ii) a SPARQL-based extraction method leveragingRDF engines' built-in indices. Hence, it achieves negligible preprocessingoverhead compared to the sampling techniques. We develop a benchmark of realKGs of large sizes and various tasks for node classification and linkprediction. Our experiments show that KG-TOSA helps state-of-the-art HGNNmethods reduce training time and memory usage by up to 70% while improving themodel performance, e.g., accuracy and inference time.</description><author>Hussein Abdallah, Waleed Afandi, Panos Kalnis, Essam Mansour</author><pubDate>Fri, 22 Mar 2024 15:44:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05752v2</guid></item><item><title>FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions</title><link>http://arxiv.org/abs/2403.15246v1</link><description>Modern Large Language Models (LLMs) are capable of following long and complexinstructions that enable a diverse amount of user tasks. However, despiteInformation Retrieval (IR) models using LLMs as the backbone of theirarchitectures, nearly all of them still only take queries as input, with noinstructions. For the handful of recent models that do take instructions, it'sunclear how they use them. We introduce our dataset FollowIR, which contains arigorous instruction evaluation benchmark as well as a training set for helpingIR models learn to better follow real-world instructions. FollowIR builds offthe long history of the TREC conferences: as TREC provides human annotatorswith instructions (also known as narratives) to determine document relevance,so should IR models be able to understand and decide relevance based on thesedetailed instructions. Our evaluation benchmark starts with three deeply judgedTREC collections and alters the annotator instructions, re-annotating relevantdocuments. Through this process, we can measure how well IR models followinstructions, through a new pairwise evaluation framework. Our results indicatethat existing retrieval models fail to correctly use instructions, using themfor basic keywords and struggling to understand long-form information. However,we show that it is possible for IR models to learn to follow complexinstructions: our new FollowIR-7B model has significant improvements (over 13%)after fine-tuning on our training set.</description><author>Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, Luca Soldaini</author><pubDate>Fri, 22 Mar 2024 15:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15246v1</guid></item><item><title>Reasoning-Enhanced Object-Centric Learning for Videos</title><link>http://arxiv.org/abs/2403.15245v1</link><description>Object-centric learning aims to break down complex visual scenes into moremanageable object representations, enhancing the understanding and reasoningabilities of machine learning systems toward the physical world. Recently,slot-based video models have demonstrated remarkable proficiency in segmentingand tracking objects, but they overlook the importance of the effectivereasoning module. In the real world, reasoning and predictive abilities play acrucial role in human perception and object tracking; in particular, theseabilities are closely related to human intuitive physics. Inspired by this, wedesigned a novel reasoning module called the Slot-based Time-Space Transformerwith Memory buffer (STATM) to enhance the model's perception ability in complexscenes. The memory buffer primarily serves as storage for slot information fromupstream modules, the Slot-based Time-Space Transformer makes predictionsthrough slot-based spatiotemporal attention computations and fusion. Ourexperiment results on various datasets show that STATM can significantlyenhance object-centric learning capabilities of slot-based video models.</description><author>Jian Li, Pu Ren, Yang Liu, Hao Sun</author><pubDate>Fri, 22 Mar 2024 15:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15245v1</guid></item><item><title>A Stochastic Quasi-Newton Method for Non-convex Optimization with Non-uniform Smoothness</title><link>http://arxiv.org/abs/2403.15244v1</link><description>Classical convergence analyses for optimization algorithms rely on thewidely-adopted uniform smoothness assumption. However, recent experimentalstudies have demonstrated that many machine learning problems exhibitnon-uniform smoothness, meaning the smoothness factor is a function of themodel parameter instead of a universal constant. In particular, it has beenobserved that the smoothness grows with respect to the gradient norm along thetraining trajectory. Motivated by this phenomenon, the recently introduced$(L_0, L_1)$-smoothness is a more general notion, compared to traditional$L$-smoothness, that captures such positive relationship between smoothness andgradient norm. Under this type of non-uniform smoothness, existing literaturehas designed stochastic first-order algorithms by utilizing gradient clippingtechniques to obtain the optimal $\mathcal{O}(\epsilon^{-3})$ sample complexityfor finding an $\epsilon$-approximate first-order stationary solution.Nevertheless, the studies of quasi-Newton methods are still lacking.Considering higher accuracy and more robustness for quasi-Newton methods, inthis paper we propose a fast stochastic quasi-Newton method when there existsnon-uniformity in smoothness. Leveraging gradient clipping and variancereduction, our algorithm can achieve the best-known$\mathcal{O}(\epsilon^{-3})$ sample complexity and enjoys convergence speedupwith simple hyperparameter tuning. Our numerical experiments show that ourproposed algorithm outperforms the state-of-the-art approaches.</description><author>Zhenyu Sun, Ermin Wei</author><pubDate>Fri, 22 Mar 2024 15:40:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15244v1</guid></item><item><title>Robust Utility Optimization via a GAN Approach</title><link>http://arxiv.org/abs/2403.15243v1</link><description>Robust utility optimization enables an investor to deal with marketuncertainty in a structured way, with the goal of maximizing the worst-caseoutcome. In this work, we propose a generative adversarial network (GAN)approach to (approximately) solve robust utility optimization problems ingeneral and realistic settings. In particular, we model both the investor andthe market by neural networks (NN) and train them in a mini-max zero-sum game.This approach is applicable for any continuous utility function and inrealistic market settings with trading costs, where only observable informationof the market can be used. A large empirical study shows the versatileusability of our method. Whenever an optimal reference strategy is available,our method performs on par with it and in the (many) settings without knownoptimal strategy, our method outperforms all other reference strategies.Moreover, we can conclude from our study that the trained path-dependentstrategies do not outperform Markovian ones. Lastly, we uncover that ourgenerative approach for learning optimal, (non-) robust investments undertrading costs generates universally applicable alternatives to well knownasymptotic strategies of idealized settings.</description><author>Florian Krach, Josef Teichmann, Hanna Wutte</author><pubDate>Fri, 22 Mar 2024 15:36:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15243v1</guid></item><item><title>An axiomatized PDE model of deep neural networks</title><link>http://arxiv.org/abs/2307.12333v2</link><description>Inspired by the relation between deep neural network (DNN) and partialdifferential equations (PDEs), we study the general form of the PDE models ofdeep neural networks. To achieve this goal, we formulate DNN as an evolutionoperator from a simple base model. Based on several reasonable assumptions, weprove that the evolution operator is actually determined byconvection-diffusion equation. This convection-diffusion equation model givesmathematical explanation for several effective networks. Moreover, we show thatthe convection-diffusion model improves the robustness and reduces theRademacher complexity. Based on the convection-diffusion equation, we design anew training method for ResNets. Experiments validate the performance of theproposed method.</description><author>Tangjun Wang, Wenqi Tao, Chenglong Bao, Zuoqiang Shi</author><pubDate>Fri, 22 Mar 2024 15:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12333v2</guid></item><item><title>IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection</title><link>http://arxiv.org/abs/2403.15241v1</link><description>Bird's eye view (BEV) representation has emerged as a dominant solution fordescribing 3D space in autonomous driving scenarios. However, objects in theBEV representation typically exhibit small sizes, and the associated pointcloud context is inherently sparse, which leads to great challenges forreliable 3D perception. In this paper, we propose IS-Fusion, an innovativemultimodal fusion framework that jointly captures the Instance- and Scene-levelcontextual information. IS-Fusion essentially differs from existing approachesthat only focus on the BEV scene-level fusion by explicitly incorporatinginstance-level multimodal information, thus facilitating the instance-centrictasks like 3D object detection. It comprises a Hierarchical Scene Fusion (HSF)module and an Instance-Guided Fusion (IGF) module. HSF applies Point-to-Gridand Grid-to-Region transformers to capture the multimodal scene context atdifferent granularities. IGF mines instance candidates, explores theirrelationships, and aggregates the local multimodal context for each instance.These instances then serve as guidance to enhance the scene feature and yieldan instance-aware BEV representation. On the challenging nuScenes benchmark,IS-Fusion outperforms all the published multimodal works to date. Code isavailable at: https://github.com/yinjunbo/IS-Fusion.</description><author>Junbo Yin, Jianbing Shen, Runnan Chen, Wei Li, Ruigang Yang, Pascal Frossard, Wenguan Wang</author><pubDate>Fri, 22 Mar 2024 15:34:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15241v1</guid></item><item><title>Guided Decoding for Robot Motion Generation and Adaption</title><link>http://arxiv.org/abs/2403.15239v1</link><description>We address motion generation for high-DoF robot arms in complex settings withobstacles, via points, etc. A significant advancement in this domain isachieved by integrating Learning from Demonstration (LfD) into the motiongeneration process. This integration facilitates rapid adaptation to new tasksand optimizes the utilization of accumulated expertise by allowing robots tolearn and generalize from demonstrated trajectories. We train a transformer architecture on a large dataset of simulatedtrajectories. This architecture, based on a conditional variational autoencodertransformer, learns essential motion generation skills and adapts these to meetauxiliary tasks and constraints. Our auto-regressive approach enables real-timeintegration of feedback from the physical system, enhancing the adaptabilityand efficiency of motion generation. We show that our model can generate motionfrom initial and target points, but also that it can adapt trajectories innavigating complex tasks, including obstacle avoidance, via points, and meetingvelocity and acceleration constraints, across platforms.</description><author>Nutan Chen, Elie Aljalbout, Botond Cseke, Patrick van der Smagt</author><pubDate>Fri, 22 Mar 2024 15:32:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15239v1</guid></item><item><title>WEEP: A method for spatial interpretation of weakly supervised CNN models in computational pathology</title><link>http://arxiv.org/abs/2403.15238v1</link><description>Deep learning enables the modelling of high-resolution histopathologywhole-slide images (WSI). Weakly supervised learning of tile-level data istypically applied for tasks where labels only exist on the patient or WSI level(e.g. patient outcomes or histological grading). In this context, there is aneed for improved spatial interpretability of predictions from such models. Wepropose a novel method, Wsi rEgion sElection aPproach (WEEP), for modelinterpretation. It provides a principled yet straightforward way to establishthe spatial area of WSI required for assigning a particular prediction label.We demonstrate WEEP on a binary classification task in the area of breastcancer computational pathology. WEEP is easy to implement, is directlyconnected to the model-based decision process, and offers information relevantto both research and diagnostic applications.</description><author>Abhinav Sharma, Bojing Liu, Mattias Rantalainen</author><pubDate>Fri, 22 Mar 2024 15:32:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15238v1</guid></item><item><title>Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving $O(1/k)$ Finite-Sample Complexity</title><link>http://arxiv.org/abs/2401.12764v3</link><description>This paper proposes to develop a new variant of the two-time-scale stochasticapproximation to find the roots of two coupled nonlinear operators, assumingonly noisy samples of these operators can be observed. Our key idea is toleverage the classic Ruppert-Polyak averaging technique to dynamically estimatethe operators through their samples. The estimated values of these averagingsteps will then be used in the two-time-scale stochastic approximation updatesto find the desired solution. Our main theoretical result is to show that underthe strongly monotone condition of the underlying nonlinear operators themean-squared errors of the iterates generated by the proposed method convergeto zero at an optimal rate $O(1/k)$, where $k$ is the number of iterations. Ourresult significantly improves the existing result of two-time-scale stochasticapproximation, where the best known finite-time convergence rate is$O(1/k^{2/3})$. We illustrate this result by applying the proposed method todevelop new reinforcement learning algorithms with improved performance.</description><author>Thinh T. Doan</author><pubDate>Fri, 22 Mar 2024 15:29:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12764v3</guid></item><item><title>Multi-perspective Memory Enhanced Network for Identifying Key Nodes in Social Networks</title><link>http://arxiv.org/abs/2403.15235v1</link><description>Identifying key nodes in social networks plays a crucial role in timelyblocking false information. Existing key node identification methods usuallyconsider node influence only from the propagation structure perspective andhave insufficient generalization ability to unknown scenarios. In this paper,we propose a novel Multi-perspective Memory Enhanced Network (MMEN) foridentifying key nodes in social networks, which mines key nodes from multipleperspectives and utilizes memory networks to store historical information.Specifically, MMEN first constructs two propagation networks from theperspectives of user attributes and propagation structure and updates nodefeature representations using graph attention networks. Meanwhile, the memorynetwork is employed to store information of similar subgraphs, enhancing themodel's generalization performance in unknown scenarios. Finally, MMEN appliesadaptive weights to combine the node influence of the two propagation networksto select the ultimate key nodes. Extensive experiments demonstrate that ourmethod significantly outperforms previous methods.</description><author>Qiang Zhang, Jiawei Liu, Fanrui Zhang, Xiaoling Zhu, Zheng-Jun Zha</author><pubDate>Fri, 22 Mar 2024 15:29:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15235v1</guid></item><item><title>Shadow Generation for Composite Image Using Diffusion model</title><link>http://arxiv.org/abs/2403.15234v1</link><description>In the realm of image composition, generating realistic shadow for theinserted foreground remains a formidable challenge. Previous works havedeveloped image-to-image translation models which are trained on pairedtraining data. However, they are struggling to generate shadows with accurateshapes and intensities, hindered by data scarcity and inherent task complexity.In this paper, we resort to foundation model with rich prior knowledge ofnatural shadow images. Specifically, we first adapt ControlNet to our task andthen propose intensity modulation modules to improve the shadow intensity.Moreover, we extend the small-scale DESOBA dataset to DESOBAv2 using a noveldata acquisition pipeline. Experimental results on both DESOBA and DESOBAv2datasets as well as real composite images demonstrate the superior capabilityof our model for shadow generation task. The dataset, code, and model arereleased at https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2.</description><author>Qingyang Liu, Junqi You, Jianting Wang, Xinhao Tao, Bo Zhang, Li Niu</author><pubDate>Fri, 22 Mar 2024 15:27:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15234v1</guid></item><item><title>ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal Instance Segmentation</title><link>http://arxiv.org/abs/2403.11376v2</link><description>Amodal Instance Segmentation (AIS) presents a challenging task as it involvespredicting both visible and occluded parts of objects within images. ExistingAIS methods rely on a bidirectional approach, encompassing both the transitionfrom amodal features to visible features (amodal-to-visible) and from visiblefeatures to amodal features (visible-to-amodal). Our observation shows that theutilization of amodal features through the amodal-to-visible can confuse thevisible features due to the extra information of occluded/hidden segments notpresented in visible display. Consequently, this compromised quality of visiblefeatures during the subsequent visible-to-amodal transition. To tackle thisissue, we introduce ShapeFormer, a decoupled Transformer-based model with avisible-to-amodal transition. It facilitates the explicit relationship betweenoutput segmentations and avoids the need for amodal-to-visible transitions.ShapeFormer comprises three key modules: (i) Visible-Occluding Mask Head forpredicting visible segmentation with occlusion awareness, (ii) Shape-PriorAmodal Mask Head for predicting amodal and occluded masks, and (iii)Category-Specific Shape Prior Retriever aims to provide shape prior knowledge.Comprehensive experiments and extensive ablation studies across various AISbenchmarks demonstrate the effectiveness of our ShapeFormer. The code isavailable at: https://github.com/UARK-AICV/ShapeFormer</description><author>Minh Tran, Winston Bounsavy, Khoa Vo, Anh Nguyen, Tri Nguyen, Ngan Le</author><pubDate>Fri, 22 Mar 2024 15:25:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11376v2</guid></item><item><title>An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets</title><link>http://arxiv.org/abs/2403.15230v1</link><description>Does the training of large language models potentially infringe upon codelicenses? Furthermore, are there any datasets available that can be safely usedfor training these models without violating such licenses? In our study, weassess the current trends in the field and the importance of incorporating codeinto the training of large language models. Additionally, we examine publiclyavailable datasets to see whether these models can be trained on them withoutthe risk of legal issues in the future. To accomplish this, we compiled a listof 53 large language models trained on file-level code. We then extracted theirdatasets and analyzed how much they overlap with a dataset we created,consisting exclusively of strong copyleft code. Our analysis revealed that every dataset we examined contained licenseinconsistencies, despite being selected based on their associated repositorylicenses. We analyzed a total of 514 million code files, discovering 38 millionexact duplicates present in our strong copyleft dataset. Additionally, weexamined 171 million file-leading comments, identifying 16 million with strongcopyleft licenses and another 11 million comments that discouraged copyingwithout explicitly mentioning a license. Based on the findings of our study,which highlights the pervasive issue of license inconsistencies in largelanguage models trained on code, our recommendation for both researchers andthe community is to prioritize the development and adoption of best practicesfor dataset creation and management.</description><author>Jonathan Katzy, Răzvan-Mihai Popescu, Arie van Deursen, Maliheh Izadi</author><pubDate>Fri, 22 Mar 2024 15:23:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15230v1</guid></item><item><title>LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example</title><link>http://arxiv.org/abs/2403.15227v1</link><description>Recent advances in 3D face stylization have made significant strides in fewto zero-shot settings. However, the degree of stylization achieved by existingmethods is often not sufficient for practical applications because they aremostly based on statistical 3D Morphable Models (3DMM) with limited variations.To this end, we propose a method that can produce a highly stylized 3D facemodel with desired topology. Our methods train a surface deformation networkwith 3DMM and translate its domain to the target style using a paired exemplar.The network achieves stylization of the 3D face mesh by mimicking the style ofthe target using a differentiable renderer and directional CLIP losses.Additionally, during the inference process, we utilize a Mesh Agnostic Encoder(MAGE) that takes deformation target, a mesh of diverse topologies as input tothe stylization process and encodes its shape into our latent space. Theresulting stylized face model can be animated by commonly used 3DMM blendshapes. A set of quantitative and qualitative evaluations demonstrate that ourmethod can produce highly stylized face meshes according to a given style andoutput them in a desired topology. We also demonstrate example applications ofour method including image-based stylized avatar generation, linearinterpolation of geometric styles, and facial animation of stylized avatars.</description><author>Soyeon Yoon, Kwan Yun, Kwanggyoon Seo, Sihun Cha, Jung Eun Yoo, Junyong Noh</author><pubDate>Fri, 22 Mar 2024 15:20:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15227v1</guid></item></channel></rss>