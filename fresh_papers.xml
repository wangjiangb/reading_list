<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 12 Mar 2024 06:00:13 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling</title><link>http://arxiv.org/abs/2403.06978v1</link><description>In this paper, we introduce Attention Prompt Tuning (APT) - a computationallyefficient variant of prompt tuning for video-based applications such as actionrecognition. Prompt tuning approaches involve injecting a set of learnableprompts along with data tokens during fine-tuning while keeping the backbonefrozen. This approach greatly reduces the number of learnable parameterscompared to full tuning. For image-based downstream tasks, normally a couple oflearnable prompts achieve results close to those of full tuning. However,videos, which contain more complex spatiotemporal information, require hundredsof tunable prompts to achieve reasonably good results. This reduces theparameter efficiency observed in images and significantly increases latency andthe number of floating-point operations (FLOPs) during inference. To tacklethese issues, we directly inject the prompts into the keys and values of thenon-local attention mechanism within the transformer block. Additionally, weintroduce a novel prompt reparameterization technique to make APT more robustagainst hyperparameter selection. The proposed APT approach greatly reduces thenumber of FLOPs and latency while achieving a significant performance boostover the existing parameter-efficient tuning methods on UCF101, HMDB51, andSSv2 datasets for action recognition. The code and pre-trained models areavailable at https://github.com/wgcban/apt</description><author>Wele Gedara Chaminda Bandara, Vishal M. Patel</author><pubDate>Mon, 11 Mar 2024 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06978v1</guid></item><item><title>VideoMamba: State Space Model for Efficient Video Understanding</title><link>http://arxiv.org/abs/2403.06977v1</link><description>Addressing the dual challenges of local redundancy and global dependencies invideo understanding, this work innovatively adapts the Mamba to the videodomain. The proposed VideoMamba overcomes the limitations of existing 3Dconvolution neural networks and video transformers. Its linear-complexityoperator enables efficient long-term modeling, which is crucial forhigh-resolution long video understanding. Extensive evaluations revealVideoMamba's four core abilities: (1) Scalability in the visual domain withoutextensive dataset pretraining, thanks to a novel self-distillation technique;(2) Sensitivity for recognizing short-term actions even with fine-grainedmotion differences; (3) Superiority in long-term video understanding,showcasing significant advancements over traditional feature-based models; and(4) Compatibility with other modalities, demonstrating robustness inmulti-modal contexts. Through these distinct advantages, VideoMamba sets a newbenchmark for video understanding, offering a scalable and efficient solutionfor comprehensive video understanding. All the code and models are available athttps://github.com/OpenGVLab/VideoMamba.</description><author>Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, Yu Qiao</author><pubDate>Mon, 11 Mar 2024 18:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06977v1</guid></item><item><title>BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion</title><link>http://arxiv.org/abs/2403.06976v1</link><description>Image inpainting, the process of restoring corrupted images, has seensignificant advancements with the advent of diffusion models (DMs). Despitethese advancements, current DM adaptations for inpainting, which involvemodifications to the sampling strategy or the development ofinpainting-specific DMs, frequently suffer from semantic inconsistencies andreduced image quality. Addressing these challenges, our work introduces a novelparadigm: the division of masked image features and noisy latent into separatebranches. This division dramatically diminishes the model's learning load,facilitating a nuanced incorporation of essential masked image information in ahierarchical fashion. Herein, we present BrushNet, a novel plug-and-playdual-branch model engineered to embed pixel-level masked image features intoany pre-trained DM, guaranteeing coherent and enhanced image inpaintingoutcomes. Additionally, we introduce BrushData and BrushBench to facilitatesegmentation-based inpainting training and performance assessment. Ourextensive experimental analysis demonstrates BrushNet's superior performanceover existing models across seven key metrics, including image quality, maskregion preservation, and textual coherence.</description><author>Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, Qiang Xu</author><pubDate>Mon, 11 Mar 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06976v1</guid></item><item><title>Memory-based Adapters for Online 3D Scene Perception</title><link>http://arxiv.org/abs/2403.06974v1</link><description>In this paper, we propose a new framework for online 3D scene perception.Conventional 3D scene perception methods are offline, i.e., take an alreadyreconstructed 3D scene geometry as input, which is not applicable in roboticapplications where the input data is streaming RGB-D videos rather than acomplete 3D scene reconstructed from pre-collected RGB-D videos. To deal withonline 3D scene perception tasks where data collection and perception should beperformed simultaneously, the model should be able to process 3D scenes frameby frame and make use of the temporal information. To this end, we propose anadapter-based plug-and-play module for the backbone of 3D scene perceptionmodel, which constructs memory to cache and aggregate the extracted RGB-Dfeatures to empower offline models with temporal learning ability.Specifically, we propose a queued memory mechanism to cache the supportingpoint cloud and image features. Then we devise aggregation modules whichdirectly perform on the memory and pass temporal information to current frame.We further propose 3D-to-2D adapter to enhance image features with strongglobal context. Our adapters can be easily inserted into mainstream offlinearchitectures of different tasks and significantly boost their performance ononline tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrateour approach achieves leading performance on three 3D scene perception taskscompared with state-of-the-art online methods by simply finetuning existingoffline models, without any model and task-specific designs.\href{https://xuxw98.github.io/Online3D/}{Project page}.</description><author>Xiuwei Xu, Chong Xia, Ziwei Wang, Linqing Zhao, Yueqi Duan, Jie Zhou, Jiwen Lu</author><pubDate>Mon, 11 Mar 2024 18:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06974v1</guid></item><item><title>Bayesian Diffusion Models for 3D Shape Reconstruction</title><link>http://arxiv.org/abs/2403.06973v1</link><description>We present Bayesian Diffusion Models (BDM), a prediction algorithm thatperforms effective Bayesian inference by tightly coupling the top-down (prior)information with the bottom-up (data-driven) procedure via joint diffusionprocesses. We show the effectiveness of BDM on the 3D shape reconstructiontask. Compared to prototypical deep learning data-driven approaches trained onpaired (supervised) data-labels (e.g. image-point clouds) datasets, our BDMbrings in rich prior information from standalone labels (e.g. point clouds) toimprove the bottom-up 3D reconstruction. As opposed to the standard Bayesianframeworks where explicit prior and likelihood are required for the inference,BDM performs seamless information fusion via coupled diffusion processes withlearned gradient computation networks. The specialty of our BDM lies in itscapability to engage the active and effective information exchange and fusionof the top-down and bottom-up processes where each itself is a diffusionprocess. We demonstrate state-of-the-art results on both synthetic andreal-world benchmarks for 3D shape reconstruction.</description><author>Haiyang Xu, Yu Lei, Zeyuan Chen, Xiang Zhang, Yue Zhao, Yilin Wang, Zhuowen Tu</author><pubDate>Mon, 11 Mar 2024 18:55:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06973v1</guid></item><item><title>A representation-learning game for classes of prediction tasks</title><link>http://arxiv.org/abs/2403.06971v1</link><description>We propose a game-based formulation for learning dimensionality-reducingrepresentations of feature vectors, when only a prior knowledge on futureprediction tasks is available. In this game, the first player chooses arepresentation, and then the second player adversarially chooses a predictiontask from a given class, representing the prior knowledge. The first playeraims is to minimize, and the second player to maximize, the regret: The minimalprediction loss using the representation, compared to the same loss using theoriginal features. For the canonical setting in which the representation, theresponse to predict and the predictors are all linear functions, and under themean squared error loss function, we derive the theoretically optimalrepresentation in pure strategies, which shows the effectiveness of the priorknowledge, and the optimal regret in mixed strategies, which shows theusefulness of randomizing the representation. For general representations andloss functions, we propose an efficient algorithm to optimize a randomizedrepresentation. The algorithm only requires the gradients of the loss function,and is based on incrementally adding a representation rule to a mixture of suchrules.</description><author>Neria Uzan, Nir Weinberger</author><pubDate>Mon, 11 Mar 2024 18:54:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06971v1</guid></item><item><title>MRL Parsing Without Tears: The Case of Hebrew</title><link>http://arxiv.org/abs/2403.06970v1</link><description>Syntactic parsing remains a critical tool for relation extraction andinformation extraction, especially in resource-scarce languages where LLMs arelacking. Yet in morphologically rich languages (MRLs), where parsers need toidentify multiple lexical units in each token, existing systems suffer inlatency and setup complexity. Some use a pipeline to peel away the layers:first segmentation, then morphology tagging, and then syntax parsing; however,errors in earlier layers are then propagated forward. Others use a jointarchitecture to evaluate all permutations at once; while this improvesaccuracy, it is notoriously slow. In contrast, and taking Hebrew as a testcase, we present a new "flipped pipeline": decisions are made directly on thewhole-token units by expert classifiers, each one dedicated to one specifictask. The classifiers are independent of one another, and only at the end do wesynthesize their predictions. This blazingly fast approach sets a new SOTA inHebrew POS tagging and dependency parsing, while also reaching near-SOTAperformance on other Hebrew NLP tasks. Because our architecture does not relyon any language-specific resources, it can serve as a model to develop similarparsers for other MRLs.</description><author>Shaltiel Shmidman, Avi Shmidman, Moshe Koppel, Reut Tsarfaty</author><pubDate>Mon, 11 Mar 2024 18:54:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06970v1</guid></item><item><title>Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models</title><link>http://arxiv.org/abs/2402.05210v3</link><description>Diffusion models have enabled remarkably high-quality medical imagegeneration, yet it is challenging to enforce anatomical constraints ingenerated images. This hampers many useful applications, includingpre-registered image generation, counterfactual scenarios, and others. To thisend, we propose a diffusion model-based method that supportsanatomically-controllable medical image generation, by following a multi-classanatomical segmentation mask at each sampling step. We additionally introduce arandom mask ablation training algorithm to enable conditioning on a selectedcombination of anatomical constraints while allowing flexibility in otheranatomical areas. We compare our model ("Seg-Diff") to existing methods onbreast MRI and abdominal/neck-to-pelvis CT datasets with a wide range ofanatomical objects. Results show that it reaches a new state-of-the-art in thefaithfulness of generated images to input anatomical masks on both datasets,and is on par for general anatomical realism. Finally, our model also enjoysthe extra benefit of being able to adjust the anatomical similarity ofgenerated images to real images of choice through interpolation in its latentspace.</description><author>Nicholas Konz, Yuwen Chen, Haoyu Dong, Maciej A. Mazurowski</author><pubDate>Mon, 11 Mar 2024 18:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05210v3</guid></item><item><title>Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts</title><link>http://arxiv.org/abs/2403.06966v1</link><description>Reinforcement learning (RL) is a powerful approach for acquiring agood-performing policy. However, learning diverse skills is challenging in RLdue to the commonly used Gaussian policy parameterization. We propose\textbf{Di}verse \textbf{Skil}l \textbf{L}earning (Di-SkilL), an RL method forlearning diverse skills using Mixture of Experts, where each expert formalizesa skill as a contextual motion primitive. Di-SkilL optimizes each expert andits associate context distribution to a maximum entropy objective thatincentivizes learning diverse skills in similar contexts. The per-expertcontext distribution enables automatic curricula learning, allowing each expertto focus on its best-performing sub-region of the context space. To overcomehard discontinuities and multi-modalities without any prior knowledge of theenvironment's unknown context probability space, we leverage energy-basedmodels to represent the per-expert context distributions and demonstrate how wecan efficiently train them using the standard policy gradient objective. Weshow on challenging robot simulation tasks that Di-SkilL can learn diverse andperformant skills.</description><author>Onur Celik, Aleksandar Taranovic, Gerhard Neumann</author><pubDate>Mon, 11 Mar 2024 18:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06966v1</guid></item><item><title>Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena</title><link>http://arxiv.org/abs/2403.06965v1</link><description>Argument Structure Constructions (ASCs) are one of the most well-studiedconstruction groups, providing a unique opportunity to demonstrate theusefulness of Construction Grammar (CxG). For example, the caused-motionconstruction (CMC, ``She sneezed the foam off her cappuccino'') demonstratesthat constructions must carry meaning, otherwise the fact that ``sneeze'' inthis context causes movement cannot be explained. We form the hypothesis thatthis remains challenging even for state-of-the-art Large Language Models(LLMs), for which we devise a test based on substituting the verb with aprototypical motion verb. To be able to perform this test at statisticallysignificant scale, in the absence of adequate CxG corpora, we develop a novelpipeline of NLP-assisted collection of linguistically annotated text. We showhow dependency parsing and GPT-3.5 can be used to significantly reduceannotation cost and thus enable the annotation of rare phenomena at scale. Wethen evaluate GPT, Gemini, Llama2 and Mistral models for their understanding ofthe CMC using the newly collected corpus. We find that all models struggle withunderstanding the motion component that the CMC adds to a sentence.</description><author>Leonie Weissweiler, Abdullatif Köksal, Hinrich Schütze</author><pubDate>Mon, 11 Mar 2024 18:47:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06965v1</guid></item><item><title>The pitfalls of next-token prediction</title><link>http://arxiv.org/abs/2403.06963v1</link><description>Can a mere next-token predictor faithfully model human intelligence? Wecrystallize this intuitive concern, which is fragmented in the literature. As astarting point, we argue that the two often-conflated phases of next-tokenprediction -- autoregressive inference and teacher-forced training -- must betreated distinctly. The popular criticism that errors can compound duringautoregressive inference, crucially assumes that teacher-forcing has learned anaccurate next-token predictor. This assumption sidesteps a more deep-rootedproblem we expose: in certain classes of tasks, teacher-forcing can simply failto learn an accurate next-token predictor in the first place. We describe ageneral mechanism of how teacher-forcing can fail, and design a minimalplanning task where both the Transformer and the Mamba architecture empiricallyfail in that manner -- remarkably, despite the task being straightforward tolearn. We provide preliminary evidence that this failure can be resolved whentraining to predict multiple tokens in advance. We hope this finding can groundfuture debates and inspire explorations beyond the next-token predictionparadigm. We make our code available underhttps://github.com/gregorbachmann/Next-Token-Failures</description><author>Gregor Bachmann, Vaishnavh Nagarajan</author><pubDate>Mon, 11 Mar 2024 18:47:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06963v1</guid></item><item><title>LLM Inference Unveiled: Survey and Roofline Model Insights</title><link>http://arxiv.org/abs/2402.16363v4</link><description>The field of efficient Large Language Model (LLM) inference is rapidlyevolving, presenting a unique blend of opportunities and challenges. Althoughthe field has expanded and is vibrant, there hasn't been a concise frameworkthat analyzes the various methods of LLM Inference to provide a clearunderstanding of this domain. Our survey stands out from traditional literaturereviews by not only summarizing the current state of research but also byintroducing a framework based on roofline model for systematic analysis of LLMinference techniques. This framework identifies the bottlenecks when deployingLLMs on hardware devices and provides a clear understanding of practicalproblems, such as why LLMs are memory-bound, how much memory and computationthey need, and how to choose the right hardware. We systematically collate thelatest advancements in efficient LLM inference, covering crucial areas such asmodel compression (e.g., Knowledge Distillation and Quantization), algorithmimprovements (e.g., Early Exit and Mixture-of-Expert), and both hardware andsystem-level enhancements. Our survey stands out by analyzing these methodswith roofline model, helping us understand their impact on memory access andcomputation. This distinctive approach not only showcases the current researchlandscape but also delivers valuable insights for practical implementation,positioning our work as an indispensable resource for researchers new to thefield as well as for those seeking to deepen their understanding of efficientLLM deployment. The analyze tool, LLM-Viewer, is open-sourced.</description><author>Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen, Guangyu Sun, Kurt Keutzer</author><pubDate>Mon, 11 Mar 2024 18:46:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16363v4</guid></item><item><title>Explainable Transformer Prototypes for Medical Diagnoses</title><link>http://arxiv.org/abs/2403.06961v1</link><description>Deployments of artificial intelligence in medical diagnostics mandate notjust accuracy and efficacy but also trust, emphasizing the need forexplainability in machine decisions. The recent trend in automated medicalimage diagnostics leans towards the deployment of Transformer-basedarchitectures, credited to their impressive capabilities. Since theself-attention feature of transformers contributes towards identifying crucialregions during the classification process, they enhance the trustability of themethods. However, the complex intricacies of these attention mechanisms mayfall short of effectively pinpointing the regions of interest directlyinfluencing AI decisions. Our research endeavors to innovate a unique attentionblock that underscores the correlation between 'regions' rather than 'pixels'.To address this challenge, we introduce an innovative system grounded inprototype learning, featuring an advanced self-attention mechanism that goesbeyond conventional ad-hoc visual explanation techniques by offeringcomprehensible visual insights. A combined quantitative and qualitativemethodological approach was used to demonstrate the effectiveness of theproposed method on the large-scale NIH chest X-ray dataset. Experimentalresults showed that our proposed method offers a promising direction forexplainability, which can lead to the development of more trustable systems,which can facilitate easier and rapid adoption of such technology into routineclinics. The code is available at www.github.com/NUBagcilab/r2r_proto.</description><author>Ugur Demir, Debesh Jha, Zheyuan Zhang, Elif Keles, Bradley Allen, Aggelos K. Katsaggelos, Ulas Bagci</author><pubDate>Mon, 11 Mar 2024 18:46:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06961v1</guid></item><item><title>Accurate Crystal Structure Prediction of New 2D Hybrid Organic Inorganic Perovskites</title><link>http://arxiv.org/abs/2403.06955v1</link><description>Low dimensional hybrid organic-inorganic perovskites (HOIPs) represent apromising class of electronically active materials for both light absorptionand emission. The design space of HOIPs is extremely large, since a diversespace of organic cations can be combined with different inorganic frameworks.This immense design space allows for tunable electronic and mechanicalproperties, but also necessitates the development of new tools for in silicohigh throughput analysis of candidate structures. In this work, we present anaccurate, efficient, transferable and widely applicable machine learninginteratomic potential (MLIP) for predicting the structure of new 2D HOIPs.Using the MACE architecture, an MLIP is trained on 86 diverse experimentallyreported HOIP structures. The model is tested on 73 unseen perovskitecompositions, and achieves chemical accuracy with respect to the referenceelectronic structure method. Our model is then combined with a simple randomstructure search algorithm to predict the structure of hypothetical HOIPs givenonly the proposed composition. Success is demonstrated by correctly andreliably recovering the crystal structure of a set of experimentally known 2Dperovskites. Such a random structure search is impossible with ab initiomethods due to the associated computational cost, but is relatively inexpensivewith the MACE potential. Finally, the procedure is used to predict thestructure formed by a new organic cation with no previously known correspondingperovskite. Laboratory synthesis of the new hybrid perovskite confirms theaccuracy of our prediction. This capability, applied at scale, enablesefficient screening of thousands of combinations of organic cations andinorganic layers.</description><author>Nima Karimitari, William J. Baldwin, Evan W. Muller, Zachary J. L. Bare, W. Joshua Kennedy, Gábor Csányi, Christopher Sutton</author><pubDate>Mon, 11 Mar 2024 18:39:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06955v1</guid></item><item><title>Sharpened Lazy Incremental Quasi-Newton Method</title><link>http://arxiv.org/abs/2305.17283v2</link><description>The problem of minimizing the sum of $n$ functions in $d$ dimensions isubiquitous in machine learning and statistics. In many applications where thenumber of observations $n$ is large, it is necessary to use incremental orstochastic methods, as their per-iteration cost is independent of $n$. Ofthese, Quasi-Newton (QN) methods strike a balance between the per-iterationcost and the convergence rate. Specifically, they exhibit a superlinear ratewith $O(d^2)$ cost in contrast to the linear rate of first-order methods with$O(d)$ cost and the quadratic rate of second-order methods with $O(d^3)$ cost.However, existing incremental methods have notable shortcomings: IncrementalQuasi-Newton (IQN) only exhibits asymptotic superlinear convergence. Incontrast, Incremental Greedy BFGS (IGS) offers explicit superlinear convergencebut suffers from poor empirical performance and has a per-iteration cost of$O(d^3)$. To address these issues, we introduce the Sharpened Lazy IncrementalQuasi-Newton Method (SLIQN) that achieves the best of both worlds: an explicitsuperlinear convergence rate, and superior empirical performance at aper-iteration $O(d^2)$ cost. SLIQN features two key changes: first, itincorporates a hybrid strategy of using both classic and greedy BFGS updates,allowing it to empirically outperform both IQN and IGS. Second, it employs aclever constant multiplicative factor along with a lazy propagation strategy,which enables it to have a cost of $O(d^2)$. Additionally, our experimentsdemonstrate the superiority of SLIQN over other incremental and stochasticQuasi-Newton variants and establish its competitiveness with second-orderincremental methods.</description><author>Aakash Lahoti, Spandan Senapati, Ketan Rajawat, Alec Koppel</author><pubDate>Mon, 11 Mar 2024 18:37:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17283v2</guid></item><item><title>Optimizing Latent Graph Representations of Surgical Scenes for Zero-Shot Domain Transfer</title><link>http://arxiv.org/abs/2403.06953v1</link><description>Purpose: Advances in deep learning have resulted in effective models forsurgical video analysis; however, these models often fail to generalize acrossmedical centers due to domain shift caused by variations in surgical workflow,camera setups, and patient demographics. Recently, object-centric learning hasemerged as a promising approach for improved surgical scene understanding,capturing and disentangling visual and semantic properties of surgical toolsand anatomy to improve downstream task performance. In this work, we conduct amulti-centric performance benchmark of object-centric approaches, focusing onCritical View of Safety assessment in laparoscopic cholecystectomy, thenpropose an improved approach for unseen domain generalization. Methods: We evaluate four object-centric approaches for domaingeneralization, establishing baseline performance. Next, leveraging thedisentangled nature of object-centric representations, we dissect one of thesemethods through a series of ablations (e.g. ignoring either visual or semanticfeatures for downstream classification). Finally, based on the results of theseablations, we develop an optimized method specifically tailored for domaingeneralization, LG-DG, that includes a novel disentanglement loss function. Results: Our optimized approach, LG-DG, achieves an improvement of 9.28% overthe best baseline approach. More broadly, we show that object-centricapproaches are highly effective for domain generalization thanks to theirmodular approach to representation learning. Conclusion: We investigate the use of object-centric methods for unseendomain generalization, identify method-agnostic factors critical forperformance, and present an optimized approach that substantially outperformsexisting methods.</description><author>Siddhant Satyanaik, Aditya Murali, Deepak Alapatt, Xin Wang, Pietro Mascagni, Nicolas Padoy</author><pubDate>Mon, 11 Mar 2024 18:36:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06953v1</guid></item><item><title>SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data</title><link>http://arxiv.org/abs/2403.06952v1</link><description>Recent text-to-image (T2I) generation models have demonstrated impressivecapabilities in creating images from text descriptions. However, these T2Igeneration models often fall short of generating images that precisely matchthe details of the text inputs, such as incorrect spatial relationship ormissing objects. In this paper, we introduce SELMA: Skill-Specific ExpertLearning and Merging with Auto-Generated Data, a novel paradigm to improve thefaithfulness of T2I models by fine-tuning models on automatically generated,multi-skill image-text datasets, with skill-specific expert learning andmerging. First, SELMA leverages an LLM's in-context learning capability togenerate multiple datasets of text prompts that can teach different skills, andthen generates the images with a T2I model based on the prompts. Next, SELMAadapts the T2I model to the new skills by learning multiple single-skill LoRA(low-rank adaptation) experts followed by expert merging. Our independentexpert fine-tuning specializes multiple models for different skills, and expertmerging helps build a joint multi-skill T2I model that can generate faithfulimages given diverse text prompts, while mitigating the knowledge conflict fromdifferent datasets. We empirically demonstrate that SELMA significantlyimproves the semantic alignment and text faithfulness of state-of-the-art T2Idiffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), humanpreference metrics (PickScore, ImageReward, and HPS), as well as humanevaluation. Moreover, fine-tuning with image-text pairs auto-collected viaSELMA shows comparable performance to fine-tuning with ground truth data.Lastly, we show that fine-tuning with images from a weaker T2I model can helpimprove the generation quality of a stronger T2I model, suggesting promisingweak-to-strong generalization in T2I models.</description><author>Jialu Li, Jaemin Cho, Yi-Lin Sung, Jaehong Yoon, Mohit Bansal</author><pubDate>Mon, 11 Mar 2024 18:35:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06952v1</guid></item><item><title>DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations</title><link>http://arxiv.org/abs/2403.06951v1</link><description>The diffusion-based text-to-image model harbors immense potential intransferring reference style. However, current encoder-based approachessignificantly impair the text controllability of text-to-image models whiletransferring styles. In this paper, we introduce \textit{DEADiff} to addressthis issue using the following two strategies: 1) a mechanism to decouple thestyle and semantics of reference images. The decoupled feature representationsare first extracted by Q-Formers which are instructed by different textdescriptions. Then they are injected into mutually exclusive subsets ofcross-attention layers for better disentanglement. 2) A non-reconstructivelearning method. The Q-Formers are trained using paired images rather than theidentical target, in which the reference image and the ground-truth image arewith the same style or semantics. We show that DEADiff attains the best visualstylization results and optimal balance between the text controllabilityinherent in the text-to-image model and style similarity to the referenceimage, as demonstrated both quantitatively and qualitatively. Our project pageis~\href{https://tianhao-qi.github.io/DEADiff/}{https://tianhao-qi.github.io/DEADiff/}.</description><author>Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang Chen, Qian He, Yongdong Zhang</author><pubDate>Mon, 11 Mar 2024 18:35:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06951v1</guid></item><item><title>Applicability of oculomics for individual risk prediction: Repeatability and robustness of retinal Fractal Dimension using DART and AutoMorph</title><link>http://arxiv.org/abs/2403.06950v1</link><description>Purpose: To investigate whether Fractal Dimension (FD)-based oculomics couldbe used for individual risk prediction by evaluating repeatability androbustness. Methods: We used two datasets: Caledonia, healthy adults imagedmultiple times in quick succession for research (26 subjects, 39 eyes, 377colour fundus images), and GRAPE, glaucoma patients with baseline and follow-upvisits (106 subjects, 196 eyes, 392 images). Mean follow-up time was 18.3months in GRAPE, thus it provides a pessimistic lower-bound as vasculaturecould change. FD was computed with DART and AutoMorph. Image quality wasassessed with QuickQual, but no images were initially excluded. Pearson,Spearman, and Intraclass Correlation (ICC) were used for population-levelrepeatability. For individual-level repeatability, we introduce measurementnoise parameter {\lambda} which is within-eye Standard Deviation (SD) of FDmeasurements in units of between-eyes SD. Results: In Caledonia, ICC was 0.8153for DART and 0.5779 for AutoMorph, Pearson/Spearman correlation (first and lastimage) 0.7857/0.7824 for DART, and 0.3933/0.6253 for AutoMorph. In GRAPE,Pearson/Spearman correlation (first and next visit) was 0.7479/0.7474 for DART,and 0.7109/0.7208 for AutoMorph (all p&lt;0.0001). Median {\lambda} in Caledoniawithout exclusions was 3.55\% for DART and 12.65\% for AutoMorph, and improvedto up to 1.67\% and 6.64\% with quality-based exclusions, respectively. Qualityexclusions primarily mitigated large outliers. Worst quality in an eyecorrelated strongly with {\lambda} (Pearson 0.5350-0.7550, depending on datasetand method, all p&lt;0.0001). Conclusions: Repeatability was sufficient forindividual-level predictions in heterogeneous populations. DART performedbetter on all metrics and might be able to detect small, longitudinal changes,highlighting the potential of robust methods.</description><author>Justin Engelmann, Diana Moukaddem, Lucas Gago, Niall Strang, Miguel O. Bernabeu</author><pubDate>Mon, 11 Mar 2024 18:34:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06950v1</guid></item><item><title>Materials science in the era of large language models: a perspective</title><link>http://arxiv.org/abs/2403.06949v1</link><description>Large Language Models (LLMs) have garnered considerable interest due to theirimpressive natural language capabilities, which in conjunction with variousemergent properties make them versatile tools in workflows ranging from complexcode generation to heuristic finding for combinatorial problems. In this paperwe offer a perspective on their applicability to materials science research,arguing their ability to handle ambiguous requirements across a range of tasksand disciplines mean they could be a powerful tool to aid researchers. Wequalitatively examine basic LLM theory, connecting it to relevant propertiesand techniques in the literature before providing two case studies thatdemonstrate their use in task automation and knowledge extraction at-scale. Attheir current stage of development, we argue LLMs should be viewed less asoracles of novel insight, and more as tireless workers that can accelerate andunify exploration across domains. It is our hope that this paper canfamiliarise material science researchers with the concepts needed to leveragethese tools in their own research.</description><author>Ge Lei, Ronan Docherty, Samuel J. Cooper</author><pubDate>Mon, 11 Mar 2024 18:34:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06949v1</guid></item><item><title>Advancing Generalizable Remote Physiological Measurement through the Integration of Explicit and Implicit Prior Knowledge</title><link>http://arxiv.org/abs/2403.06947v1</link><description>Remote photoplethysmography (rPPG) is a promising technology that capturesphysiological signals from face videos, with potential applications in medicalhealth, emotional computing, and biosecurity recognition. The demand for rPPGtasks has expanded from demonstrating good performance on intra-dataset testingto cross-dataset testing (i.e., domain generalization). However, most existingmethods have overlooked the prior knowledge of rPPG, resulting in poorgeneralization ability. In this paper, we propose a novel framework thatsimultaneously utilizes explicit and implicit prior knowledge in the rPPG task.Specifically, we systematically analyze the causes of noise sources (e.g.,different camera, lighting, skin types, and movement) across different domainsand incorporate these prior knowledge into the network. Additionally, weleverage a two-branch network to disentangle the physiological featuredistribution from noises through implicit label correlation. Our extensiveexperiments demonstrate that the proposed method not only outperformsstate-of-the-art methods on RGB cross-dataset evaluation but also generalizeswell from RGB datasets to NIR datasets. The code is available athttps://github.com/keke-nice/Greip.</description><author>Yuting Zhang, Hao Lu, Xin Liu, Yingcong Chen, Kaishun Wu</author><pubDate>Mon, 11 Mar 2024 18:33:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06947v1</guid></item><item><title>Near-Optimal Non-Parametric Sequential Tests and Confidence Sequences with Possibly Dependent Observations</title><link>http://arxiv.org/abs/2212.14411v5</link><description>Sequential tests and their implied confidence sequences, which are valid atarbitrary stopping times, promise flexible statistical inference and on-the-flydecision making. However, strong guarantees are limited to parametricsequential tests that under-cover in practice or concentration-bound-basedsequences that over-cover and have suboptimal rejection times. In this work, weconsider classic delayed-start normal-mixture sequential probability ratiotests, and we provide the first asymptotic type-I-error andexpected-rejection-time guarantees under general non-parametric data generatingprocesses, where the asymptotics are indexed by the test's burn-in time. Thetype-I-error results primarily leverage a martingale strong invarianceprinciple and establish that these tests (and their implied confidencesequences) have type-I error rates asymptotically equivalent to the desired(possibly varying) $\alpha$-level. The expected-rejection-time resultsprimarily leverage an identity inspired by It\^o's lemma and imply that, incertain asymptotic regimes, the expected rejection time is asymptoticallyequivalent to the minimum possible among $\alpha$-level tests. We show how toapply our results to sequential inference on parameters defined by estimatingequations, such as average treatment effects. Together, our results establishthese (ostensibly parametric) tests as general-purpose, non-parametric, andnear-optimal. We illustrate this via numerical simulations and a real-dataapplication to A/B testing at Netflix.</description><author>Aurelien Bibaut, Nathan Kallus, Michael Lindon</author><pubDate>Mon, 11 Mar 2024 18:33:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.14411v5</guid></item><item><title>Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation</title><link>http://arxiv.org/abs/2403.06946v1</link><description>Large vision-language models (VLMs) like CLIP have demonstrated goodzero-shot learning performance in the unsupervised domain adaptation task. Yet,most transfer approaches for VLMs focus on either the language or visualbranches, overlooking the nuanced interplay between both modalities. In thiswork, we introduce a Unified Modality Separation (UniMoS) framework forunsupervised domain adaptation. Leveraging insights from modality gap studies,we craft a nimble modality separation network that distinctly disentanglesCLIP's features into language-associated and vision-associated components. Ourproposed Modality-Ensemble Training (MET) method fosters the exchange ofmodality-agnostic information while maintaining modality-specific nuances. Wealign features across domains using a modality discriminator. Comprehensiveevaluations on three benchmarks reveal our approach sets a new state-of-the-artwith minimal computational costs. Code: https://github.com/TL-UESTC/UniMoS</description><author>Xinyao Li, Yuke Li, Zhekai Du, Fengling Li, Ke Lu, Jingjing Li</author><pubDate>Mon, 11 Mar 2024 18:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06946v1</guid></item><item><title>OpenXAI: Towards a Transparent Evaluation of Model Explanations</title><link>http://arxiv.org/abs/2206.11104v4</link><description>While several types of post hoc explanation methods have been proposed inrecent literature, there is very little work on systematically benchmarkingthese methods. Here, we introduce OpenXAI, a comprehensive and extensibleopen-source framework for evaluating and benchmarking post hoc explanationmethods. OpenXAI comprises of the following key components: (i) a flexiblesynthetic data generator and a collection of diverse real-world datasets,pre-trained models, and state-of-the-art feature attribution methods, and (ii)open-source implementations of eleven quantitative metrics for evaluatingfaithfulness, stability (robustness), and fairness of explanation methods, inturn providing comparisons of several explanation methods across a wide varietyof metrics, models, and datasets. OpenXAI is easily extensible, as users canreadily evaluate custom explanation methods and incorporate them into ourleaderboards. Overall, OpenXAI provides an automated end-to-end pipeline thatnot only simplifies and standardizes the evaluation of post hoc explanationmethods, but also promotes transparency and reproducibility in benchmarkingthese methods. While the first release of OpenXAI supports only tabulardatasets, the explanation methods and metrics that we consider are generalenough to be applicable to other data modalities. OpenXAI datasets and models,implementations of state-of-the-art explanation methods and evaluation metrics,are publicly available at this GitHub link.</description><author>Chirag Agarwal, Dan Ley, Eshika Saxena, Satyapriya Krishna, Martin Pawelczyk, Nari Johnson, Isha Puri, Marinka Zitnik, Himabindu Lakkaraju</author><pubDate>Mon, 11 Mar 2024 18:31:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11104v4</guid></item><item><title>Universality of Linear Recurrences Followed by Non-linear Projections: Finite-Width Guarantees and Benefits of Complex Eigenvalues</title><link>http://arxiv.org/abs/2307.11888v2</link><description>Deep neural networks based on linear complex-valued RNNs interleaved withposition-wise MLPs are gaining traction as competitive approaches to sequencemodeling. Examples of such architectures include state-space models (SSMs) likeS4, LRU, and Mamba: recently proposed models that achieve promising performanceon text, genetics, and other data that require long-range reasoning. Despiteexperimental evidence highlighting these architectures' effectiveness andcomputational efficiency, their expressive power remains relatively unexplored,especially in connection to specific choices crucial in practice - e.g.,carefully designed initialization distribution and use of complex numbers. Inthis paper, we show that combining MLPs with both real or complex lineardiagonal recurrences leads to arbitrarily precise approximation of regularcausal sequence-to-sequence maps. At the heart of our proof, we rely on aseparation of concerns: the linear RNN provides a lossless encoding of theinput sequence, and the MLP performs non-linear processing on this encoding.While we show that using real diagonal linear recurrences is enough to achieveuniversality in this architecture, we prove that employing complex eigenvaluesnear unit disk - i.e., empirically the most successful strategy in SSMs -greatly helps the RNN in storing information. We connect this finding with thevanishing gradient issue and provide experimental evidence supporting ourclaims.</description><author>Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, Samuel L. Smith</author><pubDate>Mon, 11 Mar 2024 18:30:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11888v2</guid></item><item><title>Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI</title><link>http://arxiv.org/abs/2403.06942v1</link><description>Purpose This article presents a case for a next-generation grid monitoringand control system, leveraging recent advances in generative artificialintelligence (AI), machine learning, and statistical inference. Advancingbeyond earlier generations of wide-area monitoring systems built uponsupervisory control and data acquisition (SCADA) and synchrophasortechnologies, we argue for a monitoring and control framework based on thestreaming of continuous point-on-wave (CPOW) measurements with AI-powered datacompression and fault detection. Methods and Results: The architecture of the proposed design originates fromthe Wiener-Kallianpur innovation representation of a random process thattransforms causally a stationary random process into an innovation sequencewith independent and identically distributed random variables. This workpresents a generative AI approach that (i) learns an innovation autoencoderthat extracts innovation sequence from CPOW time series, (ii) compresses theCPOW streaming data with innovation autoencoder and subband coding, and (iii)detects unknown faults and novel trends via nonparametric sequential hypothesistesting. Conclusion: This work argues that conventional monitoring using SCADA andphasor measurement unit (PMU) technologies is ill-suited for a future grid withdeep penetration of inverter-based renewable generations and distributed energyresources. A monitoring system based on CPOW data streaming and AI dataanalytics should be the basic building blocks for situational awareness of ahighly dynamic future grid.</description><author>Lang Tong, Xinyi Wang, Qing Zhao</author><pubDate>Mon, 11 Mar 2024 18:28:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06942v1</guid></item><item><title>Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction</title><link>http://arxiv.org/abs/2403.06940v1</link><description>Alzheimer's Disease (AD) is a neurodegenerative condition characterized bydiverse progression rates among individuals, with changes in cortical thickness(CTh) closely linked to its progression. Accurately forecasting CThtrajectories can significantly enhance early diagnosis and interventionstrategies, providing timely care. However, the longitudinal data essential forthese studies often suffer from temporal sparsity and incompleteness,presenting substantial challenges in modeling the disease's progressionaccurately. Existing methods are limited, focusing primarily on datasetswithout missing entries or requiring predefined assumptions about CThprogression. To overcome these obstacles, we propose a conditional score-baseddiffusion model specifically designed to generate CTh trajectories with thegiven baseline information, such as age, sex, and initial diagnosis. Ourconditional diffusion model utilizes all available data during the trainingphase to make predictions based solely on baseline information during inferencewithout needing prior history about CTh progression. The prediction accuracy ofthe proposed CTh prediction pipeline using a conditional score-based model wascompared for sub-groups consisting of cognitively normal, mild cognitiveimpairment, and AD subjects. The Bland-Altman analysis shows ourdiffusion-based prediction model has a near-zero bias with narrow 95%confidential interval compared to the ground-truth CTh in 6-36 months. Inaddition, our conditional diffusion model has a stochastic generative nature,therefore, we demonstrated an uncertainty analysis of patient-specific CThprediction through multiple realizations.</description><author>Qing Xiao, Siyeop Yoon, Hui Ren, Matthew Tivnan, Lichao Sun, Quanzheng Li, Tianming Liu, Yu Zhang, Xiang Li</author><pubDate>Mon, 11 Mar 2024 18:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06940v1</guid></item><item><title>DT-DDNN: A Physical Layer Security Attack Detector in 5G RF Domain for CAVs</title><link>http://arxiv.org/abs/2403.02645v2</link><description>The Synchronization Signal Block (SSB) is a fundamental component of the 5GNew Radio (NR) air interface, crucial for the initial access procedure ofConnected and Automated Vehicles (CAVs), and serves several key purposes in thenetwork's operation. However, due to the predictable nature of SSBtransmission, including the Primary and Secondary Synchronization Signals (PSSand SSS), jamming attacks are critical threats. These attacks, which can beexecuted without requiring high power or complex equipment, pose substantialrisks to the 5G network, particularly as a result of the unencryptedtransmission of control signals. Leveraging RF domain knowledge, this workpresents a novel deep learning-based technique for detecting jammers in CAVnetworks. Unlike the existing jamming detection algorithms that mostly rely onnetwork parameters, we introduce a double-threshold deep learning jammingdetector by focusing on the SSB. The detection method is focused on RF domainfeatures and improves the robustness of the network without requiringintegration with the pre-existing network infrastructure. By integrating apreprocessing block to extract PSS correlation and energy per null resourceelements (EPNRE) characteristics, our method distinguishes between normal andjammed received signals with high precision. Additionally, by incorporating ofDiscrete Wavelet Transform (DWT), the efficacy of training and detection areoptimized. A double-threshold double Deep Neural Network (DT-DDNN) is alsointroduced to the architecture complemented by a deep cascade learning model toincrease the sensitivity of the model to variations of signal-to-jamming noiseratio (SJNR). Results show that the proposed method achieves 96.4% detectionrate in extra low jamming power, i.e., SJNR between 15 to 30 dB. Further,performance of DT-DDNN is validated by analyzing real 5G signals obtained froma practical testbed.</description><author>Ghazal Asemian, Mohammadreza Amini, Burak Kantarci, Melike Erol-Kantarci</author><pubDate>Mon, 11 Mar 2024 18:25:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02645v2</guid></item><item><title>Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation</title><link>http://arxiv.org/abs/2311.15100v2</link><description>In optimal transport (OT), a Monge map is known as a mapping that transportsa source distribution to a target distribution in the most cost-efficient way.Recently, multiple neural estimators for Monge maps have been developed andapplied in diverse unpaired domain translation tasks, e.g. in single-cellbiology and computer vision. However, the classic OT framework enforces massconservation, which makes it prone to outliers and limits its applicability inreal-world scenarios. The latter can be particularly harmful in OT domaintranslation tasks, where the relative position of a sample within adistribution is explicitly taken into account. While unbalanced OT tackles thischallenge in the discrete setting, its integration into neural Monge mapestimators has received limited attention. We propose a theoretically groundedmethod to incorporate unbalancedness into any Monge map estimator. We improveexisting estimators to model cell trajectories over time and to predictcellular responses to perturbations. Moreover, our approach seamlesslyintegrates with the OT flow matching (OT-FM) framework. While we show thatOT-FM performs competitively in image translation, we further improveperformance by incorporating unbalancedness (UOT-FM), which better preservesrelevant features. We hence establish UOT-FM as a principled method forunpaired image translation.</description><author>Luca Eyring, Dominik Klein, Théo Uscidda, Giovanni Palla, Niki Kilbertus, Zeynep Akata, Fabian Theis</author><pubDate>Mon, 11 Mar 2024 18:23:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15100v2</guid></item><item><title>Counterfactual Reasoning with Knowledge Graph Embeddings</title><link>http://arxiv.org/abs/2403.06936v1</link><description>Knowledge graph embeddings (KGEs) were originally developed to infer true butmissing facts in incomplete knowledge repositories. In this paper, we linkknowledge graph completion and counterfactual reasoning via our new task CFKGR.We model the original world state as a knowledge graph, hypothetical scenariosas edges added to the graph, and plausible changes to the graph as inferencesfrom logical rules. We create corresponding benchmark datasets, which containdiverse hypothetical scenarios with plausible changes to the original knowledgegraph and facts that should be retained. We develop COULDD, a general methodfor adapting existing knowledge graph embeddings given a hypothetical premise,and evaluate it on our benchmark. Our results indicate that KGEs learn patternsin the graph without explicit training. We further observe that KGEs adaptedwith COULDD solidly detect plausible counterfactual changes to the graph thatfollow these patterns. An evaluation on human-annotated data reveals that KGEsadapted with COULDD are mostly unable to recognize changes to the graph that donot follow learned inference rules. In contrast, ChatGPT mostly outperformsKGEs in detecting plausible changes to the graph but has poor knowledgeretention. In summary, CFKGR connects two previously distinct areas, namely KGcompletion and counterfactual reasoning.</description><author>Lena Zellinger, Andreas Stephan, Benjamin Roth</author><pubDate>Mon, 11 Mar 2024 18:21:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06936v1</guid></item><item><title>Naming, Describing, and Quantifying Visual Objects in Humans and LLMs</title><link>http://arxiv.org/abs/2403.06935v1</link><description>While human speakers use a variety of different expressions when describingthe same object in an image, giving rise to a distribution of plausible labelsdriven by pragmatic constraints, the extent to which current Vision \&amp; LanguageLarge Language Models (VLLMs) can mimic this crucial feature of language use isan open question. This applies to common, everyday objects, but it isparticularly interesting for uncommon or novel objects for which a categorylabel may be lacking or fuzzy. Furthermore, humans show clear productionpreferences for highly context-sensitive expressions, such as the quantifiers`few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) onthree categories (nouns, attributes, and quantifiers) where humans show greatsubjective variability concerning the distribution over plausible labels, usingdatasets and resources mostly under-explored in previous work. Our resultsreveal mixed evidence on the ability of VLLMs to capture human namingpreferences, with all models failing in tasks that require high-level reasoningsuch as assigning quantifiers.</description><author>Alberto Testoni, Juell Sprott, Sandro Pezzelle</author><pubDate>Mon, 11 Mar 2024 18:20:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06935v1</guid></item><item><title>ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis</title><link>http://arxiv.org/abs/2403.06932v1</link><description>Large language models (LLMs) have achieved commendable accomplishments invarious natural language processing tasks. However, LLMs still encountersignificant challenges when dealing with complex scenarios involving multipleentities. These challenges arise from the presence of implicit relationshipsthat demand multi-step reasoning. In this paper, we propose a novel approachERA-CoT, which aids LLMs in understanding context by capturing relationshipsbetween entities and supports the reasoning of diverse tasks throughChain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstratesthe superior performance of our proposed method compared to current CoTprompting methods, achieving a significant improvement of an average of 5.1\%on GPT3.5 compared to previous SOTA baselines. Our analysis indicates thatERA-CoT increases the LLM's understanding of entity relationships,significantly improves the accuracy of question answering, and enhances thereasoning ability of LLMs.</description><author>Yanming Liu, Xinyue Peng, Tianyu Du, Jianwei Yin, Weihao Liu, Xuhong Zhang</author><pubDate>Mon, 11 Mar 2024 18:18:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06932v1</guid></item><item><title>RSBA: Robust Statistical Backdoor Attack under Privilege-Constrained Scenarios</title><link>http://arxiv.org/abs/2304.10985v2</link><description>Learning-based systems have been demonstrated to be vulnerable to backdoorattacks, wherein malicious users manipulate model performance by injectingbackdoors into the target model and activating them with specific triggers.Previous backdoor attack methods primarily focused on two key metrics: attacksuccess rate and stealthiness. However, these methods often necessitatesignificant privileges over the target model, such as control over the trainingprocess, making them challenging to implement in real-world scenarios.Moreover, the robustness of existing backdoor attacks is not guaranteed, asthey prove sensitive to defenses such as image augmentations and modeldistillation. In this paper, we address these two limitations and introduceRSBA (Robust Statistical Backdoor Attack under Privilege-constrainedScenarios). The key insight of RSBA is that statistical features can naturallydivide images into different groups, offering a potential implementation oftriggers. This type of trigger is more robust than manually designed ones, asit is widely distributed in normal images. By leveraging these statisticaltriggers, RSBA enables attackers to conduct black-box attacks by solelypoisoning the labels or the images. We empirically and theoreticallydemonstrate the robustness of RSBA against image augmentations and modeldistillation. Experimental results show that RSBA achieves a 99.83\% attacksuccess rate in black-box scenarios. Remarkably, it maintains a high successrate even after model distillation, where attackers lack access to the trainingdataset of the student model (1.39\% success rate for baseline methods onaverage).</description><author>Xiaolei Liu, Ming Yi, Kangyi Ding, Bangzhou Xin, Yixiao Xu, Li Yan, Chao Shen</author><pubDate>Mon, 11 Mar 2024 18:14:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10985v2</guid></item><item><title>Simplicity Bias of Transformers to Learn Low Sensitivity Functions</title><link>http://arxiv.org/abs/2403.06925v1</link><description>Transformers achieve state-of-the-art accuracy and robustness across manytasks, but an understanding of the inductive biases that they have and howthose biases are different from other neural network architectures remainselusive. Various neural network architectures such as fully connected networkshave been found to have a simplicity bias towards simple functions of the data;one version of this simplicity bias is a spectral bias to learn simplefunctions in the Fourier space. In this work, we identify the notion ofsensitivity of the model to random changes in the input as a notion ofsimplicity bias which provides a unified metric to explain the simplicity andspectral bias of transformers across different data modalities. We show thattransformers have lower sensitivity than alternative architectures, such asLSTMs, MLPs and CNNs, across both vision and language tasks. We also show thatlow-sensitivity bias correlates with improved robustness; furthermore, it canalso be used as an efficient intervention to further improve the robustness oftransformers.</description><author>Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang, Vatsal Sharan</author><pubDate>Mon, 11 Mar 2024 18:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06925v1</guid></item><item><title>Unlink to Unlearn: Simplifying Edge Unlearning in GNNs</title><link>http://arxiv.org/abs/2402.10695v2</link><description>As concerns over data privacy intensify, unlearning in Graph Neural Networks(GNNs) has emerged as a prominent research frontier in academia. This conceptis pivotal in enforcing the \textit{right to be forgotten}, which entails theselective removal of specific data from trained GNNs upon user request. Ourresearch focuses on edge unlearning, a process of particular relevance toreal-world applications. Current state-of-the-art approaches like GNNDelete caneliminate the influence of specific edges yet suffer from\textit{over-forgetting}, which means the unlearning process inadvertentlyremoves excessive information beyond needed, leading to a significantperformance decline for remaining edges. Our analysis identifies the lossfunctions of GNNDelete as the primary source of over-forgetting and alsosuggests that loss functions may be redundant for effective edge unlearning.Building on these insights, we simplify GNNDelete to develop \textbf{Unlink toUnlearn} (UtU), a novel method that facilitates unlearning exclusively throughunlinking the forget edges from graph structure. Our extensive experimentsdemonstrate that UtU delivers privacy protection on par with that of aretrained model while preserving high accuracy in downstream tasks, byupholding over 97.3\% of the retrained model's privacy protection capabilitiesand 99.8\% of its link prediction accuracy. Meanwhile, UtU requires onlyconstant computational demands, underscoring its advantage as a highlylightweight and practical edge unlearning solution.</description><author>Jiajun Tan, Fei Sun, Ruichen Qiu, Du Su, Huawei Shen</author><pubDate>Mon, 11 Mar 2024 18:08:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10695v2</guid></item><item><title>MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning</title><link>http://arxiv.org/abs/2403.06914v1</link><description>Large Language models (LLMs) have demonstrated impressive in-context learning(ICL) capabilities, where a LLM makes predictions for a given test inputtogether with a few input-output pairs (demonstrations). Nevertheless, theinclusion of demonstrations leads to a quadratic increase in the computationaloverhead of the self-attention mechanism. Existing solutions attempt to distilllengthy demonstrations into compact vectors. However, they often requiretask-specific retraining or compromise LLM's in-context learning performance.To mitigate these challenges, we present Meta dEmonstratioN Distillation(MEND), where a language model learns to distill any lengthy demonstrationsinto vectors without retraining for a new downstream task. We exploit theknowledge distillation to enhance alignment between MEND and LLM, achievingboth efficiency and effectiveness simultaneously. MEND is endowed with themeta-knowledge of distilling demonstrations through a two-stage trainingprocess, which includes meta-distillation pretraining and fine-tuning.Comprehensive evaluations across seven diverse ICL task partitions usingdecoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It notonly matches but often outperforms the Vanilla ICL as well as otherstate-of-the-art distillation models, while significantly reducing thecomputational demands. This innovation promises enhanced scalability andefficiency for the practical deployment of large language models</description><author>Yichuan Li, Xiyao Ma, Sixing Lu, Kyumin Lee, Xiaohu Liu, Chenlei Guo</author><pubDate>Mon, 11 Mar 2024 18:03:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06914v1</guid></item><item><title>DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization</title><link>http://arxiv.org/abs/2403.06912v1</link><description>Radiance fields have demonstrated impressive performance in synthesizingnovel views from sparse input views, yet prevailing methods suffer from hightraining costs and slow inference speed. This paper introduces DNGaussian, adepth-regularized framework based on 3D Gaussian radiance fields, offeringreal-time and high-quality few-shot novel view synthesis at low costs. Ourmotivation stems from the highly efficient representation and surprisingquality of the recent 3D Gaussian Splatting, despite it will encounter ageometry degradation when input views decrease. In the Gaussian radiancefields, we find this degradation in scene geometry primarily lined to thepositioning of Gaussian primitives and can be mitigated by depth constraint.Consequently, we propose a Hard and Soft Depth Regularization to restoreaccurate scene geometry under coarse monocular depth supervision whilemaintaining a fine-grained color appearance. To further refine detailedgeometry reshaping, we introduce Global-Local Depth Normalization, enhancingthe focus on small local depth changes. Extensive experiments on LLFF, DTU, andBlender datasets demonstrate that DNGaussian outperforms state-of-the-artmethods, achieving comparable or better results with significantly reducedmemory cost, a $25 \times$ reduction in training time, and over $3000 \times$faster rendering speed.</description><author>Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu</author><pubDate>Mon, 11 Mar 2024 18:02:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06912v1</guid></item><item><title>Responsible Artificial Intelligence: A Structured Literature Review</title><link>http://arxiv.org/abs/2403.06910v1</link><description>Our research endeavors to advance the concept of responsible artificialintelligence (AI), a topic of increasing importance within EU policydiscussions. The EU has recently issued several publications emphasizing thenecessity of trust in AI, underscoring the dual nature of AI as both abeneficial tool and a potential weapon. This dichotomy highlights the urgentneed for international regulation. Concurrently, there is a need for frameworksthat guide companies in AI development, ensuring compliance with suchregulations. Our research aims to assist lawmakers and machine learningpractitioners in navigating the evolving landscape of AI regulation,identifying focal areas for future attention. This paper introduces acomprehensive and, to our knowledge, the first unified definition ofresponsible AI. Through a structured literature review, we elucidate thecurrent understanding of responsible AI. Drawing from this analysis, we proposean approach for developing a future framework centered around this concept. Ourfindings advocate for a human-centric approach to Responsible AI. This approachencompasses the implementation of AI methods with a strong emphasis on ethics,model explainability, and the pillars of privacy, security, and trust.</description><author>Sabrina Goellner, Marina Tropmann-Frick, Bostjan Brumen</author><pubDate>Mon, 11 Mar 2024 18:01:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06910v1</guid></item><item><title>FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization</title><link>http://arxiv.org/abs/2403.06908v1</link><description>3D Gaussian splatting has achieved very impressive performance in real-timenovel view synthesis. However, it often suffers from over-reconstruction duringGaussian densification where high-variance image regions are covered by a fewlarge Gaussians only, leading to blur and artifacts in the rendered images. Wedesign a progressive frequency regularization (FreGS) technique to tackle theover-reconstruction issue within the frequency space. Specifically, FreGSperforms coarse-to-fine Gaussian densification by exploiting low-to-highfrequency components that can be easily extracted with low-pass and high-passfilters in the Fourier space. By minimizing the discrepancy between thefrequency spectrum of the rendered image and the corresponding ground truth, itachieves high-quality Gaussian densification and alleviates theover-reconstruction of Gaussian splatting effectively. Experiments overmultiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples andDeep Blending) show that FreGS achieves superior novel view synthesis andoutperforms the state-of-the-art consistently.</description><author>Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, Eric Xing</author><pubDate>Mon, 11 Mar 2024 18:00:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06908v1</guid></item><item><title>Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints</title><link>http://arxiv.org/abs/2403.06906v1</link><description>Learning to defer (L2D) aims to improve human-AI collaboration systems bylearning how to defer decisions to humans when they are more likely to becorrect than an ML classifier. Existing research in L2D overlooks key aspectsof real-world systems that impede its practical adoption, namely: i) neglectingcost-sensitive scenarios, where type 1 and type 2 errors have different costs;ii) requiring concurrent human predictions for every instance of the trainingdataset and iii) not dealing with human work capacity constraints. To addressthese issues, we propose the deferral under cost and capacity constraintsframework (DeCCaF). DeCCaF is a novel L2D approach, employing supervisedlearning to model the probability of human error under less restrictive datarequirements (only one expert prediction per instance) and using constraintprogramming to globally minimize the error cost subject to workloadlimitations. We test DeCCaF in a series of cost-sensitive fraud detectionscenarios with different teams of 9 synthetic fraud analysts, with individualwork capacity constraints. The results demonstrate that our approach performssignificantly better than the baselines in a wide array of scenarios, achievingan average 8.4% reduction in the misclassification cost.</description><author>Jean V. Alves, Diogo Leitão, Sérgio Jesus, Marco O. P. Sampaio, Javier Liébana, Pedro Saleiro, Mário A. T. Figueiredo, Pedro Bizarro</author><pubDate>Mon, 11 Mar 2024 17:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06906v1</guid></item><item><title>FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in Human-Centric Tasks</title><link>http://arxiv.org/abs/2403.06904v1</link><description>We propose FocusCLIP, integrating subject-level guidance--a specializedmechanism for target-specific supervision--into the CLIP framework for improvedzero-shot transfer on human-centric tasks. Our novel contributions enhance CLIPon both the vision and text sides. On the vision side, we incorporate ROIheatmaps emulating human visual attention mechanisms to emphasizesubject-relevant image regions. On the text side, we introduce human posedescriptions to provide rich contextual information. For human-centric tasks,FocusCLIP is trained with images from the MPII Human Pose dataset. The proposedapproach surpassed CLIP by an average of 8.61% across five previously unseendatasets covering three human-centric tasks. FocusCLIP achieved an averageaccuracy of 33.65% compared to 25.04% by CLIP. We observed a 3.98% improvementin activity recognition, a 14.78% improvement in age classification, and a7.06% improvement in emotion recognition. Moreover, using our proposedsingle-shot LLM prompting strategy, we release a high-quality MPII PoseDescriptions dataset to encourage further research in multimodal learning forhuman-centric tasks. Furthermore, we also demonstrate the effectiveness of oursubject-level supervision on non-human-centric tasks. FocusCLIP shows a 2.47%improvement over CLIP in zero-shot bird classification using the CUB dataset.Our findings emphasize the potential of integrating subject-level guidance withgeneral pretraining methods for enhanced downstream performance.</description><author>Muhammad Saif Ullah Khan, Muhammad Ferjad Naeem, Federico Tombari, Luc Van Gool, Didier Stricker, Muhammad Zeshan Afzal</author><pubDate>Mon, 11 Mar 2024 17:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06904v1</guid></item><item><title>Benign overfitting in leaky ReLU networks with moderate input dimension</title><link>http://arxiv.org/abs/2403.06903v1</link><description>The problem of benign overfitting asks whether it is possible for a model toperfectly fit noisy training data and still generalize well. We study benignoverfitting in two-layer leaky ReLU networks trained with the hinge loss on abinary classification task. We consider input data which can be decomposed intothe sum of a common signal and a random noise component, which lie on subspacesorthogonal to one another. We characterize conditions on the signal to noiseratio (SNR) of the model parameters giving rise to benign versus non-benign, orharmful, overfitting: in particular, if the SNR is high then benign overfittingoccurs, conversely if the SNR is low then harmful overfitting occurs. Weattribute both benign and non-benign overfitting to an approximate marginmaximization property and show that leaky ReLU networks trained on hinge losswith Gradient Descent (GD) satisfy this property. In contrast to prior work wedo not require near orthogonality conditions on the training data: notably, forinput dimension $d$ and training sample size $n$, while prior work showsasymptotically optimal error when $d = \Omega(n^2 \log n)$, here we requireonly $d = \Omega\left(n \log \frac{1}{\epsilon}\right)$ to obtain error within$\epsilon$ of optimal.</description><author>Kedar Karhadkar, Erin George, Michael Murray, Guido Montúfar, Deanna Needell</author><pubDate>Mon, 11 Mar 2024 17:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06903v1</guid></item><item><title>Deep adaptative spectral zoom for improved remote heart rate estimation</title><link>http://arxiv.org/abs/2403.06902v1</link><description>Recent advances in remote heart rate measurement, motivated by data-drivenapproaches, have notably enhanced accuracy. However, these improvementsprimarily focus on recovering the rPPG signal, overlooking the implicitchallenges of estimating the heart rate (HR) from the derived signal. Whilemany methods employ the Fast Fourier Transform (FFT) for HR estimation, theperformance of the FFT is inherently affected by a limited frequencyresolution. In contrast, the Chirp-Z Transform (CZT), a generalization form ofFFT, can refine the spectrum to the narrow-band range of interest for heartrate, providing improved frequential resolution and, consequently, moreaccurate estimation. This paper presents the advantages of employing the CZTfor remote HR estimation and introduces a novel data-driven adaptive CZTestimator. The objective of our proposed model is to tailor the CZT to matchthe characteristics of each specific dataset sensor, facilitating a moreoptimal and accurate estimation of HR from the rPPG signal without compromisinggeneralization across diverse datasets. This is achieved through a SparseMatrix Optimization (SMO). We validate the effectiveness of our model throughexhaustive evaluations on three publicly available datasets UCLA-rPPG, PURE,and UBFC-rPPG employing both intra- and cross-database performance metrics. Theresults reveal outstanding heart rate estimation capabilities, establishing theproposed approach as a robust and versatile estimator for any rPPG method.</description><author>Joaquim Comas, Adria Ruiz, Federico Sukno</author><pubDate>Mon, 11 Mar 2024 17:55:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06902v1</guid></item><item><title>LIBR+: Improving Intraoperative Liver Registration by Learning the Residual of Biomechanics-Based Deformable Registration</title><link>http://arxiv.org/abs/2403.06901v1</link><description>The surgical environment imposes unique challenges to the intraoperativeregistration of organ shapes to their preoperatively-imaged geometry.Biomechanical model-based registration remains popular, while deep learningsolutions remain limited due to the sparsity and variability of intraoperativemeasurements and the limited ground-truth deformation of an organ that can beobtained during the surgery. In this paper, we propose a novel \textit{hybrid}registration approach that leverage a linearized iterative boundaryreconstruction (LIBR) method based on linear elastic biomechanics, and use deepneural networks to learn its residual to the ground-truth deformation (LIBR+).We further formulate a dual-branch spline-residual graph convolutional neuralnetwork (SR-GCN) to assimilate information from sparse and variableintraoperative measurements and effectively propagate it through the geometryof the 3D organ. Experiments on a large intraoperative liver registrationdataset demonstrated the consistent improvements achieved by LIBR+ incomparison to existing rigid, biomechnical model-based non-rigid, anddeep-learning based non-rigid approaches to intraoperative liver registration.</description><author>Dingrong Wang, Soheil Azadvar, Jon Heiselman, Xiajun Jiang, Michael Miga, Linwei Wang</author><pubDate>Mon, 11 Mar 2024 17:54:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06901v1</guid></item><item><title>GRITv2: Efficient and Light-weight Social Relation Recognition</title><link>http://arxiv.org/abs/2403.06895v1</link><description>Our research focuses on the analysis and improvement of the Graph-basedRelation Inference Transformer (GRIT), which serves as an important benchmarkin the field. We conduct a comprehensive ablation study using the PISC-finedataset, to find and explore improvement in efficiency and performance ofGRITv2. Our research has provided a new state-of-the-art relation recognitionmodel on the PISC relation dataset. We introduce several features in the GRITmodel and analyse our new benchmarks in two versions: GRITv2-L (large) andGRITv2-S (small). Our proposed GRITv2-L surpasses existing methods on relationrecognition and the GRITv2-S is within 2% performance gap of GRITv2-L, whichhas only 0.0625x the model size and parameters of GRITv2-L. Furthermore, wealso address the need for model compression, an area crucial for deployingefficient models on resource-constrained platforms. By applying quantizationtechniques, we efficiently reduced the GRITv2-S size to 22MB and deployed it onthe flagship OnePlus 12 mobile which still surpasses the PISC-fine benchmarksin performance, highlighting the practical viability and improved efficiency ofour model on mobile devices.</description><author>N K Sagar Reddy, Neeraj Kasera, Avinash Thakur</author><pubDate>Mon, 11 Mar 2024 17:49:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06895v1</guid></item><item><title>Visual CPG-RL: Learning Central Pattern Generators for Visually-Guided Quadruped Locomotion</title><link>http://arxiv.org/abs/2212.14400v2</link><description>We present a framework for learning visually-guided quadruped locomotion byintegrating exteroceptive sensing and central pattern generators (CPGs), i.e.systems of coupled oscillators, into the deep reinforcement learning (DRL)framework. Through both exteroceptive and proprioceptive sensing, the agentlearns to coordinate rhythmic behavior among different oscillators to trackvelocity commands, while at the same time override these commands to avoidcollisions with the environment. We investigate several open robotics andneuroscience questions: 1) What is the role of explicit interoscillatorcouplings between oscillators, and can such coupling improve sim-to-realtransfer for navigation robustness? 2) What are the effects of using amemory-enabled vs. a memory-free policy network with respect to robustness,energy-efficiency, and tracking performance in sim-to-real navigation tasks? 3)How do animals manage to tolerate high sensorimotor delays, yet still producesmooth and robust gaits? To answer these questions, we train our perceptivelocomotion policies in simulation and perform sim-to-real transfers to theUnitree Go1 quadruped, where we observe robust navigation in a variety ofscenarios. Our results show that the CPG, explicit interoscillator couplings,and memory-enabled policy representations are all beneficial for energyefficiency, robustness to noise and sensory delays of 90 ms, and trackingperformance for successful sim-to-real transfer for navigation tasks. Videoresults can be found at https://youtu.be/wpsbSMzIwgM.</description><author>Guillaume Bellegarda, Milad Shafiee, Auke Ijspeert</author><pubDate>Mon, 11 Mar 2024 17:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.14400v2</guid></item><item><title>Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head</title><link>http://arxiv.org/abs/2403.06892v1</link><description>End-to-end transformer-based detectors (DETRs) have shown exceptionalperformance in both closed-set and open-vocabulary object detection (OVD) tasksthrough the integration of language modalities. However, their demandingcomputational requirements have hindered their practical application inreal-time object detection (OD) scenarios. In this paper, we scrutinize thelimitations of two leading models in the OVDEval benchmark, OmDet andGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-basedreal-time OVD model features an innovative Efficient Fusion Head (EFH) moduledesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) withTensorRT and language cache techniques applied. Notably, in zero-shot scenarioson COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly onpar with current state-of-the-art supervised models. Furthermore, itestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting anAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality ofOmDet-Turbo in industrial applications is underscored by its exceptionalperformance on benchmark datasets and superior inference speed, positioning itas a compelling choice for real-time object detection tasks. Code:\url{https://github.com/om-ai-lab/OmDet}</description><author>Tiancheng Zhao, Peng Liu, Xuan He, Lu Zhang, Kyusong Lee</author><pubDate>Mon, 11 Mar 2024 17:48:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06892v1</guid></item><item><title>DeepSeek-VL: Towards Real-World Vision-Language Understanding</title><link>http://arxiv.org/abs/2403.05525v2</link><description>We present DeepSeek-VL, an open-source Vision-Language (VL) Model designedfor real-world vision and language understanding applications. Our approach isstructured around three key dimensions: We strive to ensure our data is diverse, scalable, and extensively coversreal-world scenarios including web screenshots, PDFs, OCR, charts, andknowledge-based content, aiming for a comprehensive representation of practicalcontexts. Further, we create a use case taxonomy from real user scenarios andconstruct an instruction tuning dataset accordingly. The fine-tuning with thisdataset substantially improves the model's user experience in practicalapplications. Considering efficiency and the demands of most real-worldscenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficientlyprocesses high-resolution images (1024 x 1024), while maintaining a relativelylow computational overhead. This design choice ensures the model's ability tocapture critical semantic and detailed information across various visual tasks.We posit that a proficient Vision-Language Model should, foremost, possessstrong language abilities. To ensure the preservation of LLM capabilitiesduring pretraining, we investigate an effective VL pretraining strategy byintegrating LLM training from the beginning and carefully managing thecompetitive dynamics observed between vision and language modalities. The DeepSeek-VL family (both 1.3B and 7B models) showcases superior userexperiences as a vision-language chatbot in real-world applications, achievingstate-of-the-art or competitive performance across a wide range ofvisual-language benchmarks at the same model size while maintaining robustperformance on language-centric benchmarks. We have made both 1.3B and 7Bmodels publicly accessible to foster innovations based on this foundationmodel.</description><author>Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, Chong Ruan</author><pubDate>Mon, 11 Mar 2024 17:47:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05525v2</guid></item><item><title>Application of Quantum Tensor Networks for Protein Classification</title><link>http://arxiv.org/abs/2403.06890v1</link><description>We show that protein sequences can be thought of as sentences in naturallanguage processing and can be parsed using the existing Quantum NaturalLanguage framework into parameterized quantum circuits of reasonable qubits,which can be trained to solve various protein-related machine-learningproblems. We classify proteins based on their subcellular locations, a pivotaltask in bioinformatics that is key to understanding biological processes anddisease mechanisms. Leveraging the quantum-enhanced processing capabilities, wedemonstrate that Quantum Tensor Networks (QTN) can effectively handle thecomplexity and diversity of protein sequences. We present a detailedmethodology that adapts QTN architectures to the nuanced requirements ofprotein data, supported by comprehensive experimental results. We demonstratetwo distinct QTNs, inspired by classical recurrent neural networks (RNN) andconvolutional neural networks (CNN), to solve the binary classification taskmentioned above. Our top-performing quantum model has achieved a 94% accuracyrate, which is comparable to the performance of a classical model that uses theESM2 protein language model embeddings. It's noteworthy that the ESM2 model isextremely large, containing 8 million parameters in its smallest configuration,whereas our best quantum model requires only around 800 parameters. Wedemonstrate that these hybrid models exhibit promising performance, showcasingtheir potential to compete with classical models of similar complexity.</description><author>Debarshi Kundu, Archisman Ghosh, Srinivasan Ekambaram, Jian Wang, Nikolay Dokholyan, Swaroop Ghosh</author><pubDate>Mon, 11 Mar 2024 17:47:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06890v1</guid></item><item><title>HiRA-Pro: High resolution alignment of multimodal spatio-temporal data: a process physics driven approach</title><link>http://arxiv.org/abs/2403.06888v1</link><description>We present HiRA-Pro, a novel procedure to align, at high spatio-temporalresolutions, multimodal signals from real-world processes and systems thatexhibit diverse transient, nonlinear stochastic dynamics, such as manufacturingmachines. It is based on discerning and synchronizing the process signatures ofsalient kinematic and dynamic events in these disparate signals. HiRA-Proaddresses the challenge of aligning data with sub-millisecond phenomena, wheretraditional timestamp, external trigger, or clock-based alignment methods fallshort. The effectiveness of HiRA-Pro is demonstrated in a smart manufacturingcontext, where it aligns data from 13+ channels acquired during 3D-printing andmilling operations on an Optomec-LENS MTS 500 hybrid machine. The aligned datais then voxelized to generate 0.25 second aligned data chunks that correspondto physical voxels on the produced part. The superiority of HiRA-Pro is furthershowcased through case studies in additive manufacturing, demonstratingimproved machine learning-based predictive performance due to precisemultimodal data alignment. Specifically, testing classification accuraciesimproved by almost 35% with the application of HiRA-Pro, even with limiteddata, allowing for precise localization of artifacts. The paper also provides acomprehensive discussion on the proposed method, its applications, andcomparative qualitative analysis with a few other alignment methods. HiRA-Proachieves temporal-spatial resolutions of 10-1000 us and 100 um in order togenerate datasets that register with physical voxels on the 3D-printed andmilled part. These resolutions are at least an order of magnitude finer thanthe existing alignment methods that employ individual timestamps, statisticalcorrelations, or common clocks, which achieve precision of hundreds ofmilliseconds.</description><author>Abhishek Hanchate, Himanshu Balhara, Vishal S. Chindepalli, Satish T. S. Bukkapatnam</author><pubDate>Mon, 11 Mar 2024 17:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06888v1</guid></item><item><title>A Holistic Framework Towards Vision-based Traffic Signal Control with Microscopic Simulation</title><link>http://arxiv.org/abs/2403.06884v1</link><description>Traffic signal control (TSC) is crucial for reducing traffic congestion thatleads to smoother traffic flow, reduced idling time, and mitigated CO2emissions. In this study, we explore the computer vision approach for TSC thatmodulates on-road traffic flows through visual observation. Unlike traditionalfeature-based approaches, vision-based methods depend much less on heuristicsand predefined features, bringing promising potentials for end-to-end learningand optimization of traffic signals. Thus, we introduce a holistic trafficsimulation framework called TrafficDojo towards vision-based TSC and itsbenchmarking by integrating the microscopic traffic flow provided in SUMO intothe driving simulator MetaDrive. This proposed framework offers a versatiletraffic environment for in-depth analysis and comprehensive evaluation oftraffic signal controllers across diverse traffic conditions and scenarios. Weestablish and compare baseline algorithms including both traditional andReinforecment Learning (RL) approaches. This work sheds insights into thedesign and development of vision-based TSC approaches and open up new researchopportunities. All the code and baselines will be made publicly available.</description><author>Pan He, Quanyi Li, Xiaoyong Yuan, Bolei Zhou</author><pubDate>Mon, 11 Mar 2024 17:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06884v1</guid></item><item><title>BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning of SAM</title><link>http://arxiv.org/abs/2402.16338v4</link><description>The Segment Anything Model (SAM), a foundation model pretrained on millionsof images and segmentation masks, has significantly advanced semanticsegmentation, a fundamental task in computer vision. Despite its strengths, SAMencounters two major challenges. Firstly, it struggles with segmenting specificobjects autonomously, as it relies on users to manually input prompts likepoints or bounding boxes to identify targeted objects. Secondly, SAM faceschallenges in excelling at specific downstream tasks, like medical imaging, dueto a disparity between the distribution of its pretraining data, whichpredominantly consists of general-domain images, and the data used indownstream tasks. Current solutions to these problems, which involve finetuningSAM, often lead to overfitting, a notable issue in scenarios with very limiteddata, like in medical imaging. To overcome these limitations, we introduceBLO-SAM, which finetunes SAM based on bi-level optimization (BLO). Our approachallows for automatic image segmentation without the need for manual prompts, byoptimizing a learnable prompt embedding. Furthermore, it significantly reducesthe risk of overfitting by training the model's weight parameters and theprompt embedding on two separate subsets of the training dataset, each at adifferent level of optimization. We apply BLO-SAM to diverse semanticsegmentation tasks in general and medical domains. The results demonstrateBLO-SAM's superior performance over various state-of-the-art image semanticsegmentation methods.</description><author>Li Zhang, Youwei Liang, Ruiyi Zhang, Amirhosein Javadi, Pengtao Xie</author><pubDate>Mon, 11 Mar 2024 17:40:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16338v4</guid></item><item><title>Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning</title><link>http://arxiv.org/abs/2403.06880v1</link><description>Toddlers evolve from free exploration with sparse feedback to exploitingprior experiences for goal-directed learning with denser rewards. Drawinginspiration from this Toddler-Inspired Reward Transition, we set out to explorethe implications of varying reward transitions when incorporated intoReinforcement Learning (RL) tasks. Central to our inquiry is the transitionfrom sparse to potential-based dense rewards, which share optimal strategiesregardless of reward changes. Through various experiments, including those inegocentric navigation and robotic arm manipulation tasks, we found that properreward transitions significantly influence sample efficiency and success rates.Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense(S2D) transition. Beyond these performance metrics, using Cross-DensityVisualizer technique, we observed that transitions, especially the S2D, smooththe policy loss landscape, promoting wide minima that enhance generalization inRL models.</description><author>Junseok Park, Yoonsung Kim, Hee Bin Yoo, Min Whoo Lee, Kibeom Kim, Won-Seok Choi, Minsu Lee, Byoung-Tak Zhang</author><pubDate>Mon, 11 Mar 2024 17:34:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06880v1</guid></item><item><title>SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields for Robotic Inspection</title><link>http://arxiv.org/abs/2403.06877v1</link><description>We present a neural-field-based large-scale reconstruction system that fuseslidar and vision data to generate high-quality reconstructions that aregeometrically accurate and capture photo-realistic textures. This system adaptsthe state-of-the-art neural radiance field (NeRF) representation to alsoincorporate lidar data which adds strong geometric constraints on the depth andsurface normals. We exploit the trajectory from a real-time lidar SLAM systemto bootstrap a Structure-from-Motion (SfM) procedure to both significantlyreduce the computation time and to provide metric scale which is crucial forlidar depth loss. We use submapping to scale the system to large-scaleenvironments captured over long trajectories. We demonstrate the reconstructionsystem with data from a multi-camera, lidar sensor suite onboard a leggedrobot, hand-held while scanning building scenes for 600 metres, and onboard anaerial robot surveying a multi-storey mock disaster site-building. Website:https://ori-drs.github.io/projects/silvr/</description><author>Yifu Tao, Yash Bhalgat, Lanke Frank Tarimo Fu, Matias Mattamala, Nived Chebrolu, Maurice Fallon</author><pubDate>Mon, 11 Mar 2024 17:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06877v1</guid></item><item><title>STARC: A General Framework For Quantifying Differences Between Reward Functions</title><link>http://arxiv.org/abs/2309.15257v2</link><description>In order to solve a task using reinforcement learning, it is necessary tofirst formalise the goal of that task as a reward function. However, for manyreal-world tasks, it is very difficult to manually specify a reward functionthat never incentivises undesirable behaviour. As a result, it is increasinglypopular to use \emph{reward learning algorithms}, which attempt to \emph{learn}a reward function from data. However, the theoretical foundations of rewardlearning are not yet well-developed. In particular, it is typically not knownwhen a given reward learning algorithm with high probability will learn areward function that is safe to optimise. This means that reward learningalgorithms generally must be evaluated empirically, which is expensive, andthat their failure modes are difficult to anticipate in advance. One of theroadblocks to deriving better theoretical guarantees is the lack of goodmethods for quantifying the difference between reward functions. In this paperwe provide a solution to this problem, in the form of a class of pseudometricson the space of all reward functions that we call STARC (STAndardised RewardComparison) metrics. We show that STARC metrics induce both an upper and alower bound on worst-case regret, which implies that our metrics are tight, andthat any metric with the same properties must be bilipschitz equivalent toours. Moreover, we also identify a number of issues with reward metricsproposed by earlier works. Finally, we evaluate our metrics empirically, todemonstrate their practical efficacy. STARC metrics can be used to make boththeoretical and empirical analysis of reward learning algorithms both easierand more principled.</description><author>Joar Skalse, Lucy Farnik, Sumeet Ramesh Motwani, Erik Jenner, Adam Gleave, Alessandro Abate</author><pubDate>Mon, 11 Mar 2024 17:29:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15257v2</guid></item><item><title>FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices using a Computing Power Aware Scheduler</title><link>http://arxiv.org/abs/2309.14675v2</link><description>Cross-silo federated learning offers a promising solution to collaborativelytrain robust and generalized AI models without compromising the privacy oflocal datasets, e.g., healthcare, financial, as well as scientific projectsthat lack a centralized data facility. Nonetheless, because of the disparity ofcomputing resources among different clients (i.e., device heterogeneity),synchronous federated learning algorithms suffer from degraded efficiency whenwaiting for straggler clients. Similarly, asynchronous federated learningalgorithms experience degradation in the convergence rate and final modelaccuracy on non-identically and independently distributed (non-IID)heterogeneous datasets due to stale local models and client drift. To addressthese limitations in cross-silo federated learning with heterogeneous clientsand data, we propose FedCompass, an innovative semi-asynchronous federatedlearning algorithm with a computing power-aware scheduler on the server side,which adaptively assigns varying amounts of training tasks to different clientsusing the knowledge of the computing power of individual clients. FedCompassensures that multiple locally trained models from clients are received almostsimultaneously as a group for aggregation, effectively reducing the stalenessof local models. At the same time, the overall training process remainsasynchronous, eliminating prolonged waiting periods from straggler clients.Using diverse non-IID heterogeneous distributed datasets, we demonstrate thatFedCompass achieves faster convergence and higher accuracy than otherasynchronous algorithms while remaining more efficient than synchronousalgorithms when performing federated learning on heterogeneous clients. Thesource code for FedCompass is available at https://github.com/APPFL/FedCompass.</description><author>Zilinghan Li, Pranshu Chaturvedi, Shilan He, Han Chen, Gagandeep Singh, Volodymyr Kindratenko, E. A. Huerta, Kibaek Kim, Ravi Madduri</author><pubDate>Mon, 11 Mar 2024 17:28:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14675v2</guid></item><item><title>COOD: Combined out-of-distribution detection using multiple measures for anomaly &amp; novel class detection in large-scale hierarchical classification</title><link>http://arxiv.org/abs/2403.06874v1</link><description>High-performing out-of-distribution (OOD) detection, both anomaly and novelclass, is an important prerequisite for the practical use of classificationmodels. In this paper, we focus on the species recognition task in imagesconcerned with large databases, a large number of fine-grained hierarchicalclasses, severe class imbalance, and varying image quality. We propose aframework for combining individual OOD measures into one combined OOD (COOD)measure using a supervised model. The individual measures are several existingstate-of-the-art measures and several novel OOD measures developed with novelclass detection and hierarchical class structure in mind. COOD was extensivelyevaluated on three large-scale (500k+ images) biodiversity datasets in thecontext of anomaly and novel class detection. We show that COOD outperformsindividual, including state-of-the-art, OOD measures by a large margin in termsof TPR@1% FPR in the majority of experiments, e.g., improving detectingImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset.SHAP (feature contribution) analysis shows that different individual OODmeasures are essential for various tasks, indicating that multiple OOD measuresand combinations are needed to generalize. Additionally, we show thatexplicitly considering ID images that are incorrectly classified for theoriginal (species) recognition task is important for constructinghigh-performing OOD detection methods and for practical applicability. Theframework can easily be extended or adapted to other tasks and mediamodalities.</description><author>L. E. Hogeweg, R. Gangireddy, D. Brunink, V. J. Kalkman, L. Cornelissen, J. W. Kamminga</author><pubDate>Mon, 11 Mar 2024 17:26:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06874v1</guid></item><item><title>Last Iterate Convergence of Incremental Methods and Applications in Continual Learning</title><link>http://arxiv.org/abs/2403.06873v1</link><description>Incremental gradient methods and incremental proximal methods are afundamental class of optimization algorithms used for solving finite sumproblems, broadly studied in the literature. Yet, when it comes to theirconvergence guarantees, nonasymptotic (first-order or proximal) oraclecomplexity bounds have been obtained fairly recently, almost exclusivelyapplying to the average iterate. Motivated by applications in continuallearning, we obtain the first convergence guarantees for the last iterate ofboth incremental gradient and incremental proximal methods, in general convexsmooth (for both) and convex Lipschitz (for the proximal variants) settings.Our oracle complexity bounds for the last iterate nearly match (i.e., match upto a square-root-log or a log factor) the best known oracle complexity boundsfor the average iterate, for both classes of methods. We further obtaingeneralizations of our results to weighted averaging of the iterates withincreasing weights, which can be seen as interpolating between the last iterateand the average iterate guarantees. Additionally, we discuss how our resultscan be generalized to variants of studied incremental methods with permutedordering of updates. Our results generalize last iterate guarantees forincremental methods compared to state of the art, as such results werepreviously known only for overparameterized linear models, which correspond toconvex quadratic problems with infinitely many solutions.</description><author>Xufeng Cai, Jelena Diakonikolas</author><pubDate>Mon, 11 Mar 2024 17:24:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06873v1</guid></item><item><title>Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents</title><link>http://arxiv.org/abs/2403.06872v1</link><description>Legal judgment prediction suffers from the problem of long case documentsexceeding tens of thousands of words, in general, and having a non-uniformstructure. Predicting judgments from such documents becomes a challenging task,more so on documents with no structural annotation. We explore theclassification of these large legal documents and their lack of structuralinformation with a deep-learning-based hierarchical framework which we callMESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgmentprediction. Specifically, we divide a document into parts to extract theirembeddings from the last four layers of a custom fine-tuned Large LanguageModel, and try to approximate their structure through unsupervised clustering.Which we use in another set of transformer encoder layers to learn theinter-chunk representations. We analyze the adaptability of Large LanguageModels (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with thehierarchical framework of MESc and compare them with their standaloneperformance on legal texts. We also study their intra-domain(legal) transferlearning capability and the impact of combining embeddings from their lastlayers in MESc. We test these methods and their effectiveness with extensiveexperiments and ablation studies on legal documents from India, the EuropeanUnion, and the United States with the ILDC dataset and a subset of the LexGLUEdataset. Our approach achieves a minimum total performance gain ofapproximately 2 points over previous state-of-the-art methods.</description><author>Nishchal Prasad, Mohand Boughanem, Taoufiq Dkaki</author><pubDate>Mon, 11 Mar 2024 17:24:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06872v1</guid></item><item><title>On the Generalization Ability of Unsupervised Pretraining</title><link>http://arxiv.org/abs/2403.06871v1</link><description>Recent advances in unsupervised learning have shown that unsupervisedpre-training, followed by fine-tuning, can improve model generalization.However, a rigorous understanding of how the representation function learned onan unlabeled dataset affects the generalization of the fine-tuned model islacking. Existing theoretical research does not adequately account for theheterogeneity of the distribution and tasks in pre-training and fine-tuningstage. To bridge this gap, this paper introduces a novel theoretical frameworkthat illuminates the critical factor influencing the transferability ofknowledge acquired during unsupervised pre-training to the subsequentfine-tuning phase, ultimately affecting the generalization capabilities of thefine-tuned model on downstream tasks. We apply our theoretical framework toanalyze generalization bound of two distinct scenarios: Context Encoderpre-training with deep neural networks and Masked Autoencoder pre-training withdeep transformers, followed by fine-tuning on a binary classification task.Finally, inspired by our findings, we propose a novel regularization methodduring pre-training to further enhances the generalization of fine-tuned model.Overall, our results contribute to a better understanding of unsupervisedpre-training and fine-tuning paradigm, and can shed light on the design of moreeffective pre-training algorithms.</description><author>Yuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi</author><pubDate>Mon, 11 Mar 2024 17:23:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06871v1</guid></item><item><title>Semantic Residual Prompts for Continual Learning</title><link>http://arxiv.org/abs/2403.06870v1</link><description>Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trainedmodel and focus training on a few parameter vectors termed prompts. Most ofthese methods organize these vectors in a pool of key-value pairs, and use theinput image as query to retrieve the prompts (values). However, as keys arelearned while tasks progress, the prompting selection strategy is itselfsubject to catastrophic forgetting, an issue often overlooked by existingapproaches. For instance, prompts introduced to accommodate new tasks might endup interfering with previously learned prompts. To make the selection strategymore stable, we ask a foundational model (CLIP) to select our prompt within atwo-level adaptation mechanism. Specifically, the first level leveragesstandard textual prompts for the CLIP textual encoder, leading to stable classprototypes. The second level, instead, uses these prototypes along with thequery image as keys to index a second pool. The retrieved prompts serve toadapt a pre-trained ViT, granting plasticity. In doing so, we also propose anovel residual mechanism to transfer CLIP semantics to the ViT layers. Throughextensive analysis on established CL benchmarks, we show that our methodsignificantly outperforms both state-of-the-art CL approaches and the zero-shotCLIP test. Notably, our findings hold true even for datasets with a substantialdomain gap w.r.t. the pre-training knowledge of the backbone model, asshowcased by experiments on satellite imagery and medical datasets.</description><author>Martin Menabue, Emanuele Frascaroli, Matteo Boschini, Enver Sangineto, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara</author><pubDate>Mon, 11 Mar 2024 17:23:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06870v1</guid></item><item><title>Learning with Noisy Foundation Models</title><link>http://arxiv.org/abs/2403.06869v1</link><description>Foundation models are usually pre-trained on large-scale datasets and thenadapted to downstream tasks through tuning. However, the large-scalepre-training datasets, often inaccessible or too expensive to handle, cancontain label noise that may adversely affect the generalization of the modeland pose unexpected risks. This paper stands out as the first work tocomprehensively understand and analyze the nature of noise in pre-trainingdatasets and then effectively mitigate its impacts on downstream tasks.Specifically, through extensive experiments of fully-supervised and image-textcontrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12Mdatasets, we demonstrate that, while slight noise in pre-training can benefitin-domain (ID) performance, where the training and testing data share a similardistribution, it always deteriorates out-of-domain (OOD) performance, wheretraining and testing distributions are significantly different. Theseobservations are agnostic to scales of pre-training datasets, pre-trainingnoise types, model architectures, pre-training objectives, downstream tuningmethods, and downstream applications. We empirically ascertain that the reasonbehind this is that the pre-training noise shapes the feature spacedifferently. We then propose a tuning method (NMTune) to affine the featurespace to mitigate the malignant effect of noise and improve generalization,which is applicable in both parameter-efficient and black-box tuning manners.We additionally conduct extensive experiments on popular vision and languagemodels, including APIs, which are supervised and self-supervised pre-trained onrealistic noisy data for evaluation. Our analysis and results demonstrate theimportance of this novel and fundamental research direction, which we term asNoisy Model Learning.</description><author>Hao Chen, Jindong Wang, Zihan Wang, Ran Tao, Hongxin Wei, Xing Xie, Masashi Sugiyama, Bhiksha Raj</author><pubDate>Mon, 11 Mar 2024 17:22:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06869v1</guid></item><item><title>QUASAR: QUality and Aesthetics Scoring with Advanced Representations</title><link>http://arxiv.org/abs/2403.06866v1</link><description>This paper introduces a new data-driven, non-parametric method for imagequality and aesthetics assessment, surpassing existing approaches and requiringno prompt engineering or fine-tuning. We eliminate the need for expressivetextual embeddings by proposing efficient image anchors in the data. Throughextensive evaluations of 7 state-of-the-art self-supervised models, our methoddemonstrates superior performance and robustness across various datasets andbenchmarks. Notably, it achieves high agreement with human assessments evenwith limited data and shows high robustness to the nature of data and theirpre-processing pipeline. Our contributions offer a streamlined solution forassessment of images while providing insights into the perception of visualinformation.</description><author>Sergey Kastryulin, Denis Prokopenko, Artem Babenko, Dmitry V. Dylov</author><pubDate>Mon, 11 Mar 2024 17:21:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06866v1</guid></item><item><title>Rethinking Mobile AI Ecosystem in the LLM Era</title><link>http://arxiv.org/abs/2308.14363v2</link><description>In today's landscape, smartphones have evolved into hubs for hosting amultitude of deep learning models aimed at local execution. A key realizationdriving this work is the notable fragmentation among these models,characterized by varied architectures, operators, and implementations. Thisfragmentation imposes a significant burden on the comprehensive optimization ofhardware, system settings, and algorithms. Buoyed by the recent strides in large foundation models, this work introducesa pioneering paradigm for mobile AI: a collaborative management approachbetween the mobile OS and hardware, overseeing a foundational model capable ofserving a broad spectrum of mobile AI tasks, if not all. This foundationalmodel resides within the NPU and remains impervious to app or OS revisions,akin to firmware. Concurrently, each app contributes a concise, offlinefine-tuned "adapter" tailored to distinct downstream tasks. From this conceptemerges a concrete instantiation known as \sys. It amalgamates a curatedselection of publicly available Large Language Models (LLMs) and facilitatesdynamic data flow. This concept's viability is substantiated through thecreation of an exhaustive benchmark encompassing 38 mobile AI tasks spanning 50datasets, including domains such as Computer Vision (CV), Natural LanguageProcessing (NLP), audio, sensing, and multimodal inputs. Spanning thisbenchmark, \sys unveils its impressive performance. It attains accuracy parityin 85\% of tasks, demonstrates improved scalability in terms of storage andmemory, and offers satisfactory inference speed on Commercial Off-The-Shelf(COTS) mobile devices fortified with NPU support. This stands in stark contrastto task-specific models tailored for individual applications.</description><author>Jinliang Yuan, Chen Yang, Dongqi Cai, Shihe Wang, Xin Yuan, Zeling Zhang, Xiang Li, Dingge Zhang, Hanzi Mei, Xianqing Jia, Shangguang Wang, Mengwei Xu</author><pubDate>Mon, 11 Mar 2024 17:18:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14363v2</guid></item><item><title>The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism, AI, and Health in Africa</title><link>http://arxiv.org/abs/2403.03357v2</link><description>With growing application of machine learning (ML) technologies in healthcare,there have been calls for developing techniques to understand and mitigatebiases these systems may exhibit. Fair-ness considerations in the developmentof ML-based solutions for health have particular implications for Africa, whichalready faces inequitable power imbalances between the Global North andSouth.This paper seeks to explore fairness for global health, with Africa as acase study. We conduct a scoping review to propose axes of disparities forfairness consideration in the African context and delineate where they may comeinto play in different ML-enabled medical modalities. We then conductqualitative research studies with 672 general population study participants and28 experts inML, health, and policy focused on Africa to obtain corroborativeevidence on the proposed axes of disparities. Our analysis focuses oncolonialism as the attribute of interest and examines the interplay betweenartificial intelligence (AI), health, and colonialism. Among the pre-identifiedattributes, we found that colonial history, country of origin, and nationalincome level were specific axes of disparities that participants believed wouldcause an AI system to be biased.However, there was also divergence of opinionbetween experts and general population participants. Whereas experts generallyexpressed a shared view about the relevance of colonial history for thedevelopment and implementation of AI technologies in Africa, the majority ofthe general population participants surveyed did not think there was a directlink between AI and colonialism. Based on these findings, we provide practicalrecommendations for developing fairness-aware ML solutions for health inAfrica.</description><author>Mercy Asiedu, Awa Dieng, Iskandar Haykel, Negar Rostamzadeh, Stephen Pfohl, Chirag Nagpal, Maria Nagawa, Abigail Oppong, Sanmi Koyejo, Katherine Heller</author><pubDate>Mon, 11 Mar 2024 17:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03357v2</guid></item><item><title>Real-Time Simulated Avatar from Head-Mounted Sensors</title><link>http://arxiv.org/abs/2403.06862v1</link><description>We present SimXR, a method for controlling a simulated avatar frominformation (headset pose and cameras) obtained from AR / VR headsets. Due tothe challenging viewpoint of head-mounted cameras, the human body is oftenclipped out of view, making traditional image-based egocentric pose estimationchallenging. On the other hand, headset poses provide valuable informationabout overall body motion, but lack fine-grained details about the hands andfeet. To synergize headset poses with cameras, we control a humanoid to trackheadset movement while analyzing input images to decide body movement. Whenbody parts are seen, the movements of hands and feet will be guided by theimages; when unseen, the laws of physics guide the controller to generateplausible motion. We design an end-to-end method that does not rely on anyintermediate representations and learns to directly map from images and headsetposes to humanoid control signals. To train our method, we also propose alarge-scale synthetic dataset created using camera configurations compatiblewith a commercially available VR headset (Quest 2) and show promising resultson real-world captures. To demonstrate the applicability of our framework, wealso test it on an AR headset with a forward-facing camera.</description><author>Zhengyi Luo, Jinkun Cao, Rawal Khirodkar, Alexander Winkler, Kris Kitani, Weipeng Xu</author><pubDate>Mon, 11 Mar 2024 17:15:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06862v1</guid></item><item><title>A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa</title><link>http://arxiv.org/abs/2403.06860v1</link><description>Desert locust swarms present a major threat to agriculture and food security.Addressing this challenge, our study develops an operationally-ready model forpredicting locust breeding grounds, which has the potential to enhance earlywarning systems and targeted control measures. We curated a dataset from theUnited Nations Food and Agriculture Organization's (UN-FAO) locust observationrecords and analyzed it using two types of spatio-temporal input features:remotely-sensed environmental and climate data as well as multi-spectral earthobservation images. Our approach employed custom deep learning models(three-dimensional and LSTM-based recurrent convolutional networks), along withthe geospatial foundational model Prithvi recently released by Jakubik et al.,2023. These models notably outperformed existing baselines, with thePrithvi-based model, fine-tuned on multi-spectral images from NASA's HarmonizedLandsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 andROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant findingfrom our research is that multi-spectral earth observation images alone aresufficient for effective locust breeding ground prediction without the need toexplicitly incorporate climatic or environmental features.</description><author>Ibrahim Salihu Yusuf, Mukhtar Opeyemi Yusuf, Kobby Panford-Quainoo, Arnu Pretorius</author><pubDate>Mon, 11 Mar 2024 17:13:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06860v1</guid></item><item><title>Development of a Reliable and Accessible Caregiving Language Model (CaLM)</title><link>http://arxiv.org/abs/2403.06857v1</link><description>Unlike professional caregivers, family caregivers often assume this rolewithout formal preparation or training. Because of this, there is an urgentneed to enhance the capacity of family caregivers to provide quality care.Large language models can potentially be used as a foundation technology forsupporting caregivers as educational tools or as adjunct to care. This studyaimed to develop a reliable Caregiving Language Model (CaLM) by using FMs and acaregiving knowledge base, develop an accessible CaLM using a small FM thatrequires fewer computing resources, and evaluate the performance of the modelcompared to a large FM. We developed CaLM using the Retrieval AugmentedGeneration (RAG) framework combined with FM fine-tuning for improving thequality of FM answers by grounding the model on a caregiving knowledge base. Weused two small FMs as candidates for the FM of CaLM (LLaMA-2 and Falcon with 7Bparameters) and larger FM GPT-3.5 as a benchmark. We developed the caregivingknowledge base by gathering various types of documents from the Internet. Inthis study, we focused on caregivers of individuals with Alzheimer's DiseaseRelated Dementias. We evaluated the models' performance using the benchmarkmetrics commonly used in evaluating language models and their reliability toprovide accurate references with the answers. The RAG framework improved theperformance of all FMs used in this study across all measures. As expected, thelarge FM performed better than small FMs across all metrics. The mostinteresting result is that small fine-tuned FMs with RAG performedsignificantly better than GPT 3.5 across all metrics. The fine-tuned LLaMA-2small FM performed better than GPT 3.5 (even with RAG) in returning referenceswith the answers. The study shows that reliable and accessible CaLM can bedeveloped by using small FMs with a knowledge base specific to the caregivingdomain.</description><author>Bambang Parmanto, Bayu Aryoyudanta, Wilbert Soekinto, I Made Agus Setiawan, Yuhan Wang, Haomin Hu, Andi Saptono, Yong K. Choi</author><pubDate>Mon, 11 Mar 2024 17:12:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06857v1</guid></item><item><title>Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification</title><link>http://arxiv.org/abs/2403.06854v1</link><description>Inverse reinforcement learning (IRL) aims to infer an agent's preferences(represented as a reward function $R$) from their behaviour (represented as apolicy $\pi$). To do this, we need a behavioural model of how $\pi$ relates to$R$. In the current literature, the most common behavioural models areoptimality, Boltzmann-rationality, and causal entropy maximisation. However,the true relationship between a human's preferences and their behaviour is muchmore complex than any of these behavioural models. This means that thebehavioural models are misspecified, which raises the concern that they maylead to systematic errors if applied to real data. In this paper, we analysehow sensitive the IRL problem is to misspecification of the behavioural model.Specifically, we provide necessary and sufficient conditions that completelycharacterise how the observed data may differ from the assumed behaviouralmodel without incurring an error above a given threshold. In addition to this,we also characterise the conditions under which a behavioural model is robustto small perturbations of the observed policy, and we analyse how robust manybehavioural models are to misspecification of their parameter values (such ase.g.\ the discount rate). Our analysis suggests that the IRL problem is highlysensitive to misspecification, in the sense that very mild misspecification canlead to very large errors in the inferred reward function.</description><author>Joar Skalse, Alessandro Abate</author><pubDate>Mon, 11 Mar 2024 17:09:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06854v1</guid></item><item><title>DiaLoc: An Iterative Approach to Embodied Dialog Localization</title><link>http://arxiv.org/abs/2403.06846v1</link><description>Multimodal learning has advanced the performance for many vision-languagetasks. However, most existing works in embodied dialog research focus onnavigation and leave the localization task understudied. The few existingdialog-based localization approaches assume the availability of entire dialogprior to localizaiton, which is impractical for deployed dialog-basedlocalization. In this paper, we propose DiaLoc, a new dialog-based localizationframework which aligns with a real human operator behavior. Specifically, weproduce an iterative refinement of location predictions which can visualizecurrent pose believes after each dialog turn. DiaLoc effectively utilizes themultimodal data for multi-shot localization, where a fusion encoder fusesvision and dialog information iteratively. We achieve state-of-the-art resultson embodied dialog-based localization task, in single-shot (+7.08% inAcc5@valUnseen) and multi- shot settings (+10.85% in Acc5@valUnseen). DiaLocnarrows the gap between simulation and real-world applications, opening doorsfor future research on collaborative localization and navigation.</description><author>Chao Zhang, Mohan Li, Ignas Budvytis, Stephan Liwicki</author><pubDate>Mon, 11 Mar 2024 17:03:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06846v1</guid></item><item><title>DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation</title><link>http://arxiv.org/abs/2403.06845v1</link><description>World models have demonstrated superiority in autonomous driving,particularly in the generation of multi-view driving videos. However,significant challenges still exist in generating customized driving videos. Inthis paper, we propose DriveDreamer-2, which builds upon the framework ofDriveDreamer and incorporates a Large Language Model (LLM) to generateuser-defined driving videos. Specifically, an LLM interface is initiallyincorporated to convert a user's query into agent trajectories. Subsequently, aHDMap, adhering to traffic regulations, is generated based on the trajectories.Ultimately, we propose the Unified Multi-View Model to enhance temporal andspatial coherence in the generated driving videos. DriveDreamer-2 is the firstworld model to generate customized driving videos, it can generate uncommondriving videos (e.g., vehicles abruptly cut in) in a user-friendly manner.Besides, experimental results demonstrate that the generated videos enhance thetraining of driving perception methods (e.g., 3D detection and tracking).Furthermore, video generation quality of DriveDreamer-2 surpasses otherstate-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7,representing relative improvements of 30% and 50%.</description><author>Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, Xingang Wang</author><pubDate>Mon, 11 Mar 2024 17:03:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06845v1</guid></item><item><title>Towards an educational tool for supporting neonatologists in the delivery room</title><link>http://arxiv.org/abs/2403.06843v1</link><description>Nowadays, there is evidence that several factors may increase the risk, foran infant, to require stabilisation or resuscitation manoeuvres at birth.However, this risk factors are not completely known, and a universallyapplicable model for predicting high-risk situations is not available yet.Considering both these limitations and the fact that the need for resuscitationat birth is a rare event, periodic training of the healthcare personnelresponsible for newborn caring in the delivery room is mandatory. In this paper, we propose a machine learning approach for identifying riskfactors and their impact on the birth event from real data, which can be usedby personnel to progressively increase and update their knowledge. Our finalgoal will be the one of designing a user-friendly mobile application, able toimprove the recognition rate and the planning of the appropriate interventionson high-risk patients.</description><author>Giorgio Leonardi, Clara Maldarizzi, Stefania Montani, Manuel Striani, Mariachiara Martina Strozzi</author><pubDate>Mon, 11 Mar 2024 17:03:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06843v1</guid></item><item><title>RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback</title><link>http://arxiv.org/abs/2403.06840v1</link><description>Large language models (LLMs) demonstrate exceptional performance in numeroustasks but still heavily rely on knowledge stored in their parameters. Moreover,updating this knowledge incurs high training costs. Retrieval-augmentedgeneration (RAG) methods address this issue by integrating external knowledge.The model can answer questions it couldn't previously by retrieving knowledgerelevant to the query. This approach improves performance in certain scenariosfor specific tasks. However, if irrelevant texts are retrieved, it may impairmodel performance. In this paper, we propose Retrieval Augmented IterativeSelf-Feedback (RA-ISF), a framework that iteratively decomposes tasks andprocesses them in three submodules to enhance the model's problem-solvingcapabilities. Experiments show that our method outperforms existing benchmarks,performing well on models like GPT3.5, Llama2, significantly enhancing factualreasoning capabilities and reducing hallucinations.</description><author>Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan Cao, Tianyu Du</author><pubDate>Mon, 11 Mar 2024 17:01:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06840v1</guid></item><item><title>Stochastic Cortical Self-Reconstruction</title><link>http://arxiv.org/abs/2403.06837v1</link><description>Magnetic resonance imaging (MRI) is critical for diagnosing neurodegenerativediseases, yet accurately assessing mild cortical atrophy remains a challengedue to its subtlety. Automated cortex reconstruction, paired with healthyreference ranges, aids in pinpointing pathological atrophy, yet theirgeneralization is limited by biases from image acquisition and processing. Weintroduce the concept of stochastic cortical self-reconstruction (SCSR) thatcreates a subject-specific healthy reference by taking MRI-derived thicknessesas input and, therefore, implicitly accounting for potential confounders. SCSRrandomly corrupts parts of the cortex and self-reconstructs them from theremaining information. Trained exclusively on healthy individuals, repeatedself-reconstruction generates a stochastic reference cortex for assessingdeviations from the norm. We present three implementations of this concept:XGBoost applied on parcels, and two autoencoders on vertex level -- one basedon a multilayer perceptron and the other using a spherical U-Net. These modelswere trained on healthy subjects from the UK Biobank and subsequently evaluatedacross four public Alzheimer's datasets. Finally, we deploy the model onclinical in-house data, where deviation maps' high spatial resolution aids indiscriminating between four types of dementia.</description><author>Christian Wachinger, Dennis Hedderich, Fabian Bongratz</author><pubDate>Mon, 11 Mar 2024 16:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06837v1</guid></item><item><title>Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks</title><link>http://arxiv.org/abs/2309.17002v2</link><description>Pre-training on large-scale datasets and then fine-tuning on downstream taskshave become a standard practice in deep learning. However, pre-training dataoften contain label noise that may adversely affect the generalization of themodel. This paper aims to understand the nature of noise in pre-trainingdatasets and to mitigate its impact on downstream tasks. More specifically,through extensive experiments of supervised pre-training models on syntheticnoisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noisein pre-training can benefit in-domain (ID) transfer performance, where thetraining and testing data share the same distribution, it always deterioratesout-of-domain (OOD) performance, where training and testing data distributionare different. We empirically verify that the reason behind is noise inpre-training shapes the feature space differently. We then propose alight-weight black-box tuning method (NMTune) to affine the feature space tomitigate the malignant effect of noise and improve generalization on both IDand OOD tasks, considering one may not be able to fully fine-tune or evenaccess the pre-trained models. We conduct practical experiments on popularvision and language models that are pre-trained on noisy data for evaluation ofour approach. Our analysis and results show the importance of this interestingand novel research direction, which we term Noisy Model Learning.</description><author>Hao Chen, Jindong Wang, Ankit Shah, Ran Tao, Hongxin Wei, Xing Xie, Masashi Sugiyama, Bhiksha Raj</author><pubDate>Mon, 11 Mar 2024 16:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17002v2</guid></item><item><title>Switching the Loss Reduces the Cost in Batch Reinforcement Learning</title><link>http://arxiv.org/abs/2403.05385v2</link><description>We propose training fitted Q-iteration with log-loss (FQI-LOG) for batchreinforcement learning (RL). We show that the number of samples needed to learna near-optimal policy with FQI-LOG scales with the accumulated cost of theoptimal policy, which is zero in problems where acting optimally achieves thegoal and incurs no cost. In doing so, we provide a general framework forproving $\textit{small-cost}$ bounds, i.e. bounds that scale with the optimalachievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG usesfewer samples than FQI trained with squared loss on problems where the optimalpolicy reliably achieves the goal.</description><author>Alex Ayoub, Kaiwen Wang, Vincent Liu, Samuel Robertson, James McInerney, Dawen Liang, Nathan Kallus, Csaba Szepesvári</author><pubDate>Mon, 11 Mar 2024 16:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05385v2</guid></item><item><title>Medical Image Synthesis via Fine-Grained Image-Text Alignment and Anatomy-Pathology Prompting</title><link>http://arxiv.org/abs/2403.06835v1</link><description>Data scarcity and privacy concerns limit the availability of high-qualitymedical images for public use, which can be mitigated through medical imagesynthesis. However, current medical image synthesis methods often struggle toaccurately capture the complexity of detailed anatomical structures andpathological conditions. To address these challenges, we propose a novelmedical image synthesis model that leverages fine-grained image-text alignmentand anatomy-pathology prompts to generate highly detailed and accuratesynthetic medical images. Our method integrates advanced natural languageprocessing techniques with image generative modeling, enabling precisealignment between descriptive text prompts and the synthesized images'anatomical and pathological details. The proposed approach consists of two keycomponents: an anatomy-pathology prompting module and a fine-grainedalignment-based synthesis module. The anatomy-pathology prompting moduleautomatically generates descriptive prompts for high-quality medical images. Tofurther synthesize high-quality medical images from the generated prompts, thefine-grained alignment-based synthesis module pre-defines a visual codebook forthe radiology dataset and performs fine-grained alignment between the codebookand generated prompts to obtain key patches as visual clues, facilitatingaccurate image synthesis. We validate the superiority of our method throughexperiments on public chest X-ray datasets and demonstrate that our syntheticimages preserve accurate semantic information, making them valuable for variousmedical applications.</description><author>Wenting Chen, Pengyu Wang, Hui Ren, Lichao Sun, Quanzheng Li, Yixuan Yuan, Xiang Li</author><pubDate>Mon, 11 Mar 2024 16:56:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06835v1</guid></item><item><title>Prompt-Driven Building Footprint Extraction in Aerial Images with Offset-Building Model</title><link>http://arxiv.org/abs/2310.16717v3</link><description>More accurate extraction of invisible building footprints fromvery-high-resolution (VHR) aerial images relies on roof segmentation androof-to-footprint offset extraction. Existing state-of-the-art methods based oninstance segmentation suffer from poor generalization when extended tolarge-scale data production and fail to achieve low-cost human interactiveannotation. The latest prompt paradigms inspire us to design a promptableframework for roof and offset extraction, which transforms end-to-endalgorithms into promptable methods. Within this framework, we propose a novelOffset-Building Model (OBM). To rigorously evaluate the algorithm'scapabilities, we introduce a prompt-based evaluation method, where our modelreduces offset errors by 16.6% and improves roof Intersection over Union (IoU)by 10.8% compared to other models. Leveraging the common patterns in predictingoffsets, we propose Distance-NMS (DNMS) algorithms, enabling the model tofurther reduce offset vector loss by 6.5%. To further validate thegeneralization of models, we tested them using a new dataset with over 7,000manually annotated instance samples. Our algorithms and dataset are availableat https://anonymous.4open.science/r/OBM-B3EC.</description><author>Kai Li, Yupeng Deng, Yunlong Kong, Diyou Liu, Jingbo Chen, Yu Meng, Junxian Ma</author><pubDate>Mon, 11 Mar 2024 16:54:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16717v3</guid></item><item><title>PhAST: Physics-Aware, Scalable, and Task-specific GNNs for Accelerated Catalyst Design</title><link>http://arxiv.org/abs/2211.12020v4</link><description>Mitigating the climate crisis requires a rapid transition towardslower-carbon energy. Catalyst materials play a crucial role in theelectrochemical reactions involved in numerous industrial processes key to thistransition, such as renewable energy storage and electrofuel synthesis. Toreduce the energy spent on such activities, we must quickly discover moreefficient catalysts to drive electrochemical reactions. Machine learning (ML)holds the potential to efficiently model materials properties from largeamounts of data, accelerating electrocatalyst design. The Open Catalyst ProjectOC20 dataset was constructed to that end. However, ML models trained on OC20are still neither scalable nor accurate enough for practical applications. Inthis paper, we propose task-specific innovations applicable to mostarchitectures, enhancing both computational efficiency and accuracy. Thisincludes improvements in (1) the graph creation step, (2) atom representations,(3) the energy prediction head, and (4) the force prediction head. We describethese contributions, referred to as PhAST, and evaluate them thoroughly onmultiple architectures. Overall, PhAST improves energy MAE by 4 to 42$\%$ whiledividing compute time by 3 to 8$\times$ depending on the targeted task/model.PhAST also enables CPU training, leading to 40$\times$ speedups in highlyparallelized settings. Python package: \url{https://phast.readthedocs.io}.</description><author>Alexandre Duval, Victor Schmidt, Santiago Miret, Yoshua Bengio, Alex Hernández-García, David Rolnick</author><pubDate>Mon, 11 Mar 2024 16:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.12020v4</guid></item><item><title>Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?</title><link>http://arxiv.org/abs/2403.06833v1</link><description>Instruction-tuned Large Language Models (LLMs) have achieved breakthroughresults, opening countless new possibilities for many practical applications.However, LLMs lack elementary safety features that are established norms inother areas of computer science, such as the separation between instructionsand data, causing them to malfunction or rendering them vulnerable tomanipulation and interference by third parties e.g., via indirectprompt/command injection. Even worse, so far, there is not even an establisheddefinition of what precisely such a separation would mean and how its violationcould be tested. In this work, we aim to close this gap. We introduce a formalmeasure to quantify the phenomenon of instruction-data separation as well as anempirical variant of the measure that can be computed from a model`s black-boxoutputs. We also introduce a new dataset, SEP (Should it be Executed orProcessed?), which allows estimating the measure, and we report results onseveral state-of-the-art open-source and closed LLMs. Finally, wequantitatively demonstrate that all evaluated LLMs fail to achieve a highamount of separation, according to our measure. The source code and SEP datasetare openly accessible athttps://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.</description><author>Egor Zverev, Sahar Abdelnabi, Mario Fritz, Christoph H. Lampert</author><pubDate>Mon, 11 Mar 2024 16:48:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06833v1</guid></item><item><title>The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework</title><link>http://arxiv.org/abs/2403.06832v1</link><description>The advancement of Multi-modal Pre-training highlights the necessity for arobust Multi-Modal Knowledge Graph (MMKG) representation learning framework.This framework is crucial for integrating structured knowledge into multi-modalLarge Language Models (LLMs) at scale, aiming to alleviate issues likeknowledge misconceptions and multi-modal hallucinations. In this work, toevaluate models' ability to accurately embed entities within MMKGs, we focus ontwo widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) andMulti-modal Entity Alignment (MMEA). Building on this foundation, we propose anovel SNAG method that utilizes a Transformer-based architecture equipped withmodality-level noise masking for the robust integration of multi-modal entityfeatures in KGs. By incorporating specific training objectives for both MKGCand MMEA, our approach achieves SOTA performance across a total of ten datasets(three for MKGC and seven for MEMA), demonstrating its robustness andversatility. Besides, SNAG can not only function as a standalone model but alsoenhance other existing methods, providing stable performance improvements. Ourcode and data are available at: https://github.com/zjukg/SNAG.</description><author>Zhuo Chen, Yin Fang, Yichi Zhang, Lingbing Guo, Jiaoyan Chen, Huajun Chen, Wen Zhang</author><pubDate>Mon, 11 Mar 2024 16:48:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06832v1</guid></item><item><title>HDRTransDC: High Dynamic Range Image Reconstruction with Transformer Deformation Convolution</title><link>http://arxiv.org/abs/2403.06831v1</link><description>High Dynamic Range (HDR) imaging aims to generate an artifact-free HDR imagewith realistic details by fusing multi-exposure Low Dynamic Range (LDR) images.Caused by large motion and severe under-/over-exposure among input LDR images,HDR imaging suffers from ghosting artifacts and fusion distortions. To addressthese critical issues, we propose an HDR Transformer Deformation Convolution(HDRTransDC) network to generate high-quality HDR images, which consists of theTransformer Deformable Convolution Alignment Module (TDCAM) and the DynamicWeight Fusion Block (DWFB). To solve the ghosting artifacts, the proposed TDCAMextracts long-distance content similar to the reference feature in the entirenon-reference features, which can accurately remove misalignment and fill thecontent occluded by moving objects. For the purpose of eliminating fusiondistortions, we propose DWFB to spatially adaptively select useful informationacross frames to effectively fuse multi-exposed features. Extensive experimentsshow that our method quantitatively and qualitatively achieves state-of-the-artperformance.</description><author>Shuaikang Shang, Xuejing Kang, Anlong Ming</author><pubDate>Mon, 11 Mar 2024 16:48:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06831v1</guid></item><item><title>Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators</title><link>http://arxiv.org/abs/2307.05358v3</link><description>Federated learning has become a popular method to learn from decentralizedheterogeneous data. Federated semi-supervised learning (FSSL) emerges to trainmodels from a small fraction of labeled data due to label scarcity ondecentralized clients. Existing FSSL methods assume independent and identicallydistributed (IID) labeled data across clients and consistent class distributionbetween labeled and unlabeled data within a client. This work studies a morepractical and challenging scenario of FSSL, where data distribution isdifferent not only across clients but also within a client between labeled andunlabeled data. To address this challenge, we propose a novel FSSL frameworkwith dual regulators, FedDure. FedDure lifts the previous assumption with acoarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-regregularizes the updating of the local model by tracking the learning effect onlabeled data distribution; F-reg learns an adaptive weighting scheme tailoredfor unlabeled instances in each client. We further formulate the client modeltraining as bi-level optimization that adaptively optimizes the model in theclient with two regulators. Theoretically, we show the convergence guarantee ofthe dual regulators. Empirically, we demonstrate that FedDure is superior tothe existing methods across a wide range of settings, notably by more than 11on CIFAR-10 and CINIC-10 datasets.</description><author>Sikai Bai, Shuaicheng Li, Weiming Zhuang, Jie Zhang, Song Guo, Kunlin Yang, Jun Hou, Shuai Zhang, Junyu Gao, Shuai Yi</author><pubDate>Mon, 11 Mar 2024 16:48:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05358v3</guid></item><item><title>Machine learning reveals features of spinon Fermi surface</title><link>http://arxiv.org/abs/2306.03143v2</link><description>With rapid progress in simulation of strongly interacting quantumHamiltonians, the challenge in characterizing unknown phases becomes abottleneck for scientific progress. We demonstrate that a Quantum-Classicalhybrid approach (QuCl) of mining sampled projective snapshots withinterpretable classical machine learning can unveil signatures of seeminglyfeatureless quantum states. The Kitaev-Heisenberg model on a honeycomb latticeunder external magnetic field presents an ideal system to test QuCl, wheresimulations have found an intermediate gapless phase (IGP) sandwiched betweenknown phases, launching a debate over its elusive nature. We use the correlatorconvolutional neural network, trained on labeled projective snapshots, inconjunction with regularization path analysis to identify signatures of phases.We show that QuCl reproduces known features of established phases.Significantly, we also identify a signature of the IGP in the spin channelperpendicular to the field direction, which we interpret as a signature ofFriedel oscillations of gapless spinons forming a Fermi surface. Ourpredictions can guide future experimental searches for spin liquids.</description><author>Kevin Zhang, Shi Feng, Yuri D. Lensky, Nandini Trivedi, Eun-Ah Kim</author><pubDate>Mon, 11 Mar 2024 16:46:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03143v2</guid></item><item><title>Constructing Variables Using Classifiers as an Aid to Regression: An Empirical Assessment</title><link>http://arxiv.org/abs/2403.06829v1</link><description>This paper proposes a method for the automatic creation of variables (in thecase of regression) that complement the information contained in the initialinput vector. The method works as a pre-processing step in which the continuousvalues of the variable to be regressed are discretized into a set of intervalswhich are then used to define value thresholds. Then classifiers are trained topredict whether the value to be regressed is less than or equal to each ofthese thresholds. The different outputs of the classifiers are thenconcatenated in the form of an additional vector of variables that enriches theinitial vector of the regression problem. The implemented system can thus beconsidered as a generic pre-processing tool. We tested the proposed enrichmentmethod with 5 types of regressors and evaluated it in 33 regression datasets.Our experimental results confirm the interest of the approach.</description><author>Colin Troisemaine, Vincent Lemaire</author><pubDate>Mon, 11 Mar 2024 16:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06829v1</guid></item><item><title>NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning</title><link>http://arxiv.org/abs/2403.06828v1</link><description>Navigating a nonholonomic robot in a cluttered environment requires extremelyaccurate perception and locomotion for collision avoidance. This paper presentsNeuPAN: a real-time, highly-accurate, map-free, robot-agnostic, andenvironment-invariant robot navigation solution. Leveraging a tightly-coupledperception-locomotion framework, NeuPAN has two key innovations compared toexisting approaches: 1) it directly maps raw points to a learned multi-framedistance space, avoiding error propagation from perception to control; 2) it isinterpretable from an end-to-end model-based learning perspective, enablingprovable convergence. The crux of NeuPAN is to solve a high-dimensionalend-to-end mathematical model with various point-level constraints using theplug-and-play (PnP) proximal alternating-minimization network (PAN) withneurons in the loop. This allows NeuPAN to generate real-time, end-to-end,physically-interpretable motions directly from point clouds, which seamlesslyintegrates data- and knowledge-engines, where its network parameters areadjusted via back propagation. We evaluate NeuPAN on car-like robot,wheel-legged robot, and passenger autonomous vehicle, in both simulated andreal-world environments. Experiments demonstrate that NeuPAN outperformsvarious benchmarks, in terms of accuracy, efficiency, robustness, andgeneralization capability across various environments, including the clutteredsandbox, office, corridor, and parking lot. We show that NeuPAN works well inunstructured environments with arbitrary-shape undetectable objects, makingimpassable ways passable.</description><author>Ruihua Han, Shuai Wang, Shuaijun Wang, Zeqing Zhang, Jianjun Chen, Shijie Lin, Chengyang Li, Chengzhong Xu, Yonina C. Eldar, Qi Hao, Jia Pan</author><pubDate>Mon, 11 Mar 2024 16:44:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06828v1</guid></item><item><title>In-context Exploration-Exploitation for Reinforcement Learning</title><link>http://arxiv.org/abs/2403.06826v1</link><description>In-context learning is a promising approach for online policy learning ofoffline reinforcement learning (RL) methods, which can be achieved at inferencetime without gradient optimization. However, this method is hindered bysignificant computational costs resulting from the gathering of large trainingtrajectory sets and the need to train large Transformer models. We address thischallenge by introducing an In-context Exploration-Exploitation (ICEE)algorithm, designed to optimize the efficiency of in-context policy learning.Unlike existing models, ICEE performs an exploration-exploitation trade-off atinference time within a Transformer model, without the need for explicitBayesian inference. Consequently, ICEE can solve Bayesian optimization problemsas efficiently as Gaussian process biased methods do, but in significantly lesstime. Through experiments in grid world environments, we demonstrate that ICEEcan learn to solve new RL tasks using only tens of episodes, marking asubstantial improvement over the hundreds of episodes needed by the previousin-context learning method.</description><author>Zhenwen Dai, Federico Tomasi, Sina Ghiassian</author><pubDate>Mon, 11 Mar 2024 16:43:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06826v1</guid></item><item><title>Spectral invariance and maximality properties of the frequency spectrum of quantum neural networks</title><link>http://arxiv.org/abs/2402.14515v2</link><description>Quantum Neural Networks (QNNs) are a popular approach in Quantum MachineLearning due to their close connection to Variational Quantum Circuits, makingthem a promising candidate for practical applications on NoisyIntermediate-Scale Quantum (NISQ) devices. A QNN can be expressed as a finiteFourier series, where the set of frequencies is called the frequency spectrum.We analyse this frequency spectrum and prove, for a large class of models,various maximality results. Furthermore, we prove that under some mildconditions there exists a bijection between classes of models with the samearea $A = RL$ that preserves the frequency spectrum, where $R$ denotes thenumber of qubits and $L$ the number of layers, which we consequently callspectral invariance under area-preserving transformations. With this we explainthe symmetry in $R$ and $L$ in the results often observed in the literature andshow that the maximal frequency spectrum depends only on the area $A = RL$ andnot on the individual values of $R$ and $L$. Moreover, we extend existingresults and specify the maximum possible frequency spectrum of a QNN witharbitrarily many layers as a function of the spectrum of its generators. If thegenerators of the QNN can be further decomposed into 2-dimensionalsub-generators, then this specification follows from elementarynumber-theoretical considerations. In the case of arbitrary dimensionalgenerators, we extend existing results based on the so-called Golomb ruler andintroduce a second novel approach based on a variation of the turnpike problem,which we call the relaxed turnpike problem.</description><author>Patrick Holzer, Ivica Turkalj</author><pubDate>Mon, 11 Mar 2024 16:40:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14515v2</guid></item><item><title>Distributional Reinforcement Learning with Online Risk-awareness Adaption</title><link>http://arxiv.org/abs/2310.05179v2</link><description>The use of reinforcement learning (RL) in practical applications requiresconsidering sub-optimal outcomes, which depend on the agent's familiarity withthe uncertain environment. Dynamically adjusting the level of epistemic riskover the course of learning can tactically achieve reliable optimal policy insafety-critical environments and tackle the sub-optimality of a static risklevel. In this work, we introduce a novel framework, Distributional RL withOnline Risk Adaption (DRL-ORA), which can quantify the aleatory and epistemicuncertainties compositely and dynamically select the epistemic risk levels viasolving a total variation minimization problem online. The risk level selectioncan be efficiently achieved through grid search using a Follow-The-Leader typealgorithm, and its offline oracle is related to "satisficing measure" (in thedecision analysis community) under a special modification of the loss function.We show multiple classes of tasks where DRL-ORA outperforms existing methodsthat rely on either a fixed risk level or manually predetermined risk leveladaption. Given the simplicity of our modifications, we believe the frameworkcan be easily incorporated into most RL algorithm variants.</description><author>Yupeng Wu, Wenjie Huang</author><pubDate>Mon, 11 Mar 2024 16:36:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05179v2</guid></item><item><title>Are Targeted Messages More Effective?</title><link>http://arxiv.org/abs/2403.06817v1</link><description>Graph neural networks (GNN) are deep learning architectures for graphs.Essentially, a GNN is a distributed message passing algorithm, which iscontrolled by parameters learned from data. It operates on the vertices of agraph: in each iteration, vertices receive a message on each incoming edge,aggregate these messages, and then update their state based on their currentstate and the aggregated messages. The expressivity of GNNs can becharacterised in terms of certain fragments of first-order logic with countingand the Weisfeiler-Lehman algorithm. The core GNN architecture comes in two different versions. In the firstversion, a message only depends on the state of the source vertex, whereas inthe second version it depends on the states of the source and target vertices.In practice, both of these versions are used, but the theory of GNNs so farmostly focused on the first one. On the logical side, the two versionscorrespond to two fragments of first-order logic with counting that we callmodal and guarded. The question whether the two versions differ in their expressivity has beenmostly overlooked in the GNN literature and has only been asked recently(Grohe, LICS'23). We answer this question here. It turns out that the answer isnot as straightforward as one might expect. By proving that the modal andguarded fragment of first-order logic with counting have the same expressivityover labelled undirected graphs, we show that in a non-uniform setting the twoGNN versions have the same expressivity. However, we also prove that in auniform setting the second version is strictly more expressive.</description><author>Martin Grohe, Eran Rosenbluth</author><pubDate>Mon, 11 Mar 2024 16:34:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06817v1</guid></item><item><title>Efficient first-order algorithms for large-scale, non-smooth maximum entropy models with application to wildfire science</title><link>http://arxiv.org/abs/2403.06816v1</link><description>Maximum entropy (Maxent) models are a class of statistical models that usethe maximum entropy principle to estimate probability distributions from data.Due to the size of modern data sets, Maxent models need efficient optimizationalgorithms to scale well for big data applications. State-of-the-art algorithmsfor Maxent models, however, were not originally designed to handle big datasets; these algorithms either rely on technical devices that may yieldunreliable numerical results, scale poorly, or require smoothness assumptionsthat many practical Maxent models lack. In this paper, we present noveloptimization algorithms that overcome the shortcomings of state-of-the-artalgorithms for training large-scale, non-smooth Maxent models. Our proposedfirst-order algorithms leverage the Kullback-Leibler divergence to trainlarge-scale and non-smooth Maxent models efficiently. For Maxent models withdiscrete probability distribution of $n$ elements built from samples, eachcontaining $m$ features, the stepsize parameters estimation and iterations inour algorithms scale on the order of $O(mn)$ operations and can be triviallyparallelized. Moreover, the strong $\ell_{1}$ convexity of theKullback--Leibler divergence allows for larger stepsize parameters, therebyspeeding up the convergence rate of our algorithms. To illustrate theefficiency of our novel algorithms, we consider the problem of estimatingprobabilities of fire occurrences as a function of ecological features in theWestern US MTBS-Interagency wildfire data set. Our numerical results show thatour algorithms outperform the state of the arts by one order of magnitude andyield results that agree with physical models of wildfire occurrence andprevious statistical analyses of wildfire drivers.</description><author>Gabriel P. Langlois, Jatan Buch, Jérôme Darbon</author><pubDate>Mon, 11 Mar 2024 16:33:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06816v1</guid></item><item><title>ε-Neural Thompson Sampling of Deep Brain Stimulation for Parkinson Disease Treatment</title><link>http://arxiv.org/abs/2403.06814v1</link><description>Deep Brain Stimulation (DBS) stands as an effective intervention foralleviating the motor symptoms of Parkinson's disease (PD). Traditionalcommercial DBS devices are only able to deliver fixed-frequency periodic pulsesto the basal ganglia (BG) regions of the brain, i.e., continuous DBS (cDBS).However, they in general suffer from energy inefficiency and side effects, suchas speech impairment. Recent research has focused on adaptive DBS (aDBS) toresolve the limitations of cDBS. Specifically, reinforcement learning (RL)based approaches have been developed to adapt the frequencies of the stimuli inorder to achieve both energy efficiency and treatment efficacy. However, RLapproaches in general require significant amount of training data andcomputational resources, making it intractable to integrate RL policies intoreal-time embedded systems as needed in aDBS. In contrast, contextualmulti-armed bandits (CMAB) in general lead to better sample efficiency comparedto RL. In this study, we propose a CMAB solution for aDBS. Specifically, wedefine the context as the signals capturing irregular neuronal firingactivities in the BG regions (i.e., beta-band power spectral density), whileeach arm signifies the (discretized) pulse frequency of the stimulation.Moreover, an {\epsilon}-exploring strategy is introduced on top of the classicThompson sampling method, leading to an algorithm called {\epsilon}-NeuralThompson sampling ({\epsilon}-NeuralTS), such that the learned CMAB policy canbetter balance exploration and exploitation of the BG environment. The{\epsilon}-NeuralTS algorithm is evaluated using a computation BG model thatcaptures the neuronal activities in PD patients' brains. The results show thatour method outperforms both existing cDBS methods and CMAB baselines.</description><author>Hao-Lun Hsu, Qitong Gao, Miroslav Pajic</author><pubDate>Mon, 11 Mar 2024 16:33:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06814v1</guid></item><item><title>LeOCLR: Leveraging Original Images for Contrastive Learning of Visual Representations</title><link>http://arxiv.org/abs/2403.06813v1</link><description>Contrastive instance discrimination outperforms supervised learning indownstream tasks like image classification and object detection. However, thisapproach heavily relies on data augmentation during representation learning,which may result in inferior results if not properly implemented. Randomcropping followed by resizing is a common form of data augmentation used incontrastive learning, but it can lead to degraded representation learning ifthe two random crops contain distinct semantic content. To address this issue,this paper introduces LeOCLR (Leveraging Original Images for ContrastiveLearning of Visual Representations), a framework that employs a new instancediscrimination approach and an adapted loss function that ensures the sharedregion between positive pairs is semantically correct. The experimental resultsshow that our approach consistently improves representation learning acrossdifferent datasets compared to baseline models. For example, our approachoutperforms MoCo-v2 by 5.1% on ImageNet-1K in linear evaluation and severalother methods on transfer learning tasks.</description><author>Mohammad Alkhalefi, Georgios Leontidis, Mingjun Zhong</author><pubDate>Mon, 11 Mar 2024 16:33:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06813v1</guid></item><item><title>Monotone Individual Fairness</title><link>http://arxiv.org/abs/2403.06812v1</link><description>We revisit the problem of online learning with individual fairness, where anonline learner strives to maximize predictive accuracy while ensuring thatsimilar individuals are treated similarly. We first extend the frameworks ofGillen et al. (2018); Bechavod et al. (2020), which rely on feedback from humanauditors regarding fairness violations, as we consider auditing schemes thatare capable of aggregating feedback from any number of auditors, using a richclass we term monotone aggregation functions. We then prove a characterizationfor such auditing schemes, practically reducing the analysis of auditing forindividual fairness by multiple auditors to that of auditing by(instance-specific) single auditors. Using our generalized framework, wepresent an oracle-efficient algorithm achieving an upper bound frontier of$(\mathcal{O}(T^{1/2+2b}),\mathcal{O}(T^{3/4-b}))$ respectively for regret,number of fairness violations, for $0\leq b \leq 1/4$. We then study an onlineclassification setting where label feedback is available forpositively-predicted individuals only, and present an oracle-efficientalgorithm achieving an upper bound frontier of$(\mathcal{O}(T^{2/3+2b}),\mathcal{O}(T^{5/6-b}))$ for regret, number offairness violations, for $0\leq b \leq 1/6$. In both settings, our algorithmsimprove on the best known bounds for oracle-efficient algorithms. Furthermore,our algorithms offer significant improvements in computational efficiency,greatly reducing the number of required calls to an (offline) optimizationoracle per round, to $\tilde{\mathcal{O}}(\alpha^{-2})$ in the full informationsetting, and $\tilde{\mathcal{O}}(\alpha^{-2} + k^2T^{1/3})$ in the partialinformation setting, where $\alpha$ is the sensitivity for reporting fairnessviolations, and $k$ is the number of individuals in a round.</description><author>Yahav Bechavod</author><pubDate>Mon, 11 Mar 2024 16:32:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06812v1</guid></item><item><title>Deep Learning Approaches for Human Action Recognition in Video Data</title><link>http://arxiv.org/abs/2403.06810v1</link><description>Human action recognition in videos is a critical task with significantimplications for numerous applications, including surveillance, sportsanalytics, and healthcare. The challenge lies in creating models that are bothprecise in their recognition capabilities and efficient enough for practicaluse. This study conducts an in-depth analysis of various deep learning modelsto address this challenge. Utilizing a subset of the UCF101 Videos dataset, wefocus on Convolutional Neural Networks (CNNs), Recurrent Neural Networks(RNNs), and Two-Stream ConvNets. The research reveals that while CNNseffectively capture spatial features and RNNs encode temporal sequences,Two-Stream ConvNets exhibit superior performance by integrating spatial andtemporal dimensions. These insights are distilled from the evaluation metricsof accuracy, precision, recall, and F1-score. The results of this studyunderscore the potential of composite models in achieving robust human actionrecognition and suggest avenues for future research in optimizing these modelsfor real-world deployment.</description><author>Yufei Xie</author><pubDate>Mon, 11 Mar 2024 16:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06810v1</guid></item><item><title>Multistep Consistency Models</title><link>http://arxiv.org/abs/2403.06807v1</link><description>Diffusion models are relatively easy to train but require many steps togenerate samples. Consistency models are far more difficult to train, butgenerate samples in a single step. In this paper we propose Multistep Consistency Models: A unification betweenConsistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) thatcan interpolate between a consistency model and a diffusion model: a trade-offbetween sampling speed and sampling quality. Specifically, a 1-step consistencymodel is a conventional consistency model whereas we show that a $\infty$-stepconsistency model is a diffusion model. Multistep Consistency Models work really well in practice. By increasing thesample budget from a single step to 2-8 steps, we can train models more easilythat generate higher quality samples, while retaining much of the samplingspeed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1FID on Imagenet128 in 8 steps with consistency distillation. We also show thatour method scales to a text-to-image diffusion model, generating samples thatare very close to the quality of the original model.</description><author>Jonathan Heek, Emiel Hoogeboom, Tim Salimans</author><pubDate>Mon, 11 Mar 2024 16:26:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06807v1</guid></item><item><title>Compositional Generative Inverse Design</title><link>http://arxiv.org/abs/2401.13171v2</link><description>Inverse design, where we seek to design input variables in order to optimizean underlying objective function, is an important problem that arises acrossfields such as mechanical engineering to aerospace engineering. Inverse designis typically formulated as an optimization problem, with recent worksleveraging optimization across learned dynamics models. However, as models areoptimized they tend to fall into adversarial modes, preventing effectivesampling. We illustrate that by instead optimizing over the learned energyfunction captured by the diffusion model, we can avoid such adversarialexamples and significantly improve design performance. We further illustratehow such a design system is compositional, enabling us to combine multipledifferent diffusion models representing subcomponents of our desired system todesign systems with every specified component. In an N-body interaction taskand a challenging 2D multi-airfoil design task, we demonstrate that bycomposing the learned diffusion model at test time, our method allows us todesign initial states and boundary shapes that are more complex than those inthe training data. Our method generalizes to more objects for N-body datasetand discovers formation flying to minimize drag in the multi-airfoil designtask. Project website and code can be found athttps://github.com/AI4Science-WestlakeU/cindm.</description><author>Tailin Wu, Takashi Maruyama, Long Wei, Tao Zhang, Yilun Du, Gianluca Iaccarino, Jure Leskovec</author><pubDate>Mon, 11 Mar 2024 16:25:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13171v2</guid></item><item><title>On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes</title><link>http://arxiv.org/abs/2403.06806v1</link><description>We present the first finite time global convergence analysis of policygradient in the context of infinite horizon average reward Markov decisionprocesses (MDPs). Specifically, we focus on ergodic tabular MDPs with finitestate and action spaces. Our analysis shows that the policy gradient iteratesconverge to the optimal policy at a sublinear rate of$O\left({\frac{1}{T}}\right),$ which translates to $O\left({\log(T)}\right)$regret, where $T$ represents the number of iterations. Prior work onperformance bounds for discounted reward MDPs cannot be extended to averagereward MDPs because the bounds grow proportional to the fifth power of theeffective horizon. Thus, our primary contribution is in proving that the policygradient algorithm converges for average-reward MDPs and in obtainingfinite-time performance guarantees. In contrast to the existing discountedreward performance bounds, our performance bounds have an explicit dependenceon constants that capture the complexity of the underlying MDP. Motivated bythis observation, we reexamine and improve the existing performance bounds fordiscounted reward MDPs. We also present simulations to empirically evaluate theperformance of average reward policy gradient algorithm.</description><author>Navdeep Kumar, Yashaswini Murthy, Itai Shufaro, Kfir Y. Levy, R. Srikant, Shie Mannor</author><pubDate>Mon, 11 Mar 2024 16:25:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06806v1</guid></item><item><title>On the Robustness of Lexicase Selection to Contradictory Objectives</title><link>http://arxiv.org/abs/2403.06805v1</link><description>Lexicase and epsilon-lexicase selection are state of the art parent selectiontechniques for problems featuring multiple selection criteria. Originally,lexicase selection was developed for cases where these selection criteria areunlikely to be in conflict with each other, but preliminary work suggests it isalso a highly effective many-objective optimization algorithm. However, topredict whether these results generalize, we must understand lexicaseselection's performance on contradictory objectives. Prior work has shown mixedresults on this question. Here, we develop theory identifying circumstancesunder which lexicase selection will succeed or fail to find a Pareto-optimalsolution. To make this analysis tractable, we restrict our investigation to atheoretical problem with maximally contradictory objectives. Ultimately, wefind that lexicase and epsilon-lexicase selection each have a region ofparameter space where they are incapable of optimizing contradictoryobjectives. Outside of this region, however, they perform well despite thepresence of contradictory objectives. Based on these findings, we proposetheoretically-backed guidelines for parameter choice. Additionally, we identifyother properties that may affect whether a many-objective optimization problemis a good fit for lexicase or epsilon-lexicase selection.</description><author>Shakiba Shahbandegan, Emily Dolson</author><pubDate>Mon, 11 Mar 2024 16:23:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06805v1</guid></item><item><title>Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape Matching via Unsupervised Functional Map Regularized Reconstruction</title><link>http://arxiv.org/abs/2403.06804v1</link><description>We present Shape Non-rigid Kinematics (SNK), a novel zero-shot method fornon-rigid shape matching that eliminates the need for extensive training orground truth data. SNK operates on a single pair of shapes, and employs areconstruction-based strategy using an encoder-decoder architecture, whichdeforms the source shape to closely match the target shape. During the process,an unsupervised functional map is predicted and converted into a point-to-pointmap, serving as a supervisory mechanism for the reconstruction. To aid intraining, we have designed a new decoder architecture that generates smooth,realistic deformations. SNK demonstrates competitive results on traditionalbenchmarks, simplifying the shape-matching process without compromisingaccuracy. Our code can be found online: https://github.com/pvnieo/SNK</description><author>Souhaib Attaiki, Maks Ovsjanikov</author><pubDate>Mon, 11 Mar 2024 16:23:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06804v1</guid></item></channel></rss>