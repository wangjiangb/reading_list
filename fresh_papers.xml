<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 08 Sep 2024 13:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding</title><link>http://arxiv.org/abs/2409.03757v1</link><description>Complex 3D scene understanding has gained increasing attention, with sceneencoding strategies playing a crucial role in this success. However, theoptimal scene encoding strategies for various scenarios remain unclear,particularly compared to their image-based counterparts. To address this issue,we present a comprehensive study that probes various visual encoding models for3D scene understanding, identifying the strengths and limitations of each modelacross different scenarios. Our evaluation spans seven vision foundationencoders, including image-based, video-based, and 3D foundation models. Weevaluate these models in four tasks: Vision-Language Scene Reasoning, VisualGrounding, Segmentation, and Registration, each focusing on different aspectsof scene understanding. Our evaluations yield key findings: DINOv2 demonstratessuperior performance, video models excel in object-level tasks, diffusionmodels benefit geometric tasks, and language-pretrained models show unexpectedlimitations in language-related tasks. These insights challenge someconventional understandings, provide novel perspectives on leveraging visualfoundation models, and highlight the need for more flexible encoder selectionin future vision-language and scene-understanding tasks.</description><author>Yunze Man, Shuhong Zheng, Zhipeng Bao, Martial Hebert, Liang-Yan Gui, Yu-Xiong Wang</author><pubDate>Thu, 05 Sep 2024 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03757v1</guid></item><item><title>DC-Solver: Improving Predictor-Corrector Diffusion Sampler via Dynamic Compensation</title><link>http://arxiv.org/abs/2409.03755v1</link><description>Diffusion probabilistic models (DPMs) have shown remarkable performance invisual synthesis but are computationally expensive due to the need for multipleevaluations during the sampling. Recent predictor-corrector diffusion samplershave significantly reduced the required number of function evaluations (NFE),but inherently suffer from a misalignment issue caused by the extra correctorstep, especially with a large classifier-free guidance scale (CFG). In thispaper, we introduce a new fast DPM sampler called DC-Solver, which leveragesdynamic compensation (DC) to mitigate the misalignment of thepredictor-corrector samplers. The dynamic compensation is controlled bycompensation ratios that are adaptive to the sampling steps and can beoptimized on only 10 datapoints by pushing the sampling trajectory toward aground truth trajectory. We further propose a cascade polynomial regression(CPR) which can instantly predict the compensation ratios on unseen samplingconfigurations. Additionally, we find that the proposed dynamic compensationcan also serve as a plug-and-play module to boost the performance ofpredictor-only samplers. Extensive experiments on both unconditional samplingand conditional sampling demonstrate that our DC-Solver can consistentlyimprove the sampling quality over previous methods on different DPMs with awide range of resolutions up to 1024$\times$1024. Notably, we achieve 10.38 FID(NFE=5) on unconditional FFHQ and 0.394 MSE (NFE=5, CFG=7.5) onStable-Diffusion-2.1. Code is available at https://github.com/wl-zhao/DC-Solver</description><author>Wenliang Zhao, Haolin Wang, Jie Zhou, Jiwen Lu</author><pubDate>Thu, 05 Sep 2024 17:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03755v1</guid></item><item><title>Foundation Model or Finetune? Evaluation of few-shot semantic segmentation for river pollution</title><link>http://arxiv.org/abs/2409.03754v1</link><description>Foundation models (FMs) are a popular topic of research in AI. Their abilityto generalize to new tasks and datasets without retraining or needing anabundance of data makes them an appealing candidate for applications onspecialist datasets. In this work, we compare the performance of FMs tofinetuned pre-trained supervised models in the task of semantic segmentation onan entirely new dataset. We see that finetuned models consistently outperformthe FMs tested, even in cases were data is scarce. We release the code anddataset for this work on GitHub.</description><author>Marga Don, Stijn Pinson, Blanca Guillen Cebrian, Yuki M. Asano</author><pubDate>Thu, 05 Sep 2024 17:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03754v1</guid></item><item><title>WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild</title><link>http://arxiv.org/abs/2409.03753v1</link><description>The increasing availability of real-world conversation data offers excitingopportunities for researchers to study user-chatbot interactions. However, thesheer volume of this data makes manually examining individual conversationsimpractical. To overcome this challenge, we introduce WildVis, an interactivetool that enables fast, versatile, and large-scale conversation analysis.WildVis provides search and visualization capabilities in the text andembedding spaces based on a list of criteria. To manage million-scale datasets,we implemented optimizations including search index construction, embeddingprecomputation and compression, and caching to ensure responsive userinteractions within seconds. We demonstrate WildVis's utility through threecase studies: facilitating chatbot misuse research, visualizing and comparingtopic distributions across datasets, and characterizing user-specificconversation patterns. WildVis is open-source and designed to be extendable,supporting additional datasets and customized search and visualizationfunctionalities.</description><author>Yuntian Deng, Wenting Zhao, Jack Hessel, Xiang Ren, Claire Cardie, Yejin Choi</author><pubDate>Thu, 05 Sep 2024 17:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03753v1</guid></item><item><title>Attention Heads of Large Language Models: A Survey</title><link>http://arxiv.org/abs/2409.03752v1</link><description>Since the advent of ChatGPT, Large Language Models (LLMs) have excelled invarious tasks but remain largely as black-box systems. Consequently, theirdevelopment relies heavily on data-driven approaches, limiting performanceenhancement through changes in internal architecture and reasoning pathways. Asa result, many researchers have begun exploring the potential internalmechanisms of LLMs, aiming to identify the essence of their reasoningbottlenecks, with most studies focusing on attention heads. Our survey aims toshed light on the internal reasoning processes of LLMs by concentrating on theinterpretability and underlying mechanisms of attention heads. We first distillthe human thought process into a four-stage framework: Knowledge Recalling,In-Context Identification, Latent Reasoning, and Expression Preparation. Usingthis framework, we systematically review existing research to identify andcategorize the functions of specific attention heads. Furthermore, we summarizethe experimental methodologies used to discover these special heads, dividingthem into two categories: Modeling-Free methods and Modeling-Required methods.Also, we outline relevant evaluation methods and benchmarks. Finally, wediscuss the limitations of current research and propose several potentialfuture directions. Our reference list is open-sourced at\url{https://github.com/IAAR-Shanghai/Awesome-Attention-Heads}.</description><author>Zifan Zheng, Yezhaohui Wang, Yuxin Huang, Shichao Song, Bo Tang, Feiyu Xiong, Zhiyu Li</author><pubDate>Thu, 05 Sep 2024 17:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03752v1</guid></item><item><title>Dynamics of Supervised and Reinforcement Learning in the Non-Linear Perceptron</title><link>http://arxiv.org/abs/2409.03749v1</link><description>The ability of a brain or a neural network to efficiently learn dependscrucially on both the task structure and the learning rule. Previous works haveanalyzed the dynamical equations describing learning in the relativelysimplified context of the perceptron under assumptions of a student-teacherframework or a linearized output. While these assumptions have facilitatedtheoretical understanding, they have precluded a detailed understanding of theroles of the nonlinearity and input-data distribution in determining thelearning dynamics, limiting the applicability of the theories to realbiological or artificial neural networks. Here, we use a stochastic-processapproach to derive flow equations describing learning, applying this frameworkto the case of a nonlinear perceptron performing binary classification. Wecharacterize the effects of the learning rule (supervised or reinforcementlearning, SL/RL) and input-data distribution on the perceptron's learning curveand the forgetting curve as subsequent tasks are learned. In particular, wefind that the input-data noise differently affects the learning speed under SLvs. RL, as well as determines how quickly learning of a task is overwritten bysubsequent learning. Additionally, we verify our approach with real data usingthe MNIST dataset. This approach points a way toward analyzing learningdynamics for more-complex circuit architectures.</description><author>Christian Schmid, James M. Murray</author><pubDate>Thu, 05 Sep 2024 17:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03749v1</guid></item><item><title>ArtiFade: Learning to Generate High-quality Subject from Blemished Images</title><link>http://arxiv.org/abs/2409.03745v1</link><description>Subject-driven text-to-image generation has witnessed remarkable advancementsin its ability to learn and capture characteristics of a subject using only alimited number of images. However, existing methods commonly rely onhigh-quality images for training and may struggle to generate reasonable imageswhen the input images are blemished by artifacts. This is primarily attributedto the inadequate capability of current techniques in distinguishingsubject-related features from disruptive artifacts. In this paper, we introduceArtiFade to tackle this issue and successfully generate high-qualityartifact-free images from blemished datasets. Specifically, ArtiFade exploitsfine-tuning of a pre-trained text-to-image model, aiming to remove artifacts.The elimination of artifacts is achieved by utilizing a specialized datasetthat encompasses both unblemished images and their corresponding blemishedcounterparts during fine-tuning. ArtiFade also ensures the preservation of theoriginal generative capabilities inherent within the diffusion model, therebyenhancing the overall performance of subject-driven methods in generatinghigh-quality and artifact-free images. We further devise evaluation benchmarkstailored for this task. Through extensive qualitative and quantitativeexperiments, we demonstrate the generalizability of ArtiFade in effectiveartifact removal under both in-distribution and out-of-distribution scenarios.</description><author>Shuya Yang, Shaozhe Hao, Yukang Cao, Kwan-Yee K. Wong</author><pubDate>Thu, 05 Sep 2024 17:57:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03745v1</guid></item><item><title>Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?</title><link>http://arxiv.org/abs/2409.03741v1</link><description>Machine learning has revolutionized numerous domains, playing a crucial rolein driving advancements and enabling data-centric processes. The significanceof data in training models and shaping their performance cannot be overstated.Recent research has highlighted the heterogeneous impact of individual datasamples, particularly the presence of valuable data that significantlycontributes to the utility and effectiveness of machine learning models.However, a critical question remains unanswered: are these valuable datasamples more vulnerable to machine learning attacks? In this work, weinvestigate the relationship between data importance and machine learningattacks by analyzing five distinct attack types. Our findings reveal notableinsights. For example, we observe that high importance data samples exhibitincreased vulnerability in certain attacks, such as membership inference andmodel stealing. By analyzing the linkage between membership inferencevulnerability and data importance, we demonstrate that sample characteristicscan be integrated into membership metrics by introducing sample-specificcriteria, therefore enhancing the membership inference performance. Thesefindings emphasize the urgent need for innovative defense mechanisms thatstrike a balance between maximizing utility and safeguarding valuable dataagainst potential exploitation.</description><author>Rui Wen, Michael Backes, Yang Zhang</author><pubDate>Thu, 05 Sep 2024 17:54:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03741v1</guid></item><item><title>Differentiable Discrete Event Simulation for Queuing Network Control</title><link>http://arxiv.org/abs/2409.03740v1</link><description>Queuing network control is essential for managing congestion injob-processing systems such as service systems, communication networks, andmanufacturing processes. Despite growing interest in applying reinforcementlearning (RL) techniques, queueing network control poses distinct challenges,including high stochasticity, large state and action spaces, and lack ofstability. To tackle these challenges, we propose a scalable framework forpolicy optimization based on differentiable discrete event simulation. Our maininsight is that by implementing a well-designed smoothing technique fordiscrete event dynamics, we can compute pathwise policy gradients forlarge-scale queueing networks using auto-differentiation software (e.g.,Tensorflow, PyTorch) and GPU parallelization. Through extensive empiricalexperiments, we observe that our policy gradient estimators are several ordersof magnitude more accurate than typical REINFORCE-based estimators. Inaddition, We propose a new policy architecture, which drastically improvesstability while maintaining the flexibility of neural-network policies. In awide variety of scheduling and admission control tasks, we demonstrate thattraining control policies with pathwise gradients leads to a 50-1000ximprovement in sample efficiency over state-of-the-art RL methods. Unlike priortailored approaches to queueing, our methods can flexibly handle realisticscenarios, including systems operating in non-stationary environments and thosewith non-exponential interarrival/service times.</description><author>Ethan Che, Jing Dong, Hongseok Namkoong</author><pubDate>Thu, 05 Sep 2024 17:53:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03740v1</guid></item><item><title>LLM-CI: Assessing Contextual Integrity Norms in Language Models</title><link>http://arxiv.org/abs/2409.03735v1</link><description>Large language models (LLMs), while memorizing parts of their training datascraped from the Internet, may also inadvertently encode societal preferencesand norms. As these models are integrated into sociotechnical systems, it iscrucial that the norms they encode align with societal expectations. Thesenorms could vary across models, hyperparameters, optimization techniques, anddatasets. This is especially challenging due to prompt sensitivity$-$smallvariations in prompts yield different responses, rendering existing assessmentmethodologies unreliable. There is a need for a comprehensive frameworkcovering various models, optimization, and datasets, along with a reliablemethodology to assess encoded norms. We present LLM-CI, the first open-sourced framework to assess privacy normsencoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignettemethodology to assess the encoded norms across different contexts and LLMs. Wepropose the multi-prompt assessment methodology to address prompt sensitivityby assessing the norms from only the prompts that yield consistent responsesacross multiple variants. Using LLM-CI and our proposed methodology, wecomprehensively evaluate LLMs using IoT and COPPA vignettes datasets from priorwork, examining the impact of model properties (e.g., hyperparameters,capacity) and optimization strategies (e.g., alignment, quantization).</description><author>Yan Shvartzshnaider, Vasisht Duddu, John Lacalamita</author><pubDate>Thu, 05 Sep 2024 17:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03735v1</guid></item><item><title>PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity</title><link>http://arxiv.org/abs/2305.07893v3</link><description>One of the components of natural language processing that has received a lotof investigation recently is semantic textual similarity. In computationallinguistics and natural language processing, assessing the semantic similarityof words, phrases, paragraphs, and texts is crucial. Calculating the degree ofsemantic resemblance between two textual pieces, paragraphs, or phrasesprovided in both monolingual and cross-lingual versions is known as semanticsimilarity. Cross lingual semantic similarity requires corpora in which thereare sentence pairs in both the source and target languages with a degree ofsemantic similarity between them. Many existing cross lingual semanticsimilarity models use a machine translation due to the unavailability of crosslingual semantic similarity dataset, which the propagation of the machinetranslation error reduces the accuracy of the model. On the other hand, when wewant to use semantic similarity features for machine translation the samemachine translations should not be used for semantic similarity. For Persian,which is one of the low resource languages, no effort has been made in thisregard and the need for a model that can understand the context of twolanguages is felt more than ever. In this article, the corpus of semantictextual similarity between sentences in Persian and English languages has beenproduced for the first time by using linguistic experts. We named this datasetPESTS (Persian English Semantic Textual Similarity). This corpus contains 5375sentence pairs. Also, different models based on transformers have beenfine-tuned using this dataset. The results show that using the PESTS dataset,the Pearson correlation of the XLM ROBERTa model increases from 85.87% to95.62%.</description><author>Mohammad Abdous, Poorya Piroozfar, Behrouz Minaei Bidgoli</author><pubDate>Thu, 05 Sep 2024 17:45:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07893v3</guid></item><item><title>Safety vs. Performance: How Multi-Objective Learning Reduces Barriers to Market Entry</title><link>http://arxiv.org/abs/2409.03734v1</link><description>Emerging marketplaces for large language models and other large-scale machinelearning (ML) models appear to exhibit market concentration, which has raisedconcerns about whether there are insurmountable barriers to entry in suchmarkets. In this work, we study this issue from both an economic and analgorithmic point of view, focusing on a phenomenon that reduces barriers toentry. Specifically, an incumbent company risks reputational damage unless itsmodel is sufficiently aligned with safety objectives, whereas a new company canmore easily avoid reputational damage. To study this issue formally, we definea multi-objective high-dimensional regression framework that capturesreputational damage, and we characterize the number of data points that a newcompany needs to enter the market. Our results demonstrate how multi-objectiveconsiderations can fundamentally reduce barriers to entry -- the requirednumber of data points can be significantly smaller than the incumbent company'sdataset size. En route to proving these results, we develop scaling laws forhigh-dimensional linear regression in multi-objective environments, showingthat the scaling rate becomes slower when the dataset size is large, whichcould be of independent interest.</description><author>Meena Jagadeesan, Michael I. Jordan, Jacob Steinhardt</author><pubDate>Thu, 05 Sep 2024 17:45:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03734v1</guid></item><item><title>Planning In Natural Language Improves LLM Search For Code Generation</title><link>http://arxiv.org/abs/2409.03733v1</link><description>While scaling training compute has led to remarkable improvements in largelanguage models (LLMs), scaling inference compute has not yet yielded analogousgains. We hypothesize that a core missing component is a lack of diverse LLMoutputs, leading to inefficient search due to models repeatedly sampling highlysimilar, yet incorrect generations. We empirically demonstrate that this lackof diversity can be mitigated by searching over candidate plans for solving aproblem in natural language. Based on this insight, we propose PLANSEARCH, anovel search algorithm which shows strong results across HumanEval+, MBPP+, andLiveCodeBench (a contamination-free benchmark for competitive coding).PLANSEARCH generates a diverse set of observations about the problem and thenuses these observations to construct plans for solving the problem. Bysearching over plans in natural language rather than directly over codesolutions, PLANSEARCH explores a significantly more diverse range of potentialsolutions compared to baseline search methods. Using PLANSEARCH on top ofClaude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% onLiveCodeBench, outperforming both the best score achieved without search(pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%).Finally, we show that, across all models, search algorithms, and benchmarksanalyzed, we can accurately predict performance gains due to search as a directfunction of the diversity over generated ideas.</description><author>Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, Hugh Zhang</author><pubDate>Thu, 05 Sep 2024 17:44:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03733v1</guid></item><item><title>A Deep Generative Learning Approach for Two-stage Adaptive Robust Optimization</title><link>http://arxiv.org/abs/2409.03731v1</link><description>Two-stage adaptive robust optimization is a powerful approach for planningunder uncertainty that aims to balance costs of "here-and-now" first-stagedecisions with those of "wait-and-see" recourse decisions made afteruncertainty is realized. To embed robustness against uncertainty, modelerstypically assume a simple polyhedral or ellipsoidal set over whichcontingencies may be realized. However, these simple uncertainty sets tend toyield highly conservative decision-making when uncertainties arehigh-dimensional. In this work, we introduce AGRO, a column-and-constraintgeneration algorithm that performs adversarial generation for two-stageadaptive robust optimization using a variational autoencoder. AGRO identifiesrealistic and cost-maximizing contingencies by optimizing over sphericaluncertainty sets in a latent space using a projected gradient ascent approachthat differentiates the optimal recourse cost with respect to the latentvariable. To demonstrate the cost- and time-efficiency of our approachexperimentally, we apply AGRO to an adaptive robust capacity expansion problemfor a regional power system and show that AGRO is able to reduce costs by up to7.8% and runtimes by up to 77% in comparison to the conventionalcolumn-and-constraint generation algorithm.</description><author>Aron Brenner, Rahman Khorramfar, Jennifer Sun, Saurabh Amin</author><pubDate>Thu, 05 Sep 2024 17:42:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03731v1</guid></item><item><title>SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner</title><link>http://arxiv.org/abs/2406.05498v2</link><description>Jailbreaking is an emerging adversarial attack that bypasses the safetyalignment deployed in off-the-shelf large language models (LLMs) and hasevolved into multiple categories: human-based, optimization-based,generation-based, and the recent indirect and multilingual jailbreaks. However,delivering a practical jailbreak defense is challenging because it needs to notonly handle all the above jailbreak attacks but also incur negligible delays touser prompts, as well as be compatible with both open-source and closed-sourceLLMs. Inspired by how the traditional security concept of shadow stacks defendsagainst memory overflow attacks, this paper introduces a generic LLM jailbreakdefense framework called SelfDefend, which establishes a shadow LLM as adefense instance to concurrently protect the target LLM instance in the normalstack and collaborate with it for checkpoint-based access control. Theeffectiveness of SelfDefend builds upon our observation that existing LLMs(both target and defense LLMs) have the capability to identify harmful promptsor intentions in user queries, which we empirically validate using the commonlyused GPT-3.5/4 models across all major jailbreak attacks. To further improvethe defense's robustness and minimize costs, we employ a data distillationapproach to tune dedicated open-source defense models. These models outperformsix state-of-the-art defenses and match the performance of GPT-4-basedSelfDefend, with significantly lower extra delays. We also empirically showthat the tuned models are robust to adaptive jailbreaks and prompt injections.</description><author>Xunguang Wang, Daoyuan Wu, Zhenlan Ji, Zongjie Li, Pingchuan Ma, Shuai Wang, Yingjiu Li, Yang Liu, Ning Liu, Juergen Rahmel</author><pubDate>Thu, 05 Sep 2024 17:33:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05498v2</guid></item><item><title>Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation</title><link>http://arxiv.org/abs/2401.06477v3</link><description>In this paper, we introduce Kun, a novel approach for creating high-qualityinstruction-tuning datasets for large language models (LLMs) without relying onmanual annotations. Adapting a self-training algorithm based on instructionback-translation and answer polishment, Kun leverages unlabelled data fromdiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantialdataset of over a million Chinese instructional data points. This approachsignificantly deviates from traditional methods by using a self-curationprocess to refine and select the most effective instruction-output pairs. Ourexperiments with the 6B-parameter Yi model across various benchmarksdemonstrate Kun's robustness and scalability. Our method's core contributionslie in its algorithmic advancement, which enhances data retention and clarity,and its innovative data generation approach that substantially reduces thereliance on costly and time-consuming manual annotations. This methodologypresents a scalable and efficient solution for improving theinstruction-following capabilities of LLMs, with significant implications fortheir application across diverse fields. The code and dataset can be found athttps://github.com/Zheng0428/COIG-Kun</description><author>Tianyu Zheng, Shuyue Guo, Xingwei Qu, Jiawei Guo, Xinrun Du, Qi Jia, Chenghua Lin, Wenhao Huang, Jie Fu, Ge Zhang</author><pubDate>Thu, 05 Sep 2024 17:25:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06477v3</guid></item><item><title>Geometry Image Diffusion: Fast and Data-Efficient Text-to-3D with Image-Based Surface Representation</title><link>http://arxiv.org/abs/2409.03718v1</link><description>Generating high-quality 3D objects from textual descriptions remains achallenging problem due to computational cost, the scarcity of 3D data, andcomplex 3D representations. We introduce Geometry Image Diffusion(GIMDiffusion), a novel Text-to-3D model that utilizes geometry images toefficiently represent 3D shapes using 2D images, thereby avoiding the need forcomplex 3D-aware architectures. By integrating a Collaborative Controlmechanism, we exploit the rich 2D priors of existing Text-to-Image models suchas Stable Diffusion. This enables strong generalization even with limited 3Dtraining data (allowing us to use only high-quality training data) as well asretaining compatibility with guidance techniques such as IPAdapter. In short,GIMDiffusion enables the generation of 3D assets at speeds comparable tocurrent Text-to-Image models. The generated objects consist of semanticallymeaningful, separate parts and include internal structures, enhancing bothusability and versatility.</description><author>Slava Elizarov, Ciara Rowles, Simon Donné</author><pubDate>Thu, 05 Sep 2024 17:21:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03718v1</guid></item><item><title>RAG based Question-Answering for Contextual Response Prediction System</title><link>http://arxiv.org/abs/2409.03708v1</link><description>Large Language Models (LLMs) have shown versatility in various NaturalLanguage Processing (NLP) tasks, including their potential as effectivequestion-answering systems. However, to provide precise and relevantinformation in response to specific customer queries in industry settings, LLMsrequire access to a comprehensive knowledge base to avoid hallucinations.Retrieval Augmented Generation (RAG) emerges as a promising technique toaddress this challenge. Yet, developing an accurate question-answeringframework for real-world applications using RAG entails several challenges: 1)data availability issues, 2) evaluating the quality of generated content, and3) the costly nature of human evaluation. In this paper, we introduce anend-to-end framework that employs LLMs with RAG capabilities for industry usecases. Given a customer query, the proposed system retrieves relevant knowledgedocuments and leverages them, along with previous chat history, to generateresponse suggestions for customer service agents in the contact centers of amajor retail company. Through comprehensive automated and human evaluations, weshow that this solution outperforms the current BERT-based algorithms inaccuracy and relevance. Our findings suggest that RAG-based LLMs can be anexcellent support to human customer service representatives by lightening theirworkload.</description><author>Sriram Veturi, Saurabh Vaichal, Nafis Irtiza Tripto, Reshma Lal Jagadheesh, Nian Yan</author><pubDate>Thu, 05 Sep 2024 17:14:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03708v1</guid></item><item><title>A Different Level Text Protection Mechanism With Differential Privacy</title><link>http://arxiv.org/abs/2409.03707v1</link><description>The article introduces a method for extracting words of different degrees ofimportance based on the BERT pre-training model and proves the effectiveness ofthis method. The article also discusses the impact of maintaining the sameperturbation results for words of different importance on the overall textutility. This method can be applied to long text protection.</description><author>Qingwen Fu</author><pubDate>Thu, 05 Sep 2024 17:13:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03707v1</guid></item><item><title>How to Train your Antivirus: RL-based Hardening through the Problem-Space</title><link>http://arxiv.org/abs/2402.19027v2</link><description>ML-based malware detection on dynamic analysis reports is vulnerable to bothevasion and spurious correlations. In this work, we investigate a specific MLarchitecture employed in the pipeline of a widely-known commercial antiviruscompany, with the goal to harden it against adversarial malware. Adversarialtraining, the sole defensive technique that can confer empirical robustness, isnot applicable out of the box in this domain, for the principal reason thatgradient-based perturbations rarely map back to feasible problem-spaceprograms. We introduce a novel Reinforcement Learning approach for constructingadversarial examples, a constituent part of adversarially training a modelagainst evasion. Our approach comes with multiple advantages. It performsmodifications that are feasible in the problem-space, and only those; thus itcircumvents the inverse mapping problem. It also makes possible to providetheoretical guarantees on the robustness of the model against a particular setof adversarial capabilities. Our empirical exploration validates ourtheoretical insights, where we can consistently reach 0% Attack Success Rateafter a few adversarial retraining iterations.</description><author>Ilias Tsingenopoulos, Jacopo Cortellazzi, Branislav Bošanský, Simone Aonzo, Davy Preuveneers, Wouter Joosen, Fabio Pierazzi, Lorenzo Cavallaro</author><pubDate>Thu, 05 Sep 2024 17:07:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19027v2</guid></item><item><title>A Graph-based Adversarial Imitation Learning Framework for Reliable &amp; Realtime Fleet Scheduling in Urban Air Mobility</title><link>http://arxiv.org/abs/2407.12113v2</link><description>The advent of Urban Air Mobility (UAM) presents the scope for atransformative shift in the domain of urban transportation. However, itswidespread adoption and economic viability depends in part on the ability tooptimally schedule the fleet of aircraft across vertiports in a UAM network,under uncertainties attributed to airspace congestion, changing weatherconditions, and varying demands. This paper presents a comprehensiveoptimization formulation of the fleet scheduling problem, while alsoidentifying the need for alternate solution approaches, since directly solvingthe resulting integer nonlinear programming problem is computationallyprohibitive for daily fleet scheduling. Previous work has shown theeffectiveness of using (graph) reinforcement learning (RL) approaches to trainreal-time executable policy models for fleet scheduling. However, such policiescan often be brittle on out-of-distribution scenarios or edge cases. Moreover,training performance also deteriorates as the complexity (e.g., number ofconstraints) of the problem increases. To address these issues, this paperpresents an imitation learning approach where the RL-based policy exploitsexpert demonstrations yielded by solving the exact optimization using a GeneticAlgorithm. The policy model comprises Graph Neural Network (GNN) based encodersthat embed the space of vertiports and aircraft, Transformer networks to encodedemand, passenger fare, and transport cost profiles, and a Multi-head attention(MHA) based decoder. Expert demonstrations are used through the GenerativeAdversarial Imitation Learning (GAIL) algorithm. Interfaced with a UAMsimulation environment involving 8 vertiports and 40 aircrafts, in terms of thedaily profits earned reward, the new imitative approach achieves better meanperformance and remarkable improvement in the case of unseen worst-casescenarios, compared to pure RL results.</description><author>Prithvi Poddar, Steve Paul, Souma Chowdhury</author><pubDate>Thu, 05 Sep 2024 17:01:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12113v2</guid></item><item><title>Iterative thresholding for non-linear learning in the strong $\varepsilon$-contamination model</title><link>http://arxiv.org/abs/2409.03703v1</link><description>We derive approximation bounds for learning single neuron models usingthresholded gradient descent when both the labels and the covariates arepossibly corrupted adversarially. We assume the data follows the model $y =\sigma(\mathbf{w}^{*} \cdot \mathbf{x}) + \xi,$ where $\sigma$ is a nonlinearactivation function, the noise $\xi$ is Gaussian, and the covariate vector$\mathbf{x}$ is sampled from a sub-Gaussian distribution. We study sigmoidal,leaky-ReLU, and ReLU activation functions and derive a$O(\nu\sqrt{\epsilon\log(1/\epsilon)})$ approximation bound in $\ell_{2}$-norm,with sample complexity $O(d/\epsilon)$ and failure probability$e^{-\Omega(d)}$. We also study the linear regression problem, where $\sigma(\mathbf{x}) =\mathbf{x}$. We derive a $O(\nu\epsilon\log(1/\epsilon))$ approximation bound,improving upon the previous $O(\nu)$ approximation bounds for thegradient-descent based iterative thresholding algorithms of Bhatia et al.(NeurIPS 2015) and Shen and Sanghavi (ICML 2019). Our algorithm has a$O(\textrm{polylog}(N,d)\log(R/\epsilon))$ runtime complexity when$\|\mathbf{w}^{*}\|_2 \leq R$, improving upon the$O(\text{polylog}(N,d)/\epsilon^2)$ runtime complexity of Awasthi et al.(NeurIPS 2022).</description><author>Arvind Rathnashyam, Alex Gittens</author><pubDate>Thu, 05 Sep 2024 16:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03703v1</guid></item><item><title>Physics-Informed Machine Learning Towards A Real-Time Spacecraft Thermal Simulator</title><link>http://arxiv.org/abs/2407.06099v2</link><description>Modeling thermal states for complex space missions, such as the surfaceexploration of airless bodies, requires high computation, whether used inground-based analysis for spacecraft design or during onboard reasoning forautonomous operations. For example, a finite-element thermal model withhundreds of elements can take significant time to simulate, which makes itunsuitable for onboard reasoning during time-sensitive scenarios such asdescent and landing, proximity operations, or in-space assembly. Further, thelack of fast and accurate thermal modeling drives thermal designs to be moreconservative and leads to spacecraft with larger mass and higher power budgets.The emerging paradigm of physics-informed machine learning (PIML) presents aclass of hybrid modeling architectures that address this challenge by combiningsimplified physics models with machine learning (ML) models resulting in modelswhich maintain both interpretability and robustness. Such techniques enabledesigns with reduced mass and power through onboard thermal-state estimationand control and may lead to improved onboard handling of off-nominal states,including unplanned down-time. The PIML model or hybrid model presented hereconsists of a neural network which predicts reduced nodalizations (distributionand size of coarse mesh) given on-orbit thermal load conditions, andsubsequently a (relatively coarse) finite-difference model operates on thismesh to predict thermal states. We compare the computational performance andaccuracy of the hybrid model to a data-driven neural net model, and ahigh-fidelity finite-difference model of a prototype Earth-orbiting smallspacecraft. The PIML based active nodalization approach provides significantlybetter generalization than the neural net model and coarse mesh model, whilereducing computing cost by up to 1.7x compared to the high-fidelity model.</description><author>Manaswin Oddiraju, Zaki Hasnain, Saptarshi Bandyopadhyay, Eric Sunada, Souma Chowdhury</author><pubDate>Thu, 05 Sep 2024 16:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06099v2</guid></item><item><title>LAST: Language Model Aware Speech Tokenization</title><link>http://arxiv.org/abs/2409.03701v1</link><description>Speech tokenization serves as the foundation of speech language model (LM),enabling them to perform various tasks such as spoken language modeling,text-to-speech, speech-to-text, etc. Most speech tokenizers are trainedindependently of the LM training process, relying on separate acoustic modelsand quantization methods. Following such an approach may create a mismatchbetween the tokenization process and its usage afterward. In this study, wepropose a novel approach to training a speech tokenizer by leveragingobjectives from pre-trained textual LMs. We advocate for the integration ofthis objective into the process of learning discrete speech representations.Our aim is to transform features from a pre-trained speech model into a newfeature space that enables better clustering for speech LMs. We empiricallyinvestigate the impact of various model design choices, including speechvocabulary size and text LM size. Our results demonstrate the proposedtokenization method outperforms the evaluated baselines considering both spokenlanguage modeling and speech-to-text. More importantly, unlike prior work, theproposed method allows the utilization of a single pre-trained LM forprocessing both speech and text inputs, setting it apart from conventionaltokenization approaches.</description><author>Arnon Turetzky, Yossi Adi</author><pubDate>Thu, 05 Sep 2024 16:57:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03701v1</guid></item><item><title>Decision Theoretic Foundations for Experiments Evaluating Human Decisions</title><link>http://arxiv.org/abs/2401.15106v3</link><description>DeHow well people use information displays to make decisions is of primaryinterest in human-centered AI, model explainability, data visualization, andrelated areas. However, what constitutes a decision problem, and what isrequired for a study to establish that human decisions could be improved remainopen to speculation. We propose a widely applicable definition of a decisionproblem synthesized from statistical decision theory and information economicsas a standard for establishing when human decisions can be improved in HCI. Weargue that to attribute loss in human performance to forms of bias, anexperiment must provide participants with the information that a rational agentwould need to identify the utility-maximizing decision. As a demonstration, weevaluate the extent to which recent evaluations of decision-making from theliterature on AI-assisted decisions achieve these criteria. We find that only10 (26\%) of 39 studies that claim to identify biased behavior presentparticipants with sufficient information to characterize their behavior asdeviating from good decision-making in at least one treatment condition. Wemotivate the value of studying well-defined decision problems by describing acharacterization of performance losses they allow us to conceive. In contrast,the ambiguities of a poorly communicated decision problem preclude normativeinterpretation. We conclude with recommendations for practice.</description><author>Jessica Hullman, Alex Kale, Jason Hartline</author><pubDate>Thu, 05 Sep 2024 16:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15106v3</guid></item><item><title>Classification and Prediction of Heart Diseases using Machine Learning Algorithms</title><link>http://arxiv.org/abs/2409.03697v1</link><description>Heart disease is a serious worldwide health issue because it claims the livesof many people who might have been treated if the disease had been identifiedearlier. The leading cause of death in the world is cardiovascular disease,usually referred to as heart disease. Creating reliable, effective, and precisepredictions for these diseases is one of the biggest issues facing the medicalworld today. Although there are tools for predicting heart diseases, they areeither expensive or challenging to apply for determining a patient's risk. Thebest classifier for foretelling and spotting heart disease was the aim of thisresearch. This experiment examined a range of machine learning approaches,including Logistic Regression, K-Nearest Neighbor, Support Vector Machine, andArtificial Neural Networks, to determine which machine learning algorithm wasmost effective at predicting heart diseases. One of the most often utilizeddata sets for this purpose, the UCI heart disease repository provided the dataset for this study. The K-Nearest Neighbor technique was shown to be the mosteffective machine learning algorithm for determining whether a patient hasheart disease. It will be beneficial to conduct further studies on theapplication of additional machine learning algorithms for heart diseaseprediction.</description><author>Akua Sekyiwaa Osei-Nkwantabisa, Redeemer Ntumy</author><pubDate>Thu, 05 Sep 2024 16:52:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03697v1</guid></item><item><title>Deep Neural Implicit Representation of Accessibility for Multi-Axis Manufacturing</title><link>http://arxiv.org/abs/2409.02115v2</link><description>One of the main concerns in design and process planning for multi-axisadditive and subtractive manufacturing is collision avoidance between movingobjects (e.g., tool assemblies) and stationary objects (e.g., a part unifiedwith fixtures). The collision measure for various pairs of relative rigidtranslations and rotations between the two pointsets can be conceptualized by acompactly supported scalar field over the 6D non-Euclidean configuration space.Explicit representation and computation of this field is costly in both timeand space. If we fix $O(m)$ sparsely sampled rotations (e.g., toolorientations), computation of the collision measure field as a convolution ofindicator functions of the 3D pointsets over a uniform grid (i.e., voxelizedgeometry) of resolution $O(n^3)$ via fast Fourier transforms (FFTs) scales asin $O(mn^3 \log n)$ in time and $O(mn^3)$ in space. In this paper, we developan implicit representation of the collision measure field via deep neuralnetworks (DNNs). We show that our approach is able to accurately interpolatethe collision measure from a sparse sampling of rotations, and can representthe collision measure field with a small memory footprint. Moreover, we showthat this representation can be efficiently updated through fine-tuning to moreefficiently train the network on multi-resolution data, as well as accommodateincremental changes to the geometry (such as might occur in iterative processessuch as topology optimization of the part subject to CNC tool accessibilityconstraints).</description><author>George P. Harabin, Amir Mirzendehdel, Morad Behandish</author><pubDate>Thu, 05 Sep 2024 16:47:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02115v2</guid></item><item><title>Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?</title><link>http://arxiv.org/abs/2407.16607v3</link><description>The pretraining data of today's strongest language models is opaque; inparticular, little is known about the proportions of various domains orlanguages represented. In this work, we tackle a task which we call datamixture inference, which aims to uncover the distributional make-up of trainingdata. We introduce a novel attack based on a previously overlooked source ofinformation: byte-pair encoding (BPE) tokenizers, used by the vast majority ofmodern language models. Our key insight is that the ordered list of merge ruleslearned by a BPE tokenizer naturally reveals information about the tokenfrequencies in its training data. Given a tokenizer's merge list along withexample data for each category of interest, we formulate a linear program thatsolves for the proportion of each category in the tokenizer's training set. Incontrolled experiments, we show that our attack recovers mixture ratios withhigh precision for tokenizers trained on known mixtures of natural languages,programming languages, and data sources. We then apply our approach tooff-the-shelf tokenizers released with recent LMs. We confirm much publiclydisclosed information about these models, and also make several new inferences:GPT-4o and Mistral NeMo's tokenizers are much more multilingual than theirpredecessors, training on 39% and 47% non-English language data, respectively;Llama 3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use;GPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%). Wehope our work sheds light on current design practices for pretraining data, andinspires continued research into data mixture inference for LMs.</description><author>Jonathan Hayase, Alisa Liu, Yejin Choi, Sewoong Oh, Noah A. Smith</author><pubDate>Thu, 05 Sep 2024 16:39:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16607v3</guid></item><item><title>View-Invariant Policy Learning via Zero-Shot Novel View Synthesis</title><link>http://arxiv.org/abs/2409.03685v1</link><description>Large-scale visuomotor policy learning is a promising approach towarddeveloping generalizable manipulation systems. Yet, policies that can bedeployed on diverse embodiments, environments, and observational modalitiesremain elusive. In this work, we investigate how knowledge from large-scalevisual data of the world may be used to address one axis of variation forgeneralizable manipulation: observational viewpoint. Specifically, we studysingle-image novel view synthesis models, which learn 3D-aware scene-levelpriors by rendering images of the same scene from alternate camera viewpointsgiven a single input image. For practical application to diverse robotic data,these models must operate zero-shot, performing view synthesis on unseen tasksand environments. We empirically analyze view synthesis models within a simpledata-augmentation scheme that we call View Synthesis Augmentation (VISTA) tounderstand their capabilities for learning viewpoint-invariant policies fromsingle-viewpoint demonstration data. Upon evaluating the robustness of policiestrained with our method to out-of-distribution camera viewpoints, we find thatthey outperform baselines in both simulated and real-world manipulation tasks.Videos and additional visualizations are available athttps://s-tian.github.io/projects/vista.</description><author>Stephen Tian, Blake Wulfe, Kyle Sargent, Katherine Liu, Sergey Zakharov, Vitor Guizilini, Jiajun Wu</author><pubDate>Thu, 05 Sep 2024 16:39:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03685v1</guid></item><item><title>Predicting quantum channels over general product distributions</title><link>http://arxiv.org/abs/2409.03684v1</link><description>We investigate the problem of predicting the output behavior of unknownquantum channels. Given query access to an $n$-qubit channel $E$ and anobservable $O$, we aim to learn the mapping \begin{equation*} \rho \mapsto \mathrm{Tr}(O E[\rho]) \end{equation*} to within a small errorfor most $\rho$ sampled from a distribution $D$. Previously, Huang, Chen, andPreskill proved a surprising result that even if $E$ is arbitrary, this taskcan be solved in time roughly $n^{O(\log(1/\epsilon))}$, where $\epsilon$ isthe target prediction error. However, their guarantee applied only to inputdistributions $D$ invariant under all single-qubit Clifford gates, and theiralgorithm fails for important cases such as general product distributions overproduct states $\rho$. In this work, we propose a new approach that achieves accurate predictionover essentially any product distribution $D$, provided it is not "classical"in which case there is a trivial exponential lower bound. Our method employs a"biased Pauli analysis," analogous to classical biased Fourier analysis.Implementing this approach requires overcoming several challenges unique to thequantum setting, including the lack of a basis with appropriate orthogonalityproperties. The techniques we develop to address these issues may have broaderapplications in quantum information.</description><author>Sitan Chen, Jaume de Dios Pont, Jun-Ting Hsieh, Hsin-Yuan Huang, Jane Lange, Jerry Li</author><pubDate>Thu, 05 Sep 2024 16:39:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03684v1</guid></item><item><title>A New First-Order Meta-Learning Algorithm with Convergence Guarantees</title><link>http://arxiv.org/abs/2409.03682v1</link><description>Learning new tasks by drawing on prior experience gathered from other(related) tasks is a core property of any intelligent system. Gradient-basedmeta-learning, especially MAML and its variants, has emerged as a viablesolution to accomplish this goal. One problem MAML encounters is itscomputational and memory burdens needed to compute the meta-gradients. Wepropose a new first-order variant of MAML that we prove converges to astationary point of the MAML objective, unlike other first-order variants. Wealso show that the MAML objective does not satisfy the smoothness assumptionassumed in previous works; we show instead that its smoothness constant growswith the norm of the meta-gradient, which theoretically suggests the use ofnormalized or clipped-gradient methods compared to the plain gradient methodused in previous works. We validate our theory on a synthetic experiment.</description><author>El Mahdi Chayti, Martin Jaggi</author><pubDate>Thu, 05 Sep 2024 16:37:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03682v1</guid></item><item><title>Evaluations of Machine Learning Privacy Defenses are Misleading</title><link>http://arxiv.org/abs/2404.17399v2</link><description>Empirical defenses for machine learning privacy forgo the provable guaranteesof differential privacy in the hope of achieving higher utility while resistingrealistic adversaries. We identify severe pitfalls in existing empiricalprivacy evaluations (based on membership inference attacks) that result inmisleading conclusions. In particular, we show that prior evaluations fail tocharacterize the privacy leakage of the most vulnerable samples, use weakattacks, and avoid comparisons with practical differential privacy baselines.In 5 case studies of empirical privacy defenses, we find that prior evaluationsunderestimate privacy leakage by an order of magnitude. Under our strongerevaluation, none of the empirical defenses we study are competitive with aproperly tuned, high-utility DP-SGD baseline (with vacuous provableguarantees).</description><author>Michael Aerni, Jie Zhang, Florian Tramèr</author><pubDate>Thu, 05 Sep 2024 16:34:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17399v2</guid></item><item><title>Practical Forecasting of Cryptocoins Timeseries using Correlation Patterns</title><link>http://arxiv.org/abs/2409.03674v1</link><description>Cryptocoins (i.e., Bitcoin, Ether, Litecoin) are tradable digital assets.Ownerships of cryptocoins are registered on distributed ledgers (i.e.,blockchains). Secure encryption techniques guarantee the security of thetransactions (transfers of coins among owners), registered into the ledger.Cryptocoins are exchanged for specific trading prices. The extreme volatilityof such trading prices across all different sets of crypto-assets remainsundisputed. However, the relations between the trading prices across differentcryptocoins remains largely unexplored. Major coin exchanges indicate trendcorrelation to advise for sells or buys. However, price correlations remainlargely unexplored. We shed some light on the trend correlations across a largevariety of cryptocoins, by investigating their coin/price correlation trendsover the past two years. We study the causality between the trends, and exploitthe derived correlations to understand the accuracy of state-of-the-artforecasting techniques for time series modeling (e.g., GBMs, LSTM and GRU) ofcorrelated cryptocoins. Our evaluation shows (i) strong correlation patternsbetween the most traded coins (e.g., Bitcoin and Ether) and other types ofcryptocurrencies, and (ii) state-of-the-art time series forecasting algorithmscan be used to forecast cryptocoins price trends. We released datasets and codeto reproduce our analysis to the research community.</description><author>Pasquale De Rosa, Pascal Felber, Valerio Schiavoni</author><pubDate>Thu, 05 Sep 2024 16:27:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03674v1</guid></item><item><title>Wind turbine condition monitoring based on intra- and inter-farm federated learning</title><link>http://arxiv.org/abs/2409.03672v1</link><description>As wind energy adoption is growing, ensuring the efficient operation andmaintenance of wind turbines becomes essential for maximizing energy productionand minimizing costs and downtime. Many AI applications in wind energy, such asin condition monitoring and power forecasting, may benefit from usingoperational data not only from individual wind turbines but from multipleturbines and multiple wind farms. Collaborative distributed AI which preservesdata privacy holds a strong potential for these applications. Federatedlearning has emerged as a privacy-preserving distributed machine learningapproach in this context. We explore federated learning in wind turbinecondition monitoring, specifically for fault detection using normal behaviourmodels. We investigate various federated learning strategies, includingcollaboration across different wind farms and turbine models, as well ascollaboration restricted to the same wind farm and turbine model. Our casestudy results indicate that federated learning across multiple wind turbinesconsistently outperforms models trained on a single turbine, especially whentraining data is scarce. Moreover, the amount of historical data necessary totrain an effective model can be significantly reduced by employing acollaborative federated learning strategy. Finally, our findings show thatextending the collaboration to multiple wind farms may result in inferiorperformance compared to restricting learning within a farm, specifically whenfaced with statistical heterogeneity and imbalanced datasets.</description><author>Albin Grataloup, Stefan Jonas, Angela Meyer</author><pubDate>Thu, 05 Sep 2024 16:25:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03672v1</guid></item><item><title>TRACE-cs: Trustworthy Reasoning for Contrastive Explanations in Course Scheduling Problems</title><link>http://arxiv.org/abs/2409.03671v1</link><description>We present TRACE-cs, a novel hybrid system that combines symbolic reasoningwith large language models (LLMs) to address contrastive queries in schedulingproblems. TRACE-cs leverages SAT solving techniques to encode schedulingconstraints and generate explanations for user queries, while utilizing an LLMto process the user queries into logical clauses as well as refine theexplanations generated by the symbolic solver to natural language sentences. Byintegrating these components, our approach demonstrates the potential ofcombining symbolic methods with LLMs to create explainable AI agents withcorrectness guarantees.</description><author>Stylianos Loukas Vasileiou, William Yeoh</author><pubDate>Thu, 05 Sep 2024 16:24:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03671v1</guid></item><item><title>A method to benchmark high-dimensional process drift detection</title><link>http://arxiv.org/abs/2409.03669v1</link><description>Process curves are multi-variate finite time series data coming frommanufacturing processes. This paper studies machine learning methods for driftsof process curves. A theoretic framework to synthetically generate processcurves in a controlled way is introduced in order to benchmark machine learningalgorithms for process drift detection. A evaluation score, called the temporalarea under the curve, is introduced, which allows to quantify how well machinelearning models unveil curves belonging to drift segments. Finally, a benchmarkstudy comparing popular machine learning approaches on synthetic data generatedwith the introduced framework shown.</description><author>Edgar Wolf, Tobias Windisch</author><pubDate>Thu, 05 Sep 2024 16:23:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03669v1</guid></item><item><title>A Fused Large Language Model for Predicting Startup Success</title><link>http://arxiv.org/abs/2409.03668v1</link><description>Investors are continuously seeking profitable investment opportunities instartups and, hence, for effective decision-making, need to predict a startup'sprobability of success. Nowadays, investors can use not only variousfundamental information about a startup (e.g., the age of the startup, thenumber of founders, and the business sector) but also textual description of astartup's innovation and business model, which is widely available throughonline venture capital (VC) platforms such as Crunchbase. To support thedecision-making of investors, we develop a machine learning approach with theaim of locating successful startups on VC platforms. Specifically, we develop,train, and evaluate a tailored, fused large language model to predict startupsuccess. Thereby, we assess to what extent self-descriptions on VC platformsare predictive of startup success. Using 20,172 online profiles fromCrunchbase, we find that our fused large language model can predict startupsuccess, with textual self-descriptions being responsible for a significantpart of the predictive power. Our work provides a decision support tool forinvestors to find profitable investment opportunities.</description><author>Abdurahman Maarouf, Stefan Feuerriegel, Nicolas Pröllochs</author><pubDate>Thu, 05 Sep 2024 16:22:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03668v1</guid></item><item><title>Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation</title><link>http://arxiv.org/abs/2402.14101v2</link><description>In subjective NLP tasks, where a single ground truth does not exist, theinclusion of diverse annotators becomes crucial as their unique perspectivessignificantly influence the annotations. In realistic scenarios, the annotationbudget often becomes the main determinant of the number of perspectives (i.e.,annotators) included in the data and subsequent modeling. We introduce a novelframework for annotation collection and modeling in subjective tasks that aimsto minimize the annotation budget while maximizing the predictive performancefor each annotator. Our framework has a two-stage design: first, we rely on asmall set of annotators to build a multitask model, and second, we augment themodel for a new perspective by strategically annotating a few samples perannotator. To test our framework at scale, we introduce and release a uniquedataset, Moral Foundations Subjective Corpus, of 2000 Reddit posts annotated by24 annotators for moral sentiment. We demonstrate that our framework surpassesthe previous SOTA in capturing the annotators' individual perspectives with aslittle as 25% of the original annotation budget on two datasets. Furthermore,our framework results in more equitable models, reducing the performancedisparity among annotators.</description><author>Preni Golazizian, Alireza S. Ziabari, Ali Omrani, Morteza Dehghani</author><pubDate>Thu, 05 Sep 2024 16:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14101v2</guid></item><item><title>Threat Classification on Deployed Optical Networks Using MIMO Digital Fiber Sensing, Wavelets, and Machine Learning</title><link>http://arxiv.org/abs/2409.03667v1</link><description>We demonstrate mechanical threats classification including jackhammers andexcavators, leveraging wavelet transform of MIMO-DFS output data across a 57-kmoperational network link. Our machine learning framework incorporates transferlearning and shows 93% classification accuracy from field data, with benefitsfor optical network supervision.</description><author>Khouloud Abdelli, Henrique Pavani, Christian Dorize, Sterenn Guerrier, Haik Mardoyan, Patricia Layec, Jeremie Renaudier</author><pubDate>Thu, 05 Sep 2024 16:21:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03667v1</guid></item><item><title>Exploring Group and Symmetry Principles in Large Language Models</title><link>http://arxiv.org/abs/2402.06120v3</link><description>Large Language Models (LLMs) have demonstrated impressive performance acrossa wide range of applications; however, assessing their reasoning capabilitiesremains a significant challenge. In this paper, we introduce a frameworkgrounded in group and symmetry principles, which have played a crucial role infields such as physics and mathematics, and offer another way to evaluate theircapabilities. While the proposed framework is general, to showcase the benefitsof employing these properties, we focus on arithmetic reasoning and investigatethe performance of these models on four group properties: closure, identity,inverse, and associativity. Our findings reveal that LLMs studied in this workstruggle to preserve group properties across different test regimes. In theclosure test, we observe biases towards specific outputs and an abruptdegradation in their performance from 100% to 0% after a specific sequencelength. They also perform poorly in the identity test, which represents addingirrelevant information in the context, and show sensitivity when subjected toinverse test, which examines the robustness of the model with respect tonegation. In addition, we demonstrate that breaking down problems into smallersteps helps LLMs in the associativity test that we have conducted. To supportthese tests we have developed a synthetic dataset which will be released.</description><author>Shima Imani, Hamid Palangi</author><pubDate>Thu, 05 Sep 2024 16:19:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06120v3</guid></item><item><title>Positioning Political Texts with Large Language Models by Asking and Averaging</title><link>http://arxiv.org/abs/2311.16639v3</link><description>We use instruction-tuned Large Language Models (LLMs) like GPT-4, Llama 3,MiXtral, or Aya to position political texts within policy and ideologicalspaces. We ask an LLM where a tweet or a sentence of a political text stands onthe focal dimension and take the average of the LLM responses to positionpolitical actors such as US Senators, or longer texts such as UK partymanifestos or EU policy speeches given in 10 different languages. Thecorrelations between the position estimates obtained with the best LLMs andbenchmarks based on text coding by experts, crowdworkers, or roll call votesexceed .90. This approach is generally more accurate than the positionsobtained with supervised classifiers trained on large amounts of research data.Using instruction-tuned LLMs to position texts in policy and ideological spacesis fast, cost-efficient, reliable, and reproducible (in the case of open LLMs)even if the texts are short and written in different languages. We concludewith cautionary notes about the need for empirical validation.</description><author>Gaël Le Mens, Aina Gallego</author><pubDate>Thu, 05 Sep 2024 16:17:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16639v3</guid></item><item><title>Weather-Adaptive Multi-Step Forecasting of State of Polarization Changes in Aerial Fibers Using Wavelet Neural Networks</title><link>http://arxiv.org/abs/2409.03663v1</link><description>We introduce a novel weather-adaptive approach for multi-step forecasting ofmulti-scale SOP changes in aerial fiber links. By harnessing the discretewavelet transform and incorporating weather data, our approach improvesforecasting accuracy by over 65% in RMSE and 63% in MAPE compared to baselines.</description><author>Khouloud Abdelli, Matteo Lonardi, Jurgen Gripp, Samuel Olsson Fabien Boitier, Patricia Layec</author><pubDate>Thu, 05 Sep 2024 16:15:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03663v1</guid></item><item><title>Serialized Speech Information Guidance with Overlapped Encoding Separation for Multi-Speaker Automatic Speech Recognition</title><link>http://arxiv.org/abs/2409.00815v2</link><description>Serialized output training (SOT) attracts increasing attention due to itsconvenience and flexibility for multi-speaker automatic speech recognition(ASR). However, it is not easy to train with attention loss only. In thispaper, we propose the overlapped encoding separation (EncSep) to fully utilizethe benefits of the connectionist temporal classification (CTC) and attentionhybrid loss. This additional separator is inserted after the encoder to extractthe multi-speaker information with CTC losses. Furthermore, we propose theserialized speech information guidance SOT (GEncSep) to further utilize theseparated encodings. The separated streams are concatenated to providesingle-speaker information to guide attention during decoding. The experimentalresults on LibriMix show that the single-speaker encoding can be separated fromthe overlapped encoding. The CTC loss helps to improve the encoderrepresentation under complex scenarios. GEncSep further improved performance.</description><author>Hao Shi, Yuan Gao, Zhaoheng Ni, Tatsuya Kawahara</author><pubDate>Thu, 05 Sep 2024 16:15:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00815v2</guid></item><item><title>The representation landscape of few-shot learning and fine-tuning in large language models</title><link>http://arxiv.org/abs/2409.03662v1</link><description>In-context learning (ICL) and supervised fine-tuning (SFT) are two commonstrategies for improving the performance of modern large language models (LLMs)on specific tasks. Despite their different natures, these strategies often leadto comparable performance gains. However, little is known about whether theyinduce similar representations inside LLMs. We approach this problem byanalyzing the probability landscape of their hidden representations in the twocases. More specifically, we compare how LLMs solve the same question-answeringtask, finding that ICL and SFT create very different internal structures, inboth cases undergoing a sharp transition in the middle of the network. In thefirst half of the network, ICL shapes interpretable representationshierarchically organized according to their semantic content. In contrast, theprobability landscape obtained with SFT is fuzzier and semantically mixed. Inthe second half of the model, the fine-tuned representations developprobability modes that better encode the identity of answers, while thelandscape of ICL representations is characterized by less defined peaks. Ourapproach reveals the diverse computational strategies developed inside LLMs tosolve the same task across different conditions, allowing us to make a steptowards designing optimal methods to extract information from language models.</description><author>Diego Doimo, Alessandro Serra, Alessio Ansuini, Alberto Cazzaniga</author><pubDate>Thu, 05 Sep 2024 16:15:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03662v1</guid></item><item><title>Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions</title><link>http://arxiv.org/abs/2408.04168v2</link><description>This paper considers a scenario in city navigation: an AI agent is providedwith language descriptions of the goal location with respect to some well-knownlandmarks; By only observing the scene around, including recognizing landmarksand road network connections, the agent has to make decisions to navigate tothe goal location without instructions. This problem is very challenging,because it requires agent to establish self-position and acquire spatialrepresentation of complex urban environment, where landmarks are ofteninvisible. In the absence of navigation instructions, such abilities are vitalfor the agent to make high-quality decisions in long-range city navigation.With the emergent reasoning ability of large language models (LLMs), a temptingbaseline is to prompt LLMs to "react" on each observation and make decisionsaccordingly. However, this baseline has very poor performance that the agentoften repeatedly visits same locations and make short-sighted, inconsistentdecisions. To address these issues, this paper introduces a novel agenticworkflow featured by its abilities to perceive, reflect and plan. Specifically,we find LLaVA-7B can be fine-tuned to perceive the direction and distance oflandmarks with sufficient accuracy for city navigation. Moreover, reflection isachieved through a memory mechanism, where past experiences are stored and canbe retrieved with current perception for effective decision argumentation.Planning uses reflection results to produce long-term plans, which can avoidshort-sighted decisions in long-range navigation. We show the designed workflowsignificantly improves navigation ability of the LLM agent compared with thestate-of-the-art baselines.</description><author>Qingbin Zeng, Qinglong Yang, Shunan Dong, Heming Du, Liang Zheng, Fengli Xu, Yong Li</author><pubDate>Thu, 05 Sep 2024 16:14:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04168v2</guid></item><item><title>LLM-based multi-agent poetry generation in non-cooperative environments</title><link>http://arxiv.org/abs/2409.03659v1</link><description>Despite substantial progress of large language models (LLMs) for automaticpoetry generation, the generated poetry lacks diversity while the trainingprocess differs greatly from human learning. Under the rationale that thelearning process of the poetry generation systems should be more human-like andtheir output more diverse and novel, we introduce a framework based on sociallearning where we emphasize non-cooperative interactions besides cooperativeinteractions to encourage diversity. Our experiments are the first attempt atLLM-based multi-agent systems in non-cooperative environments for poetrygeneration employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASEDagents (GPT-3 and GPT-4). Our evaluation based on 96k generated poems showsthat our framework benefits the poetry generation process for TRAINING-BASEDagents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversityand a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams.The generated poetry from TRAINING-BASED agents also exhibits group divergencein terms of lexicons, styles and semantics. PROMPTING-BASED agents in ourframework also benefit from non-cooperative environments and a more diverseensemble of models with non-homogeneous agents has the potential to furtherenhance diversity, with an increase of 7.0-17.5 pp according to ourexperiments. However, PROMPTING-BASED agents show a decrease in lexicaldiversity over time and do not exhibit the group-based divergence intended inthe social network. Our paper argues for a paradigm shift in creative taskssuch as automatic poetry generation to include social learning processes (viaLLM-based agent modeling) similar to human interaction.</description><author>Ran Zhang, Steffen Eger</author><pubDate>Thu, 05 Sep 2024 16:12:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03659v1</guid></item><item><title>A DNN Biophysics Model with Topological and Electrostatic Features</title><link>http://arxiv.org/abs/2409.03658v1</link><description>In this project, we provide a deep-learning neural network (DNN) basedbiophysics model to predict protein properties. The model uses multi-scale anduniform topological and electrostatic features generated with proteinstructural information and force field, which governs the molecular mechanics.The topological features are generated using the element specified persistenthomology (ESPH) while the electrostatic features are fast computed using aCartesian treecode. These features are uniform in number for proteins withvarious sizes thus the broadly available protein structure database can be usedin training the network. These features are also multi-scale thus theresolution and computational cost can be balanced by the users. The machinelearning simulation on over 4000 protein structures shows the efficiency andfidelity of these features in representing the protein structure and forcefield for the predication of their biophysical properties such as electrostaticsolvation energy. Tests on topological or electrostatic features alone and thecombination of both showed the optimal performance when both features are used.This model shows its potential as a general tool in assisting biophysicalproperties and function prediction for the broad biomolecules using data fromboth theoretical computing and experiments.</description><author>Elyssa Sliheet, Md Abu Talha, Weihua Geng</author><pubDate>Thu, 05 Sep 2024 16:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03658v1</guid></item><item><title>Unsupervised Anomaly Detection and Localization with Generative Adversarial Networks</title><link>http://arxiv.org/abs/2409.03657v1</link><description>We propose a novel unsupervised anomaly detection approach using generativeadversarial networks and SOP-derived spectrograms. Demonstrating remarkableefficacy, our method achieves over 97% accuracy on SOP datasets from bothsubmarine and terrestrial fiber links, all achieved without the need forlabelled data.</description><author>Khouloud Abdelli, Matteo Lonardi, Jurgen Gripp, Samuel Olsson, Fabien Boitier, Patricia Layec</author><pubDate>Thu, 05 Sep 2024 16:11:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03657v1</guid></item><item><title>Privacy versus Emotion Preservation Trade-offs in Emotion-Preserving Speaker Anonymization</title><link>http://arxiv.org/abs/2409.03655v1</link><description>Advances in speech technology now allow unprecedented access to personallyidentifiable information through speech. To protect such information, thedifferential privacy field has explored ways to anonymize speech whilepreserving its utility, including linguistic and paralinguistic aspects.However, anonymizing speech while maintaining emotional state remainschallenging. We explore this problem in the context of the VoicePrivacy 2024challenge. Specifically, we developed various speaker anonymization pipelinesand find that approaches either excel at anonymization or preserving emotionstate, but not both simultaneously. Achieving both would require an in-domainemotion recognizer. Additionally, we found that it is feasible to train asemi-effective speaker verification system using only emotion representations,demonstrating the challenge of separating these two modalities.</description><author>Zexin Cai, Henry Li Xinyuan, Ashi Garg, Leibny Paola García-Perera, Kevin Duh, Sanjeev Khudanpur, Nicholas Andrews, Matthew Wiesner</author><pubDate>Thu, 05 Sep 2024 16:10:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03655v1</guid></item><item><title>On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization</title><link>http://arxiv.org/abs/2409.03650v1</link><description>Reinforcement Learning from Human Feedback (RLHF) is an effective approachfor aligning language models to human preferences. Central to RLHF is learninga reward function for scoring human preferences. Two main approaches forlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as inRLHF, and 2) using an implicit reward learned from preference data throughmethods such as Direct Preference Optimization (DPO). Prior work has shown thatthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM inthe limit. DPORM's effectiveness directly implies the optimality of the learnedpolicy, and also has practical implication for LLM alignment methods includingiterative DPO. However, it is unclear how well DPORM empirically matches theperformance of EXRM. This work studies the accuracy at distinguishing preferredand rejected answers for both DPORM and EXRM. Our findings indicate that eventhough DPORM fits the training dataset comparably, it generalizes lesseffectively than EXRM, especially when the validation datasets containdistribution shifts. Across five out-of-distribution settings, DPORM has a meandrop in accuracy of 3% and a maximum drop of 7%. These findings highlight thatDPORM has limited generalization ability and substantiates the integration ofan explicit reward model in iterative DPO approaches.</description><author>Yong Lin, Skyler Seto, Maartje ter Hoeve, Katherine Metcalf, Barry-John Theobald, Xuan Wang, Yizhe Zhang, Chen Huang, Tong Zhang</author><pubDate>Thu, 05 Sep 2024 16:08:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03650v1</guid></item><item><title>Towards Evaluating and Building Versatile Large Language Models for Medicine</title><link>http://arxiv.org/abs/2408.12547v2</link><description>In this study, we present MedS-Bench, a comprehensive benchmark designed toevaluate the performance of large language models (LLMs) in clinical contexts.Unlike existing benchmarks that focus on multiple-choice question answering,MedS-Bench spans 11 high-level clinical tasks, including clinical reportsummarization, treatment recommendations, diagnosis, named entity recognition,and medical concept explanation, among others. We evaluated six leading LLMs,e.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 usingfew-shot prompting, and found that even the most sophisticated models strugglewith these complex tasks. To address these limitations, we developed MedS-Ins,a large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58medically oriented language corpora, totaling 13.5 million samples across 122tasks. To demonstrate the dataset's utility, we conducted a proof-of-conceptexperiment by performing instruction tuning on a lightweight, open-sourcemedical language model. The resulting model, MMedIns-Llama 3, significantlyoutperformed existing models across nearly all clinical tasks. To promotefurther advancements in the application of LLMs to clinical challenges, we havemade the MedS-Ins dataset fully accessible and invite the research community tocontribute to its expansion.Additionally, we have launched a dynamicleaderboard for MedS-Bench, which we plan to regularly update the test set totrack progress and enhance the adaptation of general LLMs to the medicaldomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:https://github.com/MAGIC-AI4Med/MedS-Ins.</description><author>Chaoyi Wu, Pengcheng Qiu, Jinxin Liu, Hongfei Gu, Na Li, Ya Zhang, Yanfeng Wang, Weidi Xie</author><pubDate>Thu, 05 Sep 2024 16:07:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12547v2</guid></item><item><title>Segment Beyond View: Handling Partially Missing Modality for Audio-Visual Semantic Segmentation</title><link>http://arxiv.org/abs/2312.08673v3</link><description>Augmented Reality (AR) devices, emerging as prominent mobile interactionplatforms, face challenges in user safety, particularly concerning oncomingvehicles. While some solutions leverage onboard camera arrays, these camerasoften have limited field-of-view (FoV) with front or downward perspectives.Addressing this, we propose a new out-of-view semantic segmentation task andSegment Beyond View (SBV), a novel audio-visual semantic segmentation method.SBV supplements the visual modality, which miss the information beyond FoV,with the auditory information using a teacher-student distillation model(Omni2Ego). The model consists of a vision teacher utilising panoramicinformation, an auditory teacher with 8-channel audio, and an audio-visualstudent that takes views with limited FoV and binaural audio as input andproduce semantic segmentation for objects outside FoV. SBV outperforms existingmodels in comparative evaluations and shows a consistent performance acrossvarying FoV ranges and in monaural audio settings.</description><author>Renjie Wu, Hu Wang, Feras Dayoub, Hsiang-Ting Chen</author><pubDate>Thu, 05 Sep 2024 16:05:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08673v3</guid></item><item><title>Limited but consistent gains in adversarial robustness by co-training object recognition models with human EEG</title><link>http://arxiv.org/abs/2409.03646v1</link><description>In contrast to human vision, artificial neural networks (ANNs) remainrelatively susceptible to adversarial attacks. To address this vulnerability,efforts have been made to transfer inductive bias from human brains to ANNs,often by training the ANN representations to match their biologicalcounterparts. Previous works relied on brain data acquired in rodents orprimates using invasive techniques, from specific regions of the brain, undernon-natural conditions (anesthetized animals), and with stimulus datasetslacking diversity and naturalness. In this work, we explored whether aligningmodel representations to human EEG responses to a rich set of real-world imagesincreases robustness to ANNs. Specifically, we trained ResNet50-backbone modelson a dual task of classification and EEG prediction; and evaluated their EEGprediction accuracy and robustness to adversarial attacks. We observedsignificant correlation between the networks' EEG prediction accuracy, oftenhighest around 100 ms post stimulus onset, and their gains in adversarialrobustness. Although effect size was limited, effects were consistent acrossdifferent random initializations and robust for architectural variants. Wefurther teased apart the data from individual EEG channels and observedstrongest contribution from electrodes in the parieto-occipital regions. Thedemonstrated utility of human EEG for such tasks opens up avenues for futureefforts that scale to larger datasets under diverse stimuli conditions with thepromise of stronger effects.</description><author>Manshan Guo, Bhavin Choksi, Sari Sadiya, Alessandro T. Gifford, Martina G. Vilas, Radoslaw M. Cichy, Gemma Roig</author><pubDate>Thu, 05 Sep 2024 16:04:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03646v1</guid></item><item><title>RealisHuman: A Two-Stage Approach for Refining Malformed Human Parts in Generated Images</title><link>http://arxiv.org/abs/2409.03644v1</link><description>In recent years, diffusion models have revolutionized visual generation,outperforming traditional frameworks like Generative Adversarial Networks(GANs). However, generating images of humans with realistic semantic parts,such as hands and faces, remains a significant challenge due to their intricatestructural complexity. To address this issue, we propose a novelpost-processing solution named RealisHuman. The RealisHuman framework operatesin two stages. First, it generates realistic human parts, such as hands orfaces, using the original malformed parts as references, ensuring consistentdetails with the original image. Second, it seamlessly integrates the rectifiedhuman parts back into their corresponding positions by repainting thesurrounding areas to ensure smooth and realistic blending. The RealisHumanframework significantly enhances the realism of human generation, asdemonstrated by notable improvements in both qualitative and quantitativemetrics. Code is available at https://github.com/Wangbenzhi/RealisHuman.</description><author>Benzhi Wang, Jingkai Zhou, Jingqi Bai, Yang Yang, Weihua Chen, Fan Wang, Zhen Lei</author><pubDate>Thu, 05 Sep 2024 16:02:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03644v1</guid></item><item><title>CDM: A Reliable Metric for Fair and Accurate Formula Recognition Evaluation</title><link>http://arxiv.org/abs/2409.03643v1</link><description>Formula recognition presents significant challenges due to the complicatedstructure and varied notation of mathematical expressions. Despite continuousadvancements in formula recognition models, the evaluation metrics employed bythese models, such as BLEU and Edit Distance, still exhibit notablelimitations. They overlook the fact that the same formula has diverserepresentations and is highly sensitive to the distribution of training data,thereby causing the unfairness in formula recognition evaluation. To this end,we propose a Character Detection Matching (CDM) metric, ensuring the evaluationobjectivity by designing a image-level rather than LaTex-level metric score.Specifically, CDM renders both the model-predicted LaTeX and the ground-truthLaTeX formulas into image-formatted formulas, then employs visual featureextraction and localization techniques for precise character-level matching,incorporating spatial position information. Such a spatially-aware andcharacter-matching method offers a more accurate and equitable evaluationcompared with previous BLEU and Edit Distance metrics that rely solely ontext-based character matching. Experimentally, we evaluated various formularecognition models using CDM, BLEU, and ExpRate metrics. Their resultsdemonstrate that the CDM aligns more closely with human evaluation standardsand provides a fairer comparison across different models by eliminatingdiscrepancies caused by diverse formula representations.</description><author>Bin Wang, Fan Wu, Linke Ouyang, Zhuangcheng Gu, Rui Zhang, Renqiu Xia, Bo Zhang, Conghui He</author><pubDate>Thu, 05 Sep 2024 16:01:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03643v1</guid></item><item><title>Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field Representation and Generation</title><link>http://arxiv.org/abs/2403.19319v2</link><description>We present Mesh2NeRF, an approach to derive ground-truth radiance fields fromtextured meshes for 3D generation tasks. Many 3D generative approachesrepresent 3D scenes as radiance fields for training. Their ground-truthradiance fields are usually fitted from multi-view renderings from alarge-scale synthetic 3D dataset, which often results in artifacts due toocclusions or under-fitting issues. In Mesh2NeRF, we propose an analyticsolution to directly obtain ground-truth radiance fields from 3D meshes,characterizing the density field with an occupancy function featuring a definedsurface thickness, and determining view-dependent color through a reflectionfunction considering both the mesh and environment lighting. Mesh2NeRF extractsaccurate radiance fields which provides direct supervision for traininggenerative NeRFs and single scene representation. We validate the effectivenessof Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement inPSNR for view synthesis in single scene representation on the ABO dataset, a0.69 PSNR enhancement in the single-view conditional generation of ShapeNetCars, and notably improved mesh extraction from NeRF in the unconditionalgeneration of Objaverse Mugs.</description><author>Yujin Chen, Yinyu Nie, Benjamin Ummenhofer, Reiner Birkl, Michael Paulitsch, Matthias Müller, Matthias Nießner</author><pubDate>Thu, 05 Sep 2024 15:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19319v2</guid></item><item><title>Towards Neural Network based Cognitive Models of Dynamic Decision-Making by Humans</title><link>http://arxiv.org/abs/2407.17622v2</link><description>Modeling human cognitive processes in dynamic decision-making tasks has beenan endeavor in AI for a long time because such models can help make AI systemsmore intuitive, personalized, mitigate any human biases, and enhance trainingin simulation. Some initial work has attempted to utilize neural networks (andlarge language models) but often assumes one common model for all humans andaims to emulate human behavior in aggregate. However, the behavior of eachhuman is distinct, heterogeneous, and relies on specific past experiences incertain tasks. For instance, consider two individuals responding to a phishingemail: one who has previously encountered and identified similar threats mayrecognize it quickly, while another without such experience might fall for thescam. In this work, we build on Instance Based Learning (IBL) that posits thathuman decisions are based on similar situations encountered in the past.However, IBL relies on simple fixed form functions to capture the mapping frompast situations to current decisions. To that end, we propose two newattention-based neural network models to have open form non-linear functions tomodel distinct and heterogeneous human decision-making in dynamic settings. Weexperiment with two distinct datasets gathered from human subject experimentdata, one focusing on detection of phishing email by humans and another wherehumans act as attackers in a cybersecurity setting and decide on an attackoption. We conducted extensive experiments with our two neural network models,IBL, and GPT3.5, and demonstrate that the neural network models outperform IBLsignificantly in representing human decision-making, while providing similarinterpretability of human decisions as IBL. Overall, our work yields promisingresults for further use of neural networks in cognitive modeling of humandecision making.</description><author>Changyu Chen, Shashank Reddy Chirra, Maria José Ferreira, Cleotilde Gonzalez, Arunesh Sinha, Pradeep Varakantham</author><pubDate>Thu, 05 Sep 2024 15:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17622v2</guid></item><item><title>Legilimens: Practical and Unified Content Moderation for Large Language Model Services</title><link>http://arxiv.org/abs/2408.15488v2</link><description>Given the societal impact of unsafe content generated by large languagemodels (LLMs), ensuring that LLM services comply with safety standards is acrucial concern for LLM service providers. Common content moderation methodsare limited by an effectiveness-and-efficiency dilemma, where simple models arefragile while sophisticated models consume excessive computational resources.In this paper, we reveal for the first time that effective and efficientcontent moderation can be achieved by extracting conceptual features fromchat-oriented LLMs, despite their initial fine-tuning for conversation ratherthan content moderation. We propose a practical and unified content moderationframework for LLM services, named Legilimens, which features both effectivenessand efficiency. Our red-team model-based data augmentation enhances therobustness of Legilimens against state-of-the-art jailbreaking. Additionally,we develop a framework to theoretically analyze the cost-effectiveness ofLegilimens compared to other methods. We have conducted extensive experimentson five host LLMs, seventeen datasets, and nine jailbreaking methods to verifythe effectiveness, efficiency, and robustness of Legilimens against normal andadaptive adversaries. A comparison of Legilimens with both commercial andacademic baselines demonstrates the superior performance of Legilimens.Furthermore, we confirm that Legilimens can be applied to few-shot scenariosand extended to multi-label classification tasks.</description><author>Jialin Wu, Jiangyi Deng, Shengyuan Pang, Yanjiao Chen, Jiayang Xu, Xinfeng Li, Wenyuan Xu</author><pubDate>Thu, 05 Sep 2024 15:50:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15488v2</guid></item><item><title>Surface-Centric Modeling for High-Fidelity Generalizable Neural Surface Reconstruction</title><link>http://arxiv.org/abs/2409.03634v1</link><description>Reconstructing the high-fidelity surface from multi-view images, especiallysparse images, is a critical and practical task that has attracted widespreadattention in recent years. However, existing methods are impeded by the memoryconstraint or the requirement of ground-truth depths and cannot recoversatisfactory geometric details. To this end, we propose SuRF, a newSurface-centric framework that incorporates a new Region sparsification basedon a matching Field, achieving good trade-offs between performance, efficiencyand scalability. To our knowledge, this is the first unsupervised methodachieving end-to-end sparsification powered by the introduced matching field,which leverages the weight distribution to efficiently locate the boundaryregions containing surface. Instead of predicting an SDF value for each voxel,we present a new region sparsification approach to sparse the volume by judgingwhether the voxel is inside the surface region. In this way, our model canexploit higher frequency features around the surface with less memory andcomputational consumption. Extensive experiments on multiple benchmarkscontaining complex large-scale scenes show that our reconstructions exhibithigh-quality details and achieve new state-of-the-art performance, i.e., 46%improvements with 80% less memory consumption. Code is available athttps://github.com/prstrive/SuRF.</description><author>Rui Peng, Shihe Shen, Kaiqiang Xiong, Huachen Gao, Jianbo Jiao, Xiaodong Gu, Ronggang Wang</author><pubDate>Thu, 05 Sep 2024 15:48:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03634v1</guid></item><item><title>Tracing Privacy Leakage of Language Models to Training Data via Adjusted Influence Functions</title><link>http://arxiv.org/abs/2408.10468v4</link><description>The responses generated by Large Language Models (LLMs) can include sensitiveinformation from individuals and organizations, leading to potential privacyleakage. This work implements Influence Functions (IFs) to trace privacyleakage back to the training data, thereby mitigating privacy concerns ofLanguage Models (LMs). However, we notice that current IFs struggle toaccurately estimate the influence of tokens with large gradient norms,potentially overestimating their influence. When tracing the most influentialsamples, this leads to frequently tracing back to samples with large gradientnorm tokens, overshadowing the actual most influential samples even if theirinfluences are well estimated. To address this issue, we propose HeuristicallyAdjusted IF (HAIF), which reduces the weight of tokens with large gradientnorms, thereby significantly improving the accuracy of tracing the mostinfluential samples. To establish easily obtained groundtruth for tracingprivacy leakage, we construct two datasets, PII-E and PII-CR, representing twodistinct scenarios: one with identical text in the model outputs andpre-training data, and the other where models leverage their reasoningabilities to generate text divergent from pre-training data. HAIF significantlyimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-Edataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTAIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFson real-world pretraining data CLUECorpus2020, demonstrating strong robustnessregardless prompt and response lengths.</description><author>Jinxin Liu, Zao Yang</author><pubDate>Thu, 05 Sep 2024 15:47:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10468v4</guid></item><item><title>Beyond Model Interpretability: Socio-Structural Explanations in Machine Learning</title><link>http://arxiv.org/abs/2409.03632v1</link><description>What is it to interpret the outputs of an opaque machine learning model. Oneapproach is to develop interpretable machine learning techniques. Thesetechniques aim to show how machine learning models function by providing eithermodel centric local or global explanations, which can be based on mechanisticinterpretations revealing the inner working mechanisms of models ornonmechanistic approximations showing input feature output data relationships.In this paper, we draw on social philosophy to argue that interpreting machinelearning outputs in certain normatively salient domains could require appealingto a third type of explanation that we call sociostructural explanation. Therelevance of this explanation type is motivated by the fact that machinelearning models are not isolated entities but are embedded within and shaped bysocial structures. Sociostructural explanations aim to illustrate how socialstructures contribute to and partially explain the outputs of machine learningmodels. We demonstrate the importance of sociostructural explanations byexamining a racially biased healthcare allocation algorithm. Our proposalhighlights the need for transparency beyond model interpretability,understanding the outputs of machine learning systems could require a broaderanalysis that extends beyond the understanding of the machine learning modelitself.</description><author>Andrew Smart, Atoosa Kasirzadeh</author><pubDate>Thu, 05 Sep 2024 15:47:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03632v1</guid></item><item><title>UniMERNet: A Universal Network for Real-World Mathematical Expression Recognition</title><link>http://arxiv.org/abs/2404.15254v2</link><description>The paper introduces the UniMER dataset, marking the first study onMathematical Expression Recognition (MER) targeting complex real-worldscenarios. The UniMER dataset includes a large-scale training set, UniMER-1M,which offers unprecedented scale and diversity with one million traininginstances to train high-quality, robust models. Additionally, UniMER features ameticulously designed, diverse test set, UniMER-Test, which covers a variety offormula distributions found in real-world scenarios, providing a morecomprehensive and fair evaluation. To better utilize the UniMER dataset, thepaper proposes a Universal Mathematical Expression Recognition Network(UniMERNet), tailored to the characteristics of formula recognition. UniMERNetconsists of a carefully designed encoder that incorporates detail-aware andlocal context features, and an optimized decoder for accelerated performance.Extensive experiments conducted using the UniMER-1M dataset and UniMERNetdemonstrate that training on the large-scale UniMER-1M dataset can produce amore generalizable formula recognition model, significantly outperforming allprevious datasets. Furthermore, the introduction of UniMERNet enhances themodel's performance in formula recognition, achieving higher accuracy andspeeds. All data, models, and code are available athttps://github.com/opendatalab/UniMERNet.</description><author>Bin Wang, Zhuangcheng Gu, Guang Liang, Chao Xu, Bo Zhang, Botian Shi, Conghui He</author><pubDate>Thu, 05 Sep 2024 15:42:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15254v2</guid></item><item><title>Robust Clustering on High-Dimensional Data with Stochastic Quantization</title><link>http://arxiv.org/abs/2409.02066v2</link><description>This paper addresses the limitations of traditional vector quantization(clustering) algorithms, particularly K-Means and its variant K-Means++, andexplores the Stochastic Quantization (SQ) algorithm as a scalable alternativefor high-dimensional unsupervised and semi-supervised learning problems. Sometraditional clustering algorithms suffer from inefficient memory utilizationduring computation, necessitating the loading of all data samples into memory,which becomes impractical for large-scale datasets. While variants such asMini-Batch K-Means partially mitigate this issue by reducing memory usage, theylack robust theoretical convergence guarantees due to the non-convex nature ofclustering problems. In contrast, the Stochastic Quantization algorithmprovides strong theoretical convergence guarantees, making it a robustalternative for clustering tasks. We demonstrate the computational efficiencyand rapid convergence of the algorithm on an image classification problem withpartially labeled data, comparing model accuracy across various ratios oflabeled to unlabeled data. To address the challenge of high dimensionality, wetrained Triplet Network to encode images into low-dimensional representationsin a latent space, which serve as a basis for comparing the efficiency of boththe Stochastic Quantization algorithm and traditional quantization algorithms.Furthermore, we enhance the algorithm's convergence speed by introducingmodifications with an adaptive learning rate.</description><author>Anton Kozyriev, Vladimir Norkin</author><pubDate>Thu, 05 Sep 2024 15:35:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02066v2</guid></item><item><title>Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers</title><link>http://arxiv.org/abs/2409.03621v1</link><description>In decoder-based LLMs, the representation of a given layer serves twopurposes: as input to the next layer during the computation of the currenttoken; and as input to the attention mechanism of future tokens. In this work,we show that the importance of the latter role might be overestimated. To showthat, we start by manipulating the representations of previous tokens; e.g. byreplacing the hidden states at some layer k with random vectors. Ourexperimenting with four LLMs and four tasks show that this operation oftenleads to small to negligible drop in performance. Importantly, this happens ifthe manipulation occurs in the top part of the model-k is in the final 30-50%of the layers. In contrast, doing the same manipulation in earlier layers mightlead to chance level performance. We continue by switching the hidden state ofcertain tokens with hidden states of other tokens from another prompt; e.g.,replacing the word "Italy" with "France" in "What is the capital of Italy?". Wefind that when applying this switch in the top 1/3 of the model, the modelignores it (answering "Rome"). However if we apply it before, the modelconforms to the switch ("Paris"). Our results hint at a two stage process intransformer-based LLMs: the first part gathers input from previous tokens,while the second mainly processes that information internally.</description><author>Amit Ben Artzy, Roy Schwartz</author><pubDate>Thu, 05 Sep 2024 15:33:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03621v1</guid></item><item><title>Rethinking Molecular Design: Integrating Latent Variable and Auto-Regressive Models for Goal Directed Generation</title><link>http://arxiv.org/abs/2409.00046v2</link><description>De novo molecule design has become a highly active research area, advancedsignificantly through the use of state-of-the-art generative models. Despitethese advances, several fundamental questions remain unanswered as the fieldincreasingly focuses on more complex generative models and sophisticatedmolecular representations as an answer to the challenges of drug design. Inthis paper, we return to the simplest representation of molecules, andinvestigate overlooked limitations of classical generative approaches,particularly Variational Autoencoders (VAEs) and auto-regressive models. Wepropose a hybrid model in the form of a novel regularizer that leverages thestrengths of both to improve validity, conditional generation, and styletransfer of molecular sequences. Additionally, we provide an in depthdiscussion of overlooked assumptions of these models' behaviour.</description><author>Heath Arthur-Loui, Amina Mollaysa, Michael Krauthammer</author><pubDate>Thu, 05 Sep 2024 15:24:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00046v2</guid></item><item><title>Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments</title><link>http://arxiv.org/abs/2302.04823v5</link><description>Deriving robust control policies for realistic urban navigation scenarios isnot a trivial task. In an end-to-end approach, these policies must maphigh-dimensional images from the vehicle's cameras to low-level actions such assteering and throttle. While pure Reinforcement Learning (RL) approaches arebased exclusively on engineered rewards, Generative Adversarial ImitationLearning (GAIL) agents learn from expert demonstrations while interacting withthe environment, which favors GAIL on tasks for which a reward signal isdifficult to derive, such as autonomous driving. However, training deepnetworks directly from raw images on RL tasks is known to be unstable andtroublesome. To deal with that, this work proposes a hierarchical GAIL-basedarchitecture (hGAIL) which decouples representation learning from the drivingtask to solve the autonomous navigation of a vehicle. The proposed architectureconsists of two modules: a GAN (Generative Adversarial Net) which generates anabstract mid-level input representation, which is the Bird's-Eye View (BEV)from the surroundings of the vehicle; and the GAIL which learns to control thevehicle based on the BEV predictions from the GAN as input. hGAIL is able tolearn both the policy and the mid-level representation simultaneously as theagent interacts with the environment. Our experiments made in the CARLAsimulation environment have shown that GAIL exclusively from cameras (withoutBEV) fails to even learn the task, while hGAIL, after training exclusively onone city, was able to autonomously navigate successfully in 98% of theintersections of a new city not used in training phase. Videos and codeavailable at: https://sites.google.com/view/hgail</description><author>Gustavo Claudio Karl Couto, Eric Aislan Antonelo</author><pubDate>Thu, 05 Sep 2024 15:22:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04823v5</guid></item><item><title>DART2: a robust multiple testing method to smartly leverage helpful or misleading ancillary information</title><link>http://arxiv.org/abs/2409.03618v1</link><description>In many applications of multiple testing, ancillary information is available,reflecting the hypothesis null or alternative status. Several methods have beendeveloped to leverage this ancillary information to enhance testing power,typically requiring the ancillary information is helpful enough to ensurefavorable performance. In this paper, we develop a robust and effectivedistance-assisted multiple testing procedure named DART2, designed to bepowerful and robust regardless of the quality of ancillary information. Whenthe ancillary information is helpful, DART2 can asymptotically control FDRwhile improving power; otherwise, DART2 can still control FDR and maintainpower at least as high as ignoring the ancillary information. We demonstratedDART2's superior performance compared to existing methods through numericalstudies under various settings. In addition, DART2 has been applied to a geneassociation study where we have shown its superior accuracy and robustnessunder two different types of ancillary information.</description><author>Xuechan Li, Jichun Xie</author><pubDate>Thu, 05 Sep 2024 15:22:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03618v1</guid></item><item><title>1 Modular Parallel Manipulator for Long-Term Soft Robotic Data Collection</title><link>http://arxiv.org/abs/2409.03614v1</link><description>Performing long-term experimentation or large-scale data collection formachine learning in the field of soft robotics is challenging, due to thehardware robustness and experimental flexibility required. In this work, wepropose a modular parallel robotic manipulation platform suitable for suchlarge-scale data collection and compatible with various soft-roboticfabrication methods. Considering the computational and theoretical difficultyof replicating the high-fidelity, faster-than-real-time simulations that enablelarge-scale data collection in rigid robotic systems, a robust soft-robotichardware platform becomes a high priority development task for the field. The platform's modules consist of a pair of off-the-shelf electrical motorswhich actuate a customizable finger consisting of a compliant parallelstructure. The parallel mechanism of the finger can be as simple as a single3D-printed urethane or molded silicone bulk structure, due to the motors beingable to fully actuate a passive structure. This design flexibility allowsexperimentation with soft mechanism varied geometries, bulk properties andsurface properties. Additionally, while the parallel mechanism does not requireseparate electronics or additional parts, these can be included, and it can beconstructed using multi-functional soft materials to study compatible softsensors and actuators in the learning process. In this work, we validate theplatform's ability to be used for policy gradient reinforcement learningdirectly on hardware in a benchmark 2D manipulation task. We additionallydemonstrate compatibility with multiple fingers and characterize the designconstraints for compatible extensions.</description><author>Kiyn Chin, Carmel Majidi, Abhinav Gupta</author><pubDate>Thu, 05 Sep 2024 15:18:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03614v1</guid></item><item><title>VFLGAN-TS: Vertical Federated Learning-based Generative Adversarial Networks for Publication of Vertically Partitioned Time-Series Data</title><link>http://arxiv.org/abs/2409.03612v1</link><description>In the current artificial intelligence (AI) era, the scale and quality of thedataset play a crucial role in training a high-quality AI model. However, oftenoriginal data cannot be shared due to privacy concerns and regulations. Apotential solution is to release a synthetic dataset with a similardistribution to the private dataset. Nevertheless, in some scenarios, theattributes required to train an AI model are distributed among differentparties, and the parties cannot share the local data for synthetic dataconstruction due to privacy regulations. In PETS 2024, we recently introducedthe first Vertical Federated Learning-based Generative Adversarial Network(VFLGAN) for publishing vertically partitioned static data. However, VFLGANcannot effectively handle time-series data, presenting both temporal andattribute dimensions. In this article, we proposed VFLGAN-TS, which combinesthe ideas of attribute discriminator and vertical federated learning togenerate synthetic time-series data in the vertically partitioned scenario. Theperformance of VFLGAN-TS is close to that of its counterpart, which is trainedin a centralized manner and represents the upper limit for VFLGAN-TS. Tofurther protect privacy, we apply a Gaussian mechanism to make VFLGAN-TSsatisfy an $(\epsilon,\delta)$-differential privacy. Besides, we develop anenhanced privacy auditing scheme to evaluate the potential privacy breachthrough the framework of VFLGAN-TS and synthetic datasets.</description><author>Xun Yuan, Zilong Zhao, Prosanta Gope, Biplab Sikdar</author><pubDate>Thu, 05 Sep 2024 15:17:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03612v1</guid></item><item><title>MimicTouch: Leveraging Multi-modal Human Tactile Demonstrations for Contact-rich Manipulation</title><link>http://arxiv.org/abs/2310.16917v3</link><description>Tactile sensing is critical to fine-grained, contact-rich manipulation tasks,such as insertion and assembly. Prior research has shown the possibility oflearning tactile-guided policy from teleoperated demonstration data. However,to provide the demonstration, human users often rely on visual feedback tocontrol the robot. This creates a gap between the sensing modality used forcontrolling the robot (visual) and the modality of interest (tactile). Tobridge this gap, we introduce "MimicTouch", a novel framework for learningpolicies directly from demonstrations provided by human users with their hands.The key innovations are i) a human tactile data collection system whichcollects multi-modal tactile dataset for learning human's tactile-guidedcontrol strategy, ii) an imitation learning-based framework for learninghuman's tactile-guided control strategy through such data, and iii) an onlineresidual RL framework to bridge the embodiment gap between the human hand andthe robot gripper. Through comprehensive experiments, we highlight the efficacyof utilizing human's tactile-guided control strategy to resolve contact-richmanipulation tasks. The project website is athttps://sites.google.com/view/MimicTouch.</description><author>Kelin Yu, Yunhai Han, Qixian Wang, Vaibhav Saxena, Danfei Xu, Ye Zhao</author><pubDate>Thu, 05 Sep 2024 15:14:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16917v3</guid></item><item><title>CW-CNN &amp; CW-AN: Convolutional Networks and Attention Networks for CW-Complexes</title><link>http://arxiv.org/abs/2408.16686v2</link><description>We present a novel framework for learning on CW-complex structured datapoints. Recent advances have discussed CW-complexes as ideal learningrepresentations for problems in cheminformatics. However, there is a lack ofavailable machine learning methods suitable for learning on CW-complexes. Inthis paper we develop notions of convolution and attention that are welldefined for CW-complexes. These notions enable us to create the first Hodgeinformed neural network that can receive a CW-complex as input. We illustrateand interpret this framework in the context of supervised prediction.</description><author>Rahul Khorana</author><pubDate>Thu, 05 Sep 2024 15:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16686v2</guid></item><item><title>SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing</title><link>http://arxiv.org/abs/2409.03605v1</link><description>Audio-driven talking face generation aims to synthesize video with lipmovements synchronized to input audio. However, current generative techniquesface challenges in preserving intricate regional textures (skin, teeth). Toaddress the aforementioned challenges, we propose a novel framework calledSegTalker to decouple lip movements and image textures by introducingsegmentation as intermediate representation. Specifically, given the mask ofimage employed by a parsing network, we first leverage the speech to drive themask and generate talking segmentation. Then we disentangle semantic regions ofimage into style codes using a mask-guided encoder. Ultimately, we inject thepreviously generated talking segmentation and style codes into a mask-guidedStyleGAN to synthesize video frame. In this way, most of textures are fullypreserved. Moreover, our approach can inherently achieve background separationand facilitate mask-guided facial local editing. In particular, by editing themask and swapping the region textures from a given reference image (e.g. hair,lip, eyebrows), our approach enables facial editing seamlessly when generatingtalking face video. Experiments demonstrate that our proposed approach caneffectively preserve texture details and generate temporally consistent videowhile remaining competitive in lip synchronization. Quantitative andqualitative results on the HDTF and MEAD datasets illustrate the superiorperformance of our method over existing methods.</description><author>Lingyu Xiong, Xize Cheng, Jintao Tan, Xianjia Wu, Xiandong Li, Lei Zhu, Fei Ma, Minglei Li, Huang Xu, Zhihu Hu</author><pubDate>Thu, 05 Sep 2024 15:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03605v1</guid></item><item><title>Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS)</title><link>http://arxiv.org/abs/2308.08561v3</link><description>The Research &amp; Development (R&amp;D) phase of drug development is a lengthy andcostly process. To revolutionize this process, we introduce our new conceptQMLS to shorten the whole R&amp;D phase to three to six months and decrease thecost to merely fifty to eighty thousand USD. For Hit Generation, MachineLearning Molecule Generation (MLMG) generates possible hits according to themolecular structure of the target protein while the Quantum Simulation (QS)filters molecules from the primary essay based on the reaction and bindingeffectiveness with the target protein. Then, For Lead Optimization, theresultant molecules generated and filtered from MLMG and QS are compared, andmolecules that appear as a result of both processes will be made into dozens ofmolecular variations through Machine Learning Molecule Variation (MLMV), whileothers will only be made into a few variations. Lastly, all optimized moleculeswould undergo multiple rounds of QS filtering with a high standard for reactioneffectiveness and safety, creating a few dozen pre-clinical-trail-ready drugs.This paper is based on our first paper, where we pitched the concept of machinelearning combined with quantum simulations. In this paper we will go over thedetailed design and framework of QMLS, including MLMG, MLMV, and QS.</description><author>Yifan Zhou, Yan Shing Liang, Yew Kee Wong, Haichuan Qiu, Yu Xi Wu, Bin He</author><pubDate>Thu, 05 Sep 2024 15:09:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08561v3</guid></item><item><title>LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning</title><link>http://arxiv.org/abs/2401.16185v2</link><description>Large language models (LLMs) have demonstrated significant potential invarious tasks, including vulnerability detection. However, current efforts inthis area are preliminary, lacking clarity on whether LLMs' vulnerabilityreasoning capabilities stem from the models themselves or external aids such asknowledge retrieval and tooling support. This paper aims to isolate LLMs' vulnerability reasoning from othercapabilities, such as vulnerability knowledge adoption, context informationretrieval, and structured output generation. We introduce LLM4Vuln, a unifiedevaluation framework that separates and assesses LLMs' vulnerability reasoningcapabilities and examines improvements when combined with other enhancements. We conducted controlled experiments with 97 ground-truth vulnerabilities and97 non-vulnerable cases in Solidity and Java, testing them in a total of 9,312scenarios across four LLMs (GPT-4, GPT-3.5, Mixtral, and Llama 3). Our findingsreveal the varying impacts of knowledge enhancement, context supplementation,prompt schemes, and models. Additionally, we identified 14 zero-dayvulnerabilities in four pilot bug bounty programs, resulting in \$3,576 inbounties.</description><author>Yuqiang Sun, Daoyuan Wu, Yue Xue, Han Liu, Wei Ma, Lyuye Zhang, Yang Liu, Yingjiu Li</author><pubDate>Thu, 05 Sep 2024 15:03:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16185v2</guid></item><item><title>Large-Batch, Iteration-Efficient Neural Bayesian Design Optimization</title><link>http://arxiv.org/abs/2306.01095v4</link><description>Bayesian optimization (BO) provides a powerful framework for optimizingblack-box, expensive-to-evaluate functions. It is therefore an attractive toolfor engineering design problems, typically involving multiple objectives.Thanks to the rapid advances in fabrication and measurement methods as well asparallel computing infrastructure, querying many design problems can be heavilyparallelized. This class of problems challenges BO with an unprecedented setupwhere it has to deal with very large batches, shifting its focus from sampleefficiency to iteration efficiency. We present a novel Bayesian optimizationframework specifically tailored to address these limitations. Our keycontribution is a highly scalable, sample-based acquisition function thatperforms a non-dominated sorting of not only the objectives but also theirassociated uncertainty. We show that our acquisition function in combinationwith different Bayesian neural network surrogates is effective indata-intensive environments with a minimal number of iterations. We demonstratethe superiority of our method by comparing it with state-of-the-artmulti-objective optimizations. We perform our evaluation on two real-worldproblems -- airfoil design and 3D printing -- showcasing the applicability andefficiency of our approach. Our code is available at:https://github.com/an-on-ym-ous/lbn_mobo</description><author>Navid Ansari, Alireza Javanmardi, Eyke Hüllermeier, Hans-Peter Seidel, Vahid Babaei</author><pubDate>Thu, 05 Sep 2024 15:01:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01095v4</guid></item><item><title>TCDiff: Triple Condition Diffusion Model with 3D Constraints for Stylizing Synthetic Faces</title><link>http://arxiv.org/abs/2409.03600v1</link><description>A robust face recognition model must be trained using datasets that include alarge number of subjects and numerous samples per subject under varyingconditions (such as pose, expression, age, noise, and occlusion). Due toethical and privacy concerns, large-scale real face datasets have beendiscontinued, such as MS1MV3, and synthetic face generators have been proposed,utilizing GANs and Diffusion Models, such as SYNFace, SFace, DigiFace-1M,IDiff-Face, DCFace, and GANDiffFace, aiming to supply this demand. Some ofthese methods can produce high-fidelity realistic faces, but with lowintra-class variance, while others generate high-variance faces with lowidentity consistency. In this paper, we propose a Triple Condition DiffusionModel (TCDiff) to improve face style transfer from real to synthetic facesthrough 2D and 3D facial constraints, enhancing face identity consistency whilekeeping the necessary high intra-class variance. Face recognition experimentsusing 1k, 2k, and 5k classes of our new dataset for training outperformstate-of-the-art synthetic datasets in real face benchmarks such as LFW,CFP-FP, AgeDB, and BUPT. Our source code is available at:https://github.com/BOVIFOCR/tcdiff.</description><author>Bernardo Biesseck, Pedro Vidal, Luiz Coelho, Roger Granada, David Menotti|</author><pubDate>Thu, 05 Sep 2024 14:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03600v1</guid></item><item><title>A practical approach to evaluating the adversarial distance for machine learning classifiers</title><link>http://arxiv.org/abs/2409.03598v1</link><description>Robustness is critical for machine learning (ML) classifiers to ensureconsistent performance in real-world applications where models may encountercorrupted or adversarial inputs. In particular, assessing the robustness ofclassifiers to adversarial inputs is essential to protect systems fromvulnerabilities and thus ensure safety in use. However, methods to accuratelycompute adversarial robustness have been challenging for complex ML models andhigh-dimensional data. Furthermore, evaluations typically measure adversarialaccuracy on specific attack budgets, limiting the informative value of theresulting metrics. This paper investigates the estimation of the moreinformative adversarial distance using iterative adversarial attacks and acertification approach. Combined, the methods provide a comprehensiveevaluation of adversarial robustness by computing estimates for the upper andlower bounds of the adversarial distance. We present visualisations andablation studies that provide insights into how this evaluation method shouldbe applied and parameterised. We find that our adversarial attack approach iseffective compared to related implementations, while the certification methodfalls short of expectations. The approach in this paper should encourage a moreinformative way of evaluating the adversarial robustness of ML classifiers.</description><author>Georg Siedel, Ekagra Gupta, Andrey Morozov</author><pubDate>Thu, 05 Sep 2024 14:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03598v1</guid></item><item><title>Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis</title><link>http://arxiv.org/abs/2409.03597v1</link><description>This paper presents the Multimodal Analyzing System for Laryngoscope (MASL),a system that combines audio and video data to automatically extract keysegments and metrics from laryngeal videostroboscopic videos for clinicalassessment. MASL integrates glottis detection with keyword spotting to analyzepatient vocalizations and refine video highlights for better inspection ofvocal cord movements. The system includes a strobing video extraction modulethat identifies frames by analyzing hue, saturation, and value fluctuations.MASL also provides effective metrics for vocal cord paralysis detection,employing a two-stage glottis segmentation process using U-Net followed bydiffusion-based refinement to reduce false positives. Instead of glottal areawaveforms, MASL estimates anterior glottic angle waveforms (AGAW) from glottismasks, evaluating both left and right vocal cords to detect unilateral vocalcord paralysis (UVFP). By comparing AGAW variances, MASL distinguishes betweenleft and right paralysis. Ablation studies and experiments on public andreal-world datasets validate MASL's segmentation module and demonstrate itsability to provide reliable metrics for UVFP diagnosis.</description><author>Yucong Zhang, Xin Zou, Jinshan Yang, Wenjun Chen, Faya Liang, Ming Li</author><pubDate>Thu, 05 Sep 2024 14:56:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03597v1</guid></item><item><title>FDNet: Feature Decoupled Segmentation Network for Tooth CBCT Image</title><link>http://arxiv.org/abs/2311.06551v2</link><description>Precise Tooth Cone Beam Computed Tomography (CBCT) image segmentation iscrucial for orthodontic treatment planning. In this paper, we propose FDNet, aFeature Decoupled Segmentation Network, to excel in the face of the variabledental conditions encountered in CBCT scans, such as complex artifacts andindistinct tooth boundaries. The Low-Frequency Wavelet Transform (LF-Wavelet)is employed to enrich the semantic content by emphasizing the global structuralintegrity of the teeth, while the SAM encoder is leveraged to refine theboundary delineation, thus improving the contrast between adjacent dentalstructures. By integrating these dual aspects, FDNet adeptly addresses thesemantic gap, providing a detailed and accurate segmentation. The framework'seffectiveness is validated through rigorous benchmarks, achieving the top Diceand IoU scores of 85.28% and 75.23%, respectively. This innovative decouplingof semantic and boundary features capitalizes on the unique strengths of eachelement to elevate the quality of segmentation performance.</description><author>Xiang Feng, Chengkai Wang, Chengyu Wu, Yunxiang Li, Yongbo He, Shuai Wang, Yaiqi Wang</author><pubDate>Thu, 05 Sep 2024 14:47:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06551v2</guid></item><item><title>Costs Estimation in Unit Commitment Problems using Simulation-Based Inference</title><link>http://arxiv.org/abs/2409.03588v1</link><description>The Unit Commitment (UC) problem is a key optimization task in power systemsto forecast the generation schedules of power units over a finite time periodby minimizing costs while meeting demand and technical constraints. However,many parameters required by the UC problem are unknown, such as the costs. Inthis work, we estimate these unknown costs using simulation-based inference onan illustrative UC problem, which provides an approximated posteriordistribution of the parameters given observed generation schedules and demands.Our results highlight that the learned posterior distribution effectivelycaptures the underlying distribution of the data, providing a range of possiblevalues for the unknown parameters given a past observation. This posteriorallows for the estimation of past costs using observed past generationschedules, enabling operators to better forecast future costs and make morerobust generation scheduling forecasts. We present avenues for future researchto address overconfidence in posterior estimation, enhance the scalability ofthe methodology and apply it to more complex UC problems modeling the networkconstraints and renewable energy sources.</description><author>Matthias Pirlet, Adrien Bolland, Gilles Louppe, Damien Ernst</author><pubDate>Thu, 05 Sep 2024 14:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03588v1</guid></item><item><title>Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities</title><link>http://arxiv.org/abs/2408.07666v4</link><description>Model merging is an efficient empowerment technique in the machine learningcommunity that does not require the collection of raw training data and doesnot require expensive computation. As model merging becomes increasinglyprevalent across various fields, it is crucial to understand the availablemodel merging techniques comprehensively. However, there is a significant gapin the literature regarding a systematic and thorough review of thesetechniques. This survey provides a comprehensive overview of model mergingmethods and theories, their applications in various domains and settings, andfuture research directions. Specifically, we first propose a new taxonomicapproach that exhaustively discusses existing model merging methods. Secondly,we discuss the application of model merging techniques in large languagemodels, multimodal large language models, and 10+ machine learning subfields,including continual learning, multi-task learning, few-shot learning, etc.Finally, we highlight the remaining challenges of model merging and discussfuture research directions. A comprehensive list of papers about model mergingis available at\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.</description><author>Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, Dacheng Tao</author><pubDate>Thu, 05 Sep 2024 14:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07666v4</guid></item><item><title>Text-Guided Mixup Towards Long-Tailed Image Categorization</title><link>http://arxiv.org/abs/2409.03583v1</link><description>In many real-world applications, the frequency distribution of class labelsfor training data can exhibit a long-tailed distribution, which challengestraditional approaches of training deep neural networks that require heavyamounts of balanced data. Gathering and labeling data to balance out the classlabel distribution can be both costly and time-consuming. Many existingsolutions that enable ensemble learning, re-balancing strategies, orfine-tuning applied to deep neural networks are limited by the inert problem offew class samples across a subset of classes. Recently, vision-language modelslike CLIP have been observed as effective solutions to zero-shot or few-shotlearning by grasping a similarity between vision and language features forimage and text pairs. Considering that large pre-trained vision-language modelsmay contain valuable side textual information for minor classes, we propose toleverage text supervision to tackle the challenge of long-tailed learning.Concretely, we propose a novel text-guided mixup technique that takes advantageof the semantic relations between classes recognized by the pre-trained textencoder to help alleviate the long-tailed problem. Our empirical study onbenchmark long-tailed tasks demonstrates the effectiveness of our proposal witha theoretical guarantee. Our code is available athttps://github.com/rsamf/text-guided-mixup.</description><author>Richard Franklin, Jiawei Yao, Deyang Zhong, Qi Qian, Juhua Hu</author><pubDate>Thu, 05 Sep 2024 14:37:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03583v1</guid></item><item><title>HiVG: Hierarchical Multimodal Fine-grained Modulation for Visual Grounding</title><link>http://arxiv.org/abs/2404.13400v2</link><description>Visual grounding, which aims to ground a visual region via natural language,is a task that heavily relies on cross-modal alignment. Existing works utilizeduni-modal pre-trained models to transfer visual or linguistic knowledgeseparately while ignoring the multimodal corresponding information. Motivatedby recent advancements in contrastive language-image pre-training and low-rankadaptation (LoRA) methods, we aim to solve the grounding task based onmultimodal pre-training. However, there exists significant task gaps betweenpre-training and grounding. Therefore, to address these gaps, we propose aconcise and efficient hierarchical multimodal fine-grained modulationframework, namely HiVG. Specifically, HiVG consists of a multi-layer adaptivecross-modal bridge and a hierarchical multimodal low-rank adaptation (HiLoRA)paradigm. The cross-modal bridge can address the inconsistency between visualfeatures and those required for grounding, and establish a connection betweenmulti-level visual and text features. HiLoRA prevents the accumulation ofperceptual errors by adapting the cross-modal features from shallow to deeplayers in a hierarchical manner. Experimental results on five datasetsdemonstrate the effectiveness of our approach and showcase the significantgrounding capabilities as well as promising energy efficiency advantages. Theproject page: https://github.com/linhuixiao/HiVG.</description><author>Linhui Xiao, Xiaoshan Yang, Fang Peng, Yaowei Wang, Changsheng Xu</author><pubDate>Thu, 05 Sep 2024 14:33:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13400v2</guid></item><item><title>CHIRPs: Change-Induced Regret Proxy metrics for Lifelong Reinforcement Learning</title><link>http://arxiv.org/abs/2409.03577v1</link><description>Reinforcement learning agents can achieve superhuman performance in statictasks but are costly to train and fragile to task changes. This limits theirdeployment in real-world scenarios where training experience is expensive orthe context changes through factors like sensor degradation, environmentalprocesses or changing mission priorities. Lifelong reinforcement learning aimsto improve sample efficiency and adaptability by studying how agents perform inevolving problems. The difficulty that these changes pose to an agent is rarelymeasured directly, however. Agent performances can be compared across a change,but this is often prohibitively expensive. We propose Change-Induced RegretProxy (CHIRP) metrics, a class of metrics for approximating a change'sdifficulty while avoiding the high costs of using trained agents. Arelationship between a CHIRP metric and agent performance is identified in twoenvironments, a simple grid world and MetaWorld's suite of robotic arm tasks.We demonstrate two uses for these metrics: for learning, an agent that clustersMDPs based on a CHIRP metric achieves $17\%$ higher average returns than threeexisting agents in a sequence of MetaWorld tasks. We also show how a CHIRP canbe calibrated to compare the difficulty of changes across distinctly differentenvironments.</description><author>John Birkbeck, Adam Sobey, Federico Cerutti, Katherine Heseltine Hurley Flynn, Timothy J. Norman</author><pubDate>Thu, 05 Sep 2024 14:31:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03577v1</guid></item><item><title>Boosting the Power of Kernel Two-Sample Tests</title><link>http://arxiv.org/abs/2302.10687v2</link><description>The kernel two-sample test based on the maximum mean discrepancy (MMD) is oneof the most popular methods for detecting differences between two distributionsover general metric spaces. In this paper we propose a method to boost thepower of the kernel test by combining MMD estimates over multiple kernels usingtheir Mahalanobis distance. We derive the asymptotic null distribution of theproposed test statistic and use a multiplier bootstrap approach to efficientlycompute the rejection region. The resulting test is universally consistent and,since it is obtained by aggregating over a collection of kernels/bandwidths, ismore powerful in detecting a wide range of alternatives in finite samples. Wealso derive the distribution of the test statistic for both fixed and localcontiguous alternatives. The latter, in particular, implies that the proposedtest is statistically efficient, that is, it has non-trivial asymptotic(Pitman) efficiency. The consistency properties of the Mahalanobis and othernatural aggregation methods are also explored when the number of kernels isallowed to grow with the sample size. Extensive numerical experiments areperformed on both synthetic and real-world datasets to illustrate the efficacyof the proposed method over single kernel tests. The computational complexityof the proposed method is also studied, both theoretically and in simulations.Our asymptotic results rely on deriving the joint distribution of MMD estimatesusing the framework of multiple stochastic integrals, which is more broadlyuseful, specifically, in understanding the efficiency properties of recentlyproposed adaptive MMD tests based on kernel aggregation and also in developingmore computationally efficient (linear time) tests that combine multiplekernels. We conclude with an application of the Mahalanobis aggregation methodfor kernels with diverging scaling parameters.</description><author>Anirban Chatterjee, Bhaswar B. Bhattacharya</author><pubDate>Thu, 05 Sep 2024 14:21:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10687v2</guid></item><item><title>100 instances is all you need: predicting the success of a new LLM on unseen data by testing on a few instances</title><link>http://arxiv.org/abs/2409.03563v1</link><description>Predicting the performance of LLMs on individual task instances is essentialto ensure their reliability in high-stakes applications. To do so, apossibility is to evaluate the considered LLM on a set of task instances andtrain an assessor to predict its performance based on features of theinstances. However, this approach requires evaluating each new LLM on asufficiently large set of task instances to train an assessor specific to it.In this work, we leverage the evaluation results of previously tested LLMs toreduce the number of evaluations required to predict the performance of a newLLM. In practice, we propose to test the new LLM on a small set of referenceinstances and train a generic assessor which predicts the performance of theLLM on an instance based on the performance of the former on the reference setand features of the instance of interest. We conduct empirical studies onHELM-Lite and KindsOfReasoning, a collection of existing reasoning datasetsthat we introduce, where we evaluate all instruction-fine-tuned OpenAI modelsuntil the January 2024 version of GPT4. When predicting performance oninstances with the same distribution as those used to train the genericassessor, we find this achieves performance comparable to the LLM-specificassessors trained on the full set of instances. Additionally, we find thatrandomly selecting the reference instances performs as well as some advancedselection methods we tested. For out of distribution, however, no clear winneremerges and the overall performance is worse, suggesting that the inherentpredictability of LLMs is low.</description><author>Lorenzo Pacchiardi, Lucy G. Cheke, José Hernández-Orallo</author><pubDate>Thu, 05 Sep 2024 14:19:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03563v1</guid></item><item><title>Reducing Spatial Discretization Error on Coarse CFD Simulations Using an OpenFOAM-Embedded Deep Learning Framework</title><link>http://arxiv.org/abs/2405.07441v3</link><description>We propose a method for reducing the spatial discretization error of coarsecomputational fluid dynamics (CFD) problems by enhancing the quality oflow-resolution simulations using deep learning. We feed the model withfine-grid data after projecting it to the coarse-grid discretization. Wesubstitute the default differencing scheme for the convection term by afeed-forward neural network that interpolates velocities from cell centers toface values to produce velocities that approximate the down-sampled fine-griddata well. The deep learning framework incorporates the open-source CFD codeOpenFOAM, resulting in an end-to-end differentiable model. We automaticallydifferentiate the CFD physics using a discrete adjoint code version. We presenta fast communication method between TensorFlow (Python) and OpenFOAM (c++) thataccelerates the training process. We applied the model to the flow past asquare cylinder problem, reducing the error from 120% to 25% in the velocityfor simulations inside the training distribution compared to the traditionalsolver using an x8 coarser mesh. For simulations outside the trainingdistribution, the error reduction in the velocities was about 50%. The trainingis affordable in terms of time and data samples since the architecture exploitsthe local features of the physics.</description><author>Jesus Gonzalez-Sieiro, David Pardo, Vincenzo Nava, Victor M. Calo, Markus Towara</author><pubDate>Thu, 05 Sep 2024 14:17:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07441v3</guid></item><item><title>MaskVal: Simple but Effective Uncertainty Quantification for 6D Pose Estimation</title><link>http://arxiv.org/abs/2409.03556v1</link><description>For the use of 6D pose estimation in robotic applications, reliable poses areof utmost importance to ensure a safe, reliable and predictable operationalperformance. Despite these requirements, state-of-the-art 6D pose estimatorsoften do not provide any uncertainty quantification for their pose estimates atall, or if they do, it has been shown that the uncertainty provided is onlyweakly correlated with the actual true error. To address this issue, weinvestigate a simple but effective uncertainty quantification, that we callMaskVal, which compares the pose estimates with their corresponding instancesegmentations by rendering and does not require any modification of the poseestimator itself. Despite its simplicity, MaskVal significantly outperforms astate-of-the-art ensemble method on both a dataset and a robotic setup. We showthat by using MaskVal, the performance of a state-of-the-art 6D pose estimatoris significantly improved towards a safe and reliable operation. In addition,we propose a new and specific approach to compare and evaluate uncertaintyquantification methods for 6D pose estimation in the context of roboticmanipulation.</description><author>Philipp Quentin, Daniel Goehring</author><pubDate>Thu, 05 Sep 2024 14:17:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03556v1</guid></item><item><title>Triple-domain Feature Learning with Frequency-aware Memory Enhancement for Moving Infrared Small Target Detection</title><link>http://arxiv.org/abs/2406.06949v2</link><description>As a sub-field of object detection, moving infrared small target detectionpresents significant challenges due to tiny target sizes and low contrastagainst backgrounds. Currently-existing methods primarily rely on the featuresextracted only from spatio-temporal domain. Frequency domain has hardly beenconcerned yet, although it has been widely applied in image processing. Toextend feature source domains and enhance feature representation, we propose anew Triple-domain Strategy (Tridos) with the frequency-aware memory enhancementon spatio-temporal domain for infrared small target detection. In this scheme,it effectively detaches and enhances frequency features by a local-globalfrequency-aware module with Fourier transform. Inspired by human visual system,our memory enhancement is designed to capture the spatial relations of infraredtargets among video frames. Furthermore, it encodes temporal dynamics motionfeatures via differential learning and residual enhancing. Additionally, wefurther design a residual compensation to reconcile possible cross-domainfeature mismatches. To our best knowledge, proposed Tridos is the first work toexplore infrared target feature learning comprehensively inspatio-temporal-frequency domains. The extensive experiments on three datasets(i.e., DAUB, ITSDT-15K and IRDST) validate that our triple-domain infraredfeature learning scheme could often be obviously superior to state-of-the-artones. Source codes are available at https://github.com/UESTC-nnLab/Tridos.</description><author>Weiwei Duan, Luping Ji, Shengjia Chen, Sicheng Zhu, Mao Ye</author><pubDate>Thu, 05 Sep 2024 14:16:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06949v2</guid></item><item><title>Unified Framework for Neural Network Compression via Decomposition and Optimal Rank Selection</title><link>http://arxiv.org/abs/2409.03555v1</link><description>Despite their high accuracy, complex neural networks demand significantcomputational resources, posing challenges for deployment onresource-constrained devices such as mobile phones and embedded systems.Compression algorithms have been developed to address these challenges byreducing model size and computational demands while maintaining accuracy. Amongthese approaches, factorization methods based on tensor decomposition aretheoretically sound and effective. However, they face difficulties in selectingthe appropriate rank for decomposition. This paper tackles this issue bypresenting a unified framework that simultaneously applies decomposition andoptimal rank selection, employing a composite compression loss within definedrank constraints. Our approach includes an automatic rank search in acontinuous space, efficiently identifying optimal rank configurations withoutthe use of training data, making it computationally efficient. Combined with asubsequent fine-tuning step, our approach maintains the performance of highlycompressed models on par with their original counterparts. Using variousbenchmark datasets, we demonstrate the efficacy of our method through acomprehensive analysis.</description><author>Ali Aghababaei-Harandi, Massih-Reza Amini</author><pubDate>Thu, 05 Sep 2024 14:15:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03555v1</guid></item><item><title>Organized Grouped Discrete Representation for Object-Centric Learning</title><link>http://arxiv.org/abs/2409.03553v1</link><description>Object-Centric Learning (OCL) represents dense image or video pixels assparse object features. Representative methods utilize discrete representationcomposed of Variational Autoencoder (VAE) template features to suppresspixel-level information redundancy and guide object-level feature aggregation.The most recent advancement, Grouped Discrete Representation (GDR), furtherdecomposes these template features into attributes. However, its naive channelgrouping as decomposition may erroneously group channels belonging to differentattributes together and discretize them as sub-optimal template attributes,which losses information and harms expressivity. We propose Organized GDR(OGDR) to organize channels belonging to the same attributes together forcorrect decomposition from features into attributes. In unsupervisedsegmentation experiments, OGDR is fully superior to GDR in augmentatingclassical transformer-based OCL methods; it even improves state-of-the-artdiffusion-based ones. Codebook PCA and representation similarity analyses showthat compared with GDR, our OGDR eliminates redundancy and preservesinformation better for guiding object representation learning. The source codeis available in the supplementary material.</description><author>Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen</author><pubDate>Thu, 05 Sep 2024 14:13:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03553v1</guid></item><item><title>DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture</title><link>http://arxiv.org/abs/2409.03550v1</link><description>Diffusion models (DMs) have demonstrated exceptional generative capabilitiesacross various areas, while they are hindered by slow inference speeds and highcomputational demands during deployment. The most common way to accelerate DMsinvolves reducing the number of denoising steps during generation, achievedthrough faster sampling solvers or knowledge distillation (KD). In contrast toprior approaches, we propose a novel method that transfers the capability oflarge pretrained DMs to faster architectures. Specifically, we employ KD in adistinct manner to compress DMs by distilling their generative ability intomore rapid variants. Furthermore, considering that the source data is eitherunaccessible or too enormous to store for current generative models, weintroduce a new paradigm for their distillation without source data, termedData-Free Knowledge Distillation for Diffusion Models (DKDM). Generally, ourestablished DKDM framework comprises two main components: 1) a DKDM objectivethat uses synthetic denoising data produced by pretrained DMs to optimizefaster DMs without source data, and 2) a dynamic iterative distillation methodthat flexibly organizes the synthesis of denoising data, preventing it fromslowing down the optimization process as the generation is slow. To ourknowledge, this is the first attempt at using KD to distill DMs into anyarchitecture in a data-free manner. Importantly, our DKDM is orthogonal to mostexisting acceleration methods, such as denoising step reduction, quantizationand pruning. Experiments show that our DKDM is capable of deriving 2x fasterDMs with performance remaining on par with the baseline. Notably, our DKDMenables pretrained DMs to function as "datasets" for training new DMs.</description><author>Qianlong Xiang, Miao Zhang, Yuzhang Shang, Jianlong Wu, Yan Yan, Liqiang Nie</author><pubDate>Thu, 05 Sep 2024 14:12:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03550v1</guid></item><item><title>The Power of Second Chance: Personalized Submodular Maximization with Two Candidates</title><link>http://arxiv.org/abs/2409.03545v1</link><description>Most of existing studies on submodular maximization focus on selecting asubset of items that maximizes a \emph{single} submodular function. However, inmany real-world scenarios, we might have multiple user-specific functions, eachof which models the utility of a particular type of user. In these settings,our goal would be to choose a set of items that performs well across all theuser-specific functions. One way to tackle this problem is to select a singlesubset that maximizes the sum of all of the user-specific functions. Althoughthis aggregate approach is efficient in the sense that it avoids computation ofsets for individual functions, it really misses the power of personalization -for it does not allow to choose different sets for different functions. In thispaper, we introduce the problem of personalized submodular maximization withtwo candidate solutions. For any two candidate solutions, the utility of eachuser-specific function is defined as the better of these two candidates. Ourobjective is, therefore, to select the best set of two candidates that maximizethe sum of utilities of all the user-specific functions. We have designedeffective algorithms for this problem. We also discuss how our approachgeneralizes to multiple candidate solutions, increasing flexibility andpersonalization in our solution.</description><author>Jing Yuan, Shaojie Tang</author><pubDate>Thu, 05 Sep 2024 14:07:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03545v1</guid></item><item><title>Prediction Accuracy &amp; Reliability: Classification and Object Localization under Distribution Shift</title><link>http://arxiv.org/abs/2409.03543v1</link><description>Natural distribution shift causes a deterioration in the perceptionperformance of convolutional neural networks (CNNs). This comprehensiveanalysis for real-world traffic data addresses: 1) investigating the effect ofnatural distribution shift and weather augmentations on both detection qualityand confidence estimation, 2) evaluating model performance for bothclassification and object localization, and 3) benchmarking two commonuncertainty quantification methods - Ensembles and different variants ofMonte-Carlo (MC) Dropout - under natural and close-to-natural distributionshift. For this purpose, a novel dataset has been curated from publiclyavailable autonomous driving datasets. The in-distribution (ID) data is basedon cutouts of a single object, for which both class and bounding boxannotations are available. The six distribution-shift datasets cover adverseweather scenarios, simulated rain and fog, corner cases, andout-of-distribution data. A granular analysis of CNNs under distribution shiftallows to quantize the impact of different types of shifts on both, taskperformance and confidence estimation: ConvNeXt-Tiny is more robust thanEfficientNet-B0; heavy rain degrades classification stronger than localization,contrary to heavy fog; integrating MC-Dropout into selected layers only has thepotential to enhance task performance and confidence estimation, whereby theidentification of these layers depends on the type of distribution shift andthe considered task.</description><author>Fabian Diet, Moussa Kassem Sbeyti, Michelle Karg</author><pubDate>Thu, 05 Sep 2024 14:06:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03543v1</guid></item><item><title>Risk-based Calibration for Probabilistic Classifiers</title><link>http://arxiv.org/abs/2409.03542v1</link><description>We introduce a general iterative procedure called risk-based calibration (RC)designed to minimize the empirical risk under the 0-1 loss (empirical error)for probabilistic classifiers. These classifiers are based on modelingprobability distributions, including those constructed from the jointdistribution (generative) and those based on the class conditional distribution(conditional). RC can be particularized to any probabilistic classifierprovided a specific learning algorithm that computes the classifier'sparameters in closed form using data statistics. RC reinforces the statisticsaligned with the true class while penalizing those associated with otherclasses, guided by the 0-1 loss. The proposed method has been empiricallytested on 30 datasets using na\"ive Bayes, quadratic discriminant analysis, andlogistic regression classifiers. RC improves the empirical error of theoriginal closed-form learning algorithms and, more notably, consistentlyoutperforms the gradient descent approach with the three classifiers.</description><author>Aritz Pérez, Carlos Echegoyen, Guzmán Santafé</author><pubDate>Thu, 05 Sep 2024 14:06:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03542v1</guid></item><item><title>GarmentCodeData: A Dataset of 3D Made-to-Measure Garments With Sewing Patterns</title><link>http://arxiv.org/abs/2405.17609v3</link><description>Recent research interest in the learning-based processing of garments, fromvirtual fitting to generation and reconstruction, stumbles on a scarcity ofhigh-quality public data in the domain. We contribute to resolving this need bypresenting the first large-scale synthetic dataset of 3D made-to-measuregarments with sewing patterns, as well as its generation pipeline.GarmentCodeData contains 115,000 data points that cover a variety of designs inmany common garment categories: tops, shirts, dresses, jumpsuits, skirts,pants, etc., fitted to a variety of body shapes sampled from a customstatistical body model based on CAESAR, as well as a standard reference bodyshape, applying three different textile materials. To enable the creation ofdatasets of such complexity, we introduce a set of algorithms for automaticallytaking tailor's measures on sampled body shapes, sampling strategies for sewingpattern design, and propose an automatic, open-source 3D garment drapingpipeline based on a fast XPBD simulator, while contributing several solutionsfor collision resolution and drape correctness to enable scalability. Project Page: https://igl.ethz.ch/projects/GarmentCodeData/</description><author>Maria Korosteleva, Timur Levent Kesdogan, Fabian Kemper, Stephan Wenninger, Jasmin Koller, Yuhan Zhang, Mario Botsch, Olga Sorkine-Hornung</author><pubDate>Thu, 05 Sep 2024 14:00:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17609v3</guid></item><item><title>AI-Driven Intrusion Detection Systems (IDS) on the ROAD Dataset: A Comparative Analysis for Automotive Controller Area Network (CAN)</title><link>http://arxiv.org/abs/2408.17235v2</link><description>The integration of digital devices in modern vehicles has revolutionizedautomotive technology, enhancing safety and the overall driving experience. TheController Area Network (CAN) bus is a central system for managing in-vehiclecommunication between the electronic control units (ECUs). However, the CANprotocol poses security challenges due to inherent vulnerabilities, lackingencryption and authentication, which, combined with an expanding attacksurface, necessitates robust security measures. In response to this challenge,numerous Intrusion Detection Systems (IDS) have been developed and deployed.Nonetheless, an open, comprehensive, and realistic dataset to test theeffectiveness of such IDSs remains absent in the existing literature. Thispaper addresses this gap by considering the latest ROAD dataset, containingstealthy and sophisticated injections. The methodology involves datasetlabelling and the implementation of both state-of-the-art deep learning modelsand traditional machine learning models to show the discrepancy in performancebetween the datasets most commonly used in the literature and the ROAD dataset,a more realistic alternative.</description><author>Lorenzo Guerra, Linhan Xu, Paolo Bellavista, Thomas Chapuis, Guillaume Duc, Pavlo Mozharovskyi, Van-Tam Nguyen</author><pubDate>Thu, 05 Sep 2024 13:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.17235v2</guid></item><item><title>What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to Prompt Engineering</title><link>http://arxiv.org/abs/2406.12334v2</link><description>Large Language Models (LLMs) changed the way we design and interact withsoftware systems. Their ability to process and extract information from texthas drastically improved productivity in a number of routine tasks. Developersthat want to include these models in their software stack, however, face adreadful challenge: debugging LLMs' inconsistent behavior across minorvariations of the prompt. We therefore introduce two metrics for classificationtasks, namely sensitivity and consistency, which are complementary to taskperformance. First, sensitivity measures changes of predictions acrossrephrasings of the prompt, and does not require access to ground truth labels.Instead, consistency measures how predictions vary across rephrasings forelements of the same class. We perform an empirical comparison of these metricson text classification tasks, using them as guideline for understanding failuremodes of the LLM. Our hope is that sensitivity and consistency will be helpfulto guide prompt engineering and obtain LLMs that balance robustness withperformance.</description><author>Federico Errica, Giuseppe Siracusano, Davide Sanvito, Roberto Bifulco</author><pubDate>Thu, 05 Sep 2024 13:47:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12334v2</guid></item><item><title>Use of triplet loss for facial restoration in low-resolution images</title><link>http://arxiv.org/abs/2409.03530v1</link><description>In recent years, facial recognition (FR) models have become the most widelyused biometric tool, achieving impressive results on numerous datasets.However, inherent hardware challenges or shooting distances often result inlow-resolution images, which significantly impact the performance of FR models.To address this issue, several solutions have been proposed, includingsuper-resolution (SR) models that generate highly realistic faces. Despitethese efforts, significant improvements in FR algorithms have not beenachieved. We propose a novel SR model FTLGAN, which focuses on generatinghigh-resolution images that preserve individual identities rather than merelyimproving image quality, thereby maximizing the performance of FR models. Theresults are compelling, demonstrating a mean value of d' 21% above the bestcurrent state-of-the-art models, specifically having a value of d' = 1.099 andAUC = 0.78 for 14x14 pixels, d' = 2.112 and AUC = 0.92 for 28x28 pixels, and d'= 3.049 and AUC = 0.98 for 56x56 pixels. The contributions of this study aresignificant in several key areas. Firstly, a notable improvement in facialrecognition performance has been achieved in low-resolution images,specifically at resolutions of 14x14, 28x28, and 56x56 pixels. Secondly, theenhancements demonstrated by FTLGAN show a consistent response across allresolutions, delivering outstanding performance uniformly, unlike othercomparative models. Thirdly, an innovative approach has been implemented usingtriplet loss logic, enabling the training of the super-resolution model solelywith real images, contrasting with current models, and expanding potentialreal-world applications. Lastly, this study introduces a novel model thatspecifically addresses the challenge of improving classification performance infacial recognition systems by integrating facial recognition quality as a lossduring model training.</description><author>Sebastian Pulgar, Domingo Mery</author><pubDate>Thu, 05 Sep 2024 13:42:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03530v1</guid></item><item><title>Unified Convergence Theory of Stochastic and Variance-Reduced Cubic Newton Methods</title><link>http://arxiv.org/abs/2302.11962v4</link><description>We study stochastic Cubic Newton methods for solving general possiblynon-convex minimization problems. We propose a new framework, which we call thehelper framework, that provides a unified view of the stochastic andvariance-reduced second-order algorithms equipped with global complexityguarantees. It can also be applied to learning with auxiliary information. Ourhelper framework offers the algorithm designer high flexibility forconstructing and analyzing the stochastic Cubic Newton methods, allowingarbitrary size batches, and the use of noisy and possibly biased estimates ofthe gradients and Hessians, incorporating both the variance reduction and thelazy Hessian updates. We recover the best-known complexities for the stochasticand variance-reduced Cubic Newton, under weak assumptions on the noise. Adirect consequence of our theory is the new lazy stochastic second-ordermethod, which significantly improves the arithmetic complexity for largedimension problems. We also establish complexity bounds for the classes ofgradient-dominated objectives, that include convex and strongly convexproblems. For Auxiliary Learning, we show that using a helper (auxiliaryfunction) can outperform training alone if a given similarity measure is small.</description><author>El Mahdi Chayti, Nikita Doikov, Martin Jaggi</author><pubDate>Thu, 05 Sep 2024 13:42:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11962v4</guid></item></channel></rss>