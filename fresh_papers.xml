<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 30 Jun 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Dataset Size Recovery from LoRA Weights</title><link>http://arxiv.org/abs/2406.19395v1</link><description>Model inversion and membership inference attacks aim to reconstruct andverify the data which a model was trained on. However, they are not guaranteedto find all training samples as they do not know the size of the training set.In this paper, we introduce a new task: dataset size recovery, that aims todetermine the number of samples used to train a model, directly from itsweights. We then propose DSiRe, a method for recovering the number of imagesused to fine-tune a model, in the common case where fine-tuning uses LoRA. Wediscover that both the norm and the spectrum of the LoRA matrices are closelylinked to the fine-tuning dataset size; we leverage this finding to propose asimple yet effective prediction algorithm. To evaluate dataset size recovery ofLoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting ofover 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.Our best classifier can predict the number of fine-tuning images with a meanabsolute error of 0.36 images, establishing the feasibility of this attack.</description><author>Mohammad Salama, Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen</author><pubDate>Thu, 27 Jun 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19395v1</guid></item><item><title>HUWSOD: Holistic Self-training for Unified Weakly Supervised Object Detection</title><link>http://arxiv.org/abs/2406.19394v1</link><description>Most WSOD methods rely on traditional object proposals to generate candidateregions and are confronted with unstable training, which easily gets stuck in apoor local optimum. In this paper, we introduce a unified, high-capacity weaklysupervised object detection (WSOD) network called HUWSOD, which utilizes acomprehensive self-training framework without needing external modules oradditional supervision. HUWSOD innovatively incorporates a self-supervisedproposal generator and an autoencoder proposal generator with a multi-rateresampling pyramid to replace traditional object proposals, enabling end-to-endWSOD training and inference. Additionally, we implement a holisticself-training scheme that refines detection scores and coordinates throughstep-wise entropy minimization and consistency-constraint regularization,ensuring consistent predictions across stochastic augmentations of the sameimage. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSODcompetes with state-of-the-art WSOD methods, eliminating the need for offlineproposals and additional data. The peak performance of HUWSOD approaches thatof fully-supervised Faster R-CNN. Our findings also indicate that randomlyinitialized boxes, although significantly different from well-designed offlineobject proposals, are effective for WSOD training.</description><author>Liujuan Cao, Jianghang Lin, Zebo Hong, Yunhang Shen, Shaohui Lin, Chao Chen, Rongrong Ji</author><pubDate>Thu, 27 Jun 2024 18:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19394v1</guid></item><item><title>Looking 3D: Anomaly Detection with 2D-3D Alignment</title><link>http://arxiv.org/abs/2406.19393v1</link><description>Automatic anomaly detection based on visual cues holds practical significancein various domains, such as manufacturing and product quality assessment. Thispaper introduces a new conditional anomaly detection problem, which involvesidentifying anomalies in a query image by comparing it to a reference shape. Toaddress this challenge, we have created a large dataset, BrokenChairs-180K,consisting of around 180K images, with diverse anomalies, geometries, andtextures paired with 8,143 reference 3D shapes. To tackle this task, we haveproposed a novel transformer-based approach that explicitly learns thecorrespondence between the query image and reference 3D shape via featurealignment and leverages a customized attention mechanism for anomaly detection.Our approach has been rigorously evaluated through comprehensive experiments,serving as a benchmark for future research in this domain.</description><author>Ankan Bhunia, Changjian Li, Hakan Bilen</author><pubDate>Thu, 27 Jun 2024 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19393v1</guid></item><item><title>ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos</title><link>http://arxiv.org/abs/2406.19392v1</link><description>We introduce ReXTime, a benchmark designed to rigorously test AI models'ability to perform temporal reasoning within video events. Specifically,ReXTime focuses on reasoning across time, i.e. human-like understanding whenthe question and its corresponding answer occur in different video segments.This form of reasoning, requiring advanced understanding of cause-and-effectrelationships across video segments, poses significant challenges to even thefrontier multimodal large language models. To facilitate this evaluation, wedevelop an automated pipeline for generating temporal reasoning question-answerpairs, significantly reducing the need for labor-intensive manual annotations.Our benchmark includes 921 carefully vetted validation samples and 2,143 testsamples, each manually curated for accuracy and relevance. Evaluation resultsshow that while frontier large language models outperform academic models, theystill lag behind human performance by a significant 14.3% accuracy gap.Additionally, our pipeline creates a training dataset of 9,695 machinegenerated samples without manual effort, which empirical studies suggest canenhance the across-time reasoning via fine-tuning.</description><author>Jr-Jen Chen, Yu-Chien Liao, Hsi-Che Lin, Yu-Chu Yu, Yen-Chun Chen, Yu-Chiang Frank Wang</author><pubDate>Thu, 27 Jun 2024 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19392v1</guid></item><item><title>Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads</title><link>http://arxiv.org/abs/2406.19391v1</link><description>Visual perception tasks are predominantly solved by Vision Transformer (ViT)architectures, which, despite their effectiveness, encounter a computationalbottleneck due to the quadratic complexity of computing self-attention. Thisinefficiency is largely due to the self-attention heads capturing redundanttoken interactions, reflecting inherent redundancy within visual data. Manyworks have aimed to reduce the computational complexity of self-attention inViTs, leading to the development of efficient and sparse transformerarchitectures. In this paper, viewing through the efficiency lens, we realizedthat introducing any sparse self-attention strategy in ViTs can keep thecomputational overhead low. However, these strategies are sub-optimal as theyoften fail to capture fine-grained visual details. This observation leads us topropose a general, efficient, sparse architecture, named Fibottention, forapproximating self-attention with superlinear complexity that is built uponFibonacci sequences. The key strategies in Fibottention include: it excludesproximate tokens to reduce redundancy, employs structured sparsity by design todecrease computational demands, and incorporates inception-like diversityacross attention heads. This diversity ensures the capture of complementaryinformation through non-overlapping token interactions, optimizing bothperformance and resource utilization in ViTs for visual representationlearning. We embed our Fibottention mechanism into multiple state-of-the-arttransformer architectures dedicated to visual tasks. Leveraging only 2-6% ofthe elements in the self-attention heads, Fibottention in conjunction with ViTand its variants, consistently achieves significant performance boosts comparedto standard ViTs in nine datasets across three domains $\unicode{x2013}$ imageclassification, video understanding, and robot learning tasks.</description><author>Ali Khaleghi Rahimian, Manish Kumar Govind, Subhajit Maity, Dominick Reilly, Christian KÃ¼mmerle, Srijan Das, Aritra Dutta</author><pubDate>Thu, 27 Jun 2024 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19391v1</guid></item><item><title>SALVe: Semantic Alignment Verification for Floorplan Reconstruction from Sparse Panoramas</title><link>http://arxiv.org/abs/2406.19390v1</link><description>We propose a new system for automatic 2D floorplan reconstruction that isenabled by SALVe, our novel pairwise learned alignment verifier. The inputs toour system are sparsely located 360$^\circ$ panoramas, whose semantic features(windows, doors, and openings) are inferred and used to hypothesize pairwiseroom adjacency or overlap. SALVe initializes a pose graph, which issubsequently optimized using GTSAM. Once the room poses are computed, roomlayouts are inferred using HorizonNet, and the floorplan is constructed bystitching the most confident layout boundaries. We validate our systemqualitatively and quantitatively as well as through ablation studies, showingthat it outperforms state-of-the-art SfM systems in completeness by over 200%,without sacrificing accuracy. Our results point to the significance of ourwork: poses of 81% of panoramas are localized in the first 2 connectedcomponents (CCs), and 89% in the first 3 CCs. Code and models are publiclyavailable at https://github.com/zillow/salve.</description><author>John Lambert, Yuguang Li, Ivaylo Boyadzhiev, Lambert Wixson, Manjunath Narayana, Will Hutchcroft, James Hays, Frank Dellaert, Sing Bing Kang</author><pubDate>Thu, 27 Jun 2024 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19390v1</guid></item><item><title>OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding</title><link>http://arxiv.org/abs/2406.19389v1</link><description>Current universal segmentation methods demonstrate strong capabilities inpixel-level image and video understanding. However, they lack reasoningabilities and cannot be controlled via text instructions. In contrast, largevision-language multimodal models exhibit powerful vision-based conversationand reasoning capabilities but lack pixel-level understanding and havedifficulty accepting visual prompts for flexible user interaction. This paperproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-levelvision understanding with reasoning abilities. It can accept various visual andtext prompts for flexible user interaction. Specifically, we use a universalsegmentation method as the visual encoder, integrating image information,perception priors, and visual prompts into visual tokens provided to the LLM.The LLM is responsible for understanding the user's text instructions andproviding text responses and pixel-level segmentation results based on thevisual information. We propose perception prior embedding to better integrateperception priors with image features. OMG-LLaVA achieves image-level,object-level, and pixel-level reasoning and understanding in a single model,matching or surpassing the performance of specialized methods on multiplebenchmarks. Rather than using LLM to connect each specialist, our work aims atend-to-end training on one encoder, one decoder, and one LLM. The code andmodel have been released for further research.</description><author>Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, Shuicheng Yan</author><pubDate>Thu, 27 Jun 2024 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19389v1</guid></item><item><title>Taming Data and Transformers for Audio Generation</title><link>http://arxiv.org/abs/2406.19388v1</link><description>Generating ambient sounds and effects is a challenging problem due to datascarcity and often insufficient caption quality, making it difficult to employlarge-scale generative models for the task. In this work, we tackle the problemby introducing two new models. First, we propose AutoCap, a high-quality andefficient automatic audio captioning model. We show that by leveraging metadataavailable with the audio modality, we can substantially improve the quality ofcaptions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement fromthe best available captioning model at four times faster inference speed. Wethen use AutoCap to caption clips from existing datasets, obtaining 761,000audio clips with high-quality captions, forming the largest availableaudio-text dataset. Second, we propose GenAu, a scalable transformer-basedaudio generation architecture that we scale up to 1.25B parameters and trainwith our new dataset. When compared to state-of-the-art audio generators, GenAuobtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%in CLAP score, indicating significantly improved quality of generated audiocompared to previous works. This shows that the quality of data is often asimportant as its quantity. Besides, since AutoCap is fully automatic, new audiosamples can be added to the training dataset, unlocking the training of evenlarger generative models for audio synthesis.</description><author>Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Guha Balakrishnan, Sergey Tulyakov, Vicente Ordonez</author><pubDate>Thu, 27 Jun 2024 18:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19388v1</guid></item><item><title>The Remarkable Robustness of LLMs: Stages of Inference?</title><link>http://arxiv.org/abs/2406.19384v1</link><description>We demonstrate and investigate the remarkable robustness of Large LanguageModels by deleting and swapping adjacent layers. We find that deleting andswapping interventions retain 72-95\% of the original model's predictionaccuracy without fine-tuning, whereas models with more layers exhibit morerobustness. Based on the results of the layer-wise intervention and furtherexperiments, we hypothesize the existence of four universal stages of inferenceacross eight different models: detokenization, feature engineering, predictionensembling, and residual sharpening. The first stage integrates localinformation, lifting raw token representations into higher-level contextualrepresentations. Next is the iterative refinement of task and entity-specificfeatures. Then, the second half of the model begins with a phase transition,where hidden representations align more with the vocabulary space due tospecialized model components. Finally, the last layer sharpens the followingtoken distribution by eliminating obsolete features that add noise to theprediction.</description><author>Vedang Lad, Wes Gurnee, Max Tegmark</author><pubDate>Thu, 27 Jun 2024 18:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19384v1</guid></item><item><title>TabReD: A Benchmark of Tabular Machine Learning in-the-Wild</title><link>http://arxiv.org/abs/2406.19380v1</link><description>Benchmarks that closely reflect downstream application scenarios areessential for the streamlined adoption of new research in tabular machinelearning (ML). In this work, we examine existing tabular benchmarks and findtwo common characteristics of industry-grade tabular data that areunderrepresented in the datasets available to the academic community. First,tabular data often changes over time in real-world deployment scenarios. Thisimpacts model performance and requires time-based train and test splits forcorrect model evaluation. Yet, existing academic tabular datasets often lacktimestamp metadata to enable such evaluation. Second, a considerable portion ofdatasets in production settings stem from extensive data acquisition andfeature engineering pipelines. For each specific dataset, this can have adifferent impact on the absolute and relative number of predictive,uninformative, and correlated features, which in turn can affect modelselection. To fill the aforementioned gaps in academic benchmarks, we introduceTabReD -- a collection of eight industry-grade tabular datasets covering a widerange of domains from finance to food delivery services. We assess a largenumber of tabular ML models in the feature-rich, temporally-evolving datasetting facilitated by TabReD. We demonstrate that evaluation on time-baseddata splits leads to different methods ranking, compared to evaluation onrandom splits more common in academic benchmarks. Furthermore, on the TabReDdatasets, MLP-like architectures and GBDT show the best results, while moresophisticated DL models are yet to prove their effectiveness.</description><author>Ivan Rubachev, Nikolay Kartashev, Yury Gorishniy, Artem Babenko</author><pubDate>Thu, 27 Jun 2024 18:55:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19380v1</guid></item><item><title>Suri: Multi-constraint Instruction Following for Long-form Text Generation</title><link>http://arxiv.org/abs/2406.19371v1</link><description>Existing research on instruction following largely focuses on tasks withsimple instructions and short responses. In this work, we exploremulti-constraint instruction following for generating long-form text. We createSuri, a dataset with 20K human-written long-form texts paired withLLM-generated backtranslated instructions that contain multiple complexconstraints. Because of prohibitive challenges associated with collecting humanpreference judgments on long-form texts, preference-tuning algorithms such asDPO are infeasible in our setting; thus, we propose Instructional ORPO(I-ORPO), an alignment method based on the ORPO algorithm. Instead of receivingnegative feedback from dispreferred responses, I-ORPO obtains negative feedbackfrom synthetically corrupted instructions generated by an LLM. Using Suri, weperform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. Theresulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts(~5K tokens) than base models without significant quality deterioration. Ourhuman evaluation shows that while both SFT and I-ORPO models satisfy mostconstraints, Suri-I-ORPO generations are generally preferred for their coherentand informative incorporation of the constraints. We release our code athttps://github.com/chtmp223/suri.</description><author>Chau Minh Pham, Simeng Sun, Mohit Iyyer</author><pubDate>Thu, 27 Jun 2024 18:50:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19371v1</guid></item><item><title>Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space</title><link>http://arxiv.org/abs/2406.19370v1</link><description>Modern generative models demonstrate impressive capabilities, likely stemmingfrom an ability to identify and manipulate abstract concepts underlying theirtraining data. However, fundamental questions remain: what determines theconcepts a model learns, the order in which it learns them, and its ability tomanipulate those concepts? To address these questions, we propose analyzing amodel's learning dynamics via a framework we call the concept space, where eachaxis represents an independent concept underlying the data generating process.By characterizing learning dynamics in this space, we identify how the speed atwhich a concept is learned, and hence the order of concept learning, iscontrolled by properties of the data we term concept signal. Further, weobserve moments of sudden turns in the direction of a model's learning dynamicsin concept space. Surprisingly, these points precisely correspond to theemergence of hidden capabilities, i.e., where latent interventions show themodel possesses the capability to manipulate a concept, but these capabilitiescannot yet be elicited via naive input prompting. While our results focus onsynthetically defined toy datasets, we hypothesize a general claim on emergenceof hidden capabilities may hold: generative models possess latent capabilitiesthat emerge suddenly and consistently during training, though a model might notexhibit these capabilities under naive input prompting.</description><author>Core Francisco Park, Maya Okawa, Andrew Lee, Ekdeep Singh Lubana, Hidenori Tanaka</author><pubDate>Thu, 27 Jun 2024 18:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19370v1</guid></item><item><title>Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment Anything Model</title><link>http://arxiv.org/abs/2406.19369v1</link><description>Transformer-based segmentation methods face the challenge of efficientinference when dealing with high-resolution images. Recently, several linearattention architectures, such as Mamba and RWKV, have attracted much attentionas they can process long sequences efficiently. In this work, we focus ondesigning an efficient segment-anything model by exploring these differentarchitectures. Specifically, we design a mixed backbone that containsconvolution and RWKV operation, which achieves the best for both accuracy andefficiency. In addition, we design an efficient decoder to utilize themultiscale tokens to obtain high-quality masks. We denote our method asRWKV-SAM, a simple, effective, fast baseline for SAM-like models. Moreover, webuild a benchmark containing various high-quality segmentation datasets andjointly train one efficient yet high-quality segmentation model using thisbenchmark. Based on the benchmark results, our RWKV-SAM achieves outstandingperformance in efficiency and segmentation quality compared to transformers andother linear attention models. For example, compared with the same-scaletransformer model, RWKV-SAM achieves more than 2x speedup and can achievebetter segmentation performance on various datasets. In addition, RWKV-SAMoutperforms recent vision Mamba models with better classification and semanticsegmentation results. Code and models will be publicly available.</description><author>Haobo Yuan, Xiangtai Li, Lu Qi, Tao Zhang, Ming-Hsuan Yang, Shuicheng Yan, Chen Change Loy</author><pubDate>Thu, 27 Jun 2024 18:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19369v1</guid></item><item><title>SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text Cues</title><link>http://arxiv.org/abs/2406.19364v1</link><description>Weakly-supervised medical image segmentation is a challenging task that aimsto reduce the annotation cost while keep the segmentation performance. In thispaper, we present a novel framework, SimTxtSeg, that leverages simple text cuesto generate high-quality pseudo-labels and study the cross-modal fusion intraining segmentation models, simultaneously. Our contribution consists of twokey components: an effective Textual-to-Visual Cue Converter that producesvisual prompts from text prompts on medical images, and a text-guidedsegmentation model with Text-Vision Hybrid Attention that fuses text and imagefeatures. We evaluate our framework on two medical image segmentation tasks:colonic polyp segmentation and MRI brain tumor segmentation, and achieveconsistent state-of-the-art performance.</description><author>Yuxin Xie, Tao Zhou, Yi Zhou, Geng Chen</author><pubDate>Thu, 27 Jun 2024 18:46:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19364v1</guid></item><item><title>STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via Collaborating Self-Training and Adversarial Learning</title><link>http://arxiv.org/abs/2406.19362v1</link><description>Existing 3D object detection suffers from expensive annotation costs and poortransferability to unknown data due to the domain gap, Unsupervised DomainAdaptation (UDA) aims to generalize detection models trained in labeled sourcedomains to perform robustly on unexplored target domains, providing a promisingsolution for cross-domain 3D object detection. Although Self-Training (ST)based cross-domain 3D detection methods with the assistance of pseudo-labelingtechniques have achieved remarkable progress, they still face the issue oflow-quality pseudo-labels when there are significant domain disparities due tothe absence of a process for feature distribution alignment. While AdversarialLearning (AL) based methods can effectively align the feature distributions ofthe source and target domains, the inability to obtain labels in the targetdomain forces the adoption of asymmetric optimization losses, resulting in achallenging issue of source domain bias. To overcome these limitations, wepropose a novel unsupervised domain adaptation framework for 3D objectdetection via collaborating ST and AL, dubbed as STAL3D, unleashing thecomplementary advantages of pseudo labels and feature distribution alignment.Additionally, a Background Suppression Adversarial Learning (BS-AL) module anda Scale Filtering Module (SFM) are designed tailored for 3D cross-domainscenes, effectively alleviating the issues of the large proportion ofbackground interference and source domain size bias. Our STAL3D achievesstate-of-the-art performance on multiple cross-domain tasks and even surpassesthe Oracle results on Waymo $\rightarrow$ KITTI and Waymo $\rightarrow$KITTI-rain.</description><author>Yanan Zhang, Chao Zhou, Di Huang</author><pubDate>Thu, 27 Jun 2024 18:43:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19362v1</guid></item><item><title>The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models</title><link>http://arxiv.org/abs/2406.19358v1</link><description>Sentiment analysis serves as a pivotal component in Natural LanguageProcessing (NLP). Advancements in multilingual pre-trained models such as XLM-Rand mT5 have contributed to the increasing interest in cross-lingual sentimentanalysis. The recent emergence in Large Language Models (LLM) has significantlyadvanced general NLP tasks, however, the capability of such LLMs incross-lingual sentiment analysis has not been fully studied. This workundertakes an empirical analysis to compare the cross-lingual transfercapability of public Small Multilingual Language Models (SMLM) like XLM-R,against English-centric LLMs such as Llama-3, in the context of sentimentanalysis across English, Spanish, French and Chinese. Our findings reveal thatamong public models, SMLMs exhibit superior zero-shot cross-lingual performancerelative to LLMs. However, in few-shot cross-lingual settings, public LLMsdemonstrate an enhanced adaptive potential. In addition, we observe thatproprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, butare outpaced by public models in few-shot scenarios.</description><author>Xiliang Zhu, Shayna Gardiner, Tere RoldÃ¡n, David Rossouw</author><pubDate>Thu, 27 Jun 2024 18:38:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19358v1</guid></item><item><title>Submodular Information Selection for Hypothesis Testing with Misclassification Penalties</title><link>http://arxiv.org/abs/2405.10930v2</link><description>We consider the problem of selecting an optimal subset of information sourcesfor a hypothesis testing/classification task where the goal is to identify thetrue state of the world from a finite set of hypotheses, based on finiteobservation samples from the sources. In order to characterize the learningperformance, we propose a misclassification penalty framework, which enablesnon-uniform treatment of different misclassification errors. In a centralizedBayesian learning setting, we study two variants of the subset selectionproblem: (i) selecting a minimum cost information set to ensure that themaximum penalty of misclassifying the true hypothesis remains bounded and (ii)selecting an optimal information set under a limited budget to minimize themaximum penalty of misclassifying the true hypothesis. Under certainassumptions, we prove that the objective (or constraints) of thesecombinatorial optimization problems are weak (or approximate) submodular, andestablish high-probability performance guarantees for greedy algorithms.Further, we propose an alternate metric for information set selection which isbased on the total penalty of misclassification. We prove that this metric issubmodular and establish near-optimal guarantees for the greedy algorithms forboth the information set selection problems. Finally, we present numericalsimulations to validate our theoretical results over several randomly generatedinstances.</description><author>Jayanth Bhargav, Mahsa Ghasemi, Shreyas Sundaram</author><pubDate>Thu, 27 Jun 2024 18:38:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10930v2</guid></item><item><title>DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions</title><link>http://arxiv.org/abs/2406.19356v1</link><description>High-quality distractors are crucial to both the assessment and pedagogicalvalue of multiple-choice questions (MCQs), where manually crafting ones thatanticipate knowledge deficiencies or misconceptions among real students isdifficult. Meanwhile, automated distractor generation, even with the help oflarge language models (LLMs), remains challenging for subjects like math. It iscrucial to not only identify plausible distractors but also understand theerror behind them. In this paper, we introduce DiVERT (Distractor Generationwith Variational Errors Represented as Text), a novel variational approach thatlearns an interpretable representation of errors behind distractors in mathMCQs. Through experiments on a real-world math MCQ dataset with 1,434 questionsused by hundreds of thousands of students, we show that DiVERT, despite using abase open-source LLM with 7B parameters, outperforms state-of-the-artapproaches using GPT-4o on downstream distractor generation. We also conduct ahuman evaluation with math educators and find that DiVERT leads to error labelsthat are of comparable quality to human-authored ones.</description><author>Nigel Fernandez, Alexander Scarlatos, Simon Woodhead, Andrew Lan</author><pubDate>Thu, 27 Jun 2024 18:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19356v1</guid></item><item><title>Towards Semantic Equivalence of Tokenization in Multimodal LLM</title><link>http://arxiv.org/abs/2406.05127v2</link><description>Multimodal Large Language Models (MLLMs) have demonstrated exceptionalcapabilities in processing vision-language tasks. One of the crux of MLLMs liesin vision tokenization, which involves efficiently transforming input visualsignals into feature representations that are most beneficial for LLMs.However, existing vision tokenizers, essential for semantic alignment betweenvision and language, remain problematic. Existing methods aggressively fragmentvisual input, corrupting the visual semantic integrity. To address this, thispaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),which groups visual features into semantic units via a dynamic clusteringalgorithm, flexibly determining the number of tokens based on image complexity.The resulting vision tokens effectively preserve semantic integrity and captureboth low-frequency and high-frequency visual features. The proposed MLLM(Setokim) equipped with SeTok significantly demonstrates superior performanceacross various tasks, as evidenced by our experimental results. The projectpage is at https://chocowu.github.io/SeTok-web/.</description><author>Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, Shuicheng Yan</author><pubDate>Thu, 27 Jun 2024 18:35:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05127v2</guid></item><item><title>Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?</title><link>http://arxiv.org/abs/2406.19354v1</link><description>The model editing problem concerns how language models should learn new factsabout the world over time. While empirical research on model editing has drawnwidespread attention, the conceptual foundations of model editing remain shaky-- perhaps unsurprisingly, since model editing is essentially belief revision,a storied problem in philosophy that has eluded succinct solutions for decades.Model editing nonetheless demands a solution, since we need to be able tocontrol the knowledge within language models. With this goal in mind, thispaper critiques the standard formulation of the model editing problem andproposes a formal testbed for model editing research. We first describe 12 openproblems with model editing, based on challenges with (1) defining the problem,(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in thefirst place. Many of these challenges are extremely difficult to address, e.g.determining far-reaching consequences of edits, labeling probabilisticentailments between facts, and updating beliefs of agent simulators. Next, weintroduce a semi-synthetic dataset for model editing based on Wikidata, wherewe can evaluate edits against labels given by an idealized Bayesian agent. Thisenables us to say exactly how belief revision in language models falls short ofa desirable epistemic standard. We encourage further research exploringsettings where such a gold standard can be compared against. Our code ispublicly available at: https://github.com/peterbhase/LLM-belief-revision</description><author>Peter Hase, Thomas Hofweber, Xiang Zhou, Elias Stengel-Eskin, Mohit Bansal</author><pubDate>Thu, 27 Jun 2024 18:33:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19354v1</guid></item><item><title>CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement</title><link>http://arxiv.org/abs/2406.19353v1</link><description>Understanding how humans cooperatively rearrange household objects iscritical for VR/AR and human-robot interaction. However, in-depth studies onmodeling these behaviors are under-researched due to the lack of relevantdatasets. We fill this gap by presenting CORE4D, a novel large-scale 4Dhuman-object-human interaction dataset focusing on collaborative objectrearrangement, which encompasses diverse compositions of various objectgeometries, collaboration modes, and 3D scenes. With 1K human-object-humanmotion sequences captured in the real world, we enrich CORE4D by contributingan iterative collaboration retargeting strategy to augment motions to a varietyof novel objects. Leveraging this approach, CORE4D comprises a total of 11Kcollaboration sequences spanning 3K real and virtual object shapes. Benefitingfrom extensive motion patterns provided by CORE4D, we benchmark two tasksaiming at generating human-object interaction: human-object motion forecastingand interaction synthesis. Extensive experiments demonstrate the effectivenessof our collaboration retargeting strategy and indicate that CORE4D has posednew challenges to existing human-object interaction generation methodologies.Our dataset and code are available athttps://github.com/leolyliu/CORE4D-Instructions.</description><author>Chengwen Zhang, Yun Liu, Ruofan Xing, Bingda Tang, Li Yi</author><pubDate>Thu, 27 Jun 2024 18:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19353v1</guid></item><item><title>Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping</title><link>http://arxiv.org/abs/2403.13040v2</link><description>Intraventricular vector flow mapping (iVFM) seeks to enhance and quantifycolor Doppler in cardiac imaging. In this study, we propose novel alternativesto the traditional iVFM optimization scheme by utilizing physics-informedneural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.When evaluated on simulated color Doppler images derived from apatient-specific computational fluid dynamics model and in vivo Doppleracquisitions, both approaches demonstrate comparable reconstruction performanceto the original iVFM algorithm. The efficiency of PINNs is boosted throughdual-stage optimization and pre-optimized weights. On the other hand, thennU-Net method excels in generalizability and real-time capabilities. Notably,nnU-Net shows superior robustness on sparse and truncated Doppler data whilemaintaining independence from explicit boundary conditions. Overall, ourresults highlight the effectiveness of these methods in reconstructingintraventricular vector blood flow. The study also suggests potentialapplications of PINNs in ultrafast color Doppler imaging and the incorporationof fluid dynamics equations to derive biomarkers for cardiovascular diseasesbased on blood flow.</description><author>Hang Jung Ling, SalomÃ© Bru, Julia Puig, Florian VixÃ¨ge, Simon Mendez, Franck Nicoud, Pierre-Yves Courand, Olivier Bernard, Damien Garcia</author><pubDate>Thu, 27 Jun 2024 18:27:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13040v2</guid></item><item><title>IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language</title><link>http://arxiv.org/abs/2406.19349v1</link><description>Hate speech poses a significant threat to social harmony. Over the past twoyears, Indonesia has seen a ten-fold increase in the online hate speech ratio,underscoring the urgent need for effective detection mechanisms. However,progress is hindered by the limited availability of labeled data for Indonesiantexts. The condition is even worse for marginalized minorities, such as Shia,LGBTQ, and other ethnic minorities because hate speech is underreported andless understood by detection tools. Furthermore, the lack of accommodation forsubjectivity in current datasets compounds this issue. To address this, weintroduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicityclassification dataset. Comprising 43,692 entries annotated by 19 diverseindividuals, the dataset focuses on texts targeting vulnerable groups inIndonesia, specifically during the hottest political event in the country: thepresidential election. We establish baselines for seven binary classificationtasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)fine-tuned for hate speech classification. Furthermore, we demonstrate howincorporating demographic information can enhance the zero-shot performance ofthe large language model, gpt-3.5-turbo. However, we also caution that anoveremphasis on demographic information can negatively impact the fine-tunedmodel performance due to data fragmentation.</description><author>Lucky Susanto, Musa Izzanardi Wijanarko, Prasetia Anugrah Pratama, Traci Hong, Ika Idris, Alham Fikri Aji, Derry Wijaya</author><pubDate>Thu, 27 Jun 2024 18:26:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19349v1</guid></item><item><title>Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications</title><link>http://arxiv.org/abs/2402.05162v2</link><description>Large language models (LLMs) show inherent brittleness in their safetymechanisms, as evidenced by their susceptibility to jailbreaking and evennon-malicious fine-tuning. This study explores this brittleness of safetyalignment by leveraging pruning and low-rank modifications. We develop methodsto identify critical regions that are vital for safety guardrails, and that aredisentangled from utility-relevant regions at both the neuron and rank levels.Surprisingly, the isolated regions we find are sparse, comprising about $3\%$at the parameter level and $2.5\%$ at the rank level. Removing these regionscompromises safety without significantly impacting utility, corroborating theinherent brittleness of the model's safety mechanisms. Moreover, we show thatLLMs remain vulnerable to low-cost fine-tuning attacks even when modificationsto the safety-critical regions are restricted. These findings underscore theurgent need for more robust safety strategies in LLMs.</description><author>Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, Peter Henderson</author><pubDate>Thu, 27 Jun 2024 18:23:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05162v2</guid></item><item><title>Learning Visual Conditioning Tokens to Correct Domain Shift for Fully Test-time Adaptation</title><link>http://arxiv.org/abs/2406.19341v1</link><description>Fully test-time adaptation aims to adapt the network model based onsequential analysis of input samples during the inference stage to address thecross-domain performance degradation problem of deep neural networks. This workis based on the following interesting finding: in transformer-based imageclassification, the class token at the first transformer encoder layer can belearned to capture the domain-specific characteristics of target samples duringtest-time adaptation. This learned token, when combined with input image patchembeddings, is able to gradually remove the domain-specific information fromthe feature representations of input samples during the transformer encodingprocess, thereby significantly improving the test-time adaptation performanceof the source model across different domains. We refer to this class token asvisual conditioning token (VCT). To successfully learn the VCT, we propose abi-level learning approach to capture the long-term variations ofdomain-specific characteristics while accommodating local variations ofinstance-specific characteristics. Experimental results on the benchmarkdatasets demonstrate that our proposed bi-level visual conditioning tokenlearning method is able to achieve significantly improved test-time adaptationperformance by up to 1.9%.</description><author>Yushun Tang, Shuoshuo Chen, Zhehan Kan, Yi Zhang, Qinghai Guo, Zhihai He</author><pubDate>Thu, 27 Jun 2024 18:16:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19341v1</guid></item><item><title>CHESS: Contextual Harnessing for Efficient SQL Synthesis</title><link>http://arxiv.org/abs/2405.16755v2</link><description>Utilizing large language models (LLMs) for transforming natural languagequestions into SQL queries (text-to-SQL) is a promising yet challengingapproach, particularly when applied to real-world databases with complex andextensive schemas. In particular, effectively incorporating data catalogs anddatabase values for SQL generation remains an obstacle, leading to suboptimalsolutions. We address this problem by proposing a new pipeline that effectivelyretrieves relevant data and context, selects an efficient schema, andsynthesizes correct and efficient SQL queries. To increase retrieval precision,our pipeline introduces a hierarchical retrieval method leveragingmodel-generated keywords, locality-sensitive hashing indexing, and vectordatabases. Additionally, we have developed an adaptive schema pruning techniquethat adjusts based on the complexity of the problem and the model's contextsize. Our approach generalizes to both frontier proprietary models like GPT-4and open-source models such as Llama-3-70B. Through a series of ablationstudies, we demonstrate the effectiveness of each component of our pipeline andits impact on the end-to-end performance. Our method achieves newstate-of-the-art performance on the cross-domain challenging BIRD dataset.</description><author>Shayan Talaei, Mohammadreza Pourreza, Yu-Chen Chang, Azalia Mirhoseini, Amin Saberi</author><pubDate>Thu, 27 Jun 2024 18:13:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16755v2</guid></item><item><title>LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver with a Few Partial Ultrasound Scans</title><link>http://arxiv.org/abs/2406.19336v1</link><description>3D reconstruction of the liver for volumetry is important for qualitativeanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,although advantageous due to less acquisition time and safety, is challengingdue to the inherent noisiness in US scans, blurry boundaries, and partial livervisibility. We address these challenges by using the segmentation masks of afew incomplete sagittal-plane US scans of the liver in conjunction with astatistical shape model (SSM) built using a set of CT scans of the liver. Wecompute the shape parameters needed to warp this canonical SSM to fit the USscans through a parametric regression network. The resulting 3D liverreconstruction is accurate and leads to automatic liver volume calculation. Weevaluate the accuracy of the estimated liver volumes with respect to CTsegmentation volumes using RMSE. Our volume computation is statistically muchcloser to the volume estimated using CT scans than the volume computed usingChilds' method by radiologists: p-value of 0.094 (&gt;0.05) says that there is nosignificant difference between CT segmentation volumes and ours in contrast toChilds' method. We validate our method using investigations (ablation studies)on the US image resolution, the number of CT scans used for SSM, the number ofprincipal components, and the number of input US scans. To the best of ourknowledge, this is the first automatic liver volumetry system using a fewincomplete US scans given a set of CT scans of livers for SSM.</description><author>Kaushalya Sivayogaraj, Sahan T. Guruge, Udari Liyanage, Jeevani Udupihille, Saroj Jayasinghe, Gerard Fernando, Ranga Rodrigo, M. Rukshani Liyanaarachchi</author><pubDate>Thu, 27 Jun 2024 18:10:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19336v1</guid></item><item><title>VDebugger: Harnessing Execution Feedback for Debugging Visual Programs</title><link>http://arxiv.org/abs/2406.13444v2</link><description>Visual programs are executable code generated by large language models toaddress visual reasoning problems. They decompose complex questions intomultiple reasoning steps and invoke specialized models for each step to solvethe problems. However, these programs are prone to logic errors, with ourpreliminary evaluation showing that 58% of the total errors are caused byprogram logic errors. Debugging complex visual programs remains a majorbottleneck for visual reasoning. To address this, we introduce VDebugger, anovel critic-refiner framework trained to localize and debug visual programs bytracking execution step by step. VDebugger identifies and corrects programerrors leveraging detailed execution feedback, improving interpretability andaccuracy. The training data is generated through an automated pipeline thatinjects errors into correct visual programs using a novel mask-best decodingtechnique. Evaluations on six datasets demonstrate VDebugger's effectiveness,showing performance improvements of up to 3.2% in downstream task accuracy.Further studies show VDebugger's ability to generalize to unseen tasks,bringing a notable improvement of 2.3% on the unseen COVR task. Code, data andmodels are made publicly available at https://github.com/shirley-wu/vdebugger/</description><author>Xueqing Wu, Zongyu Lin, Songyan Zhao, Te-Lin Wu, Pan Lu, Nanyun Peng, Kai-Wei Chang</author><pubDate>Thu, 27 Jun 2024 18:09:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13444v2</guid></item><item><title>Subtractive Training for Music Stem Insertion using Latent Diffusion Models</title><link>http://arxiv.org/abs/2406.19328v1</link><description>We present Subtractive Training, a simple and novel method for synthesizingindividual musical instrument stems given other instruments as context. Thismethod pairs a dataset of complete music mixes with 1) a variant of the datasetlacking a specific stem, and 2) LLM-generated instructions describing how themissing stem should be reintroduced. We then fine-tune a pretrainedtext-to-audio diffusion model to generate the missing instrument stem, guidedby both the existing stems and the text instruction. Our results demonstrateSubtractive Training's efficacy in creating authentic drum stems thatseamlessly blend with the existing tracks. We also show that we can use thetext instruction to control the generation of the inserted stem in terms ofrhythm, dynamics, and genre, allowing us to modify the style of a singleinstrument in a full song while keeping the remaining instruments the same.Lastly, we extend this technique to MIDI formats, successfully generatingcompatible bass, drum, and guitar parts for incomplete arrangements.</description><author>Ivan Villa-Renteria, Mason L. Wang, Zachary Shah, Zhe Li, Soohyun Kim, Neelesh Ramachandran, Mert Pilanci</author><pubDate>Thu, 27 Jun 2024 17:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19328v1</guid></item><item><title>WebCanvas: Benchmarking Web Agents in Online Environments</title><link>http://arxiv.org/abs/2406.12373v2</link><description>For web agents to be practically useful, they must adapt to the continuouslyevolving web environment characterized by frequent updates to user interfacesand content. However, most existing benchmarks only capture the static aspectsof the web. To bridge this gap, we introduce WebCanvas, an innovative onlineevaluation framework for web agents that effectively addresses the dynamicnature of web interactions. WebCanvas contains three main components tofacilitate realistic assessments: (1) A novel evaluation metric which reliablycapture critical intermediate actions or states necessary for task completionswhile disregarding noise caused by insignificant events or changedweb-elements. (2) A benchmark dataset called Mind2Web-Live, a refined versionof original Mind2Web static dataset containing 542 tasks with 2439 intermediateevaluation states; (3) Lightweight and generalizable annotation tools andtesting pipelines that enables the community to collect and maintain thehigh-quality, up-to-date dataset. Building on WebCanvas, we open-source anagent framework with extensible modules for reasoning, providing a foundationfor the community to conduct online inference and evaluations. Ourbest-performing agent achieves a task success rate of 23.1% and a taskcompletion rate of 48.8% on the Mind2Web-Live test set. Additionally, weanalyze the performance discrepancies across various websites, domains, andexperimental environments. We encourage the community to contribute furtherinsights on online agent evaluation, thereby advancing this field of research.</description><author>Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, Zhengyang Wu</author><pubDate>Thu, 27 Jun 2024 17:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12373v2</guid></item><item><title>Efficient World Models with Context-Aware Tokenization</title><link>http://arxiv.org/abs/2406.19320v1</link><description>Scaling up deep Reinforcement Learning (RL) methods presents a significantchallenge. Following developments in generative modelling, model-based RLpositions itself as a strong contender. Recent advances in sequence modellinghave led to effective transformer-based world models, albeit at the price ofheavy computations due to the long sequences of tokens required to accuratelysimulate environments. In this work, we propose $\Delta$-IRIS, a new agent witha world model architecture composed of a discrete autoencoder that encodesstochastic deltas between time steps and an autoregressive transformer thatpredicts future deltas by summarizing the current state of the world withcontinuous tokens. In the Crafter benchmark, $\Delta$-IRIS sets a new state ofthe art at multiple frame budgets, while being an order of magnitude faster totrain than previous attention-based approaches. We release our code and modelsat https://github.com/vmicheli/delta-iris.</description><author>Vincent Micheli, Eloi Alonso, FranÃ§ois Fleuret</author><pubDate>Thu, 27 Jun 2024 17:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19320v1</guid></item><item><title>Jump Starting Bandits with LLM-Generated Prior Knowledge</title><link>http://arxiv.org/abs/2406.19317v1</link><description>We present substantial evidence demonstrating the benefits of integratingLarge Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.Contextual bandits have been widely used in recommendation systems to generatepersonalized suggestions based on user-specific contexts. We show that LLMs,pre-trained on extensive corpora rich in human knowledge and preferences, cansimulate human behaviours well enough to jump-start contextual multi-armedbandits to reduce online learning regret. We propose an initializationalgorithm for contextual bandits by prompting LLMs to produce a pre-trainingdataset of approximate human preferences for the bandit. This significantlyreduces online learning regret and data-gathering costs for training suchmodels. Our approach is validated empirically through two sets of experimentswith different bandit setups: one which utilizes LLMs to serve as an oracle anda real-world experiment utilizing data from a conjoint survey experiment.</description><author>Parand A. Alamdari, Yanshuai Cao, Kevin H. Wilson</author><pubDate>Thu, 27 Jun 2024 17:52:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19317v1</guid></item><item><title>Enhanced Data Transfer Cooperating with Artificial Triplets for Scene Graph Generation</title><link>http://arxiv.org/abs/2406.19316v1</link><description>This work focuses on training dataset enhancement of informative relationaltriplets for Scene Graph Generation (SGG). Due to the lack of effectivesupervision, the current SGG model predictions perform poorly for informativerelational triplets with inadequate training samples. Therefore, we propose twonovel training dataset enhancement modules: Feature Space Triplet Augmentation(FSTA) and Soft Transfer. FSTA leverages a feature generator trained togenerate representations of an object in relational triplets. The biasedprediction based sampling in FSTA efficiently augments artificial tripletsfocusing on the challenging ones. In addition, we introduce Soft Transfer,which assigns soft predicate labels to general relational triplets to make moresupervisions for informative predicate classes effectively. Experimentalresults show that integrating FSTA and Soft Transfer achieve high levels ofboth Recall and mean Recall in Visual Genome dataset. The mean of Recall andmean Recall is the highest among all the existing model-agnostic methods.</description><author>KuanChao Chu, Satoshi Yamazaki, Hideki Nakayama</author><pubDate>Thu, 27 Jun 2024 17:52:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19316v1</guid></item><item><title>GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism</title><link>http://arxiv.org/abs/2303.13775v2</link><description>Graph neural networks (GNNs), an emerging class of machine learning modelsfor graphs, have gained popularity for their superior performance in variousgraph analytical tasks. Mini-batch training is commonly used to train GNNs onlarge graphs, and data parallelism is the standard approach to scale mini-batchtraining across multiple GPUs. One of the major performance costs in GNNtraining is the loading of input features, which prevents GPUs from being fullyutilized. In this paper, we argue that this problem is exacerbated byredundancies that are inherent to the data parallel approach. To address thisissue, we introduce a hybrid parallel mini-batch training paradigm called splitparallelism. Split parallelism avoids redundant data loads and splits thesampling and training of each mini-batch across multiple GPUs online, at eachiteration, using a lightweight splitting algorithm. We implement splitparallelism in GSplit and show that it outperforms state-of-the-art mini-batchtraining systems like DGL, Quiver, and $P^3$.</description><author>Sandeep Polisetty, Juelin Liu, Kobi Falus, Yi Ren Fung, Seung-Hwan Lim, Hui Guan, Marco Serafini</author><pubDate>Thu, 27 Jun 2024 17:51:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13775v2</guid></item><item><title>LiveBench: A Challenging, Contamination-Free LLM Benchmark</title><link>http://arxiv.org/abs/2406.19314v1</link><description>Test set contamination, wherein test data from a benchmark ends up in a newermodel's training set, is a well-documented obstacle for fair LLM evaluation andcan quickly render benchmarks obsolete. To mitigate this, many recentbenchmarks crowdsource new prompts and evaluations from human or LLM judges;however, these can introduce significant biases, and break down when scoringhard questions. In this work, we introduce a new benchmark for LLMs designed tobe immune to both test set contamination and the pitfalls of LLM judging andhuman crowdsourcing. We release LiveBench, the first benchmark that (1)contains frequently-updated questions from recent information sources, (2)scores answers automatically according to objective ground-truth values, and(3) contains a wide variety of challenging tasks, spanning math, coding,reasoning, language, instruction following, and data analysis. To achieve this,LiveBench contains questions that are based on recently-released mathcompetitions, arXiv papers, news articles, and datasets, and it containsharder, contamination-free versions of tasks from previous benchmarks such asBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-sourcemodels, as well as dozens of open-source models ranging from 0.5B to 110B insize. LiveBench is difficult, with top models achieving below 65% accuracy. Werelease all questions, code, and model answers. Questions will be added andupdated on a monthly basis, and we will release new tasks and harder versionsof tasks over time so that LiveBench can distinguish between the capabilitiesof LLMs as they improve in the future. We welcome community engagement andcollaboration for expanding the benchmark tasks and models.</description><author>Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, Micah Goldblum</author><pubDate>Thu, 27 Jun 2024 17:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19314v1</guid></item><item><title>Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping</title><link>http://arxiv.org/abs/2402.07610v3</link><description>Self-alignment is an effective way to reduce the cost of human annotationwhile ensuring promising model capability. However, most current methodscomplete the data collection and training steps in a single round, which mayoverlook the continuously improving ability of self-aligned models. This givesrise to a key query: What if we do multi-time bootstrapping self-alignment?Does this strategy enhance model performance or lead to rapid degradation? Inthis paper, our pioneering exploration delves into the impact of bootstrappingself-alignment on large language models. Our findings reveal that bootstrappingself-alignment markedly surpasses the single-round approach, by guaranteeingdata diversity from in-context learning. To further exploit the capabilities ofbootstrapping, we investigate and adjust the training order of data, whichyields improved performance of the model. Drawing on these findings, we proposeStep-On-Feet Tuning (SOFT) which leverages model's continuously enhancedfew-shot ability to boost zero or one-shot performance. Based on easy-to-hardtraining recipe, we propose SOFT+ which further boost self-alignment'sperformance. Our experiments demonstrate the efficiency of SOFT (SOFT+) acrossvarious classification and generation tasks, highlighting the potential ofbootstrapping self-alignment on continually enhancing model alignmentperformance.</description><author>Haoyu Wang, Guozheng Ma, Ziqiao Meng, Zeyu Qin, Li Shen, Zhong Zhang, Bingzhe Wu, Liu Liu, Yatao Bian, Tingyang Xu, Xueqian Wang, Peilin Zhao</author><pubDate>Thu, 27 Jun 2024 17:38:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07610v3</guid></item><item><title>$Î¼$GUIDE: a framework for quantitative imaging via generalized uncertainty-driven inference using deep learning</title><link>http://arxiv.org/abs/2312.17293v3</link><description>This work proposes $\mu$GUIDE: a general Bayesian framework to estimateposterior distributions of tissue microstructure parameters from any givenbiophysical model or MRI signal representation, with exemplar demonstration indiffusion-weighted MRI. Harnessing a new deep learning architecture forautomatic signal feature selection combined with simulation-based inference andefficient sampling of the posterior distributions, $\mu$GUIDE bypasses the highcomputational and time cost of conventional Bayesian approaches and does notrely on acquisition constraints to define model-specific summary statistics.The obtained posterior distributions allow to highlight degeneracies present inthe model definition and quantify the uncertainty and ambiguity of theestimated parameters.</description><author>MaÃ«liss Jallais, Marco Palombo</author><pubDate>Thu, 27 Jun 2024 17:38:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17293v3</guid></item><item><title>The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning</title><link>http://arxiv.org/abs/2406.19307v1</link><description>Understanding commonsense causality is a unique mark of intelligence forhumans. It helps people understand the principles of the real world better andbenefits the decision-making process related to causation. For instance,commonsense causality is crucial in judging whether a defendant's action causesthe plaintiff's loss in determining legal liability. Despite its significance,a systematic exploration of this topic is notably lacking. Our comprehensivesurvey bridges this gap by focusing on taxonomies, benchmarks, acquisitionmethods, qualitative reasoning, and quantitative measurements in commonsensecausality, synthesizing insights from over 200 representative articles. Ourwork aims to provide a systematic overview, update scholars on recentadvancements, provide a pragmatic guide for beginners, and highlight promisingfuture research directions in this vital field.</description><author>Shaobo Cui, Zhijing Jin, Bernhard SchÃ¶lkopf, Boi Faltings</author><pubDate>Thu, 27 Jun 2024 17:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19307v1</guid></item><item><title>SpatialBot: Precise Spatial Understanding with Vision Language Models</title><link>http://arxiv.org/abs/2406.13642v2</link><description>Vision Language Models (VLMs) have achieved impressive performance in 2Dimage understanding, however they are still struggling with spatialunderstanding which is the foundation of Embodied AI. In this paper, we proposeSpatialBot for better spatial understanding by feeding both RGB and depthimages. Additionally, we have constructed the SpatialQA dataset, which involvesmulti-level depth-related questions to train VLMs for depth understanding.Finally, we present SpatialBench to comprehensively evaluate VLMs' capabilitiesin spatial understanding at different levels. Extensive experiments on ourspatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks,demonstrate the remarkable improvements of SpatialBot trained on SpatialQA. Themodel, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.</description><author>Wenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, Bo Zhao</author><pubDate>Thu, 27 Jun 2024 17:30:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13642v2</guid></item><item><title>Thermometer: Towards Universal Calibration for Large Language Models</title><link>http://arxiv.org/abs/2403.08819v2</link><description>We consider the issue of calibration in large language models (LLM). Recentstudies have found that common interventions such as instruction tuning oftenresult in poorly calibrated LLMs. Although calibration is well-explored intraditional applications, calibrating LLMs is uniquely challenging. Thesechallenges stem as much from the severe computational requirements of LLMs asfrom their versatility, which allows them to be applied to diverse tasks.Addressing these challenges, we propose THERMOMETER, a calibration approachtailored to LLMs. THERMOMETER learns an auxiliary model, given data frommultiple tasks, for calibrating a LLM. It is computationally efficient,preserves the accuracy of the LLM, and produces better-calibrated responses fornew tasks. Extensive empirical evaluations across various benchmarksdemonstrate the effectiveness of the proposed method.</description><author>Maohao Shen, Subhro Das, Kristjan Greenewald, Prasanna Sattigeri, Gregory Wornell, Soumya Ghosh</author><pubDate>Thu, 27 Jun 2024 17:30:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08819v2</guid></item><item><title>Mapping Land Naturalness from Sentinel-2 using Deep Contextual and Geographical Priors</title><link>http://arxiv.org/abs/2406.19302v1</link><description>In recent decades, the causes and consequences of climate change haveaccelerated, affecting our planet on an unprecedented scale. This change isclosely tied to the ways in which humans alter their surroundings. As ouractions continue to impact natural areas, using satellite images to observe andmeasure these effects has become crucial for understanding and combatingclimate change. Aiming to map land naturalness on the continuum of modern humanpressure, we have developed a multi-modal supervised deep learning frameworkthat addresses the unique challenges of satellite data and the task at hand. Weincorporate contextual and geographical priors, represented by correspondingcoordinate information and broader contextual information, including andsurrounding the immediate patch to be predicted. Our framework improves themodel's predictive performance in mapping land naturalness from Sentinel-2data, a type of multi-spectral optical satellite imagery. Recognizing that ourprotective measures are only as effective as our understanding of theecosystem, quantifying naturalness serves as a crucial step toward enhancingour environmental stewardship.</description><author>Burak Ekim, Michael Schmitt</author><pubDate>Thu, 27 Jun 2024 17:17:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19302v1</guid></item><item><title>MCNC: Manifold Constrained Network Compression</title><link>http://arxiv.org/abs/2406.19301v1</link><description>The outstanding performance of large foundational models across diversetasks-from computer vision to speech and natural language processing-hassignificantly increased their demand. However, storing and transmitting thesemodels pose significant challenges due to their massive size (e.g., 350GB forGPT-3). Recent literature has focused on compressing the original weights orreducing the number of parameters required for fine-tuning these models. Thesecompression methods typically involve constraining the parameter space, forexample, through low-rank reparametrization (e.g., LoRA) or quantization (e.g.,QLoRA) during model training. In this paper, we present MCNC as a novel modelcompression method that constrains the parameter space to low-dimensionalpre-defined and frozen nonlinear manifolds, which effectively cover this space.Given the prevalence of good solutions in over-parameterized deep neuralnetworks, we show that by constraining the parameter space to our proposedmanifold, we can identify high-quality solutions while achieving unprecedentedcompression rates across a wide variety of tasks. Through extensive experimentsin computer vision and natural language processing tasks, we demonstrate thatour method, MCNC, significantly outperforms state-of-the-art baselines in termsof compression, accuracy, and/or model reconstruction time.</description><author>Chayne Thrash, Ali Abbasi, Parsa Nooralinejad, Soroush Abbasi Koohpayegani, Reed Andreas, Hamed Pirsiavash, Soheil Kolouri</author><pubDate>Thu, 27 Jun 2024 17:17:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19301v1</guid></item><item><title>scTree: Discovering Cellular Hierarchies in the Presence of Batch Effects in scRNA-seq Data</title><link>http://arxiv.org/abs/2406.19300v1</link><description>We propose a novel method, scTree, for single-cell Tree VariationalAutoencoders, extending a hierarchical clustering approach to single-cell RNAsequencing data. scTree corrects for batch effects while simultaneouslylearning a tree-structured data representation. This VAE-based method allowsfor a more in-depth understanding of complex cellular landscapes independentlyof the biasing effects of batches. We show empirically on seven datasets thatscTree discovers the underlying clusters of the data and the hierarchicalrelations between them, as well as outperforms established baseline methodsacross these datasets. Additionally, we analyze the learned hierarchy tounderstand its biological relevance, thus underpinning the importance ofintegrating batch correction directly into the clustering procedure.</description><author>Moritz Vandenhirtz, Florian Barkmann, Laura Manduchi, Julia E. Vogt, Valentina Boeva</author><pubDate>Thu, 27 Jun 2024 17:16:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19300v1</guid></item><item><title>Optimistic Information Directed Sampling</title><link>http://arxiv.org/abs/2402.15411v2</link><description>We study the problem of online learning in contextual bandit problems wherethe loss function is assumed to belong to a known parametric function class. Wepropose a new analytic framework for this setting that bridges the Bayesiantheory of information-directed sampling due to Russo and Van Roy (2018) and theworst-case theory of Foster, Kakade, Qian, and Rakhlin (2021) based on thedecision-estimation coefficient. Drawing from both lines of work, we propose aalgorithmic template called Optimistic Information-Directed Sampling and showthat it can achieve instance-dependent regret guarantees similar to the onesachievable by the classic Bayesian IDS method, but with the major advantage ofnot requiring any Bayesian assumptions. The key technical innovation of ouranalysis is introducing an optimistic surrogate model for the regret and usingit to define a frequentist version of the Information Ratio of Russo and VanRoy (2018), and a less conservative version of the Decision EstimationCoefficient of Foster et al. (2021). Keywords: Contextual bandits,information-directed sampling, decision estimation coefficient, first-orderregret bounds.</description><author>Gergely Neu, Matteo Papini, Ludovic Schwartz</author><pubDate>Thu, 27 Jun 2024 17:15:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15411v2</guid></item><item><title>PNeRV: A Polynomial Neural Representation for Videos</title><link>http://arxiv.org/abs/2406.19299v1</link><description>Extracting Implicit Neural Representations (INRs) on video data poses uniquechallenges due to the additional temporal dimension. In the context of videos,INRs have predominantly relied on a frame-only parameterization, whichsacrifices the spatiotemporal continuity observed in pixel-level (spatial)representations. To mitigate this, we introduce Polynomial NeuralRepresentation for Videos (PNeRV), a parameter-wise efficient, patch-wise INRfor videos that preserves spatiotemporal continuity. PNeRV leverages themodeling capabilities of Polynomial Neural Networks to perform the modulationof a continuous spatial (patch) signal with a continuous time (frame) signal.We further propose a custom Hierarchical Patch-wise Spatial Sampling Schemethat ensures spatial continuity while retaining parameter efficiency. We alsoemploy a carefully designed Positional Embedding methodology to further enhancePNeRV's performance. Our extensive experimentation demonstrates that PNeRVoutperforms the baselines in conventional Implicit Neural Representation taskslike compression along with downstream applications that require spatiotemporalcontinuity in the underlying representation. PNeRV not only addresses thechallenges posed by video data in the realm of INRs but also opens new avenuesfor advanced video processing and analysis.</description><author>Sonam Gupta, Snehal Singh Tomar, Grigorios G Chrysos, Sukhendu Das, A. N. Rajagopalan</author><pubDate>Thu, 27 Jun 2024 17:15:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19299v1</guid></item><item><title>Compositional Image Decomposition with Diffusion Models</title><link>http://arxiv.org/abs/2406.19298v1</link><description>Given an image of a natural scene, we are able to quickly decompose it into aset of components such as objects, lighting, shadows, and foreground. We canthen envision a scene where we combine certain components with those from otherimages, for instance a set of objects from our bedroom and animals from a zoounder the lighting conditions of a forest, even if we have never encounteredsuch a scene before. In this paper, we present a method to decompose an imageinto such compositional components. Our approach, Decomp Diffusion, is anunsupervised method which, when given a single image, infers a set of differentcomponents in the image, each represented by a diffusion model. We demonstratehow components can capture different factors of the scene, ranging from globalscene descriptors like shadows or facial expression to local scene descriptorslike constituent objects. We further illustrate how inferred factors can beflexibly composed, even with factors inferred from other models, to generate avariety of scenes sharply different than those seen in training time. Websiteand code at https://energy-based-model.github.io/decomp-diffusion.</description><author>Jocelin Su, Nan Liu, Yanbo Wang, Joshua B. Tenenbaum, Yilun Du</author><pubDate>Thu, 27 Jun 2024 17:13:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19298v1</guid></item><item><title>Enhancing Continual Learning in Visual Question Answering with Modality-Aware Feature Distillation</title><link>http://arxiv.org/abs/2406.19297v1</link><description>Continual learning focuses on incrementally training a model on a sequence oftasks with the aim of learning new tasks while minimizing performance drop onprevious tasks. Existing approaches at the intersection of Continual Learningand Visual Question Answering (VQA) do not study how the multimodal nature ofthe input affects the learning dynamics of a model. In this paper, wedemonstrate that each modality evolves at different rates across a continuum oftasks and that this behavior occurs in established encoder-only models as wellas modern recipes for developing Vision &amp; Language (VL) models. Motivated bythis observation, we propose a modality-aware feature distillation (MAFED)approach which outperforms existing baselines across models of varying scale inthree multimodal continual learning settings. Furthermore, we provide ablationsshowcasing that modality-aware distillation complements experience replay.Overall, our results emphasize the importance of addressing modality-specificdynamics to prevent forgetting in multimodal continual learning.</description><author>Malvina Nikandrou, Georgios Pantazopoulos, Ioannis Konstas, Alessandro Suglia</author><pubDate>Thu, 27 Jun 2024 17:12:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19297v1</guid></item><item><title>MuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot Detector</title><link>http://arxiv.org/abs/2401.05060v2</link><description>Research in toxicity detection in natural language processing for the speechmodality (audio-based) is quite limited, particularly for languages other thanEnglish. To address these limitations and lay the groundwork for trulymultilingual audio-based toxicity detection, we introduce MuTox, the firsthighly multilingual audio-based dataset with toxicity labels. The datasetcomprises 20,000 audio utterances for English and Spanish, and 4,000 for theother 19 languages. To demonstrate the quality of this dataset, we trained theMuTox audio-based toxicity classifier, which enables zero-shot toxicitydetection across a wide range of languages. This classifier outperformsexisting text-based trainable classifiers by more than 1% AUC, while expandingthe language coverage more than tenfold. When compared to a wordlist-basedclassifier that covers a similar number of languages, MuTox improves precisionand recall by approximately 2.5 times. This significant improvement underscoresthe potential of MuTox in advancing the field of audio-based toxicitydetection.</description><author>Marta R. Costa-jussÃ , Mariano Coria Meglioli, Pierre Andrews, David Dale, Prangthip Hansanti, Elahe Kalbassi, Alex Mourachko, Christophe Ropers, Carleigh Wood</author><pubDate>Thu, 27 Jun 2024 17:05:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05060v2</guid></item><item><title>From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data</title><link>http://arxiv.org/abs/2406.19292v1</link><description>Recent studies have shown that Large Language Models (LLMs) struggle toaccurately retrieve information and maintain reasoning capabilities whenprocessing long-context inputs. To address these limitations, we propose afinetuning approach utilizing a carefully designed synthetic dataset comprisingnumerical key-value retrieval tasks. Our experiments on models like GPT-3.5Turbo and Mistral 7B demonstrate that finetuning LLMs on this datasetsignificantly improves LLMs' information retrieval and reasoning capabilitiesin longer-context settings. We present an analysis of the finetuned models,illustrating the transfer of skills from synthetic to real task evaluations(e.g., $10.5\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5Turbo). We also find that finetuned LLMs' performance on general benchmarksremains almost constant while LLMs finetuned on other baseline long-contextaugmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7Bfinetuned on our synthetic data cause no performance drop while other baselinedata can cause a drop that ranges from $2.33\%$ to $6.19\%$). Our studyhighlights the potential of finetuning on synthetic data for improving theperformance of LLMs on longer-context tasks.</description><author>Zheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, Dimitris Papailiopoulos</author><pubDate>Thu, 27 Jun 2024 17:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19292v1</guid></item><item><title>Human Modelling and Pose Estimation Overview</title><link>http://arxiv.org/abs/2406.19290v1</link><description>Human modelling and pose estimation stands at the crossroads of ComputerVision, Computer Graphics, and Machine Learning. This paper presents a thoroughinvestigation of this interdisciplinary field, examining various algorithms,methodologies, and practical applications. It explores the diverse range ofsensor technologies relevant to this domain and delves into a wide array ofapplication areas. Additionally, we discuss the challenges and advancements in2D and 3D human modelling methodologies, along with popular datasets, metrics,and future research directions. The main contribution of this paper lies in itsup-to-date comparison of state-of-the-art (SOTA) human pose estimationalgorithms in both 2D and 3D domains. By providing this comprehensive overview,the paper aims to enhance understanding of 3D human modelling and poseestimation, offering insights into current SOTA achievements, challenges, andfuture prospects within the field.</description><author>Pawel Knap</author><pubDate>Thu, 27 Jun 2024 17:04:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19290v1</guid></item><item><title>MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic</title><link>http://arxiv.org/abs/2406.11385v2</link><description>The advent of large language models (LLMs) like GPT-4 has catalyzed theexploration of multi-task learning (MTL), in which a single model demonstratesproficiency across diverse tasks. Task arithmetic has emerged as acost-effective approach for MTL. It enables performance enhancement acrossmultiple tasks by adding their corresponding task vectors to a pre-trainedmodel. However, the current lack of a method that can simultaneously achieveoptimal performance, computational efficiency, and data privacy limits theirapplication to LLMs. In this paper, we propose \textbf{M}odel\textbf{E}xclusive \textbf{T}ask \textbf{A}rithmetic for merging\textbf{GPT}-scale models, which formalizes the objective of model merging intoa multi-task learning framework, aiming to minimize the average loss differencebetween the merged model and each individual task model. Since data privacylimits the use of multi-task training data, we leverage LLMs' local linearityand task vectors' orthogonality to separate the data term and scalingcoefficients term and derive a model-exclusive task arithmetic method. Ourproposed MetaGPT is data-agnostic and bypasses the heavy search process, makingit cost-effective and easy to implement for LLMs.Extensive experimentsdemonstrate that MetaGPT leads to improvements in task arithmetic and achievesstate-of-the-art performance on multiple tasks.</description><author>Yuyan Zhou, Liang Song, Bingning Wang, Weipeng Chen</author><pubDate>Thu, 27 Jun 2024 17:01:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11385v2</guid></item><item><title>GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts</title><link>http://arxiv.org/abs/2309.10253v4</link><description>Large language models (LLMs) have recently experienced tremendous popularityand are widely used from casual conversations to AI-driven programming.However, despite their considerable success, LLMs are not entirely reliable andcan give detailed guidance on how to conduct harmful or illegal activities.While safety measures can reduce the risk of such outputs, adversarialjailbreak attacks can still exploit LLMs to produce harmful content. Thesejailbreak templates are typically manually crafted, making large-scale testingchallenging. In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzingframework inspired by the AFL fuzzing framework. Instead of manual engineering,GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs.At its core, GPTFuzz starts with human-written templates as initial seeds, thenmutates them to produce new templates. We detail three key components ofGPTFuzz: a seed selection strategy for balancing efficiency and variability,mutate operators for creating semantically equivalent or similar sentences, anda judgment model to assess the success of a jailbreak attack. We evaluate GPTFuzz against various commercial and open-source LLMs,including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Ourresults indicate that GPTFuzz consistently produces jailbreak templates with ahigh success rate, surpassing human-crafted templates. Remarkably, GPTFuzzachieves over 90% attack success rates against ChatGPT and Llama-2 models, evenwith suboptimal initial seed templates. We anticipate that GPTFuzz will beinstrumental for researchers and practitioners in examining LLM robustness andwill encourage further exploration into enhancing LLM safety.</description><author>Jiahao Yu, Xingwei Lin, Zheng Yu, Xinyu Xing</author><pubDate>Thu, 27 Jun 2024 17:01:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10253v4</guid></item><item><title>Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops</title><link>http://arxiv.org/abs/2307.14938v3</link><description>In this paper, we propose a computationally efficient framework for intervalreachability of systems with neural network controllers. Our approach leveragesinclusion functions for the open-loop system and the neural network controllerto embed the closed-loop system into a larger-dimensional embedding system,where a single trajectory over-approximates the original system's behaviorunder uncertainty. We propose two methods for constructing closed-loopembedding systems, which account for the interactions between the system andthe controller in different ways. The interconnection-based approach considersthe worst-case evolution of each coordinate separately by substituting theneural network inclusion function into the open-loop inclusion function. Theinteraction-based approach uses novel Jacobian-based inclusion functions tocapture the first-order interactions between the open-loop system and thecontroller by leveraging state-of-the-art neural network verifiers. Finally, weimplement our approach in a Python framework called ReachMM to demonstrate itsefficiency and scalability on benchmarks and examples ranging to $200$ statedimensions.</description><author>Saber Jafarpour, Akash Harapanahalli, Samuel Coogan</author><pubDate>Thu, 27 Jun 2024 17:00:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14938v3</guid></item><item><title>CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented Analysis Generation</title><link>http://arxiv.org/abs/2406.17186v2</link><description>Legal professionals need to write analyses that rely on citations to relevantprecedents, i.e., previous case decisions. Intelligent systems assisting legalprofessionals in writing such documents provide great benefits but arechallenging to design. Such systems need to help locate, summarize, and reasonover salient precedents in order to be useful. To enable systems for suchtasks, we work with legal professionals to transform a large open-source legalcorpus into a dataset supporting two important backbone tasks: informationretrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC(Case Law Evaluation Retrieval Corpus), is constructed for training andevaluating models on their ability to (1) find corresponding citations for agiven piece of legal analysis and to (2) compile the text of these citations(as well as previous context) into a cogent analysis that supports a reasoninggoal. We benchmark state-of-the-art models on CLERC, showing that currentapproaches still struggle: GPT-4o generates analyses with the highest ROUGEF-scores but hallucinates the most, while zero-shot IR models only achieve48.3% recall@1000.</description><author>Abe Bohan Hou, Orion Weller, Guanghui Qin, Eugene Yang, Dawn Lawrie, Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme</author><pubDate>Thu, 27 Jun 2024 16:55:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17186v2</guid></item><item><title>Assessing the nature of large language models: A caution against anthropocentrism</title><link>http://arxiv.org/abs/2309.07683v3</link><description>Generative AI models garnered a large amount of public attention andspeculation with the release of OpenAIs chatbot, ChatGPT. At least two opinioncamps exist: one excited about possibilities these models offer for fundamentalchanges to human tasks, and another highly concerned about power these modelsseem to have. To address these concerns, we assessed several LLMs, primarilyGPT 3.5, using standard, normed, and validated cognitive and personalitymeasures. For this seedling project, we developed a battery of tests thatallowed us to estimate the boundaries of some of these models capabilities, howstable those capabilities are over a short period of time, and how they compareto humans. Our results indicate that LLMs are unlikely to have developedsentience, although its ability to respond to personality inventories isinteresting. GPT3.5 did display large variability in both cognitive andpersonality measures over repeated observations, which is not expected if ithad a human-like personality. Variability notwithstanding, LLMs display what ina human would be considered poor mental health, including low self-esteem,marked dissociation from reality, and in some cases narcissism and psychopathy,despite upbeat and helpful responses.</description><author>Ann Speed</author><pubDate>Thu, 27 Jun 2024 16:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07683v3</guid></item><item><title>Coarse-to-Fine Concept Bottleneck Models</title><link>http://arxiv.org/abs/2310.02116v2</link><description>Deep learning algorithms have recently gained significant attention due totheir impressive performance. However, their high complexity andun-interpretable mode of operation hinders their confident deployment inreal-world safety-critical tasks. This work targets ante hoc interpretability,and specifically Concept Bottleneck Models (CBMs). Our goal is to design aframework that admits a highly interpretable decision making process withrespect to human understandable concepts, on two levels of granularity. To thisend, we propose a novel two-level concept discovery formulation leveraging: (i)recent advances in vision-language models, and (ii) an innovative formulationfor coarse-to-fine concept selection via data-driven and sparsity-inducingBayesian arguments. Within this framework, concept information does not solelyrely on the similarity between the whole image and general unstructuredconcepts; instead, we introduce the notion of concept hierarchy to uncover andexploit more granular concept information residing in patch-specific regions ofthe image scene. As we experimentally show, the proposed construction not onlyoutperforms recent CBM approaches, but also yields a principled frameworktowards interpetability.</description><author>Konstantinos P. Panousis, Dino Ienco, Diego Marcos</author><pubDate>Thu, 27 Jun 2024 16:53:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02116v2</guid></item><item><title>HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale</title><link>http://arxiv.org/abs/2406.19280v1</link><description>The rapid development of multimodal large language models (MLLMs), such asGPT-4V, has led to significant advancements. However, these models still facechallenges in medical multimodal capabilities due to limitations in thequantity and quality of medical vision-text data, stemming from data privacyconcerns and high annotation costs. While pioneering approaches utilizePubMed's large-scale, de-identified medical image-text pairs to address theselimitations, they still fall short due to inherent data noise. To tackle this,we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) inan 'unblinded' capacity to denoise and reformat the data, resulting in thecreation of the PubMedVision dataset with 1.3 million medical VQA samples. Ourvalidation demonstrates that: (1) PubMedVision can significantly enhance themedical multimodal capabilities of current MLLMs, showing significantimprovement in benchmarks including the MMMU Health &amp; Medicine track; (2)manual checks by medical experts and empirical results validate the superiordata quality of our dataset compared to other data construction methods. UsingPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which showssuperior performance in medical multimodal scenarios among open-source MLLMs.</description><author>Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang</author><pubDate>Thu, 27 Jun 2024 16:50:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19280v1</guid></item><item><title>VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation</title><link>http://arxiv.org/abs/2406.19276v1</link><description>Existing metrics for evaluating the factuality of long-form text, such asFACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an inputtext into "atomic claims" and verify each against a knowledge base likeWikipedia. These metrics are not suitable for most generation tasks becausethey assume that every claim is verifiable (i.e., can plausibly be proven trueor false). We address this issue with VERISCORE, a metric for diverse long-formgeneration tasks that contain both verifiable and unverifiable content.VERISCORE can be effectively implemented with either closed or fine-tunedopen-weight language models, and human evaluation confirms that VERISCORE'sextracted claims are more sensible than those from competing methods acrosseight different long-form tasks. We use VERISCORE to evaluate generations from16 different models across multiple long-form tasks and find that while GPT-4ois the best-performing model overall, open-weight models such as Mixtral-8x22are closing the gap. We show that an LM's VERISCORE on one task (e.g.,biography generation) does not necessarily correlate to its VERISCORE on adifferent task (e.g., long-form QA), highlighting the need for expandingfactuality evaluation across tasks with varying fact density.</description><author>Yixiao Song, Yekyung Kim, Mohit Iyyer</author><pubDate>Thu, 27 Jun 2024 16:43:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19276v1</guid></item><item><title>AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical Interaction Simulator</title><link>http://arxiv.org/abs/2402.09742v3</link><description>Artificial intelligence has significantly advanced healthcare, particularlythrough large language models (LLMs) that excel in medical question answeringbenchmarks. However, their real-world clinical application remains limited dueto the complexities of doctor-patient interactions. To address this, weintroduce \textbf{AI Hospital}, a multi-agent framework simulating dynamicmedical interactions between \emph{Doctor} as player and NPCs including\emph{Patient}, \emph{Examiner}, \emph{Chief Physician}. This setup allows forrealistic assessments of LLMs in clinical scenarios. We develop the Multi-ViewMedical Evaluation (MVME) benchmark, utilizing high-quality Chinese medicalrecords and NPCs to evaluate LLMs' performance in symptom collection,examination recommendations, and diagnoses. Additionally, a dispute resolutioncollaborative mechanism is proposed to enhance diagnostic accuracy throughiterative discussions. Despite improvements, current LLMs exhibit significantperformance gaps in multi-turn interactions compared to one-step approaches.Our findings highlight the need for further research to bridge these gaps andimprove LLMs' clinical diagnostic capabilities. Our data, code, andexperimental results are all open-sourced at\url{https://github.com/LibertFan/AI_Hospital}.</description><author>Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, Jingren Zhou</author><pubDate>Thu, 27 Jun 2024 16:40:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09742v3</guid></item><item><title>Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification</title><link>http://arxiv.org/abs/2402.06530v3</link><description>Early detection of myocardial infarction (MI), a critical condition arisingfrom coronary artery disease (CAD), is vital to prevent further myocardialdamage. This study introduces a novel method for early MI detection using aone-class classification (OCC) algorithm in echocardiography. Our studyovercomes the challenge of limited echocardiography data availability byadopting a novel approach based on Multi-modal Subspace Support Vector DataDescription. The proposed technique involves a specialized MI detectionframework employing multi-view echocardiography incorporating a compositekernel in the non-linear projection trick, fusing Gaussian and Laplaciansigmoid functions. Additionally, we enhance the update strategy of theprojection matrices by adapting maximization for both or one of the modalitiesin the optimization process. Our method boosts MI detection capability byefficiently transforming features extracted from echocardiography data into anoptimized lower-dimensional subspace. The OCC model trained specifically ontarget class instances from the comprehensive HMC-QU dataset that includesmultiple echocardiography views indicates a marked improvement in MI detectionaccuracy. Our findings reveal that our proposed multi-view approach achieves ageometric mean of 71.24%, signifying a substantial advancement inechocardiography-based MI diagnosis and offering more precise and efficientdiagnostic tools.</description><author>Muhammad Uzair Zahid, Aysen Degerli, Fahad Sohrab, Serkan Kiranyaz, Tahir Hamid, Rashid Mazhar, Moncef Gabbouj</author><pubDate>Thu, 27 Jun 2024 16:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06530v3</guid></item><item><title>Stochastic Concept Bottleneck Models</title><link>http://arxiv.org/abs/2406.19272v1</link><description>Concept Bottleneck Models (CBMs) have emerged as a promising interpretablemethod whose final prediction is based on intermediate, human-understandableconcepts rather than the raw input. Through time-consuming manualinterventions, a user can correct wrongly predicted concept values to enhancethe model's downstream performance. We propose Stochastic Concept BottleneckModels (SCBMs), a novel approach that models concept dependencies. In SCBMs, asingle-concept intervention affects all correlated concepts, thereby improvingintervention effectiveness. Unlike previous approaches that model the conceptrelations via an autoregressive structure, we introduce an explicit,distributional parameterization that allows SCBMs to retain the CBMs' efficienttraining and inference procedure. Additionally, we leverage theparameterization to derive an effective intervention strategy based on theconfidence region. We show empirically on synthetic tabular and natural imagedatasets that our approach improves intervention effectiveness significantly.Notably, we showcase the versatility and usability of SCBMs by examining asetting with CLIP-inferred concepts, alleviating the need for manual conceptannotations.</description><author>Moritz Vandenhirtz, Sonia Laguna, RiÄards MarcinkeviÄs, Julia E. Vogt</author><pubDate>Thu, 27 Jun 2024 16:38:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19272v1</guid></item><item><title>Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipanel VQA</title><link>http://arxiv.org/abs/2401.15847v3</link><description>Multipanel images, commonly seen as web screenshots, posters, etc., pervadeour daily lives. These images, characterized by their composition of multiplesubfigures in distinct layouts, effectively convey information to people.Toward building advanced multimodal AI applications, such as agents thatunderstand complex scenes and navigate through webpages, the skill ofmultipanel visual reasoning is essential, and a comprehensive evaluation ofmodels in this regard is important. Therefore, we introduce Multipanel VisualQuestion Answering (MultipanelVQA), a novel benchmark comprising 6,600 tripletsof questions, answers, and multipanel images that specifically challenge modelsin comprehending multipanel images. Our evaluation shows that questions in theMultipanelVQA benchmark pose significant challenges to the state-of-the-artMultimodal Large Language Models (MLLMs) tested, even though humans can attainapproximately 99% accuracy on these questions. Distinctively, the MultipanelVQAbenchmark features synthetically generated multipanel images specificallycrafted to isolate and assess the impact of various factors, such as thelayout, on MLLMs' multipanel image comprehension abilities. As a result, inaddition to benchmarking the capabilities of MLLMs in understanding multipanelimages, we analyze various factors of the multipanel image that affect MLLMs'performance with synthetic data and offer insights for enhancement. Code anddata are released at https://sites.google.com/view/multipanelvqa/home.</description><author>Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Xinze Guan, Xin Eric Wang</author><pubDate>Thu, 27 Jun 2024 16:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15847v3</guid></item><item><title>AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning</title><link>http://arxiv.org/abs/2406.19271v1</link><description>Up-to-date and reliable Large Language Models (LLMs) are consistently soughtafter. Typically, LLMs are trained on a fixed dataset and then deployed.However, the training data continually becomes outdated. Enable automatictraining of AI using web data involves significant concerns regarding dataquality and safety due to bias, spam, and other unsafe or unwanted text. Puredata is essential for producing reliable models. Training a model on impuredata may result in undesirable outcomes. This research proposes a system thatcollects web data and automatically filters out unwanted text with theassistance of existing trusted AI models. In the experiment, a small sample ofweb data was collected and filtered, demonstrating the system's effectivenessin purifying the data.</description><author>Praneeth Vadlapati</author><pubDate>Thu, 27 Jun 2024 16:37:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19271v1</guid></item><item><title>Unified Active Retrieval for Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2406.12534v3</link><description>In Retrieval-Augmented Generation (RAG), retrieval is not always helpful andapplying it to every instruction is sub-optimal. Therefore, determining whetherto retrieve is crucial for RAG, which is usually referred to as ActiveRetrieval. However, existing active retrieval methods face two challenges: 1.They usually rely on a single criterion, which struggles with handling varioustypes of instructions. 2. They depend on specialized and highly differentiatedprocedures, and thus combining them makes the RAG system more complicated andleads to higher response latency. To address these challenges, we proposeUnified Active Retrieval (UAR). UAR contains four orthogonal criteria and caststhem into plug-and-play classification tasks, which achieves multifacetedretrieval timing judgements with negligible extra inference cost. We furtherintroduce the Unified Active Retrieval Criteria (UAR-Criteria), designed toprocess diverse active retrieval scenarios through a standardized procedure.Experiments on four representative types of user instructions show that UARsignificantly outperforms existing work on the retrieval timing judgement andthe performance of downstream tasks, which shows the effectiveness of UAR andits helpfulness to downstream tasks.</description><author>Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun, Hang Yan, Xipeng Qiu</author><pubDate>Thu, 27 Jun 2024 16:37:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12534v3</guid></item><item><title>Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding</title><link>http://arxiv.org/abs/2406.19263v1</link><description>Graphical User Interfaces (GUIs) are central to our interaction with digitaldevices. Recently, growing efforts have been made to build models for variousGUI understanding tasks. However, these efforts largely overlook an importantGUI-referring task: screen reading based on user-indicated points, which wename the Screen Point-and-Read (SPR) task. This task is predominantly handledby rigid accessible screen reading tools, in great need of new models driven byadvancements in Multimodal Large Language Models (MLLMs). In this paper, wepropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,to address the SPR task. Based on the input point coordinate and thecorresponding GUI screenshot, our ToL agent constructs a Hierarchical LayoutTree. Based on the tree, our ToL agent not only comprehends the content of theindicated area but also articulates the layout and spatial relationshipsbetween elements. Such layout information is crucial for accuratelyinterpreting information on the screen, distinguishing our ToL agent from otherscreen reading tools. We also thoroughly evaluate the ToL agent against otherbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,web, and operating systems. Last but not least, we test the ToL agent on mobileGUI navigation tasks, demonstrating its utility in identifying incorrectactions along the path of agent execution trajectories. Code and data:screen-point-and-read.github.io</description><author>Yue Fan, Lei Ding, Ching-Chen Kuo, Shan Jiang, Yang Zhao, Xinze Guan, Jie Yang, Yi Zhang, Xin Eric Wang</author><pubDate>Thu, 27 Jun 2024 16:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19263v1</guid></item><item><title>Commodification of Compute</title><link>http://arxiv.org/abs/2406.19261v1</link><description>The rapid advancements in artificial intelligence, big data analytics, andcloud computing have precipitated an unprecedented demand for computationalresources. However, the current landscape of computational resource allocationis characterized by significant inefficiencies, including underutilization andprice volatility. This paper addresses these challenges by introducing a novelglobal platform for the commodification of compute hours, termed the GlobalCompute Exchange (GCX) (Patent Pending). The GCX leverages blockchaintechnology and smart contracts to create a secure, transparent, and efficientmarketplace for buying and selling computational power. The GCX is built in alayered fashion, comprising Market, App, Clearing, Risk Management, Exchange(Offchain), and Blockchain (Onchain) layers, each ensuring a robust andefficient operation. This platform aims to revolutionize the computationalresource market by fostering a decentralized, efficient, and transparentecosystem that ensures equitable access to computing power, stimulatesinnovation, and supports diverse user needs on a global scale. By transformingcompute hours into a tradable commodity, the GCX seeks to optimize resourceutilization, stabilize pricing, and democratize access to computationalresources. This paper explores the technological infrastructure, marketpotential, and societal impact of the GCX, positioning it as a pioneeringsolution poised to drive the next wave of innovation in commodities andcompute.</description><author>Jesper Kristensen, David Wender, Carl Anthony</author><pubDate>Thu, 27 Jun 2024 16:32:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19261v1</guid></item><item><title>Leveraging Contrastive Learning for Enhanced Node Representations in Tokenized Graph Transformers</title><link>http://arxiv.org/abs/2406.19258v1</link><description>While tokenized graph Transformers have demonstrated strong performance innode classification tasks, their reliance on a limited subset of nodes withhigh similarity scores for constructing token sequences overlooks valuableinformation from other nodes, hindering their ability to fully harness graphinformation for learning optimal node representations. To address thislimitation, we propose a novel graph Transformer called GCFormer. Unlikeprevious approaches, GCFormer develops a hybrid token generator to create twotypes of token sequences, positive and negative, to capture diverse graphinformation. And a tailored Transformer-based backbone is adopted to learnmeaningful node representations from these generated token sequences.Additionally, GCFormer introduces contrastive learning to extract valuableinformation from both positive and negative token sequences, enhancing thequality of learned node representations. Extensive experimental results acrossvarious datasets, including homophily and heterophily graphs, demonstrate thesuperiority of GCFormer in node classification, when compared to representativegraph neural networks (GNNs) and graph Transformers.</description><author>Jinsong Chen, Hanpeng Liu, John E. Hopcroft, Kun He</author><pubDate>Thu, 27 Jun 2024 16:29:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19258v1</guid></item><item><title>ReFT: Reasoning with Reinforced Fine-Tuning</title><link>http://arxiv.org/abs/2401.08967v2</link><description>One way to enhance the reasoning capability of Large Language Models (LLMs)is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)annotations. This approach does not show sufficiently strong generalizationability, however, because the training only relies on the given CoT data. Inmath problem-solving, for example, there is usually only one annotatedreasoning path for each question in the training data. Intuitively, it would bebetter for the algorithm to learn from multiple annotated reasoning paths givena question. To address this issue, we propose a simple yet effective approachcalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability oflearning LLMs for reasoning, with math problem-solving as an example. ReFTfirst warmups the model with SFT, and then employs on-line reinforcementlearning, specifically the PPO algorithm in this paper, to further fine-tunethe model, where an abundance of reasoning paths are automatically sampledgiven the question and the rewards are naturally derived from the ground-truthanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show thatReFT significantly outperforms SFT, and the performance can be potentiallyfurther boosted by combining inference-time strategies such as majority votingand re-ranking. Note that ReFT obtains the improvement by learning from thesame training questions as SFT, without relying on extra or augmented trainingquestions. This indicates a superior generalization ability for ReFT.</description><author>Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li</author><pubDate>Thu, 27 Jun 2024 16:29:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08967v2</guid></item><item><title>Token-level Direct Preference Optimization</title><link>http://arxiv.org/abs/2404.11999v4</link><description>Fine-tuning pre-trained Large Language Models (LLMs) is essential to alignthem with human values and intentions. This process often utilizes methods likepairwise comparisons and KL divergence against a reference LLM, focusing on theevaluation of full answers generated by the models. However, the generation ofthese responses occurs in a token level, following a sequential,auto-regressive fashion. In this paper, we introduce Token-level DirectPreference Optimization (TDPO), a novel approach to align LLMs with humanpreferences by optimizing policy at the token level. Unlike previous methods,which face challenges in divergence efficiency, TDPO incorporates forward KLdivergence constraints for each token, improving alignment and diversity.Utilizing the Bradley-Terry model for a token-based reward system, TDPOenhances the regulation of KL divergence, while preserving simplicity withoutthe need for explicit reward modeling. Experimental results across various texttasks demonstrate TDPO's superior performance in balancing alignment withgeneration diversity. Notably, fine-tuning with TDPO strikes a better balancethan DPO in the controlled sentiment generation and single-turn dialoguedatasets, and significantly improves the quality of generated responsescompared to both DPO and PPO-based RLHF methods. Our code is open-sourced athttps://github.com/Vance0124/Token-level-Direct-Preference-Optimization.</description><author>Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, Jun Wang</author><pubDate>Thu, 27 Jun 2024 16:27:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11999v4</guid></item><item><title>AI Data Readiness Inspector (AIDRIN) for Quantitative Assessment of Data Readiness for AI</title><link>http://arxiv.org/abs/2406.19256v1</link><description>"Garbage In Garbage Out" is a universally agreed quote by computer scientistsfrom various domains, including Artificial Intelligence (AI). As data is thefuel for AI, models trained on low-quality, biased data are often ineffective.Computer scientists who use AI invest a considerable amount of time and effortin preparing the data for AI. However, there are no standard methods orframeworks for assessing the "readiness" of data for AI. To provide aquantifiable assessment of the readiness of data for AI processes, we defineparameters of AI data readiness and introduce AIDRIN (AI Data ReadinessInspector). AIDRIN is a framework covering a broad range of readinessdimensions available in the literature that aid in evaluating the readiness ofdata quantitatively and qualitatively. AIDRIN uses metrics in traditional dataquality assessment such as completeness, outliers, and duplicates for dataevaluation. Furthermore, AIDRIN uses metrics specific to assess data for AI,such as feature importance, feature correlations, class imbalance, fairness,privacy, and FAIR (Findability, Accessibility, Interoperability, andReusability) principle compliance. AIDRIN provides visualizations and reportsto assist data scientists in further investigating the readiness of data. TheAIDRIN framework enhances the efficiency of the machine learning pipeline tomake informed decisions on data readiness for AI applications.</description><author>Kaveen Hiniduma, Suren Byna, Jean Luca Bez, Ravi Madduri</author><pubDate>Thu, 27 Jun 2024 16:26:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19256v1</guid></item><item><title>MedCalc-Bench: Evaluating Large Language Models for Medical Calculations</title><link>http://arxiv.org/abs/2406.12036v3</link><description>As opposed to evaluating computation and logic-based reasoning, currentbenchmarks for evaluating large language models (LLMs) in medicine areprimarily focused on question-answering involving domain knowledge anddescriptive reasoning. While such qualitative capabilities are vital to medicaldiagnosis, in real-world scenarios, doctors frequently use clinical calculatorsthat follow quantitative equations and rule-based reasoning paradigms forevidence-based decision support. To this end, we propose MedCalc-Bench, afirst-of-its-kind dataset focused on evaluating the medical calculationcapability of LLMs. MedCalc-Bench contains an evaluation set of over 1000manually reviewed instances from 55 different medical calculation tasks. Eachinstance in MedCalc-Bench consists of a patient note, a question requesting tocompute a specific medical value, a ground truth answer, and a step-by-stepexplanation showing how the answer is obtained. While our evaluation resultsshow the potential of LLMs in this area, none of them are effective enough forclinical settings. Common issues include extracting the incorrect entities, notusing the correct equation or rules for a calculation task, or incorrectlyperforming the arithmetic for the computation. We hope our study highlights thequantitative knowledge and reasoning gaps in LLMs within medical settings,encouraging future improvements of LLMs for various clinical calculation tasks.</description><author>Nikhil Khandekar, Qiao Jin, Guangzhi Xiong, Soren Dunn, Serina S Applebaum, Zain Anwar, Maame Sarfo-Gyamfi, Conrad W Safranek, Abid A Anwar, Andrew Zhang, Aidan Gilson, Maxwell B Singer, Amisha Dave, Andrew Taylor, Aidong Zhang, Qingyu Chen, Zhiyong Lu</author><pubDate>Thu, 27 Jun 2024 16:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12036v3</guid></item><item><title>Shortcut Learning in Medical Image Segmentation</title><link>http://arxiv.org/abs/2403.06748v2</link><description>Shortcut learning is a phenomenon where machine learning models prioritizelearning simple, potentially misleading cues from data that do not generalizewell beyond the training set. While existing research primarily investigatesthis in the realm of image classification, this study extends the explorationof shortcut learning into medical image segmentation. We demonstrate thatclinical annotations such as calipers, and the combination of zero-paddedconvolutions and center-cropped training sets in the dataset can inadvertentlyserve as shortcuts, impacting segmentation accuracy. We identify and evaluatethe shortcut learning on two different but common medical image segmentationtasks. In addition, we suggest strategies to mitigate the influence of shortcutlearning and improve the generalizability of the segmentation models. Byuncovering the presence and implications of shortcuts in medical imagesegmentation, we provide insights and methodologies for evaluating andovercoming this pervasive challenge and call for attention in the community forshortcuts in segmentation. Our code is public athttps://github.com/nina-weng/shortcut_skinseg .</description><author>Manxi Lin, Nina Weng, Kamil Mikolaj, Zahra Bashir, Morten Bo SÃ¸ndergaard Svendsen, Martin Tolsgaard, Anders Nymark Christensen, Aasa Feragen</author><pubDate>Thu, 27 Jun 2024 16:24:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06748v2</guid></item><item><title>Enhancing Video-Language Representations with Structural Spatio-Temporal Alignment</title><link>http://arxiv.org/abs/2406.19255v1</link><description>While pre-training large-scale video-language models (VLMs) has shownremarkable potential for various downstream video-language tasks, existing VLMscan still suffer from certain commonly seen limitations, e.g., coarse-grainedcross-modal aligning , under-modeling of temporal dynamics, detachedvideo-language view. In this work, we target enhancing VLMs with a fine-grainedstructural spatio-temporal alignment learning method (namely Finsta). First ofall, we represent the input texts and videos with fine-grained scene graph (SG)structures, both of which are further unified into a holistic SG (HSG) forbridging two modalities. Then, an SG-based framework is built, where thetextual SG (TSG) is encoded with a graph Transformer, while the video dynamicSG (DSG) and the HSG are modeled with a novel recurrent graph Transformer forspatial and temporal feature propagation. A spatial-temporal Gaussiandifferential graph Transformer is further devised to strengthen the sense ofthe changes in objects across spatial and temporal dimensions. Next, based onthe fine-grained structural features of TSG and DSG, we perform object-centeredspatial alignment and predicate-centered temporal alignment respectively,enhancing the video-language grounding in both the spatiality and temporality.We design our method as a plug&amp;play system, which can be integrated intoexisting well-trained VLMs for further representation augmentation, withouttraining from scratch or relying on SG annotations in downstream applications.On 6 representative VL modeling tasks over 12 datasets in both standard andlong-form video scenarios, Finsta consistently improves the existing 13strong-performing VLMs persistently, and refreshes the current state-of-the-artend task performance significantly in both the fine-tuning and zero-shotsettings.</description><author>Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, Shuicheng Yan</author><pubDate>Thu, 27 Jun 2024 16:23:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19255v1</guid></item><item><title>Advection Augmented Convolutional Neural Networks</title><link>http://arxiv.org/abs/2406.19253v1</link><description>Many problems in physical sciences are characterized by the prediction ofspace-time sequences. Such problems range from weather prediction to theanalysis of disease propagation and video prediction. Modern techniques for thesolution of these problems typically combine Convolution Neural Networks (CNN)architecture with a time prediction mechanism. However, oftentimes, suchapproaches underperform in the long-range propagation of information and lackexplainability. In this work, we introduce a physically inspired architecturefor the solution of such problems. Namely, we propose to augment CNNs withadvection by designing a novel semi-Lagrangian push operator. We show that theproposed operator allows for the non-local transformation of informationcompared with standard convolutional kernels. We then complement it withReaction and Diffusion neural components to form a network that mimics theReaction-Advection-Diffusion equation, in high dimensions. We demonstrate theeffectiveness of our network on a number of spatio-temporal datasets that showtheir merit.</description><author>Niloufar Zakariaei, Siddharth Rout, Eldad Haber, Moshe Eliasof</author><pubDate>Thu, 27 Jun 2024 16:22:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19253v1</guid></item><item><title>AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2406.19251v1</link><description>Recent advancements in Large Language Models have transformed ML/AIdevelopment, necessitating a reevaluation of AutoML principles for theRetrieval-Augmented Generation (RAG) systems. To address the challenges ofhyper-parameter optimization and online adaptation in RAG, we propose theAutoRAG-HP framework, which formulates the hyper-parameter tuning as an onlinemulti-armed bandit (MAB) problem and introduces a novel two-level HierarchicalMAB (Hier-MAB) method for efficient exploration of large search spaces. Weconduct extensive experiments on tuning hyper-parameters, such as top-kretrieved documents, prompt compression ratio, and embedding methods, using theALCE-ASQA and Natural Questions datasets. Our evaluation from jointlyoptimization all three hyper-parameters demonstrate that MAB-based onlinelearning methods can achieve Recall@5 $\approx 0.8$ for scenarios withprominent gradients in search space, using only $\sim20\%$ of the LLM API callsrequired by the Grid Search approach. Additionally, the proposed Hier-MABapproach outperforms other baselines in more challenging optimizationscenarios. The code will be made available at https://aka.ms/autorag.</description><author>Jia Fu, Xiaoting Qin, Fangkai Yang, Lu Wang, Jue Zhang, Qingwei Lin, Yubo Chen, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</author><pubDate>Thu, 27 Jun 2024 16:18:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19251v1</guid></item><item><title>NTFormer: A Composite Node Tokenized Graph Transformer for Node Classification</title><link>http://arxiv.org/abs/2406.19249v1</link><description>Recently, the emerging graph Transformers have made significant advancementsfor node classification on graphs. In most graph Transformers, a crucial stepinvolves transforming the input graph into token sequences as the model input,enabling Transformer to effectively learn the node representations. However, weobserve that existing methods only express partial graph information of nodesthrough single-type token generation. Consequently, they require tailoredstrategies to encode additional graph-specific features into the Transformer toensure the quality of node representation learning, limiting the modelflexibility to handle diverse graphs. To this end, we propose a new graphTransformer called NTFormer to address this issue. NTFormer introduces a noveltoken generator called Node2Par, which constructs various token sequences usingdifferent token elements for each node. This flexibility allows Node2Par togenerate valuable token sequences from different perspectives, ensuringcomprehensive expression of rich graph features. Benefiting from the merits ofNode2Par, NTFormer only leverages a Transformer-based backbone withoutgraph-specific modifications to learn node representations, eliminating theneed for graph-specific modifications. Extensive experiments conducted onvarious benchmark datasets containing homophily and heterophily graphs withdifferent scales demonstrate the superiority of NTFormer over representativegraph Transformers and graph neural networks for node classification.</description><author>Jinsong Chen, Siyu Jiang, Kun He</author><pubDate>Thu, 27 Jun 2024 16:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19249v1</guid></item><item><title>Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition</title><link>http://arxiv.org/abs/2402.14523v2</link><description>We often verbally express emotions in a multifaceted manner, they may vary intheir intensities and may be expressed not just as a single but as a mixture ofemotions. This wide spectrum of emotions is well-studied in the structuralmodel of emotions, which represents variety of emotions as derivative productsof primary emotions with varying degrees of intensity. In this paper, wepropose an emotional text-to-speech design to simulate a wider spectrum ofemotions grounded on the structural model. Our proposed design, Daisy-TTS,incorporates a prosody encoder to learn emotionally-separable prosody embeddingas a proxy for emotion. This emotion representation allows the model tosimulate: (1) Primary emotions, as learned from the training samples, (2)Secondary emotions, as a mixture of primary emotions, (3) Intensity-level, byscaling the emotion embedding, and (4) Emotions polarity, by negating theemotion embedding. Through a series of perceptual evaluations, Daisy-TTSdemonstrated overall higher emotional speech naturalness and emotionperceiveability compared to the baseline.</description><author>Rendi Chevi, Alham Fikri Aji</author><pubDate>Thu, 27 Jun 2024 16:14:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14523v2</guid></item><item><title>Local Manifold Learning for No-Reference Image Quality Assessment</title><link>http://arxiv.org/abs/2406.19247v1</link><description>Contrastive learning has considerably advanced the field of Image QualityAssessment (IQA), emerging as a widely adopted technique. The core mechanism ofcontrastive learning involves minimizing the distance between quality-similar(positive) examples while maximizing the distance between quality-dissimilar(negative) examples. Despite its successes, current contrastive learningmethods often neglect the importance of preserving the local manifoldstructure. This oversight can result in a high degree of similarity among hardexamples within the feature space, thereby impeding effective differentiationand assessment. To address this issue, we propose an innovative framework thatintegrates local manifold learning with contrastive learning for No-ReferenceImage Quality Assessment (NR-IQA). Our method begins by sampling multiple cropsfrom a given image, identifying the most visually salient crop. This crop isthen used to cluster other crops from the same image as the positive class,while crops from different images are treated as negative classes to increaseinter-class distance. Uniquely, our approach also considers non-saliency cropsfrom the same image as intra-class negative classes to preserve theirdistinctiveness. Additionally, we employ a mutual learning framework, whichfurther enhances the model's ability to adaptively learn and identify visualsaliency regions. Our approach demonstrates a better performance compared tostate-of-the-art methods in 7 standard datasets, achieving PLCC values of 0.942(compared to 0.908 in TID2013) and 0.914 (compared to 0.894 in LIVEC).</description><author>Timin Gao, Wensheng Pan, Yan Zhang, Sicheng Zhao, Shengchuan Zhang, Xiawu Zheng, Ke Li, Liujuan Cao, Rongrong Ji</author><pubDate>Thu, 27 Jun 2024 16:14:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19247v1</guid></item><item><title>Stable Differentiable Causal Discovery</title><link>http://arxiv.org/abs/2311.10263v2</link><description>Inferring causal relationships as directed acyclic graphs (DAGs) is animportant but challenging problem. Differentiable Causal Discovery (DCD) is apromising approach to this problem, framing the search as a continuousoptimization. But existing DCD methods are numerically unstable, with poorperformance beyond tens of variables. In this paper, we propose StableDifferentiable Causal Discovery (SDCD), a new method that improves previous DCDmethods in two ways: (1) It employs an alternative constraint for acyclicity;this constraint is more stable, both theoretically and empirically, and fast tocompute. (2) It uses a training procedure tailored for sparse causal graphs,which are common in real-world scenarios. We first derive SDCD and prove itsstability and correctness. We then evaluate it with both observational andinterventional data and on both small-scale and large-scale settings. We findthat SDCD outperforms existing methods in both convergence speed and accuracyand can scale to thousands of variables. We provide code athttps://github.com/azizilab/sdcd.</description><author>Achille Nazaret, Justin Hong, Elham Azizi, David Blei</author><pubDate>Thu, 27 Jun 2024 16:11:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10263v2</guid></item><item><title>Improving the Expressiveness of $K$-hop Message-Passing GNNs by Injecting Contextualized Substructure Information</title><link>http://arxiv.org/abs/2406.19244v1</link><description>Graph neural networks (GNNs) have become the \textit{de facto} standard forrepresentational learning in graphs, and have achieved state-of-the-artperformance in many graph-related tasks; however, it has been shown that theexpressive power of standard GNNs are equivalent maximally to 1-dimensionalWeisfeiler-Lehman (1-WL) Test. Recently, there is a line of works aiming toenhance the expressive power of graph neural networks. One line of such worksaim at developing $K$-hop message-passing GNNs where node representation isupdated by aggregating information from not only direct neighbors but allneighbors within $K$-hop of the node. Another line of works leverages subgraphinformation to enhance the expressive power which is proven to be strictly morepowerful than 1-WL test. In this work, we discuss the limitation of $K$-hopmessage-passing GNNs and propose \textit{substructure encoding function} touplift the expressive power of any $K$-hop message-passing GNN. We furtherinject contextualized substructure information to enhance the expressiveness of$K$-hop message-passing GNNs. Our method is provably more powerful thanprevious works on $K$-hop graph neural networks and 1-WL subgraph GNNs, whichis a specific type of subgraph based GNN models, and not less powerful than3-WL. Empirically, our proposed method set new state-of-the-art performance orachieves comparable performance for a variety of datasets. Our code isavailable at \url{https://github.com/tianyao-aka/Expresive_K_hop_GNNs}.</description><author>Tianjun Yao, Yiongxu Wang, Kun Zhang, Shangsong Liang</author><pubDate>Thu, 27 Jun 2024 16:10:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19244v1</guid></item><item><title>Application of ASV for Voice Identification after VC and Duration Predictor Improvement in TTS Models</title><link>http://arxiv.org/abs/2406.19243v1</link><description>One of the most crucial components in the field of biometric security is theautomatic speaker verification system, which is based on the speaker's voice.It is possible to utilise ASVs in isolation or in conjunction with other AImodels. In the contemporary era, the quality and quantity of neural networksare increasing exponentially. Concurrently, there is a growing number ofsystems that aim to manipulate data through the use of voice conversion andtext-to-speech models. The field of voice biometrics forgery is aided by anumber of challenges, including SSTC, ASVSpoof, and SingFake. This paper presents a system for automatic speaker verification. The primaryobjective of our model is the extraction of embeddings from the targetspeaker's audio in order to obtain information about important characteristicsof his voice, such as pitch, energy, and the duration of phonemes. Thisinformation is used in our multivoice TTS pipeline, which is currently underdevelopment. However, this model was employed within the SSTC challenge toverify users whose voice had undergone voice conversion, where it demonstratedan EER of 20.669.</description><author>Borodin Kirill Nikolayevich, Kudryavtsev Vasiliy Dmitrievich, Mkrtchian Grach Maratovich, Gorodnichev Mikhail Genadievich, Korzh Dmitrii Sergeevich</author><pubDate>Thu, 27 Jun 2024 16:08:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19243v1</guid></item><item><title>S4: Self-Supervised Sensing Across the Spectrum</title><link>http://arxiv.org/abs/2405.01656v2</link><description>Satellite image time series (SITS) segmentation is crucial for manyapplications like environmental monitoring, land cover mapping and agriculturalcrop type classification. However, training models for SITS segmentationremains a challenging task due to the lack of abundant training data, whichrequires fine grained annotation. We propose S4 a new self-supervisedpre-training approach that significantly reduces the requirement for labeledtraining data by utilizing two new insights: (a) Satellites capture images indifferent parts of the spectrum such as radio frequencies, and visiblefrequencies. (b) Satellite imagery is geo-registered allowing for fine-grainedspatial alignment. We use these insights to formulate pre-training tasks in S4.We also curate m2s2-SITS, a large-scale dataset of unlabeled,spatially-aligned, multi-modal and geographic specific SITS that serves asrepresentative pre-training data for S4. Finally, we evaluate S4 on multipleSITS segmentation datasets and demonstrate its efficacy against competingbaselines while using limited labeled data.</description><author>Jayanth Shenoy, Xingjian Davis Zhang, Shlok Mehrotra, Bill Tao, Rem Yang, Han Zhao, Deepak Vasisht</author><pubDate>Thu, 27 Jun 2024 16:07:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01656v2</guid></item><item><title>Local to Global: Learning Dynamics and Effect of Initialization for Transformers</title><link>http://arxiv.org/abs/2406.03072v2</link><description>In recent years, transformer-based models have revolutionized deep learning,particularly in sequence modeling. To better understand this phenomenon, thereis a growing interest in using Markov input processes to study transformers.However, our current understanding in this regard remains limited with manyfundamental questions about how transformers learn Markov chains stillunanswered. In this paper, we address this by focusing on first-order Markovchains and single-layer transformers, providing a comprehensivecharacterization of the learning dynamics in this context. Specifically, weprove that transformer parameters trained on next-token prediction loss caneither converge to global or local minima, contingent on the initialization andthe Markovian data properties, and we characterize the precise conditions underwhich this occurs. To the best of our knowledge, this is the first result ofits kind highlighting the role of initialization. We further demonstrate thatour theoretical findings are corroborated by empirical evidence. Based on theseinsights, we provide guidelines for the initialization of transformerparameters and demonstrate their effectiveness. Finally, we outline severalopen problems in this arena. Code is available at:https://github.com/Bond1995/Markov.</description><author>Ashok Vardhan Makkuva, Marco Bondaschi, Chanakya Ekbote, Adway Girish, Alliot Nagle, Hyeji Kim, Michael Gastpar</author><pubDate>Thu, 27 Jun 2024 16:05:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03072v2</guid></item><item><title>ALMA: a mathematics-driven approach for determining tuning parameters in generalized LASSO problems, with applications to MRI</title><link>http://arxiv.org/abs/2406.19239v1</link><description>Magnetic Resonance Imaging (MRI) is a powerful technique employed fornon-invasive in vivo visualization of internal structures. Sparsity is oftendeployed to accelerate the signal acquisition or overcome the presence ofmotion artifacts, improving the quality of image reconstruction. Imagereconstruction algorithms use TV-regularized LASSO (Total Variation-regularizedLASSO) to retrieve the missing information of undersampled signals, by cleaningthe data of noise and while optimizing sparsity. A tuning parameter moderatesthe balance between these two aspects; its choice affecting the quality of thereconstructions. Currently, there is a lack of general deterministic techniquesto choose these parameters, which are oftentimes manually selected and thushinder the reliability of the reconstructions. Here, we present ALMA (Algorithmfor Lagrange Multipliers Approximation), an iterative mathematics-inspiredtechnique that computes tuning parameters for generalized LASSO problems duringMRI reconstruction. We analyze quantitatively the performance of theseparameters for imaging reconstructions via TV-LASSO in an MRI context onphantoms. Although our study concentrates on TV-LASSO, the techniques developedhere hold significant promise for a wide array of applications. ALMA is notonly adaptable to more generalized LASSO problems but is also robust toaccommodate other forms of regularization beyond total variation. Moreover, itextends effectively to handle non-Cartesian sampling trajectories, broadeningits utility in complex data reconstruction scenarios. More generally, ALMAprovides a powerful tool for numerically solving constrained optimizationproblems across various disciplines, offering a versatile and impactfulsolution for advanced computational challenges.</description><author>Gianluca Giacchi, Isidoros Iakovidis, Bastien Milani, Matthias Stuber, Micah Murray, Benedetta Franceschiello</author><pubDate>Thu, 27 Jun 2024 16:02:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19239v1</guid></item><item><title>Revealing Fine-Grained Values and Opinions in Large Language Models</title><link>http://arxiv.org/abs/2406.19238v1</link><description>Uncovering latent values and opinions in large language models (LLMs) canhelp identify biases and mitigate potential harm. Recently, this has beenapproached by presenting LLMs with survey questions and quantifying theirstances towards morally and politically charged statements. However, thestances generated by LLMs can vary greatly depending on how they are prompted,and there are many ways to argue for or against a given position. In this work,we propose to address this by analysing a large and robust dataset of 156k LLMresponses to the 62 propositions of the Political Compass Test (PCT) generatedby 6 LLMs using 420 prompt variations. We perform coarse-grained analysis oftheir generated stances and fine-grained analysis of the plain textjustifications for those stances. For fine-grained analysis, we propose toidentify tropes in the responses: semantically similar phrases that arerecurrent and consistent across different prompts, revealing patterns in thetext that a given LLM is prone to produce. We find that demographic featuresadded to prompts significantly affect outcomes on the PCT, reflecting bias, aswell as disparities between the results of tests when eliciting closed-form vs.open domain responses. Additionally, patterns in the plain text rationales viatropes show that similar justifications are repeatedly generated across modelsand prompts even with disparate stances.</description><author>Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, Isabelle Augenstein</author><pubDate>Thu, 27 Jun 2024 16:01:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19238v1</guid></item><item><title>FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts</title><link>http://arxiv.org/abs/2406.19237v1</link><description>Existing benchmarks for visual question answering lack in visual groundingand complexity, particularly in evaluating spatial reasoning skills. Weintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities ofvisual question-answering multimodal language models in reasoning withflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated andhuman-verified flowchart images from three distinct content sources, along with22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,including information localization, decision-making, and logical progression.We conduct a thorough baseline evaluation on a suite of both open-source andproprietary multimodal language models using various strategies, followed by ananalysis of directional bias. The results underscore the benchmark's potentialas a vital tool for advancing the field of multimodal modeling, providing afocused and challenging environment for enhancing model performance in visualand logical reasoning tasks.</description><author>Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, Vivek Gupta, Dan Roth</author><pubDate>Thu, 27 Jun 2024 16:01:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19237v1</guid></item><item><title>Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions</title><link>http://arxiv.org/abs/2406.19236v1</link><description>Vision-and-Language Navigation (VLN) aims to develop embodied agents thatnavigate based on human instructions. However, current VLN frameworks oftenrely on static environments and optimal expert supervision, limiting theirreal-world applicability. To address this, we introduce Human-AwareVision-and-Language Navigation (HA-VLN), extending traditional VLN byincorporating dynamic human activities and relaxing key assumptions. We proposethe Human-Aware 3D (HA3D) simulator, which combines dynamic human activitieswith the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R)dataset, extending R2R with human activity descriptions. To tackle HA-VLNchallenges, we present the Expert-Supervised Cross-Modal (VLN-CM) andNon-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizingcross-modal fusion and diverse training strategies for effective navigation indynamic human environments. A comprehensive evaluation, including metricsconsidering human activities, and systematic analysis of HA-VLN's uniquechallenges, underscores the need for further research to enhance HA-VLN agents'real-world robustness and adaptability. Ultimately, this work providesbenchmarks and insights for future research on embodied AI and Sim2Realtransfer, paving the way for more realistic and applicable VLN systems inhuman-populated environments.</description><author>Minghan Li, Heng Li, Zhi-Qi Cheng, Yifei Dong, Yuxuan Zhou, Jun-Yan He, Qi Dai, Teruko Mitamura, Alexander G. Hauptmann</author><pubDate>Thu, 27 Jun 2024 16:01:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19236v1</guid></item><item><title>Automatic infant 2D pose estimation from videos: comparing seven deep neural network methods</title><link>http://arxiv.org/abs/2406.17382v2</link><description>Automatic markerless estimation of infant posture and motion from ordinaryvideos carries great potential for movement studies "in the wild", facilitatingunderstanding of motor development and massively increasing the chances ofearly diagnosis of disorders. There is rapid development of human poseestimation methods in computer vision thanks to advances in deep learning andmachine learning. However, these methods are trained on datasets featuringadults in different contexts. This work tests and compares seven popularmethods (AlphaPose, DeepLabCut/DeeperCut, Detectron2, HRNet,MediaPipe/BlazePose, OpenPose, and ViTPose) on videos of infants in supineposition. Surprisingly, all methods except DeepLabCut and MediaPipe havecompetitive performance without additional finetuning, with ViTPose performingbest. Next to standard performance metrics (object keypoint similarity, averageprecision and recall), we introduce errors expressed in the neck-mid-hip ratioand additionally study missed and redundant detections and the reliability ofthe internal confidence ratings of the different methods, which are relevantfor downstream tasks. Among the networks with competitive performance, onlyAlphaPose could run close to real time (27 fps) on our machine. We providedocumented Docker containers or instructions for all the methods we used, ouranalysis scripts, and processed data at https://hub.docker.com/u/humanoidsctuand https://osf.io/x465b/.</description><author>Filipe Gama, Matej Misar, Lukas Navara, Sergiu T. Popescu, Matej Hoffmann</author><pubDate>Thu, 27 Jun 2024 15:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17382v2</guid></item><item><title>Seeing Is Believing: Black-Box Membership Inference Attacks Against Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2406.19234v1</link><description>Retrieval-Augmented Generation (RAG) is a state-of-the-art technique thatenhances Large Language Models (LLMs) by retrieving relevant knowledge from anexternal, non-parametric database. This approach aims to mitigate common LLMissues such as hallucinations and outdated knowledge. Although existingresearch has demonstrated security and privacy vulnerabilities within RAGsystems, making them susceptible to attacks like jailbreaks and promptinjections, the security of the RAG system's external databases remains largelyunderexplored. In this paper, we employ Membership Inference Attacks (MIA) todetermine whether a sample is part of the knowledge database of a RAG system,using only black-box API access. Our core hypothesis posits that if a sample isa member, it will exhibit significant similarity to the text generated by theRAG system. To test this, we compute the cosine similarity and the model'sperplexity to establish a membership score, thereby building robust features.We then introduce two novel attack strategies: a Threshold-based Attack and aMachine Learning-based Attack, designed to accurately identify membership.Experimental validation of our methods has achieved a ROC AUC of 82%.</description><author>Yuying Li, Gaoyang Liu, Yang Yang, Chen Wang</author><pubDate>Thu, 27 Jun 2024 15:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19234v1</guid></item><item><title>SRC-Net: Bi-Temporal Spatial Relationship Concerned Network for Change Detection</title><link>http://arxiv.org/abs/2406.05668v2</link><description>Change detection (CD) in remote sensing imagery is a crucial task withapplications in environmental monitoring, urban development, and disastermanagement. CD involves utilizing bi-temporal images to identify changes overtime. The bi-temporal spatial relationships between features at the samelocation at different times play a key role in this process. However, existingchange detection networks often do not fully leverage these spatialrelationships during bi-temporal feature extraction and fusion. In this work,we propose SRC-Net: a bi-temporal spatial relationship concerned network forCD. The proposed SRC-Net includes a Perception and Interaction Module thatincorporates spatial relationships and establishes a cross-branch perceptionmechanism to enhance the precision and robustness of feature extraction.Additionally, a Patch-Mode joint Feature Fusion Module is introduced to addressinformation loss in current methods. It considers different change modes andconcerns about spatial relationships, resulting in more expressive fusionfeatures. Furthermore, we construct a novel network using these tworelationship concerned modules and conducted experiments on the LEVIR-CD andWHU Building datasets. The experimental results demonstrate that our networkoutperforms state-of-the-art (SOTA) methods while maintaining a modestparameter count. We believe our approach sets a new paradigm for changedetection and will inspire further advancements in the field. The code andmodels are publicly available at https://github.com/Chnja/SRCNet.</description><author>Hongjia Chen, Xin Xu, Fangling Pu</author><pubDate>Thu, 27 Jun 2024 15:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05668v2</guid></item><item><title>RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs</title><link>http://arxiv.org/abs/2406.19232v1</link><description>Minimal pairs are a well-established approach to evaluating the grammaticalknowledge of language models. However, existing resources for minimal pairsaddress a limited number of languages and lack diversity of language-specificgrammatical phenomena. This paper introduces the Russian Benchmark ofLinguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences thatdiffer in grammaticality and isolate a morphological, syntactic, or semanticphenomenon. In contrast to existing benchmarks of linguistic minimal pairs,RuBLiMP is created by applying linguistic perturbations to automaticallyannotated sentences from open text corpora and carefully curating test data. Wedescribe the data collection protocol and present the results of evaluating 25language models in various scenarios. We find that the widely used languagemodels for Russian are sensitive to morphological and agreement-orientedcontrasts but fall behind humans on phenomena requiring understanding ofstructural relations, negation, transitivity, and tense. RuBLiMP, the codebase,and other materials are publicly available.</description><author>Ekaterina Taktasheva, Maxim Bazhukov, Kirill Koncha, Alena Fenogenova, Ekaterina Artemova</author><pubDate>Thu, 27 Jun 2024 15:55:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19232v1</guid></item><item><title>Spiking Convolutional Neural Networks for Text Classification</title><link>http://arxiv.org/abs/2406.19230v1</link><description>Spiking neural networks (SNNs) offer a promising pathway to implement deepneural networks (DNNs) in a more energy-efficient manner since their neuronsare sparsely activated and inferences are event-driven. However, there havebeen very few works that have demonstrated the efficacy of SNNs in languagetasks partially because it is non-trivial to represent words in the forms ofspikes and to deal with variable-length texts by SNNs. This work presents a"conversion + fine-tuning" two-step method for training SNNs for textclassification and proposes a simple but effective way to encode pre-trainedword embeddings as spike trains. We show empirically that after fine-tuningwith surrogate gradients, the converted SNNs achieve comparable results totheir DNN counterparts with much less energy consumption across multipledatasets for both English and Chinese. We also show that such SNNs are morerobust to adversarial attacks than DNNs.</description><author>Changze Lv, Jianhan Xu, Xiaoqing Zheng</author><pubDate>Thu, 27 Jun 2024 15:54:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19230v1</guid></item><item><title>Tools Fail: Detecting Silent Errors in Faulty Tools</title><link>http://arxiv.org/abs/2406.19228v1</link><description>Tools have become a mainstay of LLMs, allowing them to retrieve knowledge notin their weights, to perform tasks on the web, and even to control robots.However, most ontologies and surveys of tool-use have assumed the corechallenge for LLMs is choosing the tool. Instead, we introduce a framework fortools more broadly which guides us to explore a model's ability to detect"silent" tool errors, and reflect on how to plan. This more directly alignswith the increasingly popular use of models as tools. We provide an initialapproach to failure recovery with promising results both on a controlledcalculator setting and embodied agent planning.</description><author>Jimin Sun, So Yeon Min, Yingshan Chang, Yonatan Bisk</author><pubDate>Thu, 27 Jun 2024 15:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19228v1</guid></item><item><title>Aligning Teacher with Student Preferences for Tailored Training Data Generation</title><link>http://arxiv.org/abs/2406.19227v1</link><description>Large Language Models (LLMs) have shown significant promise as copilots invarious tasks. Local deployment of LLMs on edge devices is necessary whenhandling privacy-sensitive data or latency-sensitive tasks. The computationalconstraints of such devices make direct deployment of powerful large-scale LLMsimpractical, necessitating the Knowledge Distillation from large-scale modelsto lightweight models. Lots of work has been done to elicit diversity andquality training examples from LLMs, but little attention has been paid toaligning teacher instructional content based on student preferences, akin to"responsive teaching" in pedagogy. Thus, we propose ARTE, dubbed AligningTeacheR with StudenT PreferencEs, a framework that aligns the teacher modelwith student preferences to generate tailored training examples for KnowledgeDistillation. Specifically, we elicit draft questions and rationales from theteacher model, then collect student preferences on these questions andrationales using students' performance with in-context learning as a proxy, andfinally align the teacher model with student preferences. In the end, we repeatthe first step with the aligned teacher model to elicit tailored trainingexamples for the student model on the target task. Extensive experiments onacademic benchmarks demonstrate the superiority of ARTE over existinginstruction-tuning datasets distilled from powerful LLMs. Moreover, wethoroughly investigate the generalization of ARTE, including the generalizationof fine-tuned student models in reasoning ability and the generalization ofaligned teacher models to generate tailored training data across tasks andstudents. In summary, our contributions lie in proposing a novel framework fortailored training example generation, demonstrating its efficacy inexperiments, and investigating the generalization of both student &amp; alignedteacher models in ARTE.</description><author>Yantao Liu, Zhao Zhang, Zijun Yao, Shulin Cao, Lei Hou, Juanzi Li</author><pubDate>Thu, 27 Jun 2024 15:51:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19227v1</guid></item><item><title>Simulating Classroom Education with LLM-Empowered Agents</title><link>http://arxiv.org/abs/2406.19226v1</link><description>Large language models (LLMs) have been employed in various intelligenteducational tasks to assist teaching. While preliminary explorations havefocused on independent LLM-empowered agents for specific educational tasks, thepotential for LLMs within a multi-agent collaborative framework to simulate aclassroom with real user participation remains unexplored. In this work, wepropose SimClass, a multi-agent classroom simulation framework involving userparticipation. We recognize representative class roles and introduce a novelclass control mechanism for automatic classroom teaching, and conduct userexperiments in two real-world courses. Utilizing the Flanders InteractiveAnalysis System and Community of Inquiry theoretical frame works fromeducational analysis, we demonstrate that LLMs can simulate traditionalclassroom interaction patterns effectively while enhancing user's experience.We also observe emergent group behaviors among agents in SimClass, where agentscollaborate to create enlivening interactions in classrooms to improve userlearning process. We hope this work pioneers the application of LLM-empoweredmulti-agent systems in virtual classroom teaching.</description><author>Zheyuan Zhang, Daniel Zhang-Li, Jifan Yu, Linlu Gong, Jinchang Zhou, Zhiyuan Liu, Lei Hou, Juanzi Li</author><pubDate>Thu, 27 Jun 2024 15:51:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19226v1</guid></item><item><title>ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model for Semantic Segmentation</title><link>http://arxiv.org/abs/2406.19225v1</link><description>Domain adaptive semantic segmentation aims to generate accurate and densepredictions for an unlabeled target domain by leveraging a supervised modeltrained on a labeled source domain. The prevalent self-training approachinvolves retraining the dense discriminative classifier of $p(class|pixelfeature)$ using the pseudo-labels from the target domain. While many methodsfocus on mitigating the issue of noisy pseudo-labels, they often overlook theunderlying data distribution p(pixel feature|class) in both the source andtarget domains. To address this limitation, we propose the multi-prototypeGaussian-Mixture-based (ProtoGMM) model, which incorporates the GMM intocontrastive losses to perform guided contrastive learning. Contrastive lossesare commonly executed in the literature using memory banks, which can lead toclass biases due to underrepresented classes. Furthermore, memory banks oftenhave fixed capacities, potentially restricting the model's ability to capturediverse representations of the target/source domains. An alternative approachis to use global class prototypes (i.e. averaged features per category).However, the global prototypes are based on the unimodal distributionassumption per class, disregarding within-class variation. To address thesechallenges, we propose the ProtoGMM model. This novel approach involvesestimating the underlying multi-prototype source distribution by utilizing theGMM on the feature space of the source samples. The components of the GMM modelact as representative prototypes. To achieve increased intra-class semanticsimilarity, decreased inter-class similarity, and domain alignment between thesource and target domains, we employ multi-prototype contrastive learningbetween source distribution and target samples. The experiments show theeffectiveness of our method on UDA benchmarks.</description><author>Nazanin Moradinasab, Laura S. Shankman, Rebecca A. Deaton, Gary K. Owens, Donald E. Brown</author><pubDate>Thu, 27 Jun 2024 15:50:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19225v1</guid></item><item><title>T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings</title><link>http://arxiv.org/abs/2406.19223v1</link><description>Tokenizers are crucial for encoding information in Large Language Models, buttheir development has recently stagnated, and they contain inherent weaknesses.Major limitations include computational overhead, ineffective vocabulary use,and unnecessarily large embedding and head layers. Additionally, theirperformance is biased towards a reference corpus, leading to reducedeffectiveness for underrepresented languages. To remedy these issues, we propose T-FREE, which directly embeds wordsthrough sparse activation patterns over character triplets, and does notrequire a reference corpus. T-FREE inherently exploits morphologicalsimilarities and allows for strong compression of embedding layers. In ourexhaustive experimental evaluation, we achieve competitive downstreamperformance with a parameter reduction of more than 85% on these layers.Further, T-FREE shows significant improvements in cross-lingual transferlearning.</description><author>BjÃ¶rn Deiseroth, Manuel Brack, Patrick Schramowski, Kristian Kersting, Samuel Weinbach</author><pubDate>Thu, 27 Jun 2024 15:49:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19223v1</guid></item><item><title>Hack Me If You Can: Aggregating AutoEncoders for Countering Persistent Access Threats Within Highly Imbalanced Data</title><link>http://arxiv.org/abs/2406.19220v1</link><description>Advanced Persistent Threats (APTs) are sophisticated, targeted cyberattacksdesigned to gain unauthorized access to systems and remain undetected forextended periods. To evade detection, APT cyberattacks deceive defense layerswith breaches and exploits, thereby complicating exposure by traditionalanomaly detection-based security methods. The challenge of detecting APTs withmachine learning is compounded by the rarity of relevant datasets and thesignificant imbalance in the data, which makes the detection process highlyburdensome. We present AE-APT, a deep learning-based tool for APT detectionthat features a family of AutoEncoder methods ranging from a basic one to aTransformer-based one. We evaluated our tool on a suite of provenance tracedatabases produced by the DARPA Transparent Computing program, where APT-likeattacks constitute as little as 0.004% of the data. The datasets span multipleoperating systems, including Android, Linux, BSD, and Windows, and cover twoattack scenarios. The outcomes showed that AE-APT has significantly higherdetection rates compared to its competitors, indicating superior performance indetecting and ranking anomalies.</description><author>Sidahmed Benabderrahmane, Ngoc Hoang, Petko Valtchev, James Cheney, Talal Rahwan</author><pubDate>Thu, 27 Jun 2024 15:45:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19220v1</guid></item><item><title>CLIMATELI: Evaluating Entity Linking on Climate Change Data</title><link>http://arxiv.org/abs/2406.16732v2</link><description>Climate Change (CC) is a pressing topic of global importance, attractingincreasing attention across research fields, from social sciences to NaturalLanguage Processing (NLP). CC is also discussed in various settings andcommunication platforms, from academic publications to social media forums.Understanding who and what is mentioned in such data is a first critical stepto gaining new insights into CC. We present CLIMATELI (CLIMATe Entity LInking),the first manually annotated CC dataset that links 3,087 entity spans toWikipedia. Using CLIMATELI (CLIMATe Entity LInking), we evaluate existingentity linking (EL) systems on the CC topic across various genres and proposeautomated filtering methods for CC entities. We find that the performance of ELmodels notably lags behind humans at both token and entity levels. Testingwithin the scope of retaining or excluding non-nominal and/or non-CC entitiesparticularly impacts the models' performances.</description><author>Shijia Zhou, Siyao Peng, Barbara Plank</author><pubDate>Thu, 27 Jun 2024 15:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16732v2</guid></item><item><title>Think Step by Step: Chain-of-Gesture Prompting for Error Detection in Robotic Surgical Videos</title><link>http://arxiv.org/abs/2406.19217v1</link><description>Despite significant advancements in robotic systems and surgical datascience, ensuring safe and optimal execution in robot-assisted minimallyinvasive surgery (RMIS) remains a complex challenge. Current surgical errordetection methods involve two parts: identifying surgical gestures and thendetecting errors within each gesture clip. These methods seldom consider therich contextual and semantic information inherent in surgical videos, limitingtheir performance due to reliance on accurate gesture identification. Motivatedby the chain-of-thought prompting in natural language processing, this letterpresents a novel and real-time end-to-end error detection framework,Chain-of-Thought (COG) prompting, leveraging contextual information fromsurgical videos. This encompasses two reasoning modules designed to mimic thedecision-making processes of expert surgeons. Concretely, we first design aGestural-Visual Reasoning module, which utilizes transformer and attentionarchitectures for gesture prompting, while the second, a Multi-Scale TemporalReasoning module, employs a multi-stage temporal convolutional network withboth slow and fast paths for temporal information extraction. We extensivelyvalidate our method on the public benchmark RMIS dataset JIGSAWS. Our methodencapsulates the reasoning processes inherent to surgical activities enablingit to outperform the state-of-the-art by 4.6% in F1 score, 4.6% in Accuracy,and 5.9% in Jaccard index while processing each frame in 6.69 milliseconds onaverage, demonstrating the great potential of our approach in enhancing thesafety and efficacy of RMIS procedures and surgical education. The code will beavailable.</description><author>Zhimin Shao, Jialang Xu, Danail Stoyanov, Evangelos B. Mazomenos, Yueming Jin</author><pubDate>Thu, 27 Jun 2024 15:43:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19217v1</guid></item></channel></rss>