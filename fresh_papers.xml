<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 18 Jun 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video</title><link>http://arxiv.org/abs/2306.09349v1</link><description>We show how to build a model that allows realistic, free-viewpoint renderingsof a scene under novel lighting conditions from video. Our method -- UrbanIR:Urban Scene Inverse Rendering -- computes an inverse graphics representationfrom the video. UrbanIR jointly infers shape, albedo, visibility, and sun andsky illumination from a single video of unbounded outdoor scenes with unknownlighting. UrbanIR uses videos from cameras mounted on cars (in contrast to manyviews of the same points in typical NeRF-style estimation). As a result,standard methods produce poor geometry estimates (for example, roofs), andthere are numerous ''floaters''. Errors in inverse graphics inference canresult in strong rendering artifacts. UrbanIR uses novel losses to controlthese and other sources of error. UrbanIR uses a novel loss to make very goodestimates of shadow volumes in the original scene. The resultingrepresentations facilitate controllable editing, delivering photorealisticfree-viewpoint renderings of relit scenes and inserted objects. Qualitativeevaluation demonstrates strong improvements over the state-of-the-art.</description><author>Zhi-Hao Lin, Bohan Liu, Yi-Ting Chen, David Forsyth, Jia-Bin Huang, Anand Bhattad, Shenlong Wang</author><pubDate>Thu, 15 Jun 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09349v1</guid></item><item><title>Seeing the World through Your Eyes</title><link>http://arxiv.org/abs/2306.09348v1</link><description>The reflective nature of the human eye is an underappreciated source ofinformation about what the world around us looks like. By imaging the eyes of amoving person, we can collect multiple views of a scene outside the camera'sdirect line of sight through the reflections in the eyes. In this paper, wereconstruct a 3D scene beyond the camera's line of sight using portrait imagescontaining eye reflections. This task is challenging due to 1) the difficultyof accurately estimating eye poses and 2) the entangled appearance of the eyeiris and the scene reflections. Our method jointly refines the cornea poses,the radiance field depicting the scene, and the observer's eye iris texture. Wefurther propose a simple regularization prior on the iris texture pattern toimprove reconstruction quality. Through various experiments on synthetic andreal-world captures featuring people with varied eye colors, we demonstrate thefeasibility of our approach to recover 3D scenes using eye reflections.</description><author>Hadi Alzayer, Kevin Zhang, Brandon Feng, Christopher Metzler, Jia-Bin Huang</author><pubDate>Thu, 15 Jun 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09348v1</guid></item><item><title>Rosetta Neurons: Mining the Common Units in a Model Zoo</title><link>http://arxiv.org/abs/2306.09346v1</link><description>Do different neural networks, trained for various vision tasks, share somecommon representations? In this paper, we demonstrate the existence of common features we call"Rosetta Neurons" across a range of models with different architectures,different tasks (generative and discriminative), and different types ofsupervision (class-supervised, text-supervised, self-supervised). We present analgorithm for mining a dictionary of Rosetta Neurons across several popularvision models: Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50,BigGAN, StyleGAN-2, StyleGAN-XL. Our findings suggest that certain visual concepts and structures areinherently embedded in the natural world and can be learned by different modelsregardless of the specific task or architecture, and without the use ofsemantic labels. We can visualize shared concepts directly due to generativemodels included in our analysis. The Rosetta Neurons facilitate model-to-modeltranslation enabling various inversion-based manipulations, includingcross-class alignments, shifting, zooming, and more, without the need forspecialized training.</description><author>Amil Dravid, Yossi Gandelsman, Alexei Efros, Assaf Shocher</author><pubDate>Thu, 15 Jun 2023 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09346v1</guid></item><item><title>Segment Any Point Cloud Sequences by Distilling Vision Foundation Models</title><link>http://arxiv.org/abs/2306.09347v1</link><description>Recent advancements in vision foundation models (VFMs) have opened up newpossibilities for versatile and efficient visual perception. In this work, weintroduce Seal, a novel framework that harnesses VFMs for segmenting diverseautomotive point cloud sequences. Seal exhibits three appealing properties: i)Scalability: VFMs are directly distilled into point clouds, eliminating theneed for annotations in either 2D or 3D during pretraining. ii) Consistency:Spatial and temporal relationships are enforced at both the camera-to-LiDAR andpoint-to-segment stages, facilitating cross-modal representation learning. iii)Generalizability: Seal enables knowledge transfer in an off-the-shelf manner todownstream tasks involving diverse point clouds, including those fromreal/synthetic, low/high-resolution, large/small-scale, and clean/corrupteddatasets. Extensive experiments conducted on eleven different point clouddatasets showcase the effectiveness and superiority of Seal. Notably, Sealachieves a remarkable 45.0% mIoU on nuScenes after linear probing, surpassingrandom initialization by 36.9% mIoU and outperforming prior arts by 6.1% mIoU.Moreover, Seal demonstrates significant performance gains over existing methodsacross 20 different few-shot fine-tuning tasks on all eleven tested point clouddatasets.</description><author>Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu</author><pubDate>Thu, 15 Jun 2023 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09347v1</guid></item><item><title>Evaluating Data Attribution for Text-to-Image Models</title><link>http://arxiv.org/abs/2306.09345v1</link><description>While large text-to-image models are able to synthesize "novel" images, theseimages are necessarily a reflection of the training data. The problem of dataattribution in such models -- which of the images in the training set are mostresponsible for the appearance of a given generated image -- is a difficult yetimportant one. As an initial step toward this problem, we evaluate attributionthrough "customization" methods, which tune an existing large-scale modeltoward a given exemplar object or style. Our key insight is that this allows usto efficiently create synthetic images that are computationally influenced bythe exemplar by construction. With our new dataset of such exemplar-influencedimages, we are able to evaluate various data attribution algorithms anddifferent possible feature spaces. Furthermore, by training on our dataset, wecan tune standard models, such as DINO, CLIP, and ViT, toward the attributionproblem. Even though the procedure is tuned towards small exemplar sets, weshow generalization to larger sets. Finally, by taking into account theinherent uncertainty of the problem, we can assign soft attribution scores overa set of training images.</description><author>Sheng-Yu Wang, Alexei A. Efros, Jun-Yan Zhu, Richard Zhang</author><pubDate>Thu, 15 Jun 2023 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09345v1</guid></item><item><title>DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data</title><link>http://arxiv.org/abs/2306.09344v1</link><description>Current perceptual similarity metrics operate at the level of pixels andpatches. These metrics compare images in terms of their low-level colors andtextures, but fail to capture mid-level similarities and differences in imagelayout, object pose, and semantic content. In this paper, we develop aperceptual metric that assesses images holistically. Our first step is tocollect a new dataset of human similarity judgments over image pairs that arealike in diverse ways. Critical to this dataset is that judgments are nearlyautomatic and shared by all observers. To achieve this we use recenttext-to-image models to create synthetic pairs that are perturbed along variousdimensions. We observe that popular perceptual metrics fall short of explainingour new data, and we introduce a new metric, DreamSim, tuned to better alignwith human perception. We analyze how our metric is affected by differentvisual attributes, and find that it focuses heavily on foreground objects andsemantic content while also being sensitive to color and layout. Notably,despite being trained on synthetic data, our metric generalizes to real images,giving strong results on retrieval and reconstruction tasks. Furthermore, ourmetric outperforms both prior learned metrics and recent large vision models onthese tasks.</description><author>Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, Phillip Isola</author><pubDate>Thu, 15 Jun 2023 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09344v1</guid></item><item><title>SIGHT: A Large Annotated Dataset on Student Insights Gathered from Higher Education Transcripts</title><link>http://arxiv.org/abs/2306.09343v1</link><description>Lectures are a learning experience for both students and teachers. Studentslearn from teachers about the subject material, while teachers learn fromstudents about how to refine their instruction. However, online studentfeedback is unstructured and abundant, making it challenging for teachers tolearn and improve. We take a step towards tackling this challenge. First, wecontribute a dataset for studying this problem: SIGHT is a large dataset of 288math lecture transcripts and 15,784 comments collected from the MassachusettsInstitute of Technology OpenCourseWare (MIT OCW) YouTube channel. Second, wedevelop a rubric for categorizing feedback types using qualitative analysis.Qualitative analysis methods are powerful in uncovering domain-specificinsights, however they are costly to apply to large data sources. To overcomethis challenge, we propose a set of best practices for using large languagemodels (LLMs) to cheaply classify the comments at scale. We observe a strikingcorrelation between the model's and humans' annotation: Categories withconsistent human annotations (&gt;$0.9$ inter-rater reliability, IRR) also displayhigher human-model agreement (&gt;$0.7$), while categories with less consistenthuman annotations ($0.7$-$0.8$ IRR) correspondingly demonstrate lowerhuman-model agreement ($0.3$-$0.5$). These techniques uncover useful studentfeedback from thousands of comments, costing around $\$0.002$ per comment. Weconclude by discussing exciting future directions on using online studentfeedback and improving automated annotation techniques for qualitativeresearch.</description><author>Rose E. Wang, Pawan Wirawarn, Noah Goodman, Dorottya Demszky</author><pubDate>Thu, 15 Jun 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09343v1</guid></item><item><title>PaReprop: Fast Parallelized Reversible Backpropagation</title><link>http://arxiv.org/abs/2306.09342v1</link><description>The growing size of datasets and deep learning models has made faster andmemory-efficient training crucial. Reversible transformers have recently beenintroduced as an exciting new method for extremely memory-efficient training,but they come with an additional computation overhead of activationre-computation in the backpropagation phase. We present PaReprop, a fastParallelized Reversible Backpropagation algorithm that parallelizes theadditional activation re-computation overhead in reversible training with thegradient computation itself in backpropagation phase. We demonstrate theeffectiveness of the proposed PaReprop algorithm through extensive benchmarkingacross model families (ViT, MViT, Swin and RoBERTa), data modalities (Vision &amp;NLP), model sizes (from small to giant), and training batch sizes. Ourempirical results show that PaReprop achieves up to 20% higher trainingthroughput than vanilla reversible training, largely mitigating the theoreticaloverhead of 25% lower throughput from activation recomputation in reversibletraining. Project page: https://tylerzhu.com/pareprop.</description><author>Tyler Zhu, Karttikeya Mangalam</author><pubDate>Thu, 15 Jun 2023 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09342v1</guid></item><item><title>Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis</title><link>http://arxiv.org/abs/2306.09341v1</link><description>Recent text-to-image generative models can generate high-fidelity images fromtext inputs, but the quality of these generated images cannot be accuratelyevaluated by existing evaluation metrics. To address this issue, we introduceHuman Preference Dataset v2 (HPD v2), a large-scale dataset that captures humanpreferences on images from a wide range of sources. HPD v2 comprises 798,090human preference choices on 430,060 pairs of images, making it the largestdataset of its kind. The text prompts and images are deliberately collected toeliminate potential bias, which is a common issue in previous datasets. Byfine-tuning CLIP on HPD v2, we obtain Human Preference Score v2 (HPS v2), ascoring model that can more accurately predict text-generated images' humanpreferences. Our experiments demonstrate that HPS v2 generalizes better thanprevious metrics across various image distributions and is responsive toalgorithmic improvements of text-to-image generative models, making it apreferable evaluation metric for these models. We also investigate the designof the evaluation prompts for text-to-image generative models, to make theevaluation stable, fair and easy-to-use. Finally, we establish a benchmark fortext-to-image generative models using HPS v2, which includes a set of recenttext-to-image models from the academia, community and industry. The code anddataset is / will be available at https://github.com/tgxs002/HPSv2.</description><author>Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, Hongsheng Li</author><pubDate>Thu, 15 Jun 2023 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09341v1</guid></item><item><title>Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking</title><link>http://arxiv.org/abs/2306.09340v1</link><description>In schema-guided dialogue state tracking models estimate the current state ofa conversation using natural language descriptions of the service schema forgeneralization to unseen services. Prior generative approaches which decodeslot values sequentially do not generalize well to variations in schema, whilediscriminative approaches separately encode history and schema and fail toaccount for inter-slot and intent-slot dependencies. We introduce SPLAT, anovel architecture which achieves better generalization and efficiency thanprior approaches by constraining outputs to a limited prediction space. At thesame time, our model allows for rich attention among descriptions and historywhile keeping computation costs constrained by incorporating linear-timeattention. We demonstrate the effectiveness of our model on the Schema-GuidedDialogue (SGD) and MultiWOZ datasets. Our approach significantly improves uponexisting models achieving 85.3 JGA on the SGD dataset. Further, we showincreased robustness on the SGD-X benchmark: our model outperforms the morethan 30$\times$ larger D3ST-XXL model by 5.0 points.</description><author>Björn Bebensee, Haejun Lee</author><pubDate>Thu, 15 Jun 2023 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09340v1</guid></item><item><title>Understanding Optimization of Deep Learning</title><link>http://arxiv.org/abs/2306.09338v1</link><description>This article provides a comprehensive understanding of optimization in deeplearning, with a primary focus on the challenges of gradient vanishing andgradient exploding, which normally lead to diminished model representationalability and training instability, respectively. We analyze these two challengesthrough several strategic measures, including the improvement of gradient flowand the imposition of constraints on a network's Lipschitz constant. To helpunderstand the current optimization methodologies, we categorize them into twoclasses: explicit optimization and implicit optimization. Explicit optimizationmethods involve direct manipulation of optimizer parameters, including weight,gradient, learning rate, and weight decay. Implicit optimization methods, bycontrast, focus on improving the overall landscape of a network by enhancingits modules, such as residual shortcuts, normalization methods, attentionmechanisms, and activations. In this article, we provide an in-depth analysisof these two optimization classes and undertake a thorough examination of theJacobian matrices and the Lipschitz constants of many widely used deep learningmodules, highlighting existing issues as well as potential improvements.Moreover, we also conduct a series of analytical experiments to substantiateour theoretical discussions. This article does not aim to propose a newoptimizer or network. Rather, our intention is to present a comprehensiveunderstanding of optimization in deep learning. We hope that this article willassist readers in gaining a deeper insight in this field and encourages thedevelopment of more robust, efficient, and high-performing models.</description><author>Xianbiao Qi, Jianan Wang, Lei Zhang</author><pubDate>Thu, 15 Jun 2023 18:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09338v1</guid></item><item><title>Generative Proxemics: A Prior for 3D Social Interaction from Images</title><link>http://arxiv.org/abs/2306.09337v1</link><description>Social interaction is a fundamental aspect of human behavior andcommunication. The way individuals position themselves in relation to others,also known as proxemics, conveys social cues and affects the dynamics of socialinteraction. We present a novel approach that learns a 3D proxemics prior oftwo people in close social interaction. Since collecting a large 3D dataset ofinteracting people is a challenge, we rely on 2D image collections where socialinteractions are abundant. We achieve this by reconstructing pseudo-groundtruth 3D meshes of interacting people from images with an optimization approachusing existing ground-truth contact maps. We then model the proxemics using anovel denoising diffusion model called BUDDI that learns the joint distributionof two people in close social interaction directly in the SMPL-X parameterspace. Sampling from our generative proxemics model produces realistic 3D humaninteractions, which we validate through a user study. Additionally, weintroduce a new optimization method that uses the diffusion prior toreconstruct two people in close proximity from a single image without anycontact annotation. Our approach recovers more accurate and plausible 3D socialinteractions from noisy initial estimates and outperforms state-of-the-artmethods. See our project site for code, data, and model:muelea.github.io/buddi.</description><author>Lea Müller, Vickie Ye, Georgios Pavlakos, Michael Black, Angjoo Kanazawa</author><pubDate>Thu, 15 Jun 2023 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09337v1</guid></item><item><title>Class-Conditional Conformal Prediction With Many Classes</title><link>http://arxiv.org/abs/2306.09335v1</link><description>Standard conformal prediction methods provide a marginal coverage guarantee,which means that for a random test point, the conformal prediction set containsthe true label with a user-chosen probability. In many classification problems,we would like to obtain a stronger guarantee -- that for test points of aspecific class, the prediction set contains the true label with the sameuser-chosen probability. Existing conformal prediction methods do not work wellwhen there is a limited amount of labeled data per class, as is often the casein real applications where the number of classes is large. We propose a methodcalled clustered conformal prediction, which clusters together classes thathave "similar" conformal scores and then performs conformal prediction at thecluster level. Based on empirical evaluation across four image data sets withmany (up to 1000) classes, we find that clustered conformal typicallyoutperforms existing methods in terms of class-conditional coverage and setsize metrics.</description><author>Tiffany Ding, Anastasios N. Angelopoulos, Stephen Bates, Michael I. Jordan, Ryan J. Tibshirani</author><pubDate>Thu, 15 Jun 2023 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09335v1</guid></item><item><title>Personalized Image Enhancement Featuring Masked Style Modeling</title><link>http://arxiv.org/abs/2306.09334v1</link><description>We address personalized image enhancement in this study, where we enhanceinput images for each user based on the user's preferred images. Previousmethods apply the same preferred style to all input images (i.e., only onestyle for each user); in contrast to these methods, we aim to achievecontent-aware personalization by applying different styles to each imageconsidering the contents. For content-aware personalization, we make twocontributions. First, we propose a method named masked style modeling, whichcan predict a style for an input image considering the contents by using theframework of masked language modeling. Second, to allow this model to considerthe contents of images, we propose a novel training scheme where we downloadimages from Flickr and create pseudo input and retouched image pairs using adegrading model. We conduct quantitative evaluations and a user study, and ourmethod trained using our training scheme successfully achieves content-awarepersonalization; moreover, our method outperforms other previous methods inthis field. Our source code is available athttps://github.com/satoshi-kosugi/masked-style-modeling.</description><author>Satoshi Kosugi, Toshihiko Yamasaki</author><pubDate>Thu, 15 Jun 2023 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09334v1</guid></item><item><title>Fit Like You Sample: Sample-Efficient Generalized Score Matching from Fast Mixing Markov Chains</title><link>http://arxiv.org/abs/2306.09332v1</link><description>Score matching is an approach to learning probability distributionsparametrized up to a constant of proportionality (e.g. Energy-Based Models).The idea is to fit the score of the distribution, rather than the likelihood,thus avoiding the need to evaluate the constant of proportionality. Whilethere's a clear algorithmic benefit, the statistical "cost'' can be steep:recent work by Koehler et al. 2022 showed that for distributions that have poorisoperimetric properties (a large Poincar\'e or log-Sobolev constant), scorematching is substantially statistically less efficient than maximum likelihood.However, many natural realistic distributions, e.g. multimodal distributions assimple as a mixture of two Gaussians in one dimension -- have a poor Poincar\'econstant. In this paper, we show a close connection between the mixing time of anarbitrary Markov process with generator $\mathcal{L}$ and a generalized scorematching loss that tries to fit $\frac{\mathcal{O} p}{p}$. If $\mathcal{L}$corresponds to a Markov process corresponding to a continuous version ofsimulated tempering, we show the corresponding generalized score matching lossis a Gaussian-convolution annealed score matching loss, akin to the oneproposed in Song and Ermon 2019. Moreover, we show that if the distributionbeing learned is a finite mixture of Gaussians in $d$ dimensions with a sharedcovariance, the sample complexity of annealed score matching is polynomial inthe ambient dimension, the diameter the means, and the smallest and largesteigenvalues of the covariance -- obviating the Poincar\'e constant-based lowerbounds of the basic score matching loss shown in Koehler et al. 2022. This isthe first result characterizing the benefits of annealing for score matching --a crucial component in more sophisticated score-based approaches like Song andErmon 2019.</description><author>Yilong Qin, Andrej Risteski</author><pubDate>Thu, 15 Jun 2023 18:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09332v1</guid></item><item><title>ArtFusion: Arbitrary Style Transfer using Dual Conditional Latent Diffusion Models</title><link>http://arxiv.org/abs/2306.09330v1</link><description>Arbitrary Style Transfer (AST) aims to transform images by adopting the stylefrom any selected artwork. Nonetheless, the need to accommodate diverse andsubjective user preferences poses a significant challenge. While some userswish to preserve distinct content structures, others might favor a morepronounced stylization. Despite advances in feed-forward AST methods, theirlimited customizability hinders their practical application. We propose a newapproach, ArtFusion, which provides a flexible balance between content andstyle. In contrast to traditional methods reliant on biased similarity losses,ArtFusion utilizes our innovative Dual Conditional Latent DiffusionProbabilistic Models (Dual-cLDM). This approach mitigates repetitive patternsand enhances subtle artistic aspects like brush strokes and genre-specificfeatures. Despite the promising results of conditional diffusion probabilisticmodels (cDM) in various generative tasks, their introduction to style transferis challenging due to the requirement for paired training data. ArtFusionsuccessfully navigates this issue, offering more practical and controllablestylization. A key element of our approach involves using a single image forboth content and style during model training, all the while maintainingeffective stylization during inference. ArtFusion outperforms existingapproaches on outstanding controllability and faithful presentation of artisticdetails, providing evidence of its superior style transfer capabilities.Furthermore, the Dual-cLDM utilized in ArtFusion carries the potential for avariety of complex multi-condition generative tasks, thus greatly broadeningthe impact of our research.</description><author>Dar-Yen Chen</author><pubDate>Thu, 15 Jun 2023 18:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09330v1</guid></item><item><title>DreamHuman: Animatable 3D Avatars from Text</title><link>http://arxiv.org/abs/2306.09329v1</link><description>We present DreamHuman, a method to generate realistic animatable 3D humanavatar models solely from textual descriptions. Recent text-to-3D methods havemade considerable strides in generation, but are still lacking in importantaspects. Control and often spatial resolution remain limited, existing methodsproduce fixed rather than animated 3D human models, and anthropometricconsistency for complex structures like people remains a challenge. DreamHumanconnects large text-to-image synthesis models, neural radiance fields, andstatistical human body models in a novel modeling and optimization framework.This makes it possible to generate dynamic 3D human avatars with high-qualitytextures and learned, instance-specific, surface deformations. We demonstratethat our method is capable to generate a wide variety of animatable, realistic3D human models from text. Our 3D models have diverse appearance, clothing,skin tones and body shapes, and significantly outperform both generictext-to-3D approaches and previous text-based 3D avatar generators in visualfidelity. For more results and animations please check our website athttps://dream-human.github.io.</description><author>Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Fieraru, Cristian Sminchisescu</author><pubDate>Thu, 15 Jun 2023 18:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09329v1</guid></item><item><title>WizMap: Scalable Interactive Visualization for Exploring Large Machine Learning Embeddings</title><link>http://arxiv.org/abs/2306.09328v1</link><description>Machine learning models often learn latent embedding representations thatcapture the domain semantics of their training data. These embeddingrepresentations are valuable for interpreting trained models, building newmodels, and analyzing new datasets. However, interpreting and using embeddingscan be challenging due to their opaqueness, high dimensionality, and the largesize of modern datasets. To tackle these challenges, we present WizMap, aninteractive visualization tool to help researchers and practitioners easilyexplore large embeddings. With a novel multi-resolution embedding summarizationmethod and a familiar map-like interaction design, WizMap enables users tonavigate and interpret embedding spaces with ease. Leveraging modern webtechnologies such as WebGL and Web Workers, WizMap scales to millions ofembedding points directly in users' web browsers and computational notebookswithout the need for dedicated backend servers. WizMap is open-source andavailable at the following public demo link: https://poloclub.github.io/wizmap.</description><author>Zijie J. Wang, Fred Hohman, Duen Horng Chau</author><pubDate>Thu, 15 Jun 2023 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09328v1</guid></item><item><title>Language-Guided Music Recommendation for Video via Prompt Analogies</title><link>http://arxiv.org/abs/2306.09327v1</link><description>We propose a method to recommend music for an input video while allowing auser to guide music selection with free-form natural language. A key challengeof this problem setting is that existing music video datasets provide theneeded (video, music) training pairs, but lack text descriptions of the music.This work addresses this challenge with the following three contributions.First, we propose a text-synthesis approach that relies on an analogy-basedprompting procedure to generate natural language music descriptions from alarge-scale language model (BLOOM-176B) given pre-trained music tagger outputsand a small number of human text descriptions. Second, we use these synthesizedmusic descriptions to train a new trimodal model, which fuses text and videoinput representations to query music samples. For training, we introduce a textdropout regularization mechanism which we show is critical to modelperformance. Our model design allows for the retrieved music audio to agreewith the two input modalities by matching visual style depicted in the videoand musical genre, mood, or instrumentation described in the natural languagequery. Third, to evaluate our approach, we collect a testing dataset for ourproblem by annotating a subset of 4k clips from the YT8M-MusicVideo datasetwith natural language music descriptions which we make publicly available. Weshow that our approach can match or exceed the performance of prior methods onvideo-to-music retrieval while significantly improving retrieval accuracy whenusing text guidance.</description><author>Daniel McKee, Justin Salamon, Josef Sivic, Bryan Russell</author><pubDate>Thu, 15 Jun 2023 18:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09327v1</guid></item><item><title>Single-Stage Visual Query Localization in Egocentric Videos</title><link>http://arxiv.org/abs/2306.09324v1</link><description>Visual Query Localization on long-form egocentric videos requiresspatio-temporal search and localization of visually specified objects and isvital to build episodic memory systems. Prior work develops complex multi-stagepipelines that leverage well-established object detection and tracking methodsto perform VQL. However, each stage is independently trained and the complexityof the pipeline results in slow inference speeds. We propose VQLoC, a novelsingle-stage VQL framework that is end-to-end trainable. Our key idea is tofirst build a holistic understanding of the query-video relationship and thenperform spatio-temporal localization in a single shot manner. Specifically, weestablish the query-video relationship by jointly considering query-to-framecorrespondences between the query and each video frame and frame-to-framecorrespondences between nearby video frames. Our experiments demonstrate thatour approach outperforms prior VQL methods by 20% accuracy while obtaining a10x improvement in inference speed. VQLoC is also the top entry on the Ego4DVQ2D challenge leaderboard. Project page: https://hwjiang1510.github.io/VQLoC/</description><author>Hanwen Jiang, Santhosh Kumar Ramakrishnan, Kristen Grauman</author><pubDate>Thu, 15 Jun 2023 18:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09324v1</guid></item><item><title>On Pre-Training for Visuo-Motor Control: Revisiting a Learning-from-Scratch Baseline</title><link>http://arxiv.org/abs/2212.05749v2</link><description>In this paper, we examine the effectiveness of pre-training for visuo-motorcontrol tasks. We revisit a simple Learning-from-Scratch (LfS) baseline thatincorporates data augmentation and a shallow ConvNet, and find that thisbaseline is surprisingly competitive with recent approaches (PVR, MVP, R3M)that leverage frozen visual representations trained on large-scale visiondatasets -- across a variety of algorithms, task domains, and metrics insimulation and on a real robot. Our results demonstrate that these methods arehindered by a significant domain gap between the pre-training datasets andcurrent benchmarks for visuo-motor control, which is alleviated by finetuning.Based on our findings, we provide recommendations for future research inpre-training for control and hope that our simple yet strong baseline will aidin accurately benchmarking progress in this area.</description><author>Nicklas Hansen, Zhecheng Yuan, Yanjie Ze, Tongzhou Mu, Aravind Rajeswaran, Hao Su, Huazhe Xu, Xiaolong Wang</author><pubDate>Thu, 15 Jun 2023 18:57:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.05749v2</guid></item><item><title>On the Feasibility of Cross-Task Transfer with Model-Based Reinforcement Learning</title><link>http://arxiv.org/abs/2210.10763v2</link><description>Reinforcement Learning (RL) algorithms can solve challenging control problemsdirectly from image observations, but they often require millions ofenvironment interactions to do so. Recently, model-based RL algorithms havegreatly improved sample-efficiency by concurrently learning an internal modelof the world, and supplementing real environment interactions with imaginedrollouts for policy improvement. However, learning an effective model of theworld from scratch is challenging, and in stark contrast to humans that relyheavily on world understanding and visual cues for learning new skills. In thiswork, we investigate whether internal models learned by modern model-based RLalgorithms can be leveraged to solve new, distinctly different tasks faster. Wepropose Model-Based Cross-Task Transfer (XTRA), a framework forsample-efficient online RL with scalable pretraining and finetuning of learnedworld models. By offline multi-task pretraining and online cross-taskfinetuning, we achieve substantial improvements over a baseline trained fromscratch; we improve mean performance of model-based algorithm EfficientZero by23%, and by as much as 71% in some instances.</description><author>Yifan Xu, Nicklas Hansen, Zirui Wang, Yung-Chieh Chan, Hao Su, Zhuowen Tu</author><pubDate>Thu, 15 Jun 2023 18:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.10763v2</guid></item><item><title>Neural Relighting with Subsurface Scattering by Learning the Radiance Transfer Gradient</title><link>http://arxiv.org/abs/2306.09322v1</link><description>Reconstructing and relighting objects and scenes under varying lightingconditions is challenging: existing neural rendering methods often cannothandle the complex interactions between materials and light. Incorporatingpre-computed radiance transfer techniques enables global illumination, butstill struggles with materials with subsurface scattering effects. We propose anovel framework for learning the radiance transfer field via volume renderingand utilizing various appearance cues to refine geometry end-to-end. Thisframework extends relighting and reconstruction capabilities to handle a widerrange of materials in a data-driven fashion. The resulting models produceplausible rendering results in existing and novel conditions. We will releaseour code and a novel light stage dataset of objects with subsurface scatteringeffects publicly available.</description><author>Shizhan Zhu, Shunsuke Saito, Aljaz Bozic, Carlos Aliaga, Trevor Darrell, Christop Lassner</author><pubDate>Thu, 15 Jun 2023 18:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09322v1</guid></item><item><title>Crowd-Powered Photo Enhancement Featuring an Active Learning Based Local Filter</title><link>http://arxiv.org/abs/2306.09321v1</link><description>In this study, we address local photo enhancement to improve the aestheticquality of an input image by applying different effects to different regions.Existing photo enhancement methods are either not content-aware or not local;therefore, we propose a crowd-powered local enhancement method forcontent-aware local enhancement, which is achieved by asking crowd workers tolocally optimize parameters for image editing functions. To make it easier tolocally optimize the parameters, we propose an active learning based localfilter. The parameters need to be determined at only a few key pixels selectedby an active learning method, and the parameters at the other pixels areautomatically predicted using a regression model. The parameters at theselected key pixels are independently optimized, breaking down the optimizationproblem into a sequence of single-slider adjustments. Our experiments show thatthe proposed filter outperforms existing filters, and our enhanced results aremore visually pleasing than the results by the existing enhancement methods.Our source code and results are available athttps://github.com/satoshi-kosugi/crowd-powered.</description><author>Satoshi Kosugi, Toshihiko Yamasaki</author><pubDate>Thu, 15 Jun 2023 18:55:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09321v1</guid></item><item><title>Learnable Weight Initialization for Volumetric Medical Image Segmentation</title><link>http://arxiv.org/abs/2306.09320v1</link><description>Hybrid volumetric medical image segmentation models, combining the advantagesof local convolution and global attention, have recently received considerableattention. While mainly focusing on architectural modifications, most existinghybrid approaches still use conventional data-independent weight initializationschemes which restrict their performance due to ignoring the inherentvolumetric nature of the medical data. To address this issue, we propose alearnable weight initialization approach that utilizes the available medicaltraining data to effectively learn the contextual and structural cues via theproposed self-supervised objectives. Our approach is easy to integrate into anyhybrid model and requires no external training data. Experiments on multi-organand lung cancer segmentation tasks demonstrate the effectiveness of ourapproach, leading to state-of-the-art segmentation performance.</description><author>Shahina Kunhimon, Abdelrahman Shaker, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan</author><pubDate>Thu, 15 Jun 2023 18:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09320v1</guid></item><item><title>Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving</title><link>http://arxiv.org/abs/2304.14365v2</link><description>Robotic perception requires the modeling of both 3D geometry and semantics.Existing methods typically focus on estimating 3D bounding boxes, neglectingfiner geometric details and struggling to handle general, out-of-vocabularyobjects. 3D occupancy prediction, which estimates the detailed occupancy statesand semantics of a scene, is an emerging task to overcome these limitations. Tosupport 3D occupancy prediction, we develop a label generation pipeline thatproduces dense, visibility-aware labels for any given scene. This pipelinecomprises three stages: voxel densification, occlusion reasoning, andimage-guided voxel refinement. We establish two benchmarks, derived from theWaymo Open Dataset and the nuScenes Dataset, namely Occ3D-Waymo andOcc3D-nuScenes benchmarks. Furthermore, we provide an extensive analysis of theproposed dataset with various baseline models. Lastly, we propose a new model,dubbed Coarse-to-Fine Occupancy (CTF-Occ) network, which demonstrates superiorperformance on the Occ3D benchmarks. The code, data, and benchmarks arereleased at https://tsinghua-mars-lab.github.io/Occ3D/.</description><author>Xiaoyu Tian, Tao Jiang, Longfei Yun, Yucheng Mao, Huitong Yang, Yue Wang, Yilun Wang, Hang Zhao</author><pubDate>Thu, 15 Jun 2023 18:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14365v2</guid></item><item><title>Inroads into Autonomous Network Defence using Explained Reinforcement Learning</title><link>http://arxiv.org/abs/2306.09318v1</link><description>Computer network defence is a complicated task that has necessitated a highdegree of human involvement. However, with recent advancements in machinelearning, fully autonomous network defence is becoming increasingly plausible.This paper introduces an end-to-end methodology for studying attack strategies,designing defence agents and explaining their operation. First, using statediagrams, we visualise adversarial behaviour to gain insight about potentialpoints of intervention and inform the design of our defensive models. We opt touse a set of deep reinforcement learning agents trained on different parts ofthe task and organised in a shallow hierarchy. Our evaluation shows that theresulting design achieves a substantial performance improvement compared toprior work. Finally, to better investigate the decision-making process of ouragents, we complete our analysis with a feature ablation and importance study.</description><author>Myles Foley, Mia Wang, Zoe M, Chris Hicks, Vasilios Mavroudis</author><pubDate>Thu, 15 Jun 2023 18:53:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09318v1</guid></item><item><title>Diffusion Models for Zero-Shot Open-Vocabulary Segmentation</title><link>http://arxiv.org/abs/2306.09316v1</link><description>The variety of objects in the real world is nearly unlimited and is thusimpossible to capture using models trained on a fixed set of categories. As aresult, in recent years, open-vocabulary methods have attracted the interest ofthe community. This paper proposes a new method for zero-shot open-vocabularysegmentation. Prior work largely relies on contrastive training usingimage-text pairs, leveraging grouping mechanisms to learn image features thatare both aligned with language and well-localised. This however can introduceambiguity as the visual appearance of images with similar captions oftenvaries. Instead, we leverage the generative properties of large-scaletext-to-image diffusion models to sample a set of support images for a giventextual category. This provides a distribution of appearances for a given textcircumventing the ambiguity problem. We further propose a mechanism thatconsiders the contextual background of the sampled images to better localiseobjects and segment the background directly. We show that our method can beused to ground several existing pre-trained self-supervised feature extractorsin natural language and provide explainable predictions by mapping back toregions in the support set. Our proposal is training-free, relying onpre-trained components only, yet, shows strong performance on a range ofopen-vocabulary segmentation benchmarks, obtaining a lead of more than 10% onthe Pascal VOC benchmark.</description><author>Laurynas Karazija, Iro Laina, Andrea Vedaldi, Christian Rupprecht</author><pubDate>Thu, 15 Jun 2023 18:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09316v1</guid></item><item><title>Lexical Speaker Error Correction: Leveraging Language Models for Speaker Diarization Error Correction</title><link>http://arxiv.org/abs/2306.09313v1</link><description>Speaker diarization (SD) is typically used with an automatic speechrecognition (ASR) system to ascribe speaker labels to recognized words. Theconventional approach reconciles outputs from independently optimized ASR andSD systems, where the SD system typically uses only acoustic information toidentify the speakers in the audio stream. This approach can lead to speakererrors especially around speaker turns and regions of speaker overlap. In thispaper, we propose a novel second-pass speaker error correction system usinglexical information, leveraging the power of modern language models (LMs). Ourexperiments across multiple telephony datasets show that our approach is botheffective and robust. Training and tuning only on the Fisher dataset, thiserror correction approach leads to relative word-level diarization error rate(WDER) reductions of 15-30% on three telephony datasets: RT03-CTS, CallhomeAmerican English and held-out portions of Fisher.</description><author>Rohit Paturi, Sundararajan Srinivasan, Xiang Li</author><pubDate>Thu, 15 Jun 2023 18:47:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09313v1</guid></item><item><title>Semantic HELM: An Interpretable Memory for Reinforcement Learning</title><link>http://arxiv.org/abs/2306.09312v1</link><description>Reinforcement learning agents deployed in the real world often have to copewith partially observable environments. Therefore, most agents employ memorymechanisms to approximate the state of the environment. Recently, there havebeen impressive success stories in mastering partially observable environments,mostly in the realm of computer games like Dota 2, StarCraft II, or MineCraft.However, none of these methods are interpretable in the sense that it is notcomprehensible for humans how the agent decides which actions to take based onits inputs. Yet, human understanding is necessary in order to deploy suchmethods in high-stake domains like autonomous driving or medical applications.We propose a novel memory mechanism that operates on human language toilluminate the decision-making process. First, we use CLIP to associate visualinputs with language tokens. Then we feed these tokens to a pretrained languagemodel that serves the agent as memory and provides it with a coherent andinterpretable representation of the past. Our memory mechanism achievesstate-of-the-art performance in environments where memorizing the past iscrucial to solve tasks. Further, we present situations where our memorycomponent excels or fails to demonstrate strengths and weaknesses of our newapproach.</description><author>Fabian Paischer, Thomas Adler, Markus Hofmarcher, Sepp Hochreiter</author><pubDate>Thu, 15 Jun 2023 18:47:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09312v1</guid></item><item><title>Infinite Photorealistic Worlds using Procedural Generation</title><link>http://arxiv.org/abs/2306.09310v1</link><description>We introduce Infinigen, a procedural generator of photorealistic 3D scenes ofthe natural world. Infinigen is entirely procedural: every asset, from shape totexture, is generated from scratch via randomized mathematical rules, using noexternal source and allowing infinite variation and composition. Infinigenoffers broad coverage of objects and scenes in the natural world includingplants, animals, terrains, and natural phenomena such as fire, cloud, rain, andsnow. Infinigen can be used to generate unlimited, diverse training data for awide range of computer vision tasks including object detection, semanticsegmentation, optical flow, and 3D reconstruction. We expect Infinigen to be auseful resource for computer vision research and beyond. Please visithttps://infinigen.org for videos, code and pre-generated data.</description><author>Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, Alejandro Newell, Hei Law, Ankit Goyal, Kaiyu Yang, Jia Deng</author><pubDate>Thu, 15 Jun 2023 18:46:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09310v1</guid></item><item><title>Who Needs to Know? Minimal Knowledge for Optimal Coordination</title><link>http://arxiv.org/abs/2306.09309v1</link><description>To optimally coordinate with others in cooperative games, it is often crucialto have information about one's collaborators: successful driving requiresunderstanding which side of the road to drive on. However, not every feature ofcollaborators is strategically relevant: the fine-grained acceleration ofdrivers may be ignored while maintaining optimal coordination. We show thatthere is a well-defined dichotomy between strategically relevant and irrelevantinformation. Moreover, we show that, in dynamic games, this dichotomy has acompact representation that can be efficiently computed via a Bellman backupoperator. We apply this algorithm to analyze the strategically relevantinformation for tasks in both a standard and a partially observable version ofthe Overcooked environment. Theoretical and empirical results show that ouralgorithms are significantly more efficient than baselines. Videos areavailable at https://minknowledge.github.io.</description><author>Niklas Lauffer, Ameesh Shah, Micah Carroll, Michael Dennis, Stuart Russell</author><pubDate>Thu, 15 Jun 2023 18:43:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09309v1</guid></item><item><title>Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models</title><link>http://arxiv.org/abs/2306.09308v1</link><description>The wide applicability and adaptability of generative large language models(LLMs) has enabled their rapid adoption. While the pre-trained models canperform many tasks, such models are often fine-tuned to improve theirperformance on various downstream applications. However, this leads to issuesover violation of model licenses, model theft, and copyright infringement.Moreover, recent advances show that generative technology is capable ofproducing harmful content which exacerbates the problems of accountabilitywithin model supply chains. Thus, we need a method to investigate how a modelwas trained or a piece of text was generated and what their pre-trained basemodel was. In this paper we take the first step to address this open problem bytracing back the origin of a given fine-tuned LLM to its correspondingpre-trained base model. We consider different knowledge levels and attributionstrategies, and find that we can correctly trace back 8 out of the 10 finetuned models with our best method.</description><author>Myles Foley, Ambrish Rawat, Taesung Lee, Yufang Hou, Gabriele Picco, Giulio Zizzo</author><pubDate>Thu, 15 Jun 2023 18:42:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09308v1</guid></item><item><title>Quality and Efficiency of Manual Annotation: Pre-annotation Bias</title><link>http://arxiv.org/abs/2306.09307v1</link><description>This paper presents an analysis of annotation using an automaticpre-annotation for a mid-level annotation complexity task -- dependency syntaxannotation. It compares the annotation efforts made by annotators using apre-annotated version (with a high-accuracy parser) and those made by fullymanual annotation. The aim of the experiment is to judge the final annotationquality when pre-annotation is used. In addition, it evaluates the effect ofautomatic linguistically-based (rule-formulated) checks and another annotationon the same data available to the annotators, and their influence on annotationquality and efficiency. The experiment confirmed that the pre-annotation is anefficient tool for faster manual syntactic annotation which increases theconsistency of the resulting annotation without reducing its quality.</description><author>Marie Mikulová, Milan Straka, Jan Štěpánek, Barbora Štěpánková, Jan Hajič</author><pubDate>Thu, 15 Jun 2023 18:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09307v1</guid></item><item><title>Propagating Knowledge Updates to LMs Through Distillation</title><link>http://arxiv.org/abs/2306.09306v1</link><description>Modern language models have the capacity to store and use immense amounts ofknowledge about real-world entities, but it remains unclear how to update theirimplicit "knowledge bases.'' While prior methods for updating knowledge in LMssuccessfully inject facts, updated LMs then fail to make inferences based onthese injected facts. In this work, we demonstrate that a contextdistillation-based approach can both impart knowledge about entities andpropagate that knowledge to enable broader inferences. Our approach consists oftwo stages: transfer set generation and distillation on the transfer set. Wefirst generate a transfer set by simply prompting a language model to generatea continuation from the entity definition. Then, we update the model parametersso that the distribution of the LM (the student) matches the distribution ofthe LM conditioned on the definition (the teacher) on the transfer set. Ourexperiments demonstrate that this approach is more effective in propagatingknowledge updates compared to fine-tuning and other gradient-basedknowledge-editing methods without compromising performance in other contexts,even when injecting the definitions of up to 150 entities at once.</description><author>Shankar Padmanabhan, Yasumasa Onoe, Michael J. Q. Zhang, Greg Durrett, Eunsol Choi</author><pubDate>Thu, 15 Jun 2023 18:39:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09306v1</guid></item><item><title>Fast Training of Diffusion Models with Masked Transformers</title><link>http://arxiv.org/abs/2306.09305v1</link><description>We propose an efficient approach to train large diffusion models with maskedtransformers. While masked transformers have been extensively explored forrepresentation learning, their application to generative learning is lessexplored in the vision domain. Our work is the first to exploit masked trainingto reduce the training cost of diffusion models significantly. Specifically, werandomly mask out a high proportion (\emph{e.g.}, 50\%) of patches in diffusedinput images during training. For masked training, we introduce an asymmetricencoder-decoder architecture consisting of a transformer encoder that operatesonly on unmasked patches and a lightweight transformer decoder on full patches.To promote a long-range understanding of full patches, we add an auxiliary taskof reconstructing masked patches to the denoising score matching objective thatlearns the score of unmasked patches. Experiments on ImageNet-256$\times$256show that our approach achieves the same performance as the state-of-the-artDiffusion Transformer (DiT) model, using only 31\% of its original trainingtime. Thus, our method allows for efficient training of diffusion modelswithout sacrificing the generative performance.</description><author>Hongkai Zheng, Weili Nie, Arash Vahdat, Anima Anandkumar</author><pubDate>Thu, 15 Jun 2023 18:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09305v1</guid></item><item><title>Radars for Autonomous Driving: A Review of Deep Learning Methods and Challenges</title><link>http://arxiv.org/abs/2306.09304v1</link><description>Radar is a key component of the suite of perception sensors used for safe andreliable navigation of autonomous vehicles. Its unique capabilities includehigh-resolution velocity imaging, detection of agents in occlusion and overlong ranges, and robust performance in adverse weather conditions. However, theusage of radar data presents some challenges: it is characterized by lowresolution, sparsity, clutter, high uncertainty, and lack of good datasets.These challenges have limited radar deep learning research. As a result,current radar models are often influenced by lidar and vision models, which arefocused on optical features that are relatively weak in radar data, thusresulting in under-utilization of radar's capabilities and diminishing itscontribution to autonomous perception. This review seeks to encourage furtherdeep learning research on autonomous radar data by 1) identifying key researchthemes, and 2) offering a comprehensive overview of current opportunities andchallenges in the field. Topics covered include early and late fusion,occupancy flow estimation, uncertainty modeling, and multipath detection. Thepaper also discusses radar fundamentals and data representation, presents acurated list of recent radar datasets, and reviews state-of-the-art lidar andvision models relevant for radar research. For a summary of the paper and moreresults, visit the website: autonomous-radars.github.io.</description><author>Arvind Srivastav, Soumyajit Mandal</author><pubDate>Thu, 15 Jun 2023 18:37:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09304v1</guid></item><item><title>Benchmarking simulated and physical quantum processing units using quantum and hybrid algorithms</title><link>http://arxiv.org/abs/2211.15631v2</link><description>Powerful hardware services and software libraries are vital tools for quicklyand affordably designing, testing, and executing quantum algorithms. A robustlarge-scale study of how the performance of these platforms scales with thenumber of qubits is key to providing quantum solutions to challenging industryproblems. This work benchmarks the runtime and accuracy for a representativesample of specialized high-performance simulated and physical quantumprocessing units. Results show the QMware simulator can reduce the runtime forexecuting a quantum circuit by up to 78% compared to the next fastest optionfor algorithms with fewer than 27 qubits. The AWS SV1 simulator offers aruntime advantage for larger circuits, up to the maximum 34 qubits availablewith SV1. Beyond this limit, QMware can execute circuits as large as 40 qubits.Physical quantum devices, such as Rigetti's Aspen-M2, can provide anexponential runtime advantage for circuits with more than 30 qubits. However,the high financial cost of physical quantum processing units presents a seriousbarrier to practical use. Moreover, only IonQ's Harmony quantum device achieveshigh fidelity with more than four qubits. This study paves the way tounderstanding the optimal combination of available software and hardware forexecuting practical quantum algorithms.</description><author>Mohammad Kordzanganeh, Markus Buchberger, Basil Kyriacou, Maxim Povolotskii, Wilhelm Fischer, Andrii Kurkin, Wilfrid Somogyi, Asel Sagingalieva, Markus Pflitsch, Alexey Melnikov</author><pubDate>Thu, 15 Jun 2023 18:36:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15631v2</guid></item><item><title>Datasets and Benchmarks for Offline Safe Reinforcement Learning</title><link>http://arxiv.org/abs/2306.09303v1</link><description>This paper presents a comprehensive benchmarking suite tailored to offlinesafe reinforcement learning (RL) challenges, aiming to foster progress in thedevelopment and evaluation of safe learning algorithms in both the training anddeployment phases. Our benchmark suite contains three packages: 1) expertlycrafted safe policies, 2) D4RL-styled datasets along with environment wrappers,and 3) high-quality offline safe RL baseline implementations. We feature amethodical data collection pipeline powered by advanced safe RL algorithms,which facilitates the generation of diverse datasets across 38 popular safe RLtasks, from robot control to autonomous driving. We further introduce an arrayof data post-processing filters, capable of modifying each dataset's diversity,thereby simulating various data collection conditions. Additionally, we provideelegant and extensible implementations of prevalent offline safe RL algorithmsto accelerate research in this area. Through extensive experiments with over50000 CPU and 800 GPU hours of computations, we evaluate and compare theperformance of these baseline algorithms on the collected datasets, offeringinsights into their strengths, limitations, and potential areas of improvement.Our benchmarking framework serves as a valuable resource for researchers andpractitioners, facilitating the development of more robust and reliable offlinesafe RL solutions in safety-critical applications. The benchmark website isavailable at \url{www.offline-saferl.org}.</description><author>Zuxin Liu, Zijian Guo, Haohong Lin, Yihang Yao, Jiacheng Zhu, Zhepeng Cen, Hanjiang Hu, Wenhao Yu, Tingnan Zhang, Jie Tan, Ding Zhao</author><pubDate>Thu, 15 Jun 2023 18:31:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09303v1</guid></item><item><title>Knowledge Guided Representation Learning and Causal Structure Learning in Soil Science</title><link>http://arxiv.org/abs/2306.09302v1</link><description>An improved understanding of soil can enable more sustainable land-usepractices. Nevertheless, soil is called a complex, living medium due to thecomplex interaction of different soil processes that limit our understanding ofsoil. Process-based models and analyzing observed data provide two avenues forimproving our understanding of soil processes. Collecting observed data iscost-prohibitive but reflects real-world behavior, while process-based modelscan be used to generate ample synthetic data which may not be representative ofreality. We propose a framework, knowledge-guided representation learning, andcausal structure learning (KGRCL), to accelerate scientific discoveries in soilscience. The framework improves representation learning for simulated soilprocesses via conditional distribution matching with observed soil processes.Simultaneously, the framework leverages both observed and simulated data tolearn a causal structure among the soil processes. The learned causal graph ismore representative of ground truth than other graphs generated from othercausal discovery methods. Furthermore, the learned causal graph is leveraged ina supervised learning setup to predict the impact of fertilizer use andchanging weather on soil carbon. We present the results in five differentlocations to show the improvement in the prediction performance inout-of-sample and few-shots setting.</description><author>Somya Sharma, Swati Sharma, Licheng Liu, Rishabh Tushir, Andy Neal, Robert Ness, John Crawford, Emre Kiciman, Ranveer Chandra</author><pubDate>Thu, 15 Jun 2023 18:31:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09302v1</guid></item><item><title>OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection</title><link>http://arxiv.org/abs/2306.09301v1</link><description>Out-of-Distribution (OOD) detection is critical for the reliable operation ofopen-world intelligent systems. Despite the emergence of an increasing numberof OOD detection methods, the evaluation inconsistencies present challenges fortracking the progress in this field. OpenOOD v1 initiated the unification ofthe OOD detection evaluation but faced limitations in scalability andusability. In response, this paper presents OpenOOD v1.5, a significantimprovement from its predecessor that ensures accurate, standardized, anduser-friendly evaluation of OOD detection methodologies. Notably, OpenOOD v1.5extends its evaluation capabilities to large-scale datasets such as ImageNet,investigates full-spectrum OOD detection which is important yet underexplored,and introduces new features including an online leaderboard and an easy-to-useevaluator. This work also contributes in-depth analysis and insights derivedfrom comprehensive experimental results, thereby enriching the knowledge poolof OOD detection methodologies. With these enhancements, OpenOOD v1.5 aims todrive advancements and offer a more robust and comprehensive evaluationbenchmark for OOD detection research.</description><author>Jingyang Zhang, Jingkang Yang, Pengyun Wang, Haoqi Wang, Yueqian Lin, Haoran Zhang, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, Yixuan Li, Ziwei Liu, Yiran Chen, Hai Li</author><pubDate>Thu, 15 Jun 2023 18:28:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09301v1</guid></item><item><title>Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Theory of Mind</title><link>http://arxiv.org/abs/2306.09299v1</link><description>Large Language Models (LLMs) perform complex reasoning by generatingexplanations for their predictions. However, a complementary goal ofexplanations is to also communicate useful knowledge that improves weakeragents. Hence, we investigate whether LLMs also make good teachers for weakeragents. In particular, we consider a student-teacher framework between two LLMagents and study if, when, and how the teacher should intervene with naturallanguage explanations to improve the student's performance. Since communicationis expensive, we define a budget such that the teacher only communicatesexplanations for a fraction of the data, after which the student should performwell on its own. We decompose the teaching problem along four axes: (1) ifteacher's test time intervention improve student predictions, (2) when it isworth explaining a data point, (3) how the teacher should personalizeexplanations to better teach the student, and (4) if teacher explanations alsoimprove student performance on future unexplained data. We first show thatteacher LLMs can indeed intervene on student reasoning to improve theirperformance. Next, we propose a Theory of Mind approach, in which the teacherbuilds two few-shot mental models of the student. The first model defines anIntervention Function that simulates the utility of an intervention, allowingthe teacher to intervene when this utility is the highest and improving studentperformance at lower budgets. The second model enables the teacher topersonalize explanations for a particular student and outperform unpersonalizedteachers. We also demonstrate that in multi-turn interactions, teacherexplanations generalize and learning from explained data improves studentperformance on future unexplained data. Finally, we also verify that misalignedteachers can lower student performance to random chance by intentionallymisleading them.</description><author>Swarnadeep Saha, Peter Hase, Mohit Bansal</author><pubDate>Thu, 15 Jun 2023 18:27:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09299v1</guid></item><item><title>Fix Fairness, Don't Ruin Accuracy: Performance Aware Fairness Repair using AutoML</title><link>http://arxiv.org/abs/2306.09297v1</link><description>Machine learning (ML) is increasingly being used in critical decision-makingsoftware, but incidents have raised questions about the fairness of MLpredictions. To address this issue, new tools and methods are needed tomitigate bias in ML-based software. Previous studies have proposed biasmitigation algorithms that only work in specific situations and often result ina loss of accuracy. Our proposed solution is a novel approach that utilizesautomated machine learning (AutoML) techniques to mitigate bias. Our approachincludes two key innovations: a novel optimization function and afairness-aware search space. By improving the default optimization function ofAutoML and incorporating fairness objectives, we are able to mitigate bias withlittle to no loss of accuracy. Additionally, we propose a fairness-aware searchspace pruning method for AutoML to reduce computational cost and repair time.Our approach, built on the state-of-the-art Auto-Sklearn tool, is designed toreduce bias in real-world scenarios. In order to demonstrate the effectivenessof our approach, we evaluated our approach on four fairness problems and 16different ML models, and our results show a significant improvement over thebaseline and existing bias mitigation techniques. Our approach, Fair-AutoML,successfully repaired 60 out of 64 buggy cases, while existing bias mitigationtechniques only repaired up to 44 out of 64 cases.</description><author>Giang Nguyen, Sumon Biswas, Hridesh Rajan</author><pubDate>Thu, 15 Jun 2023 18:25:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09297v1</guid></item><item><title>How Does Pseudo-Labeling Affect the Generalization Error of the Semi-Supervised Gibbs Algorithm?</title><link>http://arxiv.org/abs/2210.08188v2</link><description>We provide an exact characterization of the expected generalization error(gen-error) for semi-supervised learning (SSL) with pseudo-labeling via theGibbs algorithm. The gen-error is expressed in terms of the symmetrized KLinformation between the output hypothesis, the pseudo-labeled dataset, and thelabeled dataset. Distribution-free upper and lower bounds on the gen-error canalso be obtained. Our findings offer new insights that the generalizationperformance of SSL with pseudo-labeling is affected not only by the informationbetween the output hypothesis and input training data but also by theinformation {\em shared} between the {\em labeled} and {\em pseudo-labeled}data samples. This serves as a guideline to choose an appropriatepseudo-labeling method from a given family of methods. To deepen ourunderstanding, we further explore two examples -- mean estimation and logisticregression. In particular, we analyze how the ratio of the number of unlabeledto labeled data $\lambda$ affects the gen-error under both scenarios. As$\lambda$ increases, the gen-error for mean estimation decreases and thensaturates at a value larger than when all the samples are labeled, and the gapcan be quantified {\em exactly} with our analysis, and is dependent on the\emph{cross-covariance} between the labeled and pseudo-labeled data samples.For logistic regression, the gen-error and the variance component of the excessrisk also decrease as $\lambda$ increases.</description><author>Haiyun He, Gholamali Aminian, Yuheng Bu, Miguel Rodrigues, Vincent Y. F. Tan</author><pubDate>Thu, 15 Jun 2023 18:22:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.08188v2</guid></item><item><title>KoLA: Carefully Benchmarking World Knowledge of Large Language Models</title><link>http://arxiv.org/abs/2306.09296v1</link><description>The unprecedented performance of large language models (LLMs) necessitatesimprovements in evaluations. Rather than merely exploring the breadth of LLMabilities, we believe meticulous and thoughtful designs are essential tothorough, unbiased, and applicable evaluations. Given the importance of worldknowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark(KoLA), in which we carefully design three crucial factors: (1) For abilitymodeling, we mimic human cognition to form a four-level taxonomy ofknowledge-related abilities, covering $19$ tasks. (2) For data, to ensure faircomparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs,along with continuously collected emerging corpora, aiming to evaluate thecapacity to handle unseen data and evolving knowledge. (3) For evaluationcriteria, we adopt a contrastive system, including overall standard scores forbetter numerical comparability across tasks and models and a uniqueself-contrast metric for automatically evaluating knowledge hallucination. Weevaluate $21$ open-source and commercial LLMs and obtain some intriguingfindings. The KoLA dataset and open-participation leaderboard are publiclyreleased at https://kola.xlore.cn and will be continuously updated to providereferences for developing LLMs and knowledge-related systems.</description><author>Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, Chunyang Li, Zheyuan Zhang, Yushi Bai, Yantao Liu, Amy Xin, Nianyi Lin, Kaifeng Yun, Linlu Gong, Jianhui Chen, Zhili Wu, Yunjia Qi, Weikai Li, Yong Guan, Kaisheng Zeng, Ji Qi, Hailong Jin, Jinxin Liu, Yu Gu, Yuan Yao, Ning Ding, Lei Hou, Zhiyuan Liu, Bin Xu, Jie Tang, Juanzi Li</author><pubDate>Thu, 15 Jun 2023 18:20:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09296v1</guid></item><item><title>Neural Fine-Tuning Search for Few-Shot Learning</title><link>http://arxiv.org/abs/2306.09295v1</link><description>In few-shot recognition, a classifier that has been trained on one set ofclasses is required to rapidly adapt and generalize to a disjoint, novel set ofclasses. To that end, recent studies have shown the efficacy of fine-tuningwith carefully crafted adaptation architectures. However this raises thequestion of: How can one design the optimal adaptation strategy? In this paper,we study this question through the lens of neural architecture search (NAS).Given a pre-trained neural network, our algorithm discovers the optimalarrangement of adapters, which layers to keep frozen and which to fine-tune. Wedemonstrate the generality of our NAS method by applying it to both residualnetworks and vision transformers and report state-of-the-art performance onMeta-Dataset and Meta-Album.</description><author>Panagiotis Eustratiadis, Łukasz Dudziak, Da Li, Timothy Hospedales</author><pubDate>Thu, 15 Jun 2023 18:20:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09295v1</guid></item><item><title>Sampling-Based Techniques for Training Deep Neural Networks with Limited Computational Resources: A Scalability Evaluation</title><link>http://arxiv.org/abs/2306.09293v1</link><description>Deep neural networks are superior to shallow networks in learning complexrepresentations. As such, there is a fast-growing interest in utilizing them inlarge-scale settings. The training process of neural networks is already knownto be time-consuming, and having a deep architecture only aggravates the issue.This process consists mostly of matrix operations, among which matrixmultiplication is the bottleneck. Several sampling-based techniques have beenproposed for speeding up the training time of deep neural networks byapproximating the matrix products. These techniques fall under two categories:(i) sampling a subset of nodes in every hidden layer as active at everyiteration and (ii) sampling a subset of nodes from the previous layer toapproximate the current layer's activations using the edges from the samplednodes. In both cases, the matrix products are computed using only the selectedsamples. In this paper, we evaluate the scalability of these approaches on CPUmachines with limited computational resources. Making a connection between thetwo research directions as special cases of approximating matrixmultiplications in the context of neural networks, we provide a negativetheoretical analysis that shows feedforward approximation is an obstacleagainst scalability. We conduct comprehensive experimental evaluations thatdemonstrate the most pressing challenges and limitations associated with thestudied approaches. We observe that the hashing-based node selection method isnot scalable to a large number of layers, confirming our theoretical analysis.Finally, we identify directions for future research.</description><author>Sana Ebrahimi, Rishi Advani, Abolfazl Asudeh</author><pubDate>Thu, 15 Jun 2023 18:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09293v1</guid></item><item><title>Large language models predict human sensory judgments across six modalities</title><link>http://arxiv.org/abs/2302.01308v2</link><description>Determining the extent to which the perceptual world can be recovered fromlanguage is a longstanding problem in philosophy and cognitive science. We showthat state-of-the-art large language models can unlock new insights into thisproblem by providing a lower bound on the amount of perceptual information thatcan be extracted from language. Specifically, we elicit pairwise similarityjudgments from GPT models across six psychophysical datasets. We show that thejudgments are significantly correlated with human data across all domains,recovering well-known representations like the color wheel and pitch spiral.Surprisingly, we find that a model (GPT-4) co-trained on vision and languagedoes not necessarily lead to improvements specific to the visual modality. Tostudy the influence of specific languages on perception, we also apply themodels to a multilingual color-naming task. We find that GPT-4 replicatescross-linguistic variation in English and Russian illuminating the interactionof language and perception.</description><author>Raja Marjieh, Ilia Sucholutsky, Pol van Rijn, Nori Jacoby, Thomas L. Griffiths</author><pubDate>Thu, 15 Jun 2023 18:18:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01308v2</guid></item><item><title>Generalizable Resource Scaling of 5G Slices using Constrained Reinforcement Learning</title><link>http://arxiv.org/abs/2306.09290v1</link><description>Network slicing is a key enabler for 5G to support various applications.Slices requested by service providers (SPs) have heterogeneous quality ofservice (QoS) requirements, such as latency, throughput, and jitter. It isimperative that the 5G infrastructure provider (InP) allocates the right amountof resources depending on the slice's traffic, such that the specified QoSlevels are maintained during the slice's lifetime while maximizing resourceefficiency. However, there is a non-trivial relationship between the QoS andresource allocation. In this paper, this relationship is learned using aregression-based model. We also leverage a risk-constrained reinforcementlearning agent that is trained offline using this model and domainrandomization for dynamically scaling slice resources while maintaining thedesired QoS level. Our novel approach reduces the effects of network modelingerrors since it is model-free and does not require QoS metrics to bemathematically formulated in terms of traffic. In addition, it providesrobustness against uncertain network conditions, generalizes to differentreal-world traffic patterns, and caters to various QoS metrics. The resultsshow that the state-of-the-art approaches can lead to QoS degradation as highas 44.5% when tested on previously unseen traffic. On the other hand, ourapproach maintains the QoS degradation below a preset 10% threshold on suchtraffic, while minimizing the allocated resources. Additionally, we demonstratethat the proposed approach is robust against varying network conditions andinaccurate traffic predictions.</description><author>Muhammad Sulaiman, Mahdieh Ahmadi, Mohammad A. Salahuddin, Raouf Boutaba, Aladdin Saleh</author><pubDate>Thu, 15 Jun 2023 18:16:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09290v1</guid></item><item><title>DaMuEL: A Large Multilingual Dataset for Entity Linking</title><link>http://arxiv.org/abs/2306.09288v1</link><description>We present DaMuEL, a large Multilingual Dataset for Entity Linking containingdata in 53 languages. DaMuEL consists of two components: a knowledge base thatcontains language-agnostic information about entities, including their claimsfrom Wikidata and named entity types (PER, ORG, LOC, EVENT, BRAND, WORK_OF_ART,MANUFACTURED); and Wikipedia texts with entity mentions linked to the knowledgebase, along with language-specific text from Wikidata such as labels, aliases,and descriptions, stored separately for each language. The Wikidata QID is usedas a persistent, language-agnostic identifier, enabling the combination of theknowledge base with language-specific texts and information for each entity.Wikipedia documents deliberately annotate only a single mention for everyentity present; we further automatically detect all mentions of named entitieslinked from each document. The dataset contains 27.9M named entities in theknowledge base and 12.3G tokens from Wikipedia texts. The dataset is publishedunder the CC BY-SA license at https://hdl.handle.net/11234/1-5047.</description><author>David Kubeša, Milan Straka</author><pubDate>Thu, 15 Jun 2023 18:15:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09288v1</guid></item><item><title>Stable Deep MRI Reconstruction using Generative Priors</title><link>http://arxiv.org/abs/2210.13834v3</link><description>Data-driven approaches recently achieved remarkable success in magneticresonance imaging (MRI) reconstruction, but integration into clinical routineremains challenging due to a lack of generalizability and interpretability. Inthis paper, we address these challenges in a unified framework based ongenerative image priors. We propose a novel deep neural network basedregularizer which is trained in a generative setting on reference magnitudeimages only. After training, the regularizer encodes higher-level domainstatistics which we demonstrate by synthesizing images without data. Embeddingthe trained model in a classical variational approach yields high-qualityreconstructions irrespective of the sub-sampling pattern. In addition, themodel shows stable behavior when confronted with out-of-distribution data inthe form of contrast variation. Furthermore, a probabilistic interpretationprovides a distribution of reconstructions and hence allows uncertaintyquantification. To reconstruct parallel MRI, we propose a fast algorithm tojointly estimate the image and the sensitivity maps. The results demonstratecompetitive performance, on par with state-of-the-art end-to-end deep learningmethods, while preserving the flexibility with respect to sub-sampling patternsand allowing for uncertainty quantification.</description><author>Martin Zach, Florian Knoll, Thomas Pock</author><pubDate>Thu, 15 Jun 2023 18:10:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.13834v3</guid></item><item><title>PLAtE: A Large-scale Dataset for List Page Web Extraction</title><link>http://arxiv.org/abs/2205.12386v2</link><description>Recently, neural models have been leveraged to significantly improve theperformance of information extraction from semi-structured websites. However, abarrier for continued progress is the small number of datasets large enough totrain these models. In this work, we introduce the PLAtE (Pages of ListsAttribute Extraction) benchmark dataset as a challenging new web extractiontask. PLAtE focuses on shopping data, specifically extractions from productreview pages with multiple items encompassing the tasks of: (1) findingproduct-list segmentation boundaries and (2) extracting attributes for eachproduct. PLAtE is composed of 52, 898 items collected from 6, 694 pages and156, 014 attributes, making it the first largescale list page web extractiondataset. We use a multi-stage approach to collect and annotate the dataset andadapt three state-of-the-art web extraction models to the two tasks comparingtheir strengths and weaknesses both quantitatively and qualitatively.</description><author>Aidan San, Yuan Zhuang, Jan Bakus, Colin Lockard, David Ciemiewicz, Sandeep Atluri, Yangfeng Ji, Kevin Small, Heba Elfardy</author><pubDate>Thu, 15 Jun 2023 18:06:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.12386v2</guid></item><item><title>Exploring the Intersection between Neural Architecture Search and Continual Learning</title><link>http://arxiv.org/abs/2206.05625v2</link><description>Despite the significant advances achieved in Artificial Neural Networks(ANNs), their design process remains notoriously tedious, depending primarilyon intuition, experience and trial-and-error. This human-dependent process isoften time-consuming and prone to errors. Furthermore, the models are generallybound to their training contexts, with no considerations to their surroundingenvironments. Continual adaptiveness and automation of neural networks is ofparamount importance to several domains where model accessibility is limitedafter deployment (e.g IoT devices, self-driving vehicles, etc.). Additionally,even accessible models require frequent maintenance post-deployment to overcomeissues such as Concept/Data Drift, which can be cumbersome and restrictive. Byleveraging and combining approaches from Neural Architecture Search (NAS) andContinual Learning (CL), more robust and adaptive agents can be developed. Thisstudy conducts the first extensive review on the intersection between NAS andCL, formalizing the prospective Continually-Adaptive Neural Networks (CANNs)paradigm and outlining research directions for lifelong autonomous ANNs.</description><author>Mohamed Shahawy, Elhadj Benkhelifa, David White</author><pubDate>Thu, 15 Jun 2023 18:04:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.05625v2</guid></item><item><title>Challenges of Using Real-World Sensory Inputs for Motion Forecasting in Autonomous Driving</title><link>http://arxiv.org/abs/2306.09281v1</link><description>Motion forecasting plays a critical role in enabling robots to anticipatefuture trajectories of surrounding agents and plan accordingly. However,existing forecasting methods often rely on curated datasets that are notfaithful to what real-world perception pipelines can provide. In reality,upstream modules that are responsible for detecting and tracking agents, andthose that gather road information to build the map, can introduce variouserrors, including misdetections, tracking errors, and difficulties in beingaccurate for distant agents and road elements. This paper aims to uncover thechallenges of bringing motion forecasting models to this more realistic settingwhere inputs are provided by perception modules. In particular, we quantify theimpacts of the domain gap through extensive evaluation. Furthermore, we designsynthetic perturbations to better characterize their consequences, thusproviding insights into areas that require improvement in upstream perceptionmodules and guidance toward the development of more robust forecasting methods.</description><author>Yihong Xu, Loïck Chambon, Éloi Zablocki, Mickaël Chen, Matthieu Cord, Patrick Pérez</author><pubDate>Thu, 15 Jun 2023 18:03:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09281v1</guid></item><item><title>HiveNAS: Neural Architecture Search using Artificial Bee Colony Optimization</title><link>http://arxiv.org/abs/2211.10250v2</link><description>The traditional Neural Network-development process requires substantialexpert knowledge and relies heavily on intuition and trial-and-error. NeuralArchitecture Search (NAS) frameworks were introduced to robustly search fornetwork topologies, as well as facilitate the automated development of NeuralNetworks. While some optimization approaches -- such as Genetic Algorithms --have been extensively explored in the NAS context, other MetaheuristicOptimization algorithms have not yet been investigated. In this study, weevaluate the viability of Artificial Bee Colony optimization for NeuralArchitecture Search. Our proposed framework, HiveNAS, outperforms existingstate-of-the-art Swarm Intelligence-based NAS frameworks in a fraction of thetime.</description><author>Mohamed Shahawy, Elhadj Benkhelifa</author><pubDate>Thu, 15 Jun 2023 18:02:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10250v2</guid></item><item><title>Robustness Analysis on Foundational Segmentation Models</title><link>http://arxiv.org/abs/2306.09278v1</link><description>Due to the increase in computational resources and accessibility of data, anincrease in large, deep learning models trained on copious amounts of datausing self-supervised or semi-supervised learning have emerged. These"foundation" models are often adapted to a variety of downstream tasks likeclassification, object detection, and segmentation with little-to-no trainingon the target dataset. In this work, we perform a robustness analysis of VisualFoundation Models (VFMs) for segmentation tasks and compare them to supervisedmodels of smaller scale. We focus on robustness against real-world distributionshift perturbations.We benchmark four state-of-the-art segmentationarchitectures using 2 different datasets, COCO and ADE20K, with 17 differentperturbations with 5 severity levels each. We find interesting insights thatinclude (1) VFMs are not robust to compression-based corruptions, (2) while theselected VFMs do not significantly outperform or exhibit more robustnesscompared to non-VFM models, they remain competitively robust in zero-shotevaluations, particularly when non-VFM are under supervision and (3) selectedVFMs demonstrate greater resilience to specific categories of objects, likelydue to their open-vocabulary training paradigm, a feature that non-VFM modelstypically lack. We posit that the suggested robustness evaluation introducesnew requirements for foundational models, thus sparking further research toenhance their performance.</description><author>Madeline Chantry Schiappa, Sachidanand VS, Yunhao Ge, Ondrej Miksik, Yogesh S. Rawat, Vibhav Vineet</author><pubDate>Thu, 15 Jun 2023 17:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09278v1</guid></item><item><title>Compositor: Bottom-up Clustering and Compositing for Robust Part and Object Segmentation</title><link>http://arxiv.org/abs/2306.07404v2</link><description>In this work, we present a robust approach for joint part and objectsegmentation. Specifically, we reformulate object and part segmentation as anoptimization problem and build a hierarchical feature representation includingpixel, part, and object-level embeddings to solve it in a bottom-up clusteringmanner. Pixels are grouped into several clusters where the part-levelembeddings serve as cluster centers. Afterwards, object masks are obtained bycompositing the part proposals. This bottom-up interaction is shown to beeffective in integrating information from lower semantic levels to highersemantic levels. Based on that, our novel approach Compositor produces part andobject segmentation masks simultaneously while improving the mask quality.Compositor achieves state-of-the-art performance on PartImageNet andPascal-Part by outperforming previous methods by around 0.9% and 1.3% onPartImageNet, 0.4% and 1.7% on Pascal-Part in terms of part and object mIoU anddemonstrates better robustness against occlusion by around 4.4% and 7.1% onpart and object respectively. Code will be available athttps://github.com/TACJu/Compositor.</description><author>Ju He, Jieneng Chen, Ming-Xian Lin, Qihang Yu, Alan Yuille</author><pubDate>Thu, 15 Jun 2023 17:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07404v2</guid></item><item><title>Conditional Human Sketch Synthesis with Explicit Abstraction Control</title><link>http://arxiv.org/abs/2306.09274v1</link><description>This paper presents a novel free-hand sketch synthesis approach addressingexplicit abstraction control in class-conditional and photo-to-sketchsynthesis. Abstraction is a vital aspect of sketches, as it defines thefundamental distinction between a sketch and an image. Previous works relied onimplicit control to achieve different levels of abstraction, leading toinaccurate control and synthesized sketches deviating from human sketches. Toresolve this challenge, we propose two novel abstraction control mechanisms,state embeddings and the stroke token, integrated into a transformer-basedlatent diffusion model (LDM). These mechanisms explicitly provide the requiredamount of points or strokes to the model, enabling accurate point-level andstroke-level control in synthesized sketches while preserving recognizability.Outperforming state-of-the-art approaches, our method effectively generatesdiverse, non-rigid and human-like sketches. The proposed approach enablescoherent sketch synthesis and excels in representing human habits with desiredabstraction levels, highlighting the potential of sketch synthesis forreal-world applications.</description><author>Dar-Yen Chen</author><pubDate>Thu, 15 Jun 2023 17:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09274v1</guid></item><item><title>Your Room is not Private: Gradient Inversion Attack for Deep Q-Learning</title><link>http://arxiv.org/abs/2306.09273v1</link><description>The prominence of embodied Artificial Intelligence (AI), which empowersrobots to navigate, perceive, and engage within virtual environments, hasattracted significant attention, owing to the remarkable advancements incomputer vision and large language models. Privacy emerges as a pivotal concernwithin the realm of embodied AI, as the robot access substantial personalinformation. However, the issue of privacy leakage in embodied AI tasks,particularly in relation to decision-making algorithms, has not receivedadequate consideration in research. This paper aims to address this gap byproposing an attack on the Deep Q-Learning algorithm, utilizing gradientinversion to reconstruct states, actions, and Q-values. The choice of usinggradients for the attack is motivated by the fact that commonly employedfederated learning techniques solely utilize gradients computed based onprivate user data to optimize models, without storing or transmitting the datato public servers. Nevertheless, these gradients contain sufficient informationto potentially expose private data. To validate our approach, we conductexperiments on the AI2THOR simulator and evaluate our algorithm on activeperception, a prevalent task in embodied AI. The experimental resultsconvincingly demonstrate the effectiveness of our method in successfullyrecovering all information from the data across all 120 room layouts.</description><author>Miao Li, Wenhao Ding, Ding Zhao</author><pubDate>Thu, 15 Jun 2023 17:53:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09273v1</guid></item><item><title>Symmetry &amp; Critical Points for Symmetric Tensor Decomposition Problems</title><link>http://arxiv.org/abs/2306.07886v2</link><description>We consider the non-convex optimization problem associated with thedecomposition of a real symmetric tensor into a sum of rank one terms. Use ismade of the rich symmetry structure to derive Puiseux series representations offamilies of critical points, and so obtain precise analytic estimates on thecritical values and the Hessian spectrum. The sharp results make possible ananalytic characterization of various geometric obstructions to localoptimization methods, revealing in particular a complex array of saddles andlocal minima which differ by their symmetry, structure and analytic properties.A desirable phenomenon, occurring for all critical points considered, concernsthe index of a point, i.e., the number of negative Hessian eigenvalues,increasing with the value of the objective function. Lastly, a Newton polytopeargument is used to give a complete enumeration of all critical points of fixedsymmetry, and it is shown that contrarily to the set of global minima whichremains invariant under different choices of tensor norms, certain families ofnon-global minima emerge, others disappear.</description><author>Yossi Arjevani, Gal Vinograd</author><pubDate>Thu, 15 Jun 2023 17:51:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07886v2</guid></item><item><title>Zero-Shot Anomaly Detection with Pre-trained Segmentation Models</title><link>http://arxiv.org/abs/2306.09269v1</link><description>This technical report outlines our submission to the zero-shot track of theVisual Anomaly and Novelty Detection (VAND) 2023 Challenge. Building on theperformance of the WINCLIP framework, we aim to enhance the system'slocalization capabilities by integrating zero-shot segmentation models. Inaddition, we perform foreground instance segmentation which enables the modelto focus on the relevant parts of the image, thus allowing the models to betteridentify small or subtle deviations. Our pipeline requires no external data orinformation, allowing for it to be directly applied to new datasets. Our team(Variance Vigilance Vanguard) ranked third in the zero-shot track of the VANDchallenge, and achieve an average F1-max score of 81.5/24.2 at a sample/pixellevel on the VisA dataset.</description><author>Matthew Baugh, James Batten, Johanna P. Müller, Bernhard Kainz</author><pubDate>Thu, 15 Jun 2023 17:43:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09269v1</guid></item><item><title>Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?</title><link>http://arxiv.org/abs/2306.09267v1</link><description>The rise of Generative Artificial Intelligence systems (``AI systems'') hascreated unprecedented social engagement. AI code generation systems provideresponses (output) to questions or requests by accessing the vast library ofopen-source code created by developers over decades. However, they do so byallegedly stealing the open-source code stored in virtual libraries, known asrepositories. How all this happens and whether there is a solution short ofyears of litigation that can protect innovation is the focus of this article.We also peripherally touch upon the array of issues raised by the relationshipbetween AI and copyright. Looking ahead, we propose the following: (a)immediate changes to the licenses for open-source code created by developersthat will allow access and/or use of any open-source code to humans only; (b)we suggest revisions to the Massachusetts Institute of Technology (``MIT'')license so that AI systems procure appropriate licenses from open-source codedevelopers, which we believe will harmonize standards and build socialconsensus for the benefit of all of humanity rather than profit-driven centersof innovation; (c) We call for urgent legislative action to protect the futureof AI systems while also promoting innovation; and (d) we propose that there isa shift in the burden of proof to AI systems in obfuscation cases.</description><author>Dimitrios Ioannidis, Jeremy Kepner, Andrew Bowne, Harriet S. Bryant</author><pubDate>Thu, 15 Jun 2023 17:40:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09267v1</guid></item><item><title>A9 Intersection Dataset: All You Need for Urban 3D Camera-LiDAR Roadside Perception</title><link>http://arxiv.org/abs/2306.09266v1</link><description>Intelligent Transportation Systems (ITS) allow a drastic expansion of thevisibility range and decrease occlusions for autonomous driving. To obtainaccurate detections, detailed labeled sensor data for training is required.Unfortunately, high-quality 3D labels of LiDAR point clouds from theinfrastructure perspective of an intersection are still rare. Therefore, weprovide the A9 Intersection Dataset, which consists of labeled LiDAR pointclouds and synchronized camera images. Here, we recorded the sensor output fromtwo roadside cameras and LiDARs mounted on intersection gantry bridges. Thepoint clouds were labeled in 3D by experienced annotators. Furthermore, weprovide calibration data between all sensors, which allow the projection of the3D labels into the camera images and an accurate data fusion. Our datasetconsists of 4.8k images and point clouds with more than 57.4k manually labeled3D boxes. With ten object classes, it has a high diversity of road users incomplex driving maneuvers, such as left and right turns, overtaking, andU-turns. In experiments, we provided multiple baselines for the perceptiontasks. Overall, our dataset is a valuable contribution to the scientificcommunity to perform complex 3D camera-LiDAR roadside perception tasks. Finddata, code, and more information at https://a9-dataset.com.</description><author>Walter Zimmer, Christian Creß, Huu Tung Nguyen, Alois C. Knoll</author><pubDate>Thu, 15 Jun 2023 17:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09266v1</guid></item><item><title>LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models</title><link>http://arxiv.org/abs/2306.09265v1</link><description>Large Vision-Language Models (LVLMs) have recently played a dominant role inmultimodal vision-language learning. Despite the great success, it lacks aholistic evaluation of their efficacy. This paper presents a comprehensiveevaluation of publicly available large multimodal models by building a LVLMevaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMssuch as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by aquantitative capability evaluation and an online arena platform. The formerevaluates $6$ categories of multimodal capabilities of LVLMs such as visualquestion answering and embodied artificial intelligence on $47$ standardtext-related visual benchmarks, while the latter provides the user-levelevaluation of LVLMs in an open-world question-answering scenario. The studyreveals several innovative findings. First, instruction-tuned LVLM with massivein-domain data such as InstructBLIP heavily overfits many existing tasks,generalizing poorly in the open-world scenario. Second, instruction-tuned LVLMwith moderate instruction-following data may result in object hallucinationissues (i.e., generate objects that are inconsistent with target images in thedescriptions). It either makes the current evaluation metric such as CIDEr forimage captioning ineffective or generates wrong answers. Third, employing amulti-turn reasoning evaluation framework can mitigate the issue of objecthallucination, shedding light on developing an effective pipeline for LVLMevaluation. The findings provide a foundational framework for the conceptionand assessment of innovative strategies aimed at enhancing zero-shot multimodaltechniques. Our LVLM-eHub will be available athttps://github.com/OpenGVLab/Multi-Modality-Arena</description><author>Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, Ping Luo</author><pubDate>Thu, 15 Jun 2023 17:39:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09265v1</guid></item><item><title>Harvard Glaucoma Fairness: A Retinal Nerve Disease Dataset for Fairness Learning and Fair Identity Normalization</title><link>http://arxiv.org/abs/2306.09264v1</link><description>Fairness in machine learning is important for societal well-being, butlimited public datasets hinder its progress. Currently, no dedicated publicmedical datasets with imaging data for fairness learning are available, thoughminority groups suffer from more health issues. To address this gap, weintroduce Harvard Glaucoma Fairness (Harvard-GF), a retinal nerve diseasedataset with both 2D and 3D imaging data and balanced racial groups forglaucoma detection. Glaucoma is the leading cause of irreversible blindnessglobally with Blacks having doubled glaucoma prevalence than other races. Wealso propose a fair identity normalization (FIN) approach to equalize thefeature importance between different identity groups. Our FIN approach iscompared with various the-state-of-the-arts fairness learning methods withsuperior performance in both racial and gender fairness tasks with 2D and 3Dimaging data, which demonstrate the utilities of our dataset Harvard-GF forfairness learning. To facilitate fairness comparisons between different models,we propose an equity-scaled performance measure, which can be flexibly used tocompare all kinds of performance metrics in the context of fairness. Thedataset and code are publicly accessible via https://doi.org/10.7910/DVN/A4XMO1and https://github.com/luoyan407/Harvard-GF, respectively.</description><author>Yan Luo, Yu Tian, Min Shi, Tobias Elze, Mengyu Wang</author><pubDate>Thu, 15 Jun 2023 17:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09264v1</guid></item><item><title>A Heavy-Tailed Algebra for Probabilistic Programming</title><link>http://arxiv.org/abs/2306.09262v1</link><description>Despite the successes of probabilistic models based on passing noise throughneural networks, recent work has identified that such methods often fail tocapture tail behavior accurately, unless the tails of the base distribution areappropriately calibrated. To overcome this deficiency, we propose a systematicapproach for analyzing the tails of random variables, and we illustrate howthis approach can be used during the static analysis (before drawing samples)pass of a probabilistic programming language compiler. To characterize how thetails change under various operations, we develop an algebra which acts on athree-parameter family of tail asymptotics and which is based on thegeneralized Gamma distribution. Our algebraic operations are closed underaddition and multiplication; they are capable of distinguishing sub-Gaussianswith differing scales; and they handle ratios sufficiently well to reproducethe tails of most important statistical distributions directly from theirdefinitions. Our empirical results confirm that inference algorithms thatleverage our heavy-tailed algebra attain superior performance across a numberof density modeling and variational inference tasks.</description><author>Feynman Liang, Liam Hodgkinson, Michael W. Mahoney</author><pubDate>Thu, 15 Jun 2023 17:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09262v1</guid></item><item><title>SepViT: Separable Vision Transformer</title><link>http://arxiv.org/abs/2203.15380v4</link><description>Vision Transformers have witnessed prevailing success in a series of visiontasks. However, these Transformers often rely on extensive computational coststo achieve high performance, which is burdensome to deploy onresource-constrained devices. To alleviate this issue, we draw lessons fromdepthwise separable convolution and imitate its ideology to design an efficientTransformer backbone, i.e., Separable Vision Transformer, abbreviated asSepViT. SepViT helps to carry out the local-global information interactionwithin and among the windows in sequential order via a depthwise separableself-attention. The novel window token embedding and grouped self-attention areemployed to compute the attention relationship among windows with negligiblecost and establish long-range visual interactions across multiple windows,respectively. Extensive experiments on general-purpose vision benchmarksdemonstrate that SepViT can achieve a state-of-the-art trade-off betweenperformance and latency. Among them, SepViT achieves 84.2% top-1 accuracy onImageNet-1K classification while decreasing the latency by 40%, compared to theones with similar accuracy (e.g., CSWin). Furthermore, SepViT achieves 51.0%mIoU on ADE20K semantic segmentation task, 47.9 AP on the RetinaNet-based COCOdetection task, 49.4 box AP and 44.6 mask AP on Mask R-CNN-based COCO objectdetection and instance segmentation tasks.</description><author>Wei Li, Xing Wang, Xin Xia, Jie Wu, Jiashi Li, Xuefeng Xiao, Min Zheng, Shiping Wen</author><pubDate>Thu, 15 Jun 2023 17:37:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.15380v4</guid></item><item><title>Adversarial Cheap Talk</title><link>http://arxiv.org/abs/2211.11030v2</link><description>Adversarial attacks in reinforcement learning (RL) often assumehighly-privileged access to the victim's parameters, environment, or data.Instead, this paper proposes a novel adversarial setting called a Cheap TalkMDP in which an Adversary can merely append deterministic messages to theVictim's observation, resulting in a minimal range of influence. The Adversarycannot occlude ground truth, influence underlying environment dynamics orreward signals, introduce non-stationarity, add stochasticity, see the Victim'sactions, or access their parameters. Additionally, we present a simplemeta-learning algorithm called Adversarial Cheap Talk (ACT) to trainAdversaries in this setting. We demonstrate that an Adversary trained with ACTstill significantly influences the Victim's training and testing performance,despite the highly constrained setting. Affecting train-time performancereveals a new attack vector and provides insight into the success and failuremodes of existing RL algorithms. More specifically, we show that an ACTAdversary is capable of harming performance by interfering with the learner'sfunction approximation, or instead helping the Victim's performance byoutputting useful features. Finally, we show that an ACT Adversary canmanipulate messages during train-time to directly and arbitrarily control theVictim at test-time. Project video and code are available athttps://sites.google.com/view/adversarial-cheap-talk</description><author>Chris Lu, Timon Willi, Alistair Letcher, Jakob Foerster</author><pubDate>Thu, 15 Jun 2023 17:37:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11030v2</guid></item><item><title>Mitigating Cold-start Forecasting using Cold Causal Demand Forecasting Model</title><link>http://arxiv.org/abs/2306.09261v1</link><description>Forecasting multivariate time series data, which involves predicting futurevalues of variables over time using historical data, has significant practicalapplications. Although deep learning-based models have shown promise in thisfield, they often fail to capture the causal relationship between dependentvariables, leading to less accurate forecasts. Additionally, these modelscannot handle the cold-start problem in time series data, where certainvariables lack historical data, posing challenges in identifying dependenciesamong variables. To address these limitations, we introduce the Cold CausalDemand Forecasting (CDF-cold) framework that integrates causal inference withdeep learning-based models to enhance the forecasting accuracy of multivariatetime series data affected by the cold-start problem. To validate theeffectiveness of the proposed approach, we collect 15 multivariate time-seriesdatasets containing the network traffic of different Google data centers. Ourexperiments demonstrate that the CDF-cold framework outperformsstate-of-the-art forecasting models in predicting future values of multivariatetime series data.</description><author>Zahra Fatemi, Minh Huynh, Elena Zheleva, Zamir Syed, Xiaojun Di</author><pubDate>Thu, 15 Jun 2023 17:36:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09261v1</guid></item><item><title>A Survey of Some Density Based Clustering Techniques</title><link>http://arxiv.org/abs/2306.09256v1</link><description>Density Based Clustering are a type of Clustering methods using in datamining for extracting previously unknown patterns from data sets. There are anumber of density based clustering methods such as DBSCAN, OPTICS, DENCLUE,VDBSCAN, DVBSCAN, DBCLASD and ST-DBSCAN. In this paper, a study of thesemethods is done along with their characteristics, advantages and disadvantagesand most importantly, their applicability to different types of data sets tomine useful and appropriate patterns.</description><author>Rupanka Bhuyan, Samarjeet Borah</author><pubDate>Thu, 15 Jun 2023 17:32:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09256v1</guid></item><item><title>MinMax Networks</title><link>http://arxiv.org/abs/2306.09253v1</link><description>While much progress has been achieved over the last decades in neuro-inspiredmachine learning, there are still fundamental theoretical problems ingradient-based learning using combinations of neurons. These problems, such assaddle points and suboptimal plateaus of the cost function, can lead in theoryand practice to failures of learning. In addition, the discrete step sizeselection of the gradient is problematic since too large steps can lead toinstability and too small steps slow down the learning. This paper describes analternative discrete MinMax learning approach for continuous piece-wise linearfunctions. Global exponential convergence of the algorithm is established usingContraction Theory with Inequality Constraints, which is extended from thecontinuous to the discrete case in this paper: The parametrization of each linear function piece is, in contrast to deeplearning, linear in the proposed MinMax network. This allows a linearregression stability proof as long as measurements do not transit from onelinear region to its neighbouring linear region. The step size of the discrete gradient descent is Lagrangian limitedorthogonal to the edge of two neighbouring linear functions. It will be shownthat this Lagrangian step limitation does not decrease the convergence of theunconstrained system dynamics in contrast to a step size limitation in thedirection of the gradient. We show that the convergence rate of a constrained piece-wise linear functionlearning is equivalent to the exponential convergence rates of the individuallocal linear regions.</description><author>Winfried Lohmiller, Philipp Gassert, Jean-Jacques Slotine</author><pubDate>Thu, 15 Jun 2023 17:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09253v1</guid></item><item><title>Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models</title><link>http://arxiv.org/abs/2306.09251v1</link><description>Diffusion models, which convert noise into new data instances by learning toreverse a Markov diffusion process, have become a cornerstone in contemporarygenerative modeling. While their practical power has now been widelyrecognized, the theoretical underpinnings remain far from mature. In this work,we develop a suite of non-asymptotic theory towards understanding the datageneration process of diffusion models in discrete time, assuming access toreliable estimates of the (Stein) score functions. For a popular deterministicsampler (based on the probability flow ODE), we establish a convergence rateproportional to $1/T$ (with $T$ the total number of steps), improving upon pastresults; for another mainstream stochastic sampler (i.e., a type of thedenoising diffusion probabilistic model (DDPM)), we derive a convergence rateproportional to $1/\sqrt{T}$, matching the state-of-the-art theory. Our theoryimposes only minimal assumptions on the target data distribution (e.g., nosmoothness assumption is imposed), and is developed based on an elementary yetversatile non-asymptotic approach without resorting to toolboxes for SDEs andODEs. Further, we design two accelerated variants, improving the convergence to$1/T^2$ for the ODE-based sampler and $1/T$ for the DDPM-type sampler, whichmight be of independent theoretical and empirical interest.</description><author>Gen Li, Yuting Wei, Yuxin Chen, Yuejie Chi</author><pubDate>Thu, 15 Jun 2023 17:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09251v1</guid></item><item><title>BeGin: Extensive Benchmark Scenarios and An Easy-to-use Framework for Graph Continual Learning</title><link>http://arxiv.org/abs/2211.14568v2</link><description>Continual Learning (CL) is the process of learning ceaselessly a sequence oftasks. Most existing CL methods deal with independent data (e.g., images andtext) for which many benchmark frameworks and results under standardexperimental settings are available. However, CL methods for graph data (graphCL) are surprisingly underexplored because of (a) the lack of standardexperimental settings, especially regarding how to deal with the dependencybetween instances, (b) the lack of benchmark datasets and scenarios, and (c)high complexity in implementation and evaluation due to the dependency. In thispaper, regarding (a), we define four standard incremental settings (task-,class-, domain-, and time-incremental) for graph data, which are naturallyapplied to many node-, link-, and graph-level problems. Regarding (b), weprovide 25 benchmark scenarios based on 15 real-world graphs. Regarding (c), wedevelop BeGin, an easy and fool-proof framework for graph CL. BeGin is easilyextended since it is modularized with reusable modules for data processing,algorithm design, and evaluation. Especially, the evaluation module iscompletely separated from user code to eliminate potential mistakes. Using allthe above, we report extensive benchmark results of 10 graph CL methods.Compared to the latest benchmark for graph CL, using BeGin, we cover 3x morecombinations of incremental settings and levels of problems. All assets for thebenchmark framework are available at https://github.com/ShinhwanKang/BeGin.</description><author>Jihoon Ko, Shinhwan Kang, Taehyung Kwon, Heechan Moon, Kijung Shin</author><pubDate>Thu, 15 Jun 2023 17:29:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14568v2</guid></item><item><title>Improving Training Stability for Multitask Ranking Models in Recommender Systems</title><link>http://arxiv.org/abs/2302.09178v2</link><description>Recommender systems play an important role in many content platforms. Whilemost recommendation research is dedicated to designing better models to improveuser experience, we found that research on stabilizing the training for suchmodels is severely under-explored. As recommendation models become larger andmore sophisticated, they are more susceptible to training instability issues,i.e., loss divergence, which can make the model unusable, waste significantresources and block model developments. In this paper, we share our findingsand best practices we learned for improving the training stability of areal-world multitask ranking model for YouTube recommendations. We show someproperties of the model that lead to unstable training and conjecture on thecauses. Furthermore, based on our observations of training dynamics near thepoint of training instability, we hypothesize why existing solutions wouldfail, and propose a new algorithm to mitigate the limitations of existingsolutions. Our experiments on YouTube production dataset show the proposedalgorithm can significantly improve training stability while not compromisingconvergence, comparing with several commonly used baseline methods.</description><author>Jiaxi Tang, Yoel Drori, Daryl Chang, Maheswaran Sathiamoorthy, Justin Gilmer, Li Wei, Xinyang Yi, Lichan Hong, Ed H. Chi</author><pubDate>Thu, 15 Jun 2023 17:28:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09178v2</guid></item><item><title>Unprocessing Seven Years of Algorithmic Fairness</title><link>http://arxiv.org/abs/2306.07261v2</link><description>Seven years ago, researchers proposed a postprocessing method to equalize theerror rates of a model across different demographic groups. The work launchedhundreds of papers purporting to improve over the postprocessing baseline. Weempirically evaluate these claims through thousands of model evaluations onseveral tabular datasets. We find that the fairness-accuracy Pareto frontierachieved by postprocessing contains all other methods we were feasibly able toevaluate. In doing so, we address two common methodological errors that haveconfounded previous observations. One relates to the comparison of methods withdifferent unconstrained base models. The other concerns methods achievingdifferent levels of constraint relaxation. At the heart of our study is asimple idea we call unprocessing that roughly corresponds to the inverse ofpostprocessing. Unprocessing allows for a direct comparison of methods usingdifferent underlying models and levels of relaxation. Interpreting ourfindings, we recall a widely overlooked theoretical argument, present sevenyears ago, that accurately predicted what we observe.</description><author>André F. Cruz, Moritz Hardt</author><pubDate>Thu, 15 Jun 2023 17:27:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07261v2</guid></item><item><title>Text Promptable Surgical Instrument Segmentation with Vision-Language Models</title><link>http://arxiv.org/abs/2306.09244v1</link><description>In this paper, we propose a novel text promptable surgical instrumentsegmentation approach to overcome challenges associated with diversity anddifferentiation of surgical instruments in minimally invasive surgeries. Weredefine the task as text promptable, thereby enabling a more nuancedcomprehension of surgical instruments and adaptability to new instrument types.Inspired by recent advancements in vision-language models, we leveragepretrained image and text encoders as our model backbone and design a textpromptable mask decoder consisting of attention- and convolution-basedprompting schemes for surgical instrument segmentation prediction. Our modelleverages multiple text prompts for each surgical instrument through a newmixture of prompts mechanism, resulting in enhanced segmentation performance.Additionally, we introduce a hard instrument area reinforcement module toimprove image feature comprehension and segmentation precision. Extensiveexperiments on EndoVis2017 and EndoVis2018 datasets demonstrate our model'ssuperior performance and promising generalization capability. To our knowledge,this is the first implementation of a promptable approach to surgicalinstrument segmentation, offering significant potential for practicalapplication in the field of robotic-assisted surgery.</description><author>Zijian Zhou, Oluwatosin Alabi, Meng Wei, Tom Vercauteren, Miaojing Shi</author><pubDate>Thu, 15 Jun 2023 17:26:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09244v1</guid></item><item><title>Exploiting the Brain's Network Structure for Automatic Identification of ADHD Subjects</title><link>http://arxiv.org/abs/2306.09239v1</link><description>Attention Deficit Hyperactive Disorder (ADHD) is a common behavioral problemaffecting children. In this work, we investigate the automatic classificationof ADHD subjects using the resting state Functional Magnetic Resonance Imaging(fMRI) sequences of the brain. We show that the brain can be modeled as afunctional network, and certain properties of the networks differ in ADHDsubjects from control subjects. We compute the pairwise correlation of brainvoxels' activity over the time frame of the experimental protocol which helpsto model the function of a brain as a network. Different network features arecomputed for each of the voxels constructing the network. The concatenation ofthe network features of all the voxels in a brain serves as the feature vector.Feature vectors from a set of subjects are then used to train a PCA-LDA(principal component analysis-linear discriminant analysis) based classifier.We hypothesized that ADHD-related differences lie in some specific regions ofthe brain and using features only from those regions is sufficient todiscriminate ADHD and control subjects. We propose a method to create a brainmask that includes the useful regions only and demonstrate that using thefeature from the masked regions improves classification accuracy on the testdata set. We train our classifier with 776 subjects and test on 171 subjectsprovided by The Neuro Bureau for the ADHD-200 challenge. We demonstrate theutility of graph-motif features, specifically the maps that represent thefrequency of participation of voxels in network cycles of length 3. The bestclassification performance (69.59%) is achieved using 3-cycle map features withmasking. Our proposed approach holds promise in being able to diagnose andunderstand the disorder.</description><author>Soumyabrata Dey, Ravishankar Rao, Mubarak Shah</author><pubDate>Thu, 15 Jun 2023 17:22:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09239v1</guid></item><item><title>SCALE: Scaling up the Complexity for Advanced Language Model Evaluation</title><link>http://arxiv.org/abs/2306.09237v1</link><description>Recent strides in Large Language Models (LLMs) have saturated many NLPbenchmarks (even professional domain-specific ones), emphasizing the need fornovel, more challenging novel ones to properly assess LLM capabilities. In thispaper, we introduce a novel NLP benchmark that poses challenges to current LLMsacross four key dimensions: processing long documents (up to 50K tokens),utilizing domain specific knowledge (embodied in legal texts), multilingualunderstanding (covering five languages), and multitasking (comprising legaldocument to document Information Retrieval, Court View Generation, LeadingDecision Summarization, Citation Extraction, and eight challenging TextClassification tasks). Our benchmark comprises diverse legal NLP datasets fromthe Swiss legal system, allowing for a comprehensive study of the underlyingNon-English, inherently multilingual, federal legal system. Despite recentadvances, efficiently processing long documents for intense review/analysistasks remains an open challenge for language models. Also, comprehensive,domain-specific benchmarks requiring high expertise to develop are rare, as aremultilingual benchmarks. This scarcity underscores our contribution's value,considering most public models are trained predominantly on English corpora,while other languages remain understudied, particularly for practicaldomain-specific NLP tasks. Our benchmark allows for testing and advancing thestate-of-the-art LLMs. As part of our study, we evaluate several pre-trainedmultilingual language models on our benchmark to establish strong baselines asa point of reference. Despite the large size of our datasets (tens to hundredsof thousands of examples), existing publicly available models struggle withmost tasks, even after in-domain pretraining. We publish all resources(benchmark suite, pre-trained models, code) under a fully permissive open CCBY-SA license.</description><author>Vishvaksenan Rasiah, Ronja Stern, Veton Matoshi, Matthias Stürmer, Ilias Chalkidis, Daniel E. Ho, Joel Niklaus</author><pubDate>Thu, 15 Jun 2023 17:19:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09237v1</guid></item><item><title>ppAURORA: Privacy Preserving Area Under Receiver Operating Characteristic and Precision-Recall Curves</title><link>http://arxiv.org/abs/2102.08788v3</link><description>Computing an AUC as a performance measure to compare the quality of differentmachine learning models is one of the final steps of many research projects.Many of these methods are trained on privacy-sensitive data and there areseveral different approaches like $\epsilon$-differential privacy, federatedmachine learning and cryptography if the datasets cannot be shared or usedjointly at one place for training and/or testing. In this setting, it can alsobe a problem to compute the global AUC, since the labels might also containprivacy-sensitive information. There have been approaches based on$\epsilon$-differential privacy to address this problem, but to the best of ourknowledge, no exact privacy preserving solution has been introduced. In thispaper, we propose an MPC-based solution, called ppAURORA, with private mergingof individually sorted lists from multiple sources to compute the exact AUC asone could obtain on the pooled original test samples. With ppAURORA, thecomputation of the exact area under precision-recall and receiver operatingcharacteristic curves is possible even when ties between prediction confidencevalues exist. We use ppAURORA to evaluate two different models predicting acutemyeloid leukemia therapy response and heart disease, respectively. We alsoassess its scalability via synthetic data experiments. All these experimentsshow that we efficiently and privately compute the exact same AUC with bothevaluation metrics as one can obtain on the pooled test samples in plaintextaccording to the semi-honest adversary setting.</description><author>Ali Burak Ünal, Nico Pfeifer, Mete Akgün</author><pubDate>Thu, 15 Jun 2023 17:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.08788v3</guid></item><item><title>Volterra Neural Networks (VNNs)</title><link>http://arxiv.org/abs/1910.09616v5</link><description>The importance of inference in Machine Learning (ML) has led to an explosivenumber of different proposals in ML, and particularly in Deep Learning. In anattempt to reduce the complexity of Convolutional Neural Networks, we propose aVolterra filter-inspired Network architecture. This architecture introducescontrolled non-linearities in the form of interactions between the delayedinput samples of data. We propose a cascaded implementation of VolterraFiltering so as to significantly reduce the number of parameters required tocarry out the same classification task as that of a conventional NeuralNetwork. We demonstrate an efficient parallel implementation of this VolterraNeural Network (VNN), along with its remarkable performance while retaining arelatively simpler and potentially more tractable structure. Furthermore, weshow a rather sophisticated adaptation of this network to nonlinearly fuse theRGB (spatial) information and the Optical Flow (temporal) information of avideo sequence for action recognition. The proposed approach is evaluated onUCF-101 and HMDB-51 datasets for action recognition, and is shown to outperformstate of the art CNN approaches.</description><author>Siddharth Roheda, Hamid Krim</author><pubDate>Thu, 15 Jun 2023 17:06:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1910.09616v5</guid></item><item><title>Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories</title><link>http://arxiv.org/abs/2306.09224v1</link><description>We propose Encyclopedic-VQA, a large scale visual question answering (VQA)dataset featuring visual questions about detailed properties of fine-grainedcategories and instances. It contains 221k unique question+answer pairs eachmatched with (up to) 5 images, resulting in a total of 1M VQA samples.Moreover, our dataset comes with a controlled knowledge base derived fromWikipedia, marking the evidence to support each answer. Empirically, we showthat our dataset poses a hard challenge for large vision+language models asthey perform poorly on our dataset: PaLI [14] is state-of-the-art on OK-VQA[37], yet it only achieves 13.0% accuracy on our dataset. Moreover, weexperimentally show that progress on answering our encyclopedic questions canbe achieved by augmenting large models with a mechanism that retrieves relevantinformation from the knowledge base. An oracle experiment with perfectretrieval achieves 87.0% accuracy on the single-hop portion of our dataset, andan automatic retrieval-augmented prototype yields 48.8%. We believe that ourdataset enables future research on retrieval-augmented vision+language models.</description><author>Thomas Mensink, Jasper Uijlings, Lluis Castrejon, Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha, André Araujo, Vittorio Ferrari</author><pubDate>Thu, 15 Jun 2023 17:03:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09224v1</guid></item><item><title>Few-shot bioacoustic event detection at the DCASE 2023 challenge</title><link>http://arxiv.org/abs/2306.09223v1</link><description>Few-shot bioacoustic event detection consists in detecting sound events ofspecified types, in varying soundscapes, while having access to only a fewexamples of the class of interest. This task ran as part of the DCASE challengefor the third time this year with an evaluation set expanded to include newanimal species, and a new rule: ensemble models were no longer allowed. The2023 few shot task received submissions from 6 different teams with F-scoresreaching as high as 63% on the evaluation set. Here we describe the task,focusing on describing the elements that differed from previous years. We alsotake a look back at past editions to describe how the task has evolved. Notonly have the F-score results steadily improved (40% to 60% to 63%), but thetype of systems proposed have also become more complex. Sound event detectionsystems are no longer simple variations of the baselines provided: multiplefew-shot learning methodologies are still strong contenders for the task.</description><author>Ines Nolasco, Burooj Ghani, Shubhr Singh, Ester Vidaña-Vila, Helen Whitehead, Emily Grout, Michael Emmerson, Frants Jensen, Ivan Kiskin, Joe Morford, Ariana Strandburg-Peshkin, Lisa Gill, Hanna Pamuła, Vincent Lostanlen, Dan Stowell</author><pubDate>Thu, 15 Jun 2023 16:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09223v1</guid></item><item><title>Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization</title><link>http://arxiv.org/abs/2306.09222v1</link><description>We develop a re-weighted gradient descent technique for boosting theperformance of deep neural networks. Our algorithm involves the importanceweighting of data points during each optimization step. Our approach isinspired by distributionally robust optimization with $f$-divergences, whichhas been known to result in models with improved generalization guarantees. Ourre-weighting scheme is simple, computationally efficient, and can be combinedwith any popular optimization algorithms such as SGD and Adam. Empirically, wedemonstrate our approach's superiority on various tasks, including vanillaclassification, classification with label imbalance, noisy labels, domainadaptation, and tabular representation learning. Notably, we obtainimprovements of +0.7% and +1.44% over SOTA on DomainBed and Tabular benchmarks,respectively. Moreover, our algorithm boosts the performance of BERT on GLUEbenchmarks by +1.94%, and ViT on ImageNet-1K by +0.9%. These resultsdemonstrate the effectiveness of the proposed approach, indicating itspotential for improving performance in diverse domains.</description><author>Ramnath Kumar, Kushal Majmundar, Dheeraj Nagaraj, Arun Sai Suggala</author><pubDate>Thu, 15 Jun 2023 16:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09222v1</guid></item><item><title>CMMLU: Measuring massive multitask language understanding in Chinese</title><link>http://arxiv.org/abs/2306.09212v1</link><description>As the capabilities of large language models (LLMs) continue to advance,evaluating their performance becomes increasingly crucial and challenging. Thispaper aims to bridge this gap by introducing CMMLU, a comprehensive Chinesebenchmark that covers various subjects, including natural science, socialsciences, engineering, and humanities. We conduct a thorough evaluation of 18advanced multilingual- and Chinese-oriented LLMs, assessing their performanceacross different subjects and settings. The results reveal that most existingLLMs struggle to achieve an average accuracy of 50%, even when provided within-context examples and chain-of-thought prompts, whereas the random baselinestands at 25%. This highlights significant room for improvement in LLMs.Additionally, we conduct extensive experiments to identify factors impactingthe models' performance and propose directions for enhancing LLMs. CMMLU fillsthe gap in evaluating the knowledge and reasoning capabilities of largelanguage models within the Chinese context.</description><author>Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin</author><pubDate>Thu, 15 Jun 2023 16:49:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09212v1</guid></item><item><title>A Framework for Learning from Demonstration with Minimal Human Effort</title><link>http://arxiv.org/abs/2306.09211v1</link><description>We consider robot learning in the context of shared autonomy, where controlof the system can switch between a human teleoperator and autonomous control.In this setting we address reinforcement learning, and learning fromdemonstration, where there is a cost associated with human time. This costrepresents the human time required to teleoperate the robot, or recover therobot from failures. For each episode, the agent must choose between requestinghuman teleoperation, or using one of its autonomous controllers. In ourapproach, we learn to predict the success probability for each controller,given the initial state of an episode. This is used in a contextual multi-armedbandit algorithm to choose the controller for the episode. A controller islearnt online from demonstrations and reinforcement learning so that autonomousperformance improves, and the system becomes less reliant on the teleoperatorwith more experience. We show that our approach to controller selection reducesthe human cost to perform two simulated tasks and a single real-world task.</description><author>Marc Rigter, Bruno Lacerda, Nick Hawes</author><pubDate>Thu, 15 Jun 2023 16:49:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09211v1</guid></item><item><title>Optimal Exploration for Model-Based RL in Nonlinear Systems</title><link>http://arxiv.org/abs/2306.09210v1</link><description>Learning to control unknown nonlinear dynamical systems is a fundamentalproblem in reinforcement learning and control theory. A commonly appliedapproach is to first explore the environment (exploration), learn an accuratemodel of it (system identification), and then compute an optimal controllerwith the minimum cost on this estimated system (policy optimization). Whileexisting work has shown that it is possible to learn a uniformly good model ofthe system~\citep{mania2020active}, in practice, if we aim to learn a goodcontroller with a low cost on the actual system, certain system parameters maybe significantly more critical than others, and we therefore ought to focus ourexploration on learning such parameters. In this work, we consider the setting of nonlinear dynamical systems and seekto formally quantify, in such settings, (a) which parameters are most relevantto learning a good controller, and (b) how we can best explore so as tominimize uncertainty in such parameters. Inspired by recent work in linearsystems~\citep{wagenmaker2021task}, we show that minimizing the controller lossin nonlinear systems translates to estimating the system parameters in aparticular, task-dependent metric. Motivated by this, we develop an algorithmable to efficiently explore the system to reduce uncertainty in this metric,and prove a lower bound showing that our approach learns a controller at anear-instance-optimal rate. Our algorithm relies on a general reduction frompolicy optimization to optimal experiment design in arbitrary systems, and maybe of independent interest. We conclude with experiments demonstrating theeffectiveness of our method in realistic nonlinear robotic systems.</description><author>Andrew Wagenmaker, Guanya Shi, Kevin Jamieson</author><pubDate>Thu, 15 Jun 2023 16:47:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09210v1</guid></item><item><title>Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches</title><link>http://arxiv.org/abs/2306.07220v2</link><description>We present Strokes2Surface, an offline geometry-reconstruction pipeline builtupon a 4D Sketching Interface, MR.Sketch, targeted at architectural design. Thepipeline recovers a curve network from designer-drawn strokes, thus bridgingbetween concept design and digital modeling stages in architectural design. Theinput to our pipeline consists of 3D strokes' polyline vertices and theircorresponding timestamps (as of the fourth dimension), along with additionalgeometric and stylus-related recorded properties. Inspired by sketchconsolidation and sketch-based modeling methods, our pipeline leverages suchdata and combines three Machine Learning (ML) models; a classifier and twoclustering models. In particular, based on observations of practices designerstypically employ in architectural design sketches, we solve a binaryclassification problem to recognize whether a stroke depicts a boundary andedge or is used to fill in the enclosing areas and faces of the intendedarchitectural object. Followed by the two clustering models, strokes of eachtype are further parsed into groups, each representing either a single edge ora single face. Next, groups representing edges are approximated with B-splinecurves, followed by a topology-recovering process identifying and fixingdesired connectivities between the curves forming a well-connected curvenetwork. Next, groups representing the faces are employed to detect the cyclesbounding patches in the curve network, resulting in the final surface meshgeometry of the architectural object. We confirm the usability ofStrokes2Surface via a user study and further validate and compare our resultsagainst a range of reconstructions computed using alternative methods. We alsointroduce our manually labeled dataset of 4D architectural design sketches forfurther use in the community.</description><author>S. Rasoulzadeh, M. Wimmer, I. Kovacic</author><pubDate>Thu, 15 Jun 2023 16:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07220v2</guid></item><item><title>Reward-Free Curricula for Training Robust World Models</title><link>http://arxiv.org/abs/2306.09205v1</link><description>There has been a recent surge of interest in developing generally-capableagents that can adapt to new tasks without additional training in theenvironment. Learning world models from reward-free exploration is a promisingapproach, and enables policies to be trained using imagined experience for newtasks. Achieving a general agent requires robustness across differentenvironments. However, different environments may require different amounts ofdata to learn a suitable world model. In this work, we address the problem ofefficiently learning robust world models in the reward-free setting. As ameasure of robustness, we consider the minimax regret objective. We show thatthe minimax regret objective can be connected to minimising the maximum errorin the world model across environments. This informs our algorithm, WAKER:Weighted Acquisition of Knowledge across Environments for Robustness. WAKERselects environments for data collection based on the estimated error of theworld model for each environment. Our experiments demonstrate that WAKERoutperforms naive domain randomisation, resulting in improved robustness,efficiency, and generalisation.</description><author>Marc Rigter, Minqi Jiang, Ingmar Posner</author><pubDate>Thu, 15 Jun 2023 16:40:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09205v1</guid></item><item><title>Transferring Knowledge for Food Image Segmentation using Transformers and Convolutions</title><link>http://arxiv.org/abs/2306.09203v1</link><description>Food image segmentation is an important task that has ubiquitousapplications, such as estimating the nutritional value of a plate of food.Although machine learning models have been used for segmentation in thisdomain, food images pose several challenges. One challenge is that food itemscan overlap and mix, making them difficult to distinguish. Another challenge isthe degree of inter-class similarity and intra-class variability, which iscaused by the varying preparation methods and dishes a food item may be servedin. Additionally, class imbalance is an inevitable issue in food datasets. Toaddress these issues, two models are trained and compared, one based onconvolutional neural networks and the other on Bidirectional Encoderrepresentation for Image Transformers (BEiT). The models are trained andvaluated using the FoodSeg103 dataset, which is identified as a robustbenchmark for food image segmentation. The BEiT model outperforms the previousstate-of-the-art model by achieving a mean intersection over union of 49.4 onFoodSeg103. This study provides insights into transfering knowledge usingconvolution and Transformer-based approaches in the food image domain.</description><author>Grant Sinha, Krish Parmar, Hilda Azimi, Amy Tai, Yuhao Chen, Alexander Wong, Pengcheng Xi</author><pubDate>Thu, 15 Jun 2023 16:38:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09203v1</guid></item><item><title>Combinatorial Pure Exploration of Multi-Armed Bandit with a Real Number Action Class</title><link>http://arxiv.org/abs/2306.09202v1</link><description>The combinatorial pure exploration (CPE) in the stochastic multi-armed banditsetting (MAB) is a well-studied online decision-making problem: A player wantsto find the optimal \emph{action} $\boldsymbol{\pi}^*$ from \emph{action class}$\mathcal{A}$, which is a collection of subsets of arms with certaincombinatorial structures. Though CPE can represent many combinatorialstructures such as paths, matching, and spanning trees, most existing worksfocus only on binary action class $\mathcal{A}\subseteq\{0, 1\}^d$ for somepositive integer $d$. This binary formulation excludes important problems suchas the optimal transport, knapsack, and production planning problems. Toovercome this limitation, we extend the binary formulation to real,$\mathcal{A}\subseteq\mathbb{R}^d$, and propose a new algorithm. The onlyassumption we make is that the number of actions in $\mathcal{A}$ is polynomialin $d$. We show an upper bound of the sample complexity for our algorithm andthe action class-dependent lower bound for R-CPE-MAB, by introducing a quantitythat characterizes the problem's difficulty, which is a generalization of thenotion \emph{width} introduced in Chen et al.[2014].</description><author>Shintaro Nakamura, Masashi Sugiyama</author><pubDate>Thu, 15 Jun 2023 16:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09202v1</guid></item><item><title>Subject Granular Differential Privacy in Federated Learning</title><link>http://arxiv.org/abs/2206.03617v2</link><description>This paper considers subject level privacy in the FL setting, where a subjectis an individual whose private information is embodied by several data itemseither confined within a single federation user or distributed across multiplefederation users. We propose two new algorithms that enforce subject level DPat each federation user locally. Our first algorithm, called LocalGroupDP, is astraightforward application of group differential privacy in the popular DP-SGDalgorithm. Our second algorithm is based on a novel idea of hierarchicalgradient averaging (HiGradAvgDP) for subjects participating in a trainingmini-batch. We also show that user level Local Differential Privacy (LDP)naturally guarantees subject level DP. We observe the problem of horizontalcomposition of subject level privacy loss in FL - subject level privacy lossincurred at individual users composes across the federation. We formally provethe subject level DP guarantee for our algorithms, and also show their effecton model utility loss. Our empirical evaluation on FEMNIST and Shakespearedatasets shows that LocalGroupDP delivers the best performance among ouralgorithms. However, its model utility lags behind that of models trained usinga DP-SGD based algorithm that provides a weaker item level privacy guarantee.Privacy loss amplification due to subject sampling fractions and horizontalcomposition remain key challenges for model utility.</description><author>Virendra J. Marathe, Pallika Kanani, Daniel W. Peterson, Guy Steele Jr</author><pubDate>Thu, 15 Jun 2023 16:37:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.03617v2</guid></item><item><title>ChessGPT: Bridging Policy Learning and Language Modeling</title><link>http://arxiv.org/abs/2306.09200v1</link><description>When solving decision-making tasks, humans typically depend on informationfrom two key sources: (1) Historical policy data, which provides interactionreplay from the environment, and (2) Analytical insights in natural languageform, exposing the invaluable thought process or strategic considerations.Despite this, the majority of preceding research focuses on only one source:they either use historical replay exclusively to directly learn policy or valuefunctions, or engaged in language model training utilizing mere languagecorpus. In this paper, we argue that a powerful autonomous agent should coverboth sources. Thus, we propose ChessGPT, a GPT model bridging policy learningand language modeling by integrating data from these two sources in Chessgames. Specifically, we build a large-scale game and language dataset relatedto chess. Leveraging the dataset, we showcase two model examples ChessCLIP andChessGPT, integrating policy learning and language modeling. Finally, wepropose a full evaluation framework for evaluating language model's chessability. Experimental results validate our model and dataset's effectiveness.We open source our code, model, and dataset athttps://github.com/waterhorse1/ChessGPT.</description><author>Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, Jun Wang</author><pubDate>Thu, 15 Jun 2023 16:35:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09200v1</guid></item><item><title>Infrastructure Crack Segmentation: Boundary Guidance Method and Benchmark Dataset</title><link>http://arxiv.org/abs/2306.09196v1</link><description>Cracks provide an essential indicator of infrastructure performancedegradation, and achieving high-precision pixel-level crack segmentation is anissue of concern. Unlike the common research paradigms that adopt novelartificial intelligence (AI) methods directly, this paper examines the inherentcharacteristics of cracks so as to introduce boundary features into crackidentification and then builds a boundary guidance crack segmentation model(BGCrack) with targeted structures and modules, including a high frequencymodule, global information modeling module, joint optimization module, etc.Extensive experimental results verify the feasibility of the proposed designsand the effectiveness of the edge information in improving segmentationresults. In addition, considering that notable open-source datasets mainlyconsist of asphalt pavement cracks because of ease of access, there is nostandard and widely recognized dataset yet for steel structures, one of theprimary structural forms in civil infrastructure. This paper provides a steelcrack dataset that establishes a unified and fair benchmark for theidentification of steel cracks.</description><author>Zhili He, Wang Chen, Jian Zhang, Yu-Hsing Wang</author><pubDate>Thu, 15 Jun 2023 16:25:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09196v1</guid></item><item><title>Training Diffusion Classifiers with Denoising Assistance</title><link>http://arxiv.org/abs/2306.09192v1</link><description>Score-matching and diffusion models have emerged as state-of-the-artgenerative models for both conditional and unconditional generation.Classifier-guided diffusion models are created by training a classifier onsamples obtained from the forward-diffusion process (i.e., from data to noise).In this paper, we propose denoising-assisted (DA) classifiers wherein thediffusion classifier is trained using both noisy and denoised examples assimultaneous inputs to the model. We differentiate between denoising-assisted(DA) classifiers and noisy classifiers, which are diffusion classifiers thatare only trained on noisy examples. Our experiments on Cifar10 and Imagenetshow that DA-classifiers improve over noisy classifiers both quantitatively interms of generalization to test data and qualitatively in terms ofperceptually-aligned classifier-gradients and generative modeling metrics.Finally, we describe a semi-supervised framework for training diffusionclassifiers and our experiments, that also include positive-unlabeled settings,demonstrate improved generalization of DA-classifiers over noisy classifiers.</description><author>Chandramouli Sastry, Sri Harsha Dumpala, Sageev Oore</author><pubDate>Thu, 15 Jun 2023 16:19:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09192v1</guid></item><item><title>Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language</title><link>http://arxiv.org/abs/2212.07525v2</link><description>Current self-supervised learning algorithms are often modality-specific andrequire large amounts of computational resources. To address these issues, weincrease the training efficiency of data2vec, a learning objective thatgeneralizes across several modalities. We do not encode masked tokens, use afast convolutional decoder and amortize the effort to build teacherrepresentations. data2vec 2.0 benefits from the rich contextualized targetrepresentations introduced in data2vec which enable a fast self-supervisedlearner. Experiments on ImageNet-1K image classification show that data2vec 2.0matches the accuracy of Masked Autoencoders in 16.4x lower pre-training time,on Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6xless time, and on GLUE natural language understanding it matches a retrainedRoBERTa model in half the time. Trading some speed for accuracy results inImageNet-1K top-1 accuracy of 86.8\% with a ViT-L model trained for 150 epochs.</description><author>Alexei Baevski, Arun Babu, Wei-Ning Hsu, Michael Auli</author><pubDate>Thu, 15 Jun 2023 16:19:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07525v2</guid></item><item><title>A Search for Nonlinear Balanced Boolean Functions by Leveraging Phenotypic Properties</title><link>http://arxiv.org/abs/2306.09190v1</link><description>In this paper, we consider the problem of finding perfectly balanced Booleanfunctions with high non-linearity values. Such functions have extensiveapplications in domains such as cryptography and error-correcting codingtheory. We provide an approach for finding such functions by a local searchmethod that exploits the structure of the underlying problem. Previous attemptsin this vein typically focused on using the properties of the fitness landscapeto guide the search. We opt for a different path in which we leverage thephenotype landscape (the mapping from genotypes to phenotypes) instead. In thecontext of the underlying problem, the phenotypes are represented byWalsh-Hadamard spectra of the candidate solutions (Boolean functions). Wepropose a novel selection criterion, under which the phenotypes are compareddirectly, and test whether its use increases the convergence speed (measured bythe number of required spectra calculations) when compared to a competitivefitness function used in the literature. The results reveal promisingconvergence speed improvements for Boolean functions of sizes $N=6$ to $N=9$.</description><author>Bruno Gašperov, Marko Đurasević, Domagoj Jakobović</author><pubDate>Thu, 15 Jun 2023 16:16:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09190v1</guid></item><item><title>High-Resolution Convolutional Neural Networks on Homomorphically Encrypted Data via Sharding Ciphertexts</title><link>http://arxiv.org/abs/2306.09189v1</link><description>Recently, Deep Convolutional Neural Networks (DCNNs) including the ResNet-20architecture have been privately evaluated on encrypted, low-resolution datawith the Residue-Number-System Cheon-Kim-Kim-Song (RNS-CKKS) homomorphicencryption scheme. We extend methods for evaluating DCNNs on images with largerdimensions and many channels, beyond what can be stored in single ciphertexts.Additionally, we simplify and improve the efficiency of the recently introducedmultiplexed image format, demonstrating that homomorphic evaluation can workwith standard, row-major matrix packing and results in encrypted inference timespeedups by $4.6-6.5\times$. We also show how existing DCNN models can beregularized during the training process to further improve efficiency andaccuracy. These techniques are applied to homomorphically evaluate a DCNN withhigh accuracy on the high-resolution ImageNet dataset for the first time,achieving $80.2\%$ top-1 accuracy. We also achieve the highest reportedaccuracy of homomorphically evaluated CNNs on the CIFAR-10 dataset of $98.3\%$.</description><author>Vivian Maloney, Richard F. Obrecht, Vikram Saraph, Prathibha Rama, Kate Tallaksen</author><pubDate>Thu, 15 Jun 2023 16:16:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09189v1</guid></item><item><title>Improving Knowledge Extraction from LLMs for Robotic Task Learning through Agent Analysis</title><link>http://arxiv.org/abs/2306.06770v2</link><description>Large language models (LLMs) offer significant promise as a knowledge sourcefor robotic task learning. Prompt engineering has been shown to be effectivefor eliciting knowledge from an LLM but alone is insufficient for acquiringrelevant, situationally grounded knowledge for an embodied robotic agentlearning novel tasks. We describe a cognitive-agent approach that extends andcomplements prompt engineering, mitigating its limitations, and thus enabling arobot to acquire new task knowledge matched to its native languagecapabilities, embodiment, environment, and user preferences. The approach is toincrease the response space of LLMs and deploy general strategies, embeddedwithin the autonomous robot, to evaluate, repair, and select among candidateresponses produced by the LLM. We describe the approach and experiments thatshow how a robot, by retrieving and evaluating a breadth of responses from theLLM, can achieve &gt;75% task completion in one-shot learning without useroversight. The approach achieves 100% task completion when human oversight(such as indication of preference) is provided, while greatly reducing how muchhuman oversight is needed.</description><author>James R. Kirk, Robert E. Wray, Peter Lindes</author><pubDate>Thu, 15 Jun 2023 16:05:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06770v2</guid></item><item><title>Evolving Populations of Diverse RL Agents with MAP-Elites</title><link>http://arxiv.org/abs/2303.12803v2</link><description>Quality Diversity (QD) has emerged as a powerful alternative optimizationparadigm that aims at generating large and diverse collections of solutions,notably with its flagship algorithm MAP-ELITES (ME) which evolves solutionsthrough mutations and crossovers. While very effective for some unstructuredproblems, early ME implementations relied exclusively on random search toevolve the population of solutions, rendering them notoriouslysample-inefficient for high-dimensional problems, such as when evolving neuralnetworks. Follow-up works considered exploiting gradient information to guidethe search in order to address these shortcomings through techniques borrowedfrom either Black-Box Optimization (BBO) or Reinforcement Learning (RL). Whilemixing RL techniques with ME unlocked state-of-the-art performance for roboticscontrol problems that require a good amount of exploration, it also plaguedthese ME variants with limitations common among RL algorithms that ME was freeof, such as hyperparameter sensitivity, high stochasticity as well as traininginstability, including when the population size increases as some componentsare shared across the population in recent approaches. Furthermore, existingapproaches mixing ME with RL tend to be tied to a specific RL algorithm, whicheffectively prevents their use on problems where the corresponding RL algorithmfails. To address these shortcomings, we introduce a flexible framework thatallows the use of any RL algorithm and alleviates the aforementionedlimitations by evolving populations of agents (whose definition includehyperparameters and all learnable parameters) instead of just policies. Wedemonstrate the benefits brought about by our framework through extensivenumerical experiments on a number of robotics control problems, some of whichwith deceptive rewards, taken from the QD-RL literature.</description><author>Thomas Pierrot, Arthur Flajolet</author><pubDate>Thu, 15 Jun 2023 16:04:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12803v2</guid></item></channel></rss>