<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 05 Apr 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning</title><link>http://arxiv.org/abs/2404.03658v1</link><description>Recovering the 3D scene geometry from a single view is a fundamental yetill-posed problem in computer vision. While classical depth estimation methodsinfer only a 2.5D scene representation limited to the image plane, recentapproaches based on radiance fields reconstruct a full 3D representation.However, these methods still struggle with occluded regions since inferringgeometry without visual observation requires (i) semantic knowledge of thesurroundings, and (ii) reasoning about spatial context. We propose KYN, a novelmethod for single-view scene reconstruction that reasons about semantic andspatial context to predict each point's density. We introduce a vision-languagemodulation module to enrich point features with fine-grained semanticinformation. We aggregate point representations across the scene through alanguage-guided spatial attention mechanism to yield per-point densitypredictions aware of the 3D semantic context. We show that KYN improves 3Dshape recovery compared to predicting density for each 3D point in isolation.We achieve state-of-the-art results in scene and object reconstruction onKITTI-360, and show improved zero-shot generalization compared to prior work.Project page: https://ruili3.github.io/kyn.</description><author>Rui Li, Tobias Fischer, Mattia Segu, Marc Pollefeys, Luc Van Gool, Federico Tombari</author><pubDate>Thu, 04 Apr 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03658v1</guid></item><item><title>OW-VISCap: Open-World Video Instance Segmentation and Captioning</title><link>http://arxiv.org/abs/2404.03657v1</link><description>Open-world video instance segmentation is an important video understandingtask. Yet most methods either operate in a closed-world setting, require anadditional user-input, or use classic region-based proposals to identify neverbefore seen objects. Further, these methods only assign a one-word label todetected objects, and don't generate rich object-centric descriptions. Theyalso often suffer from highly overlapping predictions. To address these issues,we propose Open-World Video Instance Segmentation and Captioning (OW-VISCap),an approach to jointly segment, track, and caption previously seen or unseenobjects in a video. For this, we introduce open-world object queries todiscover never before seen objects without additional user-input. We generaterich and descriptive object-centric captions for each detected object via amasked attention augmented LLM input. We introduce an inter-query contrastiveloss to ensure that the object queries differ from one another. Our generalizedapproach matches or surpasses state-of-the-art on three tasks: open-world videoinstance segmentation on the BURST dataset, dense video object captioning onthe VidSTG dataset, and closed-world video instance segmentation on the OVISdataset.</description><author>Anwesa Choudhuri, Girish Chowdhary, Alexander G. Schwing</author><pubDate>Thu, 04 Apr 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03657v1</guid></item><item><title>MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation</title><link>http://arxiv.org/abs/2404.03656v1</link><description>We present MVD-Fusion: a method for single-view 3D inference via generativemodeling of multi-view-consistent RGB-D images. While recent methods pursuing3D inference advocate learning novel-view generative models, these generationsare not 3D-consistent and require a distillation process to generate a 3Doutput. We instead cast the task of 3D inference as directly generatingmutually-consistent multiple views and build on the insight that additionallyinferring depth can provide a mechanism for enforcing this consistency.Specifically, we train a denoising diffusion model to generate multi-view RGB-Dimages given a single RGB input image and leverage the (intermediate noisy)depth estimates to obtain reprojection-based conditioning to maintainmulti-view consistency. We train our model using large-scale synthetic datasetObajverse as well as the real-world CO3D dataset comprising of generic cameraviewpoints. We demonstrate that our approach can yield more accurate synthesiscompared to recent state-of-the-art, including distillation-based 3D inferenceand prior multi-view generation methods. We also evaluate the geometry inducedby our multi-view depth prediction and find that it yields a more accuraterepresentation than other direct 3D inference approaches.</description><author>Hanzhe Hu, Zhizhuo Zhou, Varun Jampani, Shubham Tulsiani</author><pubDate>Thu, 04 Apr 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03656v1</guid></item><item><title>RaFE: Generative Radiance Fields Restoration</title><link>http://arxiv.org/abs/2404.03654v1</link><description>NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novelview synthesis and 3D reconstruction, but its performance is sensitive to inputimage quality, which struggles to achieve high-fidelity rendering when providedwith low-quality sparse input viewpoints. Previous methods for NeRF restorationare tailored for specific degradation type, ignoring the generality ofrestoration. To overcome this limitation, we propose a generic radiance fieldsrestoration pipeline, named RaFE, which applies to various types ofdegradations, such as low resolution, blurriness, noise, compression artifacts,or their combinations. Our approach leverages the success of off-the-shelf 2Drestoration methods to recover the multi-view images individually. Instead ofreconstructing a blurred NeRF by averaging inconsistencies, we introduce anovel approach using Generative Adversarial Networks (GANs) for NeRF generationto better accommodate the geometric and appearance inconsistencies present inthe multi-view images. Specifically, we adopt a two-level tri-planearchitecture, where the coarse level remains fixed to represent the low-qualityNeRF, and a fine-level residual tri-plane to be added to the coarse level ismodeled as a distribution with GAN to capture potential variations inrestoration. We validate RaFE on both synthetic and real cases for variousrestoration tasks, demonstrating superior performance in both quantitative andqualitative evaluations, surpassing other 3D restoration methods specific tosingle task. Please see our project websitehttps://zkaiwu.github.io/RaFE-Project/.</description><author>Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</author><pubDate>Thu, 04 Apr 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03654v1</guid></item><item><title>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching</title><link>http://arxiv.org/abs/2404.03653v1</link><description>Diffusion models have demonstrated great success in the field oftext-to-image generation. However, alleviating the misalignment between thetext prompts and images is still challenging. The root reason behind themisalignment has not been extensively investigated. We observe that themisalignment is caused by inadequate token attention activation. We furtherattribute this phenomenon to the diffusion model's insufficient conditionutilization, which is caused by its training paradigm. To address the issue, wepropose CoMat, an end-to-end diffusion model fine-tuning strategy with animage-to-text concept matching mechanism. We leverage an image captioning modelto measure image-to-text alignment and guide the diffusion model to revisitignored tokens. A novel attribute concentration module is also proposed toaddress the attribute binding problem. Without any image or human preferencedata, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.Extensive experiments show that CoMat-SDXL significantly outperforms thebaseline model SDXL in two text-to-image alignment benchmarks and achievesstart-of-the-art performance.</description><author>Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, Hongsheng Li</author><pubDate>Thu, 04 Apr 2024 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03653v1</guid></item><item><title>The More You See in 2D, the More You Perceive in 3D</title><link>http://arxiv.org/abs/2404.03652v1</link><description>Humans can infer 3D structure from 2D images of an object based on pastexperience and improve their 3D understanding as they see more images. Inspiredby this behavior, we introduce SAP3D, a system for 3D reconstruction and novelview synthesis from an arbitrary number of unposed images. Given a few unposedimages of an object, we adapt a pre-trained view-conditioned diffusion modeltogether with the camera poses of the images via test-time fine-tuning. Theadapted diffusion model and the obtained camera poses are then utilized asinstance-specific priors for 3D reconstruction and novel view synthesis. Weshow that as the number of input images increases, the performance of ourapproach improves, bridging the gap between optimization-based prior-less 3Dreconstruction methods and single-image-to-3D diffusion-based methods. Wedemonstrate our system on real images as well as standard synthetic benchmarks.Our ablation studies confirm that this adaption behavior is key for moreaccurate 3D understanding.</description><author>Xinyang Han, Zelin Gao, Angjoo Kanazawa, Shubham Goel, Yossi Gandelsman</author><pubDate>Thu, 04 Apr 2024 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03652v1</guid></item><item><title>OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views</title><link>http://arxiv.org/abs/2404.03650v1</link><description>Large visual-language models (VLMs), like CLIP, enable open-set imagesegmentation to segment arbitrary concepts from an image in a zero-shot manner.This goes beyond the traditional closed-set assumption, i.e., where models canonly segment classes from a pre-defined training set. More recently, firstworks on open-set segmentation in 3D scenes have appeared in the literature.These methods are heavily influenced by closed-set 3D convolutional approachesthat process point clouds or polygon meshes. However, these 3D scenerepresentations do not align well with the image-based nature of thevisual-language models. Indeed, point cloud and 3D meshes typically have alower resolution than images and the reconstructed 3D scene geometry might notproject well to the underlying 2D image sequences used to compute pixel-alignedCLIP features. To address these challenges, we propose OpenNeRF which naturallyoperates on posed images and directly encodes the VLM features within the NeRF.This is similar in spirit to LERF, however our work shows that using pixel-wiseVLM features (instead of global CLIP features) results in an overall lesscomplex architecture without the need for additional DINO regularization. OurOpenNeRF further leverages NeRF's ability to render novel views and extractopen-set VLM features from areas that are not well observed in the initialposed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRFoutperforms recent open-vocabulary methods such as LERF and OpenScene by atleast +4.9 mIoU.</description><author>Francis Engelmann, Fabian Manhardt, Michael Niemeyer, Keisuke Tateno, Marc Pollefeys, Federico Tombari</author><pubDate>Thu, 04 Apr 2024 18:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03650v1</guid></item><item><title>AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent</title><link>http://arxiv.org/abs/2404.03648v1</link><description>Large language models (LLMs) have fueled many intelligent agent tasks, suchas web navigation -- but most existing agents perform far from satisfying inreal-world webpages due to three factors: (1) the versatility of actions onwebpages, (2) HTML text exceeding model processing capacity, and (3) thecomplexity of decision-making due to the open-domain nature of web. In light ofthe challenge, we develop AutoWebGLM, a GPT-4-outperforming automated webnavigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns,we design an HTML simplification algorithm to represent webpages, preservingvital information succinctly. We employ a hybrid human-AI method to build webbrowsing data for curriculum training. Then, we bootstrap the model byreinforcement learning and rejection sampling to further facilitate webpagecomprehension, browser operations, and efficient task decomposition by itself.For testing, we establish a bilingual benchmark -- AutoWebBench -- forreal-world web browsing tasks. We evaluate AutoWebGLM across diverse webnavigation benchmarks, revealing its improvements but also underlyingchallenges to tackle real environments. Related code, model, and data will bereleased at \url{https://github.com/THUDM/AutoWebGLM}.</description><author>Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, Jie Tang</author><pubDate>Thu, 04 Apr 2024 18:58:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03648v1</guid></item><item><title>Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra</title><link>http://arxiv.org/abs/2404.03647v1</link><description>In this paper, we explore the capabilities of state-of-the-art large languagemodels (LLMs) such as GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra in solvingundergraduate-level control problems. Controls provides an interesting casestudy for LLM reasoning due to its combination of mathematical theory andengineering design. We introduce ControlBench, a benchmark dataset tailored toreflect the breadth, depth, and complexity of classical control design. We usethis dataset to study and evaluate the problem-solving abilities of these LLMsin the context of control engineering. We present evaluations conducted by apanel of human experts, providing insights into the accuracy, reasoning, andexplanatory prowess of LLMs in control engineering. Our analysis reveals thestrengths and limitations of each LLM in the context of classical control, andour results imply that Claude 3 Opus has become the state-of-the-art LLM forsolving undergraduate control problems. Our study serves as an initial steptowards the broader goal of employing artificial general intelligence incontrol engineering.</description><author>Darioush Kevian, Usman Syed, Xingang Guo, Aaron Havens, Geir Dullerud, Peter Seiler, Lianhui Qin, Bin Hu</author><pubDate>Thu, 04 Apr 2024 18:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03647v1</guid></item><item><title>Locating and Editing Factual Associations in Mamba</title><link>http://arxiv.org/abs/2404.03646v1</link><description>We investigate the mechanisms of factual recall in the Mamba state spacemodel. Our work is inspired by previous findings in autoregressive transformerlanguage models suggesting that their knowledge recall is localized toparticular modules at specific token locations; we therefore ask whetherfactual recall in Mamba can be similarly localized. To investigate this, weconduct four lines of experiments on Mamba. First, we apply causal tracing orinterchange interventions to localize key components inside Mamba that areresponsible for recalling facts, revealing that specific components withinmiddle layers show strong causal effects at the last token of the subject,while the causal effect of intervening on later layers is most pronounced atthe last token of the prompt, matching previous findings on autoregressivetransformers. Second, we show that rank-one model editing methods cansuccessfully insert facts at specific locations, again resembling findings ontransformer models. Third, we examine the linearity of Mamba's representationsof factual relations. Finally we adapt attention-knockout techniques to Mambato dissect information flow during factual recall. We compare Mamba directly toa similar-sized transformer and conclude that despite significant differencesin architectural approach, when it comes to factual recall, the twoarchitectures share many similarities.</description><author>Arnab Sen Sharma, David Atkinson, David Bau</author><pubDate>Thu, 04 Apr 2024 18:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03646v1</guid></item><item><title>Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation</title><link>http://arxiv.org/abs/2404.03645v1</link><description>Referring video segmentation relies on natural language expressions toidentify and segment objects, often emphasizing motion clues. Previous workstreat a sentence as a whole and directly perform identification at thevideo-level, mixing up static image-level cues with temporal motion cues.However, image-level features cannot well comprehend motion cues in sentences,and static cues are not crucial for temporal perception. In fact, static cuescan sometimes interfere with temporal perception by overshadowing motion cues.In this work, we propose to decouple video-level referring expressionunderstanding into static and motion perception, with a specific emphasis onenhancing temporal comprehension. Firstly, we introduce anexpression-decoupling module to make static cues and motion cues perform theirdistinct role, alleviating the issue of sentence embeddings overlooking motioncues. Secondly, we propose a hierarchical motion perception module to capturetemporal information effectively across varying timescales. Furthermore, weemploy contrastive learning to distinguish the motions of visually similarobjects. These contributions yield state-of-the-art performance across fivedatasets, including a remarkable $\textbf{9.2%}$ $\mathcal{J\&amp;F}$ improvementon the challenging $\textbf{MeViS}$ dataset. Code is available athttps://github.com/heshuting555/DsHmp.</description><author>Shuting He, Henghui Ding</author><pubDate>Thu, 04 Apr 2024 18:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03645v1</guid></item><item><title>DiffBody: Human Body Restoration by Imagining with Generative Diffusion Prior</title><link>http://arxiv.org/abs/2404.03642v1</link><description>Human body restoration plays a vital role in various applications related tothe human body. Despite recent advances in general image restoration usinggenerative models, their performance in human body restoration remainsmediocre, often resulting in foreground and background blending, over-smoothingsurface textures, missing accessories, and distorted limbs. Addressing thesechallenges, we propose a novel approach by constructing a human body-awarediffusion model that leverages domain-specific knowledge to enhanceperformance. Specifically, we employ a pretrained body attention module toguide the diffusion model's focus on the foreground, addressing issues causedby blending between the subject and background. We also demonstrate the valueof revisiting the language modality of the diffusion model in restoration tasksby seamlessly incorporating text prompt to improve the quality of surfacetexture and additional clothing and accessories details. Additionally, weintroduce a diffusion sampler tailored for fine-grained human body parts,utilizing local semantic information to rectify limb distortions. Lastly, wecollect a comprehensive dataset for benchmarking and advancing the field ofhuman body restoration. Extensive experimental validation showcases thesuperiority of our approach, both quantitatively and qualitatively, overexisting methods.</description><author>Yiming Zhang, Zhe Wang, Xinjie Li, Yunchen Yuan, Chengsong Zhang, Xiao Sun, Zhihang Zhong, Jian Wang</author><pubDate>Thu, 04 Apr 2024 18:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03642v1</guid></item><item><title>$CrowdDiff$: Multi-hypothesis Crowd Density Estimation using Diffusion Models</title><link>http://arxiv.org/abs/2303.12790v3</link><description>Crowd counting is a fundamental problem in crowd analysis which is typicallyaccomplished by estimating a crowd density map and summing over the densityvalues. However, this approach suffers from background noise accumulation andloss of density due to the use of broad Gaussian kernels to create the groundtruth density maps. This issue can be overcome by narrowing the Gaussiankernel. However, existing approaches perform poorly when trained with groundtruth density maps with broad kernels. To deal with this limitation, we proposeusing conditional diffusion models to predict density maps, as diffusion modelsshow high fidelity to training data during generation. With that, we present$CrowdDiff$ that generates the crowd density map as a reverse diffusionprocess. Furthermore, as the intermediate time steps of the diffusion processare noisy, we incorporate a regression branch for direct crowd estimation onlyduring training to improve the feature learning. In addition, owing to thestochastic nature of the diffusion model, we introduce producing multipledensity maps to improve the counting performance contrary to the existing crowdcounting pipelines. We conduct extensive experiments on publicly availabledatasets to validate the effectiveness of our method. $CrowdDiff$ outperformsexisting state-of-the-art crowd counting methods on several public crowdanalysis benchmarks with significant improvements.</description><author>Yasiru Ranasinghe, Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, Vishal M. Patel</author><pubDate>Thu, 04 Apr 2024 18:55:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12790v3</guid></item><item><title>WorDepth: Variational Language Prior for Monocular Depth Estimation</title><link>http://arxiv.org/abs/2404.03635v1</link><description>Three-dimensional (3D) reconstruction from a single image is an ill-posedproblem with inherent ambiguities, i.e. scale. Predicting a 3D scene from textdescription(s) is similarly ill-posed, i.e. spatial arrangements of objectsdescribed. We investigate the question of whether two inherently ambiguousmodalities can be used in conjunction to produce metric-scaled reconstructions.To test this, we focus on monocular depth estimation, the problem of predictinga dense depth map from a single image, but with an additional text captiondescribing the scene. To this end, we begin by encoding the text caption as amean and standard deviation; using a variational framework, we learn thedistribution of the plausible metric reconstructions of 3D scenes correspondingto the text captions as a prior. To "select" a specific reconstruction or depthmap, we encode the given image through a conditional sampler that samples fromthe latent space of the variational text encoder, which is then decoded to theoutput depth map. Our approach is trained alternatingly between the text andimage branches: in one optimization step, we predict the mean and standarddeviation from the text description and sample from a standard Gaussian, and inthe other, we sample using a (image) conditional sampler. Once trained, wedirectly predict depth from the encoded text using the conditional sampler. Wedemonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, wherewe show that language can consistently improve performance in both.</description><author>Ziyao Zeng, Daniel Wang, Fengyu Yang, Hyoungseob Park, Yangchao Wu, Stefano Soatto, Byung-Woo Hong, Dong Lao, Alex Wong</author><pubDate>Thu, 04 Apr 2024 18:54:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03635v1</guid></item><item><title>PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments</title><link>http://arxiv.org/abs/2404.03634v1</link><description>Robotic manipulation of ungraspable objects with two-finger grippers presentssignificant challenges due to the paucity of graspable features, whiletraditional pre-grasping techniques, which rely on repositioning objects andleveraging external aids like table edges, lack the adaptability across objectcategories and scenes. Addressing this, we introduce PreAfford, a novelpre-grasping planning framework that utilizes a point-level affordancerepresentation and a relay training approach to enhance adaptability across abroad range of environments and object types, including those previouslyunseen. Demonstrated on the ShapeNet-v2 dataset, PreAfford significantlyimproves grasping success rates by 69% and validates its practicality throughreal-world experiments. This work offers a robust and adaptable solution formanipulating ungraspable objects.</description><author>Kairui Ding, Boyuan Chen, Ruihai Wu, Yuyang Li, Zongzheng Zhang, Huan-ang Gao, Siqi Li, Yixin Zhu, Guyue Zhou, Hao Dong, Hao Zhao</author><pubDate>Thu, 04 Apr 2024 18:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03634v1</guid></item><item><title>Reference-Based 3D-Aware Image Editing with Triplane</title><link>http://arxiv.org/abs/2404.03632v1</link><description>Generative Adversarial Networks (GANs) have emerged as powerful tools notonly for high-quality image generation but also for real image editing throughmanipulation of their interpretable latent spaces. Recent advancements in GANsinclude the development of 3D-aware models such as EG3D, characterized byefficient triplane-based architectures enabling the reconstruction of 3Dgeometry from single images. However, scant attention has been devoted toproviding an integrated framework for high-quality reference-based 3D-awareimage editing within this domain. This study addresses this gap by exploringand demonstrating the effectiveness of EG3D's triplane space for achievingadvanced reference-based edits, presenting a unique perspective on 3D-awareimage editing through our novel pipeline. Our approach integrates the encodingof triplane features, spatial disentanglement and automatic localization offeatures in the triplane domain, and fusion learning for desired image editing.Moreover, our framework demonstrates versatility across domains, extending itseffectiveness to animal face edits and partial stylization of cartoonportraits. The method shows significant improvements over relevant 3D-awarelatent editing and 2D reference-based editing methods, both qualitatively andquantitatively. Project page: https://three-bee.github.io/triplane_edit</description><author>Bahri Batuhan Bilecen, Yigit Yalin, Ning Yu, Aysegul Dundar</author><pubDate>Thu, 04 Apr 2024 18:53:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03632v1</guid></item><item><title>Robust Concept Erasure Using Task Vectors</title><link>http://arxiv.org/abs/2404.03631v1</link><description>With the rapid growth of text-to-image models, a variety of techniques havebeen suggested to prevent undesirable image generations. Yet, these methodsoften only protect against specific user prompts and have been shown to allowunsafe generations with other inputs. Here we focus on unconditionally erasinga concept from a text-to-image model rather than conditioning the erasure onthe user's prompt. We first show that compared to input-dependent erasuremethods, concept erasure that uses Task Vectors (TV) is more robust tounexpected user inputs, not seen during training. However, TV-based erasure canalso affect the core performance of the edited model, particularly when therequired edit strength is unknown. To this end, we propose a method calledDiverse Inversion, which we use to estimate the required strength of the TVedit. Diverse Inversion finds within the model input space a large set of wordembeddings, each of which induces the generation of the target concept. We findthat encouraging diversity in the set makes our estimation more robust tounexpected prompts. Finally, we show that Diverse Inversion enables us to applya TV edit only to a subset of the model weights, enhancing the erasurecapabilities while better maintaining the core functionality of the model.</description><author>Minh Pham, Kelly O. Marshall, Chinmay Hegde, Niv Cohen</author><pubDate>Thu, 04 Apr 2024 18:52:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03631v1</guid></item><item><title>Training LLMs over Neurally Compressed Text</title><link>http://arxiv.org/abs/2404.03626v1</link><description>In this paper, we explore the idea of training large language models (LLMs)over highly compressed text. While standard subword tokenizers compress text bya small factor, neural text compressors can achieve much higher rates ofcompression. If it were possible to train LLMs directly over neurallycompressed text, this would confer advantages in training and servingefficiency, as well as easier handling of long text spans. The main obstacle tothis goal is that strong compression tends to produce opaque outputs that arenot well-suited for learning. In particular, we find that text na\"ivelycompressed via Arithmetic Coding is not readily learnable by LLMs. To overcomethis, we propose Equal-Info Windows, a novel compression technique whereby textis segmented into blocks that each compress to the same bit length. Using thismethod, we demonstrate effective learning over neurally compressed text thatimproves with scale, and outperforms byte-level baselines by a wide margin onperplexity and inference speed benchmarks. While our method delivers worseperplexity than subword tokenizers for models trained with the same parametercount, it has the benefit of shorter sequence lengths. Shorter sequence lengthsrequire fewer autoregressive generation steps, and reduce latency. Finally, weprovide extensive analysis of the properties that contribute to learnability,and offer concrete suggestions for how to further improve the performance ofhigh-compression tokenizers.</description><author>Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant</author><pubDate>Thu, 04 Apr 2024 18:48:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03626v1</guid></item><item><title>Standardizing Knowledge Engineering Practices with a Reference Architecture</title><link>http://arxiv.org/abs/2404.03624v1</link><description>Knowledge engineering is the process of creating and maintainingknowledge-producing systems. Throughout the history of computer science and AI,knowledge engineering workflows have been widely used given the importance ofhigh-quality knowledge for reliable intelligent agents. Meanwhile, the scope ofknowledge engineering, as apparent from its target tasks and use cases, hasbeen shifting, together with its paradigms such as expert systems, semanticweb, and language modeling. The intended use cases and supported userrequirements between these paradigms have not been analyzed globally, as newparadigms often satisfy prior pain points while possibly introducing new ones.The recent abstraction of systemic patterns into a boxology provides an openingfor aligning the requirements and use cases of knowledge engineering with thesystems, components, and software that can satisfy them best. This paperproposes a vision of harmonizing the best practices in the field of knowledgeengineering by leveraging the software engineering methodology of creatingreference architectures. We describe how a reference architecture can beiteratively designed and implemented to associate user needs with recurringsystemic patterns, building on top of existing knowledge engineering workflowsand boxologies. We provide a six-step roadmap that can enable the developmentof such an architecture, providing an initial design and outcome of thedefinition of architectural scope, selection of information sources, andanalysis. We expect that following through on this vision will lead towell-grounded reference architectures for knowledge engineering, will advancethe ongoing initiatives of organizing the neurosymbolic knowledge engineeringspace, and will build new links to the software architectures and data sciencecommunities.</description><author>Bradley P. Allen, Filip Ilievski</author><pubDate>Thu, 04 Apr 2024 18:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03624v1</guid></item><item><title>Unveiling LLMs: The Evolution of Latent Representations in a Temporal Knowledge Graph</title><link>http://arxiv.org/abs/2404.03623v1</link><description>Large Language Models (LLMs) demonstrate an impressive capacity to recall avast range of common factual knowledge information. However, unravelling theunderlying reasoning of LLMs and explaining their internal mechanisms ofexploiting this factual knowledge remain active areas of investigation. Ourwork analyzes the factual knowledge encoded in the latent representation ofLLMs when prompted to assess the truthfulness of factual claims. We propose anend-to-end framework that jointly decodes the factual knowledge embedded in thelatent space of LLMs from a vector space to a set of ground predicates andrepresents its evolution across the layers using a temporal knowledge graph.Our framework relies on the technique of activation patching which intervenesin the inference computation of a model by dynamically altering its latentrepresentations. Consequently, we neither rely on external models nor trainingprocesses. We showcase our framework with local and global interpretabilityanalyses using two claim verification datasets: FEVER and CLIMATE-FEVER. Thelocal interpretability analysis exposes different latent errors fromrepresentation to multi-hop reasoning errors. On the other hand, the globalanalysis uncovered patterns in the underlying evolution of the model's factualknowledge (e.g., store-and-seek factual information). By enabling graph-basedanalyses of the latent representations, this work represents a step towards themechanistic interpretability of LLMs.</description><author>Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini</author><pubDate>Thu, 04 Apr 2024 18:45:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03623v1</guid></item><item><title>KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</title><link>http://arxiv.org/abs/2401.18079v3</link><description>LLMs are seeing growing use for applications such as document analysis andsummarization which require large context windows, and with these large contextwindows KV cache activations surface as the dominant contributor to memoryconsumption during inference. Quantization is a promising approach forcompressing KV cache activations; however, existing solutions fail to representactivations accurately in ultra-low precisions, such as sub-4-bit. In thiswork, we present KVQuant, which addresses this problem by incorporating novelmethods for quantizing cached KV activations, including: (i) Per-Channel KeyQuantization, where we adjust the dimension along which we quantize the Keyactivations to better match the distribution; (ii) Pre-RoPE Key Quantization,where we quantize Key activations before the rotary positional embedding tomitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization,where we derive per-layer sensitivity-weighted non-uniform datatypes thatbetter represent the distributions; (iv) Per-Vector Dense-and-SparseQuantization, where we isolate outliers separately for each vector to minimizeskews in quantization ranges; and (v) Q-Norm, where we normalize quantizationcentroids in order to mitigate distribution shift, providing additionalbenefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2,and Mistral models, we achieve $&lt;0.1$ perplexity degradation with 3-bitquantization on both Wikitext-2 and C4, outperforming existing approaches. Ourmethod enables serving the LLaMA-7B model with a context length of up to 1million on a single A100-80GB GPU and up to 10 million on an 8-GPU system.</description><author>Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir Gholami</author><pubDate>Thu, 04 Apr 2024 18:45:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18079v3</guid></item><item><title>Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2404.03622v1</link><description>Large language models (LLMs) have exhibited impressive performance inlanguage comprehension and various reasoning tasks. However, their abilities inspatial reasoning, a crucial aspect of human cognition, remain relativelyunexplored. Human possess a remarkable ability to create mental images ofunseen objects and actions through a process known as \textbf{the Mind's Eye},enabling the imagination of the unseen world. Inspired by this cognitivecapacity, we propose Visualization-of-Thought (\textbf{VoT}) prompting. VoTaims to elicit spatial reasoning of LLMs by visualizing their reasoning traces,thereby guiding subsequent reasoning steps. We employed VoT for multi-hopspatial reasoning tasks, including natural language navigation, visualnavigation, and visual tiling in 2D grid worlds. Experimental resultsdemonstrated that VoT significantly enhances the spatial reasoning abilities ofLLMs. Notably, VoT outperformed existing multimodal large language models(MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the abilityto generate \textit{mental images} to facilitate spatial reasoning resemblesthe mind's eye process, suggesting its potential viability in MLLMs.</description><author>Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, Furu Wei</author><pubDate>Thu, 04 Apr 2024 18:45:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03622v1</guid></item><item><title>LCM-Lookahead for Encoder-based Text-to-Image Personalization</title><link>http://arxiv.org/abs/2404.03620v1</link><description>Recent advancements in diffusion models have introduced fast sampling methodsthat can effectively produce high-quality images in just one or a few denoisingsteps. Interestingly, when these are distilled from existing diffusion models,they often maintain alignment with the original model, retaining similaroutputs for similar prompts and seeds. These properties present opportunitiesto leverage fast sampling methods as a shortcut-mechanism, using them to createa preview of denoised outputs through which we can backpropagate image-spacelosses. In this work, we explore the potential of using suchshortcut-mechanisms to guide the personalization of text-to-image models tospecific facial identities. We focus on encoder-based personalizationapproaches, and demonstrate that by tuning them with a lookahead identity loss,we can achieve higher identity fidelity, without sacrificing layout diversityor prompt alignment. We further explore the use of attention sharing mechanismsand consistent data generation for the task of personalization, and find thatencoder training can benefit from both.</description><author>Rinon Gal, Or Lichter, Elad Richardson, Or Patashnik, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or</author><pubDate>Thu, 04 Apr 2024 18:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03620v1</guid></item><item><title>Ziya2: Data-centric Learning is All LLMs Need</title><link>http://arxiv.org/abs/2311.03301v2</link><description>Various large language models (LLMs) have been proposed in recent years,including closed- and open-source ones, continually setting new records onmultiple benchmarks. However, the development of LLMs still faces severalissues, such as high cost of training models from scratch, and continualpre-training leading to catastrophic forgetting, etc. Although many such issuesare addressed along the line of research on LLMs, an important yet practicallimitation is that many studies overly pursue enlarging model sizes withoutcomprehensively analyzing and optimizing the use of pre-training data in theirlearning process, as well as appropriate organization and leveraging of suchdata in training LLMs under cost-effective settings. In this work, we proposeZiya2, a model with 13 billion parameters adopting LLaMA2 as the foundationmodel, and further pre-trained on 700 billion tokens, where we focus onpre-training techniques and use data-centric optimization to enhance thelearning process of Ziya2 on different stages. We define three data attributesand firstly establish data-centric scaling laws to illustrate how differentdata impacts LLMs. Experiments show that Ziya2 significantly outperforms othermodels in multiple benchmarks especially with promising results compared torepresentative open-source ones. Ziya2 (Base) is released athttps://huggingface.co/IDEA-CCNL/Ziya2-13B-Base andhttps://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary.</description><author>Ruyi Gan, Ziwei Wu, Renliang Sun, Junyu Lu, Xiaojun Wu, Dixiang Zhang, Kunhao Pan, Junqing He, Yuanhe Tian, Ping Yang, Qi Yang, Hao Wang, Jiaxing Zhang, Yan Song</author><pubDate>Thu, 04 Apr 2024 18:41:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03301v2</guid></item><item><title>DeViDe: Faceted medical knowledge for improved medical vision-language pre-training</title><link>http://arxiv.org/abs/2404.03618v1</link><description>Vision-language pre-training for chest X-rays has made significant strides,primarily by utilizing paired radiographs and radiology reports. However,existing approaches often face challenges in encoding medical knowledgeeffectively. While radiology reports provide insights into the current diseasemanifestation, medical definitions (as used by contemporary methods) tend to beoverly abstract, creating a gap in knowledge. To address this, we proposeDeViDe, a novel transformer-based method that leverages radiographicdescriptions from the open web. These descriptions outline general visualcharacteristics of diseases in radiographs, and when combined with abstractdefinitions and radiology reports, provide a holistic snapshot of knowledge.DeViDe incorporates three key features for knowledge-augmented vision languagealignment: First, a large-language model-based augmentation is employed tohomogenise medical knowledge from diverse sources. Second, this knowledge isaligned with image information at various levels of granularity. Third, a novelprojection layer is proposed to handle the complexity of aligning each imagewith multiple descriptions arising in a multi-label setting. In zero-shotsettings, DeViDe performs comparably to fully supervised models on externaldatasets and achieves state-of-the-art results on three large-scale datasets.Additionally, fine-tuning DeViDe on four downstream tasks and six segmentationtasks showcases its superior performance across data from diversedistributions.</description><author>Haozhe Luo, Ziyu Zhou, Corentin Royer, Anjany Sekuboyina, Bjoern Menze</author><pubDate>Thu, 04 Apr 2024 18:40:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03618v1</guid></item><item><title>On the Efficiency of Convolutional Neural Networks</title><link>http://arxiv.org/abs/2404.03617v1</link><description>Since the breakthrough performance of AlexNet in 2012, convolutional neuralnetworks (convnets) have grown into extremely powerful vision models. Deeplearning researchers have used convnets to produce accurate results that wereunachievable a decade ago. Yet computer scientists make computationalefficiency their primary objective. Accuracy with exorbitant cost is notacceptable; an algorithm must also minimize its computational requirements.Confronted with the daunting computation that convnets use, deep learningresearchers also became interested in efficiency. Researchers appliedtremendous effort to find the convnet architectures that have the greatestefficiency. However, skepticism grew among researchers and engineers alikeabout the relevance of arithmetic complexity. Contrary to the prevailing viewthat latency and arithmetic complexity are irreconcilable, a simple formularelates both through computational efficiency. This insight enabled us toco-optimize the separate factors that determine latency. We observed that thedegenerate conv2d layers that produce the best accuracy-complexity trade-offalso have low operational intensity. Therefore, kernels that implement theselayers use significant memory resources. We solved this optimization problemwith block-fusion kernels that implement all layers of a residual block,thereby creating temporal locality, avoiding communication, and reducingworkspace size. Our ConvFirst model with block-fusion kernels ran approximatelyfour times as fast as the ConvNeXt baseline with PyTorch Inductor, at equalaccuracy on the ImageNet-1K classification task. Our unified approach toconvnet efficiency envisions a new era of models and kernels that achievegreater accuracy at lower cost.</description><author>Andrew Lavin</author><pubDate>Thu, 04 Apr 2024 18:39:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03617v1</guid></item><item><title>Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2404.03613v1</link><description>As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel viewsynthesis, it is a natural extension to deform a canonical 3DGS to multipleframes. However, previous works fail to accurately reconstruct dynamic scenes,especially 1) static parts moving along nearby dynamic parts, and 2) somedynamic areas are blurry. We attribute the failure to the wrong design of thedeformation field, which is built as a coordinate-based function. This approachis problematic because 3DGS is a mixture of multiple fields centered at theGaussians, not just a single coordinate-based framework. To resolve thisproblem, we define the deformation as a function of per-Gaussian embeddings andtemporal embeddings. Moreover, we decompose deformations as coarse and finedeformations to model slow and fast movements, respectively. Also, we introducean efficient training strategy for faster convergence and higher quality.Project page: https://jeongminb.github.io/e-d3dgs/</description><author>Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh</author><pubDate>Thu, 04 Apr 2024 18:34:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03613v1</guid></item><item><title>InsectMamba: Insect Pest Classification with State Space Model</title><link>http://arxiv.org/abs/2404.03611v1</link><description>The classification of insect pests is a critical task in agriculturaltechnology, vital for ensuring food security and environmental sustainability.However, the complexity of pest identification, due to factors like highcamouflage and species diversity, poses significant obstacles. Existing methodsstruggle with the fine-grained feature extraction needed to distinguish betweenclosely related pest species. Although recent advancements have utilizedmodified network structures and combined deep learning approaches to improveaccuracy, challenges persist due to the similarity between pests and theirsurroundings. To address this problem, we introduce InsectMamba, a novelapproach that integrates State Space Models (SSMs), Convolutional NeuralNetworks (CNNs), Multi-Head Self-Attention mechanism (MSA), and MultilayerPerceptrons (MLPs) within Mix-SSM blocks. This integration facilitates theextraction of comprehensive visual features by leveraging the strengths of eachencoding strategy. A selective module is also proposed to adaptively aggregatethese features, enhancing the model's ability to discern pest characteristics.InsectMamba was evaluated against strong competitors across five insect pestclassification datasets. The results demonstrate its superior performance andverify the significance of each model component by an ablation study.</description><author>Qianning Wang, Chenglin Wang, Zhixin Lai, Yucheng Zhou</author><pubDate>Thu, 04 Apr 2024 18:34:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03611v1</guid></item><item><title>Sailor: Open Language Models for South-East Asia</title><link>http://arxiv.org/abs/2404.03608v1</link><description>We present Sailor, a family of open language models ranging from 0.5B to 7Bparameters, tailored for South-East Asian (SEA) languages. These models arecontinually pre-trained from Qwen1.5, a great language model for multilingualuse cases. From Qwen1.5, Sailor models accept 200B to 400B tokens, primarilycovering the languages of English, Chinese, Vietnamese, Thai, Indonesian,Malay, and Lao. The training leverages several techniques, including BPEdropout for improving the model robustness, aggressive data cleaning anddeduplication, and small proxy models to optimize data mixture. Experimentalresults on four typical tasks indicate that Sailor models demonstrate strongperformance across different benchmarks, including commonsense reasoning,question answering, reading comprehension and examination. Embracing theopen-source spirit, we share our insights through this report to spark a widerinterest in developing large language models for multilingual use cases.</description><author>Longxu Dou, Qian Liu, Guangtao Zeng, Jia Guo, Jiahui Zhou, Wei Lu, Min Lin</author><pubDate>Thu, 04 Apr 2024 18:31:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03608v1</guid></item><item><title>Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens</title><link>http://arxiv.org/abs/2401.17377v3</link><description>Are $n$-gram language models still relevant in this era of neural largelanguage models (LLMs)? Our answer is yes, and we showcase their values in bothtext analysis and improving neural LLMs. This was done by modernizing $n$-gramLMs in two aspects. First, we train them at the same data scale as neural LLMs-- 5 trillion tokens. This is the largest $n$-gram LM ever built. Second,existing $n$-gram LMs use small $n$ which hinders their performance; we insteadallow $n$ to be arbitrarily large, by introducing a new $\infty$-gram LM withbackoff. Instead of pre-computing $n$-gram count tables (which would be veryexpensive), we develop an engine named infini-gram -- powered by suffix arrays-- that can compute $\infty$-gram (as well as $n$-gram with arbitrary $n$)probabilities with millisecond-level latency. The $\infty$-gram framework andinfini-gram engine enable us to conduct many novel and interesting analyses ofhuman-written and machine-generated text: we find that the $\infty$-gram LM hasfairly high accuracy for next-token prediction (47%), and can complement neuralLLMs to greatly reduce their perplexity. When analyzing machine-generated text,we also observe irregularities in the machine--$\infty$-gram agreement levelwith respect to the suffix length, which indicates deficiencies in neural LLMpretraining and the positional embeddings of Transformers.</description><author>Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, Hannaneh Hajishirzi</author><pubDate>Thu, 04 Apr 2024 18:28:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17377v3</guid></item><item><title>Analyzing Musical Characteristics of National Anthems in Relation to Global Indices</title><link>http://arxiv.org/abs/2404.03606v1</link><description>Music plays a huge part in shaping peoples' psychology and behavioralpatterns. This paper investigates the connection between national anthems anddifferent global indices with computational music analysis and statisticalcorrelation analysis. We analyze national anthem musical data to determinewhether certain musical characteristics are associated with peace, happiness,suicide rate, crime rate, etc. To achieve this, we collect national anthemsfrom 169 countries and use computational music analysis techniques to extractpitch, tempo, beat, and other pertinent audio features. We then compare thesemusical characteristics with data on different global indices to ascertainwhether a significant correlation exists. Our findings indicate that there maybe a correlation between the musical characteristics of national anthems andthe indices we investigated. The implications of our findings for musicpsychology and policymakers interested in promoting social well-being arediscussed. This paper emphasizes the potential of musical data analysis insocial research and offers a novel perspective on the relationship betweenmusic and social indices. The source code and data are made open-access forreproducibility and future research endeavors. It can be accessed athttp://bit.ly/na_code.</description><author>S M Rakib Hasan, Aakar Dhakal, Ms. Ayesha Siddiqua, Mohammad Mominur Rahman, Md Maidul Islam, Mohammed Arfat Raihan Chowdhury, S M Masfequier Rahman Swapno, SM Nuruzzaman Nobel</author><pubDate>Thu, 04 Apr 2024 18:25:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03606v1</guid></item><item><title>Mitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization</title><link>http://arxiv.org/abs/2404.03605v1</link><description>We consider the problem of accurate quantization for language models, whereboth the weights and activations are uniformly quantized to 4 bits perparameter, the lowest bitwidth format natively supported by GPU hardware. Inthis context, the key challenge is activation quantization: it is known thatlanguage models contain outlier channels whose values on average are orders ofmagnitude higher than than other channels, which prevents accurate low-bitwidthquantization with known techniques. We systematically study this phenomena andfind that these outlier channels emerge early in training, and that they occurmore frequently in layers with residual streams. We then propose a simplestrategy which regularizes a layer's inputs via quantization-aware training(QAT) and its outputs via activation kurtosis regularization. We show thatregularizing both the inputs and outputs is crucial for preventing a model's"migrating" the difficulty in input quantization to the weights, which makespost-training quantization (PTQ) of weights more difficult. When combined withweight PTQ, we show that our approach can obtain a W4A4 model that performscompetitively to the standard-precision W16A16 baseline.</description><author>Aniruddha Nrusimha, Mayank Mishra, Naigang Wang, Dan Alistarh, Rameswar Panda, Yoon Kim</author><pubDate>Thu, 04 Apr 2024 18:25:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03605v1</guid></item><item><title>Evaluating LLMs at Detecting Errors in LLM Responses</title><link>http://arxiv.org/abs/2404.03602v1</link><description>With Large Language Models (LLMs) being widely used across various tasks,detecting errors in their responses is increasingly crucial. However, littleresearch has been conducted on error detection of LLM responses. Collectingerror annotations on LLM responses is challenging due to the subjective natureof many NLP tasks, and thus previous research focuses on tasks of littlepractical value (e.g., word sorting) or limited error types (e.g., faithfulnessin summarization). This work introduces ReaLMistake, the first error detectionbenchmark consisting of objective, realistic, and diverse errors made by LLMs.ReaLMistake contains three challenging and meaningful tasks that introduceobjectively assessable errors in four categories (reasoning correctness,instruction-following, context-faithfulness, and parameterized knowledge),eliciting naturally observed and diverse errors in responses of GPT-4 and Llama2 70B annotated by experts. We use ReaLMistake to evaluate error detectorsbased on 12 LLMs. Our findings show: 1) Top LLMs like GPT-4 and Claude 3 detecterrors made by LLMs at very low recall, and all LLM-based error detectorsperform much worse than humans. 2) Explanations by LLM-based error detectorslack reliability. 3) LLMs-based error detection is sensitive to small changesin prompts but remains challenging to improve. 4) Popular approaches toimproving LLMs, including self-consistency and majority vote, do not improvethe error detection performance. Our benchmark and code are provided athttps://github.com/psunlpgroup/ReaLMistake.</description><author>Ryo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, Rui Zhang</author><pubDate>Thu, 04 Apr 2024 18:19:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03602v1</guid></item><item><title>Online Estimation with Rolling Validation: Adaptive Nonparametric Estimation with Streaming Data</title><link>http://arxiv.org/abs/2310.12140v2</link><description>Online nonparametric estimators are gaining popularity due to their efficientcomputation and competitive generalization abilities. An important exampleincludes variants of stochastic gradient descent. These algorithms often takeone sample point at a time and instantly update the parameter estimate ofinterest. In this work we consider model selection and hyperparameter tuningfor such online algorithms. We propose a weighted rolling-validation procedure,an online variant of leave-one-out cross-validation, that costs minimal extracomputation for many typical stochastic gradient descent estimators. Similar tobatch cross-validation, it can boost base estimators to achieve a better,adaptive convergence rate. Our theoretical analysis is straightforward, relyingmainly on some general statistical stability assumptions. The simulation studyunderscores the significance of diverging weights in rolling validation inpractice and demonstrates its sensitivity even when there is only a slimdifference between candidate estimators.</description><author>Tianyu Zhang, Jing Lei</author><pubDate>Thu, 04 Apr 2024 18:15:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12140v2</guid></item><item><title>TableLlama: Towards Open Large Generalist Models for Tables</title><link>http://arxiv.org/abs/2311.09206v3</link><description>Semi-structured tables are ubiquitous. There has been a variety of tasks thataim to automatically interpret, augment, and query tables. Current methodsoften require pretraining on tables or special model architecture design, arerestricted to specific table types, or have simplifying assumptions abouttables and tasks. This paper makes the first step towards developingopen-source large language models (LLMs) as generalists for a diversity oftable-based tasks. Towards that end, we construct TableInstruct, a new datasetwith a variety of realistic tables and tasks, for instruction tuning andevaluating LLMs. We further develop the first open-source generalist model fortables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address thelong context challenge. We experiment under both in-domain setting andout-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achievescomparable or better performance than the SOTA for each task, despite thelatter often has task-specific design. On 6 out-of-domain datasets, it achieves5-44 absolute point gains compared with the base model, showing that trainingon TableInstruct enhances the model's generalizability. We open-source ourdataset and trained model to boost future work on developing open generalistmodels for tables.</description><author>Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun</author><pubDate>Thu, 04 Apr 2024 18:10:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09206v3</guid></item><item><title>Intent Detection and Entity Extraction from BioMedical Literature</title><link>http://arxiv.org/abs/2404.03598v1</link><description>Biomedical queries have become increasingly prevalent in web searches,reflecting the growing interest in accessing biomedical literature. Despiterecent research on large-language models (LLMs) motivated by endeavours toattain generalized intelligence, their efficacy in replacing task anddomain-specific natural language understanding approaches remains questionable.In this paper, we address this question by conducting a comprehensive empiricalevaluation of intent detection and named entity recognition (NER) tasks frombiomedical text. We show that Supervised Fine Tuned approaches are stillrelevant and more effective than general-purpose LLMs. Biomedical transformermodels such as PubMedBERT can surpass ChatGPT on NER task with only 5supervised examples.</description><author>Ankan Mullick, Mukur Gupta, Pawan Goyal</author><pubDate>Thu, 04 Apr 2024 18:09:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03598v1</guid></item><item><title>Laser Learning Environment: A new environment for coordination-critical multi-agent tasks</title><link>http://arxiv.org/abs/2404.03596v1</link><description>We introduce the Laser Learning Environment (LLE), a collaborativemulti-agent reinforcement learning environment in which coordination iscentral. In LLE, agents depend on each other to make progress(interdependence), must jointly take specific sequences of actions to succeed(perfect coordination), and accomplishing those joint actions does not yieldany intermediate reward (zero-incentive dynamics). The challenge of suchproblems lies in the difficulty of escaping state space bottlenecks caused byinterdependence steps since escaping those bottlenecks is not rewarded. We testmultiple state-of-the-art value-based MARL algorithms against LLE and show thatthey consistently fail at the collaborative task because of their inability toescape state space bottlenecks, even though they successfully achieve perfectcoordination. We show that Q-learning extensions such as prioritized experiencereplay and n-steps return hinder exploration in environments withzero-incentive dynamics, and find that intrinsic curiosity with random networkdistillation is not sufficient to escape those bottlenecks. We demonstrate theneed for novel methods to solve this problem and the relevance of LLE ascooperative MARL benchmark.</description><author>Yannick Molinghen, Raphaël Avalos, Mark Van Achter, Ann Nowé, Tom Lenaerts</author><pubDate>Thu, 04 Apr 2024 18:05:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03596v1</guid></item><item><title>ReFT: Representation Finetuning for Language Models</title><link>http://arxiv.org/abs/2404.03592v1</link><description>Parameter-efficient fine-tuning (PEFT) methods seek to adapt large models viaupdates to a small number of weights. However, much prior interpretability workhas shown that representations encode rich semantic information, suggestingthat editing representations might be a more powerful alternative. Here, wepursue this hypothesis by developing a family of $\textbf{RepresentationFinetuning (ReFT)}$ methods. ReFT methods operate on a frozen base model andlearn task-specific interventions on hidden representations. We define a stronginstance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT). LoReFT isa drop-in replacement for existing PEFTs and learns interventions that are10x-50x more parameter-efficient than prior state-of-the-art PEFTs. We showcaseLoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks,Alpaca-Eval v1.0, and GLUE. In all these evaluations, LoReFT delivers the bestbalance of efficiency and performance, and almost always outperformsstate-of-the-art PEFTs. We release a generic ReFT training library publicly athttps://github.com/stanfordnlp/pyreft.</description><author>Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher D. Manning, Christopher Potts</author><pubDate>Thu, 04 Apr 2024 18:00:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03592v1</guid></item><item><title>SemGrasp: Semantic Grasp Generation via Language Aligned Discretization</title><link>http://arxiv.org/abs/2404.03590v1</link><description>Generating natural human grasps necessitates consideration of not just objectgeometry but also semantic information. Solely depending on object shape forgrasp generation confines the applications of prior methods in downstreamtasks. This paper presents a novel semantic-based grasp generation method,termed SemGrasp, which generates a static human grasp pose by incorporatingsemantic information into the grasp representation. We introduce a discreterepresentation that aligns the grasp space with semantic space, enabling thegeneration of grasp postures in accordance with language instructions. AMultimodal Large Language Model (MLLM) is subsequently fine-tuned, integratingobject, grasp, and language within a unified semantic space. To facilitate thetraining of SemGrasp, we have compiled a large-scale, grasp-text-aligneddataset named CapGrasp, featuring about 260k detailed captions and 50k diversegrasps. Experimental findings demonstrate that SemGrasp efficiently generatesnatural human grasps in alignment with linguistic intentions. Our code, models,and dataset are available publicly at: https://kailinli.github.io/SemGrasp.</description><author>Kailin Li, Jingbo Wang, Lixin Yang, Cewu Lu, Bo Dai</author><pubDate>Thu, 04 Apr 2024 17:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03590v1</guid></item><item><title>Fairness Improvement with Multiple Protected Attributes: How Far Are We?</title><link>http://arxiv.org/abs/2308.01923v3</link><description>Existing research mostly improves the fairness of Machine Learning (ML)software regarding a single protected attribute at a time, but this isunrealistic given that many users have multiple protected attributes. Thispaper conducts an extensive study of fairness improvement regarding multipleprotected attributes, covering 11 state-of-the-art fairness improvementmethods. We analyze the effectiveness of these methods with different datasets,metrics, and ML models when considering multiple protected attributes. Theresults reveal that improving fairness for a single protected attribute canlargely decrease fairness regarding unconsidered protected attributes. Thisdecrease is observed in up to 88.3% of scenarios (57.5% on average). Moresurprisingly, we find little difference in accuracy loss when consideringsingle and multiple protected attributes, indicating that accuracy can bemaintained in the multiple-attribute paradigm. However, the effect on F1-scorewhen handling two protected attributes is about twice that of a singleattribute. This has important implications for future fairness research:reporting only accuracy as the ML performance metric, which is currently commonin the literature, is inadequate.</description><author>Zhenpeng Chen, Jie M. Zhang, Federica Sarro, Mark Harman</author><pubDate>Thu, 04 Apr 2024 17:54:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01923v3</guid></item><item><title>Anticipate &amp; Collab: Data-driven Task Anticipation and Knowledge-driven Planning for Human-robot Collaboration</title><link>http://arxiv.org/abs/2404.03587v1</link><description>An agent assisting humans in daily living activities can collaborate moreeffectively by anticipating upcoming tasks. Data-driven methods represent thestate of the art in task anticipation, planning, and related problems, butthese methods are resource-hungry and opaque. Our prior work introduced a proofof concept framework that used an LLM to anticipate 3 high-level tasks thatserved as goals for a classical planning system that computed a sequence oflow-level actions for the agent to achieve these goals. This paper describesDaTAPlan, our framework that significantly extends our prior work towardhuman-robot collaboration. Specifically, DaTAPlan planner computes actions foran agent and a human to collaboratively and jointly achieve the tasksanticipated by the LLM, and the agent automatically adapts to unexpectedchanges in human action outcomes and preferences. We evaluate DaTAPlancapabilities in a realistic simulation environment, demonstrating accurate taskanticipation, effective human-robot collaboration, and the ability to adapt tounexpected changes. Project website: https://dataplan-hrc.github.io</description><author>Shivam Singh, Karthik Swaminathan, Raghav Arora, Ramandeep Singh, Ahana Datta, Dipanjan Das, Snehasis Banerjee, Mohan Sridharan, Madhava Krishna</author><pubDate>Thu, 04 Apr 2024 17:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03587v1</guid></item><item><title>Leveraging Interpolation Models and Error Bounds for Verifiable Scientific Machine Learning</title><link>http://arxiv.org/abs/2404.03586v1</link><description>Effective verification and validation techniques for modern scientificmachine learning workflows are challenging to devise. Statistical methods areabundant and easily deployed, but often rely on speculative assumptions aboutthe data and methods involved. Error bounds for classical interpolationtechniques can provide mathematically rigorous estimates of accuracy, but oftenare difficult or impractical to determine computationally. In this work, wepresent a best-of-both-worlds approach to verifiable scientific machinelearning by demonstrating that (1) multiple standard interpolation techniqueshave informative error bounds that can be computed or estimated efficiently;(2) comparative performance among distinct interpolants can aid in validationgoals; (3) deploying interpolation methods on latent spaces generated by deeplearning techniques enables some interpretability for black-box models. Wepresent a detailed case study of our approach for predicting lift-drag ratiosfrom airfoil images. Code developed for this work is available in a publicGithub repository.</description><author>Tyler Chang, Andrew Gillette, Romit Maulik</author><pubDate>Thu, 04 Apr 2024 17:52:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03586v1</guid></item><item><title>Towards more realistic human motion prediction with attention to motion coordination</title><link>http://arxiv.org/abs/2404.03584v1</link><description>Joint relation modeling is a curial component in human motion prediction.Most existing methods rely on skeletal-based graphs to build the jointrelations, where local interactive relations between joint pairs are welllearned. However, the motion coordination, a global joint relation reflectingthe simultaneous cooperation of all joints, is usually weakened because it islearned from part to whole progressively and asynchronously. Thus, the finalpredicted motions usually appear unrealistic. To tackle this issue, we learn amedium, called coordination attractor (CA), from the spatiotemporal features ofmotion to characterize the global motion features, which is subsequently usedto build new relative joint relations. Through the CA, all joints are relatedsimultaneously, and thus the motion coordination of all joints can be betterlearned. Based on this, we further propose a novel joint relation modelingmodule, Comprehensive Joint Relation Extractor (CJRE), to combine this motioncoordination with the local interactions between joint pairs in a unifiedmanner. Additionally, we also present a Multi-timescale Dynamics Extractor(MTDE) to extract enriched dynamics from the raw position information foreffective prediction. Extensive experiments show that the proposed frameworkoutperforms state-of-the-art methods in both short- and long-term predictionson H3.6M, CMU-Mocap, and 3DPW.</description><author>Pengxiang Ding, Jianqin Yin</author><pubDate>Thu, 04 Apr 2024 17:48:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03584v1</guid></item><item><title>SoK: Unintended Interactions among Machine Learning Defenses and Risks</title><link>http://arxiv.org/abs/2312.04542v2</link><description>Machine learning (ML) models cannot neglect risks to security, privacy, andfairness. Several defenses have been proposed to mitigate such risks. When adefense is effective in mitigating one risk, it may correspond to increased ordecreased susceptibility to other risks. Existing research lacks an effectiveframework to recognize and explain these unintended interactions. We presentsuch a framework, based on the conjecture that overfitting and memorizationunderlie unintended interactions. We survey existing literature on unintendedinteractions, accommodating them within our framework. We use our framework toconjecture on two previously unexplored interactions, and empirically validateour conjectures.</description><author>Vasisht Duddu, Sebastian Szyller, N. Asokan</author><pubDate>Thu, 04 Apr 2024 17:43:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04542v2</guid></item><item><title>Expressive Forecasting of 3D Whole-body Human Motions</title><link>http://arxiv.org/abs/2312.11972v2</link><description>Human motion forecasting, with the goal of estimating future human behaviorover a period of time, is a fundamental task in many real-world applications.However, existing works typically concentrate on predicting the major joints ofthe human body without considering the delicate movements of the human hands.In practical applications, hand gesture plays an important role in humancommunication with the real world, and expresses the primary intention of humanbeings. In this work, we are the first to formulate a whole-body human poseforecasting task, which jointly predicts the future body and hand activities.Correspondingly, we propose a novel Encoding-Alignment-Interaction (EAI)framework that aims to predict both coarse (body joints) and fine-grained(gestures) activities collaboratively, enabling expressive andcross-facilitated forecasting of 3D whole-body human motions. Specifically, ourmodel involves two key constituents: cross-context alignment (XCA) andcross-context interaction (XCI). Considering the heterogeneous informationwithin the whole-body, XCA aims to align the latent features of various humancomponents, while XCI focuses on effectively capturing the context interactionamong the human components. We conduct extensive experiments on anewly-introduced large-scale benchmark and achieve state-of-the-artperformance. The code is public for research purposes athttps://github.com/Dingpx/EAI.</description><author>Pengxiang Ding, Qiongjie Cui, Min Zhang, Mengyuan Liu, Haofan Wang, Donglin Wang</author><pubDate>Thu, 04 Apr 2024 17:41:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11972v2</guid></item><item><title>Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithm</title><link>http://arxiv.org/abs/2404.03578v1</link><description>The sim-to-real gap, which represents the disparity between training andtesting environments, poses a significant challenge in reinforcement learning(RL). A promising approach to addressing this challenge is distributionallyrobust RL, often framed as a robust Markov decision process (RMDP). In thisframework, the objective is to find a robust policy that achieves goodperformance under the worst-case scenario among all environments within apre-specified uncertainty set centered around the training environment. Unlikeprevious work, which relies on a generative model or a pre-collected offlinedataset enjoying good coverage of the deployment environment, we tackle robustRL via interactive data collection, where the learner interacts with thetraining environment only and refines the policy through trial and error. Inthis robust RL paradigm, two main challenges emerge: managing distributionalrobustness while striking a balance between exploration and exploitation duringdata collection. Initially, we establish that sample-efficient learning withoutadditional assumptions is unattainable owing to the curse of support shift;i.e., the potential disjointedness of the distributional supports between thetraining and testing environments. To circumvent such a hardness result, weintroduce the vanishing minimal value assumption to RMDPs with atotal-variation (TV) distance robust set, postulating that the minimal value ofthe optimal robust value function is zero. We prove that such an assumptioneffectively eliminates the support shift issue for RMDPs with a TV distancerobust set, and present an algorithm with a provable sample complexityguarantee. Our work makes the initial step to uncovering the inherentdifficulty of robust RL via interactive data collection and sufficientconditions for designing a sample-efficient algorithm accompanied by sharpsample complexity analysis.</description><author>Miao Lu, Han Zhong, Tong Zhang, Jose Blanchet</author><pubDate>Thu, 04 Apr 2024 17:40:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03578v1</guid></item><item><title>Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models</title><link>http://arxiv.org/abs/2404.03577v1</link><description>Providing knowledge documents for large language models (LLMs) has emerged asa promising solution to update the static knowledge inherent in theirparameters. However, knowledge in the document may conflict with the memory ofLLMs due to outdated or incorrect knowledge in the LLMs' parameters. This leadsto the necessity of examining the capability of LLMs to assimilate supplementalexternal knowledge that conflicts with their memory. While previous studieshave explained to what extent LLMs extract conflicting knowledge from theprovided text, they neglect the necessity to reason with conflicting knowledge.Furthermore, there lack a detailed analysis on strategies to enable LLMs toresolve conflicting knowledge via prompting, decoding strategy, and supervisedfine-tuning. To address these limitations, we construct a new dataset, dubbedKNOT, for knowledge conflict resolution examination in the form of questionanswering. KNOT facilitates in-depth analysis by dividing reasoning withconflicting knowledge into three levels: (1) Direct Extraction, which directlyextracts conflicting knowledge to answer questions. (2) Explicit Reasoning,which reasons with conflicting knowledge when the reasoning path is explicitlyprovided in the question. (3) Implicit Reasoning, where reasoning withconflicting knowledge requires LLMs to infer the reasoning path independentlyto answer questions. We also conduct extensive experiments on KNOT to establishempirical guidelines for LLMs to utilize conflicting knowledge in complexcircumstances. Dataset and associated codes can be accessed athttps://github.com/THU-KEG/KNOT .</description><author>Yantao Liu, Zijun Yao, Xin Lv, Yuchen Fan, Shulin Cao, Jifan Yu, Lei Hou, Juanzi Li</author><pubDate>Thu, 04 Apr 2024 17:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03577v1</guid></item><item><title>DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation Pattern Sampling</title><link>http://arxiv.org/abs/2404.03575v1</link><description>Text-to-3D scene generation holds immense potential for the gaming, film, andarchitecture sectors. Despite significant progress, existing methods strugglewith maintaining high quality, consistency, and editing flexibility. In thispaper, we propose DreamScene, a 3D Gaussian-based novel text-to-3D scenegeneration framework, to tackle the aforementioned three challenges mainly viatwo strategies. First, DreamScene employs Formation Pattern Sampling (FPS), amulti-timestep sampling strategy guided by the formation patterns of 3Dobjects, to form fast, semantically rich, and high-quality representations. FPSuses 3D Gaussian filtering for optimization stability, and leveragesreconstruction techniques to generate plausible textures. Second, DreamSceneemploys a progressive three-stage camera sampling strategy, specificallydesigned for both indoor and outdoor settings, to effectively ensureobject-environment integration and scene-wide 3D consistency. Last, DreamSceneenhances scene editing flexibility by integrating objects and environments,enabling targeted adjustments. Extensive experiments validate DreamScene'ssuperiority over current state-of-the-art techniques, heralding itswide-ranging potential for diverse applications. Code and demos will bereleased at https://dreamscene-project.github.io .</description><author>Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, Pengyuan Zhou</author><pubDate>Thu, 04 Apr 2024 17:38:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03575v1</guid></item><item><title>TinyVQA: Compact Multimodal Deep Neural Network for Visual Question Answering on Resource-Constrained Devices</title><link>http://arxiv.org/abs/2404.03574v1</link><description>Traditional machine learning models often require powerful hardware, makingthem unsuitable for deployment on resource-limited devices. Tiny MachineLearning (tinyML) has emerged as a promising approach for running machinelearning models on these devices, but integrating multiple data modalities intotinyML models still remains a challenge due to increased complexity, latency,and power consumption. This paper proposes TinyVQA, a novel multimodal deepneural network for visual question answering tasks that can be deployed onresource-constrained tinyML hardware. TinyVQA leverages a supervisedattention-based model to learn how to answer questions about images using bothvision and language modalities. Distilled knowledge from the supervisedattention-based VQA model trains the memory aware compact TinyVQA model and lowbit-width quantization technique is employed to further compress the model fordeployment on tinyML devices. The TinyVQA model was evaluated on the FloodNetdataset, which is used for post-disaster damage assessment. The compact modelachieved an accuracy of 79.5%, demonstrating the effectiveness of TinyVQA forreal-world applications. Additionally, the model was deployed on a Crazyflie2.0 drone, equipped with an AI deck and GAP8 microprocessor. The TinyVQA modelachieved low latencies of 56 ms and consumes 693 mW power while deployed on thetiny drone, showcasing its suitability for resource-constrained embeddedsystems.</description><author>Hasib-Al Rashid, Argho Sarkar, Aryya Gangopadhyay, Maryam Rahnemoonfar, Tinoosh Mohsenin</author><pubDate>Thu, 04 Apr 2024 17:38:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03574v1</guid></item><item><title>Terrain Point Cloud Inpainting via Signal Decomposition</title><link>http://arxiv.org/abs/2404.03572v1</link><description>The rapid development of 3D acquisition technology has made it possible toobtain point clouds of real-world terrains. However, due to limitations insensor acquisition technology or specific requirements, point clouds oftencontain defects such as holes with missing data. Inpainting algorithms arewidely used to patch these holes. However, existing traditional inpaintingalgorithms rely on precise hole boundaries, which limits their ability tohandle cases where the boundaries are not well-defined. On the other hand,learning-based completion methods often prioritize reconstructing the entirepoint cloud instead of solely focusing on hole filling. Based on the fact thatreal-world terrain exhibits both global smoothness and rich local detail, wepropose a novel representation for terrain point clouds. This representationcan help to repair the holes without clear boundaries. Specifically, itdecomposes terrains into low-frequency and high-frequency components, which arerepresented by B-spline surfaces and relative height maps respectively. In thisway, the terrain point cloud inpainting problem is transformed into a B-splinesurface fitting and 2D image inpainting problem. By solving the two problems,the highly complex and irregular holes on the terrain point clouds can bewell-filled, which not only satisfies the global terrain undulation but alsoexhibits rich geometric details. The experimental results also demonstrate theeffectiveness of our method.</description><author>Yizhou Xie, Xiangning Xie, Yuran Wang, Yanci Zhang, Zejun Lv</author><pubDate>Thu, 04 Apr 2024 17:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03572v1</guid></item><item><title>From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning</title><link>http://arxiv.org/abs/2310.00492v3</link><description>Large Language Models (LLMs) have achieved remarkable success, whereinstruction tuning is the critical step in aligning LLMs with user intentions.In this work, we investigate how the instruction tuning adjusts pre-trainedmodels with a focus on intrinsic changes. Specifically, we first developseveral local and global explanation methods, including a gradient-based methodfor input-output attribution, and techniques for interpreting patterns andconcepts in self-attention and feed-forward layers. The impact of instructiontuning is then studied by comparing the explanations derived from thepre-trained and instruction-tuned models. This approach provides an internalperspective of the model shifts on a human-comprehensible level. Our findingsreveal three significant impacts of instruction tuning: 1) It empowers LLMs torecognize the instruction parts of user prompts, and promotes the responsegeneration constantly conditioned on the instructions. 2) It encourages theself-attention heads to capture more word-word relationships about instructionverbs. 3) It encourages the feed-forward networks to rotate their pre-trainedknowledge toward user-oriented tasks. These insights contribute to a morecomprehensive understanding of instruction tuning and lay the groundwork forfuture work that aims at explaining and optimizing LLMs for variousapplications. Our code and data are publicly available athttps://github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs.</description><author>Xuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu, Dong Yu</author><pubDate>Thu, 04 Apr 2024 17:30:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00492v3</guid></item><item><title>Hybrid Ground-State Quantum Algorithms based on Neural Schrödinger Forging</title><link>http://arxiv.org/abs/2307.02633v2</link><description>Entanglement forging based variational algorithms leverage the bi-partitionof quantum systems for addressing ground state problems. The primary limitationof these approaches lies in the exponential summation required over thenumerous potential basis states, or bitstrings, when performing the Schmidtdecomposition of the whole system. To overcome this challenge, we propose a newmethod for entanglement forging employing generative neural networks toidentify the most pertinent bitstrings, eliminating the need for theexponential sum. Through empirical demonstrations on systems of increasingcomplexity, we show that the proposed algorithm achieves comparable or superiorperformance compared to the existing standard implementation of entanglementforging. Moreover, by controlling the amount of required resources, this schemecan be applied to larger, as well as non permutation invariant systems, wherethe latter constraint is associated with the Heisenberg forging procedure. Wesubstantiate our findings through numerical simulations conducted on spinsmodels exhibiting one-dimensional ring, two-dimensional triangular latticetopologies, and nuclear shell model configurations.</description><author>Paulin de Schoulepnikoff, Oriel Kiss, Sofia Vallecorsa, Giuseppe Carleo, Michele Grossi</author><pubDate>Thu, 04 Apr 2024 17:27:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02633v2</guid></item><item><title>Cameras as Rays: Pose Estimation via Ray Diffusion</title><link>http://arxiv.org/abs/2402.14817v3</link><description>Estimating camera poses is a fundamental task for 3D reconstruction andremains challenging given sparsely sampled views (&lt;10). In contrast to existingapproaches that pursue top-down prediction of global parametrizations of cameraextrinsics, we propose a distributed representation of camera pose that treatsa camera as a bundle of rays. This representation allows for a tight couplingwith spatial image features improving pose precision. We observe that thisrepresentation is naturally suited for set-level transformers and develop aregression-based approach that maps image patches to corresponding rays. Tocapture the inherent uncertainties in sparse-view pose inference, we adapt thisapproach to learn a denoising diffusion model which allows us to sampleplausible modes while improving performance. Our proposed methods, bothregression- and diffusion-based, demonstrate state-of-the-art performance oncamera pose estimation on CO3D while generalizing to unseen object categoriesand in-the-wild captures.</description><author>Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, Shubham Tulsiani</author><pubDate>Thu, 04 Apr 2024 17:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14817v3</guid></item><item><title>PointInfinity: Resolution-Invariant Point Diffusion Models</title><link>http://arxiv.org/abs/2404.03566v1</link><description>We present PointInfinity, an efficient family of point cloud diffusionmodels. Our core idea is to use a transformer-based architecture with afixed-size, resolution-invariant latent representation. This enables efficienttraining with low-resolution point clouds, while allowing high-resolution pointclouds to be generated during inference. More importantly, we show that scalingthe test-time resolution beyond the training resolution improves the fidelityof generated point clouds and surfaces. We analyze this phenomenon and draw alink to classifier-free guidance commonly used in diffusion models,demonstrating that both allow trading off fidelity and variability duringinference. Experiments on CO3D show that PointInfinity can efficiently generatehigh-resolution point clouds (up to 131k points, 31 times more than Point-E)with state-of-the-art quality.</description><author>Zixuan Huang, Justin Johnson, Shoubhik Debnath, James M. Rehg, Chao-Yuan Wu</author><pubDate>Thu, 04 Apr 2024 17:24:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03566v1</guid></item><item><title>Long-term Forecasting with TiDE: Time-series Dense Encoder</title><link>http://arxiv.org/abs/2304.08424v5</link><description>Recent work has shown that simple linear models can outperform severalTransformer based approaches in long term time-series forecasting. Motivated bythis, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model,Time-series Dense Encoder (TiDE), for long-term time-series forecasting thatenjoys the simplicity and speed of linear models while also being able tohandle covariates and non-linear dependencies. Theoretically, we prove that thesimplest linear analogue of our model can achieve near optimal error rate forlinear dynamical systems (LDS) under some assumptions. Empirically, we showthat our method can match or outperform prior approaches on popular long-termtime-series forecasting benchmarks while being 5-10x faster than the bestTransformer based model.</description><author>Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, Rose Yu</author><pubDate>Thu, 04 Apr 2024 17:24:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08424v5</guid></item><item><title>LLM-based Medical Assistant Personalization with Short- and Long-Term Memory Coordination</title><link>http://arxiv.org/abs/2309.11696v3</link><description>Large Language Models (LLMs), such as GPT3.5, have exhibited remarkableproficiency in comprehending and generating natural language. On the otherhand, medical assistants hold the potential to offer substantial benefits forindividuals. However, the exploration of LLM-based personalized medicalassistant remains relatively scarce. Typically, patients converse differentlybased on their background and preferences which necessitates the task ofenhancing user-oriented medical assistant. While one can fully train an LLM forthis objective, the resource consumption is unaffordable. Prior research hasexplored memory-based methods to enhance the response with aware of previousmistakes for new queries during a dialogue session. We contend that a merememory module is inadequate and fully training an LLM can be excessivelycostly. In this study, we propose a novel computational bionic memorymechanism, equipped with a parameter-efficient fine-tuning (PEFT) schema, topersonalize medical assistants.</description><author>Kai Zhang, Yangyang Kang, Fubang Zhao, Xiaozhong Liu</author><pubDate>Thu, 04 Apr 2024 17:23:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11696v3</guid></item><item><title>Personalized LLM Response Generation with Parameterized Memory Injection</title><link>http://arxiv.org/abs/2404.03565v1</link><description>Large Language Models (LLMs) have exhibited remarkable proficiency incomprehending and generating natural language. On the other hand, personalizedLLM response generation holds the potential to offer substantial benefits forindividuals in critical areas such as medical. Existing research has exploredmemory-augmented methods to prompt the LLM with pre-stored user-specificknowledge for personalized response generation in terms of new queries. Wecontend that such paradigm is unable to perceive fine-granularity information.In this study, we propose a novel \textbf{M}emory-\textbf{i}njected approachusing parameter-efficient fine-tuning (PEFT) and along with a BayesianOptimisation searching strategy to achieve \textbf{L}LM\textbf{P}ersonalization(\textbf{MiLP}).</description><author>Kai Zhang, Lizhi Qing, Yangyang Kang, Xiaozhong Liu</author><pubDate>Thu, 04 Apr 2024 17:20:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03565v1</guid></item><item><title>EASSE-DE: Easier Automatic Sentence Simplification Evaluation for German</title><link>http://arxiv.org/abs/2404.03563v1</link><description>In this work, we propose EASSE-multi, a framework for easier automaticsentence evaluation for languages other than English. Compared to the originalEASSE framework, EASSE-multi does not focus only on English. It containstokenizers and versions of text simplification evaluation metrics which aresuitable for multiple languages. In this paper, we exemplify the usage ofEASSE-multi for German TS, resulting in EASSE-DE. Further, we compare textsimplification results when evaluating with different language or tokenizationsettings of the metrics. Based on this, we formulate recommendations on how tomake the evaluation of (German) TS models more transparent and bettercomparable. The code of EASSE-multi and its German specialisation (EASSE-DE)can be found at https://github.com/rstodden/easse-de.</description><author>Regina Stodden</author><pubDate>Thu, 04 Apr 2024 17:18:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03563v1</guid></item><item><title>Select and Summarize: Scene Saliency for Movie Script Summarization</title><link>http://arxiv.org/abs/2404.03561v1</link><description>Abstractive summarization for long-form narrative texts such as movie scriptsis challenging due to the computational and memory constraints of currentlanguage models. A movie script typically comprises a large number of scenes;however, only a fraction of these scenes are salient, i.e., important forunderstanding the overall narrative. The salience of a scene can beoperationalized by considering it as salient if it is mentioned in the summary.Automatically identifying salient scenes is difficult due to the lack ofsuitable datasets. In this work, we introduce a scene saliency dataset thatconsists of human-annotated salient scenes for 100 movies. We propose atwo-stage abstractive summarization approach which first identifies the salientscenes in script and then generates a summary using only those scenes. UsingQA-based evaluation, we show that our model outperforms previousstate-of-the-art summarization methods and reflects the information content ofa movie more accurately than a model that takes the whole movie script asinput.</description><author>Rohit Saxena, Frank Keller</author><pubDate>Thu, 04 Apr 2024 17:16:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03561v1</guid></item><item><title>How does Multi-Task Training Affect Transformer In-Context Capabilities? Investigations with Function Classes</title><link>http://arxiv.org/abs/2404.03558v1</link><description>Large language models (LLM) have recently shown the extraordinary ability toperform unseen tasks based on few-shot examples provided as text, also known asin-context learning (ICL). While recent works have attempted to understand themechanisms driving ICL, few have explored training strategies that incentivizethese models to generalize to multiple tasks. Multi-task learning (MTL) forgeneralist models is a promising direction that offers transfer learningpotential, enabling large parameterized models to be trained from simpler,related tasks. In this work, we investigate the combination of MTL with ICL tobuild models that efficiently learn tasks while being robust toout-of-distribution examples. We propose several effective curriculum learningstrategies that allow ICL models to achieve higher data efficiency and morestable convergence. Our experiments reveal that ICL models can effectivelylearn difficult tasks by training on progressively harder tasks while mixing inprior tasks, denoted as mixed curriculum in this work. Our code and models areavailable at https://github.com/harmonbhasin/curriculum_learning_icl .</description><author>Harmon Bhasin, Timothy Ossowski, Yiqiao Zhong, Junjie Hu</author><pubDate>Thu, 04 Apr 2024 17:15:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03558v1</guid></item><item><title>APISR: Anime Production Inspired Real-World Anime Super-Resolution</title><link>http://arxiv.org/abs/2403.01598v2</link><description>While real-world anime super-resolution (SR) has gained increasing attentionin the SR community, existing methods still adopt techniques from thephotorealistic domain. In this paper, we analyze the anime production workflowand rethink how to use characteristics of it for the sake of the real-worldanime SR. First, we argue that video networks and datasets are not necessaryfor anime SR due to the repetition use of hand-drawing frames. Instead, wepropose an anime image collection pipeline by choosing the least compressed andthe most informative frames from the video sources. Based on this pipeline, weintroduce the Anime Production-oriented Image (API) dataset. In addition, weidentify two anime-specific challenges of distorted and faint hand-drawn linesand unwanted color artifacts. We address the first issue by introducing aprediction-oriented compression module in the image degradation model and apseudo-ground truth preparation with enhanced hand-drawn lines. In addition, weintroduce the balanced twin perceptual loss combining both anime andphotorealistic high-level features to mitigate unwanted color artifacts andincrease visual clarity. We evaluate our method through extensive experimentson the public benchmark, showing our method outperforms state-of-the-art animedataset-trained approaches.</description><author>Boyang Wang, Fengyu Yang, Xihang Yu, Chao Zhang, Hanbin Zhao</author><pubDate>Thu, 04 Apr 2024 17:12:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01598v2</guid></item><item><title>From News to Summaries: Building a Hungarian Corpus for Extractive and Abstractive Summarization</title><link>http://arxiv.org/abs/2404.03555v1</link><description>Training summarization models requires substantial amounts of training data.However for less resourceful languages like Hungarian, openly available modelsand datasets are notably scarce. To address this gap our paper introducesHunSum-2 an open-source Hungarian corpus suitable for training abstractive andextractive summarization models. The dataset is assembled from segments of theCommon Crawl corpus undergoing thorough cleaning, preprocessing anddeduplication. In addition to abstractive summarization we generatesentence-level labels for extractive summarization using sentence similarity.We train baseline models for both extractive and abstractive summarizationusing the collected dataset. To demonstrate the effectiveness of the trainedmodels, we perform both quantitative and qualitative evaluation. Our dataset,models and code are publicly available, encouraging replication, furtherresearch, and real-world applications across various domains.</description><author>Botond Barta, Dorina Lakatos, Attila Nagy, Milán Konor Nyist, Judit Ács</author><pubDate>Thu, 04 Apr 2024 17:07:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03555v1</guid></item><item><title>Alzheimer's disease detection in PSG signals</title><link>http://arxiv.org/abs/2404.03549v1</link><description>Alzheimer's disease (AD) and sleep disorders exhibit a close association,where disruptions in sleep patterns often precede the onset of Mild CognitiveImpairment (MCI) and early-stage AD. This study delves into the potential ofutilizing sleep-related electroencephalography (EEG) signals acquired throughpolysomnography (PSG) for the early detection of AD. Our primary focus is onexploring semi-supervised Deep Learning techniques for the classification ofEEG signals due to the clinical scenario characterized by the limited dataavailability. The methodology entails testing and comparing the performance ofsemi-supervised SMATE and TapNet models, benchmarked against the supervised XCMmodel, and unsupervised Hidden Markov Models (HMMs). The study highlights thesignificance of spatial and temporal analysis capabilities, conductingindependent analyses of each sleep stage. Results demonstrate the effectivenessof SMATE in leveraging limited labeled data, achieving stable metrics acrossall sleep stages, and reaching 90% accuracy in its supervised form. Comparativeanalyses reveal SMATE's superior performance over TapNet and HMM, while XCMexcels in supervised scenarios with an accuracy range of 92 - 94%. Thesefindings underscore the potential of semi-supervised models in early ADdetection, particularly in overcoming the challenges associated with thescarcity of labeled data. Ablation tests affirm the critical role ofspatio-temporal feature extraction in semi-supervised predictive performance,and t-SNE visualizations validate the model's proficiency in distinguishing ADpatterns. Overall, this research contributes to the advancement of AD detectionthrough innovative Deep Learning approaches, highlighting the crucial role ofsemi-supervised learning in addressing data limitations.</description><author>Lorena Gallego-Viñarás, Juan Miguel Mira-Tomás, Anna Michela-Gaeta, Gerard Pinol-Ripoll, Ferrán Barbé, Pablo M. Olmos, Arrate Muñoz-Barrutia</author><pubDate>Thu, 04 Apr 2024 16:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03549v1</guid></item><item><title>CodeEditorBench: Evaluating Code Editing Capability of Large Language Models</title><link>http://arxiv.org/abs/2404.03543v1</link><description>Large Language Models (LLMs) for code are rapidly evolving, with code editingemerging as a critical capability. We introduce CodeEditorBench, an evaluationframework designed to rigorously assess the performance of LLMs in code editingtasks, including debugging, translating, polishing, and requirement switching.Unlike existing benchmarks focusing solely on code generation, CodeEditorBenchemphasizes real-world scenarios and practical aspects of software development.We curate diverse coding challenges and scenarios from five sources, coveringvarious programming languages, complexity levels, and editing tasks. Evaluationof 19 LLMs reveals that closed-source models (particularly Gemini-Ultra andGPT-4), outperform open-source models in CodeEditorBench, highlightingdifferences in model performance based on problem types and promptsensitivities. CodeEditorBench aims to catalyze advancements in LLMs byproviding a robust platform for assessing code editing capabilities. We willrelease all prompts and datasets to enable the community to expand the datasetand benchmark emerging LLMs. By introducing CodeEditorBench, we contribute tothe advancement of LLMs in code editing and provide a valuable resource forresearchers and practitioners.</description><author>Jiawei Guo, Ziming Li, Xueling Liu, Kaijing Ma, Tianyu Zheng, Zhouliang Yu, Ding Pan, Yizhi LI, Ruibo Liu, Yue Wang, Shuyue Guo, Xingwei Qu, Xiang Yue, Ge Zhang, Wenhu Chen, Jie Fu</author><pubDate>Thu, 04 Apr 2024 16:49:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03543v1</guid></item><item><title>Segmentation-Guided Knee Radiograph Generation using Conditional Diffusion Models</title><link>http://arxiv.org/abs/2404.03541v1</link><description>Deep learning-based medical image processing algorithms requirerepresentative data during development. In particular, surgical data might bedifficult to obtain, and high-quality public datasets are limited. To overcomethis limitation and augment datasets, a widely adopted solution is thegeneration of synthetic images. In this work, we employ conditional diffusionmodels to generate knee radiographs from contour and bone segmentations.Remarkably, two distinct strategies are presented by incorporating thesegmentation as a condition into the sampling and training process, namely,conditional sampling and conditional training. The results demonstrate thatboth methods can generate realistic images while adhering to the conditioningsegmentation. The conditional training method outperforms the conditionalsampling method and the conventional U-Net.</description><author>Siyuan Mei, Fuxin Fan, Fabian Wagner, Mareike Thies, Mingxuan Gu, Yipeng Sun, Andreas Maier</author><pubDate>Thu, 04 Apr 2024 16:49:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03541v1</guid></item><item><title>Is CLIP the main roadblock for fine-grained open-world perception?</title><link>http://arxiv.org/abs/2404.03539v1</link><description>Modern applications increasingly demand flexible computer vision models thatadapt to novel concepts not encountered during training. This necessity ispivotal in emerging domains like extended reality, robotics, and autonomousdriving, which require the ability to respond to open-world stimuli. A keyingredient is the ability to identify objects based on free-form textualqueries defined at inference time - a task known as open-vocabulary objectdetection. Multimodal backbones like CLIP are the main enabling technology forcurrent open-world perception solutions. Despite performing well on genericqueries, recent studies highlighted limitations on the fine-grained recognitioncapabilities in open-vocabulary settings - i.e., for distinguishing subtleobject features like color, shape, and material. In this paper, we perform adetailed examination of these open-vocabulary object recognition limitations tofind the root cause. We evaluate the performance of CLIP, the most commonlyused vision-language backbone, against a fine-grained object-matchingbenchmark, revealing interesting analogies between the limitations ofopen-vocabulary object detectors and their backbones. Experiments suggest thatthe lack of fine-grained understanding is caused by the poor separability ofobject characteristics in the CLIP latent space. Therefore, we try tounderstand whether fine-grained knowledge is present in CLIP embeddings but notexploited at inference time due, for example, to the unsuitability of thecosine similarity matching function, which may discard important objectcharacteristics. Our preliminary experiments show that simple CLIP latent-spacere-projections help separate fine-grained concepts, paving the way towards thedevelopment of backbones inherently able to process fine-grained details. Thecode for reproducing these experiments is available athttps://github.com/lorebianchi98/FG-CLIP.</description><author>Lorenzo Bianchi, Fabio Carrara, Nicola Messina, Fabrizio Falchi</author><pubDate>Thu, 04 Apr 2024 16:47:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03539v1</guid></item><item><title>If It's Not Enough, Make It So: Reducing Authentic Data Demand in Face Recognition through Synthetic Faces</title><link>http://arxiv.org/abs/2404.03537v1</link><description>Recent advances in deep face recognition have spurred a growing demand forlarge, diverse, and manually annotated face datasets. Acquiring authentic,high-quality data for face recognition has proven to be a challenge, primarilydue to privacy concerns. Large face datasets are primarily sourced fromweb-based images, lacking explicit user consent. In this paper, we examinewhether and how synthetic face data can be used to train effective facerecognition models with reduced reliance on authentic images, therebymitigating data collection concerns. First, we explored the performance gapamong recent state-of-the-art face recognition models, trained with syntheticdata only and authentic (scarce) data only. Then, we deepened our analysis bytraining a state-of-the-art backbone with various combinations of synthetic andauthentic data, gaining insights into optimizing the limited use of the latterfor verification accuracy. Finally, we assessed the effectiveness of dataaugmentation approaches on synthetic and authentic data, with the same goal inmind. Our results highlighted the effectiveness of FR trained on combineddatasets, particularly when combined with appropriate augmentation techniques.</description><author>Andrea Atzori, Fadi Boutros, Naser Damer, Gianni Fenu, Mirko Marras</author><pubDate>Thu, 04 Apr 2024 16:45:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03537v1</guid></item><item><title>Evaluating Generative Language Models in Information Extraction as Subjective Question Correction</title><link>http://arxiv.org/abs/2404.03532v1</link><description>Modern Large Language Models (LLMs) have showcased remarkable prowess invarious tasks necessitating sophisticated cognitive behaviors. Nevertheless, aparadoxical performance discrepancy is observed, where these modelsunderperform in seemingly elementary tasks like relation extraction and eventextraction due to two issues in conventional evaluation. (1) The imprecision ofexisting evaluation metrics that struggle to effectively gauge semanticconsistency between model outputs and ground truth, and (2) The inherentincompleteness of evaluation benchmarks, primarily due to restrictive humanannotation schemas, resulting in underestimated LLM performances. Inspired bythe principles in subjective question correction, we propose a new evaluationmethod, SQC-Score. This method innovatively utilizes LLMs, fine-tuned throughsubjective question correction data, to refine matching between model outputsand golden labels. Additionally, by incorporating a Natural Language Inference(NLI) model, SQC-Score enriches golden labels, addressing benchmarkincompleteness by acknowledging correct yet previously omitted answers. Resultson three information extraction tasks show that SQC-Score is more preferred byhuman annotators than the baseline metrics. Utilizing SQC-Score, we conduct acomprehensive evaluation of the state-of-the-art LLMs and provide insights forfuture research for information extraction. Dataset and associated codes can beaccessed at https://github.com/THU-KEG/SQC-Score.</description><author>Yuchen Fan, Yantao Liu, Zijun Yao, Jifan Yu, Lei Hou, Juanzi Li</author><pubDate>Thu, 04 Apr 2024 16:36:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03532v1</guid></item><item><title>COMO: Compact Mapping and Odometry</title><link>http://arxiv.org/abs/2404.03531v1</link><description>We present COMO, a real-time monocular mapping and odometry system thatencodes dense geometry via a compact set of 3D anchor points. Decoding anchorpoint projections into dense geometry via per-keyframe depth covariancefunctions guarantees that depth maps are joined together at visible anchorpoints. The representation enables joint optimization of camera poses and densegeometry, intrinsic 3D consistency, and efficient second-order inference. Tomaintain a compact yet expressive map, we introduce a frontend that leveragesthe covariance function for tracking and initializing potentially visuallyindistinct 3D points across frames. Altogether, we introduce a real-time systemcapable of estimating accurate poses and consistent geometry.</description><author>Eric Dexheimer, Andrew J. Davison</author><pubDate>Thu, 04 Apr 2024 16:35:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03531v1</guid></item><item><title>Trust in AI: Progress, Challenges, and Future Directions</title><link>http://arxiv.org/abs/2403.14680v3</link><description>The increasing use of artificial intelligence (AI) systems in our daily lifethrough various applications, services, and products explains the significanceof trust/distrust in AI from a user perspective. AI-driven systems (as opposedto other technologies) have ubiquitously diffused in our life not only as somebeneficial tools to be used by human agents but also are going to besubstitutive agents on our behalf, or manipulative minds that would influencehuman thought, decision, and agency. Trust/distrust in AI plays the role of aregulator and could significantly control the level of this diffusion, as trustcan increase, and distrust may reduce the rate of adoption of AI. Recently,varieties of studies have paid attention to the variant dimension oftrust/distrust in AI, and its relevant considerations. In this systematicliterature review, after conceptualization of trust in the current AIliterature review, we will investigate trust in different types ofhuman-Machine interaction, and its impact on technology acceptance in differentdomains. In addition to that, we propose a taxonomy of technical (i.e., safety,accuracy, robustness) and non-technical axiological (i.e., ethical, legal, andmixed) trustworthiness metrics, and some trustworthy measurements. Moreover, weexamine some major trust-breakers in AI (e.g., autonomy and dignity threat),and trust makers; and propose some future directions and probable solutions forthe transition to a trustworthy AI.</description><author>Saleh Afroogh, Ali Akbari, Evan Malone, Mohammadali Kargar, Hananeh Alambeigi</author><pubDate>Thu, 04 Apr 2024 16:34:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14680v3</guid></item><item><title>BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering</title><link>http://arxiv.org/abs/2404.03528v1</link><description>Knowledge Graphs (KGs) have proven essential in information processing andreasoning applications because they link related entities and give context-richinformation, supporting efficient information retrieval and knowledgediscovery; presenting information flow in a very effective manner. Despitebeing widely used globally, Bangla is relatively underrepresented in KGs due toa lack of comprehensive datasets, encoders, NER (named entity recognition)models, POS (part-of-speech) taggers, and lemmatizers, hindering efficientinformation processing and reasoning applications in the language. Addressingthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering frameworkthat is able to automatically construct Bengali KGs from any Bangla text. Weutilize multilingual LLMs to understand various languages and correlateentities and relations universally. By employing a translation dictionary toidentify English equivalents and extracting word features from pre-trained BERTmodels, we construct the foundational KG. To reduce noise and align wordembeddings with our goal, we employ graph-based polynomial filters. Lastly, weimplement a GNN-based semantic filter, which elevates contextual understandingand trims unnecessary edges, culminating in the formation of the definitive KG.Empirical findings and case studies demonstrate the universal effectiveness ofour model, capable of autonomously constructing semantically enriched KGs fromany text.</description><author>Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae</author><pubDate>Thu, 04 Apr 2024 16:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03528v1</guid></item><item><title>HAPNet: Toward Superior RGB-Thermal Scene Parsing via Hybrid, Asymmetric, and Progressive Heterogeneous Feature Fusion</title><link>http://arxiv.org/abs/2404.03527v1</link><description>Data-fusion networks have shown significant promise for RGB-thermal sceneparsing. However, the majority of existing studies have relied on symmetricduplex encoders for heterogeneous feature extraction and fusion, payinginadequate attention to the inherent differences between RGB and thermalmodalities. Recent progress in vision foundation models (VFMs) trained throughself-supervision on vast amounts of unlabeled data has proven their ability toextract informative, general-purpose features. However, this potential has yetto be fully leveraged in the domain. In this study, we take one step towardthis new research area by exploring a feasible strategy to fully exploit VFMfeatures for RGB-thermal scene parsing. Specifically, we delve deeper into theunique characteristics of RGB and thermal modalities, thereby designing ahybrid, asymmetric encoder that incorporates both a VFM and a convolutionalneural network. This design allows for more effective extraction ofcomplementary heterogeneous features, which are subsequently fused in adual-path, progressive manner. Moreover, we introduce an auxiliary task tofurther enrich the local semantics of the fused features, thereby improving theoverall performance of RGB-thermal scene parsing. Our proposed HAPNet, equippedwith all these components, demonstrates superior performance compared to allother state-of-the-art RGB-thermal scene parsing networks, achieving top ranksacross three widely used public RGB-thermal scene parsing datasets. We believethis new paradigm has opened up new opportunities for future developments indata-fusion scene parsing approaches.</description><author>Jiahang Li, Peng Yun, Qijun Chen, Rui Fan</author><pubDate>Thu, 04 Apr 2024 16:31:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03527v1</guid></item><item><title>SemEval Task 1: Semantic Textual Relatedness for African and Asian Languages</title><link>http://arxiv.org/abs/2403.18933v4</link><description>We present the first shared task on Semantic Textual Relatedness (STR). Whileearlier shared tasks primarily focused on semantic similarity, we insteadinvestigate the broader phenomenon of semantic relatedness across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian,Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi,Spanish, and Telugu. These languages originate from five distinct languagefamilies and are predominantly spoken in Africa and Asia -- regionscharacterised by the relatively limited availability of NLP resources. Eachinstance in the datasets is a sentence pair associated with a score thatrepresents the degree of semantic textual relatedness between the twosentences. Participating systems were asked to rank sentence pairs by theircloseness in meaning (i.e., their degree of semantic relatedness) in the 14languages in three main tracks: (a) supervised, (b) unsupervised, and (c)crosslingual. The task attracted 163 participants. We received 70 submissionsin total (across all tasks) from 51 different teams, and 38 system descriptionpapers. We report on the best-performing systems as well as the most common andthe most effective approaches for the three different tracks.</description><author>Nedjma Ousidhoum, Shamsuddeen Hassan Muhammad, Mohamed Abdalla, Idris Abdulmumin, Ibrahim Said Ahmad, Sanchit Ahuja, Alham Fikri Aji, Vladimir Araujo, Meriem Beloucif, Christine De Kock, Oumaima Hourrane, Manish Shrivastava, Thamar Solorio, Nirmal Surange, Krishnapriya Vishnubhotla, Seid Muhie Yimam, Saif M. Mohammad</author><pubDate>Thu, 04 Apr 2024 16:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18933v4</guid></item><item><title>Approximate Gradient Coding for Privacy-Flexible Federated Learning with Non-IID Data</title><link>http://arxiv.org/abs/2404.03524v1</link><description>This work focuses on the challenges of non-IID data and stragglers/dropoutsin federated learning. We introduce and explore a privacy-flexible paradigmthat models parts of the clients' local data as non-private, offering a moreversatile and business-oriented perspective on privacy. Within this framework,we propose a data-driven strategy for mitigating the effects of labelheterogeneity and client straggling on federated learning. Our solutioncombines both offline data sharing and approximate gradient coding techniques.Through numerical simulations using the MNIST dataset, we demonstrate that ourapproach enables achieving a deliberate trade-off between privacy and utility,leading to improved model convergence and accuracy while using an adaptableportion of non-private data.</description><author>Okko Makkonen, Sampo Niemelä, Camilla Hollanti, Serge Kas Hanna</author><pubDate>Thu, 04 Apr 2024 16:29:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03524v1</guid></item><item><title>Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation</title><link>http://arxiv.org/abs/2309.12545v2</link><description>Counterfactual Explanations (CEs) have received increasing interest as amajor methodology for explaining neural network classifiers. Usually, CEs foran input-output pair are defined as data points with minimum distance to theinput that are classified with a different label than the output. To tackle theestablished problem that CEs are easily invalidated when model parameters areupdated (e.g. retrained), studies have proposed ways to certify the robustnessof CEs under model parameter changes bounded by a norm ball. However, existingmethods targeting this form of robustness are not sound or complete, and theymay generate implausible CEs, i.e., outliers wrt the training dataset. In fact,no existing method simultaneously optimises for closeness and plausibilitywhile preserving robustness guarantees. In this work, we propose ProvablyRObust and PLAusible Counterfactual Explanations (PROPLACE), a methodleveraging on robust optimisation techniques to address the aforementionedlimitations in the literature. We formulate an iterative algorithm to computeprovably robust CEs and prove its convergence, soundness and completeness.Through a comparative experiment involving six baselines, five of which targetrobustness, we show that PROPLACE achieves state-of-the-art performancesagainst metrics on three evaluation aspects.</description><author>Junqi Jiang, Jianglin Lan, Francesco Leofante, Antonio Rago, Francesca Toni</author><pubDate>Thu, 04 Apr 2024 16:29:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12545v2</guid></item><item><title>SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation</title><link>http://arxiv.org/abs/2404.03518v1</link><description>Recently, transformer-based methods have achieved state-of-the-art predictionquality on human pose estimation(HPE). Nonetheless, most of thesetop-performing transformer-based models are too computation-consuming andstorage-demanding to deploy on edge computing platforms. Thosetransformer-based models that require fewer resources are prone tounder-fitting due to their smaller scale and thus perform notably worse thantheir larger counterparts. Given this conundrum, we introduce SDPose, a newself-distillation method for improving the performance of smalltransformer-based models. To mitigate the problem of under-fitting, we design atransformer module named Multi-Cycled Transformer(MCT) based on multiple-cycledforwards to more fully exploit the potential of small model parameters.Further, in order to prevent the additional inference compute-consuming broughtby MCT, we introduce a self-distillation scheme, extracting the knowledge fromthe MCT module to a naive forward model. Specifically, on the MSCOCO validationdataset, SDPose-T obtains 69.7% mAP with 4.4M parameters and 1.8 GFLOPs.Furthermore, SDPose-S-V2 obtains 73.5% mAP on the MSCOCO validation datasetwith 6.2M parameters and 4.7 GFLOPs, achieving a new state-of-the-art amongpredominant tiny neural network methods. Our code is available athttps://github.com/MartyrPenink/SDPose.</description><author>Sichen Chen, Yingyi Zhang, Siming Huang, Ran Yi, Ke Fan, Ruixin Zhang, Peixian Chen, Jun Wang, Shouhong Ding, Lizhuang Ma</author><pubDate>Thu, 04 Apr 2024 16:23:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03518v1</guid></item><item><title>Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive Model-Aware Approach</title><link>http://arxiv.org/abs/2404.03514v1</link><description>Retrieval-augmented large language models (LLMs) have been remarkablycompetent in various NLP tasks. Despite their great success, the knowledgeprovided by the retrieval process is not always useful for improving the modelprediction, since in some samples LLMs may already be quite knowledgeable andthus be able to answer the question correctly without retrieval. Aiming to savethe cost of retrieval, previous work has proposed to determine when to do/skipthe retrieval in a data-aware manner by analyzing the LLMs' pretraining data.However, these data-aware methods pose privacy risks and memory limitations,especially when requiring access to sensitive or extensive pretraining data.Moreover, these methods offer limited adaptability under fine-tuning orcontinual learning settings. We hypothesize that token embeddings are able tocapture the model's intrinsic knowledge, which offers a safer and morestraightforward way to judge the need for retrieval without the privacy risksassociated with accessing pre-training data. Moreover, it alleviates the needto retain all the data utilized during model pre-training, necessitating onlythe upkeep of the token embeddings. Extensive experiments and in-depth analysesdemonstrate the superiority of our model-aware approach.</description><author>Chengkai Huang, Rui Wang, Kaige Xie, Tong Yu, Lina Yao</author><pubDate>Thu, 04 Apr 2024 16:21:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03514v1</guid></item><item><title>RELIC: Investigating Large Language Model Responses using Self-Consistency</title><link>http://arxiv.org/abs/2311.16842v2</link><description>Large Language Models (LLMs) are notorious for blending fact with fiction andgenerating non-factual content, known as hallucinations. To address thischallenge, we propose an interactive system that helps users gain insight intothe reliability of the generated text. Our approach is based on the idea thatthe self-consistency of multiple samples generated by the same LLM relates toits confidence in individual claims in the generated texts. Using this idea, wedesign RELIC, an interactive system that enables users to investigate andverify semantic-level variations in multiple long-form responses. This allowsusers to recognize potentially inaccurate information in the generated text andmake necessary corrections. From a user study with ten participants, wedemonstrate that our approach helps users better verify the reliability of thegenerated text. We further summarize the design implications and lessonslearned from this research for future studies of reliable human-LLMinteractions.</description><author>Furui Cheng, Vilém Zouhar, Simran Arora, Mrinmaya Sachan, Hendrik Strobelt, Mennatallah El-Assady</author><pubDate>Thu, 04 Apr 2024 16:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16842v2</guid></item><item><title>CMB: A Comprehensive Medical Benchmark in Chinese</title><link>http://arxiv.org/abs/2308.08833v2</link><description>Large Language Models (LLMs) provide a possibility to make a greatbreakthrough in medicine. The establishment of a standardized medical benchmarkbecomes a fundamental cornerstone to measure progression. However, medicalenvironments in different regions have their local characteristics, e.g., theubiquity and significance of traditional Chinese medicine within China.Therefore, merely translating English-based medical evaluation may result in\textit{contextual incongruities} to a local region. To solve the issue, wepropose a localized medical benchmark called CMB, a Comprehensive MedicalBenchmark in Chinese, designed and rooted entirely within the native Chineselinguistic and cultural framework. While traditional Chinese medicine isintegral to this evaluation, it does not constitute its entirety. Using thisbenchmark, we have evaluated several prominent large-scale LLMs, includingChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medicaldomain. We hope this benchmark provide first-hand experience in existing LLMsfor medicine and also facilitate the widespread adoption and enhancement ofmedical LLMs within China. Our data and code are publicly available athttps://github.com/FreedomIntelligence/CMB.</description><author>Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, Haizhou Li</author><pubDate>Thu, 04 Apr 2024 16:16:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08833v2</guid></item><item><title>DQ-DETR: DETR with Dynamic Query for Tiny Object Detection</title><link>http://arxiv.org/abs/2404.03507v1</link><description>Despite previous DETR-like methods having performed successfully in genericobject detection, tiny object detection is still a challenging task for themsince the positional information of object queries is not customized fordetecting tiny objects, whose scale is extraordinarily smaller than generalobjects. Also, DETR-like methods using a fixed number of queries make themunsuitable for aerial datasets, which only contain tiny objects, and thenumbers of instances are imbalanced between different images. Thus, we presenta simple yet effective model, named DQ-DETR, which consists of three differentcomponents: categorical counting module, counting-guided feature enhancement,and dynamic query selection to solve the above-mentioned problems. DQ-DETR usesthe prediction and density maps from the categorical counting module todynamically adjust the number of object queries and improve the positionalinformation of queries. Our model DQ-DETR outperforms previous CNN-based andDETR-like methods, achieving state-of-the-art mAP 30.2% on the AI-TOD-V2dataset, which mostly consists of tiny objects.</description><author>Yi-Xin Huang, Hou-I Liu, Hong-Han Shuai, Wen-Huang Cheng</author><pubDate>Thu, 04 Apr 2024 16:10:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03507v1</guid></item><item><title>NEMTO: Neural Environment Matting for Novel View and Relighting Synthesis of Transparent Objects</title><link>http://arxiv.org/abs/2303.11963v2</link><description>We propose NEMTO, the first end-to-end neural rendering pipeline to model 3Dtransparent objects with complex geometry and unknown indices of refraction.Commonly used appearance modeling such as the Disney BSDF model cannotaccurately address this challenging problem due to the complex light pathsbending through refractions and the strong dependency of surface appearance onillumination. With 2D images of the transparent object as input, our method iscapable of high-quality novel view and relighting synthesis. We leverageimplicit Signed Distance Functions (SDF) to model the object geometry andpropose a refraction-aware ray bending network to model the effects of lightrefraction within the object. Our ray bending network is more tolerant togeometric inaccuracies than traditional physically-based methods for renderingtransparent objects. We provide extensive evaluations on both synthetic andreal-world datasets to demonstrate our high-quality synthesis and theapplicability of our method.</description><author>Dongqing Wang, Tong Zhang, Sabine Süsstrunk</author><pubDate>Thu, 04 Apr 2024 16:10:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11963v2</guid></item><item><title>CountARFactuals -- Generating plausible model-agnostic counterfactual explanations with adversarial random forests</title><link>http://arxiv.org/abs/2404.03506v1</link><description>Counterfactual explanations elucidate algorithmic decisions by pointing toscenarios that would have led to an alternative, desired outcome. Givinginsight into the model's behavior, they hint users towards possible actions andgive grounds for contesting decisions. As a crucial factor in achieving thesegoals, counterfactuals must be plausible, i.e., describing realisticalternative scenarios within the data manifold. This paper leverages a recentlydeveloped generative modeling technique -- adversarial random forests (ARFs) --to efficiently generate plausible counterfactuals in a model-agnostic way. ARFscan serve as a plausibility measure or directly generate counterfactualexplanations. Our ARF-based approach surpasses the limitations of existingmethods that aim to generate plausible counterfactual explanations: It is easyto train and computationally highly efficient, handles continuous andcategorical data naturally, and allows integrating additional desiderata suchas sparsity in a straightforward manner.</description><author>Susanne Dandl, Kristin Blesch, Timo Freiesleben, Gunnar König, Jan Kapar, Bernd Bischl, Marvin Wright</author><pubDate>Thu, 04 Apr 2024 16:10:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03506v1</guid></item><item><title>AI and the Problem of Knowledge Collapse</title><link>http://arxiv.org/abs/2404.03502v1</link><description>While artificial intelligence has the potential to process vast amounts ofdata, generate new insights, and unlock greater productivity, its widespreadadoption may entail unforeseen consequences. We identify conditions under whichAI, by reducing the cost of access to certain modes of knowledge, canparadoxically harm public understanding. While large language models aretrained on vast amounts of diverse data, they naturally generate output towardsthe 'center' of the distribution. This is generally useful, but widespreadreliance on recursive AI systems could lead to a process we define as"knowledge collapse", and argue this could harm innovation and the richness ofhuman understanding and culture. However, unlike AI models that cannot choosewhat data they are trained on, humans may strategically seek out diverse formsof knowledge if they perceive them to be worthwhile. To investigate this, weprovide a simple model in which a community of learners or innovators choose touse traditional methods or to rely on a discounted AI-assisted process andidentify conditions under which knowledge collapse occurs. In our defaultmodel, a 20% discount on AI-generated content generates public beliefs 2.3times further from the truth than when there is no discount. Finally, based onthe results, we consider further research directions to counteract suchoutcomes.</description><author>Andrew J. Peterson</author><pubDate>Thu, 04 Apr 2024 16:06:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03502v1</guid></item><item><title>3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2312.09228v3</link><description>We introduce an approach that creates animatable human avatars from monocularvideos using 3D Gaussian Splatting (3DGS). Existing methods based on neuralradiance fields (NeRFs) achieve high-quality novel-view/novel-pose imagesynthesis but often require days of training, and are extremely slow atinference time. Recently, the community has explored fast grid structures forefficient training of clothed avatars. Albeit being extremely fast at training,these methods can barely achieve an interactive rendering frame rate witharound 15 FPS. In this paper, we use 3D Gaussian Splatting and learn anon-rigid deformation network to reconstruct animatable clothed human avatarsthat can be trained within 30 minutes and rendered at real-time frame rates(50+ FPS). Given the explicit nature of our representation, we furtherintroduce as-isometric-as-possible regularizations on both the Gaussian meanvectors and the covariance matrices, enhancing the generalization of our modelon highly articulated unseen poses. Experimental results show that our methodachieves comparable and even better performance compared to state-of-the-artapproaches on animatable avatar creation from a monocular input, while being400x and 250x faster in training and inference, respectively.</description><author>Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, Siyu Tang</author><pubDate>Thu, 04 Apr 2024 16:06:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09228v3</guid></item><item><title>Comprehensible Artificial Intelligence on Knowledge Graphs: A survey</title><link>http://arxiv.org/abs/2404.03499v1</link><description>Artificial Intelligence applications gradually move outside the safe walls ofresearch labs and invade our daily lives. This is also true for MachineLearning methods on Knowledge Graphs, which has led to a steady increase intheir application since the beginning of the 21st century. However, in manyapplications, users require an explanation of the Artificial Intelligencesdecision. This led to increased demand for Comprehensible ArtificialIntelligence. Knowledge Graphs epitomize fertile soil for ComprehensibleArtificial Intelligence, due to their ability to display connected data, i.e.knowledge, in a human- as well as machine-readable way. This survey gives ashort history to Comprehensible Artificial Intelligence on Knowledge Graphs.Furthermore, we contribute by arguing that the concept Explainable ArtificialIntelligence is overloaded and overlapping with Interpretable Machine Learning.By introducing the parent concept Comprehensible Artificial Intelligence, weprovide a clear-cut distinction of both concepts while accounting for theirsimilarities. Thus, we provide in this survey a case for ComprehensibleArtificial Intelligence on Knowledge Graphs consisting of Interpretable MachineLearning on Knowledge Graphs and Explainable Artificial Intelligence onKnowledge Graphs. This leads to the introduction of a novel taxonomy forComprehensible Artificial Intelligence on Knowledge Graphs. In addition, acomprehensive overview of the research on Comprehensible ArtificialIntelligence on Knowledge Graphs is presented and put into the context of thetaxonomy. Finally, research gaps in the field of Comprehensible ArtificialIntelligence on Knowledge Graphs are identified for future research.</description><author>Simon Schramm, Christoph Wehner, Ute Schmid</author><pubDate>Thu, 04 Apr 2024 15:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03499v1</guid></item><item><title>As Good As A Coin Toss: Human detection of AI-generated images, videos, audio, and audiovisual stimuli</title><link>http://arxiv.org/abs/2403.16760v3</link><description>As synthetic media becomes progressively more realistic and barriers to usingit continue to lower, the technology has been increasingly utilized formalicious purposes, from financial fraud to nonconsensual pornography. Today,the principal defense against being misled by synthetic media relies on theability of the human observer to visually and auditorily discern between realand fake. However, it remains unclear just how vulnerable people actually areto deceptive synthetic media in the course of their day to day lives. Weconducted a perceptual study with 1276 participants to assess how accuratepeople were at distinguishing synthetic images, audio only, video only, andaudiovisual stimuli from authentic. To reflect the circumstances under whichpeople would likely encounter synthetic media in the wild, testing conditionsand stimuli emulated a typical online platform, while all synthetic media usedin the survey was sourced from publicly accessible generative AI technology. We find that overall, participants struggled to meaningfully discern betweensynthetic and authentic content. We also find that detection performanceworsens when the stimuli contains synthetic content as compared to authenticcontent, images featuring human faces as compared to non face objects, a singlemodality as compared to multimodal stimuli, mixed authenticity as compared tobeing fully synthetic for audiovisual stimuli, and features foreign languagesas compared to languages the observer is fluent in. Finally, we also find thatprior knowledge of synthetic media does not meaningfully impact their detectionperformance. Collectively, these results indicate that people are highlysusceptible to being tricked by synthetic media in their daily lives and thathuman perceptual detection capabilities can no longer be relied upon as aneffective counterdefense.</description><author>Di Cooke, Abigail Edwards, Sophia Barkoff, Kathryn Kelly</author><pubDate>Thu, 04 Apr 2024 15:51:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16760v3</guid></item><item><title>About Test-time training for outlier detection</title><link>http://arxiv.org/abs/2404.03495v1</link><description>In this paper, we introduce DOUST, our method applying test-time training foroutlier detection, significantly improving the detection performance. Afterthoroughly evaluating our algorithm on common benchmark datasets, we discuss acommon problem and show that it disappears with a large enough test set. Thus,we conclude that under reasonable conditions, our algorithm can reach almostsupervised performance even when no labeled outliers are given.</description><author>Simon Klüttermann, Emmanuel Müller</author><pubDate>Thu, 04 Apr 2024 15:50:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03495v1</guid></item><item><title>A Methodology to Study the Impact of Spiking Neural Network Parameters considering Event-Based Automotive Data</title><link>http://arxiv.org/abs/2404.03493v1</link><description>Autonomous Driving (AD) systems are considered as the future of humanmobility and transportation. Solving computer vision tasks such as imageclassification and object detection/segmentation, with high accuracy and lowpower/energy consumption, is highly needed to realize AD systems in real life.These requirements can potentially be satisfied by Spiking Neural Networks(SNNs). However, the state-of-the-art works in SNN-based AD systems still focuson proposing network models that can achieve high accuracy, and they have notsystematically studied the roles of SNN parameters when used for learningevent-based automotive data. Therefore, we still lack understanding of how toeffectively develop SNN models for AD systems. Toward this, we propose a novelmethodology to systematically study and analyze the impact of SNN parametersconsidering event-based automotive data, then leverage this analysis forenhancing SNN developments. To do this, we first explore different settings ofSNN parameters that directly affect the learning mechanism (i.e., batch size,learning rate, neuron threshold potential, and weight decay), then analyze theaccuracy results. Afterward, we propose techniques that jointly improve SNNaccuracy and reduce training time. Experimental results show that ourmethodology can improve the SNN models for AD systems than thestate-of-the-art, as it achieves higher accuracy (i.e., 86%) for the NCARSdataset, and it can also achieve iso-accuracy (i.e., ~85% with standarddeviation less than 0.5%) while speeding up the training time by 1.9x. In thismanner, our research work provides a set of guidelines for SNN parameterenhancements, thereby enabling the practical developments of SNN-based ADsystems.</description><author>Iqra Bano, Rachmad Vidya Wicaksana Putra, Alberto Marchisio, Muhammad Shafique</author><pubDate>Thu, 04 Apr 2024 15:48:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03493v1</guid></item><item><title>A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation</title><link>http://arxiv.org/abs/2404.03491v1</link><description>Empowered by the large-scale pretrained language models, existing dialoguesystems have demonstrated impressive performance conducting fluent andnatural-sounding conversations. However, they are still plagued by thehallucination problem, causing unpredictable factual errors in the generatedresponses. Recently, knowledge-grounded dialogue generation models, thatintentionally invoke external knowledge resources to more informativeresponses, are also proven to be effective in reducing hallucination. Followingthe idea of getting high-quality knowledge, a few efforts have achieved prettygood performance on this issue. As some inevitable knowledge noises may alsolead to hallucinations, it is emergent to investigate the reason and futuredirections for building noise-tolerant methods in KGD tasks. In this paper, weanalyze the causal story behind this problem with counterfactual reasoningmethods. Based on the causal effect analysis, we propose a possible solutionfor alleviating the hallucination in KGD by exploiting the dialogue-knowledgeinteraction. Experimental results of our example implementation show that thismethod can reduce hallucination without disrupting other dialogue performance,while keeping adaptive to different generation models. We hope our efforts cansupport and call for more attention to developing lightweight techniquestowards robust and trusty dialogue systems.</description><author>Jifan Yu, Xiaohan Zhang, Yifan Xu, Xuanyu Lei, Zijun Yao, Jing Zhang, Lei Hou, Juanzi Li</author><pubDate>Thu, 04 Apr 2024 15:45:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03491v1</guid></item><item><title>ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D</title><link>http://arxiv.org/abs/2403.19612v2</link><description>Effective recognition of spatial patterns and learning their hierarchy iscrucial in modern spatial data analysis. Volumetric data applications seektechniques ensuring invariance not only to shifts but also to patternrotations. While traditional methods can readily achieve translationalinvariance, rotational invariance possesses multiple challenges and remains anactive area of research. Here, we present ILPO-Net (Invariant to Local PatternsOrientation Network), a novel approach that handles arbitrarily shaped patternswith the convolutional operation inherently invariant to local spatial patternorientations using the Wigner matrix expansions. Our architecture seamlesslyintegrates the new convolution operator and, when benchmarked on diversevolumetric datasets such as MedMNIST and CATH, demonstrates superiorperformance over the baselines with significantly reduced parameter counts - upto 1000 times fewer in the case of MedMNIST. Beyond these demonstrations,ILPO-Net's rotational invariance paves the way for other applications acrossmultiple disciplines. Our code is publicly available athttps://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet.</description><author>Dmitrii Zhemchuzhnikov, Sergei Grudinin</author><pubDate>Thu, 04 Apr 2024 15:44:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19612v2</guid></item><item><title>Bootstrapping SparseFormers from Vision Foundation Models</title><link>http://arxiv.org/abs/2312.01987v2</link><description>The recently proposed SparseFormer architecture provides an alternativeapproach to visual understanding by utilizing a significantly lower number ofvisual tokens via adjusting RoIs, greatly reducing computational costs whilestill achieving promising performance. However, training SparseFormers fromscratch is still expensive, and scaling up the number of parameters can bechallenging. In this paper, we propose to bootstrap SparseFormers fromViT-based vision foundation models in a simple and efficient way. Since themajority of SparseFormer blocks are the standard transformer ones, we caninherit weights from large-scale pre-trained vision transformers and freezethem as much as possible. Therefore, we only need to train theSparseFormer-specific lightweight focusing transformer to adjust token RoIs andfine-tune a few early pre-trained blocks to align the final tokenrepresentation. In such a way, we can bootstrap SparseFormer architectures fromvarious large-scale pre-trained models (e.g., IN-21K pre-trained AugRegs orCLIPs) using a rather smaller amount of training samples (e.g., IN-1K) andwithout labels or captions within just a few hours. As a result, thebootstrapped unimodal SparseFormer (from AugReg-ViT-L/16-384) can reach 84.9%accuracy on IN-1K with only 49 tokens, and the multimodal SparseFormer fromCLIPs also demonstrates notable zero-shot performance with highly reducedcomputational cost without seeing any caption during the bootstrappingprocedure. In addition, CLIP-bootstrapped SparseFormers, which align the outputspace with language without seeing a word, can serve as efficient visionencoders in multimodal large language models. Code and models are available athttps://github.com/showlab/sparseformer</description><author>Ziteng Gao, Zhan Tong, Kevin Qinghong Lin, Joya Chen, Mike Zheng Shou</author><pubDate>Thu, 04 Apr 2024 15:40:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01987v2</guid></item><item><title>Generative AI and Teachers -- For Us or Against Us? A Case Study</title><link>http://arxiv.org/abs/2404.03486v1</link><description>We present insightful results of a survey on the adoption of generativeartificial intelligence (GenAI) by university teachers in their teachingactivities. The transformation of education by GenAI, particularly largelanguage models (LLMs), has been presenting both opportunities and challenges,including cheating by students. We prepared the online survey according to bestpractices and the questions were created by the authors, who have pedagogyexperience. The survey contained 12 questions and a pilot study was firstconducted. The survey was then sent to all teachers in multiple departmentsacross different campuses of the university of interest in Sweden: Lule{\aa}University of Technology. The survey was available in both Swedish and English.The results show that 35 teachers (more than half) use GenAI out of 67respondents. Preparation is the teaching activity with the most frequency thatGenAI is used for and ChatGPT is the most commonly used GenAI. 59% say it hasimpacted their teaching, however, 55% say there should be legislation aroundthe use of GenAI, especially as inaccuracies and cheating are the biggestconcerns.</description><author>Jenny Pettersson, Elias Hult, Tim Eriksson, Tosin Adewumi</author><pubDate>Thu, 04 Apr 2024 15:40:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03486v1</guid></item><item><title>AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale</title><link>http://arxiv.org/abs/2404.03482v1</link><description>Active Visual Exploration (AVE) is a task that involves dynamically selectingobservations (glimpses), which is critical to facilitate comprehension andnavigation within an environment. While modern AVE methods have demonstratedimpressive performance, they are constrained to fixed-scale glimpses from rigidgrids. In contrast, existing mobile platforms equipped with optical zoomcapabilities can capture glimpses of arbitrary positions and scales. To addressthis gap between software and hardware capabilities, we introduce AdaGlimpse.It uses Soft Actor-Critic, a reinforcement learning algorithm tailored forexploration tasks, to select glimpses of arbitrary position and scale. Thisapproach enables our model to rapidly establish a general awareness of theenvironment before zooming in for detailed analysis. Experimental resultsdemonstrate that AdaGlimpse surpasses previous methods across various visualtasks while maintaining greater applicability in realistic AVE scenarios.</description><author>Adam Pardyl, Michał Wronka, Maciej Wołczyk, Kamil Adamczewski, Tomasz Trzciński, Bartosz Zieliński</author><pubDate>Thu, 04 Apr 2024 15:35:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03482v1</guid></item><item><title>Reinforcement learning-based estimation for partial differential equations</title><link>http://arxiv.org/abs/2302.01189v2</link><description>In systems governed by nonlinear partial differential equations such as fluidflows, the design of state estimators such as Kalman filters relies on areduced-order model (ROM) that projects the original high-dimensional dynamicsonto a computationally tractable low-dimensional space. However, ROMs are proneto large errors, which negatively affects the performance of the estimator.Here, we introduce the reinforcement learning reduced-order estimator (RL-ROE),a ROM-based estimator in which the correction term that takes in themeasurements is given by a nonlinear policy trained through reinforcementlearning. The nonlinearity of the policy enables the RL-ROE to compensateefficiently for errors of the ROM, while still taking advantage of theimperfect knowledge of the dynamics. Using examples involving the Burgers andNavier-Stokes equations, we show that in the limit of very few sensors, thetrained RL-ROE outperforms a Kalman filter designed using the same ROM.Moreover, it yields accurate high-dimensional state estimates for trajectoriescorresponding to various physical parameter values, without direct knowledge ofthe latter.</description><author>Saviz Mowlavi, Mouhacine Benosman</author><pubDate>Thu, 04 Apr 2024 15:35:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01189v2</guid></item><item><title>EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention</title><link>http://arxiv.org/abs/2403.17729v2</link><description>To capture user preference, transformer models have been widely applied tomodel sequential user behavior data. The core of transformer architecture liesin the self-attention mechanism, which computes the pairwise attention scoresin a sequence. Due to the permutation-equivariant nature, positional encodingis used to enhance the attention between token representations. In thissetting, the pairwise attention scores can be derived by both semanticdifference and positional difference. However, prior studies often model thetwo kinds of difference measurements in different ways, which potentiallylimits the expressive capacity of sequence modeling. To address this issue,this paper proposes a novel transformer variant with complex vector attention,named EulerFormer, which provides a unified theoretical framework to formulateboth semantic difference and positional difference. The EulerFormer involvestwo key technical improvements. First, it employs a new transformation functionfor efficiently transforming the sequence tokens into polar-form complexvectors using Euler's formula, enabling the unified modeling of both semanticand positional information in a complex rotation form.Secondly, it develops adifferential rotation mechanism, where the semantic rotation angles can becontrolled by an adaptation function, enabling the adaptive integration of thesemantic and positional information according to the semanticcontexts.Furthermore, a phase contrastive learning task is proposed to improvethe isotropy of contextual representations in EulerFormer. Our theoreticalframework possesses a high degree of completeness and generality. It is morerobust to semantic variations and possesses moresuperior theoretical propertiesin principle. Extensive experiments conducted on four public datasetsdemonstrate the effectiveness and efficiency of our approach.</description><author>Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma, Ji-Rong Wen</author><pubDate>Thu, 04 Apr 2024 15:29:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17729v2</guid></item><item><title>Towards Automated Movie Trailer Generation</title><link>http://arxiv.org/abs/2404.03477v1</link><description>Movie trailers are an essential tool for promoting films and attractingaudiences. However, the process of creating trailers can be time-consuming andexpensive. To streamline this process, we propose an automatic trailergeneration framework that generates plausible trailers from a full movie byautomating shot selection and composition. Our approach draws inspiration frommachine translation techniques and models the movies and trailers as sequencesof shots, thus formulating the trailer generation problem as asequence-to-sequence task. We introduce Trailer Generation Transformer (TGT), adeep-learning framework utilizing an encoder-decoder architecture. TGT movieencoder is tasked with contextualizing each movie shot representation viaself-attention, while the autoregressive trailer decoder predicts the featurerepresentation of the next trailer shot, accounting for the relevance of shots'temporal order in trailers. Our TGT significantly outperforms previous methodson a comprehensive suite of metrics.</description><author>Dawit Mureja Argaw, Mattia Soldan, Alejandro Pardo, Chen Zhao, Fabian Caba Heilbron, Joon Son Chung, Bernard Ghanem</author><pubDate>Thu, 04 Apr 2024 15:28:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03477v1</guid></item><item><title>Performance of computer vision algorithms for fine-grained classification using crowdsourced insect images</title><link>http://arxiv.org/abs/2404.03474v1</link><description>With fine-grained classification, we identify unique characteristics todistinguish among classes of the same super-class. We are focusing on speciesrecognition in Insecta, as they are critical for biodiversity monitoring and atthe base of many ecosystems. With citizen science campaigns, billions of imagesare collected in the wild. Once these are labelled, experts can use them tocreate distribution maps. However, the labelling process is time-consuming,which is where computer vision comes in. The field of computer vision offers awide range of algorithms, each with its strengths and weaknesses; how do weidentify the algorithm that is in line with our application? To answer thisquestion, we provide a full and detailed evaluation of nine algorithms amongdeep convolutional networks (CNN), vision transformers (ViT), andlocality-based vision transformers (LBVT) on 4 different aspects:classification performance, embedding quality, computational cost, and gradientactivity. We offer insights that we haven't yet had in this domain proving towhich extent these algorithms solve the fine-grained tasks in Insecta. We foundthat the ViT performs the best on inference speed and computational cost whilethe LBVT outperforms the others on performance and embedding quality; the CNNprovide a trade-off among the metrics.</description><author>Rita Pucci, Vincent J. Kalkman, Dan Stowell</author><pubDate>Thu, 04 Apr 2024 15:26:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03474v1</guid></item><item><title>Generalization Bounds for Message Passing Networks on Mixture of Graphons</title><link>http://arxiv.org/abs/2404.03473v1</link><description>We study the generalization capabilities of Message Passing Neural Networks(MPNNs), a prevalent class of Graph Neural Networks (GNN). We derivegeneralization bounds specifically for MPNNs with normalized sum aggregationand mean aggregation. Our analysis is based on a data generation modelincorporating a finite set of template graphons. Each graph within thisframework is generated by sampling from one of the graphons with a certaindegree of perturbation. In particular, we extend previous MPNN generalizationresults to a more realistic setting, which includes the followingmodifications: 1) we analyze simple random graphs with Bernoulli-distributededges instead of weighted graphs; 2) we sample both graphs and graph signalsfrom perturbed graphons instead of clean graphons; and 3) we analyze sparsegraphs instead of dense graphs. In this more realistic and challengingscenario, we provide a generalization bound that decreases as the averagenumber of nodes in the graphs increases. Our results imply that MPNNs withhigher complexity than the size of the training set can still generalizeeffectively, as long as the graphs are sufficiently large.</description><author>Sohir Maskey, Gitta Kutyniok, Ron Levie</author><pubDate>Thu, 04 Apr 2024 15:26:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03473v1</guid></item><item><title>Challenges for Reinforcement Learning in Quantum Circuit Design</title><link>http://arxiv.org/abs/2312.11337v2</link><description>Quantum computing (QC) in the current NISQ era is still limited in size andprecision. Hybrid applications mitigating those shortcomings are prevalent togain early insight and advantages. Hybrid quantum machine learning (QML)comprises both the application of QC to improve machine learning (ML) and ML toimprove QC architectures. This work considers the latter, leveragingreinforcement learning (RL) to improve the search for viable quantumarchitectures, which we formalize by a set of generic challenges. Furthermore,we propose a concrete framework, formalized as a Markov decision process, toenable learning policies capable of controlling a universal set of continuouslyparameterized quantum gates. Finally, we provide benchmark comparisons toassess the shortcomings and strengths of current state-of-the-art RLalgorithms.</description><author>Philipp Altmann, Jonas Stein, Michael Kölle, Adelina Bärligea, Thomas Gabor, Thomy Phan, Sebastian Feld, Claudia Linnhoff-Popien</author><pubDate>Thu, 04 Apr 2024 15:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11337v2</guid></item><item><title>Reevaluating Bias Detection in Language Models: The Role of Implicit Norm</title><link>http://arxiv.org/abs/2404.03471v1</link><description>Large language models (LLMs), trained on vast datasets, can carry biases thatmanifest in various forms, from overt discrimination to implicit stereotypes.One facet of bias is performance disparities in LLMs, often harmingunderprivileged groups, such as racial minorities. A common approach toquantifying bias is to use template-based bias probes, which explicitly stategroup membership (e.g. White) and evaluate if the outcome of a task, sentimentanalysis for instance, is invariant to the change of group membership (e.g.change White race to Black). This approach is widely used in biasquantification. However, in this work, we find evidence of an unexpectedlyoverlooked consequence of using template-based probes for LLM biasquantification. We find that in doing so, text examples associated with Whiteethnicities appear to be classified as exhibiting negative sentiment atelevated rates. We hypothesize that the scenario arises artificially through amismatch between the pre-training text of LLMs and the templates used tomeasure bias through reporting bias, unstated norms that imply group membershipwithout explicit statement. Our finding highlights the potential misleadingimpact of varying group membership through explicit mention in biasquantification</description><author>Farnaz Kohankhaki, Jacob-Junqi Tian, David Emerson, Laleh Seyyed-Kalantari, Faiza Khan Khattak</author><pubDate>Thu, 04 Apr 2024 15:24:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03471v1</guid></item></channel></rss>