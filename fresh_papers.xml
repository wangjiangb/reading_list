<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 20 Aug 2023 06:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>TeCH: Text-guided Reconstruction of Lifelike Clothed Humans</title><link>http://arxiv.org/abs/2308.08545v1</link><description>Despite recent research advancements in reconstructing clothed humans from asingle image, accurately restoring the "unseen regions" with high-level detailsremains an unsolved challenge that lacks attention. Existing methods oftengenerate overly smooth back-side surfaces with a blurry texture. But how toeffectively capture all visual attributes of an individual from a single image,which are sufficient to reconstruct unseen areas (e.g., the back view)?Motivated by the power of foundation models, TeCH reconstructs the 3D human byleveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles)which are automatically generated via a garment parsing model and VisualQuestion Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusionmodel (T2I) which learns the "indescribable" appearance. To representhigh-resolution 3D clothed humans at an affordable cost, we propose a hybrid 3Drepresentation based on DMTet, which consists of an explicit body shape gridand an implicit distance field. Guided by the descriptive prompts +personalized T2I diffusion model, the geometry and texture of the 3D humans areoptimized through multi-view Score Distillation Sampling (SDS) andreconstruction losses based on the original observation. TeCH produceshigh-fidelity 3D clothed humans with consistent &amp; delicate texture, anddetailed full-body geometry. Quantitative and qualitative experimentsdemonstrate that TeCH outperforms the state-of-the-art methods in terms ofreconstruction accuracy and rendering quality. The code will be publiclyavailable for research purposes at https://huangyangyi.github.io/tech</description><author>Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Jiaxiang Tang, Deng Cai, Justus Thies</author><pubDate>Wed, 16 Aug 2023 18:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08545v1</guid></item><item><title>LLM-Rec: Personalized Recommendation via Prompting Large Language Models</title><link>http://arxiv.org/abs/2307.15780v2</link><description>We investigate various prompting strategies for enhancing personalizedrecommendation performance with large language models (LLMs) through inputaugmentation. Our proposed approach, termed LLM-Rec, encompasses four distinctprompting strategies: (1) basic prompting, (2) recommendation-driven prompting,(3) engagement-guided prompting, and (4) recommendation-driven +engagement-guided prompting. Our empirical experiments show that incorporatingthe augmented input text generated by LLM leads to improved recommendationperformance. Recommendation-driven and engagement-guided prompting strategiesare found to elicit LLM's understanding of global and local itemcharacteristics. This finding highlights the importance of leveraging diverseprompts and input augmentation techniques to enhance the recommendationcapabilities with LLMs.</description><author>Hanjia Lyu, Song Jiang, Hanqing Zeng, Qifan Wang, Si Zhang, Ren Chen, Chris Leung, Jiajie Tang, Yinglong Xia, Jiebo Luo</author><pubDate>Wed, 16 Aug 2023 18:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15780v2</guid></item><item><title>SHERF: Generalizable Human NeRF from a Single Image</title><link>http://arxiv.org/abs/2303.12791v2</link><description>Existing Human NeRF methods for reconstructing 3D humans typically rely onmultiple 2D images from multi-view cameras or monocular videos captured fromfixed camera views. However, in real-world scenarios, human images are oftencaptured from random camera angles, presenting challenges for high-quality 3Dhuman reconstruction. In this paper, we propose SHERF, the first generalizableHuman NeRF model for recovering animatable 3D humans from a single input image.SHERF extracts and encodes 3D human representations in canonical space,enabling rendering and animation from free views and poses. To achievehigh-fidelity novel view and pose synthesis, the encoded 3D humanrepresentations should capture both global appearance and local fine-grainedtextures. To this end, we propose a bank of 3D-aware hierarchical features,including global, point-level, and pixel-aligned features, to facilitateinformative encoding. Global features enhance the information extracted fromthe single input image and complement the information missing from the partial2D observation. Point-level features provide strong clues of 3D humanstructure, while pixel-aligned features preserve more fine-grained details. Toeffectively integrate the 3D-aware hierarchical feature bank, we design afeature fusion transformer. Extensive experiments on THuman, RenderPeople,ZJU_MoCap, and HuMMan datasets demonstrate that SHERF achieves state-of-the-artperformance, with better generalizability for novel view and pose synthesis.</description><author>Shoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei Yang, Ziwei Liu</author><pubDate>Wed, 16 Aug 2023 18:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12791v2</guid></item><item><title>MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions</title><link>http://arxiv.org/abs/2308.08544v1</link><description>This paper strives for motion expressions guided video segmentation, whichfocuses on segmenting objects in video content based on a sentence describingthe motion of the objects. Existing referring video object datasets typicallyfocus on salient objects and use language expressions that contain excessivestatic attributes that could potentially enable the target object to beidentified in a single frame. These datasets downplay the importance of motionin video content for language-guided video object segmentation. To investigatethe feasibility of using motion expressions to ground and segment objects invideos, we propose a large-scale dataset called MeViS, which contains numerousmotion expressions to indicate target objects in complex environments. Webenchmarked 5 existing referring video object segmentation (RVOS) methods andconducted a comprehensive comparison on the MeViS dataset. The results showthat current RVOS methods cannot effectively address motion expression-guidedvideo segmentation. We further analyze the challenges and propose a baselineapproach for the proposed MeViS dataset. The goal of our benchmark is toprovide a platform that enables the development of effective language-guidedvideo segmentation algorithms that leverage motion expressions as a primary cuefor object segmentation in complex video scenes. The proposed MeViS dataset hasbeen released at https://henghuiding.github.io/MeViS.</description><author>Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Chen Change Loy</author><pubDate>Wed, 16 Aug 2023 18:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08544v1</guid></item><item><title>InsightMapper: A Closer Look at Inner-instance Information for Vectorized High-Definition Mapping</title><link>http://arxiv.org/abs/2308.08543v1</link><description>Vectorized high-definition (HD) maps contain detailed information aboutsurrounding road elements, which are crucial for various downstream tasks inmodern autonomous driving vehicles, such as vehicle planning and control.Recent works have attempted to directly detect the vectorized HD map as a pointset prediction task, resulting in significant improvements in detectionperformance. However, these approaches fail to analyze and exploit theinner-instance correlations between predicted points, impeding furtheradvancements. To address these challenges, we investigate the utilization ofinner-$\textbf{INS}$tance information for vectorized h$\textbf{IGH}$-definitionmapping through $\textbf{T}$ransformers and introduce InsightMapper. This paperpresents three novel designs within InsightMapper that leverage inner-instanceinformation in distinct ways, including hybrid query generation, inner-instancequery fusion, and inner-instance feature aggregation. Comparative experimentsare conducted on the NuScenes dataset, showcasing the superiority of ourproposed method. InsightMapper surpasses previous state-of-the-art (SOTA)methods by 5.78 mAP and 5.12 TOPO, which assess topology correctness.Simultaneously, InsightMapper maintains high efficiency during both trainingand inference phases, resulting in remarkable comprehensive performance. Theproject page for this work is available athttps://tonyxuqaq.github.io/projects/InsightMapper .</description><author>Zhenhua Xu, Kenneth K. Y. Wong, Hengshuang Zhao</author><pubDate>Wed, 16 Aug 2023 18:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08543v1</guid></item><item><title>Normalizing Flows for Human Pose Anomaly Detection</title><link>http://arxiv.org/abs/2211.10946v2</link><description>Video anomaly detection is an ill-posed problem because it relies on manyparameters such as appearance, pose, camera angle, background, and more. Wedistill the problem to anomaly detection of human pose, thus decreasing therisk of nuisance parameters such as appearance affecting the result. Focusingon pose alone also has the side benefit of reducing bias against distinctminority groups. Our model works directly on human pose graph sequences and isexceptionally lightweight (~1K parameters), capable of running on any machineable to run the pose estimation with negligible additional resources. Weleverage the highly compact pose representation in a normalizing flowsframework, which we extend to tackle the unique characteristics ofspatio-temporal pose data and show its advantages in this use case. Thealgorithm is quite general and can handle training data of only normal examplesas well as a supervised setting that consists of labeled normal and abnormalexamples. We report state-of-the-art results on two anomaly detectionbenchmarks - the unsupervised ShanghaiTech dataset and the recent supervisedUBnormal dataset.</description><author>Or Hirschorn, Shai Avidan</author><pubDate>Wed, 16 Aug 2023 18:54:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10946v2</guid></item><item><title>DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars</title><link>http://arxiv.org/abs/2303.09375v3</link><description>We present DINAR, an approach for creating realistic rigged fullbody avatarsfrom single RGB images. Similarly to previous works, our method uses neuraltextures combined with the SMPL-X body model to achieve photo-realistic qualityof avatars while keeping them easy to animate and fast to infer. To restore thetexture, we use a latent diffusion model and show how such model can be trainedin the neural texture space. The use of the diffusion model allows us torealistically reconstruct large unseen regions such as the back of a persongiven the frontal view. The models in our pipeline are trained using 2D imagesand videos only. In the experiments, our approach achieves state-of-the-artrendering quality and good generalization to new poses and viewpoints. Inparticular, the approach improves state-of-the-art on the SnapshotPeople publicbenchmark.</description><author>David Svitov, Dmitrii Gudkov, Renat Bashirov, Victor Lempitsky</author><pubDate>Wed, 16 Aug 2023 18:54:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09375v3</guid></item><item><title>Proprioceptive Learning with Soft Polyhedral Networks</title><link>http://arxiv.org/abs/2308.08538v1</link><description>Proprioception is the "sixth sense" that detects limb postures with motorneurons. It requires a natural integration between the musculoskeletal systemsand sensory receptors, which is challenging among modern robots that aim forlightweight, adaptive, and sensitive designs at a low cost. Here, we presentthe Soft Polyhedral Network with an embedded vision for physical interactions,capable of adaptive kinesthesia and viscoelastic proprioception by learningkinetic features. This design enables passive adaptations to omni-directionalinteractions, visually captured by a miniature high-speed motion trackingsystem embedded inside for proprioceptive learning. The results show that thesoft network can infer real-time 6D forces and torques with accuracies of0.25/0.24/0.35 N and 0.025/0.034/0.006 Nm in dynamic interactions. We alsoincorporate viscoelasticity in proprioception during static adaptation byadding a creep and relaxation modifier to refine the predicted results. Theproposed soft network combines simplicity in design, omni-adaptation, andproprioceptive sensing with high accuracy, making it a versatile solution forrobotics at a low cost with more than 1 million use cycles for tasks such assensitive and competitive grasping, and touch-based geometry reconstruction.This study offers new insights into vision-based proprioception for soft robotsin adaptive grasping, soft manipulation, and human-robot interaction.</description><author>Xiaobo Liu, Xudong Han, Wei Hong, Fang Wan, Chaoyang Song</author><pubDate>Wed, 16 Aug 2023 18:53:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08538v1</guid></item><item><title>Can Transformers Learn Optimal Filtering for Unknown Systems?</title><link>http://arxiv.org/abs/2308.08536v1</link><description>Transformers have demonstrated remarkable success in natural languageprocessing; however, their potential remains mostly unexplored for problemsarising in dynamical systems. In this work, we investigate the optimal outputestimation problem using transformers, which generate output predictions usingall the past ones. We train the transformer using various systems drawn from aprior distribution and then evaluate its performance on previously unseensystems from the same distribution. As a result, the obtained transformer actslike a prediction algorithm that learns in-context and quickly adapts to andpredicts well for different systems - thus we call it meta-output-predictor(MOP). MOP matches the performance of the optimal output estimator, based onKalman filter, for most linear dynamical systems even though it does not haveaccess to a model. We observe via extensive numerical experiments that MOP alsoperforms well in challenging scenarios with non-i.i.d. noise, time-varyingdynamics, and nonlinear dynamics like a quadrotor system with unknownparameters. To further support this observation, in the second part of thepaper, we provide statistical guarantees on the performance of MOP and quantifythe required amount of training to achieve a desired excess risk duringtest-time. Finally, we point out some limitations of MOP by identifying twoclasses of problems MOP fails to perform well, highlighting the need forcaution when using transformers for control and estimation.</description><author>Haldun Balim, Zhe Du, Samet Oymak, Necmiye Ozay</author><pubDate>Wed, 16 Aug 2023 18:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08536v1</guid></item><item><title>AI-Assisted Discovery of Quantitative and Formal Models in Social Science</title><link>http://arxiv.org/abs/2210.00563v3</link><description>In social science, formal and quantitative models, such as ones describingeconomic growth and collective action, are used to formulate mechanisticexplanations, provide predictions, and uncover questions about observedphenomena. Here, we demonstrate the use of a machine learning system to aid thediscovery of symbolic models that capture nonlinear and dynamical relationshipsin social science datasets. By extending neuro-symbolic methods to find compactfunctions and differential equations in noisy and longitudinal data, we showthat our system can be used to discover interpretable models from real-worlddata in economics and sociology. Augmenting existing workflows with symbolicregression can help uncover novel relationships and explore counterfactualmodels during the scientific process. We propose that this AI-assistedframework can bridge parametric and non-parametric models commonly employed insocial science research by systematically exploring the space of nonlinearmodels and enabling fine-grained control over expressivity andinterpretability.</description><author>Julia Balla, Sihao Huang, Owen Dugan, Rumen Dangovski, Marin Soljacic</author><pubDate>Wed, 16 Aug 2023 18:45:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.00563v3</guid></item><item><title>Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes</title><link>http://arxiv.org/abs/2306.04306v2</link><description>This paper proposes Allophant, a multilingual phoneme recognizer. It requiresonly a phoneme inventory for cross-lingual transfer to a target language,allowing for low-resource recognition. The architecture combines acompositional phone embedding approach with individually supervised phoneticattribute classifiers in a multi-task architecture. We also introduceAllophoible, an extension of the PHOIBLE database. When combined with adistance based mapping approach for grapheme-to-phoneme outputs, it allows usto train on PHOIBLE inventories directly. By training and evaluating on 34languages, we found that the addition of multi-task learning improves themodel's capability of being applied to unseen phonemes and phoneme inventories.On supervised languages we achieve phoneme error rate improvements of 11percentage points (pp.) compared to a baseline without multi-task learning.Evaluation of zero-shot transfer on 84 languages yielded a decrease in PER of2.63 pp. over the baseline.</description><author>Kevin Glocker, Aaricia Herygers, Munir Georges</author><pubDate>Wed, 16 Aug 2023 18:44:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04306v2</guid></item><item><title>Data Selection: A Surprisingly Effective and General Principle for Building Small Interpretable Models</title><link>http://arxiv.org/abs/2210.03921v2</link><description>We present convincing empirical evidence for an effective and generalstrategy for building accurate small models. Such models are attractive forinterpretability and also find use in resource-constrained environments. Thestrategy is to learn the training distribution instead of using data from thetest distribution. The distribution learning algorithm is not a contribution ofthis work; we highlight the broad usefulness of this simple strategy on adiverse set of tasks, and as such these rigorous empirical results are ourcontribution. We apply it to the tasks of (1) building cluster explanationtrees, (2) prototype-based classification, and (3) classification using RandomForests, and show that it improves the accuracy of weak traditional baselinesto the point that they are surprisingly competitive with specialized moderntechniques. This strategy is also versatile wrt the notion of model size. In the firsttwo tasks, model size is identified by number of leaves in the tree and thenumber of prototypes respectively. In the final task involving Random Foreststhe strategy is shown to be effective even when model size is determined bymore than one factor: number of trees and their maximum depth. Positive results using multiple datasets are presented that are shown to bestatistically significant. These lead us to conclude that this strategy is botheffective, i.e, leads to significant improvements, and general, i.e., isapplicable to different tasks and model families, and therefore merits furtherattention in domains that require small accurate models.</description><author>Abhishek Ghose</author><pubDate>Wed, 16 Aug 2023 18:42:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03921v2</guid></item><item><title>Ref-DVGO: Reflection-Aware Direct Voxel Grid Optimization for an Improved Quality-Efficiency Trade-Off in Reflective Scene Reconstructio</title><link>http://arxiv.org/abs/2308.08530v1</link><description>Neural Radiance Fields (NeRFs) have revolutionized the field of novel viewsynthesis, demonstrating remarkable performance. However, the modeling andrendering of reflective objects remain challenging problems. Recent methodshave shown significant improvements over the baselines in handling reflectivescenes, albeit at the expense of efficiency. In this work, we aim to strike abalance between efficiency and quality. To this end, we investigate animplicit-explicit approach based on conventional volume rendering to enhancethe reconstruction quality and accelerate the training and rendering processes.We adopt an efficient density-based grid representation and reparameterize thereflected radiance in our pipeline. Our proposed reflection-aware approachachieves a competitive quality efficiency trade-off compared to competingmethods. Based on our experimental results, we propose and discuss hypothesesregarding the factors influencing the results of density-based methods forreconstructing reflective objects. The source code is available at:https://github.com/gkouros/ref-dvgo</description><author>Georgios Kouros, Minye Wu, Sushruth Nagesh, Shubham Shrivastava, Punarjay Chakravarty, Tinne Tuytelaars</author><pubDate>Wed, 16 Aug 2023 18:40:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08530v1</guid></item><item><title>EndoDepthL: Lightweight Endoscopic Monocular Depth Estimation with CNN-Transformer</title><link>http://arxiv.org/abs/2308.02716v2</link><description>In this study, we address the key challenges concerning the accuracy andeffectiveness of depth estimation for endoscopic imaging, with a particularemphasis on real-time inference and the impact of light reflections. We proposea novel lightweight solution named EndoDepthL that integrates ConvolutionalNeural Networks (CNN) and Transformers to predict multi-scale depth maps. Ourapproach includes optimizing the network architecture, incorporatingmulti-scale dilated convolution, and a multi-channel attention mechanism. Wealso introduce a statistical confidence boundary mask to minimize the impact ofreflective areas. To better evaluate the performance of monocular depthestimation in endoscopic imaging, we propose a novel complexity evaluationmetric that considers network parameter size, floating-point operations, andinference frames per second. We comprehensively evaluate our proposed methodand compare it with existing baseline solutions. The results demonstrate thatEndoDepthL ensures depth estimation accuracy with a lightweight structure.</description><author>Yangke Li</author><pubDate>Wed, 16 Aug 2023 18:39:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02716v2</guid></item><item><title>Diagnosing Human-object Interaction Detectors</title><link>http://arxiv.org/abs/2308.08529v1</link><description>Although we have witnessed significant progress in human-object interaction(HOI) detection with increasingly high mAP (mean Average Precision), a singlemAP score is too concise to obtain an informative summary of a model'sperformance and to understand why one approach is better than another. In thispaper, we introduce a diagnosis toolbox for analyzing the error sources of theexisting HOI detection models. We first conduct holistic investigations in thepipeline of HOI detection, consisting of human-object pair detection and theninteraction classification. We define a set of errors and the oracles to fixeach of them. By measuring the mAP improvement obtained from fixing an errorusing its oracle, we can have a detailed analysis of the significance ofdifferent errors. We then delve into the human-object detection and interactionclassification, respectively, and check the model's behavior. For the firstdetection task, we investigate both recall and precision, measuring thecoverage of ground-truth human-object pairs as well as the noisiness level inthe detections. For the second classification task, we compute mAP forinteraction classification only, without considering the detection scores. Wealso measure the performance of the models in differentiating human-objectpairs with and without actual interactions using the AP (Average Precision)score. Our toolbox is applicable for different methods across differentdatasets and available at https://github.com/neu-vi/Diag-HOI.</description><author>Fangrui Zhu, Yiming Xie, Weidi Xie, Huaizu Jiang</author><pubDate>Wed, 16 Aug 2023 18:39:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08529v1</guid></item><item><title>Likelihood-Based Text-to-Image Evaluation with Patch-Level Perceptual and Semantic Credit Assignment</title><link>http://arxiv.org/abs/2308.08525v1</link><description>Text-to-image synthesis has made encouraging progress and attracted lots ofpublic attention recently. However, popular evaluation metrics in this area,like the Inception Score and Fr'echet Inception Distance, incur several issues.First of all, they cannot explicitly assess the perceptual quality of generatedimages and poorly reflect the semantic alignment of each text-image pair. Also,they are inefficient and need to sample thousands of images to stabilise theirevaluation results. In this paper, we propose to evaluate text-to-imagegeneration performance by directly estimating the likelihood of the generatedimages using a pre-trained likelihood-based text-to-image generative model,i.e., a higher likelihood indicates better perceptual quality and bettertext-image alignment. To prevent the likelihood of being dominated by thenon-crucial part of the generated image, we propose several new designs todevelop a credit assignment strategy based on the semantic and perceptualsignificance of the image patches. In the experiments, we evaluate the proposedmetric on multiple popular text-to-image generation models and datasets inaccessing both the perceptual quality and the text-image alignment. Moreover,it can successfully assess the generation ability of these models with as fewas a hundred samples, making it very efficient in practice.</description><author>Qi Chen, Chaorui Deng, Zixiong Huang, Bowen Zhang, Mingkui Tan, Qi Wu</author><pubDate>Wed, 16 Aug 2023 18:26:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08525v1</guid></item><item><title>Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities</title><link>http://arxiv.org/abs/2307.13565v2</link><description>Decision-focused learning (DFL) is an emerging paradigm in machine learningwhich trains a model to optimize decisions, integrating prediction andoptimization in an end-to-end system. This paradigm holds the promise torevolutionize decision-making in many real-world applications which operateunder uncertainty, where the estimation of unknown parameters within thesedecision models often becomes a substantial roadblock. This paper presents acomprehensive review of DFL. It provides an in-depth analysis of the varioustechniques devised to integrate machine learning and optimization models,introduces a taxonomy of DFL methods distinguished by their uniquecharacteristics, and conducts an extensive empirical evaluation of thesemethods proposing suitable benchmark dataset and tasks for DFL. Finally, thestudy provides valuable insights into current and potential future avenues inDFL research.</description><author>Jayanta Mandi, James Kotary, Senne Berden, Maxime Mulamba, Victor Bucarey, Tias Guns, Ferdinando Fioretto</author><pubDate>Wed, 16 Aug 2023 18:26:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13565v2</guid></item><item><title>Painter: Teaching Auto-regressive Language Models to Draw Sketches</title><link>http://arxiv.org/abs/2308.08520v1</link><description>Large language models (LLMs) have made tremendous progress in naturallanguage understanding and they have also been successfully adopted in otherdomains such as computer vision, robotics, reinforcement learning, etc. In thiswork, we apply LLMs to image generation tasks by directly generating thevirtual brush strokes to paint an image. We present Painter, an LLM that canconvert user prompts in text description format to sketches by generating thecorresponding brush strokes in an auto-regressive way. We construct Painterbased on off-the-shelf LLM that is pre-trained on a large text corpus, byfine-tuning it on the new task while preserving language understandingcapabilities. We create a dataset of diverse multi-object sketches paired withtextual prompts that covers several object types and tasks. Painter cangenerate sketches from text descriptions, remove objects from canvas, anddetect and classify objects in sketches. Although this is an unprecedentedpioneering work in using LLMs for auto-regressive image generation, the resultsare very encouraging.</description><author>Reza Pourreza, Apratim Bhattacharyya, Sunny Panchal, Mingu Lee, Pulkit Madan, Roland Memisevic</author><pubDate>Wed, 16 Aug 2023 18:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08520v1</guid></item><item><title>Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction</title><link>http://arxiv.org/abs/2308.08518v1</link><description>Traditional geometric registration based estimation methods only exploit theCAD model implicitly, which leads to their dependence on observation qualityand deficiency to occlusion.To address the problem,the paper proposes abidirectional correspondence prediction network with a point-wiseattention-aware mechanism. This network not only requires the model points topredict the correspondence but also explicitly models the geometricsimilarities between observations and the model prior.} Our key insight is thatthe correlations between each model point and scene point provide essentialinformation for learning point-pair matches. To further tackle the correlationnoises brought by feature distribution divergence, we design a simple buteffective pseudo-siamese network to improve feature homogeneity.Experimentalresults on the public datasets of LineMOD, YCB-Video, and Occ-LineMOD show thatthe proposed method achieves better performance than other state-of-the-artmethods under the same evaluation criteria. Its robustness in estimating posesis greatly improved, especially in an environment with severe occlusions.</description><author>Yuhao Yang, Jun Wu, Guangjian Zhang, Rong Xiong</author><pubDate>Wed, 16 Aug 2023 18:13:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08518v1</guid></item><item><title>Adaptive Split-Fusion Transformer</title><link>http://arxiv.org/abs/2204.12196v2</link><description>Neural networks for visual content understanding have recently evolved fromconvolutional ones (CNNs) to transformers. The prior (CNN) relies onsmall-windowed kernels to capture the regional clues, demonstrating solid localexpressiveness. On the contrary, the latter (transformer) establisheslong-range global connections between localities for holistic learning.Inspired by this complementary nature, there is a growing interest in designinghybrid models to best utilize each technique. Current hybrids merely replaceconvolutions as simple approximations of linear projection or juxtapose aconvolution branch with attention, without concerning the importance oflocal/global modeling. To tackle this, we propose a new hybrid named AdaptiveSplit-Fusion Transformer (ASF-former) to treat convolutional and attentionbranches differently with adaptive weights. Specifically, an ASF-former encoderequally splits feature channels into half to fit dual-path inputs. Then, theoutputs of dual-path are fused with weighting scalars calculated from visualcues. We also design the convolutional path compactly for efficiency concerns.Extensive experiments on standard benchmarks, such as ImageNet-1K, CIFAR-10,and CIFAR-100, show that our ASF-former outperforms its CNN, transformercounterparts, and hybrid pilots in terms of accuracy (83.9% on ImageNet-1K),under similar conditions (12.9G MACs/56.7M Params, without large-scalepre-training). The code is available at:https://github.com/szx503045266/ASF-former.</description><author>Zixuan Su, Hao Zhang, Jingjing Chen, Lei Pang, Chong-Wah Ngo, Yu-Gang Jiang</author><pubDate>Wed, 16 Aug 2023 18:09:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.12196v2</guid></item><item><title>Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems</title><link>http://arxiv.org/abs/2308.08511v1</link><description>Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are crucialtechnologies in the field of medical imaging. Score-based models have proven tobe effective in addressing different inverse problems encountered in CT andMRI, such as sparse-view CT and fast MRI reconstruction. However, these modelsface challenges in achieving accurate three dimensional (3D) volumetricreconstruction. The existing score-based models primarily focus onreconstructing two dimensional (2D) data distribution, leading toinconsistencies between adjacent slices in the reconstructed 3D volumetricimages. To overcome this limitation, we propose a novel two-and-a-half orderscore-based model (TOSM). During the training phase, our TOSM learns datadistributions in 2D space, which reduces the complexity of training compared todirectly working on 3D volumes. However, in the reconstruction phase, the TOSMupdates the data distribution in 3D space, utilizing complementary scores alongthree directions (sagittal, coronal, and transaxial) to achieve a more precisereconstruction. The development of TOSM is built on robust theoreticalprinciples, ensuring its reliability and efficacy. Through extensiveexperimentation on large-scale sparse-view CT and fast MRI datasets, our methoddemonstrates remarkable advancements and attains state-of-the-art results insolving 3D ill-posed inverse problems. Notably, the proposed TOSM effectivelyaddresses the inter-slice inconsistency issue, resulting in high-quality 3Dvolumetric reconstruction.</description><author>Zirong Li, Yanyang Wang, Jianjia Zhang, Weiwen Wu, Hengyong Yu</author><pubDate>Wed, 16 Aug 2023 18:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08511v1</guid></item><item><title>Autoencoding a Soft Touch to Learn Grasping from On-land to Underwater</title><link>http://arxiv.org/abs/2308.08510v1</link><description>Robots play a critical role as the physical agent of human operators inexploring the ocean. However, it remains challenging to grasp objects reliablywhile fully submerging under a highly pressurized aquatic environment withlittle visible light, mainly due to the fluidic interference on the tactilemechanics between the finger and object surfaces. This study investigates thetransferability of grasping knowledge from on-land to underwater via avision-based soft robotic finger that learns 6D forces and torques (FT) using aSupervised Variational Autoencoder (SVAE). A high-framerate camera captures thewhole-body deformations while a soft robotic finger interacts with physicalobjects on-land and underwater. Results show that the trained SVAE modellearned a series of latent representations of the soft mechanics transferrablefrom land to water, presenting a superior adaptation to the changingenvironments against commercial FT sensors. Soft, delicate, and reactivegrasping enabled by tactile intelligence enhances the gripper's underwaterinteraction with improved reliability and robustness at a much-reduced cost,paving the path for learning-based intelligent grasping to support fundamentalscientific discoveries in environmental and ocean research.</description><author>Ning Guo, Xudong Han, Xiaobo Liu, Shuqiao Zhong, Zhiyuan Zhou, Jian Lin, Jiansheng Dai, Fang Wan, Chaoyang Song</author><pubDate>Wed, 16 Aug 2023 18:07:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08510v1</guid></item><item><title>Large-Scale Traffic Congestion Prediction based on Multimodal Fusion and Representation Mapping</title><link>http://arxiv.org/abs/2208.11061v2</link><description>With the progress of the urbanisation process, the urban transportationsystem is extremely critical to the development of cities and the quality oflife of the citizens. Among them, it is one of the most important tasks tojudge traffic congestion by analysing the congestion factors. Recently, varioustraditional and machine-learning-based models have been introduced forpredicting traffic congestion. However, these models are either poorlyaggregated for massive congestion factors or fail to make accurate predictionsfor every precise location in large-scale space. To alleviate these problems, anovel end-to-end framework based on convolutional neural networks is proposedin this paper. With learning representations, the framework proposes a novelmultimodal fusion module and a novel representation mapping module to achievetraffic congestion predictions on arbitrary query locations on a large-scalemap, combined with various global reference information. The proposed frameworkachieves significant results and efficient inference on real-world large-scaledatasets.</description><author>Bodong Zhou, Jiahui Liu, Songyi Cui, Yaping Zhao</author><pubDate>Wed, 16 Aug 2023 18:02:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.11061v2</guid></item><item><title>ResBuilder: Automated Learning of Depth with Residual Structures</title><link>http://arxiv.org/abs/2308.08504v1</link><description>In this work, we develop a neural architecture search algorithm, termedResbuilder, that develops ResNet architectures from scratch that achieve highaccuracy at moderate computational cost. It can also be used to modify existingarchitectures and has the capability to remove and insert ResNet blocks, inthis way searching for suitable architectures in the space of ResNetarchitectures. In our experiments on different image classification datasets,Resbuilder achieves close to state-of-the-art performance while savingcomputational cost compared to off-the-shelf ResNets. Noteworthy, we once tunethe parameters on CIFAR10 which yields a suitable default choice for all otherdatasets. We demonstrate that this property generalizes even to industrialapplications by applying our method with default parameters on a proprietaryfraud detection dataset.</description><author>Julian Burghoff, Matthias Rottmann, Jill von Conta, Sebastian Schoenen, Andreas Witte, Hanno Gottschalk</author><pubDate>Wed, 16 Aug 2023 17:58:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08504v1</guid></item><item><title>Explanations as Programs in Probabilistic Logic Programming</title><link>http://arxiv.org/abs/2210.03021v2</link><description>The generation of comprehensible explanations is an essential feature ofmodern artificial intelligence systems. In this work, we consider probabilisticlogic programming, an extension of logic programming which can be useful tomodel domains with relational structure and uncertainty. Essentially, a programspecifies a probability distribution over possible worlds (i.e., sets offacts). The notion of explanation is typically associated with that of a world,so that one often looks for the most probable world as well as for the worldswhere the query is true. Unfortunately, such explanations exhibit no causalstructure. In particular, the chain of inferences required for a specificprediction (represented by a query) is not shown. In this paper, we propose anovel approach where explanations are represented as programs that aregenerated from a given query by a number of unfolding-like transformations.Here, the chain of inferences that proves a given query is made explicit.Furthermore, the generated explanations are minimal (i.e., contain noirrelevant information) and can be parameterized w.r.t. a specification ofvisible predicates, so that the user may hide uninteresting details fromexplanations.</description><author>Germán Vidal</author><pubDate>Wed, 16 Aug 2023 17:53:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03021v2</guid></item><item><title>Physics-Based Task Generation through Causal Sequence of Physical Interactions</title><link>http://arxiv.org/abs/2308.02835v2</link><description>Performing tasks in a physical environment is a crucial yet challengingproblem for AI systems operating in the real world. Physics simulation-basedtasks are often employed to facilitate research that addresses this challenge.In this paper, first, we present a systematic approach for defining a physicalscenario using a causal sequence of physical interactions between objects.Then, we propose a methodology for generating tasks in a physics-simulatingenvironment using these defined scenarios as inputs. Our approach enables abetter understanding of the granular mechanics required for solvingphysics-based tasks, thereby facilitating accurate evaluation of AI systems'physical reasoning capabilities. We demonstrate our proposed task generationmethodology using the physics-based puzzle game Angry Birds and evaluate thegenerated tasks using a range of metrics, including physical stability,solvability using intended physical interactions, and accidental solvabilityusing unintended solutions. We believe that the tasks generated using ourproposed methodology can facilitate a nuanced evaluation of physical reasoningagents, thus paving the way for the development of agents for moresophisticated real-world applications.</description><author>Chathura Gamage, Vimukthini Pinto, Matthew Stephenson, Jochen Renz</author><pubDate>Wed, 16 Aug 2023 17:51:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02835v2</guid></item><item><title>Self-Supervised Online Camera Calibration for Automated Driving and Parking Applications</title><link>http://arxiv.org/abs/2308.08495v1</link><description>Camera-based perception systems play a central role in modern autonomousvehicles. These camera based perception algorithms require an accuratecalibration to map the real world distances to image pixels. In practice,calibration is a laborious procedure requiring specialised data collection andcareful tuning. This process must be repeated whenever the parameters of thecamera change, which can be a frequent occurrence in autonomous vehicles. Hencethere is a need to calibrate at regular intervals to ensure the camera isaccurate. Proposed is a deep learning framework to learn intrinsic andextrinsic calibration of the camera in real time. The framework isself-supervised and doesn't require any labelling or supervision to learn thecalibration parameters. The framework learns calibration without the need forany physical targets or to drive the car on special planar surfaces.</description><author>Ciarán Hogan, Ganesh Sistu, Ciarán Eising</author><pubDate>Wed, 16 Aug 2023 17:49:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08495v1</guid></item><item><title>Time Travel in LLMs: Tracing Data Contamination in Large Language Models</title><link>http://arxiv.org/abs/2308.08493v1</link><description>Data contamination, i.e., the presence of test data from downstream tasks inthe training data of large language models (LLMs), is a potential major issuein understanding LLMs' effectiveness on other tasks. We propose astraightforward yet effective method for identifying data contamination withinLLMs. At its core, our approach starts by identifying potential contaminationin individual instances that are drawn from a small random sample; using thisinformation, our approach then assesses if an entire dataset partition iscontaminated. To estimate contamination of individual instances, we employ"guided instruction:" a prompt consisting of the dataset name, partition type,and the initial segment of a reference instance, asking the LLM to complete it.An instance is flagged as contaminated if the LLM's output either exactly orclosely matches the latter segment of the reference. To understand if an entirepartition is contaminated, we propose two ideas. The first idea marks a datasetpartition as contaminated if the average overlap score with the referenceinstances (as measured by ROUGE or BLEURT) is statistically significantlybetter with the guided instruction vs. a general instruction that does notinclude the dataset and partition name. The second idea marks a dataset ascontaminated if a classifier based on GPT-4 with in-context learning promptingmarks multiple instances as contaminated. Our best method achieves an accuracybetween 92% and 100% in detecting if an LLM is contaminated with sevendatasets, containing train and test/validation partitions, when contrasted withmanual evaluation by human expert. Further, our findings indicate that GPT-4 iscontaminated with AG News, WNLI, and XSum datasets.</description><author>Shahriar Golchin, Mihai Surdeanu</author><pubDate>Wed, 16 Aug 2023 17:48:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08493v1</guid></item><item><title>Label Propagation Techniques for Artifact Detection in Imbalanced Classes using Photoplethysmogram Signals</title><link>http://arxiv.org/abs/2308.08480v1</link><description>Photoplethysmogram (PPG) signals are widely used in healthcare for monitoringvital signs, but they are susceptible to motion artifacts that can lead toinaccurate interpretations. In this study, the use of label propagationtechniques to propagate labels among PPG samples is explored, particularly inimbalanced class scenarios where clean PPG samples are significantlyoutnumbered by artifact-contaminated samples. With a precision of 91%, a recallof 90% and an F1 score of 90% for the class without artifacts, the resultsdemonstrate its effectiveness in labeling a medical dataset, even when cleansamples are rare. For the classification of artifacts our study comparessupervised classifiers such as conventional classifiers and neural networks(MLP, Transformers, FCN) with the semi-supervised label propagation algorithm.With a precision of 89%, a recall of 95% and an F1 score of 92%, the KNNsupervised model gives good results, but the semi-supervised algorithm performsbetter in detecting artifacts. The findings suggest that the semi-supervisedalgorithm label propagation hold promise for artifact detection in PPG signals,which can enhance the reliability of PPG-based health monitoring systems inreal-world applications.</description><author>Clara Macabiau, Thanh-Dung Le, Kevin Albert, Philippe Jouvet, Rita Noumeir</author><pubDate>Wed, 16 Aug 2023 17:38:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08480v1</guid></item><item><title>DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</title><link>http://arxiv.org/abs/2308.08479v1</link><description>Keypoint detection is a pivotal step in 3D reconstruction, whereby sets of(up to) K points are detected in each view of a scene. Crucially, the detectedpoints need to be consistent between views, i.e., correspond to the same 3Dpoint in the scene. One of the main challenges with keypoint detection is theformulation of the learning objective. Previous learning-based methodstypically jointly learn descriptors with keypoints, and treat the keypointdetection as a binary classification task on mutual nearest neighbours.However, basing keypoint detection on descriptor nearest neighbours is a proxytask, which is not guaranteed to produce 3D-consistent keypoints. Furthermore,this ties the keypoints to a specific descriptor, complicating downstreamusage. In this work, we instead learn keypoints directly from 3D consistency.To this end, we train the detector to detect tracks from large-scale SfM. Asthese points are often overly sparse, we derive a semi-supervised two-viewdetection objective to expand this set to a desired number of detections. Totrain a descriptor, we maximize the mutual nearest neighbour objective over thekeypoints with a separate network. Results show that our approach, DeDoDe,achieves significant gains on multiple geometry benchmarks. Code is provided athttps://github.com/Parskatt/DeDoDe .</description><author>Johan Edstedt, Georg Bökman, Mårten Wadenbäck, Michael Felsberg</author><pubDate>Wed, 16 Aug 2023 17:37:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08479v1</guid></item><item><title>Classification Committee for Active Deep Object Detection</title><link>http://arxiv.org/abs/2308.08476v1</link><description>In object detection, the cost of labeling is much high because it needs notonly to confirm the categories of multiple objects in an image but also toaccurately determine the bounding boxes of each object. Thus, integratingactive learning into object detection will raise pretty positive significance.In this paper, we propose a classification committee for active deep objectdetection method by introducing a discrepancy mechanism of multiple classifiersfor samples' selection when training object detectors. The model contains amain detector and a classification committee. The main detector denotes thetarget object detector trained from a labeled pool composed of the selectedinformative images. The role of the classification committee is to select themost informative images according to their uncertainty values from the view ofclassification, which is expected to focus more on the discrepancy andrepresentative of instances. Specifically, they compute the uncertainty for aspecified instance within the image by measuring its discrepancy output by thecommittee pre-trained via the proposed Maximum Classifiers Discrepancy GroupLoss (MCDGL). The most informative images are finally determined by selectingthe ones with many high-uncertainty instances. Besides, to mitigate the impactof interference instances, we design a Focus on Positive Instances Loss (FPIL)to make the committee the ability to automatically focus on the representativeinstances as well as precisely encode their discrepancies for the sameinstance. Experiments are conducted on Pascal VOC and COCO datasets versus somepopular object detectors. And results show that our method outperforms thestate-of-the-art active learning methods, which verifies the effectiveness ofthe proposed method.</description><author>Lei Zhao, Bo Li, Xingxing Wei</author><pubDate>Wed, 16 Aug 2023 17:31:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08476v1</guid></item><item><title>LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs</title><link>http://arxiv.org/abs/2308.08469v1</link><description>In this work, we leverage pre-trained Large Language Models (LLMs) to enhancetime-series forecasting. Mirroring the growing interest in unifying models forNatural Language Processing and Computer Vision, we envision creating ananalogous model for long-term time-series forecasting. Due to limitedlarge-scale time-series data for building robust foundation models, ourapproach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. Bycombining time-series patching with temporal encoding, we have enhanced thecapability of LLMs to handle time-series data effectively. Inspired by thesupervised fine-tuning in chatbot domains, we prioritize a two-stagefine-tuning process: first conducting supervised fine-tuning to orient the LLMtowards time-series data, followed by task-specific downstream fine-tuning.Furthermore, to unlock the flexibility of pre-trained LLMs without extensiveparameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT)techniques. Drawing on these innovations, LLM4TS has yielded state-of-the-artresults in long-term forecasting. Our model has also shown exceptionalcapabilities as both a robust representation learner and an effective few-shotlearner, thanks to the knowledge transferred from the pre-trained LLM.</description><author>Ching Chang, Wen-Chih Peng, Tien-Fu Chen</author><pubDate>Wed, 16 Aug 2023 17:19:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08469v1</guid></item><item><title>An Expert's Guide to Training Physics-informed Neural Networks</title><link>http://arxiv.org/abs/2308.08468v1</link><description>Physics-informed neural networks (PINNs) have been popularized as a deeplearning framework that can seamlessly synthesize observational data andpartial differential equation (PDE) constraints. Their practical effectivenesshowever can be hampered by training pathologies, but also oftentimes by poorchoices made by users who lack deep learning expertise. In this paper wepresent a series of best practices that can significantly improve the trainingefficiency and overall accuracy of PINNs. We also put forth a series ofchallenging benchmark problems that highlight some of the most prominentdifficulties in training PINNs, and present comprehensive and fullyreproducible ablation studies that demonstrate how different architecturechoices and training strategies affect the test accuracy of the resultingmodels. We show that the methods and guiding principles put forth in this studylead to state-of-the-art results and provide strong baselines that futurestudies should use for comparison purposes. To this end, we also release ahighly optimized library in JAX that can be used to reproduce all resultsreported in this paper, enable future research studies, as well as facilitateeasy adaptation to new use-case scenarios.</description><author>Sifan Wang, Shyam Sankaran, Hanwen Wang, Paris Perdikaris</author><pubDate>Wed, 16 Aug 2023 17:19:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08468v1</guid></item><item><title>Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation</title><link>http://arxiv.org/abs/2308.06422v2</link><description>As the complexity and computational demands of deep learning models rise, theneed for effective optimization methods for neural network designs becomesparamount. This work introduces an innovative search mechanism forautomatically selecting the best bit-width and layer-width for individualneural network layers. This leads to a marked enhancement in deep neuralnetwork efficiency. The search domain is strategically reduced by leveragingHessian-based pruning, ensuring the removal of non-crucial parameters.Subsequently, we detail the development of surrogate models for favorable andunfavorable outcomes by employing a cluster-based tree-structured Parzenestimator. This strategy allows for a streamlined exploration of architecturalpossibilities and swift pinpointing of top-performing designs. Through rigoroustesting on well-known datasets, our method proves its distinct advantage overexisting methods. Compared to leading compression strategies, our approachrecords an impressive 20% decrease in model size without compromising accuracy.Additionally, our method boasts a 12x reduction in search time relative to thebest search-focused strategies currently available. As a result, our proposedmethod represents a leap forward in neural network design optimization, pavingthe way for quick model design and implementation in settings with limitedresources, thereby propelling the potential of scalable deep learningsolutions.</description><author>Seyedarmin Azizi, Mahdi Nazemi, Arash Fayyazi, Massoud Pedram</author><pubDate>Wed, 16 Aug 2023 17:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06422v2</guid></item><item><title>On Neural Quantum Support Vector Machines</title><link>http://arxiv.org/abs/2308.08467v1</link><description>In \cite{simon2023algorithms} we introduced four algorithms for the trainingof neural support vector machines (NSVMs) and demonstrated their feasibility.In this note we introduce neural quantum support vector machines, that is,NSVMs with a quantum kernel, and extend our results to this setting.</description><author>Lars Simon, Manuel Radons</author><pubDate>Wed, 16 Aug 2023 17:15:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08467v1</guid></item><item><title>Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++</title><link>http://arxiv.org/abs/2301.11118v3</link><description>Description logic (DL) ontologies extend knowledge graphs (KGs) withconceptual information and logical background knowledge. In recent years, therehas been growing interest in inductive reasoning techniques for suchontologies, which promise to complement classical deductive reasoningalgorithms. Similar to KG completion, several existing approaches learnontology embeddings in a latent space, while additionally ensuring that theyfaithfully capture the logical semantics of the underlying DL. However, theysuffer from several shortcomings, mainly due to a limiting role representation.We propose Box$^2$EL, which represents both concepts and roles as boxes (i.e.,axis-aligned hyperrectangles) and demonstrate how it overcomes the limitationsof previous methods. We theoretically prove the soundness of our model andconduct an extensive experimental evaluation, achieving state-of-the-artresults across a variety of datasets. As part of our evaluation, we introduce anovel benchmark for subsumption prediction involving both atomic and complexconcepts.</description><author>Mathias Jackermeier, Jiaoyan Chen, Ian Horrocks</author><pubDate>Wed, 16 Aug 2023 17:11:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11118v3</guid></item><item><title>Hierarchical Uncertainty Estimation for Medical Image Segmentation Networks</title><link>http://arxiv.org/abs/2308.08465v1</link><description>Learning a medical image segmentation model is an inherently ambiguous task,as uncertainties exist in both images (noise) and manual annotations (humanerrors and bias) used for model training. To build a trustworthy imagesegmentation model, it is important to not just evaluate its performance butalso estimate the uncertainty of the model prediction. Most state-of-the-artimage segmentation networks adopt a hierarchical encoder architecture,extracting image features at multiple resolution levels from fine to coarse. Inthis work, we leverage this hierarchical image representation and propose asimple yet effective method for estimating uncertainties at multiple levels.The multi-level uncertainties are modelled via the skip-connection module andthen sampled to generate an uncertainty map for the predicted imagesegmentation. We demonstrate that a deep learning segmentation network such asU-net, when implemented with such hierarchical uncertainty estimation module,can achieve a high segmentation performance, while at the same time providemeaningful uncertainty maps that can be used for out-of-distribution detection.</description><author>Xinyu Bai, Wenjia Bai</author><pubDate>Wed, 16 Aug 2023 17:09:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08465v1</guid></item><item><title>Learning to Distill Global Representation for Sparse-View CT</title><link>http://arxiv.org/abs/2308.08463v1</link><description>Sparse-view computed tomography (CT) -- using a small number of projectionsfor tomographic reconstruction -- enables much lower radiation dose to patientsand accelerated data acquisition. The reconstructed images, however, sufferfrom strong artifacts, greatly limiting their diagnostic value. Current trendsfor sparse-view CT turn to the raw data for better information recovery. Theresultant dual-domain methods, nonetheless, suffer from secondary artifacts,especially in ultra-sparse view scenarios, and their generalization to otherscanners/protocols is greatly limited. A crucial question arises: have theimage post-processing methods reached the limit? Our answer is not yet. In thispaper, we stick to image post-processing methods due to great flexibility andpropose global representation (GloRe) distillation framework for sparse-viewCT, termed GloReDi. First, we propose to learn GloRe with Fourier convolution,so each element in GloRe has an image-wide receptive field. Second, unlikemethods that only use the full-view images for supervision, we propose todistill GloRe from intermediate-view reconstructed images that are readilyavailable but not explored in previous literature. The success of GloRedistillation is attributed to two key components: representation directionaldistillation to align the GloRe directions, and band-pass-specific contrastivedistillation to gain clinically important details. Extensive experimentsdemonstrate the superiority of the proposed GloReDi over the state-of-the-artmethods, including dual-domain ones. The source code is available athttps://github.com/longzilicart/GloReDi.</description><author>Zilong Li, Chenglong Ma, Jie Chen, Junping Zhang, Hongming shan</author><pubDate>Wed, 16 Aug 2023 17:08:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08463v1</guid></item><item><title>Disentangled Representation Learning</title><link>http://arxiv.org/abs/2211.11695v2</link><description>Disentangled Representation Learning (DRL) aims to learn a model capable ofidentifying and disentangling the underlying factors hidden in the observabledata in representation form. The process of separating underlying factors ofvariation into variables with semantic meaning benefits in learning explainablerepresentations of data, which imitates the meaningful understanding process ofhumans when observing an object or relation. As a general learning strategy,DRL has demonstrated its power in improving the model explainability,controlability, robustness, as well as generalization capacity in a wide rangeof scenarios such as computer vision, natural language processing, data miningetc. In this article, we comprehensively review DRL from various aspectsincluding motivations, definitions, methodologies, evaluations, applicationsand model designs. We discuss works on DRL based on two well-recognizeddefinitions, i.e., Intuitive Definition and Group Theory Definition. We furthercategorize the methodologies for DRL into four groups, i.e., TraditionalStatistical Approaches, Variational Auto-encoder Based Approaches, GenerativeAdversarial Networks Based Approaches, Hierarchical Approaches and OtherApproaches. We also analyze principles to design different DRL models that maybenefit different tasks in practical applications. Finally, we point outchallenges in DRL as well as potential research directions deserving futureinvestigations. We believe this work may provide insights for promoting the DRLresearch in the community.</description><author>Xin Wang, Hong Chen, Si'ao Tang, Zihao Wu, Wenwu Zhu</author><pubDate>Wed, 16 Aug 2023 17:05:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11695v2</guid></item><item><title>Black Box Few-Shot Adaptation for Vision-Language models</title><link>http://arxiv.org/abs/2304.01752v2</link><description>Vision-Language (V-L) models trained with contrastive learning to align thevisual and language modalities have been shown to be strong few-shot learners.Soft prompt learning is the method of choice for few-shot downstream adaptionaiming to bridge the modality gap caused by the distribution shift induced bythe new domain. While parameter-efficient, prompt learning still requiresaccess to the model weights and can be computationally infeasible for largemodels with billions of parameters. To address these shortcomings, in thiswork, we describe a black-box method for V-L few-shot adaptation that (a)operates on pre-computed image and text features and hence works without accessto the model's weights, (b) it is orders of magnitude faster at training time,(c) it is amenable to both supervised and unsupervised training, and (d) it canbe even used to align image and text features computed from uni-modal models.To achieve this, we propose Linear Feature Alignment (LFA), a simple linearapproach for V-L re-alignment in the target domain. LFA is initialized from aclosed-form solution to a least-squares problem and then it is iterativelyupdated by minimizing a re-ranking loss. Despite its simplicity, our approachcan even surpass soft-prompt learning methods as shown by extensive experimentson 11 image and 2 video datasets.</description><author>Yassine Ouali, Adrian Bulat, Brais Martinez, Georgios Tzimiropoulos</author><pubDate>Wed, 16 Aug 2023 16:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01752v2</guid></item><item><title>High-Fidelity Lake Extraction via Two-Stage Prompt Enhancement: Establishing a Novel Baseline and Benchmark</title><link>http://arxiv.org/abs/2308.08443v1</link><description>The extraction of lakes from remote sensing images is a complex challenge dueto the varied lake shapes and data noise. Current methods rely on multispectralimage datasets, making it challenging to learn lake features accurately frompixel arrangements. This, in turn, affects model learning and the creation ofaccurate segmentation masks. This paper introduces a unified prompt-baseddataset construction approach that provides approximate lake locations usingpoint, box, and mask prompts. We also propose a two-stage prompt enhancementframework, LEPrompter, which involves prompt-based and prompt-free stagesduring training. The prompt-based stage employs a prompt encoder to extractprior information, integrating prompt tokens and image embeddings through self-and cross-attention in the prompt decoder. Prompts are deactivated once themodel is trained to ensure independence during inference, enabling automatedlake extraction. Evaluations on Surface Water and Qinghai-Tibet Plateau Lakedatasets show consistent performance improvements compared to the previousstate-of-the-art method. LEPrompter achieves mIoU scores of 91.48% and 97.43%on the respective datasets without introducing additional parameters or GFLOPs.Supplementary materials provide the source code, pre-trained models, anddetailed user studies.</description><author>Ben Chen, Xuechao Zou, Kai Li, Yu Zhang, Junliang Xing, Pin Tao</author><pubDate>Wed, 16 Aug 2023 16:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08443v1</guid></item><item><title>LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization</title><link>http://arxiv.org/abs/2306.01102v2</link><description>Large Language Models (LLMs) have emerged as powerful tools capable ofaccomplishing a broad spectrum of tasks. Their abilities span numerous areas,and one area where they have made a significant impact is in the domain of codegeneration. In this context, we view LLMs as mutation and crossover tools.Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse androbust solutions. By merging the code-generating abilities of LLMs with thediversity and robustness of QD solutions, we introduce LLMatic, a NeuralArchitecture Search (NAS) algorithm. While LLMs struggle to conduct NASdirectly through prompts, LLMatic uses a procedural approach, leveraging QD forprompts and network architecture to create diverse and highly performantnetworks. We test LLMatic on the CIFAR-10 image classification benchmark,demonstrating that it can produce competitive networks with just $2,000$searches, even without prior knowledge of the benchmark domain or exposure toany previous top-performing models for the benchmark.</description><author>Muhammad U. Nasir, Sam Earle, Julian Togelius, Steven James, Christopher Cleghorn</author><pubDate>Wed, 16 Aug 2023 16:49:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01102v2</guid></item><item><title>Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction</title><link>http://arxiv.org/abs/2308.08442v1</link><description>Text-to-Text Transfer Transformer (T5) has recently been considered for theGrapheme-to-Phoneme (G2P) transduction. As a follow-up, a tokenizer-freebyte-level model based on T5 referred to as ByT5, recently gave promisingresults on word-level G2P conversion by representing each input character withits corresponding UTF-8 encoding. Although it is generally understood thatsentence-level or paragraph-level G2P can improve usability in real-worldapplications as it is better suited to perform on heteronyms and linking soundsbetween words, we find that using ByT5 for these scenarios is nontrivial. SinceByT5 operates on the character level, it requires longer decoding steps, whichdeteriorates the performance due to the exposure bias commonly observed inauto-regressive generation models. This paper shows that the performance ofsentence-level and paragraph-level G2P can be improved by mitigating suchexposure bias using our proposed loss-based sampling method.</description><author>Eunseop Yoon, Hee Suk Yoon, Dhananjaya Gowda, SooHwan Eom, Daehyeok Kim, John Harvill, Heting Gao, Mark Hasegawa-Johnson, Chanwoo Kim, Chang D. Yoo</author><pubDate>Wed, 16 Aug 2023 16:49:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08442v1</guid></item><item><title>Accurate synthesis of Dysarthric Speech for ASR data augmentation</title><link>http://arxiv.org/abs/2308.08438v1</link><description>Dysarthria is a motor speech disorder often characterized by reduced speechintelligibility through slow, uncoordinated control of speech productionmuscles. Automatic Speech recognition (ASR) systems can help dysarthric talkerscommunicate more effectively. However, robust dysarthria-specific ASR requiresa significant amount of training speech, which is not readily available fordysarthric talkers. This paper presents a new dysarthric speech synthesismethod for the purpose of ASR training data augmentation. Differences inprosodic and acoustic characteristics of dysarthric spontaneous speech atvarying severity levels are important components for dysarthric speechmodeling, synthesis, and augmentation. For dysarthric speech synthesis, amodified neural multi-talker TTS is implemented by adding a dysarthria severitylevel coefficient and a pause insertion model to synthesize dysarthric speechfor varying severity levels. To evaluate the effectiveness for synthesis oftraining data for ASR, dysarthria-specific speech recognition was used. Resultsshow that a DNN-HMM model trained on additional synthetic dysarthric speechachieves WER improvement of 12.2% compared to the baseline, and that theaddition of the severity level and pause insertion controls decrease WER by6.5%, showing the effectiveness of adding these parameters. Overall results onthe TORGO database demonstrate that using dysarthric synthetic speech toincrease the amount of dysarthric-patterned speech for training has significantimpact on the dysarthric ASR systems. In addition, we have conducted asubjective evaluation to evaluate the dysarthric-ness and similarity ofsynthesized speech. Our subjective evaluation shows that the perceiveddysartrhic-ness of synthesized speech is similar to that of true dysarthricspeech, especially for higher levels of dysarthria</description><author>Mohammad Soleymanpour, Michael T. Johnson, Rahim Soleymanpour, Jeffrey Berry</author><pubDate>Wed, 16 Aug 2023 16:42:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08438v1</guid></item><item><title>HGCN-GJS: Hierarchical Graph Convolutional Network with Groupwise Joint Sampling for Trajectory Prediction</title><link>http://arxiv.org/abs/2009.07140v2</link><description>Accurate pedestrian trajectory prediction is of great importance fordownstream tasks such as autonomous driving and mobile robot navigation. Fullyinvestigating the social interactions within the crowd is crucial for accuratepedestrian trajectory prediction. However, most existing methods do not capturegroup level interactions well, focusing only on pairwise interactions andneglecting group-wise interactions. In this work, we propose a hierarchicalgraph convolutional network, HGCN-GJS, for trajectory prediction which wellleverages group level interactions within the crowd. Furthermore, we introducea novel joint sampling scheme for modeling the joint distribution of multiplepedestrians in the future trajectories. Based on the group information, thisscheme associates the trajectory of one person with the trajectory of otherpeople in the group, but maintains the independence of the trajectories ofoutsiders. We demonstrate the performance of our network on several trajectoryprediction datasets, achieving state-of-the-art results on all datasetsconsidered.</description><author>Yuying Chen, Congcong Liu, Xiaodong Mei, Bertram E. Shi, Ming Liu</author><pubDate>Wed, 16 Aug 2023 16:40:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2009.07140v2</guid></item><item><title>A Distributionally Robust Approach to Regret Optimal Control using the Wasserstein Distance</title><link>http://arxiv.org/abs/2304.06783v2</link><description>This paper proposes a distributionally robust approach to regret optimalcontrol of discrete-time linear dynamical systems with quadratic costs subjectto a stochastic additive disturbance on the state process. The underlyingprobability distribution of the disturbance process is unknown, but assumed tolie in a given ball of distributions defined in terms of the type-2 Wassersteindistance. In this framework, strictly causal linear disturbance feedbackcontrollers are designed to minimize the worst-case expected regret. The regretincurred by a controller is defined as the difference between the cost itincurs in response to a realization of the disturbance process and the costincurred by the optimal noncausal controller which has perfect knowledge of thedisturbance process realization at the outset. Building on a well-establishedduality theory for optimal transport problems, we derive a reformulation of theminimax regret optimal control problem as a tractable semidefinite program.Using the equivalent dual reformulation, we characterize a worst-casedistribution achieving the worst-case expected regret in relation to thedistribution at the center of the Wasserstein ball. We compare the minimaxregret optimal control design method with the distributionally robust optimalcontrol approach using an illustrative example and numerical experiments.</description><author>Feras Al Taha, Shuhao Yan, Eilyan Bitar</author><pubDate>Wed, 16 Aug 2023 16:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06783v2</guid></item><item><title>RD-DPP: Rate-Distortion Theory Meets Determinantal Point Process to Diversify Learning Data Samples</title><link>http://arxiv.org/abs/2304.04137v2</link><description>In some practical learning tasks, such as traffic video analysis, the numberof available training samples is restricted by different factors, such aslimited communication bandwidth and computation power. Determinantal PointProcess (DPP) is a common method for selecting the most diverse samples toenhance learning quality. However, the number of selected samples is restrictedto the rank of the kernel matrix implied by the dimensionality of data samples.Secondly, it is not easily customizable to different learning tasks. In thispaper, we propose a new way of measuring task-oriented diversity based on theRate-Distortion (RD) theory, appropriate for multi-level classification. Tothis end, we establish a fundamental relationship between DPP and RD theory. Weobserve that the upper bound of the diversity of data selected by DPP has auniversal trend of $\textit{phase transition}$, which suggests that DPP isbeneficial only at the beginning of sample accumulation. This led to the designof a bi-modal method, where RD-DPP is used in the first mode to select initialdata samples, then classification inconsistency (as an uncertainty measure) isused to select the subsequent samples in the second mode. This phase transitionsolves the limitation to the rank of the similarity matrix. Applying our methodto six different datasets and five benchmark models suggests that our methodconsistently outperforms random selection, DPP-based methods, and alternativeslike uncertainty-based and coreset methods under all sampling budgets, whileexhibiting high generalizability to different learning tasks.</description><author>Xiwen Chen, Huayu Li, Rahul Amin, Abolfazl Razi</author><pubDate>Wed, 16 Aug 2023 16:36:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04137v2</guid></item><item><title>Integrating Visual and Semantic Similarity Using Hierarchies for Image Retrieval</title><link>http://arxiv.org/abs/2308.08431v1</link><description>Most of the research in content-based image retrieval (CBIR) focus ondeveloping robust feature representations that can effectively retrieveinstances from a database of images that are visually similar to a query.However, the retrieved images sometimes contain results that are notsemantically related to the query. To address this, we propose a method forCBIR that captures both visual and semantic similarity using a visualhierarchy. The hierarchy is constructed by merging classes with overlappingfeatures in the latent space of a deep neural network trained forclassification, assuming that overlapping classes share high visual andsemantic similarities. Finally, the constructed hierarchy is integrated intothe distance calculation metric for similarity search. Experiments on standarddatasets: CUB-200-2011 and CIFAR100, and a real-life use case using diatommicroscopy images show that our method achieves superior performance comparedto the existing methods on image retrieval.</description><author>Aishwarya Venkataramanan, Martin Laviale, Cédric Pradalier</author><pubDate>Wed, 16 Aug 2023 16:23:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08431v1</guid></item><item><title>ALIP: Adaptive Language-Image Pre-training with Synthetic Caption</title><link>http://arxiv.org/abs/2308.08428v1</link><description>Contrastive Language-Image Pre-training (CLIP) has significantly boosted theperformance of various vision-language tasks by scaling up the dataset withimage-text pairs collected from the web. However, the presence of intrinsicnoise and unmatched image-text pairs in web data can potentially affect theperformance of representation learning. To address this issue, we first utilizethe OFA model to generate synthetic captions that focus on the image content.The generated captions contain complementary information that is beneficial forpre-training. Then, we propose an Adaptive Language-Image Pre-training (ALIP),a bi-path model that integrates supervision from both raw text and syntheticcaption. As the core components of ALIP, the Language Consistency Gate (LCG)and Description Consistency Gate (DCG) dynamically adjust the weights ofsamples and image-text/caption pairs during the training process. Meanwhile,the adaptive contrastive loss can effectively reduce the impact of noise dataand enhances the efficiency of pre-training data. We validate ALIP withexperiments on different scales of models and pre-training datasets.Experiments results show that ALIP achieves state-of-the-art performance onmultiple downstream tasks including zero-shot image-text retrieval and linearprobe. To facilitate future research, the code and pre-trained models arereleased at https://github.com/deepglint/ALIP.</description><author>Kaicheng Yang, Jiankang Deng, Xiang An, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, Tongliang Liu</author><pubDate>Wed, 16 Aug 2023 16:19:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08428v1</guid></item><item><title>Cross Contrasting Feature Perturbation for Domain Generalization</title><link>http://arxiv.org/abs/2307.12502v2</link><description>Domain generalization (DG) aims to learn a robust model from source domainsthat generalize well on unseen target domains. Recent studies focus ongenerating novel domain samples or features to diversify distributionscomplementary to source domains. Yet, these approaches can hardly deal with therestriction that the samples synthesized from various domains can causesemantic distortion. In this paper, we propose an online one-stage CrossContrasting Feature Perturbation (CCFP) framework to simulate domain shift bygenerating perturbed features in the latent space while regularizing the modelprediction against domain shift. Different from the previous fixed synthesizingstrategy, we design modules with learnable feature perturbations and semanticconsistency constraints. In contrast to prior work, our method does not use anygenerative-based models or domain labels. We conduct extensive experiments on astandard DomainBed benchmark with a strict evaluation protocol for a faircomparison. Comprehensive experiments show that our method outperforms theprevious state-of-the-art, and quantitative analyses illustrate that ourapproach can alleviate the domain shift problem in out-of-distribution (OOD)scenarios.</description><author>Chenming Li, Daoan Zhang, Wenjian Huang, Jianguo Zhang</author><pubDate>Wed, 16 Aug 2023 16:19:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12502v2</guid></item><item><title>Eliciting Risk Aversion with Inverse Reinforcement Learning via Interactive Questioning</title><link>http://arxiv.org/abs/2308.08427v1</link><description>This paper proposes a novel framework for identifying an agent's riskaversion using interactive questioning. Our study is conducted in twoscenarios: a one-period case and an infinite horizon case. In the one-periodcase, we assume that the agent's risk aversion is characterized by a costfunction of the state and a distortion risk measure. In the infinite horizoncase, we model risk aversion with an additional component, a discount factor.Assuming the access to a finite set of candidates containing the agent's truerisk aversion, we show that asking the agent to demonstrate her optimalpolicies in various environment, which may depend on their previous answers, isan effective means of identifying the agent's risk aversion. Specifically, weprove that the agent's risk aversion can be identified as the number ofquestions tends to infinity, and the questions are randomly designed. We alsodevelop an algorithm for designing optimal questions and provide empiricalevidence that our method learns risk aversion significantly faster thanrandomly designed questions in simulations. Our framework has importantapplications in robo-advising and provides a new approach for identifying anagent's risk preferences.</description><author>Ziteng Cheng, Anthony Coache, Sebastian Jaimungal</author><pubDate>Wed, 16 Aug 2023 16:17:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08427v1</guid></item><item><title>EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones</title><link>http://arxiv.org/abs/2211.09703v3</link><description>The superior performance of modern deep networks usually comes with a costlytraining procedure. This paper presents a new curriculum learning approach forthe efficient training of visual backbones (e.g., vision Transformers). Ourwork is inspired by the inherent learning dynamics of deep networks: weexperimentally show that at an earlier training stage, the model mainly learnsto recognize some 'easier-to-learn' discriminative patterns within eachexample, e.g., the lower-frequency components of images and the originalinformation before data augmentation. Driven by this phenomenon, we propose acurriculum where the model always leverages all the training data at eachepoch, while the curriculum starts with only exposing the 'easier-to-learn'patterns of each example, and introduces gradually more difficult patterns. Toimplement this idea, we 1) introduce a cropping operation in the Fourierspectrum of the inputs, which enables the model to learn from only thelower-frequency components efficiently, 2) demonstrate that exposing thefeatures of original images amounts to adopting weaker data augmentation, and3) integrate 1) and 2) and design a curriculum learning schedule with agreedy-search algorithm. The resulting approach, EfficientTrain, is simple,general, yet surprisingly effective. As an off-the-shelf method, it reduces thewall-time training cost of a wide variety of popular models (e.g., ResNet,ConvNeXt, DeiT, PVT, Swin, and CSWin) by &gt;1.5x on ImageNet-1K/22K withoutsacrificing accuracy. It is also effective for self-supervised learning (e.g.,MAE). Code is available at https://github.com/LeapLabTHU/EfficientTrain.</description><author>Yulin Wang, Yang Yue, Rui Lu, Tianjiao Liu, Zhao Zhong, Shiji Song, Gao Huang</author><pubDate>Wed, 16 Aug 2023 16:16:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09703v3</guid></item><item><title>Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer</title><link>http://arxiv.org/abs/2308.08414v1</link><description>Video-language pre-trained models have shown remarkable success in guidingvideo question-answering (VideoQA) tasks. However, due to the length of videosequences, training large-scale video-based models incurs considerably highercosts than training image-based ones. This motivates us to leverage theknowledge from image-based pretraining, despite the obvious gaps between imageand video domains. To bridge these gaps, in this paper, we propose Tem-Adapter,which enables the learning of temporal dynamics and complex semantics by avisual Temporal Aligner and a textual Semantic Aligner. Unlike conventionalpretrained knowledge adaptation methods that only concentrate on the downstreamtask objective, the Temporal Aligner introduces an extra language-guidedautoregressive task aimed at facilitating the learning of temporaldependencies, with the objective of predicting future states based onhistorical clues and language guidance that describes event progression.Besides, to reduce the semantic gap and adapt the textual representation forbetter event description, we introduce a Semantic Aligner that first designs atemplate to fuse question and answer pairs as event descriptions and thenlearns a Transformer decoder with the whole video sequence as guidance forrefinement. We evaluate Tem-Adapter and different pre-train transferringmethods on two VideoQA benchmarks, and the significant performance improvementdemonstrates the effectiveness of our method.</description><author>Guangyi Chen, Xiao Liu, Guangrun Wang, Kun Zhang, Philip H. S. Torr, Xiao-Ping Zhang, Yansong Tang</author><pubDate>Wed, 16 Aug 2023 16:00:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08414v1</guid></item><item><title>Knowledge-Enhanced Multi-Label Few-Shot Product Attribute-Value Extraction</title><link>http://arxiv.org/abs/2308.08413v1</link><description>Existing attribute-value extraction (AVE) models require large quantities oflabeled data for training. However, new products with new attribute-value pairsenter the market every day in real-world e-Commerce. Thus, we formulate AVE inmulti-label few-shot learning (FSL), aiming to extract unseen attribute valuepairs based on a small number of training examples. We propose aKnowledge-Enhanced Attentive Framework (KEAF) based on prototypical networks,leveraging the generated label description and category information to learnmore discriminative prototypes. Besides, KEAF integrates with hybrid attentionto reduce noise and capture more informative semantics for each class bycalculating the label-relevant and query-related weights. To achievemulti-label inference, KEAF further learns a dynamic threshold by integratingthe semantic information from both the support set and the query set. Extensiveexperiments with ablation studies conducted on two datasets demonstrate thatKEAF outperforms other SOTA models for information extraction in FSL. The codecan be found at: https://github.com/gjiaying/KEAF</description><author>Jiaying Gong, Wei-Te Chen, Hoda Eldardiry</author><pubDate>Wed, 16 Aug 2023 15:58:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08413v1</guid></item><item><title>Digital twinning of cardiac electrophysiology models from the surface ECG: a geodesic backpropagation approach</title><link>http://arxiv.org/abs/2308.08410v1</link><description>The eikonal equation has become an indispensable tool for modeling cardiacelectrical activation accurately and efficiently. In principle, by matchingclinically recorded and eikonal-based electrocardiograms (ECGs), it is possibleto build patient-specific models of cardiac electrophysiology in a purelynon-invasive manner. Nonetheless, the fitting procedure remains a challengingtask. The present study introduces a novel method, Geodesic-BP, to solve theinverse eikonal problem. Geodesic-BP is well-suited for GPU-accelerated machinelearning frameworks, allowing us to optimize the parameters of the eikonalequation to reproduce a given ECG. We show that Geodesic-BP can reconstruct asimulated cardiac activation with high accuracy in a synthetic test case, evenin the presence of modeling inaccuracies. Furthermore, we apply our algorithmto a publicly available dataset of a rabbit model, with very positive results.Given the future shift towards personalized medicine, Geodesic-BP has thepotential to help in future functionalizations of cardiac models meetingclinical time constraints while maintaining the physiological accuracy ofstate-of-the-art cardiac models.</description><author>Thomas Grandits, Jan Verhülsdonk, Gundolf Haase, Alexander Effland, Simone Pezzuto</author><pubDate>Wed, 16 Aug 2023 15:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08410v1</guid></item><item><title>LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models</title><link>http://arxiv.org/abs/2307.07889v2</link><description>Current developments in large language models (LLMs) have enabled impressivezero-shot capabilities across various natural language tasks. An interestingapplication of these systems is in the automated assessment of natural languagegeneration (NLG), a highly challenging area with great practical benefit. Inthis paper, we explore two options for exploiting the emergent abilities ofLLMs for zero-shot NLG assessment: absolute score prediction, and comparativeassessment which uses relative comparisons between pairs of candidates. Thoughcomparative assessment has not been extensively studied in NLG assessment, wenote that humans often find it more intuitive to compare two options ratherthan scoring each one independently. This work examines comparative assessmentfrom multiple perspectives: performance compared to absolute grading;positional biases in the prompt; and efficient ranking in terms of the numberof comparisons. We illustrate that LLM comparative assessment is a simple,general and effective approach for NLG assessment. For moderate-sizedopen-source LLMs, such as FlanT5 and Llama2-chat, comparative assessment issuperior to prompt scoring, and in many cases can achieve performancecompetitive with state-of-the-art methods. Additionally, we demonstrate thatLLMs often exhibit strong positional biases when making pairwise comparisons,and we propose debiasing methods that can further improve performance.</description><author>Adian Liusie, Potsawee Manakul, Mark J. F. Gales</author><pubDate>Wed, 16 Aug 2023 15:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07889v2</guid></item><item><title>Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities</title><link>http://arxiv.org/abs/2308.08407v1</link><description>Recent advancements in AI applications to healthcare have shown incrediblepromise in surpassing human performance in diagnosis and disease prognosis.With the increasing complexity of AI models, however, concerns regarding theiropacity, potential biases, and the need for interpretability. To ensure trustand reliability in AI systems, especially in clinical risk prediction models,explainability becomes crucial. Explainability is usually referred to as an AIsystem's ability to provide a robust interpretation of its decision-makinglogic or the decisions themselves to human stakeholders. In clinical riskprediction, other aspects of explainability like fairness, bias, trust, andtransparency also represent important concepts beyond just interpretability. Inthis review, we address the relationship between these concepts as they areoften used together or interchangeably. This review also discusses recentprogress in developing explainable models for clinical risk prediction,highlighting the importance of quantitative and clinical evaluation andvalidation across multiple common modalities in clinical practice. Itemphasizes the need for external validation and the combination of diverseinterpretability methods to enhance trust and fairness. Adopting rigoroustesting, such as using synthetic datasets with known generative factors, canfurther improve the reliability of explainability methods. Open access andcode-sharing resources are essential for transparency and reproducibility,enabling the growth and trustworthiness of explainable research. Whilechallenges exist, an end-to-end approach to explainability in clinical riskprediction, incorporating stakeholders from clinicians to developers, isessential for success.</description><author>Munib Mesinovic, Peter Watkinson, Tingting Zhu</author><pubDate>Wed, 16 Aug 2023 15:51:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08407v1</guid></item><item><title>Content-based Recommendation Engine for Video Streaming Platform</title><link>http://arxiv.org/abs/2308.08406v1</link><description>Recommendation engine suggest content, product or services to the user byusing machine learning algorithm. This paper proposed a content-basedrecommendation engine for providing video suggestion to the user based on theirprevious interests and choices. We will use TF-IDF text vectorization method todetermine the relevance of words in a document. Then we will find out thesimilarity between each content by calculating cosine similarity between them.Finally, engine will recommend videos to the users based on the obtainedsimilarity score value. In addition, we will measure the engine's performanceby computing precision, recall, and F1 core of the proposed system.</description><author>Puskal Khadka, Prabhav Lamichhane</author><pubDate>Wed, 16 Aug 2023 15:50:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08406v1</guid></item><item><title>QBSD: Quartile-Based Seasonality Decomposition for Cost-Effective Time Series Forecasting</title><link>http://arxiv.org/abs/2306.05989v2</link><description>In the telecom domain, precise forecasting of time series patterns, such ascell key performance indicators (KPIs), plays a pivotal role in enhancingservice quality and operational efficiency. State-of-the-art forecastingapproaches prioritize forecasting accuracy at the expense of computationalperformance, rendering them less suitable for data-intensive applicationsencompassing systems with a multitude of time series variables. To address thisissue, we introduce QBSD, a live forecasting approach tailored to optimize thetrade-off between accuracy and computational complexity. We have evaluated theperformance of QBSD against state-of-the-art forecasting approaches on publiclyavailable datasets. We have also extended this investigation to our curatednetwork KPI dataset, now publicly accessible, to showcase the effect of dynamicoperating ranges that varies with time. The results demonstrate that theproposed method excels in runtime efficiency compared to the leading algorithmsavailable while maintaining competitive forecast accuracy.</description><author>Ebenezer RHP Isaac, Bulbul Singh</author><pubDate>Wed, 16 Aug 2023 15:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05989v2</guid></item><item><title>DiffIR: Efficient Diffusion Model for Image Restoration</title><link>http://arxiv.org/abs/2303.09472v3</link><description>Diffusion model (DM) has achieved SOTA performance by modeling the imagesynthesis process into a sequential application of a denoising network.However, different from image synthesis, image restoration (IR) has a strongconstraint to generate results in accordance with ground-truth. Thus, for IR,traditional DMs running massive iterations on a large model to estimate wholeimages or feature maps is inefficient. To address this issue, we propose anefficient DM for IR (DiffIR), which consists of a compact IR prior extractionnetwork (CPEN), dynamic IR transformer (DIRformer), and denoising network.Specifically, DiffIR has two training stages: pretraining and training DM. Inpretraining, we input ground-truth images into CPEN$_{S1}$ to capture a compactIR prior representation (IPR) to guide DIRformer. In the second stage, we trainthe DM to directly estimate the same IRP as pretrained CPEN$_{S1}$ only usingLQ images. We observe that since the IPR is only a compact vector, DiffIR canuse fewer iterations than traditional DM to obtain accurate estimations andgenerate more stable and realistic results. Since the iterations are few, ourDiffIR can adopt a joint optimization of CPEN$_{S2}$, DIRformer, and denoisingnetwork, which can further reduce the estimation error influence. We conductextensive experiments on several IR tasks and achieve SOTA performance whileconsuming less computational costs. Code is available at\url{https://github.com/Zj-BinXia/DiffIR}.</description><author>Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, Luc Van Gool</author><pubDate>Wed, 16 Aug 2023 15:36:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09472v3</guid></item><item><title>Prediction of post-radiotherapy recurrence volumes in head and neck squamous cell carcinoma using 3D U-Net segmentation</title><link>http://arxiv.org/abs/2308.08396v1</link><description>Locoregional recurrences (LRR) are still a frequent site of treatment failurefor head and neck squamous cell carcinoma (HNSCC) patients. Identification of high risk subvolumes based on pretreatment imaging is keyto biologically targeted radiation therapy. We investigated the extent to whicha Convolutional neural network (CNN) is able to predict LRR volumes based onpre-treatment 18F-fluorodeoxyglucose positron emission tomography(FDG-PET)/computed tomography (CT) scans in HNSCC patients and thus thepotential to identify biological high risk volumes using CNNs. For 37 patients who had undergone primary radiotherapy for oropharyngealsquamous cell carcinoma, five oncologists contoured the relapse volumes onrecurrence CT scans. Datasets of pre-treatment FDG-PET/CT, gross tumour volume(GTV) and contoured relapse for each of the patients were randomly divided intotraining (n=23), validation (n=7) and test (n=7) datasets. We compared a CNNtrained from scratch, a pre-trained CNN, a SUVmax threshold approach, and usingthe GTV directly. The SUVmax threshold method included 5 out of the 7 relapse origin pointswithin a volume of median 4.6 cubic centimetres (cc). Both the GTV contour andbest CNN segmentations included the relapse origin 6 out of 7 times with medianvolumes of 28 and 18 cc respectively. The CNN included the same or greater number of relapse volume POs, withsignificantly smaller relapse volumes. Our novel findings indicate that CNNsmay predict LRR, yet further work on dataset development is required to attainclinically useful prediction accuracy.</description><author>Denis Kutnár, Ivan R Vogelius, Katrin Elisabet Håkansson, Jens Petersen, Jeppe Friborg, Lena Specht, Mogens Bernsdorf, Anita Gothelf, Claus Kristensen, Abraham George Smith</author><pubDate>Wed, 16 Aug 2023 15:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08396v1</guid></item><item><title>SIGMA: Scale-Invariant Global Sparse Shape Matching</title><link>http://arxiv.org/abs/2308.08393v1</link><description>We propose a novel mixed-integer programming (MIP) formulation for generatingprecise sparse correspondences for highly non-rigid shapes. To this end, weintroduce a projected Laplace-Beltrami operator (PLBO) which combines intrinsicand extrinsic geometric information to measure the deformation quality inducedby predicted correspondences. We integrate the PLBO, together with anorientation-aware regulariser, into a novel MIP formulation that can be solvedto global optimality for many practical problems. In contrast to previousmethods, our approach is provably invariant to rigid transformations and globalscaling, initialisation-free, has optimality guarantees, and scales to highresolution meshes with (empirically observed) linear time. We showstate-of-the-art results for sparse non-rigid matching on several challenging3D datasets, including data with inconsistent meshing, as well as applicationsin mesh-to-point-cloud matching.</description><author>Maolin Gao, Paul Roetzer, Marvin Eisenberger, Zorah Lähner, Michael Moeller, Daniel Cremers, Florian Bernard</author><pubDate>Wed, 16 Aug 2023 15:25:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08393v1</guid></item><item><title>Fast Uncertainty Quantification of Spent Nuclear Fuel with Neural Networks</title><link>http://arxiv.org/abs/2308.08391v1</link><description>The accurate calculation and uncertainty quantification of thecharacteristics of spent nuclear fuel (SNF) play a crucial role in ensuring thesafety, efficiency, and sustainability of nuclear energy production, wastemanagement, and nuclear safeguards. State of the art physics-based models,while reliable, are computationally intensive and time-consuming. This paperpresents a surrogate modeling approach using neural networks (NN) to predict anumber of SNF characteristics with reduced computational costs compared tophysics-based models. An NN is trained using data generated from CASMO5 latticecalculations. The trained NN accurately predicts decay heat and nuclideconcentrations of SNF, as a function of key input parameters, such asenrichment, burnup, cooling time between cycles, mean boron concentration andfuel temperature. The model is validated against physics-based decay heatsimulations and measurements of different uranium oxide fuel assemblies fromtwo different pressurized water reactors. In addition, the NN is used toperform sensitivity analysis and uncertainty quantification. The results are invery good alignment to CASMO5, while the computational costs (taking intoaccount the costs of generating training samples) are reduced by a factor of 10or more. Our findings demonstrate the feasibility of using NNs as surrogatemodels for fast characterization of SNF, providing a promising avenue forimproving computational efficiency in assessing nuclear fuel behavior andassociated risks.</description><author>Arnau Albà, Andreas Adelmann, Lucas Münster, Dimitri Rochman, Romana Boiger</author><pubDate>Wed, 16 Aug 2023 15:23:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08391v1</guid></item><item><title>Continuous Sweep: an improved, binary quantifier</title><link>http://arxiv.org/abs/2308.08387v1</link><description>Quantification is a supervised machine learning task, focused on estimatingthe class prevalence of a dataset rather than labeling its individualobservations. We introduce Continuous Sweep, a new parametric binary quantifierinspired by the well-performing Median Sweep. Median Sweep is currently one ofthe best binary quantifiers, but we have changed this quantifier on threepoints, namely 1) using parametric class distributions instead of empiricaldistributions, 2) optimizing decision boundaries instead of applying discretedecision rules, and 3) calculating the mean instead of the median. We deriveanalytic expressions for the bias and variance of Continuous Sweep undergeneral model assumptions. This is one of the first theoretical contributionsin the field of quantification learning. Moreover, these derivations enable usto find the optimal decision boundaries. Finally, our simulation study showsthat Continuous Sweep outperforms Median Sweep in a wide range of situations.</description><author>Kevin Kloos, Julian D. Karch, Quinten A. Meertens, Mark de Rooij</author><pubDate>Wed, 16 Aug 2023 15:18:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08387v1</guid></item><item><title>Latent Dynamical Implicit Diffusion Processes</title><link>http://arxiv.org/abs/2306.07077v2</link><description>Latent dynamical models are commonly used to learn the distribution of alatent dynamical process that represents a sequence of noisy data samples.However, producing samples from such models with high fidelity is challengingdue to the complexity and variability of latent and observation dynamics.Recent advances in diffusion-based generative models, such as DDPM and NCSN,have shown promising alternatives to state-of-the-art latent generative models,such as Neural ODEs, RNNs, and Normalizing flow networks, for generatinghigh-quality sequential samples from a prior distribution. However, theirapplication in modeling sequential data with latent dynamical models is yet tobe explored. Here, we propose a novel latent variable model named latentdynamical implicit diffusion processes (LDIDPs), which utilizes implicitdiffusion processes to sample from dynamical latent processes and generatesequential observation samples accordingly. We tested LDIDPs on synthetic andsimulated neural decoding problems. We demonstrate that LDIDPs can accuratelylearn the dynamics over latent dimensions. Furthermore, the implicit samplingmethod allows for the computationally efficient generation of high-qualitysequential data samples from the latent and observation spaces.</description><author>Mohammad R. Rezaei</author><pubDate>Wed, 16 Aug 2023 15:16:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07077v2</guid></item><item><title>MixCycle: Mixup Assisted Semi-Supervised 3D Single Object Tracking with Cycle Consistency</title><link>http://arxiv.org/abs/2303.09219v2</link><description>3D single object tracking (SOT) is an indispensable part of automateddriving. Existing approaches rely heavily on large, densely labeled datasets.However, annotating point clouds is both costly and time-consuming. Inspired bythe great success of cycle tracking in unsupervised 2D SOT, we introduce thefirst semi-supervised approach to 3D SOT. Specifically, we introduce twocycle-consistency strategies for supervision: 1) Self tracking cycles, whichleverage labels to help the model converge better in the early stages oftraining; 2) forward-backward cycles, which strengthen the tracker's robustnessto motion variations and the template noise caused by the template updatestrategy. Furthermore, we propose a data augmentation strategy named SOTMixupto improve the tracker's robustness to point cloud diversity. SOTMixupgenerates training samples by sampling points in two point clouds with a mixingrate and assigns a reasonable loss weight for training according to the mixingrate. The resulting MixCycle approach generalizes to appearance matching-basedtrackers. On the KITTI benchmark, based on the P2B tracker, MixCycle trainedwith $\textbf{10\%}$ labels outperforms P2B trained with $\textbf{100\%}$labels, and achieves a $\textbf{28.4\%}$ precision improvement when using$\textbf{1\%}$ labels. Our code will be released at\url{https://github.com/Mumuqiao/MixCycle}.</description><author>Qiao Wu, Jiaqi Yang, Kun Sun, Chu'ai Zhang, Yanning Zhang, Mathieu Salzmann</author><pubDate>Wed, 16 Aug 2023 15:12:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09219v2</guid></item><item><title>Precision and Recall Reject Curves for Classification</title><link>http://arxiv.org/abs/2308.08381v1</link><description>For some classification scenarios, it is desirable to use only thoseclassification instances that a trained model associates with a high certainty.To obtain such high-certainty instances, previous work has proposedaccuracy-reject curves. Reject curves allow to evaluate and compare theperformance of different certainty measures over a range of thresholds foraccepting or rejecting classifications. However, the accuracy may not be themost suited evaluation metric for all applications, and instead precision orrecall may be preferable. This is the case, for example, for data withimbalanced class distributions. We therefore propose reject curves thatevaluate precision and recall, the recall-reject curve and the precision-rejectcurve. Using prototype-based classifiers from learning vector quantization, wefirst validate the proposed curves on artificial benchmark data against theaccuracy reject curve as a baseline. We then show on imbalanced benchmarks andmedical, real-world data that for these scenarios, the proposed precision- andrecall-curves yield more accurate insights into classifier performance thanaccuracy reject curves.</description><author>Lydia Fischer, Patricia Wollstadt</author><pubDate>Wed, 16 Aug 2023 15:09:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08381v1</guid></item><item><title>Robust Autonomous Vehicle Pursuit without Expert Steering Labels</title><link>http://arxiv.org/abs/2308.08380v1</link><description>In this work, we present a learning method for lateral and longitudinalmotion control of an ego-vehicle for vehicle pursuit. The car being controlleddoes not have a pre-defined route, rather it reactively adapts to follow atarget vehicle while maintaining a safety distance. To train our model, we donot rely on steering labels recorded from an expert driver but effectivelyleverage a classical controller as an offline label generation tool. Inaddition, we account for the errors in the predicted control values, which canlead to a loss of tracking and catastrophic crashes of the controlled vehicle.To this end, we propose an effective data augmentation approach, which allowsto train a network capable of handling different views of the target vehicle.During the pursuit, the target vehicle is firstly localized using aConvolutional Neural Network. The network takes a single RGB image along withcars' velocities and estimates the target vehicle's pose with respect to theego-vehicle. This information is then fed to a Multi-Layer Perceptron, whichregresses the control commands for the ego-vehicle, namely throttle andsteering angle. We extensively validate our approach using the CARLA simulatoron a wide range of terrains. Our method demonstrates real-time performance androbustness to different scenarios including unseen trajectories and high routecompletion. The project page containing code and multimedia can be publiclyaccessed here: https://changyaozhou.github.io/Autonomous-Vehicle-Pursuit/.</description><author>Jiaxin Pan, Changyao Zhou, Mariia Gladkova, Qadeer Khan, Daniel Cremers</author><pubDate>Wed, 16 Aug 2023 15:09:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08380v1</guid></item><item><title>A distributed neural network architecture for dynamic sensor selection with application to bandwidth-constrained body-sensor networks</title><link>http://arxiv.org/abs/2308.08379v1</link><description>We propose a dynamic sensor selection approach for deep neural networks(DNNs), which is able to derive an optimal sensor subset selection for eachspecific input sample instead of a fixed selection for the entire dataset. Thisdynamic selection is jointly learned with the task model in an end-to-end way,using the Gumbel-Softmax trick to allow the discrete decisions to be learnedthrough standard backpropagation. We then show how we can use this dynamicselection to increase the lifetime of a wireless sensor network (WSN) byimposing constraints on how often each node is allowed to transmit. We furtherimprove performance by including a dynamic spatial filter that makes thetask-DNN more robust against the fact that it now needs to be able to handle amultitude of possible node subsets. Finally, we explain how the selection ofthe optimal channels can be distributed across the different nodes in a WSN. Wevalidate this method on a use case in the context of body-sensor networks,where we use real electroencephalography (EEG) sensor data to emulate an EEGsensor network. We analyze the resulting trade-offs between transmission loadand task accuracy.</description><author>Thomas Strypsteen, Alexander Bertrand</author><pubDate>Wed, 16 Aug 2023 15:04:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08379v1</guid></item><item><title>LLM Cognitive Judgements Differ From Human</title><link>http://arxiv.org/abs/2307.11787v2</link><description>Large Language Models (LLMs) have lately been on the spotlight ofresearchers, businesses, and consumers alike. While the linguistic capabilitiesof such models have been studied extensively, there is growing interest ininvestigating them as cognitive subjects. In the present work I examine GPT-3and ChatGPT capabilities on an limited-data inductive reasoning task from thecognitive science literature. The results suggest that these models' cognitivejudgements are not human-like.</description><author>Sotiris Lamprinidis</author><pubDate>Wed, 16 Aug 2023 15:03:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11787v2</guid></item><item><title>Advancing continual lifelong learning in neural information retrieval: definition, dataset, framework, and empirical evaluation</title><link>http://arxiv.org/abs/2308.08378v1</link><description>Continual learning refers to the capability of a machine learning model tolearn and adapt to new information, without compromising its performance onpreviously learned tasks. Although several studies have investigated continuallearning methods for information retrieval tasks, a well-defined taskformulation is still lacking, and it is unclear how typical learning strategiesperform in this context. To address this challenge, a systematic taskformulation of continual neural information retrieval is presented, along witha multiple-topic dataset that simulates continuous information retrieval. Acomprehensive continual neural information retrieval framework consisting oftypical retrieval models and continual learning strategies is then proposed.Empirical evaluations illustrate that the proposed framework can successfullyprevent catastrophic forgetting in neural information retrieval and enhanceperformance on previously learned tasks. The results indicate thatembedding-based retrieval models experience a decline in their continuallearning performance as the topic shift distance and dataset volume of newtasks increase. In contrast, pretraining-based models do not show any suchcorrelation. Adopting suitable learning strategies can mitigate the effects oftopic shift and data augmentation.</description><author>Jingrui Hou, Georgina Cosma, Axel Finke</author><pubDate>Wed, 16 Aug 2023 15:01:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08378v1</guid></item><item><title>Automated Semiconductor Defect Inspection in Scanning Electron Microscope Images: a Systematic Review</title><link>http://arxiv.org/abs/2308.08376v1</link><description>A growing need exists for efficient and accurate methods for detectingdefects in semiconductor materials and devices. These defects can have adetrimental impact on the efficiency of the manufacturing process, because theycause critical failures and wafer-yield limitations. As nodes and patterns getsmaller, even high-resolution imaging techniques such as Scanning ElectronMicroscopy (SEM) produce noisy images due to operating close to sensitivitylevels and due to varying physical properties of different underlayers orresist materials. This inherent noise is one of the main challenges for defectinspection. One promising approach is the use of machine learning algorithms,which can be trained to accurately classify and locate defects in semiconductorsamples. Recently, convolutional neural networks have proved to be particularlyuseful in this regard. This systematic review provides a comprehensive overviewof the state of automated semiconductor defect inspection on SEM images,including the most recent innovations and developments. 38 publications wereselected on this topic, indexed in IEEE Xplore and SPIE databases. For each ofthese, the application, methodology, dataset, results, limitations and futurework were summarized. A comprehensive overview and analysis of their methods isprovided. Finally, promising avenues for future work in the field of SEM-baseddefect inspection are suggested.</description><author>Thibault Lechien, Enrique Dehaerne, Bappaditya Dey, Victor Blanco, Stefan De Gendt, Wannes Meert</author><pubDate>Wed, 16 Aug 2023 14:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08376v1</guid></item><item><title>Bluetooth and WiFi Dataset for Real World RF Fingerprinting of Commercial Devices</title><link>http://arxiv.org/abs/2303.13538v2</link><description>RF fingerprinting is emerging as a physical layer security scheme to identifyillegitimate and/or unauthorized emitters sharing the RF spectrum. However, dueto the lack of publicly accessible real-world datasets, most research focuseson generating synthetic waveforms with software-defined radios (SDRs) which arenot suited for practical deployment settings. On other hand, the limiteddatasets that are available focus only on chipsets that generate only one kindof waveform. Commercial off-the-shelf (COTS) combo chipsets that support twowireless standards (for example WiFi and Bluetooth) over a shared dual-bandantenna such as those found in laptops, adapters, wireless chargers, RaspberryPis, among others are becoming ubiquitous in the IoT realm. Hence, to keep upwith the modern IoT environment, there is a pressing need for real-world opendatasets capturing emissions from these combo chipsets transmittingheterogeneous communication protocols. To this end, we capture the first knownemissions from the COTS IoT chipsets transmitting WiFi and Bluetooth under twodifferent time frames. The different time frames are essential to rigorouslyevaluate the generalization capability of the models. To ensure widespread use,each capture within the comprehensive 72 GB dataset is long enough (40MSamples) to support diverse input tensor lengths and formats. Finally, thedataset also comprises emissions at varying signal powers to account for thefeeble to high signal strength emissions as encountered in a real-worldsetting.</description><author>Anu Jagannath, Zackary Kane, Jithin Jagannath</author><pubDate>Wed, 16 Aug 2023 14:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13538v2</guid></item><item><title>Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems</title><link>http://arxiv.org/abs/2307.16120v2</link><description>Combining the strengths of model-based iterative algorithms and data-drivendeep learning solutions, deep unrolling networks (DuNets) have become a populartool to solve inverse imaging problems. While DuNets have been successfullyapplied to many linear inverse problems, nonlinear problems tend to impair theperformance of the method. Inspired by momentum acceleration techniques thatare often used in optimization algorithms, we propose a recurrent momentumacceleration (RMA) framework that uses a long short-term memory recurrentneural network (LSTM-RNN) to simulate the momentum acceleration process. TheRMA module leverages the ability of the LSTM-RNN to learn and retain knowledgefrom the previous gradients. We apply RMA to two popular DuNets -- the learnedproximal gradient descent (LPGD) and the learned primal-dual (LPD) methods,resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental resultson two nonlinear inverse problems: a nonlinear deconvolution problem, and anelectrical impedance tomography problem with limited boundary measurements. Inthe first experiment we have observed that the improvement due to RMA largelyincreases with respect to the nonlinearity of the problem. The results of thesecond example further demonstrate that the RMA schemes can significantlyimprove the performance of DuNets in strongly ill-posed problems.</description><author>Qingping Zhou, Jiayu Qian, Junqi Tang, Jinglai Li</author><pubDate>Wed, 16 Aug 2023 14:58:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16120v2</guid></item><item><title>Echoes: Unsupervised Debiasing via Pseudo-bias Labeling in an Echo Chamber</title><link>http://arxiv.org/abs/2305.04043v2</link><description>Neural networks often learn spurious correlations when exposed to biasedtraining data, leading to poor performance on out-of-distribution data. Abiased dataset can be divided, according to biased features, into bias-alignedsamples (i.e., with biased features) and bias-conflicting samples (i.e.,without biased features). Recent debiasing works typically assume that no biaslabel is available during the training phase, as obtaining such information ischallenging and labor-intensive. Following this unsupervised assumption,existing methods usually train two models: a biased model specialized to learnbiased features and a target model that uses information from the biased modelfor debiasing. This paper first presents experimental analyses revealing thatthe existing biased models overfit to bias-conflicting samples in the trainingdata, which negatively impacts the debiasing performance of the target models.To address this issue, we propose a straightforward and effective method calledEchoes, which trains a biased model and a target model with a differentstrategy. We construct an "echo chamber" environment by reducing the weights ofsamples which are misclassified by the biased model, to ensure the biased modelfully learns the biased features without overfitting to the bias-conflictingsamples. The biased model then assigns lower weights on the bias-conflictingsamples. Subsequently, we use the inverse of the sample weights of the biasedmodel for training the target model. Experiments show that our approachachieves superior debiasing results compared to the existing baselines on bothsynthetic and real-world datasets. Our code is available athttps://github.com/isruihu/Echoes.</description><author>Rui Hu, Yahan Tu, Jitao Sang</author><pubDate>Wed, 16 Aug 2023 14:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04043v2</guid></item><item><title>PDPK: A Framework to Synthesise Process Data and Corresponding Procedural Knowledge for Manufacturing</title><link>http://arxiv.org/abs/2308.08371v1</link><description>Procedural knowledge describes how to accomplish tasks and mitigate problems.Such knowledge is commonly held by domain experts, e.g. operators inmanufacturing who adjust parameters to achieve quality targets. To the best ofour knowledge, no real-world datasets containing process data and correspondingprocedural knowledge are publicly available, possibly due to corporateapprehensions regarding the loss of knowledge advances. Therefore, we provide aframework to generate synthetic datasets that can be adapted to differentdomains. The design choices are inspired by two real-world datasets ofprocedural knowledge we have access to. Apart from containing representationsof procedural knowledge in Resource Description Framework (RDF)-compliantknowledge graphs, the framework simulates parametrisation processes andprovides consistent process data. We compare established embedding methods onthe resulting knowledge graphs, detailing which out-of-the-box methods have thepotential to represent procedural knowledge. This provides a baseline which canbe used to increase the comparability of future work. Furthermore, we validatethe overall characteristics of a synthesised dataset by comparing the resultsto those achievable on a real-world dataset. The framework and evaluation code,as well as the dataset used in the evaluation, are available open source.</description><author>Richard Nordsieck, André Schweizer, Michael Heider, Jörg Hähner</author><pubDate>Wed, 16 Aug 2023 14:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08371v1</guid></item><item><title>Agglomerative Transformer for Human-Object Interaction Detection</title><link>http://arxiv.org/abs/2308.08370v1</link><description>We propose an agglomerative Transformer (AGER) that enables Transformer-basedhuman-object interaction (HOI) detectors to flexibly exploit extrainstance-level cues in a single-stage and end-to-end manner for the first time.AGER acquires instance tokens by dynamically clustering patch tokens andaligning cluster centers to instances with textual guidance, thus enjoying twobenefits: 1) Integrality: each instance token is encouraged to contain alldiscriminative feature regions of an instance, which demonstrates a significantimprovement in the extraction of different instance-level cues and subsequentlyleads to a new state-of-the-art performance of HOI detection with 36.75 mAP onHICO-Det. 2) Efficiency: the dynamical clustering mechanism allows AGER togenerate instance tokens jointly with the feature learning of the Transformerencoder, eliminating the need of an additional object detector or instancedecoder in prior methods, thus allowing the extraction of desirable extra cuesfor HOI detection in a single-stage and end-to-end pipeline. Concretely, AGERreduces GFLOPs by 8.5% and improves FPS by 36%, even compared to a vanillaDETR-like pipeline without extra cue extraction.</description><author>Danyang Tu, Wei Sun, Guangtao Zhai, Wei Shen</author><pubDate>Wed, 16 Aug 2023 14:48:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08370v1</guid></item><item><title>Diff-CAPTCHA: An Image-based CAPTCHA with Security Enhanced by Denoising Diffusion Model</title><link>http://arxiv.org/abs/2308.08367v1</link><description>To enhance the security of text CAPTCHAs, various methods have been employed,such as adding the interference lines on the text, randomly distorting thecharacters, and overlapping multiple characters. These methods partly increasethe difficulty of automated segmentation and recognition attacks. However,facing the rapid development of the end-to-end breaking algorithms, theirsecurity has been greatly weakened. The diffusion model is a novel imagegeneration model that can generate the text images with deep fusion ofcharacters and background images. In this paper, an image-click CAPTCHA schemecalled Diff-CAPTCHA is proposed based on denoising diffusion models. Thebackground image and characters of the CAPTCHA are treated as a whole to guidethe generation process of a diffusion model, thus weakening the characterfeatures available for machine learning, enhancing the diversity of characterfeatures in the CAPTCHA, and increasing the difficulty of breaking algorithms.To evaluate the security of Diff-CAPTCHA, this paper develops several attackmethods, including end-to-end attacks based on Faster R-CNN and two-stageattacks, and Diff-CAPTCHA is compared with three baseline schemes, includingcommercial CAPTCHA scheme and security-enhanced CAPTCHA scheme based on styletransfer. The experimental results show that diffusion models can effectivelyenhance CAPTCHA security while maintaining good usability in human testing.</description><author>Ran Jiang, Sanfeng Zhang, Linfeng Liu, Yanbing Peng</author><pubDate>Wed, 16 Aug 2023 14:41:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08367v1</guid></item><item><title>Dual-Branch Temperature Scaling Calibration for Long-Tailed Recognition</title><link>http://arxiv.org/abs/2308.08366v1</link><description>The calibration for deep neural networks is currently receiving widespreadattention and research. Miscalibration usually leads to overconfidence of themodel. While, under the condition of long-tailed distribution of data, theproblem of miscalibration is more prominent due to the different confidencelevels of samples in minority and majority categories, and it will result inmore serious overconfidence. To address this problem, some current researchhave designed diverse temperature coefficients for different categories basedon temperature scaling (TS) method. However, in the case of rare samples inminority classes, the temperature coefficient is not generalizable, and thereis a large difference between the temperature coefficients of the training setand the validation set. To solve this challenge, this paper proposes adual-branch temperature scaling calibration model (Dual-TS), which considersthe diversities in temperature parameters of different categories and thenon-generalizability of temperature parameters for rare samples in minorityclasses simultaneously. Moreover, we noticed that the traditional calibrationevaluation metric, Excepted Calibration Error (ECE), gives a higher weight tolow-confidence samples in the minority classes, which leads to inaccurateevaluation of model calibration. Therefore, we also propose Equal Sample BinExcepted Calibration Error (Esbin-ECE) as a new calibration evaluation metric.Through experiments, we demonstrate that our model yields state-of-the-art inboth traditional ECE and Esbin-ECE metrics.</description><author>Jialin Guo, Zhenyu Wu, Zhiqiang Zhan, Yang Ji</author><pubDate>Wed, 16 Aug 2023 14:40:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08366v1</guid></item><item><title>DeepContrast: Deep Tissue Contrast Enhancement using Synthetic Data Degradations and OOD Model Predictions</title><link>http://arxiv.org/abs/2308.08365v1</link><description>Microscopy images are crucial for life science research, allowing detailedinspection and characterization of cellular and tissue-level structures andfunctions. However, microscopy data are unavoidably affected by imagedegradations, such as noise, blur, or others. Many such degradations alsocontribute to a loss of image contrast, which becomes especially pronounced indeeper regions of thick samples. Today, best performing methods to increase thequality of images are based on Deep Learning approaches, which typicallyrequire ground truth (GT) data during training. Our inability to counteractblurring and contrast loss when imaging deep into samples prevents theacquisition of such clean GT data. The fact that the forward process ofblurring and contrast loss deep into tissue can be modeled, allowed us topropose a new method that can circumvent the problem of unobtainable GT data.To this end, we first synthetically degraded the quality of microscopy imageseven further by using an approximate forward model for deep tissue imagedegradations. Then we trained a neural network that learned the inverse of thisdegradation function from our generated pairs of raw and degraded images. Wedemonstrated that networks trained in this way can be used out-of-distribution(OOD) to improve the quality of less severely degraded images, e.g. the rawdata imaged in a microscope. Since the absolute level of degradation in suchmicroscopy images can be stronger than the additional degradation introduced byour forward model, we also explored the effect of iterative predictions. Here,we observed that in each iteration the measured image contrast kept improvingwhile detailed structures in the images got increasingly removed. Therefore,dependent on the desired downstream analysis, a balance between contrastimprovement and retention of image details has to be found.</description><author>Nuno Pimpão Martins, Yannis Kalaidzidis, Marino Zerial, Florian Jug</author><pubDate>Wed, 16 Aug 2023 14:40:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08365v1</guid></item><item><title>SummHelper: Collaborative Human-Computer Summarization</title><link>http://arxiv.org/abs/2308.08363v1</link><description>Current approaches for text summarization are predominantly automatic, withrather limited space for human intervention and control over the process. Inthis paper, we introduce SummHelper, a 2-phase summarization assistant designedto foster human-machine collaboration. The initial phase involves contentselection, where the system recommends potential content, allowing users toaccept, modify, or introduce additional selections. The subsequent phase,content consolidation, involves SummHelper generating a coherent summary fromthese selections, which users can then refine using visual mappings between thesummary and the source text. Small-scale user studies reveal the effectivenessof our application, with participants being especially appreciative of thebalance between automated guidance and opportunities for personal input.</description><author>Aviv Slobodkin, Niv Nachum, Shmuel Amar, Ori Shapira, Ido Dagan</author><pubDate>Wed, 16 Aug 2023 14:39:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08363v1</guid></item><item><title>KernelWarehouse: Towards Parameter-Efficient Dynamic Convolution</title><link>http://arxiv.org/abs/2308.08361v1</link><description>Dynamic convolution learns a linear mixture of $n$ static kernels weightedwith their sample-dependent attentions, demonstrating superior performancecompared to normal convolution. However, existing designs areparameter-inefficient: they increase the number of convolutional parameters by$n$ times. This and the optimization difficulty lead to no research progress indynamic convolution that can allow us to use a significant large value of $n$(e.g., $n&gt;100$ instead of typical setting $n&lt;10$) to push forward theperformance boundary. In this paper, we propose $KernelWarehouse$, a moregeneral form of dynamic convolution, which can strike a favorable trade-offbetween parameter efficiency and representation power. Its key idea is toredefine the basic concepts of "$kernels$" and "$assembling$ $kernels$" indynamic convolution from the perspective of reducing kernel dimension andincreasing kernel number significantly. In principle, KernelWarehouse enhancesconvolutional parameter dependencies within the same layer and acrosssuccessive layers via tactful kernel partition and warehouse sharing, yieldinga high degree of freedom to fit a desired parameter budget. We validate ourmethod on ImageNet and MS-COCO datasets with different ConvNet architectures,and show that it attains state-of-the-art results. For instance, theResNet18|ResNet50|MobileNetV2|ConvNeXt-Tiny model trained with KernelWarehouseon ImageNet reaches 76.05%|81.05%|75.52%|82.51% top-1 accuracy. Thanks to itsflexible design, KernelWarehouse can even reduce the model size of a ConvNetwhile improving the accuracy, e.g., our ResNet18 model with 36.45%|65.10%parameter reduction to the baseline shows 2.89%|2.29% absolute improvement totop-1 accuracy.</description><author>Chao Li, Anbang Yao</author><pubDate>Wed, 16 Aug 2023 14:35:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08361v1</guid></item><item><title>SpecInfer: Accelerating Generative Large Language Model Serving with Speculative Inference and Token Tree Verification</title><link>http://arxiv.org/abs/2305.09781v2</link><description>The high computational and memory requirements of generative large languagemodels (LLMs) make it challenging to serve them quickly and cheaply. This paperintroduces SpecInfer, an LLM serving system that accelerates generative LLMinference with speculative inference and token tree verification. A key insightbehind Specinfer is to combine various collectively boost-tuned small languagemodels to jointly predict the LLM's outputs; the predictions are organized as atoken tree, whose nodes each represent a candidate token sequence. Thecorrectness of all candidate token sequences represented by a token tree isverified against the LLM in parallel using a novel tree-based parallel decodingmechanism. SpecInfer uses an LLM as a token tree verifier instead of anincremental decoder, which significantly reduces the end-to-end latency andcomputational requirement for serving generative LLMs while provably preservingmodel quality. Our evaluation shows that SpecInfer outperforms existing LLMserving systems by 1.3-2.4x for distributed LLM inference and by 2.6-3.5x foroffloading-based LLM inference, while preserving the same generativeperformance. SpecInfer is publicly available athttps://github.com/flexflow/FlexFlow/tree/inference.</description><author>Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, Zhihao Jia</author><pubDate>Wed, 16 Aug 2023 14:33:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09781v2</guid></item><item><title>Independent Distribution Regularization for Private Graph Embedding</title><link>http://arxiv.org/abs/2308.08360v1</link><description>Learning graph embeddings is a crucial task in graph mining tasks. Aneffective graph embedding model can learn low-dimensional representations fromgraph-structured data for data publishing benefiting various downstreamapplications such as node classification, link prediction, etc. However, recentstudies have revealed that graph embeddings are susceptible to attributeinference attacks, which allow attackers to infer private node attributes fromthe learned graph embeddings. To address these concerns, privacy-preservinggraph embedding methods have emerged, aiming to simultaneously consider primarylearning and privacy protection through adversarial learning. However, mostexisting methods assume that representation models have access to all sensitiveattributes in advance during the training stage, which is not always the casedue to diverse privacy preferences. Furthermore, the commonly used adversariallearning technique in privacy-preserving representation learning suffers fromunstable training issues. In this paper, we propose a novel approach calledPrivate Variational Graph AutoEncoders (PVGAE) with the aid of independentdistribution penalty as a regularization term. Specifically, we split theoriginal variational graph autoencoder (VGAE) to learn sensitive andnon-sensitive latent representations using two sets of encoders. Additionally,we introduce a novel regularization to enforce the independence of theencoders. We prove the theoretical effectiveness of regularization from theperspective of mutual information. Experimental results on three real-worlddatasets demonstrate that PVGAE outperforms other baselines in privateembedding learning regarding utility performance and privacy protection.</description><author>Qi Hu, Yangqiu Song</author><pubDate>Wed, 16 Aug 2023 14:32:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08360v1</guid></item><item><title>Membrane Potential Batch Normalization for Spiking Neural Networks</title><link>http://arxiv.org/abs/2308.08359v1</link><description>As one of the energy-efficient alternatives of conventional neural networks(CNNs), spiking neural networks (SNNs) have gained more and more interestrecently. To train the deep models, some effective batch normalization (BN)techniques are proposed in SNNs. All these BNs are suggested to be used afterthe convolution layer as usually doing in CNNs. However, the spiking neuron ismuch more complex with the spatio-temporal dynamics. The regulated data flowafter the BN layer will be disturbed again by the membrane potential updatingoperation before the firing function, i.e., the nonlinear activation.Therefore, we advocate adding another BN layer before the firing function tonormalize the membrane potential again, called MPBN. To eliminate the inducedtime cost of MPBN, we also propose a training-inference-decoupledre-parameterization technique to fold the trained MPBN into the firingthreshold. With the re-parameterization technique, the MPBN will not introduceany extra time burden in the inference. Furthermore, the MPBN can also adoptthe element-wised form, while these BNs after the convolution layer can onlyuse the channel-wised form. Experimental results show that the proposed MPBNperforms well on both popular non-spiking static and neuromorphic datasets. Ourcode is open-sourced at \href{https://github.com/yfguo91/MPBN}{MPBN}.</description><author>Yufei Guo, Yuhan Zhang, Yuanpei Chen, Weihang Peng, Xiaode Liu, Liwen Zhang, Xuhui Huang, Zhe Ma</author><pubDate>Wed, 16 Aug 2023 14:32:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08359v1</guid></item><item><title>Convergence of Two-Layer Regression with Nonlinear Units</title><link>http://arxiv.org/abs/2308.08358v1</link><description>Large language models (LLMs), such as ChatGPT and GPT4, have shownoutstanding performance in many human life task. Attention computation plays animportant role in training LLMs. Softmax unit and ReLU unit are the keystructure in attention computation. Inspired by them, we put forward a softmaxReLU regression problem. Generally speaking, our goal is to find an optimalsolution to the regression problem involving the ReLU unit. In this work, wecalculate a close form representation for the Hessian of the loss function.Under certain assumptions, we prove the Lipschitz continuous and the PSDness ofthe Hessian. Then, we introduce an greedy algorithm based on approximate Newtonmethod, which converges in the sense of the distance to optimal solution. Last,We relax the Lipschitz condition and prove the convergence in the sense of lossvalue.</description><author>Yichuan Deng, Zhao Song, Shenghao Xie</author><pubDate>Wed, 16 Aug 2023 14:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08358v1</guid></item><item><title>Is Meta-Learning the Right Approach for the Cold-Start Problem in Recommender Systems?</title><link>http://arxiv.org/abs/2308.08354v1</link><description>Recommender systems have become fundamental building blocks of modern onlineproducts and services, and have a substantial impact on user experience. In thepast few years, deep learning methods have attracted a lot of research, and arenow heavily used in modern real-world recommender systems. Nevertheless,dealing with recommendations in the cold-start setting, e.g., when a user hasdone limited interactions in the system, is a problem that remains far fromsolved. Meta-learning techniques, and in particular optimization-basedmeta-learning, have recently become the most popular approaches in the academicresearch literature for tackling the cold-start problem in deep learning modelsfor recommender systems. However, current meta-learning approaches are notpractical for real-world recommender systems, which have billions of users anditems, and strict latency requirements. In this paper we show that it ispossible to obtaining similar, or higher, performance on commonly usedbenchmarks for the cold-start problem without using meta-learning techniques.In more detail, we show that, when tuned correctly, standard and widely adopteddeep learning models perform just as well as newer meta-learning models. Wefurther show that an extremely simple modular approach using commonrepresentation learning techniques, can perform comparably to meta-learningtechniques specifically designed for the cold-start setting while being muchmore easily deployable in real-world applications.</description><author>Davide Buffelli, Ashish Gupta, Agnieszka Strzalka, Vassilis Plachouras</author><pubDate>Wed, 16 Aug 2023 14:24:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08354v1</guid></item><item><title>Adaptive Segmentation Network for Scene Text Detection</title><link>http://arxiv.org/abs/2307.15029v2</link><description>Inspired by deep convolution segmentation algorithms, scene text detectorsbreak the performance ceiling of datasets steadily. However, these methodsoften encounter threshold selection bottlenecks and have poor performance ontext instances with extreme aspect ratios. In this paper, we propose toautomatically learn the discriminate segmentation threshold, whichdistinguishes text pixels from background pixels for segmentation-based scenetext detectors and then further reduces the time-consuming manual parameteradjustment. Besides, we design a Global-information Enhanced Feature PyramidNetwork (GE-FPN) for capturing text instances with macro size and extremeaspect ratios. Following the GE-FPN, we introduce a cascade optimizationstructure to further refine the text instances. Finally, together with theproposed threshold learning strategy and text detection structure, we design anAdaptive Segmentation Network (ASNet) for scene text detection. Extensiveexperiments are carried out to demonstrate that the proposed ASNet can achievethe state-of-the-art performance on four text detection benchmarks, i.e., ICDAR2015, MSRA-TD500, ICDAR 2017 MLT and CTW1500. The ablation experiments alsoverify the effectiveness of our contributions.</description><author>Guiqin Zhao</author><pubDate>Wed, 16 Aug 2023 14:22:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15029v2</guid></item><item><title>How does over-squashing affect the power of GNNs?</title><link>http://arxiv.org/abs/2306.03589v2</link><description>Graph Neural Networks (GNNs) are the state-of-the-art model for machinelearning on graph-structured data. The most popular class of GNNs operate byexchanging information between adjacent nodes, and are known as Message PassingNeural Networks (MPNNs). Given their widespread use, understanding theexpressive power of MPNNs is a key question. However, existing resultstypically consider settings with uninformative node features. In this paper, weprovide a rigorous analysis to determine which function classes of nodefeatures can be learned by an MPNN of a given capacity. We do so by measuringthe level of pairwise interactions between nodes that MPNNs allow for. Thismeasure provides a novel quantitative characterization of the so-calledover-squashing effect, which is observed to occur when a large volume ofmessages is aggregated into fixed-size vectors. Using our measure, we provethat, to guarantee sufficient communication between pairs of nodes, thecapacity of the MPNN must be large enough, depending on properties of the inputgraph structure, such as commute times. For many relevant scenarios, ouranalysis results in impossibility statements in practice, showing thatover-squashing hinders the expressive power of MPNNs. We validate ourtheoretical findings through extensive controlled experiments and ablationstudies.</description><author>Francesco Di Giovanni, T. Konstantin Rusch, Michael M. Bronstein, Andreea Deac, Marc Lackenby, Siddhartha Mishra, Petar Veličković</author><pubDate>Wed, 16 Aug 2023 14:17:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03589v2</guid></item><item><title>End-to-end Remote Sensing Change Detection of Unregistered Bi-temporal Images for Natural Disasters</title><link>http://arxiv.org/abs/2307.15128v2</link><description>Change detection based on remote sensing images has been a prominent area ofinterest in the field of remote sensing. Deep networks have demonstratedsignificant success in detecting changes in bi-temporal remote sensing imagesand have found applications in various fields. Given the degradation of naturalenvironments and the frequent occurrence of natural disasters, accurately andswiftly identifying damaged buildings in disaster-stricken areas through remotesensing images holds immense significance. This paper aims to investigatechange detection specifically for natural disasters. Considering that existingpublic datasets used in change detection research are registered, which doesnot align with the practical scenario where bi-temporal images are not matched,this paper introduces an unregistered end-to-end change detection syntheticdataset called xBD-E2ECD. Furthermore, we propose an end-to-end changedetection network named E2ECDNet, which takes an unregistered bi-temporal imagepair as input and simultaneously generates the flow field prediction result andthe change detection prediction result. It is worth noting that our E2ECDNetalso supports change detection for registered image pairs, as registration canbe seen as a special case of non-registration. Additionally, this paperredefines the criteria for correctly predicting a positive case and introducesneighborhood-based change detection evaluation metrics. The experimentalresults have demonstrated significant improvements.</description><author>Guiqin Zhao, Lianlei Shan, Weiqiang Wang</author><pubDate>Wed, 16 Aug 2023 14:15:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15128v2</guid></item><item><title>GAEI-UNet: Global Attention and Elastic Interaction U-Net for Vessel Image Segmentation</title><link>http://arxiv.org/abs/2308.08345v1</link><description>Vessel image segmentation plays a pivotal role in medical diagnostics, aidingin the early detection and treatment of vascular diseases. While segmentationbased on deep learning has shown promising results, effectively segmentingsmall structures and maintaining connectivity between them remains challenging.To address these limitations, we propose GAEI-UNet, a novel model that combinesglobal attention and elastic interaction-based techniques. GAEI-UNet leveragesglobal spatial and channel context information to enhance high-level semanticunderstanding within the U-Net architecture, enabling precise segmentation ofsmall vessels. Additionally, we adopt an elastic interaction-based lossfunction to improve connectivity among these fine structures. By capturing theforces generated by misalignment between target and predicted shapes, our modeleffectively learns to preserve the correct topology of vessel networks.Evaluation on retinal vessel dataset -- DRIVE demonstrates the superiorperformance of GAEI-UNet in terms of SE and connectivity of small structures,without significantly increasing computational complexity. This research aimsto advance the field of vessel image segmentation, providing more accurate andreliable diagnostic tools for the medical community. The implementation code isavailable on Code.</description><author>Ruiqiang Xiao, Zhuoyue Wan, Yang Xiang</author><pubDate>Wed, 16 Aug 2023 14:10:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08345v1</guid></item><item><title>Graph Out-of-Distribution Generalization with Controllable Data Augmentation</title><link>http://arxiv.org/abs/2308.08344v1</link><description>Graph Neural Network (GNN) has demonstrated extraordinary performance inclassifying graph properties. However, due to the selection bias of trainingand testing data (e.g., training on small graphs and testing on large graphs,or training on dense graphs and testing on sparse graphs), distributiondeviation is widespread. More importantly, we often observe \emph{hybridstructure distribution shift} of both scale and density, despite of one-sidedbiased data partition. The spurious correlations over hybrid distributiondeviation degrade the performance of previous GNN methods and show largeinstability among different datasets. To alleviate this problem, we propose\texttt{OOD-GMixup} to jointly manipulate the training distribution with\emph{controllable data augmentation} in metric space. Specifically, we firstextract the graph rationales to eliminate the spurious correlations due toirrelevant information. Secondly, we generate virtual samples with perturbationon graph rationale representation domain to obtain potential OOD trainingsamples. Finally, we propose OOD calibration to measure the distributiondeviation of virtual samples by leveraging Extreme Value Theory, and furtheractively control the training distribution by emphasizing the impact of virtualOOD samples. Extensive studies on several real-world datasets on graphclassification demonstrate the superiority of our proposed method overstate-of-the-art baselines.</description><author>Bin Lu, Xiaoying Gan, Ze Zhao, Shiyu Liang, Luoyi Fu, Xinbing Wang, Chenghu Zhou</author><pubDate>Wed, 16 Aug 2023 14:10:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08344v1</guid></item><item><title>LiDAR Meta Depth Completion</title><link>http://arxiv.org/abs/2307.12761v2</link><description>Depth estimation is one of the essential tasks to be addressed when creatingmobile autonomous systems. While monocular depth estimation methods haveimproved in recent times, depth completion provides more accurate and reliabledepth maps by additionally using sparse depth information from other sensorssuch as LiDAR. However, current methods are specifically trained for a singleLiDAR sensor. As the scanning pattern differs between sensors, every new sensorwould require re-training a specialized depth completion model, which iscomputationally inefficient and not flexible. Therefore, we propose todynamically adapt the depth completion model to the used sensor type enablingLiDAR adaptive depth completion. Specifically, we propose a meta depthcompletion network that uses data patterns derived from the data to learn atask network to alter weights of the main depth completion network to solve agiven depth completion task effectively. The method demonstrates a strongcapability to work on multiple LiDAR scanning patterns and can also generalizeto scanning patterns that are unseen during training. While using a singlemodel, our method yields significantly better results than a non-adaptivebaseline trained on different LiDAR patterns. It outperforms LiDAR-specificexpert models for very sparse cases. These advantages allow flexible deploymentof a single depth completion model on different sensors, which could also provevaluable to process the input of nascent LiDAR technology with adaptive insteadof fixed scanning patterns.</description><author>Wolfgang Boettcher, Lukas Hoyer, Ozan Unal, Ke Li, Dengxin Dai</author><pubDate>Wed, 16 Aug 2023 14:03:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12761v2</guid></item><item><title>Denoising Diffusion Probabilistic Model for Retinal Image Generation and Segmentation</title><link>http://arxiv.org/abs/2308.08339v1</link><description>Experts use retinal images and vessel trees to detect and diagnose variouseye, blood circulation, and brain-related diseases. However, manualsegmentation of retinal images is a time-consuming process that requires highexpertise and is difficult due to privacy issues. Many methods have beenproposed to segment images, but the need for large retinal image datasetslimits the performance of these methods. Several methods synthesize deeplearning models based on Generative Adversarial Networks (GAN) to generatelimited sample varieties. This paper proposes a novel Denoising DiffusionProbabilistic Model (DDPM) that outperformed GANs in image synthesis. Wedeveloped a Retinal Trees (ReTree) dataset consisting of retinal images,corresponding vessel trees, and a segmentation network based on DDPM trainedwith images from the ReTree dataset. In the first stage, we develop a two-stageDDPM that generates vessel trees from random numbers belonging to a standardnormal distribution. Later, the model is guided to generate fundus images fromgiven vessel trees and random distribution. The proposed dataset has beenevaluated quantitatively and qualitatively. Quantitative evaluation metricsinclude Frechet Inception Distance (FID) score, Jaccard similarity coefficient,Cohen's kappa, Matthew's Correlation Coefficient (MCC), precision, recall,F1-score, and accuracy. We trained the vessel segmentation model with syntheticdata to validate our dataset's efficiency and tested it on authentic data. Ourdeveloped dataset and source code is available athttps://github.com/AAleka/retree.</description><author>Alnur Alimanov, Md Baharul Islam</author><pubDate>Wed, 16 Aug 2023 14:01:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08339v1</guid></item><item><title>T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering</title><link>http://arxiv.org/abs/2305.03453v3</link><description>Large Language Models (LLMs) have recently demonstrated exceptionalperformance in various Natural Language Processing (NLP) tasks. They have alsoshown the ability to perform chain-of-thought (CoT) reasoning to solve complexproblems. Recent studies have explored CoT reasoning in complex multimodalscenarios, such as the science question answering task, by fine-tuningmultimodal models with high-quality human-annotated CoT rationales. However,collecting high-quality COT rationales is usually time-consuming and costly.Besides, the annotated rationales are hardly accurate due to the externalessential information missed. To address these issues, we propose a novelmethod termed \emph{T-SciQ} that aims at teaching science question answeringwith LLM signals. The T-SciQ approach generates high-quality CoT rationales asteaching signals and is advanced to train much smaller models to perform CoTreasoning in complex modalities. Additionally, we introduce a novel data mixingstrategy to produce more effective teaching data samples by policy for simpleand complex science question answer problems. Extensive experimental resultsshow that our T-SciQ method achieves a new state-of-the-art performance on theScienceQA benchmark, with an accuracy of 96.18\%. Moreover, our approachoutperforms the most powerful fine-tuned baseline by 4.5\%.</description><author>Lei Wang, Yi Hu, Jiabang He, Xing Xu, Ning Liu, Hui Liu, Heng Tao Shen</author><pubDate>Wed, 16 Aug 2023 13:50:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03453v3</guid></item><item><title>Learning Logic Programs by Discovering Higher-Order Abstractions</title><link>http://arxiv.org/abs/2308.08334v1</link><description>Discovering novel abstractions is important for human-level AI. We introducean approach to discover higher-order abstractions, such as map, filter, andfold. We focus on inductive logic programming, which induces logic programsfrom examples and background knowledge. We introduce the higher-orderrefactoring problem, where the goal is to compress a logic program byintroducing higher-order abstractions. We implement our approach in STEVIE,which formulates the higher-order refactoring problem as a constraintoptimisation problem. Our experimental results on multiple domains, includingprogram synthesis and visual reasoning, show that, compared to no refactoring,STEVIE can improve predictive accuracies by 27% and reduce learning times by47%. We also show that STEVIE can discover abstractions that transfer todifferent domains</description><author>Céline Hocquette, Sebastijan Dumančić, Andrew Cropper</author><pubDate>Wed, 16 Aug 2023 13:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08334v1</guid></item><item><title>Improving Depth Gradient Continuity in Transformers: A Comparative Study on Monocular Depth Estimation with CNN</title><link>http://arxiv.org/abs/2308.08333v1</link><description>Monocular depth estimation is an ongoing challenge in computer vision. Recentprogress with Transformer models has demonstrated notable advantages overconventional CNNs in this area. However, there's still a gap in understandinghow these models prioritize different regions in 2D images and how theseregions affect depth estimation performance. To explore the differences betweenTransformers and CNNs, we employ a sparse pixel approach to contrastivelyanalyze the distinctions between the two. Our findings suggest that whileTransformers excel in handling global context and intricate textures, they lagbehind CNNs in preserving depth gradient continuity. To further enhance theperformance of Transformer models in monocular depth estimation, we propose theDepth Gradient Refinement (DGR) module that refines depth estimation throughhigh-order differentiation, feature fusion, and recalibration. Additionally, weleverage optimal transport theory, treating depth maps as spatial probabilitydistributions, and employ the optimal transport distance as a loss function tooptimize our model. Experimental results demonstrate that models integratedwith the plug-and-play Depth Gradient Refinement (DGR) module and the proposedloss function enhance performance without increasing complexity andcomputational costs. This research not only offers fresh insights into thedistinctions between Transformers and CNNs in depth estimation but also pavesthe way for novel depth estimation methodologies.</description><author>Jiawei Yao, Tong Wu, Xiaofeng Zhang</author><pubDate>Wed, 16 Aug 2023 13:46:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08333v1</guid></item><item><title>AdaBrowse: Adaptive Video Browser for Efficient Continuous Sign Language Recognition</title><link>http://arxiv.org/abs/2308.08327v1</link><description>Raw videos have been proven to own considerable feature redundancy where inmany cases only a portion of frames can already meet the requirements foraccurate recognition. In this paper, we are interested in whether suchredundancy can be effectively leveraged to facilitate efficient inference incontinuous sign language recognition (CSLR). We propose a novel adaptive model(AdaBrowse) to dynamically select a most informative subsequence from inputvideo sequences by modelling this problem as a sequential decision task. Inspecific, we first utilize a lightweight network to quickly scan input videosto extract coarse features. Then these features are fed into a policy networkto intelligently select a subsequence to process. The corresponding subsequenceis finally inferred by a normal CSLR model for sentence prediction. As only aportion of frames are processed in this procedure, the total computations canbe considerably saved. Besides temporal redundancy, we are also interested inwhether the inherent spatial redundancy can be seamlessly integrated togetherto achieve further efficiency, i.e., dynamically selecting a lowest inputresolution for each sample, whose model is referred to as AdaBrowse+. Extensiveexperimental results on four large-scale CSLR datasets, i.e., PHOENIX14,PHOENIX14-T, CSL-Daily and CSL, demonstrate the effectiveness of AdaBrowse andAdaBrowse+ by achieving comparable accuracy with state-of-the-art methods with1.44$\times$ throughput and 2.12$\times$ fewer FLOPs. Comparisons with othercommonly-used 2D CNNs and adaptive efficient methods verify the effectivenessof AdaBrowse. Code is available at\url{https://github.com/hulianyuyy/AdaBrowse}.</description><author>Lianyu Hu, Liqing Gao, Zekang Liu, Chi-Man Pun, Wei Feng</author><pubDate>Wed, 16 Aug 2023 13:40:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08327v1</guid></item><item><title>Visually-Aware Context Modeling for News Image Captioning</title><link>http://arxiv.org/abs/2308.08325v1</link><description>The goal of News Image Captioning is to generate an image caption accordingto the content of both a news article and an image. To leverage the visualinformation effectively, it is important to exploit the connection between thecontext in the articles/captions and the images. Psychological studies indicatethat human faces in images draw higher attention priorities. On top of that,humans often play a central role in news stories, as also proven by theface-name co-occurrence pattern we discover in existing News Image Captioningdatasets. Therefore, we design a face-naming module for faces in images andnames in captions/articles to learn a better name embedding. Apart from names,which can be directly linked to an image area (faces), news image captionsmostly contain context information that can only be found in the article.Humans typically address this by searching for relevant information from thearticle based on the image. To emulate this thought process, we design aretrieval strategy using CLIP to retrieve sentences that are semantically closeto the image. We conduct extensive experiments to demonstrate the efficacy ofour framework. Without using additional paired data, we establish the newstate-of-the-art performance on two News Image Captioning datasets, exceedingthe previous state-of-the-art by 5 CIDEr points. We will release code uponacceptance.</description><author>Tingyu Qu, Tinne Tuytelaars, Marie-Francine Moens</author><pubDate>Wed, 16 Aug 2023 13:39:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08325v1</guid></item><item><title>Bi-level Contrastive Learning for Knowledge-Enhanced Molecule Representations</title><link>http://arxiv.org/abs/2306.01631v2</link><description>Molecule representation learning underpins diverse downstream applicationssuch as molecular property and side effect understanding and prediction. Inthis paper, we recognize the two-level structure of individual molecule ashaving intrinsic graph structure as well as being a node in a large moleculeknowledge graph, and present GODE, a new approach that seamlessly integratesgraph representations of individual molecules with multi-domain biomedical datafrom knowledge graphs. By pre-training two graph neural networks (GNNs) ondifferent graph structures, combined with contrastive learning, GODE adeptlyfuses molecular structures with their corresponding knowledge graphsubstructures. This fusion results in a more robust and informativerepresentation, enhancing molecular property prediction by harnessing bothchemical and biological information. Finetuned on 11 chemical property tasks,our model surpasses benchmarks, achieving an average ROC-AUC improvement of14.5%, 9.8%, and 7.3% on BBBP, SIDER, and Tox21 datasets. In regression taskson ESOL and QM7 datasets, we achieve average improvements of 21.0% and 29.6%improvements in RMSE and MAE, setting a new field benchmark.</description><author>Pengcheng Jiang, Cao Xiao, Tianfan Fu, Jimeng Sun</author><pubDate>Wed, 16 Aug 2023 13:30:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01631v2</guid></item></channel></rss>