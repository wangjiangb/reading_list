<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 15 Dec 2023 14:01:37 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>CLIP in Medical Imaging: A Comprehensive Survey</title><link>http://arxiv.org/abs/2312.07353v2</link><description>Contrastive Language-Image Pre-training (CLIP), a simple yet effectivepre-training paradigm, successfully introduces text supervision to visionmodels. It has shown promising results across various tasks, attributable toits generalizability and interpretability. The use of CLIP has recently gainedincreasing interest in the medical imaging domain, serving both as apre-training paradigm for aligning medical vision and language, and as acritical component in diverse clinical tasks. With the aim of facilitating adeeper understanding of this promising direction, this survey offers anin-depth exploration of the CLIP paradigm within the domain of medical imaging,regarding both refined CLIP pre-training and CLIP-driven applications. In thisstudy, We (1) start with a brief introduction to the fundamentals of CLIPmethodology. (2) Then, we investigate the adaptation of CLIP pre-training inthe medical domain, focusing on how to optimize CLIP given characteristics ofmedical images and reports. (3) Furthermore, we explore the practicalutilization of CLIP pre-trained models in various tasks, includingclassification, dense prediction, and cross-modal tasks. (4) Finally, wediscuss existing limitations of CLIP in the context of medical imaging andpropose forward-looking directions to address the demands of medical imagingdomain. We expect that this comprehensive survey will provide researchers inthe field of medical image analysis with a holistic understanding of the CLIPparadigm and its potential implications. The project page can be found onhttps://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging.</description><author>Zihao Zhao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Zhiming Cui, Qian Wang, Dinggang Shen</author><pubDate>Thu, 14 Dec 2023 18:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07353v2</guid></item><item><title>Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques</title><link>http://arxiv.org/abs/2307.12971v3</link><description>This article intends to systematically identify and comparatively analyzestate-of-the-art supply chain (SC) forecasting strategies and technologies. Anovel framework has been proposed incorporating Big Data Analytics in SCManagement (problem identification, data sources, exploratory data analysis,machine-learning model training, hyperparameter tuning, performance evaluation,and optimization), forecasting effects on human-workforce, inventory, andoverall SC. Initially, the need to collect data according to SC strategy andhow to collect them has been discussed. The article discusses the need fordifferent types of forecasting according to the period or SC objective. The SCKPIs and the error-measurement systems have been recommended to optimize thetop-performing model. The adverse effects of phantom inventory on forecastingand the dependence of managerial decisions on the SC KPIs for determining modelperformance parameters and improving operations management, transparency, andplanning efficiency have been illustrated. The cyclic connection within theframework introduces preprocessing optimization based on the post-process KPIs,optimizing the overall control process (inventory management, workforcedetermination, cost, production and capacity planning). The contribution ofthis research lies in the standard SC process framework proposal, recommendedforecasting data analysis, forecasting effects on SC performance, machinelearning algorithms optimization followed, and in shedding light on futureresearch.</description><author>Md Abrar Jahin, Md Sakib Hossain Shovon, Jungpil Shin, Istiyaque Ahmed Ridoy, Yoichi Tomioka, M. F. Mridha</author><pubDate>Thu, 14 Dec 2023 17:30:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12971v3</guid></item><item><title>Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context</title><link>http://arxiv.org/abs/2312.06528v2</link><description>Many neural network architectures have been shown to be Turing Complete, andcan thus implement arbitrary algorithms. However, Transformers are unique inthat they can implement gradient-based learning algorithms \emph{under simpleparameter configurations}. A line of recent work shows that linear Transformersnaturally learn to implement gradient descent (GD) when trained on a linearregression in-context learning task. But the linearity assumption (either inthe Transformer architecture or in the learning task) is far from realisticsettings where non-linear activations crucially enable Transformers to learncomplicated non-linear functions. In this paper, we provide theoretical andempirical evidence that non-linear Transformers can, and \emph{in fact do},learn to implement learning algorithms to learn non-linear functions incontext. Our results apply to a broad class of combinations of non-lineararchitectures, and non-linear in-context learning tasks. Interestingly, we showthat the optimal choice of non-linear activation depends in a natural way onthe non-linearity of the learning task.</description><author>Xiang Cheng, Yuxin Chen, Suvrit Sra</author><pubDate>Thu, 14 Dec 2023 17:19:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06528v2</guid></item><item><title>Exploring the Naturalness of AI-Generated Images</title><link>http://arxiv.org/abs/2312.05476v2</link><description>The proliferation of Artificial Intelligence-Generated Images (AGIs) hasgreatly expanded the Image Naturalness Assessment (INA) problem. Different fromearly definitions that mainly focus on tone-mapped images with limiteddistortions (e.g., exposure, contrast, and color reproduction), INA onAI-generated images is especially challenging as it has more diverse contentsand could be affected by factors from multiple perspectives, includinglow-level technical distortions and high-level rationality distortions. In thispaper, we take the first step to benchmark and assess the visual naturalness ofAI-generated images. First, we construct the AI-Generated Image Naturalness(AGIN) database by conducting a large-scale subjective study to collect humanopinions on the overall naturalness as well as perceptions from technical andrationality perspectives. AGIN verifies that naturalness is universally anddisparately affected by both technical and rationality distortions. Second, wepropose the Joint Objective Image Naturalness evaluaTor (JOINT), toautomatically learn the naturalness of AGIs that aligns human ratings.Specifically, JOINT imitates human reasoning in naturalness evaluation byjointly learning both technical and rationality perspectives. Experimentalresults show our proposed JOINT significantly surpasses baselines for providingmore subjectively consistent results on naturalness assessment. Our databaseand code will be released in https://github.com/zijianchen98/AGIN.</description><author>Zijian Chen, Wei Sun, Haoning Wu, Zicheng Zhang, Jun Jia, Xiongkuo Min, Guangtao Zhai, Wenjun Zhang</author><pubDate>Thu, 14 Dec 2023 15:46:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05476v2</guid></item><item><title>Learning Differentiable Particle Filter on the Fly</title><link>http://arxiv.org/abs/2312.05955v2</link><description>Differentiable particle filters are an emerging class of sequential Bayesianinference techniques that use neural networks to construct components in statespace models. Existing approaches are mostly based on offline supervisedtraining strategies. This leads to the delay of the model deployment and theobtained filters are susceptible to distribution shift of test-time data. Inthis paper, we propose an online learning framework for differentiable particlefilters so that model parameters can be updated as data arrive. The technicalconstraint is that there is no known ground truth state information in theonline inference setting. We address this by adopting an unsupervised loss toconstruct the online model updating procedure, which involves a sequence offiltering operations for online maximum likelihood-based parameter estimation.We empirically evaluate the effectiveness of the proposed method, and compareit with supervised learning methods in simulation settings including amultivariate linear Gaussian state-space model and a simulated object trackingexperiment.</description><author>Jiaxi Li, Xiongjie Chen, Yunpeng Li</author><pubDate>Thu, 14 Dec 2023 15:06:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05955v2</guid></item><item><title>HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot Prompt Learning</title><link>http://arxiv.org/abs/2312.01878v3</link><description>Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs)are prominent techniques for homogeneous and heterogeneous graph representationlearning, yet their performance in an end-to-end supervised framework greatlydepends on the availability of task-specific supervision. To reduce thelabeling cost, pre-training on self-supervised pretext tasks has become apopular paradigm,but there is often a gap between the pre-trained model anddownstream tasks, stemming from the divergence in their objectives. To bridgethe gap, prompt learning has risen as a promising direction especially infew-shot settings, without the need to fully fine-tune the pre-trained model.While there has been some early exploration of prompt-based learning on graphs,they primarily deal with homogeneous graphs, ignoring the heterogeneous graphsthat are prevalent in downstream applications. In this paper, we proposeHGPROMPT, a novel pre-training and prompting framework to unify not onlypre-training and downstream tasks but also homogeneous and heterogeneous graphsvia a dual-template design. Moreover, we propose dual-prompt in HGPROMPT toassist a downstream task in locating the most relevant prior to bridge the gapscaused by not only feature variations but also heterogeneity differences acrosstasks. Finally, we thoroughly evaluate and analyze HGPROMPT through extensiveexperiments on three public datasets.</description><author>Xingtong Yu, Yuan Fang, Zemin Liu, Xinming Zhang</author><pubDate>Thu, 14 Dec 2023 14:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01878v3</guid></item><item><title>Understanding and Leveraging the Learning Phases of Neural Networks</title><link>http://arxiv.org/abs/2312.06887v2</link><description>The learning dynamics of deep neural networks are not well understood. Theinformation bottleneck (IB) theory proclaimed separate fitting and compressionphases. But they have since been heavily debated. We comprehensively analyzethe learning dynamics by investigating a layer's reconstruction ability of theinput and prediction performance based on the evolution of parameters duringtraining. We empirically show the existence of three phases using commondatasets and architectures such as ResNet and VGG: (i) near constantreconstruction loss, (ii) decrease, and (iii) increase. We also derive anempirically grounded data model and prove the existence of phases forsingle-layer networks. Technically, our approach leverages classical complexityanalysis. It differs from IB by relying on measuring reconstruction loss ratherthan information theoretic measures to relate information of intermediatelayers and inputs. Our work implies a new best practice for transfer learning:We show empirically that the pre-training of a classifier should stop wellbefore its performance is optimal.</description><author>Johannes Schneider, Mohit Prabhushankar</author><pubDate>Thu, 14 Dec 2023 14:32:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06887v2</guid></item><item><title>TULIP: Transformer for Upsampling of LiDAR Point Cloud</title><link>http://arxiv.org/abs/2312.06733v2</link><description>LiDAR Upsampling is a challenging task for the perception systems of robotsand autonomous vehicles, due to the sparse and irregular structure oflarge-scale scene contexts. Recent works propose to solve this problem byconverting LiDAR data from 3D Euclidean space into an image super-resolutionproblem in 2D image space. Although their methods can generate high-resolutionrange images with fine-grained details, the resulting 3D point clouds oftenblur out details and predict invalid points. In this paper, we propose TULIP, anew method to reconstruct high-resolution LiDAR point clouds fromlow-resolution LiDAR input. We also follow a range image-based approach butspecifically modify the patch and window geometries of a Swin-Transformer-basednetwork to better fit the characteristics of range images. We conducted severalexperiments on three different public real-world and simulated datasets. TULIPoutperforms state-of-the-art methods in all relevant metrics and generatesrobust and more realistic point clouds than prior works.</description><author>Bin Yang, Patrick Pfreundschuh, Roland Siegwart, Marco Hutter, Peyman Moghadam, Vaishakh Patil</author><pubDate>Thu, 14 Dec 2023 13:41:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06733v2</guid></item><item><title>Controller-Guided Partial Label Consistency Regularization with Unlabeled Data</title><link>http://arxiv.org/abs/2210.11194v3</link><description>Partial label learning (PLL) learns from training examples each associatedwith multiple candidate labels, among which only one is valid. In recent years,benefiting from the strong capability of dealing with ambiguous supervision andthe impetus of modern data augmentation methods, consistencyregularization-based PLL methods have achieved a series of successes and becomemainstream. However, as the partial annotation becomes insufficient, theirperformances drop significantly. In this paper, we leverage easily accessibleunlabeled examples to facilitate the partial label consistency regularization.In addition to a partial supervised loss, our method performs acontroller-guided consistency regularization at both the label-level andrepresentation-level with the help of unlabeled data. To minimize thedisadvantages of insufficient capabilities of the initial supervised model, weuse the controller to estimate the confidence of each current prediction toguide the subsequent consistency regularization. Furthermore, we dynamicallyadjust the confidence thresholds so that the number of samples of each classparticipating in consistency regularization remains roughly equal to alleviatethe problem of class-imbalance. Experiments show that our method achievessatisfactory performances in more practical situations, and its modules can beapplied to existing PLL methods to enhance their capabilities.</description><author>Qian-Wei Wang, Bowen Zhao, Mingyan Zhu, Tianxiang Li, Zimo Liu, Shu-Tao Xia</author><pubDate>Thu, 14 Dec 2023 13:23:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.11194v3</guid></item><item><title>Beyond Surface: Probing LLaMA Across Scales and Layers</title><link>http://arxiv.org/abs/2312.04333v3</link><description>This paper presents an in-depth analysis of Large Language Models (LLMs),focusing on LLaMA, a prominent open-source foundational model in naturallanguage processing. Instead of assessing LLaMA through its generative output,we design multiple-choice tasks to probe its intrinsic understanding inhigh-order tasks such as reasoning and computation. We examine the modelhorizontally, comparing different sizes, and vertically, assessing differentlayers. We unveil several key and uncommon findings based on the designedprobing tasks: (1) Horizontally, enlarging model sizes almost could notautomatically impart additional knowledge or computational prowess. Instead, itcan enhance reasoning abilities, especially in math problem solving, and helpsreduce hallucinations, but only beyond certain size thresholds; (2) In verticalanalysis, the lower layers of LLaMA lack substantial arithmetic and factualknowledge, showcasing logical thinking, multilingual and recognitive abilities,with top layers housing most computational power and real-world knowledge.</description><author>Nuo Chen, Ning Wu, Shining Liang, Ming Gong, Linjun Shou, Dongmei Zhang, Jia Li</author><pubDate>Thu, 14 Dec 2023 13:11:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04333v3</guid></item><item><title>History Matters: Temporal Knowledge Editing in Large Language Model</title><link>http://arxiv.org/abs/2312.05497v3</link><description>The imperative task of revising or updating the knowledge stored within largelanguage models arises from two distinct sources: intrinsic errors inherent inthe model which should be corrected and outdated knowledge due to externalshifts in the real world which should be updated. Prevailing efforts in modelediting conflate these two distinct categories of edits arising from distinctreasons and directly modify the original knowledge in models into newknowledge. However, we argue that preserving the model's original knowledgeremains pertinent. Specifically, if a model's knowledge becomes outdated due toevolving worldly dynamics, it should retain recollection of the historicalknowledge while integrating the newfound knowledge. In this work, we introducethe task of Temporal Knowledge Editing (TKE) and establish a benchmark AToKe(Assessment of TempOral Knowledge Editing) to evaluate current model editingmethods. We find that while existing model editing methods are effective atmaking models remember new knowledge, the edited model catastrophically forgetshistorical knowledge. To address this gap, we propose a simple and generalframework termed Multi-Editing with Time Objective (METO) for enhancingexisting editing models, which edits both historical and new knowledgeconcurrently and optimizes the model's prediction for the time of each fact.Our assessments demonstrate that while AToKe is still difficult, METO maintainsthe effectiveness of learning new knowledge and meanwhile substantiallyimproves the performance of edited models on utilizing historical knowledge.</description><author>Xunjian Yin, Jin Jiang, Liming Yang, Xiaojun Wan</author><pubDate>Thu, 14 Dec 2023 12:06:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05497v3</guid></item><item><title>Semi-Supervised Class-Agnostic Motion Prediction with Pseudo Label Regeneration and BEVMix</title><link>http://arxiv.org/abs/2312.08009v2</link><description>Class-agnostic motion prediction methods aim to comprehend motion withinopen-world scenarios, holding significance for autonomous driving systems.However, training a high-performance model in a fully-supervised manner alwaysrequires substantial amounts of manually annotated data, which can be bothexpensive and time-consuming to obtain. To address this challenge, our studyexplores the potential of semi-supervised learning (SSL) for class-agnosticmotion prediction. Our SSL framework adopts a consistency-based self-trainingparadigm, enabling the model to learn from unlabeled data by generating pseudolabels through test-time inference. To improve the quality of pseudo labels, wepropose a novel motion selection and re-generation module. This moduleeffectively selects reliable pseudo labels and re-generates unreliable ones.Furthermore, we propose two data augmentation strategies: temporal sampling andBEVMix. These strategies facilitate consistency regularization in SSL.Experiments conducted on nuScenes demonstrate that our SSL method can surpassthe self-supervised approach by a large margin by utilizing only a tinyfraction of labeled data. Furthermore, our method exhibits comparableperformance to weakly and some fully supervised methods. These resultshighlight the ability of our method to strike a favorable balance betweenannotation costs and performance. Code will be available athttps://github.com/kwwcv/SSMP.</description><author>Kewei Wang, Yizheng Wu, Zhiyu Pan, Xingyi Li, Ke Xian, Zhe Wang, Zhiguo Cao, Guosheng Lin</author><pubDate>Thu, 14 Dec 2023 11:16:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08009v2</guid></item><item><title>AI Competitions and Benchmarks: towards impactful challenges with post-challenge papers, benchmarks and other dissemination actions</title><link>http://arxiv.org/abs/2312.06036v3</link><description>Organising an AI challenge does not end with the final event. Thelong-lasting impact also needs to be organised. This chapter covers the variousactivities after the challenge is formally finished. The target audience ofdifferent post-challenge activities is identified. The various outputs of thechallenge are listed with the means to collect them. The main part of thechapter is a template for a typical post-challenge paper, including possiblegraphs as well as advice on how to turn the challenge into a long-lastingbenchmark.</description><author>Antoine Marot, David Rousseau, Zhen Xu</author><pubDate>Thu, 14 Dec 2023 10:41:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06036v3</guid></item><item><title>Curriculum-Enhanced Residual Soft An-Isotropic Normalization for Over-smoothness in Deep GNNs</title><link>http://arxiv.org/abs/2312.08221v2</link><description>Despite Graph neural networks' significant performance gain over many classictechniques in various graph-related downstream tasks, their successes arerestricted in shallow models due to over-smoothness and the difficulties ofoptimizations among many other issues. In this paper, to alleviate theover-smoothing issue, we propose a soft graph normalization method to preservethe diversities of node embeddings and prevent indiscrimination due to possibleover-closeness. Combined with residual connections, we analyze the reason whythe method can effectively capture the knowledge in both input graph structuresand node features even with deep networks. Additionally, inspired by CurriculumLearning that learns easy examples before the hard ones, we propose a novellabel-smoothing-based learning framework to enhance the optimization of deepGNNs, which iteratively smooths labels in an auxiliary graph and constructsmany gradual non-smooth tasks for extracting increasingly complex knowledge andgradually discriminating nodes from coarse to fine. The method arguably reducesthe risk of overfitting and generalizes better results. Finally, extensiveexperiments are carried out to demonstrate the effectiveness and potential ofthe proposed model and learning framework through comparison with twelveexisting baselines including the state-of-the-art methods on twelve real-worldnode classification benchmarks.</description><author>Jin Li, Qirong Zhang, Shuling Xu, Xinlong Chen, Longkun Guo, Yang-Geng Fu</author><pubDate>Thu, 14 Dec 2023 09:38:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08221v2</guid></item><item><title>Compositional Inversion for Stable Diffusion Models</title><link>http://arxiv.org/abs/2312.08048v2</link><description>Inversion methods, such as Textual Inversion, generate personalized images byincorporating concepts of interest provided by user images. However, existingmethods often suffer from overfitting issues, where the dominant presence ofinverted concepts leads to the absence of other desired concepts. It stems fromthe fact that during inversion, the irrelevant semantics in the user images arealso encoded, forcing the inverted concepts to occupy locations far from thecore distribution in the embedding space. To address this issue, we propose amethod that guides the inversion process towards the core distribution forcompositional embeddings. Additionally, we introduce a spatial regularizationapproach to balance the attention on the concepts being composed. Our method isdesigned as a post-training approach and can be seamlessly integrated withother inversion methods. Experimental results demonstrate the effectiveness ofour proposed approach in mitigating the overfitting problem and generating morediverse and balanced compositions of concepts in the synthesized images. Thesource code is available athttps://github.com/zhangxulu1996/Compositional-Inversion.</description><author>Xu-Lu Zhang, Xiao-Yong Wei, Jin-Lin Wu, Tian-Yi Zhang, Zhaoxiang Zhang, Zhen Lei, Qing Li</author><pubDate>Thu, 14 Dec 2023 08:40:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08048v2</guid></item><item><title>High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models</title><link>http://arxiv.org/abs/2312.08274v2</link><description>Objective: To develop a high-throughput biomedical relation extraction systemthat takes advantage of the large language models' (LLMs) reading comprehensionability and biomedical world knowledge in a scalable and evidential manner.Methods: We formulate the relation extraction task as a simple binaryclassification problem for large language models such as ChatGPT. Specifically,LLMs make the decision based on the external corpus and its world knowledge,giving the reason for the judgment to factual verification. This method istailored for semi-structured web articles, wherein we designate the main titleas the tail entity and explicitly incorporate it into the context, and thepotential head entities are matched based on a biomedical thesaurus. Moreover,lengthy contents are sliced into text chunks, embedded, and retrieved withadditional embedding models, ensuring compatibility with the context windowsize constraints of available open-source LLMs. Results: Using an open-sourceLLM, we extracted 304315 relation triplets of three distinct relation typesfrom four reputable biomedical websites. To assess the efficacy of the basicpipeline employed for biomedical relation extraction, we curated a benchmarkdataset annotated by a medical expert. Evaluation results indicate that thepipeline exhibits performance comparable to that of GPT-4. Case studies furtherilluminate challenges faced by contemporary LLMs in the context of biomedicalrelation extraction for semi-structured web articles. Conclusion: The proposedmethod has demonstrated its effectiveness in leveraging the strengths of LLMsfor high-throughput biomedical relation extraction. Its adaptability isevident, as it can be seamlessly extended to diverse semi-structured biomedicalwebsites, facilitating the extraction of various types of biomedical relationswith ease.</description><author>Songchi Zhou, Sheng Yu</author><pubDate>Thu, 14 Dec 2023 07:28:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08274v2</guid></item><item><title>Challenges of YOLO Series for Object Detection in Extremely Heavy Rain: CALRA Simulator based Synthetic Evaluation Dataset</title><link>http://arxiv.org/abs/2312.07976v2</link><description>Recently, as many studies of autonomous vehicles have been achieved forlevels 4 and 5, there has been also increasing interest in the advancement ofperception, decision, and control technologies, which are the three majoraspects of autonomous vehicles. As for the perception technologies achievingreliable maneuvering of autonomous vehicles, object detection by using diversesensors (e.g., LiDAR, radar, and camera) should be prioritized. These sensorsrequire to detect objects accurately and quickly in diverse weather conditions,but they tend to have challenges to consistently detect objects in bad weatherconditions with rain, snow, or fog. Thus, in this study, based on theexperimentally obtained raindrop data from precipitation conditions, weconstructed a novel dataset that could test diverse network model in variousprecipitation conditions through the CARLA simulator. Consequently, based onour novel dataset, YOLO series, a one-stage-detector, was used toquantitatively verify how much object detection performance could be decreasedunder various precipitation conditions from normal to extreme heavy rainsituations.</description><author>T. Kim, H. Jeon, Y. Lim</author><pubDate>Thu, 14 Dec 2023 07:09:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07976v2</guid></item><item><title>SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention</title><link>http://arxiv.org/abs/2312.07987v2</link><description>The costly self-attention layers in modern Transformers require memory andcompute quadratic in sequence length. Existing approximation methods usuallyunderperform and fail to obtain significant speedups in practice. Here wepresent SwitchHead - a novel method that reduces both compute and memoryrequirements and achieves wall-clock speedup, while matching the languagemodeling performance of baseline Transformers with the same parameter budget.SwitchHead uses Mixture-of-Experts (MoE) layers for the value and outputprojections and requires 4 to 8 times fewer attention matrices than standardTransformers. Our novel attention can also be combined with MoE MLP layers,resulting in an efficient fully-MoE "SwitchAll" Transformer model. Our code ispublic.</description><author>Róbert Csordás, Piotr Piękos, Kazuki Irie, Jürgen Schmidhuber</author><pubDate>Thu, 14 Dec 2023 06:35:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07987v2</guid></item><item><title>Context-Aware Iteration Policy Network for Efficient Optical Flow Estimation</title><link>http://arxiv.org/abs/2312.07180v3</link><description>Existing recurrent optical flow estimation networks are computationallyexpensive since they use a fixed large number of iterations to update the flowfield for each sample. An efficient network should skip iterations when theflow improvement is limited. In this paper, we develop a Context-AwareIteration Policy Network for efficient optical flow estimation, whichdetermines the optimal number of iterations per sample. The policy networkachieves this by learning contextual information to realize whether flowimprovement is bottlenecked or minimal. On the one hand, we use iterationembedding and historical hidden cell, which include previous iterationsinformation, to convey how flow has changed from previous iterations. On theother hand, we use the incremental loss to make the policy network implicitlyperceive the magnitude of optical flow improvement in the subsequent iteration.Furthermore, the computational complexity in our dynamic network iscontrollable, allowing us to satisfy various resource preferences with a singletrained model. Our policy network can be easily integrated intostate-of-the-art optical flow networks. Extensive experiments show that ourmethod maintains performance while reducing FLOPs by about 40%/20% for theSintel/KITTI datasets.</description><author>Ri Cheng, Ruian He, Xuhao Jiang, Shili Zhou, Weimin Tan, Bo Yan</author><pubDate>Thu, 14 Dec 2023 06:19:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07180v3</guid></item><item><title>Enhancing Robotic Navigation: An Evaluation of Single and Multi-Objective Reinforcement Learning Strategies</title><link>http://arxiv.org/abs/2312.07953v2</link><description>This study presents a comparative analysis between single-objective andmulti-objective reinforcement learning methods for training a robot to navigateeffectively to an end goal while efficiently avoiding obstacles. Traditionalreinforcement learning techniques, namely Deep Q-Network (DQN), DeepDeterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3), have beenevaluated using the Gazebo simulation framework in a variety of environmentswith parameters such as random goal and robot starting locations. These methodsprovide a numerical reward to the robot, offering an indication of actionquality in relation to the goal. However, their limitations become apparent incomplex settings where multiple, potentially conflicting, objectives arepresent. To address these limitations, we propose an approach employingMulti-Objective Reinforcement Learning (MORL). By modifying the reward functionto return a vector of rewards, each pertaining to a distinct objective, therobot learns a policy that effectively balances the different goals, aiming toachieve a Pareto optimal solution. This comparative study highlights thepotential for MORL in complex, dynamic robotic navigation tasks, setting thestage for future investigations into more adaptable and robust roboticbehaviors.</description><author>Vicki Young, Jumman Hossain, Nirmalya Roy</author><pubDate>Thu, 14 Dec 2023 06:01:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07953v2</guid></item><item><title>MammoFL: Mammographic Breast Density Estimation using Federated Learning</title><link>http://arxiv.org/abs/2206.05575v5</link><description>In this study, we automate quantitative mammographic breast densityestimation with neural networks and show that this tool is a strong use casefor federated learning on multi-institutional datasets. Our dataset includedbilateral CC-view and MLO-view mammographic images from two separateinstitutions. Two U-Nets were separately trained on algorithm-generated labelsto perform segmentation of the breast and dense tissue from these images andsubsequently calculate breast percent density (PD). The networks were trainedwith federated learning and compared to three non-federated baselines, onetrained on each single-institution dataset and one trained on the aggregatedmulti-institution dataset. We demonstrate that training on multi-institutiondatasets is critical to algorithm generalizability. We further show thatfederated learning on multi-institutional datasets improves modelgeneralization to unseen data at nearly the same level as centralized trainingon multi-institutional datasets, indicating that federated learning can beapplied to our method to improve algorithm generalizability while maintainingpatient privacy.</description><author>Ramya Muthukrishnan, Angelina Heyler, Keshava Katti, Sarthak Pati, Walter Mankowski, Aprupa Alahari, Michael Sanborn, Emily F. Conant, Christopher Scott, Stacey Winham, Celine Vachon, Pratik Chaudhari, Despina Kontos, Spyridon Bakas</author><pubDate>Thu, 14 Dec 2023 04:29:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.05575v5</guid></item><item><title>Mathematical Language Models: A Survey</title><link>http://arxiv.org/abs/2312.07622v2</link><description>In recent years, there has been remarkable progress in leveraging LanguageModels (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scaleLanguage Models (LLMs), within the domain of mathematics. This paper conducts acomprehensive survey of mathematical LMs, systematically categorizing pivotalresearch endeavors from two distinct perspectives: tasks and methodologies. Thelandscape reveals a large number of proposed mathematical LLMs, which arefurther delineated into instruction learning, tool-based methods, fundamentalCoT techniques, and advanced CoT methodologies. In addition, our survey entailsthe compilation of over 60 mathematical datasets, including training datasets,benchmark datasets, and augmented datasets. Addressing the primary challengesand delineating future trajectories within the field of mathematical LMs, thissurvey is positioned as a valuable resource, poised to facilitate and inspirefuture innovation among researchers invested in advancing this domain.</description><author>Wentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong Li, Jiayi Zeng, Mengliang He, Qin Chen, Bo Jiang, Aimin Zhou, Liang He</author><pubDate>Thu, 14 Dec 2023 04:01:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07622v2</guid></item><item><title>VILA: On Pre-training for Visual Language Models</title><link>http://arxiv.org/abs/2312.07533v2</link><description>Visual language models (VLMs) rapidly progressed with the recent success oflarge language models. There have been growing efforts on visual instructiontuning to extend the LLM with visual inputs, but lacks an in-depth study of thevisual language pre-training process, where the model learns to perform jointmodeling on both modalities. In this work, we examine the design options forVLM pre-training by augmenting LLM towards VLM through step-by-stepcontrollable comparisons. We introduce three main findings: (1) freezing LLMsduring pre-training can achieve decent zero-shot performance, but lackin-context learning capability, which requires unfreezing the LLM; (2)interleaved pre-training data is beneficial whereas image-text pairs alone arenot optimal; (3) re-blending text-only instruction data to image-text dataduring instruction fine-tuning not only remedies the degradation of text-onlytasks, but also boosts VLM task accuracy. With an enhanced pre-training recipewe build VILA, a Visual Language model family that consistently outperforms thestate-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bellsand whistles. Multi-modal pre-training also helps unveil appealing propertiesof VILA, including multi-image reasoning, enhanced in-context learning, andbetter world knowledge.</description><author>Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, Song Han</author><pubDate>Thu, 14 Dec 2023 03:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07533v2</guid></item><item><title>ThinkBot: Embodied Instruction Following with Thought Chain Reasoning</title><link>http://arxiv.org/abs/2312.07062v2</link><description>Embodied Instruction Following (EIF) requires agents to complete humaninstruction by interacting objects in complicated surrounding environments.Conventional methods directly consider the sparse human instruction to generateaction plans for agents, which usually fail to achieve human goals because ofthe instruction incoherence in action descriptions. On the contrary, we proposeThinkBot that reasons the thought chain in human instruction to recover themissing action descriptions, so that the agent can successfully complete humangoals by following the coherent instruction. Specifically, we first design aninstruction completer based on large language models to recover the missingactions with interacted objects between consecutive human instruction, wherethe perceived surrounding environments and the completed sub-goals areconsidered for instruction completion. Based on the partially observed scenesemantic maps, we present an object localizer to infer the position ofinteracted objects for agents to achieve complex human goals. Extensiveexperiments in the simulated environment show that our ThinkBot outperforms thestate-of-the-art EIF methods by a sizable margin in both success rate andexecution efficiency.</description><author>Guanxing Lu, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang</author><pubDate>Thu, 14 Dec 2023 03:28:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07062v2</guid></item><item><title>Focus on Local Regions for Query-based Object Detection</title><link>http://arxiv.org/abs/2310.06470v3</link><description>Query-based methods have garnered significant attention in object detectionsince the advent of DETR, the pioneering query-based detector. However, thesemethods face challenges like slow convergence and suboptimal performance.Notably, self-attention in object detection often hampers convergence due toits global focus. To address these issues, we propose FoLR, a transformer-likearchitecture with only decoders. We improve the self-attention by isolatingconnections between irrelevant objects that makes it focus on local regions butnot global regions. We also design the adaptive sampling method to extracteffective features based on queries' local regions from feature maps.Additionally, we employ a look-back strategy for decoders to retain previousinformation, followed by the Feature Mixer module to fuse features and queries.Experimental results demonstrate FoLR's state-of-the-art performance inquery-based detectors, excelling in convergence speed and computationalefficiency. Index Terms: Local regions, Attention mechanism, Object detection</description><author>Hongbin Xu, Yamei Xia, Shuai Zhao, Bo Cheng</author><pubDate>Thu, 14 Dec 2023 03:22:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06470v3</guid></item><item><title>MWSIS: Multimodal Weakly Supervised Instance Segmentation with 2D Box Annotations for Autonomous Driving</title><link>http://arxiv.org/abs/2312.06988v3</link><description>Instance segmentation is a fundamental research in computer vision,especially in autonomous driving. However, manual mask annotation for instancesegmentation is quite time-consuming and costly. To address this problem, someprior works attempt to apply weakly supervised manner by exploring 2D or 3Dboxes. However, no one has ever successfully segmented 2D and 3D instancessimultaneously by only using 2D box annotations, which could further reduce theannotation cost by an order of magnitude. Thus, we propose a novel frameworkcalled Multimodal Weakly Supervised Instance Segmentation (MWSIS), whichincorporates various fine-grained label generation and correction modules forboth 2D and 3D modalities to improve the quality of pseudo labels, along with anew multimodal cross-supervision approach, named Consistency Sparse Cross-modalSupervision (CSCS), to reduce the inconsistency of multimodal predictions byresponse distillation. Particularly, transferring the 3D backbone to downstreamtasks not only improves the performance of the 3D detectors, but alsooutperforms fully supervised instance segmentation with only 5% fullysupervised annotations. On the Waymo dataset, the proposed frameworkdemonstrates significant improvements over the baseline, especially achieving2.59% mAP and 12.75% mAP increases for 2D and 3D instance segmentation tasks,respectively. The code is available athttps://github.com/jiangxb98/mwsis-plugin.</description><author>Guangfeng Jiang, Jun Liu, Yuzhi Wu, Wenlong Liao, Tao He, Pai Peng</author><pubDate>Thu, 14 Dec 2023 03:00:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06988v3</guid></item><item><title>Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation</title><link>http://arxiv.org/abs/2312.08078v2</link><description>To address these issues, we propose a novel Adaptive patch-word Matching(AdaMatch) model to correlate chest X-ray (CXR) image regions with words inmedical reports and apply it to CXR-report generation to provide explainabilityfor the generation process. AdaMatch exploits the fine-grained relation betweenadaptive patches and words to provide explanations of specific image regionswith corresponding words. To capture the abnormal regions of varying sizes andpositions, we introduce the Adaptive Patch extraction (AdaPatch) module toacquire the adaptive patches for these regions adaptively. In order to provideexplicit explainability for CXR-report generation task, we propose anAdaMatch-based bidirectional large language model for Cyclic CXR-reportgeneration (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywordsfor CXR images and `keypatches' for medical reports as hints to guideCXR-report generation. Extensive experiments on two publicly available CXRdatasets prove the effectiveness of our method and its superior performance toexisting methods.</description><author>Wenting Chen, Xiang Li, Linlin Shen, Yixuan Yuan</author><pubDate>Thu, 14 Dec 2023 02:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08078v2</guid></item><item><title>MLNet: Mutual Learning Network with Neighborhood Invariance for Universal Domain Adaptation</title><link>http://arxiv.org/abs/2312.07871v2</link><description>Universal domain adaptation (UniDA) is a practical but challenging problem,in which information about the relation between the source and the targetdomains is not given for knowledge transfer. Existing UniDA methods may sufferfrom the problems of overlooking intra-domain variations in the target domainand difficulty in separating between the similar known and unknown class. Toaddress these issues, we propose a novel Mutual Learning Network (MLNet) withneighborhood invariance for UniDA. In our method, confidence-guided invariantfeature learning with self-adaptive neighbor selection is designed to reducethe intra-domain variations for more generalizable feature representation. Byusing the cross-domain mixup scheme for better unknown-class identification,the proposed method compensates for the misidentified known-class errors bymutual learning between the closed-set and open-set classifiers. Extensiveexperiments on three publicly available benchmarks demonstrate that our methodachieves the best results compared to the state-of-the-arts in most cases andsignificantly outperforms the baseline across all the four settings in UniDA.Code is available at https://github.com/YanzuoLu/MLNet.</description><author>Yanzuo Lu, Meng Shen, Andy J Ma, Xiaohua Xie, Jian-Huang Lai</author><pubDate>Thu, 14 Dec 2023 02:30:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07871v2</guid></item><item><title>Can a Transformer Represent a Kalman Filter?</title><link>http://arxiv.org/abs/2312.06937v2</link><description>Transformers are a class of autoregressive deep learning architectures whichhave recently achieved state-of-the-art performance in various vision,language, and robotics tasks. We revisit the problem of Kalman Filtering inlinear dynamical systems and show that Transformers can approximate the KalmanFilter in a strong sense. Specifically, for any observable LTI system weconstruct an explicit causally-masked Transformer which implements the KalmanFilter, up to a small additive error which is bounded uniformly in time; wecall our construction the Transformer Filter. Our construction is based on atwo-step reduction. We first show that a softmax self-attention block canexactly represent a certain Gaussian kernel smoothing estimator. We then showthat this estimator closely approximates the Kalman Filter. We also investigatehow the Transformer Filter can be used for measurement-feedback control andprove that the resulting nonlinear controllers closely approximate theperformance of standard optimal control policies such as the LQG controller.</description><author>Gautam Goel, Peter Bartlett</author><pubDate>Thu, 14 Dec 2023 02:19:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06937v2</guid></item><item><title>Noise in the reverse process improves the approximation capabilities of diffusion models</title><link>http://arxiv.org/abs/2312.07851v2</link><description>In Score based Generative Modeling (SGMs), the state-of-the-art in generativemodeling, stochastic reverse processes are known to perform better than theirdeterministic counterparts. This paper delves into the heart of thisphenomenon, comparing neural ordinary differential equations (ODEs) and neuralstochastic differential equations (SDEs) as reverse processes. We use a controltheoretic perspective by posing the approximation of the reverse process as atrajectory tracking problem. We analyze the ability of neural SDEs toapproximate trajectories of the Fokker-Planck equation, revealing theadvantages of stochasticity. First, neural SDEs exhibit a powerful regularizingeffect, enabling $L^2$ norm trajectory approximation surpassing the Wassersteinmetric approximation achieved by neural ODEs under similar conditions, evenwhen the reference vector field or score function is not Lipschitz. Applyingthis result, we establish the class of distributions that can be sampled usingscore matching in SGMs, relaxing the Lipschitz requirement on the gradient ofthe data distribution in existing literature. Second, we show that thisapproximation property is preserved when network width is limited to the inputdimension of the network. In this limited width case, the weights act ascontrol inputs, framing our analysis as a controllability problem for neuralSDEs in probability density space. This sheds light on how noise helps to steerthe system towards the desired solution and illuminates the empirical successof stochasticity in generative modeling.</description><author>Karthik Elamvazhuthi, Samet Oymak, Fabio Pasqualetti</author><pubDate>Thu, 14 Dec 2023 02:17:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07851v2</guid></item><item><title>High-Order Structure Based Middle-Feature Learning for Visible-Infrared Person Re-Identification</title><link>http://arxiv.org/abs/2312.07853v2</link><description>Visible-infrared person re-identification (VI-ReID) aims to retrieve imagesof the same persons captured by visible (VIS) and infrared (IR) cameras.Existing VI-ReID methods ignore high-order structure information of featureswhile being relatively difficult to learn a reasonable common feature space dueto the large modality discrepancy between VIS and IR images. To address theabove problems, we propose a novel high-order structure based middle-featurelearning network (HOS-Net) for effective VI-ReID. Specifically, we firstleverage a short- and long-range feature extraction (SLE) module to effectivelyexploit both short-range and long-range features. Then, we propose a high-orderstructure learning (HSL) module to successfully model the high-orderrelationship across different local features of each person image based on awhitened hypergraph network.This greatly alleviates model collapse and enhancesfeature representations. Finally, we develop a common feature space learning(CFL) module to learn a discriminative and reasonable common feature spacebased on middle features generated by aligning features from differentmodalities and ranges. In particular, a modality-range identity-centercontrastive (MRIC) loss is proposed to reduce the distances between the VIS,IR, and middle features, smoothing the training process. Extensive experimentson the SYSU-MM01, RegDB, and LLCM datasets show that our HOS-Net achievessuperior state-of-the-art performance. Our code is available at\url{https://github.com/Jaulaucoeng/HOS-Net}.</description><author>Liuxiang Qiu, Si Chen, Yan Yan, Jing-Hao Xue, Da-Han Wang, Shunzhi Zhu</author><pubDate>Thu, 14 Dec 2023 02:05:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07853v2</guid></item><item><title>V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models</title><link>http://arxiv.org/abs/2308.09300v4</link><description>Building artificial intelligence (AI) systems on top of a set of foundationmodels (FMs) is becoming a new paradigm in AI research. Their representativeand generative abilities learnt from vast amounts of data can be easily adaptedand transferred to a wide range of downstream tasks without extra training fromscratch. However, leveraging FMs in cross-modal generation remainsunder-researched when audio modality is involved. On the other hand,automatically generating semantically-relevant sound from visual input is animportant problem in cross-modal generation studies. To solve thisvision-to-audio (V2A) generation problem, existing methods tend to design andbuild complex systems from scratch using modestly sized datasets. In thispaper, we propose a lightweight solution to this problem by leveragingfoundation models, specifically CLIP, CLAP, and AudioLDM. We first investigatethe domain gap between the latent space of the visual CLIP and the auditoryCLAP models. Then we propose a simple yet effective mapper mechanism(V2A-Mapper) to bridge the domain gap by translating the visual input betweenCLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrainedaudio generative FM AudioLDM is adopted to produce high-fidelity andvisually-aligned sound. Compared to previous approaches, our method onlyrequires a quick training of the V2A-Mapper. We further analyze and conductextensive experiments on the choice of the V2A-Mapper and show that agenerative mapper is better at fidelity and variability (FD) while a regressionmapper is slightly better at relevance (CS). Both objective and subjectiveevaluation on two V2A datasets demonstrate the superiority of our proposedmethod compared to current state-of-the-art approaches - trained with 86% fewerparameters but achieving 53% and 19% improvement in FD and CS, respectively.</description><author>Heng Wang, Jianbo Ma, Santiago Pascual, Richard Cartwright, Weidong Cai</author><pubDate>Thu, 14 Dec 2023 00:15:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09300v4</guid></item><item><title>SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models</title><link>http://arxiv.org/abs/2312.07492v2</link><description>Current datasets for unwanted social bias auditing are limited to studyingprotected demographic features such as race and gender. In this work, weintroduce a comprehensive benchmark that is meant to capture the amplificationof social bias, via stigmas, in generative language models. We start with acomprehensive list of 93 stigmas documented in social science literature andcurate a question-answering (QA) dataset which involves simple socialsituations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with avariety of prompt styles, carefully constructed to systematically test for bothsocial bias and model robustness. We present results for SocialStigmaQA withtwo widely used open source generative language models and we demonstrate thatthe output generated by these models considerably amplifies existing socialbias against stigmatized groups. Specifically, we find that the proportion ofsocially biased output ranges from 45% to 59% across a variety of decodingstrategies and prompting styles. We discover that the deliberate design of thetemplates in our benchmark (e.g., by adding biasing text to the prompt orvarying the answer that indicates bias) impact the model tendencies to generatesocially biased output. Additionally, we report on patterns in the generatedchain-of-thought output, finding a variety of problems from subtle bias toevidence of a lack of reasoning. Warning: This paper contains examples of text which is toxic, biased, andharmful.</description><author>Manish Nagireddy, Lamogha Chiazor, Moninder Singh, Ioana Baldini</author><pubDate>Wed, 13 Dec 2023 20:34:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07492v2</guid></item><item><title>Relax Image-Specific Prompt Requirement in SAM: A Single Generic Prompt for Segmenting Camouflaged Objects</title><link>http://arxiv.org/abs/2312.07374v2</link><description>Camouflaged object detection (COD) approaches heavily rely on pixel-levelannotated datasets. Weakly-supervised COD (WSCOD) approaches use sparseannotations like scribbles or points to reduce annotation effort, but this canlead to decreased accuracy. The Segment Anything Model (SAM) shows remarkablesegmentation ability with sparse prompts like points. However, manual prompt isnot always feasible, as it may not be accessible in real-world application.Additionally, it only provides localization information instead of semanticone, which can intrinsically cause ambiguity in interpreting the targets. Inthis work, we aim to eliminate the need for manual prompt. The key idea is toemploy Cross-modal Chains of Thought Prompting (CCTP) to reason visual promptsusing the semantic information given by a generic text prompt. To that end, weintroduce a test-time adaptation per-instance mechanism called GeneralizableSAM (GenSAM) to automatically enerate and optimize visual prompts the generictask prompt for WSCOD. In particular, CCTP maps a single generic text promptonto image-specific consensus foreground and background heatmaps usingvision-language models, acquiring reliable visual prompts. Moreover, totest-time adapt the visual prompts, we further propose Progressive MaskGeneration (PMG) to iteratively reweight the input image, guiding the model tofocus on the targets in a coarse-to-fine manner. Crucially, all networkparameters are fixed, avoiding the need for additional training. Experimentsdemonstrate the superiority of GenSAM. Experiments on three benchmarksdemonstrate that GenSAM outperforms point supervision approaches and achievescomparable results to scribble supervision ones, solely relying on general taskdescriptions as prompts. our codes is in: https://lwpyh.github.io/GenSAM/.</description><author>Jian Hu, Jiayi Lin, Weitong Cai, Shaogang Gong</author><pubDate>Wed, 13 Dec 2023 20:12:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07374v2</guid></item><item><title>SAM-guided Graph Cut for 3D Instance Segmentation</title><link>http://arxiv.org/abs/2312.08372v1</link><description>This paper addresses the challenge of 3D instance segmentation bysimultaneously leveraging 3D geometric and multi-view image information. Manyprevious works have applied deep learning techniques to 3D point clouds forinstance segmentation. However, these methods often failed to generalize tovarious types of scenes due to the scarcity and low-diversity of labeled 3Dpoint cloud data. Some recent works have attempted to lift 2D instancesegmentations to 3D within a bottom-up framework. The inconsistency in 2Dinstance segmentations among views can substantially degrade the performance of3D segmentation. In this work, we introduce a novel 3D-to-2D query framework toeffectively exploit 2D segmentation models for 3D instance segmentation.Specifically, we pre-segment the scene into several superpoints in 3D,formulating the task into a graph cut problem. The superpoint graph isconstructed based on 2D segmentation models, where node features are obtainedfrom multi-view image features and edge weights are computed based onmulti-view segmentation results, enabling the better generalization ability. Toprocess the graph, we train a graph neural network using pseudo 3D labels from2D segmentation models. Experimental results on the ScanNet, ScanNet++ andKITTI-360 datasets demonstrate that our method achieves robust segmentationperformance and can generalize across different types of scenes. Our projectpage is available at https://zju3dv.github.io/sam_graph.</description><author>Haoyu Guo, He Zhu, Sida Peng, Yuang Wang, Yujun Shen, Ruizhen Hu, Xiaowei Zhou</author><pubDate>Wed, 13 Dec 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08372v1</guid></item><item><title>PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object Detection</title><link>http://arxiv.org/abs/2312.08371v1</link><description>Recent temporal LiDAR-based 3D object detectors achieve promising performancebased on the two-stage proposal-based approach. They generate 3D box candidatesfrom the first-stage dense detector, followed by different temporal aggregationmethods. However, these approaches require per-frame objects or whole pointclouds, posing challenges related to memory bank utilization. Moreover, pointclouds and trajectory features are combined solely based on concatenation,which may neglect effective interactions between them. In this paper, wepropose a point-trajectory transformer with long short-term memory forefficient temporal 3D object detection. To this end, we only utilize pointclouds of current-frame objects and their historical trajectories as input tominimize the memory bank storage requirement. Furthermore, we introduce modulesto encode trajectory features, focusing on long short-term and future-awareperspectives, and then effectively aggregate them with point cloud features. Weconduct extensive experiments on the large-scale Waymo dataset to demonstratethat our approach performs well against state-of-the-art methods. Code andmodels will be made publicly available at https://github.com/kuanchihhuang/PTT.</description><author>Kuan-Chih Huang, Weijie Lyu, Ming-Hsuan Yang, Yi-Hsuan Tsai</author><pubDate>Wed, 13 Dec 2023 18:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08371v1</guid></item><item><title>The Effective Horizon Explains Deep RL Performance in Stochastic Environments</title><link>http://arxiv.org/abs/2312.08369v1</link><description>Reinforcement learning (RL) theory has largely focused on proving minimaxsample complexity bounds. These require strategic exploration algorithms thatuse relatively limited function classes for representing the policy or valuefunction. Our goal is to explain why deep RL algorithms often perform well inpractice, despite using random exploration and much more expressive functionclasses like neural networks. Our work arrives at an explanation by showingthat many stochastic MDPs can be solved by performing only a few steps of valueiteration on the random policy's Q function and then acting greedily. When thisis true, we find that it is possible to separate the exploration and learningcomponents of RL, making it much easier to analyze. We introduce a new RLalgorithm, SQIRL, that iteratively learns a near-optimal policy by exploringrandomly to collect rollouts and then performing a limited number of steps offitted-Q iteration over those rollouts. Any regression algorithm that satisfiesbasic in-distribution generalization properties can be used in SQIRL toefficiently solve common MDPs. This can explain why deep RL works neuralnetworks, since it is empirically established that neural networks generalizewell in-distribution. Furthermore, SQIRL explains why random exploration workswell in practice, since we show many environments can be solved by estimatingthe random policy's Q-function and then applying zero or a few steps of valueiteration. We leverage SQIRL to derive instance-dependent sample complexitybounds for RL that are exponential only in an "effective horizon" of lookaheadand on the complexity of the class used for function approximation.Empirically, we also find that SQIRL performance strongly correlates with PPOand DQN performance in a variety of stochastic environments, supporting thatour theoretical analysis is predictive of practical performance.</description><author>Cassidy Laidlaw, Banghua Zhu, Stuart Russell, Anca Dragan</author><pubDate>Wed, 13 Dec 2023 18:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08369v1</guid></item><item><title>VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering</title><link>http://arxiv.org/abs/2312.08367v1</link><description>In this work, we propose an efficient Video-Language Alignment viaFrame-Prompting and Distilling (VLAP) network. Our VLAP model addresses bothefficient frame sampling and effective cross-modal alignment in a unified way.In our VLAP network, we design a new learnable question-aware Frame-Promptertogether with a new cross-modal distillation (QFormer-Distiller) module.Pre-trained large image-language models have shown promising results onproblems such as visual question answering. However, how to efficiently andeffectively sample image frames when adapting pre-trained large image-languagemodel to video-language alignment is still the major challenge. Compared withprior work, our VLAP model demonstrates the capability of selecting key frameswith critical contents, thus improving the video-language alignment accuracywhile reducing the inference latency (+3.3% on NExT-QA Temporal with 3.0X speedup). Overall, our VLAP network outperforms (e.g. +4.6% on STAR Interaction and+2.2% on STAR average with 3.0X speed up, ours 2-frames out-perform SeViLA4-frames on VLEP with 4.2X speed up) the state-of-the-art methods on the videoquestion-answering benchmarks.</description><author>Xijun Wang, Junbang Liang, Chun-Kai Wang, Kenan Deng, Yu Lou, Ming Lin, Shan Yang</author><pubDate>Wed, 13 Dec 2023 18:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08367v1</guid></item><item><title>See, Say, and Segment: Teaching LMMs to Overcome False Premises</title><link>http://arxiv.org/abs/2312.08366v1</link><description>Current open-source Large Multimodal Models (LMMs) excel at tasks such asopen-vocabulary language grounding and segmentation but can suffer under falsepremises when queries imply the existence of something that is not actuallypresent in the image. We observe that existing methods that fine-tune an LMM tosegment images significantly degrade their ability to reliably determine("see") if an object is present and to interact naturally with humans ("say"),a form of catastrophic forgetting. In this work, we propose a cascading andjoint training approach for LMMs to solve this task, avoiding catastrophicforgetting of previous skills. Our resulting model can "see" by detectingwhether objects are present in an image, "say" by telling the user if they arenot, proposing alternative queries or correcting semantic errors in the query,and finally "segment" by outputting the mask of the desired objects if theyexist. Additionally, we introduce a novel False Premise Correction benchmarkdataset, an extension of existing RefCOCO(+/g) referring segmentation datasets(which we call FP-RefCOCO(+/g)). The results show that our method not onlydetects false premises up to 55% better than existing approaches, but underfalse premise conditions produces relative cIOU improvements of more than 31%over baselines, and produces natural language feedback judged helpful up to 67%of the time.</description><author>Tsung-Han Wu, Giscard Biamby, David Chan, Lisa Dunlap, Ritwik Gupta, Xudong Wang, Joseph E. Gonzalez, Trevor Darrell</author><pubDate>Wed, 13 Dec 2023 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08366v1</guid></item><item><title>An Invitation to Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2312.08365v1</link><description>Training a deep neural network to maximize a target objective has become thestandard recipe for successful machine learning over the last decade. Thesenetworks can be optimized with supervised learning, if the target objective isdifferentiable. For many interesting problems, this is however not the case.Common objectives like intersection over union (IoU), bilingual evaluationunderstudy (BLEU) score or rewards cannot be optimized with supervisedlearning. A common workaround is to define differentiable surrogate losses,leading to suboptimal solutions with respect to the actual objective.Reinforcement learning (RL) has emerged as a promising alternative foroptimizing deep neural networks to maximize non-differentiable objectives inrecent years. Examples include aligning large language models via humanfeedback, code generation, object detection or control problems. This makes RLtechniques relevant to the larger machine learning audience. The subject is,however, time intensive to approach due to the large range of methods, as wellas the often very theoretical presentation. In this introduction, we take analternative approach, different from classic reinforcement learning textbooks.Rather than focusing on tabular problems, we introduce reinforcement learningas a generalization of supervised learning, which we first apply tonon-differentiable objectives and later to temporal problems. Assuming onlybasic knowledge of supervised learning, the reader will be able to understandstate-of-the-art deep RL algorithms like proximal policy optimization (PPO)after reading this tutorial.</description><author>Bernhard Jaeger, Andreas Geiger</author><pubDate>Wed, 13 Dec 2023 18:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08365v1</guid></item><item><title>View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data</title><link>http://arxiv.org/abs/2312.08364v1</link><description>Procedural synthetic data generation has received increasing attention incomputer vision. Procedural signed distance functions (SDFs) are a powerfultool for modeling large-scale detailed scenes, but existing mesh extractionmethods have artifacts or performance profiles that limit their use forsynthetic data. We propose OcMesher, a mesh extraction algorithm thatefficiently handles high-detail unbounded scenes with perfect view-consistency,with easy export to downstream real-time engines. The main novelty of oursolution is an algorithm to construct an octree based on a given SDF andmultiple camera views. We performed extensive experiments, and show oursolution produces better synthetic data for training and evaluation of computervision models.</description><author>Zeyu Ma, Alexander Raistrick, Lahav Lipson, Jia Deng</author><pubDate>Wed, 13 Dec 2023 18:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08364v1</guid></item><item><title>Distributed Inference and Fine-tuning of Large Language Models Over The Internet</title><link>http://arxiv.org/abs/2312.08361v1</link><description>Large language models (LLMs) are useful in many NLP tasks and become morecapable with size, with the best open-source models having over 50 billionparameters. However, using these 50B+ models requires high-end hardware, makingthem inaccessible to most researchers. In this work, we investigate methods forcost-efficient inference and fine-tuning of LLMs, comparing local anddistributed strategies. We observe that a large enough model (50B+) can runefficiently even on geodistributed devices in a consumer-grade network. Thiscould allow running LLM efficiently by pooling together idle compute resourcesof multiple research groups and volunteers. We address two open problems: (1)how to perform inference and fine-tuning reliably if any device can disconnectabruptly and (2) how to partition LLMs between devices with uneven hardware,joining and leaving at will. In order to do that, we develop specialfault-tolerant inference algorithms and load-balancing protocols thatautomatically assign devices to maximize the total system throughput. Weshowcase these algorithms in Petals - a decentralized system that runs Llama 2(70B) and BLOOM (176B) over the Internet up to 10x faster than offloading forinteractive generation. We evaluate the performance of our system in simulatedconditions and a real-world setup spanning two continents.</description><author>Alexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, Colin Raffel</author><pubDate>Wed, 13 Dec 2023 18:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08361v1</guid></item><item><title>Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF</title><link>http://arxiv.org/abs/2312.08358v1</link><description>In practice, preference learning from human feedback depends on incompletedata with hidden context. Hidden context refers to data that affects thefeedback received, but which is not represented in the data used to train apreference model. This captures common issues of data collection, such ashaving human annotators with varied preferences, cognitive processes thatresult in seemingly irrational behavior, and combining data labeled accordingto different criteria. We prove that standard applications of preferencelearning, including reinforcement learning from human feedback (RLHF),implicitly aggregate over hidden contexts according to a well-known voting rulecalled Borda count. We show this can produce counter-intuitive results that arevery different from other methods which implicitly aggregate via expectedutility. Furthermore, our analysis formalizes the way that preference learningfrom users with diverse values tacitly implements a social choice function. Akey implication of this result is that annotators have an incentive tomisreport their preferences in order to influence the learned model, leading tovulnerabilities in the deployment of RLHF. As a step towards mitigating theseproblems, we introduce a class of methods called distributional preferencelearning (DPL). DPL methods estimate a distribution of possible score valuesfor each alternative in order to better account for hidden context.Experimental results indicate that applying DPL to RLHF for LLM chatbotsidentifies hidden context in the data and significantly reduces subsequentjailbreak vulnerability. Our code and data are available athttps://github.com/cassidylaidlaw/hidden-context</description><author>Anand Siththaranjan, Cassidy Laidlaw, Dylan Hadfield-Menell</author><pubDate>Wed, 13 Dec 2023 18:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08358v1</guid></item><item><title>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</title><link>http://arxiv.org/abs/2305.18290v2</link><description>While large-scale unsupervised language models (LMs) learn broad worldknowledge and some reasoning skills, achieving precise control of theirbehavior is difficult due to the completely unsupervised nature of theirtraining. Existing methods for gaining such steerability collect human labelsof the relative quality of model generations and fine-tune the unsupervised LMto align with these preferences, often with reinforcement learning from humanfeedback (RLHF). However, RLHF is a complex and often unstable procedure, firstfitting a reward model that reflects the human preferences, and thenfine-tuning the large unsupervised LM using reinforcement learning to maximizethis estimated reward without drifting too far from the original model. In thispaper we introduce a new parameterization of the reward model in RLHF thatenables extraction of the corresponding optimal policy in closed form, allowingus to solve the standard RLHF problem with only a simple classification loss.The resulting algorithm, which we call Direct Preference Optimization (DPO), isstable, performant, and computationally lightweight, eliminating the need forsampling from the LM during fine-tuning or performing significanthyperparameter tuning. Our experiments show that DPO can fine-tune LMs to alignwith human preferences as well as or better than existing methods. Notably,fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment ofgenerations, and matches or improves response quality in summarization andsingle-turn dialogue while being substantially simpler to implement and train.</description><author>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn</author><pubDate>Wed, 13 Dec 2023 18:48:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18290v2</guid></item><item><title>Saturn: An Optimized Data System for Large Model Deep Learning Workloads</title><link>http://arxiv.org/abs/2309.01226v2</link><description>Large language models such as GPT-3 &amp; ChatGPT have transformed deep learning(DL), powering applications that have captured the public's imagination. Thesemodels are rapidly being adopted across domains for analytics on variousmodalities, often by finetuning pre-trained base models. Such models needmultiple GPUs due to both their size and computational load, driving thedevelopment of a bevy of "model parallelism" techniques &amp; tools. Navigatingsuch parallelism choices, however, is a new burden for end users of DL such asdata scientists, domain scientists, etc. who may lack the necessary systemsknowhow. The need for model selection, which leads to many models to train dueto hyper-parameter tuning or layer-wise finetuning, compounds the situationwith two more burdens: resource apportioning and scheduling. In this work, wetackle these three burdens for DL users in a unified manner by formalizing themas a joint problem that we call SPASE: Select a Parallelism, Allocateresources, and SchedulE. We propose a new information system architecture totackle the SPASE problem holistically, representing a key step toward enablingwider adoption of large DL models. We devise an extensible template forexisting parallelism schemes and combine it with an automated empiricalprofiler for runtime estimation. We then formulate SPASE as an MILP. We find that direct use of an MILP-solver is significantly more effectivethan several baseline heuristics. We optimize the system runtime further withan introspective scheduling approach. We implement all these techniques into anew data system we call Saturn. Experiments with benchmark DL workloads showthat Saturn achieves 39-49% lower model selection runtimes than typical currentDL practice.</description><author>Kabir Nagrecha, Arun Kumar</author><pubDate>Wed, 13 Dec 2023 18:42:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01226v2</guid></item><item><title>Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes</title><link>http://arxiv.org/abs/2309.01922v2</link><description>In this paper, we consider an infinite horizon average reward Markov DecisionProcess (MDP). Distinguishing itself from existing works within this context,our approach harnesses the power of the general policy gradient-basedalgorithm, liberating it from the constraints of assuming a linear MDPstructure. We propose a policy gradient-based algorithm and show its globalconvergence property. We then prove that the proposed algorithm has$\tilde{\mathcal{O}}({T}^{3/4})$ regret. Remarkably, this paper marks apioneering effort by presenting the first exploration into regret-boundcomputation for the general parameterized policy gradient algorithm in thecontext of average reward scenarios.</description><author>Qinbo Bai, Washim Uddin Mondal, Vaneet Aggarwal</author><pubDate>Wed, 13 Dec 2023 18:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01922v2</guid></item><item><title>FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects</title><link>http://arxiv.org/abs/2312.08344v1</link><description>We present FoundationPose, a unified foundation model for 6D object poseestimation and tracking, supporting both model-based and model-free setups. Ourapproach can be instantly applied at test-time to a novel object withoutfine-tuning, as long as its CAD model is given, or a small number of referenceimages are captured. We bridge the gap between these two setups with a neuralimplicit representation that allows for effective novel view synthesis, keepingthe downstream pose estimation modules invariant under the same unifiedframework. Strong generalizability is achieved via large-scale synthetictraining, aided by a large language model (LLM), a novel transformer-basedarchitecture, and contrastive learning formulation. Extensive evaluation onmultiple public datasets involving challenging scenarios and objects indicateour unified approach outperforms existing methods specialized for each task bya large margin. In addition, it even achieves comparable results toinstance-level methods despite the reduced assumptions. Project page:https://nvlabs.github.io/FoundationPose/</description><author>Bowen Wen, Wei Yang, Jan Kautz, Stan Birchfield</author><pubDate>Wed, 13 Dec 2023 18:28:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08344v1</guid></item><item><title>Ehancing CT Image synthesis from multi-modal MRI data based on a multi-task neural network framework</title><link>http://arxiv.org/abs/2312.08343v1</link><description>Image segmentation, real-value prediction, and cross-modal translation arecritical challenges in medical imaging. In this study, we propose a versatilemulti-task neural network framework, based on an enhanced Transformer U-Netarchitecture, capable of simultaneously, selectively, and adaptively addressingthese medical image tasks. Validation is performed on a public repository ofhuman brain MR and CT images. We decompose the traditional problem ofsynthesizing CT images into distinct subtasks, which include skullsegmentation, Hounsfield unit (HU) value prediction, and image sequentialreconstruction. To enhance the framework's versatility in handling multi-modaldata, we expand the model with multiple image channels. Comparisons betweensynthesized CT images derived from T1-weighted and T2-Flair images wereconducted, evaluating the model's capability to integrate multi-modalinformation from both morphological and pixel value perspectives.</description><author>Zhuoyao Xin, Christopher Wu, Dong Liu, Chunming Gu, Jia Guo, Jun Hua</author><pubDate>Wed, 13 Dec 2023 18:22:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08343v1</guid></item><item><title>Structured Voronoi Sampling</title><link>http://arxiv.org/abs/2306.03061v2</link><description>Gradient-based sampling algorithms have demonstrated their effectiveness intext generation, especially in the context of controlled text generation.However, there exists a lack of theoretically grounded and principledapproaches for this task. In this paper, we take an important step towardbuilding a principled approach for sampling from language models withgradient-based methods. We use discrete distributions given by language modelsto define densities and develop an algorithm based on Hamiltonian Monte Carloto sample from them. We name our gradient-based technique Structured VoronoiSampling (SVS). In an experimental setup where the reference distribution isknown, we show that the empirical distribution of SVS samples is closer to thereference distribution compared to alternative sampling schemes. Furthermore,in a controlled generation task, SVS is able to generate fluent and diversesamples while following the control targets significantly better than othermethods.</description><author>Afra Amini, Li Du, Ryan Cotterell</author><pubDate>Wed, 13 Dec 2023 18:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03061v2</guid></item><item><title>Global Latent Neural Rendering</title><link>http://arxiv.org/abs/2312.08338v1</link><description>A recent trend among generalizable novel view synthesis methods is to learn arendering operator acting over single camera rays. This approach is promisingbecause it removes the need for explicit volumetric rendering, but iteffectively treats target images as collections of independent pixels. Here, wepropose to learn a global rendering operator acting over all camera raysjointly. We show that the right representation to enable such rendering is the5-dimensional plane sweep volume, consisting of the projection of the inputimages on a set of planes facing the target camera. Based on thisunderstanding, we introduce our Convolutional Global Latent Renderer (ConvGLR),an efficient convolutional architecture that performs the rendering operationglobally in a low-resolution latent space. Experiments on various datasetsunder sparse and generalizable setups show that our approach consistentlyoutperforms existing methods by significant margins.</description><author>Thomas Tanay, Matteo Maggioni</author><pubDate>Wed, 13 Dec 2023 18:14:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08338v1</guid></item><item><title>LD-SDM: Language-Driven Hierarchical Species Distribution Modeling</title><link>http://arxiv.org/abs/2312.08334v1</link><description>We focus on the problem of species distribution modeling using global-scalepresence-only data. Most previous studies have mapped the range of a givenspecies using geographical and environmental features alone. To capture astronger implicit relationship between species, we encode the taxonomichierarchy of species using a large language model. This enables range mappingfor any taxonomic rank and unseen species without additional supervision.Further, we propose a novel proximity-aware evaluation metric that enablesevaluating species distribution models using any pixel-level representation ofground-truth species range map. The proposed metric penalizes the predictionsof a model based on its proximity to the ground truth. We describe theeffectiveness of our model by systematically evaluating on the task of speciesrange prediction, zero-shot prediction and geo-feature regression against thestate-of-the-art. Results show our model outperforms the strong baselines whentrained with a variety of multi-label learning losses.</description><author>Srikumar Sastry, Xin Xing, Aayush Dhakal, Subash Khanal, Adeel Ahmad, Nathan Jacobs</author><pubDate>Wed, 13 Dec 2023 18:11:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08334v1</guid></item><item><title>Discretization-Induced Dirichlet Posterior for Robust Uncertainty Quantification on Regression</title><link>http://arxiv.org/abs/2308.09065v2</link><description>Uncertainty quantification is critical for deploying deep neural networks(DNNs) in real-world applications. An Auxiliary Uncertainty Estimator (AuxUE)is one of the most effective means to estimate the uncertainty of the main taskprediction without modifying the main task model. To be considered robust, anAuxUE must be capable of maintaining its performance and triggering higheruncertainties while encountering Out-of-Distribution (OOD) inputs, i.e., toprovide robust aleatoric and epistemic uncertainty. However, for visionregression tasks, current AuxUE designs are mainly adopted for aleatoricuncertainty estimates, and AuxUE robustness has not been explored. In thiswork, we propose a generalized AuxUE scheme for more robust uncertaintyquantification on regression tasks. Concretely, to achieve a more robustaleatoric uncertainty estimation, different distribution assumptions areconsidered for heteroscedastic noise, and Laplace distribution is finallychosen to approximate the prediction error. For epistemic uncertainty, wepropose a novel solution named Discretization-Induced Dirichlet pOsterior(DIDO), which models the Dirichlet posterior on the discretized predictionerror. Extensive experiments on age estimation, monocular depth estimation, andsuper-resolution tasks show that our proposed method can provide robustuncertainty estimates in the face of noisy inputs and that it can be scalableto both image-level and pixel-wise tasks. Code is available athttps://github.com/ENSTA-U2IS/DIDO .</description><author>Xuanlong Yu, Gianni Franchi, Jindong Gu, Emanuel Aldea</author><pubDate>Wed, 13 Dec 2023 18:01:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09065v2</guid></item><item><title>FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts</title><link>http://arxiv.org/abs/2311.05608v2</link><description>Ensuring the safety of artificial intelligence-generated content (AIGC) is alongstanding topic in the artificial intelligence (AI) community, and thesafety concerns associated with Large Language Models (LLMs) have been widelyinvestigated. Recently, large vision-language models (VLMs) represent anunprecedented revolution, as they are built upon LLMs but can incorporateadditional modalities (e.g., images). However, the safety of VLMs lackssystematic evaluation, and there may be an overconfidence in the safetyguarantees provided by their underlying LLMs. In this paper, to demonstratethat introducing additional modality modules leads to unforeseen AI safetyissues, we propose FigStep, a straightforward yet effective jailbreakingalgorithm against VLMs. Instead of feeding textual harmful instructionsdirectly, FigStep converts the harmful content into images through typographyto bypass the safety alignment within the textual module of the VLMs, inducingVLMs to output unsafe responses that violate common AI safety policies. In ourevaluation, we manually review 46,500 model responses generated by 3 familiesof the promising open-source VLMs, i.e., LLaVA, MiniGPT4, and CogVLM (a totalof 6 VLMs). The experimental results show that FigStep can achieve an averageattack success rate of 82.50% on 500 harmful queries in 10 topics. Moreover, wedemonstrate that the methodology of FigStep can even jailbreak GPT-4V, whichalready leverages an OCR detector to filter harmful queries. Above all, ourwork reveals that VLMs are vulnerable to jailbreaking attacks, which highlightsthe necessity of novel safety alignments between visual and textual modalities.</description><author>Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, Xiaoyun Wang</author><pubDate>Wed, 13 Dec 2023 17:54:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05608v2</guid></item><item><title>Segment Every Out-of-Distribution Object</title><link>http://arxiv.org/abs/2311.16516v3</link><description>Semantic segmentation models, while effective for in-distribution categories,face challenges in real-world deployment due to encounteringout-of-distribution (OoD) objects. Detecting these OoD objects is crucial forsafety-critical applications. Existing methods rely on anomaly scores, butchoosing a suitable threshold for generating masks presents difficulties andcan lead to fragmentation and inaccuracy. This paper introduces a method toconvert anomaly \textbf{S}core \textbf{T}o segmentation \textbf{M}ask, calledS2M, a simple and effective framework for OoD detection in semanticsegmentation. Unlike assigning anomaly scores to pixels, S2M directly segmentsthe entire OoD object. By transforming anomaly scores into prompts for apromptable segmentation model, S2M eliminates the need for threshold selection.Extensive experiments demonstrate that S2M outperforms the state-of-the-art byapproximately 10% in IoU and 30% in mean F1 score, on average, across variousbenchmarks including Fishyscapes, Segment-Me-If-You-Can, and RoadAnomalydatasets.</description><author>Wenjie Zhao, Jia Li, Xin Dong, Yu Xiang, Yunhui Guo</author><pubDate>Wed, 13 Dec 2023 17:51:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16516v3</guid></item><item><title>ADCNet: Learning from Raw Radar Data via Distillation</title><link>http://arxiv.org/abs/2303.11420v3</link><description>As autonomous vehicles and advanced driving assistance systems have enteredwider deployment, there is an increased interest in building robust perceptionsystems using radars. Radar-based systems are lower cost and more robust toadverse weather conditions than their LiDAR-based counterparts; however thepoint clouds produced are typically noisy and sparse by comparison. In order tocombat these challenges, recent research has focused on consuming the raw radardata, instead of the final radar point cloud. We build on this line of work anddemonstrate that by bringing elements of the signal processing pipeline intoour network and then pre-training on the signal processing task, we are able toachieve state of the art detection performance on the RADIal dataset. Ourmethod uses expensive offline signal processing algorithms to pseudo-label dataand trains a network to distill this information into a fast convolutionalbackbone, which can then be finetuned for perception tasks. Extensiveexperiment results corroborate the effectiveness of the proposed techniques.</description><author>Bo Yang, Ishan Khatri, Michael Happold, Chulong Chen</author><pubDate>Wed, 13 Dec 2023 17:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11420v3</guid></item><item><title>PnPNet: Pull-and-Push Networks for Volumetric Segmentation with Boundary Confusion</title><link>http://arxiv.org/abs/2312.08323v1</link><description>Precise boundary segmentation of volumetric images is a critical task forimage-guided diagnosis and computer-assisted intervention, especially forboundary confusion in clinical practice. However, U-shape networks cannoteffectively resolve this challenge due to the lack of boundary shapeconstraints. Besides, existing methods of refining boundaries overemphasize theslender structure, which results in the overfitting phenomenon due to networks'limited abilities to model tiny objects. In this paper, we reconceptualize themechanism of boundary generation by encompassing the interaction dynamics withadjacent regions. Moreover, we propose a unified network termed PnPNet to modelshape characteristics of the confused boundary region. Core ingredients ofPnPNet contain the pushing and pulling branches. Specifically, based ondiffusion theory, we devise the semantic difference module (SDM) from thepushing branch to squeeze the boundary region. Explicit and implicitdifferential information inside SDM significantly boost representationabilities for inter-class boundaries. Additionally, motivated by the K-meansalgorithm, the class clustering module (CCM) from the pulling branch isintroduced to stretch the intersected boundary region. Thus, pushing andpulling branches will shrink and enlarge the boundary uncertainty respectively.They furnish two adversarial forces to promote models to output a more precisedelineation of boundaries. We carry out experiments on three challenging publicdatasets and one in-house dataset, containing three types of boundary confusionin model predictions. Experimental results demonstrate the superiority ofPnPNet over other segmentation networks, especially on evaluation metrics of HDand ASSD. Besides, pushing and pulling branches can serve as plug-and-playmodules to enhance classic U-shape baseline models. Codes are available.</description><author>Xin You, Ming Ding, Minghui Zhang, Hanxiao Zhang, Yi Yu, Jie Yang, Yun Gu</author><pubDate>Wed, 13 Dec 2023 17:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08323v1</guid></item><item><title>Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving</title><link>http://arxiv.org/abs/2304.14365v3</link><description>Robotic perception requires the modeling of both 3D geometry and semantics.Existing methods typically focus on estimating 3D bounding boxes, neglectingfiner geometric details and struggling to handle general, out-of-vocabularyobjects. 3D occupancy prediction, which estimates the detailed occupancy statesand semantics of a scene, is an emerging task to overcome these limitations. Tosupport 3D occupancy prediction, we develop a label generation pipeline thatproduces dense, visibility-aware labels for any given scene. This pipelinecomprises three stages: voxel densification, occlusion reasoning, andimage-guided voxel refinement. We establish two benchmarks, derived from theWaymo Open Dataset and the nuScenes Dataset, namely Occ3D-Waymo andOcc3D-nuScenes benchmarks. Furthermore, we provide an extensive analysis of theproposed dataset with various baseline models. Lastly, we propose a new model,dubbed Coarse-to-Fine Occupancy (CTF-Occ) network, which demonstrates superiorperformance on the Occ3D benchmarks. The code, data, and benchmarks arereleased at https://tsinghua-mars-lab.github.io/Occ3D/.</description><author>Xiaoyu Tian, Tao Jiang, Longfei Yun, Yucheng Mao, Huitong Yang, Yue Wang, Yilun Wang, Hang Zhao</author><pubDate>Wed, 13 Dec 2023 17:41:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14365v3</guid></item><item><title>Prompt Engineering-assisted Malware Dynamic Analysis Using GPT-4</title><link>http://arxiv.org/abs/2312.08317v1</link><description>Dynamic analysis methods effectively identify shelled, wrapped, or obfuscatedmalware, thereby preventing them from invading computers. As a significantrepresentation of dynamic malware behavior, the API (Application ProgrammingInterface) sequence, comprised of consecutive API calls, has progressivelybecome the dominant feature of dynamic analysis methods. Though there have beennumerous deep learning models for malware detection based on API sequences, thequality of API call representations produced by those models is limited. Thesemodels cannot generate representations for unknown API calls, which weakensboth the detection performance and the generalization. Further, the conceptdrift phenomenon of API calls is prominent. To tackle these issues, weintroduce a prompt engineering-assisted malware dynamic analysis using GPT-4.In this method, GPT-4 is employed to create explanatory text for each API callwithin the API sequence. Afterward, the pre-trained language model BERT is usedto obtain the representation of the text, from which we derive therepresentation of the API sequence. Theoretically, this proposed method iscapable of generating representations for all API calls, excluding thenecessity for dataset training during the generation process. Utilizing therepresentation, a CNN-based detection model is designed to extract the feature.We adopt five benchmark datasets to validate the performance of the proposedmodel. The experimental results reveal that the proposed detection algorithmperforms better than the state-of-the-art method (TextCNN). Specifically, incross-database experiments and few-shot learning experiments, the proposedmodel achieves excellent detection performance and almost a 100% recall ratefor malware, verifying its superior generalization performance. The code isavailable at: github.com/yan-scnu/Prompted_Dynamic_Detection.</description><author>Pei Yan, Shunquan Tan, Miaohui Wang, Jiwu Huang</author><pubDate>Wed, 13 Dec 2023 17:39:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08317v1</guid></item><item><title>Motion2Language, unsupervised learning of synchronized semantic motion segmentation</title><link>http://arxiv.org/abs/2310.10594v2</link><description>In this paper, we investigate building a sequence to sequence architecturefor motion to language translation and synchronization. The aim is to translatemotion capture inputs into English natural-language descriptions, such that thedescriptions are generated synchronously with the actions performed, enablingsemantic segmentation as a byproduct, but without requiring synchronizedtraining data. We propose a new recurrent formulation of local attention thatis suited for synchronous/live text generation, as well as an improved motionencoder architecture better suited to smaller data and for synchronousgeneration. We evaluate both contributions in individual experiments, using thestandard BLEU4 metric, as well as a simple semantic equivalence measure, on theKIT motion language dataset. In a follow-up experiment, we assess the qualityof the synchronization of generated text in our proposed approaches throughmultiple evaluation metrics. We find that both contributions to the attentionmechanism and the encoder architecture additively improve the quality ofgenerated text (BLEU and semantic equivalence), but also of synchronization.Our code is available athttps://github.com/rd20karim/M2T-Segmentation/tree/main</description><author>Karim Radouane, Andon Tchechmedjiev, Julien Lagarde, Sylvie Ranwez</author><pubDate>Wed, 13 Dec 2023 17:29:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10594v2</guid></item><item><title>EquiReact: An equivariant neural network for chemical reactions</title><link>http://arxiv.org/abs/2312.08307v1</link><description>Equivariant neural networks have considerably improved the accuracy anddata-efficiency of predictions of molecular properties. Building on thissuccess, we introduce EquiReact, an equivariant neural network to inferproperties of chemical reactions, built from three-dimensional structures ofreactants and products. We illustrate its competitive performance on theprediction of activation barriers on the GDB7-22-TS, Cyclo-23-TS andProparg-21-TS datasets with different regimes according to the inclusion ofatom-mapping information. We show that, compared to state-of-the-art models forreaction property prediction, EquiReact offers: (i) a flexible model withreduced sensitivity between atom-mapping regimes, (ii) better extrapolationcapabilities to unseen chemistries, (iii) impressive prediction errors fordatasets exhibiting subtle variations in three-dimensional geometries ofreactants/products, (iv) reduced sensitivity to geometry quality and (iv)excellent data efficiency.</description><author>Puck van Gerwen, Ksenia R. Briling, Charlotte Bunne, Vignesh Ram Somnath, Ruben Laplaza, Andreas Krause, Clemence Corminboeuf</author><pubDate>Wed, 13 Dec 2023 17:26:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08307v1</guid></item><item><title>PG-Video-LLaVA: Pixel Grounding Large Video-Language Models</title><link>http://arxiv.org/abs/2311.13435v2</link><description>Extending image-based Large Multimodal Models (LMMs) to videos is challengingdue to the inherent complexity of video data. The recent approaches extendingimage-based LMMs to videos either lack the grounding capabilities (e.g.,VideoChat, Video-ChatGPT, Video-LLaMA) or do not utilize the audio-signals forbetter video understanding (e.g., Video-ChatGPT). Addressing these gaps, wepropose PG-Video-LLaVA, the first LMM with pixel-level grounding capability,integrating audio cues by transcribing them into text to enrich video-contextunderstanding. Our framework uses an off-the-shelf tracker and a novelgrounding module, enabling it to spatially localize objects in videos followinguser instructions. We evaluate PG-Video-LLaVA using video-based generative andquestion-answering benchmarks and introduce new benchmarks specificallydesigned to measure prompt-based object grounding performance in videos.Further, we propose the use of Vicuna over GPT-3.5, as utilized inVideo-ChatGPT, for video-based conversation benchmarking, ensuringreproducibility of results which is a concern with the proprietary nature ofGPT-3.5. Our framework builds on SoTA image-based LLaVA model and extends itsadvantages to the video domain, delivering promising gains on video-basedconversation and grounding tasks. Project Page:https://github.com/mbzuai-oryx/Video-LLaVA</description><author>Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Mubarak Shah, Fahad Khan</author><pubDate>Wed, 13 Dec 2023 17:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13435v2</guid></item><item><title>Conformal Prediction Regions for Time Series using Linear Complementarity Programming</title><link>http://arxiv.org/abs/2304.01075v4</link><description>Conformal prediction is a statistical tool for producing prediction regionsof machine learning models that are valid with high probability. However,applying conformal prediction to time series data leads to conservativeprediction regions. In fact, to obtain prediction regions over $T$ time stepswith confidence $1-\delta$, {previous works require that each individualprediction region is valid} with confidence $1-\delta/T$. We propose anoptimization-based method for reducing this conservatism to enable long horizonplanning and verification when using learning-enabled time series predictors.Instead of considering prediction errors individually at each time step, weconsider a parameterized prediction error over multiple time steps. Byoptimizing the parameters over an additional dataset, we find predictionregions that are not conservative. We show that this problem can be cast as amixed integer linear complementarity program (MILCP), which we then relax intoa linear complementarity program (LCP). Additionally, we prove that the relaxedLP has the same optimal cost as the original MILCP. Finally, we demonstrate theefficacy of our method on case studies using pedestrian trajectory predictorsand F16 fighter jet altitude predictors.</description><author>Matthew Cleaveland, Insup Lee, George J. Pappas, Lars Lindemann</author><pubDate>Wed, 13 Dec 2023 17:23:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01075v4</guid></item><item><title>Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models</title><link>http://arxiv.org/abs/2312.08303v1</link><description>Toxic content detection is crucial for online services to removeinappropriate content that violates community standards. To automate thedetection process, prior works have proposed varieties of machine learning (ML)approaches to train Language Models (LMs) for toxic content detection. However,both their accuracy and transferability across datasets are limited. Recently,Large Language Models (LLMs) have shown promise in toxic content detection dueto their superior zero-shot and few-shot in-context learning ability as well asbroad transferability on ML tasks. However, efficiently designing prompts forLLMs remains challenging. Moreover, the high run-time cost of LLMs may hindertheir deployments in production. To address these challenges, in this work, wepropose BD-LLM, a novel and efficient approach to Bootstrapping and DistillingLLMs for toxic content detection. Specifically, we design a novel promptingmethod named Decision-Tree-of-Thought (DToT) to bootstrap LLMs' detectionperformance and extract high-quality rationales. DToT can automatically selectmore fine-grained context to re-prompt LLMs when their responses lackconfidence. Additionally, we use the rationales extracted via DToT to fine-tunestudent LMs. Our experimental results on various datasets demonstrate that DToTcan improve the accuracy of LLMs by up to 4.6%. Furthermore, student LMsfine-tuned with rationales extracted via DToT outperform baselines on alldatasets with up to 16.9\% accuracy improvement, while being more than 60xsmaller than conventional LLMs. Finally, we observe that student LMs fine-tunedwith rationales exhibit better cross-dataset transferability.</description><author>Jiang Zhang, Qiong Wu, Yiming Xu, Cheng Cao, Zheng Du, Konstantinos Psounis</author><pubDate>Wed, 13 Dec 2023 17:22:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08303v1</guid></item><item><title>Conceptualizing Suicidal Behavior: Utilizing Explanations of Predicted Outcomes to Analyze Longitudinal Social Media Data</title><link>http://arxiv.org/abs/2312.08299v1</link><description>The COVID-19 pandemic has escalated mental health crises worldwide, withsocial isolation and economic instability contributing to a rise in suicidalbehavior. Suicide can result from social factors such as shame, abuse,abandonment, and mental health conditions like depression, Post-TraumaticStress Disorder (PTSD), Attention-Deficit/Hyperactivity Disorder (ADHD),anxiety disorders, and bipolar disorders. As these conditions develop, signs ofsuicidal ideation may manifest in social media interactions. Analyzing socialmedia data using artificial intelligence (AI) techniques can help identifypatterns of suicidal behavior, providing invaluable insights for suicideprevention agencies, professionals, and broader community awarenessinitiatives. Machine learning algorithms for this purpose require large volumesof accurately labeled data. Previous research has not fully explored thepotential of incorporating explanations in analyzing and labeling longitudinalsocial media data. In this study, we employed a model explanation method, LayerIntegrated Gradients, on top of a fine-tuned state-of-the-art language model,to assign each token from Reddit users' posts an attribution score forpredicting suicidal ideation. By extracting and analyzing attributions oftokens from the data, we propose a methodology for preliminary screening ofsocial media posts for suicidal ideation without using large language modelsduring inference.</description><author>Van Minh Nguyen, Nasheen Nur, William Stern, Thomas Mercer, Chiradeep Sen, Siddhartha Bhattacharyya, Victor Tumbiolo, Seng Jhing Goh</author><pubDate>Wed, 13 Dec 2023 17:15:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08299v1</guid></item><item><title>Venn: Resource Management Across Federated Learning Jobs</title><link>http://arxiv.org/abs/2312.08298v1</link><description>In recent years, federated learning (FL) has emerged as a promising approachfor machine learning (ML) and data science across distributed edge devices.With the increasing popularity of FL, resource contention between multiple FLjobs training on the same device population is increasing as well. Schedulingedge resources among multiple FL jobs is different from GPU scheduling forcloud ML because of the ephemeral nature and planetary scale of participatingdevices as well as the overlapping resource requirements of diverse FL jobs.Existing resource managers for FL jobs opt for random assignment of devices toFL jobs for simplicity and scalability, which leads to poor performance. Inthis paper, we present Venn, an FL resource manager, that efficiently schedulesephemeral, heterogeneous devices among many FL jobs, with the goal of reducingtheir average job completion time (JCT). Venn formulates the IntersectionResource Scheduling (IRS) problem to identify complex resource contention amongmultiple FL jobs. Then, Venn proposes a contention-aware scheduling heuristicto minimize the average scheduling delay. Furthermore, it proposes aresource-aware device-to-job matching heuristic that focuses on optimizingresponse collection time by mitigating stragglers. Our evaluation shows that,compared to the state-of-the-art FL resource managers, Venn improves theaverage JCT by up to 1.88X.</description><author>Jiachen Liu, Fan Lai, Ding Ding, Yiwen Zhang, Mosharaf Chowdhury</author><pubDate>Wed, 13 Dec 2023 17:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08298v1</guid></item><item><title>Inferring Atmospheric Properties of Exoplanets with Flow Matching and Neural Importance Sampling</title><link>http://arxiv.org/abs/2312.08295v1</link><description>Atmospheric retrievals (AR) characterize exoplanets by estimating atmosphericparameters from observed light spectra, typically by framing the task as aBayesian inference problem. However, traditional approaches such as nestedsampling are computationally expensive, thus sparking an interest in solutionsbased on machine learning (ML). In this ongoing work, we first explore flowmatching posterior estimation (FMPE) as a new ML-based method for AR and findthat, in our case, it is more accurate than neural posterior estimation (NPE),but less accurate than nested sampling. We then combine both FMPE and NPE withimportance sampling, in which case both methods outperform nested sampling interms of accuracy and simulation efficiency. Going forward, our analysissuggests that simulation-based inference with likelihood-based importancesampling provides a framework for accurate and efficient AR that may become avaluable tool not only for the analysis of observational data from existingtelescopes, but also for the development of new missions and instruments.</description><author>Timothy D. Gebhard, Jonas Wildberger, Maximilian Dax, Daniel Angerhausen, Sascha P. Quanz, Bernhard Schölkopf</author><pubDate>Wed, 13 Dec 2023 17:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08295v1</guid></item><item><title>VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space</title><link>http://arxiv.org/abs/2312.08291v1</link><description>Human Pose and Shape Estimation (HPSE) from RGB images can be broadlycategorized into two main groups: parametric and non-parametric approaches.Parametric techniques leverage a low-dimensional statistical body model forrealistic results, whereas recent non-parametric methods achieve higherprecision by directly regressing the 3D coordinates of the human body. Despitetheir strengths, both approaches face limitations: the parameters ofstatistical body models pose challenges as regression targets, and predicting3D coordinates introduces computational complexities and issues related tosmoothness. In this work, we take a novel approach to address the HPSE problem.We introduce a unique method involving a low-dimensional discrete latentrepresentation of the human mesh, framing HPSE as a classification task.Instead of predicting body model parameters or 3D vertex coordinates, our focusis on forecasting the proposed discrete latent representation, which can bedecoded into a registered human mesh. This innovative paradigm offers two keyadvantages: firstly, predicting a low-dimensional discrete representationconfines our predictions to the space of anthropomorphic poses and shapes;secondly, by framing the problem as a classification task, we can harness thediscriminative power inherent in neural networks. Our proposed model, VQ-HPS, atransformer-based architecture, forecasts the discrete latent representation ofthe mesh, trained through minimizing a cross-entropy loss. Our resultsdemonstrate that VQ-HPS outperforms the current state-of-the-art non-parametricapproaches while yielding results as realistic as those produced by parametricmethods. This highlights the significant potential of the classificationapproach for HPSE.</description><author>Guénolé Fiche, Simon Leglaive, Xavier Alameda-Pineda, Antonio Agudo, Francesc Moreno-Noguer</author><pubDate>Wed, 13 Dec 2023 17:08:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08291v1</guid></item><item><title>PhenDiff: Revealing Invisible Phenotypes with Conditional Diffusion Models</title><link>http://arxiv.org/abs/2312.08290v1</link><description>Over the last five years, deep generative models have gradually been adoptedfor various tasks in biological research. Notably, image-to-image translationmethods showed to be effective in revealing subtle phenotypic cell variationsotherwise invisible to the human eye. Current methods to achieve this goalmainly rely on Generative Adversarial Networks (GANs). However, these modelsare known to suffer from some shortcomings such as training instability andmode collapse. Furthermore, the lack of robustness to invert a real image intothe latent of a trained GAN prevents flexible editing of real images. In thiswork, we propose PhenDiff, an image-to-image translation method based onconditional diffusion models to identify subtle phenotypes in microscopyimages. We evaluate this approach on biological datasets against previous worksuch as CycleGAN. We show that PhenDiff outperforms this baseline in terms ofquality and diversity of the generated images. We then apply this method todisplay invisible phenotypic changes triggered by a rare neurodevelopmentaldisorder on microscopy images of organoids. Altogether, we demonstrate thatPhenDiff is able to perform high quality biological image-to-image translationallowing to spot subtle phenotype variations on a real image.</description><author>Anis Bourou, Thomas Boyer, Kévin Daupin, Véronique Dubreuil, Aurélie De Thonel, Valérie Mezger, Auguste Genovesio</author><pubDate>Wed, 13 Dec 2023 17:06:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08290v1</guid></item><item><title>Hybrid Sample Synthesis-based Debiasing of Classifier in Limited Data Setting</title><link>http://arxiv.org/abs/2312.08288v1</link><description>Deep learning models are known to suffer from the problem of bias, andresearchers have been exploring methods to address this issue. However, most ofthese methods require prior knowledge of the bias and are not always practical.In this paper, we focus on a more practical setting with no prior informationabout the bias. Generally, in this setting, there are a large number ofbias-aligned samples that cause the model to produce biased predictions and afew bias-conflicting samples that do not conform to the bias. If the trainingdata is limited, the influence of the bias-aligned samples may become evenstronger on the model predictions, and we experimentally demonstrate thatexisting debiasing techniques suffer severely in such cases. In this paper, weexamine the effects of unknown bias in small dataset regimes and present anovel approach to mitigate this issue. The proposed approach directly addressesthe issue of the extremely low occurrence of bias-conflicting samples inlimited data settings through the synthesis of hybrid samples that can be usedto reduce the effect of bias. We perform extensive experiments on severalbenchmark datasets and experimentally demonstrate the effectiveness of ourproposed approach in addressing any unknown bias in the presence of limiteddata. Specifically, our approach outperforms the vanilla, LfF, LDD, and DebiANdebiasing methods by absolute margins of 10.39%, 9.08%, 8.07%, and 9.67% whenonly 10% of the Corrupted CIFAR-10 Type 1 dataset is available with abias-conflicting sample ratio of 0.05.</description><author>Piyush Arora, Pratik Mazumder</author><pubDate>Wed, 13 Dec 2023 17:04:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08288v1</guid></item><item><title>On the verification of Embeddings using Hybrid Markov Logic</title><link>http://arxiv.org/abs/2312.08287v1</link><description>The standard approach to verify representations learned by Deep NeuralNetworks is to use them in specific tasks such as classification or regression,and measure their performance based on accuracy in such tasks. However, in manycases, we would want to verify more complex properties of a learnedrepresentation. To do this, we propose a framework based on a probabilisticfirst-order language, namely, Hybrid Markov Logic Networks (HMLNs) where wespecify properties over embeddings mixed with symbolic domain knowledge. Wepresent an approach to learn parameters for the properties within thisframework. Further, we develop a verification method to test embeddings in thisframework by encoding this task as a Mixed Integer Linear Program for which wecan leverage existing state-of-the-art solvers. We illustrate verification inGraph Neural Networks, Deep Knowledge Tracing and Intelligent Tutoring Systemsto demonstrate the generality of our approach.</description><author>Anup Shakya, Abisha Thapa Magar, Somdeb Sarkhel, Deepak Venugopal</author><pubDate>Wed, 13 Dec 2023 17:04:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08287v1</guid></item><item><title>Prompting LLMs with content plans to enhance the summarization of scientific articles</title><link>http://arxiv.org/abs/2312.08282v1</link><description>This paper presents novel prompting techniques to improve the performance ofautomatic summarization systems for scientific articles. Scientific articlesummarization is highly challenging due to the length and complexity of thesedocuments. We conceive, implement, and evaluate prompting techniques thatprovide additional contextual information to guide summarization systems.Specifically, we feed summarizers with lists of key terms extracted fromarticles, such as author keywords or automatically generated keywords. Ourtechniques are tested with various summarization models and input texts.Results show performance gains, especially for smaller models summarizingsections separately. This evidences that prompting is a promising approach toovercoming the limitations of less powerful systems. Our findings introduce anew research direction of using prompts to aid smaller models.</description><author>Aldan Creo, Manuel Lama, Juan C. Vidal</author><pubDate>Wed, 13 Dec 2023 16:57:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08282v1</guid></item><item><title>GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values</title><link>http://arxiv.org/abs/2311.03426v2</link><description>Massive transformer-based models face several challenges, including slow andcomputationally intensive pre-training and over-parametrization. This paperaddresses these challenges by proposing a versatile method called GQKVA, whichgeneralizes query, key, and value grouping techniques. GQKVA is designed tospeed up transformer pre-training while reducing the model size. Ourexperiments with various GQKVA variants highlight a clear trade-off betweenperformance and model size, allowing for customized choices based on resourceand time limitations. Our findings also indicate that the conventionalmulti-head attention approach is not always the best choice, as there arelighter and faster alternatives available. We tested our method on ViT, whichachieved an approximate 0.3% increase in accuracy while reducing the model sizeby about 4% in the task of image classification. Additionally, our mostaggressive model reduction experiment resulted in a reduction of approximately15% in model size, with only around a 1% drop in accuracy.</description><author>Farnoosh Javadi, Walid Ahmed, Habib Hajimolahoseini, Foozhan Ataiefard, Mohammad Hassanpour, Saina Asani, Austin Wen, Omar Mohamed Awad, Kangling Liu, Yang Liu</author><pubDate>Wed, 13 Dec 2023 16:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03426v2</guid></item><item><title>High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models</title><link>http://arxiv.org/abs/2312.08274v1</link><description>Objective: To develop a high-throughput biomedical relation extraction systemthat takes advantage of the large language models' (LLMs) reading comprehensionability and biomedical world knowledge in a scalable and evidential manner.Methods: We formulate the relation extraction task as a simple binaryclassification problem for large language models such as ChatGPT. Specifically,LLMs make the decision based on the external corpus and its world knowledge,giving the reason for the judgment to factual verification. This method istailored for semi-structured web articles, wherein we designate the main titleas the tail entity and explicitly incorporate it into the context, and thepotential head entities are matched based on a biomedical thesaurus. Moreover,lengthy contents are sliced into text chunks, embedded, and retrieved withadditional embedding models, ensuring compatibility with the context windowsize constraints of available open-source LLMs. Results: Using an open-sourceLLM, we extracted 304315 relation triplets of three distinct relation typesfrom four reputable biomedical websites. To assess the efficacy of the basicpipeline employed for biomedical relation extraction, we curated a benchmarkdataset annotated by a medical expert. Evaluation results indicate that thepipeline exhibits performance comparable to that of GPT-4. Case studies furtherilluminate challenges faced by contemporary LLMs in the context of biomedicalrelation extraction for semi-structured web articles. Conclusion: The proposedmethod has demonstrated its effectiveness in leveraging the strengths of LLMsfor high-throughput biomedical relation extraction. Its adaptability isevident, as it can be seamlessly extended to diverse semi-structured biomedicalwebsites, facilitating the extraction of various types of biomedical relationswith ease.</description><author>Songchi Zhou, Sheng Yu</author><pubDate>Wed, 13 Dec 2023 16:43:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08274v1</guid></item><item><title>Mixed moving average field guided learning for spatio-temporal data</title><link>http://arxiv.org/abs/2301.00736v3</link><description>Influenced mixed moving average fields are a versatile modeling class forspatio-temporal data. However, their predictive distribution is not generallyknown. Under this modeling assumption, we define a novel spatio-temporalembedding and a theory-guided machine learning approach that employs ageneralized Bayesian algorithm to make ensemble forecasts. We employ Lipschitzpredictors and determine fixed-time and any-time PAC Bayesian bounds in thebatch learning setting. Performing causal forecast is a highlight of ourmethodology as its potential application to data with spatial and temporalshort and long-range dependence. We then test the performance of our learningmethodology by using linear predictors and data sets simulated from aspatio-temporal Ornstein-Uhlenbeck process.</description><author>Imma Valentina Curato, Orkun Furat, Lorenzo Proietti, Bennet Stroeh</author><pubDate>Wed, 13 Dec 2023 16:33:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00736v3</guid></item><item><title>Fit Like You Sample: Sample-Efficient Generalized Score Matching from Fast Mixing Diffusions</title><link>http://arxiv.org/abs/2306.09332v3</link><description>Score matching is an approach to learning probability distributionsparametrized up to a constant of proportionality (e.g. Energy-Based Models).The idea is to fit the score of the distribution, rather than the likelihood,thus avoiding the need to evaluate the constant of proportionality. Whilethere's a clear algorithmic benefit, the statistical "cost'' can be steep:recent work by Koehler et al. 2022 showed that for distributions that have poorisoperimetric properties (a large Poincar\'e or log-Sobolev constant), scorematching is substantially statistically less efficient than maximum likelihood.However, many natural realistic distributions, e.g. multimodal distributions assimple as a mixture of two Gaussians in one dimension -- have a poor Poincar\'econstant. In this paper, we show a close connection between the mixing time of a broadclass of Markov processes with generator $\mathcal{L}$ and an appropriatelychosen generalized score matching loss that tries to fit $\frac{\mathcal{O}p}{p}$. This allows us to adapt techniques to speed up Markov chains toconstruct better score-matching losses. In particular, ``preconditioning'' thediffusion can be translated to an appropriate ``preconditioning'' of the scoreloss. Lifting the chain by adding a temperature like in simulated tempering canbe shown to result in a Gaussian-convolution annealed score matching loss,similar to Song and Ermon, 2019. Moreover, we show that if the distributionbeing learned is a finite mixture of Gaussians in $d$ dimensions with a sharedcovariance, the sample complexity of annealed score matching is polynomial inthe ambient dimension, the diameter of the means, and the smallest and largesteigenvalues of the covariance -- obviating the Poincar\'e constant-based lowerbounds of the basic score matching loss shown in Koehler et al. 2022.</description><author>Yilong Qin, Andrej Risteski</author><pubDate>Wed, 13 Dec 2023 16:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09332v3</guid></item><item><title>Efficient Multi-Object Pose Estimation using Multi-Resolution Deformable Attention and Query Aggregation</title><link>http://arxiv.org/abs/2312.08268v1</link><description>Object pose estimation is a long-standing problem in computer vision.Recently, attention-based vision transformer models have achievedstate-of-the-art results in many computer vision applications. Exploiting thepermutation-invariant nature of the attention mechanism, a family of visiontransformer models formulate multi-object pose estimation as a set predictionproblem. However, existing vision transformer models for multi-object poseestimation rely exclusively on the attention mechanism. Convolutional neuralnetworks, on the other hand, hard-wire various inductive biases into theirarchitecture. In this paper, we investigate incorporating inductive biases invision transformer models for multi-object pose estimation, which facilitateslearning long-range dependencies while circumventing the costly globalattention. In particular, we use multi-resolution deformable attention, wherethe attention operation is performed only between a few deformed referencepoints. Furthermore, we propose a query aggregation mechanism that enablesincreasing the number of object queries without increasing the computationalcomplexity. We evaluate the proposed model on the challenging YCB-Video datasetand report state-of-the-art results.</description><author>Arul Selvam Periyasamy, Vladimir Tsaturyan, Sven Behnke</author><pubDate>Wed, 13 Dec 2023 16:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08268v1</guid></item><item><title>TABSurfer: a Hybrid Deep Learning Architecture for Subcortical Segmentation</title><link>http://arxiv.org/abs/2312.08267v1</link><description>Subcortical segmentation remains challenging despite its importantapplications in quantitative structural analysis of brain MRI scans. The mostaccurate method, manual segmentation, is highly labor intensive, so automatedtools like FreeSurfer have been adopted to handle this task. However, thesetraditional pipelines are slow and inefficient for processing large datasets.In this study, we propose TABSurfer, a novel 3D patch-based CNN-Transformerhybrid deep learning model designed for superior subcortical segmentationcompared to existing state-of-the-art tools. To evaluate, we first demonstrateTABSurfer's consistent performance across various T1w MRI datasets withsignificantly shorter processing times compared to FreeSurfer. Then, wevalidate against manual segmentations, where TABSurfer outperforms FreeSurferbased on the manual ground truth. In each test, we also establish TABSurfer'sadvantage over a leading deep learning benchmark, FastSurferVINN. Together,these studies highlight TABSurfer's utility as a powerful tool for fullyautomated subcortical segmentation with high fidelity.</description><author>Aaron Cao, Vishwanatha M. Rao, Kejia Liu, Xinru Liu, Andrew F. Laine, Jia Guo</author><pubDate>Wed, 13 Dec 2023 16:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08267v1</guid></item><item><title>Rethinking Label Smoothing on Multi-hop Question Answering</title><link>http://arxiv.org/abs/2212.09512v3</link><description>Multi-Hop Question Answering (MHQA) is a significant area in questionanswering, requiring multiple reasoning components, including documentretrieval, supporting sentence prediction, and answer span extraction. In thiswork, we analyze the primary factors limiting the performance of multi-hopreasoning and introduce label smoothing into the MHQA task. This is aimed atenhancing the generalization capabilities of MHQA systems and mitigatingoverfitting of answer spans and reasoning paths in training set. We propose anovel label smoothing technique, F1 Smoothing, which incorporates uncertaintyinto the learning process and is specifically tailored for Machine ReadingComprehension (MRC) tasks. Inspired by the principles of curriculum learning,we introduce the Linear Decay Label Smoothing Algorithm (LDLA), whichprogressively reduces uncertainty throughout the training process. Experimenton the HotpotQA dataset demonstrates the effectiveness of our methods inenhancing performance and generalizability in multi-hop reasoning, achievingnew state-of-the-art results on the leaderboard.</description><author>Zhangyue Yin, Yuxin Wang, Xiannian Hu, Yiguang Wu, Hang Yan, Xinyu Zhang, Zhao Cao, Xuanjing Huang, Xipeng Qiu</author><pubDate>Wed, 13 Dec 2023 16:27:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09512v3</guid></item><item><title>Crystal-GFN: sampling crystals with desirable properties and constraints</title><link>http://arxiv.org/abs/2310.04925v2</link><description>Accelerating material discovery holds the potential to greatly help mitigatethe climate crisis. Discovering new solid-state materials such aselectrocatalysts, super-ionic conductors or photovoltaic materials can have acrucial impact, for instance, in improving the efficiency of renewable energyproduction and storage. In this paper, we introduce Crystal-GFN, a generativemodel of crystal structures that sequentially samples structural properties ofcrystalline materials, namely the space group, composition and latticeparameters. This domain-inspired approach enables the flexible incorporation ofphysical and structural hard constraints, as well as the use of any availablepredictive model of a desired physicochemical property as an objectivefunction. To design stable materials, one must target the candidates with thelowest formation energy. Here, we use as objective the formation energy peratom of a crystal structure predicted by a new proxy machine learning modeltrained on MatBench. The results demonstrate that Crystal-GFN is able to samplehighly diverse crystals with low (median -3.1 eV/atom) predicted formationenergy.</description><author>Mila AI4Science, Alex Hernandez-Garcia, Alexandre Duval, Alexandra Volokhova, Yoshua Bengio, Divya Sharma, Pierre Luc Carrier, Yasmine Benabed, Michał Koziarski, Victor Schmidt</author><pubDate>Wed, 13 Dec 2023 16:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04925v2</guid></item><item><title>Towards Optimal Sobolev Norm Rates for the Vector-Valued Regularized Least-Squares Algorithm</title><link>http://arxiv.org/abs/2312.07186v2</link><description>We present the first optimal rates for infinite-dimensional vector-valuedridge regression on a continuous scale of norms that interpolate between $L_2$and the hypothesis space, which we consider as a vector-valued reproducingkernel Hilbert space. These rates allow to treat the misspecified case in whichthe true regression function is not contained in the hypothesis space. Wecombine standard assumptions on the capacity of the hypothesis space with anovel tensor product construction of vector-valued interpolation spaces inorder to characterize the smoothness of the regression function. Our upperbound not only attains the same rate as real-valued kernel ridge regression,but also removes the assumption that the target regression function is bounded.For the lower bound, we reduce the problem to the scalar setting using aprojection argument. We show that these rates are optimal in most cases andindependent of the dimension of the output space. We illustrate our results forthe special case of vector-valued Sobolev spaces.</description><author>Zhu Li, Dimitri Meunier, Mattes Mollenhauer, Arthur Gretton</author><pubDate>Wed, 13 Dec 2023 16:23:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07186v2</guid></item><item><title>HappyFeat -- An interactive and efficient BCI framework for clinical applications</title><link>http://arxiv.org/abs/2310.02948v2</link><description>Brain-Computer Interface (BCI) systems allow users to perform actions bytranslating their brain activity into commands. Such systems usually need atraining phase, consisting in training a classification algorithm todiscriminate between mental states using specific features from the recordedsignals. This phase of feature selection and training is crucial for BCIperformance and presents specific constraints to be met in a clinical context,such as post-stroke rehabilitation. In this paper, we present HappyFeat, a software making Motor Imagery (MI)based BCI experiments easier, by gathering all necessary manipulations andanalysis in a single convenient GUI and via automation of experiment oranalysis parameters. The resulting workflow allows for effortlessly selectingthe best features, helping to achieve good BCI performance in time-constrainedenvironments. Alternative features based on Functional Connectivity can be usedand compared or combined with Power Spectral Density, allowing anetwork-oriented approach. We then give details of HappyFeat's main mechanisms, and a review of itsperformances in typical use cases. We also show that it can be used as anefficient tool for comparing different metrics extracted from the signals, totrain the classification algorithm. To this end, we show a comparison betweenthe commonly-used Power Spectral Density and network metrics based onFunctional Connectivity. HappyFeat is available as an open-source project which can be freelydownloaded on GitHub.</description><author>Arthur Desbois, Tristan Venot, Fabrizio De Vico Fallani, Marie-Constance Corsi</author><pubDate>Wed, 13 Dec 2023 16:21:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02948v2</guid></item><item><title>\emph{Lifted} RDT based capacity analysis of the 1-hidden layer treelike \emph{sign} perceptrons neural networks</title><link>http://arxiv.org/abs/2312.08257v1</link><description>We consider the memorization capabilities of multilayered \emph{sign}perceptrons neural networks (SPNNs). A recent rigorous upper-bounding capacitycharacterization, obtained in \cite{Stojnictcmspnncaprdt23} utilizing theRandom Duality Theory (RDT), demonstrated that adding neurons in a networkconfiguration may indeed be very beneficial. Moreover, for particular\emph{treelike committee machines} (TCM) architectures with $d\leq 5$ neuronsin the hidden layer, \cite{Stojnictcmspnncaprdt23} made a very firstmathematically rigorous progress in over 30 years by lowering the previouslybest known capacity bounds of \cite{MitchDurb89}. Here, we first establish thatthe RDT bounds from \cite{Stojnictcmspnncaprdt23} scale as $\sim \sqrt{d}$ andcan not on their own \emph{universally} (over the entire range of $d$) beat thebest known $\sim \log(d)$ scaling of the bounds from \cite{MitchDurb89}. Afterrecognizing that the progress from \cite{Stojnictcmspnncaprdt23} is thereforepromising, but yet without a complete concretization, we then proceed byconsidering the recently developed fully lifted RDT (fl RDT) as an alternative.While the fl RDT is indeed a powerful juggernaut, it typically relies on heavynumerical evaluations. To avoid such heavy numerics, we here focus on asimplified, \emph{partially lifted}, variant and show that it allows for veryneat, closed form, analytical capacity characterizations. Moreover, we obtainthe concrete capacity bounds that \emph{universally} improve for \emph{any} $d$over the best known ones of \cite{MitchDurb89}.</description><author>Mihailo Stojnic</author><pubDate>Wed, 13 Dec 2023 16:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08257v1</guid></item><item><title>A Compact and Semantic Latent Space for Disentangled and Controllable Image Editing</title><link>http://arxiv.org/abs/2312.08256v1</link><description>Recent advances in the field of generative models and in particulargenerative adversarial networks (GANs) have lead to substantial progress forcontrolled image editing, especially compared with the pre-deep learning era.Despite their powerful ability to apply realistic modifications to an image,these methods often lack properties like disentanglement (the capacity to editattributes independently). In this paper, we propose an auto-encoder whichre-organizes the latent space of StyleGAN, so that each attribute which we wishto edit corresponds to an axis of the new latent space, and furthermore thatthe latent axes are decorrelated, encouraging disentanglement. We work in acompressed version of the latent space, using Principal Component Analysis,meaning that the parameter complexity of our autoencoder is reduced, leading toshort training times ($\sim$ 45 mins). Qualitative and quantitative resultsdemonstrate the editing capabilities of our approach, with greaterdisentanglement than competing methods, while maintaining fidelity to theoriginal image with respect to identity. Our autoencoder architecture simpleand straightforward, facilitating implementation.</description><author>Gwilherm Lesné, Yann Gousseau, Saïd Ladjal, Alasdair Newson</author><pubDate>Wed, 13 Dec 2023 16:18:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08256v1</guid></item><item><title>OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods</title><link>http://arxiv.org/abs/2312.08255v1</link><description>Optical coherence tomography (OCT) is a non-invasive imaging technique withextensive clinical applications in ophthalmology. OCT enables the visualizationof the retinal layers, playing a vital role in the early detection andmonitoring of retinal diseases. OCT uses the principle of light waveinterference to create detailed images of the retinal microstructures, makingit a valuable tool for diagnosing ocular conditions. This work presents anopen-access OCT dataset (OCTDL) comprising over 1600 high-resolution OCT imageslabeled according to disease group and retinal pathology. The dataset consistsof OCT records of patients with Age-related Macular Degeneration (AMD),Diabetic Macular Edema (DME), Epiretinal Membrane (ERM), Retinal ArteryOcclusion (RAO), Retinal Vein Occlusion (RVO), and Vitreomacular InterfaceDisease (VID). The images were acquired with an Optovue Avanti RTVue XR usingraster scanning protocols with dynamic scan length and image resolution. Eachretinal b-scan was acquired by centering on the fovea and interpreted andcataloged by an experienced retinal specialist. In this work, we applied DeepLearning classification techniques to this new open-access dataset.</description><author>Mikhail Kulyabin, Aleksei Zhdanov, Anastasia Nikiforova, Andrey Stepichev, Anna Kuznetsova, Mikhail Ronkin, Vasilii Borisov, Alexander Bogachev, Sergey Korotkich, Paul A Constable, Andreas Maier</author><pubDate>Wed, 13 Dec 2023 16:18:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08255v1</guid></item><item><title>A Survey of Generative AI for Intelligent Transportation Systems</title><link>http://arxiv.org/abs/2312.08248v1</link><description>Intelligent transportation systems play a crucial role in modern trafficmanagement and optimization, greatly improving traffic efficiency and safety.With the rapid development of generative artificial intelligence (GenerativeAI) technologies in the fields of image generation and natural languageprocessing, generative AI has also played a crucial role in addressing keyissues in intelligent transportation systems, such as data sparsity, difficultyin observing abnormal scenarios, and in modeling data uncertainty. In thisreview, we systematically investigate the relevant literature on generative AItechniques in addressing key issues in different types of tasks in intelligenttransportation systems. First, we introduce the principles of differentgenerative AI techniques, and their potential applications. Then, we classifytasks in intelligent transportation systems into four types: trafficperception, traffic prediction, traffic simulation, and trafficdecision-making. We systematically illustrate how generative AI techniquesaddresses key issues in these four different types of tasks. Finally, wesummarize the challenges faced in applying generative AI to intelligenttransportation systems, and discuss future research directions based ondifferent application scenarios.</description><author>Huan Yan, Yong Li</author><pubDate>Wed, 13 Dec 2023 16:13:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08248v1</guid></item><item><title>Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening</title><link>http://arxiv.org/abs/2308.07707v2</link><description>Machine unlearning, the ability for a machine learning model to forget, isbecoming increasingly important to comply with data privacy regulations, aswell as to remove harmful, manipulated, or outdated information. The keychallenge lies in forgetting specific information while protecting modelperformance on the remaining data. While current state-of-the-art methodsperform well, they typically require some level of retraining over the retaineddata, in order to protect or restore model performance. This adds computationaloverhead and mandates that the training data remain available and accessible,which may not be feasible. In contrast, other methods employ a retrain-freeparadigm, however, these approaches are prohibitively computationally expensiveand do not perform on par with their retrain-based counterparts. We presentSelective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-freeapproach to machine unlearning which is fast, performant, and does not requirelong-term storage of the training data. First, SSD uses the Fisher informationmatrix of the training and forgetting data to select parameters that aredisproportionately important to the forget set. Second, SSD induces forgettingby dampening these parameters proportional to their relative importance to theforget set with respect to the wider training data. We evaluate our methodagainst several existing unlearning methods in a range of experiments usingResNet18 and Vision Transformer. Results show that the performance of SSD iscompetitive with retrain-based post hoc methods, demonstrating the viability ofretrain-free post hoc unlearning approaches.</description><author>Jack Foster, Stefan Schoepf, Alexandra Brintrup</author><pubDate>Wed, 13 Dec 2023 16:11:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07707v2</guid></item><item><title>Capacity of the treelike sign perceptrons neural networks with one hidden layer -- RDT based upper bounds</title><link>http://arxiv.org/abs/2312.08244v1</link><description>We study the capacity of \emph{sign} perceptrons neural networks (SPNN) andparticularly focus on 1-hidden layer \emph{treelike committee machine} (TCM)architectures. Similarly to what happens in the case of a single perceptronneuron, it turns out that, in a statistical sense, the capacity of acorresponding multilayered network architecture consisting of multiple\emph{sign} perceptrons also undergoes the so-called phase transition (PT)phenomenon. This means: (i) for certain range of system parameters (size ofdata, number of neurons), the network can be properly trained to accuratelymemorize \emph{all} elements of the input dataset; and (ii) outside the regionsuch a training does not exist. Clearly, determining the corresponding phasetransition curve that separates these regions is an extraordinary task andamong the most fundamental questions related to the performance of any network.Utilizing powerful mathematical engine called Random Duality Theory (RDT), weestablish a generic framework for determining the upper bounds on the 1-hiddenlayer TCM SPNN capacity. Moreover, we do so for \emph{any} given (odd) numberof neurons. We further show that the obtained results \emph{exactly} match thereplica symmetry predictions of \cite{EKTVZ92,BHS92}, thereby proving that thestatistical physics based results are not only nice estimates but alsomathematically rigorous bounds as well. Moreover, for $d\leq 5$, we obtain thecapacity values that improve on the best known rigorous ones of\cite{MitchDurb89}, thereby establishing a first, mathematically rigorous,progress in well over 30 years.</description><author>Mihailo Stojnic</author><pubDate>Wed, 13 Dec 2023 16:06:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08244v1</guid></item><item><title>CenterGrasp: Object-Aware Implicit Representation Learning for Simultaneous Shape Reconstruction and 6-DoF Grasp Estimation</title><link>http://arxiv.org/abs/2312.08240v1</link><description>Reliable object grasping is a crucial capability for autonomous robots.However, many existing grasping approaches focus on general clutter removalwithout explicitly modeling objects and thus only relying on the visible localgeometry. We introduce CenterGrasp, a novel framework that combines objectawareness and holistic grasping. CenterGrasp learns a general object prior byencoding shapes and valid grasps in a continuous latent space. It consists ofan RGB-D image encoder that leverages recent advances to detect objects andinfer their pose and latent code, and a decoder to predict shape and grasps foreach object in the scene. We perform extensive experiments on simulated as wellas real-world cluttered scenes and demonstrate strong scene reconstruction and6-DoF grasp-pose estimation performance. Compared to the state of the art,CenterGrasp achieves an improvement of 38.5 mm in shape reconstruction and 33percentage points on average in grasp success. We make the code and trainedmodels publicly available at http://centergrasp.cs.uni-freiburg.de.</description><author>Eugenio Chisari, Nick Heppert, Tim Welschehold, Wolfram Burgard, Abhinav Valada</author><pubDate>Wed, 13 Dec 2023 16:01:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08240v1</guid></item><item><title>Beyond the Label Itself: Latent Labels Enhance Semi-supervised Point Cloud Panoptic Segmentation</title><link>http://arxiv.org/abs/2312.08234v1</link><description>As the exorbitant expense of labeling autopilot datasets and the growingtrend of utilizing unlabeled data, semi-supervised segmentation on point cloudsbecomes increasingly imperative. Intuitively, finding out more ``unspokenwords'' (i.e., latent instance information) beyond the label itself should behelpful to improve performance. In this paper, we discover two types of latentlabels behind the displayed label embedded in LiDAR and image data. First, inthe LiDAR Branch, we propose a novel augmentation, Cylinder-Mix, which is ableto augment more yet reliable samples for training. Second, in the Image Branch,we propose the Instance Position-scale Learning (IPSL) Module to learn and fusethe information of instance position and scale, which is from a 2D pre-traineddetector and a type of latent label obtained from 3D to 2D projection. Finally,the two latent labels are embedded into the multi-modal panoptic segmentationnetwork. The ablation of the IPSL module demonstrates its robust adaptability,and the experiments evaluated on SemanticKITTI and nuScenes demonstrate thatour model outperforms the state-of-the-art method, LaserMix.</description><author>Yujun Chen, Xin Tan, Zhizhong Zhang, Yanyun Qu, Yuan Xie</author><pubDate>Wed, 13 Dec 2023 15:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08234v1</guid></item><item><title>Partial Symmetry Detection for 3D Geometry using Contrastive Learning with Geodesic Point Cloud Patches</title><link>http://arxiv.org/abs/2312.08230v1</link><description>Symmetry detection, especially partial and extrinsic symmetry, is essentialfor various downstream tasks, like 3D geometry completion, segmentation,compression and structure-aware shape encoding or generation. In order todetect partial extrinsic symmetries, we propose to learn rotation, reflection,translation and scale invariant local shape features for geodesic point cloudpatches via contrastive learning, which are robust across multiple classes andgeneralize over different datasets. We show that our approach is able toextract multiple valid solutions for this ambiguous problem. Furthermore, weintroduce a novel benchmark test for partial extrinsic symmetry detection toevaluate our method. Lastly, we incorporate the detected symmetries togetherwith a region growing algorithm to demonstrate a downstream task with the goalof computing symmetry-aware partitions of 3D shapes. To our knowledge, we arethe first to propose a self-supervised data-driven method for partial extrinsicsymmetry detection.</description><author>Gregor Kobsik, Isaak Lim, Leif Kobbelt</author><pubDate>Wed, 13 Dec 2023 15:48:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08230v1</guid></item><item><title>BIRB: A Generalization Benchmark for Information Retrieval in Bioacoustics</title><link>http://arxiv.org/abs/2312.07439v2</link><description>The ability for a machine learning model to cope with differences in trainingand deployment conditions--e.g. in the presence of distribution shift or thegeneralization to new classes altogether--is crucial for real-world use cases.However, most empirical work in this area has focused on the image domain withartificial benchmarks constructed to measure individual aspects ofgeneralization. We present BIRB, a complex benchmark centered on the retrievalof bird vocalizations from passively-recorded datasets given focal recordingsfrom a large citizen science corpus available for training. We propose abaseline system for this collection of tasks using representation learning anda nearest-centroid search. Our thorough empirical evaluation and analysissurfaces open research directions, suggesting that BIRB fills the need for amore realistic and complex benchmark to drive progress on robustness todistribution shifts and generalization of ML models.</description><author>Jenny Hamer, Eleni Triantafillou, Bart van Merriënboer, Stefan Kahl, Holger Klinck, Tom Denton, Vincent Dumoulin</author><pubDate>Wed, 13 Dec 2023 15:48:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07439v2</guid></item><item><title>Differentially Private Gradient Flow based on the Sliced Wasserstein Distance for Non-Parametric Generative Modeling</title><link>http://arxiv.org/abs/2312.08227v1</link><description>Safeguarding privacy in sensitive training data is paramount, particularly inthe context of generative modeling. This is done through either differentiallyprivate stochastic gradient descent, or with a differentially private metricfor training models or generators. In this paper, we introduce a noveldifferentially private generative modeling approach based on parameter-freegradient flows in the space of probability measures. The proposed algorithm isa new discretized flow which operates through a particle scheme, utilizingdrift derived from the sliced Wasserstein distance and computed in a privatemanner. Our experiments show that compared to a generator-based model, ourproposed model can generate higher-fidelity data at a low privacy budget,offering a viable alternative to generator-based approaches.</description><author>Ilana Sebag, Muni Sreenivas PYDI, Jean-Yves Franceschi, Alain Rakotomamonjy, Mike Gartrell, Jamal Atif, Alexandre Allauzen</author><pubDate>Wed, 13 Dec 2023 15:47:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08227v1</guid></item><item><title>GLOP: Learning Global Partition and Local Construction for Solving Large-scale Routing Problems in Real-time</title><link>http://arxiv.org/abs/2312.08224v1</link><description>The recent end-to-end neural solvers have shown promise for small-scalerouting problems but suffered from limited real-time scaling-up performance.This paper proposes GLOP (Global and Local Optimization Policies), a unifiedhierarchical framework that efficiently scales toward large-scale routingproblems. GLOP partitions large routing problems into Travelling SalesmanProblems (TSPs) and TSPs into Shortest Hamiltonian Path Problems. For the firsttime, we hybridize non-autoregressive neural heuristics for coarse-grainedproblem partitions and autoregressive neural heuristics for fine-grained routeconstructions, leveraging the scalability of the former and the meticulousnessof the latter. Experimental results show that GLOP achieves competitive andstate-of-the-art real-time performance on large-scale routing problems,including TSP, ATSP, CVRP, and PCTSP.</description><author>Haoran Ye, Jiarui Wang, Helan Liang, Zhiguang Cao, Yong Li, Fanzhang Li</author><pubDate>Wed, 13 Dec 2023 15:46:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08224v1</guid></item><item><title>Patch-wise Graph Contrastive Learning for Image Translation</title><link>http://arxiv.org/abs/2312.08223v1</link><description>Recently, patch-wise contrastive learning is drawing attention for the imagetranslation by exploring the semantic correspondence between the input andoutput images. To further explore the patch-wise topology for high-levelsemantic understanding, here we exploit the graph neural network to capture thetopology-aware features. Specifically, we construct the graph based on thepatch-wise similarity from a pretrained encoder, whose adjacency matrix isshared to enhance the consistency of patch-wise relation between the input andthe output. Then, we obtain the node feature from the graph neural network, andenhance the correspondence between the nodes by increasing mutual informationusing the contrastive loss. In order to capture the hierarchical semanticstructure, we further propose the graph pooling. Experimental resultsdemonstrate the state-of-art results for the image translation thanks to thesemantic encoding by the constructed graphs.</description><author>Chanyong Jung, Gihyun Kwon, Jong Chul Ye</author><pubDate>Wed, 13 Dec 2023 15:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08223v1</guid></item><item><title>Curriculum-Enhanced Residual Soft An-Isotropic Normalization for Over-smoothness in Deep GNNs</title><link>http://arxiv.org/abs/2312.08221v1</link><description>Despite Graph neural networks' significant performance gain over many classictechniques in various graph-related downstream tasks, their successes arerestricted in shallow models due to over-smoothness and the difficulties ofoptimizations among many other issues. In this paper, to alleviate theover-smoothing issue, we propose a soft graph normalization method to preservethe diversities of node embeddings and prevent indiscrimination due to possibleover-closeness. Combined with residual connections, we analyze the reason whythe method can effectively capture the knowledge in both input graph structuresand node features even with deep networks. Additionally, inspired by CurriculumLearning that learns easy examples before the hard ones, we propose a novellabel-smoothing-based learning framework to enhance the optimization of deepGNNs, which iteratively smooths labels in an auxiliary graph and constructsmany gradual non-smooth tasks for extracting increasingly complex knowledge andgradually discriminating nodes from coarse to fine. The method arguably reducesthe risk of overfitting and generalizes better results. Finally, extensiveexperiments are carried out to demonstrate the effectiveness and potential ofthe proposed model and learning framework through comparison with twelveexisting baselines including the state-of-the-art methods on twelve real-worldnode classification benchmarks.</description><author>Jin Li, Qirong Zhang, Shuling Xu, Xinlong Chen, Longkun Guo, Yang-Geng Fu</author><pubDate>Wed, 13 Dec 2023 15:42:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08221v1</guid></item><item><title>EventAid: Benchmarking Event-aided Image/Video Enhancement Algorithms with Real-captured Hybrid Dataset</title><link>http://arxiv.org/abs/2312.08220v1</link><description>Event cameras are emerging imaging technology that offers advantages overconventional frame-based imaging sensors in dynamic range and sensing speed.Complementing the rich texture and color perception of traditional imageframes, the hybrid camera system of event and frame-based cameras enableshigh-performance imaging. With the assistance of event cameras, high-qualityimage/video enhancement methods make it possible to break the limits oftraditional frame-based cameras, especially exposure time, resolution, dynamicrange, and frame rate limits. This paper focuses on five event-aided image andvideo enhancement tasks (i.e., event-based video reconstruction, event-aidedhigh frame rate video reconstruction, image deblurring, image super-resolution,and high dynamic range image reconstruction), provides an analysis of theeffects of different event properties, a real-captured and ground truth labeledbenchmark dataset, a unified benchmarking of state-of-the-art methods, and anevaluation for two mainstream event simulators. In detail, this paper collectsa real-captured evaluation dataset EventAid for five event-aided image/videoenhancement tasks, by using "Event-RGB" multi-camera hybrid system, taking intoaccount scene diversity and spatiotemporal synchronization. We further performquantitative and visual comparisons for state-of-the-art algorithms, provide acontrolled experiment to analyze the performance limit of event-aided imagedeblurring methods, and discuss open problems to inspire future research.</description><author>Peiqi Duan, Boyu Li, Yixin Yang, Hanyue Lou, Minggui Teng, Yi Ma, Boxin Shi</author><pubDate>Wed, 13 Dec 2023 15:42:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08220v1</guid></item><item><title>Loci-Segmented: Improving Scene Segmentation Learning</title><link>http://arxiv.org/abs/2310.10410v2</link><description>Slot-oriented approaches for compositional scene segmentation from images andvideos still depend on provided background information or slot assignments. Wepresent Loci-Segmented (Loci-s) building on the slot-based location andidentity tracking architecture Loci (Traub et al., ICLR 2023). Loci-s enablesdynamic (i) background processing by means of a foreground identifying moduleand a background re-generator; (ii) top-down modified object-focused bottom-upprocessing; and (iii) depth estimate generation. We also improve automatic slotassignment via a slot-location-entity regularization mechanism and a priorsegmentation network. The results reveal superior video decompositionperformance in the MOVi datasets and in another established dataset collectiontargeting scene segmentation. Loci-s outperforms the state-of-the-art withrespect to the intersection over union (IoU) score in the multi-object videodataset MOVi-E by a large margin and even without supervised slot assignmentsand without the provision of background information. We furthermore show thatLoci-s generates well-interpretable latent representations. Theserepresentations may serve as a foundation-model-like interpretable basis forsolving downstream tasks, such as grounding language, forming compositionalrules, or solving one-shot reinforcement learning tasks.</description><author>Manuel Traub, Frederic Becker, Adrian Sauter, Sebastian Otte, Martin V. Butz</author><pubDate>Wed, 13 Dec 2023 15:30:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10410v2</guid></item><item><title>Accelerated Event-Based Feature Detection and Compression for Surveillance Video Systems</title><link>http://arxiv.org/abs/2312.08213v1</link><description>The strong temporal consistency of surveillance video enables compellingcompression performance with traditional methods, but downstream visionapplications operate on decoded image frames with a high data rate. Since it isnot straightforward for applications to extract information on temporalredundancy from the compressed video representations, we propose a novel systemwhich conveys temporal redundancy within a sparse decompressed representation.We leverage a video representation framework called ADDER to transcode framedvideos to sparse, asynchronous intensity samples. We introduce mechanisms forcontent adaptation, lossy compression, and asynchronous forms of classicalvision algorithms. We evaluate our system on the VIRAT surveillance videodataset, and we show a median 43.7% speed improvement in FAST feature detectioncompared to OpenCV. We run the same algorithm as OpenCV, but only processpixels that receive new asynchronous events, rather than process every pixel inan image frame. Our work paves the way for upcoming neuromorphic sensors and isamenable to future applications with spiking neural networks.</description><author>Andrew C. Freeman, Ketan Mayer-Patel, Montek Singh</author><pubDate>Wed, 13 Dec 2023 15:30:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08213v1</guid></item><item><title>LAMM: Label Alignment for Multi-Modal Prompt Learning</title><link>http://arxiv.org/abs/2312.08212v1</link><description>With the success of pre-trained visual-language (VL) models such as CLIP invisual representation tasks, transferring pre-trained models to downstreamtasks has become a crucial paradigm. Recently, the prompt tuning paradigm,which draws inspiration from natural language processing (NLP), has madesignificant progress in VL field. However, preceding methods mainly focus onconstructing prompt templates for text and visual inputs, neglecting the gap inclass label representations between the VL models and downstream tasks. Toaddress this challenge, we introduce an innovative label alignment method named\textbf{LAMM}, which can dynamically adjust the category embeddings ofdownstream datasets through end-to-end training. Moreover, to achieve a moreappropriate label distribution, we propose a hierarchical loss, encompassingthe alignment of the parameter space, feature space, and logits space. Weconduct experiments on 11 downstream vision datasets and demonstrate that ourmethod significantly improves the performance of existing multi-modal promptlearning models in few-shot scenarios, exhibiting an average accuracyimprovement of 2.31(\%) compared to the state-of-the-art methods on 16 shots.Moreover, our methodology exhibits the preeminence in continual learningcompared to other prompt tuning methods. Importantly, our method is synergisticwith existing prompt tuning methods and can boost the performance on top ofthem. Our code and dataset will be publicly available athttps://github.com/gaojingsheng/LAMM.</description><author>Jingsheng Gao, Jiacheng Ruan, Suncheng Xiang, Zefang Yu, Ke Ji, Mingye Xie, Ting Liu, Yuzhuo Fu</author><pubDate>Wed, 13 Dec 2023 15:29:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08212v1</guid></item><item><title>Big Data -- Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques</title><link>http://arxiv.org/abs/2307.12971v2</link><description>This article intends to systematically identify and comparatively analyzestate-of-the-art supply chain (SC) forecasting strategies and technologies. Anovel framework has been proposed incorporating Big Data Analytics in SCManagement (problem identification, data sources, exploratory data analysis,machine-learning model training, hyperparameter tuning, performance evaluation,and optimization), forecasting effects on human-workforce, inventory, andoverall SC. Initially, the need to collect data according to SC strategy andhow to collect them has been discussed. The article discusses the need fordifferent types of forecasting according to the period or SC objective. The SCKPIs and the error-measurement systems have been recommended to optimize thetop-performing model. The adverse effects of phantom inventory on forecastingand the dependence of managerial decisions on the SC KPIs for determining modelperformance parameters and improving operations management, transparency, andplanning efficiency have been illustrated. The cyclic connection within theframework introduces preprocessing optimization based on the post-process KPIs,optimizing the overall control process (inventory management, workforcedetermination, cost, production and capacity planning). The contribution ofthis research lies in the standard SC process framework proposal, recommendedforecasting data analysis, forecasting effects on SC performance, machinelearning algorithms optimization followed, and in shedding light on futureresearch.</description><author>Md Abrar Jahin, Md Sakib Hossain Shovon, Jungpil Shin, Istiyaque Ahmed Ridoy, Yoichi Tomioka, M. F. Mridha</author><pubDate>Wed, 13 Dec 2023 15:10:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12971v2</guid></item></channel></rss>