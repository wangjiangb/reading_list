<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 15 Aug 2024 01:01:02 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Robust online reconstruction of continuous-time signals from a lean spike train ensemble code</title><link>http://arxiv.org/abs/2408.05950v2</link><description>Sensory stimuli in animals are encoded into spike trains by neurons, offeringadvantages such as sparsity, energy efficiency, and high temporal resolution.This paper presents a signal processing framework that deterministicallyencodes continuous-time signals into biologically feasible spike trains, andaddresses the questions about representable signal classes and reconstructionbounds. The framework considers encoding of a signal through spike trainsgenerated by an ensemble of neurons using a convolve-then-threshold mechanismwith various convolution kernels. A closed-form solution to the inverseproblem, from spike trains to signal reconstruction, is derived in the Hilbertspace of shifted kernel functions, ensuring sparse representation of ageneralized Finite Rate of Innovation (FRI) class of signals. Additionally,inspired by real-time processing in biological systems, an efficient iterativeversion of the optimal reconstruction is formulated that considers only afinite window of past spikes, ensuring robustness of the technique toill-conditioned encoding; convergence guarantees of the windowed reconstructionto the optimal solution are then provided. Experiments on a large audio datasetdemonstrate excellent reconstruction accuracy at spike rates as low asone-fifth of the Nyquist rate, while showing clear competitive advantage incomparison to state-of-the-art sparse coding techniques in the low spike rateregime.</description><author>Anik Chattopadhyay, Arunava Banerjee</author><pubDate>Wed, 14 Aug 2024 16:43:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05950v2</guid></item><item><title>RepoHyper: Search-Expand-Refine on Semantic Graphs for Repository-Level Code Completion</title><link>http://arxiv.org/abs/2403.06095v4</link><description>Code Large Language Models (CodeLLMs) have demonstrated impressiveproficiency in code completion tasks. However, they often fall short of fullyunderstanding the extensive context of a project repository, such as theintricacies of relevant files and class hierarchies, which can result in lessprecise completions. To overcome these limitations, we present \tool, amultifaceted framework designed to address the complex challenges associatedwith repository-level code completion. Central to RepoHYPER is the {\emRepo-level Semantic Graph} (RSG), a novel semantic graph structure thatencapsulates the vast context of code repositories. Furthermore, RepoHyperleverages Expand and Refine retrieval method, including a graph expansion and alink prediction algorithm applied to the RSG, enabling the effective retrievaland prioritization of relevant code snippets. Our evaluations show that \toolmarkedly outperforms existing techniques in repository-level code completion,showcasing enhanced accuracy across various datasets when compared to severalstrong baselines. Our implementation of RepoHYPER can be found athttps://github.com/FSoft-AI4Code/RepoHyper.</description><author>Huy N. Phan, Hoang N. Phan, Tien N. Nguyen, Nghi D. Q. Bui</author><pubDate>Wed, 14 Aug 2024 16:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06095v4</guid></item><item><title>An Event Structure-aware Generative Model for Biomedical Event Extraction</title><link>http://arxiv.org/abs/2408.06583v2</link><description>Biomedical Event Extraction (BEE) is a challenging task that involvesmodeling complex relationships between fine-grained entities in biomedicaltext. Most existing BEE models rely on classification methods that ignore labelsemantics and argument dependencies in the data. Although generative modelsthat use prompts are increasingly being used for event extraction, they facetwo main challenges: creating effective prompts for the biomedical domain anddealing with events with complex structures in the text. To address theselimitations, we propose GenBEE, a generative model enhanced withstructure-aware prefixes for biomedical event extraction. GenBEE constructsevent prompts that leverage knowledge distilled from large language models(LLMs), thereby incorporating both label semantics and argument dependencyrelationships. Additionally, GenBEE introduces a structural prefix learningmodule that generates structure-aware prefixes with structural prompts,enriching the generation process with structural features. Extensiveexperiments on three benchmark datasets demonstrate the effectiveness of GenBEEand it achieves state-of-the-art performance on the MLEE and GE11 datasets.Moreover, our analysis shows that the structural prefixes effectively bridgethe gap between structural prompts and the representation space of generativemodels, enabling better integration of event structural information.</description><author>Haohan Yuan, Siu Cheung Hui, Haopeng Zhang</author><pubDate>Wed, 14 Aug 2024 15:44:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06583v2</guid></item><item><title>Amuro &amp; Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models</title><link>http://arxiv.org/abs/2408.06663v2</link><description>The development of large language models leads to the formation of apre-train-then-align paradigm, in which the model is typically pre-trained on alarge text corpus and undergoes a tuning stage to align the model with humanpreference or downstream tasks. In this work, we investigate the relationshipbetween pre-training and fine-tuning by fine-tuning multiple intermediatepre-trained model checkpoints. Our results on 18 datasets suggest that i)continual pre-training improves the model in a latent way that unveils afterfine-tuning; ii) with extra fine-tuning, the datasets that the model does notdemonstrate capability gain much more than those that the model performs wellduring the pre-training stage; iii) although model benefits significantlythrough supervised fine-tuning, it may forget previously known domain knowledgeand the tasks that are not seen during fine-tuning; iv) the model resembleshigh sensitivity to evaluation prompts after supervised fine-tuning, but thissensitivity can be alleviated by more pre-training.</description><author>Kaiser Sun, Mark Dredze</author><pubDate>Wed, 14 Aug 2024 15:23:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06663v2</guid></item><item><title>DeepFace-Attention: Multimodal Face Biometrics for Attention Estimation with Application to e-Learning</title><link>http://arxiv.org/abs/2408.05523v2</link><description>This work introduces an innovative method for estimating attention levels(cognitive load) using an ensemble of facial analysis techniques applied towebcam videos. Our method is particularly useful, among others, in e-learningapplications, so we trained, evaluated, and compared our approach on the mEBAL2database, a public multi-modal database acquired in an e-learning environment.mEBAL2 comprises data from 60 users who performed 8 different tasks. Thesetasks varied in difficulty, leading to changes in their cognitive loads. Ourapproach adapts state-of-the-art facial analysis technologies to quantify theusers' cognitive load in the form of high or low attention. Several behavioralsignals and physiological processes related to the cognitive load are used,such as eyeblink, heart rate, facial action units, and head pose, among others.Furthermore, we conduct a study to understand which individual features obtainbetter results, the most efficient combinations, explore local and globalfeatures, and how temporary time intervals affect attention level estimation,among other aspects. We find that global facial features are more appropriatefor multimodal systems using score-level fusion, particularly as the temporalwindow increases. On the other hand, local features are more suitable forfusion through neural network training with score-level fusion approaches. Ourmethod outperforms existing state-of-the-art accuracies using the public mEBAL2benchmark.</description><author>Roberto Daza, Luis F. Gomez, Julian Fierrez, Aythami Morales, Ruben Tolosana, Javier Ortega-Garcia</author><pubDate>Wed, 14 Aug 2024 14:34:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05523v2</guid></item><item><title>MEEG and AT-DGNN: Improving EEG Emotion Recognition with Music Introducing and Graph-based Learning</title><link>http://arxiv.org/abs/2407.05550v3</link><description>We present the MEEG dataset, a multi-modal collection of music-inducedelectroencephalogram (EEG) recordings designed to capture emotional responsesto various musical stimuli across different valence and arousal levels. Thispublic dataset facilitates an in-depth examination of brainwave patterns withinmusical contexts, providing a robust foundation for studying brain networktopology during emotional processing. Leveraging the MEEG dataset, we introducethe Attention-based Temporal Learner with Dynamic Graph Neural Network(AT-DGNN), a novel framework for EEG-based emotion recognition. This modelcombines an attention mechanism with a dynamic graph neural network (DGNN) tocapture intricate EEG dynamics. The AT-DGNN achieves state-of-the-art (SOTA)performance with an accuracy of 83.74% in arousal recognition and 86.01% invalence recognition, outperforming existing SOTA methods. Comparative analysiswith traditional datasets, such as DEAP, further validates the model'seffectiveness and underscores the potency of music as an emotional stimulus.This study advances graph-based learning methodology in brain-computerinterfaces (BCI), significantly improving the accuracy of EEG-based emotionrecognition. The MEEG dataset and source code are publicly available athttps://github.com/xmh1011/AT-DGNN.</description><author>Minghao Xiao, Zhengxi Zhu, Bin Jiang, Meixia Qu, Wenyu Wang</author><pubDate>Wed, 14 Aug 2024 14:06:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05550v3</guid></item><item><title>Tree Attention: Topology-aware Decoding for Long-Context Attention on GPU clusters</title><link>http://arxiv.org/abs/2408.04093v3</link><description>Self-attention is the core mathematical operation of modern transformerarchitectures and is also a significant computational bottleneck due to itsquadratic complexity in the sequence length. In this work, we derive the scalarenergy function whose gradient computes the self-attention block, thuselucidating the theoretical underpinnings of self-attention, providing aBayesian interpretation of the operation and linking it closely withenergy-based models such as Hopfield Networks. Our formulation reveals that thereduction across the sequence axis can be efficiently computed in parallelthrough a tree reduction. Our algorithm, for parallelizing attentioncomputation across multiple GPUs enables cross-device decoding to be performedasymptotically faster (up to 8x faster in our experiments) than alternativeapproaches such as Ring Attention, while also requiring significantly lesscommunication volume and incurring 2x less peak memory. Our code is publiclyavailable here: \url{https://github.com/Zyphra/tree_attention}.</description><author>Vasudev Shyam, Jonathan Pilault, Emily Shepperd, Quentin Anthony, Beren Millidge</author><pubDate>Wed, 14 Aug 2024 12:47:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04093v3</guid></item><item><title>Bayesian Learning in a Nonlinear Multiscale State-Space Model</title><link>http://arxiv.org/abs/2408.06425v2</link><description>The ubiquity of multiscale interactions in complex systems iswell-recognized, with development and heredity serving as a prime example ofhow processes at different temporal scales influence one another. This workintroduces a novel multiscale state-space model to explore the dynamicinterplay between systems interacting across different time scales, withfeedback between each scale. We propose a Bayesian learning framework toestimate unknown states by learning the unknown process noise covarianceswithin this multiscale model. We develop a Particle Gibbs with AncestorSampling (PGAS) algorithm for inference and demonstrate through simulations theefficacy of our approach.</description><author>Nayely Vélez-Cruz, Manfred D. Laubichler</author><pubDate>Wed, 14 Aug 2024 12:22:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06425v2</guid></item><item><title>Measuring User Understanding in Dialogue-based XAI Systems</title><link>http://arxiv.org/abs/2408.06960v2</link><description>The field of eXplainable Artificial Intelligence (XAI) is increasinglyrecognizing the need to personalize and/or interactively adapt the explanationto better reflect users' explanation needs. While dialogue-based approaches toXAI have been proposed recently, the state-of-the-art in XAI is stillcharacterized by what we call one-shot, non-personalized and one-wayexplanations. In contrast, dialogue-based systems that can adapt explanationsthrough interaction with a user promise to be superior to GUI-based ordashboard explanations as they offer a more intuitive way of requestinginformation. In general, while interactive XAI systems are often evaluated interms of user satisfaction, there are limited studies that access user'sobjective model understanding. This is in particular the case fordialogue-based XAI approaches. In this paper, we close this gap by carrying outcontrolled experiments within a dialogue framework in which we measureunderstanding of users in three phases by asking them to simulate thepredictions of the model they are learning about. By this, we can quantify thelevel of (improved) understanding w.r.t. how the model works, comparing thestate prior, and after the interaction. We further analyze the data to revealpatterns of how the interaction between groups with high vs. low understandinggain differ. Overall, our work thus contributes to our understanding about theeffectiveness of XAI approaches.</description><author>Dimitry Mindlin, Amelie Sophie Robrecht, Michael Morasch, Philipp Cimiano</author><pubDate>Wed, 14 Aug 2024 12:11:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06960v2</guid></item><item><title>A Comprehensive Survey on Synthetic Infrared Image synthesis</title><link>http://arxiv.org/abs/2408.06868v2</link><description>Synthetic infrared (IR) scene and target generation is an important computervision problem as it allows the generation of realistic IR images and targetsfor training and testing of various applications, such as remote sensing,surveillance, and target recognition. It also helps reduce the cost and riskassociated with collecting real-world IR data. This survey paper aims toprovide a comprehensive overview of the conventional mathematicalmodelling-based methods and deep learning-based methods used for generatingsynthetic IR scenes and targets. The paper discusses the importance ofsynthetic IR scene and target generation and briefly covers the mathematics ofblackbody and grey body radiations, as well as IR image-capturing methods. Thepotential use cases of synthetic IR scenes and target generation are alsodescribed, highlighting the significance of these techniques in various fields.Additionally, the paper explores possible new ways of developing new techniquesto enhance the efficiency and effectiveness of synthetic IR scenes and targetgeneration while highlighting the need for further research to advance thisfield.</description><author>Avinash Upadhyay, Manoj sharma, Prerana Mukherjee, Amit Singhal, Brejesh Lall</author><pubDate>Wed, 14 Aug 2024 11:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06868v2</guid></item><item><title>Advancing Interactive Explainable AI via Belief Change Theory</title><link>http://arxiv.org/abs/2408.06875v2</link><description>As AI models become ever more complex and intertwined in humans' daily lives,greater levels of interactivity of explainable AI (XAI) methods are needed. Inthis paper, we propose the use of belief change theory as a formal foundationfor operators that model the incorporation of new information, i.e. userfeedback in interactive XAI, to logical representations of data-drivenclassifiers. We argue that this type of formalisation provides a framework anda methodology to develop interactive explanations in a principled manner,providing warranted behaviour and favouring transparency and accountability ofsuch interactions. Concretely, we first define a novel, logic-based formalismto represent explanatory information shared between humans and machines. Wethen consider real world scenarios for interactive XAI, with differentprioritisations of new and existing knowledge, where our formalism may beinstantiated. Finally, we analyse a core set of belief change postulates,discussing their suitability for our real world settings and pointing toparticular challenges that may require the relaxation or reinterpretation ofsome of the theoretical assumptions underlying existing operators.</description><author>Antonio Rago, Maria Vanina Martinez</author><pubDate>Wed, 14 Aug 2024 11:23:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06875v2</guid></item><item><title>Iterative Improvement of an Additively Regularized Topic Model</title><link>http://arxiv.org/abs/2408.05840v2</link><description>Topic modelling is fundamentally a soft clustering problem (of known objects-- documents, over unknown clusters -- topics). That is, the task isincorrectly posed. In particular, the topic models are unstable and incomplete.All this leads to the fact that the process of finding a good topic model(repeated hyperparameter selection, model training, and topic qualityassessment) can be particularly long and labor-intensive. We aim to simplifythe process, to make it more deterministic and provable. To this end, wepresent a method for iterative training of a topic model. The essence of themethod is that a series of related topic models are trained so that eachsubsequent model is at least as good as the previous one, i.e., that it retainsall the good topics found earlier. The connection between the models isachieved by additive regularization. The result of this iterative training isthe last topic model in the series, which we call the iteratively updatedadditively regularized topic model (ITAR). Experiments conducted on severalcollections of natural language texts show that the proposed ITAR modelperforms better than other popular topic models (LDA, ARTM, BERTopic), itstopics are diverse, and its perplexity (ability to "explain" the underlyingdata) is moderate.</description><author>Alex Gorbulev, Vasiliy Alekseev, Konstantin Vorontsov</author><pubDate>Wed, 14 Aug 2024 11:07:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05840v2</guid></item><item><title>Detecting Audio-Visual Deepfakes with Fine-Grained Inconsistencies</title><link>http://arxiv.org/abs/2408.06753v2</link><description>Existing methods on audio-visual deepfake detection mainly focus onhigh-level features for modeling inconsistencies between audio and visual data.As a result, these approaches usually overlook finer audio-visual artifacts,which are inherent to deepfakes. Herein, we propose the introduction offine-grained mechanisms for detecting subtle artifacts in both spatial andtemporal domains. First, we introduce a local audio-visual model capable ofcapturing small spatial regions that are prone to inconsistencies with audio.For that purpose, a fine-grained mechanism based on a spatially-local distancecoupled with an attention module is adopted. Second, we introduce atemporally-local pseudo-fake augmentation to include samples incorporatingsubtle temporal inconsistencies in our training set. Experiments on the DFDCand the FakeAVCeleb datasets demonstrate the superiority of the proposed methodin terms of generalization as compared to the state-of-the-art under bothin-dataset and cross-dataset settings.</description><author>Marcella Astrid, Enjie Ghorbel, Djamila Aouada</author><pubDate>Wed, 14 Aug 2024 10:53:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06753v2</guid></item><item><title>MetMamba: Regional Weather Forecasting with Spatial-Temporal Mamba Model</title><link>http://arxiv.org/abs/2408.06400v2</link><description>Deep Learning based Weather Prediction (DLWP) models have been improvingrapidly over the last few years, surpassing state of the art numerical weatherforecasts by significant margins. While much of the optimization effort isfocused on training curriculum to extend forecast range in the global context,two aspects remains less explored: limited area modeling and better backbonesfor weather forecasting. We show in this paper that MetMamba, a DLWP modelbuilt on a state-of-the-art state-space model, Mamba, offers notableperformance gains and unique advantages over other popular backbones usingtraditional attention mechanisms and neural operators. We also demonstrate thefeasibility of deep learning based limited area modeling via coupled trainingwith a global host model.</description><author>Haoyu Qin, Yungang Chen, Qianchuan Jiang, Pengchao Sun, Xiancai Ye, Chao Lin</author><pubDate>Wed, 14 Aug 2024 09:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06400v2</guid></item><item><title>On a Scale-Invariant Approach to Bundle Recommendations in Candy Crush Saga</title><link>http://arxiv.org/abs/2408.06799v2</link><description>A good understanding of player preferences is crucial for increasing contentrelevancy, especially in mobile games. This paper illustrates the use ofattentive models for producing item recommendations in a mobile game scenario.The methodology comprises a combination of supervised and unsupervisedapproaches to create user-level recommendations while introducing a novelscale-invariant approach to the prediction. The methodology is subsequentlyapplied to a bundle recommendation in Candy Crush Saga. The strategy ofdeployment, maintenance, and monitoring of ML models that are scaled up toserve millions of users is presented, along with the best practices and designpatterns adopted to minimize technical debt typical of ML systems. Therecommendation approach is evaluated both offline and online, with a focus onunderstanding the increase in engagement, click- and take rates, noveltyeffects, recommendation diversity, and the impact of degenerate feedback loops.We have demonstrated that the recommendation enhances user engagement by 30%concerning click rate and by more than 40% concerning take rate. In addition,we empirically quantify the diminishing effects of recommendation accuracy onuser engagement.</description><author>Styliani Katsarou, Francesca Carminati, Martin Dlask, Marta Braojos, Lavena Patra, Richard Perkins, Carlos Garcia Ling, Maria Paskevich</author><pubDate>Wed, 14 Aug 2024 05:44:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06799v2</guid></item><item><title>Multi-layer Sequence Labeling-based Joint Biomedical Event Extraction</title><link>http://arxiv.org/abs/2408.05545v2</link><description>In recent years, biomedical event extraction has been dominated bycomplicated pipeline and joint methods, which need to be simplified. Inaddition, existing work has not effectively utilized trigger word informationexplicitly. Hence, we propose MLSL, a method based on multi-layer sequencelabeling for joint biomedical event extraction. MLSL does not introduce priorknowledge and complex structures. Moreover, it explicitly incorporates theinformation of candidate trigger words into the sequence labeling to learn theinteraction relationships between trigger words and argument roles. Based onthis, MLSL can learn well with just a simple workflow. Extensiveexperimentation demonstrates the superiority of MLSL in terms of extractionperformance compared to other state-of-the-art methods.</description><author>Gongchi Chen, Pengchao Wu, Jinghang Gu, Longhua Qian, Guodong Zhou</author><pubDate>Wed, 14 Aug 2024 05:43:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05545v2</guid></item><item><title>Automatic Feature Recognition and Dimensional Attributes Extraction From CAD Models for Hybrid Additive-Subtractive Manufacturing</title><link>http://arxiv.org/abs/2408.06891v2</link><description>The integration of Computer-Aided Design (CAD), Computer-Aided ProcessPlanning (CAPP), and Computer-Aided Manufacturing (CAM) plays a crucial role inmodern manufacturing, facilitating seamless transitions from digital designs tophysical products. However, a significant challenge within this integration isthe Automatic Feature Recognition (AFR) of CAD models, especially in thecontext of hybrid manufacturing that combines subtractive and additivemanufacturing processes. Traditional AFR methods, focused mainly on theidentification of subtractive (machined) features including holes, fillets,chamfers, pockets, and slots, fail to recognize features pertinent to additivemanufacturing. Furthermore, the traditional methods fall short in accuratelyextracting geometric dimensions and orientations, which are also key factorsfor effective manufacturing process planning. This paper presents a novelapproach for creating a synthetic CAD dataset that encompasses featuresrelevant to both additive and subtractive machining through Python OpenCascade. The Hierarchical Graph Convolutional Neural Network (HGCNN) model isimplemented to accurately identify the composite additive-subtractive featureswithin the synthetic CAD dataset. The key novelty and contribution of theproposed methodology lie in its ability to recognize a wide range ofmanufacturing features, and precisely extracting their dimensions,orientations, and stock sizes. The proposed model demonstrates remarkablefeature recognition accuracy exceeding 97% and a dimension extraction accuracyof 100% for identified features. Therefore, the proposed methodology enhancesthe integration of CAD, CAPP, and CAM within hybrid manufacturing by providingprecise feature recognition and dimension extraction. It facilitates improvedmanufacturing process planning, by enabling more informed decision-making.</description><author>Muhammad Tayyab Khan, Wenhe Feng, Lequn Chen, Ye Han Ng, Nicholas Yew Jin Tan, Seung Ki Moon</author><pubDate>Wed, 14 Aug 2024 05:16:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06891v2</guid></item><item><title>\textit{re}CSE: Portable Reshaping Features for Sentence Embedding in Self-supervised Contrastive Learning</title><link>http://arxiv.org/abs/2408.04975v3</link><description>We propose \textit{re}CSE, a self supervised contrastive learning sentencerepresentation framework based on feature reshaping. This framework isdifferent from the current advanced models that use discrete data augmentationmethods, but instead reshapes the input features of the original sentence,aggregates the global information of each token in the sentence, and alleviatesthe common problems of representation polarity and GPU memory consumptionlinear increase in current advanced models. In addition, our \textit{re}CSE hasachieved competitive performance in semantic similarity tasks. And theexperiment proves that our proposed feature reshaping method has stronguniversality, which can be transplanted to other self supervised contrastivelearning frameworks and enhance their representation ability, even achievingstate-of-the-art performance. Our code is available athttps://github.com/heavenhellchen/reCSE.</description><author>Fufangchen Zhao, Jian Gao, Danfeng Yan</author><pubDate>Wed, 14 Aug 2024 05:09:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04975v3</guid></item><item><title>Graph Agent Network: Empowering Nodes with Inference Capabilities for Adversarial Resilience</title><link>http://arxiv.org/abs/2306.06909v3</link><description>End-to-end training with global optimization have popularized graph neuralnetworks (GNNs) for node classification, yet inadvertently introducedvulnerabilities to adversarial edge-perturbing attacks. Adversaries can exploitthe inherent opened interfaces of GNNs' input and output, perturbing criticaledges and thus manipulating the classification results. Current defenses, dueto their persistent utilization of global-optimization-based end-to-endtraining schemes, inherently encapsulate the vulnerabilities of GNNs. This isspecifically evidenced in their inability to defend against targeted secondaryattacks. In this paper, we propose the Graph Agent Network (GAgN) to addressthe aforementioned vulnerabilities of GNNs. GAgN is a graph-structured agentnetwork in which each node is designed as an 1-hop-view agent. Through thedecentralized interactions between agents, they can learn to infer globalperceptions to perform tasks including inferring embeddings, degrees andneighbor relationships for given nodes. This empowers nodes to filteringadversarial edges while carrying out classification tasks. Furthermore, agents'limited view prevents malicious messages from propagating globally in GAgN,thereby resisting global-optimization-based secondary attacks. We prove thatsingle-hidden-layer multilayer perceptrons (MLPs) are theoretically sufficientto achieve these functionalities. Experimental results show that GAgNeffectively implements all its intended capabilities and, compared tostate-of-the-art defenses, achieves optimal classification accuracy on theperturbed datasets.</description><author>Ao Liu, Wenshan Li, Tao Li, Beibei Li, Guangquan Xu, Pan Zhou, Wengang Ma, Hanyuan Huang</author><pubDate>Wed, 14 Aug 2024 04:26:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06909v3</guid></item><item><title>Unleashing Artificial Cognition: Integrating Multiple AI Systems</title><link>http://arxiv.org/abs/2408.04910v3</link><description>In this study, we present an innovative fusion of language models and queryanalysis techniques to unlock cognition in artificial intelligence. Our systemseamlessly integrates a Chess engine with a language model, enabling it topredict moves and provide strategic explanations. Leveraging a vector databaseto achieve retrievable answer generation, our OpenSI AI system elucidates itsdecision-making process, bridging the gap between raw computation andhuman-like understanding. Our choice of Chess as the demonstration environmentunderscores the versatility of our approach. Beyond Chess, our system holdspromise for diverse applications, from medical diagnostics to financialforecasting.</description><author>Muntasir Adnan, Buddhi Gamage, Zhiwei Xu, Damith Herath, Carlos C. N. Kuhn</author><pubDate>Wed, 14 Aug 2024 02:28:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04910v3</guid></item><item><title>Camera Perspective Transformation to Bird's Eye View via Spatial Transformer Model for Road Intersection Monitoring</title><link>http://arxiv.org/abs/2408.05577v2</link><description>Road intersection monitoring and control research often utilize bird's eyeview (BEV) simulators. In real traffic settings, achieving a BEV akin to thatin a simulator necessitates the deployment of drones or specific sensormounting, which is neither feasible nor practical. Consequently, trafficintersection management remains confined to simulation environments given theseconstraints. In this paper, we address the gap between simulated environmentsand real-world implementation by introducing a novel deep-learning model thatconverts a single camera's perspective of a road intersection into a BEV. Wecreated a simulation environment that closely resembles a real-world trafficjunction. The proposed model transforms the vehicles into BEV images,facilitating road intersection monitoring and control model processing.Inspired by image transformation techniques, we propose a Spatial-TransformerDouble Decoder-UNet (SDD-UNet) model that aims to eliminate the transformedimage distortions. In addition, the model accurately estimates the vehicle'spositions and enables the direct application of simulation-trained models inreal-world contexts. SDD-UNet model achieves an average dice similaritycoefficient (DSC) above 95% which is 40% better than the original UNet model.The mean absolute error (MAE) is 0.102 and the centroid of the predicted maskis 0.14 meters displaced, on average, indicating high accuracy.</description><author>Rukesh Prajapati, Amr S. El-Wakeel</author><pubDate>Wed, 14 Aug 2024 02:20:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05577v2</guid></item><item><title>Decentralized Health Intelligence Network (DHIN)</title><link>http://arxiv.org/abs/2408.06240v3</link><description>Decentralized Health Intelligence Network (DHIN) is a theoretical frameworkaddressing significant challenges of health data sovereignty and AI utilizationin healthcare caused by data fragmentation across providers and institutions.It establishes a sovereign architecture for healthcare provision as aprerequisite to a sovereign health network, then facilitates effective AIutilization by overcoming barriers to accessing diverse medical data sources.This comprehensive framework leverages: 1) self-sovereign identity architecturecoupled with a personal health record (PHR) as a prerequisite for health datasovereignty; 2) a scalable federated learning (FL) protocol implemented on apublic blockchain for decentralized AI training in healthcare, where healthdata remains with participants and only model parameter updates are shared; and3) a scalable, trustless rewards mechanism to incentivize participation andensure fair reward distribution. This framework ensures that no entity canprevent or control access to training on health data offered by participants ordetermine financial benefits, as these processes operate on a public blockchainwith an immutable record and without a third party. It supports effective AItraining in healthcare, allowing patients to maintain control over their healthdata, benefit financially, and contribute to a decentralized, scalableecosystem that leverages collective AI to develop beneficial healthcarealgorithms. Patients receive rewards into their digital wallets as an incentiveto opt-in to the FL protocol, with a long-term roadmap to funding decentralizedinsurance solutions. This approach introduces a novel, self-financed healthcaremodel that adapts to individual needs, complements existing systems, andredefines universal coverage. It highlights the potential to transformhealthcare data management and AI utilization while empowering patients.</description><author>Abraham Nash</author><pubDate>Wed, 14 Aug 2024 01:47:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06240v3</guid></item><item><title>OpenEP: Open-Ended Future Event Prediction</title><link>http://arxiv.org/abs/2408.06578v2</link><description>Future event prediction (FEP) is a long-standing and crucial task in theworld, as understanding the evolution of events enables early riskidentification, informed decision-making, and strategic planning. Existing worktypically treats event prediction as classification tasks and confines theoutcomes of future events to a fixed scope, such as yes/no questions, candidateset, and taxonomy, which is difficult to include all possible outcomes offuture events. In this paper, we introduce OpenEP (an Open-Ended Future EventPrediction task), which generates flexible and diverse predictions aligned withreal-world scenarios. This is mainly reflected in two aspects: firstly, thepredictive questions are diverse, covering different stages of eventdevelopment and perspectives; secondly, the outcomes are flexible, withoutconstraints on scope or format. To facilitate the study of this task, weconstruct OpenEPBench, an open-ended future event prediction dataset. Forquestion construction, we pose questions from seven perspectives, includinglocation, time, event development, event outcome, event impact, event response,and other, to facilitate an in-depth analysis and understanding of thecomprehensive evolution of events. For outcome construction, we collectfree-form text containing the outcomes as ground truth to provide semanticallycomplete and detail-enriched outcomes. Furthermore, we propose StkFEP, astakeholder-enhanced future event prediction framework, that incorporates eventcharacteristics for open-ended settings. Our method extracts stakeholdersinvolved in events to extend questions to gather diverse information. We alsocollect historically events that are relevant and similar to the question toreveal potential evolutionary patterns. Experiment results indicate thataccurately predicting future events in open-ended settings is challenging forexisting LLMs.</description><author>Yong Guan, Hao Peng, Xiaozhi Wang, Lei Hou, Juanzi Li</author><pubDate>Wed, 14 Aug 2024 01:37:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06578v2</guid></item><item><title>Dynamic and Compressive Adaptation of Transformers From Images to Videos</title><link>http://arxiv.org/abs/2408.06840v2</link><description>Recently, the remarkable success of pre-trained Vision Transformers (ViTs)from image-text matching has sparked an interest in image-to-video adaptation.However, most current approaches retain the full forward pass for each frame,leading to a high computation overhead for processing entire videos. In thispaper, we present InTI, a novel approach for compressive image-to-videoadaptation using dynamic Inter-frame Token Interpolation. InTI aims to softlypreserve the informative tokens without disrupting their coherentspatiotemporal structure. Specifically, each token pair at identical positionswithin neighbor frames is linearly aggregated into a new token, where theaggregation weights are generated by a multi-scale context-aware network. Inthis way, the information of neighbor frames can be adaptively compressed in apoint-by-point manner, thereby effectively reducing the number of processedframes by half each time. Importantly, InTI can be seamlessly integrated withexisting adaptation methods, achieving strong performance without extra-complexdesign. On Kinetics-400, InTI reaches a top-1 accuracy of 87.1 with aremarkable 37.5% reduction in GFLOPs compared to naive adaptation. Whencombined with additional temporal modules, InTI achieves a top-1 accuracy of87.6 with a 37% reduction in GFLOPs. Similar conclusions have been verified inother common datasets.</description><author>Guozhen Zhang, Jingyu Liu, Shengming Cao, Xiaotong Zhao, Kevin Zhao, Kai Ma, Limin Wang</author><pubDate>Wed, 14 Aug 2024 01:37:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06840v2</guid></item><item><title>AI-Powered Energy Algorithmic Trading: Integrating Hidden Markov Models with Neural Networks</title><link>http://arxiv.org/abs/2407.19858v4</link><description>In quantitative finance, machine learning methods are essential for alphageneration. This study introduces a new approach that combines Hidden MarkovModels (HMM) and neural networks, integrated with Black-Litterman portfoliooptimization. During the COVID period (2019-2022), this dual-model approachachieved a 83% return with a Sharpe ratio of 0.77. It incorporates two riskmodels to enhance risk management, showing efficiency during volatile periods.The methodology was implemented on the QuantConnect platform, which was chosenfor its robust framework and experimental reproducibility. The system, whichpredicts future price movements, includes a three-year warm-up to ensure properalgorithm function. It targets highly liquid, large-cap energy stocks to ensurestable and predictable performance while also considering broker payments. Thedual-model alpha system utilizes log returns to select the optimal state basedon the historical performance. It combines state predictions with neuralnetwork outputs, which are based on historical data, to generate tradingsignals. This study examined the architecture of the trading system, datapre-processing, training, and performance. The full code and backtesting dataare available under the QuantConnect terms.</description><author>Tiago Monteiro</author><pubDate>Tue, 13 Aug 2024 21:52:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19858v4</guid></item><item><title>DataNarrative: Automated Data-Driven Storytelling with Visualizations and Texts</title><link>http://arxiv.org/abs/2408.05346v2</link><description>Data-driven storytelling is a powerful method for conveying insights bycombining narrative techniques with visualizations and text. These storiesintegrate visual aids, such as highlighted bars and lines in charts, along withtextual annotations explaining insights. However, creating such storiesrequires a deep understanding of the data and meticulous narrative planning,often necessitating human intervention, which can be time-consuming andmentally taxing. While Large Language Models (LLMs) excel in various NLP tasks,their ability to generate coherent and comprehensive data stories remainsunderexplored. In this work, we introduce a novel task for data storygeneration and a benchmark containing 1,449 stories from diverse sources. Toaddress the challenges of crafting coherent data stories, we propose amultiagent framework employing two LLM agents designed to replicate the humanstorytelling process: one for understanding and describing the data(Reflection), generating the outline, and narration, and another forverification at each intermediary step. While our agentic framework generallyoutperforms non-agentic counterparts in both model-based and human evaluations,the results also reveal unique challenges in data story generation.</description><author>Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty</author><pubDate>Tue, 13 Aug 2024 20:46:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05346v2</guid></item><item><title>Layer-Specific Optimization: Sensitivity Based Convolution Layers Basis Search</title><link>http://arxiv.org/abs/2408.06024v2</link><description>Deep neural network models have a complex architecture and areoverparameterized. The number of parameters is more than the whole dataset,which is highly resource-consuming. This complicates their application andlimits its usage on different devices. Reduction in the number of networkparameters helps to reduce the size of the model, but at the same time,thoughtlessly applied, can lead to a deterioration in the quality of thenetwork. One way to reduce the number of model parameters is matrixdecomposition, where a matrix is represented as a product of smaller matrices.In this paper, we propose a new way of applying the matrix decomposition withrespect to the weights of convolutional layers. The essence of the method is totrain not all convolutions, but only the subset of convolutions (basisconvolutions), and represent the rest as linear combinations of the basis ones.Experiments on models from the ResNet family and the CIFAR-10 datasetdemonstrate that basis convolutions can not only reduce the size of the modelbut also accelerate the forward and backward passes of the network. Anothercontribution of this work is that we propose a fast method for selecting asubset of network layers in which the use of matrix decomposition does notdegrade the quality of the final model.</description><author>Vasiliy Alekseev, Ilya Lukashevich, Ilia Zharikov, Ilya Vasiliev</author><pubDate>Tue, 13 Aug 2024 20:01:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06024v2</guid></item><item><title>Explainable AI Reloaded: Challenging the XAI Status Quo in the Era of Large Language Models</title><link>http://arxiv.org/abs/2408.05345v2</link><description>When the initial vision of Explainable (XAI) was articulated, the mostpopular framing was to open the (proverbial) "black-box" of AI so that we couldunderstand the inner workings. With the advent of Large Language Models (LLMs),the very ability to open the black-box is increasingly limited especially whenit comes to non-AI expert end-users. In this paper, we challenge the assumptionof "opening" the black-box in the LLM era and argue for a shift in our XAIexpectations. Highlighting the epistemic blind spots of an algorithm-centeredXAI view, we argue that a human-centered perspective can be a path forward. Weoperationalize the argument by synthesizing XAI research along threedimensions: explainability outside the black-box, explainability around theedges of the black box, and explainability that leverages infrastructuralseams. We conclude with takeaways that reflexively inform XAI as a domain.</description><author>Upol Ehsan, Mark O. Riedl</author><pubDate>Tue, 13 Aug 2024 19:39:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05345v2</guid></item><item><title>Approaches for enhancing extrapolability in process-based and data-driven models in hydrology</title><link>http://arxiv.org/abs/2408.07071v1</link><description>The application of process-based and data-driven hydrological models iscrucial in modern hydrological research, especially for predicting key watercycle variables such as runoff, evapotranspiration (ET), and soil moisture.These models provide a scientific basis for water resource management, floodforecasting, and ecological protection. Process-based models simulate thephysical mechanisms of watershed hydrological processes, while data-drivenmodels leverage large datasets and advanced machine learning algorithms. Thispaper reviewed and compared methods for assessing and enhancing theextrapolability of both model types, discussing their prospects andlimitations. Key strategies include the use of leave-one-out cross-validationand similarity-based methods to evaluate model performance in ungauged regions.Deep learning, transfer learning, and domain adaptation techniques are alsopromising in their potential to improve model predictions in data-sparse andextreme conditions. Interdisciplinary collaboration and continuous algorithmicadvancements are also important to strengthen the global applicability andreliability of hydrological models.</description><author>Haiyang Shi</author><pubDate>Tue, 13 Aug 2024 17:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07071v1</guid></item><item><title>Fingerspelling within Sign Language Translation</title><link>http://arxiv.org/abs/2408.07065v1</link><description>Fingerspelling poses challenges for sign language processing due to itshigh-frequency motion and use for open-vocabulary terms. While prior work hasstudied fingerspelling recognition, there has been little attention toevaluating how well sign language translation models understand fingerspellingin the context of entire sentences -- and improving this capability. Wemanually annotate instances of fingerspelling within FLEURS-ASL and use them toevaluate the effect of two simple measures to improve fingerspellingrecognition within American Sign Language to English translation: 1) use amodel family (ByT5) with character- rather than subword-level tokenization, and2) mix fingerspelling recognition data into the translation training mixture.We find that 1) substantially improves understanding of fingerspelling (andtherefore translation quality overall), but the effect of 2) is mixed.</description><author>Garrett Tanzer</author><pubDate>Tue, 13 Aug 2024 17:57:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07065v1</guid></item><item><title>TraceFL: Achieving Interpretability in Federated Learning via Neuron Provenance</title><link>http://arxiv.org/abs/2312.13632v2</link><description>In Federated Learning, clients train models on local data and send updates toa central server, which aggregates them into a global model using a fusionalgorithm. This collaborative yet privacy-preserving training comes at acost--FL developers face significant challenges in attributing global modelpredictions to specific clients. Localizing responsible clients is a crucialstep towards (a) excluding clients primarily responsible for incorrectpredictions and (b) encouraging clients who contributed high-quality models tocontinue participating in the future. Existing ML explainability approaches areinherently inapplicable as they are designed for single-model, centralizedtraining. We introduce TraceFL, a fine-grained neuron provenance capturing mechanismthat identifies clients responsible for the global model's prediction bytracking the flow of information from individual clients to the global model.Since inference on different inputs activates a different set of neurons of theglobal model, TraceFL dynamically quantifies the significance of the globalmodel's neurons in a given prediction. It then selectively picks a slice of themost crucial neurons in the global model and maps them to the correspondingneurons in every participating client to determine each client's contribution,ultimately localizing the responsible client. We evaluate TraceFL on sixdatasets, including two real-world medical imaging datasets and four neuralnetworks, including advanced models such as GPT. TraceFL achieves 99% accuracyin localizing the responsible client in FL tasks spanning both image and textclassification tasks. At a time when state-of-the-art ML debugging approachesare mostly domain-specific (e.g., image classification only), TraceFL is thefirst technique to enable highly accurate automated reasoning across a widerange of FL applications.</description><author>Waris Gill, Ali Anwar, Muhammad Ali Gulzar</author><pubDate>Tue, 13 Aug 2024 17:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13632v2</guid></item><item><title>Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents</title><link>http://arxiv.org/abs/2408.07060v1</link><description>Large language model (LLM) agents have shown great potential in solvingreal-world software engineering (SWE) problems. The most advanced open-sourceSWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.However, these sophisticated agent frameworks exhibit varying strengths,excelling in certain tasks while underperforming in others. To fully harnessthe diversity of these agents, we propose DEI (Diversity EmpoweredIntelligence), a framework that leverages their unique expertise. DEI functionsas a meta-module atop existing SWE agent frameworks, managing agent collectivesfor enhanced problem-solving. Experimental results show that a DEI-guidedcommittee of agents is able to surpass the best individual agent's performanceby a large margin. For instance, a group of open-source SWE agents, with amaximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%resolve rate with DEI, making a 25% improvement and beating most closed-sourcesolutions. Our best-performing group excels with a 55% resolve rate, securingthe highest ranking on SWE-Bench Lite. Our findings contribute to the growingbody of research on collaborative AI systems and their potential to solvecomplex software engineering challenges.</description><author>Kexun Zhang, Weiran Yao, Zuxin Liu, Yihao Feng, Zhiwei Liu, Rithesh Murthy, Tian Lan, Lei Li, Renze Lou, Jiacheng Xu, Bo Pang, Yingbo Zhou, Shelby Heinecke, Silvio Savarese, Huan Wang, Caiming Xiong</author><pubDate>Tue, 13 Aug 2024 17:50:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07060v1</guid></item><item><title>Model Counting in the Wild</title><link>http://arxiv.org/abs/2408.07059v1</link><description>Model counting is a fundamental problem in automated reasoning withapplications in probabilistic inference, network reliability, neural networkverification, and more. Although model counting is computationally intractablefrom a theoretical perspective due to its #P-completeness, the past decade hasseen significant progress in developing state-of-the-art model counters toaddress scalability challenges. In this work, we conduct a rigorous assessment of the scalability of modelcounters in the wild. To this end, we surveyed 11 application domains andcollected an aggregate of 2262 benchmarks from these domains. We then evaluatedsix state-of-the-art model counters on these instances to assess scalabilityand runtime performance. Our empirical evaluation demonstrates that the performance of model countersvaries significantly across different application domains, underscoring theneed for careful selection by the end user. Additionally, we investigated thebehavior of different counters with respect to two parameters suggested by themodel counting community, finding only a weak correlation. Our analysishighlights the challenges and opportunities for portfolio-based approaches inmodel counting.</description><author>Arijit Shaw, Kuldeep S. Meel</author><pubDate>Tue, 13 Aug 2024 17:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07059v1</guid></item><item><title>A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning</title><link>http://arxiv.org/abs/2408.07057v1</link><description>The availability of performant pre-trained models has led to a proliferationof fine-tuned expert models that are specialized to a particular domain ortask. Model MoErging methods aim to recycle expert models to create anaggregate system with improved performance or generalization. A key componentof MoErging methods is the creation of a router that decides which expertmodel(s) to use for a particular input or application. The promise,effectiveness, and large design space of MoErging has spurred the developmentof many new methods over the past few years. This rapid pace of development hasmade it challenging to compare different MoErging methods, which are rarelycompared to one another and are often validated in different experimentalsetups. To remedy such gaps, we present a comprehensive survey of MoErgingmethods that includes a novel taxonomy for cataloging key design choices andclarifying suitable applications for each method. Apart from surveying MoErgingresearch, we inventory software tools and applications that make use ofMoErging. We additionally discuss related fields of study such as modelmerging, multitask learning, and mixture-of-experts models. Taken as a whole,our survey provides a unified overview of existing MoErging methods and createsa solid foundation for future work in this burgeoning field.</description><author>Prateek Yadav, Colin Raffel, Mohammed Muqeeth, Lucas Caccia, Haokun Liu, Tianlong Chen, Mohit Bansal, Leshem Choshen, Alessandro Sordoni</author><pubDate>Tue, 13 Aug 2024 17:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07057v1</guid></item><item><title>LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs</title><link>http://arxiv.org/abs/2408.07055v1</link><description>Current long context large language models (LLMs) can process inputs up to100,000 tokens, yet struggle to generate outputs exceeding even a modest lengthof 2,000 words. Through controlled experiments, we find that the model'seffective generation length is inherently bounded by the sample it has seenduring supervised fine-tuning (SFT). In other words, their output limitation isdue to the scarcity of long-output examples in existing SFT datasets. Toaddress this, we introduce AgentWrite, an agent-based pipeline that decomposesultra-long generation tasks into subtasks, enabling off-the-shelf LLMs togenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, weconstruct LongWriter-6k, a dataset containing 6,000 SFT data with outputlengths ranging from 2k to 32k words. By incorporating this dataset into modeltraining, we successfully scale the output length of existing models to over10,000 words while maintaining output quality. We also develop LongBench-Write,a comprehensive benchmark for evaluating ultra-long generation capabilities.Our 9B parameter model, further improved through DPO, achieves state-of-the-artperformance on this benchmark, surpassing even much larger proprietary models.In general, our work demonstrates that existing long context LLM alreadypossesses the potential for a larger output window--all you need is data withextended output during model alignment to unlock this capability. Our code &amp;models are at: https://github.com/THUDM/LongWriter.</description><author>Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li</author><pubDate>Tue, 13 Aug 2024 17:46:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07055v1</guid></item><item><title>The News Comment Gap and Algorithmic Agenda Setting in Online Forums</title><link>http://arxiv.org/abs/2408.07052v1</link><description>The disparity between news stories valued by journalists and those preferredby readers, known as the "News Gap", is well-documented. However, thedifference in expectations regarding news related user-generated content isless studied. Comment sections, hosted by news websites, are popular venues forreader engagement, yet still subject to editorial decisions. It is thusimportant to understand journalist vs reader comment preferences and how theseare served by various comment ranking algorithms that represent discussionsdifferently. We analyse 1.2 million comments from Austrian newspaper DerStandard to understand the "News Comment Gap" and the effects of differentranking algorithms. We find that journalists prefer positive, timely, complex,direct responses, while readers favour comments similar to article content fromelite authors. We introduce the versatile Feature-Oriented Ranking UtilityMetric (FORUM) to assess the impact of different ranking algorithms and finddramatic differences in how they prioritise the display of comments bysentiment, topical relevance, lexical diversity, and readability. Journalistscan exert substantial influence over the discourse through both curatorial andalgorithmic means. Understanding these choices' implications is vital infostering engaging and civil discussions while aligning with journalisticobjectives, especially given the increasing legal scrutiny and societalimportance of online discourse.</description><author>Flora Böwing, Patrick Gildersleve</author><pubDate>Tue, 13 Aug 2024 17:43:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07052v1</guid></item><item><title>PSM: Learning Probabilistic Embeddings for Multi-scale Zero-Shot Soundscape Mapping</title><link>http://arxiv.org/abs/2408.07050v1</link><description>A soundscape is defined by the acoustic environment a person perceives at alocation. In this work, we propose a framework for mapping soundscapes acrossthe Earth. Since soundscapes involve sound distributions that span varyingspatial scales, we represent locations with multi-scale satellite imagery andlearn a joint representation among this imagery, audio, and text. To capturethe inherent uncertainty in the soundscape of a location, we design therepresentation space to be probabilistic. We also fuse ubiquitous metadata(including geolocation, time, and data source) to enable learning of spatiallyand temporally dynamic representations of soundscapes. We demonstrate theutility of our framework by creating large-scale soundscape maps integratingboth audio and text with temporal control. To facilitate future research onthis task, we also introduce a large-scale dataset, GeoSound, containing over$300k$ geotagged audio samples paired with both low- and high-resolutionsatellite imagery. We demonstrate that our method outperforms the existingstate-of-the-art on both GeoSound and the existing SoundingEarth dataset. Ourdataset and code is available at https://github.com/mvrl/PSM.</description><author>Subash Khanal, Eric Xing, Srikumar Sastry, Aayush Dhakal, Zhexiao Xiong, Adeel Ahmad, Nathan Jacobs</author><pubDate>Tue, 13 Aug 2024 17:37:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07050v1</guid></item><item><title>TableGuard -- Securing Structured &amp; Unstructured Data</title><link>http://arxiv.org/abs/2408.07045v1</link><description>With the increasing demand for data sharing across platforms andorganizations, ensuring the privacy and security of sensitive information hasbecome a critical challenge. This paper introduces "TableGuard". An innovativeapproach to data obfuscation tailored for relational databases. Building on theprinciples and techniques developed in prior work on context-sensitiveobfuscation, TableGuard applies these methods to ensure that API calls returnonly obfuscated data, thereby safeguarding privacy when sharing data with thirdparties. TableGuard leverages advanced context-sensitive obfuscation techniquesto replace sensitive data elements with contextually appropriate alternatives.By maintaining the relational integrity and coherence of the data, our approachmitigates the risks of cognitive dissonance and data leakage. We demonstratethe implementation of TableGuard using a BERT based transformer model, whichidentifies and obfuscates sensitive entities within relational tables. Ourevaluation shows that TableGuard effectively balances privacy protection withdata utility, minimizing information loss while ensuring that the obfuscateddata remains functionally useful for downstream applications. The resultshighlight the importance of domain-specific obfuscation strategies and the roleof context length in preserving data integrity. The implications of thisresearch are significant for organizations that need to share data securelywith external parties. TableGuard offers a robust framework for implementingprivacy-preserving data sharing mechanisms, thereby contributing to the broaderfield of data privacy and security.</description><author>Anantha Sharma, Ajinkya Deshmukh</author><pubDate>Tue, 13 Aug 2024 17:20:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07045v1</guid></item><item><title>RealGen: Retrieval Augmented Generation for Controllable Traffic Scenarios</title><link>http://arxiv.org/abs/2312.13303v2</link><description>Simulation plays a crucial role in the development of autonomous vehicles(AVs) due to the potential risks associated with real-world testing. Althoughsignificant progress has been made in the visual aspects of simulators,generating complex behavior among agents remains a formidable challenge. It isnot only imperative to ensure realism in the scenarios generated but alsoessential to incorporate preferences and conditions to facilitate controllablegeneration for AV training and evaluation. Traditional methods, mainly relyingon memorizing the distribution of training datasets, often fall short ingenerating unseen scenarios. Inspired by the success of retrieval augmentedgeneration in large language models, we present RealGen, a novelretrieval-based in-context learning framework for traffic scenario generation.RealGen synthesizes new scenarios by combining behaviors from multipleretrieved examples in a gradient-free way, which may originate from templatesor tagged scenarios. This in-context learning framework endows versatilegenerative capabilities, including the ability to edit scenarios, composevarious behaviors, and produce critical scenarios. Evaluations show thatRealGen offers considerable flexibility and controllability, marking a newdirection in the field of controllable traffic scenario generation. Check ourproject website for more information: https://realgen.github.io.</description><author>Wenhao Ding, Yulong Cao, Ding Zhao, Chaowei Xiao, Marco Pavone</author><pubDate>Tue, 13 Aug 2024 17:20:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13303v2</guid></item><item><title>The logic of rational graph neural networks</title><link>http://arxiv.org/abs/2310.13139v8</link><description>The expressivity of Graph Neural Networks (GNNs) can be described viaappropriate fragments of the first order logic. Any query of the two variablefragment of graded modal logic (GC2) interpreted over labeled graphs can beexpressed using a Rectified Linear Unit (ReLU) GNN whose size does not growwith graph input sizes [Barcelo &amp; Al., 2020]. Conversely, a GNN expresses atmost a query of GC2, for any choice of activation function. In this article, weprove that some GC2 queries of depth $3$ cannot be expressed by GNNs with anyrational activation function. This shows that not all non-polynomial activationfunctions confer GNNs maximal expressivity, answering a open questionformulated by [Grohe, 2021]. This result is also in contrast with the efficientuniversal approximation properties of rational feedforward neural networksinvestigated by [Boull\'e &amp; Al., 2020]. We also present a rational subfragmentof the first order logic (RGC2), and prove that rational GNNs can express RGC2queries uniformly over all graphs.</description><author>Sammy Khalife</author><pubDate>Tue, 13 Aug 2024 17:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13139v8</guid></item><item><title>The Physics-Informed Neural Network Gravity Model: Generation III</title><link>http://arxiv.org/abs/2312.10257v2</link><description>Scientific machine learning and the advent of the Physics-Informed NeuralNetwork (PINN) have shown high potential in their ability to solve complexdifferential equations. One example is the use of PINNs to solve the gravityfield modeling problem -- learning convenient representations of thegravitational potential from position and acceleration data. These PINN gravitymodels, or PINN-GMs, have demonstrated advantages in model compactness,robustness to noise, and sample efficiency when compared to popularalternatives; however, further investigation has revealed various failure modesfor these and other machine learning gravity models which this manuscript aimsto address. Specifically, this paper introduces the third generationPhysics-Informed Neural Network Gravity Model (PINN-GM-III) which includesdesign changes that solve the problems of feature divergence, bias towardslow-altitude samples, numerical instability, and extrapolation error. Sixevaluation metrics are proposed to expose these past pitfalls and illustratethe PINN-GM-III's robustness to them. This study concludes by evaluating thePINN-GM-III modeling accuracy on a heterogeneous density asteroid, andcomparing its performance to other analytic and machine learning gravitymodels.</description><author>John Martin, Hanspeter Schaub</author><pubDate>Tue, 13 Aug 2024 17:10:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10257v2</guid></item><item><title>KAN You See It? KANs and Sentinel for Effective and Explainable Crop Field Segmentation</title><link>http://arxiv.org/abs/2408.07040v1</link><description>Segmentation of crop fields is essential for enhancing agriculturalproductivity, monitoring crop health, and promoting sustainable practices. Deeplearning models adopted for this task must ensure accurate and reliablepredictions to avoid economic losses and environmental impact. The newlyproposed Kolmogorov-Arnold networks (KANs) offer promising advancements in theperformance of neural networks. This paper analyzes the integration of KANlayers into the U-Net architecture (U-KAN) to segment crop fields usingSentinel-2 and Sentinel-1 satellite images and provides an analysis of theperformance and explainability of these networks. Our findings indicate a 2\%improvement in IoU compared to the traditional full-convolutional U-Net modelin fewer GFLOPs. Furthermore, gradient-based explanation techniques show thatU-KAN predictions are highly plausible and that the network has a very highability to focus on the boundaries of cultivated areas rather than on the areasthemselves. The per-channel relevance analysis also reveals that some channelsare irrelevant to this task.</description><author>Daniele Rege Cambrin, Eleonora Poeta, Eliana Pastor, Tania Cerquitelli, Elena Baralis, Paolo Garza</author><pubDate>Tue, 13 Aug 2024 17:07:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07040v1</guid></item><item><title>PathInsight: Instruction Tuning of Multimodal Datasets and Models for Intelligence Assisted Diagnosis in Histopathology</title><link>http://arxiv.org/abs/2408.07037v1</link><description>Pathological diagnosis remains the definitive standard for identifyingtumors. The rise of multimodal large models has simplified the process ofintegrating image analysis with textual descriptions. Despite this advancement,the substantial costs associated with training and deploying these complexmultimodal models, together with a scarcity of high-quality training datasets,create a significant divide between cutting-edge technology and its applicationin the clinical setting. We had meticulously compiled a dataset ofapproximately 45,000 cases, covering over 6 different tasks, including theclassification of organ tissues, generating pathology report descriptions, andaddressing pathology-related questions and answers. We have fine-tunedmultimodal large models, specifically LLaVA, Qwen-VL, InternLM, with thisdataset to enhance instruction-based performance. We conducted a qualitativeassessment of the capabilities of the base model and the fine-tuned model inperforming image captioning and classification tasks on the specific dataset.The evaluation results demonstrate that the fine-tuned model exhibitsproficiency in addressing typical pathological questions. We hope that bymaking both our models and datasets publicly available, they can be valuable tothe medical and research communities.</description><author>Xiaomin Wu, Rui Xu, Pengchen Wei, Wenkang Qin, Peixiang Huang, Ziheng Li, Lin Luo</author><pubDate>Tue, 13 Aug 2024 17:05:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07037v1</guid></item><item><title>From NeRFs to Gaussian Splats, and Back</title><link>http://arxiv.org/abs/2405.09717v3</link><description>For robotics applications where there is a limited number of (typicallyego-centric) views, parametric representations such as neural radiance fields(NeRFs) generalize better than non-parametric ones such as Gaussian splatting(GS) to views that are very different from those in the training data; GShowever can render much faster than NeRFs. We develop a procedure to convertback and forth between the two. Our approach achieves the best of both NeRFs(superior PSNR, SSIM, and LPIPS on dissimilar views, and a compactrepresentation) and GS (real-time rendering and ability for easily modifyingthe representation); the computational cost of these conversions is minorcompared to training the two from scratch.</description><author>Siming He, Zach Osman, Pratik Chaudhari</author><pubDate>Tue, 13 Aug 2024 16:49:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09717v3</guid></item><item><title>Efficient Human-Object-Interaction (EHOI) Detection via Interaction Label Coding and Conditional Decision</title><link>http://arxiv.org/abs/2408.07018v1</link><description>Human-Object Interaction (HOI) detection is a fundamental task in imageunderstanding. While deep-learning-based HOI methods provide high performancein terms of mean Average Precision (mAP), they are computationally expensiveand opaque in training and inference processes. An Efficient HOI (EHOI)detector is proposed in this work to strike a good balance between detectionperformance, inference complexity, and mathematical transparency. EHOI is atwo-stage method. In the first stage, it leverages a frozen object detector tolocalize the objects and extract various features as intermediate outputs. Inthe second stage, the first-stage outputs predict the interaction type usingthe XGBoost classifier. Our contributions include the application of errorcorrection codes (ECCs) to encode rare interaction cases, which reduces themodel size and the complexity of the XGBoost classifier in the second stage.Additionally, we provide a mathematical formulation of the relabeling anddecision-making process. Apart from the architecture, we present qualitativeresults to explain the functionalities of the feedforward modules. Experimentalresults demonstrate the advantages of ECC-coded interaction labels and theexcellent balance of detection performance and complexity of the proposed EHOImethod.</description><author>Tsung-Shan Yang, Yun-Cheng Wang, Chengwei Wei, Suya You, C. -C. Jay Kuo</author><pubDate>Tue, 13 Aug 2024 16:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07018v1</guid></item><item><title>Defining and Measuring Disentanglement for non-Independent Factors of Variation</title><link>http://arxiv.org/abs/2408.07016v1</link><description>Representation learning is an approach that allows to discover and extractthe factors of variation from the data. Intuitively, a representation is saidto be disentangled if it separates the different factors of variation in a waythat is understandable to humans. Definitions of disentanglement and metrics tomeasure it usually assume that the factors of variation are independent of eachother. However, this is generally false in the real world, which limits the useof these definitions and metrics to very specific and unrealistic scenarios. Inthis paper we give a definition of disentanglement based on information theorythat is also valid when the factors of variation are not independent.Furthermore, we relate this definition to the Information Bottleneck Method.Finally, we propose a method to measure the degree of disentanglement from thegiven definition that works when the factors of variation are not independent.We show through different experiments that the method proposed in this papercorrectly measures disentanglement with non-independent factors of variation,while other methods fail in this scenario.</description><author>Antonio Almudévar, Alfonso Ortega, Luis Vicente, Antonio Miguel, Eduardo Lleida</author><pubDate>Tue, 13 Aug 2024 16:30:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07016v1</guid></item><item><title>GarmentCodeData: A Dataset of 3D Made-to-Measure Garments With Sewing Patterns</title><link>http://arxiv.org/abs/2405.17609v2</link><description>Recent research interest in the learning-based processing of garments, fromvirtual fitting to generation and reconstruction, stumbles on a scarcity ofhigh-quality public data in the domain. We contribute to resolving this need bypresenting the first large-scale synthetic dataset of 3D made-to-measuregarments with sewing patterns, as well as its generation pipeline.GarmentCodeData contains 115,000 data points that cover a variety of designs inmany common garment categories: tops, shirts, dresses, jumpsuits, skirts,pants, etc., fitted to a variety of body shapes sampled from a customstatistical body model based on CAESAR, as well as a standard reference bodyshape, applying three different textile materials. To enable the creation ofdatasets of such complexity, we introduce a set of algorithms for automaticallytaking tailor's measures on sampled body shapes, sampling strategies for sewingpattern design, and propose an automatic, open-source 3D garment drapingpipeline based on a fast XPBD simulator, while contributing several solutionsfor collision resolution and drape correctness to enable scalability. Project Page: https://igl.ethz.ch/projects/GarmentCodeData/ Dataset: https://doi.org/10.3929/ethz-b-000673889</description><author>Maria Korosteleva, Timur Levent Kesdogan, Fabian Kemper, Stephan Wenninger, Jasmin Koller, Yuhan Zhang, Mario Botsch, Olga Sorkine-Hornung</author><pubDate>Tue, 13 Aug 2024 16:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17609v2</guid></item><item><title>Hierarchical Quantum Control Gates for Functional MRI Understanding</title><link>http://arxiv.org/abs/2408.03596v2</link><description>Quantum computing has emerged as a powerful tool for solving complex problemsintractable for classical computers, particularly in popular fields such ascryptography, optimization, and neurocomputing. In this paper, we present a newquantum-based approach named the Hierarchical Quantum Control Gates (HQCG)method for efficient understanding of Functional Magnetic Resonance Imaging(fMRI) data. This approach includes two novel modules: the Local QuantumControl Gate (LQCG) and the Global Quantum Control Gate (GQCG), which aredesigned to extract local and global features of fMRI signals, respectively.Our method operates end-to-end on a quantum machine, leveraging quantummechanics to learn patterns within extremely high-dimensional fMRI signals,such as 30,000 samples which is a challenge for classical computers. Empiricalresults demonstrate that our approach significantly outperforms classicalmethods. Additionally, we found that the proposed quantum model is more stableand less prone to overfitting than the classical methods.</description><author>Xuan-Bac Nguyen, Hoang-Quan Nguyen, Hugh Churchill, Samee U. Khan, Khoa Luu</author><pubDate>Tue, 13 Aug 2024 16:18:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03596v2</guid></item><item><title>Imagen 3</title><link>http://arxiv.org/abs/2408.07009v1</link><description>We introduce Imagen 3, a latent diffusion model that generates high qualityimages from text prompts. We describe our quality and responsibilityevaluations. Imagen 3 is preferred over other state-of-the-art (SOTA) models atthe time of evaluation. In addition, we discuss issues around safety andrepresentation, as well as methods we used to minimize the potential harm ofour models.</description><author>Imagen-Team-Google, :, Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, Hongliang Fei, Nando de Freitas, Yilin Gao, Evgeny Gladchenko, Sergio Gómez Colmenarejo, Mandy Guo, Alex Haig, Will Hawkins, Hexiang Hu, Huilian Huang, Tobenna Peter Igwe, Christos Kaplanis, Siavash Khodadadeh, Yelin Kim, Ksenia Konyushkova, Karol Langner, Eric Lau, Shixin Luo, Soňa Mokrá, Henna Nandwani, Yasumasa Onoe, Aäron van den Oord, Zarana Parekh, Jordi Pont-Tuset, Hang Qi, Rui Qian, Deepak Ramachandran, Poorva Rane, Abdullah Rashwan, Ali Razavi, Robert Riachi, Hansa Srinivasan, Srivatsan Srinivasan, Robin Strudel, Benigno Uria, Oliver Wang, Su Wang, Austin Waters, Chris Wolff, Auriel Wright, Zhisheng Xiao, Hao </author><pubDate>Tue, 13 Aug 2024 16:15:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07009v1</guid></item><item><title>SSHPool: The Separated Subgraph-based Hierarchical Pooling</title><link>http://arxiv.org/abs/2403.16133v2</link><description>In this paper, we develop a novel local graph pooling method, namely theSeparated Subgraph-based Hierarchical Pooling (SSHPool), for graphclassification. We commence by assigning the nodes of a sample graph intodifferent clusters, resulting in a family of separated subgraphs. Weindividually employ the local graph convolution units as the local structure tofurther compress each subgraph into a coarsened node, transforming the originalgraph into a coarsened graph. Since these subgraphs are separated by differentclusters and the structural information cannot be propagated between them, thelocal convolution operation can significantly avoid the over-smoothing problemcaused by message passing through edges in most existing Graph Neural Networks(GNNs). By hierarchically performing the proposed procedures on the resultingcoarsened graph, the proposed SSHPool can effectively extract the hierarchicalglobal features of the original graph structure, encapsulating rich intrinsicstructural characteristics. Furthermore, we develop an end-to-end GNN frameworkassociated with the SSHPool module for graph classification. Experimentalresults demonstrate the superior performance of the proposed model onreal-world datasets.</description><author>Zhuo Xu, Lixin Cui, Ming Li, Yue Wang, Ziyu Lyu, Hangyuan Du, Lu Bai, Philip S. Yu, Edwin R. Hancock</author><pubDate>Tue, 13 Aug 2024 16:15:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16133v2</guid></item><item><title>The Distributional Uncertainty of the SHAP score in Explainable Machine Learning</title><link>http://arxiv.org/abs/2401.12731v4</link><description>Attribution scores reflect how important the feature values in an inputentity are for the output of a machine learning model. One of the most popularattribution scores is the SHAP score, which is an instantiation of the generalShapley value used in coalition game theory. The definition of this scorerelies on a probability distribution on the entity population. Since the exactdistribution is generally unknown, it needs to be assigned subjectively or beestimated from data, which may lead to misleading feature scores. In thispaper, we propose a principled framework for reasoning on SHAP scores underunknown entity population distributions. In our framework, we consider anuncertainty region that contains the potential distributions, and the SHAPscore of a feature becomes a function defined over this region. We study thebasic problems of finding maxima and minima of this function, which allows usto determine tight ranges for the SHAP scores of all features. In particular,we pinpoint the complexity of these problems, and other related ones, showingthem to be NP-complete. Finally, we present experiments on a real-worlddataset, showing that our framework may contribute to a more robust featurescoring.</description><author>Santiago Cifuentes, Leopoldo Bertossi, Nina Pardal, Sergio Abriola, Maria Vanina Martinez, Miguel Romero</author><pubDate>Tue, 13 Aug 2024 16:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12731v4</guid></item><item><title>Active Learning for Control-Oriented Identification of Nonlinear Systems</title><link>http://arxiv.org/abs/2404.09030v2</link><description>Model-based reinforcement learning is an effective approach for controllingan unknown system. It is based on a longstanding pipeline familiar to thecontrol community in which one performs experiments on the environment tocollect a dataset, uses the resulting dataset to identify a model of thesystem, and finally performs control synthesis using the identified model. Asinteracting with the system may be costly and time consuming, targetedexploration is crucial for developing an effective control-oriented model withminimal experimentation. Motivated by this challenge, recent work has begun tostudy finite sample data requirements and sample efficient algorithms for theproblem of optimal exploration in model-based reinforcement learning. However,existing theory and algorithms are limited to model classes which are linear inthe parameters. Our work instead focuses on models with nonlinear parameterdependencies, and presents the first finite sample analysis of an activelearning algorithm suitable for a general class of nonlinear dynamics. Incertain settings, the excess control cost of our algorithm achieves the optimalrate, up to logarithmic factors. We validate our approach in simulation,showcasing the advantage of active, control-oriented exploration forcontrolling nonlinear systems.</description><author>Bruce D. Lee, Ingvar Ziemann, George J. Pappas, Nikolai Matni</author><pubDate>Tue, 13 Aug 2024 16:11:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09030v2</guid></item><item><title>Casper: Prompt Sanitization for Protecting User Privacy in Web-Based Large Language Models</title><link>http://arxiv.org/abs/2408.07004v1</link><description>Web-based Large Language Model (LLM) services have been widely adopted andhave become an integral part of our Internet experience. Third-party pluginsenhance the functionalities of LLM by enabling access to real-world data andservices. However, the privacy consequences associated with these services andtheir third-party plugins are not well understood. Sensitive prompt data arestored, processed, and shared by cloud-based LLM providers and third-partyplugins. In this paper, we propose Casper, a prompt sanitization technique thataims to protect user privacy by detecting and removing sensitive informationfrom user inputs before sending them to LLM services. Casper runs entirely onthe user's device as a browser extension and does not require any changes tothe online LLM services. At the core of Casper is a three-layered sanitizationmechanism consisting of a rule-based filter, a Machine Learning (ML)-basednamed entity recognizer, and a browser-based local LLM topic identifier. Weevaluate Casper on a dataset of 4000 synthesized prompts and show that it caneffectively filter out Personal Identifiable Information (PII) andprivacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.</description><author>Chun Jie Chong, Chenxi Hou, Zhihao Yao, Seyed Mohammadjavad Seyed Talebi</author><pubDate>Tue, 13 Aug 2024 16:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07004v1</guid></item><item><title>Generative AI for automatic topic labelling</title><link>http://arxiv.org/abs/2408.07003v1</link><description>Topic Modeling has become a prominent tool for the study of scientificfields, as they allow for a large scale interpretation of research trends.Nevertheless, the output of these models is structured as a list of keywordswhich requires a manual interpretation for the labelling. This paper proposesto assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 minifor topic labelling. Drawing on previous research leveraging BERTopic, wegenerate topics from a dataset of all the scientific articles (n=34,797)authored by all biology professors in Switzerland (n=465) between 2008 and2020, as recorded in the Web of Science database. We assess the output of thethree models both quantitatively and qualitatively and find that, first, bothGPT models are capable of accurately and precisely label topics from themodels' output keywords. Second, 3-word labels are preferable to grasp thecomplexity of research topics.</description><author>Diego Kozlowski, Carolina Pradier, Pierre Benz</author><pubDate>Tue, 13 Aug 2024 16:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07003v1</guid></item><item><title>EEG-MACS: Manifold Attention and Confidence Stratification for EEG-based Cross-Center Brain Disease Diagnosis under Unreliable Annotations</title><link>http://arxiv.org/abs/2405.00734v2</link><description>Cross-center data heterogeneity and annotation unreliability significantlychallenge the intelligent diagnosis of diseases using brain signals. A notableexample is the EEG-based diagnosis of neurodegenerative diseases, whichfeatures subtler abnormal neural dynamics typically observed in small-groupsettings. To advance this area, in this work, we introduce a transferableframework employing Manifold Attention and Confidence Stratification (MACS) todiagnose neurodegenerative disorders based on EEG signals sourced from fourcenters with unreliable annotations. The MACS framework's effectiveness stemsfrom these features: 1) The Augmentor generates various EEG-represented brainvariants to enrich the data space; 2) The Switcher enhances the feature spacefor trusted samples and reduces overfitting on incorrectly labeled samples; 3)The Encoder uses the Riemannian manifold and Euclidean metrics to capturespatiotemporal variations and dynamic synchronization in EEG; 4) The Projector,equipped with dual heads, monitors consistency across multiple brain variantsand ensures diagnostic accuracy; 5) The Stratifier adaptively stratifieslearned samples by confidence levels throughout the training process; 6)Forward and backpropagation in MACS are constrained by confidencestratification to stabilize the learning system amid unreliable annotations.Our subject-independent experiments, conducted on both neurocognitive andmovement disorders using cross-center corpora, have demonstrated superiorperformance compared to existing related algorithms. This work not onlyimproves EEG-based diagnostics for cross-center and small-setting braindiseases but also offers insights into extending MACS techniques to other dataanalyses, tackling data heterogeneity and annotation unreliability inmultimedia and multimodal content understanding.</description><author>Zhenxi Song, Ruihan Qin, Huixia Ren, Zhen Liang, Yi Guo, Min Zhang, Zhiguo Zhang</author><pubDate>Tue, 13 Aug 2024 16:03:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00734v2</guid></item><item><title>AKBR: Learning Adaptive Kernel-based Representations for Graph Classification</title><link>http://arxiv.org/abs/2403.16130v2</link><description>In this paper, we propose a new model to learn Adaptive Kernel-basedRepresentations (AKBR) for graph classification. Unlike state-of-the-artR-convolution graph kernels that are defined by merely counting any pair ofisomorphic substructures between graphs and cannot provide an end-to-endlearning mechanism for the classifier, the proposed AKBR approach aims todefine an end-to-end representation learning model to construct an adaptivekernel matrix for graphs. To this end, we commence by leveraging a novelfeature-channel attention mechanism to capture the interdependencies betweendifferent substructure invariants of original graphs. The proposed AKBR modelcan thus effectively identify the structural importance of differentsubstructures, and compute the R-convolution kernel between pairwise graphsassociated with the more significant substructures specified by theirstructural attentions. Since each row of the resulting kernel matrix can betheoretically seen as the embedding vector of a sample graph, the proposed AKBRmodel is able to directly employ the resulting kernel matrix as the graphfeature matrix and input it into the classifier for classification (i.e., theSoftMax layer), naturally providing an end-to-end learning architecture betweenthe kernel computation as well as the classifier. Experimental results showthat the proposed AKBR model outperforms existing state-of-the-art graphkernels and deep learning methods on standard graph benchmarks.</description><author>Feifei Qian, Lixin Cui, Ming Li, Yue Wang, Hangyuan Du, Lixiang Xu, Lu Bai, Philip S. Yu, Edwin R. Hancock</author><pubDate>Tue, 13 Aug 2024 16:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16130v2</guid></item><item><title>The Visual Experience Dataset: Over 200 Recorded Hours of Integrated Eye Movement, Odometry, and Egocentric Video</title><link>http://arxiv.org/abs/2404.18934v2</link><description>We introduce the Visual Experience Dataset (VEDB), a compilation of over 240hours of egocentric video combined with gaze- and head-tracking data thatoffers an unprecedented view of the visual world as experienced by humanobservers. The dataset consists of 717 sessions, recorded by 58 observersranging from 6-49 years old. This paper outlines the data collection,processing, and labeling protocols undertaken to ensure a representative sampleand discusses the potential sources of error or bias within the dataset. TheVEDB's potential applications are vast, including improving gaze trackingmethodologies, assessing spatiotemporal image statistics, and refining deepneural networks for scene and activity recognition. The VEDB is accessiblethrough established open science platforms and is intended to be a livingdataset with plans for expansion and community contributions. It is releasedwith an emphasis on ethical considerations, such as participant privacy and themitigation of potential biases. By providing a dataset grounded in real-worldexperiences and accompanied by extensive metadata and supporting code, theauthors invite the research community to utilize and contribute to the VEDB,facilitating a richer understanding of visual perception and behavior innaturalistic settings.</description><author>Michelle R. Greene, Benjamin J. Balas, Mark D. Lescroart, Paul R. MacNeilage, Jennifer A. Hart, Kamran Binaee, Peter A. Hausamann, Ronald Mezile, Bharath Shankar, Christian B. Sinnott, Kaylie Capurro, Savannah Halow, Hunter Howe, Mariam Josyula, Annie Li, Abraham Mieses, Amina Mohamed, Ilya Nudnou, Ezra Parkhill, Peter Riley, Brett Schmidt, Matthew W. Shinkle, Wentao Si, Brian Szekely, Joaquin M. Torres, Eliana Weissmann</author><pubDate>Tue, 13 Aug 2024 16:01:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18934v2</guid></item><item><title>Faster Private Minimum Spanning Trees</title><link>http://arxiv.org/abs/2408.06997v1</link><description>Motivated by applications in clustering and synthetic data generation, weconsider the problem of releasing a minimum spanning tree (MST) underedge-weight differential privacy constraints where a graph topology $G=(V,E)$with $n$ vertices and $m$ edges is public, the weight matrix $\vec{W}\in\mathbb{R}^{n \times n}$ is private, and we wish to release an approximate MSTunder $\rho$-zero-concentrated differential privacy. Weight matrices areconsidered neighboring if they differ by at most $\Delta_\infty$ in each entry,i.e., we consider an $\ell_\infty$ neighboring relationship. Existing privateMST algorithms either add noise to each entry in $\vec{W}$ and estimate the MSTby post-processing or add noise to weights in-place during the execution of aspecific MST algorithm. Using the post-processing approach with an efficientMST algorithm takes $O(n^2)$ time on dense graphs but results in an additiveerror on the weight of the MST of magnitude $O(n^2\log n)$. In-place algorithmsgive asymptotically better utility, but the running time of existing in-placealgorithms is $O(n^3)$ for dense graphs. Our main result is a newdifferentially private MST algorithm that matches the utility of existingin-place methods while running in time $O(m + n^{3/2}\log n)$ for fixed privacyparameter $\rho$. The technical core of our algorithm is an efficient sublineartime simulation of Report-Noisy-Max that works by discretizing all edge weightsto a multiple of $\Delta_\infty$ and forming groups of edges with identicalweights. Specifically, we present a data structure that allows us to sample anoisy minimum weight edge among at most $O(n^2)$ cut edges in $O(\sqrt{n} \logn)$ time. Experimental evaluations support our claims that our algorithmsignificantly improves previous algorithms either in utility or running time.</description><author>Rasmus Pagh, Lukas Retschmeier</author><pubDate>Tue, 13 Aug 2024 16:00:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06997v1</guid></item><item><title>HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2312.02902v2</link><description>3D head animation has seen major quality and runtime improvements over thelast few years, particularly empowered by the advances in differentiablerendering and neural radiance fields. Real-time rendering is a highly desirablegoal for real-world applications. We propose HeadGaS, a model that uses 3DGaussian Splats (3DGS) for 3D head reconstruction and animation. In this paperwe introduce a hybrid model that extends the explicit 3DGS representation witha base of learnable latent features, which can be linearly blended withlow-dimensional parameters from parametric head models to obtainexpression-dependent color and opacity values. We demonstrate that HeadGaSdelivers state-of-the-art results in real-time inference frame rates,surpassing baselines by up to 2dB, while accelerating rendering speed by overx10.</description><author>Helisa Dhamo, Yinyu Nie, Arthur Moreau, Jifei Song, Richard Shaw, Yiren Zhou, Eduardo Pérez-Pellitero</author><pubDate>Tue, 13 Aug 2024 15:56:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02902v2</guid></item><item><title>Blessing of Dimensionality for Approximating Sobolev Classes on Manifolds</title><link>http://arxiv.org/abs/2408.06996v1</link><description>The manifold hypothesis says that natural high-dimensional data is actuallysupported on or around a low-dimensional manifold. Recent success ofstatistical and learning-based methods empirically supports this hypothesis,due to outperforming classical statistical intuition in very high dimensions. Anatural step for analysis is thus to assume the manifold hypothesis and derivebounds that are independent of any embedding space. Theoretical implications inthis direction have recently been explored in terms of generalization of ReLUnetworks and convergence of Langevin methods. We complement existing results byproviding theoretical statistical complexity results, which directly relates togeneralization properties. In particular, we demonstrate that the statisticalcomplexity required to approximate a class of bounded Sobolev functions on acompact manifold is bounded from below, and moreover that this bound isdependent only on the intrinsic properties of the manifold. These providecomplementary bounds for existing approximation results for ReLU networks onmanifolds, which give upper bounds on generalization capacity.</description><author>Hong Ye Tan, Subhadip Mukherjee, Junqi Tang, Carola-Bibiane Schönlieb</author><pubDate>Tue, 13 Aug 2024 15:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06996v1</guid></item><item><title>Low-Bitwidth Floating Point Quantization for Efficient High-Quality Diffusion Models</title><link>http://arxiv.org/abs/2408.06995v1</link><description>Diffusion models are emerging models that generate images by iterativelydenoising random Gaussian noise using deep neural networks. These modelstypically exhibit high computational and memory demands, necessitatingeffective post-training quantization for high-performance inference. Recentworks propose low-bitwidth (e.g., 8-bit or 4-bit) quantization for diffusionmodels, however 4-bit integer quantization typically results in low-qualityimages. We observe that on several widely used hardware platforms, there islittle or no difference in compute capability between floating-point andinteger arithmetic operations of the same bitwidth (e.g., 8-bit or 4-bit).Therefore, we propose an effective floating-point quantization method fordiffusion models that provides better image quality compared to integerquantization methods. We employ a floating-point quantization method that waseffective for other processing tasks, specifically computer vision and naturallanguage tasks, and tailor it for diffusion models by integrating weightrounding learning during the mapping of the full-precision values to thequantized values in the quantization process. We comprehensively study integerand floating-point quantization methods in state-of-the-art diffusion models.Our floating-point quantization method not only generates higher-quality imagesthan that of integer quantization methods, but also shows no noticeabledegradation compared to full-precision models (32-bit floating-point), whenboth weights and activations are quantized to 8-bit floating-point values,while has minimal degradation with 4-bit weights and 8-bit activations.</description><author>Cheng Chen, Christina Giannoula, Andreas Moshovos</author><pubDate>Tue, 13 Aug 2024 15:56:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06995v1</guid></item><item><title>LLMs can Schedule</title><link>http://arxiv.org/abs/2408.06993v1</link><description>The job shop scheduling problem (JSSP) remains a significant hurdle inoptimizing production processes. This challenge involves efficiently allocatingjobs to a limited number of machines while minimizing factors like totalprocessing time or job delays. While recent advancements in artificialintelligence have yielded promising solutions, such as reinforcement learningand graph neural networks, this paper explores the potential of Large LanguageModels (LLMs) for JSSP. We introduce the very first supervised 120k datasetspecifically designed to train LLMs for JSSP. Surprisingly, our findingsdemonstrate that LLM-based scheduling can achieve performance comparable toother neural approaches. Furthermore, we propose a sampling method thatenhances the effectiveness of LLMs in tackling JSSP.</description><author>Henrik Abgaryan, Ararat Harutyunyan, Tristan Cazenave</author><pubDate>Tue, 13 Aug 2024 15:53:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06993v1</guid></item><item><title>How Transformers Learn Causal Structure with Gradient Descent</title><link>http://arxiv.org/abs/2402.14735v2</link><description>The incredible success of transformers on sequence modeling tasks can belargely attributed to the self-attention mechanism, which allows information tobe transferred between different parts of a sequence. Self-attention allowstransformers to encode causal structure which makes them particularly suitablefor sequence modeling. However, the process by which transformers learn suchcausal structure via gradient-based training algorithms remains poorlyunderstood. To better understand this process, we introduce an in-contextlearning task that requires learning latent causal structure. We prove thatgradient descent on a simplified two-layer transformer learns to solve thistask by encoding the latent causal graph in the first attention layer. The keyinsight of our proof is that the gradient of the attention matrix encodes themutual information between tokens. As a consequence of the data processinginequality, the largest entries of this gradient correspond to edges in thelatent causal graph. As a special case, when the sequences are generated fromin-context Markov chains, we prove that transformers learn an induction head(Olsson et al., 2022). We confirm our theoretical findings by showing thattransformers trained on our in-context learning task are able to recover a widevariety of causal structures.</description><author>Eshaan Nichani, Alex Damian, Jason D. Lee</author><pubDate>Tue, 13 Aug 2024 15:45:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14735v2</guid></item><item><title>V4d: voxel for 4d novel view synthesis</title><link>http://arxiv.org/abs/2205.14332v4</link><description>Neural radiance fields have made a remarkable breakthrough in the novel viewsynthesis task at the 3D static scene. However, for the 4D circumstance (e.g.,dynamic scene), the performance of the existing method is still limited by thecapacity of the neural network, typically in a multilayer perceptron network(MLP). In this paper, we utilize 3D Voxel to model the 4D neural radiancefield, short as V4D, where the 3D voxel has two formats. The first one is toregularly model the 3D space and then use the sampled local 3D feature with thetime index to model the density field and the texture field by a tiny MLP. Thesecond one is in look-up tables (LUTs) format that is for the pixel-levelrefinement, where the pseudo-surface produced by the volume rendering isutilized as the guidance information to learn a 2D pixel-level refinementmapping. The proposed LUTs-based refinement module achieves the performancegain with little computational cost and could serve as the plug-and-play modulein the novel view synthesis task. Moreover, we propose a more effectiveconditional positional encoding toward the 4D data that achieves performancegain with negligible computational burdens. Extensive experiments demonstratethat the proposed method achieves state-of-the-art performance at a lowcomputational cost.</description><author>Wanshui Gan, Hongbin Xu, Yi Huang, Shifeng Chen, Naoto Yokoya</author><pubDate>Tue, 13 Aug 2024 15:43:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.14332v4</guid></item><item><title>Optimizing Emotion Recognition with Wearable Sensor Data: Unveiling Patterns in Body Movements and Heart Rate through Random Forest Hyperparameter Tuning</title><link>http://arxiv.org/abs/2408.03958v2</link><description>This research delves into the utilization of smartwatch sensor data and heartrate monitoring to discern individual emotions based on body movement and heartrate. Emotions play a pivotal role in human life, influencing mentalwell-being, quality of life, and even physical and physiological responses. Thedata were sourced from prior research by Juan C. Quiroz, PhD. The studyenlisted 50 participants who donned smartwatches and heart rate monitors whilecompleting a 250-meter walk. Emotions were induced through both audio-visualand audio stimuli, with participants' emotional states evaluated using thePANAS questionnaire. The study scrutinized three scenarios: viewing a moviebefore walking, listening to music before walking, and listening to music whilewalking. Personal baselines were established using DummyClassifier with the'most_frequent' strategy from the sklearn library, and various models,including Logistic Regression and Random Forest, were employed to gauge theimpacts of these activities. Notably, a novel approach was undertaken byincorporating hyperparameter tuning to the Random Forest model usingRandomizedSearchCV. The outcomes showcased substantial enhancements withhyperparameter tuning in the Random Forest model, yielding mean accuracies of86.63% for happy vs. sad and 76.33% for happy vs. neutral vs. sad.</description><author>Zikri Kholifah Nur, Rifki Wijaya, Gia Septiana Wulandari</author><pubDate>Tue, 13 Aug 2024 15:35:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03958v2</guid></item><item><title>SpectralGaussians: Semantic, spectral 3D Gaussian splatting for multi-spectral scene representation, visualization and analysis</title><link>http://arxiv.org/abs/2408.06975v1</link><description>We propose a novel cross-spectral rendering framework based on 3D GaussianSplatting (3DGS) that generates realistic and semantically meaningful splatsfrom registered multi-view spectrum and segmentation maps. This extensionenhances the representation of scenes with multiple spectra, providing insightsinto the underlying materials and segmentation. We introduce an improvedphysically-based rendering approach for Gaussian splats, estimating reflectanceand lights per spectra, thereby enhancing accuracy and realism. In acomprehensive quantitative and qualitative evaluation, we demonstrate thesuperior performance of our approach with respect to other recentlearning-based spectral scene representation approaches (i.e., XNeRF andSpectralNeRF) as well as other non-spectral state-of-the-art learning-basedapproaches. Our work also demonstrates the potential of spectral sceneunderstanding for precise scene editing techniques like style transfer,inpainting, and removal. Thereby, our contributions address challenges inmulti-spectral scene representation, rendering, and editing, offering newpossibilities for diverse applications.</description><author>Saptarshi Neil Sinha, Holger Graf, Michael Weinmann</author><pubDate>Tue, 13 Aug 2024 15:32:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06975v1</guid></item><item><title>On the Correspondence of Non-flat Assumption-based Argumentation and Logic Programming with Negation as Failure in the Head</title><link>http://arxiv.org/abs/2405.09415v3</link><description>The relation between (a fragment of) assumption-based argumentation (ABA) andlogic programs (LPs) under stable model semantics is well-studied. However, forobtaining this relation, the ABA framework needs to be restricted to beingflat, i.e., a fragment where the (defeasible) assumptions can never beentailed, only assumed to be true or false. Here, we remove this restrictionand show a correspondence between non-flat ABA and LPs with negation as failurein their head. We then extend this result to so-called set-stable ABAsemantics, originally defined for the fragment of non-flat ABA called bipolarABA. We showcase how to define set-stable semantics for LPs with negation asfailure in their head and show the correspondence to set-stable ABA semantics.</description><author>Anna Rapberger, Markus Ulbricht, Francesca Toni</author><pubDate>Tue, 13 Aug 2024 15:32:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09415v3</guid></item><item><title>Maintaining Adversarial Robustness in Continuous Learning</title><link>http://arxiv.org/abs/2402.11196v2</link><description>Adversarial robustness is essential for security and reliability of machinelearning systems. However, adversarial robustness enhanced by defensealgorithms is easily erased as the neural network's weights update to learn newtasks. To address this vulnerability, it is essential to improve the capabilityof neural networks in terms of robust continual learning. Specially, we proposea novel gradient projection technique that effectively stabilizes samplegradients from previous data by orthogonally projecting back-propagationgradients onto a crucial subspace before using them for weight updates. Thistechnique can maintaining robustness by collaborating with a class of defensealgorithms through sample gradient smoothing. The experimental results on fourbenchmarks including Split-CIFAR100 and Split-miniImageNet, demonstrate thatthe superiority of the proposed approach in mitigating rapidly degradation ofrobustness during continual learning even when facing strong adversarialattacks.</description><author>Xiaolei Ru, Xiaowei Cao, Zijia Liu, Jack Murdoch Moore, Xin-Ya Zhang, Xia Zhu, Wenjia Wei, Gang Yan</author><pubDate>Tue, 13 Aug 2024 15:28:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11196v2</guid></item><item><title>Prompt-Based Segmentation at Multiple Resolutions and Lighting Conditions using Segment Anything Model 2</title><link>http://arxiv.org/abs/2408.06970v1</link><description>This paper provides insight into the effectiveness of zero-shot,prompt-based, Segment Anything Model (SAM), and its updated version, SAM 2, andthe non-promptable, conventional convolutional network (CNN), in segmentingsolar panels, in RGB aerial imagery, across lighting conditions, spatialresolutions, and prompt strategies. SAM 2 demonstrates improvements over SAM,particularly in sub-optimal lighting conditions when prompted by points. BothSAMs, prompted by user-box, outperformed CNN, in all scenarios. Additionally,YOLOv9 prompting outperformed user points prompting. In high-resolutionimagery, both in optimal and sub-optimal lighting conditions, Eff-UNetoutperformed both SAM models prompted by YOLOv9 boxes, positioning Eff-UNet asthe appropriate model for automatic segmentation in high-resolution data. Inlow-resolution data, user box prompts were found crucial to achieve areasonable performance. This paper provides details on strengths andlimitations of each model and outlines robustness of user prompted imagesegmentation models in inconsistent resolution and lighting conditions ofremotely sensed data.</description><author>Osher Rafaeli, Tal Svoray, Ariel Nahlieli</author><pubDate>Tue, 13 Aug 2024 15:27:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06970v1</guid></item><item><title>IRS-Assisted Lossy Communications Under Correlated Rayleigh Fading: Outage Probability Analysis and Optimization</title><link>http://arxiv.org/abs/2408.06969v1</link><description>This paper focuses on an intelligent reflecting surface (IRS)-assisted lossycommunication system with correlated Rayleigh fading. We analyze the correlatedchannel model and derive the outage probability of the system. Then, we designa deep reinforce learning (DRL) method to optimize the phase shift of IRS, inorder to maximize the received signal power. Moreover, this paper presentsresults of the simulations conducted to evaluate the performance of theDRL-based method. The simulation results indicate that the outage probabilityof the considered system increases significantly with more correlated channelcoefficients. Moreover, the performance gap between DRL and theoretical limitincreases with higher transmit power and/or larger distortion requirement.</description><author>Guanchang Li, Wensheng Lin, Lixin Li, Yixuan He, Fucheng Yang, Zhu Han</author><pubDate>Tue, 13 Aug 2024 15:27:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06969v1</guid></item><item><title>Event-Stream Super Resolution using Sigma-Delta Neural Network</title><link>http://arxiv.org/abs/2408.06968v1</link><description>This study introduces a novel approach to enhance the spatial-temporalresolution of time-event pixels based on luminance changes captured by eventcameras. These cameras present unique challenges due to their low resolutionand the sparse, asynchronous nature of the data they collect. Current eventsuper-resolution algorithms are not fully optimized for the distinct datastructure produced by event cameras, resulting in inefficiencies in capturingthe full dynamism and detail of visual scenes with improved computationalcomplexity. To bridge this gap, our research proposes a method that integratesbinary spikes with Sigma Delta Neural Networks (SDNNs), leveragingspatiotemporal constraint learning mechanism designed to simultaneously learnthe spatial and temporal distributions of the event stream. The proposednetwork is evaluated using widely recognized benchmark datasets, includingN-MNIST, CIFAR10-DVS, ASL-DVS, and Event-NFS. A comprehensive evaluationframework is employed, assessing both the accuracy, through root mean squareerror (RMSE), and the computational efficiency of our model. The findingsdemonstrate significant improvements over existing state-of-the-art methods,specifically, the proposed method outperforms state-of-the-art performance incomputational efficiency, achieving a 17.04-fold improvement in event sparsityand a 32.28-fold increase in synaptic operation efficiency over traditionalartificial neural networks, alongside a two-fold better performance overspiking neural networks.</description><author>Waseem Shariff, Joe Lemley, Peter Corcoran</author><pubDate>Tue, 13 Aug 2024 15:25:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06968v1</guid></item><item><title>Stabilizer bootstrapping: A recipe for efficient agnostic tomography and magic estimation</title><link>http://arxiv.org/abs/2408.06967v1</link><description>We study the task of agnostic tomography: given copies of an unknown$n$-qubit state $\rho$ which has fidelity $\tau$ with some state in a givenclass $C$, find a state which has fidelity $\ge \tau - \epsilon$ with $\rho$.We give a new framework, stabilizer bootstrapping, for designingcomputationally efficient protocols for this task, and use this to get newagnostic tomography protocols for the following classes: Stabilizer states: We give a protocol that runs in time$\mathrm{poly}(n,1/\epsilon)\cdot (1/\tau)^{O(\log(1/\tau))}$, answering anopen question posed by Grewal, Iyer, Kretschmer, Liang [40] and Anshu andArunachalam [6]. Previous protocols ran in time $\mathrm{exp}(\Theta(n))$ orrequired $\tau&gt;\cos^2(\pi/8)$. States with stabilizer dimension $n - t$: We give a protocol that runs intime $n^3\cdot(2^t/\tau)^{O(\log(1/\epsilon))}$, extending recent work onlearning quantum states prepared by circuits with few non-Clifford gates, whichonly applied in the realizable setting where $\tau = 1$ [30, 37, 46, 61]. Discrete product states: If $C = K^{\otimes n}$ for some $\mu$-separateddiscrete set $K$ of single-qubit states, we give a protocol that runs in time$(n/\mu)^{O((1 + \log (1/\tau))/\mu)}/\epsilon^2$. This strictly generalizes aprior guarantee which applied to stabilizer product states [39]. For stabilizerproduct states, we give a further improved protocol that runs in time$(n^2/\epsilon^2)\cdot (1/\tau)^{O(\log(1/\tau))}$. As a corollary, we give the first protocol for estimating stabilizerfidelity, a standard measure of magic for quantum states, to error $\epsilon$in $n^3 \mathrm{quasipoly}(1/\epsilon)$ time.</description><author>Sitan Chen, Weiyuan Gong, Qi Ye, Zhihan Zhang</author><pubDate>Tue, 13 Aug 2024 15:23:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06967v1</guid></item><item><title>DyG-Mamba: Continuous State Space Modeling on Dynamic Graphs</title><link>http://arxiv.org/abs/2408.06966v1</link><description>Dynamic graph learning aims to uncover evolutionary laws in real-worldsystems, enabling accurate social recommendation (link prediction) or earlydetection of cancer cells (classification). Inspired by the success of statespace models, e.g., Mamba, for efficiently capturing long-term dependencies inlanguage modeling, we propose DyG-Mamba, a new continuous state space model(SSM) for dynamic graph learning. Specifically, we first found that usinginputs as control signals for SSM is not suitable for continuous-time dynamicnetwork data with irregular sampling intervals, resulting in models beinginsensitive to time information and lacking generalization properties. Drawinginspiration from the Ebbinghaus forgetting curve, which suggests that memory ofpast events is strongly correlated with time intervals rather than specificdetails of the events themselves, we directly utilize irregular time spans ascontrol signals for SSM to achieve significant robustness and generalization.Through exhaustive experiments on 12 datasets for dynamic link prediction anddynamic node classification tasks, we found that DyG-Mamba achievesstate-of-the-art performance on most of the datasets, while also demonstratingsignificantly improved computation and memory efficiency.</description><author>Dongyuan Li, Shiyin Tan, Ying Zhang, Ming Jin, Shirui Pan, Manabu Okumura, Renhe Jiang</author><pubDate>Tue, 13 Aug 2024 15:21:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06966v1</guid></item><item><title>NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms</title><link>http://arxiv.org/abs/2402.12261v4</link><description>The performance of Large Language Models (LLMs) degrades from the temporaldrift between data used for model training and newer text seen duringinference. One understudied avenue of language change causing data drift is theemergence of neologisms -- new word forms -- over time. We create a diverseresource of recent English neologisms by using several popular collectionmethods. We analyze temporal drift using neologisms by comparing sentencescontaining new words with near-identical sentences that replace neologisms withexisting substitute words. Model performance is nearly halved in machinetranslation when a single neologism is introduced in a sentence. Motivated bythese results, we construct a benchmark to evaluate LLMs' ability to generalizeto neologisms with various natural language understanding tasks and modelperplexity. Models with later knowledge cutoff dates yield lower perplexitiesand perform better in downstream tasks. LLMs are also affected differentlybased on the linguistic origins of words, indicating that neologisms arecomplex for static LLMs to address. We will release our benchmark and code forreproducing our experiments.</description><author>Jonathan Zheng, Alan Ritter, Wei Xu</author><pubDate>Tue, 13 Aug 2024 15:20:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12261v4</guid></item><item><title>Measuring User Understanding in Dialogue-based XAI Systems</title><link>http://arxiv.org/abs/2408.06960v1</link><description>The field of eXplainable Artificial Intelligence (XAI) is increasinglyrecognizing the need to personalize and/or interactively adapt the explanationto better reflect users' explanation needs. While dialogue-based approaches toXAI have been proposed recently, the state-of-the-art in XAI is stillcharacterized by what we call one-shot, non-personalized and one-wayexplanations. In contrast, dialogue-based systems that can adapt explanationsthrough interaction with a user promise to be superior to GUI-based ordashboard explanations as they offer a more intuitive way of requestinginformation. In general, while interactive XAI systems are often evaluated interms of user satisfaction, there are limited studies that access user'sobjective model understanding. This is in particular the case fordialogue-based XAI approaches. In this paper, we close this gap by carrying outcontrolled experiments within a dialogue framework in which we measureunderstanding of users in three phases by asking them to simulate thepredictions of the model they are learning about. By this, we can quantify thelevel of (improved) understanding w.r.t. how the model works, comparing thestate prior, and after the interaction. We further analyze the data to revealpatterns of how the interaction between groups with high vs. low understandinggain differ. Overall, our work thus contributes to our understanding about theeffectiveness of XAI approaches.</description><author>Dimitry Mindlin, Amelie Sophie Robrecht, Michael Morasch, Philipp Cimiano</author><pubDate>Tue, 13 Aug 2024 15:17:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06960v1</guid></item><item><title>AuToMATo: A Parameter-Free Persistence-Based Clustering Algorithm</title><link>http://arxiv.org/abs/2408.06958v1</link><description>We present AuToMATo, a novel parameter-free clustering algorithm based onpersistent homology. AuToMATo combines the existing ToMATo clustering algorithmwith a bootstrapping procedure in order to separate significant peaks of anestimated density function from non-significant ones. We perform a thoroughcomparison of AuToMATo against many other state-of-the-art clusteringalgorithms. We find that not only that AuToMATo compares favorably againstother parameter-free clustering algorithms, but in many instances alsosignificantly outperforms even the best selection of parameters for otheralgorithms. AuToMATo is motivated by applications in topological data analysis,in particular the Mapper algorithm, where it is desirable to work with aparameter-free clustering algorithm. Indeed, we provide evidence that AuToMAToperforms well when used with Mapper. Finally, we provide an open-sourceimplementation of AuToMATo in Python that is fully compatible with thestandardscikit-learn architecture.</description><author>Marius Huber, Sara Kalisnik, Patrick Schnider</author><pubDate>Tue, 13 Aug 2024 15:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06958v1</guid></item><item><title>Neural Speech and Audio Coding</title><link>http://arxiv.org/abs/2408.06954v1</link><description>This paper explores the integration of model-based and data-driven approacheswithin the realm of neural speech and audio coding systems. It highlights thechallenges posed by the subjective evaluation processes of speech and audiocodecs and discusses the limitations of purely data-driven approaches, whichoften require inefficiently large architectures to match the performance ofmodel-based methods. The study presents hybrid systems as a viable solution,offering significant improvements to the performance of conventional codecsthrough meticulously chosen design enhancements. Specifically, it introduces aneural network-based signal enhancer designed to post-process existing codecs'output, along with the autoencoder-based end-to-end models and LPCNet--hybridsystems that combine linear predictive coding (LPC) with neural networks.Furthermore, the paper delves into predictive models operating within customfeature spaces (TF-Codec) or predefined transform domains (MDCTNet) andexamines the use of psychoacoustically calibrated loss functions to trainend-to-end neural audio codecs. Through these investigations, the paperdemonstrates the potential of hybrid systems to advance the field of speech andaudio coding by bridging the gap between traditional model-based approaches andmodern data-driven techniques.</description><author>Minje Kim, Jan Skoglund</author><pubDate>Tue, 13 Aug 2024 15:13:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06954v1</guid></item><item><title>Deepfake Media Forensics: State of the Art and Challenges Ahead</title><link>http://arxiv.org/abs/2408.00388v2</link><description>AI-generated synthetic media, also called Deepfakes, have significantlyinfluenced so many domains, from entertainment to cybersecurity. GenerativeAdversarial Networks (GANs) and Diffusion Models (DMs) are the main frameworksused to create Deepfakes, producing highly realistic yet fabricated content.While these technologies open up new creative possibilities, they also bringsubstantial ethical and security risks due to their potential misuse. The riseof such advanced media has led to the development of a cognitive bias known asImpostor Bias, where individuals doubt the authenticity of multimedia due tothe awareness of AI's capabilities. As a result, Deepfake detection has becomea vital area of research, focusing on identifying subtle inconsistencies andartifacts with machine learning techniques, especially Convolutional NeuralNetworks (CNNs). Research in forensic Deepfake technology encompasses five mainareas: detection, attribution and recognition, passive authentication,detection in realistic scenarios, and active authentication. This paper reviewsthe primary algorithms that address these challenges, examining theiradvantages, limitations, and future prospects.</description><author>Irene Amerini, Mauro Barni, Sebastiano Battiato, Paolo Bestagini, Giulia Boato, Tania Sari Bonaventura, Vittoria Bruni, Roberto Caldelli, Francesco De Natale, Rocco De Nicola, Luca Guarnera, Sara Mandelli, Gian Luca Marcialis, Marco Micheletto, Andrea Montibeller, Giulia Orru', Alessandro Ortis, Pericle Perazzo, Giovanni Puglisi, Davide Salvi, Stefano Tubaro, Claudia Melis Tonti, Massimo Villari, Domenico Vitulano</author><pubDate>Tue, 13 Aug 2024 15:10:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00388v2</guid></item><item><title>SE(3)-Hyena Operator for Scalable Equivariant Learning</title><link>http://arxiv.org/abs/2407.01049v2</link><description>Modeling global geometric context while maintaining equivariance is crucialfor accurate predictions in many fields such as biology, chemistry, or vision.Yet, this is challenging due to the computational demands of processinghigh-dimensional data at scale. Existing approaches such as equivariantself-attention or distance-based message passing, suffer from quadraticcomplexity with respect to sequence length, while localized methods sacrificeglobal information. Inspired by the recent success of state-space andlong-convolutional models, in this work, we introduce SE(3)-Hyena operator, anequivariant long-convolutional model based on the Hyena operator. TheSE(3)-Hyena captures global geometric context at sub-quadratic complexity whilemaintaining equivariance to rotations and translations. Evaluated onequivariant associative recall and n-body modeling, SE(3)-Hyena matches oroutperforms equivariant self-attention while requiring significantly lessmemory and computational resources for long sequences. Our model processes thegeometric context of 20k tokens x3.5 times faster than the equivarianttransformer and allows x175 longer a context within the same memory budget.</description><author>Artem Moskalev, Mangal Prakash, Rui Liao, Tommaso Mansi</author><pubDate>Tue, 13 Aug 2024 15:06:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01049v2</guid></item><item><title>FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks</title><link>http://arxiv.org/abs/2405.17034v2</link><description>Fairness-aware Graph Neural Networks (GNNs) often face a challengingtrade-off, where prioritizing fairness may require compromising utility. Inthis work, we re-examine fairness through the lens of spectral graph theory,aiming to reconcile fairness and utility within the framework of spectral graphlearning. We explore the correlation between sensitive features and spectrum inGNNs, using theoretical analysis to delineate the similarity between originalsensitive features and those after convolution under different spectra. Ouranalysis reveals a reduction in the impact of similarity when the eigenvectorsassociated with the largest magnitude eigenvalue exhibit directionalsimilarity. Based on these theoretical insights, we propose FUGNN, a novelspectral graph learning approach that harmonizes the conflict between fairnessand utility. FUGNN ensures algorithmic fairness and utility by truncating thespectrum and optimizing eigenvector distribution during the encoding process.The fairness-aware eigenvector selection reduces the impact of convolution onsensitive features while concurrently minimizing the sacrifice of utility.FUGNN further optimizes the distribution of eigenvectors through a transformerarchitecture. By incorporating the optimized spectrum into the graphconvolution network, FUGNN effectively learns node representations. Experimentson six real-world datasets demonstrate the superiority of FUGNN over baselinemethods. The codes are available at https://github.com/yushuowiki/FUGNN.</description><author>Renqiang Luo, Huafei Huang, Shuo Yu, Zhuoyang Han, Estrid He, Xiuzhen Zhang, Feng Xia</author><pubDate>Tue, 13 Aug 2024 15:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17034v2</guid></item><item><title>Heavy-Ball Momentum Accelerated Actor-Critic With Function Approximation</title><link>http://arxiv.org/abs/2408.06945v1</link><description>By using an parametric value function to replace the Monte-Carlo rollouts forvalue estimation, the actor-critic (AC) algorithms can reduce the variance ofstochastic policy gradient so that to improve the convergence rate. Whileexisting works mainly focus on analyzing convergence rate of AC algorithmsunder Markovian noise, the impacts of momentum on AC algorithms remain largelyunexplored. In this work, we first propose a heavy-ball momentum basedadvantage actor-critic (\mbox{HB-A2C}) algorithm by integrating the heavy-ballmomentum into the critic recursion that is parameterized by a linear function.When the sample trajectory follows a Markov decision process, we quantitativelycertify the acceleration capability of the proposed HB-A2C algorithm. Ourtheoretical results demonstrate that the proposed HB-A2C finds an$\epsilon$-approximate stationary point with $\oo{\epsilon^{-2}}$ iterationsfor reinforcement learning tasks with Markovian noise. Moreover, we also revealthe dependence of learning rates on the length of the sample trajectory. Bycarefully selecting the momentum factor of the critic recursion, the proposedHB-A2C can balance the errors introduced by the initialization and thestoschastic approximation.</description><author>Yanjie Dong, Haijun Zhang, Gang Wang, Shisheng Cui, Xiping Hu</author><pubDate>Tue, 13 Aug 2024 15:03:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06945v1</guid></item><item><title>Towards Holistic Disease Risk Prediction using Small Language Models</title><link>http://arxiv.org/abs/2408.06943v1</link><description>Data in the healthcare domain arise from a variety of sources and modalities,such as x-ray images, continuous measurements, and clinical notes. Medicalpractitioners integrate these diverse data types daily to make informed andaccurate decisions. With recent advancements in language models capable ofhandling multimodal data, it is a logical progression to apply these models tothe healthcare sector. In this work, we introduce a framework that connectssmall language models to multiple data sources, aiming to predict the risk ofvarious diseases simultaneously. Our experiments encompass 12 different taskswithin a multitask learning setup. Although our approach does not surpassstate-of-the-art methods specialized for single tasks, it demonstratescompetitive performance and underscores the potential of small language modelsfor multimodal reasoning in healthcare.</description><author>Liv Björkdahl, Oskar Pauli, Johan Östman, Chiara Ceccobello, Sara Lundell, Magnus Kjellberg</author><pubDate>Tue, 13 Aug 2024 15:01:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06943v1</guid></item><item><title>FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data</title><link>http://arxiv.org/abs/2408.06273v2</link><description>Large language models (LLMs) have demonstrated prowess in a wide range oftasks. However, many LLMs exhibit significant performance discrepancies betweenhigh- and low-resource languages. To mitigate this challenge, we presentFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy theneed of the research community for balanced and high-performing multilingualcapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, istrained from scratch on a meticulously balanced multilingual data repositorythat contains 600 billion tokens covering 43 natural languages and 16programming languages. In addition to the base model, we also develop twoinstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diversemultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refinedwith DPO on a preference dataset for enhanced alignment ability. Extensiveexperiments on a wide range of multilingual benchmarks demonstrate thecompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,BLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretabilityanalyses at both the neuron and representation level suggest that FuxiTranyu isable to learn consistent multilingual representations across differentlanguages. To promote further research into multilingual LLMs and their workingmechanisms, we release both the base and instruction-tuned FuxiTranyu modelstogether with 58 pretraining checkpoints at HuggingFace and Github.</description><author>Haoran Sun, Renren Jin, Shaoyang Xu, Leiyu Pan, Supryadi, Menglong Cui, Jiangcun Du, Yikun Lei, Lei Yang, Ling Shi, Juesi Xiao, Shaolin Zhu, Deyi Xiong</author><pubDate>Tue, 13 Aug 2024 14:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06273v2</guid></item><item><title>Learning Minimal Neural Specifications</title><link>http://arxiv.org/abs/2404.04662v3</link><description>Formal verification is only as good as the specification of a system, whichis also true for neural network verification. Existing specifications followthe paradigm of data as specification, where the local neighborhood around areference data point is considered correct or robust. While thesespecifications provide a fair testbed for assessing model robustness, they aretoo restrictive for verifying unseen test data-a challenging task withsignificant real-world implications. Recent work shows great promise through anew paradigm, neural representation as specification, which uses neuralactivation patterns (NAPs) for this purpose. However, it computes the mostrefined NAPs, which include many redundant neurons. In this paper, we study thefollowing problem: Given a neural network, find a minimal (general) NAPspecification that is sufficient for formal verification of the network'srobustness. Finding the minimal NAP specification not only expands verifiablebounds but also provides insights into which neurons contribute to the model'srobustness. To address this problem, we propose several exact and approximateapproaches. Our exact approaches leverage the verification tool to find minimalNAP specifications in either a deterministic or statistical manner. Whereas theapproximate methods efficiently estimate minimal NAPs using adversarialexamples and local gradients, without making calls to the verification tool.This allows us to inspect potential causal links between neurons and therobustness of state-of-the art neural networks, a task for which existingverification frameworks fail to scale. Our experimental results suggest thatminimal NAP specifications require much smaller fractions of neurons comparedto the most refined NAP specifications computed by previous work, yet they cansignificantly expand the verifiable boundaries to several orders of magnitudelarger.</description><author>Chuqin Geng, Zhaoyue Wang, Haolin Ye, Saifei Liao, Xujie Si</author><pubDate>Tue, 13 Aug 2024 14:56:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04662v3</guid></item><item><title>An Ensemble Score Filter for Tracking High-Dimensional Nonlinear Dynamical Systems</title><link>http://arxiv.org/abs/2309.00983v2</link><description>We propose an ensemble score filter (EnSF) for solving high-dimensionalnonlinear filtering problems with superior accuracy. A major drawback ofexisting filtering methods, e.g., particle filters or ensemble Kalman filters,is the low accuracy in handling high-dimensional and highly nonlinear problems.EnSF attacks this challenge by exploiting the score-based diffusion model,defined in a pseudo-temporal domain, to characterizing the evolution of thefiltering density. EnSF stores the information of the recursively updatedfiltering density function in the score function, instead of storing theinformation in a set of finite Monte Carlo samples (used in particle filtersand ensemble Kalman filters). Unlike existing diffusion models that trainneural networks to approximate the score function, we develop a training-freescore estimation that uses a mini-batch-based Monte Carlo estimator to directlyapproximate the score function at any pseudo-spatial-temporal location, whichprovides sufficient accuracy in solving high-dimensional nonlinear problems aswell as saves a tremendous amount of time spent on training neural networks.High-dimensional Lorenz-96 systems are used to demonstrate the performance ofour method. EnSF provides surprising performance, compared with thestate-of-the-art Local Ensemble Transform Kalman Filter method, in reliably andefficiently tracking extremely high-dimensional Lorenz systems (up to 1,000,000dimensions) with highly nonlinear observation processes.</description><author>Feng Bao, Zezhong Zhang, Guannan Zhang</author><pubDate>Tue, 13 Aug 2024 14:48:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00983v2</guid></item><item><title>DynaSeg: A Deep Dynamic Fusion Method for Unsupervised Image Segmentation Incorporating Feature Similarity and Spatial Continuity</title><link>http://arxiv.org/abs/2405.05477v3</link><description>Our work tackles the fundamental challenge of image segmentation in computervision, which is crucial for diverse applications. While supervised methodsdemonstrate proficiency, their reliance on extensive pixel-level annotationslimits scalability. We introduce DynaSeg, an innovative unsupervised imagesegmentation approach that overcomes the challenge of balancing featuresimilarity and spatial continuity without relying on extensive hyperparametertuning. Unlike traditional methods, DynaSeg employs a dynamic weighting schemethat automates parameter tuning, adapts flexibly to image characteristics, andfacilitates easy integration with other segmentation networks. By incorporatinga Silhouette Score Phase, DynaSeg prevents undersegmentation failures where thenumber of predicted clusters might converge to one. DynaSeg uses CNN-based andpre-trained ResNet feature extraction, making it computationally efficient andmore straightforward than other complex models. Experimental results showcasestate-of-the-art performance, achieving a 12.2% and 14.12% mIOU improvementover current unsupervised segmentation approaches on COCO-All and COCO-Stuffdatasets, respectively. We provide qualitative and quantitative results on fivebenchmark datasets, demonstrating the efficacy of the proposed approach.Code isavailable at https://github.com/RyersonMultimediaLab/DynaSeg</description><author>Boujemaa Guermazi, Naimul Khan</author><pubDate>Tue, 13 Aug 2024 14:44:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05477v3</guid></item><item><title>Prioritize Alignment in Dataset Distillation</title><link>http://arxiv.org/abs/2408.03360v2</link><description>Dataset Distillation aims to compress a large dataset into a significantlymore compact, synthetic one without compromising the performance of the trainedmodels. To achieve this, existing methods use the agent model to extractinformation from the target dataset and embed it into the distilled dataset.Consequently, the quality of extracted and embedded information determines thequality of the distilled dataset. In this work, we find that existing methodsintroduce misaligned information in both information extraction and embeddingstages. To alleviate this, we propose Prioritize Alignment in DatasetDistillation (PAD), which aligns information from the following twoperspectives. 1) We prune the target dataset according to the compressing ratioto filter the information that can be extracted by the agent model. 2) We useonly deep layers of the agent model to perform the distillation to avoidexcessively introducing low-level information. This simple strategy effectivelyfilters out misaligned information and brings non-trivial improvement formainstream matching-based distillation algorithms. Furthermore, built ontrajectory matching, \textbf{PAD} achieves remarkable improvements on variousbenchmarks, achieving state-of-the-art performance.</description><author>Zekai Li, Ziyao Guo, Wangbo Zhao, Tianle Zhang, Zhi-Qi Cheng, Samir Khaki, Kaipeng Zhang, Ahmad Sajedi, Konstantinos N Plataniotis, Kai Wang, Yang You</author><pubDate>Tue, 13 Aug 2024 14:39:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03360v2</guid></item><item><title>A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus</title><link>http://arxiv.org/abs/2405.11877v4</link><description>Natural language inference (NLI), the task of recognizing the entailmentrelationship in sentence pairs, is an actively studied topic serving as a proxyfor natural language understanding. Despite the relevance of the task inbuilding conversational agents and improving text classification, machinetranslation and other NLP tasks, to the best of our knowledge, there is nopublicly available NLI corpus for the Romanian language. To this end, weintroduce the first Romanian NLI corpus (RoNLI) comprising 58K trainingsentence pairs, which are obtained via distant supervision, and 6K validationand test sentence pairs, which are manually annotated with the correct labels.We conduct experiments with multiple machine learning methods based on distantlearning, ranging from shallow models based on word embeddings totransformer-based neural networks, to establish a set of competitive baselines.Furthermore, we improve on the best model by employing a new curriculumlearning strategy based on data cartography. Our dataset and code to reproducethe baselines are available at https://github.com/Eduard6421/RONLI.</description><author>Eduard Poesina, Cornelia Caragea, Radu Tudor Ionescu</author><pubDate>Tue, 13 Aug 2024 14:38:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11877v4</guid></item><item><title>The advantages of context specific language models: the case of the Erasmian Language Model</title><link>http://arxiv.org/abs/2408.06931v1</link><description>The current trend to improve language model performance seems to be based onscaling up with the number of parameters (e.g. the state of the art GPT4 modelhas approximately 1.7 trillion parameters) or the amount of training data fedinto the model. However this comes at significant costs in terms ofcomputational resources and energy costs that compromise the sustainability ofAI solutions, as well as risk relating to privacy and misuse. In this paper wepresent the Erasmian Language Model (ELM) a small context specific, 900 millionparameter model, pre-trained and fine-tuned by and for Erasmus UniversityRotterdam. We show how the model performs adequately in a classroom context foressay writing, and how it achieves superior performance in subjects that arepart of its context. This has implications for a wide range of institutions andorganizations, showing that context specific language models may be a viablealternative for resource constrained, privacy sensitive use cases.</description><author>João Gonçalves, Nick Jelicic, Michele Murgia, Evert Stamhuis</author><pubDate>Tue, 13 Aug 2024 14:34:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06931v1</guid></item><item><title>A Universal Flexible Near-sensor Neuromorphic Tactile System with Multi-threshold strategy for Pressure Characteristic Detection</title><link>http://arxiv.org/abs/2408.05846v2</link><description>Constructing the new generation information processing system by mimickingbiological nervous system is a feasible way for implement of high-efficientintelligent sensing device and bionic robot. However, most biological nervoussystem, especially the tactile system, have various powerful functions. This isa big challenge for bionic system design. Here we report a universal fullyflexible neuromorphic tactile perception system with strong compatibility and amultithreshold signal processing strategy. Like nervous system, signal in oursystem is transmitted as pulses and processed as threshold information. Forfeasibility verification, recognition of three different type pressure signals(continuous changing signal, Morse code signal and symbol pattern) is testedrespectively. Our system can output trend of these signals accurately and havea high accuracy in the recognition of symbol pattern and Morse code. Comparingto conventional system, consumption of our system significantly decreases in asame recognition task. Meanwhile, we give the detail introduction anddemonstration of our system universality.</description><author>Jialin Liu, Diansheng Liao</author><pubDate>Tue, 13 Aug 2024 14:33:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05846v2</guid></item><item><title>Diagnosis extraction from unstructured Dutch echocardiogram reports using span- and document-level characteristic classification</title><link>http://arxiv.org/abs/2408.06930v1</link><description>Clinical machine learning research and AI driven clinical decision supportmodels rely on clinically accurate labels. Manually extracting these labelswith the help of clinical specialists is often time-consuming and expensive.This study tests the feasibility of automatic span- and document-leveldiagnosis extraction from unstructured Dutch echocardiogram reports. We included 115,692 unstructured echocardiogram reports from the UMCU a largeuniversity hospital in the Netherlands. A randomly selected subset was manuallyannotated for the occurrence and severity of eleven commonly described cardiaccharacteristics. We developed and tested several automatic labelling techniquesat both span and document levels, using weighted and macro F1-score, precision,and recall for performance evaluation. We compared the performance of spanlabelling against document labelling methods, which included both directdocument classifiers and indirect document classifiers that rely on spanclassification results. The SpanCategorizer and MedRoBERTa.nl models outperformed all other span anddocument classifiers, respectively. The weighted F1-score varied betweencharacteristics, ranging from 0.60 to 0.93 in SpanCategorizer and 0.96 to 0.98in MedRoBERTa.nl. Direct document classification was superior to indirectdocument classification using span classifiers. SetFit achieved competitivedocument classification performance using only 10\% of the training data.Utilizing a reduced label set yielded near-perfect document classificationresults. We recommend using our published SpanCategorizer and MedRoBERTa.nl models forspan- and document-level diagnosis extraction from Dutch echocardiographyreports. For settings with limited training data, SetFit may be a promisingalternative for document classification.</description><author>Bauke Arends, Melle Vessies, Dirk van Osch, Arco Teske, Pim van der Harst, René van Es, Bram van Es</author><pubDate>Tue, 13 Aug 2024 14:33:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06930v1</guid></item><item><title>Evaluating Cultural Adaptability of a Large Language Model via Simulation of Synthetic Personas</title><link>http://arxiv.org/abs/2408.06929v1</link><description>The success of Large Language Models (LLMs) in multicultural environmentshinges on their ability to understand users' diverse cultural backgrounds. Wemeasure this capability by having an LLM simulate human profiles representingvarious nationalities within the scope of a questionnaire-style psychologicalexperiment. Specifically, we employ GPT-3.5 to reproduce reactions topersuasive news articles of 7,286 participants from 15 countries; comparing theresults with a dataset of real participants sharing the same demographictraits. Our analysis shows that specifying a person's country of residenceimproves GPT-3.5's alignment with their responses. In contrast, using nativelanguage prompting introduces shifts that significantly reduce overallalignment, with some languages particularly impairing performance. Thesefindings suggest that while direct nationality information enhances the model'scultural adaptability, native language cues do not reliably improve simulationfidelity and can detract from the model's effectiveness.</description><author>Louis Kwok, Michal Bravansky, Lewis D. Griffin</author><pubDate>Tue, 13 Aug 2024 14:32:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06929v1</guid></item><item><title>Regularizing Self-supervised 3D Scene Flows with Surface Awareness and Cyclic Consistency</title><link>http://arxiv.org/abs/2312.08879v3</link><description>Learning without supervision how to predict 3D scene flows from point cloudsis essential to many perception systems. We propose a novel learning frameworkfor this task which improves the necessary regularization. Relying on theassumption that scene elements are mostly rigid, current smoothness losses arebuilt on the definition of "rigid clusters" in the input point clouds. Thedefinition of these clusters is challenging and has a significant impact on thequality of predicted flows. We introduce two new consistency losses thatenlarge clusters while preventing them from spreading over distinct objects. Inparticular, we enforce \emph{temporal} consistency with a forward-backwardcyclic loss and \emph{spatial} consistency by considering surface orientationsimilarity in addition to spatial proximity. The proposed losses aremodel-independent and can thus be used in a plug-and-play fashion tosignificantly improve the performance of existing models, as demonstrated ontwo most widely used architectures. We also showcase the effectiveness andgeneralization capability of our framework on four standard sensor-uniquedriving datasets, achieving state-of-the-art performance in 3D scene flowestimation. Our codes are available on https://github.com/ctu-vras/sac-flow.</description><author>Patrik Vacek, David Hurych, Karel Zimmermann, Patrick Perez, Tomas Svoboda</author><pubDate>Tue, 13 Aug 2024 14:32:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08879v3</guid></item><item><title>Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class Feature Compensator</title><link>http://arxiv.org/abs/2408.06927v1</link><description>Dataset distillation has emerged as a technique aiming to condenseinformative features from large, natural datasets into a compact and syntheticform. While recent advancements have refined this technique, its performance isbottlenecked by the prevailing class-specific synthesis paradigm. Under thisparadigm, synthetic data is optimized exclusively for a pre-assigned one-hotlabel, creating an implicit class barrier in feature condensation. This leadsto inefficient utilization of the distillation budget and oversight ofinter-class feature distributions, which ultimately limits the effectivenessand efficiency, as demonstrated in our analysis. To overcome these constraints, this paper presents the Inter-class FeatureCompensator (INFER), an innovative distillation approach that transcends theclass-specific data-label framework widely utilized in current datasetdistillation methods. Specifically, INFER leverages a Universal FeatureCompensator (UFC) to enhance feature integration across classes, enabling thegeneration of multiple additional synthetic instances from a single UFC input.This significantly improves the efficiency of the distillation budget. Moreover, INFER enriches inter-class interactions during the distillation,thereby enhancing the effectiveness and generalizability of the distilled data.By allowing for the linear interpolation of labels similar to those in theoriginal dataset, INFER meticulously optimizes the synthetic data anddramatically reduces the size of soft labels in the synthetic dataset to almostzero, establishing a new benchmark for efficiency and effectiveness in datasetdistillation.</description><author>Xin Zhang, Jiawei Du, Ping Liu, Joey Tianyi Zhou</author><pubDate>Tue, 13 Aug 2024 14:29:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06927v1</guid></item><item><title>Continual Driving Policy Optimization with Closed-Loop Individualized Curricula</title><link>http://arxiv.org/abs/2309.14209v4</link><description>The safety of autonomous vehicles (AV) has been a long-standing top concern,stemming from the absence of rare and safety-critical scenarios in thelong-tail naturalistic driving distribution. To tackle this challenge, a surgeof research in scenario-based autonomous driving has emerged, with a focus ongenerating high-risk driving scenarios and applying them to conductsafety-critical testing of AV models. However, limited work has been exploredon the reuse of these extensive scenarios to iteratively improve AV models.Moreover, it remains intractable and challenging to filter through giganticscenario libraries collected from other AV models with distinct behaviors,attempting to extract transferable information for current AV improvement.Therefore, we develop a continual driving policy optimization frameworkfeaturing Closed-Loop Individualized Curricula (CLIC), which we factorize intoa set of standardized sub-modules for flexible implementation choices: AVEvaluation, Scenario Selection, and AV Training. CLIC frames AV Evaluation as acollision prediction task, where it estimates the chance of AV failures inthese scenarios at each iteration. Subsequently, by re-sampling from historicalscenarios based on these failure probabilities, CLIC tailors individualizedcurricula for downstream training, aligning them with the evaluated capabilityof AV. Accordingly, CLIC not only maximizes the utilization of the vastpre-collected scenario library for closed-loop driving policy optimization butalso facilitates AV improvement by individualizing its training with morechallenging cases out of those poorly organized scenarios. Experimental resultsclearly indicate that CLIC surpasses other curriculum-based trainingstrategies, showing substantial improvement in managing risky scenarios, whilestill maintaining proficiency in handling simpler cases.</description><author>Haoyi Niu, Yizhou Xu, Xingjian Jiang, Jianming Hu</author><pubDate>Tue, 13 Aug 2024 14:27:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14209v4</guid></item><item><title>SceneGPT: A Language Model for 3D Scene Understanding</title><link>http://arxiv.org/abs/2408.06926v1</link><description>Building models that can understand and reason about 3D scenes is difficultowing to the lack of data sources for 3D supervised training and large-scaletraining regimes. In this work we ask - How can the knowledge in a pre-trainedlanguage model be leveraged for 3D scene understanding without any 3Dpre-training. The aim of this work is to establish whether pre-trained LLMspossess priors/knowledge required for reasoning in 3D space and how can weprompt them such that they can be used for general purpose spatial reasoningand object understanding in 3D. To this end, we present SceneGPT, an LLM basedscene understanding system which can perform 3D spatial reasoning withouttraining or explicit 3D supervision. The key components of our framework are -1) a 3D scene graph, that serves as scene representation, encoding the objectsin the scene and their spatial relationships 2) a pre-trained LLM that can beadapted with in context learning for 3D spatial reasoning. We evaluate ourframework qualitatively on object and scene understanding tasks includingobject semantics, physical properties and affordances (object-level) andspatial understanding (scene-level).</description><author>Shivam Chandhok</author><pubDate>Tue, 13 Aug 2024 14:26:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06926v1</guid></item><item><title>Let-It-Flow: Simultaneous Optimization of 3D Flow and Object Clustering</title><link>http://arxiv.org/abs/2404.08363v3</link><description>We study the problem of self-supervised 3D scene flow estimation from reallarge-scale raw point cloud sequences, which is crucial to various tasks liketrajectory prediction or instance segmentation. In the absence of ground truthscene flow labels, contemporary approaches concentrate on deducing optimizingflow across sequential pairs of point clouds by incorporating structure basedregularization on flow and object rigidity. The rigid objects are estimated bya variety of 3D spatial clustering methods. While state-of-the-art methodssuccessfully capture overall scene motion using the Neural Prior structure,they encounter challenges in discerning multi-object motions. We identified thestructural constraints and the use of large and strict rigid clusters as themain pitfall of the current approaches and we propose a novel clusteringapproach that allows for combination of overlapping soft clusters as well asnon-overlapping rigid clusters representation. Flow is then jointly estimatedwith progressively growing non-overlapping rigid clusters together with fixedsize overlapping soft clusters. We evaluate our method on multiple datasetswith LiDAR point clouds, demonstrating the superior performance over theself-supervised baselines reaching new state of the art results. Our methodespecially excels in resolving flow in complicated dynamic scenes with multipleindependently moving objects close to each other which includes pedestrians,cyclists and other vulnerable road users. Our codes are publicly available onhttps://github.com/ctu-vras/let-it-flow.</description><author>Patrik Vacek, David Hurych, Tomáš Svoboda, Karel Zimmermann</author><pubDate>Tue, 13 Aug 2024 14:24:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08363v3</guid></item><item><title>Improved Random Features for Dot Product Kernels</title><link>http://arxiv.org/abs/2201.08712v4</link><description>Dot product kernels, such as polynomial and exponential (softmax) kernels,are among the most widely used kernels in machine learning, as they enablemodeling the interactions between input features, which is crucial inapplications like computer vision, natural language processing, and recommendersystems. We make several novel contributions for improving the efficiency ofrandom feature approximations for dot product kernels, to make these kernelsmore useful in large scale learning. First, we present a generalization ofexisting random feature approximations for polynomial kernels, such asRademacher and Gaussian sketches and TensorSRHT, using complex-valued randomfeatures. We show empirically that the use of complex features cansignificantly reduce the variances of these approximations. Second, we providea theoretical analysis for understanding the factors affecting the efficiencyof various random feature approximations, by deriving closed-form expressionsfor their variances. These variance formulas elucidate conditions under whichcertain approximations (e.g., TensorSRHT) achieve lower variances than others(e.g., Rademacher sketches), and conditions under which the use of complexfeatures leads to lower variances than real features. Third, by using thesevariance formulas, which can be evaluated in practice, we develop a data-drivenoptimization approach to improve random feature approximations for general dotproduct kernels, which is also applicable to the Gaussian kernel. We describethe improvements brought by these contributions with extensive experiments on avariety of tasks and datasets.</description><author>Jonas Wacker, Motonobu Kanagawa, Maurizio Filippone</author><pubDate>Tue, 13 Aug 2024 14:22:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.08712v4</guid></item><item><title>Temporal Variability and Multi-Viewed Self-Supervised Representations to Tackle the ASVspoof5 Deepfake Challenge</title><link>http://arxiv.org/abs/2408.06922v1</link><description>ASVspoof5, the fifth edition of the ASVspoof series, is one of the largestglobal audio security challenges. It aims to advance the development ofcountermeasure (CM) to discriminate bonafide and spoofed speech utterances. Inthis paper, we focus on addressing the problem of open-domain audio deepfakedetection, which corresponds directly to the ASVspoof5 Track1 open condition.At first, we comprehensively investigate various CM on ASVspoof5, includingdata expansion, data augmentation, and self-supervised learning (SSL) features.Due to the high-frequency gaps characteristic of the ASVspoof5 dataset, weintroduce Frequency Mask, a data augmentation method that masks specificfrequency bands to improve CM robustness. Combining various scale of temporalinformation with multiple SSL features, our experiments achieved a minDCF of0.0158 and an EER of 0.55% on the ASVspoof 5 Track 1 evaluation progress set.</description><author>Yuankun Xie, Xiaopeng Wang, Zhiyong Wang, Ruibo Fu, Zhengqi Wen, Haonan Cheng, Long Ye</author><pubDate>Tue, 13 Aug 2024 14:15:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06922v1</guid></item><item><title>Multi-Agent Continuous Control with Generative Flow Networks</title><link>http://arxiv.org/abs/2408.06920v1</link><description>Generative Flow Networks (GFlowNets) aim to generate diverse trajectoriesfrom a distribution in which the final states of the trajectories areproportional to the reward, serving as a powerful alternative to reinforcementlearning for exploratory control tasks. However, the individual-flow matchingconstraint in GFlowNets limits their applications for multi-agent systems,especially continuous joint-control problems. In this paper, we propose a novelMulti-Agent generative Continuous Flow Networks (MACFN) method to enablemultiple agents to perform cooperative exploration for various compositionalcontinuous objects. Technically, MACFN trains decentralizedindividual-flow-based policies in a centralized global-flow-based matchingfashion. During centralized training, MACFN introduces a continuous flowdecomposition network to deduce the flow contributions of each agent in thepresence of only global rewards. Then agents can deliver actions solely basedon their assigned local flow in a decentralized way, forming a joint policydistribution proportional to the rewards. To guarantee the expressiveness ofcontinuous flow decomposition, we theoretically derive a consistency conditionon the decomposition network. Experimental results demonstrate that theproposed method yields results superior to the state-of-the-art counterpartsand better exploration capability. Our code is available athttps://github.com/isluoshuang/MACFN.</description><author>Shuang Luo, Yinchuan Li, Shunyu Liu, Xu Zhang, Yunfeng Shao, Chao Wu</author><pubDate>Tue, 13 Aug 2024 14:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06920v1</guid></item></channel></rss>