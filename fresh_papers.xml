<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 27 Sep 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Generating Visual Scenes from Touch</title><link>http://arxiv.org/abs/2309.15117v1</link><description>An emerging line of work has sought to generate plausible imagery from touch.Existing approaches, however, tackle only narrow aspects of the visuo-tactilesynthesis problem, and lag significantly behind the quality of cross-modalsynthesis methods in other domains. We draw on recent advances in latentdiffusion to create a model for synthesizing images from tactile signals (andvice versa) and apply it to a number of visuo-tactile synthesis tasks. Usingthis model, we significantly outperform prior work on the tactile-drivenstylization problem, i.e., manipulating an image to match a touch signal, andwe are the first to successfully generate images from touch without additionalsources of information about the scene. We also successfully use our model toaddress two novel synthesis problems: generating images that do not contain thetouch sensor or the hand holding it, and estimating an image's shading from itsreflectance and touch.</description><author>Fengyu Yang, Jiacheng Zhang, Andrew Owens</author><pubDate>Tue, 26 Sep 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15117v1</guid></item><item><title>InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition</title><link>http://arxiv.org/abs/2309.15112v1</link><description>We propose InternLM-XComposer, a vision-language large model that enablesadvanced image-text comprehension and composition. The innovative nature of ourmodel is highlighted by three appealing properties: 1) Interleaved Text-ImageComposition: InternLM-XComposer can effortlessly generate coherent andcontextual articles that seamlessly integrate images, providing a more engagingand immersive reading experience. Simply provide a title, and our system willgenerate the corresponding manuscript. It can intelligently identify the areasin the text where images would enhance the content and automatically insert themost appropriate visual candidates. 2) Comprehension with Rich MultilingualKnowledge: The text-image comprehension is empowered by training on extensivemulti-modal multilingual concepts with carefully crafted strategies, resultingin a deep understanding of visual content. 3) State-of-the-art Performance: Ourmodel consistently achieves state-of-the-art results across various mainstreambenchmarks for vision-language foundational models, including MME Benchmark,MMBench, MMBench-CN, Seed-Bench, and CCBench (Chinese Cultural Benchmark).Collectively, InternLM-XComposer seamlessly blends advanced text-imagecomprehension and composition, revolutionizing vision-language interaction andoffering new insights and opportunities. The InternLM-XComposer models with 7Bparameters are publicly available athttps://github.com/InternLM/InternLM-XComposer.</description><author>Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang</author><pubDate>Tue, 26 Sep 2023 18:58:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15112v1</guid></item><item><title>SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem</title><link>http://arxiv.org/abs/2309.15111v1</link><description>In this work, we consider the optimization process of minibatch stochasticgradient descent (SGD) on a 2-layer neural network with data separated by aquadratic ground truth function. We prove that with data drawn from the$d$-dimensional Boolean hypercube labeled by the quadratic ``XOR'' function $y= -x_ix_j$, it is possible to train to a population error $o(1)$ with $d\:\text{polylog}(d)$ samples. Our result considers simultaneously training bothlayers of the two-layer-neural network with ReLU activations via standardminibatch SGD on the logistic loss. To our knowledge, this work is the first togive a sample complexity of $\tilde{O}(d)$ for efficiently learning the XORfunction on isotropic data on a standard neural network with standard training.Our main technique is showing that the network evolves in two phases: a$\textit{signal-finding}$ phase where the network is small and many of theneurons evolve independently to find features, and a $\textit{signal-heavy}$phase, where SGD maintains and balances the features. We leverage thesimultaneous training of the layers to show that it is sufficient for only asmall fraction of the neurons to learn features, since those neurons will beamplified by the simultaneous growth of their second layer weights.</description><author>Margalit Glasgow</author><pubDate>Tue, 26 Sep 2023 18:57:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15111v1</guid></item><item><title>Doduo: Learning Dense Visual Correspondence from Unsupervised Semantic-Aware Flow</title><link>http://arxiv.org/abs/2309.15110v1</link><description>Dense visual correspondence plays a vital role in robotic perception. Thiswork focuses on establishing the dense correspondence between a pair of imagesthat captures dynamic scenes undergoing substantial transformations. Weintroduce Doduo to learn general dense visual correspondence from in-the-wildimages and videos without ground truth supervision. Given a pair of images, itestimates the dense flow field encoding the displacement of each pixel in oneimage to its corresponding pixel in the other image. Doduo uses flow-basedwarping to acquire supervisory signals for the training. Incorporating semanticpriors with self-supervised flow training, Doduo produces accurate densecorrespondence robust to the dynamic changes of the scenes. Trained on anin-the-wild video dataset, Doduo illustrates superior performance onpoint-level correspondence estimation over existing self-supervisedcorrespondence learning baselines. We also apply Doduo to articulationestimation and zero-shot goal-conditioned manipulation, underlining itspractical applications in robotics. Code and additional visualizations areavailable at https://ut-austin-rpl.github.io/Doduo</description><author>Zhenyu Jiang, Hanwen Jiang, Yuke Zhu</author><pubDate>Tue, 26 Sep 2023 18:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15110v1</guid></item><item><title>DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal Knowledge Distillation</title><link>http://arxiv.org/abs/2309.15109v1</link><description>3D perception based on the representations learned from multi-camerabird's-eye-view (BEV) is trending as cameras are cost-effective for massproduction in autonomous driving industry. However, there exists a distinctperformance gap between multi-camera BEV and LiDAR based 3D object detection.One key reason is that LiDAR captures accurate depth and other geometrymeasurements, while it is notoriously challenging to infer such 3D informationfrom merely image input. In this work, we propose to boost the representationlearning of a multi-camera BEV based student detector by training it to imitatethe features of a well-trained LiDAR based teacher detector. We proposeeffective balancing strategy to enforce the student to focus on learning thecrucial features from the teacher, and generalize knowledge transfer tomulti-scale layers with temporal fusion. We conduct extensive evaluations onmultiple representative models of multi-camera BEV. Experiments reveal that ourapproach renders significant improvement over the student models, leading tothe state-of-the-art performance on the popular benchmark nuScenes.</description><author>Zeyu Wang, Dingwen Li, Chenxu Luo, Cihang Xie, Xiaodong Yang</author><pubDate>Tue, 26 Sep 2023 18:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15109v1</guid></item><item><title>LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models</title><link>http://arxiv.org/abs/2309.15103v1</link><description>This work aims to learn a high-quality text-to-video (T2V) generative modelby leveraging a pre-trained text-to-image (T2I) model as a basis. It is ahighly desirable yet challenging task to simultaneously a) accomplish thesynthesis of visually realistic and temporally coherent videos while b)preserving the strong creative generation nature of the pre-trained T2I model.To this end, we propose LaVie, an integrated video generation framework thatoperates on cascaded video latent diffusion models, comprising a base T2Vmodel, a temporal interpolation model, and a video super-resolution model. Ourkey insights are two-fold: 1) We reveal that the incorporation of simpletemporal self-attentions, coupled with rotary positional encoding, adequatelycaptures the temporal correlations inherent in video data. 2) Additionally, wevalidate that the process of joint image-video fine-tuning plays a pivotal rolein producing high-quality and creative outcomes. To enhance the performance ofLaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M,consisting of 25 million text-video pairs that prioritize quality, diversity,and aesthetic appeal. Extensive experiments demonstrate that LaVie achievesstate-of-the-art performance both quantitatively and qualitatively.Furthermore, we showcase the versatility of pre-trained LaVie models in variouslong video generation and personalized video synthesis applications.</description><author>Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yuming Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua Lin, Yu Qiao, Ziwei Liu</author><pubDate>Tue, 26 Sep 2023 18:52:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15103v1</guid></item><item><title>Binarized Spectral Compressive Imaging</title><link>http://arxiv.org/abs/2305.10299v2</link><description>Existing deep learning models for hyperspectral image (HSI) reconstructionachieve good performance but require powerful hardwares with enormous memoryand computational resources. Consequently, these methods can hardly be deployedon resource-limited mobile devices. In this paper, we propose a novel method,Binarized Spectral-Redistribution Network (BiSRNet), for efficient andpractical HSI restoration from compressed measurement in snapshot compressiveimaging (SCI) systems. Firstly, we redesign a compact and easy-to-deploy basemodel to be binarized. Then we present the basic unit, BinarizedSpectral-Redistribution Convolution (BiSR-Conv). BiSR-Conv can adaptivelyredistribute the HSI representations before binarizing activation and uses ascalable hyperbolic tangent function to closer approximate the Sign function inbackpropagation. Based on our BiSR-Conv, we customize four binarizedconvolutional modules to address the dimension mismatch and propagatefull-precision information throughout the whole network. Finally, our BiSRNetis derived by using the proposed techniques to binarize the base model.Comprehensive quantitative and qualitative experiments manifest that ourproposed BiSRNet outperforms state-of-the-art binarization methods and achievescomparable performance with full-precision algorithms. Code and models arepublicly available at https://github.com/caiyuanhao1998/BiSCI andhttps://github.com/caiyuanhao1998/MST</description><author>Yuanhao Cai, Yuxin Zheng, Jing Lin, Xin Yuan, Yulun Zhang, Haoqian Wang</author><pubDate>Tue, 26 Sep 2023 18:50:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10299v2</guid></item><item><title>Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models</title><link>http://arxiv.org/abs/2309.15098v1</link><description>We investigate the internal behavior of Transformer-based Large LanguageModels (LLMs) when they generate factually incorrect text. We propose modelingfactual queries as Constraint Satisfaction Problems and use this framework toinvestigate how the model interacts internally with factual constraints.Specifically, we discover a strong positive relation between the model'sattention to constraint tokens and the factual accuracy of its responses. Inour curated suite of 11 datasets with over 40,000 prompts, we study the task ofpredicting factual errors with the Llama-2 family across all scales (7B, 13B,70B). We propose SAT Probe, a method probing self-attention patterns, that canpredict constraint satisfaction and factual errors, and allows early erroridentification. The approach and findings demonstrate how using the mechanisticunderstanding of factuality in LLMs can enhance reliability.</description><author>Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, Besmira Nushi</author><pubDate>Tue, 26 Sep 2023 18:48:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15098v1</guid></item><item><title>Case Study: Ensemble Decision-Based Annotation of Unconstrained Real Estate Images</title><link>http://arxiv.org/abs/2309.15097v1</link><description>We describe a proof-of-concept for annotating real estate images using simpleiterative rule-based semi-supervised learning. In this study, we have gainedimportant insights into the content characteristics and uniqueness ofindividual image classes as well as essential requirements for a practicalimplementation.</description><author>Miroslav Despotovic, Zedong Zhang, Eric Stumpe, Matthias Zeppelzauer</author><pubDate>Tue, 26 Sep 2023 18:43:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15097v1</guid></item><item><title>MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods</title><link>http://arxiv.org/abs/2309.10966v2</link><description>Recent research in decoding methods for Natural Language Generation (NLG)tasks has shown that MAP decoding is not optimal, because model probabilitiesdo not always align with human preferences. Stronger decoding methods,including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR)decoding, have since been proposed to mitigate the model-perplexity-vs-qualitymismatch. While these decoding methods achieve state-of-the-art performance,they are prohibitively expensive to compute. In this work, we propose MBRfinetuning and QE finetuning which distill the quality gains from thesedecoding methods at training time, while using an efficient decoding algorithmat inference time. Using the canonical NLG task of Neural Machine Translation(NMT), we show that even with self-training, these finetuning methodssignificantly outperform the base model. Moreover, when using an external LLMas a teacher model, these finetuning methods outperform finetuning onhuman-generated references. These findings suggest new ways to leveragemonolingual data to achieve improvements in model quality that are on par with,or even exceed, improvements from human-curated data, while maintaining maximumefficiency during decoding.</description><author>Mara Finkelstein, Markus Freitag</author><pubDate>Tue, 26 Sep 2023 18:43:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10966v2</guid></item><item><title>Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs</title><link>http://arxiv.org/abs/2309.15096v1</link><description>Recently, theoretical analyses of deep neural networks have broadly focusedon two directions: 1) Providing insight into neural network training by SGD inthe limit of infinite hidden-layer width and infinitesimally small learningrate (also known as gradient flow) via the Neural Tangent Kernel (NTK), and 2)Globally optimizing the regularized training objective via cone-constrainedconvex reformulations of ReLU networks. The latter research direction alsoyielded an alternative formulation of the ReLU network, called a gated ReLUnetwork, that is globally optimizable via efficient unconstrained convexprograms. In this work, we interpret the convex program for this gated ReLUnetwork as a Multiple Kernel Learning (MKL) model with a weighted data maskingfeature map and establish a connection to the NTK. Specifically, we show thatfor a particular choice of mask weights that do not depend on the learningtargets, this kernel is equivalent to the NTK of the gated ReLU network on thetraining data. A consequence of this lack of dependence on the targets is thatthe NTK cannot perform better than the optimal MKL kernel on the training set.By using iterative reweighting, we improve the weights induced by the NTK toobtain the optimal MKL kernel which is equivalent to the solution of the exactconvex reformulation of the gated ReLU network. We also provide severalnumerical simulations corroborating our theory. Additionally, we provide ananalysis of the prediction error of the resulting optimal kernel viaconsistency results for the group lasso.</description><author>Rajat Vadiraj Dwaraknath, Tolga Ergen, Mert Pilanci</author><pubDate>Tue, 26 Sep 2023 18:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15096v1</guid></item><item><title>Identifying Simulation Model Through Alternative Techniques for a Medical Device Assembly Process</title><link>http://arxiv.org/abs/2309.15094v1</link><description>This scientific paper explores two distinct approaches for identifying andapproximating the simulation model, particularly in the context of the snapprocess crucial to medical device assembly. Simulation models play a pivotalrole in providing engineers with insights into industrial processes, enablingexperimentation and troubleshooting before physical assembly. However, theircomplexity often results in time-consuming computations. To mitigate this complexity, we present two distinct methods for identifyingsimulation models: one utilizing Spline functions and the other harnessingMachine Learning (ML) models. Our goal is to create adaptable models thataccurately represent the snap process and can accommodate diverse scenarios.Such models hold promise for enhancing process understanding and aiding indecision-making, especially when data availability is limited.</description><author>Fatemeh Kakavandi</author><pubDate>Tue, 26 Sep 2023 18:40:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15094v1</guid></item><item><title>E2EG: End-to-End Node Classification Using Graph Topology and Text-based Node Attributes</title><link>http://arxiv.org/abs/2208.04609v2</link><description>Node classification utilizing text-based node attributes has many real-worldapplications, ranging from prediction of paper topics in academic citationgraphs to classification of user characteristics in social media networks.State-of-the-art node classification frameworks, such as GIANT, use a two-stagepipeline: first embedding the text attributes of graph nodes then feeding theresulting embeddings into a node classification model. In this paper, weeliminate these two stages and develop an end-to-end node classification modelthat builds upon GIANT, called End-to-End-GIANT (E2EG). The tandem utilizationof a main and an auxiliary classification objectives in our approach results ina more robust model, enabling the BERT backbone to be switched out for adistilled encoder with a 25% - 40% reduction in the number of parameters.Moreover, the model's end-to-end nature increases ease of use, as it avoids theneed of chaining multiple models for node classification. Compared to aGIANT+MLP baseline on the ogbn-arxiv and ogbn-products datasets, E2EG obtainsslightly better accuracy in the transductive setting (+0.5%), while reducingmodel training time by up to 40%. Our model is also applicable in the inductivesetting, outperforming GIANT+MLP by up to +2.23%.</description><author>Tu Anh Dinh, Jeroen den Boef, Joran Cornelisse, Paul Groth</author><pubDate>Tue, 26 Sep 2023 18:39:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.04609v2</guid></item><item><title>VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning</title><link>http://arxiv.org/abs/2309.15091v1</link><description>Although recent text-to-video (T2V) generation methods have seen significantadvancements, most of these works focus on producing short video clips of asingle event with a single background (i.e., single-scene videos). Meanwhile,recent large language models (LLMs) have demonstrated their capability ingenerating layouts and programs to control downstream visual modules such asimage generation models. This raises an important question: can we leverage theknowledge embedded in these LLMs for temporally consistent long videogeneration? In this paper, we propose VideoDirectorGPT, a novel framework forconsistent multi-scene video generation that uses the knowledge of LLMs forvideo content planning and grounded video generation. Specifically, given asingle text prompt, we first ask our video planner LLM (GPT-4) to expand itinto a 'video plan', which involves generating the scene descriptions, theentities with their respective layouts, the background for each scene, andconsistency groupings of the entities and backgrounds. Next, guided by thisoutput from the video planner, our video generator, Layout2Vid, has explicitcontrol over spatial layouts and can maintain temporal consistency ofentities/backgrounds across scenes, while only trained with image-levelannotations. Our experiments demonstrate that VideoDirectorGPT frameworksubstantially improves layout and movement control in both single- andmulti-scene video generation and can generate multi-scene videos with visualconsistency across scenes, while achieving competitive performance with SOTAsin open-domain single-scene T2V generation. We also demonstrate that ourframework can dynamically control the strength for layout guidance and can alsogenerate videos with user-provided images. We hope our framework can inspirefuture work on better integrating the planning ability of LLMs into consistentlong video generation.</description><author>Han Lin, Abhay Zala, Jaemin Cho, Mohit Bansal</author><pubDate>Tue, 26 Sep 2023 18:36:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15091v1</guid></item><item><title>Single Biological Neurons as Temporally Precise Spatio-Temporal Pattern Recognizers</title><link>http://arxiv.org/abs/2309.15090v1</link><description>This PhD thesis is focused on the central idea that single neurons in thebrain should be regarded as temporally precise and highly complexspatio-temporal pattern recognizers. This is opposed to the prevalent view ofbiological neurons as simple and mainly spatial pattern recognizers by mostneuroscientists today. In this thesis, I will attempt to demonstrate that thisis an important distinction, predominantly because the above-mentionedcomputational properties of single neurons have far-reaching implications withrespect to the various brain circuits that neurons compose, and on howinformation is encoded by neuronal activity in the brain. Namely, that theseparticular "low-level" details at the single neuron level have substantialsystem-wide ramifications. In the introduction we will highlight the maincomponents that comprise a neural microcircuit that can perform usefulcomputations and illustrate the inter-dependence of these components from asystem perspective. In chapter 1 we discuss the great complexity of thespatio-temporal input-output relationship of cortical neurons that are theresult of morphological structure and biophysical properties of the neuron. Inchapter 2 we demonstrate that single neurons can generate temporally preciseoutput patterns in response to specific spatio-temporal input patterns with avery simple biologically plausible learning rule. In chapter 3, we use thedifferentiable deep network analog of a realistic cortical neuron as a tool toapproximate the gradient of the output of the neuron with respect to its inputand use this capability in an attempt to teach the neuron to perform nonlinearXOR operation. In chapter 4 we expand chapter 3 to describe extension of ourideas to neuronal networks composed of many realistic biological spikingneurons that represent either small microcircuits or entire brain regions.</description><author>David Beniaguev</author><pubDate>Tue, 26 Sep 2023 18:32:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15090v1</guid></item><item><title>RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models</title><link>http://arxiv.org/abs/2309.15088v1</link><description>Researchers have successfully applied large language models (LLMs) such asChatGPT to reranking in an information retrieval context, but to date, suchwork has mostly been built on proprietary models hidden behind opaque APIendpoints. This approach yields experimental results that are not reproducibleand non-deterministic, threatening the veracity of outcomes that build on suchshaky foundations. To address this significant shortcoming, we presentRankVicuna, the first fully open-source LLM capable of performing high-qualitylistwise reranking in a zero-shot setting. Experimental results on the TREC2019 and 2020 Deep Learning Tracks show that we can achieve effectivenesscomparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parametermodel, although our effectiveness remains slightly behind reranking with GPT-4.We hope our work provides the foundation for future research on reranking withmodern LLMs. All the code necessary to reproduce our results is available athttps://github.com/castorini/rank_llm.</description><author>Ronak Pradeep, Sahel Sharifymoghaddam, Jimmy Lin</author><pubDate>Tue, 26 Sep 2023 18:31:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15088v1</guid></item><item><title>Video-adverb retrieval with compositional adverb-action embeddings</title><link>http://arxiv.org/abs/2309.15086v1</link><description>Retrieving adverbs that describe an action in a video poses a crucial steptowards fine-grained video understanding. We propose a framework forvideo-to-adverb retrieval (and vice versa) that aligns video embeddings withtheir matching compositional adverb-action text embedding in a joint embeddingspace. The compositional adverb-action text embedding is learned using aresidual gating mechanism, along with a novel training objective consisting oftriplet losses and a regression target. Our method achieves state-of-the-artperformance on five recent benchmarks for video-adverb retrieval. Furthermore,we introduce dataset splits to benchmark video-adverb retrieval for unseenadverb-action compositions on subsets of the MSR-VTT Adverbs and ActivityNetAdverbs datasets. Our proposed framework outperforms all prior works for thegeneralisation task of retrieving adverbs from videos for unseen adverb-actioncompositions. Code and dataset splits are available athttps://hummelth.github.io/ReGaDa/.</description><author>Thomas Hummel, Otniel-Bogdan Mercea, A. Sophia Koepke, Zeynep Akata</author><pubDate>Tue, 26 Sep 2023 18:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15086v1</guid></item><item><title>Permutation invariant matrix statistics and computational language tasks</title><link>http://arxiv.org/abs/2202.06829v2</link><description>The Linguistic Matrix Theory programme introduced by Kartsaklis, Ramgoolamand Sadrzadeh is an approach to the statistics of matrices that are generatedin type-driven distributional semantics, based on permutation invariantpolynomial functions which are regarded as the key observables encoding thesignificant statistics. In this paper we generalize the previous results on theapproximate Gaussianity of matrix distributions arising from compositionaldistributional semantics. We also introduce a geometry of observable vectorsfor words, defined by exploiting the graph-theoretic basis for the permutationinvariants and the statistical characteristics of the ensemble of matricesassociated with the words. We describe successful applications of this unifiedframework to a number of tasks in computational linguistics, associated withthe distinctions between synonyms, antonyms, hypernyms and hyponyms.</description><author>Manuel Accettulli Huber, Adriana Correia, Sanjaye Ramgoolam, Mehrnoosh Sadrzadeh</author><pubDate>Tue, 26 Sep 2023 18:29:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.06829v2</guid></item><item><title>The Surveillance AI Pipeline</title><link>http://arxiv.org/abs/2309.15084v1</link><description>A rapidly growing number of voices have argued that AI research, and computervision in particular, is closely tied to mass surveillance. Yet the direct pathfrom computer vision research to surveillance has remained obscured anddifficult to assess. This study reveals the Surveillance AI pipeline. We obtainthree decades of computer vision research papers and downstream patents (morethan 20,000 documents) and present a rich qualitative and quantitativeanalysis. This analysis exposes the nature and extent of the Surveillance AIpipeline, its institutional roots and evolution, and ongoing patterns ofobfuscation. We first perform an in-depth content analysis of computer visionpapers and downstream patents, identifying and quantifying key features and themany, often subtly expressed, forms of surveillance that appear. On the basisof this analysis, we present a topology of Surveillance AI that characterizesthe prevalent targeting of human data, practices of data transferal, andinstitutional data use. We find stark evidence of close ties between computervision and surveillance. The majority (68%) of annotated computer vision papersand patents self-report their technology enables data extraction about humanbodies and body parts and even more (90%) enable data extraction about humansin general.</description><author>Pratyusha Ria Kalluri, William Agnew, Myra Cheng, Kentrell Owens, Luca Soldaini, Abeba Birhane</author><pubDate>Tue, 26 Sep 2023 18:27:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15084v1</guid></item><item><title>RPEFlow: Multimodal Fusion of RGB-PointCloud-Event for Joint Optical Flow and Scene Flow Estimation</title><link>http://arxiv.org/abs/2309.15082v1</link><description>Recently, the RGB images and point clouds fusion methods have been proposedto jointly estimate 2D optical flow and 3D scene flow. However, as bothconventional RGB cameras and LiDAR sensors adopt a frame-based data acquisitionmechanism, their performance is limited by the fixed low sampling rates,especially in highly-dynamic scenes. By contrast, the event camera canasynchronously capture the intensity changes with a very high temporalresolution, providing complementary dynamic information of the observed scenes.In this paper, we incorporate RGB images, Point clouds and Events for jointoptical flow and scene flow estimation with our proposed multi-stage multimodalfusion model, RPEFlow. First, we present an attention fusion module with across-attention mechanism to implicitly explore the internal cross-modalcorrelation for 2D and 3D branches, respectively. Second, we introduce a mutualinformation regularization term to explicitly model the complementaryinformation of three modalities for effective multimodal feature learning. Wealso contribute a new synthetic dataset to advocate further research.Experiments on both synthetic and real datasets show that our model outperformsthe existing state-of-the-art by a wide margin. Code and dataset is availableat https://npucvr.github.io/RPEFlow.</description><author>Zhexiong Wan, Yuxin Mao, Jing Zhang, Yuchao Dai</author><pubDate>Tue, 26 Sep 2023 18:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15082v1</guid></item><item><title>On Excess Risk Convergence Rates of Neural Network Classifiers</title><link>http://arxiv.org/abs/2309.15075v1</link><description>The recent success of neural networks in pattern recognition andclassification problems suggests that neural networks possess qualitiesdistinct from other more classical classifiers such as SVMs or boostingclassifiers. This paper studies the performance of plug-in classifiers based onneural networks in a binary classification setting as measured by their excessrisks. Compared to the typical settings imposed in the literature, we considera more general scenario that resembles actual practice in two respects: first,the function class to be approximated includes the Barron functions as a propersubset, and second, the neural network classifier constructed is the minimizerof a surrogate loss instead of the $0$-$1$ loss so that gradient descent-basednumerical optimizations can be easily applied. While the class of functions weconsider is quite large that optimal rates cannot be faster than$n^{-\frac{1}{3}}$, it is a regime in which dimension-free rates are possibleand approximation power of neural networks can be taken advantage of. Inparticular, we analyze the estimation and approximation properties of neuralnetworks to obtain a dimension-free, uniform rate of convergence for the excessrisk. Finally, we show that the rate obtained is in fact minimax optimal up toa logarithmic factor, and the minimax lower bound shows the effect of themargin assumption in this regime.</description><author>Hyunouk Ko, Namjoon Suh, Xiaoming Huo</author><pubDate>Tue, 26 Sep 2023 18:14:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15075v1</guid></item><item><title>Language-EXtended Indoor SLAM (LEXIS): A Versatile System for Real-time Visual Scene Understanding</title><link>http://arxiv.org/abs/2309.15065v1</link><description>Versatile and adaptive semantic understanding would enable autonomous systemsto comprehend and interact with their surroundings. Existing fixed-class modelslimit the adaptability of indoor mobile and assistive autonomous systems. Inthis work, we introduce LEXIS, a real-time indoor Simultaneous Localization andMapping (SLAM) system that harnesses the open-vocabulary nature of LargeLanguage Models (LLMs) to create a unified approach to scene understanding andplace recognition. The approach first builds a topological SLAM graph of theenvironment (using visual-inertial odometry) and embeds ContrastiveLanguage-Image Pretraining (CLIP) features in the graph nodes. We use thisrepresentation for flexible room classification and segmentation, serving as abasis for room-centric place recognition. This allows loop closure searches tobe directed towards semantically relevant places. Our proposed system isevaluated using both public, simulated data and real-world data, coveringoffice and home environments. It successfully categorizes rooms with varyinglayouts and dimensions and outperforms the state-of-the-art (SOTA). For placerecognition and trajectory estimation tasks we achieve equivalent performanceto the SOTA, all also utilizing the same pre-trained model. Lastly, wedemonstrate the system's potential for planning.</description><author>Christina Kassab, Matias Mattamala, Lintong Zhang, Maurice Fallon</author><pubDate>Tue, 26 Sep 2023 17:50:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15065v1</guid></item><item><title>QUILT: Effective Multi-Class Classification on Quantum Computers Using an Ensemble of Diverse Quantum Classifiers</title><link>http://arxiv.org/abs/2309.15056v1</link><description>Quantum computers can theoretically have significant acceleration overclassical computers; but, the near-future era of quantum computing is limiteddue to small number of qubits that are also error prone. Quilt is a frameworkfor performing multi-class classification task designed to work effectively oncurrent error-prone quantum computers. Quilt is evaluated with real quantummachines as well as with projected noise levels as quantum machines become morenoise-free. Quilt demonstrates up to 85% multi-class classification accuracywith the MNIST dataset on a five-qubit system.</description><author>Daniel Silver, Tirthak Patel, Devesh Tiwari</author><pubDate>Tue, 26 Sep 2023 17:36:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15056v1</guid></item><item><title>When Prolog meets generative models: a new approach for managing knowledge and planning in robotic applications</title><link>http://arxiv.org/abs/2309.15049v1</link><description>In this paper, we propose a robot oriented knowledge management system basedon the use of the Prolog language. Our framework hinges on a specialorganisation of knowledge base that enables: 1. its efficient population fromnatural language texts using semi-automated procedures based on Large LanguageModels, 2. the bumpless generation of temporal parallel plans for multi-robotsystems through a sequence of transformations, 3. the automated translation ofthe plan into an executable formalism (the behaviour trees). The framework issupported by a set of open source tools and is shown on a realisticapplication.</description><author>Enrico Saccon, Ahmet Tikna, Davide De Martini, Edoardo Lamon, Marco Roveri, Luigi Palopoli</author><pubDate>Tue, 26 Sep 2023 17:26:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15049v1</guid></item><item><title>Class Incremental Learning via Likelihood Ratio Based Task Prediction</title><link>http://arxiv.org/abs/2309.15048v1</link><description>Class incremental learning (CIL) is a challenging setting of continuallearning, which learns a series of tasks sequentially. Each task consists of aset of unique classes. The key feature of CIL is that no task identifier (ortask-id) is provided at test time for each test sample. Predicting the task-idfor each test sample is a challenging problem. An emerging theoreticallyjustified and effective approach is to train a task-specific model for eachtask in a shared network for all tasks based on a task-incremental learning(TIL) method to deal with forgetting. The model for each task in this approachis an out-of-distribution (OOD) detector rather than a conventional classifier.The OOD detector can perform both within-task (in-distribution (IND)) classprediction and OOD detection. The OOD detection capability is the key fortask-id prediction during inference for each test sample. However, this paperargues that using a traditional OOD detector for task-id prediction issub-optimal because additional information (e.g., the replay data and thelearned tasks) available in CIL can be exploited to design a better andprincipled method for task-id prediction. We call the new method TPLR (Task-idPrediction based on Likelihood Ratio}). TPLR markedly outperforms strong CILbaselines.</description><author>Haowei Lin, Yijia Shao, Weinan Qian, Ningxin Pan, Yiduo Guo, Bing Liu</author><pubDate>Tue, 26 Sep 2023 17:25:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15048v1</guid></item><item><title>Emotional Speech-Driven Animation with Content-Emotion Disentanglement</title><link>http://arxiv.org/abs/2306.08990v2</link><description>To be widely adopted, 3D facial avatars must be animated easily,realistically, and directly from speech signals. While the best recent methodsgenerate 3D animations that are synchronized with the input audio, they largelyignore the impact of emotions on facial expressions. Realistic facial animationrequires lip-sync together with the natural expression of emotion. To that end,we propose EMOTE (Expressive Model Optimized for Talking with Emotion), whichgenerates 3D talking-head avatars that maintain lip-sync from speech whileenabling explicit control over the expression of emotion. To achieve this, wesupervise EMOTE with decoupled losses for speech (i.e., lip-sync) and emotion.These losses are based on two key observations: (1) deformations of the facedue to speech are spatially localized around the mouth and have high temporalfrequency, whereas (2) facial expressions may deform the whole face and occurover longer intervals. Thus, we train EMOTE with a per-frame lip-reading lossto preserve the speech-dependent content, while supervising emotion at thesequence level. Furthermore, we employ a content-emotion exchange mechanism inorder to supervise different emotions on the same audio, while maintaining thelip motion synchronized with the speech. To employ deep perceptual losseswithout getting undesirable artifacts, we devise a motion prior in the form ofa temporal VAE. Due to the absence of high-quality aligned emotional 3D facedatasets with speech, EMOTE is trained with 3D pseudo-ground-truth extractedfrom an emotional video dataset (i.e., MEAD). Extensive qualitative andperceptual evaluations demonstrate that EMOTE produces speech-driven facialanimations with better lip-sync than state-of-the-art methods trained on thesame data, while offering additional, high-quality emotional control.</description><author>Radek Daněček, Kiran Chhatre, Shashank Tripathi, Yandong Wen, Michael J. Black, Timo Bolkart</author><pubDate>Tue, 26 Sep 2023 17:25:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08990v2</guid></item><item><title>Combining Survival Analysis and Machine Learning for Mass Cancer Risk Prediction using EHR data</title><link>http://arxiv.org/abs/2309.15039v1</link><description>Purely medical cancer screening methods are often costly, time-consuming, andweakly applicable on a large scale. Advanced Artificial Intelligence (AI)methods greatly help cancer detection but require specific or deep medicaldata. These aspects affect the mass implementation of cancer screening methods.For these reasons, it is a disruptive change for healthcare to apply AI methodsfor mass personalized assessment of the cancer risk among patients based on theexisting Electronic Health Records (EHR) volume. This paper presents a novel method for mass cancer risk prediction using EHRdata. Among other methods, our one stands out by the minimum data greedypolicy, requiring only a history of medical service codes and diagnoses fromEHR. We formulate the problem as a binary classification. This dataset contains175 441 de-identified patients (2 861 diagnosed with cancer). As a baseline, weimplement a solution based on a recurrent neural network (RNN). We propose amethod that combines machine learning and survival analysis since theseapproaches are less computationally heavy, can be combined into an ensemble(the Survival Ensemble), and can be reproduced in most medical institutions. We test the Survival Ensemble in some studies. Firstly, we obtain asignificant difference between values of the primary metric (Average Precision)with 22.8% (ROC AUC 83.7%, F1 17.8%) for the Survival Ensemble versus 15.1%(ROC AUC 84.9%, F1 21.4%) for the Baseline. Secondly, the performance of theSurvival Ensemble is also confirmed during the ablation study. Thirdly, ourmethod exceeds age baselines by a significant margin. Fourthly, in the blindretrospective out-of-time experiment, the proposed method is reliable in cancerpatient detection (9 out of 100 selected). Such results exceed the estimates ofmedical screenings, e.g., the best Number Needed to Screen (9 out of 1000screenings).</description><author>Petr Philonenko, Vladimir Kokh, Pavel Blinov</author><pubDate>Tue, 26 Sep 2023 17:15:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15039v1</guid></item><item><title>HPCR: Holistic Proxy-based Contrastive Replay for Online Continual Learning</title><link>http://arxiv.org/abs/2309.15038v1</link><description>Online continual learning (OCL) aims to continuously learn new data from asingle pass over the online data stream. It generally suffers from thecatastrophic forgetting issue. Existing replay-based methods effectivelyalleviate this issue by replaying part of old data in a proxy-based orcontrastive-based replay manner. In this paper, we conduct a comprehensiveanalysis of these two replay manners and find they can be complementary.Inspired by this finding, we propose a novel replay-based method calledproxy-based contrastive replay (PCR), which replaces anchor-to-sample pairswith anchor-to-proxy pairs in the contrastive-based loss to alleviate thephenomenon of forgetting. Based on PCR, we further develop a more advancedmethod named holistic proxy-based contrastive replay (HPCR), which consists ofthree components. The contrastive component conditionally incorporatesanchor-to-sample pairs to PCR, learning more fine-grained semantic informationwith a large training batch. The second is a temperature component thatdecouples the temperature coefficient into two parts based on their impacts onthe gradient and sets different values for them to learn more novel knowledge.The third is a distillation component that constrains the learning process tokeep more historical knowledge. Experiments on four datasets consistentlydemonstrate the superiority of HPCR over various state-of-the-art methods.</description><author>Huiwei Lin, Shanshan Feng, Baoquan Zhang, Xutao Li, Yew-soon Ong, Yunming Ye</author><pubDate>Tue, 26 Sep 2023 17:12:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15038v1</guid></item><item><title>Turbulence in Focus: Benchmarking Scaling Behavior of 3D Volumetric Super-Resolution with BLASTNet 2.0 Data</title><link>http://arxiv.org/abs/2309.13457v2</link><description>Analysis of compressible turbulent flows is essential for applicationsrelated to propulsion, energy generation, and the environment. Here, we presentBLASTNet 2.0, a 2.2 TB network-of-datasets containing 744 full-domain samplesfrom 34 high-fidelity direct numerical simulations, which addresses the currentlimited availability of 3D high-fidelity reacting and non-reacting compressibleturbulent flow simulation data. With this data, we benchmark a total of 49variations of five deep learning approaches for 3D super-resolution - which canbe applied for improving scientific imaging, simulations, turbulence models, aswell as in computer vision applications. We perform neural scaling analysis onthese models to examine the performance of different machine learning (ML)approaches, including two scientific ML techniques. We demonstrate that (i)predictive performance can scale with model size and cost, (ii) architecturematters significantly, especially for smaller models, and (iii) the benefits ofphysics-based losses can persist with increasing model size. The outcomes ofthis benchmark study are anticipated to offer insights that can aid the designof 3D super-resolution models, especially for turbulence models, while thisdata is expected to foster ML methods for a broad range of flow physicsapplications. This data is publicly available with download links and browsingtools consolidated at https://blastnet.github.io.</description><author>Wai Tong Chung, Bassem Akoush, Pushan Sharma, Alex Tamkin, Ki Sung Jung, Jacqueline H. Chen, Jack Guo, Davy Brouzet, Mohsen Talei, Bruno Savard, Alexei Y. Poludnenko, Matthias Ihme</author><pubDate>Tue, 26 Sep 2023 17:06:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13457v2</guid></item><item><title>SOFARI: High-Dimensional Manifold-Based Inference</title><link>http://arxiv.org/abs/2309.15032v1</link><description>Multi-task learning is a widely used technique for harnessing informationfrom various tasks. Recently, the sparse orthogonal factor regression (SOFAR)framework, based on the sparse singular value decomposition (SVD) within thecoefficient matrix, was introduced for interpretable multi-task learning,enabling the discovery of meaningful latent feature-response associationnetworks across different layers. However, conducting precise inference on thelatent factor matrices has remained challenging due to orthogonalityconstraints inherited from the sparse SVD constraint. In this paper, we suggesta novel approach called high-dimensional manifold-based SOFAR inference(SOFARI), drawing on the Neyman near-orthogonality inference whileincorporating the Stiefel manifold structure imposed by the SVD constraints. Byleveraging the underlying Stiefel manifold structure, SOFARI providesbias-corrected estimators for both latent left factor vectors and singularvalues, for which we show to enjoy the asymptotic mean-zero normaldistributions with estimable variances. We introduce two SOFARI variants tohandle strongly and weakly orthogonal latent factors, where the latter covers abroader range of applications. We illustrate the effectiveness of SOFARI andjustify our theoretical results through simulation examples and a real dataapplication in economic forecasting.</description><author>Zemin Zheng, Xin Zhou, Yingying Fan, Jinchi Lv</author><pubDate>Tue, 26 Sep 2023 17:01:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15032v1</guid></item><item><title>Nuclear Morphometry using a Deep Learning-based Algorithm has Prognostic Relevance for Canine Cutaneous Mast Cell Tumors</title><link>http://arxiv.org/abs/2309.15031v1</link><description>Variation in nuclear size and shape is an important criterion of malignancyfor many tumor types; however, categorical estimates by pathologists have poorreproducibility. Measurements of nuclear characteristics (morphometry) canimprove reproducibility, but manual methods are time consuming. In this study,we evaluated fully automated morphometry using a deep learning-based algorithmin 96 canine cutaneous mast cell tumors with information on patient survival.Algorithmic morphometry was compared with karyomegaly estimates by 11pathologists, manual nuclear morphometry of 12 cells by 9 pathologists, and themitotic count as a benchmark. The prognostic value of automated morphometry washigh with an area under the ROC curve regarding the tumor-specific survival of0.943 (95% CI: 0.889 - 0.996) for the standard deviation (SD) of nuclear area,which was higher than manual morphometry of all pathologists combined (0.868,95% CI: 0.737 - 0.991) and the mitotic count (0.885, 95% CI: 0.765 - 1.00). Atthe proposed thresholds, the hazard ratio for algorithmic morphometry (SD ofnuclear area $\geq 9.0 \mu m^2$) was 18.3 (95% CI: 5.0 - 67.1), for manualmorphometry (SD of nuclear area $\geq 10.9 \mu m^2$) 9.0 (95% CI: 6.0 - 13.4),for karyomegaly estimates 7.6 (95% CI: 5.7 - 10.1), and for the mitotic count30.5 (95% CI: 7.8 - 118.0). Inter-rater reproducibility for karyomegalyestimates was fair ($\kappa$ = 0.226) with highly variablesensitivity/specificity values for the individual pathologists. Reproducibilityfor manual morphometry (SD of nuclear area) was good (ICC = 0.654). This studysupports the use of algorithmic morphometry as a prognostic test to overcomethe limitations of estimates and manual measurements.</description><author>Andreas Haghofer, Eda Parlak, Alexander Bartel, Taryn A. Donovan, Charles-Antoine Assenmacher, Pompei Bolfa, Michael J. Dark, Andrea Fuchs-Baumgartinger, Andrea Klang, Kathrin Jäger, Robert Klopfleisch, Sophie Merz, Barbara Richter, F. Yvonne Schulman, Jonathan Ganz, Josef Scharinger, Marc Aubreville, Stephan M. Winkler, Matti Kiupel, Christof A. Bertram</author><pubDate>Tue, 26 Sep 2023 17:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15031v1</guid></item><item><title>Interpretable and Interactive Deep Multiple Instance Learning for Dental Caries Classification in Bitewing X-rays</title><link>http://arxiv.org/abs/2112.09694v2</link><description>We propose a simple and efficient image classification architecture based ondeep multiple instance learning, and apply it to the challenging task of cariesdetection in dental radiographs. Technically, our approach contributes in twoways: First, it outputs a heatmap of local patch classification probabilitiesdespite being trained with weak image-level labels. Second, it is amenable tolearning from segmentation labels to guide training. In contrast to existingmethods, the human user can faithfully interpret predictions and interact withthe model to decide which regions to attend to. Experiments are conducted on alarge clinical dataset of $\sim$38k bitewings ($\sim$316k teeth), where weachieve competitive performance compared to various baselines. When guided byan external caries segmentation model, a significant improvement inclassification and localization performance is observed.</description><author>Benjamin Bergner, Csaba Rohrer, Aiham Taleb, Martha Duchrau, Guilherme De Leon, Jonas Almeida Rodrigues, Falk Schwendicke, Joachim Krois, Christoph Lippert</author><pubDate>Tue, 26 Sep 2023 16:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.09694v2</guid></item><item><title>Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding</title><link>http://arxiv.org/abs/2309.15028v1</link><description>Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) mayseem unnecessary when generating natural language text based onstate-of-the-art reinforcement learning such as Proximal Policy Optimization(PPO). In this paper, we demonstrate that it is possible to get extra mileageout of PPO by integrating MCTS on top. The key idea is not to throw out thevalue network, a byproduct of PPO training for evaluating partial outputsequences, when decoding text out of the policy network. More concretely, wepresent a novel value-guided decoding algorithm called PPO-MCTS, which canintegrate the value network from PPO to work closely with the policy networkduring inference-time generation. Compared to prior approaches based on MCTSfor controlled text generation, the key strength of our approach is to reducethe fundamental mismatch of the scoring mechanisms of the partial outputsbetween training and test. Evaluation on four text generation tasks demonstratethat PPO-MCTS greatly improves the preferability of generated text compared tothe standard practice of using only the PPO policy. Our results demonstrate thepromise of search algorithms even on top of the aligned language models fromPPO, and the under-explored benefit of the value network.</description><author>Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, Asli Celikyilmaz</author><pubDate>Tue, 26 Sep 2023 16:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15028v1</guid></item><item><title>Large Language Model Alignment: A Survey</title><link>http://arxiv.org/abs/2309.15025v1</link><description>Recent years have witnessed remarkable progress made in large language models(LLMs). Such advancements, while garnering significant attention, haveconcurrently elicited various concerns. The potential of these models isundeniably vast; however, they may yield texts that are imprecise, misleading,or even detrimental. Consequently, it becomes paramount to employ alignmenttechniques to ensure these models to exhibit behaviors consistent with humanvalues. This survey endeavors to furnish an extensive exploration of alignmentmethodologies designed for LLMs, in conjunction with the extant capabilityresearch in this domain. Adopting the lens of AI alignment, we categorize theprevailing methods and emergent proposals for the alignment of LLMs into outerand inner alignment. We also probe into salient issues including the models'interpretability, and potential vulnerabilities to adversarial attacks. Toassess LLM alignment, we present a wide variety of benchmarks and evaluationmethodologies. After discussing the state of alignment research for LLMs, wefinally cast a vision toward the future, contemplating the promising avenues ofresearch that lie ahead. Our aspiration for this survey extends beyond merely spurring researchinterests in this realm. We also envision bridging the gap between the AIalignment research community and the researchers engrossed in the capabilityexploration of LLMs for both capable and safe LLMs.</description><author>Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong</author><pubDate>Tue, 26 Sep 2023 16:49:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15025v1</guid></item><item><title>OpenPodcar: an Open Source Vehicle for Self-Driving Car Research</title><link>http://arxiv.org/abs/2205.04454v2</link><description>OpenPodcar is a low-cost, open source hardware and software, autonomousvehicle research platform based on an off-the-shelf, hard-canopy, mobilityscooter donor vehicle. Hardware and software build instructions are provided toconvert the donor vehicle into a low-cost and fully autonomous platform. Theopen platform consists of (a) hardware components: CAD designs, bill ofmaterials, and build instructions; (b) Arduino, ROS and Gazebo control andsimulation software files which provide standard ROS interfaces and simulationof the vehicle; and (c) higher-level ROS software implementations andconfigurations of standard robot autonomous planning and control, including themove_base interface with Timed-Elastic-Band planner which enacts commands todrive the vehicle from a current to a desired pose around obstacles. Thevehicle is large enough to transport a human passenger or similar load atspeeds up to 15km/h, for example for use as a last-mile autonomous taxi serviceor to transport delivery containers similarly around a city center. It is smalland safe enough to be parked in a standard research lab and be used forrealistic human-vehicle interaction studies. System build cost from newcomponents is around USD7,000 in total in 2022. OpenPodcar thus provides a goodbalance between real world utility, safety, cost and research convenience.</description><author>Fanta Camara, Chris Waltham, Grey Churchill, Charles Fox</author><pubDate>Tue, 26 Sep 2023 16:48:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.04454v2</guid></item><item><title>Synthia's Melody: A Benchmark Framework for Unsupervised Domain Adaptation in Audio</title><link>http://arxiv.org/abs/2309.15024v1</link><description>Despite significant advancements in deep learning for vision and naturallanguage, unsupervised domain adaptation in audio remains relativelyunexplored. We, in part, attribute this to the lack of an appropriate benchmarkdataset. To address this gap, we present Synthia's melody, a novel audio datageneration framework capable of simulating an infinite variety of 4-secondmelodies with user-specified confounding structures characterised by musicalkeys, timbre, and loudness. Unlike existing datasets collected underobservational settings, Synthia's melody is free of unobserved biases, ensuringthe reproducibility and comparability of experiments. To showcase its utility,we generate two types of distribution shifts-domain shift and sample selectionbias-and evaluate the performance of acoustic deep learning models under theseshifts. Our evaluations reveal that Synthia's melody provides a robust testbedfor examining the susceptibility of these models to varying levels ofdistribution shift.</description><author>Chia-Hsin Lin, Charles Jones, Björn W. Schuller, Harry Coppock</author><pubDate>Tue, 26 Sep 2023 16:46:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15024v1</guid></item><item><title>IFT: Image Fusion Transformer for Ghost-free High Dynamic Range Imaging</title><link>http://arxiv.org/abs/2309.15019v1</link><description>Multi-frame high dynamic range (HDR) imaging aims to reconstruct ghost-freeimages with photo-realistic details from content-complementary but spatiallymisaligned low dynamic range (LDR) images. Existing HDR algorithms are prone toproducing ghosting artifacts as their methods fail to capture long-rangedependencies between LDR frames with large motion in dynamic scenes. To addressthis issue, we propose a novel image fusion transformer, referred to as IFT,which presents a fast global patch searching (FGPS) module followed by aself-cross fusion module (SCF) for ghost-free HDR imaging. The FGPS searchesthe patches from supporting frames that have the closest dependency to eachpatch of the reference frame for long-range dependency modeling, while the SCFconducts intra-frame and inter-frame feature fusion on the patches obtained bythe FGPS with linear complexity to input resolution. By matching similarpatches between frames, objects with large motion ranges in dynamic scenes canbe aligned, which can effectively alleviate the generation of artifacts. Inaddition, the proposed FGPS and SCF can be integrated into various deep HDRmethods as efficient plug-in modules. Extensive experiments on multiplebenchmarks show that our method achieves state-of-the-art performance bothquantitatively and qualitatively.</description><author>Hailing Wang, Wei Li, Yuanyuan Xi, Jie Hu, Hanting Chen, Longyu Li, Yunhe Wang</author><pubDate>Tue, 26 Sep 2023 16:38:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15019v1</guid></item><item><title>Unidirectional brain-computer interface: Artificial neural network encoding natural images to fMRI response in the visual cortex</title><link>http://arxiv.org/abs/2309.15018v1</link><description>While significant advancements in artificial intelligence (AI) have catalyzedprogress across various domains, its full potential in understanding visualperception remains underexplored. We propose an artificial neural networkdubbed VISION, an acronym for "Visual Interface System for Imaging Output ofNeural activity," to mimic the human brain and show how it can fosterneuroscientific inquiries. Using visual and contextual inputs, this multimodalmodel predicts the brain's functional magnetic resonance imaging (fMRI) scanresponse to natural images. VISION successfully predicts human hemodynamicresponses as fMRI voxel values to visual inputs with an accuracy exceedingstate-of-the-art performance by 45%. We further probe the trained networks toreveal representational biases in different visual areas, generateexperimentally testable hypotheses, and formulate an interpretable metric toassociate these hypotheses with cortical functions. With both a model andevaluation metric, the cost and time burdens associated with designing andimplementing functional analysis on the visual cortex could be reduced. Ourwork suggests that the evolution of computational models may shed light on ourfundamental understanding of the visual cortex and provide a viable approachtoward reliable brain-machine interfaces.</description><author>Ruixing Liang, Xiangyu Zhang, Qiong Li, Lai Wei, Hexin Liu, Avisha Kumar, Kelley M. Kempski Leadingham, Joshua Punnoose, Leibny Paola Garcia, Amir Manbachi</author><pubDate>Tue, 26 Sep 2023 16:38:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15018v1</guid></item><item><title>Learning Stable and Robust Linear Parameter-Varying State-Space Models</title><link>http://arxiv.org/abs/2304.01828v2</link><description>This paper presents two direct parameterizations of stable and robust linearparameter-varying state-space (LPV-SS) models. The model parametrizationsguarantee a priori that for all parameter values during training, the allowedmodels are stable in the contraction sense or have their Lipschitz constantbounded by a user-defined value $\gamma$. Furthermore, since theparametrizations are direct, the models can be trained using unconstrainedoptimization. The fact that the trained models are of the LPV-SS class makesthem useful for, e.g., further convex analysis or controller design. Theeffectiveness of the approach is demonstrated on an LPV identification problem.</description><author>Chris Verhoek, Ruigang Wang, Roland Tóth</author><pubDate>Tue, 26 Sep 2023 16:36:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01828v2</guid></item><item><title>Question-Answering Approach to Evaluate Legal Summaries</title><link>http://arxiv.org/abs/2309.15016v1</link><description>Traditional evaluation metrics like ROUGE compare lexical overlap between thereference and generated summaries without taking argumentative structure intoaccount, which is important for legal summaries. In this paper, we propose anovel legal summarization evaluation framework that utilizes GPT-4 to generatea set of question-answer pairs that cover main points and information in thereference summary. GPT-4 is then used to generate answers based on thegenerated summary for the questions from the reference summary. Finally, GPT-4grades the answers from the reference summary and the generated summary. Weexamined the correlation between GPT-4 grading with human grading. The resultssuggest that this question-answering approach with GPT-4 can be a useful toolfor gauging the quality of the summary.</description><author>Huihui Xu, Kevin Ashley</author><pubDate>Tue, 26 Sep 2023 16:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15016v1</guid></item><item><title>Updated Corpora and Benchmarks for Long-Form Speech Recognition</title><link>http://arxiv.org/abs/2309.15013v1</link><description>The vast majority of ASR research uses corpora in which both the training andtest data have been pre-segmented into utterances. In most real-word ASRuse-cases, however, test audio is not segmented, leading to a mismatch betweeninference-time conditions and models trained on segmented utterances. In thispaper, we re-release three standard ASR corpora - TED-LIUM 3, Gigapeech, andVoxPopuli-en - with updated transcription and alignments to enable their usefor long-form ASR research. We use these reconstituted corpora to study thetrain-test mismatch problem for transducers and attention-basedencoder-decoders (AEDs), confirming that AEDs are more susceptible to thisissue. Finally, we benchmark a simple long-form training for these models,showing its efficacy for model robustness under this domain shift.</description><author>Jennifer Drexler Fox, Desh Raj, Natalie Delworth, Quinn McNamara, Corey Miller, Migüel Jetté</author><pubDate>Tue, 26 Sep 2023 16:32:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15013v1</guid></item><item><title>Uncovering local aggregated air quality index with smartphone captured images leveraging efficient deep convolutional neural network</title><link>http://arxiv.org/abs/2308.03200v2</link><description>The prevalence and mobility of smartphones make these a widely used tool forenvironmental health research. However, their potential for determiningaggregated air quality index (AQI) based on PM2.5 concentration in specificlocations remains largely unexplored in the existing literature. In this paper,we thoroughly examine the challenges associated with predictinglocation-specific PM2.5 concentration using images taken with smartphonecameras. The focus of our study is on Dhaka, the capital of Bangladesh, due toits significant air pollution levels and the large population exposed to it.Our research involves the development of a Deep Convolutional Neural Network(DCNN), which we train using over a thousand outdoor images taken andannotated. These photos are captured at various locations in Dhaka, and theirlabels are based on PM2.5 concentration data obtained from the local USconsulate, calculated using the NowCast algorithm. Through supervised learning,our model establishes a correlation index during training, enhancing itsability to function as a Picture-based Predictor of PM2.5 Concentration (PPPC).This enables the algorithm to calculate an equivalent daily averaged AQI indexfrom a smartphone image. Unlike, popular overly parameterized models, our modelshows resource efficiency since it uses fewer parameters. Furthermore, testresults indicate that our model outperforms popular models like ViT and INN, aswell as popular CNN-based models such as VGG19, ResNet50, and MobileNetV2, inpredicting location-specific PM2.5 concentration. Our dataset is the firstpublicly available collection that includes atmospheric images andcorresponding PM2.5 measurements from Dhaka. Our code and dataset will be madepublic when publishing the paper.</description><author>Joyanta Jyoti Mondal, Md. Farhadul Islam, Raima Islam, Nowsin Kabir Rhidi, Sarfaraz Newaz, A. B. M. Alim Al Islam, Meem Arafat Manab, Jannatun Noor</author><pubDate>Tue, 26 Sep 2023 16:20:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03200v2</guid></item><item><title>Automating question generation from educational text</title><link>http://arxiv.org/abs/2309.15004v1</link><description>The use of question-based activities (QBAs) is wide-spread in education,traditionally forming an integral part of the learning and assessment process.In this paper, we design and evaluate an automated question generation tool forformative and summative assessment in schools. We present an expert survey ofone hundred and four teachers, demonstrating the need for automated generationof QBAs, as a tool that can significantly reduce the workload of teachers andfacilitate personalized learning experiences. Leveraging the recentadvancements in generative AI, we then present a modular framework employingtransformer based language models for automatic generation of multiple-choicequestions (MCQs) from textual content. The presented solution, with distinctmodules for question generation, correct answer prediction, and distractorformulation, enables us to evaluate different language models and generationtechniques. Finally, we perform an extensive quantitative and qualitativeevaluation, demonstrating trade-offs in the use of different techniques andmodels.</description><author>Ayan Kumar Bhowmick, Ashish Jagmohan, Aditya Vempaty, Prasenjit Dey, Leigh Hall, Jeremy Hartman, Ravi Kokku, Hema Maheshwari</author><pubDate>Tue, 26 Sep 2023 16:18:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15004v1</guid></item><item><title>Convergence guarantees for forward gradient descent in the linear regression model</title><link>http://arxiv.org/abs/2309.15001v1</link><description>Renewed interest in the relationship between artificial and biological neuralnetworks motivates the study of gradient-free methods. Considering the linearregression model with random design, we theoretically analyze in this work thebiologically motivated (weight-perturbed) forward gradient scheme that is basedon random linear combination of the gradient. If d denotes the number ofparameters and k the number of samples, we prove that the mean squared error ofthis method converges for $k\gtrsim d^2\log(d)$ with rate $d^2\log(d)/k.$Compared to the dimension dependence d for stochastic gradient descent, anadditional factor $d\log(d)$ occurs.</description><author>Thijs Bos, Johannes Schmidt-Hieber</author><pubDate>Tue, 26 Sep 2023 16:15:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15001v1</guid></item><item><title>Object-Centric Open-Vocabulary Image-Retrieval with Aggregated Features</title><link>http://arxiv.org/abs/2309.14999v1</link><description>The task of open-vocabulary object-centric image retrieval involves theretrieval of images containing a specified object of interest, delineated by anopen-set text query. As working on large image datasets becomes standard,solving this task efficiently has gained significant practical importance.Applications include targeted performance analysis of retrieved images usingad-hoc queries and hard example mining during training. Recent advancements incontrastive-based open vocabulary systems have yielded remarkablebreakthroughs, facilitating large-scale open vocabulary image retrieval.However, these approaches use a single global embedding per image, therebyconstraining the system's ability to retrieve images containing relativelysmall object instances. Alternatively, incorporating local embeddings fromdetection pipelines faces scalability challenges, making it unsuitable forretrieval from large databases. In this work, we present a simple yet effective approach to object-centricopen-vocabulary image retrieval. Our approach aggregates dense embeddingsextracted from CLIP into a compact representation, essentially combining thescalability of image retrieval pipelines with the object identificationcapabilities of dense detection methods. We show the effectiveness of ourscheme to the task by achieving significantly better results than globalfeature approaches on three datasets, increasing accuracy by up to 15 mAPpoints. We further integrate our scheme into a large scale retrieval frameworkand demonstrate our method's advantages in terms of scalability andinterpretability.</description><author>Hila Levi, Guy Heller, Dan Levi, Ethan Fetaya</author><pubDate>Tue, 26 Sep 2023 16:13:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14999v1</guid></item><item><title>An Ensemble Model for Distorted Images in Real Scenarios</title><link>http://arxiv.org/abs/2309.14998v1</link><description>Image acquisition conditions and environments can significantly affecthigh-level tasks in computer vision, and the performance of most computervision algorithms will be limited when trained on distortion-free datasets.Even with updates in hardware such as sensors and deep learning methods, itwill still not work in the face of variable conditions in real-worldapplications. In this paper, we apply the object detector YOLOv7 to detectdistorted images from the dataset CDCOCO. Through carefully designedoptimizations including data enhancement, detection box ensemble, denoiserensemble, super-resolution models, and transfer learning, our model achievesexcellent performance on the CDCOCO test set. Our denoising detection model candenoise and repair distorted images, making the model useful in a variety ofreal-world scenarios and environments.</description><author>Boyuan Ji, Jianchang Huang, Wenzhuo Huang, Shuke He</author><pubDate>Tue, 26 Sep 2023 16:12:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14998v1</guid></item><item><title>IAIFNet: An Illumination-Aware Infrared and Visible Image Fusion Network</title><link>http://arxiv.org/abs/2309.14997v1</link><description>Infrared and visible image fusion (IVIF) is used to generate fusion imageswith comprehensive features of both images, which is beneficial for downstreamvision tasks. However, current methods rarely consider the illuminationcondition in low-light environments, and the targets in the fused images areoften not prominent. To address the above issues, we propose anIllumination-Aware Infrared and Visible Image Fusion Network, named as IAIFNet.In our framework, an illumination enhancement network first estimates theincident illumination maps of input images. Afterwards, with the help ofproposed adaptive differential fusion module (ADFM) and salient target awaremodule (STAM), an image fusion network effectively integrates the salientfeatures of the illumination-enhanced infrared and visible images into a fusionimage of high visual quality. Extensive experimental results verify that ourmethod outperforms five state-of-the-art methods of fusing infrared and visibleimages.</description><author>Qiao Yang, Yu Zhang, Jian Zhang, Zijing Zhao, Shunli Zhang, Jinqiao Wang, Junzhe Chen</author><pubDate>Tue, 26 Sep 2023 16:12:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14997v1</guid></item><item><title>YOLOX-PAI: An Improved YOLOX, Stronger and Faster than YOLOv6</title><link>http://arxiv.org/abs/2208.13040v3</link><description>We develop an all-in-one computer vision toolbox named EasyCV to facilitatethe use of various SOTA computer vision methods. Recently, we add YOLOX-PAI, animproved version of YOLOX, into EasyCV. We conduct ablation studies toinvestigate the influence of some detection methods on YOLOX. We also providean easy use for PAI-Blade which is used to accelerate the inference processbased on BladeDISC and TensorRT. Finally, we receive 42.8 mAP on COCO datesetwithin 1.0 ms on a single NVIDIA V100 GPU, which is a bit faster than YOLOv6. Asimple but efficient predictor api is also designed in EasyCV to conductend2end object detection. Codes and models are now available at:https://github.com/alibaba/EasyCV.</description><author>Ziheng Wu, Xinyi Zou, Wenmeng Zhou, Jun Huang</author><pubDate>Tue, 26 Sep 2023 16:05:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.13040v3</guid></item><item><title>Measurement Models For Sailboats Price vs. Features And Regional Areas</title><link>http://arxiv.org/abs/2309.14994v1</link><description>In this study, we investigated the relationship between sailboat technicalspecifications and their prices, as well as regional pricing influences.Utilizing a dataset encompassing characteristics like length, beam, draft,displacement, sail area, and waterline, we applied multiple machine learningmodels to predict sailboat prices. The gradient descent model demonstratedsuperior performance, producing the lowest MSE and MAE. Our analysis revealedthat monohulled boats are generally more affordable than catamarans, and thatcertain specifications such as length, beam, displacement, and sail areadirectly correlate with higher prices. Interestingly, lower draft wasassociated with higher listing prices. We also explored regional pricedeterminants and found that the United States tops the list in average sailboatprices, followed by Europe, Hong Kong, and the Caribbean. Contrary to ourinitial hypothesis, a country's GDP showed no direct correlation with sailboatprices. Utilizing a 50% cross-validation method, our models yielded consistentresults across test groups. Our research offers a machine learning-enhancedperspective on sailboat pricing, aiding prospective buyers in making informeddecisions.</description><author>Jiaqi Weng, Chunlin Feng, Yihan Shao</author><pubDate>Tue, 26 Sep 2023 16:03:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14994v1</guid></item><item><title>Robust Sequential DeepFake Detection</title><link>http://arxiv.org/abs/2309.14991v1</link><description>Since photorealistic faces can be readily generated by facial manipulationtechnologies nowadays, potential malicious abuse of these technologies hasdrawn great concerns. Numerous deepfake detection methods are thus proposed.However, existing methods only focus on detecting one-step facial manipulation.As the emergence of easy-accessible facial editing applications, people caneasily manipulate facial components using multi-step operations in a sequentialmanner. This new threat requires us to detect a sequence of facialmanipulations, which is vital for both detecting deepfake media and recoveringoriginal faces afterwards. Motivated by this observation, we emphasize the needand propose a novel research problem called Detecting Sequential DeepFakeManipulation (Seq-DeepFake). Unlike the existing deepfake detection task onlydemanding a binary label prediction, detecting Seq-DeepFake manipulationrequires correctly predicting a sequential vector of facial manipulationoperations. To support a large-scale investigation, we construct the firstSeq-DeepFake dataset, where face images are manipulated sequentially withcorresponding annotations of sequential facial manipulation vectors. Based onthis new dataset, we cast detecting Seq-DeepFake manipulation as a specificimage-to-sequence task and propose a concise yet effective Seq-DeepFakeTransformer (SeqFakeFormer). To better reflect real-world deepfake datadistributions, we further apply various perturbations on the originalSeq-DeepFake dataset and construct the more challenging Sequential DeepFakedataset with perturbations (Seq-DeepFake-P). To exploit deeper correlationbetween images and sequences when facing Seq-DeepFake-P, a dedicatedSeq-DeepFake Transformer with Image-Sequence Reasoning (SeqFakeFormer++) isdevised, which builds stronger correspondence between image-sequence pairs formore robust Seq-DeepFake detection.</description><author>Rui Shao, Tianxing Wu, Ziwei Liu</author><pubDate>Tue, 26 Sep 2023 16:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14991v1</guid></item><item><title>Tempo Adaption in Non-stationary Reinforcement Learning</title><link>http://arxiv.org/abs/2309.14989v1</link><description>We first raise and tackle ``time synchronization'' issue between the agentand the environment in non-stationary reinforcement learning (RL), a crucialfactor hindering its real-world applications. In reality, environmental changesoccur over wall-clock time ($\mathfrak{t}$) rather than episode progress ($k$),where wall-clock time signifies the actual elapsed time within the fixedduration $\mathfrak{t} \in [0, T]$. In existing works, at episode $k$, theagent rollouts a trajectory and trains a policy before transitioning to episode$k+1$. In the context of the time-desynchronized environment, however, theagent at time $\mathfrak{t}_k$ allocates $\Delta \mathfrak{t}$ for trajectorygeneration and training, subsequently moves to the next episode at$\mathfrak{t}_{k+1}=\mathfrak{t}_{k}+\Delta \mathfrak{t}$. Despite a fixedtotal episode ($K$), the agent accumulates different trajectories influenced bythe choice of \textit{interaction times}($\mathfrak{t}_1,\mathfrak{t}_2,...,\mathfrak{t}_K$), significantly impactingthe sub-optimality gap of policy. We propose a Proactively Synchronizing Tempo(ProST) framework that computes optimal $\{\mathfrak{t}_1,\mathfrak{t}_2,...,\mathfrak{t}_K \} (= \{ \mathfrak{t}\}_{1:K})$. Our main contribution is that we show optimal $\{ \mathfrak{t}\}_{1:K}$ trades-off between the policy training time (agent tempo) and howfast the environment changes (environment tempo). Theoretically, this workestablishes an optimal $\{ \mathfrak{t} \}_{1:K}$ as a function of the degreeof the environment's non-stationarity while also achieving a sublinear dynamicregret. Our experimental evaluation on various high dimensional non-stationaryenvironments shows that the ProST framework achieves a higher online return atoptimal $\{ \mathfrak{t} \}_{1:K}$ than the existing methods.</description><author>Hyunin Lee, Yuhao Ding, Jongmin Lee, Ming Jin, Javad Lavaei, Somayeh Sojoudi</author><pubDate>Tue, 26 Sep 2023 16:01:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14989v1</guid></item><item><title>Online Kernel CUSUM for Change-Point Detection</title><link>http://arxiv.org/abs/2211.15070v4</link><description>We propose an efficient online kernel Cumulative Sum (CUSUM) method forchange-point detection that utilizes the maximum over a set of kernelstatistics to account for the unknown change-point location. Our approachexhibits increased sensitivity to small changes compared to existing methods,such as the Scan-B statistic, which corresponds to a non-parametric Shewhartchart-type procedure. We provide accurate analytic approximations for two keyperformance metrics: the Average Run Length (ARL) and Expected Detection Delay(EDD), which enable us to establish an optimal window length on the order ofthe logarithm of ARL to ensure minimal power loss relative to an oracleprocedure with infinite memory. Such a finding parallels the classic result forwindow-limited Generalized Likelihood Ratio (GLR) procedure in parametricchange-point detection literature. Moreover, we introduce a recursivecalculation procedure for detection statistics to ensure constant computationaland memory complexity, which is essential for online procedures. Throughextensive experiments on both simulated and real data, we demonstrate thecompetitive performance of our method and validate our theoretical results.</description><author>Song Wei, Yao Xie</author><pubDate>Tue, 26 Sep 2023 16:01:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15070v4</guid></item><item><title>From Hours to Seconds: Towards 100x Faster Quantitative Phase Imaging via Differentiable Microscopy</title><link>http://arxiv.org/abs/2205.11521v2</link><description>With applications ranging from metabolomics to histopathology, quantitativephase microscopy (QPM) is a powerful label-free imaging modality. Despitesignificant advances in fast multiplexed imaging sensors anddeep-learning-based inverse solvers, the throughput of QPM is currently limitedby the speed of electronic hardware. Complementarily, to improve throughputfurther, here we propose to acquire images in a compressed form such that moreinformation can be transferred beyond the existing electronic hardwarebottleneck. To this end, we present a learnable opticalcompression-decompression framework that learns content-specific features. Theproposed differentiable quantitative phase microscopy ($\partial \mu$) firstuses learnable optical feature extractors as image compressors. The intensityrepresentation produced by these networks is then captured by the imagingsensor. Finally, a reconstruction network running on electronic hardwaredecompresses the QPM images. In numerical experiments, the proposed systemachieves compression of $\times$ 64 while maintaining the SSIM of $\sim 0.90$and PSNR of $\sim 30$ dB on cells. The results demonstrated by our experimentsopen up a new pathway for achieving end-to-end optimized (i.e., optics andelectronic) compact QPM systems that may provide unprecedented throughputimprovements.</description><author>Udith Haputhanthri, Kithmini Herath, Ramith Hettiarachchi, Hasindu Kariyawasam, Azeem Ahmad, Balpreet S. Ahluwalia, Chamira U. S. Edussooriya, Dushan N. Wadduwage</author><pubDate>Tue, 26 Sep 2023 16:00:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.11521v2</guid></item><item><title>Statistical Analysis of Quantum State Learning Process in Quantum Neural Networks</title><link>http://arxiv.org/abs/2309.14980v1</link><description>Quantum neural networks (QNNs) have been a promising framework in pursuingnear-term quantum advantage in various fields, where many applications can beviewed as learning a quantum state that encodes useful data. As a quantumanalog of probability distribution learning, quantum state learning istheoretically and practically essential in quantum machine learning. In thispaper, we develop a no-go theorem for learning an unknown quantum state withQNNs even starting from a high-fidelity initial state. We prove that when theloss value is lower than a critical threshold, the probability of avoidinglocal minima vanishes exponentially with the qubit count, while only growspolynomially with the circuit depth. The curvature of local minima isconcentrated to the quantum Fisher information times a loss-dependent constant,which characterizes the sensibility of the output state with respect toparameters in QNNs. These results hold for any circuit structures,initialization strategies, and work for both fixed ansatzes and adaptivemethods. Extensive numerical simulations are performed to validate ourtheoretical results. Our findings place generic limits on good initial guessesand adaptive methods for improving the learnability and scalability of QNNs,and deepen the understanding of prior information's role in QNNs.</description><author>Hao-kai Zhang, Chenghong Zhu, Mingrui Jing, Xin Wang</author><pubDate>Tue, 26 Sep 2023 15:54:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14980v1</guid></item><item><title>MoCaE: Mixture of Calibrated Experts Significantly Improves Object Detection</title><link>http://arxiv.org/abs/2309.14976v1</link><description>We propose an extremely simple and highly effective approach to faithfullycombine different object detectors to obtain a Mixture of Experts (MoE) thathas a superior accuracy to the individual experts in the mixture. We find thatnaively combining these experts in a similar way to the well-known DeepEnsembles (DEs), does not result in an effective MoE. We identify theincompatibility between the confidence score distribution of differentdetectors to be the primary reason for such failure cases. Therefore, toconstruct the MoE, our proposal is to first calibrate each individual detectoragainst a target calibration function. Then, filter and refine all thepredictions from different detectors in the mixture. We term this approach asMoCaE and demonstrate its effectiveness through extensive experiments on objectdetection, instance segmentation and rotated object detection tasks.Specifically, MoCaE improves (i) three strong object detectors on COCO test-devby $2.4$ $\mathrm{AP}$ by reaching $59.0$ $\mathrm{AP}$; (ii) instancesegmentation methods on the challenging long-tailed LVIS dataset by $2.3$$\mathrm{AP}$; and (iii) all existing rotated object detectors by reaching$82.62$ $\mathrm{AP_{50}}$ on DOTA dataset, establishing a new state-of-the-art(SOTA). Code will be made public.</description><author>Kemal Oksuz, Selim Kuzucu, Tom Joy, Puneet K. Dokania</author><pubDate>Tue, 26 Sep 2023 15:52:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14976v1</guid></item><item><title>Causal Graph Discovery from Self and Mutually Exciting Time Series</title><link>http://arxiv.org/abs/2106.02600v5</link><description>We present a generalized linear structural causal model, coupled with a noveldata-adaptive linear regularization, to recover causal directed acyclic graphs(DAGs) from time series. By leveraging a recently developed stochastic monotoneVariational Inequality (VI) formulation, we cast the causal discovery problemas a general convex optimization. Furthermore, we develop a non-asymptoticrecovery guarantee and quantifiable uncertainty by solving a linear program toestablish confidence intervals for a wide range of non-linear monotone linkfunctions. We validate our theoretical results and show the competitiveperformance of our method via extensive numerical experiments. Mostimportantly, we demonstrate the effectiveness of our approach in recoveringhighly interpretable causal DAGs over Sepsis Associated Derangements (SADs)while achieving comparable prediction performance to powerful ``black-box''models such as XGBoost. Thus, the future adoption of our proposed method toconduct continuous surveillance of high-risk patients by clinicians is muchmore likely.</description><author>Song Wei, Yao Xie, Christopher S. Josef, Rishikesan Kamaleswaran</author><pubDate>Tue, 26 Sep 2023 15:50:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.02600v5</guid></item><item><title>Improving Unsupervised Visual Program Inference with Code Rewriting Families</title><link>http://arxiv.org/abs/2309.14972v1</link><description>Programs offer compactness and structure that makes them an attractiverepresentation for visual data. We explore how code rewriting can be used toimprove systems for inferring programs from visual data. We first proposeSparse Intermittent Rewrite Injection (SIRI), a framework for unsupervisedbootstrapped learning. SIRI sparsely applies code rewrite operations over adataset of training programs, injecting the improved programs back into thetraining set. We design a family of rewriters for visual programming domains:parameter optimization, code pruning, and code grafting. For three shapeprogramming languages in 2D and 3D, we show that using SIRI with our family ofrewriters improves performance: better reconstructions and faster convergencerates, compared with bootstrapped learning methods that do not use rewriters oruse them naively. Finally, we demonstrate that our family of rewriters can beeffectively used at test time to improve the output of SIRI predictions. For 2Dand 3D CSG, we outperform or match the reconstruction performance of recentdomain-specific neural architectures, while producing more parsimoniousprograms that use significantly fewer primitives.</description><author>Aditya Ganeshan, R. Kenny Jones, Daniel Ritchie</author><pubDate>Tue, 26 Sep 2023 15:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14972v1</guid></item><item><title>A Unified Algorithm Framework for Unsupervised Discovery of Skills based on Determinantal Point Process</title><link>http://arxiv.org/abs/2212.00211v3</link><description>Learning rich skills under the option framework without supervision ofexternal rewards is at the frontier of reinforcement learning research.Existing works mainly fall into two distinctive categories: variational optiondiscovery that maximizes the diversity of the options through a mutualinformation loss (while ignoring coverage) and Laplacian-based methods thatfocus on improving the coverage of options by increasing connectivity of thestate space (while ignoring diversity). In this paper, we show that diversityand coverage in unsupervised option discovery can indeed be unified under thesame mathematical framework. To be specific, we explicitly quantify thediversity and coverage of the learned options through a novel use ofDeterminantal Point Process (DPP) and optimize these objectives to discoveroptions with both superior diversity and coverage. Our proposed algorithm,ODPP, has undergone extensive evaluation on challenging tasks created withMujoco and Atari. The results demonstrate that our algorithm outperformsstate-of-the-art baselines in both diversity- and coverage-driven categories.</description><author>Jiayu Chen, Vaneet Aggarwal, Tian Lan</author><pubDate>Tue, 26 Sep 2023 15:44:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00211v3</guid></item><item><title>Recurrent Hypernetworks are Surprisingly Strong in Meta-RL</title><link>http://arxiv.org/abs/2309.14970v1</link><description>Deep reinforcement learning (RL) is notoriously impractical to deploy due tosample inefficiency. Meta-RL directly addresses this sample inefficiency bylearning to perform few-shot learning when a distribution of related tasks isavailable for meta-training. While many specialized meta-RL methods have beenproposed, recent work suggests that end-to-end learning in conjunction with anoff-the-shelf sequential model, such as a recurrent network, is a surprisinglystrong baseline. However, such claims have been controversial due to limitedsupporting evidence, particularly in the face of prior work establishingprecisely the opposite. In this paper, we conduct an empirical investigation.While we likewise find that a recurrent network can achieve strong performance,we demonstrate that the use of hypernetworks is crucial to maximizing theirpotential. Surprisingly, when combined with hypernetworks, the recurrentbaselines that are far simpler than existing specialized methods actuallyachieve the strongest performance of all methods evaluated.</description><author>Jacob Beck, Risto Vuorio, Zheng Xiong, Shimon Whiteson</author><pubDate>Tue, 26 Sep 2023 15:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14970v1</guid></item><item><title>Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantization</title><link>http://arxiv.org/abs/2309.13575v2</link><description>Weight-sharing quantization has emerged as a technique to reduce energyexpenditure during inference in large neural networks by constraining theirweights to a limited set of values. However, existing methods forweight-sharing quantization often make assumptions about the treatment ofweights based on value alone that neglect the unique role weight positionplays. This paper proposes a probabilistic framework based on Bayesian neuralnetworks (BNNs) and a variational relaxation to identify which weights can bemoved to which cluster centre and to what degree based on their individualposition-specific learned uncertainty distributions. We introduce a newinitialisation setting and a regularisation term which allow for the trainingof BNNs under complex dataset-model combinations. By leveraging the flexibilityof weight values captured through a probability distribution, we enhance noiseresilience and downstream compressibility. Our iterative clustering proceduredemonstrates superior compressibility and higher accuracy compared tostate-of-the-art methods on both ResNet models and the more complextransformer-based architectures. In particular, our method outperforms thestate-of-the-art quantization method top-1 accuracy by 1.6% on ImageNet usingDeiT-Tiny, with its 5 million+ weights now represented by only 296 uniquevalues.</description><author>Christopher Subia-Waud, Srinandan Dasmahapatra</author><pubDate>Tue, 26 Sep 2023 15:42:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13575v2</guid></item><item><title>A novel approach for holographic 3D content generation without depth map</title><link>http://arxiv.org/abs/2309.14967v1</link><description>In preparation for observing holographic 3D content, acquiring a set of RGBcolor and depth map images per scene is necessary to generatecomputer-generated holograms (CGHs) when using the fast Fourier transform (FFT)algorithm. However, in real-world situations, these paired formats of RGB colorand depth map images are not always fully available. We propose a deeplearning-based method to synthesize the volumetric digital holograms using onlythe given RGB image, so that we can overcome environments where RGB color anddepth map images are partially provided. The proposed method uses only theinput of RGB image to estimate its depth map and then generate its CGHsequentially. Through experiments, we demonstrate that the volumetric hologramgenerated through our proposed model is more accurate than that of competitivemodels, under the situation that only RGB color data can be provided.</description><author>Hakdong Kim, Minkyu Jee, Yurim Lee, Kyudam Choi, MinSung Yoon, Cheongwon Kim</author><pubDate>Tue, 26 Sep 2023 15:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14967v1</guid></item><item><title>Interactively Learning Social Media Representations Improves News Source Factuality Detection</title><link>http://arxiv.org/abs/2309.14966v1</link><description>The rise of social media has enabled the widespread propagation of fake news,text that is published with an intent to spread misinformation and swaybeliefs. Rapidly detecting fake news, especially as new events arise, isimportant to prevent misinformation. While prior works have tackled this problem using supervised learningsystems, automatedly modeling the complexities of the social media landscapethat enables the spread of fake news is challenging. On the contrary, havinghumans fact check all news is not scalable. Thus, in this paper, we propose toapproach this problem interactively, where humans can interact to help anautomated system learn a better social media representation quality. On realworld events, our experiments show performance improvements in detectingfactuality of news sources, even after few human interactions.</description><author>Nikhil Mehta, Dan Goldwasser</author><pubDate>Tue, 26 Sep 2023 15:36:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14966v1</guid></item><item><title>LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning</title><link>http://arxiv.org/abs/2307.02345v2</link><description>Modern reinforcement learning (RL) can be categorized into online and offlinevariants. As a pivotal aspect of both online and offline RL, current researchon the Bellman equation revolves primarily around optimization techniques andperformance enhancement rather than exploring the inherent structuralproperties of the Bellman error, such as its distribution characteristics. Thisstudy investigates the distribution of the Bellman approximation error in bothonline and offline settings through iterative exploration of the Bellmanequation. We observed that both in online RL and offline RL, the Bellman errorconforms to a Logistic distribution. Building upon this discovery, this studyemployed the Logistics maximum likelihood function (LLoss) as an alternative tothe commonly used MSE Loss, assuming that Bellman errors adhere to a normaldistribution. We validated our hypotheses through extensive numericalexperiments across diverse online and offline environments. In particular, weapplied corrections to the loss function across various baseline algorithms andconsistently observed that the loss function with Logistic correctionsoutperformed the MSE counterpart significantly. Additionally, we conductedKolmogorov-Smirnov tests to confirm the reliability of the Logisticdistribution. This study's theoretical and empirical insights provide valuablegroundwork for future investigations and enhancements centered on thedistribution of Bellman errors.</description><author>Outongyi Lv, Bingxin Zhou</author><pubDate>Tue, 26 Sep 2023 15:34:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02345v2</guid></item><item><title>GridFormer: Towards Accurate Table Structure Recognition via Grid Prediction</title><link>http://arxiv.org/abs/2309.14962v1</link><description>All tables can be represented as grids. Based on this observation, we proposeGridFormer, a novel approach for interpreting unconstrained table structures bypredicting the vertex and edge of a grid. First, we propose a flexible tablerepresentation in the form of an MXN grid. In this representation, the vertexesand edges of the grid store the localization and adjacency information of thetable. Then, we introduce a DETR-style table structure recognizer toefficiently predict this multi-objective information of the grid in a singleshot. Specifically, given a set of learned row and column queries, therecognizer directly outputs the vertexes and edges information of thecorresponding rows and columns. Extensive experiments on five challengingbenchmarks which include wired, wireless, multi-merge-cell, oriented, anddistorted tables demonstrate the competitive performance of our model overother methods.</description><author>Pengyuan Lyu, Weihong Ma, Hongyi Wang, Yuechen Yu, Chengquan Zhang, Kun Yao, Yang Xue, Jingdong Wang</author><pubDate>Tue, 26 Sep 2023 15:29:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14962v1</guid></item><item><title>Exploiting Semantic Role Contextualized Video Features for Multi-Instance Text-Video Retrieval EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022</title><link>http://arxiv.org/abs/2206.14381v2</link><description>In this report, we present our approach for EPIC-KITCHENS-100 Multi-InstanceRetrieval Challenge 2022. We first parse sentences into semantic rolescorresponding to verbs and nouns; then utilize self-attentions to exploitsemantic role contextualized video features along with textual features viatriplet losses in multiple embedding spaces. Our method overpasses the strongbaseline in normalized Discounted Cumulative Gain (nDCG), which is morevaluable for semantic similarity. Our submission is ranked 3rd for nDCG andranked 4th for mAP.</description><author>Burak Satar, Hongyuan Zhu, Hanwang Zhang, Joo Hwee Lim</author><pubDate>Tue, 26 Sep 2023 15:27:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.14381v2</guid></item><item><title>Context-Aware Generative Models for Prediction of Aircraft Ground Tracks</title><link>http://arxiv.org/abs/2309.14957v1</link><description>Trajectory prediction (TP) plays an important role in supporting thedecision-making of Air Traffic Controllers (ATCOs). Traditional TP methods aredeterministic and physics-based, with parameters that are calibrated usingaircraft surveillance data harvested across the world. These models are,therefore, agnostic to the intentions of the pilots and ATCOs, which can have asignificant effect on the observed trajectory, particularly in the lateralplane. This work proposes a generative method for lateral TP, usingprobabilistic machine learning to model the effect of the epistemic uncertaintyarising from the unknown effect of pilot behaviour and ATCO intentions. Themodels are trained to be specific to a particular sector, allowing localprocedures such as coordinated entry and exit points to be modelled. A datasetcomprising a week's worth of aircraft surveillance data, passing through a busysector of the United Kingdom's upper airspace, was used to train and test themodels. Specifically, a piecewise linear model was used as a functional,low-dimensional representation of the ground tracks, with its control pointsdetermined by a generative model conditioned on partial context. It was foundthat, of the investigated models, a Bayesian Neural Network using the Laplaceapproximation was able to generate the most plausible trajectories in order toemulate the flow of traffic through the sector.</description><author>Nick Pepper, George De Ath, Marc Thomas, Richard Everson, Tim Dodwell</author><pubDate>Tue, 26 Sep 2023 15:20:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14957v1</guid></item><item><title>Addressing preferred orientation in single-particle cryo-EM through AI-generated auxiliary particles</title><link>http://arxiv.org/abs/2309.14954v1</link><description>The single-particle cryo-EM field faces the persistent challenge of preferredorientation, lacking general computational solutions. We introduce cryoPROS, anAI-based approach designed to address the above issue. By generating theauxiliary particles with a conditional deep generative model, cryoPROSaddresses the intrinsic bias in orientation estimation for the observedparticles. We effectively employed cryoPROS in the cryo-EM single particleanalysis of the hemagglutinin trimer, showing the ability to restore thenear-atomic resolution structure on non-tilt data. Moreover, the enhancedversion named cryoPROS-MP significantly improves the resolution of the membraneprotein NaX using the no-tilted data that contains the effects of micelles.Compared to the classical approaches, cryoPROS does not need specialexperimental or image acquisition techniques, providing a purely computationalyet effective solution for the preferred orientation problem. Finally, weconduct extensive experiments that establish the low risk of model bias and thehigh robustness of cryoPROS.</description><author>Hui Zhang, Dihan Zheng, Qiurong Wu, Nieng Yan, Zuoqiang Shi, Mingxu Hu, Chenglong Bao</author><pubDate>Tue, 26 Sep 2023 15:14:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14954v1</guid></item><item><title>Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean-teacher</title><link>http://arxiv.org/abs/2309.14950v1</link><description>Adapting visual object detectors to operational target domains is achallenging task, commonly achieved using unsupervised domain adaptation (UDA)methods. When the labeled dataset is coming from multiple source domains,treating them as separate domains and performing a multi-source domainadaptation (MSDA) improves the accuracy and robustness over mixing these sourcedomains and performing a UDA, as observed by recent studies in MSDA. ExistingMSDA methods learn domain invariant and domain-specific parameters (for eachsource domain) for the adaptation. However, unlike single-source UDA methods,learning domain-specific parameters makes them grow significantly proportionalto the number of source domains used. This paper proposes a novel MSDA methodcalled Prototype-based Mean-Teacher (PMT), which uses class prototypes insteadof domain-specific subnets to preserve domain-specific information. Theseprototypes are learned using a contrastive loss, aligning the same categoriesacross domains and separating different categories far apart. Because of theuse of prototypes, the parameter size of our method does not increasesignificantly with the number of source domains, thus reducing memory issuesand possible overfitting. Empirical studies show PMT outperformsstate-of-the-art MSDA methods on several challenging object detection datasets.</description><author>Atif Belal, Akhil Meethal, Francisco Perdigon Romero, Marco Pedersoli, Eric Granger</author><pubDate>Tue, 26 Sep 2023 15:08:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14950v1</guid></item><item><title>Towards Real-World Test-Time Adaptation: Tri-Net Self-Training with Balanced Normalization</title><link>http://arxiv.org/abs/2309.14949v1</link><description>Test-Time Adaptation aims to adapt source domain model to testing data atinference stage with success demonstrated in adapting to unseen corruptions.However, these attempts may fail under more challenging real-world scenarios.Existing works mainly consider real-world test-time adaptation under non-i.i.d.data stream and continual domain shift. In this work, we first complement theexisting real-world TTA protocol with a globally class imbalanced testing set.We demonstrate that combining all settings together poses new challenges toexisting methods. We argue the failure of state-of-the-art methods is firstcaused by indiscriminately adapting normalization layers to imbalanced testingdata. To remedy this shortcoming, we propose a balanced batchnorm layer to swapout the regular batchnorm at inference stage. The new batchnorm layer iscapable of adapting without biasing towards majority classes. We are furtherinspired by the success of self-training~(ST) in learning from unlabeled dataand adapt ST for test-time adaptation. However, ST alone is prone to overadaption which is responsible for the poor performance under continual domainshift. Hence, we propose to improve self-training under continual domain shiftby regularizing model updates with an anchored loss. The final TTA model,termed as TRIBE, is built upon a tri-net architecture with balanced batchnormlayers. We evaluate TRIBE on four datasets representing real-world TTAsettings. TRIBE consistently achieves the state-of-the-art performance acrossmultiple evaluation protocols. The code is available at\url{https://github.com/Gorilla-Lab-SCUT/TRIBE}.</description><author>Yongyi Su, Xun Xu, Kui Jia</author><pubDate>Tue, 26 Sep 2023 15:06:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14949v1</guid></item><item><title>A Machine Learning-oriented Survey on Tiny Machine Learning</title><link>http://arxiv.org/abs/2309.11932v2</link><description>The emergence of Tiny Machine Learning (TinyML) has positively revolutionizedthe field of Artificial Intelligence by promoting the joint design ofresource-constrained IoT hardware devices and their learning-based softwarearchitectures. TinyML carries an essential role within the fourth and fifthindustrial revolutions in helping societies, economies, and individuals employeffective AI-infused computing technologies (e.g., smart cities, automotive,and medical robotics). Given its multidisciplinary nature, the field of TinyMLhas been approached from many different angles: this comprehensive surveywishes to provide an up-to-date overview focused on all the learning algorithmswithin TinyML-based solutions. The survey is based on the Preferred ReportingItems for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow,allowing for a systematic and complete literature survey. In particular,firstly we will examine the three different workflows for implementing aTinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly,we propose a taxonomy that covers the learning panorama under the TinyML lens,examining in detail the different families of model optimization and design, aswell as the state-of-the-art learning techniques. Thirdly, this survey willpresent the distinct features of hardware devices and software tools thatrepresent the current state-of-the-art for TinyML intelligent edgeapplications. Finally, we discuss the challenges and future directions.</description><author>Luigi Capogrosso, Federico Cunico, Dong Seon Cheng, Franco Fummi, Marco Cristani</author><pubDate>Tue, 26 Sep 2023 15:03:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11932v2</guid></item><item><title>Kinship Representation Learning with Face Componential Relation</title><link>http://arxiv.org/abs/2304.04546v4</link><description>Kinship recognition aims to determine whether the subjects in two facialimages are kin or non-kin, which is an emerging and challenging problem.However, most previous methods focus on heuristic designs without consideringthe spatial correlation between face images. In this paper, we aim to learndiscriminative kinship representations embedded with the relation informationbetween face components (e.g., eyes, nose, etc.). To achieve this goal, wepropose the Face Componential Relation Network, which learns the relationshipbetween face components among images with a cross-attention mechanism, whichautomatically learns the important facial regions for kinship recognition.Moreover, we propose Face Componential Relation Network (FaCoRNet), whichadapts the loss function by the guidance from cross-attention to learn morediscriminative feature representations. The proposed FaCoRNet outperformsprevious state-of-the-art methods by large margins for the largest publickinship recognition FIW benchmark.</description><author>Weng-Tai Su, Min-Hung Chen, Chien-Yi Wang, Shang-Hong Lai, Trista Pei-Chun Chen</author><pubDate>Tue, 26 Sep 2023 15:00:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04546v4</guid></item><item><title>Hierarchical Deep Counterfactual Regret Minimization</title><link>http://arxiv.org/abs/2305.17327v2</link><description>Imperfect Information Games (IIGs) offer robust models for scenarios wheredecision-makers face uncertainty or lack complete information. CounterfactualRegret Minimization (CFR) has been one of the most successful family ofalgorithms for tackling IIGs. The integration of skill-based strategy learningwith CFR could potentially mirror more human-like decision-making process andenhance the learning performance for complex IIGs. It enables the learning of ahierarchical strategy, wherein low-level components represent skills forsolving subgames and the high-level component manages the transition betweenskills. In this paper, we introduce the first hierarchical version of Deep CFR(HDCFR), an innovative method that boosts learning efficiency in tasksinvolving extensively large state spaces and deep game trees. A notableadvantage of HDCFR over previous works is its ability to facilitate learningwith predefined (human) expertise and foster the acquisition of skills that canbe transferred to similar tasks. To achieve this, we initially construct ouralgorithm on a tabular setting, encompassing hierarchical CFR updating rulesand a variance-reduced Monte Carlo sampling extension. Notably, we offer thetheoretical justifications, including the convergence rate of the proposedupdating rule, the unbiasedness of the Monte Carlo regret estimator, and idealcriteria for effective variance reduction. Then, we employ neural networks asfunction approximators and develop deep learning objectives to adapt ourproposed algorithms for large-scale tasks, while maintaining the theoreticalsupport.</description><author>Jiayu Chen, Tian Lan, Vaneet Aggarwal</author><pubDate>Tue, 26 Sep 2023 14:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17327v2</guid></item><item><title>Learning Generative Models for Climbing Aircraft from Radar Data</title><link>http://arxiv.org/abs/2309.14941v1</link><description>Accurate trajectory prediction (TP) for climbing aircraft is hampered by thepresence of epistemic uncertainties concerning aircraft operation, which canlead to significant misspecification between predicted and observedtrajectories. This paper proposes a generative model for climbing aircraft inwhich the standard Base of Aircraft Data (BADA) model is enriched by afunctional correction to the thrust that is learned from data. The methodoffers three features: predictions of the arrival time with 66.3% less errorwhen compared to BADA; generated trajectories that are realistic when comparedto test data; and a means of computing confidence bounds for minimalcomputational cost.</description><author>Nick Pepper, Marc Thomas</author><pubDate>Tue, 26 Sep 2023 14:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14941v1</guid></item><item><title>Parallel Multi-Objective Hyperparameter Optimization with Uniform Normalization and Bounded Objectives</title><link>http://arxiv.org/abs/2309.14936v1</link><description>Machine learning (ML) methods offer a wide range of configurablehyperparameters that have a significant influence on their performance. Whileaccuracy is a commonly used performance objective, in many settings, it is notsufficient. Optimizing the ML models with respect to multiple objectives suchas accuracy, confidence, fairness, calibration, privacy, latency, and memoryconsumption is becoming crucial. To that end, hyperparameter optimization, theapproach to systematically optimize the hyperparameters, which is alreadychallenging for a single objective, is even more challenging for multipleobjectives. In addition, the differences in objective scales, the failures, andthe presence of outlier values in objectives make the problem even harder. Wepropose a multi-objective Bayesian optimization (MoBO) algorithm that addressesthese problems through uniform objective normalization and randomized weightsin scalarization. We increase the efficiency of our approach by imposingconstraints on the objective to avoid exploring unnecessary configurations(e.g., insufficient accuracy). Finally, we leverage an approach to parallelizethe MoBO which results in a 5x speed-up when using 16x more workers.</description><author>Romain Egele, Tyler Chang, Yixuan Sun, Venkatram Vishwanath, Prasanna Balaprakash</author><pubDate>Tue, 26 Sep 2023 14:48:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14936v1</guid></item><item><title>Zero-shot Visual Relation Detection via Composite Visual Cues from Large Language Models</title><link>http://arxiv.org/abs/2305.12476v2</link><description>Pretrained vision-language models, such as CLIP, have demonstrated stronggeneralization capabilities, making them promising tools in the realm ofzero-shot visual recognition. Visual relation detection (VRD) is a typical taskthat identifies relationship (or interaction) types between object pairs withinan image. However, naively utilizing CLIP with prevalent class-based promptsfor zero-shot VRD has several weaknesses, e.g., it struggles to distinguishbetween different fine-grained relation types and it neglects essential spatialinformation of two objects. To this end, we propose a novel method forzero-shot VRD: RECODE, which solves RElation detection via COmpositeDEscription prompts. Specifically, RECODE first decomposes each predicatecategory into subject, object, and spatial components. Then, it leverages largelanguage models (LLMs) to generate description-based prompts (or visual cues)for each component. Different visual cues enhance the discriminability ofsimilar relation categories from different perspectives, which significantlyboosts performance in VRD. To dynamically fuse different cues, we furtherintroduce a chain-of-thought method that prompts LLMs to generate reasonableweights for different visual cues. Extensive experiments on four VRD benchmarkshave demonstrated the effectiveness and interpretability of RECODE.</description><author>Lin Li, Jun Xiao, Guikun Chen, Jian Shao, Yueting Zhuang, Long Chen</author><pubDate>Tue, 26 Sep 2023 14:44:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12476v2</guid></item><item><title>FEC: Three Finetuning-free Methods to Enhance Consistency for Real Image Editing</title><link>http://arxiv.org/abs/2309.14934v1</link><description>Text-conditional image editing is a very useful task that has recentlyemerged with immeasurable potential. Most current real image editing methodsfirst need to complete the reconstruction of the image, and then editing iscarried out by various methods based on the reconstruction. Most methods useDDIM Inversion for reconstruction, however, DDIM Inversion often fails toguarantee reconstruction performance, i.e., it fails to produce results thatpreserve the original image content. To address the problem of reconstructionfailure, we propose FEC, which consists of three sampling methods, eachdesigned for different editing types and settings. Our three methods of FECachieve two important goals in image editing task: 1) ensuring successfulreconstruction, i.e., sampling to get a generated result that preserves thetexture and features of the original real image. 2) these sampling methods canbe paired with many editing methods and greatly improve the performance ofthese editing methods to accomplish various editing tasks. In addition, none ofour sampling methods require fine-tuning of the diffusion model ortime-consuming training on large-scale datasets. Hence the cost of time as wellas the use of computer memory and computation can be significantly reduced.</description><author>Songyan Chen, Jiancheng Huang</author><pubDate>Tue, 26 Sep 2023 14:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14934v1</guid></item><item><title>Addressing Data Misalignment in Image-LiDAR Fusion on Point Cloud Segmentation</title><link>http://arxiv.org/abs/2309.14932v1</link><description>With the advent of advanced multi-sensor fusion models, there has been anotable enhancement in the performance of perception tasks within in terms ofautonomous driving. Despite these advancements, the challenges persist,particularly in the fusion of data from cameras and LiDAR sensors. A critialconcern is the accurate alignment of data from these disparate sensors. Ourobservations indicate that the projected positions of LiDAR points oftenmisalign on the corresponding image. Furthermore, fusion models appear tostruggle in accurately segmenting these misaligned points. In this paper, wewould like to address this problem carefully, with a specific focus on thenuScenes dataset and the SOTA of fusion models 2DPASS, and providing thepossible solutions or potential improvements.</description><author>Wei Jong Yang, Guan Cheng Lee</author><pubDate>Tue, 26 Sep 2023 14:41:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14932v1</guid></item><item><title>Noise-Tolerant Unsupervised Adapter for Vision-Language Models</title><link>http://arxiv.org/abs/2309.14928v1</link><description>Recent advances in large-scale vision-language models have achieved veryimpressive performance in various zero-shot image classification tasks. Whileprior studies have demonstrated significant improvements by introducingfew-shot labelled target samples, they still require labelling of targetsamples, which greatly degrades their scalability while handling various visualrecognition tasks. We design NtUA, a Noise-tolerant Unsupervised Adapter thatallows learning superior target models with few-shot unlabelled target samples.NtUA works as a key-value cache that formulates visual features and predictedpseudo-labels of the few-shot unlabelled target samples as key-value pairs. Itconsists of two complementary designs. The first is adaptive cache formationthat combats pseudo-label noises by weighting the key-value pairs according totheir prediction confidence. The second is pseudo-label rectification, whichcorrects both pair values (i.e., pseudo-labels) and cache weights by leveragingknowledge distillation from large-scale vision language models. Extensiveexperiments show that NtUA achieves superior performance consistently acrossmultiple widely adopted benchmarks.</description><author>Eman Ali, Dayan Guan, Shijian Lu, Abdulmotaleb Elsaddik</author><pubDate>Tue, 26 Sep 2023 14:35:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14928v1</guid></item><item><title>A Democratic Platform for Engaging with Disabled Community in Generative AI Development</title><link>http://arxiv.org/abs/2309.14921v1</link><description>Artificial Intelligence (AI) systems, especially generative AI technologiesare becoming more relevant in our society. Tools like ChatGPT are being used bymembers of the disabled community e.g., Autistic people may use it to helpcompose emails. The growing impact and popularity of generative AI tools haveprompted us to examine their relevance within the disabled community. Thedesign and development phases often neglect this marginalized group, leading toinaccurate predictions and unfair discrimination directed towards them. Thiscould result from bias in data sets, algorithms, and systems at various phasesof creation and implementation. This workshop paper proposes a platform toinvolve the disabled community while building generative AI systems. With thisplatform, our aim is to gain insight into the factors that contribute to biasin the outputs generated by generative AI when used by the disabled community.Furthermore, we expect to comprehend which algorithmic factors are the maincontributors to the output's incorrectness or irrelevancy. The proposedplatform calls on both disabled and non-disabled people from variousgeographical and cultural backgrounds to collaborate asynchronously andremotely in a democratic approach to decision-making.</description><author>Deepak Giri, Erin Brady</author><pubDate>Tue, 26 Sep 2023 14:30:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14921v1</guid></item><item><title>PHRIT: Parametric Hand Representation with Implicit Template</title><link>http://arxiv.org/abs/2309.14916v1</link><description>We propose PHRIT, a novel approach for parametric hand mesh modeling with animplicit template that combines the advantages of both parametric meshes andimplicit representations. Our method represents deformable hand shapes usingsigned distance fields (SDFs) with part-based shape priors, utilizing adeformation field to execute the deformation. The model offers efficienthigh-fidelity hand reconstruction by deforming the canonical template atinfinite resolution. Additionally, it is fully differentiable and can be easilyused in hand modeling since it can be driven by the skeleton and shape latentcodes. We evaluate PHRIT on multiple downstream tasks, includingskeleton-driven hand reconstruction, shapes from point clouds, and single-view3D reconstruction, demonstrating that our approach achieves realistic andimmersive hand modeling with state-of-the-art performance.</description><author>Zhisheng Huang, Yujin Chen, Di Kang, Jinlu Zhang, Zhigang Tu</author><pubDate>Tue, 26 Sep 2023 14:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14916v1</guid></item><item><title>Optimization with Access to Auxiliary Information</title><link>http://arxiv.org/abs/2206.00395v3</link><description>We investigate the fundamental optimization question of minimizing a targetfunction $f$, whose gradients are expensive to compute or have limitedavailability, given access to some auxiliary side function $h$ whose gradientsare cheap or more available. This formulation captures many settings ofpractical relevance, such as i) re-using batches in SGD, ii) transfer learning,iii) federated learning, iv) training with compressed models/dropout, etc. Wepropose two generic new algorithms that apply in all these settings and provethat we can benefit from this framework using only an assumption on the Hessiansimilarity between the target and side information. A benefit is obtained whenthis similarity measure is small, we also show a potential benefit fromstochasticity when the auxiliary noise is correlated with that of the targetfunction.</description><author>El Mahdi Chayti, Sai Praneeth Karimireddy</author><pubDate>Tue, 26 Sep 2023 14:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.00395v3</guid></item><item><title>Global-correlated 3D-decoupling Transformer for Clothed Avatar Reconstruction</title><link>http://arxiv.org/abs/2309.13524v2</link><description>Reconstructing 3D clothed human avatars from single images is a challengingtask, especially when encountering complex poses and loose clothing. Currentmethods exhibit limitations in performance, largely attributable to theirdependence on insufficient 2D image features and inconsistent query methods.Owing to this, we present the Global-correlated 3D-decoupling Transformer forclothed Avatar reconstruction (GTA), a novel transformer-based architecturethat reconstructs clothed human avatars from monocular images. Our approachleverages transformer architectures by utilizing a Vision Transformer model asan encoder for capturing global-correlated image features. Subsequently, ourinnovative 3D-decoupling decoder employs cross-attention to decouple tri-planefeatures, using learnable embeddings as queries for cross-plane generation. Toeffectively enhance feature fusion with the tri-plane 3D feature and human bodyprior, we propose a hybrid prior fusion strategy combining spatial andprior-enhanced queries, leveraging the benefits of spatial localization andhuman body prior knowledge. Comprehensive experiments on CAPE and THuman2.0datasets illustrate that our method outperforms state-of-the-art approaches inboth geometry and texture reconstruction, exhibiting high robustness tochallenging poses and loose clothing, and producing higher-resolution textures.Codes will be available at https://github.com/River-Zhang/GTA.</description><author>Zechuan Zhang, Li Sun, Zongxin Yang, Ling Chen, Yi Yang</author><pubDate>Tue, 26 Sep 2023 14:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13524v2</guid></item><item><title>Robustness of the Random Language Model</title><link>http://arxiv.org/abs/2309.14913v1</link><description>The Random Language Model (De Giuli 2019) is an ensemble of stochasticcontext-free grammars, quantifying the syntax of human and computer languages.The model suggests a simple picture of first language learning as a type ofannealing in the vast space of potential languages. In its simplestformulation, it implies a single continuous transition to grammatical syntax,at which the symmetry among potential words and categories is spontaneouslybroken. Here this picture is scrutinized by considering its robustness againstexplicit symmetry breaking, an inevitable component of learning in the realworld. It is shown that the scenario is robust to such symmetry breaking.Comparison with human data on the clustering coefficient of syntax networkssuggests that the observed transition is equivalent to that normallyexperienced by children at age 24 months.</description><author>Fatemeh Lalegani, Eric De Giuli</author><pubDate>Tue, 26 Sep 2023 14:14:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14913v1</guid></item><item><title>Geodesic Sinkhorn for Fast and Accurate Optimal Transport on Manifolds</title><link>http://arxiv.org/abs/2211.00805v2</link><description>Efficient computation of optimal transport distance between distributions isof growing importance in data science. Sinkhorn-based methods are currently thestate-of-the-art for such computations, but require $O(n^2)$ computations. Inaddition, Sinkhorn-based methods commonly use an Euclidean ground distancebetween datapoints. However, with the prevalence of manifold structuredscientific data, it is often desirable to consider geodesic ground distance.Here, we tackle both issues by proposing Geodesic Sinkhorn -- based ondiffusing a heat kernel on a manifold graph. Notably, Geodesic Sinkhornrequires only $O(n\log n)$ computation, as we approximate the heat kernel withChebyshev polynomials based on the sparse graph Laplacian. We apply our methodto the computation of barycenters of several distributions of high dimensionalsingle cell data from patient samples undergoing chemotherapy. In particular,we define the barycentric distance as the distance between two suchbarycenters. Using this definition, we identify an optimal transport distanceand path associated with the effect of treatment on cellular data.</description><author>Guillaume Huguet, Alexander Tong, María Ramos Zapatero, Christopher J. Tape, Guy Wolf, Smita Krishnaswamy</author><pubDate>Tue, 26 Sep 2023 14:12:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.00805v2</guid></item><item><title>Face Cartoonisation For Various Poses Using StyleGAN</title><link>http://arxiv.org/abs/2309.14908v1</link><description>This paper presents an innovative approach to achieve face cartoonisationwhile preserving the original identity and accommodating various poses. Unlikeprevious methods in this field that relied on conditional-GANs, which posedchallenges related to dataset requirements and pose training, our approachleverages the expressive latent space of StyleGAN. We achieve this byintroducing an encoder that captures both pose and identity information fromimages and generates a corresponding embedding within the StyleGAN latentspace. By subsequently passing this embedding through a pre-trained generator,we obtain the desired cartoonised output. While many other approaches based onStyleGAN necessitate a dedicated and fine-tuned StyleGAN model, our methodstands out by utilizing an already-trained StyleGAN designed to producerealistic facial images. We show by extensive experimentation how our encoderadapts the StyleGAN output to better preserve identity when the objective iscartoonisation.</description><author>Kushal Jain, Ankith Varun J, Anoop Namboodiri</author><pubDate>Tue, 26 Sep 2023 14:10:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14908v1</guid></item><item><title>Label Deconvolution for Node Representation Learning on Large-scale Attributed Graphs against Learning Bias</title><link>http://arxiv.org/abs/2309.14907v1</link><description>Node representation learning on attributed graphs -- whose nodes areassociated with rich attributes (e.g., texts and protein sequences) -- plays acrucial role in many important downstream tasks. To encode the attributes andgraph structures simultaneously, recent studies integrate pre-trained modelswith graph neural networks (GNNs), where pre-trained models serve as nodeencoders (NEs) to encode the attributes. As jointly training large NEs and GNNson large-scale graphs suffers from severe scalability issues, many methodspropose to train NEs and GNNs separately. Consequently, they do not takefeature convolutions in GNNs into consideration in the training phase of NEs,leading to a significant learning bias from that by the joint training. Toaddress this challenge, we propose an efficient label regularization technique,namely Label Deconvolution (LD), to alleviate the learning bias by a novel andhighly scalable approximation to the inverse mapping of GNNs. The inversemapping leads to an objective function that is equivalent to that by the jointtraining, while it can effectively incorporate GNNs in the training phase ofNEs against the learning bias. More importantly, we show that LD converges tothe optimal objective function values by thejoint training under mildassumptions. Experiments demonstrate LD significantly outperformsstate-of-the-art methods on Open Graph Benchmark datasets.</description><author>Zhihao Shi, Jie Wang, Fanghua Lu, Hanzhu Chen, Defu Lian, Zheng Wang, Jieping Ye, Feng Wu</author><pubDate>Tue, 26 Sep 2023 14:09:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14907v1</guid></item><item><title>Pre-training-free Image Manipulation Localization through Non-Mutually Exclusive Contrastive Learning</title><link>http://arxiv.org/abs/2309.14900v1</link><description>Deep Image Manipulation Localization (IML) models suffer from training datainsufficiency and thus heavily rely on pre-training. We argue that contrastivelearning is more suitable to tackle the data insufficiency problem for IML.Crafting mutually exclusive positives and negatives is the prerequisite forcontrastive learning. However, when adopting contrastive learning in IML, weencounter three categories of image patches: tampered, authentic, and contourpatches. Tampered and authentic patches are naturally mutually exclusive, butcontour patches containing both tampered and authentic pixels are non-mutuallyexclusive to them. Simply abnegating these contour patches results in a drasticperformance loss since contour patches are decisive to the learning outcomes.Hence, we propose the Non-mutually exclusive Contrastive Learning (NCL)framework to rescue conventional contrastive learning from the above dilemma.In NCL, to cope with the non-mutually exclusivity, we first establish a pivotstructure with dual branches to constantly switch the role of contour patchesbetween positives and negatives while training. Then, we devise apivot-consistent loss to avoid spatial corruption caused by the role-switchingprocess. In this manner, NCL both inherits the self-supervised merits toaddress the data insufficiency and retains a high manipulation localizationaccuracy. Extensive experiments verify that our NCL achieves state-of-the-artperformance on all five benchmarks without any pre-training and is more robuston unseen real-life samples. The code is available at:https://github.com/Knightzjz/NCL-IML.</description><author>Jizhe Zhou, Xiaochen Ma, Xia Du, Ahmed Y. Alhammadi, Wentao Feng</author><pubDate>Tue, 26 Sep 2023 13:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14900v1</guid></item><item><title>FDLS: A Deep Learning Approach to Production Quality, Controllable, and Retargetable Facial Performances</title><link>http://arxiv.org/abs/2309.14897v1</link><description>Visual effects commonly requires both the creation of realistic synthetichumans as well as retargeting actors' performances to humanoid characters suchas aliens and monsters. Achieving the expressive performances demanded inentertainment requires manipulating complex models with hundreds of parameters.Full creative control requires the freedom to make edits at any stage of theproduction, which prohibits the use of a fully automatic ``black box'' solutionwith uninterpretable parameters. On the other hand, producing realisticanimation with these sophisticated models is difficult and laborious. Thispaper describes FDLS (Facial Deep Learning Solver), which is Weta Digital'ssolution to these challenges. FDLS adopts a coarse-to-fine andhuman-in-the-loop strategy, allowing a solved performance to be verified andedited at several stages in the solving process. To train FDLS, we firsttransform the raw motion-captured data into robust graph features. Secondly,based on the observation that the artists typically finalize the jaw passanimation before proceeding to finer detail, we solve for the jaw motion firstand predict fine expressions with region-based networks conditioned on the jawposition. Finally, artists can optionally invoke a non-linear finetuningprocess on top of the FDLS solution to follow the motion-captured virtualmarkers as closely as possible. FDLS supports editing if needed to improve theresults of the deep learning solution and it can handle small daily changes inthe actor's face shape. FDLS permits reliable and production-qualityperformance solving with minimal training and little or no manual effort inmany cases, while also allowing the solve to be guided and edited in unusualand difficult cases. The system has been under development for several yearsand has been used in major movies.</description><author>Wan-Duo Kurt Ma, Muhammad Ghifary, J. P. Lewis, Byungkuk Choi, Haekwang Eom</author><pubDate>Tue, 26 Sep 2023 13:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14897v1</guid></item><item><title>Verifiable Learned Behaviors via Motion Primitive Composition: Applications to Scooping of Granular Media</title><link>http://arxiv.org/abs/2309.14894v1</link><description>A robotic behavior model that can reliably generate behaviors from naturallanguage inputs in real time would substantially expedite the adoption ofindustrial robots due to enhanced system flexibility. To facilitate theseefforts, we construct a framework in which learned behaviors, created by anatural language abstractor, are verifiable by construction. Leveraging recentadvancements in motion primitives and probabilistic verification, we constructa natural-language behavior abstractor that generates behaviors by synthesizinga directed graph over the provided motion primitives. If these component motionprimitives are constructed according to the criteria we specify, the resultingbehaviors are probabilistically verifiable. We demonstrate this verifiablebehavior generation capacity in both simulation on an exploration task and onhardware with a robot scooping granular media.</description><author>Andrew Benton, Eugen Solowjow, Prithvi Akella</author><pubDate>Tue, 26 Sep 2023 13:51:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14894v1</guid></item><item><title>ddml: Double/debiased machine learning in Stata</title><link>http://arxiv.org/abs/2301.09397v2</link><description>We introduce the package ddml for Double/Debiased Machine Learning (DDML) inStata. Estimators of causal parameters for five different econometric modelsare supported, allowing for flexible estimation of causal effects of endogenousvariables in settings with unknown functional forms and/or many exogenousvariables. ddml is compatible with many existing supervised machine learningprograms in Stata. We recommend using DDML in combination with stackingestimation which combines multiple machine learners into a final predictor. Weprovide Monte Carlo evidence to support our recommendation.</description><author>Achim Ahrens, Christian B. Hansen, Mark E. Schaffer, Thomas Wiemann</author><pubDate>Tue, 26 Sep 2023 13:50:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.09397v2</guid></item><item><title>Nearest Neighbor Guidance for Out-of-Distribution Detection</title><link>http://arxiv.org/abs/2309.14888v1</link><description>Detecting out-of-distribution (OOD) samples are crucial for machine learningmodels deployed in open-world environments. Classifier-based scores are astandard approach for OOD detection due to their fine-grained detectioncapability. However, these scores often suffer from overconfidence issues,misclassifying OOD samples distant from the in-distribution region. To addressthis challenge, we propose a method called Nearest Neighbor Guidance (NNGuide)that guides the classifier-based score to respect the boundary geometry of thedata manifold. NNGuide reduces the overconfidence of OOD samples whilepreserving the fine-grained capability of the classifier-based score. Weconduct extensive experiments on ImageNet OOD detection benchmarks underdiverse settings, including a scenario where the ID data undergoes naturaldistribution shift. Our results demonstrate that NNGuide provides a significantperformance improvement on the base detection scores, achievingstate-of-the-art results on both AUROC, FPR95, and AUPR metrics. The code isgiven at \url{https://github.com/roomo7time/nnguide}.</description><author>Jaewoo Park, Yoon Gyo Jung, Andrew Beng Jin Teoh</author><pubDate>Tue, 26 Sep 2023 13:40:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14888v1</guid></item><item><title>Test Time Adaptation for Blind Image Quality Assessment</title><link>http://arxiv.org/abs/2307.14735v3</link><description>While the design of blind image quality assessment (IQA) algorithms hasimproved significantly, the distribution shift between the training and testingscenarios often leads to a poor performance of these methods at inference time.This motivates the study of test time adaptation (TTA) techniques to improvetheir performance at inference time. Existing auxiliary tasks and lossfunctions used for TTA may not be relevant for quality-aware adaptation of thepre-trained model. In this work, we introduce two novel quality-relevantauxiliary tasks at the batch and sample levels to enable TTA for blind IQA. Inparticular, we introduce a group contrastive loss at the batch level and arelative rank loss at the sample level to make the model quality aware andadapt to the target data. Our experiments reveal that even using a small batchof images from the test distribution helps achieve significant improvement inperformance by updating the batch normalization statistics of the source model.</description><author>Subhadeep Roy, Shankhanil Mitra, Soma Biswas, Rajiv Soundararajan</author><pubDate>Tue, 26 Sep 2023 13:37:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14735v3</guid></item><item><title>RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model</title><link>http://arxiv.org/abs/2308.05345v2</link><description>Inspired by the recent success of large language models (LLMs) like ChatGPT,researchers start to explore the adoption of LLMs for agile hardware design,such as generating design RTL based on natural-language instructions. However,in existing works, their target designs are all relatively simple and in asmall scale, and proposed by the authors themselves, making a fair comparisonamong different LLM solutions challenging. In addition, many prior works onlyfocus on the design correctness, without evaluating the design qualities ofgenerated design RTL. In this work, we propose an open-source benchmark namedRTLLM, for generating design RTL with natural language instructions. Tosystematically evaluate the auto-generated design RTL, we summarized threeprogressive goals, named syntax goal, functionality goal, and design qualitygoal. This benchmark can automatically provide a quantitative evaluation of anygiven LLM-based solution. Furthermore, we propose an easy-to-use yetsurprisingly effective prompt engineering technique named self-planning, whichproves to significantly boost the performance of GPT-3.5 in our proposedbenchmark.</description><author>Yao Lu, Shang Liu, Qijun Zhang, Zhiyao Xie</author><pubDate>Tue, 26 Sep 2023 13:33:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05345v2</guid></item><item><title>Runtime Analysis for the NSGA-II: Proving, Quantifying, and Explaining the Inefficiency For Many Objectives</title><link>http://arxiv.org/abs/2211.13084v4</link><description>The NSGA-II is one of the most prominent algorithms to solve multi-objectiveoptimization problems. Despite numerous successful applications, severalstudies have shown that the NSGA-II is less effective for larger numbers ofobjectives. In this work, we use mathematical runtime analyses to rigorouslydemonstrate and quantify this phenomenon. We show that even on the simple$m$-objective generalization of the discrete OneMinMax benchmark, where everysolution is Pareto optimal, the NSGA-II also with large population sizes cannotcompute the full Pareto front (objective vectors of all Pareto optima) insub-exponential time when the number of objectives is at least three. Thereason for this unexpected behavior lies in the fact that in the computation ofthe crowding distance, the different objectives are regarded independently.This is not a problem for two objectives, where any sorting of a pair-wiseincomparable set of solutions according to one objective is also such a sortingaccording to the other objective (in the inverse order).</description><author>Weijie Zheng, Benjamin Doerr</author><pubDate>Tue, 26 Sep 2023 13:32:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13084v4</guid></item><item><title>Locality-preserving Directions for Interpreting the Latent Space of Satellite Image GANs</title><link>http://arxiv.org/abs/2309.14883v1</link><description>We present a locality-aware method for interpreting the latent space ofwavelet-based Generative Adversarial Networks (GANs), that can well capture thelarge spatial and spectral variability that is characteristic to satelliteimagery. By focusing on preserving locality, the proposed method is able todecompose the weight-space of pre-trained GANs and recover interpretabledirections that correspond to high-level semantic concepts (such asurbanization, structure density, flora presence) - that can subsequently beused for guided synthesis of satellite imagery. In contrast to typically usedapproaches that focus on capturing the variability of the weight-space in areduced dimensionality space (i.e., based on Principal Component Analysis,PCA), we show that preserving locality leads to vectors with different angles,that are more robust to artifacts and can better preserve class information.Via a set of quantitative and qualitative examples, we further show that theproposed approach can outperform both baseline geometric augmentations, as wellas global, PCA-based approaches for data synthesis in the context of dataaugmentation for satellite scene classification.</description><author>Georgia Kourmouli, Nikos Kostagiolas, Yannis Panagakis, Mihalis A. Nicolaou</author><pubDate>Tue, 26 Sep 2023 13:29:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14883v1</guid></item><item><title>Credit Card Fraud Detection with Subspace Learning-based One-Class Classification</title><link>http://arxiv.org/abs/2309.14880v1</link><description>In an increasingly digitalized commerce landscape, the proliferation ofcredit card fraud and the evolution of sophisticated fraudulent techniques haveled to substantial financial losses. Automating credit card fraud detection isa viable way to accelerate detection, reducing response times and minimizingpotential financial losses. However, addressing this challenge is complicatedby the highly imbalanced nature of the datasets, where genuine transactionsvastly outnumber fraudulent ones. Furthermore, the high number of dimensionswithin the feature set gives rise to the ``curse of dimensionality". In thispaper, we investigate subspace learning-based approaches centered on One-ClassClassification (OCC) algorithms, which excel in handling imbalanced datadistributions and possess the capability to anticipate and counter thetransactions carried out by yet-to-be-invented fraud techniques. The studyhighlights the potential of subspace learning-based OCC algorithms byinvestigating the limitations of current fraud detection strategies and thespecific challenges of credit card fraud detection. These algorithms integratesubspace learning into the data description; hence, the models transform thedata into a lower-dimensional subspace optimized for OCC. Through rigorousexperimentation and analysis, the study validated that the proposed approachhelps tackle the curse of dimensionality and the imbalanced nature of creditcard data for automatic fraud detection to mitigate financial losses caused byfraudulent activities.</description><author>Zaffar Zaffar, Fahad Sohrab, Juho Kanniainen, Moncef Gabbouj</author><pubDate>Tue, 26 Sep 2023 13:26:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14880v1</guid></item><item><title>Directed Diffusion: Direct Control of Object Placement through Attention Guidance</title><link>http://arxiv.org/abs/2302.13153v3</link><description>Text-guided diffusion models such as DALLE-2, Imagen, eDiff-I, and StableDiffusion are able to generate an effectively endless variety of images givenonly a short text prompt describing the desired image content. In many casesthe images are of very high quality. However, these models often struggle tocompose scenes containing several key objects such as characters in specifiedpositional relationships. The missing capability to ``direct'' the placement ofcharacters and objects both within and across images is crucial instorytelling, as recognized in the literature on film and animation theory. Inthis work, we take a particularly straightforward approach to providing theneeded direction. Drawing on the observation that the cross-attention maps forprompt words reflect the spatial layout of objects denoted by those words, weintroduce an optimization objective that produces ``activation'' at desiredpositions in these cross-attention maps. The resulting approach is a steptoward generalizing the applicability of text-guided diffusion models beyondsingle images to collections of related images, as in storybooks. DirectedDiffusion provides easy high-level positional control over multiple objects,while making use of an existing pre-trained model and maintaining a coherentblend between the positioned objects and the background. Moreover, it requiresonly a few lines to implement.</description><author>Wan-Duo Kurt Ma, J. P. Lewis, Avisek Lahiri, Thomas Leung, W. Bastiaan Kleijn</author><pubDate>Tue, 26 Sep 2023 13:22:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13153v3</guid></item><item><title>Explainable Sustainability for AI in the Arts</title><link>http://arxiv.org/abs/2309.14877v1</link><description>AI is becoming increasingly popular in artistic practices, but the tools forinforming practitioners about the environmental impact (and othersustainability implications) of AI are adapted for other contexts than creativepractices -- making the tools and sustainability implications of AI notaccessible for artists and creative practitioners. In this position paper, Idescribe two empirical studies that aim to develop environmental sustainabilityreflection systems for AI Arts, and discuss and introduce ExplainableSustainability in for AI Arts.</description><author>Petra JÄÄskelÄinen</author><pubDate>Tue, 26 Sep 2023 13:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14877v1</guid></item><item><title>A post-selection algorithm for improving dynamic ensemble selection methods</title><link>http://arxiv.org/abs/2309.14307v2</link><description>Dynamic Ensemble Selection (DES) is a Multiple Classifier Systems (MCS)approach that aims to select an ensemble for each query sample during theselection phase. Even with the proposal of several DES approaches, noparticular DES technique is the best choice for different problems. Thus, wehypothesize that selecting the best DES approach per query instance can lead tobetter accuracy. To evaluate this idea, we introduce the Post-Selection DynamicEnsemble Selection (PS-DES) approach, a post-selection scheme that evaluatesensembles selected by several DES techniques using different metrics.Experimental results show that using accuracy as a metric to select theensembles, PS-DES performs better than individual DES techniques. PS-DES sourcecode is available in a GitHub repository</description><author>Paulo R. G. Cordeiro, George D. C. Cavalcanti, Rafael M. O. Cruz</author><pubDate>Tue, 26 Sep 2023 13:18:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14307v2</guid></item><item><title>ITEM3D: Illumination-Aware Directional Texture Editing for 3D Models</title><link>http://arxiv.org/abs/2309.14872v1</link><description>Texture editing is a crucial task in 3D modeling that allows users toautomatically manipulate the surface materials of 3D models. However, theinherent complexity of 3D models and the ambiguous text description lead to thechallenge in this task. To address this challenge, we propose ITEM3D, anillumination-aware model for automatic 3D object editing according to the textprompts. Leveraging the diffusion models and the differentiable rendering,ITEM3D takes the rendered images as the bridge of text and 3D representation,and further optimizes the disentangled texture and environment map. Previousmethods adopt the absolute editing direction namely score distillation sampling(SDS) as the optimization objective, which unfortunately results in the noisyappearance and text inconsistency. To solve the problem caused by the ambiguoustext, we introduce a relative editing direction, an optimization objectivedefined by the noise difference between the source and target texts, to releasethe semantic ambiguity between the texts and images. Additionally, we graduallyadjust the direction during optimization to further address the unexpecteddeviation in the texture domain. Qualitative and quantitative experiments showthat our ITEM3D outperforms the state-of-the-art methods on various 3D objects.We also perform text-guided relighting to show explicit control over lighting.</description><author>Shengqi Liu, Zhuo Chen, Jingnan Gao, Yichao Yan, Wenhan Zhu, Xiaobo Li, Ke Gao, Jiangjiang Lyu, Xiaokang Yang</author><pubDate>Tue, 26 Sep 2023 13:01:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14872v1</guid></item></channel></rss>