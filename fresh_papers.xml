<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 21 Oct 2024 01:01:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback</title><link>http://arxiv.org/abs/2410.13191v2</link><description>Automatic question generation (QG) is essential for AI and NLP, particularlyin intelligent tutoring, dialogue systems, and fact verification. Generatingmultiple-choice questions (MCQG) for professional exams, like the United StatesMedical Licensing Examination (USMLE), is particularly challenging, requiringdomain expertise and complex multi-hop reasoning for high-quality questions.However, current large language models (LLMs) like GPT-4 struggle withprofessional MCQG due to outdated knowledge, hallucination issues, and promptsensitivity, resulting in unsatisfactory quality and difficulty. To addressthese challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critiqueand Correction) framework for converting medical cases into high-qualityUSMLE-style questions. By integrating expert-driven prompt engineering withiterative self-critique and self-correction feedback, MCQG-SRefinesignificantly enhances human expert satisfaction regarding both the quality anddifficulty of the questions. Furthermore, we introduce an LLM-as-Judge-basedautomatic metric to replace the complex and costly expert evaluation process,ensuring reliable and expert-aligned assessments.</description><author>Zonghai Yao, Aditya Parashar, Huixue Zhou, Won Seok Jang, Feiyun Ouyang, Zhichao Yang, Hong Yu</author><pubDate>Fri, 18 Oct 2024 16:42:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13191v2</guid></item><item><title>Privacy-Preserving Decentralized AI with Confidential Computing</title><link>http://arxiv.org/abs/2410.13752v2</link><description>This paper addresses privacy protection in decentralized ArtificialIntelligence (AI) using Confidential Computing (CC) within the Atoma Network, adecentralized AI platform designed for the Web3 domain. Decentralized AIdistributes AI services among multiple entities without centralized oversight,fostering transparency and robustness. However, this structure introducessignificant privacy challenges, as sensitive assets such as proprietary modelsand personal data may be exposed to untrusted participants. Cryptography-basedprivacy protection techniques such as zero-knowledge machine learning (zkML)suffers prohibitive computational overhead. To address the limitation, wepropose leveraging Confidential Computing (CC). Confidential Computingleverages hardware-based Trusted Execution Environments (TEEs) to provideisolation for processing sensitive data, ensuring that both model parametersand user data remain secure, even in decentralized, potentially untrustedenvironments. While TEEs face a few limitations, we believe they can bridge theprivacy gap in decentralized AI. We explore how we can integrate TEEs intoAtoma's decentralized framework.</description><author>Dayeol Lee, Jorge Ant√≥nio, Hisham Khan</author><pubDate>Fri, 18 Oct 2024 16:33:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13752v2</guid></item><item><title>Scalable Drift Monitoring in Medical Imaging AI</title><link>http://arxiv.org/abs/2410.13174v2</link><description>The integration of artificial intelligence (AI) into medical imaging hasadvanced clinical diagnostics but poses challenges in managing model drift andensuring long-term reliability. To address these challenges, we develop MMC+,an enhanced framework for scalable drift monitoring, building upon theCheXstray framework that introduced real-time drift detection for medicalimaging AI models using multi-modal data concordance. This work extends theoriginal framework's methodologies, providing a more scalable and adaptablesolution for real-world healthcare settings and offers a reliable andcost-effective alternative to continuous performance monitoring addressinglimitations of both continuous and periodic monitoring methods. MMC+ introducescritical improvements to the original framework, including more robust handlingof diverse data streams, improved scalability with the integration offoundation models like MedImageInsight for high-dimensional image embeddingswithout site-specific training, and the introduction of uncertainty bounds tobetter capture drift in dynamic clinical environments. Validated withreal-world data from Massachusetts General Hospital during the COVID-19pandemic, MMC+ effectively detects significant data shifts and correlates themwith model performance changes. While not directly predicting performancedegradation, MMC+ serves as an early warning system, indicating when AI systemsmay deviate from acceptable performance bounds and enabling timelyinterventions. By emphasizing the importance of monitoring diverse data streamsand evaluating data shifts alongside model performance, this work contributesto the broader adoption and integration of AI solutions in clinical settings.</description><author>Jameson Merkow, Felix J. Dorfner, Xiyu Yang, Alexander Ersoy, Giridhar Dasegowda, Mannudeep Kalra, Matthew P. Lungren, Christopher P. Bridge, Ivan Tarapov</author><pubDate>Fri, 18 Oct 2024 16:26:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13174v2</guid></item><item><title>Fundus to Fluorescein Angiography Video Generation as a Retinal Generative Foundation Model</title><link>http://arxiv.org/abs/2410.13242v2</link><description>Fundus fluorescein angiography (FFA) is crucial for diagnosing and monitoringretinal vascular issues but is limited by its invasive nature and restrictedaccessibility compared to color fundus (CF) imaging. Existing methods thatconvert CF images to FFA are confined to static image generation, missing thedynamic lesional changes. We introduce Fundus2Video, an autoregressivegenerative adversarial network (GAN) model that generates dynamic FFA videosfrom single CF images. Fundus2Video excels in video generation, achieving anFVD of 1497.12 and a PSNR of 11.77. Clinical experts have validated thefidelity of the generated videos. Additionally, the model's generatordemonstrates remarkable downstream transferability across ten external publicdatasets, including blood vessel segmentation, retinal disease diagnosis,systemic disease prediction, and multimodal retrieval, showcasing impressivezero-shot and few-shot capabilities. These findings position Fundus2Video as apowerful, non-invasive alternative to FFA exams and a versatile retinalgenerative foundation model that captures both static and temporal retinalfeatures, enabling the representation of complex inter-modality relationships.</description><author>Weiyi Zhang, Jiancheng Yang, Ruoyu Chen, Siyu Huang, Pusheng Xu, Xiaolan Chen, Shanfu Lu, Hongyu Cao, Mingguang He, Danli Shi</author><pubDate>Fri, 18 Oct 2024 15:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13242v2</guid></item><item><title>Sample Compression Scheme Reductions</title><link>http://arxiv.org/abs/2410.13012v2</link><description>We present novel reductions from sample compression schemes in multiclassclassification, regression, and adversarially robust learning settings tobinary sample compression schemes. Assuming we have a compression scheme forbinary classes of size $f(d_\mathrm{VC})$, where $d_\mathrm{VC}$ is the VCdimension, then we have the following results: (1) If the binary compressionscheme is a majority-vote or a stable compression scheme, then there exists amulticlass compression scheme of size $O(f(d_\mathrm{G}))$, where$d_\mathrm{G}$ is the graph dimension. Moreover, for general binary compressionschemes, we obtain a compression of size $O(f(d_\mathrm{G})\log|Y|)$, where $Y$is the label space. (2) If the binary compression scheme is a majority-vote ora stable compression scheme, then there exists an $\epsilon$-approximatecompression scheme for regression over $[0,1]$-valued functions of size$O(f(d_\mathrm{P}))$, where $d_\mathrm{P}$ is the pseudo-dimension. For generalbinary compression schemes, we obtain a compression of size$O(f(d_\mathrm{P})\log(1/\epsilon))$. These results would have significantimplications if the sample compression conjecture, which posits that any binaryconcept class with a finite VC dimension admits a binary compression scheme ofsize $O(d_\mathrm{VC})$, is resolved (Littlestone and Warmuth, 1986; Floyd andWarmuth, 1995; Warmuth, 2003). Our results would then extend the proof of theconjecture immediately to other settings. We establish similar results foradversarially robust learning and also provide an example of a concept classthat is robustly learnable but has no bounded-size compression scheme,demonstrating that learnability is not equivalent to having a compressionscheme independent of the sample size, unlike in binary classification, wherecompression of size $2^{O(d_\mathrm{VC})}$ is attainable (Moran and Yehudayoff,2016).</description><author>Idan Attias, Steve Hanneke, Arvind Ramaswami</author><pubDate>Fri, 18 Oct 2024 14:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13012v2</guid></item><item><title>BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated Bangla</title><link>http://arxiv.org/abs/2410.13281v2</link><description>The proliferation of transliterated texts in digital spaces has emphasizedthe need for detecting and classifying hate speech in languages beyond English,particularly in low-resource languages. As online discourse can perpetuatediscrimination based on target groups, e.g. gender, religion, and origin,multi-label classification of hateful content can help in comprehending hatemotivation and enhance content moderation. While previous efforts have focusedon monolingual or binary hate classification tasks, no work has yet addressedthe challenge of multi-label hate speech classification in transliteratedBangla. We introduce BanTH, the first multi-label transliterated Bangla hatespeech dataset comprising 37.3k samples. The samples are sourced from YouTubecomments, where each instance is labeled with one or more target groups,reflecting the regional demographic. We establish novel transformerencoder-based baselines by further pre-training on transliterated Banglacorpus. We also propose a novel translation-based LLM prompting strategy fortransliterated text. Experiments reveal that our further pre-trained encodersare achieving state-of-the-art performance on the BanTH dataset, while ourtranslation-based prompting outperforms other strategies in the zero-shotsetting. The introduction of BanTH not only fills a critical gap in hate speechresearch for Bangla but also sets the stage for future exploration intocode-mixed and multi-label classification challenges in underrepresentedlanguages.</description><author>Fabiha Haider, Fariha Tanjim Shifat, Md Farhan Ishmam, Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, Md Fahim, Md Farhad Alam</author><pubDate>Fri, 18 Oct 2024 09:50:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13281v2</guid></item><item><title>Harnessing Webpage UIs for Text-Rich Visual Understanding</title><link>http://arxiv.org/abs/2410.13824v2</link><description>Text-rich visual understanding-the ability to process environments wheredense textual content is integrated with visuals-is crucial for multimodallarge language models (MLLMs) to interact effectively with structuredenvironments. To enhance this capability, we propose synthesizing generalmultimodal instructions from webpage UIs using text-based large language models(LLMs). Despite lacking direct visual input, text-based LLMs are able toprocess structured text representations from webpage accessibility trees. Theseinstructions are then paired with UI screenshots to train multimodal models. Weintroduce MultiUI, a dataset containing 7.3 million samples from 1 millionwebsites, covering diverse multimodal tasks and UI layouts. Models trained onMultiUI not only excel in web UI tasks-achieving up to a 48% improvement onVisualWebBench and a 19.1% boost in element accuracy on a web agent datasetMind2Web-but also generalize surprisingly well to non-web UI tasks and even tonon-UI domains, such as document understanding, OCR, and chart interpretation.These results highlight the broad applicability of web UI data for advancingtext-rich visual understanding across various scenarios.</description><author>Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue</author><pubDate>Fri, 18 Oct 2024 09:01:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13824v2</guid></item><item><title>MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures</title><link>http://arxiv.org/abs/2410.13754v2</link><description>Perceiving and generating diverse modalities are crucial for AI models toeffectively learn from and engage with real-world signals, necessitatingreliable evaluations for their development. We identify two major issues incurrent evaluations: (1) inconsistent standards, shaped by differentcommunities with varying protocols and maturity levels; and (2) significantquery, grading, and generalization biases. To address these, we introduceMixEval-X, the first any-to-any, real-world benchmark designed to optimize andstandardize evaluations across diverse input and output modalities. We proposemulti-modal benchmark mixture and adaptation-rectification pipelines toreconstruct real-world task distributions, ensuring evaluations generalizeeffectively to real-world use cases. Extensive meta-evaluations show ourapproach effectively aligns benchmark samples with real-world taskdistributions. Meanwhile, MixEval-X's model rankings correlate strongly withthat of crowd-sourced real-world evaluations (up to 0.98) while being much moreefficient. We provide comprehensive leaderboards to rerank existing models andorganizations and offer insights to enhance understanding of multi-modalevaluations and inform future research.</description><author>Jinjie Ni, Yifan Song, Deepanway Ghosal, Bo Li, David Junhao Zhang, Xiang Yue, Fuzhao Xue, Zian Zheng, Kaichen Zhang, Mahir Shah, Kabir Jain, Yang You, Michael Shieh</author><pubDate>Fri, 18 Oct 2024 08:56:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13754v2</guid></item><item><title>Context-Enhanced Multi-View Trajectory Representation Learning: Bridging the Gap through Self-Supervised Models</title><link>http://arxiv.org/abs/2410.13196v2</link><description>Modeling trajectory data with generic-purpose dense representations hasbecome a prevalent paradigm for various downstream applications, such astrajectory classification, travel time estimation and similarity computation.However, existing methods typically rely on trajectories from a single spatialview, limiting their ability to capture the rich contextual information that iscrucial for gaining deeper insights into movement patterns across differentgeospatial contexts. To this end, we propose MVTraj, a novel multi-viewmodeling method for trajectory representation learning. MVTraj integratesdiverse contextual knowledge, from GPS to road network and points-of-interestto provide a more comprehensive understanding of trajectory data. To align thelearning process across multiple views, we utilize GPS trajectories as a bridgeand employ self-supervised pretext tasks to capture and distinguish movementpatterns across different spatial views. Following this, we treat trajectoriesfrom different views as distinct modalities and apply a hierarchicalcross-modal interaction module to fuse the representations, thereby enrichingthe knowledge derived from multiple sources. Extensive experiments onreal-world datasets demonstrate that MVTraj significantly outperforms existingbaselines in tasks associated with various spatial views, validating itseffectiveness and practical utility in spatio-temporal modeling.</description><author>Tangwen Qian, Junhe Li, Yile Chen, Gao Cong, Tao Sun, Fei Wang, Yongjun Xu</author><pubDate>Fri, 18 Oct 2024 08:33:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13196v2</guid></item><item><title>Enhanced Prompt-leveraged Weakly Supervised Cancer Segmentation based on Segment Anything</title><link>http://arxiv.org/abs/2410.13621v2</link><description>This work proposes a novel approach beyond supervised learning for effectivepathological image analysis, addressing the challenge of limited robust labeleddata. Pathological diagnosis of diseases like cancer has conventionally reliedon the evaluation of morphological features by physicians and pathologists.However, recent advancements in compute-aided diagnosis (CAD) systems aregaining significant attention as diagnostic support tools. Although theadvancement of deep learning has improved CAD significantly, segmentationmodels typically require large pixel-level annotated dataset, and such labelingis expensive. Existing studies not based on supervised approaches stillstruggle with limited generalization, and no practical approach has emergedyet. To address this issue, we present a weakly supervised semanticsegmentation (WSSS) model by combining class activation map and SegmentAnything Model (SAM)-based pseudo-labeling. For effective pretraining, we adoptthe SAM-a foundation model that is pretrained on large datasets and operates inzero-shot configurations using only coarse prompts. The proposed approachtransfer enhanced Attention Dropout Layer's knowledge to SAM, therebygenerating pseudo-labels. To demonstrate the superiority of the proposedmethod, experimental studies are conducted on histopathological breast cancerdatasets. The proposed method outperformed other WSSS methods across threedatasets, demonstrating its efficiency by achieving this with only 12GB of GPUmemory during training. Our code is available at :https://github.com/QI-NemoSong/EPLC-SAM</description><author>Joonhyeon Song, Seohwan Yun, Seongho Yoon, Joohyeok Kim, Sangmin Lee</author><pubDate>Fri, 18 Oct 2024 08:01:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13621v2</guid></item><item><title>Identifying treatment response subgroups in observational time-to-event data</title><link>http://arxiv.org/abs/2408.03463v3</link><description>Identifying patient subgroups with different treatment responses is animportant task to inform medical recommendations, guidelines, and the design offuture clinical trials. Existing approaches for subgroup analysis primarilyrely on Randomised Controlled Trials (RCTs), in which treatment assignment israndomised. RCTs' patient cohorts are often constrained by cost, rendering themnot representative of the heterogeneity of patients likely to receive treatmentin real-world clinical practice. When applied to observational studies,subgroup analysis approaches suffer from significant statistical biasesparticularly because of the non-randomisation of treatment. Our work introducesa novel, outcome-guided method for identifying treatment response subgroups inobservational studies. Our approach assigns each patient to a subgroupassociated with two time-to-event distributions: one under treatment and oneunder control regime. It hence positions itself in between individualised andaverage treatment effect estimation. The assumptions of our model result in asimple correction of the statistical bias from treatment non-randomisationthrough inverse propensity weighting. In experiments, our approachsignificantly outperforms the current state-of-the-art method foroutcome-guided subgroup analysis in both randomised and observational treatmentregimes.</description><author>Vincent Jeanselme, Chang Ho Yoon, Fabian Falck, Brian Tom, Jessica Barrett</author><pubDate>Fri, 18 Oct 2024 07:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03463v3</guid></item><item><title>Towards Satellite Non-IID Imagery: A Spectral Clustering-Assisted Federated Learning Approach</title><link>http://arxiv.org/abs/2410.13602v2</link><description>Low Earth orbit (LEO) satellites are capable of gathering abundant Earthobservation data (EOD) to enable different Internet of Things (IoT)applications. However, to accomplish an effective EOD processing mechanism, itis imperative to investigate: 1) the challenge of processing the observed datawithout transmitting those large-size data to the ground because the connectionbetween the satellites and the ground stations is intermittent, and 2) thechallenge of processing the non-independent and identically distributed(non-IID) satellite data. In this paper, to cope with those challenges, wepropose an orbit-based spectral clustering-assisted clustered federatedself-knowledge distillation (OSC-FSKD) approach for each orbit of an LEOsatellite constellation, which retains the advantage of FL that the observeddata does not need to be sent to the ground. Specifically, we introducenormalized Laplacian-based spectral clustering (NLSC) into federated learning(FL) to create clustered FL in each round to address the challenge resultingfrom non-IID data. Particularly, NLSC is adopted to dynamically group clientsinto several clusters based on cosine similarities calculated by model updates.In addition, self-knowledge distillation is utilized to construct each localclient, where the most recent updated local model is used to guide currentlocal model training. Experiments demonstrate that the observation accuracyobtained by the proposed method is separately 1.01x, 2.15x, 1.10x, and 1.03xhigher than that of pFedSD, FedProx, FedAU, and FedALA approaches using theSAT4 dataset. The proposed method also shows superiority when using otherdatasets.</description><author>Luyao Zou, Yu Min Park, Chu Myaet Thwal, Yan Kyaw Tun, Zhu Han, Choong Seon Hong</author><pubDate>Fri, 18 Oct 2024 07:04:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13602v2</guid></item><item><title>UniG: Modelling Unitary 3D Gaussians for View-consistent 3D Reconstruction</title><link>http://arxiv.org/abs/2410.13195v2</link><description>In this work, we present UniG, a view-consistent 3D reconstruction and novelview synthesis model that generates a high-fidelity representation of 3DGaussians from sparse images. Existing 3D Gaussians-based methods usuallyregress Gaussians per-pixel of each view, create 3D Gaussians per viewseparately, and merge them through point concatenation. Such a view-independentreconstruction approach often results in a view inconsistency issue, where thepredicted positions of the same 3D point from different views may havediscrepancies. To address this problem, we develop a DETR (DEtectionTRansformer)-like framework, which treats 3D Gaussians as decoder queries andupdates their parameters layer by layer by performing multi-viewcross-attention (MVDFA) over multiple input images. In this way, multiple viewsnaturally contribute to modeling a unitary representation of 3D Gaussians,thereby making 3D reconstruction more view-consistent. Moreover, as the numberof 3D Gaussians used as decoder queries is irrespective of the number of inputviews, allow an arbitrary number of input images without causing memoryexplosion. Extensive experiments validate the advantages of our approach,showcasing superior performance over existing methods quantitatively (improvingPSNR by 4.2 dB when trained on Objaverse and tested on the GSO benchmark) andqualitatively. The code will be released at https://github.com/jwubz123/UNIG.</description><author>Jiamin Wu, Kenkun Liu, Yukai Shi, Xiaoke Jiang, Yuan Yao, Lei Zhang</author><pubDate>Fri, 18 Oct 2024 06:02:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13195v2</guid></item><item><title>SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</title><link>http://arxiv.org/abs/2410.13276v2</link><description>Attention is the cornerstone of modern Large Language Models (LLMs). Yet itsquadratic complexity limits the efficiency and scalability of LLMs, especiallyfor those with a long-context window. A promising approach addressing thislimitation is to leverage the sparsity in attention. However, existingsparsity-based solutions predominantly rely on predefined patterns orheuristics to approximate sparsity. This practice falls short to fully capturethe dynamic nature of attention sparsity in language-based tasks. This paperargues that attention sparsity should be learned rather than predefined. Tothis end, we design SeerAttention, a new Attention mechanism that augments theconventional attention with a learnable gate that adaptively selectssignificant blocks in an attention map and deems the rest blocks sparse. Suchblock-level sparsity effectively balances accuracy and speedup. To enableefficient learning of the gating network, we develop a customizedFlashAttention implementation that extracts the block-level ground truth ofattention map with minimum overhead. SeerAttention not only applies topost-training, but also excels in long-context fine-tuning. Our results showthat at post-training stages, SeerAttention significantly outperformsstate-of-the-art static or heuristic-based sparse attention methods, while alsobeing more versatile and flexible to adapt to varying context lengths andsparsity ratios. When applied to long-context fine-tuning with YaRN,SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k contextlength with minimal perplexity loss, offering a 5.67x speedup overFlashAttention-2.</description><author>Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Hayden Kwok-Hay So, Ting Cao, Fan Yang, Mao Yang</author><pubDate>Fri, 18 Oct 2024 05:01:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13276v2</guid></item><item><title>PAPL-SLAM: Principal Axis-Anchored Monocular Point-Line SLAM</title><link>http://arxiv.org/abs/2410.12324v2</link><description>In point-line SLAM systems, the utilization of line structural informationand the optimization of lines are two significant problems. The former isusually addressed through structural regularities, while the latter typicallyinvolves using minimal parameter representations of lines in optimization.However, separating these two steps leads to the loss of constraint informationto each other. We anchor lines with similar directions to a principal axis andoptimize them with $n+2$ parameters for $n$ lines, solving both problemstogether. Our method considers scene structural information, which can beeasily extended to different world hypotheses while significantly reducing thenumber of line parameters to be optimized, enabling rapid and accurate mappingand tracking. To further enhance the system's robustness and avoid mismatch, wehave modeled the line-axis probabilistic data association and provided thealgorithm for axis creation, updating, and optimization. Additionally,considering that most real-world scenes conform to the Atlanta Worldhypothesis, we provide a structural line detection strategy based on verticalpriors and vanishing points. Experimental results and ablation studies onvarious indoor and outdoor datasets demonstrate the effectiveness of oursystem.</description><author>Guanghao Li, Yu Cao, Qi Chen, Yifan Yang, Jian Pu</author><pubDate>Fri, 18 Oct 2024 04:32:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.12324v2</guid></item><item><title>DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation</title><link>http://arxiv.org/abs/2410.13726v2</link><description>Talking head generation intends to produce vivid and realistic talking headvideos from a single portrait and speech audio clip. Although significantprogress has been made in diffusion-based talking head generation, almost allmethods rely on autoregressive strategies, which suffer from limited contextutilization beyond the current generation step, error accumulation, and slowergeneration speed. To address these challenges, we present DAWN (Dynamic frameAvatar With Non-autoregressive diffusion), a framework that enables all-at-oncegeneration of dynamic-length video sequences. Specifically, it consists of twomain components: (1) audio-driven holistic facial dynamics generation in thelatent motion space, and (2) audio-driven head pose and blink generation.Extensive experiments demonstrate that our method generates authentic and vividvideos with precise lip motions, and natural pose/blink movements.Additionally, with a high generation speed, DAWN possesses strong extrapolationcapabilities, ensuring the stable production of high-quality long videos. Theseresults highlight the considerable promise and potential impact of DAWN in thefield of talking head video generation. Furthermore, we hope that DAWN sparksfurther exploration of non-autoregressive approaches in diffusion models. Ourcode will be publicly available at https://github.com/Hanbo-Cheng/DAWN-pytorch.</description><author>Hanbo Cheng, Limin Lin, Chenyu Liu, Pengcheng Xia, Pengfei Hu, Jiefeng Ma, Jun Du, Jia Pan</author><pubDate>Fri, 18 Oct 2024 04:19:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13726v2</guid></item><item><title>LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding</title><link>http://arxiv.org/abs/2404.16710v4</link><description>We present LayerSkip, an end-to-end solution to speed-up inference of largelanguage models (LLMs). First, during training we apply layer dropout, with lowdropout rates for earlier layers and higher dropout rates for later layers, andan early exit loss where all transformer layers share the same exit. Second,during inference, we show that this training recipe increases the accuracy ofearly exit at earlier layers, without adding any auxiliary layers or modules tothe model. Third, we present a novel self-speculative decoding solution wherewe exit at early layers and verify and correct with remaining layers of themodel. Our proposed self-speculative decoding approach has less memoryfootprint than other speculative decoding approaches and benefits from sharedcompute and activations of the draft and verification stages. We runexperiments on different Llama model sizes on different types of training:pretraining from scratch, continual pretraining, finetuning on specific datadomain, and finetuning on specific task. We implement our inference solutionand show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82xon coding, and 2.0x on TOPv2 semantic parsing task. We open source our code andcheckpoints at https://github.com/facebookresearch/LayerSkip.</description><author>Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, Carole-Jean Wu</author><pubDate>Fri, 18 Oct 2024 04:02:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16710v4</guid></item><item><title>LeanAgent: Lifelong Learning for Formal Theorem Proving</title><link>http://arxiv.org/abs/2410.06209v4</link><description>Large Language Models (LLMs) have been successful in mathematical reasoningtasks such as formal theorem proving when integrated with interactive proofassistants like Lean. Existing approaches involve training or fine-tuning anLLM on a specific dataset to perform well on particular domains, such asundergraduate-level mathematics. These methods struggle with generalizabilityto advanced mathematics. A fundamental limitation is that these approachesoperate on static domains, failing to capture how mathematicians often workacross multiple domains and projects simultaneously or cyclically. We presentLeanAgent, a novel lifelong learning framework for theorem proving thatcontinuously generalizes to and improves on ever-expanding mathematicalknowledge without forgetting previously learned knowledge. LeanAgent introducesseveral key innovations, including a curriculum learning strategy thatoptimizes the learning trajectory in terms of mathematical difficulty, adynamic database for efficient management of evolving mathematical knowledge,and progressive training to balance stability and plasticity. LeanAgentsuccessfully proves 162 theorems previously unproved by humans across 23diverse Lean repositories, many from advanced mathematics. It performssignificantly better than the static LLM baseline, proving challenging theoremsin domains like abstract algebra and algebraic topology while showcasing aclear progression of learning from basic concepts to advanced topics. Inaddition, we analyze LeanAgent's superior performance on key lifelong learningmetrics. LeanAgent achieves exceptional scores in stability and backwardtransfer, where learning new tasks improves performance on previously learnedtasks. This emphasizes LeanAgent's continuous generalizability and improvement,explaining its superior theorem-proving performance.</description><author>Adarsh Kumarappan, Mo Tiwari, Peiyang Song, Robert Joseph George, Chaowei Xiao, Anima Anandkumar</author><pubDate>Fri, 18 Oct 2024 03:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06209v4</guid></item><item><title>Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion</title><link>http://arxiv.org/abs/2410.13674v2</link><description>Low-quality or scarce data has posed significant challenges for training deepneural networks in practice. While classical data augmentation cannotcontribute very different new data, diffusion models opens up a new door tobuild self-evolving AI by generating high-quality and diverse synthetic datathrough text-guided prompts. However, text-only guidance cannot controlsynthetic images' proximity to the original images, resulting inout-of-distribution data detrimental to the model performance. To overcome thelimitation, we study image guidance to achieve a spectrum of interpolationsbetween synthetic and real images. With stronger image guidance, the generatedimages are similar to the training data but hard to learn. While with weakerimage guidance, the synthetic images will be easier for model but contribute toa larger distribution gap with the original data. The generated full spectrumof data enables us to build a novel "Diffusion Curriculum (DisCL)". DisCLadjusts the image guidance level of image synthesis for each training stage: Itidentifies and focuses on hard samples for the model and assesses the mosteffective guidance level of synthetic images to improve hard data learning. Weapply DisCL to two challenging tasks: long-tail (LT) classification andlearning from low-quality data. It focuses on lower-guidance images ofhigh-quality to learn prototypical features as a warm-up of learninghigher-guidance images that might be weak on diversity or quality. Extensiveexperiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy whenapplying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the basemodel's tail-class accuracy from 4.4% to 23.64% and leads to a 4.02%improvement in all-class accuracy.</description><author>Yijun Liang, Shweta Bhardwaj, Tianyi Zhou</author><pubDate>Fri, 18 Oct 2024 03:28:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13674v2</guid></item><item><title>MlingConf: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models</title><link>http://arxiv.org/abs/2410.12478v2</link><description>The tendency of Large Language Models (LLMs) to generate hallucinationsraises concerns regarding their reliability. Therefore, confidence estimationsindicating the extent of trustworthiness of the generations become essential.However, current LLM confidence estimations in languages other than Englishremain underexplored. This paper addresses this gap by introducing acomprehensive investigation of Multilingual Confidence estimation (MlingConf)on LLMs, focusing on both language-agnostic (LA) and language-specific (LS)tasks to explore the performance and language dominance effects of multilingualconfidence estimations on different tasks. The benchmark comprises fourmeticulously checked and human-evaluate high-quality multilingual datasets forLA tasks and one for the LS task tailored to specific social, cultural, andgeographical contexts of a language. Our experiments reveal that on LA tasksEnglish exhibits notable linguistic dominance in confidence estimations thanother languages, while on LS tasks, using question-related language to promptLLMs demonstrates better linguistic dominance in multilingual confidenceestimations. The phenomena inspire a simple yet effective native-tone promptingstrategy by employing language-specific prompts for LS tasks, effectivelyimproving LLMs' reliability and accuracy on LS tasks.</description><author>Boyang Xue, Hongru Wang, Rui Wang, Sheng Wang, Zezhong Wang, Yiming Du, Bin Liang, Kam-Fai Wong</author><pubDate>Fri, 18 Oct 2024 03:19:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.12478v2</guid></item><item><title>BenTo: Benchmark Task Reduction with In-Context Transferability</title><link>http://arxiv.org/abs/2410.13804v2</link><description>Evaluating large language models (LLMs) is costly: it requires the generationand examination of LLM outputs on a large-scale benchmark of various tasks.This paper investigates how to efficiently reduce the tasks used to benchmarkLLMs without affecting the evaluation quality. Our study reveals that tasktransferability and relevance provide critical information to identify the mostrepresentative subset of tasks via optimizing a facility location function. Wepropose a practically efficient metric for estimating the transferabilitybetween two tasks via in-context learning (ICL). By analyzing the pairwisetransferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU orFLAN) to 5% while inducing only a &lt;4% difference to the evaluation on theoriginal benchmark. Compared to prior works, our method is training-free,gradient-free, and highly efficient requiring ICL only.</description><author>Hongyu Zhao, Ming Li, Lichao Sun, Tianyi Zhou</author><pubDate>Fri, 18 Oct 2024 03:15:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13804v2</guid></item><item><title>MoR: Mixture of Ranks for Low-Rank Adaptation Tuning</title><link>http://arxiv.org/abs/2410.13408v2</link><description>Low-Rank Adaptation (LoRA) drives research to align its performance with fullfine-tuning. However, significant challenges remain: (1) Simply increasing therank size of LoRA does not effectively capture high-rank information, whichleads to a performance bottleneck.(2) MoE-style LoRA methods substantiallyincrease parameters and inference latency, contradicting the goals of efficientfine-tuning and ease of application. To address these challenges, we introduceMixture of Ranks (MoR), which learns rank-specific information for differenttasks based on input and efficiently integrates multi-rank information. Wefirstly propose a new framework that equates the integration of multiple LoRAsto expanding the rank of LoRA. Moreover, we hypothesize that low-rank LoRAalready captures sufficient intrinsic information, and MoR can derive high-rankinformation through mathematical transformations of the low-rank components.Thus, MoR can reduces the learning difficulty of LoRA and enhances itsmulti-task capabilities. MoR achieves impressive results, with MoR delivering a1.31\% performance improvement while using only 93.93\% of the parameterscompared to baseline methods.</description><author>Chuanyu Tang, Yilong Chen, Zhenyu Zhang, Junyuan Shang, Wenyuan Zhang, Yong Huang, Tingwen Liu</author><pubDate>Fri, 18 Oct 2024 03:05:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13408v2</guid></item><item><title>An Evolved Universal Transformer Memory</title><link>http://arxiv.org/abs/2410.13166v2</link><description>Prior methods propose to offset the escalating costs of modern foundationmodels by dropping specific parts of their contexts with hand-designed rules,while attempting to preserve their original performance. We overcome thistrade-off with Neural Attention Memory Models (NAMMs), introducing a learnednetwork for memory management that improves both the performance and efficiencyof transformers. We evolve NAMMs atop pre-trained transformers to providedifferent latent contexts focusing on the most relevant information forindividual layers and attention heads. NAMMs are universally applicable to anymodel using self-attention as they condition exclusively on the values in theproduced attention matrices. Learning NAMMs on a small set of problems, weachieve substantial performance improvements across multiple long-contextbenchmarks while cutting the model's input contexts up to a fraction of theoriginal sizes. We show the generality of our conditioning enables zero-shottransfer of NAMMs trained only on language to entirely new transformerarchitectures even across input modalities, with their benefits carrying overto vision and reinforcement learning.</description><author>Edoardo Cetin, Qi Sun, Tianyu Zhao, Yujin Tang</author><pubDate>Fri, 18 Oct 2024 02:53:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13166v2</guid></item><item><title>Open Domain Question Answering with Conflicting Contexts</title><link>http://arxiv.org/abs/2410.12311v2</link><description>Open domain question answering systems frequently rely on informationretrieved from large collections of text (such as the Web) to answer questions.However, such collections of text often contain conflicting information, andindiscriminately depending on this information may result in untruthful andinaccurate answers. To understand the gravity of this problem, we collect ahuman-annotated dataset, Question Answering with Conflicting Contexts (QACC),and find that as much as 25% of unambiguous, open domain questions can lead toconflicting contexts when retrieved using Google Search. We evaluate andbenchmark three powerful Large Language Models (LLMs) with our dataset QACC anddemonstrate their limitations in effectively addressing questions withconflicting information. To explore how humans reason through conflictingcontexts, we request our annotators to provide explanations for theirselections of correct answers. We demonstrate that by finetuning LLMs toexplain their answers, we can introduce richer information into their trainingthat guide them through the process of reasoning with conflicting contexts.</description><author>Siyi Liu, Qiang Ning, Kishaloy Halder, Wei Xiao, Zheng Qi, Phu Mon Htut, Yi Zhang, Neha Anna John, Bonan Min, Yassine Benajiba, Dan Roth</author><pubDate>Fri, 18 Oct 2024 00:32:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.12311v2</guid></item><item><title>Comparative Analysis of Extrinsic Factors for NER in French</title><link>http://arxiv.org/abs/2410.12750v2</link><description>Named entity recognition (NER) is a crucial task that aims to identifystructured information, which is often replete with complex, technical termsand a high degree of variability. Accurate and reliable NER can facilitate theextraction and analysis of important information. However, NER for other thanEnglish is challenging due to limited data availability, as the high expertise,time, and expenses are required to annotate its data. In this paper, by usingthe limited data, we explore various factors including model structure, corpusannotation scheme and data augmentation techniques to improve the performanceof a NER model for French. Our experiments demonstrate that these approachescan significantly improve the model's F1 score from original CRF score of 62.41to 79.39. Our findings suggest that considering different extrinsic factors andcombining these techniques is a promising approach for improving NERperformance where the size of data is limited.</description><author>Grace Yang, Zhiyi Li, Yadong Liu, Jungyeul Park</author><pubDate>Thu, 17 Oct 2024 23:50:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.12750v2</guid></item><item><title>Testing Causal Explanations: A Case Study for Understanding the Effect of Interventions on Chronic Kidney Disease</title><link>http://arxiv.org/abs/2410.12047v2</link><description>Randomized controlled trials (RCTs) are the standard for evaluating theeffectiveness of clinical interventions. To address the limitations of RCTs onreal-world populations, we developed a methodology that uses a largeobservational electronic health record (EHR) dataset. Principles of regressiondiscontinuity (rd) were used to derive randomized data subsets to testexpert-driven interventions using dynamic Bayesian Networks (DBNs)do-operations. This combined method was applied to a chronic kidney disease(CKD) cohort of more than two million individuals and used to understand theassociational and causal relationships of CKD variables with respect to asurrogate outcome of &gt;=40% decline in estimated glomerular filtration rate(eGFR). The associational and causal analyses depicted similar findings acrossDBNs from two independent healthcare systems. The associational analysis showedthat the most influential variables were eGFR, urine albumin-to-creatinineratio, and pulse pressure, whereas the causal analysis showed eGFR as the mostinfluential variable, followed by modifiable factors such as medications thatmay impact kidney function over time. This methodology demonstrates howreal-world EHR data can be used to provide population-level insights to informimproved healthcare delivery.</description><author>Panayiotis Petousis, David Gordon, Susanne B. Nicholas, Alex A. T. Bui</author><pubDate>Thu, 17 Oct 2024 23:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.12047v2</guid></item><item><title>Preserving Cardiac Integrity: A Topology-Infused Approach to Whole Heart Segmentation</title><link>http://arxiv.org/abs/2410.10551v3</link><description>Whole heart segmentation (WHS) supports cardiovascular disease (CVD)diagnosis, disease monitoring, treatment planning, and prognosis. Deep learninghas become the most widely used method for WHS applications in recent years.However, segmentation of whole-heart structures faces numerous challengesincluding heart shape variability during the cardiac cycle, clinical artifactslike motion and poor contrast-to-noise ratio, domain shifts in multi-centerdata, and the distinct modalities of CT and MRI. To address these limitationsand improve segmentation quality, this paper introduces a newtopology-preserving module that is integrated into deep neural networks. Theimplementation achieves anatomically plausible segmentation by using learnedtopology-preserving fields, which are based entirely on 3D convolution and aretherefore very effective for 3D voxel data. We incorporate natural constraintsbetween structures into the end-to-end training and enrich the featurerepresentation of the neural network. The effectiveness of the proposed methodis validated on an open-source medical heart dataset, specifically using theWHS++ data. The results demonstrate that the architecture performsexceptionally well, achieving a Dice coefficient of 0.939 during testing. Thisindicates full topology preservation for individual structures andsignificantly outperforms other baselines in preserving the overall scenetopology.</description><author>Chenyu Zhang, Wenxue Guan, Xiaodan Xing, Guang Yang</author><pubDate>Thu, 17 Oct 2024 23:07:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10551v3</guid></item><item><title>Scaling laws for post-training quantized large language models</title><link>http://arxiv.org/abs/2410.12119v2</link><description>Generalization abilities of well-trained large language models (LLMs) areknown to scale predictably as a function of model size. In contrast to theexistence of practical scaling laws governing pre-training, the quality of LLMsafter post-training compression remains highly unpredictable, often requiringcase-by-case validation in practice. In this work, we attempted to close thisgap for post-training weight quantization of LLMs by conducting a systematicempirical study on multiple LLM families quantized to numerous low-precisiontensor data types using popular weight quantization techniques. We identifiedkey scaling factors pertaining to characteristics of the local loss landscape,based on which the performance of quantized LLMs can be reasonably wellpredicted by a statistical model.</description><author>Zifei Xu, Alexander Lan, Wanzin Yazar, Tristan Webb, Sayeh Sharify, Xin Wang</author><pubDate>Thu, 17 Oct 2024 22:40:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.12119v2</guid></item><item><title>UniDrive: Towards Universal Driving Perception Across Camera Configurations</title><link>http://arxiv.org/abs/2410.13864v1</link><description>Vision-centric autonomous driving has demonstrated excellent performance witheconomical sensors. As the fundamental step, 3D perception aims to infer 3Dinformation from 2D images based on 3D-2D projection. This makes drivingperception models susceptible to sensor configuration (e.g., camera intrinsicsand extrinsics) variations. However, generalizing across camera configurationsis important for deploying autonomous driving models on different car models.In this paper, we present UniDrive, a novel framework for vision-centricautonomous driving to achieve universal perception across cameraconfigurations. We deploy a set of unified virtual cameras and propose aground-aware projection method to effectively transform the original imagesinto these unified virtual views. We further propose a virtual configurationoptimization method by minimizing the expected projection error betweenoriginal cameras and virtual cameras. The proposed virtual camera projectioncan be applied to existing 3D perception methods as a plug-and-play module tomitigate the challenges posed by camera parameter variability, resulting inmore adaptable and reliable driving perception models. To evaluate theeffectiveness of our framework, we collect a dataset on Carla by driving thesame routes while only modifying the camera configurations. Experimentalresults demonstrate that our method trained on one specific cameraconfiguration can generalize to varying configurations with minor performancedegradation.</description><author>Ye Li, Wenzhao Zheng, Xiaonan Huang, Kurt Keutzer</author><pubDate>Thu, 17 Oct 2024 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13864v1</guid></item><item><title>Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens</title><link>http://arxiv.org/abs/2410.13863v1</link><description>Scaling up autoregressive models in vision has not proven as beneficial as inlarge language models. In this work, we investigate this scaling problem in thecontext of text-to-image generation, focusing on two critical factors: whethermodels use discrete or continuous tokens, and whether tokens are generated in arandom or fixed raster order using BERT- or GPT-like transformer architectures.Our empirical results show that, while all models scale effectively in terms ofvalidation loss, their evaluation performance -- measured by FID, GenEvalscore, and visual quality -- follows different trends. Models based oncontinuous tokens achieve significantly better visual quality than those usingdiscrete tokens. Furthermore, the generation order and attention mechanismssignificantly affect the GenEval score: random-order models achieve notablybetter GenEval scores compared to raster-order models. Inspired by thesefindings, we train Fluid, a random-order autoregressive model on continuoustokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16on MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope ourfindings and results will encourage future efforts to further bridge thescaling gap between vision and language models.</description><author>Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, Yonglong Tian</author><pubDate>Thu, 17 Oct 2024 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13863v1</guid></item><item><title>DepthSplat: Connecting Gaussian Splatting and Depth</title><link>http://arxiv.org/abs/2410.13862v1</link><description>Gaussian splatting and single/multi-view depth estimation are typicallystudied in isolation. In this paper, we present DepthSplat to connect Gaussiansplatting and depth estimation and study their interactions. More specifically,we first contribute a robust multi-view depth model by leveraging pre-trainedmonocular depth features, leading to high-quality feed-forward 3D Gaussiansplatting reconstructions. We also show that Gaussian splatting can serve as anunsupervised pre-training objective for learning powerful depth models fromlarge-scale unlabelled datasets. We validate the synergy between Gaussiansplatting and depth estimation through extensive ablation and cross-tasktransfer experiments. Our DepthSplat achieves state-of-the-art performance onScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation andnovel view synthesis, demonstrating the mutual benefits of connecting bothtasks. Our code, models, and video results are available athttps://haofeixu.github.io/depthsplat/.</description><author>Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, Marc Pollefeys</author><pubDate>Thu, 17 Oct 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13862v1</guid></item><item><title>PUMA: Empowering Unified MLLM with Multi-granular Visual Generation</title><link>http://arxiv.org/abs/2410.13861v1</link><description>Recent advancements in multimodal foundation models have yielded significantprogress in vision-language understanding. Initial attempts have also exploredthe potential of multimodal large language models (MLLMs) for visual contentgeneration. However, existing works have insufficiently addressed the varyinggranularity demands of different image generation tasks within a unified MLLMparadigm - from the diversity required in text-to-image generation to theprecise controllability needed in image manipulation. In this work, we proposePUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMAunifies multi-granular visual features as both inputs and outputs of MLLMs,elegantly addressing the different granularity requirements of various imagegeneration tasks within a unified MLLM framework. Following multimodalpretraining and task-specific instruction tuning, PUMA demonstrates proficiencyin a wide range of multimodal tasks. This work represents a significant steptowards a truly unified MLLM capable of adapting to the granularity demands ofvarious visual tasks. The code and model will be released inhttps://github.com/rongyaofang/PUMA.</description><author>Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Li, Xihui Liu</author><pubDate>Thu, 17 Oct 2024 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13861v1</guid></item><item><title>VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding</title><link>http://arxiv.org/abs/2410.13860v1</link><description>3D visual grounding is crucial for robots, requiring integration of naturallanguage and 3D scene understanding. Traditional methods depending onsupervised learning with 3D point clouds are limited by scarce datasets.Recently zero-shot methods leveraging LLMs have been proposed to address thedata issue. While effective, these methods only use object-centric information,limiting their ability to handle complex queries. In this work, we presentVLM-Grounder, a novel framework using vision-language models (VLMs) forzero-shot 3D visual grounding based solely on 2D images. VLM-Grounderdynamically stitches image sequences, employs a grounding and feedback schemeto find the target object, and uses a multi-view ensemble projection toaccurately estimate 3D bounding boxes. Experiments on ScanRefer and Nr3Ddatasets show VLM-Grounder outperforms previous zero-shot methods, achieving51.6% Acc@0.25 on ScanRefer and 48.0% Acc on Nr3D, without relying on 3Dgeometry or object priors. Codes are available athttps://github.com/OpenRobotLab/VLM-Grounder .</description><author>Runsen Xu, Zhiwei Huang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua Lin</author><pubDate>Thu, 17 Oct 2024 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13860v1</guid></item><item><title>$Œ≥-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2410.13859v1</link><description>Despite the significant progress in multimodal large language models (MLLMs),their high computational cost remains a barrier to real-world deployment.Inspired by the mixture of depths (MoDs) in natural language processing, we aimto address this limitation from the perspective of ``activated tokens''. Ourkey insight is that if most tokens are redundant for the layer computation,then can be skipped directly via the MoD layer. However, directly convertingthe dense layers of MLLMs to MoD layers leads to substantial performancedegradation. To address this issue, we propose an innovative MoD adaptationstrategy for existing MLLMs called $\gamma$-MoD. In $\gamma$-MoD, a novelmetric is proposed to guide the deployment of MoDs in the MLLM, namely rank ofattention maps (ARank). Through ARank, we can effectively identify which layeris redundant and should be replaced with the MoD layer. Based on ARank, wefurther propose two novel designs to maximize the computational sparsity ofMLLM while maintaining its performance, namely shared vision-language routerand masked routing learning. With these designs, more than 90% dense layers ofthe MLLM can be effectively converted to the MoD ones. To validate our method,we apply it to three popular MLLMs, and conduct extensive experiments on 9benchmark datasets. Experimental results not only validate the significantefficiency benefit of $\gamma$-MoD to existing MLLMs but also confirm itsgeneralization ability on various MLLMs. For example, with a minor performancedrop, i.e., -1.5%, $\gamma$-MoD can reduce the training and inference time ofLLaVA-HR by 31.0% and 53.2%, respectively.</description><author>Yaxin Luo, Gen Luo, Jiayi Ji, Yiyi Zhou, Xiaoshuai Sun, Zhiqiang Shen, Rongrong Ji</author><pubDate>Thu, 17 Oct 2024 17:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13859v1</guid></item><item><title>How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs</title><link>http://arxiv.org/abs/2410.13857v1</link><description>Despite the remarkable success of Transformer-based Large Language Models(LLMs) across various domains, understanding and enhancing their mathematicalcapabilities remains a significant challenge. In this paper, we conduct arigorous theoretical analysis of LLMs' mathematical abilities, with a specificfocus on their arithmetic performances. We identify numerical precision as akey factor that influences their effectiveness in mathematical tasks. Ourresults show that Transformers operating with low numerical precision fail toaddress arithmetic tasks, such as iterated addition and integer multiplication,unless the model size grows super-polynomially with respect to the inputlength. In contrast, Transformers with standard numerical precision canefficiently handle these tasks with significantly smaller model sizes. Wefurther support our theoretical findings through empirical experiments thatexplore the impact of varying numerical precision on arithmetic tasks,providing valuable insights for improving the mathematical reasoningcapabilities of LLMs.</description><author>Guhao Feng, Kai Yang, Yuntian Gu, Xinyue Ai, Shengjie Luo, Jiacheng Sun, Di He, Zhenguo Li, Liwei Wang</author><pubDate>Thu, 17 Oct 2024 17:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13857v1</guid></item><item><title>Diffusing States and Matching Scores: A New Framework for Imitation Learning</title><link>http://arxiv.org/abs/2410.13855v1</link><description>Adversarial Imitation Learning is traditionally framed as a two-playerzero-sum game between a learner and an adversarially chosen cost function, andcan therefore be thought of as the sequential generalization of a GenerativeAdversarial Network (GAN). A prominent example of this framework is GenerativeAdversarial Imitation Learning (GAIL). However, in recent years, diffusionmodels have emerged as a non-adversarial alternative to GANs that merelyrequire training a score function via regression, yet produce generations of ahigher quality. In response, we investigate how to lift insights from diffusionmodeling to the sequential setting. We propose diffusing states and performingscore-matching along diffused states to measure the discrepancy between theexpert's and learner's states. Thus, our approach only requires training scorefunctions to predict noises via standard regression, making it significantlyeasier and more stable to train than adversarial methods. Theoretically, weprove first- and second-order instance-dependent bounds with linear scaling inthe horizon, proving that our approach avoids the compounding errors thatstymie offline approaches to imitation learning. Empirically, we show ourapproach outperforms GAN-style imitation learning baselines across variouscontinuous control problems, including complex tasks like controlling humanoidsto walk, sit, and crawl.</description><author>Runzhe Wu, Yiding Chen, Gokul Swamy, Kiant√© Brantley, Wen Sun</author><pubDate>Thu, 17 Oct 2024 17:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13855v1</guid></item><item><title>Can MLLMs Understand the Deep Implication Behind Chinese Images?</title><link>http://arxiv.org/abs/2410.13854v1</link><description>As the capabilities of Multimodal Large Language Models (MLLMs) continue toimprove, the need for higher-order capability evaluation of MLLMs isincreasing. However, there is a lack of work evaluating MLLM for higher-orderperception and understanding of Chinese visual content. To fill the gap, weintroduce the **C**hinese **I**mage **I**mplication understanding**Bench**mark, **CII-Bench**, which aims to assess the higher-order perceptionand understanding capabilities of MLLMs for Chinese images. CII-Bench standsout in several ways compared to existing benchmarks. Firstly, to ensure theauthenticity of the Chinese context, images in CII-Bench are sourced from theChinese Internet and manually reviewed, with corresponding answers alsomanually crafted. Additionally, CII-Bench incorporates images that representChinese traditional culture, such as famous Chinese traditional paintings,which can deeply reflect the model's understanding of Chinese traditionalculture. Through extensive experiments on CII-Bench across multiple MLLMs, wehave made significant findings. Initially, a substantial gap is observedbetween the performance of MLLMs and humans on CII-Bench. The highest accuracyof MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at animpressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditionalculture images, suggesting limitations in their ability to understandhigh-level semantics and lack a deep knowledge base of Chinese traditionalculture. Finally, it is observed that most models exhibit enhanced accuracywhen image emotion hints are incorporated into the prompts. We believe thatCII-Bench will enable MLLMs to gain a better understanding of Chinese semanticsand Chinese-specific images, advancing the journey towards expert artificialgeneral intelligence (AGI). Our project is publicly available athttps://cii-bench.github.io/.</description><author>Chenhao Zhang, Xi Feng, Yuelin Bai, Xinrun Du, Jinchang Hou, Kaixin Deng, Guangzeng Han, Qinrui Li, Bingli Wang, Jiaheng Liu, Xingwei Qu, Yifei Zhang, Qixuan Zhao, Yiming Liang, Ziqiang Liu, Feiteng Fang, Min Yang, Wenhao Huang, Chenghua Lin, Ge Zhang, Shiwen Ni</author><pubDate>Thu, 17 Oct 2024 17:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13854v1</guid></item><item><title>AutoAL: Automated Active Learning with Differentiable Query Strategy Search</title><link>http://arxiv.org/abs/2410.13853v1</link><description>As deep learning continues to evolve, the need for data efficiency becomesincreasingly important. Considering labeling large datasets is bothtime-consuming and expensive, active learning (AL) provides a promisingsolution to this challenge by iteratively selecting the most informativesubsets of examples to train deep neural networks, thereby reducing thelabeling cost. However, the effectiveness of different AL algorithms can varysignificantly across data scenarios, and determining which AL algorithm bestfits a given task remains a challenging problem. This work presents the firstdifferentiable AL strategy search method, named AutoAL, which is designed ontop of existing AL sampling strategies. AutoAL consists of two neural nets,named SearchNet and FitNet, which are optimized concurrently under adifferentiable bi-level optimization framework. For any given task, SearchNetand FitNet are iteratively co-optimized using the labeled data, learning howwell a set of candidate AL algorithms perform on that task. With the optimal ALstrategies identified, SearchNet selects a small subset from the unlabeled poolfor querying their annotations, enabling efficient training of the task model.Experimental results demonstrate that AutoAL consistently achieves superioraccuracy compared to all candidate AL algorithms and other selective ALapproaches, showcasing its potential for adapting and integrating multipleexisting AL methods across diverse tasks and domains. Code will be availableat: https://github.com/haizailache999/AutoAL.</description><author>Yifeng Wang, Xueying Zhan, Siyu Huang</author><pubDate>Thu, 17 Oct 2024 17:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13853v1</guid></item><item><title>Retrospective Learning from Interactions</title><link>http://arxiv.org/abs/2410.13852v1</link><description>Multi-turn interactions between large language models (LLMs) and usersnaturally include implicit feedback signals. If an LLM responds in anunexpected way to an instruction, the user is likely to signal it by rephrasingthe request, expressing frustration, or pivoting to an alternative task. Suchsignals are task-independent and occupy a relatively constrained subspace oflanguage, allowing the LLM to identify them even if it fails on the actualtask. This creates an avenue for continually learning from interactions withoutadditional annotations. We introduce ReSpect, a method to learn from suchsignals in past interactions via retrospection. We deploy ReSpect in a newmultimodal interaction scenario, where humans instruct an LLM to solve anabstract reasoning task with a combinatorial solution space. Through thousandsof interactions with humans, we show how ReSpect gradually improves taskcompletion rate from 31% to 82%, all without any external annotation.</description><author>Zizhao Chen, Mustafa Omer Gul, Yiwei Chen, Gloria Geng, Anne Wu, Yoav Artzi</author><pubDate>Thu, 17 Oct 2024 17:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13852v1</guid></item><item><title>Differentiable Robot Rendering</title><link>http://arxiv.org/abs/2410.13851v1</link><description>Vision foundation models trained on massive amounts of visual data have shownunprecedented reasoning and planning skills in open-world settings. A keychallenge in applying them to robotic tasks is the modality gap between visualdata and action data. We introduce differentiable robot rendering, a methodallowing the visual appearance of a robot body to be directly differentiablewith respect to its control parameters. Our model integrates a kinematics-awaredeformable model and Gaussians Splatting and is compatible with any robot formfactors and degrees of freedom. We demonstrate its capability and usage inapplications including reconstruction of robot poses from images andcontrolling robots through vision language models. Quantitative and qualitativeresults show that our differentiable rendering model provides effectivegradients for robotic control directly from pixels, setting the foundation forthe future applications of vision foundation models in robotics.</description><author>Ruoshi Liu, Alper Canberk, Shuran Song, Carl Vondrick</author><pubDate>Thu, 17 Oct 2024 17:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13851v1</guid></item><item><title>Influence Functions for Scalable Data Attribution in Diffusion Models</title><link>http://arxiv.org/abs/2410.13850v1</link><description>Diffusion models have led to significant advancements in generativemodelling. Yet their widespread adoption poses challenges regarding dataattribution and interpretability. In this paper, we aim to help address suchchallenges in diffusion models by developing an \textit{influence functions}framework. Influence function-based data attribution methods approximate how amodel's output would have changed if some training data were removed. Insupervised learning, this is usually used for predicting how the loss on aparticular example would change. For diffusion models, we focus on predictingthe change in the probability of generating a particular example via severalproxy measurements. We show how to formulate influence functions for suchquantities and how previously proposed methods can be interpreted as particulardesign choices in our framework. To ensure scalability of the Hessiancomputations in influence functions, we systematically develop K-FACapproximations based on generalised Gauss-Newton matrices specifically tailoredto diffusion models. We recast previously proposed methods as specific designchoices in our framework and show that our recommended method outperformsprevious data attribution approaches on common evaluations, such as the LinearData-modelling Score (LDS) or retraining without top influences, without theneed for method-specific hyperparameter tuning.</description><author>Bruno Mlodozeniec, Runa Eschenhagen, Juhan Bae, Alexander Immer, David Krueger, Richard Turner</author><pubDate>Thu, 17 Oct 2024 17:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13850v1</guid></item><item><title>From Gradient Clipping to Normalization for Heavy Tailed SGD</title><link>http://arxiv.org/abs/2410.13849v1</link><description>Recent empirical evidence indicates that many machine learning applicationsinvolve heavy-tailed gradient noise, which challenges the standard assumptionsof bounded variance in stochastic optimization. Gradient clipping has emergedas a popular tool to handle this heavy-tailed noise, as it achieves goodperformance in this setting both theoretically and practically. However, ourcurrent theoretical understanding of non-convex gradient clipping has threemain shortcomings. First, the theory hinges on large, increasing clippingthresholds, which are in stark contrast to the small constant clippingthresholds employed in practice. Second, clipping thresholds require knowledgeof problem-dependent parameters to guarantee convergence. Lastly, even withthis knowledge, current sampling complexity upper bounds for the method aresub-optimal in nearly all parameters. To address these issues, we studyconvergence of Normalized SGD (NSGD). First, we establish a parameter-freesample complexity for NSGD of$\mathcal{O}\left(\varepsilon^{-\frac{2p}{p-1}}\right)$ to find an$\varepsilon$-stationary point. Furthermore, we prove tightness of this result,by providing a matching algorithm-specific lower bound. In the setting whereall problem parameters are known, we show this complexity is improved to$\mathcal{O}\left(\varepsilon^{-\frac{3p-2}{p-1}}\right)$, matching thepreviously known lower bound for all first-order methods in all problemdependent parameters. Finally, we establish high-probability convergence ofNSGD with a mild logarithmic dependence on the failure probability. Our workcomplements the studies of gradient clipping under heavy tailed noise improvingthe sample complexities of existing algorithms and offering an alternativemechanism to achieve high probability convergence.</description><author>Florian H√ºbler, Ilyas Fatkhullin, Niao He</author><pubDate>Thu, 17 Oct 2024 17:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13849v1</guid></item><item><title>Towards Multilingual LLM Evaluation for European Languages</title><link>http://arxiv.org/abs/2410.08928v2</link><description>The rise of Large Language Models (LLMs) has revolutionized natural languageprocessing across numerous languages and tasks. However, evaluating LLMperformance in a consistent and meaningful way across multiple Europeanlanguages remains challenging, especially due to the scarcity oflanguage-parallel multilingual benchmarks. We introduce a multilingualevaluation approach tailored for European languages. We employ translatedversions of five widely-used benchmarks to assess the capabilities of 40 LLMsacross 21 European languages. Our contributions include examining theeffectiveness of translated benchmarks, assessing the impact of differenttranslation services, and offering a multilingual evaluation framework for LLMsthat includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC,EU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publiclyavailable to encourage further research in multilingual LLM evaluation.</description><author>Klaudia Thellmann, Bernhard Stadler, Michael Fromm, Jasper Schulze Buschhoff, Alex Jude, Fabio Barth, Johannes Leveling, Nicolas Flores-Herr, Joachim K√∂hler, Ren√© J√§kel, Mehdi Ali</author><pubDate>Thu, 17 Oct 2024 17:58:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08928v2</guid></item><item><title>Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</title><link>http://arxiv.org/abs/2410.13848v1</link><description>In this paper, we introduce Janus, an autoregressive framework that unifiesmultimodal understanding and generation. Prior research often relies on asingle visual encoder for both tasks, such as Chameleon. However, due to thediffering levels of information granularity required by multimodalunderstanding and generation, this approach can lead to suboptimal performance,particularly in multimodal understanding. To address this issue, we decouplevisual encoding into separate pathways, while still leveraging a single,unified transformer architecture for processing. The decoupling not onlyalleviates the conflict between the visual encoder's roles in understanding andgeneration, but also enhances the framework's flexibility. For instance, boththe multimodal understanding and generation components can independently selecttheir most suitable encoding methods. Experiments show that Janus surpassesprevious unified model and matches or exceeds the performance of task-specificmodels. The simplicity, high flexibility, and effectiveness of Janus make it astrong candidate for next-generation unified multimodal models.</description><author>Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, Ping Luo</author><pubDate>Thu, 17 Oct 2024 17:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13848v1</guid></item><item><title>SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction</title><link>http://arxiv.org/abs/2410.13846v1</link><description>Recent advancements in large language models (LLMs) have extended theircapabilities to handle long contexts. However, increasing the number of modellayers and the length of input sequences significantly escalates the memoryrequired to store key-value (KV) cache, posing challenges for efficientinference. To mitigate this issue, we present SimLayerKV, a simple yeteffective method that reduces inter-layer KV cache redundancies by selectivelydropping cache in identified lazy layers. Our approach is based on theobservation that certain layers in long-context LLMs exhibit "lazy" behavior,contributing less to modeling long-range dependencies compared to non-lazylayers. By analyzing attention weight patterns, we find that the behavior ofthese lazy layers is consistent across tokens during generation for a giveninput. This insight motivates our SimLayerKV, which identifies lazy layers andreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,and can be implemented with only seven lines of code. We conduct extensiveexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, andMistral-7B across 16 tasks from the LongBench benchmark. The resultsdemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\times$with only a 1.2% performance drop when combined with 4-bit quantization. Ourcode is available at https://github.com/sail-sg/SimLayerKV.</description><author>Xuan Zhang, Cunxiao Du, Chao Du, Tianyu Pang, Wei Gao, Min Lin</author><pubDate>Thu, 17 Oct 2024 17:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13846v1</guid></item><item><title>D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement</title><link>http://arxiv.org/abs/2410.13842v1</link><description>We introduce D-FINE, a powerful real-time object detector that achievesoutstanding localization precision by redefining the bounding box regressiontask in DETR models. D-FINE comprises two key components: Fine-grainedDistribution Refinement (FDR) and Global Optimal Localization Self-Distillation(GO-LSD). FDR transforms the regression process from predicting fixedcoordinates to iteratively refining probability distributions, providing afine-grained intermediate representation that significantly enhanceslocalization accuracy. GO-LSD is a bidirectional optimization strategy thattransfers localization knowledge from refined distributions to shallower layersthrough self-distillation, while also simplifying the residual prediction tasksfor deeper layers. Additionally, D-FINE incorporates lightweight optimizationsin computationally intensive modules and operations, achieving a better balancebetween speed and accuracy. Specifically, D-FINE-L / X achieves 54.0% / 55.8%AP on the COCO dataset at 124 / 78 FPS on an NVIDIA T4 GPU. When pretrained onObjects365, D-FINE-L / X attains 57.1% / 59.3% AP, surpassing all existingreal-time detectors. Furthermore, our method significantly enhances theperformance of a wide range of DETR models by up to 5.3% AP with negligibleextra parameters and training costs. Our code and pretrained models:https://github.com/Peterande/D-FINE.</description><author>Yansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, Feng Wu</author><pubDate>Thu, 17 Oct 2024 17:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13842v1</guid></item><item><title>A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models</title><link>http://arxiv.org/abs/2410.13841v1</link><description>Post-training has emerged as a crucial paradigm for adapting large-scalepre-trained models to various tasks, whose effects are fully reflected by deltaparameters (i.e., the disparity between post-trained and pre-trainedparameters). While numerous studies have explored delta parameter propertiesvia operations like pruning, quantization, low-rank approximation, andextrapolation, a unified framework for systematically examining thesecharacteristics has been lacking. In this paper, we propose a novel perspectivebased on Riemann sum approximation of the loss function to elucidate deltaparameter editing operations. Our analysis categorizes existing methods intothree classes based on their post-editing performance: competitive, decreased,and improved, explaining how they are expressed by the Riemann sumapproximation term and how they alter the model performance. Extensiveexperiments on both visual and language models, including ViT, LLaMA 3, Qwen 2,and Mistral, corroborate our theoretical findings. Furthermore, we introduceextensions to existing techniques like DARE and BitDelta, highlighting theirlimitations in leveraging the properties of delta parameters and reorganizingthem into general expressions to enhance the applicability and effectiveness ofdelta parameter editing in post-trained models.</description><author>Qiaoyu Tang, Le Yu, Bowen Yu, Hongyu Lin, Keming Lu, Yaojie Lu, Xianpei Han, Le Sun</author><pubDate>Thu, 17 Oct 2024 17:56:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13841v1</guid></item><item><title>Accelerating Codec-based Speech Synthesis with Multi-Token Prediction and Speculative Decoding</title><link>http://arxiv.org/abs/2410.13839v1</link><description>The goal of this paper is to accelerate codec-based speech synthesis systemswith minimum sacrifice to speech quality. We propose an enhanced inferencemethod that allows for flexible trade-offs between speed and quality duringinference without requiring additional training. Our core idea is to predictmultiple tokens per inference step of the AR module using multiple predictionheads, resulting in a linear reduction in synthesis time as the number of headsincreases. Furthermore, we introduce a novel speculative decoding techniquethat utilises a Viterbi-based algorithm to select the optimal sequence ofgenerated tokens at each decoding step. In our experiments, we demonstrate thatthe time required to predict each token is reduced by a factor of 4 to 5compared to baseline models, with minimal quality trade-off or even improvementin terms of speech intelligibility. Audio samples are available at:multpletokensprediction.github.io/multipletokensprediction.github.io/.</description><author>Tan Dat Nguyen, Ji-Hoon Kim, Jeongsoo Choi, Shukjae Choi, Jinseok Park, Younglo Lee, Joon Son Chung</author><pubDate>Thu, 17 Oct 2024 17:55:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13839v1</guid></item><item><title>ORSO: Accelerating Reward Design via Online Reward Selection and Policy Optimization</title><link>http://arxiv.org/abs/2410.13837v1</link><description>Reward shaping is a critical component in reinforcement learning (RL),particularly for complex tasks where sparse rewards can hinder learning. Whileshaping rewards have been introduced to provide additional guidance, selectingeffective shaping functions remains challenging and computationally expensive.This paper introduces Online Reward Selection and Policy Optimization (ORSO), anovel approach that frames shaping reward selection as an online modelselection problem. ORSO employs principled exploration strategies toautomatically identify promising shaping reward functions without humanintervention, balancing exploration and exploitation with provable regretguarantees. We demonstrate ORSO's effectiveness across various continuouscontrol tasks using the Isaac Gym simulator. Compared to traditional methodsthat fully evaluate each shaping reward function, ORSO significantly improvessample efficiency, reduces computational time, and consistently identifieshigh-quality reward functions that produce policies comparable to thosegenerated by domain experts through hand-engineered rewards.</description><author>Chen Bo Calvin Zhang, Zhang-Wei Hong, Aldo Pacchiano, Pulkit Agrawal</author><pubDate>Thu, 17 Oct 2024 17:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13837v1</guid></item><item><title>Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs</title><link>http://arxiv.org/abs/2410.13835v1</link><description>Practitioners have consistently observed three puzzling phenomena intransformer-based large language models (LLMs): attention sinks, value-statedrains, and residual-state peaks, collectively referred to as extreme-tokenphenomena. These phenomena are characterized by certain so-called "sink tokens"receiving disproportionately high attention weights, exhibiting significantlysmaller value states, and having much larger residual-state norms than those ofother tokens. These extreme tokens give rise to various challenges in LLMinference, quantization, and interpretability. We elucidate the mechanisms behind extreme-token phenomena. First, we showthat these phenomena arise in very simple architectures -- transformers withone to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.In this setting, we identify an active-dormant mechanism, where attention headsbecome sinks for specific input domains while remaining non-sinks for others.Our theoretical analysis of the training dynamics reveals that these phenomenaare driven by a mutual reinforcement mechanism. Building on these insights, wepropose strategies to mitigate extreme-token phenomena during pretraining,including replacing softmax with ReLU and Adam with SGD. Next, we extend ouranalysis to pretrained LLMs, including Llama and OLMo, showing that manyattention heads exhibit a similar active-dormant mechanism as in the BB task,and that the mutual reinforcement mechanism also governs the emergence ofextreme-token phenomena during LLM pretraining. Our results reveal that many ofthe static and dynamic properties of extreme-token phenomena predicted by theBB task align with observations in pretrained LLMs.</description><author>Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael I. Jordan, Song Mei</author><pubDate>Thu, 17 Oct 2024 17:54:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13835v1</guid></item><item><title>VidPanos: Generative Panoramic Videos from Casual Panning Videos</title><link>http://arxiv.org/abs/2410.13832v1</link><description>Panoramic image stitching provides a unified, wide-angle view of a scene thatextends beyond the camera's field of view. Stitching frames of a panning videointo a panoramic photograph is a well-understood problem for stationary scenes,but when objects are moving, a still panorama cannot capture the scene. Wepresent a method for synthesizing a panoramic video from a casually-capturedpanning video, as if the original video were captured with a wide-angle camera.We pose panorama synthesis as a space-time outpainting problem, where we aim tocreate a full panoramic video of the same length as the input video. Consistentcompletion of the space-time volume requires a powerful, realistic prior overvideo content and motion, for which we adapt generative video models. Existinggenerative models do not, however, immediately extend to panorama completion,as we show. We instead apply video generation as a component of our panoramasynthesis system, and demonstrate how to exploit the strengths of the modelswhile minimizing their limitations. Our system can create video panoramas for arange of in-the-wild scenes including people, vehicles, and flowing water, aswell as stationary background features.</description><author>Jingwei Ma, Erika Lu, Roni Paiss, Shiran Zada, Aleksander Holynski, Tali Dekel, Brian Curless, Michael Rubinstein, Forrester Cole</author><pubDate>Thu, 17 Oct 2024 17:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13832v1</guid></item><item><title>The Disparate Benefits of Deep Ensembles</title><link>http://arxiv.org/abs/2410.13831v1</link><description>Ensembles of Deep Neural Networks, Deep Ensembles, are widely used as asimple way to boost predictive performance. However, their impact onalgorithmic fairness is not well understood yet. Algorithmic fairnessinvestigates how a model's performance varies across different groups,typically defined by protected attributes such as age, gender, or race. In thiswork, we investigate the interplay between the performance gains from DeepEnsembles and fairness. Our analysis reveals that they unevenly favor differentgroups in what we refer to as a disparate benefits effect. We empiricallyinvestigate this effect with Deep Ensembles applied to popular facial analysisand medical imaging datasets, where protected group attributes are given andfind that it occurs for multiple established group fairness metrics, includingstatistical parity and equal opportunity. Furthermore, we identify theper-group difference in predictive diversity of ensemble members as thepotential cause of the disparate benefits effect. Finally, we evaluatedifferent approaches to reduce unfairness due to the disparate benefits effect.Our findings show that post-processing is an effective method to mitigate thisunfairness while preserving the improved performance of Deep Ensembles.</description><author>Kajetan Schweighofer, Adrian Arnaiz-Rodriguez, Sepp Hochreiter, Nuria Oliver</author><pubDate>Thu, 17 Oct 2024 17:53:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13831v1</guid></item><item><title>DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control</title><link>http://arxiv.org/abs/2410.13830v1</link><description>Recent advances in customized video generation have enabled users to createvideos tailored to both specific subjects and motion trajectories. However,existing methods often require complicated test-time fine-tuning and strugglewith balancing subject learning and motion control, limiting their real-worldapplications. In this paper, we present DreamVideo-2, a zero-shot videocustomization framework capable of generating videos with a specific subjectand motion trajectory, guided by a single image and a bounding box sequence,respectively, and without the need for test-time fine-tuning. Specifically, weintroduce reference attention, which leverages the model's inherentcapabilities for subject learning, and devise a mask-guided motion module toachieve precise motion control by fully utilizing the robust motion signal ofbox masks derived from bounding boxes. While these two components achieve theirintended functions, we empirically observe that motion control tends todominate over subject learning. To address this, we propose two key designs: 1)the masked reference attention, which integrates a blended latent mask modelingscheme into reference attention to enhance subject representations at thedesired positions, and 2) a reweighted diffusion loss, which differentiates thecontributions of regions inside and outside the bounding boxes to ensure abalance between subject and motion control. Extensive experimental results on anewly curated dataset demonstrate that DreamVideo-2 outperformsstate-of-the-art methods in both subject customization and motion control. Thedataset, code, and models will be made publicly available.</description><author>Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, Yingya Zhang, Hongming Shan</author><pubDate>Thu, 17 Oct 2024 17:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13830v1</guid></item><item><title>A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement</title><link>http://arxiv.org/abs/2410.13828v1</link><description>Reinforcement Learning from Human Feedback (RLHF) has become the predominantapproach for language model (LM) alignment. At its core, RLHF uses amargin-based loss for preference optimization, specifying ideal LM behavioronly by the difference between preferred and dispreferred responses. In thispaper, we identify a common pitfall of margin-based methods -- theunder-specification of ideal LM behavior on preferred and dispreferredresponses individually, which leads to two unintended consequences as themargin increases: (1) The probability of dispreferred (e.g., unsafe) responsesmay increase, resulting in potential safety alignment failures. (2) Theprobability of preferred responses may decrease, even when those responses areideal. We demystify the reasons behind these problematic behaviors:margin-based losses couple the change in the preferred probability to thegradient of the dispreferred one, and vice versa, often preventing thepreferred probability from increasing while the dispreferred one decreases, andthus causing a synchronized increase or decrease in both probabilities. We termthis effect, inherent in margin-based objectives, gradient entanglement.Formally, we derive conditions for general margin-based alignment objectivesunder which gradient entanglement becomes concerning: the inner product of thegradients of preferred and dispreferred log-probabilities is large relative tothe individual gradient norms. We theoretically investigate why such innerproducts can be large when aligning language models and empirically validateour findings. Empirical implications of our framework extend to explainingimportant differences in the training dynamics of various preferenceoptimization algorithms, and suggesting potential algorithm designs to mitigatethe under-specification issue of margin-based methods and thereby improvinglanguage model alignment.</description><author>Hui Yuan, Yifan Zeng, Yue Wu, Huazheng Wang, Mengdi Wang, Liu Leqi</author><pubDate>Thu, 17 Oct 2024 17:52:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13828v1</guid></item><item><title>Unearthing Skill-Level Insights for Understanding Trade-Offs of Foundation Models</title><link>http://arxiv.org/abs/2410.13826v1</link><description>With models getting stronger, evaluations have grown more complex, testingmultiple skills in one benchmark and even in the same instance at once.However, skill-wise performance is obscured when inspecting aggregate accuracy,under-utilizing the rich signal modern benchmarks contain. We propose anautomatic approach to recover the underlying skills relevant for any evaluationinstance, by way of inspecting model-generated rationales. After validating therelevance of rationale-parsed skills and inferring skills for $46$k instancesover $12$ benchmarks, we observe many skills to be common across benchmarks,resulting in the curation of hundreds of skill-slices (i.e. sets of instancestesting a common skill). Inspecting accuracy over these slices yields novelinsights on model trade-offs: e.g., compared to GPT-4o and Claude 3.5 Sonnet,on average, Gemini 1.5 Pro is $18\%$ more accurate in "computing molar mass",but $19\%$ less accurate in "applying constitutional law", despite the overallaccuracies of the three models differing by a mere $0.4\%$. Furthermore, wedemonstrate the practical utility of our approach by showing that insightsderived from skill slice analysis can generalize to held-out instances: whenrouting each instance to the model strongest on the relevant skills, we see a$3\%$ accuracy improvement over our $12$ dataset corpus. Our skill-slices andframework open a new avenue in model evaluation, leveraging skill-specificanalyses to unlock a more granular and actionable understanding of modelcapabilities.</description><author>Mazda Moayeri, Vidhisha Balachandran, Varun Chandrasekaran, Safoora Yousefi, Thomas Fel, Soheil Feizi, Besmira Nushi, Neel Joshi, Vibhav Vineet</author><pubDate>Thu, 17 Oct 2024 17:51:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13826v1</guid></item><item><title>Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach</title><link>http://arxiv.org/abs/2407.16833v2</link><description>Retrieval Augmented Generation (RAG) has been a powerful tool for LargeLanguage Models (LLMs) to efficiently process overly lengthy contexts. However,recent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities tounderstand long contexts directly. We conduct a comprehensive comparisonbetween RAG and long-context (LC) LLMs, aiming to leverage the strengths ofboth. We benchmark RAG and LC across various public datasets using three latestLLMs. Results reveal that when resourced sufficiently, LC consistentlyoutperforms RAG in terms of average performance. However, RAG's significantlylower cost remains a distinct advantage. Based on this observation, we proposeSelf-Route, a simple yet effective method that routes queries to RAG or LCbased on model self-reflection. Self-Route significantly reduces thecomputation cost while maintaining a comparable performance to LC. Our findingsprovide a guideline for long-context applications of LLMs using RAG and LC.</description><author>Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky</author><pubDate>Thu, 17 Oct 2024 17:51:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16833v2</guid></item><item><title>AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents</title><link>http://arxiv.org/abs/2410.13825v1</link><description>Autonomy via agents using large language models (LLMs) for personalized,standardized tasks boosts human efficiency. Automating web tasks (like bookinghotels within a budget) is increasingly sought after. Fulfilling practicalneeds, the web agent also serves as an important proof-of-concept example forvarious agent grounding scenarios, with its success promising advancements inmany future applications. Prior research often handcrafts web agent strategies(e.g., prompting templates, multi-agent systems, search methods, etc.) and thecorresponding in-context examples, which may not generalize well across allreal-world scenarios. On the other hand, there has been limited study on themisalignment between a web agent's observation/action representation and thepre-training data of the LLM it's based on. This discrepancy is especiallynotable when LLMs are primarily trained for language completion rather thantasks involving embodied navigation actions and symbolic web elements. Ourstudy enhances an LLM-based web agent by simply refining its observation andaction space to better align with the LLM's capabilities. This approach enablesour base agent to significantly outperform previous methods on a wide varietyof web tasks. Specifically, on WebArena, a benchmark featuring general-purposeweb interaction tasks, our agent AgentOccam surpasses the previousstate-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolutepoints respectively, and boosts the success rate by 26.6 points (+161%) oversimilar plain web agents with its observation and action space alignment. Weachieve this without using in-context examples, new agent roles, onlinefeedback or search strategies. AgentOccam's simple design highlights LLMs'impressive zero-shot performance on web tasks, and underlines the critical roleof carefully tuning observation and action spaces for LLM-based agents.</description><author>Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, Huzefa Rangwala</author><pubDate>Thu, 17 Oct 2024 17:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13825v1</guid></item><item><title>Harnessing Webpage UIs for Text-Rich Visual Understanding</title><link>http://arxiv.org/abs/2410.13824v1</link><description>Text-rich visual understanding-the ability to process environments wheredense textual content is integrated with visuals-is crucial for multimodallarge language models (MLLMs) to interact effectively with structuredenvironments. To enhance this capability, we propose synthesizing generalmultimodal instructions from webpage UIs using text-based large language models(LLMs). Despite lacking direct visual input, text-based LLMs are able toprocess structured text representations from webpage accessibility trees. Theseinstructions are then paired with UI screenshots to train multimodal models. Weintroduce MultiUI, a dataset containing 7.3 million samples from 1 millionwebsites, covering diverse multimodal tasks and UI layouts. Models trained onMultiUI not only excel in web UI tasks-achieving up to a 48\% improvement onVisualWebBench and a 19.1\% boost in action accuracy on a web agent datasetMind2Web-but also generalize surprisingly well to non-web UI tasks and even tonon-UI domains, such as document understanding, OCR, and chart interpretation.These results highlight the broad applicability of web UI data for advancingtext-rich visual understanding across various scenarios.</description><author>Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue</author><pubDate>Thu, 17 Oct 2024 17:48:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13824v1</guid></item><item><title>Deep Generative Models Unveil Patterns in Medical Images Through Vision-Language Conditioning</title><link>http://arxiv.org/abs/2410.13823v1</link><description>Deep generative models have significantly advanced medical imaging analysisby enhancing dataset size and quality. Beyond mere data augmentation, ourresearch in this paper highlights an additional, significant capacity of deepgenerative models: their ability to reveal and demonstrate patterns in medicalimages. We employ a generative structure with hybrid conditions, combiningclinical data and segmentation masks to guide the image synthesis process.Furthermore, we innovatively transformed the tabular clinical data into textualdescriptions. This approach simplifies the handling of missing values and alsoenables us to leverage large pre-trained vision-language models thatinvestigate the relations between independent clinical entries and comprehendgeneral terms, such as gender and smoking status. Our approach differs from andpresents a more challenging task than traditional medical report-guidedsynthesis due to the less visual correlation of our clinical information withthe images. To overcome this, we introduce a text-visual embedding mechanismthat strengthens the conditions, ensuring the network effectively utilizes theprovided information. Our pipeline is generalizable to both GAN-based anddiffusion models. Experiments on chest CT, particularly focusing on the smokingstatus, demonstrated a consistent intensity shift in the lungs which is inagreement with clinical observations, indicating the effectiveness of ourmethod in capturing and visualizing the impact of specific attributes onmedical image patterns. Our methods offer a new avenue for the early detectionand precise visualization of complex clinical conditions with deep generativemodels. All codes are https://github.com/junzhin/DGM-VLC.</description><author>Xiaodan Xing, Junzhi Ning, Yang Nan, Guang Yang</author><pubDate>Thu, 17 Oct 2024 17:48:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13823v1</guid></item><item><title>Generalization-baed similarity</title><link>http://arxiv.org/abs/2302.10096v7</link><description>Detecting and exploiting similarities between seemingly distant objects iswithout doubt an important human ability. This paper develops \textit{from theground up} an abstract algebraic and qualitative notion of similarity based onthe observation that sets of generalizations encode important properties ofelements. We show that similarity defined in this way has appealingmathematical properties. As we construct our notion of similarity from firstprinciples using only elementary concepts of universal algebra, to convince thereader of its plausibility, we show that it can model fundamental relationsoccurring in mathematics and be naturally embedded into first-order logic viamodel-theoretic types.</description><author>Christian Antiƒá</author><pubDate>Thu, 17 Oct 2024 17:48:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10096v7</guid></item><item><title>Multi-style conversion for semantic segmentation of lesions in fundus images by adversarial attacks</title><link>http://arxiv.org/abs/2410.13822v1</link><description>The diagnosis of diabetic retinopathy, which relies on fundus images, faceschallenges in achieving transparency and interpretability when using a globalclassification approach. However, segmentation-based databases aresignificantly more expensive to acquire and combining them is oftenproblematic. This paper introduces a novel method, termed adversarial styleconversion, to address the lack of standardization in annotation styles acrossdiverse databases. By training a single architecture on combined databases, themodel spontaneously modifies its segmentation style depending on the input,demonstrating the ability to convert among different labeling styles. Theproposed methodology adds a linear probe to detect dataset origin based onencoder features and employs adversarial attacks to condition the model'ssegmentation style. Results indicate significant qualitative and quantitativethrough dataset combination, offering avenues for improved modelgeneralization, uncertainty estimation and continuous interpolation betweenannotation styles. Our approach enables training a segmentation model withdiverse databases while controlling and leveraging annotation styles forimproved retinopathy diagnosis.</description><author>Cl√©ment Playout, Renaud Duval, Marie Carole Boucher, Farida Cheriet</author><pubDate>Thu, 17 Oct 2024 17:48:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13822v1</guid></item><item><title>Artificial Kuramoto Oscillatory Neurons</title><link>http://arxiv.org/abs/2410.13821v1</link><description>It has long been known in both neuroscience and AI that ``binding'' betweenneurons leads to a form of competitive learning where representations arecompressed in order to represent more abstract concepts in deeper layers of thenetwork. More recently, it was also hypothesized that dynamic (spatiotemporal)representations play an important role in both neuroscience and AI. Building onthese ideas, we introduce Artificial Kuramoto Oscillatory Neurons (AKOrN) as adynamical alternative to threshold units, which can be combined with arbitraryconnectivity designs such as fully connected, convolutional, or attentivemechanisms. Our generalized Kuramoto updates bind neurons together throughtheir synchronization dynamics. We show that this idea provides performanceimprovements across a wide spectrum of tasks such as unsupervised objectdiscovery, adversarial robustness, calibrated uncertainty quantification, andreasoning. We believe that these empirical results show the importance ofrethinking our assumptions at the most basic neuronal level of neuralrepresentation, and in particular show the importance of dynamicalrepresentations.</description><author>Takeru Miyato, Sindy L√∂we, Andreas Geiger, Max Welling</author><pubDate>Thu, 17 Oct 2024 17:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13821v1</guid></item><item><title>Guided Reinforcement Learning for Robust Multi-Contact Loco-Manipulation</title><link>http://arxiv.org/abs/2410.13817v1</link><description>Reinforcement learning (RL) often necessitates a meticulous Markov DecisionProcess (MDP) design tailored to each task. This work aims to address thischallenge by proposing a systematic approach to behavior synthesis and controlfor multi-contact loco-manipulation tasks, such as navigating spring-loadeddoors and manipulating heavy dishwashers. We define a task-independent MDP totrain RL policies using only a single demonstration per task generated from amodel-based trajectory optimizer. Our approach incorporates an adaptive phasedynamics formulation to robustly track the demonstrations while accommodatingdynamic uncertainties and external disturbances. We compare our method againstprior motion imitation RL works and show that the learned policies achievehigher success rates across all considered tasks. These policies learn recoverymaneuvers that are not present in the demonstration, such as re-graspingobjects during execution or dealing with slippages. Finally, we successfullytransfer the policies to a real robot, demonstrating the practical viability ofour approach.</description><author>Jean-Pierre Sleiman, Mayank Mittal, Marco Hutter</author><pubDate>Thu, 17 Oct 2024 17:46:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13817v1</guid></item><item><title>Steering Your Generalists: Improving Robotic Foundation Models via Value Guidance</title><link>http://arxiv.org/abs/2410.13816v1</link><description>Large, general-purpose robotic policies trained on diverse demonstrationdatasets have been shown to be remarkably effective both for controlling avariety of robots in a range of different scenes, and for acquiring broadrepertoires of manipulation skills. However, the data that such policies aretrained on is generally of mixed quality -- not only are human-collecteddemonstrations unlikely to perform the task perfectly, but the larger thedataset is, the harder it is to curate only the highest quality examples. Italso remains unclear how optimal data from one embodiment is for training onanother embodiment. In this paper, we present a general and broadly applicableapproach that enhances the performance of such generalist robot policies atdeployment time by re-ranking their actions according to a value functionlearned via offline RL. This approach, which we call Value-Guided PolicySteering (V-GPS), is compatible with a wide range of different generalistpolicies, without needing to fine-tune or even access the weights of thepolicy. We show that the same value function can improve the performance offive different state-of-the-art policies with different architectures, eventhough they were trained on distinct datasets, attaining consistent performanceimprovement on multiple robotic platforms across a total of 12 tasks. Code andvideos can be found at: https://nakamotoo.github.io/V-GPS</description><author>Mitsuhiko Nakamoto, Oier Mees, Aviral Kumar, Sergey Levine</author><pubDate>Thu, 17 Oct 2024 17:46:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13816v1</guid></item><item><title>Many-Shot In-Context Learning</title><link>http://arxiv.org/abs/2404.11018v3</link><description>Large language models (LLMs) excel at few-shot in-context learning (ICL) --learning from a few examples provided in context at inference, without anyweight updates. Newly expanded context windows allow us to investigate ICL withhundreds or thousands of examples -- the many-shot regime. Going from few-shotto many-shot, we observe significant performance gains across a wide variety ofgenerative and discriminative tasks. While promising, many-shot ICL can bebottlenecked by the available amount of human-generated examples. To mitigatethis limitation, we explore two new settings: Reinforced and Unsupervised ICL.Reinforced ICL uses model-generated chain-of-thought rationales in place ofhuman examples. Unsupervised ICL removes rationales from the prompt altogether,and prompts the model only with domain-specific questions. We find that bothReinforced and Unsupervised ICL can be quite effective in the many-shot regime,particularly on complex reasoning tasks. Finally, we demonstrate that, unlikefew-shot learning, many-shot learning is effective at overriding pretrainingbiases, can learn high-dimensional functions with numerical inputs, andperforms comparably to fine-tuning. We also find that inference cost increaseslinearly in the many-shot regime, and frontier LLMs benefit from many-shot ICLto varying degrees. Our analysis also reveals the limitations of next-tokenprediction loss as an indicator of downstream ICL performance.</description><author>Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, Hugo Larochelle</author><pubDate>Thu, 17 Oct 2024 17:45:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11018v3</guid></item><item><title>Private Counterfactual Retrieval</title><link>http://arxiv.org/abs/2410.13812v1</link><description>Transparency and explainability are two extremely important aspects to beconsidered when employing black-box machine learning models in high-stakeapplications. Providing counterfactual explanations is one way of catering thisrequirement. However, this also poses a threat to the privacy of both theinstitution that is providing the explanation as well as the user who isrequesting it. In this work, we propose multiple schemes inspired by privateinformation retrieval (PIR) techniques which ensure the \emph{user's privacy}when retrieving counterfactual explanations. We present a scheme whichretrieves the \emph{exact} nearest neighbor counterfactual explanation from adatabase of accepted points while achieving perfect (information-theoretic)privacy for the user. While the scheme achieves perfect privacy for the user,some leakage on the database is inevitable which we quantify using a mutualinformation based metric. Furthermore, we propose strategies to reduce thisleakage to achieve an advanced degree of database privacy. We extend theseschemes to incorporate user's preference on transforming their attributes, sothat a more actionable explanation can be received. Since our schemes rely onfinite field arithmetic, we empirically validate our schemes on real datasetsto understand the trade-off between the accuracy and the finite field sizes.</description><author>Mohamed Nomeir, Pasan Dissanayake, Shreya Meel, Sanghamitra Dutta, Sennur Ulukus</author><pubDate>Thu, 17 Oct 2024 17:45:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13812v1</guid></item><item><title>De-mark: Watermark Removal in Large Language Models</title><link>http://arxiv.org/abs/2410.13808v1</link><description>Watermarking techniques offer a promising way to identify machine-generatedcontent via embedding covert information into the contents generated fromlanguage models (LMs). However, the robustness of the watermarking schemes hasnot been well explored. In this paper, we present De-mark, an advancedframework designed to remove n-gram-based watermarks effectively. Our methodutilizes a novel querying strategy, termed random selection probing, which aidsin assessing the strength of the watermark and identifying the red-green listwithin the n-gram watermark. Experiments on popular LMs, such as Llama3 andChatGPT, demonstrate the efficiency and effectiveness of De-mark in watermarkremoval and exploitation tasks.</description><author>Ruibo Chen, Yihan Wu, Junfeng Guo, Heng Huang</author><pubDate>Thu, 17 Oct 2024 17:42:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13808v1</guid></item><item><title>ConsisSR: Delving Deep into Consistency in Diffusion-based Image Super-Resolution</title><link>http://arxiv.org/abs/2410.13807v1</link><description>Real-world image super-resolution (Real-ISR) aims at restoring high-quality(HQ) images from low-quality (LQ) inputs corrupted by unknown and complexdegradations. In particular, pretrained text-to-image (T2I) diffusion modelsprovide strong generative priors to reconstruct credible and intricate details.However, T2I generation focuses on semantic consistency while Real-ISRemphasizes pixel-level reconstruction, which hinders existing methods fromfully exploiting diffusion priors. To address this challenge, we introduceConsisSR to handle both semantic and pixel-level consistency. Specifically,compared to coarse-grained text prompts, we exploit the more powerful CLIPimage embedding and effectively leverage both modalities through our HybridPrompt Adapter (HPA) for semantic guidance. Secondly, we introduce Time-awareLatent Augmentation (TALA) to mitigate the inherent gap between T2I generationand Real-ISR consistency requirements. By randomly mixing LQ and HQ latentinputs, our model not only handle timestep-specific diffusion noise but alsorefine the accumulated latent representations. Last but not least, ourGAN-Embedding strategy employs the pretrained Real-ESRGAN model to refine thediffusion start point. This accelerates the inference process to 10 steps whilepreserving sampling quality, in a training-free manner.Our method demonstratesstate-of-the-art performance among both full-scale and accelerated models. Thecode will be made publicly available.</description><author>Junhao Gu, Peng-Tao Jiang, Hao Zhang, Mi Zhou, Jinwei Chen, Wenming Yang, Bo Li</author><pubDate>Thu, 17 Oct 2024 17:41:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13807v1</guid></item><item><title>A Watermark for Order-Agnostic Language Models</title><link>http://arxiv.org/abs/2410.13805v1</link><description>Statistical watermarking techniques are well-established for sequentiallydecoded language models (LMs). However, these techniques cannot be directlyapplied to order-agnostic LMs, as the tokens in order-agnostic LMs are notgenerated sequentially. In this work, we introduce Pattern-mark, apattern-based watermarking framework specifically designed for order-agnosticLMs. We develop a Markov-chain-based watermark generator that produceswatermark key sequences with high-frequency key patterns. Correspondingly, wepropose a statistical pattern-based detection algorithm that recovers the keysequence during detection and conducts statistical tests based on the count ofhigh-frequency patterns. Our extensive evaluations on order-agnostic LMs, suchas ProteinMPNN and CMLM, demonstrate Pattern-mark's enhanced detectionefficiency, generation quality, and robustness, positioning it as a superiorwatermarking technique for order-agnostic LMs.</description><author>Ruibo Chen, Yihan Wu, Yanshuo Chen, Chenxi Liu, Junfeng Guo, Heng Huang</author><pubDate>Thu, 17 Oct 2024 17:41:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13805v1</guid></item><item><title>BenTo: Benchmark Task Reduction with In-Context Transferability</title><link>http://arxiv.org/abs/2410.13804v1</link><description>Evaluating large language models (LLMs) is costly: it requires the generationand examination of LLM outputs on a large-scale benchmark of various tasks.This paper investigates how to efficiently reduce the tasks used to benchmarkLLMs without affecting the evaluation quality. Our study reveals that tasktransferability and relevance provide critical information to identify the mostrepresentative subset of tasks via optimizing a facility location function. Wepropose a practically efficient metric for estimating the transferabilitybetween two tasks via in-context learning (ICL). By analyzing the pairwisetransferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU orFLAN) to 5% while inducing only a &lt;4% difference to the evaluation on theoriginal benchmark. Compared to prior works, our method is training-free,gradient-free, and highly efficient requiring ICL only.</description><author>Hongyu Zhao, Ming Li, Lichao Sun, Tianyi Zhou</author><pubDate>Thu, 17 Oct 2024 17:41:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13804v1</guid></item><item><title>A Pattern to Align Them All: Integrating Different Modalities to Define Multi-Modal Entities</title><link>http://arxiv.org/abs/2410.13803v1</link><description>The ability to reason with and integrate different sensory inputs is thefoundation underpinning human intelligence and it is the reason for the growinginterest in modelling multi-modal information within Knowledge Graphs.Multi-Modal Knowledge Graphs extend traditional Knowledge Graphs by associatingan entity with its possible modal representations, including text, images,audio, and videos, all of which are used to convey the semantics of the entity.Despite the increasing attention that Multi-Modal Knowledge Graphs havereceived, there is a lack of consensus about the definitions and modelling ofmodalities, whose definition is often determined by application domains. Inthis paper, we propose a novel ontology design pattern that captures theseparation of concerns between an entity (and the information it conveys),whose semantics can have different manifestations across different media, andits realisation in terms of a physical information entity. By introducing thisabstract model, we aim to facilitate the harmonisation and integration ofdifferent existing multi-modal ontologies which is crucial for many intelligentapplications across different domains spanning from medicine to digitalhumanities.</description><author>Gianluca Apriceno, Valentina Tamma, Tania Bailoni, Jacopo de Berardinis, Mauro Dragoni</author><pubDate>Thu, 17 Oct 2024 17:41:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13803v1</guid></item><item><title>Data-Driven Estimation of Heterogeneous Treatment Effects</title><link>http://arxiv.org/abs/2301.06615v2</link><description>Estimating how a treatment affects different individuals, known asheterogeneous treatment effect estimation, is an important problem in empiricalsciences. In the last few years, there has been a considerable interest inadapting machine learning algorithms to the problem of estimating heterogeneouseffects from observational and experimental data. However, these algorithmsoften make strong assumptions about the observed features in the data andignore the structure of the underlying causal model, which can lead to biasedestimation. At the same time, the underlying causal mechanism is rarely knownin real-world datasets, making it hard to take it into consideration. In thiswork, we provide a survey of state-of-the-art data-driven methods forheterogeneous treatment effect estimation using machine learning, broadlycategorizing them as methods that focus on counterfactual prediction andmethods that directly estimate the causal effect. We also provide an overviewof a third category of methods which rely on structural causal models and learnthe model structure from data. Our empirical evaluation under variousunderlying structural model mechanisms shows the advantages and deficiencies ofexisting estimators and of the metrics for measuring their performance.</description><author>Christopher Tran, Keith Burghardt, Kristina Lerman, Elena Zheleva</author><pubDate>Thu, 17 Oct 2024 17:40:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.06615v2</guid></item><item><title>Adversarial Testing as a Tool for Interpretability: Length-based Overfitting of Elementary Functions in Transformers</title><link>http://arxiv.org/abs/2410.13802v1</link><description>The Transformer model has a tendency to overfit various aspects of thetraining data, such as the overall sequence length. We study elementary stringedit functions using a defined set of error indicators to interpret thebehaviour of the sequence-to-sequence Transformer. We show that generalizationto shorter sequences is often possible, but confirm that longer sequences arehighly problematic, although partially correct answers are often obtained.Additionally, we find that other structural characteristics of the sequences,such as subsegment length, may be equally important. We hypothesize that themodels learn algorithmic aspects of the tasks simultaneously with structuralaspects but adhering to the structural aspects is unfortunately often preferredby Transformer when they come into conflict.</description><author>Patrik Zavoral, Du≈°an Vari≈°, Ond≈ôej Bojar</author><pubDate>Thu, 17 Oct 2024 17:39:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13802v1</guid></item><item><title>Discrete distributions are learnable from metastable samples</title><link>http://arxiv.org/abs/2410.13800v1</link><description>Markov chain samplers designed to sample from multi-variable distributionsoften undesirably get stuck in specific regions of their state space. Thiscauses such samplers to approximately sample from a metastable distributionwhich is usually quite different from the desired, stationary distribution ofthe chain. We show that single-variable conditionals of metastabledistributions of reversible Markov chain samplers that satisfy a strongmetastability condition are on average very close to those of the truedistribution. This holds even when the metastable distribution is far away fromthe true model in terms of global metrics like Kullback-Leibler divergence ortotal variation distance. This property allows us to learn the true model usinga conditional likelihood based estimator, even when the samples come from ametastable distribution concentrated in a small region of the state space.Explicit examples of such metastable states can be constructed from regionsthat effectively bottleneck the probability flow and cause poor mixing of theMarkov chain. For specific cases of binary pairwise undirected graphicalmodels, we extend our results to further rigorously show that data coming frommetastable states can be used to learn the parameters of the energy functionand recover the structure of the model.</description><author>Abhijith Jayakumar, Andrey Y. Lokhov, Sidhant Misra, Marc Vuffray</author><pubDate>Thu, 17 Oct 2024 17:38:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13800v1</guid></item><item><title>Machine-Learning Analysis of Radiative Decays to Dark Matter at the LHC</title><link>http://arxiv.org/abs/2410.13799v1</link><description>The search for weakly interacting matter particles (WIMPs) is one of the mainobjectives of the High Luminosity Large Hadron Collider (HL-LHC). In this workwe use Machine Learning (ML) techniques to explore WIMP radiative decays into aDark Matter (DM) candidate in a supersymmetric framework. The minimalsupersymmetric WIMP sector includes the lightest neutralino that can providethe observed DM relic density through its co-annihilation with the secondlightest neutralino and lightest chargino. Moreover, the direct DM detectioncross section rates fulfill current experimental bounds and provide discoverytargets for the same region of model parameters in which the radiative decay ofthe second lightest neutralino into a photon and the lightest neutralino isenhanced. This strongly motivates the search for radiatively decayingneutralinos which, however, suffers from strong backgrounds. We investigate theLHC reach in the search for these radiatively decaying particles by means ofcut-based and ML methods and estimate its discovery potential in thiswell-motivated, new physics scenario.</description><author>Ernesto Arganda, Marcela Carena, Mart√≠n de los Rios, Andres D. Perez, Duncan Rocha, Rosa M. Sand√° Seoane, Carlos E. M. Wagner</author><pubDate>Thu, 17 Oct 2024 17:38:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13799v1</guid></item><item><title>Learning Graph Quantized Tokenizers for Transformers</title><link>http://arxiv.org/abs/2410.13798v1</link><description>Transformers serve as the backbone architectures of Foundational Models,where a domain-specific tokenizer helps them adapt to various domains. GraphTransformers (GTs) have recently emerged as a leading model in geometric deeplearning, outperforming Graph Neural Networks (GNNs) in various graph learningtasks. However, the development of tokenizers for graphs has lagged behindother modalities, with existing approaches relying on heuristics or GNNsco-trained with Transformers. To address this, we introduce GQT (\textbf{G}raph\textbf{Q}uantized \textbf{T}okenizer), which decouples tokenizer training fromTransformer training by leveraging multi-task graph self-supervised learning,yielding robust and generalizable graph tokens. Furthermore, the GQT utilizesResidual Vector Quantization (RVQ) to learn hierarchical discrete tokens,resulting in significantly reduced memory requirements and improvedgeneralization capabilities. By combining the GQT with token modulation, aTransformer encoder achieves state-of-the-art performance on 16 out of 18benchmarks, including large-scale homophilic and heterophilic datasets. Thecode is available at: https://github.com/limei0307/graph-tokenizer</description><author>Limei Wang, Kaveh Hassani, Si Zhang, Dongqi Fu, Baichuan Yuan, Weilin Cong, Zhigang Hua, Hao Wu, Ning Yao, Bo Long</author><pubDate>Thu, 17 Oct 2024 17:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13798v1</guid></item><item><title>Dynamic Topic Language Model on Heterogeneous Children's Mental Health Clinical Notes</title><link>http://arxiv.org/abs/2312.14180v2</link><description>Mental health diseases affect children's lives and well-beings which havereceived increased attention since the COVID-19 pandemic. Analyzing psychiatricclinical notes with topic models is critical to evaluating children's mentalstatus over time. However, few topic models are built for longitudinalsettings, and most existing approaches fail to capture temporal trajectoriesfor each document. To address these challenges, we develop a dynamic topicmodel with consistent topics and individualized temporal dependencies on theevolving document metadata. Our model preserves the semantic meaning ofdiscovered topics over time and incorporates heterogeneity among documents. Inparticular, when documents can be categorized, we propose a classifier-freeapproach to maximize topic heterogeneity across different document groups. Wealso present an efficient variational optimization procedure adapted for themultistage longitudinal setting. In this case study, we apply our method to thepsychiatric clinical notes from a large tertiary pediatric hospital in SouthernCalifornia and achieve a 38% increase in the overall coherence of extractedtopics. Our real data analysis reveals that children tend to express morenegative emotions during state shutdowns and more positive when schools reopen.Furthermore, it suggests that sexual and gender minority (SGM) children displaymore pronounced reactions to major COVID-19 events and a greater sensitivity tovaccine-related news than non-SGM children. This study examines children'smental health progression during the pandemic and offers clinicians valuableinsights to recognize disparities in children's mental health related to theirsexual and gender identities.</description><author>Hanwen Ye, Tatiana Moreno, Adrianne Alpern, Louis Ehwerhemuepha, Annie Qu</author><pubDate>Thu, 17 Oct 2024 17:38:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14180v2</guid></item><item><title>Arbitrarily-Conditioned Multi-Functional Diffusion for Multi-Physics Emulation</title><link>http://arxiv.org/abs/2410.13794v1</link><description>Modern physics simulation often involves multiple functions of interests, andtraditional numerical approaches are known to be complex and computationallycostly. While machine learning-based surrogate models can offer significantcost reductions, most focus on a single task, such as forward prediction, andtypically lack uncertainty quantification -- an essential component in manyapplications. To overcome these limitations, we propose Arbitrarily-ConditionedMulti-Functional Diffusion (ACMFD), a versatile probabilistic surrogate modelfor multi-physics emulation. ACMFD can perform a wide range of tasks within asingle framework, including forward prediction, various inverse problems, andsimulating data for entire systems or subsets of quantities conditioned onothers. Specifically, we extend the standard Denoising Diffusion ProbabilisticModel (DDPM) for multi-functional generation by modeling noise as Gaussianprocesses (GP). We then introduce an innovative denoising loss. The traininginvolves randomly sampling the conditioned part and fitting the correspondingpredicted noise to zero, enabling ACMFD to flexibly generate function valuesconditioned on any other functions or quantities. To enable efficient trainingand sampling, and to flexibly handle irregularly sampled data, we use GPs tointerpolate function samples onto a grid, inducing a Kronecker productstructure for efficient computation. We demonstrate the advantages of ACMFDacross several fundamental multi-physics systems.</description><author>Da Long, Zhitong Xu, Guang Yang, Akil Narayan, Shandian Zhe</author><pubDate>Thu, 17 Oct 2024 17:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13794v1</guid></item><item><title>Analyzing Deep Transformer Models for Time Series Forecasting via Manifold Learning</title><link>http://arxiv.org/abs/2410.13792v1</link><description>Transformer models have consistently achieved remarkable results in variousdomains such as natural language processing and computer vision. However,despite ongoing research efforts to better understand these models, the fieldstill lacks a comprehensive understanding. This is particularly true for deeptime series forecasting methods, where analysis and understanding work isrelatively limited. Time series data, unlike image and text information, can bemore challenging to interpret and analyze. To address this, we approach theproblem from a manifold learning perspective, assuming that the latentrepresentations of time series forecasting models lie next to a low-dimensionalmanifold. In our study, we focus on analyzing the geometric features of theselatent data manifolds, including intrinsic dimension and principal curvatures.Our findings reveal that deep transformer models exhibit similar geometricbehavior across layers, and these geometric features are correlated with modelperformance. Additionally, we observe that untrained models initially havedifferent structures, but they rapidly converge during training. By leveragingour geometric analysis and differentiable tools, we can potentially design newand improved deep forecasting neural networks. This approach complementsexisting analysis studies and contributes to a better understanding oftransformer models in the context of time series forecasting. Code is releasedat https://github.com/azencot-group/GATLM.</description><author>Ilya Kaufman, Omri Azencot</author><pubDate>Thu, 17 Oct 2024 17:32:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13792v1</guid></item><item><title>MotionBank: A Large-scale Video Motion Benchmark with Disentangled Rule-based Annotations</title><link>http://arxiv.org/abs/2410.13790v1</link><description>In this paper, we tackle the problem of how to build and benchmark a largemotion model (LMM). The ultimate goal of LMM is to serve as a foundation modelfor versatile motion-related tasks, e.g., human motion generation, withinterpretability and generalizability. Though advanced, recent LMM-relatedworks are still limited by small-scale motion data and costly textdescriptions. Besides, previous motion benchmarks primarily focus on pure bodymovements, neglecting the ubiquitous motions in context, i.e., humansinteracting with humans, objects, and scenes. To address these limitations, weconsolidate large-scale video action datasets as knowledge banks to buildMotionBank, which comprises 13 video action datasets, 1.24M motion sequences,and 132.9M frames of natural and diverse human motions. Different fromlaboratory-captured motions, in-the-wild human-centric videos contain abundantmotions in context. To facilitate better motion text alignment, we alsometiculously devise a motion caption generation algorithm to automaticallyproduce rule-based, unbiased, and disentangled text descriptions via thekinematic characteristics for each motion. Extensive experiments show that ourMotionBank is beneficial for general motion-related tasks of human motiongeneration, motion in-context generation, and motion understanding. Videomotions together with the rule-based text annotations could serve as anefficient alternative for larger LMMs. Our dataset, codes, and benchmark willbe publicly available at https://github.com/liangxuy/MotionBank.</description><author>Liang Xu, Shaoyang Hua, Zili Lin, Yifan Liu, Feipeng Ma, Yichao Yan, Xin Jin, Xiaokang Yang, Wenjun Zeng</author><pubDate>Thu, 17 Oct 2024 17:31:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13790v1</guid></item><item><title>The Impact of Visual Information in Chinese Characters: Evaluating Large Models' Ability to Recognize and Utilize Radicals</title><link>http://arxiv.org/abs/2410.09013v2</link><description>The glyphic writing system of Chinese incorporates information-rich visualfeatures in each character, such as radicals that provide hints about meaningor pronunciation. However, there has been no investigation into whethercontemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) canharness these sub-character features in Chinese through prompting. In thisstudy, we establish a benchmark to evaluate LLMs' and VLMs' understanding ofvisual elements in Chinese characters, including radicals, compositionstructures, strokes, and stroke counts. Our results reveal that modelssurprisingly exhibit some, but still limited, knowledge of the visualinformation, regardless of whether images of characters are provided. To incitemodels' ability to use radicals, we further experiment with incorporatingradicals into the prompts for Chinese language processing (CLP) tasks. Weobserve consistent improvement in Part-Of-Speech tagging when providingadditional information about radicals, suggesting the potential to enhance CLPby integrating sub-character information.</description><author>Xiaofeng Wu, Karl Stratos, Wei Xu</author><pubDate>Thu, 17 Oct 2024 17:30:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09013v2</guid></item><item><title>Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions</title><link>http://arxiv.org/abs/2410.13788v1</link><description>Large language models (LLMs) must often respond to highly ambiguous userrequests. In such cases, the LLM's best response may be to ask a clarifyingquestion to elicit more information. We observe existing LLMs often respond bypresupposing a single interpretation of such ambiguous requests, frustratingusers who intended a different interpretation. We speculate this is caused bycurrent preference data labeling practice, where LLM responses are evaluatedonly on their prior contexts. To address this, we propose to assign preferencelabels by simulating their expected outcomes in the future turns. This allowsLLMs to learn to ask clarifying questions when it can generate responses thatare tailored to each user interpretation in future turns. In experiments onopen-domain QA, we compare systems that trained using our proposed preferencelabeling methods against standard methods, which assign preferences based ononly prior context. We evaluate systems based on their ability to askclarifying questions that can recover each user's interpretation and expectedanswer, and find that our training with our proposed method trains LLMs to askclarifying questions with a 5% improvement in F1 measured against the answerset from different interpretations of each query</description><author>Michael J. Q. Zhang, W. Bradley Knox, Eunsol Choi</author><pubDate>Thu, 17 Oct 2024 17:29:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13788v1</guid></item><item><title>Achieving Exponential Asymptotic Optimality in Average-Reward Restless Bandits without Global Attractor Assumption</title><link>http://arxiv.org/abs/2405.17882v2</link><description>We consider the infinite-horizon average-reward restless bandit problem. Wepropose a novel \emph{two-set policy} that maintains two dynamic subsets ofarms: one subset of arms has a nearly optimal state distribution and takesactions according to an Optimal Local Control routine; the other subset of armsis driven towards the optimal state distribution and gradually merged into thefirst subset. We show that our two-set policy is asymptotically optimal with an$O(\exp(-C N))$ optimality gap for an $N$-armed problem, under the mildassumptions of aperiodic-unichain, non-degeneracy, and local stability. Ourpolicy is the first to achieve \emph{exponential asymptotic optimality} underthe above set of easy-to-verify assumptions, whereas prior work either requiresa strong \emph{global attractor} assumption or only achieves an $O(1/\sqrt{N})$optimality gap. We further discuss obstacles in weakening the assumptions bydemonstrating examples where exponential asymptotic optimality is notachievable when any of the three assumptions is violated. Notably, we prove alower bound for a large class of locally unstable restless bandits, showingthat local stability is particularly fundamental for exponential asymptoticoptimality. Finally, we use simulations to demonstrate that the two-set policyoutperforms previous policies on certain RB problems and performs competitivelyoverall.</description><author>Yige Hong, Qiaomin Xie, Yudong Chen, Weina Wang</author><pubDate>Thu, 17 Oct 2024 17:28:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17882v2</guid></item><item><title>Looking Inward: Language Models Can Learn About Themselves by Introspection</title><link>http://arxiv.org/abs/2410.13787v1</link><description>Humans acquire knowledge by observing the external world, but also byintrospection. Introspection gives a person privileged access to their currentstate of mind (e.g., thoughts and feelings) that is not accessible to externalobservers. Can LLMs introspect? We define introspection as acquiring knowledgethat is not contained in or derived from training data but instead originatesfrom internal states. Such a capability could enhance model interpretability.Instead of painstakingly analyzing a model's internal workings, we could simplyask the model about its beliefs, world models, and goals. More speculatively,an introspective model might self-report on whether it possesses certaininternal states such as subjective feelings or desires and this could inform usabout the moral status of these states. Such self-reports would not be entirelydictated by the model's training data. We study introspection by finetuning LLMs to predict properties of their ownbehavior in hypothetical scenarios. For example, "Given the input P, would youroutput favor the short- or long-term option?" If a model M1 can introspect, itshould outperform a different model M2 in predicting M1's behavior even if M2is trained on M1's ground-truth behavior. The idea is that M1 has privilegedaccess to its own behavioral tendencies, and this enables it to predict itselfbetter than M2 (even if M2 is generally stronger). In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned topredict itself), we find that the model M1 outperforms M2 in predicting itself,providing evidence for introspection. Notably, M1 continues to predict itsbehavior accurately even after we intentionally modify its ground-truthbehavior. However, while we successfully elicit introspection on simple tasks,we are unsuccessful on more complex tasks or those requiringout-of-distribution generalization.</description><author>Felix J Binder, James Chua, Tomek Korbak, Henry Sleight, John Hughes, Robert Long, Ethan Perez, Miles Turpin, Owain Evans</author><pubDate>Thu, 17 Oct 2024 17:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13787v1</guid></item><item><title>Emphasizing Semantic Consistency of Salient Posture for Speech-Driven Gesture Generation</title><link>http://arxiv.org/abs/2410.13786v1</link><description>Speech-driven gesture generation aims at synthesizing a gesture sequencesynchronized with the input speech signal. Previous methods leverage neuralnetworks to directly map a compact audio representation to the gesturesequence, ignoring the semantic association of different modalities and failingto deal with salient gestures. In this paper, we propose a novel speech-drivengesture generation method by emphasizing the semantic consistency of salientposture. Specifically, we first learn a joint manifold space for the individualrepresentation of audio and body pose to exploit the inherent semanticassociation between two modalities, and propose to enforce semantic consistencyvia a consistency loss. Furthermore, we emphasize the semantic consistency ofsalient postures by introducing a weakly-supervised detector to identifysalient postures, and reweighting the consistency loss to focus more onlearning the correspondence between salient postures and the high-levelsemantics of speech content. In addition, we propose to extract audio featuresdedicated to facial expression and body gesture separately, and design separatebranches for face and body gesture synthesis. Extensive experimental resultsdemonstrate the superiority of our method over the state-of-the-art approaches.</description><author>Fengqi Liu, Hexiang Wang, Jingyu Gong, Ran Yi, Qianyu Zhou, Xuequan Lu, Jiangbo Lu, Lizhuang Ma</author><pubDate>Thu, 17 Oct 2024 17:22:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13786v1</guid></item><item><title>PopAlign: Diversifying Contrasting Patterns for a More Comprehensive Alignment</title><link>http://arxiv.org/abs/2410.13785v1</link><description>Alignment of large language models (LLMs) involves training models onpreference-contrastive output pairs to adjust their responses according tohuman preferences. To obtain such contrastive pairs, traditional methods likeRLHF and RLAIF rely on limited contrasting patterns, such as varying modelvariants or decoding temperatures. This singularity leads to two issues: (1)alignment is not comprehensive; and thereby (2) models are susceptible tojailbreaking attacks. To address these issues, we investigate how to constructmore comprehensive and diversified contrasting patterns to enhance preferencedata (RQ1) and verify the impact of the diversification of contrasting patternson model alignment (RQ2). For RQ1, we propose PopAlign, a framework thatintegrates diversified contrasting patterns across the prompt, model, andpipeline levels, introducing six contrasting strategies that do not requireadditional feedback labeling procedures. Regarding RQ2, we conduct thoroughexperiments demonstrating that PopAlign significantly outperforms existingmethods, leading to more comprehensive alignment.</description><author>Zekun Moore Wang, Shawn Wang, Kang Zhu, Jiaheng Liu, Ke Xu, Jie Fu, Wangchunshu Zhou, Wenhao Huang</author><pubDate>Thu, 17 Oct 2024 17:22:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13785v1</guid></item><item><title>Quantity vs. Quality of Monolingual Source Data in Automatic Text Translation: Can It Be Too Little If It Is Too Good?</title><link>http://arxiv.org/abs/2410.13783v1</link><description>Monolingual data, being readily available in large quantities, has been usedto upscale the scarcely available parallel data to train better models forautomatic translation. Self-learning, where a model is made to learn from itsoutput, is one approach to exploit such data. However, it has been shown thattoo much of this data can be detrimental to the performance of the model if theavailable parallel data is comparatively extremely low. In this study, weinvestigate whether the monolingual data can also be too little and if thisreduction, based on quality, has any effect on the performance of thetranslation model. Experiments have shown that on English-German low-resourceNMT, it is often better to select only the most useful additional data, basedon quality or closeness to the domain of the test data, than utilizing all ofthe available data.</description><author>Idris Abdulmumin, Bashir Shehu Galadanci, Garba Aliyu, Shamsuddeen Hassan Muhammad</author><pubDate>Thu, 17 Oct 2024 17:20:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13783v1</guid></item><item><title>DPLM-2: A Multimodal Diffusion Protein Language Model</title><link>http://arxiv.org/abs/2410.13782v1</link><description>Proteins are essential macromolecules defined by their amino acid sequences,which determine their three-dimensional structures and, consequently, theirfunctions in all living organisms. Therefore, generative protein modelingnecessitates a multimodal approach to simultaneously model, understand, andgenerate both sequences and structures. However, existing methods typically useseparate models for each modality, limiting their ability to capture theintricate relationships between sequence and structure. This results insuboptimal performance in tasks that requires joint understanding andgeneration of both modalities. In this paper, we introduce DPLM-2, a multimodalprotein foundation model that extends discrete diffusion protein language model(DPLM) to accommodate both sequences and structures. To enable structurallearning with the language model, 3D coordinates are converted to discretetokens using a lookup-free quantization-based tokenizer. By training on bothexperimental and high-quality synthetic structures, DPLM-2 learns the jointdistribution of sequence and structure, as well as their marginals andconditionals. We also implement an efficient warm-up strategy to exploit theconnection between large-scale evolutionary data and structural inductivebiases from pre-trained sequence-based protein language models. Empiricalevaluation shows that DPLM-2 can simultaneously generate highly compatibleamino acid sequences and their corresponding 3D structures eliminating the needfor a two-stage generation approach. Moreover, DPLM-2 demonstrates competitiveperformance in various conditional generation tasks, including folding, inversefolding, and scaffolding with multimodal motif inputs, as well as providingstructure-aware representations for predictive tasks.</description><author>Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, Quanquan Gu</author><pubDate>Thu, 17 Oct 2024 17:20:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13782v1</guid></item><item><title>Optimal Quantization for Matrix Multiplication</title><link>http://arxiv.org/abs/2410.13780v1</link><description>Recent work in machine learning community proposed multiple methods forperforming lossy compression (quantization) of large matrices. Thisquantization is important for accelerating matrix multiplication (maincomponent of large language models), which is often bottlenecked by the speedof loading these matrices from memory. Unlike classical vector quantization andrate-distortion theory, the goal of these new compression algorithms is to beable to approximate not the matrices themselves, but their matrix product.Specifically, given a pair of real matrices $A,B$ an encoder (compressor) isapplied to each of them independently producing descriptions with $R$ bits perentry. These representations subsequently are used by the decoder to estimatematrix product $A^\top B$. In this work, we provide a non-asymptotic lowerbound on the mean squared error of this approximation (as a function of rate$R$) for the case of matrices $A,B$ with iid Gaussian entries. Algorithmically,we construct a universal quantizer based on nested lattices with an explicitguarantee of approximation error for any (non-random) pair of matrices $A$, $B$in terms of only Frobenius norms $\|A\|_F, \|B\|_F$ and $\|A^\top B\|_F$. Foriid Gaussian matrices our quantizer achieves the lower bound and is, thus,asymptotically optimal. A practical low-complexity version of our quantizerachieves performance quite close to optimal. In information-theoretic terms wederive rate-distortion function for matrix multiplication of iid Gaussianmatrices.</description><author>Or Ordentlich, Yury Polyanskiy</author><pubDate>Thu, 17 Oct 2024 17:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13780v1</guid></item><item><title>Superlatives in Context: Modeling the Implicit Semantics of Superlatives</title><link>http://arxiv.org/abs/2405.20967v2</link><description>Superlatives are used to single out elements with a maximal/minimal property.Semantically, superlatives perform a set comparison: something (or some things)has the min/max property out of a set. As such, superlatives provide an idealphenomenon for studying implicit phenomena and discourse restrictions. Whilethis comparison set is often not explicitly defined, its (implicit)restrictions can be inferred from the discourse context the expression appearsin. In this work we provide an extensive computational study on the semanticsof superlatives. We propose a unified account of superlative semantics whichallows us to derive a broad-coverage annotation schema. Using this unifiedschema we annotated a multi-domain dataset of superlatives and their semanticinterpretations. We specifically focus on interpreting implicit or ambiguoussuperlative expressions, by analyzing how the discourse context restricts theset of interpretations. In a set of experiments we then analyze how well modelsperform at variations of predicting superlative semantics, with and withoutcontext. We show that the fine-grained semantics of superlatives in context canbe challenging for contemporary models, including GPT-4.</description><author>Valentina Pyatkin, Bonnie Webber, Ido Dagan, Reut Tsarfaty</author><pubDate>Thu, 17 Oct 2024 17:19:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20967v2</guid></item><item><title>The Mystery of the Pathological Path-star Task for Language Models</title><link>http://arxiv.org/abs/2410.13779v1</link><description>The recently introduced path-star task is a minimal task designed toexemplify limitations to the abilities of language models (Bachmann andNagarajan, 2024). It involves a path-star graph where multiple arms radiatefrom a single starting node and each node is unique. Given the start node and aspecified target node that ends an arm, the task is to generate the armcontaining that target node. This is straightforward for a human butsurprisingly difficult for language models, which did not outperform the randombaseline. The authors hypothesized this is due to a deficiency inteacher-forcing and the next-token prediction paradigm. We demonstrate the task is learnable using teacher-forcing in alternativesettings and that the issue is partially due to representation. We introduce aregularization method using structured samples of the same graph but withdiffering target nodes, improving results across a variety of model types. Weprovide RASP proofs showing the task is theoretically solvable. Finally, wefind settings where an encoder-only model can consistently solve the task.</description><author>Arvid Frydenlund</author><pubDate>Thu, 17 Oct 2024 17:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13779v1</guid></item><item><title>Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree</title><link>http://arxiv.org/abs/2410.13778v1</link><description>We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),a non-parametric change-detection algorithm that combines the Kernel-QuantTree(KQT) histogram and the EWMA statistic to monitor multivariate data streamsonline. The resulting monitoring scheme is very flexible, since histograms canbe used to model any stationary distribution, and practical, since thedistribution of test statistics does not depend on the distribution ofdatastream in stationary conditions (non-parametric monitoring). KQT-EWMAenables controlling false alarms by operating at a pre-determined Average RunLength ($ARL_0$), which measures the expected number of stationary samples tobe monitored before triggering a false alarm. The latter peculiarity is incontrast with most non-parametric change-detection tests, which rarely cancontrol the $ARL_0$ a priori. Our experiments on synthetic and real-worlddatasets demonstrate that KQT-EWMA can control $ARL_0$ while achievingdetection delays comparable to or lower than state-of-the-art methods designedto work in the same conditions.</description><author>Michelangelo Olmo Nogara Notarianni, Filippo Leveni, Diego Stucchi, Luca Frittoli, Giacomo Boracchi</author><pubDate>Thu, 17 Oct 2024 17:17:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13778v1</guid></item><item><title>Representing Model Weights with Language using Tree Experts</title><link>http://arxiv.org/abs/2410.13569v1</link><description>The increasing availability of public models begs the question: can we trainneural networks that use other networks as input? This paper learns torepresent models within a joint space that embeds both model weights andlanguage. However, machine learning on model weights is challenging as modelweights often exhibit significant variation unrelated to the models' semanticproperties (nuisance variation). We identify a key property of real-worldmodels: most public models belong to a small set of Model Trees, where allmodels within a tree are fine-tuned from a common ancestor (e.g., a foundationmodel). Importantly, we find that within each tree there is less nuisancevariation between models. For example, while classifying models according totheir training dataset generally requires complex architectures, in our case,even a linear classifier trained on a single layer is often effective. Whileeffective, linear layers are computationally expensive as model weights arevery high dimensional. To address this, we introduce Probing Experts (ProbeX),a theoretically motivated, lightweight probing method. Notably, ProbeX is thefirst probing method designed to learn from the weights of just a single modellayer. We also construct and release a dataset that simulates the structure ofpublic model repositories. Our results show that ProbeX can effectively map theweights of large models into a shared weight-language embedding space.Furthermore, we demonstrate the impressive generalization of our method,achieving zero-shot model classification and retrieval.</description><author>Eliahu Horwitz, Bar Cavia, Jonathan Kahana, Yedid Hoshen</author><pubDate>Thu, 17 Oct 2024 17:17:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13569v1</guid></item><item><title>Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors</title><link>http://arxiv.org/abs/2410.13776v1</link><description>In-context Learning (ICL) has become the primary method for performingnatural language tasks with Large Language Models (LLMs). The knowledgeacquired during pre-training is crucial for this few-shot capability, providingthe model with task priors. However, recent studies have shown that ICLpredominantly relies on retrieving task priors rather than "learning" toperform tasks. This limitation is particularly evident in complex subjectivedomains such as emotion and morality, where priors significantly influenceposterior predictions. In this work, we examine whether this is the result ofthe aggregation used in corresponding datasets, where trying to combinelow-agreement, disparate annotations might lead to annotation artifacts thatcreate detrimental noise in the prompt. Moreover, we evaluate the posteriorbias towards certain annotators by grounding our study in appropriate,quantitative measures of LLM priors. Our results indicate that aggregation is aconfounding factor in the modeling of subjective tasks, and advocate focusingon modeling individuals instead. However, aggregation does not explain theentire gap between ICL and the state of the art, meaning other factors in suchtasks also account for the observed phenomena. Finally, by rigorously studyingannotator-level labels, we find that it is possible for minority annotators toboth better align with LLMs and have their perspectives further amplified.</description><author>Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth Narayanan</author><pubDate>Thu, 17 Oct 2024 17:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13776v1</guid></item><item><title>Enhancing Retail Sales Forecasting with Optimized Machine Learning Models</title><link>http://arxiv.org/abs/2410.13773v1</link><description>In retail sales forecasting, accurately predicting future sales is crucialfor inventory management and strategic planning. Traditional methods like LRoften fall short due to the complexity of sales data, which includesseasonality and numerous product families. Recent advancements in machinelearning (ML) provide more robust alternatives. This research benefits from thepower of ML, particularly Random Forest (RF), Gradient Boosting (GB), SupportVector Regression (SVR), and XGBoost, to improve prediction accuracy. Despiteadvancements, a significant gap exists in handling complex datasets with highseasonality and multiple product families. The proposed solution involvesimplementing and optimizing a RF model, leveraging hyperparameter tuningthrough randomized search cross-validation. This approach addresses thecomplexities of the dataset, capturing intricate patterns that traditionalmethods miss. The optimized RF model achieved an R-squared value of 0.945,substantially higher than the initial RF model and traditional LR, which had anR-squared of 0.531. The model reduced the root mean squared logarithmic error(RMSLE) to 1.172, demonstrating its superior predictive capability. Theoptimized RF model did better than cutting-edge models like Gradient Boosting(R-squared: 0.942), SVR (R-squared: 0.940), and XGBoost (R-squared: 0.939),with more minor mean squared error (MSE) and mean absolute error (MAE) numbers.The results demonstrate that the optimized RF model excels in forecastingretail sales, handling the datasets complexity with higher accuracy andreliability. This research highlights the importance of advanced ML techniquesin predictive analytics, offering a significant improvement over traditionalmethods and other contemporary models.</description><author>Priyam Ganguly, Isha Mukherjee</author><pubDate>Thu, 17 Oct 2024 17:11:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13773v1</guid></item><item><title>Is Prior-Free Black-Box Non-Stationary Reinforcement Learning Feasible?</title><link>http://arxiv.org/abs/2410.13772v1</link><description>We study the problem of Non-Stationary Reinforcement Learning (NS-RL) withoutprior knowledge about the system's non-stationarity. A state-of-the-art,black-box algorithm, known as MASTER, is considered, with a focus onidentifying the conditions under which it can achieve its stated goals.Specifically, we prove that MASTER's non-stationarity detection mechanism isnot triggered for practical choices of horizon, leading to performance akin toa random restarting algorithm. Moreover, we show that the regret bound forMASTER, while being order optimal, stays above the worst-case linear regretuntil unreasonably large values of the horizon. To validate these observations,MASTER is tested for the special case of piecewise stationary multi-armedbandits, along with methods that employ random restarting, and others that usequickest change detection to restart. A simple, order optimal random restartingalgorithm, that has prior knowledge of the non-stationarity is proposed as abaseline. The behavior of the MASTER algorithm is validated in simulations, andit is shown that methods employing quickest change detection are more robustand consistently outperform MASTER and other random restarting approaches.</description><author>Argyrios Gerogiannis, Yu-Han Huang, Venugopal V. Veeravalli</author><pubDate>Thu, 17 Oct 2024 17:09:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13772v1</guid></item><item><title>Probing the Latent Hierarchical Structure of Data via Diffusion Models</title><link>http://arxiv.org/abs/2410.13770v1</link><description>High-dimensional data must be highly structured to be learnable. Although thecompositional and hierarchical nature of data is often put forward to explainlearnability, quantitative measurements establishing these properties arescarce. Likewise, accessing the latent variables underlying such a datastructure remains a challenge. In this work, we show that forward-backwardexperiments in diffusion-based models, where data is noised and then denoisedto generate new samples, are a promising tool to probe the latent structure ofdata. We predict in simple hierarchical models that, in this process, changesin data occur by correlated chunks, with a length scale that diverges at anoise level where a phase transition is known to take place. Remarkably, weconfirm this prediction in both text and image datasets using state-of-the-artdiffusion models. Our results show how latent variable changes manifest in thedata and establish how to measure these effects in real data using diffusionmodels.</description><author>Antonio Sclocchi, Alessandro Favero, Noam Itzhak Levi, Matthieu Wyart</author><pubDate>Thu, 17 Oct 2024 17:08:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13770v1</guid></item><item><title>Transformer Guided Coevolution: Improved Team Formation in Multiagent Adversarial Games</title><link>http://arxiv.org/abs/2410.13769v1</link><description>We consider the problem of team formation within multiagent adversarialgames. We propose BERTeam, a novel algorithm that uses a transformer-based deepneural network with Masked Language Model training to select the best team ofplayers from a trained population. We integrate this with coevolutionary deepreinforcement learning, which trains a diverse set of individual players tochoose teams from. We test our algorithm in the multiagent adversarial gameMarine Capture-The-Flag, and we find that BERTeam learns non-trivial teamcompositions that perform well against unseen opponents. For this game, we findthat BERTeam outperforms MCAA, an algorithm that similarly optimizes teamformation.</description><author>Pranav Rajbhandari, Prithviraj Dasgupta, Donald Sofge</author><pubDate>Thu, 17 Oct 2024 17:06:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13769v1</guid></item><item><title>Rapid and Automated Alloy Design with Graph Neural Network-Powered LLM-Driven Multi-Agent Systems</title><link>http://arxiv.org/abs/2410.13768v1</link><description>A multi-agent AI model is used to automate the discovery of new metallicalloys, integrating multimodal data and external knowledge including insightsfrom physics via atomistic simulations. Our multi-agent system features threekey components: (a) a suite of LLMs responsible for tasks such as reasoning andplanning, (b) a group of AI agents with distinct roles and expertise thatdynamically collaborate, and (c) a newly developed graph neural network (GNN)model for rapid retrieval of key physical properties. A set of LLM-driven AIagents collaborate to automate the exploration of the vast design space ofMPEAs, guided by predictions from the GNN. We focus on the NbMoTa family ofbody-centered cubic (bcc) alloys, modeled using an ML-based interatomicpotential, and target two key properties: the Peierls barrier and solute/screwdislocation interaction energy. Our GNN model accurately predicts theseatomic-scale properties, providing a faster alternative to costly brute-forcecalculations and reducing the computational burden on multi-agent systems forphysics retrieval. This AI system revolutionizes materials discovery byreducing reliance on human expertise and overcoming the limitations of directall-atom simulations. By synergizing the predictive power of GNNs with thedynamic collaboration of LLM-based agents, the system autonomously navigatesvast alloy design spaces, identifying trends in atomic-scale materialproperties and predicting macro-scale mechanical strength, as demonstrated byseveral computational experiments. This approach accelerates the discovery ofadvanced alloys and holds promise for broader applications in other complexsystems, marking a significant step forward in automated materials design.</description><author>Alireza Ghafarollahi, Markus J. Buehler</author><pubDate>Thu, 17 Oct 2024 17:06:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13768v1</guid></item><item><title>Knowledge-Aware Query Expansion with Large Language Models for Textual and Relational Retrieval</title><link>http://arxiv.org/abs/2410.13765v1</link><description>Large language models (LLMs) have been used to generate query expansionsaugmenting original queries for improving information search. Recent studiesalso explore providing LLMs with initial retrieval results to generate queryexpansions more grounded to document corpus. However, these methods mostlyfocus on enhancing textual similarities between search queries and targetdocuments, overlooking document relations. For queries like "Find me a highlyrated camera for wildlife photography compatible with my Nikon F-Mount lenses",existing methods may generate expansions that are semantically similar butstructurally unrelated to user intents. To handle such semi-structured querieswith both textual and relational requirements, in this paper we propose aknowledge-aware query expansion framework, augmenting LLMs with structureddocument relations from knowledge graph (KG). To further address the limitationof entity-based scoring in existing KG-based methods, we leverage documenttexts as rich KG node representations and use document-based relation filteringfor our Knowledge-Aware Retrieval (KAR). Extensive experiments on threedatasets of diverse domains show the advantages of our method compared againststate-of-the-art baselines on textual and relational semi-structured retrieval.</description><author>Yu Xia, Junda Wu, Sungchul Kim, Tong Yu, Ryan A. Rossi, Haoliang Wang, Julian McAuley</author><pubDate>Thu, 17 Oct 2024 17:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13765v1</guid></item></channel></rss>