<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 14 Aug 2023 06:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Foundation Model is Efficient Multimodal Multitask Model Selector</title><link>http://arxiv.org/abs/2308.06262v1</link><description>This paper investigates an under-explored but important problem: given acollection of pre-trained neural networks, predicting their performance on eachmulti-modal task without fine-tuning them, such as image recognition,referring, captioning, visual question answering, and text question answering.A brute-force approach is to finetune all models on all target datasets,bringing high computational costs. Although recent-advanced approaches employedlightweight metrics to measure models' transferability,they often dependheavily on the prior knowledge of a single task, making them inapplicable in amulti-modal multi-task scenario. To tackle this issue, we propose an efficientmulti-task model selector (EMMS), which employs large-scale foundation modelsto transform diverse label formats such as categories, texts, and boundingboxes of different downstream tasks into a unified noisy label embedding. EMMScan estimate a model's transferability through a simple weighted linearregression, which can be efficiently solved by an alternating minimizationalgorithm with a convergence guarantee. Extensive experiments on 5 downstreamtasks with 24 datasets show that EMMS is fast, effective, and generic enough toassess the transferability of pre-trained models, making it the first modelselection method in the multi-task scenario. For instance, compared with thestate-of-the-art method LogME enhanced by our label embeddings, EMMS achieves9.0\%, 26.3\%, 20.1\%, 54.8\%, 12.2\% performance gain on image recognition,referring, captioning, visual question answering, and text question answering,while bringing 5.13x, 6.29x, 3.59x, 6.19x, and 5.66x speedup in wall-clocktime, respectively. The code is available athttps://github.com/OpenGVLab/Multitask-Model-Selector.</description><author>Fanqing Meng, Wenqi Shao, Zhanglin Peng, Chonghe Jiang, Kaipeng Zhang, Yu Qiao, Ping Luo</author><pubDate>Fri, 11 Aug 2023 18:54:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06262v1</guid></item><item><title>Combining Machine Learning Classifiers for Stock Trading with Effective Feature Extraction</title><link>http://arxiv.org/abs/2107.13148v3</link><description>The unpredictability and volatility of the stock market render it challengingto make a substantial profit using any generalised scheme. Many previousstudies tried different techniques to build a machine learning model, which canmake a significant profit in the US stock market by performing live trading.However, very few studies have focused on the importance of finding the bestfeatures for a particular trading period. Our top approach used the performanceto narrow down the features from a total of 148 to about 30. Furthermore, thetop 25 features were dynamically selected before each time training our machinelearning model. It uses ensemble learning with four classifiers: Gaussian NaiveBayes, Decision Tree, Logistic Regression with L1 regularization, andStochastic Gradient Descent, to decide whether to go long or short on aparticular stock. Our best model performed daily trade between July 2011 andJanuary 2019, generating 54.35% profit. Finally, our work showcased thatmixtures of weighted classifiers perform better than any individual predictorof making trading decisions in the stock market.</description><author>A. K. M. Amanat Ullah, Fahim Imtiaz, Miftah Uddin Md Ihsan, Md. Golam Rabiul Alam, Mahbub Majumdar</author><pubDate>Fri, 11 Aug 2023 18:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.13148v3</guid></item><item><title>Detection and classification of vocal productions in large scale audio recordings</title><link>http://arxiv.org/abs/2302.07640v2</link><description>We propose an automatic data processing pipeline to extract vocal productionsfrom large-scale natural audio recordings and classify these vocal productions.The pipeline is based on a deep neural network and adresses both issuessimultaneously. Though a series of computationel steps (windowing, creation ofa noise class, data augmentation, re-sampling, transfer learning, Bayesianoptimisation), it automatically trains a neural network without requiring alarge sample of labeled data and important computing resources. Our end-to-endmethodology can handle noisy recordings made under different recordingconditions. We test it on two different natural audio data sets, one from agroup of Guinea baboons recorded from a primate research center and one fromhuman babies recorded at home. The pipeline trains a model on 72 and 77 minutesof labeled audio recordings, with an accuracy of 94.58% and 99.76%. It is thenused to process 443 and 174 hours of natural continuous recordings and itcreates two new databases of 38.8 and 35.2 hours, respectively. We discuss thestrengths and limitations of this approach that can be applied to any massiveaudio recording.</description><author>Guillem Bonafos, Pierre Pudlo, Jean-Marc Freyermuth, Thierry Legou, Joël Fagot, Samuel Tronçon, Arnaud Rey</author><pubDate>Fri, 11 Aug 2023 18:50:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07640v2</guid></item><item><title>Enhancing Network Management Using Code Generated by Large Language Models</title><link>http://arxiv.org/abs/2308.06261v1</link><description>Analyzing network topologies and communication graphs plays a crucial role incontemporary network management. However, the absence of a cohesive approachleads to a challenging learning curve, heightened errors, and inefficiencies.In this paper, we introduce a novel approach to facilitate anatural-language-based network management experience, utilizing large languagemodels (LLMs) to generate task-specific code from natural language queries.This method tackles the challenges of explainability, scalability, and privacyby allowing network operators to inspect the generated code, eliminating theneed to share network data with LLMs, and concentrating on application-specificrequests combined with general program synthesis techniques. We design andevaluate a prototype system using benchmark applications, showcasing highaccuracy, cost-effectiveness, and the potential for further enhancements usingcomplementary program synthesis techniques.</description><author>Sathiya Kumaran Mani, Yajie Zhou, Kevin Hsieh, Santiago Segarra, Ranveer Chandra, Srikanth Kandula</author><pubDate>Fri, 11 Aug 2023 18:49:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06261v1</guid></item><item><title>ChatGPT-based Investment Portfolio Selection</title><link>http://arxiv.org/abs/2308.06260v1</link><description>In this paper, we explore potential uses of generative AI models, such asChatGPT, for investment portfolio selection. Trusting investment advice fromGenerative Pre-Trained Transformer (GPT) models is a challenge due to model"hallucinations", necessitating careful verification and validation of theoutput. Therefore, we take an alternative approach. We use ChatGPT to obtain auniverse of stocks from S&amp;P500 market index that are potentially attractive forinvesting. Subsequently, we compared various portfolio optimization strategiesthat utilized this AI-generated trading universe, evaluating those againstquantitative portfolio optimization models as well as comparing to some of thepopular investment funds. Our findings indicate that ChatGPT is effective instock selection but may not perform as well in assigning optimal weights tostocks within the portfolio. But when stocks selection by ChatGPT is combinedwith established portfolio optimization models, we achieve even better results.By blending strengths of AI-generated stock selection with advancedquantitative optimization techniques, we observed the potential for more robustand favorable investment outcomes, suggesting a hybrid approach for moreeffective and reliable investment decision-making in the future.</description><author>Oleksandr Romanko, Akhilesh Narayan, Roy H. Kwon</author><pubDate>Fri, 11 Aug 2023 18:48:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06260v1</guid></item><item><title>Self-Alignment with Instruction Backtranslation</title><link>http://arxiv.org/abs/2308.06259v1</link><description>We present a scalable method to build a high quality instruction followinglanguage model by automatically labelling human-written text with correspondinginstructions. Our approach, named instruction backtranslation, starts with alanguage model finetuned on a small amount of seed data, and a given webcorpus. The seed model is used to construct training examples by generatinginstruction prompts for web documents (self-augmentation), and then selectinghigh quality examples from among these candidates (self-curation). This data isthen used to finetune a stronger model. Finetuning LLaMa on two iterations ofour approach yields a model that outperforms all other LLaMa-based models onthe Alpaca leaderboard not relying on distillation data, demonstrating highlyeffective self-alignment.</description><author>Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, Mike Lewis</author><pubDate>Fri, 11 Aug 2023 18:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06259v1</guid></item><item><title>RT-1: Robotics Transformer for Real-World Control at Scale</title><link>http://arxiv.org/abs/2212.06817v2</link><description>By transferring knowledge from large, diverse, task-agnostic datasets, modernmachine learning models can solve specific downstream tasks either zero-shot orwith small task-specific datasets to a high level of performance. While thiscapability has been demonstrated in other fields such as computer vision,natural language processing or speech recognition, it remains to be shown inrobotics, where the generalization capabilities of the models are particularlycritical due to the difficulty of collecting real-world robotic data. We arguethat one of the keys to the success of such general robotic models lies withopen-ended task-agnostic training, combined with high-capacity architecturesthat can absorb all of the diverse, robotic data. In this paper, we present amodel class, dubbed Robotics Transformer, that exhibits promising scalablemodel properties. We verify our conclusions in a study of different modelclasses and their ability to generalize as a function of the data size, modelsize, and data diversity based on a large-scale data collection on real robotsperforming real-world tasks. The project's website and videos can be found atrobotics-transformer1.github.io</description><author>Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich</author><pubDate>Fri, 11 Aug 2023 18:45:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.06817v2</guid></item><item><title>ML-SUPERB: Multilingual Speech Universal PERformance Benchmark</title><link>http://arxiv.org/abs/2305.10615v2</link><description>Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboardto benchmark the performance of Self-Supervised Learning (SSL) models onvarious speech processing tasks. However, SUPERB largely considers Englishspeech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB),covering 143 languages (ranging from high-resource to endangered), andconsidering both automatic speech recognition and language identification.Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features andemploys a simple framework for multilingual tasks by learning a shallowdownstream model. Similar to the SUPERB benchmark, we find speech SSL modelscan significantly improve performance compared to FBANK features. Furthermore,we find that multilingual models do not always perform better than theirmonolingual counterparts. We will release ML-SUPERB as a challenge withorganized datasets and reproducible training scripts for future multilingualrepresentation research.</description><author>Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Ping Huang, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe</author><pubDate>Fri, 11 Aug 2023 18:39:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10615v2</guid></item><item><title>FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods</title><link>http://arxiv.org/abs/2308.06248v1</link><description>The field of explainable artificial intelligence (XAI) aims to uncover theinner workings of complex deep neural models. While being crucial forsafety-critical domains, XAI inherently lacks ground-truth explanations, makingits automatic evaluation an unsolved problem. We address this challenge byproposing a novel synthetic vision dataset, named FunnyBirds, and accompanyingautomatic evaluation protocols. Our dataset allows performing semanticallymeaningful image interventions, e.g., removing individual object parts, whichhas three important implications. First, it enables analyzing explanations on apart level, which is closer to human comprehension than existing methods thatevaluate on a pixel level. Second, by comparing the model output for inputswith removed parts, we can estimate ground-truth part importances that shouldbe reflected in the explanations. Third, by mapping individual explanationsinto a common space of part importances, we can analyze a variety of differentexplanation types in a single common framework. Using our tools, we reportresults for 24 different combinations of neural models and XAI methods,demonstrating the strengths and weaknesses of the assessed methods in a fullyautomatic and systematic manner.</description><author>Robin Hesse, Simone Schaub-Meyer, Stefan Roth</author><pubDate>Fri, 11 Aug 2023 18:29:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06248v1</guid></item><item><title>F?D: On understanding the role of deep feature spaces on face generation evaluation</title><link>http://arxiv.org/abs/2305.20048v3</link><description>Perceptual metrics, like the Fr\'echet Inception Distance (FID), are widelyused to assess the similarity between synthetically generated and ground truth(real) images. The key idea behind these metrics is to compute errors in a deepfeature space that captures perceptually and semantically rich image features.Despite their popularity, the effect that different deep features and theirdesign choices have on a perceptual metric has not been well studied. In thiswork, we perform a causal analysis linking differences in semantic attributesand distortions between face image distributions to Fr\'echet distances (FD)using several popular deep feature spaces. A key component of our analysis isthe creation of synthetic counterfactual faces using deep face generators. Ourexperiments show that the FD is heavily influenced by its feature space'straining dataset and objective function. For example, FD using featuresextracted from ImageNet-trained models heavily emphasize hats over regions likethe eyes and mouth. Moreover, FD using features from a face gender classifieremphasize hair length more than distances in an identity (recognition) featurespace. Finally, we evaluate several popular face generation models acrossfeature spaces and find that StyleGAN2 consistently ranks higher than otherface generators, except with respect to identity (recognition) features. Thissuggests the need for considering multiple feature spaces when evaluatinggenerative models and using feature spaces that are tuned to nuances of thedomain of interest.</description><author>Krish Kabra, Guha Balakrishnan</author><pubDate>Fri, 11 Aug 2023 18:26:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.20048v3</guid></item><item><title>Private Distribution Learning with Public Data: The View from Sample Compression</title><link>http://arxiv.org/abs/2308.06239v1</link><description>We study the problem of private distribution learning with access to publicdata. In this setup, which we refer to as public-private learning, the learneris given public and private samples drawn from an unknown distribution $p$belonging to a class $\mathcal Q$, with the goal of outputting an estimate of$p$ while adhering to privacy constraints (here, pure differential privacy)only with respect to the private samples. We show that the public-private learnability of a class $\mathcal Q$ isconnected to the existence of a sample compression scheme for $\mathcal Q$, aswell as to an intermediate notion we refer to as list learning. Leveraging thisconnection: (1) approximately recovers previous results on Gaussians over$\mathbb R^d$; and (2) leads to new ones, including sample complexity upperbounds for arbitrary $k$-mixtures of Gaussians over $\mathbb R^d$, results foragnostic and distribution-shift resistant learners, as well as closureproperties for public-private learnability under taking mixtures and productsof distributions. Finally, via the connection to list learning, we show thatfor Gaussians in $\mathbb R^d$, at least $d$ public samples are necessary forprivate learnability, which is close to the known upper bound of $d+1$ publicsamples.</description><author>Shai Ben-David, Alex Bie, Clément L. Canonne, Gautam Kamath, Vikrant Singhal</author><pubDate>Fri, 11 Aug 2023 18:15:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06239v1</guid></item><item><title>KETM:A Knowledge-Enhanced Text Matching method</title><link>http://arxiv.org/abs/2308.06235v1</link><description>Text matching is the task of matching two texts and determining therelationship between them, which has extensive applications in natural languageprocessing tasks such as reading comprehension, and Question-Answering systems.The mainstream approach is to compute text representations or to interact withthe text through attention mechanism, which is effective in text matchingtasks. However, the performance of these models is insufficient for texts thatrequire commonsense knowledge-based reasoning. To this end, in this paper, Weintroduce a new model for text matching called the Knowledge Enhanced TextMatching model (KETM), to enrich contextual representations with real-worldcommon-sense knowledge from external knowledge sources to enhance our modelunderstanding and reasoning. First, we use Wiktionary to retrieve the text worddefinitions as our external knowledge. Secondly, we feed text and knowledge tothe text matching module to extract their feature vectors. The text matchingmodule is used as an interaction module by integrating the encoder layer, theco-attention layer, and the aggregation layer. Specifically, the interactionprocess is iterated several times to obtain in-depth interaction informationand extract the feature vectors of text and knowledge by multi-angle pooling.Then, we fuse text and knowledge using a gating mechanism to learn the ratio oftext and knowledge fusion by a neural network that prevents noise generated byknowledge. After that, experimental validation on four datasets are carriedout, and the experimental results show that our proposed model performs well onall four datasets, and the performance of our method is improved compared tothe base model without adding external knowledge, which validates theeffectiveness of our proposed method. The code is available athttps://github.com/1094701018/KETM</description><author>Kexin Jiang, Yahui Zhao, Guozhe Jin, Zhenguo Zhang, Rongyi Cui</author><pubDate>Fri, 11 Aug 2023 18:08:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06235v1</guid></item><item><title>Inverse Kernel Decomposition</title><link>http://arxiv.org/abs/2211.05961v2</link><description>The state-of-the-art dimensionality reduction approaches largely rely oncomplicated optimization procedures. On the other hand, closed-form approachesrequiring merely eigen-decomposition do not have enough sophistication andnonlinearity. In this paper, we propose a novel nonlinear dimensionalityreduction method -- Inverse Kernel Decomposition (IKD) -- based on aneigen-decomposition of the sample covariance matrix of data. The method isinspired by Gaussian process latent variable models (GPLVMs) and has comparableperformance with GPLVMs. To deal with very noisy data with weak correlations,we propose two solutions -- blockwise and geodesic -- to make use of locallycorrelated data points and provide better and numerically more stable latentestimations. We use synthetic datasets and four real-world datasets to showthat IKD is a better dimensionality reduction method than othereigen-decomposition-based methods, and achieves comparable performance againstoptimization-based methods with faster running speeds. Open-source IKDimplementation in Python can be accessed at this\url{https://github.com/JerrySoybean/ikd}.</description><author>Chengrui Li, Anqi Wu</author><pubDate>Fri, 11 Aug 2023 18:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.05961v2</guid></item><item><title>MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak Inundation Depth And Decoding Influencing Features</title><link>http://arxiv.org/abs/2308.06228v1</link><description>Timely, accurate, and reliable information is essential for decision-makers,emergency managers, and infrastructure operators during flood events. Thisstudy demonstrates a proposed machine learning model, MaxFloodCast, trained onphysics-based hydrodynamic simulations in Harris County, offers efficient andinterpretable flood inundation depth predictions. Achieving an averageR-squared of 0.949 and a Root Mean Square Error of 0.61 ft on unseen data, itproves reliable in forecasting peak flood inundation depths. Validated againstHurricane Harvey and Storm Imelda, MaxFloodCast shows the potential insupporting near-time floodplain management and emergency operations. Themodel's interpretability aids decision-makers in offering critical informationto inform flood mitigation strategies, to prioritize areas with criticalfacilities and to examine how rainfall in other watersheds influences floodexposure in one area. The MaxFloodCast model enables accurate and interpretableinundation depth predictions while significantly reducing computational time,thereby supporting emergency response efforts and flood risk management moreeffectively.</description><author>Cheng-Chun Lee, Lipai Huang, Federico Antolini, Matthew Garcia, Andrew Juanb, Samuel D. Brody, Ali Mostafavi</author><pubDate>Fri, 11 Aug 2023 17:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06228v1</guid></item><item><title>Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms</title><link>http://arxiv.org/abs/2308.06221v1</link><description>We propose a multi-step training method for designing generalized linearclassifiers. First, an initial multi-class linear classifier is found throughregression. Then validation error is minimized by pruning of unnecessaryinputs. Simultaneously, desired outputs are improved via a method similar tothe Ho-Kashyap rule. Next, the output discriminants are scaled to be netfunctions of sigmoidal output units in a generalized linear classifier. We thendevelop a family of batch training algorithm for the multi layer perceptronthat optimizes its hidden layer size and number of training epochs. Next, wecombine pruning with a growing approach. Later, the input units are scaled tobe the net function of the sigmoidal output units that are then feed into asinput to the MLP. We then propose resulting improvements in each of the deeplearning blocks thereby improving the overall performance of the deeparchitecture. We discuss the principles and formulation regarding learningalgorithms for deep autoencoders. We investigate several problems in deepautoencoders networks including training issues, the theoretical, mathematicaland experimental justification that the networks are linear, optimizing thenumber of hidden units in each layer and determining the depth of the deeplearning model. A direct implication of the current work is the ability toconstruct fast deep learning models using desktop level computationalresources. This, in our opinion, promotes our design philosophy of buildingsmall but powerful algorithms. Performance gains are demonstrated at each step.Using widely available datasets, the final network's ten fold testing error isshown to be less than that of several other linear, generalized linearclassifiers, multi layer perceptron and deep learners reported in theliterature.</description><author>Kanishka Tyagi, Chinmay Rane, Michael Manry</author><pubDate>Fri, 11 Aug 2023 17:48:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06221v1</guid></item><item><title>Nonlinear Permuted Granger Causality</title><link>http://arxiv.org/abs/2308.06220v1</link><description>Granger causal inference is a contentious but widespread method used infields ranging from economics to neuroscience. The original definitionaddresses the notion of causality in time series by establishing functionaldependence conditional on a specified model. Adaptation of Granger causality tononlinear data remains challenging, and many methods apply in-sample tests thatdo not incorporate out-of-sample predictability leading to concerns of modeloverfitting. To allow for out-of-sample comparison, we explicitly define ameasure of functional connectivity using permutations of the covariate set.Artificial neural networks serve as featurizers of the data to approximate anyarbitrary, nonlinear relationship, and under certain conditions on thefeaturization process and the model residuals, we prove consistent estimationof the variance for each permutation. Performance of the permutation method iscompared to penalized objective, naive replacement, and omission techniques viasimulation, and we investigate its application to neuronal responses ofacoustic stimuli in the auditory cortex of anesthetized rats. We contend thattargeted use of the Granger causal framework, when prior knowledge of thecausal mechanisms in a dataset are limited, can help to reveal potentialpredictive relationships between sets of variables that warrant further study.</description><author>Noah D. Gade, Jordan Rodu</author><pubDate>Fri, 11 Aug 2023 17:44:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06220v1</guid></item><item><title>SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling</title><link>http://arxiv.org/abs/2308.04365v4</link><description>Causal inference is a crucial goal of science, enabling researchers to arriveat meaningful conclusions regarding the predictions of hypotheticalinterventions using observational data. Path models, Structural Equation Models(SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means tounambiguously specify assumptions regarding the causal structure underlying aphenomenon. Unlike DAGs, which make very few assumptions about the functionaland parametric form, SEM assumes linearity. This can result in functionalmisspecification which prevents researchers from undertaking reliable effectsize estimation. In contrast, we propose Super Learner Equation Modeling, apath modeling technique integrating machine learning Super Learner ensembles.We empirically demonstrate its ability to provide consistent and unbiasedestimates of causal effects, its competitive performance for linear models whencompared with SEM, and highlight its superiority over SEM when dealing withnon-linear relationships. We provide open-source code, and a tutorial notebookwith example usage, accentuating the easy-to-use nature of the method.</description><author>Matthew J. Vowels</author><pubDate>Fri, 11 Aug 2023 17:40:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04365v4</guid></item><item><title>Continual Face Forgery Detection via Historical Distribution Preserving</title><link>http://arxiv.org/abs/2308.06217v1</link><description>Face forgery techniques have advanced rapidly and pose serious securitythreats. Existing face forgery detection methods try to learn generalizablefeatures, but they still fall short of practical application. Additionally,finetuning these methods on historical training data is resource-intensive interms of time and storage. In this paper, we focus on a novel and challengingproblem: Continual Face Forgery Detection (CFFD), which aims to efficientlylearn from new forgery attacks without forgetting previous ones. Specifically,we propose a Historical Distribution Preserving (HDP) framework that reservesand preserves the distributions of historical faces. To achieve this, we useuniversal adversarial perturbation (UAP) to simulate historical forgerydistribution, and knowledge distillation to maintain the distribution variationof real faces across different models. We also construct a new benchmark forCFFD with three evaluation protocols. Our extensive experiments on thebenchmarks show that our method outperforms the state-of-the-art competitors.</description><author>Ke Sun, Shen Chen, Taiping Yao, Xiaoshuai Sun, Shouhong Ding, Rongrong Ji</author><pubDate>Fri, 11 Aug 2023 17:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06217v1</guid></item><item><title>Change Point Detection With Conceptors</title><link>http://arxiv.org/abs/2308.06213v1</link><description>Offline change point detection seeks to identify points in a time serieswhere the data generating process changes. This problem is well studied forunivariate i.i.d. data, but becomes challenging with increasing dimension andtemporal dependence. For the at most one change point problem, we propose theuse of a conceptor matrix to learn the characteristic dynamics of a specifiedtraining window in a time series. The associated random recurrent neuralnetwork acts as a featurizer of the data, and change points are identified froma univariate quantification of the distance between the featurization and thespace spanned by a representative conceptor matrix. This model agnostic methodcan suggest potential locations of interest that warrant further study. Weprove that, under mild assumptions, the method provides a consistent estimateof the true change point, and quantile estimates for statistics are producedvia a moving block bootstrap of the original data. The method is tested onsimulations from several classes of processes, and we evaluate performance withclustering metrics, graphical methods, and observed Type 1 error control. Weapply our method to publicly available neural data from rats experiencing boutsof non-REM sleep prior to exploration of a radial maze.</description><author>Noah D. Gade, Jordan Rodu</author><pubDate>Fri, 11 Aug 2023 17:32:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06213v1</guid></item><item><title>A Large Language Model Enhanced Conversational Recommender System</title><link>http://arxiv.org/abs/2308.06212v1</link><description>Conversational recommender systems (CRSs) aim to recommend high-quality itemsto users through a dialogue interface. It usually contains multiple sub-tasks,such as user preference elicitation, recommendation, explanation, and iteminformation search. To develop effective CRSs, there are some challenges: 1)how to properly manage sub-tasks; 2) how to effectively solve differentsub-tasks; and 3) how to correctly generate responses that interact with users.Recently, Large Language Models (LLMs) have exhibited an unprecedented abilityto reason and generate, presenting a new opportunity to develop more powerfulCRSs. In this work, we propose a new LLM-based CRS, referred to as LLMCRS, toaddress the above challenges. For sub-task management, we leverage thereasoning ability of LLM to effectively manage sub-task. For sub-task solving,we collaborate LLM with expert models of different sub-tasks to achieve theenhanced performance. For response generation, we utilize the generationability of LLM as a language interface to better interact with users.Specifically, LLMCRS divides the workflow into four stages: sub-task detection,model matching, sub-task execution, and response generation. LLMCRS alsodesigns schema-based instruction, demonstration-based instruction, dynamicsub-task and model matching, and summary-based generation to instruct LLM togenerate desired results in the workflow. Finally, to adapt LLM toconversational recommendations, we also propose to fine-tune LLM withreinforcement learning from CRSs performance feedback, referred to as RLPF.Experimental results on benchmark datasets show that LLMCRS with RLPFoutperforms the existing methods.</description><author>Yue Feng, Shuchang Liu, Zhenghai Xue, Qingpeng Cai, Lantao Hu, Peng Jiang, Kun Gai, Fei Sun</author><pubDate>Fri, 11 Aug 2023 17:30:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06212v1</guid></item><item><title>A method for escaping limit cycles in training GANs</title><link>http://arxiv.org/abs/2010.03322v3</link><description>This paper mainly conducts further research to alleviate the issue of limitcycling behavior in training generative adversarial networks (GANs) through theproposed predictive centripetal acceleration algorithm (PCAA). Specifically, wefirst derive the upper and lower bounds on the last-iterate convergence ratesof PCAA for the general bilinear game, with the upper bound notably improvingupon previous results. Then, we combine PCAA with the adaptive momentestimation algorithm (Adam) to propose PCAA-Adam, a practical approach fortraining GANs. Finally, we validate the effectiveness of the proposed algorithmthrough experiments conducted on bilinear games, multivariate Gaussiandistributions, and the CelebA dataset, respectively.</description><author>Li Keke, Yang Xinmin</author><pubDate>Fri, 11 Aug 2023 17:28:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2010.03322v3</guid></item><item><title>RANS-PINN based Simulation Surrogates for Predicting Turbulent Flows</title><link>http://arxiv.org/abs/2306.06034v3</link><description>Physics-informed neural networks (PINNs) provide a framework to buildsurrogate models for dynamical systems governed by differential equations.During the learning process, PINNs incorporate a physics-based regularizationterm within the loss function to enhance generalization performance. Sincesimulating dynamics controlled by partial differential equations (PDEs) can becomputationally expensive, PINNs have gained popularity in learning parametricsurrogates for fluid flow problems governed by Navier-Stokes equations. In thiswork, we introduce RANS-PINN, a modified PINN framework, to predict flow fields(i.e., velocity and pressure) in high Reynolds number turbulent flow regimes.To account for the additional complexity introduced by turbulence, RANS-PINNemploys a 2-equation eddy viscosity model based on a Reynolds-averagedNavier-Stokes (RANS) formulation. Furthermore, we adopt a novel trainingapproach that ensures effective initialization and balance among the variouscomponents of the loss function. The effectiveness of the RANS-PINN frameworkis then demonstrated using a parametric PINN.</description><author>Shinjan Ghosh, Amit Chakraborty, Georgia Olympia Brikis, Biswadip Dey</author><pubDate>Fri, 11 Aug 2023 17:17:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06034v3</guid></item><item><title>Lib-SibGMU -- A University Library Circulation Dataset for Recommender Systems Developmen</title><link>http://arxiv.org/abs/2208.12356v2</link><description>We opensource under CC BY 4.0 license Lib-SibGMU - a university librarycirculation dataset - for a wide research community, and benchmark majoralgorithms for recommender systems on this dataset. For a recommenderarchitecture that consists of a vectorizer that turns the history of the booksborrowed into a vector, and a neighborhood-based recommender, trainedseparately, we show that using the fastText model as a vectorizer deliverscompetitive results.</description><author>Eduard Zubchuk, Mikhail Arhipkin, Dmitry Menshikov, Aleksandr Karaush, Nikolay Mikhaylovskiy</author><pubDate>Fri, 11 Aug 2023 17:15:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.12356v2</guid></item><item><title>Oracle Teacher: Leveraging Target Information for Better Knowledge Distillation of CTC Models</title><link>http://arxiv.org/abs/2111.03664v4</link><description>Knowledge distillation (KD), best known as an effective method for modelcompression, aims at transferring the knowledge of a bigger network (teacher)to a much smaller network (student). Conventional KD methods usually employ theteacher model trained in a supervised manner, where output labels are treatedonly as targets. Extending this supervised scheme further, we introduce a newtype of teacher model for connectionist temporal classification (CTC)-basedsequence models, namely Oracle Teacher, that leverages both the source inputsand the output labels as the teacher model's input. Since the Oracle Teacherlearns a more accurate CTC alignment by referring to the target information, itcan provide the student with more optimal guidance. One potential risk for theproposed approach is a trivial solution that the model's output directly copiesthe target input. Based on a many-to-one mapping property of the CTC algorithm,we present a training strategy that can effectively prevent the trivialsolution and thus enables utilizing both source and target inputs for modeltraining. Extensive experiments are conducted on two sequence learning tasks:speech recognition and scene text recognition. From the experimental results,we empirically show that the proposed model improves the students across thesetasks while achieving a considerable speed-up in the teacher model's trainingtime.</description><author>Ji Won Yoon, Hyung Yong Kim, Hyeonseung Lee, Sunghwan Ahn, Nam Soo Kim</author><pubDate>Fri, 11 Aug 2023 17:15:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.03664v4</guid></item><item><title>Thinking Like an Expert:Multimodal Hypergraph-of-Thought (HoT) Reasoning to boost Foundation Modals</title><link>http://arxiv.org/abs/2308.06207v1</link><description>Reasoning ability is one of the most crucial capabilities of a foundationmodel, signifying its capacity to address complex reasoning tasks.Chain-of-Thought (CoT) technique is widely regarded as one of the effectivemethods for enhancing the reasoning ability of foundation models and hasgarnered significant attention. However, the reasoning process of CoT islinear, step-by-step, similar to personal logical reasoning, suitable forsolving general and slightly complicated problems. On the contrary, thethinking pattern of an expert owns two prominent characteristics that cannot behandled appropriately in CoT, i.e., high-order multi-hop reasoning andmultimodal comparative judgement. Therefore, the core motivation of this paperis transcending CoT to construct a reasoning paradigm that can think like anexpert. The hyperedge of a hypergraph could connect various vertices, making itnaturally suitable for modelling high-order relationships. Inspired by this,this paper innovatively proposes a multimodal Hypergraph-of-Thought (HoT)reasoning paradigm, which enables the foundation models to possess theexpert-level ability of high-order multi-hop reasoning and multimodalcomparative judgement. Specifically, a textual hypergraph-of-thought isconstructed utilizing triple as the primary thought to model higher-orderrelationships, and a hyperedge-of-thought is generated through multi-hopwalking paths to achieve multi-hop inference. Furthermore, we devise a visualhypergraph-of-thought to interact with the textual hypergraph-of-thought viaCross-modal Co-Attention Graph Learning for multimodal comparativeverification. Experimentations on the ScienceQA benchmark demonstrate theproposed HoT-based T5 outperforms CoT-based GPT3.5 and chatGPT, which is on parwith CoT-based GPT4 with a lower model size.</description><author>Fanglong Yao, Changyuan Tian, Jintao Liu, Zequn Zhang, Qing Liu, Li Jin, Shuchao Li, Xiaoyu Li, Xian Sun</author><pubDate>Fri, 11 Aug 2023 17:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06207v1</guid></item><item><title>Safety in Traffic Management Systems: A Comprehensive Survey</title><link>http://arxiv.org/abs/2308.06204v1</link><description>Traffic management systems play a vital role in ensuring safe and efficienttransportation on roads. However, the use of advanced technologies in trafficmanagement systems has introduced new safety challenges. Therefore, it isimportant to ensure the safety of these systems to prevent accidents andminimize their impact on road users. In this survey, we provide a comprehensivereview of the literature on safety in traffic management systems. Specifically,we discuss the different safety issues that arise in traffic managementsystems, the current state of research on safety in these systems, and thetechniques and methods proposed to ensure the safety of these systems. We alsoidentify the limitations of the existing research and suggest future researchdirections.</description><author>Wenlu Du, Ankan Dash, Jing Li, Hua Wei, Guiling Wang</author><pubDate>Fri, 11 Aug 2023 17:09:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06204v1</guid></item><item><title>Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks</title><link>http://arxiv.org/abs/2308.06203v1</link><description>Uncertainties in the real world mean that is impossible for system designersto anticipate and explicitly design for all scenarios that a robot mightencounter. Thus, robots designed like this are fragile and fail outside ofhighly-controlled environments. Causal models provide a principled framework toencode formal knowledge of the causal relationships that govern the robot'sinteraction with its environment, in addition to probabilistic representationsof noise and uncertainty typically encountered by real-world robots. Combinedwith causal inference, these models permit an autonomous agent to understand,reason about, and explain its environment. In this work, we focus on theproblem of a robot block-stacking task due to the fundamental perception andmanipulation capabilities it demonstrates, required by many applicationsincluding warehouse logistics and domestic human support robotics. We propose anovel causal probabilistic framework to embed a physics simulation capabilityinto a structural causal model to permit robots to perceive and assess thecurrent state of a block-stacking task, reason about the next-best action fromplacement candidates, and generate post-hoc counterfactual explanations. Weprovide exemplar next-best action selection results and outline plannedexperimentation in simulated and real-world robot block-stacking tasks.</description><author>Ricardo Cannizzaro, Jonathan Routley, Lars Kunze</author><pubDate>Fri, 11 Aug 2023 16:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06203v1</guid></item><item><title>Exploring Predicate Visual Context in Detecting of Human-Object Interactions</title><link>http://arxiv.org/abs/2308.06202v1</link><description>Recently, the DETR framework has emerged as the dominant approach forhuman--object interaction (HOI) research. In particular, two-stagetransformer-based HOI detectors are amongst the most performant andtraining-efficient approaches. However, these often condition HOIclassification on object features that lack fine-grained contextualinformation, eschewing pose and orientation information in favour of visualcues about object identity and box extremities. This naturally hinders therecognition of complex or ambiguous interactions. In this work, we study theseissues through visualisations and carefully designed experiments. Accordingly,we investigate how best to re-introduce image features via cross-attention.With an improved query design, extensive exploration of keys and values, andbox pair positional embeddings as spatial guidance, our model with enhancedpredicate visual context (PViC) outperforms state-of-the-art methods on theHICO-DET and V-COCO benchmarks, while maintaining low training cost.</description><author>Frederic Z. Zhang, Yuhui Yuan, Dylan Campbell, Zhuoyao Zhong, Stephen Gould</author><pubDate>Fri, 11 Aug 2023 16:57:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06202v1</guid></item><item><title>Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models</title><link>http://arxiv.org/abs/2303.06628v2</link><description>Continual learning (CL) can help pre-trained vision-language modelsefficiently adapt to new or under-trained data distributions withoutre-training. Nevertheless, during the continual training of the ContrastiveLanguage-Image Pre-training (CLIP) model, we observe that the model's zero-shottransfer ability significantly degrades due to catastrophic forgetting.Existing CL methods can mitigate forgetting by replaying previous data.However, since the CLIP dataset is private, replay methods cannot access thepre-training dataset. In addition, replaying data of previously learneddownstream tasks can enhance their performance but comes at the cost ofsacrificing zero-shot performance. To address this challenge, we propose anovel method ZSCL to prevent zero-shot transfer degradation in the continuallearning of vision-language models in both feature and parameter space. In thefeature space, a reference dataset is introduced for distillation between thecurrent and initial models. The reference dataset should have semanticdiversity but no need to be labeled, seen in pre-training, or matchedimage-text pairs. In parameter space, we prevent a large parameter shift byaveraging weights during the training. We propose a more challengingMulti-domain Task Incremental Learning (MTIL) benchmark to evaluate differentmethods, where tasks are from various domains instead of class-separated in asingle dataset. Our method outperforms other methods in the traditionalclass-incremental learning setting and the MTIL by 9.7% average score. Our codelocates at https://github.com/Thunderbeee/ZSCL.</description><author>Zangwei Zheng, Mingyuan Ma, Kai Wang, Ziheng Qin, Xiangyu Yue, Yang You</author><pubDate>Fri, 11 Aug 2023 16:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06628v2</guid></item><item><title>Relational Action Bases: Formalization, Effective Safety Verification, and Invariants (Extended Version)</title><link>http://arxiv.org/abs/2208.06377v2</link><description>Modeling and verification of dynamic systems operating over a relationalrepresentation of states are increasingly investigated problems in AI, BusinessProcess Management, and Database Theory. To make these systems amenable toverification, the amount of information stored in each relational state needsto be bounded, or restrictions are imposed on the preconditions and effects ofactions. We introduce the general framework of relational action bases (RABs),which generalizes existing models by lifting both these restrictions: unboundedrelational states can be evolved through actions that can quantify bothexistentially and universally over the data, and that can exploit numericaldatatypes with arithmetic predicates. We then study parameterized safety ofRABs via (approximated) SMT-based backward search, singling out essentialmeta-properties of the resulting procedure, and showing how it can be realizedby an off-the-shelf combination of existing verification modules of thestate-of-the-art MCMT model checker. We demonstrate the effectiveness of thisapproach on a benchmark of data-aware business processes. Finally, we show howuniversal invariants can be exploited to make this procedure fully correct.</description><author>Silvio Ghilardi, Alessandro Gianola, Marco Montali, Andrey Rivkin</author><pubDate>Fri, 11 Aug 2023 16:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.06377v2</guid></item><item><title>Weakly Supervised Text Classification on Free Text Comments in Patient-Reported Outcome Measures</title><link>http://arxiv.org/abs/2308.06199v1</link><description>Free text comments (FTC) in patient-reported outcome measures (PROMs) dataare typically analysed using manual methods, such as content analysis, which islabour-intensive and time-consuming. Machine learning analysis methods arelargely unsupervised, necessitating post-analysis interpretation. Weaklysupervised text classification (WSTC) can be a valuable method of analysis toclassify domain-specific text data in which there is limited labelled data. Inthis paper, we apply five WSTC techniques to FTC in PROMs data to identifyhealth-related quality of life (HRQoL) themes reported by colorectal cancerpatients. The WSTC methods label all the themes mentioned in the FTC. Theresults showed moderate performance on the PROMs data, mainly due to theprecision of the models, and variation between themes. Evaluation of theclassification performance illustrated the potential and limitations of keywordbased WSTC to label PROMs FTC when labelled data is limited.</description><author>Anna-Grace Linton, Vania Dimitrova, Amy Downing, Richard Wagland, Adam Glaser</author><pubDate>Fri, 11 Aug 2023 16:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06199v1</guid></item><item><title>DIG In: Evaluating Disparities in Image Generations with Indicators for Geographic Diversity</title><link>http://arxiv.org/abs/2308.06198v1</link><description>The unprecedented photorealistic results achieved by recent text-to-imagegenerative systems and their increasing use as plug-and-play content creationsolutions make it crucial to understand their potential biases. In this work,we introduce three indicators to evaluate the realism, diversity andprompt-generation consistency of text-to-image generative systems when promptedto generate objects from across the world. Our indicators complementqualitative analysis of the broader impact of such systems by enablingautomatic and efficient benchmarking of geographic disparities, an importantstep towards building responsible visual content creation systems. We use ourproposed indicators to analyze potential geographic biases in state-of-the-artvisual content creation systems and find that: (1) models have less realism anddiversity of generations when prompting for Africa and West Asia than Europe,(2) prompting with geographic information comes at a cost to prompt-consistencyand diversity of generated images, and (3) models exhibit more region-leveldisparities for some objects than others. Perhaps most interestingly, ourindicators suggest that progress in image generation quality has come at thecost of real-world geographic representation. Our comprehensive evaluationconstitutes a crucial step towards ensuring a positive experience of visualcontent creation for everyone.</description><author>Melissa Hall, Candace Ross, Adina Williams, Nicolas Carion, Michal Drozdzal, Adriana Romero Soriano</author><pubDate>Fri, 11 Aug 2023 16:43:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06198v1</guid></item><item><title>Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features</title><link>http://arxiv.org/abs/2308.06197v1</link><description>Complex emotion recognition is a cognitive task that has so far eluded thesame excellent performance of other tasks that are at or above the level ofhuman cognition. Emotion recognition through facial expressions is particularlydifficult due to the complexity of emotions expressed by the human face. For amachine to approach the same level of performance in this domain as a human, itmay need to synthesise knowledge and understand new concepts in real-time ashumans do. Humans are able to learn new concepts using only few examples, bydistilling the important information from memories and discarding the rest.Similarly, continual learning methods learn new classes whilst retaining theknowledge of known classes, whilst few-shot learning methods are able to learnnew classes using very few training examples. We propose a novel continuallearning method inspired by human cognition and learning that can accuratelyrecognise new compound expression classes using few training samples, bybuilding on and retaining its knowledge of basic expression classes. UsingGradCAM visualisations, we demonstrate the relationship between basic andcompound facial expressions, which our method leverages through knowledgedistillation and a novel Predictive Sorting Memory Replay. Our method achievesthe current state-of-the-art in continual learning for complex facialexpression recognition with 74.28% Overall Accuracy on new classes. We alsodemonstrate that using continual learning for complex facial expressionrecognition achieves far better performance than non-continual learningmethods, improving on state-of-the-art non-continual learning methods by13.95%. To the best of our knowledge, our work is also the first to applyfew-shot learning to complex facial expression recognition, achieving thestate-of-the-art with 100% accuracy using a single training sample for eachexpression class.</description><author>Angus Maiden, Bahareh Nakisa</author><pubDate>Fri, 11 Aug 2023 16:42:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06197v1</guid></item><item><title>Deep Learning for Diverse Data Types Steganalysis: A Review</title><link>http://arxiv.org/abs/2308.04522v2</link><description>Steganography and steganalysis are two interrelated aspects of the field ofinformation security. Steganography seeks to conceal communications, whereassteganalysis is aimed to either find them or even, if possible, recover thedata they contain. Steganography and steganalysis have attracted a great dealof interest, particularly from law enforcement. Steganography is often used bycybercriminals and even terrorists to avoid being captured while in possessionof incriminating evidence, even encrypted, since cryptography is prohibited orrestricted in many countries. Therefore, knowledge of cutting-edge techniquesto uncover concealed information is crucial in exposing illegal acts. Over thelast few years, a number of strong and reliable steganography and steganalysistechniques have been introduced in the literature. This review paper provides acomprehensive overview of deep learning-based steganalysis techniques used todetect hidden information within digital media. The paper covers all types ofcover in steganalysis, including image, audio, and video, and discusses themost commonly used deep learning techniques. In addition, the paper exploresthe use of more advanced deep learning techniques, such as deep transferlearning (DTL) and deep reinforcement learning (DRL), to enhance theperformance of steganalysis systems. The paper provides a systematic review ofrecent research in the field, including data sets and evaluation metrics usedin recent studies. It also presents a detailed analysis of DTL-basedsteganalysis approaches and their performance on different data sets. Thereview concludes with a discussion on the current state of deep learning-basedsteganalysis, challenges, and future research directions.</description><author>Hamza Kheddar, Mustapha Hemis, Yassine Himeur, David Megías, Abbes Amira</author><pubDate>Fri, 11 Aug 2023 16:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04522v2</guid></item><item><title>Developmental Bootstrapping of AIs</title><link>http://arxiv.org/abs/2308.04586v2</link><description>Although some current AIs surpass human abilities especially in closedartificial worlds such as board games, their abilities in the real world arelimited. They make strange mistakes and do not notice them. They cannot beinstructed easily, fail to use common sense, and lack curiosity. They do notmake good collaborators. Mainstream approaches for creating AIs are built usingthe traditional manually-constructed symbolic AI approach and generative anddeep learning AI approaches including large language models (LLMs). Thesesystems are not well suited for creating robust and trustworthy AIs. Althoughit is outside of the mainstream, the developmental bootstrapping approach hasmore promise. In developmental bootstrapping, AIs develop competences likehuman children do. They start with innate competences. They interact with theenvironment and learn from their interactions. They incrementally extend theirinnate competences with self-developed competences. They interact and learnfrom people and establish perceptual, cognitive, and common grounding. Theyacquire the competences that they need through an incremental bootstrappingprocess. However, developmental robotics has not yet produced AIs with robustadult-level competences. Projects have typically stopped at the Toddler Barriercorresponding to human infant development at about two years of age, beforetheir speech is fluent. They also do not bridge the Reading Barrier, toskillfully and skeptically tap into the vast socially developed recordedinformation resources that power LLMs. The next competences in human cognitivedevelopment involve intrinsic motivation, imitation learning, imagination,coordination, and communication. This position paper lays out the logic,prospects, gaps, and challenges for extending the practice of developmentalbootstrapping to acquire further competences and create robust and resilientAIs.</description><author>Mark Stefik, Robert Price</author><pubDate>Fri, 11 Aug 2023 16:33:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04586v2</guid></item><item><title>MAMAF-Net: Motion-Aware and Multi-Attention Fusion Network for Stroke Diagnosis</title><link>http://arxiv.org/abs/2304.09466v2</link><description>Stroke is a major cause of mortality and disability worldwide from which onein four people are in danger of incurring in their lifetime. The pre-hospitalstroke assessment plays a vital role in identifying stroke patients accuratelyto accelerate further examination and treatment in hospitals. Accordingly, theNational Institutes of Health Stroke Scale (NIHSS), Cincinnati Pre-hospitalStroke Scale (CPSS) and Face Arm Speed Time (F.A.S.T.) are globally known testsfor stroke assessment. However, the validity of these tests is skeptical in theabsence of neurologists and access to healthcare may be limited. Therefore, inthis study, we propose a motion-aware and multi-attention fusion network(MAMAF-Net) that can detect stroke from multimodal examination videos. Contraryto other studies on stroke detection from video analysis, our study for thefirst time proposes an end-to-end solution from multiple video recordings ofeach subject with a dataset encapsulating stroke, transient ischemic attack(TIA), and healthy controls. The proposed MAMAF-Net consists of motion-awaremodules to sense the mobility of patients, attention modules to fuse themulti-input video data, and 3D convolutional layers to perform diagnosis fromthe attention-based extracted features. Experimental results over the collectedStroke-data dataset show that the proposed MAMAF-Net achieves a successfuldetection of stroke with 93.62% sensitivity and 95.33% AUC score.</description><author>Aysen Degerli, Pekka Jakala, Juha Pajula, Milla Immonen, Miguel Bordallo Lopez</author><pubDate>Fri, 11 Aug 2023 16:30:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09466v2</guid></item><item><title>CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic Languages</title><link>http://arxiv.org/abs/2308.04255v2</link><description>We present CLASSLA-Stanza, a pipeline for automatic linguistic annotation ofthe South Slavic languages, which is based on the Stanza natural languageprocessing pipeline. We describe the main improvements in CLASSLA-Stanza withrespect to Stanza, and give a detailed description of the model trainingprocess for the latest 2.1 release of the pipeline. We also report performancescores produced by the pipeline for different languages and varieties.CLASSLA-Stanza exhibits consistently high performance across all the supportedlanguages and outperforms or expands its parent pipeline Stanza at all thesupported tasks. We also present the pipeline's new functionality enablingefficient processing of web data and the reasons that led to itsimplementation.</description><author>Luka Terčon, Nikola Ljubešić</author><pubDate>Fri, 11 Aug 2023 16:24:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04255v2</guid></item><item><title>Software Doping Analysis for Human Oversight</title><link>http://arxiv.org/abs/2308.06186v1</link><description>This article introduces a framework that is meant to assist in mitigatingsocietal risks that software can pose. Concretely, this encompasses facets ofsoftware doping as well as unfairness and discrimination in high-riskdecision-making systems. The term software doping refers to software thatcontains surreptitiously added functionality that is against the interest ofthe user. A prominent example of software doping are the tampered emissioncleaning systems that were found in millions of cars around the world when thediesel emissions scandal surfaced. The first part of this article combines theformal foundations of software doping analysis with established probabilisticfalsification techniques to arrive at a black-box analysis technique foridentifying undesired effects of software. We apply this technique to emissioncleaning systems in diesel cars but also to high-risk systems that evaluatehumans in a possibly unfair or discriminating way. We demonstrate how ourapproach can assist humans-in-the-loop to make better informed and moreresponsible decisions. This is to promote effective human oversight, which willbe a central requirement enforced by the European Union's upcoming AI Act. Wecomplement our technical contribution with a juridically, philosophically, andpsychologically informed perspective on the potential problems caused by suchsystems.</description><author>Sebastian Biewer, Kevin Baum, Sarah Sterz, Holger Hermanns, Sven Hetmank, Markus Langer, Anne Lauber-Rönsberg, Franz Lehr</author><pubDate>Fri, 11 Aug 2023 16:24:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06186v1</guid></item><item><title>Larger is not Better: A Survey on the Robustness of Computer Vision Models against Common Corruptions</title><link>http://arxiv.org/abs/2305.06024v2</link><description>The performance of computer vision models are susceptible to unexpectedchanges in input images, known as common corruptions (e.g. noise, blur,illumination changes, etc.), that can hinder their reliability when deployed inreal scenarios. These corruptions are not always considered to test modelgeneralization and robustness. In this survey, we present a comprehensiveoverview of methods that improve the robustness of computer vision modelsagainst common corruptions. We categorize methods into four groups based on themodel part and training method addressed: data augmentation, representationlearning, knowledge distillation, and network components. We also coverindirect methods for generalization and mitigation of shortcut learning,potentially useful for corruption robustness. We release a unified benchmarkframework to compare robustness performance on several datasets, and addressthe inconsistencies of evaluation in the literature. We provide an experimentaloverview of the base corruption robustness of popular vision backbones, andshow that corruption robustness does not necessarily scale with model size. Thevery large models (above 100M parameters) gain negligible robustness,considering the increased computational requirements. To achieve generalizableand robust computer vision models, we foresee the need of developing newlearning strategies to efficiently exploit limited data and mitigate unwantedor unreliable learning behaviors.</description><author>Shunxin Wang, Raymond Veldhuis, Christoph Brune, Nicola Strisciuglio</author><pubDate>Fri, 11 Aug 2023 16:23:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06024v2</guid></item><item><title>Nonparametric Inference under B-bits Quantization</title><link>http://arxiv.org/abs/1901.08571v3</link><description>Statistical inference based on lossy or incomplete samples is often needed inresearch areas such as signal/image processing, medical image storage, remotesensing, signal transmission. In this paper, we propose a nonparametric testingprocedure based on samples quantized to $B$ bits through a computationallyefficient algorithm. Under mild technical conditions, we establish theasymptotic properties of the proposed test statistic and investigate how thetesting power changes as $B$ increases. In particular, we show that if $B$exceeds a certain threshold, the proposed nonparametric testing procedureachieves the classical minimax rate of testing (Shang and Cheng, 2015) forspline models. We further extend our theoretical investigations to anonparametric linearity test and an adaptive nonparametric test, expanding theapplicability of the proposed methods. Extensive simulation studies {togetherwith a real-data analysis} are used to demonstrate the validity andeffectiveness of the proposed tests.</description><author>Kexuan Li, Ruiqi Liu, Ganggang Xu, Zuofeng Shang</author><pubDate>Fri, 11 Aug 2023 16:17:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1901.08571v3</guid></item><item><title>Noise-Resilient Designs for Optical Neural Networks</title><link>http://arxiv.org/abs/2308.06182v1</link><description>All analog signal processing is fundamentally subject to noise, and this isalso the case in modern implementations of Optical Neural Networks (ONNs).Therefore, to mitigate noise in ONNs, we propose two designs that areconstructed from a given, possibly trained, Neural Network (NN) that one wishesto implement. Both designs have the capability that the resulting ONNs givesoutputs close to the desired NN. To establish the latter, we analyze the designs mathematically. Specifically,we investigate a probabilistic framework for the first design that establishesthat the design is correct, i.e., for any feed-forward NN with Lipschitzcontinuous activation functions, an ONN can be constructed that produces outputarbitrarily close to the original. ONNs constructed with the first design thusalso inherit the universal approximation property of NNs. For the seconddesign, we restrict the analysis to NNs with linear activation functions andcharacterize the ONNs' output distribution using exact formulas. Finally, we report on numerical experiments with LeNet ONNs that give insightinto the number of components required in these designs for certain accuracygains. We specifically study the effect of noise as a function of the depth ofan ONN. The results indicate that in practice, adding just a few components inthe manner of the first or the second design can already be expected toincrease the accuracy of ONNs considerably.</description><author>Gianluca Kosmella, Ripalta Stabile, Jaron Sanders</author><pubDate>Fri, 11 Aug 2023 16:11:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06182v1</guid></item><item><title>Assessing Guest Nationality Composition from Hotel Reviews</title><link>http://arxiv.org/abs/2308.06175v1</link><description>Many hotels target guest acquisition efforts to specific markets in order tobest anticipate individual preferences and needs of their guests. Likewise,such strategic positioning is a prerequisite for efficient marketing budgetallocation. Official statistics report on the number of visitors from differentcountries, but no fine-grained information on the guest composition ofindividual businesses exists. There is, however, growing interest in such datafrom competitors, suppliers, researchers and the general public. We demonstratehow machine learning can be leveraged to extract references to guestnationalities from unstructured text reviews in order to dynamically assess andmonitor the dynamics of guest composition of individual businesses. Inparticular, we show that a rather simple architecture of pre-trained embeddingsand stacked LSTM layers provides a better performance-runtime tradeoff thanmore complex state-of-the-art language models.</description><author>Fabian Gröger, Marc Pouly, Flavia Tinner, Leif Brandes</author><pubDate>Fri, 11 Aug 2023 16:04:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06175v1</guid></item><item><title>Selecting the number of clusters, clustering models, and algorithms. A unifying approach based on the quadratic discriminant score</title><link>http://arxiv.org/abs/2111.02302v3</link><description>Cluster analysis requires many decisions: the clustering method and theimplied reference model, the number of clusters and, often, severalhyper-parameters and algorithms' tunings. In practice, one produces severalpartitions, and a final one is chosen based on validation or selectioncriteria. There exist an abundance of validation methods that, implicitly orexplicitly, assume a certain clustering notion. Moreover, they are oftenrestricted to operate on partitions obtained from a specific method. In thispaper, we focus on groups that can be well separated by quadratic or linearboundaries. The reference cluster concept is defined through the quadraticdiscriminant score function and parameters describing clusters' size, centerand scatter. We develop two cluster-quality criteria called quadratic scores.We show that these criteria are consistent with groups generated from a generalclass of elliptically-symmetric distributions. The quest for this type ofgroups is common in applications. The connection with likelihood theory formixture models and model-based clustering is investigated. Based on bootstrapresampling of the quadratic scores, we propose a selection rule that allowschoosing among many clustering solutions. The proposed method has thedistinctive advantage that it can compare partitions that cannot be comparedwith other state-of-the-art methods. Extensive numerical experiments and theanalysis of real data show that, even if some competing methods turn out to besuperior in some setups, the proposed methodology achieves a better overallperformance.</description><author>Luca Coraggio, Pietro Coretto</author><pubDate>Fri, 11 Aug 2023 16:02:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.02302v3</guid></item><item><title>Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook</title><link>http://arxiv.org/abs/2308.06173v1</link><description>In this paper, we present a comprehensive survey of the current trendsfocusing specifically on physical adversarial attacks. We aim to provide athorough understanding of the concept of physical adversarial attacks,analyzing their key characteristics and distinguishing features. Furthermore,we explore the specific requirements and challenges associated with executingattacks in the physical world. Our article delves into various physicaladversarial attack methods, categorized according to their target tasks indifferent applications, including classification, detection, face recognition,semantic segmentation and depth estimation. We assess the performance of theseattack methods in terms of their effectiveness, stealthiness, and robustness.We examine how each technique strives to ensure the successful manipulation ofDNNs while mitigating the risk of detection and withstanding real-worlddistortions. Lastly, we discuss the current challenges and outline potentialfuture research directions in the field of physical adversarial attacks. Wehighlight the need for enhanced defense mechanisms, the exploration of novelattack strategies, the evaluation of attacks in different application domains,and the establishment of standardized benchmarks and evaluation criteria forphysical adversarial attacks. Through this comprehensive survey, we aim toprovide a valuable resource for researchers, practitioners, and policymakers togain a holistic understanding of physical adversarial attacks in computervision and facilitate the development of robust and secure DNN-based systems.</description><author>Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammed Shafique</author><pubDate>Fri, 11 Aug 2023 16:02:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06173v1</guid></item><item><title>ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes &amp; Progressions</title><link>http://arxiv.org/abs/2303.12364v3</link><description>In this study, we introduce ExBEHRT, an extended version of BEHRT (BERTapplied to electronic health records), and apply different algorithms tointerpret its results. While BEHRT considers only diagnoses and patient age, weextend the feature space to several multimodal records, namely demographics,clinical characteristics, vital signs, smoking status, diagnoses, procedures,medications, and laboratory tests, by applying a novel method to unify thefrequencies and temporal dimensions of the different features. We show thatadditional features significantly improve model performance for variousdownstream tasks in different diseases. To ensure robustness, we interpretmodel predictions using an adaptation of expected gradients, which has not beenpreviously applied to transformers with EHR data and provides more granularinterpretations than previous approaches such as feature and token importances.Furthermore, by clustering the model representations of oncology patients, weshow that the model has an implicit understanding of the disease and is able toclassify patients with the same cancer type into different risk groups. Giventhe additional features and interpretability, ExBEHRT can help make informeddecisions about disease trajectories, diagnoses, and risk factors of variousdiseases.</description><author>Maurice Rupp, Oriane Peter, Thirupathi Pattipaka</author><pubDate>Fri, 11 Aug 2023 15:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12364v3</guid></item><item><title>Task Conditioned BERT for Joint Intent Detection and Slot-filling</title><link>http://arxiv.org/abs/2308.06165v1</link><description>Dialogue systems need to deal with the unpredictability of user intents totrack dialogue state and the heterogeneity of slots to understand userpreferences. In this paper we investigate the hypothesis that solving thesechallenges as one unified model will allow the transfer of parameter supportdata across the different tasks. The proposed principled model is based on aTransformer encoder, trained on multiple tasks, and leveraged by a rich inputthat conditions the model on the target inferences. Conditioning theTransformer encoder on multiple target inferences over the same corpus, i.e.,intent and multiple slot types, allows learning richer language interactionsthan a single-task model would be able to. In fact, experimental resultsdemonstrate that conditioning the model on an increasing number of dialogueinference tasks leads to improved results: on the MultiWOZ dataset, the jointintent and slot detection can be improved by 3.2\% by conditioning on intent,10.8\% by conditioning on slot and 14.4\% by conditioning on both intent andslots. Moreover, on real conversations with Farfetch costumers, the proposedconditioned BERT can achieve high joint-goal and intent detection performancethroughout a dialogue.</description><author>Diogo Tavares, Pedro Azevedo, David Semedo, Ricardo Sousa, João Magalhães</author><pubDate>Fri, 11 Aug 2023 15:47:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06165v1</guid></item><item><title>Rethinking the Localization in Weakly Supervised Object Localization</title><link>http://arxiv.org/abs/2308.06161v1</link><description>Weakly supervised object localization (WSOL) is one of the most popular andchallenging tasks in computer vision. This task is to localize the objects inthe images given only the image-level supervision. Recently, dividing WSOL intotwo parts (class-agnostic object localization and object classification) hasbecome the state-of-the-art pipeline for this task. However, existing solutionsunder this pipeline usually suffer from the following drawbacks: 1) they arenot flexible since they can only localize one object for each image due to theadopted single-class regression (SCR) for localization; 2) the generated pseudobounding boxes may be noisy, but the negative impact of such noise is not welladdressed. To remedy these drawbacks, we first propose to replace SCR with abinary-class detector (BCD) for localizing multiple objects, where the detectoris trained by discriminating the foreground and background. Then we design aweighted entropy (WE) loss using the unlabeled data to reduce the negativeimpact of noisy bounding boxes. Extensive experiments on the popularCUB-200-2011 and ImageNet-1K datasets demonstrate the effectiveness of ourmethod.</description><author>Rui Xu, Yong Luo, Han Hu, Bo Du, Jialie Shen, Yonggang Wen</author><pubDate>Fri, 11 Aug 2023 15:38:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06161v1</guid></item><item><title>DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models</title><link>http://arxiv.org/abs/2308.06160v1</link><description>Current deep networks are very data-hungry and benefit from training onlargescale datasets, which are often time-consuming to collect and annotate. Bycontrast, synthetic data can be generated infinitely using generative modelssuch as DALL-E and diffusion models, with minimal effort and cost. In thispaper, we present DatasetDM, a generic dataset generation model that canproduce diverse synthetic images and the corresponding high-quality perceptionannotations (e.g., segmentation masks, and depth). Our method builds upon thepre-trained diffusion model and extends text-guided image synthesis toperception data generation. We show that the rich latent code of the diffusionmodel can be effectively decoded as accurate perception annotations using adecoder module. Training the decoder only needs less than 1% (around 100images) manually labeled images, enabling the generation of an infinitely largeannotated dataset. Then these synthetic data can be used for training variousperception models for downstream tasks. To showcase the power of the proposedapproach, we generate datasets with rich dense pixel-wise labels for a widerange of downstream tasks, including semantic segmentation, instancesegmentation, and depth estimation. Notably, it achieves 1) state-of-the-artresults on semantic segmentation and instance segmentation; 2) significantlymore robust on domain generalization than using the real data alone; andstate-of-the-art results in zero-shot segmentation setting; and 3) flexibilityfor efficient application and novel task composition (e.g., image editing). Theproject website and code can be found athttps://weijiawu.github.io/DatasetDM_page/ andhttps://github.com/showlab/DatasetDM, respectively</description><author>Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike Zheng Shou, Chunhua Shen</author><pubDate>Fri, 11 Aug 2023 15:38:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06160v1</guid></item><item><title>Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction</title><link>http://arxiv.org/abs/2308.06155v1</link><description>Inter-city highway transportation is significant for citizens' modern urbanlife and generates heterogeneous sensory data with spatio-temporalcharacteristics. As a routine analysis in transportation domain, daily trafficvolume estimation faces challenges for highway toll stations including lackingof exploration of correlative spatio-temporal features from a long-termperspective and effective means to deal with data imbalance which alwaysdeteriorates the predictive performance. In this paper, a deep spatio-temporallearning method is proposed to predict daily traffic volume in three phases. Infeature pre-processing phase, data is normalized elaborately according tolatent long-tail distribution. In spatio-temporal learning phase, a hybridmodel is employed combining fully convolution network (FCN) and long short-termmemory (LSTM), which considers time, space, meteorology, and calendar fromheterogeneous data. In decision phase, traffic volumes on a coming day atnetwork-wide toll stations would be achieved effectively, which is especiallycalibrated for vital few highway stations. Using real-world data from oneChinese provincial highway, extensive experiments show our method has distinctimprovement for predictive accuracy than various traditional models, reaching5.269 and 0.997 in MPAE and R-squre metrics, respectively.</description><author>Weilong Ding, Tianpu Zhang, Zhe Wang</author><pubDate>Fri, 11 Aug 2023 15:33:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06155v1</guid></item><item><title>Gaussian Process Regression for Maximum Entropy Distribution</title><link>http://arxiv.org/abs/2308.06149v1</link><description>Maximum-Entropy Distributions offer an attractive family of probabilitydensities suitable for moment closure problems. Yet finding the Lagrangemultipliers which parametrize these distributions, turns out to be acomputational bottleneck for practical closure settings. Motivated by recentsuccess of Gaussian processes, we investigate the suitability of Gaussianpriors to approximate the Lagrange multipliers as a map of a given set ofmoments. Examining various kernel functions, the hyperparameters are optimizedby maximizing the log-likelihood. The performance of the devised data-drivenMaximum-Entropy closure is studied for couple of test cases includingrelaxation of non-equilibrium distributions governed by Bhatnagar-Gross-Krookand Boltzmann kinetic equations.</description><author>Mohsen Sadr, Manuel Torrilhon, M. Hossein Gorji</author><pubDate>Fri, 11 Aug 2023 15:26:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06149v1</guid></item><item><title>Efficient Large-scale AUV-based Visual Seafloor Mapping</title><link>http://arxiv.org/abs/2308.06147v1</link><description>Driven by the increasing number of marine data science applications, there isa growing interest in surveying and exploring the vast, uncharted terrain ofthe deep sea with robotic platforms. Despite impressive results achieved bymany on-land visual mapping algorithms in the past decades, transferring thesemethods from land to the deep sea remains a challenge due to harshenvironmental conditions. Typically, deep-sea exploration involves the use ofautonomous underwater vehicles (AUVs) equipped with high-resolution cameras andartificial illumination systems. However, images obtained in this manner oftensuffer from heterogeneous illumination and quality degradation due toattenuation and scattering, on top of refraction of light rays. All of thistogether often lets on-land SLAM approaches fail underwater or makesStructure-from-Motion approaches drift or omit difficult images, resulting ingaps, jumps or weakly registered areas. In this work, we present a system thatincorporates recent developments in underwater imaging and visual mapping tofacilitate automated robotic 3D reconstruction of hectares of seafloor. Ourapproach is efficient in that it detects and reconsiders difficult, weaklyregistered areas, to avoid omitting images and to make better use of limiteddive time; on the other hand it is computationally efficient; leveraging ahybrid approach combining benefits from SLAM and Structure-from-Motion thatruns much faster than incremental reconstructions while achieving at leaston-par performance. The proposed system has been extensively tested andevaluated during several research cruises, demonstrating its robustness andpracticality in real-world conditions.</description><author>Mengkun She, Yifan Song, David Nakath, Kevin Köser</author><pubDate>Fri, 11 Aug 2023 15:24:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06147v1</guid></item><item><title>Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement</title><link>http://arxiv.org/abs/2306.14704v2</link><description>Mentions of new concepts appear regularly in texts and require automatedapproaches to harvest and place them into Knowledge Bases (KB), e.g.,ontologies and taxonomies. Existing datasets suffer from three issues, (i)mostly assuming that a new concept is pre-discovered and cannot supportout-of-KB mention discovery; (ii) only using the concept label as the inputalong with the KB and thus lacking the contexts of a concept label; and (iii)mostly focusing on concept placement w.r.t a taxonomy of atomic concepts,instead of complex concepts, i.e., with logical operators. To address theseissues, we propose a new benchmark, adapting MedMentions dataset (PubMedabstracts) with SNOMED CT versions in 2014 and 2017 under the Diseasessub-category and the broader categories of Clinical finding, Procedure, andPharmaceutical / biologic product. We provide usage on the evaluation with thedataset for out-of-KB mention discovery and concept placement, adapting recentLarge Language Model based methods.</description><author>Hang Dong, Jiaoyan Chen, Yuan He, Ian Horrocks</author><pubDate>Fri, 11 Aug 2023 15:17:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.14704v2</guid></item><item><title>Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models</title><link>http://arxiv.org/abs/2308.06144v1</link><description>The Forum for Information Retrieval (FIRE) started a shared task this yearfor classification of comments of different code segments. This is binary textclassification task where the objective is to identify whether comments givenfor certain code segments are relevant or not. The BioNLP-IISERB group at theIndian Institute of Science Education and Research Bhopal (IISERB) participatedin this task and submitted five runs for five different models. The paperpresents the overview of the models and other significant findings on thetraining corpus. The methods involve different feature engineering schemes andtext classification techniques. The performance of the classical bag of wordsmodel and transformer-based models were explored to identify significantfeatures from the given training corpus. We have explored different classifiersviz., random forest, support vector machine and logistic regression using thebag of words model. Furthermore, the pre-trained transformer based models likeBERT, RoBERT and ALBERT were also used by fine-tuning them on the giventraining corpus. The performance of different such models over the trainingcorpus were reported and the best five models were implemented on the giventest corpus. The empirical results show that the bag of words model outperformsthe transformer based models, however, the performance of our runs are notreasonably well in both training and test corpus. This paper also addresses thelimitations of the models and scope for further improvement.</description><author>Sruthi S, Tanmay Basu</author><pubDate>Fri, 11 Aug 2023 15:06:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06144v1</guid></item><item><title>Polarization Multi-Image Synthesis with Birefringent Metasurfaces</title><link>http://arxiv.org/abs/2307.08106v3</link><description>Optical metasurfaces composed of precisely engineered nanostructures havegained significant attention for their ability to manipulate light andimplement distinct functionalities based on the properties of the incidentfield. Computational imaging systems have started harnessing this capability toproduce sets of coded measurements that benefit certain tasks when paired withdigital post-processing. Inspired by these works, we introduce a new systemthat uses a birefringent metasurface with a polarizer-mosaicked photosensor tocapture four optically-coded measurements in a single exposure. We apply thissystem to the task of incoherent opto-electronic filtering, where digitalspatial-filtering operations are replaced by simpler, per-pixel sums across thefour polarization channels, independent of the spatial filter size. In contrastto previous work on incoherent opto-electronic filtering that can realize onlyone spatial filter, our approach can realize a continuous family of filtersfrom a single capture, with filters being selected from the family by adjustingthe post-capture digital summation weights. To find a metasurface that canrealize a set of user-specified spatial filters, we introduce a form ofgradient descent with a novel regularizer that encourages light efficiency anda high signal-to-noise ratio. We demonstrate several examples in simulation andwith fabricated prototypes, including some with spatial filters that haveprescribed variations with respect to depth and wavelength. Visit the Project Page athttps://deanhazineh.github.io/publications/Multi_Image_Synthesis/MIS_Home.html</description><author>Dean Hazineh, Soon Wei Daniel Lim, Qi Guo, Federico Capasso, Todd Zickler</author><pubDate>Fri, 11 Aug 2023 15:04:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08106v3</guid></item><item><title>CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging Handwritten Documents using Deep Feature Learning from JPEG Coefficients</title><link>http://arxiv.org/abs/2308.06142v1</link><description>Automatic localization of text-lines in handwritten documents is still anopen and challenging research problem. Various writing issues such as unevenspacing between the lines, oscillating and touching text, and the presence ofskew become much more challenging when the case of complex handwritten documentimages are considered for segmentation directly in their respective compressedrepresentation. This is because, the conventional way of processing compresseddocuments is through decompression, but here in this paper, we propose an ideathat employs deep feature learning directly from the JPEG compressedcoefficients without full decompression to accomplish text-line localization inthe JPEG compressed domain. A modified U-Net architecture known as CompressedText-Line Localization Network (CompTLL-UNet) is designed to accomplish it. Themodel is trained and tested with JPEG compressed version of benchmark datasetsincluding ICDAR2017 (cBAD) and ICDAR2019 (cBAD), reporting the state-of-the-artperformance with reduced storage and computational costs in the JPEG compresseddomain.</description><author>Bulla Rajesh, Sk Mahafuz Zaman, Mohammed Javed, P. Nagabhushan</author><pubDate>Fri, 11 Aug 2023 15:02:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06142v1</guid></item><item><title>An Efficient Incremental Simple Temporal Network Data Structure for Temporal Planning</title><link>http://arxiv.org/abs/2212.07226v2</link><description>One popular technique to solve temporal planning problems consists indecoupling the causal decisions, demanding them to heuristic search, fromtemporal decisions, demanding them to a simple temporal network (STN) solver.In this architecture, one needs to check the consistency of a series of STNsthat are related one another, therefore having methods to incrementally re-useprevious computations and that avoid expensive memory duplication is ofparamount importance. In this paper, we describe in detail how STNs are used intemporal planning, we identify a clear interface to support this use-case andwe present an efficient data-structure implementing this interface that is bothtime- and memory-efficient. We show that our data structure, called \deltastn,is superior to other state-of-the-art approaches on temporal planning sequencesof problems.</description><author>Andrea Micheli</author><pubDate>Fri, 11 Aug 2023 14:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07226v2</guid></item><item><title>Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling</title><link>http://arxiv.org/abs/2308.06138v1</link><description>Machine Learning (ML) is a powerful tool for material science applications.Artificial Neural Network (ANN) is a machine learning technique that canprovide high prediction accuracy. This study aimed to develop an ANN model topredict the cake moisture of the pressure filtration process of zincproduction. The cake moisture was influenced by seven parameters: temperature(35 and 65 Celsius), solid concentration (0.2 and 0.38 g/L), pH (2, 3.5, and5), air-blow time (2, 10, and 15 min), cake thickness (14, 20, 26, and 34 mm),pressure, and filtration time. The study conducted 288 tests using two types offabrics: polypropylene (S1) and polyester (S2). The ANN model was evaluated bythe Coefficient of determination (R2), the Mean Square Error (MSE), and theMean Absolute Error (MAE) metrics for both datasets. The results showed R2values of 0.88 and 0.83, MSE values of 6.243x10-07 and 1.086x10-06, and MAEvalues of 0.00056 and 0.00088 for S1 and S2, respectively. These resultsindicated that the ANN model could predict the cake moisture of pressurefiltration in the zinc leaching process with high accuracy.</description><author>Masoume Kazemi, Davood Moradkhani, Alireza A. Alipour</author><pubDate>Fri, 11 Aug 2023 14:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06138v1</guid></item><item><title>Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking</title><link>http://arxiv.org/abs/2302.07189v3</link><description>Discovering entity mentions that are out of a Knowledge Base (KB) from textsplays a critical role in KB maintenance, but has not yet been fully explored.The current methods are mostly limited to the simple threshold-based approachand feature-based classification, and the datasets for evaluation arerelatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL)method which can identify mentions that do not have corresponding KB entitiesby matching them to a special NIL entity. To better utilize BERT, we proposenew techniques including NIL entity representation and classification, withsynonym enhancement. We also apply KB Pruning and Versioning strategies toautomatically construct out-of-KB datasets from common in-KB EL datasets.Results on five datasets of clinical notes, biomedical publications, andWikipedia articles in various domains show the advantages of BLINKout overexisting methods to identify out-of-KB mentions for the medical ontologies,UMLS, SNOMED CT, and the general KB, WikiData.</description><author>Hang Dong, Jiaoyan Chen, Yuan He, Yinan Liu, Ian Horrocks</author><pubDate>Fri, 11 Aug 2023 14:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07189v3</guid></item><item><title>A Game-Theoretic Framework for Joint Forecasting and Planning</title><link>http://arxiv.org/abs/2308.06137v1</link><description>Planning safe robot motions in the presence of humans requires reliableforecasts of future human motion. However, simply predicting the most likelymotion from prior interactions does not guarantee safety. Such forecasts failto model the long tail of possible events, which are rarely observed in limiteddatasets. On the other hand, planning for worst-case motions leads to overtlyconservative behavior and a ``frozen robot''. Instead, we aim to learnforecasts that predict counterfactuals that humans guard against. We propose anovel game-theoretic framework for joint planning and forecasting with thepayoff being the performance of the planner against the demonstrator, andpresent practical algorithms to train models in an end-to-end fashion. Wedemonstrate that our proposed algorithm results in safer plans in a crowdnavigation simulator and real-world datasets of pedestrian motion. We releaseour code athttps://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning.</description><author>Kushal Kedia, Prithwish Dan, Sanjiban Choudhury</author><pubDate>Fri, 11 Aug 2023 14:56:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06137v1</guid></item><item><title>DiT: Efficient Vision Transformers with Dynamic Token Routing</title><link>http://arxiv.org/abs/2308.03409v2</link><description>Recently, the tokens of images share the same static data flow in many densenetworks. However, challenges arise from the variance among the objects inimages, such as large variations in the spatial scale and difficulties ofrecognition for visual entities. In this paper, we propose a data-dependenttoken routing strategy to elaborate the routing paths of image tokens forDynamic Vision Transformer, dubbed DiT. The proposed framework generates adata-dependent path per token, adapting to the object scales and visualdiscrimination of tokens. In feed-forward, the differentiable routing gates aredesigned to select the scaling paths and feature transformation paths for imagetokens, leading to multi-path feature propagation. In this way, the impact ofobject scales and visual discrimination of image representation can becarefully tuned. Moreover, the computational cost can be further reduced bygiving budget constraints to the routing gate and early-stopping of featureextraction. In experiments, our DiT achieves superior performance and favorablecomplexity/accuracy trade-offs than many SoTA methods on ImageNetclassification, object detection, instance segmentation, and semanticsegmentation. Particularly, the DiT-B5 obtains 84.8\% top-1 Acc on ImageNetwith 10.3 GFLOPs, which is 1.0\% higher than that of the SoTA method withsimilar computational complexity. These extensive results demonstrate that DiTcan serve as versatile backbones for various vision tasks.</description><author>Yuchen Ma, Zhengcong Fei, Junshi Huang</author><pubDate>Fri, 11 Aug 2023 14:53:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03409v2</guid></item><item><title>On the Trade-off between Over-smoothing and Over-squashing in Deep Graph Neural Networks</title><link>http://arxiv.org/abs/2212.02374v2</link><description>Graph Neural Networks (GNNs) have succeeded in various computer scienceapplications, yet deep GNNs underperform their shallow counterparts despitedeep learning's success in other domains. Over-smoothing and over-squashing arekey challenges when stacking graph convolutional layers, hindering deeprepresentation learning and information propagation from distant nodes. Ourwork reveals that over-smoothing and over-squashing are intrinsically relatedto the spectral gap of the graph Laplacian, resulting in an inevitabletrade-off between these two issues, as they cannot be alleviatedsimultaneously. To achieve a suitable compromise, we propose adding andremoving edges as a viable approach. We introduce the Stochastic Jost and LiuCurvature Rewiring (SJLR) algorithm, which is computationally efficient andpreserves fundamental properties compared to previous curvature-based methods.Unlike existing approaches, SJLR performs edge addition and removal during GNNtraining while maintaining the graph unchanged during testing. Comprehensivecomparisons demonstrate SJLR's competitive performance in addressingover-smoothing and over-squashing.</description><author>Jhony H. Giraldo, Konstantinos Skianis, Thierry Bouwmans, Fragkiskos D. Malliaros</author><pubDate>Fri, 11 Aug 2023 14:51:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02374v2</guid></item><item><title>Cross-modal Contrastive Learning for Multimodal Fake News Detection</title><link>http://arxiv.org/abs/2302.14057v2</link><description>Automatic detection of multimodal fake news has gained a widespread attentionrecently. Many existing approaches seek to fuse unimodal features to producemultimodal news representations. However, the potential of powerful cross-modalcontrastive learning methods for fake news detection has not been wellexploited. Besides, how to aggregate features from different modalities toboost the performance of the decision-making process is still an open question.To address that, we propose COOLANT, a cross-modal contrastive learningframework for multimodal fake news detection, aiming to achieve more accurateimage-text alignment. To further improve the alignment precision, we leveragean auxiliary task to soften the loss term of negative samples during thecontrast process. A cross-modal fusion module is developed to learn thecross-modality correlations. An attention mechanism with an attention guidancemodule is implemented to help effectively and interpretably aggregate thealigned unimodal representations and the cross-modality correlations. Finally,we evaluate the COOLANT and conduct a comparative study on two widely useddatasets, Twitter and Weibo. The experimental results demonstrate that ourCOOLANT outperforms previous approaches by a large margin and achieves newstate-of-the-art results on the two datasets.</description><author>Longzheng Wang, Chuang Zhang, Hongbo Xu, Yongxiu Xu, Xiaohan Xu, Siqi Wang</author><pubDate>Fri, 11 Aug 2023 14:48:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14057v2</guid></item><item><title>PENTACET data -- 23 Million Contextual Code Comments and 250,000 SATD comments</title><link>http://arxiv.org/abs/2303.14029v2</link><description>Most Self-Admitted Technical Debt (SATD) research utilizes explicit SATDfeatures such as 'TODO' and 'FIXME' for SATD detection. A closer look revealsseveral SATD research uses simple SATD ('Easy to Find') code comments withoutthe contextual data (preceding and succeeding source code context). This workaddresses this gap through PENTACET (or 5C dataset) data. PENTACET is a largeCurated Contextual Code Comments per Contributor and the most extensive SATDdata. We mine 9,096 Open Source Software Java projects with a total of 435million LOC. The outcome is a dataset with 23 million code comments, precedingand succeeding source code context for each comment, and more than 250,000comments labeled as SATD, including both 'Easy to Find' and 'Hard to Find'SATD. We believe PENTACET data will further SATD research using ArtificialIntelligence techniques.</description><author>Murali Sridharan, Leevi Rantala, Mika Mäntylä</author><pubDate>Fri, 11 Aug 2023 14:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14029v2</guid></item><item><title>PDE Discovery for Soft Sensors Using Coupled Physics-Informed Neural Network with Akaike's Information Criterion</title><link>http://arxiv.org/abs/2308.06132v1</link><description>Soft sensors have been extensively used to monitor key variables usingeasy-to-measure variables and mathematical models. Partial differentialequations (PDEs) are model candidates for soft sensors in industrial processeswith spatiotemporal dependence. However, gaps often exist between idealizedPDEs and practical situations. Discovering proper structures of PDEs, includingthe differential operators and source terms, can remedy the gaps. To this end,a coupled physics-informed neural network with Akaike's criterion information(CPINN-AIC) is proposed for PDE discovery of soft sensors. First, CPINN isadopted for obtaining solutions and source terms satisfying PDEs. Then, wepropose a data-physics-hybrid loss function for training CPINN, in whichundetermined combinations of differential operators are involved. Consequently,AIC is used to discover the proper combination of differential operators.Finally, the artificial and practical datasets are used to verify thefeasibility and effectiveness of CPINN-AIC for soft sensors. The proposedCPINN-AIC is a data-driven method to discover proper PDE structures and neuralnetwork-based solutions for soft sensors.</description><author>Aina Wang, Pan Qin, Xi-Ming Sun</author><pubDate>Fri, 11 Aug 2023 14:39:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06132v1</guid></item><item><title>Faithful Knowledge Distillation</title><link>http://arxiv.org/abs/2306.04431v3</link><description>Knowledge distillation (KD) has received much attention due to its success incompressing networks to allow for their deployment in resource-constrainedsystems. While the problem of adversarial robustness has been studied before inthe KD setting, previous works overlook what we term the relative calibrationof the student network with respect to its teacher in terms of softconfidences. In particular, we focus on two crucial questions with regard to ateacher-student pair: (i) do the teacher and student disagree at points closeto correctly classified dataset examples, and (ii) is the distilled student asconfident as the teacher around dataset examples? These are critical questionswhen considering the deployment of a smaller student network trained from arobust teacher within a safety-critical setting. To address these questions, weintroduce a faithful imitation framework to discuss the relative calibration ofconfidences and provide empirical and certified methods to evaluate therelative calibration of a student w.r.t. its teacher. Further, to verifiablyalign the relative calibration incentives of the student to those of itsteacher, we introduce faithful distillation. Our experiments on the MNIST,Fashion-MNIST and CIFAR-10 datasets demonstrate the need for such an analysisand the advantages of the increased verifiability of faithful distillation overalternative adversarial distillation methods.</description><author>Tom A. Lamb, Rudy Brunel, Krishnamurthy DJ Dvijotham, M. Pawan Kumar, Philip H. S. Torr, Francisco Eiras</author><pubDate>Fri, 11 Aug 2023 14:39:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04431v3</guid></item><item><title>Heatmap-based Out-of-Distribution Detection</title><link>http://arxiv.org/abs/2211.08115v2</link><description>Our work investigates out-of-distribution (OOD) detection as a neural networkoutput explanation problem. We learn a heatmap representation for detecting OODimages while visualizing in- and out-of-distribution image regions at the sametime. Given a trained and fixed classifier, we train a decoder neural networkto produce heatmaps with zero response for in-distribution samples and highresponse heatmaps for OOD samples, based on the classifier features and theclass prediction. Our main innovation lies in the heatmap definition for an OODsample, as the normalized difference from the closest in-distribution sample.The heatmap serves as a margin to distinguish between in- andout-of-distribution samples. Our approach generates the heatmaps not only forOOD detection, but also to indicate in- and out-of-distribution regions of theinput image. In our evaluations, our approach mostly outperforms the prior workon fixed classifiers, trained on CIFAR-10, CIFAR-100 and Tiny ImageNet. Thecode is publicly available at: https://github.com/jhornauer/heatmap_ood.</description><author>Julia Hornauer, Vasileios Belagiannis</author><pubDate>Fri, 11 Aug 2023 14:36:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.08115v2</guid></item><item><title>Uncertainty Quantification for Image-based Traffic Prediction across Cities</title><link>http://arxiv.org/abs/2308.06129v1</link><description>Despite the strong predictive performance of deep learning models for trafficprediction, their widespread deployment in real-world intelligenttransportation systems has been restrained by a lack of interpretability.Uncertainty quantification (UQ) methods provide an approach to induceprobabilistic reasoning, improve decision-making and enhance model deploymentpotential. To gain a comprehensive picture of the usefulness of existing UQmethods for traffic prediction and the relation between obtained uncertaintiesand city-wide traffic dynamics, we investigate their application to alarge-scale image-based traffic dataset spanning multiple cities and timeperiods. We compare two epistemic and two aleatoric UQ methods on both temporaland spatio-temporal transfer tasks, and find that meaningful uncertaintyestimates can be recovered. We further demonstrate how uncertainty estimatescan be employed for unsupervised outlier detection on changes in city trafficdynamics. We find that our approach can capture both temporal and spatialeffects on traffic behaviour in a representative case study for the city ofMoscow. Our work presents a further step towards boosting uncertainty awarenessin traffic prediction tasks, and aims to highlight the value contribution of UQmethods to a better understanding of city traffic dynamics.</description><author>Alexander Timans, Nina Wiedemann, Nishant Kumar, Ye Hong, Martin Raubal</author><pubDate>Fri, 11 Aug 2023 14:35:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06129v1</guid></item><item><title>Learning Control Policies for Variable Objectives from Offline Data</title><link>http://arxiv.org/abs/2308.06127v1</link><description>Offline reinforcement learning provides a viable approach to obtain advancedcontrol strategies for dynamical systems, in particular when direct interactionwith the environment is not available. In this paper, we introduce a conceptualextension for model-based policy search methods, called variable objectivepolicy (VOP). With this approach, policies are trained to generalizeefficiently over a variety of objectives, which parameterize the rewardfunction. We demonstrate that by altering the objectives passed as input to thepolicy, users gain the freedom to adjust its behavior or re-balanceoptimization targets at runtime, without need for collecting additionalobservation batches or re-training.</description><author>Marc Weber, Phillip Swazinna, Daniel Hein, Steffen Udluft, Volkmar Sterzing</author><pubDate>Fri, 11 Aug 2023 14:33:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06127v1</guid></item><item><title>Improving Joint Speech-Text Representations Without Alignment</title><link>http://arxiv.org/abs/2308.06125v1</link><description>The last year has seen astonishing progress in text-prompted image generationpremised on the idea of a cross-modal representation space in which the textand image domains are represented jointly. In ASR, this idea has foundapplication as joint speech-text encoders that can scale to the capacities ofvery large parameter models by being trained on both unpaired speech and text.While these methods show promise, they have required special treatment of thesequence-length mismatch inherent in speech and text, either by up-samplingheuristics or an explicit alignment model. In this work, we offer evidence thatjoint speech-text encoders naturally achieve consistent representations acrossmodalities by disregarding sequence length, and argue that consistency lossescould forgive length differences and simply assume the best alignment. We showthat such a loss improves downstream WER in both a large-parameter monolingualand multilingual system.</description><author>Cal Peyser, Zhong Meng, Ke Hu, Rohit Prabhavalkar, Andrew Rosenberg, Tara N. Sainath, Michael Picheny, Kyunghyun Cho</author><pubDate>Fri, 11 Aug 2023 14:28:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06125v1</guid></item><item><title>Robust Graph Representation Learning for Local Corruption Recovery</title><link>http://arxiv.org/abs/2202.04936v4</link><description>The performance of graph representation learning is affected by the qualityof graph input. While existing research usually pursues a globally smoothedgraph embedding, we believe the rarely observed anomalies are as well harmfulto an accurate prediction. This work establishes a graph learning scheme thatautomatically detects (locally) corrupted feature attributes and recoversrobust embedding for prediction tasks. The detection operation leverages agraph autoencoder, which does not make any assumptions about the distributionof the local corruptions. It pinpoints the positions of the anomalous nodeattributes in an unbiased mask matrix, where robust estimations are recoveredwith sparsity promoting regularizer. The optimizer approaches a new embeddingthat is sparse in the framelet domain and conditionally close to inputobservations. Extensive experiments are provided to validate our proposed modelcan recover a robust graph representation from black-box poisoning and achieveexcellent performance.</description><author>Bingxin Zhou, Yuanhong Jiang, Yu Guang Wang, Jingwei Liang, Junbin Gao, Shirui Pan, Xiaoqun Zhang</author><pubDate>Fri, 11 Aug 2023 14:05:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.04936v4</guid></item><item><title>Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping</title><link>http://arxiv.org/abs/2308.06112v1</link><description>Visual Speech Recognition (VSR) differs from the common perception tasks asit requires deeper reasoning over the video sequence, even by human experts.Despite the recent advances in VSR, current approaches rely on labeled data tofully train or finetune their models predicting the target speech. This hinderstheir ability to generalize well beyond the training set and leads toperformance degeneration under out-of-distribution challenging scenarios.Unlike previous works that involve auxiliary losses or complex trainingprocedures and architectures, we propose a simple approach, named Lip2Vec thatis based on learning a prior model. Given a robust visual speech encoder, thisnetwork maps the encoded latent representations of the lip sequence to theircorresponding latents from the audio pair, which are sufficiently invariant foreffective text decoding. The generated audio representation is then decoded totext using an off-the-shelf Audio Speech Recognition (ASR) model. The proposedmodel compares favorably with fully-supervised learning methods on the LRS3dataset achieving 26 WER. Unlike SoTA approaches, our model keeps a reasonableperformance on the VoxCeleb test set. We believe that reprogramming the VSR asan ASR task narrows the performance gap between the two and paves the way formore flexible formulations of lip reading.</description><author>Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Haithem Boussaid, Ebtessam Almazrouei, Merouane Debbah</author><pubDate>Fri, 11 Aug 2023 13:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06112v1</guid></item><item><title>Towards Defending Multiple $\ell_p$-norm Bounded Adversarial Perturbations via Gated Batch Normalization</title><link>http://arxiv.org/abs/2012.01654v2</link><description>There has been extensive evidence demonstrating that deep neural networks arevulnerable to adversarial examples, which motivates the development of defensesagainst adversarial attacks. Existing adversarial defenses typically improvemodel robustness against individual specific perturbation types (\eg,$\ell_{\infty}$-norm bounded adversarial examples). However, adversaries arelikely to generate multiple types of perturbations in practice (\eg, $\ell_1$,$\ell_2$, and $\ell_{\infty}$ perturbations). Some recent methods improve modelrobustness against adversarial attacks in multiple $\ell_p$ balls, but theirperformance against each perturbation type is still far from satisfactory. Inthis paper, we observe that different $\ell_p$ bounded adversarialperturbations induce different statistical properties that can be separated andcharacterized by the statistics of Batch Normalization (BN). We thus proposeGated Batch Normalization (GBN) to adversarially train a perturbation-invariantpredictor for defending multiple $\ell_p$ bounded adversarial perturbations.GBN consists of a multi-branch BN layer and a gated sub-network. Each BN branchin GBN is in charge of one perturbation type to ensure that the normalizedoutput is aligned towards learning perturbation-invariant representation.Meanwhile, the gated sub-network is designed to separate inputs added withdifferent perturbation types. We perform an extensive evaluation of ourapproach on commonly-used dataset including MNIST, CIFAR-10, and Tiny-ImageNet,and demonstrate that GBN outperforms previous defense proposals againstmultiple perturbation types (\ie, $\ell_1$, $\ell_2$, and $\ell_{\infty}$perturbations) by large margins.</description><author>Aishan Liu, Shiyu Tang, Xinyun Chen, Lei Huang, Haotong Qin, Xianglong Liu, Dacheng Tao</author><pubDate>Fri, 11 Aug 2023 13:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.01654v2</guid></item><item><title>Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models</title><link>http://arxiv.org/abs/2308.06111v1</link><description>Auditing financial documents is a very tedious and time-consuming process. Asof today, it can already be simplified by employing AI-based solutions torecommend relevant text passages from a report for each legal requirement ofrigorous accounting standards. However, these methods need to be fine-tunedregularly, and they require abundant annotated data, which is often lacking inindustrial environments. Hence, we present ZeroShotALI, a novel recommendersystem that leverages a state-of-the-art large language model (LLM) inconjunction with a domain-specifically optimized transformer-basedtext-matching solution. We find that a two-step approach of first retrieving anumber of best matching document sections per legal requirement with a customBERT-based model and second filtering these selections using an LLM yieldssignificant performance improvements over existing approaches.</description><author>Lars Hillebrand, Armin Berger, Tobias Deußer, Tim Dilmaghani, Mohamed Khaled, Bernd Kliem, Rüdiger Loitz, Maren Pielka, David Leonhard, Christian Bauckhage, Rafet Sifa</author><pubDate>Fri, 11 Aug 2023 13:55:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06111v1</guid></item><item><title>Hawkes Processes with Delayed Granger Causality</title><link>http://arxiv.org/abs/2308.06106v1</link><description>We aim to explicitly model the delayed Granger causal effects based onmultivariate Hawkes processes. The idea is inspired by the fact that a causalevent usually takes some time to exert an effect. Studying this time lag itselfis of interest. Given the proposed model, we first prove the identifiability ofthe delay parameter under mild conditions. We further investigate a modelestimation method under a complex setting, where we want to infer the posteriordistribution of the time lags and understand how this distribution variesacross different scenarios. We treat the time lags as latent variables andformulate a Variational Auto-Encoder (VAE) algorithm to approximate theposterior distribution of the time lags. By explicitly modeling the time lagsin Hawkes processes, we add flexibility to the model. The inferred time-lagposterior distributions are of scientific meaning and help trace the originalcausal time that supports the root cause analysis. We empirically evaluate ourmodel's event prediction and time-lag inference accuracy on synthetic and realdata, achieving promising results.</description><author>Chao Yang, Hengyuan Miao, Shuang Li</author><pubDate>Fri, 11 Aug 2023 13:43:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06106v1</guid></item><item><title>Can Self-Supervised Representation Learning Methods Withstand Distribution Shifts and Corruptions?</title><link>http://arxiv.org/abs/2308.02525v2</link><description>Self-supervised learning in computer vision aims to leverage the inherentstructure and relationships within data to learn meaningful representationswithout explicit human annotation, enabling a holistic understanding of visualscenes. Robustness in vision machine learning ensures reliable and consistentperformance, enhancing generalization, adaptability, and resistance to noise,variations, and adversarial attacks. Self-supervised paradigms, namelycontrastive learning, knowledge distillation, mutual information maximization,and clustering, have been considered to have shown advances in invariantlearning representations. This work investigates the robustness of learnedrepresentations of self-supervised learning approaches focusing on distributionshifts and image corruptions in computer vision. Detailed experiments have beenconducted to study the robustness of self-supervised learning methods ondistribution shifts and image corruptions. The empirical analysis demonstratesa clear relationship between the performance of learned representations withinself-supervised paradigms and the severity of distribution shifts andcorruptions. Notably, higher levels of shifts and corruptions are found tosignificantly diminish the robustness of the learned representations. Thesefindings highlight the critical impact of distribution shifts and imagecorruptions on the performance and resilience of self-supervised learningmethods, emphasizing the need for effective strategies to mitigate theiradverse effects. The study strongly advocates for future research in the fieldof self-supervised representation learning to prioritize the key aspects ofsafety and robustness in order to ensure practical applicability. The sourcecode and results are available on GitHub.</description><author>Prakash Chandra Chhipa, Johan Rodahl Holmgren, Kanjar De, Rajkumar Saini, Marcus Liwicki</author><pubDate>Fri, 11 Aug 2023 13:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02525v2</guid></item><item><title>Improving the Transferability of Adversarial Examples via Direction Tuning</title><link>http://arxiv.org/abs/2303.15109v2</link><description>In the transfer-based adversarial attacks, adversarial examples are onlygenerated by the surrogate models and achieve effective perturbation in thevictim models. Although considerable efforts have been developed on improvingthe transferability of adversarial examples generated by transfer-basedadversarial attacks, our investigation found that, the big deviation betweenthe actual and steepest update directions of the current transfer-basedadversarial attacks is caused by the large update step length, resulting in thegenerated adversarial examples can not converge well. However, directlyreducing the update step length will lead to serious update oscillation so thatthe generated adversarial examples also can not achieve great transferabilityto the victim models. To address these issues, a novel transfer-based attack,namely direction tuning attack, is proposed to not only decrease the updatedeviation in the large step length, but also mitigate the update oscillation inthe small sampling step length, thereby making the generated adversarialexamples converge well to achieve great transferability on victim models. Inaddition, a network pruning method is proposed to smooth the decision boundary,thereby further decreasing the update oscillation and enhancing thetransferability of the generated adversarial examples. The experiment resultson ImageNet demonstrate that the average attack success rate (ASR) of theadversarial examples generated by our method can be improved from 87.9\% to94.5\% on five victim models without defenses, and from 69.1\% to 76.2\% oneight advanced defense methods, in comparison with that of latestgradient-based attacks.</description><author>Xiangyuan Yang, Jie Lin, Hanlin Zhang, Xinyu Yang, Peng Zhao</author><pubDate>Fri, 11 Aug 2023 13:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15109v2</guid></item><item><title>Composable Function-preserving Expansions for Transformer Architectures</title><link>http://arxiv.org/abs/2308.06103v1</link><description>Training state-of-the-art neural networks requires a high cost in terms ofcompute and time. Model scale is recognized to be a critical factor to achieveand improve the state-of-the-art. Increasing the scale of a neural networknormally requires restarting from scratch by randomly initializing all theparameters of the model, as this implies a change of architecture's parametersthat does not allow for a straightforward transfer of knowledge from smallersize models. In this work, we propose six composable transformations toincrementally increase the size of transformer-based neural networks whilepreserving functionality, allowing to expand the capacity of the model asneeded. We provide proof of exact function preservation under minimalinitialization constraints for each transformation. The proposed methods mayenable efficient training pipelines for larger and more powerful models byprogressively expanding the architecture throughout training.</description><author>Andrea Gesmundo, Kaitlin Maile</author><pubDate>Fri, 11 Aug 2023 13:27:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06103v1</guid></item><item><title>Taming the Power of Diffusion Models for High-Quality Virtual Try-On with Appearance Flow</title><link>http://arxiv.org/abs/2308.06101v1</link><description>Virtual try-on is a critical image synthesis task that aims to transferclothes from one image to another while preserving the details of both humansand clothes. While many existing methods rely on Generative AdversarialNetworks (GANs) to achieve this, flaws can still occur, particularly at highresolutions. Recently, the diffusion model has emerged as a promisingalternative for generating high-quality images in various applications.However, simply using clothes as a condition for guiding the diffusion model toinpaint is insufficient to maintain the details of the clothes. To overcomethis challenge, we propose an exemplar-based inpainting approach that leveragesa warping module to guide the diffusion model's generation effectively. Thewarping module performs initial processing on the clothes, which helps topreserve the local details of the clothes. We then combine the warped clotheswith clothes-agnostic person image and add noise as the input of diffusionmodel. Additionally, the warped clothes is used as local conditions for eachdenoising process to ensure that the resulting output retains as much detail aspossible. Our approach, namely Diffusion-based Conditional Inpainting forVirtual Try-ON (DCI-VTON), effectively utilizes the power of the diffusionmodel, and the incorporation of the warping module helps to producehigh-quality and realistic virtual try-on results. Experimental results onVITON-HD demonstrate the effectiveness and superiority of our method.</description><author>Junhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, Chen Qian, Liqing Zhang</author><pubDate>Fri, 11 Aug 2023 13:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06101v1</guid></item><item><title>Diffusion-based Visual Counterfactual Explanations -- Towards Systematic Quantitative Evaluation</title><link>http://arxiv.org/abs/2308.06100v1</link><description>Latest methods for visual counterfactual explanations (VCE) harness the powerof deep generative models to synthesize new examples of high-dimensional imagesof impressive quality. However, it is currently difficult to compare theperformance of these VCE methods as the evaluation procedures largely vary andoften boil down to visual inspection of individual examples and small scaleuser studies. In this work, we propose a framework for systematic, quantitativeevaluation of the VCE methods and a minimal set of metrics to be used. We usethis framework to explore the effects of certain crucial design choices in thelatest diffusion-based generative models for VCEs of natural imageclassification (ImageNet). We conduct a battery of ablation-like experiments,generating thousands of VCEs for a suite of classifiers of various complexity,accuracy and robustness. Our findings suggest multiple directions for futureadvancements and improvements of VCE methods. By sharing our methodology andour approach to tackle the computational challenges of such a study on alimited hardware setup (including the complete code base), we offer a valuableguidance for researchers in the field fostering consistency and transparency inthe assessment of counterfactual explanations.</description><author>Philipp Vaeth, Alexander M. Fruehwald, Benjamin Paassen, Magda Gregorova</author><pubDate>Fri, 11 Aug 2023 13:22:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06100v1</guid></item><item><title>On the Design Fundamentals of Diffusion Models: A Survey</title><link>http://arxiv.org/abs/2306.04542v2</link><description>Diffusion models are generative models, which gradually add and remove noiseto learn the underlying distribution of training data for data generation. Thecomponents of diffusion models have gained significant attention with manydesign choices proposed. Existing reviews have primarily focused onhigher-level solutions, thereby covering less on the design fundamentals ofcomponents. This study seeks to address this gap by providing a comprehensiveand coherent review on component-wise design choices in diffusion models.Specifically, we organize this review according to their three key components,namely the forward process, the reverse process, and the sampling procedure.This allows us to provide a fine-grained perspective of diffusion models,benefiting future studies in the analysis of individual components, theapplicability of design choices, and the implementation of diffusion models.</description><author>Ziyi Chang, George Alex Koulieris, Hubert P. H. Shum</author><pubDate>Fri, 11 Aug 2023 13:20:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04542v2</guid></item><item><title>Automated Construction of Time-Space Diagrams for Traffic Analysis Using Street-View Video Sequence</title><link>http://arxiv.org/abs/2308.06098v1</link><description>Time-space diagrams are essential tools for analyzing traffic patterns andoptimizing transportation infrastructure and traffic management strategies.Traditional data collection methods for these diagrams have limitations interms of temporal and spatial coverage. Recent advancements in cameratechnology have overcome these limitations and provided extensive urban data.In this study, we propose an innovative approach to constructing time-spacediagrams by utilizing street-view video sequences captured by cameras mountedon moving vehicles. Using the state-of-the-art YOLOv5, StrongSORT, andphotogrammetry techniques for distance calculation, we can infer vehicletrajectories from the video data and generate time-space diagrams. To evaluatethe effectiveness of our proposed method, we utilized datasets from the KITTIcomputer vision benchmark suite. The evaluation results demonstrate that ourapproach can generate trajectories from video data, although there are someerrors that can be mitigated by improving the performance of the detector,tracker, and distance calculation components. In conclusion, the utilization ofstreet-view video sequences captured by cameras mounted on moving vehicles,combined with state-of-the-art computer vision techniques, has immensepotential for constructing comprehensive time-space diagrams. These diagramsoffer valuable insights into traffic patterns and contribute to the design oftransportation infrastructure and traffic management strategies.</description><author>Tanay Rastogi, Mårten Björkman</author><pubDate>Fri, 11 Aug 2023 13:18:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06098v1</guid></item><item><title>RIGID: Recurrent GAN Inversion and Editing of Real Face Videos</title><link>http://arxiv.org/abs/2308.06097v1</link><description>GAN inversion is indispensable for applying the powerful editability of GANto real images. However, existing methods invert video frames individuallyoften leading to undesired inconsistent results over time. In this paper, wepropose a unified recurrent framework, named \textbf{R}ecurrent v\textbf{I}deo\textbf{G}AN \textbf{I}nversion and e\textbf{D}iting (RIGID), to explicitly andsimultaneously enforce temporally coherent GAN inversion and facial editing ofreal videos. Our approach models the temporal relations between current andprevious frames from three aspects. To enable a faithful real videoreconstruction, we first maximize the inversion fidelity and consistency bylearning a temporal compensated latent code. Second, we observe incoherentnoises lie in the high-frequency domain that can be disentangled from thelatent space. Third, to remove the inconsistency after attribute manipulation,we propose an \textit{in-between frame composition constraint} such that thearbitrary frame must be a direct composite of its neighboring frames. Ourunified framework learns the inherent coherence between input frames in anend-to-end manner, and therefore it is agnostic to a specific attribute and canbe applied to arbitrary editing of the same video without re-training.Extensive experiments demonstrate that RIGID outperforms state-of-the-artmethods qualitatively and quantitatively in both inversion and editing tasks.The deliverables can be found in \url{https://cnnlstm.github.io/RIGID}</description><author>Yangyang Xu, Shengfeng He, Kwan-Yee K. Wong, Ping Luo</author><pubDate>Fri, 11 Aug 2023 13:17:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06097v1</guid></item><item><title>Structured 2D Representation of 3D Data for Shape Processing</title><link>http://arxiv.org/abs/1903.10360v2</link><description>We represent 3D shape by structured 2D representations of fixed length makingit feasible to apply well investigated 2D convolutional neural networks (CNN)for both discriminative and geometric tasks on 3D shapes. We first provide ageneral introduction to such structured descriptors, analyze their differentforms and show how a simple 2D CNN can be used to achieve good classificationresult. With a specialized classification network for images and our structuredrepresentation, we achieve the classification accuracy of 99.7\% in theModelNet40 test set - improving the previous state-of-the-art by a largemargin. We finally provide a novel framework for performing the geometric taskof 3D segmentation using 2D CNNs and the structured representation - concludingthe utility of such descriptors for both discriminative and geometric tasks.</description><author>Kripasindhu Sarkar, Elizabeth Mathews, Didier Stricker</author><pubDate>Fri, 11 Aug 2023 13:08:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1903.10360v2</guid></item><item><title>3D-EX : A Unified Dataset of Definitions and Dictionary Examples</title><link>http://arxiv.org/abs/2308.03043v2</link><description>Definitions are a fundamental building block in lexicography, linguistics andcomputational semantics. In NLP, they have been used for retrofitting wordembeddings or augmenting contextual representations in language models.However, lexical resources containing definitions exhibit a wide range ofproperties, which has implications in the behaviour of models trained andevaluated on them. In this paper, we introduce 3D- EX , a dataset that aims tofill this gap by combining well-known English resources into one centralizedknowledge repository in the form of &lt;term, definition, example&gt; triples. 3D- EXis a unified evaluation framework with carefully pre-computedtrain/validation/test splits to prevent memorization. We report experimentalresults that suggest that this dataset could be effectively leveraged indownstream NLP tasks. Code and data are available athttps://github.com/F-Almeman/3D-EX .</description><author>Fatemah Almeman, Hadi Sheikhi, Luis Espinosa-Anke</author><pubDate>Fri, 11 Aug 2023 13:07:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03043v2</guid></item><item><title>Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes</title><link>http://arxiv.org/abs/2308.06095v1</link><description>Recent conditional language models are able to continue any kind of textsource in an often seemingly fluent way. This fact encouraged research in thearea of open-domain conversational systems that are based on powerful languagemodels and aim to imitate an interlocutor by generating appropriatecontributions to a written dialogue. From a linguistic perspective, however,the complexity of contributing to a conversation is high. In this survey, weinterpret Grice's maxims of cooperative conversation from the perspective ofthis specific research area and systematize the literature under the aspect ofwhat makes a contribution appropriate: A neural conversation model has to befluent, informative, consistent, coherent, and follow social norms. In order toensure these qualities, recent approaches try to tame the underlying languagemodels at various intervention points, such as data, training regime ordecoding. Sorted by these categories and intervention points, we discusspromising attempts and suggest novel ways for future research.</description><author>Fabian Galetzka, Anne Beyer, David Schlangen</author><pubDate>Fri, 11 Aug 2023 13:07:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06095v1</guid></item><item><title>Reinforcement Logic Rule Learning for Temporal Point Processes</title><link>http://arxiv.org/abs/2308.06094v1</link><description>We propose a framework that can incrementally expand the explanatory temporallogic rule set to explain the occurrence of temporal events. Leveraging thetemporal point process modeling and learning framework, the rule content andweights will be gradually optimized until the likelihood of the observationalevent sequences is optimal. The proposed algorithm alternates between a masterproblem, where the current rule set weights are updated, and a subproblem,where a new rule is searched and included to best increase the likelihood. Theformulated master problem is convex and relatively easy to solve usingcontinuous optimization, whereas the subproblem requires searching the hugecombinatorial rule predicate and relationship space. To tackle this challenge,we propose a neural search policy to learn to generate the new rule content asa sequence of actions. The policy parameters will be trained end-to-end usingthe reinforcement learning framework, where the reward signals can beefficiently queried by evaluating the subproblem objective. The trained policycan be used to generate new rules in a controllable way. We evaluate ourmethods on both synthetic and real healthcare datasets, obtaining promisingresults.</description><author>Chao Yang, Lu Wang, Kun Gao, Shuang Li</author><pubDate>Fri, 11 Aug 2023 13:05:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06094v1</guid></item><item><title>Experts Weights Averaging: A New General Training Scheme for Vision Transformers</title><link>http://arxiv.org/abs/2308.06093v1</link><description>Structural re-parameterization is a general training scheme for ConvolutionalNeural Networks (CNNs), which achieves performance improvement withoutincreasing inference cost. As Vision Transformers (ViTs) are graduallysurpassing CNNs in various visual tasks, one may question: if a training schemespecifically for ViTs exists that can also achieve performance improvementwithout increasing inference cost? Recently, Mixture-of-Experts (MoE) hasattracted increasing attention, as it can efficiently scale up the capacity ofTransformers at a fixed cost through sparsely activated experts. Consideringthat MoE can also be viewed as a multi-branch structure, can we utilize MoE toimplement a ViT training scheme similar to structural re-parameterization? Inthis paper, we affirmatively answer these questions, with a new generaltraining strategy for ViTs. Specifically, we decouple the training andinference phases of ViTs. During training, we replace some Feed-ForwardNetworks (FFNs) of the ViT with specially designed, more efficient MoEs thatassign tokens to experts by random uniform partition, and perform ExpertsWeights Averaging (EWA) on these MoEs at the end of each iteration. Aftertraining, we convert each MoE into an FFN by averaging the experts,transforming the model back into original ViT for inference. We further providea theoretical analysis to show why and how it works. Comprehensive experimentsacross various 2D and 3D visual tasks, ViT architectures, and datasets validatethe effectiveness and generalizability of the proposed training scheme.Besides, our training scheme can also be applied to improve performance whenfine-tuning ViTs. Lastly, but equally important, the proposed EWA technique cansignificantly improve the effectiveness of naive MoE in various 2D visual smalldatasets and 3D visual tasks.</description><author>Yongqi Huang, Peng Ye, Xiaoshui Huang, Sheng Li, Tao Chen, Wanli Ouyang</author><pubDate>Fri, 11 Aug 2023 13:05:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06093v1</guid></item><item><title>Toward a Better Understanding of Loss Functions for Collaborative Filtering</title><link>http://arxiv.org/abs/2308.06091v1</link><description>Collaborative filtering (CF) is a pivotal technique in modern recommendersystems. The learning process of CF models typically consists of threecomponents: interaction encoder, loss function, and negative sampling. Althoughmany existing studies have proposed various CF models to design sophisticatedinteraction encoders, recent work shows that simply reformulating the lossfunctions can achieve significant performance gains. This paper delves intoanalyzing the relationship among existing loss functions. Our mathematicalanalysis reveals that the previous loss functions can be interpreted asalignment and uniformity functions: (i) the alignment matches user and itemrepresentations, and (ii) the uniformity disperses user and item distributions.Inspired by this analysis, we propose a novel loss function that improves thedesign of alignment and uniformity considering the unique patterns of datasetscalled Margin-aware Alignment and Weighted Uniformity (MAWU). The key noveltyof MAWU is two-fold: (i) margin-aware alignment (MA) mitigatesuser/item-specific popularity biases, and (ii) weighted uniformity (WU) adjuststhe significance between user and item uniformities to reflect the inherentcharacteristics of datasets. Extensive experimental results show that MF andLightGCN equipped with MAWU are comparable or superior to state-of-the-art CFmodels with various loss functions on three public datasets.</description><author>Seongmin Park, Mincheol Yoon, Jae-woong Lee, Hogun Park, Jongwuk Lee</author><pubDate>Fri, 11 Aug 2023 13:04:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06091v1</guid></item><item><title>An Autoethnographic Exploration of XAI in Algorithmic Composition</title><link>http://arxiv.org/abs/2308.06089v1</link><description>Machine Learning models are capable of generating complex music across arange of genres from folk to classical music. However, current generative musicAI models are typically difficult to understand and control in meaningful ways.Whilst research has started to explore how explainable AI (XAI) generativemodels might be created for music, no generative XAI models have been studiedin music making practice. This paper introduces an autoethnographic study ofthe use of the MeasureVAE generative music XAI model with interpretable latentdimensions trained on Irish folk music. Findings suggest that the exploratorynature of the music-making workflow foregrounds musical features of thetraining dataset rather than features of the generative model itself. Theappropriation of an XAI model within an iterative workflow highlights thepotential of XAI models to form part of a richer and more complex workflow thanthey were initially designed for.</description><author>Ashley Noel-Hirst, Nick Bryan-Kinns</author><pubDate>Fri, 11 Aug 2023 13:03:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06089v1</guid></item><item><title>Assessing Student Errors in Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters</title><link>http://arxiv.org/abs/2308.06088v1</link><description>Identifying logical errors in complex, incomplete or even contradictory andoverall heterogeneous data like students' experimentation protocols ischallenging. Recognizing the limitations of current evaluation methods, weinvestigate the potential of Large Language Models (LLMs) for automaticallyidentifying student errors and streamlining teacher assessments. Our aim is toprovide a foundation for productive, personalized feedback. Using a dataset of65 student protocols, an Artificial Intelligence (AI) system based on theGPT-3.5 and GPT-4 series was developed and tested against human raters. Ourresults indicate varying levels of accuracy in error detection between the AIsystem and human raters. The AI system can accurately identify many fundamentalstudent errors, for instance, the AI system identifies when a student isfocusing the hypothesis not on the dependent variable but solely on an expectedobservation (acc. = 0.90), when a student modifies the trials in an ongoinginvestigation (acc. = 1), and whether a student is conducting valid test trials(acc. = 0.82) reliably. The identification of other, usually more complexerrors, like whether a student conducts a valid control trial (acc. = .60),poses a greater challenge. This research explores not only the utility of AI ineducational settings, but also contributes to the understanding of thecapabilities of LLMs in error detection in inquiry-based learning likeexperimentation.</description><author>Arne Bewersdorff, Kathrin Seßler, Armin Baur, Enkelejda Kasneci, Claudia Nerdel</author><pubDate>Fri, 11 Aug 2023 13:03:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06088v1</guid></item><item><title>Pretraining Respiratory Sound Representations using Metadata and Contrastive Learning</title><link>http://arxiv.org/abs/2210.16192v3</link><description>Methods based on supervised learning using annotations in an end-to-endfashion have been the state-of-the-art for classification problems. However,they may be limited in their generalization capability, especially in the lowdata regime. In this study, we address this issue using supervised contrastivelearning combined with available metadata to solve multiple pretext tasks thatlearn a good representation of data. We apply our approach on respiratory soundclassification. This task is suited for this setting as demographic informationsuch as sex and age are correlated with presence of lung diseases, and learninga system that implicitly encode this information may better detect anomalies.Supervised contrastive learning is a paradigm that learns similarrepresentations to samples sharing the same class labels and dissimilarrepresentations to samples with different class labels. The feature extractorlearned using this paradigm extract useful features from the data, and we showthat it outperforms cross-entropy in classifying respiratory anomalies in twodifferent datasets. We also show that learning representations using onlymetadata, without class labels, obtains similar performance as using crossentropy with those labels only. In addition, when combining class labels withmetadata using multiple supervised contrastive learning, an extension ofsupervised contrastive learning solving an additional task of grouping patientswithin the same sex and age group, more informative features are learned. Thiswork suggests the potential of using multiple metadata sources in supervisedcontrastive settings, in particular in settings with class imbalance and fewdata. Our code is released at https://github.com/ilyassmoummad/scl_icbhi2017</description><author>Ilyass Moummad, Nicolas Farrugia</author><pubDate>Fri, 11 Aug 2023 12:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16192v3</guid></item><item><title>UNAEN: Unsupervised Abnormality Extraction Network for MRI Motion Artifact Reduction</title><link>http://arxiv.org/abs/2301.01732v4</link><description>Motion artifacts compromise the quality of magnetic resonance imaging (MRI)and pose challenges to achieving diagnostic outcomes and image-guidedtherapies. In recent years, supervised deep learning approaches have emerged assuccessful solutions for motion artifact reduction (MAR). One disadvantage ofthese methods is their dependency on acquiring paired sets of motionartifact-corrupted (MA-corrupted) and motion artifact-free (MA-free) MR imagesfor training purposes. Obtaining such image pairs is difficult and thereforelimits the application of supervised training. In this paper, we propose anovel UNsupervised Abnormality Extraction Network (UNAEN) to alleviate thisproblem. Our network is capable of working with unpaired MA-corrupted andMA-free images. It converts the MA-corrupted images to MA-reduced images byextracting abnormalities from the MA-corrupted images using a proposed artifactextractor, which intercepts the residual artifact maps from the MA-corrupted MRimages explicitly, and a reconstructor to restore the original input from theMA-reduced images. The performance of UNAEN was assessed by experimenting onvarious publicly available MRI datasets and comparing them withstate-of-the-art methods. The quantitative evaluation demonstrates thesuperiority of UNAEN over alternative MAR methods and visually exhibits fewerresidual artifacts. Our results substantiate the potential of UNAEN as apromising solution applicable in real-world clinical environments, with thecapability to enhance diagnostic accuracy and facilitate image-guidedtherapies.</description><author>Yusheng Zhou, Hao Li, Jianan Liu, Zhengmin Kong, Tao Huang, Euijoon Ahn, Zhihan Lv, Jinman Kim, David Dagan Feng</author><pubDate>Fri, 11 Aug 2023 12:55:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.01732v4</guid></item><item><title>Graph Neural Network Sensitivity Under Probabilistic Error Model</title><link>http://arxiv.org/abs/2203.07831v3</link><description>Graph convolutional networks (GCNs) can successfully learn the graph signalrepresentation by graph convolution. The graph convolution depends on the graphfilter, which contains the topological dependency of data and propagates datafeatures. However, the estimation errors in the propagation matrix (e.g., theadjacency matrix) can have a significant impact on graph filters and GCNs. Inthis paper, we study the effect of a probabilistic graph error model on theperformance of the GCNs. We prove that the adjacency matrix under the errormodel is bounded by a function of graph size and error probability. We furtheranalytically specify the upper bound of a normalized adjacency matrix withself-loop added. Finally, we illustrate the error bounds by running experimentson a synthetic dataset and study the sensitivity of a simple GCN under thisprobabilistic error model on accuracy.</description><author>Xinjue Wang, Esa Ollila, Sergiy A. Vorobyov</author><pubDate>Fri, 11 Aug 2023 12:50:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.07831v3</guid></item><item><title>Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling</title><link>http://arxiv.org/abs/2308.06077v1</link><description>Generative language models (LMs) have become omnipresent across data science.For a wide variety of tasks, inputs can be phrased as natural language promptsfor an LM, from whose output the solution can then be extracted. LM performancehas consistently been increasing with model size - but so has the monetary costof querying the ever larger models. Importantly, however, not all inputs areequally hard: some require larger LMs for obtaining a satisfactory solution,whereas for others smaller LMs suffice. Based on this fact, we design aframework for Cost-Effective Language Model Choice (CELMOC). Given a set ofinputs and a set of candidate LMs, CELMOC judiciously assigns each input to anLM predicted to do well on the input according to a so-called meta-model,aiming to achieve high overall performance at low cost. The cost-performancetrade-off can be flexibly tuned by the user. Options include, among others,maximizing total expected performance (or the number of processed inputs) whilestaying within a given cost budget, or minimizing total cost while processingall inputs. We evaluate CELMOC on 14 datasets covering five natural languagetasks, using four candidate LMs of vastly different size and cost. With CELMOC,we match the performance of the largest available LM while achieving a costreduction of 63%. Via our publicly available library, researchers as well aspractitioners can thus save large amounts of money without sacrificingperformance.</description><author>Marija Šakota, Maxime Peyrard, Robert West</author><pubDate>Fri, 11 Aug 2023 12:29:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06077v1</guid></item><item><title>Versatile Face Animator: Driving Arbitrary 3D Facial Avatar in RGBD Space</title><link>http://arxiv.org/abs/2308.06076v1</link><description>Creating realistic 3D facial animation is crucial for various applications inthe movie production and gaming industry, especially with the burgeoning demandin the metaverse. However, prevalent methods such as blendshape-basedapproaches and facial rigging techniques are time-consuming, labor-intensive,and lack standardized configurations, making facial animation productionchallenging and costly. In this paper, we propose a novel self-supervisedframework, Versatile Face Animator, which combines facial motion capture withmotion retargeting in an end-to-end manner, eliminating the need forblendshapes or rigs. Our method has the following two main characteristics: 1)we propose an RGBD animation module to learn facial motion from raw RGBD videosby hierarchical motion dictionaries and animate RGBD images rendered from 3Dfacial mesh coarse-to-fine, enabling facial animation on arbitrary 3Dcharacters regardless of their topology, textures, blendshapes, and rigs; and2) we introduce a mesh retarget module to utilize RGBD animation to create 3Dfacial animation by manipulating facial mesh with controller transformations,which are estimated from dense optical flow fields and blended together withgeodesic-distance-based weights. Comprehensive experiments demonstrate theeffectiveness of our proposed framework in generating impressive 3D facialanimation results, highlighting its potential as a promising solution for thecost-effective and efficient production of facial animation in the metaverse.</description><author>Haoyu Wang, Haozhe Wu, Junliang Xing, Jia Jia</author><pubDate>Fri, 11 Aug 2023 12:29:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06076v1</guid></item><item><title>Transformers are Short Text Classifiers: A Study of Inductive Short Text Classifiers on Benchmarks and Real-world Datasets</title><link>http://arxiv.org/abs/2211.16878v3</link><description>Short text classification is a crucial and challenging aspect of NaturalLanguage Processing. For this reason, there are numerous highly specializedshort text classifiers. However, in recent short text research, State of theArt (SOTA) methods for traditional text classification, particularly the pureuse of Transformers, have been unexploited. In this work, we examine theperformance of a variety of short text classifiers as well as the topperforming traditional text classifier. We further investigate the effects ontwo new real-world short text datasets in an effort to address the issue ofbecoming overly dependent on benchmark datasets with a limited number ofcharacteristics. Our experiments unambiguously demonstrate that Transformersachieve SOTA accuracy on short text classification tasks, raising the questionof whether specialized short text techniques are necessary.</description><author>Fabian Karl, Ansgar Scherp</author><pubDate>Fri, 11 Aug 2023 12:25:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16878v3</guid></item><item><title>Out-of-Distribution Detection for Monocular Depth Estimation</title><link>http://arxiv.org/abs/2308.06072v1</link><description>In monocular depth estimation, uncertainty estimation approaches mainlytarget the data uncertainty introduced by image noise. In contrast to priorwork, we address the uncertainty due to lack of knowledge, which is relevantfor the detection of data not represented by the training distribution, theso-called out-of-distribution (OOD) data. Motivated by anomaly detection, wepropose to detect OOD images from an encoder-decoder depth estimation modelbased on the reconstruction error. Given the features extracted with the fixeddepth encoder, we train an image decoder for image reconstruction using onlyin-distribution data. Consequently, OOD images result in a high reconstructionerror, which we use to distinguish between in- and out-of-distribution samples.We built our experiments on the standard NYU Depth V2 and KITTI benchmarks asin-distribution data. Our post hoc method performs astonishingly well ondifferent models and outperforms existing uncertainty estimation approacheswithout modifying the trained encoder-decoder depth estimation model.</description><author>Julia Hornauer, Adrian Holzbock, Vasileios Belagiannis</author><pubDate>Fri, 11 Aug 2023 12:25:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06072v1</guid></item><item><title>Towards Unified Text-based Person Retrieval: A Large-scale Multi-Attribute and Language Search Benchmark</title><link>http://arxiv.org/abs/2306.02898v3</link><description>In this paper, we introduce a large Multi-Attribute and Language Searchdataset for text-based person retrieval, called MALS, and explore thefeasibility of performing pre-training on both attribute recognition andimage-text matching tasks in one stone. In particular, MALS contains 1,510,330image-text pairs, which is about 37.5 times larger than prevailing CUHK-PEDES,and all images are annotated with 27 attributes. Considering the privacyconcerns and annotation costs, we leverage the off-the-shelf diffusion modelsto generate the dataset. To verify the feasibility of learning from thegenerated data, we develop a new joint Attribute Prompt Learning and TextMatching Learning (APTM) framework, considering the shared knowledge betweenattribute and text. As the name implies, APTM contains an attribute promptlearning stream and a text matching learning stream. (1) The attribute promptlearning leverages the attribute prompts for image-attribute alignment, whichenhances the text matching learning. (2) The text matching learning facilitatesthe representation learning on fine-grained details, and in turn, boosts theattribute prompt learning. Extensive experiments validate the effectiveness ofthe pre-training on MALS, achieving state-of-the-art retrieval performance viaAPTM on three challenging real-world benchmarks. In particular, APTM achieves aconsistent improvement of +6.96%, +7.68%, and +16.95% Recall@1 accuracy onCUHK-PEDES, ICFG-PEDES, and RSTPReid datasets by a clear margin, respectively.</description><author>Shuyu Yang, Yinan Zhou, Yaxiong Wang, Yujiao Wu, Li Zhu, Zhedong Zheng</author><pubDate>Fri, 11 Aug 2023 12:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02898v3</guid></item><item><title>Safeguarding Learning-based Control for Smart Energy Systems with Sampling Specifications</title><link>http://arxiv.org/abs/2308.06069v1</link><description>We study challenges using reinforcement learning in controlling energysystems, where apart from performance requirements, one has additional safetyrequirements such as avoiding blackouts. We detail how these safetyrequirements in real-time temporal logic can be strengthened via discretizationinto linear temporal logic (LTL), such that the satisfaction of the LTLformulae implies the satisfaction of the original safety requirements. Thediscretization enables advanced engineering methods such as synthesizingshields for safe reinforcement learning as well as formal verification, wherefor statistical model checking, the probabilistic guarantee acquired by LTLmodel checking forms a lower bound for the satisfaction of the originalreal-time safety requirements.</description><author>Chih-Hong Cheng, Venkatesh Prasad Venkataramanan, Pragya Kirti Gupta, Yun-Fei Hsu, Simon Burton</author><pubDate>Fri, 11 Aug 2023 12:09:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06069v1</guid></item><item><title>HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection</title><link>http://arxiv.org/abs/2206.15157v3</link><description>Besides standard cameras, autonomous vehicles typically include multipleadditional sensors, such as lidars and radars, which help acquire richerinformation for perceiving the content of the driving scene. While severalrecent works focus on fusing certain pairs of sensors - such as camera withlidar or radar - by using architectural components specific to the examinedsetting, a generic and modular sensor fusion architecture is missing from theliterature. In this work, we propose HRFuser, a modular architecture formulti-modal 2D object detection. It fuses multiple sensors in amulti-resolution fashion and scales to an arbitrary number of input modalities.The design of HRFuser is based on state-of-the-art high-resolution networks forimage-only dense prediction and incorporates a novel multi-windowcross-attention block as the means to perform fusion of multiple modalities atmultiple resolutions. We demonstrate via extensive experiments on nuScenes andthe adverse conditions DENSE datasets that our model effectively leveragescomplementary features from additional modalities, substantially improving uponcamera-only performance and consistently outperforming state-of-the-art 3D and2D fusion methods evaluated on 2D object detection metrics. The source code ispublicly available.</description><author>Tim Broedermann, Christos Sakaridis, Dengxin Dai, Luc Van Gool</author><pubDate>Fri, 11 Aug 2023 12:06:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.15157v3</guid></item></channel></rss>