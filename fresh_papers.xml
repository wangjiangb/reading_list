<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 19 May 2023 06:00:40 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model</title><link>http://arxiv.org/abs/2305.11176v1</link><description>Foundation models have made significant strides in various applications,including text-to-image generation, panoptic segmentation, and natural languageprocessing. This paper presents Instruct2Act, a framework that utilizes LargeLanguage Models to map multi-modal instructions to sequential actions forrobotic manipulation tasks. Specifically, Instruct2Act employs the LLM model togenerate Python programs that constitute a comprehensive perception, planning,and action loop for robotic tasks. In the perception section, pre-defined APIsare used to access multiple foundation models where the Segment Anything Model(SAM) accurately locates candidate objects, and CLIP classifies them. In thisway, the framework leverages the expertise of foundation models and roboticabilities to convert complex high-level instructions into precise policy codes.Our approach is adjustable and flexible in accommodating various instructionmodalities and input types and catering to specific task demands. We validatedthe practicality and efficiency of our approach by assessing it on robotictasks in different scenarios within tabletop manipulation domains. Furthermore,our zero-shot method outperformed many state-of-the-art learning-based policiesin several tasks. The code for our proposed approach is available athttps://github.com/OpenGVLab/Instruct2Act, serving as a robust benchmark forhigh-level robotic instruction tasks with assorted modality inputs.</description><author>Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, Hongsheng Li</author><pubDate>Thu, 18 May 2023 18:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11176v1</guid></item><item><title>VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks</title><link>http://arxiv.org/abs/2305.11175v1</link><description>Large language models (LLMs) have notably accelerated progress towardsartificial general intelligence (AGI), with their impressive zero-shot capacityfor user-tailored tasks, endowing them with immense potential across a range ofapplications. However, in the field of computer vision, despite theavailability of numerous powerful vision foundation models (VFMs), they arestill restricted to tasks in a pre-defined form, struggling to match theopen-ended task capabilities of LLMs. In this work, we present an LLM-basedframework for vision-centric tasks, termed VisionLLM. This framework provides aunified perspective for vision and language tasks by treating images as aforeign language and aligning vision-centric tasks with language tasks that canbe flexibly defined and managed using language instructions. An LLM-baseddecoder can then make appropriate predictions based on these instructions foropen-ended tasks. Extensive experiments show that the proposed VisionLLM canachieve different levels of task customization through language instructions,from fine-grained object-level to coarse-grained task-level customization, allwith good results. It's noteworthy that, with a generalist LLM-based framework,our model can achieve over 60\% mAP on COCO, on par with detection-specificmodels. We hope this model can set a new baseline for generalist vision andlanguage models. The demo shall be released based onhttps://github.com/OpenGVLab/InternGPT. The code shall be released athttps://github.com/OpenGVLab/VisionLLM.</description><author>Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai</author><pubDate>Thu, 18 May 2023 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11175v1</guid></item><item><title>Going Denser with Open-Vocabulary Part Segmentation</title><link>http://arxiv.org/abs/2305.11173v1</link><description>Object detection has been expanded from a limited number of categories toopen vocabulary. Moving forward, a complete intelligent vision system requiresunderstanding more fine-grained object descriptions, object parts. In thispaper, we propose a detector with the ability to predict both open-vocabularyobjects and their part segmentation. This ability comes from two designs.First, we train the detector on the joint of part-level, object-level andimage-level data to build the multi-granularity alignment between language andimage. Second, we parse the novel object into its parts by its dense semanticcorrespondence with the base object. These two designs enable the detector tolargely benefit from various data sources and foundation models. Inopen-vocabulary part segmentation experiments, our method outperforms thebaseline by 3.3$\sim$7.3 mAP in cross-dataset generalization on PartImageNet,and improves the baseline by 7.3 novel AP$_{50}$ in cross-categorygeneralization on Pascal Part. Finally, we train a detector that generalizes toa wide range of part segmentation datasets while achieving better performancethan dataset-specific training.</description><author>Peize Sun, Shoufa Chen, Chenchen Zhu, Fanyi Xiao, Ping Luo, Saining Xie, Zhicheng Yan</author><pubDate>Thu, 18 May 2023 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11173v1</guid></item><item><title>ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities</title><link>http://arxiv.org/abs/2305.11172v1</link><description>In this work, we explore a scalable way for building a general representationmodel toward unlimited modalities. We release ONE-PEACE, a highly extensiblemodel with 4B parameters that can seamlessly align and integraterepresentations across vision, audio, and language modalities. The architectureof ONE-PEACE comprises modality adapters, shared self-attention layers, andmodality FFNs. This design allows for the easy extension of new modalities byadding adapters and FFNs, while also enabling multi-modal fusion throughself-attention layers. To pretrain ONE-PEACE, we develop two modality-agnosticpretraining tasks, cross-modal aligning contrast and intra-modal denoisingcontrast, which align the semantic space of different modalities and capturefine-grained details within modalities concurrently. With the scaling-friendlyarchitecture and pretraining tasks, ONE-PEACE has the potential to expand tounlimited modalities. Without using any vision or language pretrained model forinitialization, ONE-PEACE achieves leading results on a wide range of uni-modaland multi-modal tasks, including image classification (ImageNet), semanticsegmentation (ADE20K), audio-text retrieval (AudioCaps, Clotho), audioclassification (ESC-50, FSD50K, VGGSound), audio question answering (AVQA),image-text retrieval (MSCOCO, Flickr30K), and visual grounding (RefCOCO/+/g).Code is available at https://github.com/OFA-Sys/ONE-PEACE.</description><author>Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, Chang Zhou</author><pubDate>Thu, 18 May 2023 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11172v1</guid></item><item><title>TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models</title><link>http://arxiv.org/abs/2305.11171v1</link><description>Factual consistency evaluation is often conducted using Natural LanguageInference (NLI) models, yet these models exhibit limited success in evaluatingsummaries. Previous work improved such models with synthetic training data.However, the data is typically based on perturbed human-written summaries,which often differ in their characteristics from real model-generated summariesand have limited coverage of possible factual errors. Alternatively, largelanguage models (LLMs) have recently shown promising results in directlyevaluating generative tasks, but are too computationally expensive forpractical use. Motivated by these limitations, we introduce TrueTeacher, amethod for generating synthetic data by annotating diverse model-generatedsummaries using a LLM. Unlike prior work, TrueTeacher does not rely onhuman-written summaries, and is multilingual by nature. Experiments on the TRUEbenchmark show that a student model trained using our data, substantiallyoutperforms both the state-of-the-art model with similar capacity, and the LLMteacher. In a systematic study, we compare TrueTeacher to existing syntheticdata generation methods and demonstrate its superiority and robustness todomain-shift. Using the the mFACE dataset, we also show that our methodgeneralizes to multilingual scenarios. Finally, we release a large-scalesynthetic dataset with 1.4M examples generated using TrueTeacher.</description><author>Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, Idan Szpektor</author><pubDate>Thu, 18 May 2023 18:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11171v1</guid></item><item><title>Efficient Prompting via Dynamic In-Context Learning</title><link>http://arxiv.org/abs/2305.11170v1</link><description>The primary way of building AI applications is shifting from trainingspecialist models to prompting generalist models. A common practice forprompting generalist models, often referred to as in-context learning, is toappend a few examples (demonstrations) to the prompt to help the model betterunderstand the task. While effective, in-context learning can be inefficientbecause it makes the input prompt much longer, consuming valuable space in thecontext window and leading to larger computational costs. In this paper, wepropose DynaICL, a recipe for efficient prompting with black-box generalistmodels that dynamically allocate in-context examples according to the inputcomplexity and the computational budget. To achieve this, we train a metacontroller that predicts the number of in-context examples suitable for thegeneralist model to make a good prediction based on the performance-efficiencytrade-off for a specific input. We then dynamically allocate the number ofdemonstrations for an input according to predictions from the meta controllerand the given computation budget. Experimental results show that dynamicexample allocation helps achieve a better performance-efficiency trade-off intwo practical settings where computational resources or the requiredperformance is constrained. Specifically, DynaICL saves up to 46% token budgetcompared to the common practice that allocates the same number of in-contextexamples to each input. We also find that a meta controller trained on acertain backbone model and tasks can successfully generalize to unseen modelsand tasks.</description><author>Wangchunshu Zhou, Yuchen Eleanor Jiang, Ryan Cotterell, Mrinmaya Sachan</author><pubDate>Thu, 18 May 2023 18:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11170v1</guid></item><item><title>Evidence of Meaning in Language Models Trained on Programs</title><link>http://arxiv.org/abs/2305.11169v1</link><description>We present evidence that language models can learn meaning despite beingtrained only to perform next token prediction on text, specifically a corpus ofprograms. Each program is preceded by a specification in the form of (textual)input-output examples. Working with programs enables us to precisely defineconcepts relevant to meaning in language (e.g., correctness and semantics),making program synthesis well-suited as an intermediate testbed forcharacterizing the presence (or absence) of meaning in language models. We first train a Transformer model on the corpus of programs, then probe thetrained model's hidden states as it completes a program given a specification.Despite providing no inductive bias toward learning the semantics of thelanguage, we find that a linear probe is able to extract abstractions of bothcurrent and future program states from the model states. Moreover, there is astrong, statistically significant correlation between the accuracy of the probeand the model's ability to generate a program that implements thespecification. To evaluate whether the semantics are represented in the modelstates rather than learned by the probe, we design a novel experimentalprocedure that intervenes on the semantics of the language while preserving thelexicon and syntax. We also demonstrate that the model learns to generatecorrect programs that are, on average, shorter than those in the training set,which is evidence that language model outputs may differ from the trainingdistribution in semantically meaningful ways. In summary, this paper does notpropose any new techniques for training language models, but develops anexperimental framework for and provides insights into the acquisition andrepresentation of (formal) meaning in language models.</description><author>Charles Jin, Martin Rinard</author><pubDate>Thu, 18 May 2023 18:58:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11169v1</guid></item><item><title>MVPSNet: Fast Generalizable Multi-view Photometric Stereo</title><link>http://arxiv.org/abs/2305.11167v1</link><description>We propose a fast and generalizable solution to Multi-view Photometric Stereo(MVPS), called MVPSNet. The key to our approach is a feature extraction networkthat effectively combines images from the same view captured under multiplelighting conditions to extract geometric features from shading cues for stereomatching. We demonstrate these features, termed `Light Aggregated Feature Maps'(LAFM), are effective for feature matching even in textureless regions, wheretraditional multi-view stereo methods fail. Our method produces similarreconstruction results to PS-NeRF, a state-of-the-art MVPS method thatoptimizes a neural network per-scene, while being 411$\times$ faster (105seconds vs. 12 hours) in inference. Additionally, we introduce a new syntheticdataset for MVPS, sMVPS, which is shown to be effective to train ageneralizable MVPS method.</description><author>Dongxu Zhao, Daniel Lichy, Pierre-Nicolas Perrin, Jan-Michael Frahm, Soumyadip Sengupta</author><pubDate>Thu, 18 May 2023 18:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11167v1</guid></item><item><title>The noise level in linear regression with dependent data</title><link>http://arxiv.org/abs/2305.11165v1</link><description>We derive upper bounds for random design linear regression with dependent($\beta$-mixing) data absent any realizability assumptions. In contrast to thestrictly realizable martingale noise regime, no sharp instance-optimalnon-asymptotics are available in the literature. Up to constant factors, ouranalysis correctly recovers the variance term predicted by the Central LimitTheorem -- the noise level of the problem -- and thus exhibits gracefuldegradation as we introduce misspecification. Past a burn-in, our result issharp in the moderate deviations regime, and in particular does not inflate theleading order term by mixing time factors.</description><author>Ingvar Ziemann, Stephen Tu, George J. Pappas, Nikolai Matni</author><pubDate>Thu, 18 May 2023 18:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11165v1</guid></item><item><title>Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs</title><link>http://arxiv.org/abs/2211.16468v2</link><description>Causal effect estimation from observational data is a fundamental task inempirical sciences. It becomes particularly challenging when unobservedconfounders are involved in a system. This paper focuses on front-dooradjustment -- a classic technique which, using observed mediators allows toidentify causal effects even in the presence of unobserved confounding. Whilethe statistical properties of the front-door estimation are quite wellunderstood, its algorithmic aspects remained unexplored for a long time.Recently, Jeong, Tian, and Barenboim [NeurIPS 2022] have presented the firstpolynomial-time algorithm for finding sets satisfying the front-door criterionin a given directed acyclic graph (DAG), with an $O(n^3(n+m))$ run time, where$n$ denotes the number of variables and $m$ the number of edges of the causalgraph. In our work, we give the first linear-time, i.e., $O(n+m)$, algorithmfor this task, which thus reaches the asymptotically optimal time complexity.This result implies an $O(n(n+m))$ delay enumeration algorithm of allfront-door adjustment sets, again improving previous work by Jeong et al.\ by afactor of $n^3$. Moreover, we provide the first linear-time algorithm forfinding a minimal front-door adjustment set. We offer implementations of ouralgorithms in multiple programming languages to facilitate practical usage andempirically validate their feasibility, even for large graphs.</description><author>Marcel Wienöbst, Benito van der Zander, Maciej Liśkiewicz</author><pubDate>Thu, 18 May 2023 18:54:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16468v2</guid></item><item><title>Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository Mining Study</title><link>http://arxiv.org/abs/2305.11164v1</link><description>The rise of machine learning (ML) systems has exacerbated their carbonfootprint due to increased capabilities and model sizes. However, there isscarce knowledge on how the carbon footprint of ML models is actually measured,reported, and evaluated. In light of this, the paper aims to analyze themeasurement of the carbon footprint of 1,417 ML models and associated datasetson Hugging Face, which is the most popular repository for pretrained ML models.The goal is to provide insights and recommendations on how to report andoptimize the carbon efficiency of ML models. The study includes the firstrepository mining study on the Hugging Face Hub API on carbon emissions. Thisstudy seeks to answer two research questions: (1) how do ML model creatorsmeasure and report carbon emissions on Hugging Face Hub?, and (2) what aspectsimpact the carbon emissions of training ML models? The study yielded severalkey findings. These include a decreasing proportion of carbonemissions-reporting models, a slight decrease in reported carbon footprint onHugging Face over the past 2 years, and a continued dominance of NLP as themain application domain. Furthermore, the study uncovers correlations betweencarbon emissions and various attributes such as model size, dataset size, andML application domains. These results highlight the need for softwaremeasurements to improve energy reporting practices and promote carbon-efficientmodel development within the Hugging Face community. In response to this issue,two classifications are proposed: one for categorizing models based on theircarbon emission reporting practices and another for their carbon efficiency.The aim of these classification proposals is to foster transparency andsustainable model development within the ML community.</description><author>Joel Castaño, Silverio Martínez-Fernández, Xavier Franch, Justus Bogner</author><pubDate>Thu, 18 May 2023 18:52:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11164v1</guid></item><item><title>Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors</title><link>http://arxiv.org/abs/2305.11159v1</link><description>Recent work has shown that fine-tuning large language models (LLMs) onlarge-scale instruction-following datasets substantially improves theirperformance on a wide range of NLP tasks, especially in the zero-shot setting.However, even advanced instruction-tuned LLMs still fail to outperform smallLMs on relation extraction (RE), a fundamental information extraction task. Wehypothesize that instruction-tuning has been unable to elicit strong REcapabilities in LLMs due to RE's low incidence in instruction-tuning datasets,making up less than 1% of all tasks (Wang et al., 2022). To address thislimitation, we propose QA4RE, a framework that aligns RE with questionanswering (QA), a predominant task in instruction-tuning datasets.Comprehensive zero-shot RE experiments over four datasets with two series ofinstruction-tuned LLMs (six LLMs in total) demonstrate that our QA4RE frameworkconsistently improves LLM performance, strongly verifying our hypothesis andenabling LLMs to outperform strong zero-shot baselines by a large margin.Additionally, we provide thorough experiments and discussions to show therobustness, few-shot effectiveness, and strong transferability of our QA4REframework. This work illustrates a promising way of adapting LLMs tochallenging and underrepresented tasks by aligning these tasks with more commoninstruction-tuning tasks like QA.</description><author>Kai Zhang, Bernal Jiménez Gutiérrez, Yu Su</author><pubDate>Thu, 18 May 2023 18:48:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11159v1</guid></item><item><title>ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations</title><link>http://arxiv.org/abs/2212.10409v2</link><description>Context is everything, even in commonsense moral reasoning. Changing contextscan flip the moral judgment of an action; "Lying to a friend" is wrong ingeneral, but may be morally acceptable if it is intended to protect their life. We present ClarifyDelphi, an interactive system that learns to askclarification questions (e.g., why did you lie to your friend?) in order toelicit additional salient contexts of a social or moral situation. We positthat questions whose potential answers lead to diverging moral judgments arethe most informative. Thus, we propose a reinforcement learning framework witha defeasibility reward that aims to maximize the divergence between moraljudgments of hypothetical answers to a question. Human evaluation demonstratesthat our system generates more relevant, informative and defeasible questionscompared to competitive baselines. Our work is ultimately inspired by studiesin cognitive science that have investigated the flexibility in moral cognition(i.e., the diverse contexts in which moral rules can be bent), and we hope thatresearch in this direction can assist both cognitive and computationalinvestigations of moral judgments.</description><author>Valentina Pyatkin, Jena D. Hwang, Vivek Srikumar, Ximing Lu, Liwei Jiang, Yejin Choi, Chandra Bhagavatula</author><pubDate>Thu, 18 May 2023 18:46:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10409v2</guid></item><item><title>UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild</title><link>http://arxiv.org/abs/2305.11147v1</link><description>Achieving machine autonomy and human control often represent divergentobjectives in the design of interactive AI systems. Visual generativefoundation models such as Stable Diffusion show promise in navigating thesegoals, especially when prompted with arbitrary languages. However, they oftenfall short in generating images with spatial, structural, or geometriccontrols. The integration of such controls, which can accommodate variousvisual conditions in a single unified model, remains an unaddressed challenge.In response, we introduce UniControl, a new generative foundation model thatconsolidates a wide array of controllable condition-to-image (C2I) tasks withina singular framework, while still allowing for arbitrary language prompts.UniControl enables pixel-level-precise image generation, where visualconditions primarily influence the generated structures and language promptsguide the style and context. To equip UniControl with the capacity to handlediverse visual conditions, we augment pretrained text-to-image diffusion modelsand introduce a task-aware HyperNet to modulate the diffusion models, enablingthe adaptation to different C2I tasks simultaneously. Trained on nine uniqueC2I tasks, UniControl demonstrates impressive zero-shot generation abilitieswith unseen visual conditions. Experimental results show that UniControl oftensurpasses the performance of single-task-controlled methods of comparable modelsizes. This control versatility positions UniControl as a significantadvancement in the realm of controllable visual generation.</description><author>Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, Ran Xu</author><pubDate>Thu, 18 May 2023 18:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11147v1</guid></item><item><title>Discourse Centric Evaluation of Machine Translation with a Densely Annotated Parallel Corpus</title><link>http://arxiv.org/abs/2305.11142v1</link><description>Several recent papers claim human parity at sentence-level MachineTranslation (MT), especially in high-resource languages. Thus, in response, theMT community has, in part, shifted its focus to document-level translation.Translating documents requires a deeper understanding of the structure andmeaning of text, which is often captured by various kinds of discoursephenomena such as consistency, coherence, and cohesion. However, this rendersconventional sentence-level MT evaluation benchmarks inadequate for evaluatingthe performance of context-aware MT systems. This paper presents a new datasetwith rich discourse annotations, built upon the large-scale parallel corpus BWBintroduced in Jiang et al. (2022). The new BWB annotation introduces four extraevaluation aspects, i.e., entity, terminology, coreference, and quotation,covering 15,095 entity mentions in both languages. Using these annotations, wesystematically investigate the similarities and differences between thediscourse structures of source and target languages, and the challenges theypose to MT. We discover that MT outputs differ fundamentally from humantranslations in terms of their latent discourse structures. This gives us a newperspective on the challenges and opportunities in document-level MT. We makeour resource publicly available to spur future research in document-level MTand the generalization to other language translation tasks.</description><author>Yuchen Eleanor Jiang, Tianyu Liu, Shuming Ma, Dongdong Zhang, Mrinmaya Sachan, Ryan Cotterell</author><pubDate>Thu, 18 May 2023 18:36:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11142v1</guid></item><item><title>Clifford Group Equivariant Neural Networks</title><link>http://arxiv.org/abs/2305.11141v1</link><description>We introduce Clifford Group Equivariant Neural Networks: a novel approach forconstructing $\mathrm{E}(n)$-equivariant networks. We identify and study the$\textit{Clifford group}$, a subgroup inside the Clifford algebra, whosedefinition we slightly adjust to achieve several favorable properties.Primarily, the group's action forms an orthogonal automorphism that extendsbeyond the typical vector space to the entire Clifford algebra while respectingthe multivector grading. This leads to several non-equivalentsubrepresentations corresponding to the multivector decomposition. Furthermore,we prove that the action respects not just the vector space structure of theClifford algebra but also its multiplicative structure, i.e., the geometricproduct. These findings imply that every polynomial in multivectors, includingtheir grade projections, constitutes an equivariant map with respect to theClifford group, allowing us to parameterize equivariant neural network layers.Notable advantages are that these layers operate directly on a vector basis andelegantly generalize to any dimension. We demonstrate, notably from a singlecore implementation, state-of-the-art performance on several distinct tasks,including a three-dimensional $n$-body experiment, a four-dimensionalLorentz-equivariant high-energy physics experiment, and a five-dimensionalconvex hull experiment.</description><author>David Ruhe, Johannes Brandstetter, Patrick Forré</author><pubDate>Thu, 18 May 2023 18:35:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11141v1</guid></item><item><title>Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model</title><link>http://arxiv.org/abs/2305.11140v1</link><description>Natural language generation models reproduce and often amplify the biasespresent in their training data. Previous research explored usingsequence-to-sequence rewriting models to transform biased model outputs (ororiginal texts) into more gender-fair language by creating pseudo training datathrough linguistic rules. However, this approach is not practical for languageswith more complex morphology than English. We hypothesise that creatingtraining data in the reverse direction, i.e. starting from gender-fair text, iseasier for morphologically complex languages and show that it matches theperformance of state-of-the-art rewriting models for English. To eliminate therule-based nature of data creation, we instead propose using machinetranslation models to create gender-biased text from real gender-fair text viaround-trip translation. Our approach allows us to train a rewriting model forGerman without the need for elaborate handcrafted rules. The outputs of thismodel increased gender-fairness as shown in a human evaluation study.</description><author>Chantal Amrhein, Florian Schottmann, Rico Sennrich, Samuel Läubli</author><pubDate>Thu, 18 May 2023 18:35:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11140v1</guid></item><item><title>Parallel development of social preferences in fish and machines</title><link>http://arxiv.org/abs/2305.11137v1</link><description>What are the computational foundations of social grouping? Traditionalapproaches to this question have focused on verbal reasoning or simple(low-dimensional) quantitative models. In the real world, however, socialpreferences emerge when high-dimensional learning systems (brains and bodies)interact with high-dimensional sensory inputs during an animal's embodiedinteractions with the world. A deep understanding of social grouping willtherefore require embodied models that learn directly from sensory inputs usinghigh-dimensional learning mechanisms. To this end, we built artificial neuralnetworks (ANNs), embodied those ANNs in virtual fish bodies, and raised theartificial fish in virtual fish tanks that mimicked the rearing conditions ofreal fish. We then compared the social preferences that emerged in real fishversus artificial fish. We found that when artificial fish had two corelearning mechanisms (reinforcement learning and curiosity-driven learning),artificial fish developed fish-like social preferences. Like real fish, theartificial fish spontaneously learned to prefer members of their own group overmembers of other groups. The artificial fish also spontaneously learned toself-segregate with their in-group, akin to self-segregation behavior seen innature. Our results suggest that social grouping can emerge from threeingredients: (1) reinforcement learning, (2) intrinsic motivation, and (3)early social experiences with in-group members. This approach lays a foundationfor reverse engineering animal-like social behavior with image-computablemodels, bridging the divide between high-dimensional sensory inputs and socialpreferences.</description><author>Joshua McGraw, Donsuk Lee, Justin Wood</author><pubDate>Thu, 18 May 2023 18:32:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11137v1</guid></item><item><title>Convergence Analysis of Over-the-Air FL with Compression and Power Control via Clipping</title><link>http://arxiv.org/abs/2305.11135v1</link><description>One of the key challenges towards the deployment of over-the-air federatedlearning (AirFL) is the design of mechanisms that can comply with the power andbandwidth constraints of the shared channel, while causing minimumdeterioration to the learning performance as compared to baseline noiselessimplementations. For additive white Gaussian noise (AWGN) channels withinstantaneous per-device power constraints, prior work has demonstrated theoptimality of a power control mechanism based on norm clipping. This was donethrough the minimization of an upper bound on the optimality gap for smoothlearning objectives satisfying the Polyak-{\L}ojasiewicz (PL) condition. Inthis paper, we make two contributions to the development of AirFL based on normclipping, which we refer to as AirFL-Clip. First, we provide a convergencebound for AirFLClip that applies to general smooth and non-convex learningobjectives. Unlike existing results, the derived bound is free fromrun-specific parameters, thus supporting an offline evaluation. Second, weextend AirFL-Clip to include Top-k sparsification and linear compression. Forthis generalized protocol, referred to as AirFL-Clip-Comp, we derive aconvergence bound for general smooth and non-convex learning objectives. Weargue, and demonstrate via experiments, that the only time-varying quantitiespresent in the bound can be efficiently estimated offline by leveraging thewell-studied properties of sparse recovery algorithms.</description><author>Haifeng Wen, Hong Xing, Osvaldo Simeone</author><pubDate>Thu, 18 May 2023 18:30:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11135v1</guid></item><item><title>LMEye: An Interactive Perception Network for Large Language Models</title><link>http://arxiv.org/abs/2305.03701v2</link><description>Training a Large Visual Language Model (LVLM) from scratch, like GPT-4, isresource-intensive. Our paper presents a play-and-plug module for LargeLanguage Models (LLMs), namely Interactive Perception Network (IPN), aiming toachieve a LVLM by incorporating the image understanding capability into LLMs.Previous methods incorporate visual information into LLMs with a simple visualmapping network, where the image feature is projected into the embedding spaceof LLMs via a linear layer. Such mapping network projects the image featureonce yet does not consider the interaction between the image and the humaninput query. Hence, the obtained visual information with no connections withhuman intention may be inadequate for LLMs to make intention-followingresponses, which we term as static visual information. IPN addresses this issueby allowing the LLM to request the desired visual information aligned withvarious human instructions, which we term as the dynamic interaction betweenthe LLM and visual information. Specifically, IPN consists of a simple visualmapping network to provide the basic perception of an image for LLMs. It alsocontains additional modules responsible for acquiring requests from LLMs,performing request-based visual information interaction, and transmitting theresulting interacted visual information to LLMs, respectively. In this way,LLMs act to understand the human query, deliver the corresponding request tothe request-based visual information interaction module, and generate theresponse based on the interleaved multimodal information. We evaluate IPNthrough extensive experiments on multimodal question answering, reasoning, andso on, demonstrating that it significantly improves the zero-shot performanceof LVLMs on various multimodal tasks compared to previous methods.</description><author>Yunxin Li, Baotian Hu, Xinyu Chen, Lin Ma, Min Zhang</author><pubDate>Thu, 18 May 2023 18:28:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03701v2</guid></item><item><title>Attacks on Online Learners: a Teacher-Student Analysis</title><link>http://arxiv.org/abs/2305.11132v1</link><description>Machine learning models are famously vulnerable to adversarial attacks: smallad-hoc perturbations of the data that can catastrophically alter the modelpredictions. While a large literature has studied the case of test-time attackson pre-trained models, the important case of attacks in an online learningsetting has received little attention so far. In this work, we use acontrol-theoretical perspective to study the scenario where an attacker mayperturb data labels to manipulate the learning dynamics of an online learner.We perform a theoretical analysis of the problem in a teacher-student setup,considering different attack strategies, and obtaining analytical results forthe steady state of simple linear learners. These results enable us to provethat a discontinuous transition in the learner's accuracy occurs when theattack strength exceeds a critical threshold. We then study empirically attackson learners with complex architectures using real data, confirming the insightsof our theoretical analysis. Our findings show that greedy attacks can beextremely efficient, especially when data stream in small batches.</description><author>Riccardo Giuseppe Margiotta, Sebastian Goldt, Guido Sanguinetti</author><pubDate>Thu, 18 May 2023 18:26:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11132v1</guid></item><item><title>SimOAP: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation</title><link>http://arxiv.org/abs/2305.11130v1</link><description>Language models trained on large-scale corpora can generate remarkably fluentresults in open-domain dialogue. However, for the persona-based dialoguegeneration task, consistency and coherence are also key factors, which aregreat challenges for language models. Existing works mainly focus on valuabledata filtering, model structure modifying, or objective function designing,while their improvements are limited and hard to generalize to all types ofpre-trained language models. However, we find that language models can produceconsistent and coherent responses if we consider enough generations. Thus, theproblems lay in large-scale response generation and target response selection.In this work, a simple but effective two-stage SimOAP strategy is proposed,i.e., over-sampling and post-evaluation. The over-sampling stage takeslarge-scale responses from existing trained models efficiently viaoff-the-shelf distilling and compressing methods, and the post-evaluation stageselects a good response based on multiple well-designed evaluation metrics fromlarge-scale candidates. Experimental results show that the proposed plug-inSimOAP strategy improves the backbone models and outperforms the baselinestrategies in both automatic and human evaluations.</description><author>Junkai Zhou, Liang Pang, Huawei Shen, Xueqi Cheng</author><pubDate>Thu, 18 May 2023 18:23:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11130v1</guid></item><item><title>mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences</title><link>http://arxiv.org/abs/2305.11129v1</link><description>We present our work on developing a multilingual, efficient text-to-texttransformer that is suitable for handling long inputs. This model, calledmLongT5, builds upon the architecture of LongT5, while leveraging themultilingual datasets used for pretraining mT5 and the pretraining tasks ofUL2. We evaluate this model on a variety of multilingual summarization andquestion-answering tasks, and the results show stronger performance for mLongT5when compared to existing multilingual models such as mBART or M-BERT.</description><author>David Uthus, Santiago Ontañón, Joshua Ainslie, Mandy Guo</author><pubDate>Thu, 18 May 2023 18:22:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11129v1</guid></item><item><title>Unlimiformer: Long-Range Transformers with Unlimited Length Input</title><link>http://arxiv.org/abs/2305.01625v2</link><description>Since the proposal of transformers, these models have been limited to boundedinput lengths, because of their need to attend to every token in the input. Inthis work, we propose Unlimiformer: a general approach that wraps any existingpretrained encoder-decoder transformer, and offloads the cross-attentioncomputation to a single k-nearest-neighbor (kNN) index, while the returned kNNdistances are the attention dot-product scores. This kNN index can be kept oneither the GPU or CPU memory and queried in sub-linear time; this way, we canindex practically unlimited input sequences, while every attention head inevery decoder layer retrieves its top-k keys, instead of attending to everykey. We evaluate Unlimiformer on several long-document and book-summarizationbenchmarks, showing that it can process even 500k token-long inputs from theBookSum dataset, without any input truncation at test time. We demonstrate thatUnlimiformer improves pretrained models such as BART and Longformer byextending them to unlimited inputs without additional learned weights andwithout modifying their code. We make our code and models publicly available athttps://github.com/abertsch72/unlimiformer .</description><author>Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley</author><pubDate>Thu, 18 May 2023 18:21:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01625v2</guid></item><item><title>Skin Lesion Diagnosis Using Convolutional Neural Networks</title><link>http://arxiv.org/abs/2305.11125v1</link><description>Cancerous skin lesions are one of the most common malignancies detected inhumans, and if not detected at an early stage, they can lead to death.Therefore, it is crucial to have access to accurate results early on tooptimize the chances of survival. Unfortunately, accurate results are typicallyobtained by highly trained dermatologists, who may not be accessible to manypeople, particularly in low-income and middle-income countries. ArtificialIntelligence (AI) appears to be a potential solution to this problem, as it hasproven to provide equal or even better diagnoses than healthcare professionals.This project aims to address the issue by collecting state-of-the-arttechniques for image classification from various fields and implementing them.Some of these techniques include mixup, presizing, and test-time augmentation,among others. Three architectures were used for the implementation:DenseNet121, VGG16 with batch normalization, and ResNet50. The models weredesigned with two main purposes. First, to classify images into sevencategories, including melanocytic nevus, melanoma, benign keratosis-likelesions, basal cell carcinoma, actinic keratoses and intraepithelial carcinoma,vascular lesions, and dermatofibroma. Second, to classify images into benign ormalignant. The models were trained using a dataset of 8012 images, and theirperformance was evaluated using 2003 images. It's worth noting that this modelis trained end-to-end, directly from the image to the labels, without the needfor handcrafted feature extraction.</description><author>Daniel Alonso Villanueva Nunez, Yongmin Li</author><pubDate>Thu, 18 May 2023 18:15:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11125v1</guid></item><item><title>Learning from Aggregated Data: Curated Bags versus Random Bags</title><link>http://arxiv.org/abs/2305.09557v2</link><description>Protecting user privacy is a major concern for many machine learning systemsthat are deployed at scale and collect from a diverse set of population. Oneway to address this concern is by collecting and releasing data labels in anaggregated manner so that the information about a single user is potentiallycombined with others. In this paper, we explore the possibility of trainingmachine learning models with aggregated data labels, rather than individuallabels. Specifically, we consider two natural aggregation procedures suggestedby practitioners: curated bags where the data points are grouped based oncommon features and random bags where the data points are grouped randomly inbag of similar sizes. For the curated bag setting and for a broad range of lossfunctions, we show that we can perform gradient-based learning without anydegradation in performance that may result from aggregating data. Our method isbased on the observation that the sum of the gradients of the loss function onindividual data examples in a curated bag can be computed from the aggregatelabel without the need for individual labels. For the random bag setting, weprovide a generalization risk bound based on the Rademacher complexity of thehypothesis class and show how empirical risk minimization can be regularized toachieve the smallest risk bound. In fact, in the random bag setting, there is atrade-off between size of the bag and the achievable error rate as our boundindicates. Finally, we conduct a careful empirical study to confirm ourtheoretical findings. In particular, our results suggest that aggregatelearning can be an effective method for preserving user privacy whilemaintaining model accuracy.</description><author>Lin Chen, Gang Fu, Amin Karbasi, Vahab Mirrokni</author><pubDate>Thu, 18 May 2023 18:13:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09557v2</guid></item><item><title>Multi-step Jailbreaking Privacy Attacks on ChatGPT</title><link>http://arxiv.org/abs/2304.05197v2</link><description>With the rapid progress of large language models (LLMs), many downstream NLPtasks can be well solved given appropriate prompts. Though model developers andresearchers work hard on dialog safety to avoid generating harmful content fromLLMs, it is still challenging to steer AI-generated content (AIGC) for thehuman good. As powerful LLMs are devouring existing text data from variousdomains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whetherthe private information is included in the training data and what privacythreats can these LLMs and their downstream applications bring. In this paper,we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced byChatGPT and show that application-integrated LLMs may cause new privacythreats. To this end, we conduct extensive experiments to support our claimsand discuss LLMs' privacy implications.</description><author>Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, Yangqiu Song</author><pubDate>Thu, 18 May 2023 18:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05197v2</guid></item><item><title>Autonomous sputter synthesis of thin film nitrides with composition controlled by Bayesian optimization of optical plasma emission</title><link>http://arxiv.org/abs/2305.11122v1</link><description>Autonomous experimentation has emerged as an efficient approach to acceleratethe pace of materials discovery. Although instruments for autonomous synthesishave become popular in molecular and polymer science, solution processing ofhybrid materials and nanoparticles, examples of autonomous tools for physicalvapour deposition are scarce yet important for the semiconductor industry.Here, we report the design and implementation of an autonomous instrument forsputter deposition of thin films with controlled composition, leveraging ahighly automated sputtering reactor custom-controlled by Python, opticalemission spectroscopy (OES), and Bayesian optimization algorithm. We modeledfilm composition, measured by x-ray fluorescence, as a linear function ofemission lines monitored during the co-sputtering from elemental Zn and Titargets in N$_2$ atmosphere. A Bayesian control algorithm, informed by OES,navigates the space of sputtering power to fabricate films with user-definedcomposition, by minimizing the absolute error between desired and measuredemission signals. We validated our approach by autonomously fabricatingZn$_x$Ti$_{1-x}$N$_y$ films with deviations from the targeted cationcomposition within relative 3.5 %, even for 15 nm thin films, demonstratingthat the proposed approach can reliably synthesize thin films with specificcomposition and minimal human interference. Moreover, the proposed method canbe extended to more difficult synthesis experiments where plasma intensitydepends non-linearly on pressure, or the elemental sticking coefficientsstrongly depend on the substrate temperature.</description><author>Davi M. Febba, Kevin R. Talley, Kendal Johnson, Stephen Schaefer, Sage R. Bauers, John S. Mangum, Rebecca W. Smaha, Andriy Zakutayev</author><pubDate>Thu, 18 May 2023 18:09:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11122v1</guid></item><item><title>A Compound Gaussian Network for Solving Linear Inverse Problems</title><link>http://arxiv.org/abs/2305.11120v1</link><description>For solving linear inverse problems, particularly of the type that appear intomographic imaging and compressive sensing, this paper develops two newapproaches. The first approach is an iterative algorithm that minimizers aregularized least squares objective function where the regularization is basedon a compound Gaussian prior distribution. The Compound Gaussian prior subsumesmany of the commonly used priors in image reconstruction, including those ofsparsity-based approaches. The developed iterative algorithm gives rise to thepaper's second new approach, which is a deep neural network that corresponds toan "unrolling" or "unfolding" of the iterative algorithm. Unrolled deep neuralnetworks have interpretable layers and outperform standard deep learningmethods. This paper includes a detailed computational theory that providesinsight into the construction and performance of both algorithms. Theconclusion is that both algorithms outperform other state-of-the-art approachesto tomographic image formation and compressive sensing, especially in thedifficult regime of low training.</description><author>Carter Lyons, Raghu G. Raj, Margaret Cheney</author><pubDate>Thu, 18 May 2023 18:05:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11120v1</guid></item><item><title>LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation</title><link>http://arxiv.org/abs/2305.11116v1</link><description>Existing automatic evaluation on text-to-image synthesis can only provide animage-text matching score, without considering the object-levelcompositionality, which results in poor correlation with human judgments. Inthis work, we propose LLMScore, a new framework that offers evaluation scoreswith multi-granularity compositionality. LLMScore leverages the large languagemodels (LLMs) to evaluate text-to-image models. Initially, it transforms theimage into image-level and object-level visual descriptions. Then an evaluationinstruction is fed into the LLMs to measure the alignment between thesynthesized image and the text, ultimately generating a score accompanied by arationale. Our substantial analysis reveals the highest correlation of LLMScorewith human judgments on a wide range of datasets (Attribute Binding Contrast,Concept Conjunction, MSCOCO, DrawBench, PaintSkills). Notably, our LLMScoreachieves Kendall's tau correlation with human evaluations that is 58.8% and31.2% higher than the commonly-used text-image matching metrics CLIP and BLIP,respectively.</description><author>Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, William Yang Wang</author><pubDate>Thu, 18 May 2023 17:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11116v1</guid></item><item><title>PPDONet: Deep Operator Networks for Fast Prediction of Steady-State Solutions in Disk-Planet Systems</title><link>http://arxiv.org/abs/2305.11111v1</link><description>We develop a tool, which we name Protoplanetary Disk Operator Network(PPDONet), that can predict the solution of disk-planet interactions inprotoplanetary disks in real-time. We base our tool on Deep Operator Networks(DeepONets), a class of neural networks capable of learning non-linearoperators to represent deterministic and stochastic differential equations.With PPDONet we map three scalar parameters in a disk-planet system -- theShakura \&amp; Sunyaev viscosity $\alpha$, the disk aspect ratio $h_\mathrm{0}$,and the planet-star mass ratio $q$ -- to steady-state solutions of the disksurface density, radial velocity, and azimuthal velocity. We demonstrate theaccuracy of the PPDONet solutions using a comprehensive set of tests. Our toolis able to predict the outcome of disk-planet interaction for one system inless than a second on a laptop. A public implementation of PPDONet is availableat \url{https://github.com/smao-astro/PPDONet}.</description><author>Shunyuan Mao, Ruobing Dong, Lu Lu, Kwang Moo Yi, Sifan Wang, Paris Perdikaris</author><pubDate>Thu, 18 May 2023 17:53:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11111v1</guid></item><item><title>MiraBest: A Dataset of Morphologically Classified Radio Galaxies for Machine Learning</title><link>http://arxiv.org/abs/2305.11108v1</link><description>The volume of data from current and future observatories has motivated theincreased development and application of automated machine learningmethodologies for astronomy. However, less attention has been given to theproduction of standardised datasets for assessing the performance of differentmachine learning algorithms within astronomy and astrophysics. Here we describein detail the MiraBest dataset, a publicly available batched dataset of 1256radio-loud AGN from NVSS and FIRST, filtered to $0.03 &lt; z &lt; 0.1$, manuallylabelled by Miraghaei and Best (2017) according to the Fanaroff-Rileymorphological classification, created for machine learning applications andcompatible for use with standard deep learning libraries. We outline theprinciples underlying the construction of the dataset, the sample selection andpre-processing methodology, dataset structure and composition, as well as acomparison of MiraBest to other datasets used in the literature. Existingapplications that utilise the MiraBest dataset are reviewed, and an extendeddataset of 2100 sources is created by cross-matching MiraBest with othercatalogues of radio-loud AGN that have been used more widely in the literaturefor machine learning applications.</description><author>Fiona A. M. Porter, Anna M. M. Scaife</author><pubDate>Thu, 18 May 2023 17:52:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11108v1</guid></item><item><title>From Data-Fitting to Discovery: Interpreting the Neural Dynamics of Motor Control through Reinforcement Learning</title><link>http://arxiv.org/abs/2305.11107v1</link><description>In motor neuroscience, artificial recurrent neural networks models oftencomplement animal studies. However, most modeling efforts are limited todata-fitting, and the few that examine virtual embodied agents in areinforcement learning context, do not draw direct comparisons to theirbiological counterparts. Our study addressing this gap, by uncoveringstructured neural activity of a virtual robot performing legged locomotion thatdirectly support experimental findings of primate walking and cycling. We findthat embodied agents trained to walk exhibit smooth dynamics that avoidtangling -- or opposing neural trajectories in neighboring neural space -- acore principle in computational neuroscience. Specifically, across a wide suiteof gaits, the agent displays neural trajectories in the recurrent layers areless tangled than those in the input-driven actuation layers. To betterinterpret the neural separation of these elliptical-shaped trajectories, weidentify speed axes that maximizes variance of mean activity across differentforward, lateral, and rotational speed conditions.</description><author>Eugene R. Rush, Kaushik Jayaram, J. Sean Humbert</author><pubDate>Thu, 18 May 2023 17:52:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11107v1</guid></item><item><title>mdctGAN: Taming transformer-based GAN for speech super-resolution with Modified DCT spectra</title><link>http://arxiv.org/abs/2305.11104v1</link><description>Speech super-resolution (SSR) aims to recover a high resolution (HR) speechfrom its corresponding low resolution (LR) counterpart. Recent SSR methodsfocus more on the reconstruction of the magnitude spectrogram, ignoring theimportance of phase reconstruction, thereby limiting the recovery quality. Toaddress this issue, we propose mdctGAN, a novel SSR framework based on modifieddiscrete cosine transform (MDCT). By adversarial learning in the MDCT domain,our method reconstructs HR speeches in a phase-aware manner without vocoders oradditional post-processing. Furthermore, by learning frequency consistentfeatures with self-attentive mechanism, mdctGAN guarantees a high qualityspeech reconstruction. For VCTK corpus dataset, the experiment results showthat our model produces natural auditory quality with high MOS and PESQ scores.It also achieves the state-of-the-art log-spectral-distance (LSD) performanceon 48 kHz target resolution from various input rates. Code is available fromhttps://github.com/neoncloud/mdctGAN</description><author>Chenhao Shuai, Chaohua Shi, Lu Gan, Hongqing Liu</author><pubDate>Thu, 18 May 2023 17:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11104v1</guid></item><item><title>Progressive Learning of 3D Reconstruction Network from 2D GAN Data</title><link>http://arxiv.org/abs/2305.11102v1</link><description>This paper presents a method to reconstruct high-quality textured 3D modelsfrom single images. Current methods rely on datasets with expensiveannotations; multi-view images and their camera parameters. Our method relieson GAN generated multi-view image datasets which have a negligible annotationcost. However, they are not strictly multi-view consistent and sometimes GANsoutput distorted images. This results in degraded reconstruction qualities. Inthis work, to overcome these limitations of generated datasets, we have twomain contributions which lead us to achieve state-of-the-art results onchallenging objects: 1) A robust multi-stage learning scheme that graduallyrelies more on the models own predictions when calculating losses, 2) A noveladversarial learning pipeline with online pseudo-ground truth generations toachieve fine details. Our work provides a bridge from 2D supervisions of GANmodels to 3D reconstruction models and removes the expensive annotationefforts. We show significant improvements over previous methods whether theywere trained on GAN generated multi-view images or on real images withexpensive annotations. Please visit our web-page for 3D visuals:https://research.nvidia.com/labs/adlr/progressive-3d-learning</description><author>Aysegul Dundar, Jun Gao, Andrew Tao, Bryan Catanzaro</author><pubDate>Thu, 18 May 2023 17:45:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11102v1</guid></item><item><title>XFormer: Fast and Accurate Monocular 3D Body Capture</title><link>http://arxiv.org/abs/2305.11101v1</link><description>We present XFormer, a novel human mesh and motion capture method thatachieves real-time performance on consumer CPUs given only monocular images asinput. The proposed network architecture contains two branches: a keypointbranch that estimates 3D human mesh vertices given 2D keypoints, and an imagebranch that makes predictions directly from the RGB image features. At the coreof our method is a cross-modal transformer block that allows information toflow across these two branches by modeling the attention between 2D keypointcoordinates and image spatial features. Our architecture is smartly designed,which enables us to train on various types of datasets including images with2D/3D annotations, images with 3D pseudo labels, and motion capture datasetsthat do not have associated images. This effectively improves the accuracy andgeneralization ability of our system. Built on a lightweight backbone(MobileNetV3), our method runs blazing fast (over 30fps on a single CPU core)and still yields competitive accuracy. Furthermore, with an HRNet backbone,XFormer delivers state-of-the-art performance on Huamn3.6 and 3DPW datasets.</description><author>Lihui Qian, Xintong Han, Faqiang Wang, Hongyu Liu, Haoye Dong, Zhiwen Li, Huawei Wei, Zhe Lin, Cheng-Bin Jin</author><pubDate>Thu, 18 May 2023 17:45:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11101v1</guid></item><item><title>A Simple Generative Model of Logical Reasoning and Statistical Learning</title><link>http://arxiv.org/abs/2305.11098v1</link><description>Statistical learning and logical reasoning are two major fields of AIexpected to be unified for human-like machine intelligence. Most existing workconsiders how to combine existing logical and statistical systems. However,there is no theory of inference so far explaining how basic approaches tostatistical learning and logical reasoning stem from a common principle.Inspired by the fact that much empirical work in neuroscience suggests Bayesian(or probabilistic generative) approaches to brain function including learningand reasoning, we here propose a simple Bayesian model of logical reasoning andstatistical learning. The theory is statistically correct as it satisfiesKolmogorov's axioms, is consistent with both Fenstad's representation theoremand maximum likelihood estimation and performs exact Bayesian inference with alinear-time complexity. The theory is logically correct as it is a data-drivengeneralisation of uncertain reasoning from consistency, possibility,inconsistency and impossibility. The theory is correct in terms of machinelearning as its solution to generation and prediction tasks on the MNISTdataset is not only empirically reasonable but also theoretically correctagainst the K nearest neighbour method. We simply model how data causessymbolic knowledge in terms of its satisfiability in formal logic. Symbolicreasoning emerges as a result of the process of going the causality forwardsand backwards. The forward and backward processes correspond to aninterpretation and inverse interpretation in formal logic, respectively. Theinverse interpretation differentiates our work from the mainstream oftenreferred to as inverse entailment, inverse deduction or inverse resolution. Theperspective gives new insights into learning and reasoning towards human-likemachine intelligence.</description><author>Hiroyuki Kido</author><pubDate>Thu, 18 May 2023 17:34:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11098v1</guid></item><item><title>Statistical Foundations of Prior-Data Fitted Networks</title><link>http://arxiv.org/abs/2305.11097v1</link><description>Prior-data fitted networks (PFNs) were recently proposed as a new paradigmfor machine learning. Instead of training the network to an observed trainingset, a fixed model is pre-trained offline on small, simulated training setsfrom a variety of tasks. The pre-trained model is then used to infer classprobabilities in-context on fresh training sets with arbitrary size anddistribution. Empirically, PFNs achieve state-of-the-art performance on taskswith similar size to the ones used in pre-training. Surprisingly, theiraccuracy further improves when passed larger data sets during inference. Thisarticle establishes a theoretical foundation for PFNs and illuminates thestatistical mechanisms governing their behavior. While PFNs are motivated byBayesian ideas, a purely frequentistic interpretation of PFNs as pre-tuned, butuntrained predictors explains their behavior. A predictor's variance vanishesif its sensitivity to individual training samples does and the bias vanishesonly if it is appropriately localized around the test feature. The transformerarchitecture used in current PFN implementations ensures only the former. Thesefindings shall prove useful for designing architectures with favorableempirical behavior.</description><author>Thomas Nagler</author><pubDate>Thu, 18 May 2023 17:34:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11097v1</guid></item><item><title>Cross-modality Data Augmentation for End-to-End Sign Language Translation</title><link>http://arxiv.org/abs/2305.11096v1</link><description>End-to-end sign language translation (SLT) aims to convert sign languagevideos into spoken language texts directly without intermediaterepresentations. It has been a challenging task due to the modality gap betweensign videos and texts and the data scarcity of labeled data. To tackle thesechallenges, we propose a novel Cross-modality Data Augmentation (XmDA)framework to transfer the powerful gloss-to-text translation capabilities toend-to-end sign language translation (i.e. video-to-text) by exploiting pseudogloss-text pairs from the sign gloss translation model. Specifically, XmDAconsists of two key components, namely, cross-modality mix-up andcross-modality knowledge distillation. The former explicitly encourages thealignment between sign video features and gloss embeddings to bridge themodality gap. The latter utilizes the generation knowledge from gloss-to-textteacher models to guide the spoken language text generation. Experimentalresults on two widely used SLT datasets, i.e., PHOENIX-2014T and CSL-Daily,demonstrate that the proposed XmDA framework significantly and consistentlyoutperforms the baseline models. Extensive analyses confirm our claim that XmDAenhances spoken language text generation by reducing the representationdistance between videos and texts, as well as improving the processing oflow-frequency words and long sentences.</description><author>Jinhui Ye, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Hui Xiong</author><pubDate>Thu, 18 May 2023 17:34:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11096v1</guid></item><item><title>Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization</title><link>http://arxiv.org/abs/2305.11095v1</link><description>We investigate the emergent abilities of the recently proposed web-scalespeech model Whisper, by adapting it to unseen tasks with prompt engineering.We selected three tasks: audio-visual speech recognition (AVSR), code-switchedspeech recognition (CS-ASR), and speech translation (ST) on unseen languagepairs. We design task-specific prompts, by either leveraging anotherlarge-scale model, or simply manipulating the special tokens in the defaultprompts. Experiments show that compared to the default prompts, our proposedprompts improve performance by 10% to 45% on the three zero-shot tasks, andeven outperform SotA supervised models on some datasets. In addition, ourexperiments reveal many interesting properties of Whisper, including itsrobustness to prompts, bias on accents, and the multilingual understanding inits latent space. Code is available athttps://github.com/jasonppy/PromptingWhisper</description><author>Puyuan Peng, Brian Yan, Shinji Watanabe, David Harwath</author><pubDate>Thu, 18 May 2023 17:32:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11095v1</guid></item><item><title>QPGesture: Quantization-Based and Phase-Guided Motion Matching for Natural Speech-Driven Gesture Generation</title><link>http://arxiv.org/abs/2305.11094v1</link><description>Speech-driven gesture generation is highly challenging due to the randomjitters of human motion. In addition, there is an inherent asynchronousrelationship between human speech and gestures. To tackle these challenges, weintroduce a novel quantization-based and phase-guided motion-matchingframework. Specifically, we first present a gesture VQ-VAE module to learn acodebook to summarize meaningful gesture units. With each code representing aunique gesture, random jittering problems are alleviated effectively. We thenuse Levenshtein distance to align diverse gestures with different speech.Levenshtein distance based on audio quantization as a similarity metric ofcorresponding speech of gestures helps match more appropriate gestures withspeech, and solves the alignment problem of speech and gestures well. Moreover,we introduce phase to guide the optimal gesture matching based on the semanticsof context or rhythm of audio. Phase guides when text-based or speech-basedgestures should be performed to make the generated gestures more natural.Extensive experiments show that our method outperforms recent approaches onspeech-driven gesture generation. Our code, database, pre-trained models, anddemos are available at https://github.com/YoungSeng/QPGesture.</description><author>Sicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang, Lei Hao, Weihong Bao, Haolin Zhuang</author><pubDate>Thu, 18 May 2023 17:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11094v1</guid></item><item><title>Universal Domain Adaptation from Foundation Models</title><link>http://arxiv.org/abs/2305.11092v1</link><description>Foundation models (e.g., CLIP or DINOv2) have shown their impressive learningand transferring capabilities on a wide range of visual tasks, by training on alarge corpus of data and adapting to specific downstream tasks. It is, however,interesting that foundation models have not been fully explored for universaldomain adaptation (UniDA), which is to learn models using labeled data in asource domain and unlabeled data in a target one, such that the learned modelscan successfully adapt to the target data. In this paper, we make comprehensiveempirical studies of state-of-the-art UniDA methods using foundation models. Wefirst demonstrate that, while foundation models greatly improve the performanceof the baseline methods that train the models on the source data alone,existing UniDA methods generally fail to improve over the baseline. Thissuggests that new research efforts are very necessary for UniDA usingfoundation models. To this end, we propose a very simple method of target datadistillation on the CLIP model, and achieves consistent improvement over thebaseline across all the UniDA benchmarks. Our studies are under a newlyproposed evaluation metric of universal classification rate (UCR), which isthreshold- and ratio-free and addresses the threshold-sensitive issueencountered when using the existing H-score metric.</description><author>Bin Deng, Kui Jia</author><pubDate>Thu, 18 May 2023 17:28:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11092v1</guid></item><item><title>Blackout Diffusion: Generative Diffusion Models in Discrete-State Spaces</title><link>http://arxiv.org/abs/2305.11089v1</link><description>Typical generative diffusion models rely on a Gaussian diffusion process fortraining the backward transformations, which can then be used to generatesamples from Gaussian noise. However, real world data often takes place indiscrete-state spaces, including many scientific applications. Here, we developa theoretical formulation for arbitrary discrete-state Markov processes in theforward diffusion process using exact (as opposed to variational) analysis. Werelate the theory to the existing continuous-state Gaussian diffusion as wellas other approaches to discrete diffusion, and identify the correspondingreverse-time stochastic process and score function in the continuous-timesetting, and the reverse-time mapping in the discrete-time setting. As anexample of this framework, we introduce ``Blackout Diffusion'', which learns toproduce samples from an empty image instead of from noise. Numericalexperiments on the CIFAR-10, Binarized MNIST, and CelebA datasets confirm thefeasibility of our approach. Generalizing from specific (Gaussian) forwardprocesses to discrete-state processes without a variational approximation shedslight on how to interpret diffusion models, which we discuss.</description><author>Javier E Santos, Zachary R. Fox, Nicholas Lubbers, Yen Ting Lin</author><pubDate>Thu, 18 May 2023 17:24:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11089v1</guid></item><item><title>Lightweight Online Learning for Sets of Related Problems in Automated Reasoning</title><link>http://arxiv.org/abs/2305.11087v1</link><description>We present Self-Driven Strategy Learning (sdsl), a lightweight onlinelearning methodology for automated reasoning tasks that involve solving a setof related problems. sdsl automatically gathers information, in form of adataset, while solving earlier problems. It utilizes the learned data to adjustthe solving strategy for later problems by fitting a machine learning model tothe obtained data on the fly. We formally define the approach as a set ofabstract transition rules. We describe a concrete instance of the sdsl calculuswhich uses conditional sampling for generating data and random forests as theunderlying machine learning model. We implement the approach on top of theKissat solver and show that the combination of Kissat+sdsl certifies largerbounds and finds more counter-examples than other state-of-the-art boundedmodel checking approaches on benchmarks obtained from the latest Hardware ModelChecking Competition.</description><author>Haoze Wu, Christopher Hahn, Florian Lonsing, Makai Mann, Raghuram Ramanujan, Clark Barrett</author><pubDate>Thu, 18 May 2023 17:23:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11087v1</guid></item><item><title>Preference or Intent? Double Disentangled Collaborative Filtering</title><link>http://arxiv.org/abs/2305.11084v1</link><description>People usually have different intents for choosing items, while theirpreferences under the same intent may also different. In traditionalcollaborative filtering approaches, both intent and preference factors areusually entangled in the modeling process, which significantly limits therobustness and interpretability of recommendation performances. For example,the low-rating items are always treated as negative feedback while theyactually could provide positive information about user intent. To this end, inthis paper, we propose a two-fold representation learning approach, namelyDouble Disentangled Collaborative Filtering (DDCF), for personalizedrecommendations. The first-level disentanglement is for separating theinfluence factors of intent and preference, while the second-leveldisentanglement is performed to build independent sparse preferencerepresentations under individual intent with limited computational complexity.Specifically, we employ two variational autoencoder networks, intentrecognition network and preference decomposition network, to learn the intentand preference factors, respectively. In this way, the low-rating items will betreated as positive samples for modeling intents while the negative samples formodeling preferences. Finally, extensive experiments on three real-worlddatasets and four evaluation metrics clearly validate the effectiveness and theinterpretability of DDCF.</description><author>Chao Wang, Hengshu Zhu, Dazhong Shen, Wei wu, Hui Xiong</author><pubDate>Thu, 18 May 2023 17:13:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11084v1</guid></item><item><title>PyDTS: A Python Package for Discrete-Time Survival (Regularized) Regression with Competing Risks</title><link>http://arxiv.org/abs/2204.05731v4</link><description>Time-to-event analysis (survival analysis) is used when the response ofinterest is the time until a pre-specified event occurs. Time-to-event data aresometimes discrete either because time itself is discrete or due to grouping offailure times into intervals or rounding off measurements. In addition, thefailure of an individual could be one of several distinct failure types, knownas competing risks (events). Most methods and software packages for survivalregression analysis assume that time is measured on a continuous scale. It iswell-known that naively applying standard continuous-time models withdiscrete-time data may result in biased estimators of the discrete-time models.The Python package PyDTS, for simulating, estimating and evaluatingsemi-parametric competing-risks models for discrete-time survival data, isintroduced. The package implements a fast procedure that enables includingregularized regression methods, such as LASSO and elastic net, among others. Asimulation study showcases flexibility and accuracy of the package. The utilityof the package is demonstrated by analysing the Medical Information Mart forIntensive Care (MIMIC) - IV dataset for prediction of hospitalization length ofstay.</description><author>Tomer Meir, Rom Gutman, Malka Gorfine</author><pubDate>Thu, 18 May 2023 17:11:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.05731v4</guid></item><item><title>Inspecting the Geographical Representativeness of Images from Text-to-Image Models</title><link>http://arxiv.org/abs/2305.11080v1</link><description>Recent progress in generative models has resulted in models that produce bothrealistic as well as relevant images for most textual inputs. These models arebeing used to generate millions of images everyday, and hold the potential todrastically impact areas such as generative art, digital marketing and dataaugmentation. Given their outsized impact, it is important to ensure that thegenerated content reflects the artifacts and surroundings across the globe,rather than over-representing certain parts of the world. In this paper, wemeasure the geographical representativeness of common nouns (e.g., a house)generated through DALL.E 2 and Stable Diffusion models using a crowdsourcedstudy comprising 540 participants across 27 countries. For deliberatelyunderspecified inputs without country names, the generated images most reflectthe surroundings of the United States followed by India, and the topgenerations rarely reflect surroundings from all other countries (average scoreless than 3 out of 5). Specifying the country names in the input increases therepresentativeness by 1.44 points on average for DALL.E 2 and 0.75 for StableDiffusion, however, the overall scores for many countries still remain low,highlighting the need for future models to be more geographically inclusive.Lastly, we examine the feasibility of quantifying the geographicalrepresentativeness of generated images without conducting user studies.</description><author>Abhipsa Basu, R. Venkatesh Babu, Danish Pruthi</author><pubDate>Thu, 18 May 2023 17:08:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11080v1</guid></item><item><title>A Study on Transformer Configuration and Training Objective</title><link>http://arxiv.org/abs/2205.10505v3</link><description>Transformer-based models have delivered impressive results on many tasks,particularly vision and language tasks. In many model training situations,conventional configurations are typically adopted. For example, we often setthe base model with hidden dimensions (i.e. model width) to be 768 and thenumber of transformer layers (i.e. model depth) to be 12. In this paper, werevisit these conventional configurations. Through theoretical analysis andexperimental evaluation, we show that the masked autoencoder is effective inalleviating the over-smoothing issue in deep transformer training. Based onthis finding, we propose Bamboo, an idea of using deeper and narrowertransformer configurations, for masked autoencoder training. On ImageNet, withsuch a simple change in configuration, re-designed model achieves 87.1% top-1accuracy and outperforms SoTA models like MAE and BEiT. On language tasks,re-designed model outperforms BERT with default setting by 1.1 points onaverage, on GLUE datasets.</description><author>Fuzhao Xue, Jianghai Chen, Aixin Sun, Xiaozhe Ren, Zangwei Zheng, Xiaoxin He, Yongming Chen, Xin Jiang, Yang You</author><pubDate>Thu, 18 May 2023 17:08:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.10505v3</guid></item><item><title>A Comparative Study of Face Detection Algorithms for Masked Face Detection</title><link>http://arxiv.org/abs/2305.11077v1</link><description>Contemporary face detection algorithms have to deal with many challenges suchas variations in pose, illumination, and scale. A subclass of the facedetection problem that has recently gained increasing attention is occludedface detection, or more specifically, the detection of masked faces. Threeyears on since the advent of the COVID-19 pandemic, there is still a completelack of evidence regarding how well existing face detection algorithms performon masked faces. This article first offers a brief review of state-of-the-artface detectors and detectors made for the masked face problem, along with areview of the existing masked face datasets. We evaluate and compare theperformances of a well-representative set of face detectors at masked facedetection and conclude with a discussion on the possible contributing factorsto their performance.</description><author>Sahel Mohammad Iqbal, Danush Shekar, Subhankar Mishra</author><pubDate>Thu, 18 May 2023 17:03:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11077v1</guid></item><item><title>Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization</title><link>http://arxiv.org/abs/2305.11074v1</link><description>Automatically generating human-readable text describing the functionality ofa program is the intent of source code summarization. Although Neural LanguageModels achieve significant performance in this field, an emerging trend iscombining neural models with external knowledge. Most previous approaches relyon the sentence-level retrieval and combination paradigm (retrieval of similarcode snippets and use of the corresponding code and summary pairs) on theencoder side. However, this paradigm is coarse-grained and cannot directly takeadvantage of the high-quality retrieved summary tokens on the decoder side. Inthis paper, we explore a fine-grained token-level retrieval-augmented mechanismon the decoder side to help the vanilla neural model generate a better codesummary. Furthermore, to mitigate the limitation of token-level retrieval oncapturing contextual code semantics, we propose to integrate code semanticsinto summary tokens. Extensive experiments and human evaluation reveal that ourtoken-level retrieval-augmented approach significantly improves performance andis more interpretive.</description><author>Tong Ye, Lingfei Wu, Tengfei Ma, Xuhong Zhang, Yangkai Du, Peiyu Liu, Wenhai Wang, Shouling Ji</author><pubDate>Thu, 18 May 2023 17:02:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11074v1</guid></item><item><title>A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks</title><link>http://arxiv.org/abs/2305.11073v1</link><description>Conformer, a convolution-augmented Transformer variant, has become the defacto encoder architecture for speech processing due to its superiorperformance in various tasks, including automatic speech recognition (ASR),speech translation (ST) and spoken language understanding (SLU). Recently, anew encoder called E-Branchformer has outperformed Conformer in the LibriSpeechASR benchmark, making it promising for more general speech applications. Thiswork compares E-Branchformer and Conformer through extensive experiments usingdifferent types of end-to-end sequence-to-sequence models. Results demonstratethat E-Branchformer achieves comparable or better performance than Conformer inalmost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, whilebeing more stable during training. We will release our training configurationsand pre-trained models for reproducibility, which can benefit the speechcommunity.</description><author>Yifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, Shinji Watanabe</author><pubDate>Thu, 18 May 2023 17:00:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11073v1</guid></item><item><title>Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering</title><link>http://arxiv.org/abs/2305.11072v1</link><description>Self-supervised speech representation models have succeeded in various tasks,but improving them for content-related problems using unlabeled data ischallenging. We propose speaker-invariant clustering (Spin), a novelself-supervised learning method that clusters speech representations andperforms swapped prediction between the original and speaker-perturbedutterances. Spin disentangles speaker information and preserves contentrepresentations with just 45 minutes of fine-tuning on a single GPU. Spinimproves pre-trained networks and outperforms prior methods in speechrecognition and acoustic unit discovery.</description><author>Heng-Jui Chang, Alexander H. Liu, James Glass</author><pubDate>Thu, 18 May 2023 16:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11072v1</guid></item><item><title>Neural Network Entropy (NNetEn): Entropy-Based EEG Signal and Chaotic Time Series Classification, Python Package for NNetEn Calculation</title><link>http://arxiv.org/abs/2303.17995v2</link><description>Entropy measures are effective features for time series classificationproblems. Traditional entropy measures, such as Shannon entropy, useprobability distribution function. However, for the effective separation oftime series, new entropy estimation methods are required to characterize thechaotic dynamic of the system. Our concept of Neural Network Entropy (NNetEn)is based on the classification of special datasets in relation to the entropyof the time series recorded in the reservoir of the neural network. NNetEnestimates the chaotic dynamics of time series in an original way and does nottake into account probability distribution functions. We propose two newclassification metrics: R2 Efficiency and Pearson Efficiency. The efficiency ofNNetEn is verified on separation of two chaotic time series of sine mappingusing dispersion analysis. For two close dynamic time series (r = 1.1918 and r= 1.2243), the F-ratio has reached the value of 124 and reflects highefficiency of the introduced method in classification problems. Theelectroenceph-alography signal classification for healthy persons and patientswith Alzheimer disease illustrates the practical application of the NNetEnfeatures. Our computations demonstrate the synergistic effect of increasingclassification accuracy when applying traditional entropy measures and theNNetEn concept conjointly. An implementation of the algorithms in Python ispresented.</description><author>Andrei Velichko, Maksim Belyaev, Yuriy Izotov, Murugappan Murugappan, Hanif Heidari</author><pubDate>Thu, 18 May 2023 16:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17995v2</guid></item><item><title>Neuromorphic Bayesian Optimization in Lava</title><link>http://arxiv.org/abs/2305.11060v1</link><description>The ever-increasing demands of computationally expensive and high-dimensionalproblems require novel optimization methods to find near-optimal solutions in areasonable amount of time. Bayesian Optimization (BO) stands as one of the bestmethodologies for learning the underlying relationships within multi-variateproblems. This allows users to optimize time consuming and computationallyexpensive black-box functions in feasible time frames. Existing BOimplementations use traditional von-Neumann architectures, in which data andmemory are separate. In this work, we introduce Lava Bayesian Optimization(LavaBO) as a contribution to the open-source Lava Software Framework. LavaBOis the first step towards developing a BO system compatible with heterogeneous,fine-grained parallel, in-memory neuromorphic computing architectures (e.g.,Intel's Loihi platform). We evaluate the algorithmic performance of the LavaBOsystem on multiple problems such as training state-of-the-art spiking neuralnetwork through back-propagation and evolutionary learning. Compared totraditional algorithms (such as grid and random search), we highlight theability of LavaBO to explore the parameter search space with fewer expensivefunction evaluations, while discovering the optimal solutions.</description><author>Shay Snyder, Sumedh R. Risbud, Maryam Parsa</author><pubDate>Thu, 18 May 2023 16:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11060v1</guid></item><item><title>Optimality and complexity of classification by random projection</title><link>http://arxiv.org/abs/2108.06339v3</link><description>The generalization error of a classifier is related to the complexity of theset of functions among which the classifier is chosen. We study a family oflow-complexity classifiers consisting of thresholding a random one-dimensionalfeature. The feature is obtained by projecting the data on a random line afterembedding it into a higher-dimensional space parametrized by monomials of orderup to k. More specifically, the extended data is projected n-times and the bestclassifier among those n, based on its performance on training data, is chosen.We show that this type of classifier is extremely flexible, as it is likely toapproximate, to an arbitrary precision, any continuous function on a compactset as well as any boolean function on a compact set that splits the supportinto measurable subsets. In particular, given full knowledge of the classconditional densities, the error of these low-complexity classifiers wouldconverge to the optimal (Bayes) error as k and n go to infinity. On the otherhand, if only a training dataset is given, we show that the classifiers willperfectly classify all the training points as k and n go to infinity. We alsobound the generalization error of our random classifiers. In general, ourbounds are better than those for any classifier with VC dimension greater thanO (ln n) . In particular, our bounds imply that, unless the number ofprojections n is extremely large, there is a significant advantageous gapbetween the generalization error of the random projection approach and that ofa linear classifier in the extended space. Asymptotically, as the number ofsamples approaches infinity, the gap persists for any such n. Thus, there is apotentially large gain in generalization properties by selecting parameters atrandom, rather than optimization.</description><author>Mireille Boutin, Evzenie Coupkova</author><pubDate>Thu, 18 May 2023 16:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.06339v3</guid></item><item><title>PETAL: Physics Emulation Through Averaged Linearizations for Solving Inverse Problems</title><link>http://arxiv.org/abs/2305.11056v1</link><description>Inverse problems describe the task of recovering an underlying signal ofinterest given observables. Typically, the observables are related via somenon-linear forward model applied to the underlying unknown signal. Invertingthe non-linear forward model can be computationally expensive, as it ofteninvolves computing and inverting a linearization at a series of estimates.Rather than inverting the physics-based model, we instead train a surrogateforward model (emulator) and leverage modern auto-grad libraries to solve forthe input within a classical optimization framework. Current methods to trainemulators are done in a black box supervised machine learning fashion and failto take advantage of any existing knowledge of the forward model. In thisarticle, we propose a simple learned weighted average model that embedslinearizations of the forward model around various reference points into themodel itself, explicitly incorporating known physics. Grounding the learnedmodel with physics based linearizations improves the forward modeling accuracyand provides richer physics based gradient information during the inversionprocess leading to more accurate signal recovery. We demonstrate the efficacyon an ocean acoustic tomography (OAT) example that aims to recover ocean soundspeed profile (SSP) variations from acoustic observations (e.g. eigenrayarrival times) within simulation of ocean dynamics in the Gulf of Mexico.</description><author>Jihui Jin, Etienne Ollivier, Richard Touret, Matthew McKinley, Karim G. Sabra, Justin K. Romberg</author><pubDate>Thu, 18 May 2023 16:50:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11056v1</guid></item><item><title>Small noise analysis for Tikhonov and RKHS regularizations</title><link>http://arxiv.org/abs/2305.11055v1</link><description>Regularization plays a pivotal role in ill-posed machine learning and inverseproblems. However, the fundamental comparative analysis of variousregularization norms remains open. We establish a small noise analysisframework to assess the effects of norms in Tikhonov and RKHS regularizations,in the context of ill-posed linear inverse problems with Gaussian noise. Thisframework studies the convergence rates of regularized estimators in the smallnoise limit and reveals the potential instability of the conventionalL2-regularizer. We solve such instability by proposing an innovative class ofadaptive fractional RKHS regularizers, which covers the L2 Tikhonov and RKHSregularizations by adjusting the fractional smoothness parameter. A surprisinginsight is that over-smoothing via these fractional RKHSs consistently yieldsoptimal convergence rates, but the optimal hyper-parameter may decay too fastto be selected in practice.</description><author>Quanjun Lang, Fei Lu</author><pubDate>Thu, 18 May 2023 16:50:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11055v1</guid></item><item><title>BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval</title><link>http://arxiv.org/abs/2305.11052v1</link><description>Dense retrieval has shown promise in the first-stage retrieval process whentrained on in-domain labeled datasets. However, previous studies have foundthat dense retrieval is hard to generalize to unseen domains due to its weakmodeling of domain-invariant and interpretable feature (i.e., matching signalbetween two texts, which is the essence of information retrieval). In thispaper, we propose a novel method to improve the generalization of denseretrieval via capturing matching signal called BERM. Fully fine-grainedexpression and query-oriented saliency are two properties of the matchingsignal. Thus, in BERM, a single passage is segmented into multiple units andtwo unit-level requirements are proposed for representation as the constraintin training to obtain the effective matching signal. One is semantic unitbalance and the other is essential matching unit extractability. Unit-levelview and balanced semantics make representation express the text in afine-grained manner. Essential matching unit extractability makes passagerepresentation sensitive to the given query to extract the pure matchinginformation from the passage containing complex context. Experiments on BEIRshow that our method can be effectively combined with different dense retrievaltraining methods (vanilla, hard negatives mining and knowledge distillation) toimprove its generalization ability without any additional inference overheadand target domain data.</description><author>Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng</author><pubDate>Thu, 18 May 2023 16:43:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11052v1</guid></item><item><title>The Water Health Open Knowledge Graph</title><link>http://arxiv.org/abs/2305.11051v1</link><description>Recently, an increasing interest in the management of water and healthresources has been recorded. This interest is fed by the global sustainabilitychallenges posed to the humanity that have water scarcity and quality at theircore. Thus, the availability of effective, meaningful and open data is crucialto address those issues in the broader context of the Sustainable DevelopmentGoals of clean water and sanitation as targeted by the United Nations. In thispaper, we present the Water Health Open Knowledge Graph (WHOW-KG) along withits design methodology and analysis on impact. WHOW-KG is a semantic knowledgegraph that models data on water consumption, pollution, infectious diseaserates and drug distribution. The WHOW-KG is developed in the context of theEU-funded WHOW (Water Health Open Knowledge) project and aims at supporting awide range of applications: from knowledge discovery to decision-making, makingit a valuable resource for researchers, policymakers, and practitioners in thewater and health domains. The WHOW-KG consists of a network of five ontologiesand related linked open data, modelled according to those ontologies.</description><author>Gianluca Carletti, Elio Giulianelli, Anna Sofia Lippolis, Giorgia Lodi, Andrea Giovanni Nuzzolese, Marco Picone, Giulio Settanta</author><pubDate>Thu, 18 May 2023 16:43:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11051v1</guid></item><item><title>DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards</title><link>http://arxiv.org/abs/2304.10770v2</link><description>Exploration is a fundamental aspect of reinforcement learning (RL), and itseffectiveness is a deciding factor in the performance of RL algorithms,especially when facing sparse extrinsic rewards. Recent studies have shown theeffectiveness of encouraging exploration with intrinsic rewards estimated fromnovelties in observations. However, there is a gap between the novelty of anobservation and an exploration, as both the stochasticity in the environmentand the agent's behavior may affect the observation. To evaluate exploratorybehaviors accurately, we propose DEIR, a novel method in which we theoreticallyderive an intrinsic reward with a conditional mutual information term thatprincipally scales with the novelty contributed by agent explorations, and thenimplement the reward with a discriminative forward model. Extensive experimentson both standard and advanced exploration tasks in MiniGrid show that DEIRquickly learns a better policy than the baselines. Our evaluations on ProcGendemonstrate both the generalization capability and the general applicability ofour intrinsic reward. Our source code is available athttps://github.com/swan-utokyo/deir.</description><author>Shanchuan Wan, Yujin Tang, Yingtao Tian, Tomoyuki Kaneko</author><pubDate>Thu, 18 May 2023 16:42:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10770v2</guid></item><item><title>NODE-ImgNet: a PDE-informed effective and robust model for image denoising</title><link>http://arxiv.org/abs/2305.11049v1</link><description>Inspired by the traditional partial differential equation (PDE) approach forimage denoising, we propose a novel neural network architecture, referred asNODE-ImgNet, that combines neural ordinary differential equations (NODEs) withconvolutional neural network (CNN) blocks. NODE-ImgNet is intrinsically a PDEmodel, where the dynamic system is learned implicitly without the explicitspecification of the PDE. This naturally circumvents the typical issuesassociated with introducing artifacts during the learning process. By invokingsuch a NODE structure, which can also be viewed as a continuous variant of aresidual network (ResNet) and inherits its advantage in image denoising, ourmodel achieves enhanced accuracy and parameter efficiency. In particular, ourmodel exhibits consistent effectiveness in different scenarios, includingdenoising gray and color images perturbed by Gaussian noise, as well asreal-noisy images, and demonstrates superiority in learning from small imagedatasets.</description><author>Xinheng Xie, Yue Wu, Hao Ni, Cuiyu He</author><pubDate>Thu, 18 May 2023 16:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11049v1</guid></item><item><title>Difference of Submodular Minimization via DC Programming</title><link>http://arxiv.org/abs/2305.11046v1</link><description>Minimizing the difference of two submodular (DS) functions is a problem thatnaturally occurs in various machine learning problems. Although it is wellknown that a DS problem can be equivalently formulated as the minimization ofthe difference of two convex (DC) functions, existing algorithms do not fullyexploit this connection. A classical algorithm for DC problems is called the DCalgorithm (DCA). We introduce variants of DCA and its complete form (CDCA) thatwe apply to the DC program corresponding to DS minimization. We extend existingconvergence properties of DCA, and connect them to convergence properties onthe DS problem. Our results on DCA match the theoretical guarantees satisfiedby existing DS algorithms, while providing a more complete characterization ofconvergence properties. In the case of CDCA, we obtain a stronger localminimality guarantee. Our numerical results show that our proposed algorithmsoutperform existing baselines on two applications: speech corpus selection andfeature selection.</description><author>Marwa El Halabi, George Orfanides, Tim Hoheisel</author><pubDate>Thu, 18 May 2023 16:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11046v1</guid></item><item><title>Improving Recommendation System Serendipity Through Lexicase Selection</title><link>http://arxiv.org/abs/2305.11044v1</link><description>Recommender systems influence almost every aspect of our digital lives.Unfortunately, in striving to give us what we want, they end up restricting ouropen-mindedness. Current recommender systems promote echo chambers, wherepeople only see the information they want to see, and homophily, where users ofsimilar background see similar content. We propose a new serendipity metric tomeasure the presence of echo chambers and homophily in recommendation systemsusing cluster analysis. We then attempt to improve the diversity-preservationqualities of well known recommendation techniques by adopting a parentselection algorithm from the evolutionary computation literature known aslexicase selection. Our results show that lexicase selection, or a mixture oflexicase selection and ranking, outperforms its purely ranked counterparts interms of personalization, coverage and our specifically designed serendipitybenchmark, while only slightly under-performing in terms of accuracy (hitrate). We verify these results across a variety of recommendation list sizes.In this work we show that lexicase selection is able to maintain multiplediverse clusters of item recommendations that are each relevant for thespecific user, while still maintaining a high hit-rate accuracy, a trade offthat is not achieved by other methods.</description><author>Ryan Boldi, Aadam Lokhandwala, Edward Annatone, Yuval Schechter, Alexander Lavrenenko, Cooper Sigrist</author><pubDate>Thu, 18 May 2023 16:37:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11044v1</guid></item><item><title>A unified framework for information-theoretic generalization bounds</title><link>http://arxiv.org/abs/2305.11042v1</link><description>This paper presents a general methodology for deriving information-theoreticgeneralization bounds for learning algorithms. The main technical tool is aprobabilistic decorrelation lemma based on a change of measure and a relaxationof Young's inequality in $L_{\psi_p}$ Orlicz spaces. Using the decorrelationlemma in combination with other techniques, such as symmetrization, couplings,and chaining in the space of probability measures, we obtain new upper boundson the generalization error, both in expectation and in high probability, andrecover as special cases many of the existing generalization bounds, includingthe ones based on mutual information, conditional mutual information,stochastic chaining, and PAC-Bayes inequalities. In addition, theFernique-Talagrand upper bound on the expected supremum of a subgaussianprocess emerges as a special case.</description><author>Yifeng Chu, Maxim Raginsky</author><pubDate>Thu, 18 May 2023 16:36:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11042v1</guid></item><item><title>DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven Text-to-Image Generation</title><link>http://arxiv.org/abs/2305.03374v2</link><description>Subject-driven text-to-image generation aims to generate customized images ofthe given subject based on the text descriptions, which has drawn increasingattention recently. Existing methods mainly resort to finetuning a pretrainedgenerative model, where the identity-relevant information and theidentity-irrelevant information are entangled in the latent embedding space.However, the highly entangled latent embedding may lead to the failure ofsubject-driven text-to-image generation as follows: (i) the identity-irrelevantinformation hidden in the entangled embedding may dominate the generationprocess, resulting in the generated images heavily dependent on the irrelevantinformation while ignoring the given text descriptions; (ii) theidentity-relevant information carried in the entangled embedding can not beappropriately preserved, resulting in identity change of the subject in thegenerated images. To tackle the problems, we propose DisenBooth, anidentity-preserving disentangled tuning framework for subject-driventext-to-image generation in this paper. Specifically, DisenBooth finetunes thepretrained diffusion model in the denoising process. Different from previousworks that utilize an entangled embedding to denoise each image, DisenBoothinstead utilizes disentangled embeddings to respectively preserve the subjectidentity and capture the identity-irrelevant information. We further design thenovel weak denoising and contrastive embedding auxiliary tuning objectives toachieve the disentanglement. Extensive experiments show that our proposedDisenBooth framework outperforms baseline models for subject-driventext-to-image generation with the identity-preserved embedding. Additionally,by combining the identity-preserved embedding and identity-irrelevantembedding, DisenBooth demonstrates more generation flexibility andcontrollability.</description><author>Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, Wenwu Zhu</author><pubDate>Thu, 18 May 2023 16:36:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03374v2</guid></item><item><title>High-dimensional Asymptotics of Denoising Autoencoders</title><link>http://arxiv.org/abs/2305.11041v1</link><description>We address the problem of denoising data from a Gaussian mixture using atwo-layer non-linear autoencoder with tied weights and a skip connection. Weconsider the high-dimensional limit where the number of training samples andthe input dimension jointly tend to infinity while the number of hidden unitsremains bounded. We provide closed-form expressions for the denoisingmean-squared test error. Building on this result, we quantitativelycharacterize the advantage of the considered architecture over the autoencoderwithout the skip connection that relates closely to principal componentanalysis. We further show that our results accurately capture the learningcurves on a range of real data sets.</description><author>Hugo Cui, Lenka Zdeborová</author><pubDate>Thu, 18 May 2023 16:35:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11041v1</guid></item><item><title>CLUSTSEG: Clustering for Universal Segmentation</title><link>http://arxiv.org/abs/2305.02187v2</link><description>We present CLUSTSEG, a general, transformer-based framework that tacklesdifferent image segmentation tasks (i.e., superpixel, semantic, instance, andpanoptic) through a unified neural clustering scheme. Regarding queries ascluster centers, CLUSTSEG is innovative in two aspects:1) cluster centers areinitialized in heterogeneous ways so as to pointedly address task-specificdemands (e.g., instance- or category-level distinctiveness), yet withoutmodifying the architecture; and 2) pixel-cluster assignment, formalized in across-attention fashion, is alternated with cluster center update, yet withoutlearning additional parameters. These innovations closely link CLUSTSEG to EMclustering and make it a transparent and powerful framework that yieldssuperior results across the above segmentation tasks.</description><author>James Liang, Tianfei Zhou, Dongfang Liu, Wenguan Wang</author><pubDate>Thu, 18 May 2023 16:34:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02187v2</guid></item><item><title>Simulation of a Variational Quantum Perceptron using Grover's Algorithm</title><link>http://arxiv.org/abs/2305.11040v1</link><description>The quantum perceptron, the variational circuit, and the Grover algorithmhave been proposed as promising components for quantum machine learning. Thispaper presents a new quantum perceptron that combines the quantum variationalcircuit and the Grover algorithm. However, this does not guarantee that thisquantum variational perceptron with Grover's algorithm (QVPG) will have anyadvantage over its quantum variational (QVP) and classical counterparts. Here,we examine the performance of QVP and QVP-G by computing their loss functionand analyzing their accuracy on the classification task, then comparing thesetwo quantum models to the classical perceptron (CP). The results show that ourtwo quantum models are more efficient than CP, and our novel suggested modelQVP-G outperforms the QVP, demonstrating that the Grover can be applied to theclassification task and even makes the model more accurate, besides theunstructured search problems.</description><author>Nouhaila Innan, Mohamed Bennai</author><pubDate>Thu, 18 May 2023 16:34:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11040v1</guid></item><item><title>Parallel Residual Bi-Fusion Feature Pyramid Network for Accurate Single-Shot Object Detection</title><link>http://arxiv.org/abs/2012.01724v5</link><description>This paper proposes the Parallel Residual Bi-Fusion Feature Pyramid Network(PRB-FPN) for fast and accurate single-shot object detection. Feature Pyramid(FP) is widely used in recent visual detection, however the top-down pathway ofFP cannot preserve accurate localization due to pooling shifting. The advantageof FP is weakened as deeper backbones with more layers are used. In addition,it cannot keep up accurate detection of both small and large objects at thesame time. To address these issues, we propose a new parallel FP structure withbi-directional (top-down and bottom-up) fusion and associated improvements toretain high-quality features for accurate localization. We provide thefollowing design improvements: (1) A parallel bifusion FP structure with abottom-up fusion module (BFM) to detect both small and large objects at oncewith high accuracy. (2) A concatenation and re-organization (CORE) moduleprovides a bottom-up pathway for feature fusion, which leads to thebi-directional fusion FP that can recover lost information from lower-layerfeature maps. (3) The CORE feature is further purified to retain richercontextual information. Such CORE purification in both top-down and bottom-uppathways can be finished in only a few iterations. (4) The adding of a residualdesign to CORE leads to a new Re-CORE module that enables easy training andintegration with a wide range of deeper or lighter backbones. The proposednetwork achieves state-of-the-art performance on the UAVDT17 and MS COCOdatasets. Code is available at https://github.com/pingyang1117/PRBNet_PyTorch.</description><author>Ping-Yang Chen, Ming-Ching Chang, Jun-Wei Hsieh, Yong-Sheng Chen</author><pubDate>Thu, 18 May 2023 16:33:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.01724v5</guid></item><item><title>Deep PackGen: A Deep Reinforcement Learning Framework for Adversarial Network Packet Generation</title><link>http://arxiv.org/abs/2305.11039v1</link><description>Recent advancements in artificial intelligence (AI) and machine learning (ML)algorithms, coupled with the availability of faster computing infrastructure,have enhanced the security posture of cybersecurity operations centers(defenders) through the development of ML-aided network intrusion detectionsystems (NIDS). Concurrently, the abilities of adversaries to evade securityhave also increased with the support of AI/ML models. Therefore, defenders needto proactively prepare for evasion attacks that exploit the detectionmechanisms of NIDS. Recent studies have found that the perturbation offlow-based and packet-based features can deceive ML models, but theseapproaches have limitations. Perturbations made to the flow-based features aredifficult to reverse-engineer, while samples generated with perturbations tothe packet-based features are not playable. Our methodological framework, Deep PackGen, employs deep reinforcementlearning to generate adversarial packets and aims to overcome the limitationsof approaches in the literature. By taking raw malicious network packets asinputs and systematically making perturbations on them, Deep PackGencamouflages them as benign packets while still maintaining their functionality.In our experiments, using publicly available data, Deep PackGen achieved anaverage adversarial success rate of 66.4\% against various ML models and acrossdifferent attack types. Our investigation also revealed that more than 45\% ofthe successful adversarial samples were out-of-distribution packets that evadedthe decision boundaries of the classifiers. The knowledge gained from our studyon the adversary's ability to make specific evasive perturbations to differenttypes of malicious packets can help defenders enhance the robustness of theirNIDS against evolving adversarial attacks.</description><author>Soumyadeep Hore, Jalal Ghadermazi, Diwas Paudel, Ankit Shah, Tapas K. Das, Nathaniel D. Bastian</author><pubDate>Thu, 18 May 2023 16:32:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11039v1</guid></item><item><title>Learning In-context Learning for Named Entity Recognition</title><link>http://arxiv.org/abs/2305.11038v1</link><description>Named entity recognition in real-world applications suffers from thediversity of entity types, the emergence of new entity types, and the lack ofhigh-quality annotations. To address the above problems, this paper proposes anin-context learning-based NER approach, which can effectively inject in-contextNER ability into PLMs and recognize entities of novel types on-the-fly usingonly a few demonstrative instances. Specifically, we model PLMs as ameta-function $\mathcal{ \lambda_ {\text{instruction, demonstrations, text}}.M}$, and a new entity extractor can be implicitly constructed by applying newinstruction and demonstrations to PLMs, i.e., $\mathcal{ (\lambda . M)}$(instruction, demonstrations) $\to$ $\mathcal{F}$ where $\mathcal{F}$ will bea new entity extractor, i.e., $\mathcal{F}$: text $\to$ entities. To inject theabove in-context NER ability into PLMs, we propose a meta-function pre-trainingalgorithm, which pre-trains PLMs by comparing the (instruction,demonstration)-initialized extractor with a surrogate golden extractor.Experimental results on 4 few-shot NER datasets show that our method caneffectively inject in-context NER ability into PLMs and significantlyoutperforms the PLMs+fine-tuning counterparts.</description><author>Jiawei Chen, Yaojie Lu, Hongyu Lin, Jie Lou, Wei Jia, Dai Dai, Hua Wu, Boxi Cao, Xianpei Han, Le Sun</author><pubDate>Thu, 18 May 2023 16:31:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11038v1</guid></item><item><title>Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement</title><link>http://arxiv.org/abs/2305.11034v1</link><description>State-of-the-art target-oriented opinion word extraction (TOWE) modelstypically use BERT-based text encoders that operate on the word level, alongwith graph convolutional networks (GCNs) that incorporate syntactic informationextracted from syntax trees. These methods achieve limited gains with GCNs andhave difficulty using BERT wordpieces. Meanwhile, BERT wordpieces are known tobe effective at representing rare words or words with insufficient contextinformation. To address this issue, this work trades syntax trees for BERTwordpieces by entirely removing the GCN component from the methods'architectures. To enhance TOWE performance, we tackle the issue of aspectrepresentation loss during encoding. Instead of solely utilizing a sentence asthe input, we use a sentence-aspect pair. Our relatively simple approachachieves state-of-the-art results on benchmark datasets and should serve as astrong baseline for further research.</description><author>Samuel Mensah, Kai Sun, Nikolaos Aletras</author><pubDate>Thu, 18 May 2023 16:22:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11034v1</guid></item><item><title>Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature</title><link>http://arxiv.org/abs/2305.11033v1</link><description>Visual Question Answering (VQA) is an emerging area of interest forresearches, being a recent problem in natural language processing and imageprediction. In this area, an algorithm needs to answer questions about certainimages. As of the writing of this survey, 25 recent studies were analyzed.Besides, 6 datasets were analyzed and provided their link to download. In thiswork, several recent pieces of research in this area were investigated and adeeper analysis and comparison among them were provided, including results, thestate-of-the-art, common errors, and possible points of improvement for futureresearchers.</description><author>Ana Cláudia Akemi Matsuki de Faria, Felype de Castro Bastos, José Victor Nogueira Alves da Silva, Vitor Lopes Fabris, Valeska de Sousa Uchoa, Décio Gonçalves de Aguiar Neto, Claudio Filipi Goncalves dos Santos</author><pubDate>Thu, 18 May 2023 16:20:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11033v1</guid></item><item><title>Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL</title><link>http://arxiv.org/abs/2305.11032v1</link><description>While policy optimization algorithms have played an important role in recentempirical success of Reinforcement Learning (RL), the existing theoreticalunderstanding of policy optimization remains rather limited -- they are eitherrestricted to tabular MDPs or suffer from highly suboptimal sample complexity,especial in online RL where exploration is necessary. This paper proposes asimple efficient policy optimization framework -- Optimistic NPG for online RL.Optimistic NPG can be viewed as simply combining of the classic natural policygradient (NPG) algorithm [Kakade, 2001] with optimistic policy evaluationsubroutines to encourage exploration. For $d$-dimensional linear MDPs,Optimistic NPG is computationally efficient, and learns an$\varepsilon$-optimal policy within $\tilde{O}(d^2/\varepsilon^3)$ samples,which is the first computationally efficient algorithm whose sample complexityhas the optimal dimension dependence $\tilde{\Theta}(d^2)$. It also improvesover state-of-the-art results of policy optimization algorithms [Zanette etal., 2021] by a factor of $d$. For general function approximation that subsumeslinear MDPs, Optimistic NPG, to our best knowledge, is also the first policyoptimization algorithm that achieves the polynomial sample complexity forlearning near-optimal policies.</description><author>Qinghua Liu, Gellért Weisz, András György, Chi Jin, Csaba Szepesvári</author><pubDate>Thu, 18 May 2023 16:19:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11032v1</guid></item><item><title>Subjective and Objective Quality Assessment for in-the-Wild Computer Graphics Images</title><link>http://arxiv.org/abs/2303.08050v2</link><description>Computer graphics images (CGIs) are artificially generated by means ofcomputer programs and are widely perceived under various scenarios, such asgames, streaming media, etc. In practice, the quality of CGIs consistentlysuffers from poor rendering during production, inevitable compression artifactsduring the transmission of multimedia applications, and low aesthetic qualityresulting from poor composition and design. However, few works have beendedicated to dealing with the challenge of computer graphics image qualityassessment (CGIQA). Most image quality assessment (IQA) metrics are developedfor natural scene images (NSIs) and validated on databases consisting of NSIswith synthetic distortions, which are not suitable for in-the-wild CGIs. Tobridge the gap between evaluating the quality of NSIs and CGIs, we construct alarge-scale in-the-wild CGIQA database consisting of 6,000 CGIs (CGIQA-6k) andcarry out the subjective experiment in a well-controlled laboratory environmentto obtain the accurate perceptual ratings of the CGIs. Then, we propose aneffective deep learning-based no-reference (NR) IQA model by utilizing bothdistortion and aesthetic quality representation. Experimental results show thatthe proposed method outperforms all other state-of-the-art NR IQA methods onthe constructed CGIQA-6k database and other CGIQA-related databases. Thedatabase will be released to facilitate further research.</description><author>Zicheng Zhang, Wei Sun, Tao Wang, Wei Lu, Quan Zhou, Jun he, Qiyuan Wang, Xiongkuo Min, Guangtao Zhai</author><pubDate>Thu, 18 May 2023 16:19:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08050v2</guid></item><item><title>ConsistentNeRF: Enhancing Neural Radiance Fields with 3D Consistency for Sparse View Synthesis</title><link>http://arxiv.org/abs/2305.11031v1</link><description>Neural Radiance Fields (NeRF) has demonstrated remarkable 3D reconstructioncapabilities with dense view images. However, its performance significantlydeteriorates under sparse view settings. We observe that learning the 3Dconsistency of pixels among different views is crucial for improvingreconstruction quality in such cases. In this paper, we propose ConsistentNeRF,a method that leverages depth information to regularize both multi-view andsingle-view 3D consistency among pixels. Specifically, ConsistentNeRF employsdepth-derived geometry information and a depth-invariant loss to concentrate onpixels that exhibit 3D correspondence and maintain consistent depthrelationships. Extensive experiments on recent representative works reveal thatour approach can considerably enhance model performance in sparse viewconditions, achieving improvements of up to 94% in PSNR, 76% in SSIM, and 31%in LPIPS compared to the vanilla baselines across various benchmarks, includingDTU, NeRF Synthetic, and LLFF.</description><author>Shoukang Hu, Kaichen Zhou, Kaiyu Li, Longhui Yu, Lanqing Hong, Tianyang Hu, Zhenguo Li, Gim Hee Lee, Ziwei Liu</author><pubDate>Thu, 18 May 2023 16:18:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11031v1</guid></item><item><title>Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction</title><link>http://arxiv.org/abs/2305.11029v1</link><description>Document-level relation extraction (DocRE) aims to infer complex semanticrelations among entities in a document. Distant supervision (DS) is able togenerate massive auto-labeled data, which can improve DocRE performance. Recentworks leverage pseudo labels generated by the pre-denoising model to reducenoise in DS data. However, unreliable pseudo labels bring new noise, e.g.,adding false pseudo labels and losing correct DS labels. Therefore, how toselect effective pseudo labels to denoise DS data is still a challenge indocument-level distant relation extraction. To tackle this issue, we introduceuncertainty estimation technology to determine whether pseudo labels can betrusted. In this work, we propose a Document-level distant Relation Extractionframework with Uncertainty Guided label denoising, UGDRE. Specifically, wepropose a novel instance-level uncertainty estimation method, which measuresthe reliability of the pseudo labels with overlapping relations. By furtherconsidering the long-tail problem, we design dynamic uncertainty thresholds fordifferent types of relations to filter high-uncertainty pseudo labels. Weconduct experiments on two public datasets. Our framework outperforms strongbaselines by 1.91 F1 and 2.28 Ign F1 on the RE-DocRED dataset.</description><author>Qi Sun, Kun Huang, Xiaocui Yang, Pengfei Hong, Kun Zhang, Soujanya Poria</author><pubDate>Thu, 18 May 2023 16:15:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11029v1</guid></item><item><title>Expected Gradients of Maxout Networks and Consequences to Parameter Initialization</title><link>http://arxiv.org/abs/2301.06956v2</link><description>We study the gradients of a maxout network with respect to inputs andparameters and obtain bounds for the moments depending on the architecture andthe parameter distribution. We observe that the distribution of theinput-output Jacobian depends on the input, which complicates a stableparameter initialization. Based on the moments of the gradients, we formulateparameter initialization strategies that avoid vanishing and explodinggradients in wide networks. Experiments with deep fully-connected andconvolutional networks show that this strategy improves SGD and Adam trainingof deep maxout networks. In addition, we obtain refined bounds on the expectednumber of linear regions, results on the expected curve length distortion, andresults on the NTK.</description><author>Hanna Tseran, Guido Montúfar</author><pubDate>Thu, 18 May 2023 16:08:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.06956v2</guid></item><item><title>CDIDN: A Registration Model with High Deformation Impedance Capability for Long-Term Tracking of Pulmonary Lesion Dynamics</title><link>http://arxiv.org/abs/2305.11024v1</link><description>We study the problem of registration for medical CT images from a novelperspective -- the sensitivity to degree of deformations in CT images. Althoughsome learning-based methods have shown success in terms of average accuracy,their ability to handle regions with local large deformation (LLD) maysignificantly decrease compared to dealing with regions with minor deformation.This motivates our research into this issue. Two main causes of LLDs are organmotion and changes in tissue structure, with the latter often being a long-termprocess. In this paper, we propose a novel registration model calledCascade-Dilation Inter-Layer Differential Network (CDIDN), which exhibits bothhigh deformation impedance capability (DIC) and accuracy. CDIDN improves itsresilience to LLDs in CT images by enhancing LLDs in the displacement field(DF). It uses a feature-based progressive decomposition of LLDs, blendingfeature flows of different levels into a main flow in a top-down manner. Itleverages Inter-Layer Differential Module (IDM) at each level to locally refinethe main flow and globally smooth the feature flow, and also integrates featurevelocity fields that can effectively handle feature deformations of variousdegrees. We assess CDIDN using lungs as representative organs with largedeformation. Our findings show that IDM significantly enhances LLDs of the DF,by which improves the DIC and accuracy of the model. Compared with otheroutstanding learning-based methods, CDIDN exhibits the best DIC and excellentaccuracy. Based on vessel enhancement and enhanced LLDs of the DF, we propose anovel method to accurately track the appearance, disappearance, enlargement,and shrinkage of pulmonary lesions, which effectively addresses detection ofearly lesions and peripheral lung lesions, issues of false enlargement, falseshrinkage, and mutilation of lesions.</description><author>Xinyu Zhao, Sa Huang, Wei Pang, You Zhou</author><pubDate>Thu, 18 May 2023 16:05:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11024v1</guid></item><item><title>Generalized Multiple Intent Conditioned Slot Filling</title><link>http://arxiv.org/abs/2305.11023v1</link><description>Natural language understanding includes the tasks of intent detection(identifying a user's objectives) and slot filling (extracting the entitiesrelevant to those objectives). Prior slot filling methods assume that eachintent type cannot occur more than once within a message, however this is oftennot a valid assumption for real-world settings. In this work, we generalizeslot filling by removing the constraint of unique intents in a message. We castthis as a JSON generation task and approach it using a language model. Wecreate a pre-training dataset by combining DBpedia and existing slot fillingdatasets that we convert for JSON generation. We also generate an in-domaindataset using GPT-3. We train T5 models for this task (with and withoutexemplars in the prompt) and find that both training datasets improveperformance, and that the model is able to generalize to intent types not seenduring training.</description><author>Harshil Shah, Arthur Wilcke, Marius Cobzarenco, Cristi Cobzarenco, Edward Challis, David Barber</author><pubDate>Thu, 18 May 2023 16:04:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11023v1</guid></item><item><title>Massively Parallel Reweighted Wake-Sleep</title><link>http://arxiv.org/abs/2305.11022v1</link><description>Reweighted wake-sleep (RWS) is a machine learning method for performingBayesian inference in a very general class of models. RWS draws $K$ samplesfrom an underlying approximate posterior, then uses importance weighting toprovide a better estimate of the true posterior. RWS then updates itsapproximate posterior towards the importance-weighted estimate of the trueposterior. However, recent work [Chattergee and Diaconis, 2018] indicates thatthe number of samples required for effective importance weighting isexponential in the number of latent variables. Attaining such a large number ofimportance samples is intractable in all but the smallest models. Here, wedevelop massively parallel RWS, which circumvents this issue by drawing $K$samples of all $n$ latent variables, and individually reasoning about all $K^n$possible combinations of samples. While reasoning about $K^n$ combinationsmight seem intractable, the required computations can be performed inpolynomial time by exploiting conditional independencies in the generativemodel. We show considerable improvements over standard "global" RWS, whichdraws $K$ samples from the full joint.</description><author>Thomas Heap, Gavin Leech, Laurence Aitchison</author><pubDate>Thu, 18 May 2023 16:03:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11022v1</guid></item><item><title>Annotation-free Audio-Visual Segmentation</title><link>http://arxiv.org/abs/2305.11019v1</link><description>The objective of Audio-Visual Segmentation (AVS) is to locate soundingobjects within visual scenes by accurately predicting pixelwise segmentationmasks. In this paper, we present the following contributions: (i), we propose ascalable and annotation-free pipeline for generating artificial data for theAVS task. We leverage existing image segmentation and audio datasets to drawlinks between category labels, image-mask pairs, and audio samples, whichallows us to easily compose (image, audio, mask) triplets for training AVSmodels; (ii), we introduce a novel Audio-Aware Transformer (AuTR) architecturethat features an audio-aware query-based transformer decoder. This architectureenables the model to search for sounding objects with the guidance of audiosignals, resulting in more accurate segmentation; (iii), we present extensiveexperiments conducted on both synthetic and real datasets, which demonstratethe effectiveness of training AVS models with synthetic data generated by ourproposed pipeline. Additionally, our proposed AuTR architecture exhibitssuperior performance and strong generalization ability on public benchmarks.The project page is https://jinxiang-liu.github.io/anno-free-AVS/.</description><author>Jinxiang Liu, Yu Wang, Chen Ju, Ya Zhang, Weidi Xie</author><pubDate>Thu, 18 May 2023 15:52:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11019v1</guid></item><item><title>Deep Metric Tensor Regularized Policy Gradient</title><link>http://arxiv.org/abs/2305.11017v1</link><description>Policy gradient algorithms are an important family of deep reinforcementlearning techniques. Many past research endeavors focused on using thefirst-order policy gradient information to train policy networks. Differentfrom these works, we conduct research in this paper driven by the believe thatproperly utilizing and controlling Hessian information associated with thepolicy gradient can noticeably improve the performance of policy gradientalgorithms. One key Hessian information that attracted our attention is theHessian trace, which gives the divergence of the policy gradient vector fieldin the Euclidean policy parametric space. We set the goal to generalize thisEuclidean policy parametric space into a general Riemmanian manifold byintroducing a metric tensor field $g_ab$ in the parametric space. This isachieved through newly developed mathematical tools, deep learning algorithms,and metric tensor deep neural networks (DNNs). Armed with these technicaldevelopments, we propose a new policy gradient algorithm that learns tominimize the absolute divergence in the Riemannian manifold as an importantregularization mechanism, allowing the Riemannian manifold to smoothen itspolicy gradient vector field. The newly developed algorithm is experimentallystudied on several benchmark reinforcement learning problems. Our experimentsclearly show that the new metric tensor regularized algorithm can significantlyoutperform its counterpart that does not use our regularization technique.Additional experimental analysis further suggests that the trained metrictensor DNN and the corresponding metric tensor $g_{ab}$ can effectively reducethe absolute divergence towards zero in the Riemannian manifold.</description><author>Gang Chen, Victoria Huang</author><pubDate>Thu, 18 May 2023 15:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11017v1</guid></item><item><title>Silver Syntax Pre-training for Cross-Domain Relation Extraction</title><link>http://arxiv.org/abs/2305.11016v1</link><description>Relation Extraction (RE) remains a challenging task, especially whenconsidering realistic out-of-domain evaluations. One of the main reasons forthis is the limited training size of current RE datasets: obtaininghigh-quality (manually annotated) data is extremely expensive and cannotrealistically be repeated for each new domain. An intermediate training step ondata from related tasks has shown to be beneficial across many NLPtasks.However, this setup still requires supplementary annotated data, which isoften not available. In this paper, we investigate intermediate pre-trainingspecifically for RE. We exploit the affinity between syntactic structure andsemantic RE, and identify the syntactic relations which are closely related toRE by being on the shortest dependency path between two entities. We then takeadvantage of the high accuracy of current syntactic parsers in order toautomatically obtain large amounts of low-cost pre-training data. Bypre-training our RE model on the relevant syntactic relations, we are able tooutperform the baseline in five out of six cross-domain setups, without anyadditional annotated data.</description><author>Elisa Bassignana, Filip Ginter, Sampo Pyysalo, Rob van der Goot, Barbara Plank</author><pubDate>Thu, 18 May 2023 15:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11016v1</guid></item><item><title>Generalized Planning in PDDL Domains with Pretrained Large Language Models</title><link>http://arxiv.org/abs/2305.11014v1</link><description>Recent work has considered whether large language models (LLMs) can functionas planners: given a task, generate a plan. We investigate whether LLMs canserve as generalized planners: given a domain and training tasks, generate aprogram that efficiently produces plans for other tasks in the domain. Inparticular, we consider PDDL domains and use GPT-4 to synthesize Pythonprograms. We also consider (1) Chain-of-Thought (CoT) summarization, where theLLM is prompted to summarize the domain and propose a strategy in words beforesynthesizing the program; and (2) automated debugging, where the program isvalidated with respect to the training tasks, and in case of errors, the LLM isre-prompted with four types of feedback. We evaluate this approach in sevenPDDL domains and compare it to four ablations and four baselines. Overall, wefind that GPT-4 is a surprisingly powerful generalized planner. We alsoconclude that automated debugging is very important, that CoT summarization hasnon-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just twotraining tasks are often sufficient for strong generalization.</description><author>Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B. Tenenbaum, Leslie Pack Kaelbling, Michael Katz</author><pubDate>Thu, 18 May 2023 15:48:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11014v1</guid></item><item><title>Sparse joint shift in multinomial classification</title><link>http://arxiv.org/abs/2303.16971v2</link><description>Sparse joint shift (SJS) was recently proposed as a tractable model forgeneral dataset shift which may cause changes to the marginal distributions offeatures and labels as well as the posterior probabilities and theclass-conditional feature distributions. Fitting SJS for a target datasetwithout label observations may produce valid predictions of labels andestimates of class prior probabilities. We present new results on thetransmission of SJS from sets of features to larger sets of features, aconditional correction formula for the class posterior probabilities under thetarget distribution, identifiability of SJS, and the relationship between SJSand covariate shift. In addition, we point out inconsistencies in thealgorithms which were proposed for estimating the characteristics of SJS, asthey could hamper the search for optimal solutions.</description><author>Dirk Tasche</author><pubDate>Thu, 18 May 2023 15:47:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16971v2</guid></item><item><title>FunASR: A Fundamental End-to-End Speech Recognition Toolkit</title><link>http://arxiv.org/abs/2305.11013v1</link><description>This paper introduces FunASR, an open-source speech recognition toolkitdesigned to bridge the gap between academic research and industrialapplications. FunASR offers models trained on large-scale industrial corporaand the ability to deploy them in applications. The toolkit's flagship model,Paraformer, is a non-autoregressive end-to-end speech recognition model thathas been trained on a manually annotated Mandarin speech recognition datasetthat contains 60,000 hours of speech. To improve the performance of Paraformer,we have added timestamp prediction and hotword customization capabilities tothe standard Paraformer backbone. In addition, to facilitate model deployment,we have open-sourced a voice activity detection model based on the FeedforwardSequential Memory Network (FSMN-VAD) and a text post-processing punctuationmodel based on the controllable time-delay Transformer (CT-Transformer), bothof which were trained on industrial corpora. These functional modules provide asolid foundation for building high-precision long audio speech recognitionservices. Compared to other models trained on open datasets, Paraformerdemonstrates superior performance.</description><author>Zhifu Gao, Zerui Li, Jiaming Wang, Haoneng Luo, Xian Shi, Mengzhe Chen, Yabin Li, Lingyun Zuo, Zhihao Du, Zhangyu Xiao, Shiliang Zhang</author><pubDate>Thu, 18 May 2023 15:45:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11013v1</guid></item><item><title>SDC-UDA: Volumetric Unsupervised Domain Adaptation Framework for Slice-Direction Continuous Cross-Modality Medical Image Segmentation</title><link>http://arxiv.org/abs/2305.11012v1</link><description>Recent advances in deep learning-based medical image segmentation studiesachieve nearly human-level performance in fully supervised manner. However,acquiring pixel-level expert annotations is extremely expensive and laboriousin medical imaging fields. Unsupervised domain adaptation (UDA) can alleviatethis problem, which makes it possible to use annotated data in one imagingmodality to train a network that can successfully perform segmentation ontarget imaging modality with no labels. In this work, we propose SDC-UDA, asimple yet effective volumetric UDA framework for slice-direction continuouscross-modality medical image segmentation which combines intra- and inter-sliceself-attentive image translation, uncertainty-constrained pseudo-labelrefinement, and volumetric self-training. Our method is distinguished fromprevious methods on UDA for medical image segmentation in that it can obtaincontinuous segmentation in the slice direction, thereby ensuring higheraccuracy and potential in clinical practice. We validate SDC-UDA with multiplepublicly available cross-modality medical image segmentation datasets andachieve state-of-the-art segmentation performance, not to mention the superiorslice-direction continuity of prediction compared to previous studies.</description><author>Hyungseob Shin, Hyeongyu Kim, Sewon Kim, Yohan Jun, Taejoon Eo, Dosik Hwang</author><pubDate>Thu, 18 May 2023 15:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11012v1</guid></item><item><title>Gradient-based Intra-attention Pruning on Pre-trained Language Models</title><link>http://arxiv.org/abs/2212.07634v2</link><description>Pre-trained language models achieve superior performance but arecomputationally expensive. Techniques such as pruning and knowledgedistillation have been developed to reduce their sizes and latencies. In thiswork, we propose a structured pruning method GRAIN (Gradient-basedIntra-attention pruning), which performs task-specific pruning with knowledgedistillation and yields highly effective models. Different from commonapproaches that prune each attention head as a whole, GRAIN inspects and prunesintra-attention structures, which greatly expands the structure search spaceand enables more flexible models. We also propose a gradient separationstrategy that reduces the interference of distillation on pruning for a bettercombination of the two approaches. Experiments on GLUE, SQuAD, and CoNLL 2003show that GRAIN notably outperforms other methods, especially in the highsparsity regime, and achieves $6\sim7\times$ speedups while maintaining$93\%\sim99\%$ performance. Under extreme compression where only $3\%$transformer weights remain, the pruned model is still competitive compared tolarger models.</description><author>Ziqing Yang, Yiming Cui, Xin Yao, Shijin Wang</author><pubDate>Thu, 18 May 2023 15:41:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07634v2</guid></item><item><title>Wide-Area Geolocalization with a Limited Field of View Camera</title><link>http://arxiv.org/abs/2209.11854v2</link><description>Cross-view geolocalization, a supplement or replacement for GPS, localizes anagent within a search area by matching images taken from a ground-view camerato overhead images taken from satellites or aircraft. Although the viewpointdisparity between ground and overhead images makes cross-view geolocalizationchallenging, significant progress has been made assuming that the ground agenthas access to a panoramic camera. For example, our prior work (WAG) introducedchanges in search area discretization, training loss, and particle filterweighting that enabled city-scale panoramic cross-view geolocalization.However, panoramic cameras are not widely used in existing robotic platformsdue to their complexity and cost. Non-panoramic cross-view geolocalization ismore applicable for robotics, but is also more challenging. This paper presentsRestricted FOV Wide-Area Geolocalization (ReWAG), a cross-view geolocalizationapproach that generalizes WAG for use with standard, non-panoramic groundcameras by creating pose-aware embeddings and providing a strategy toincorporate particle pose into the Siamese network. ReWAG is a neural networkand particle filter system that is able to globally localize a mobile agent ina GPS-denied environment with only odometry and a 90 degree FOV camera,achieving similar localization accuracy as what WAG achieved with a panoramiccamera and improving localization accuracy by a factor of 100 compared to abaseline vision transformer (ViT) approach. A video highlight that demonstratesReWAG's convergence on a test path of several dozen kilometers is available athttps://youtu.be/U_OBQrt8qCE.</description><author>Lena M. Downes, Ted J. Steiner, Rebecca L. Russell, Jonathan P. How</author><pubDate>Thu, 18 May 2023 15:41:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.11854v2</guid></item><item><title>GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation</title><link>http://arxiv.org/abs/2204.06674v4</link><description>Recent improvements in KG-to-text generation are due to additional auxiliarypre-training tasks designed to give the fine-tune task a boost in performance.These tasks require extensive computational resources while only suggestingmarginal improvements. Here, we demonstrate that by fusing graph-aware elementsinto existing pre-trained language models, we are able to outperformstate-of-the-art models and close the gap imposed by additional pre-trainingtasks. We do so by proposing a mask structure to capture neighborhoodinformation and a novel type encoder that adds a bias to the graph-attentionweights depending on the connection type. Experiments on two KG-to-textbenchmark datasets show our models are competitive while involving fewerparameters and no additional pre-training tasks. By formulating the problem asa framework, we can interchange the various proposed components and begininterpreting KG-to-text generative models based on the topological and typeinformation found in a graph.</description><author>Anthony Colas, Mehrdad Alvandipour, Daisy Zhe Wang</author><pubDate>Thu, 18 May 2023 15:36:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.06674v4</guid></item><item><title>Mode Connectivity in Auction Design</title><link>http://arxiv.org/abs/2305.11005v1</link><description>Optimal auction design is a fundamental problem in algorithmic game theory.This problem is notoriously difficult already in very simple settings. Recentwork in differentiable economics showed that neural networks can efficientlylearn known optimal auction mechanisms and discover interesting new ones. In anattempt to theoretically justify their empirical success, we focus on one ofthe first such networks, RochetNet, and a generalized version for affinemaximizer auctions. We prove that they satisfy mode connectivity, i.e., locallyoptimal solutions are connected by a simple, piecewise linear path such thatevery solution on the path is almost as good as one of the two local optima.Mode connectivity has been recently investigated as an intriguing empirical andtheoretically justifiable property of neural networks used for predictionproblems. Our results give the first such analysis in the context ofdifferentiable economics, where neural networks are used directly for solvingnon-convex optimization problems.</description><author>Christoph Hertrich, Yixin Tao, László A. Végh</author><pubDate>Thu, 18 May 2023 15:36:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11005v1</guid></item><item><title>Taxonomy Completion with Probabilistic Scorer via Box Embedding</title><link>http://arxiv.org/abs/2305.11004v1</link><description>Taxonomy completion, a task aimed at automatically enriching an existingtaxonomy with new concepts, has gained significant interest in recent years.Previous works have introduced complex modules, external information, andpseudo-leaves to enrich the representation and unify the matching process ofattachment and insertion. While they have achieved good performance, theseintroductions may have brought noise and unfairness during training andscoring. In this paper, we present TaxBox, a novel framework for taxonomycompletion that maps taxonomy concepts to box embeddings and employs twoprobabilistic scorers for concept attachment and insertion, avoiding the needfor pseudo-leaves. Specifically, TaxBox consists of three components: (1) agraph aggregation module to leverage the structural information of the taxonomyand two lightweight decoders that map features to box embedding and capturecomplex relationships between concepts; (2) two probabilistic scorers thatcorrespond to attachment and insertion operations and ensure the avoidance ofpseudo-leaves; and (3) three learning objectives that assist the model inmapping concepts more granularly onto the box embedding space. Experimentalresults on four real-world datasets suggest that TaxBox outperforms baselinemethods by a considerable margin and surpasses previous state-of-art methods toa certain extent.</description><author>Wei Xue, Yongliang Shen, Wenqi Ren, Jietian Guo, Siliang Pu, Weiming Lu</author><pubDate>Thu, 18 May 2023 15:34:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11004v1</guid></item><item><title>Learning Functional Transduction</title><link>http://arxiv.org/abs/2302.00328v2</link><description>Research in machine learning has polarized into two general approaches forregression tasks: Transductive methods construct estimates directly fromavailable data but are usually problem unspecific. Inductive methods can bemuch more specific but generally require compute-intensive solution searches.In this work, we propose a hybrid approach and show that transductiveregression principles can be meta-learned through gradient descent to formefficient in-context neural approximators by leveraging the theory ofvector-valued Reproducing Kernel Banach Spaces (RKBS). We apply this approachto function spaces defined over finite and infinite-dimensional spaces(function-valued operators) and show that once trained, the Transducer canalmost instantaneously capture an infinity of functional relationships given afew pairs of input and output examples and return new image estimates. Wedemonstrate the benefit of our meta-learned transductive approach to modelcomplex physical systems influenced by varying external factors with littledata at a fraction of the usual deep learning training computational cost forpartial differential equations and climate modeling applications.</description><author>Mathieu Chalvidal, Thomas Serre, Rufin VanRullen</author><pubDate>Thu, 18 May 2023 15:34:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00328v2</guid></item><item><title>Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping</title><link>http://arxiv.org/abs/2305.11003v1</link><description>Weakly-Supervised Concealed Object Segmentation (WSCOS) aims to segmentobjects well blended with surrounding environments using sparsely-annotateddata for model training. It remains a challenging task since (1) it is hard todistinguish concealed objects from the background due to the intrinsicsimilarity and (2) the sparsely-annotated training data only provide weaksupervision for model learning. In this paper, we propose a new WSCOS method toaddress these two challenges. To tackle the intrinsic similarity challenge, wedesign a multi-scale feature grouping module that first groups features atdifferent granularities and then aggregates these grouping results. By groupingsimilar features together, it encourages segmentation coherence, helping obtaincomplete segmentation results for both single and multiple-object images. Forthe weak supervision challenge, we utilize the recently-proposed visionfoundation model, Segment Anything Model (SAM), and use the provided sparseannotations as prompts to generate segmentation masks, which are used to trainthe model. To alleviate the impact of low-quality segmentation masks, wefurther propose a series of strategies, including multi-augmentation resultensemble, entropy-based pixel-level weighting, and entropy-based image-levelselection. These strategies help provide more reliable supervision to train thesegmentation model. We verify the effectiveness of our method on various WSCOStasks, and experiments demonstrate that our method achieves state-of-the-artperformance on these tasks.</description><author>Chunming He, Kai Li, Yachao Zhang, Guoxia Xu, Longxiang Tang, Yulun Zhang, Zhenhua Guo, Xiu Li</author><pubDate>Thu, 18 May 2023 15:31:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11003v1</guid></item><item><title>Certified Robust Neural Networks: Generalization and Corruption Resistance</title><link>http://arxiv.org/abs/2303.02251v2</link><description>Recent work have demonstrated that robustness (to "corruption") can be atodds with generalization. Adversarial training, for instance, aims to reducethe problematic susceptibility of modern neural networks to small dataperturbations. Surprisingly, overfitting is a major concern in adversarialtraining despite being mostly absent in standard training. We provide heretheoretical evidence for this peculiar "robust overfitting" phenomenon.Subsequently, we advance a novel distributionally robust loss function bridgingrobustness and generalization. We demonstrate both theoretically as well asempirically the loss to enjoy a certified level of robustness against twocommon types of corruption--data evasion and poisoning attacks--while ensuringguaranteed generalization. We show through careful numerical experiments thatour resulting holistic robust (HR) training procedure yields SOTA performance.Finally, we indicate that HR training can be interpreted as a direct extensionof adversarial training and comes with a negligible additional computationalburden. A ready-to-use python library implementing our algorithm is availableat https://github.com/RyanLucas3/HR_Neural_Networks.</description><author>Amine Bennouna, Ryan Lucas, Bart Van Parys</author><pubDate>Thu, 18 May 2023 15:28:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02251v2</guid></item><item><title>Dynamic Term Structure Models with Nonlinearities using Gaussian Processes</title><link>http://arxiv.org/abs/2305.11001v1</link><description>The importance of unspanned macroeconomic variables for Dynamic TermStructure Models has been intensively discussed in the literature. To our bestknowledge the earlier studies considered only linear interactions between theeconomy and the real-world dynamics of interest rates in DTSMs. We propose ageneralized modelling setup for Gaussian DTSMs which allows for unspannednonlinear associations between the two and we exploit it in forecasting.Specifically, we construct a custom sequential Monte Carlo estimation andforecasting scheme where we introduce Gaussian Process priors to modelnonlinearities. Sequential scheme we propose can also be used with dynamicportfolio optimization to assess the potential of generated economic value toinvestors. The methodology is presented using US Treasury data and selectedmacroeconomic indices. Namely, we look at core inflation and real economicactivity. We contrast the results obtained from the nonlinear model with thosestemming from an application of a linear model. Unlike for real economicactivity, in case of core inflation we find that, compared to linear models,application of nonlinear models leads to statistically significant gains ineconomic value across considered maturities.</description><author>Tomasz Dubiel-Teleszynski, Konstantinos Kalogeropoulos, Nikolaos Karouzakis</author><pubDate>Thu, 18 May 2023 15:24:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11001v1</guid></item><item><title>SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities</title><link>http://arxiv.org/abs/2305.11000v1</link><description>Multi-modal large language models are regarded as a crucial step towardsArtificial General Intelligence (AGI) and have garnered significant interestwith the emergence of ChatGPT. However, current speech-language modelstypically adopt the cascade paradigm, preventing inter-modal knowledgetransfer. In this paper, we propose SpeechGPT, a large language model withintrinsic cross-modal conversational abilities, capable of perceiving andgenerating multi-model content. With discrete speech representations, we firstconstruct SpeechInstruct, a large-scale cross-modal speech instruction dataset.Additionally, we employ a three-stage training strategy that includesmodality-adaptation pre-training, cross-modal instruction fine-tuning, andchain-of-modality instruction fine-tuning. The experimental results demonstratethat SpeechGPT has an impressive capacity to follow multi-modal humaninstructions and highlight the potential of handling multiple modalities withone model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.</description><author>Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, Xipeng Qiu</author><pubDate>Thu, 18 May 2023 15:23:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11000v1</guid></item><item><title>The Web Can Be Your Oyster for Improving Large Language Models</title><link>http://arxiv.org/abs/2305.10998v1</link><description>Large language models (LLMs) encode a large amount of world knowledge.However, as such knowledge is frozen at the time of model training, the modelsbecome static and limited by the training data at that time. In order tofurther improve the capacity of LLMs for knowledge-intensive tasks, we consideraugmenting LLMs with the large-scale web using search engine. Unlike previousaugmentation sources (e.g., Wikipedia data dump), the web provides broader,more comprehensive and constantly updated information. In this paper, wepresent a web-augmented LLM UNIWEB, which is trained over 16knowledge-intensive tasks in a unified text-to-text format. Instead of simplyusing the retrieved contents from web, our approach has made two majorimprovements. Firstly, we propose an adaptive search engine assisted learningmethod that can self-evaluate the confidence level of LLM's predictions, andadaptively determine when to refer to the web for more data, which can avoiduseless or noisy augmentation from web. Secondly, we design a pretraining task,i.e., continual knowledge learning, based on salient spans prediction, toreduce the discrepancy between the encoded and retrieved knowledge. Experimentson a wide range of knowledge-intensive tasks show that our model significantlyoutperforms previous retrieval-augmented methods.</description><author>Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang, Jian-Yun Nie, Ji-Rong Wen</author><pubDate>Thu, 18 May 2023 15:20:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10998v1</guid></item><item><title>Comparing Foundation Models using Data Kernels</title><link>http://arxiv.org/abs/2305.05126v2</link><description>Recent advances in self-supervised learning and neural network scaling haveenabled the creation of large models, known as foundation models, which can beeasily adapted to a wide range of downstream tasks. The current paradigm forcomparing foundation models involves evaluating them with aggregate metrics onvarious benchmark datasets. This method of model comparison is heavilydependent on the chosen evaluation metric, which makes it unsuitable forsituations where the ideal metric is either not obvious or unavailable. In thiswork, we present a methodology for directly comparing the embedding spacegeometry of foundation models, which facilitates model comparison without theneed for an explicit evaluation metric. Our methodology is grounded in randomgraph theory and enables valid hypothesis testing of embedding similarity on aper-datum basis. Further, we demonstrate how our methodology can be extended tofacilitate population level model comparison. In particular, we show how ourframework can induce a manifold of models equipped with a distance functionthat correlates strongly with several downstream metrics. We remark on theutility of this population level model comparison as a first step towards ataxonomic science of foundation models.</description><author>Brandon Duderstadt, Hayden S. Helm, Carey E. Priebe</author><pubDate>Thu, 18 May 2023 15:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05126v2</guid></item></channel></rss>