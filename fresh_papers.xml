<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 04 Nov 2024 13:00:08 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Regression-aware Inference with LLMs</title><link>http://arxiv.org/abs/2403.04182v3</link><description>Large language models (LLMs) have shown strong results on a range ofapplications, including regression and scoring tasks. Typically, one obtainsoutputs from an LLM via autoregressive sampling from the model's outputdistribution. We show that this inference strategy can be sub-optimal forcommon regression and scoring evaluation metrics. As a remedy, we build onprior work on Minimum Bayes Risk decoding, and propose alternate inferencestrategies that estimate the Bayes-optimal solution for regression and scoringmetrics in closed-form from sampled responses. We show that our proposalsignificantly improves over baselines across datasets and models.</description><author>Michal Lukasik, Harikrishna Narasimhan, Aditya Krishna Menon, Felix Yu, Sanjiv Kumar</author><pubDate>Fri, 01 Nov 2024 17:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04182v3</guid></item><item><title>Adversarially Robust Decision Transformer</title><link>http://arxiv.org/abs/2407.18414v2</link><description>Decision Transformer (DT), as one of the representative ReinforcementLearning via Supervised Learning (RvS) methods, has achieved strong performancein offline learning tasks by leveraging the powerful Transformer architecturefor sequential decision-making. However, in adversarial environments, thesemethods can be non-robust, since the return is dependent on the strategies ofboth the decision-maker and adversary. Training a probabilistic modelconditioned on observed return to predict action can fail to generalize, as thetrajectories that achieve a return in the dataset might have done so due to asuboptimal behavior adversary. To address this, we propose a worst-case-awareRvS algorithm, the Adversarially Robust Decision Transformer (ARDT), whichlearns and conditions the policy on in-sample minimax returns-to-go. ARDTaligns the target return with the worst-case return learned through minimaxexpectile regression, thereby enhancing robustness against powerful test-timeadversaries. In experiments conducted on sequential games with full datacoverage, ARDT can generate a maximin (Nash Equilibrium) strategy, the solutionwith the largest adversarial robustness. In large-scale sequential games andcontinuous adversarial RL environments with partial data coverage, ARDTdemonstrates significantly superior robustness to powerful test-timeadversaries and attains higher worst-case returns compared to contemporary DTmethods.</description><author>Xiaohang Tang, Afonso Marques, Parameswaran Kamalaruban, Ilija Bogunovic</author><pubDate>Fri, 01 Nov 2024 17:47:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18414v2</guid></item><item><title>VascX Models: Model Ensembles for Retinal Vascular Analysis from Color Fundus Images</title><link>http://arxiv.org/abs/2409.16016v2</link><description>We introduce VascX models, a comprehensive set of model ensembles foranalyzing retinal vasculature from color fundus images (CFIs). Annotated CFIswere aggregated from public datasets . Additional CFIs, mainly from thepopulation-based Rotterdam Study were annotated by graders for arteries andveins at pixel level, resulting in a dataset diverse in patient demographicsand imaging conditions. VascX models demonstrated superior segmentationperformance across datasets, image quality levels, and anatomic regions whencompared to existing, publicly available models, likely due to the increasedsize and variety of our training set. Important improvements were observed inartery-vein and disc segmentation performance, particularly in segmentations ofthese structures on CFIs of intermediate quality, common in large cohorts andclinical datasets. Importantly, these improvements translated intosignificantly more accurate vascular features when we compared featuresextracted from VascX segmentation masks with features extracted fromsegmentation masks generated by previous models. With VascX models we provide arobust, ready-to-use set of model ensembles and inference code aimed atsimplifying the implementation and enhancing the quality of automated retinalvasculature analyses. The precise vessel parameters generated by the model canserve as starting points for the identification of disease patterns in andoutside of the eye.</description><author>Jose Vargas Quiros, Bart Liefers, Karin van Garderen, Jeroen Vermeulen, Eyened Reading Center, Sinergia Consortium, Caroline Klaver</author><pubDate>Fri, 01 Nov 2024 17:44:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16016v2</guid></item><item><title>Highly Accurate Real-space Electron Densities with Neural Networks</title><link>http://arxiv.org/abs/2409.01306v2</link><description>Variational ab-initio methods in quantum chemistry stand out among othermethods in providing direct access to the wave function. This allows inprinciple straightforward extraction of any other observable of interest,besides the energy, but in practice this extraction is often technicallydifficult and computationally impractical. Here, we consider the electrondensity as a central observable in quantum chemistry and introduce a novelmethod to obtain accurate densities from real-space many-electron wavefunctions by representing the density with a neural network that captures knownasymptotic properties and is trained from the wave function by score matchingand noise-contrastive estimation. We use variational quantum Monte Carlo withdeep-learning ans\"atze (deep QMC) to obtain highly accurate wave functionsfree of basis set errors, and from them, using our novel method,correspondingly accurate electron densities, which we demonstrate bycalculating dipole moments, nuclear forces, contact densities, and otherdensity-based properties.</description><author>Lixue Cheng, P. Bernát Szabó, Zeno Schätzle, Derk P. Kooi, Jonas Köhler, Klaas J. H. Giesbertz, Frank Noé, Jan Hermann, Paola Gori-Giorgi, Adam Foster</author><pubDate>Fri, 01 Nov 2024 17:40:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.01306v2</guid></item><item><title>HDLCopilot: Natural Language Exploration of Hardware Designs and Libraries</title><link>http://arxiv.org/abs/2407.12749v2</link><description>Hardware design workflows often involve working with Process Design Kits(PDKs) from various fabrication labs, each containing its own set of standardcell libraries optimized for metrics such as speed, power, or density. Theselibraries include multiple views for information on timing and electricalproperties of cells, cell layout details, and process design rules. Engineerstypically navigate between the design and the target technology to makeinformed decisions on different design scenarios, such as selecting specificgates for area optimization or enhancing critical path speed. Navigating thiscomplex landscape to retrieve specific information about gates or design rulesis often time-consuming and error-prone. To address this, we presentHDLCopilot, a multi-agent collaborative framework powered by large languagemodels that enables engineers to streamline interactions with hardware designand PDKs through natural language queries. HDLCopilot enables engineers toquickly access relevant information on gates and design rules, evaluatetradeoffs related to area, speed, and power in order to make informed decisionsefficiently and accurately. The framework achieves an execution accuracy of96.33\% on a diverse set of complex natural language queries. HDLCopilotpositions itself as a powerful assistant in hardware design workflows,enhancing productivity and reducing potential human errors.</description><author>Manar Abdelatty, Jacob Rosenstein, Sherief Reda</author><pubDate>Fri, 01 Nov 2024 17:31:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12749v2</guid></item><item><title>DELTA: Dense Efficient Long-range 3D Tracking for any video</title><link>http://arxiv.org/abs/2410.24211v2</link><description>Tracking dense 3D motion from monocular videos remains challenging,particularly when aiming for pixel-level precision over long sequences. Weintroduce DELTA, a novel method that efficiently tracks every pixel in 3Dspace, enabling accurate motion estimation across entire videos. Our approachleverages a joint global-local attention mechanism for reduced-resolutiontracking, followed by a transformer-based upsampler to achieve high-resolutionpredictions. Unlike existing methods, which are limited by computationalinefficiency or sparse tracking, DELTA delivers dense 3D tracking at scale,running over 8x faster than previous methods while achieving state-of-the-artaccuracy. Furthermore, we explore the impact of depth representation ontracking performance and identify log-depth as the optimal choice. Extensiveexperiments demonstrate the superiority of DELTA on multiple benchmarks,achieving new state-of-the-art results in both 2D and 3D dense tracking tasks.Our method provides a robust solution for applications requiring fine-grained,long-term motion tracking in 3D space.</description><author>Tuan Duc Ngo, Peiye Zhuang, Chuang Gan, Evangelos Kalogerakis, Sergey Tulyakov, Hsin-Ying Lee, Chaoyang Wang</author><pubDate>Fri, 01 Nov 2024 17:23:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24211v2</guid></item><item><title>DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs</title><link>http://arxiv.org/abs/2406.01721v3</link><description>Quantization of large language models (LLMs) faces significant challenges,particularly due to the presence of outlier activations that impede efficientlow-bit representation. Traditional approaches predominantly address NormalOutliers, which are activations across all tokens with relatively largemagnitudes. However, these methods struggle with smoothing Massive Outliersthat display significantly larger values, which leads to significantperformance degradation in low-bit quantization. In this paper, we introduceDuQuant, a novel approach that utilizes rotation and permutationtransformations to more effectively mitigate both massive and normal outliers.First, DuQuant starts by constructing the rotation matrix, using specificoutlier dimensions as prior knowledge, to redistribute outliers to adjacentchannels by block-wise rotation. Second, We further employ a zigzag permutationto balance the distribution of outliers across blocks, thereby reducingblock-wise variance. A subsequent rotation further smooths the activationlandscape, enhancing model performance. DuQuant simplifies the quantizationprocess and excels in managing outliers, outperforming the state-of-the-artbaselines across various sizes and types of LLMs on multiple tasks, even with4-bit weight-activation quantization. Our code is available athttps://github.com/Hsu1023/DuQuant.</description><author>Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, Ying Wei</author><pubDate>Fri, 01 Nov 2024 17:12:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01721v3</guid></item><item><title>Scalable Training of Trustworthy and Energy-Efficient Predictive Graph Foundation Models for Atomistic Materials Modeling: A Case Study with HydraGNN</title><link>http://arxiv.org/abs/2406.12909v4</link><description>We present our work on developing and training scalable, trustworthy, andenergy-efficient predictive graph foundation models (GFMs) using HydraGNN, amulti-headed graph convolutional neural network architecture. HydraGNN expandsthe boundaries of graph neural network (GNN) computations in both trainingscale and data diversity. It abstracts over message passing algorithms,allowing both reproduction of and comparison across algorithmic innovationsthat define nearest-neighbor convolution in GNNs. This work discusses a seriesof optimizations that have allowed scaling up the GFMs training to tens ofthousands of GPUs on datasets consisting of hundreds of millions of graphs. OurGFMs use multi-task learning (MTL) to simultaneously learn graph-level andnode-level properties of atomistic structures, such as energy and atomicforces. Using over 154 million atomistic structures for training, we illustratethe performance of our approach along with the lessons learned on twostate-of-the-art United States Department of Energy (US-DOE) supercomputers,namely the Perlmutter petascale system at the National Energy ResearchScientific Computing Center and the Frontier exascale system at Oak RidgeLeadership Computing Facility. The HydraGNN architecture enables the GFM toachieve near-linear strong scaling performance using more than 2,000 GPUs onPerlmutter and 16,000 GPUs on Frontier.</description><author>Massimiliano Lupo Pasini, Jong Youl Choi, Kshitij Mehta, Pei Zhang, David Rogers, Jonghyun Bae, Khaled Z. Ibrahim, Ashwin M. Aji, Karl W. Schulz, Jorda Polo, Prasanna Balaprakash</author><pubDate>Fri, 01 Nov 2024 17:09:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12909v4</guid></item><item><title>Nyström Kernel Stein Discrepancy</title><link>http://arxiv.org/abs/2406.08401v3</link><description>Kernel methods underpin many of the most successful approaches in datascience and statistics, and they allow representing probability measures aselements of a reproducing kernel Hilbert space without loss of information.Recently, the kernel Stein discrepancy (KSD), which combines Stein's methodwith the flexibility of kernel techniques, gained considerable attention.Through the Stein operator, KSD allows the construction of powerfulgoodness-of-fit tests where it is sufficient to know the target distribution upto a multiplicative constant. However, the typical U- and V-statistic-based KSDestimators suffer from a quadratic runtime complexity, which hinders theirapplication in large-scale settings. In this work, we propose a Nystr\"om-basedKSD acceleration -- with runtime $\mathcal O\left(mn+m^3\right)$ for $n$samples and $m\ll n$ Nystr\"om points -- , show its $\sqrt{n}$-consistency witha classical sub-Gaussian assumption, and demonstrate its applicability forgoodness-of-fit testing on a suite of benchmarks.</description><author>Florian Kalinke, Zoltan Szabo, Bharath K. Sriperumbudur</author><pubDate>Fri, 01 Nov 2024 17:04:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08401v3</guid></item><item><title>Provable optimal transport with transformers: The essence of depth and prompt engineering</title><link>http://arxiv.org/abs/2410.19931v2</link><description>Can we establish provable performance guarantees for transformers?Establishing such theoretical guarantees is a milestone in developingtrustworthy generative AI. In this paper, we take a step toward addressing thisquestion by focusing on optimal transport, a fundamental problem at theintersection of combinatorial and continuous optimization. Leveraging thecomputational power of attention layers, we prove that a transformer with fixedparameters can effectively solve the optimal transport problem in Wasserstein-2with entropic regularization for an arbitrary number of points. Consequently,the transformer can sort lists of arbitrary sizes up to an approximationfactor. Our results rely on an engineered prompt that enables the transformerto implement gradient descent with adaptive stepsizes on the dual optimaltransport. Combining the convergence analysis of gradient descent with Sinkhorndynamics, we establish an explicit approximation bound for optimal transportwith transformers, which improves as depth increases. Our findings providenovel insights into the essence of prompt engineering and depth for solvingoptimal transport. In particular, prompt engineering boosts the algorithmicexpressivity of transformers, allowing them implement an optimization method.With increasing depth, transformers can simulate several iterations of gradientdescent.</description><author>Hadi Daneshmand</author><pubDate>Fri, 01 Nov 2024 16:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19931v2</guid></item><item><title>MAMMAL -- Molecular Aligned Multi-Modal Architecture and Language</title><link>http://arxiv.org/abs/2410.22367v2</link><description>Drug discovery typically consists of multiple steps, including identifying atarget protein key to a disease's etiology, validating that interacting withthis target could prevent symptoms or cure the disease, discovering a smallmolecule or biologic therapeutic to interact with it, and optimizing thecandidate molecule through a complex landscape of required properties. Drugdiscovery related tasks often involve prediction and generation whileconsidering multiple entities that potentially interact, which poses achallenge for typical AI models. For this purpose we present MAMMAL - MolecularAligned Multi-Modal Architecture and Language - a method that we applied tocreate a versatile multi-task multi-align foundation model that learns fromlarge-scale biological datasets (2 billion samples) across diverse modalities,including proteins, small molecules, and genes. We introduce a prompt syntaxthat supports a wide range of classification, regression, and generation tasks.It allows combining different modalities and entity types as inputs and/oroutputs. Our model handles combinations of tokens and scalars and enables thegeneration of small molecules and proteins, property prediction, andtranscriptomic lab test predictions. We evaluated the model on 11 diversedownstream tasks spanning different steps within a typical drug discoverypipeline, where it reaches new SOTA in 9 tasks and is comparable to SOTA in 2tasks. This performance is achieved while using a unified architecture servingall tasks, in contrast to the original SOTA performance achieved using tailoredarchitectures. The model code and pretrained weights are publicly available athttps://github.com/BiomedSciAI/biomed-multi-alignment andhttps://huggingface.co/ibm/biomed.omics.bl.sm.ma-ted-458m.</description><author>Yoel Shoshan, Moshiko Raboh, Michal Ozery-Flato, Vadim Ratner, Alex Golts, Jeffrey K. Weber, Ella Barkan, Simona Rabinovici-Cohen, Sagi Polaczek, Ido Amos, Ben Shapira, Liam Hazan, Matan Ninio, Sivan Ravid, Michael M. Danziger, Joseph A. Morrone, Parthasarathy Suryanarayanan, Michal Rosen-Zvi, Efrat Hexter</author><pubDate>Fri, 01 Nov 2024 16:53:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.22367v2</guid></item><item><title>BehAVE: Behaviour Alignment of Video Game Encodings</title><link>http://arxiv.org/abs/2402.01335v3</link><description>Domain randomisation enhances the transferability of vision models acrossvisually distinct domains with similar content. However, current methodsheavily depend on intricate simulation engines, hampering feasibility andscalability. This paper introduces BehAVE, a video understanding framework thatutilises existing commercial video games for domain randomisation withoutaccessing their simulation engines. BehAVE taps into the visual diversity ofvideo games for randomisation and uses textual descriptions of player actionsto align videos with similar content. We evaluate BehAVE across 25 first-personshooter (FPS) games using various video and text foundation models,demonstrating its robustness in domain randomisation. BehAVE effectively alignsplayer behavioural patterns and achieves zero-shot transfer to multiple unseenFPS games when trained on just one game. In a more challenging scenario, BehAVEenhances the zero-shot transferability of foundation models to unseen FPSgames, even when trained on a game of a different genre, with improvements ofup to 22%. BehAVE is available online at https://github.com/nrasajski/BehAVE.</description><author>Nemanja Rašajski, Chintan Trivedi, Konstantinos Makantasis, Antonios Liapis, Georgios N. Yannakakis</author><pubDate>Fri, 01 Nov 2024 16:51:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01335v3</guid></item><item><title>A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2405.19153v2</link><description>Continual learning with deep neural networks presents challenges distinctfrom both the fixed-dataset and convex continual learning regimes. One suchchallenge is plasticity loss, wherein a neural network trained in an onlinefashion displays a degraded ability to fit new tasks. This problem has beenextensively studied in both supervised learning and off-policy reinforcementlearning (RL), where a number of remedies have been proposed. Still, plasticityloss has received less attention in the on-policy deep RL setting. Here weperform an extensive set of experiments examining plasticity loss and a varietyof mitigation methods in on-policy deep RL. We demonstrate that plasticity lossis pervasive under domain shift in this regime, and that a number of methodsdeveloped to resolve it in other settings fail, sometimes even performing worsethan applying no intervention at all. In contrast, we find that a class of``regenerative'' methods are able to consistently mitigate plasticity loss in avariety of contexts, including in gridworld tasks and more challengingenvironments like Montezuma's Revenge and ProcGen.</description><author>Arthur Juliani, Jordan T. Ash</author><pubDate>Fri, 01 Nov 2024 16:47:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19153v2</guid></item><item><title>Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets</title><link>http://arxiv.org/abs/2309.12032v2</link><description>Structure learning is the crux of causal inference. Notably, causal discovery(CD) algorithms are brittle when data is scarce, possibly inferring imprecisecausal relations that contradict expert knowledge -- especially whenconsidering latent confounders. To aggravate the issue, most CD methods do notprovide uncertainty estimates, making it hard for users to interpret resultsand improve the inference process. Surprisingly, while CD is a human-centeredaffair, no works have focused on building methods that both 1) outputuncertainty estimates that can be verified by experts and 2) interact withthose experts to iteratively refine CD. To solve these issues, we start byproposing to sample (causal) ancestral graphs proportionally to a beliefdistribution based on a score function, such as the Bayesian informationcriterion (BIC), using generative flow networks. Then, we leverage thediversity in candidate graphs and introduce an optimal experimental design toiteratively probe the expert about the relations among variables, effectivelyreducing the uncertainty of our belief over ancestral graphs. Finally, weupdate our samples to incorporate human feedback via importance sampling.Importantly, our method does not require causal sufficiency (i.e., unobservedconfounders may exist). Experiments with synthetic observational data show thatour method can accurately sample from distributions over ancestral graphs andthat we can greatly improve inference quality with human aid.</description><author>Tiago da Silva, Eliezer Silva, António Góis, Dominik Heider, Samuel Kaski, Diego Mesquita, Adèle Ribeiro</author><pubDate>Fri, 01 Nov 2024 16:46:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12032v2</guid></item><item><title>Double-Ended Synthesis Planning with Goal-Constrained Bidirectional Search</title><link>http://arxiv.org/abs/2407.06334v2</link><description>Computer-aided synthesis planning (CASP) algorithms have demonstratedexpert-level abilities in planning retrosynthetic routes to molecules of low tomoderate complexity. However, current search methods assume the sufficiency ofreaching arbitrary building blocks, failing to address the common real-worldconstraint where using specific molecules is desired. To this end, we present aformulation of synthesis planning with starting material constraints. Underthis formulation, we propose Double-Ended Synthesis Planning (DESP), a novelCASP algorithm under a bidirectional graph search scheme that interleavesexpansions from the target and from the goal starting materials to ensureconstraint satisfiability. The search algorithm is guided by a goal-conditionedcost network learned offline from a partially observed hypergraph of validchemical reactions. We demonstrate the utility of DESP in improving solve ratesand reducing the number of search expansions by biasing synthesis planningtowards expert goals on multiple new benchmarks. DESP can make use of existingone-step retrosynthesis models, and we anticipate its performance to scale asthese one-step model capabilities improve.</description><author>Kevin Yu, Jihye Roh, Ziang Li, Wenhao Gao, Runzhong Wang, Connor W. Coley</author><pubDate>Fri, 01 Nov 2024 16:45:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06334v2</guid></item><item><title>Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization</title><link>http://arxiv.org/abs/2410.19499v2</link><description>Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency andefficacy of prompt optimization for Large Language Models (LLMs). Building onProTeGi, MAPO uses positive natural language "gradients" and a momentum-basedextension to refine prompts effectively. By tracking gradient history, MAPOavoids local minima and oscillations. It also utilizes beam search and an UpperConfidence Bound (UCB) algorithm for balanced candidate expansion andselection. Benchmark testing shows that MAPO achieves faster convergence timewith fewer API calls and higher F1 scores than ProTeGi, proving it as a robustand scalable solution for automated prompt engineering in LLMs.</description><author>Anthony Cui, Pranav Nandyalam, Ethan Cheung, Kevin Zhu</author><pubDate>Fri, 01 Nov 2024 16:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19499v2</guid></item><item><title>Efficient Adversarial Training in LLMs with Continuous Attacks</title><link>http://arxiv.org/abs/2405.15589v3</link><description>Large language models (LLMs) are vulnerable to adversarial attacks that canbypass their safety guardrails. In many domains, adversarial training hasproven to be one of the most promising methods to reliably improve robustnessagainst such attacks. Yet, in the context of LLMs, current methods foradversarial training are hindered by the high computational costs required toperform discrete adversarial attacks at each training iteration. We addressthis problem by instead calculating adversarial attacks in the continuousembedding space of the LLM, which is orders of magnitudes more efficient. Wepropose a fast adversarial training algorithm (C-AdvUL) composed of two losses:the first makes the model robust on continuous embedding attacks computed on anadversarial behaviour dataset; the second ensures the usefulness of the finalmodel by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, anadversarial variant of IPO that does not require utility data for adversariallyrobust alignment. Our empirical evaluation on five models from differentfamilies (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B,3.8B, 7B) shows that both algorithms substantially enhance LLM robustnessagainst discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Ourresults demonstrate that robustness to continuous perturbations can extrapolateto discrete threat models. Thereby, we present a path toward scalableadversarial training algorithms for robustly aligning LLMs.</description><author>Sophie Xhonneux, Alessandro Sordoni, Stephan Günnemann, Gauthier Gidel, Leo Schwinn</author><pubDate>Fri, 01 Nov 2024 16:39:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15589v3</guid></item><item><title>Aligning Motion-Blurred Images Using Contrastive Learning on Overcomplete Pixels</title><link>http://arxiv.org/abs/2410.07410v2</link><description>We propose a new contrastive objective for learning overcomplete pixel-levelfeatures that are invariant to motion blur. Other invariances (e.g., pose,illumination, or weather) can be learned by applying the correspondingtransformations on unlabeled images during self-supervised training. Weshowcase that a simple U-Net trained with our objective can produce localfeatures useful for aligning the frames of an unseen video captured with amoving camera under realistic and challenging conditions. Using a carefullydesigned toy example, we also show that the overcomplete pixels can encode theidentity of objects in an image and the pixel coordinates relative to theseobjects.</description><author>Leonid Pogorelyuk, Stefan T. Radev</author><pubDate>Fri, 01 Nov 2024 16:34:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07410v2</guid></item><item><title>GeoSplatting: Towards Geometry Guided Gaussian Splatting for Physically-based Inverse Rendering</title><link>http://arxiv.org/abs/2410.24204v2</link><description>We consider the problem of physically-based inverse rendering using 3DGaussian Splatting (3DGS) representations. While recent 3DGS methods haveachieved remarkable results in novel view synthesis (NVS), accurately capturinghigh-fidelity geometry, physically interpretable materials and lighting remainschallenging, as it requires precise geometry modeling to provide accuratesurface normals, along with physically-based rendering (PBR) techniques toensure correct material and lighting disentanglement. Previous 3DGS methodsresort to approximating surface normals, but often struggle with noisy localgeometry, leading to inaccurate normal estimation and suboptimalmaterial-lighting decomposition. In this paper, we introduce GeoSplatting, anovel hybrid representation that augments 3DGS with explicit geometric guidanceand differentiable PBR equations. Specifically, we bridge isosurface and 3DGStogether, where we first extract isosurface mesh from a scalar field, thenconvert it into 3DGS points and formulate PBR equations for them in a fullydifferentiable manner. In GeoSplatting, 3DGS is grounded on the mesh geometry,enabling precise surface normal modeling, which facilitates the use of PBRframeworks for material decomposition. This approach further maintains theefficiency and quality of NVS from 3DGS while ensuring accurate geometry fromthe isosurface. Comprehensive evaluations across diverse datasets demonstratethe superiority of GeoSplatting, consistently outperforming existing methodsboth quantitatively and qualitatively.</description><author>Kai Ye, Chong Gao, Guanbin Li, Wenzheng Chen, Baoquan Chen</author><pubDate>Fri, 01 Nov 2024 16:31:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24204v2</guid></item><item><title>Diffusion Spectral Representation for Reinforcement Learning</title><link>http://arxiv.org/abs/2406.16121v2</link><description>Diffusion-based models have achieved notable empirical successes inreinforcement learning (RL) due to their expressiveness in modeling complexdistributions. Despite existing methods being promising, the key challenge ofextending existing methods for broader real-world applications lies in thecomputational cost at inference time, i.e., sampling from a diffusion model isconsiderably slow as it often requires tens to hundreds of iterations togenerate even one sample. To circumvent this issue, we propose to leverage theflexibility of diffusion models for RL from a representation learningperspective. In particular, by exploiting the connection between diffusionmodels and energy-based models, we develop Diffusion Spectral Representation(Diff-SR), a coherent algorithm framework that enables extracting sufficientrepresentations for value functions in Markov decision processes (MDP) andpartially observable Markov decision processes (POMDP). We further demonstratehow Diff-SR facilitates efficient policy optimization and practical algorithmswhile explicitly bypassing the difficulty and inference cost of sampling fromthe diffusion model. Finally, we provide comprehensive empirical studies toverify the benefits of Diff-SR in delivering robust and advantageousperformance across various benchmarks with both fully and partially observablesettings.</description><author>Dmitry Shribak, Chen-Xiao Gao, Yitong Li, Chenjun Xiao, Bo Dai</author><pubDate>Fri, 01 Nov 2024 16:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16121v2</guid></item><item><title>HENASY: Learning to Assemble Scene-Entities for Egocentric Video-Language Model</title><link>http://arxiv.org/abs/2406.00307v4</link><description>Current video-language models (VLMs) rely extensively on instance-levelalignment between video and language modalities, which presents two majorlimitations: (1) visual reasoning disobeys the natural perception that humansdo in first-person perspective, leading to a lack of reasoning interpretation;and (2) learning is limited in capturing inherent fine-grained relationshipsbetween two modalities. In this paper, we take an inspiration from human perception and explore acompositional approach for egocentric video representation. We introduce HENASY(Hierarchical ENtities ASsemblY), which includes a spatiotemporal tokengrouping mechanism to explicitly assemble dynamically evolving scene entitiesthrough time and model their relationship for video representation. Byleveraging compositional structure understanding, HENASY possesses stronginterpretability via visual grounding with free-form text queries. We furtherexplore a suite of multi-grained contrastive losses to facilitateentity-centric understandings. This comprises three alignment types:video-narration, noun-entity, verb-entities alignments. Our method demonstrates strong interpretability in both quantitative andqualitative experiments; while maintaining competitive performances on fivedownstream tasks via zero-shot transfer or as video/text representation,including video/text retrieval, action recognition, multi-choice query, naturallanguage query, and moments query.</description><author>Khoa Vo, Thinh Phan, Kashu Yamazaki, Minh Tran, Ngan Le</author><pubDate>Fri, 01 Nov 2024 16:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00307v4</guid></item><item><title>On the Limitations of Fractal Dimension as a Measure of Generalization</title><link>http://arxiv.org/abs/2406.02234v2</link><description>Bounding and predicting the generalization gap of overparameterized neuralnetworks remains a central open problem in theoretical machine learning. Thereis a recent and growing body of literature that proposes the framework offractals to model optimization trajectories of neural networks, motivatinggeneralization bounds and measures based on the fractal dimension of thetrajectory. Notably, the persistent homology dimension has been proposed tocorrelate with the generalization gap. This paper performs an empiricalevaluation of these persistent homology-based generalization measures, with anin-depth statistical analysis. Our study reveals confounding effects in theobserved correlation between generalization and topological measures due to thevariation of hyperparameters. We also observe that fractal dimension fails topredict generalization of models trained from poor initializations. We lastlyreveal the intriguing manifestation of model-wise double descent in thesetopological generalization measures. Our work forms a basis for a deeperinvestigation of the causal relationships between fractal geometry, topologicaldata analysis, and neural network optimization.</description><author>Charlie B. Tan, Inés García-Redondo, Qiquan Wang, Michael M. Bronstein, Anthea Monod</author><pubDate>Fri, 01 Nov 2024 16:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02234v2</guid></item><item><title>StatWhy: Formal Verification Tool for Statistical Hypothesis Testing Programs</title><link>http://arxiv.org/abs/2405.17492v2</link><description>Statistical methods have been widely misused and misinterpreted in variousscientific fields, raising significant concerns about the integrity ofscientific research. To mitigate this problem, we propose a new method forformally specifying and automatically verifying the correctness of statisticalprograms. In this method, programmers are required to annotate the source codeof the statistical programs with the requirements for these methods. Throughthis annotation, they are reminded to check the requirements for statisticalmethods, including those that cannot be formally verified, such as thedistribution of the unknown true population. Our software tool StatWhyautomatically checks whether programmers have properly specified therequirements for the statistical methods, thereby identifying any missingrequirements that need to be addressed. This tool is implemented using the Why3platform to verify the correctness of OCaml programs that conduct statisticalhypothesis testing. We demonstrate how StatWhy can be used to avoid commonerrors in various popular statistical hypothesis testing programs.</description><author>Yusuke Kawamoto, Kentaro Kobayashi, Kohei Suenaga</author><pubDate>Fri, 01 Nov 2024 16:16:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17492v2</guid></item><item><title>CaptainCook4D: A Dataset for Understanding Errors in Procedural Activities</title><link>http://arxiv.org/abs/2312.14556v3</link><description>Following step-by-step procedures is an essential component of variousactivities carried out by individuals in their daily lives. These proceduresserve as a guiding framework that helps to achieve goals efficiently, whetherit is assembling furniture or preparing a recipe. However, the complexity andduration of procedural activities inherently increase the likelihood of makingerrors. Understanding such procedural activities from a sequence of frames is achallenging task that demands an accurate interpretation of visual informationand the ability to reason about the structure of the activity. To this end, wecollect a new egocentric 4D dataset, CaptainCook4D, comprising 384 recordings(94.5 hours) of people performing recipes in real kitchen environments. Thisdataset consists of two distinct types of activity: one in which participantsadhere to the provided recipe instructions and another in which they deviateand induce errors. We provide 5.3K step annotations and 10K fine-grained actionannotations and benchmark the dataset for the following tasks: supervised errorrecognition, multistep localization, and procedure learning</description><author>Rohith Peddi, Shivvrat Arya, Bharath Challa, Likhitha Pallapothula, Akshay Vyas, Bhavya Gouripeddi, Jikai Wang, Qifan Zhang, Vasundhara Komaragiri, Eric Ragan, Nicholas Ruozzi, Yu Xiang, Vibhav Gogate</author><pubDate>Fri, 01 Nov 2024 16:12:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14556v3</guid></item><item><title>Can Large Language Model Agents Simulate Human Trust Behavior?</title><link>http://arxiv.org/abs/2402.04559v4</link><description>Large Language Model (LLM) agents have been increasingly adopted assimulation tools to model humans in social science and role-playingapplications. However, one fundamental question remains: can LLM agents reallysimulate human behavior? In this paper, we focus on one critical and elementalbehavior in human interactions, trust, and investigate whether LLM agents cansimulate human trust behavior. We first find that LLM agents generally exhibittrust behavior, referred to as agent trust, under the framework of Trust Games,which are widely recognized in behavioral economics. Then, we discover thatGPT-4 agents manifest high behavioral alignment with humans in terms of trustbehavior, indicating the feasibility of simulating human trust behavior withLLM agents. In addition, we probe the biases of agent trust and differences inagent trust towards other LLM agents and humans. We also explore the intrinsicproperties of agent trust under conditions including external manipulations andadvanced reasoning strategies. Our study provides new insights into thebehaviors of LLM agents and the fundamental analogy between LLMs and humansbeyond value alignment. We further illustrate broader implications of ourdiscoveries for applications where trust is paramount.</description><author>Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Shiyang Lai, Kai Shu, Jindong Gu, Adel Bibi, Ziniu Hu, David Jurgens, James Evans, Philip Torr, Bernard Ghanem, Guohao Li</author><pubDate>Fri, 01 Nov 2024 16:10:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04559v4</guid></item><item><title>SelfCodeAlign: Self-Alignment for Code Generation</title><link>http://arxiv.org/abs/2410.24198v2</link><description>Instruction tuning is a supervised fine-tuning approach that significantlyimproves the ability of large language models (LLMs) to follow humaninstructions. We propose SelfCodeAlign, the first fully transparent andpermissive pipeline for self-aligning code LLMs without extensive humanannotations or distillation. SelfCodeAlign employs the same base model forinference throughout the data generation process. It first extracts diversecoding concepts from high-quality seed snippets to generate new tasks. It thensamples multiple responses per task, pairs each with test cases, and validatesthem in a sandbox environment. Finally, passing examples are selected forinstruction tuning. In our primary experiments, we use SelfCodeAlign withCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 onHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.Across all benchmarks, this finetuned model consistently outperforms theoriginal version trained with OctoPack, the previous state-of-the-art methodfor instruction tuning without human annotations or distillation. Additionally,we show that SelfCodeAlign is effective across LLMs of various sizes, from 3Bto 33B, and that the base models can benefit more from alignment with their owndata distribution. We further validate each component's effectiveness in ourpipeline, showing that SelfCodeAlign outperforms both direct distillation fromGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct andEvol-Instruct. SelfCodeAlign has also led to the creation ofStarCoder2-Instruct, the first fully transparent, permissively licensed, andself-aligned code LLM that achieves state-of-the-art coding performance.</description><author>Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro von Werra, Arjun Guha, Lingming Zhang</author><pubDate>Fri, 01 Nov 2024 16:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24198v2</guid></item><item><title>Flexible Fairness-Aware Learning via Inverse Conditional Permutation</title><link>http://arxiv.org/abs/2404.05678v3</link><description>Equalized odds, as a popular notion of algorithmic fairness, aims to ensurethat sensitive variables, such as race and gender, do not unfairly influencethe algorithm's prediction when conditioning on the true outcome. Despite rapidadvancements, current research primarily focuses on equalized odds violationscaused by a single sensitive attribute, leaving the challenge of simultaneouslyaccounting for multiple attributes largely unaddressed. We bridge this gap byintroducing an in-processing fairness-aware learning approach, FairICP, whichintegrates adversarial learning with a novel inverse conditional permutationscheme. FairICP offers a theoretically justified, flexible, and efficientscheme to promote equalized odds under fairness conditions described by complexand multidimensional sensitive attributes. The efficacy and adaptability of ourmethod are demonstrated through both simulation studies and empirical analysesof real-world datasets.</description><author>Yuheng Lai, Leying Guan</author><pubDate>Fri, 01 Nov 2024 16:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05678v3</guid></item><item><title>Comparing YOLO11 and YOLOv8 for instance segmentation of occluded and non-occluded immature green fruits in complex orchard environment</title><link>http://arxiv.org/abs/2410.19869v2</link><description>This study conducted a comprehensive performance evaluation on YOLO11 andYOLOv8, the latest in the "You Only Look Once" (YOLO) series, focusing on theirinstance segmentation capabilities for immature green apples in orchardenvironments. YOLO11n-seg achieved the highest mask precision across allcategories with a notable score of 0.831, highlighting its effectiveness infruit detection. YOLO11m-seg and YOLO11l-seg excelled in non-occluded andoccluded fruitlet segmentation with scores of 0.851 and 0.829, respectively.Additionally, YOLO11x-seg led in mask recall for all categories, achieving ascore of 0.815, with YOLO11m-seg performing best for non-occluded immaturegreen fruitlets at 0.858 and YOLOv8x-seg leading the occluded category with0.800. In terms of mean average precision at a 50\% intersection over union(mAP@50), YOLO11m-seg consistently outperformed, registering the highest scoresfor both box and mask segmentation, at 0.876 and 0.860 for the "All" class and0.908 and 0.909 for non-occluded immature fruitlets, respectively. YOLO11l-segand YOLOv8l-seg shared the top box mAP@50 for occluded immature fruitlets at0.847, while YOLO11m-seg achieved the highest mask mAP@50 of 0.810. Despite theadvancements in YOLO11, YOLOv8n surpassed its counterparts in image processingspeed, with an impressive inference speed of 3.3 milliseconds, compared to thefastest YOLO11 series model at 4.8 milliseconds, underscoring its suitabilityfor real-time agricultural applications related to complex green fruitenvironments.</description><author>Ranjan Sapkota, Manoj Karkee</author><pubDate>Fri, 01 Nov 2024 16:02:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19869v2</guid></item><item><title>Simplifying Latent Dynamics with Softly State-Invariant World Models</title><link>http://arxiv.org/abs/2401.17835v2</link><description>To solve control problems via model-based reasoning or planning, an agentneeds to know how its actions affect the state of the world. The actions anagent has at its disposal often change the state of the environment insystematic ways. However, existing techniques for world modelling do notguarantee that the effect of actions are represented in such systematic ways.We introduce the Parsimonious Latent Space Model (PLSM), a world model thatregularizes the latent dynamics to make the effect of the agent's actions morepredictable. Our approach minimizes the mutual information between latentstates and the change that an action produces in the agent's latent state, inturn minimizing the dependence the state has on the dynamics. This makes theworld model softly state-invariant. We combine PLSM with different modelclasses used for i) future latent state prediction, ii) planning, and iii)model-free reinforcement learning. We find that our regularization improvesaccuracy, generalization, and performance in downstream tasks, highlighting theimportance of systematic treatment of actions in world models.</description><author>Tankred Saanum, Peter Dayan, Eric Schulz</author><pubDate>Fri, 01 Nov 2024 15:55:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17835v2</guid></item><item><title>Language Imbalance Driven Rewarding for Multilingual Self-improving</title><link>http://arxiv.org/abs/2410.08964v2</link><description>Large Language Models (LLMs) have achieved state-of-the-art performanceacross numerous tasks. However, these advancements have predominantly benefited"first-class" languages such as English and Chinese, leaving many otherlanguages underrepresented. This imbalance, while limiting broaderapplications, generates a natural preference ranking between languages,offering an opportunity to bootstrap the multilingual capabilities of LLM in aself-improving manner. Thus, we propose $\textit{Language Imbalance DrivenRewarding}$, where the inherent imbalance between dominant and non-dominantlanguages within LLMs is leveraged as a reward signal. Iterative DPO trainingdemonstrates that this approach not only enhances LLM performance innon-dominant languages but also improves the dominant language's capacity,thereby yielding an iterative reward signal. Fine-tuningMeta-Llama-3-8B-Instruct over two iterations of this approach results incontinuous improvements in multilingual performance acrossinstruction-following and arithmetic reasoning tasks, evidenced by an averageimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%accuracy on the MGSM benchmark. This work serves as an initial exploration,paving the way for multilingual self-improvement of LLMs.</description><author>Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, Jiajun Zhang</author><pubDate>Fri, 01 Nov 2024 15:53:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08964v2</guid></item><item><title>Statistical Inference in Tensor Completion: Optimal Uncertainty Quantification and Statistical-to-Computational Gaps</title><link>http://arxiv.org/abs/2410.11225v2</link><description>This paper presents a simple yet efficient method for statistical inferenceof tensor linear forms using incomplete and noisy observations. Under theTucker low-rank tensor model and the missing-at-random assumption, we utilizean appropriate initial estimate along with a debiasing technique followed by aone-step power iteration to construct an asymptotically normal test statistic.This method is suitable for various statistical inference tasks, includingconstructing confidence intervals, inference under heteroskedastic andsub-exponential noise, and simultaneous testing. We demonstrate that theestimator achieves the Cram\'er-Rao lower bound on Riemannian manifolds,indicating its optimality in uncertainty quantification. We comprehensivelyexamine the statistical-to-computational gaps and investigate the impact ofinitialization on the minimal conditions regarding sample size andsignal-to-noise ratio required for accurate inference. Our findings show thatwith independent initialization, statistically optimal sample sizes andsignal-to-noise ratios are sufficient for accurate inference. Conversely, ifonly dependent initialization is available, computationally optimal samplesizes and signal-to-noise ratio conditions still guarantee asymptotic normalitywithout the need for data-splitting. We present the phase transition betweencomputational and statistical limits. Numerical simulation results align withthe theoretical findings.</description><author>Wanteng Ma, Dong Xia</author><pubDate>Fri, 01 Nov 2024 15:51:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11225v2</guid></item><item><title>Designing User-Centric Behavioral Interventions to Prevent Dysglycemia with Novel Counterfactual Explanations</title><link>http://arxiv.org/abs/2310.01684v2</link><description>Monitoring unexpected health events and taking actionable measures to avertthem beforehand is central to maintaining health and preventing disease.Therefore, a tool capable of predicting adverse health events and offeringusers actionable feedback about how to make changes in their diet, exercise,and medication to prevent abnormal health events could have significantsocietal impacts. Counterfactual explanations can provide insights into why amodel made a particular prediction by generating hypothetical instances thatare similar to the original input but lead to a different prediction outcome.Therefore, counterfactuals can be viewed as a means to design AI-driven healthinterventions to not only predict but also prevent adverse health outcomes suchas blood glucose spikes, diabetes, and heart disease. In this paper, we design\textit{\textbf{ExAct}}, a novel model-agnostic framework for generatingcounterfactual explanations for chronic disease prevention and management.Leveraging insights from adversarial learning, ExAct characterizes the decisionboundary for high-dimensional data and performs a grid search to generateactionable interventions. ExAct is unique in integrating prior knowledge aboutuser preferences of feasible explanations into the process of counterfactualgeneration. ExAct is evaluated extensively using four real-world datasets andexternal simulators. With $82.8\%$ average validity in the simulation-aidedvalidation, ExAct surpasses the state-of-the-art techniques for generatingcounterfactual explanations by at least $10\%$. Besides, counterfactuals fromExAct exhibit at least $6.6\%$ improved proximity compared to previousresearch.</description><author>Asiful Arefeen, Hassan Ghasemzadeh</author><pubDate>Fri, 01 Nov 2024 15:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01684v2</guid></item><item><title>Digital Twins in Additive Manufacturing: A Systematic Review</title><link>http://arxiv.org/abs/2409.00877v2</link><description>Digital Twins (DTs) are becoming popular in Additive Manufacturing (AM) dueto their ability to create virtual replicas of physical components of AMmachines, which helps in real-time production monitoring. Advanced techniquessuch as Machine Learning (ML), Augmented Reality (AR), and simulation-basedmodels play key roles in developing intelligent and adaptable DTs inmanufacturing processes. However, questions remain regarding scalability, theintegration of high-quality data, and the computational power required forreal-time applications in developing DTs. Understanding the current state ofDTs in AM is essential to address these challenges and fully utilize theirpotential in advancing AM processes. Considering this opportunity, this workaims to provide a comprehensive overview of DTs in AM by addressing thefollowing four research questions: (1) What are the key types of DTs used in AMand their specific applications? (2) What are the recent developments andimplementations of DTs? (3) How are DTs employed in process improvement andhybrid manufacturing? (4) How are DTs integrated with Industry 4.0technologies? By discussing current applications and techniques, we aim tooffer a better understanding and potential future research directions forresearchers and practitioners in AM and DTs.</description><author>Md Manjurul Ahsan, Yingtao Liu, Shivakumar Raman, Zahed Siddique</author><pubDate>Fri, 01 Nov 2024 15:41:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00877v2</guid></item><item><title>LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering</title><link>http://arxiv.org/abs/2410.18050v2</link><description>Long-Context Question Answering (LCQA), a challenging task, aims to reasonover long-context documents to yield accurate answers to questions. Existinglong-context Large Language Models (LLMs) for LCQA often struggle with the"lost in the middle" issue. Retrieval-Augmented Generation (RAG) mitigates thisissue by providing external factual evidence. However, its chunking strategydisrupts the global long-context information, and its low-quality retrieval inlong contexts hinders LLMs from identifying effective factual details due tosubstantial noise. To this end, we propose LongRAG, a general,dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhanceRAG's understanding of complex long-context knowledge (i.e., global informationand factual details). We design LongRAG as a plug-and-play paradigm,facilitating adaptation to various domains and LLMs. Extensive experiments onthree multi-hop datasets demonstrate that LongRAG significantly outperformslong-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG(up by 17.25%). Furthermore, we conduct quantitative ablation studies andmulti-dimensional analyses, highlighting the effectiveness of the system'scomponents and fine-tuning strategies. Data and code are available athttps://github.com/QingFei1/LongRAG.</description><author>Qingfei Zhao, Ruobing Wang, Yukuo Cen, Daren Zha, Shicheng Tan, Yuxiao Dong, Jie Tang</author><pubDate>Fri, 01 Nov 2024 15:36:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18050v2</guid></item><item><title>Improving Node Representation by Boosting Target-Aware Contrastive Loss</title><link>http://arxiv.org/abs/2410.03901v2</link><description>Graphs model complex relationships between entities, with nodes and edgescapturing intricate connections. Node representation learning involvestransforming nodes into low-dimensional embeddings. These embeddings aretypically used as features for downstream tasks. Therefore, their quality has asignificant impact on task performance. Existing approaches for noderepresentation learning span (semi-)supervised, unsupervised, andself-supervised paradigms. In graph domains, (semi-)supervised learning oftenonly optimizes models based on class labels, neglecting other abundant graphsignals, which limits generalization. While self-supervised or unsupervisedlearning produces representations that better capture underlying graph signals,the usefulness of these captured signals for downstream target tasks can vary.To bridge this gap, we introduce Target-Aware Contrastive Learning(Target-aware CL) which aims to enhance target task performance by maximizingthe mutual information between the target task and node representations with aself-supervised learning process. This is achieved through a sampling function,XGBoost Sampler (XGSampler), to sample proper positive examples for theproposed Target-Aware Contrastive Loss (XTCL). By minimizing XTCL, Target-awareCL increases the mutual information between the target task and noderepresentations, such that model generalization is improved. Additionally,XGSampler enhances the interpretability of each signal by showing the weightsfor sampling the proper positive examples. We show experimentally that XTCLsignificantly improves the performance on two target tasks: node classificationand link prediction tasks, compared to state-of-the-art models.</description><author>Ying-Chun Lin, Jennifer Neville</author><pubDate>Fri, 01 Nov 2024 15:19:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03901v2</guid></item><item><title>Dimension-free deterministic equivalents for random feature regression</title><link>http://arxiv.org/abs/2405.15699v2</link><description>In this work we investigate the generalization performance of random featureridge regression (RFRR). Our main contribution is a general deterministicequivalent for the test error of RFRR. Specifically, under a certainconcentration property, we show that the test error is well approximated by aclosed-form expression that only depends on the feature map eigenvalues.Notably, our approximation guarantee is non-asymptotic, multiplicative, andindependent of the feature map dimension -- allowing for infinite-dimensionalfeatures. We expect this deterministic equivalent to hold broadly beyond ourtheoretical analysis, and we empirically validate its predictions on variousreal and synthetic datasets. As an application, we derive sharp excess errorrates under standard power-law assumptions of the spectrum and target decay. Inparticular, we provide a tight result for the smallest number of featuresachieving optimal minimax error rate.</description><author>Leonardo Defilippis, Bruno Loureiro, Theodor Misiakiewicz</author><pubDate>Fri, 01 Nov 2024 15:13:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15699v2</guid></item><item><title>A survey on deep learning in medical image registration: new technologies, uncertainty, evaluation metrics, and beyond</title><link>http://arxiv.org/abs/2307.15615v4</link><description>Deep learning technologies have dramatically reshaped the field of medicalimage registration over the past decade. The initial developments, such asregression-based and U-Net-based networks, established the foundation for deeplearning in image registration. Subsequent progress has been made in variousaspects of deep learning-based registration, including similarity measures,deformation regularizations, network architectures, and uncertainty estimation.These advancements have not only enriched the field of image registration buthave also facilitated its application in a wide range of tasks, including atlasconstruction, multi-atlas segmentation, motion estimation, and 2D-3Dregistration. In this paper, we present a comprehensive overview of the mostrecent advancements in deep learning-based image registration. We begin with aconcise introduction to the core concepts of deep learning-based imageregistration. Then, we delve into innovative network architectures, lossfunctions specific to registration, and methods for estimating registrationuncertainty. Additionally, this paper explores appropriate evaluation metricsfor assessing the performance of deep learning models in registration tasks.Finally, we highlight the practical applications of these novel techniques inmedical imaging and discuss the future prospects of deep learning-based imageregistration.</description><author>Junyu Chen, Yihao Liu, Shuwen Wei, Zhangxing Bian, Shalini Subramanian, Aaron Carass, Jerry L. Prince, Yong Du</author><pubDate>Fri, 01 Nov 2024 15:13:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15615v4</guid></item><item><title>Leveraging Recurrent Neural Networks for Predicting Motor Movements from Primate Motor Cortex Neural Recordings</title><link>http://arxiv.org/abs/2410.22283v2</link><description>This paper presents an efficient deep learning solution for decoding motormovements from neural recordings in non-human primates. An Autoencoder GatedRecurrent Unit (AEGRU) model was adopted as the model architecture for thistask. The autoencoder is only used during the training stage to achieve bettergeneralization. Together with the preprocessing techniques, our model achieved0.71 $R^2$ score, surpassing the baseline models in Neurobench and is rankedfirst for $R^2$ in the IEEE BioCAS 2024 Grand Challenge on Neural Decoding.Model pruning is also applied leading to a reduction of 41.4% of themultiply-accumulate (MAC) operations with little change in the $R^2$ scorecompared to the unpruned model.</description><author>Yuanxi Wang, Zuowen Wang, Shih-Chii Liu</author><pubDate>Fri, 01 Nov 2024 15:00:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.22283v2</guid></item><item><title>XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference</title><link>http://arxiv.org/abs/2404.15420v3</link><description>In-context learning (ICL) approaches typically leverage prompting tocondition decoder-only language model generation on reference information.Just-in-time processing of a context is inefficient due to the quadratic costof self-attention operations, and caching is desirable. However, cachingtransformer states can easily require almost as much space as the modelparameters. When the right context isn't known in advance, caching ICL can bechallenging. This work addresses these limitations by introducing models that,inspired by the encoder-decoder architecture, use cross-attention to conditiongeneration on reference text without the prompt. More precisely, we leveragepre-trained decoder-only models and only train a small number of added layers.We use Question-Answering (QA) as a testbed to evaluate the ability of ourmodels to perform conditional generation and observe that they outperform ICL,are comparable to fine-tuned prompted LLMs, and drastically reduce the spacefootprint relative to standard KV caching by two orders of magnitude.</description><author>João Monteiro, Étienne Marcotte, Pierre-André Noël, Valentina Zantedeschi, David Vázquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian</author><pubDate>Fri, 01 Nov 2024 14:56:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15420v3</guid></item><item><title>DenoiseRep: Denoising Model for Representation Learning</title><link>http://arxiv.org/abs/2406.08773v3</link><description>The denoising model has been proven a powerful generative model but haslittle exploration of discriminative tasks. Representation learning isimportant in discriminative tasks, which is defined as "learningrepresentations (or features) of the data that make it easier to extract usefulinformation when building classifiers or other predictors". In this paper, wepropose a novel Denoising Model for Representation Learning (DenoiseRep) toimprove feature discrimination with joint feature extraction and denoising.DenoiseRep views each embedding layer in a backbone as a denoising layer,processing the cascaded embedding layers as if we are recursively denoisefeatures step-by-step. This unifies the frameworks of feature extraction anddenoising, where the former progressively embeds features from low-level tohigh-level, and the latter recursively denoises features step-by-step. Afterthat, DenoiseRep fuses the parameters of feature extraction and denoisinglayers, and theoretically demonstrates its equivalence before and after thefusion, thus making feature denoising computation-free. DenoiseRep is alabel-free algorithm that incrementally improves features but alsocomplementary to the label if available. Experimental results on variousdiscriminative vision tasks, including re-identification (Market-1501,DukeMTMC-reID, MSMT17, CUHK-03, vehicleID), image classification (ImageNet,UB200, Oxford-Pet, Flowers), object detection (COCO), image segmentation(ADE20K) show stability and impressive improvements. We also validate itseffectiveness on the CNN (ResNet) and Transformer (ViT, Swin, Vmamda)architectures.</description><author>Zhengrui Xu, Guan'an Wang, Xiaowen Huang, Jitao Sang</author><pubDate>Fri, 01 Nov 2024 14:55:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08773v3</guid></item><item><title>INC-Math: Integrating Natural Language and Code for Enhanced Mathematical Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2409.19381v3</link><description>Large Language Models (LLMs) are commonly used to generate solutions formathematical reasoning problems in the following formats: natural language,code, or a combination of both. In this paper, we explore fundamental questionsrelated to solving mathematical reasoning problems using natural language andcode with state-of-the-art LLMs, including GPT-4o-mini and LLama-3.1-8b-Turbo.Our findings show that LLMs are better at reasoning in natural languagecompared to code. Additionally, although natural language and code serve ascomplementary forms of reasoning, they can affect each other in a negative wayin certain scenarios. These insights motivate our development of a newprompting method, INC-Math, which leverages an LLM to dynamically select themost appropriate reasoning form, resulting in improved performance overcomparable baselines with GPT-4o-mini.</description><author>Xuyuan Xiong, Simeng Han, Ziyue Zhou, Arman Cohan</author><pubDate>Fri, 01 Nov 2024 14:51:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.19381v3</guid></item><item><title>Nova: A Practical and Advanced Alignment</title><link>http://arxiv.org/abs/2410.14940v3</link><description>We introduce Nova, a suite of practical alignment techniques employed in aseries of empirically validated high-performing models. This represents thefirst comprehensive account of alignment methodologies, offering valuableinsights for advancing AI research. We investigate the critical components thatenhance model performance during the alignment process, including optimizationmethods, data strategies, capability enhancements, and evaluation processes.The process spans three key stages: Prompt Augmentation System(PAS), SupervisedFine-Tuning(SFT), and Preference Alignment. The problems encountered, thesolutions applied, and the improvements made are thoroughly recorded. Through comparisons across well-established benchmarks, we highlight thetechnological advancements enabled by Nova Alignment. Importantly,Qwen2-Nova-72B and Llama3-PBM-Nova-70B are instruct versions of the Qwen2-72Band Llama-3-70B base models, optimized through Nova. The Nova models showsignificant core improvements, with user experience gains of 17% to 28%, andexcels on specialized benchmarks. In open-source benchmark evaluations, bothQwen2-Nova-72B and Llama3-PBM-Nova-70B consistently outperform their respectiveofficial instruct versions across nearly all datasets. This report aims toclarify the key technologies behind the alignment process, fostering a deeperunderstanding within the community. Llama3-PBM-Nova-70B model is available athttps://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.</description><author>Mingan Lin, Fan Yang, Yanjun Shen, Haoze Sun, Tianpeng Li, Tao Zhang, Chenzheng Zhu, Tao Zhang, Miao Zheng, Xu Li, Yijie Zhou, Mingyang Chen, Yanzhao Qin, Youquan Li, Hao Liang, Fei Li, Yadong Li, Mang Wang, Guosheng Dong, Kun Fang, Jianhua Xu, Bin Cui, Wentao Zhang, Zenan Zhou, Weipeng Chen</author><pubDate>Fri, 01 Nov 2024 14:49:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14940v3</guid></item><item><title>Return of Unconditional Generation: A Self-supervised Representation Generation Method</title><link>http://arxiv.org/abs/2312.03701v4</link><description>Unconditional generation -- the problem of modeling data distribution withoutrelying on human-annotated labels -- is a long-standing and fundamentalchallenge in generative models, creating a potential of learning fromlarge-scale unlabeled data. In the literature, the generation quality of anunconditional method has been much worse than that of its conditionalcounterpart. This gap can be attributed to the lack of semantic informationprovided by labels. In this work, we show that one can close this gap bygenerating semantic representations in the representation space produced by aself-supervised encoder. These representations can be used to condition theimage generator. This framework, called Representation-Conditioned Generation(RCG), provides an effective solution to the unconditional generation problemwithout using labels. Through comprehensive experiments, we observe that RCGsignificantly improves unconditional generation quality: e.g., it achieves anew state-of-the-art FID of 2.15 on ImageNet 256x256, largely reducing theprevious best of 5.91 by a relative 64%. Our unconditional results are situatedin the same tier as the leading class-conditional ones. We hope theseencouraging observations will attract the community's attention to thefundamental problem of unconditional generation. Code is available athttps://github.com/LTH14/rcg.</description><author>Tianhong Li, Dina Katabi, Kaiming He</author><pubDate>Fri, 01 Nov 2024 14:48:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03701v4</guid></item><item><title>Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking</title><link>http://arxiv.org/abs/2312.07955v2</link><description>Self-Supervised Learning (SSL) is an effective paradigm for learningrepresentations from unlabeled data, such as text, images, and videos. However,researchers have recently found that SSL is vulnerable to backdoor attacks. Theattacker can embed hidden SSL backdoors via a few poisoned examples in thetraining dataset and maliciously manipulate the behavior of downstream models.To defend against SSL backdoor attacks, a feasible route is to detect andremove the poisonous samples in the training set. However, the existing SSLbackdoor defense method fails to detect the poisonous samples precisely. Inthis paper, we propose to erase the SSL backdoor by cluster activation maskingand propose a novel PoisonCAM method. After obtaining the threat model trainedon the poisoned dataset, our method can precisely detect poisonous samplesbased on the assumption that masking the backdoor trigger can effectivelychange the activation of a downstream clustering model. In experiments, ourPoisonCAM achieves 96\% accuracy for backdoor trigger detection compared to 3\%of the state-of-the-art method on poisoned ImageNet-100. Moreover, our proposedPoisonCAM significantly improves the performance of the trained SSL model underbackdoor attacks compared to the state-of-the-art method. Our code, data, andtrained models will be open once this paper is accepted.</description><author>Shengsheng Qian, Dizhan Xue, Yifei Wang, Shengjie Zhang, Huaiwen Zhang, Changsheng Xu</author><pubDate>Fri, 01 Nov 2024 14:45:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07955v2</guid></item><item><title>Autoregressive Image Generation without Vector Quantization</title><link>http://arxiv.org/abs/2406.11838v3</link><description>Conventional wisdom holds that autoregressive models for image generation aretypically accompanied by vector-quantized tokens. We observe that while adiscrete-valued space can facilitate representing a categorical distribution,it is not a necessity for autoregressive modeling. In this work, we propose tomodel the per-token probability distribution using a diffusion procedure, whichallows us to apply autoregressive models in a continuous-valued space. Ratherthan using categorical cross-entropy loss, we define a Diffusion Loss functionto model the per-token probability. This approach eliminates the need fordiscrete-valued tokenizers. We evaluate its effectiveness across a wide rangeof cases, including standard autoregressive models and generalized maskedautoregressive (MAR) variants. By removing vector quantization, our imagegenerator achieves strong results while enjoying the speed advantage ofsequence modeling. We hope this work will motivate the use of autoregressivegeneration in other continuous-valued domains and applications. Code isavailable at: https://github.com/LTH14/mar.</description><author>Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, Kaiming He</author><pubDate>Fri, 01 Nov 2024 14:45:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11838v3</guid></item><item><title>Neur2BiLO: Neural Bilevel Optimization</title><link>http://arxiv.org/abs/2402.02552v2</link><description>Bilevel optimization deals with nested problems in which a leader takes thefirst decision to minimize their objective function while accounting for afollower's best-response reaction. Constrained bilevel problems with integervariables are particularly notorious for their hardness. While exact solvershave been proposed for mixed-integer linear bilevel optimization, they tend toscale poorly with problem size and are hard to generalize to the non-linearcase. On the other hand, problem-specific algorithms (exact and heuristic) arelimited in scope. Under a data-driven setting in which similar instances of abilevel problem are solved routinely, our proposed framework, Neur2BiLO, embedsa neural network approximation of the leader's or follower's value function,trained via supervised regression, into an easy-to-solve mixed-integer program.Neur2BiLO serves as a heuristic that produces high-quality solutions extremelyfast for four applications with linear and non-linear objectives and pure andmixed-integer variables.</description><author>Justin Dumouchelle, Esther Julien, Jannis Kurtz, Elias B. Khalil</author><pubDate>Fri, 01 Nov 2024 14:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02552v2</guid></item><item><title>TaskBench: Benchmarking Large Language Models for Task Automation</title><link>http://arxiv.org/abs/2311.18760v4</link><description>In recent years, the remarkable progress of large language models (LLMs) hassparked interest in task automation, which involves decomposing complex tasksdescribed by user instructions into sub-tasks and invoking external tools toexecute them, playing a central role in autonomous agents. However, there is alack of systematic and standardized benchmarks to promote the development ofLLMs in task automation. To address this, we introduce TaskBench, acomprehensive framework to evaluate the capability of LLMs in task automation.Specifically, task automation can be divided into three critical stages: taskdecomposition, tool selection, and parameter prediction. To tackle thecomplexities inherent in these stages, we introduce the concept of Tool Graphto represent decomposed tasks and adopt a back-instruct method to generatehigh-quality user instructions. We propose TaskEval, a multi-faceted evaluationmethodology that assesses LLM performance across these three stages. Ourapproach combines automated construction with rigorous human verification,ensuring high consistency with human evaluation. Experimental resultsdemonstrate that TaskBench effectively reflects the capabilities of variousLLMs in task automation. It provides insights into model performance acrossdifferent task complexities and domains, pushing the boundaries of what currentmodels can achieve. TaskBench offers a scalable, adaptable, and reliablebenchmark for advancing LLM-based autonomous agents.</description><author>Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, Yueting Zhuang</author><pubDate>Fri, 01 Nov 2024 14:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18760v4</guid></item><item><title>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</title><link>http://arxiv.org/abs/2308.00352v7</link><description>Remarkable progress has been made on automated problem solving throughsocieties of agents based on large language models (LLMs). Existing LLM-basedmulti-agent systems can already solve simple dialogue tasks. Solutions to morecomplex tasks, however, are complicated through logic inconsistencies due tocascading hallucinations caused by naively chaining LLMs. Here we introduceMetaGPT, an innovative meta-programming framework incorporating efficient humanworkflows into LLM-based multi-agent collaborations. MetaGPT encodesStandardized Operating Procedures (SOPs) into prompt sequences for morestreamlined workflows, thus allowing agents with human-like domain expertise toverify intermediate results and reduce errors. MetaGPT utilizes an assemblyline paradigm to assign diverse roles to various agents, efficiently breakingdown complex tasks into subtasks involving many agents working together. Oncollaborative software engineering benchmarks, MetaGPT generates more coherentsolutions than previous chat-based multi-agent systems. Our project can befound at https://github.com/geekan/MetaGPT</description><author>Sirui Hong, Mingchen Zhuge, Jiaqi Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, Jürgen Schmidhuber</author><pubDate>Fri, 01 Nov 2024 14:36:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00352v7</guid></item><item><title>QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation</title><link>http://arxiv.org/abs/2406.00132v2</link><description>We propose Quantum-informed Tensor Adaptation (QuanTA), a novel,easy-to-implement, fine-tuning method with no inference overhead forlarge-scale pre-trained language models. By leveraging quantum-inspired methodsderived from quantum circuit structures, QuanTA enables efficient high-rankfine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)--low-rankapproximation may fail for complicated downstream tasks. Our approach istheoretically supported by the universality theorem and the rank representationtheorem to achieve efficient high-rank adaptations. Experiments demonstratethat QuanTA significantly enhances commonsense reasoning, arithmetic reasoning,and scalability compared to traditional methods. Furthermore, QuanTA showssuperior performance with fewer trainable parameters compared to otherapproaches and can be designed to integrate with existing fine-tuningalgorithms for further improvement, providing a scalable and efficient solutionfor fine-tuning large language models and advancing state-of-the-art in naturallanguage processing.</description><author>Zhuo Chen, Rumen Dangovski, Charlotte Loh, Owen Dugan, Di Luo, Marin Soljačić</author><pubDate>Fri, 01 Nov 2024 14:36:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00132v2</guid></item><item><title>Reinforced In-Context Black-Box Optimization</title><link>http://arxiv.org/abs/2402.17423v3</link><description>Black-Box Optimization (BBO) has found successful applications in many fieldsof science and engineering. Recently, there has been a growing interest inmeta-learning particular components of BBO algorithms to speed up optimizationand get rid of tedious hand-crafted heuristics. As an extension, learning theentire algorithm from data requires the least labor from experts and canprovide the most flexibility. In this paper, we propose RIBBO, a method toreinforce-learn a BBO algorithm from offline data in an end-to-end fashion.RIBBO employs expressive sequence models to learn the optimization historiesproduced by multiple behavior algorithms and tasks, leveraging the in-contextlearning ability of large models to extract task information and make decisionsaccordingly. Central to our method is to augment the optimization historieswith \textit{regret-to-go} tokens, which are designed to represent theperformance of an algorithm based on cumulative regret over the future part ofthe histories. The integration of regret-to-go tokens enables RIBBO toautomatically generate sequences of query points that satisfy the user-desiredregret, which is verified by its universally good empirical performance ondiverse problems, including BBO benchmark functions, hyper-parameteroptimization and robot control problems.</description><author>Lei Song, Chenxiao Gao, Ke Xue, Chenyang Wu, Dong Li, Jianye Hao, Zongzhang Zhang, Chao Qian</author><pubDate>Fri, 01 Nov 2024 14:32:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17423v3</guid></item><item><title>Large Language Models for Patient Comments Multi-Label Classification</title><link>http://arxiv.org/abs/2410.23528v2</link><description>Patient experience and care quality are crucial for a hospital'ssustainability and reputation. The analysis of patient feedback offers valuableinsight into patient satisfaction and outcomes. However, the unstructurednature of these comments poses challenges for traditional machine learningmethods following a supervised learning paradigm. This is due to theunavailability of labeled data and the nuances these texts encompass. Thisresearch explores leveraging Large Language Models (LLMs) in conductingMulti-label Text Classification (MLTC) of inpatient comments shared after astay in the hospital. GPT-4 Turbo was leveraged to conduct the classification.However, given the sensitive nature of patients' comments, a security layer isintroduced before feeding the data to the LLM through a Protected HealthInformation (PHI) detection framework, which ensures patients'de-identification. Additionally, using the prompt engineering framework,zero-shot learning, in-context learning, and chain-of-thought prompting wereexperimented with. Results demonstrate that GPT-4 Turbo, whether following azero-shot or few-shot setting, outperforms traditional methods and Pre-trainedLanguage Models (PLMs) and achieves the highest overall performance with anF1-score of 76.12% and a weighted F1-score of 73.61% followed closely by thefew-shot learning results. Subsequently, the results' association with otherpatient experience structured variables (e.g., rating) was conducted. The studyenhances MLTC through the application of LLMs, offering healthcarepractitioners an efficient method to gain deeper insights into patient feedbackand deliver prompt, appropriate responses.</description><author>Hajar Sakai, Sarah S. Lam, Mohammadsadegh Mikaeili, Joshua Bosire, Franziska Jovin</author><pubDate>Fri, 01 Nov 2024 14:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23528v2</guid></item><item><title>Disentangling spatio-temporal knowledge for weakly supervised object detection and segmentation in surgical video</title><link>http://arxiv.org/abs/2407.15794v4</link><description>Weakly supervised video object segmentation (WSVOS) enables theidentification of segmentation maps without requiring an extensive trainingdataset of object masks, relying instead on coarse video labels indicatingobject presence. Current state-of-the-art methods either require multipleindependent stages of processing that employ motion cues or, in the case ofend-to-end trainable networks, lack in segmentation accuracy, in part due tothe difficulty of learning segmentation maps from videos with transient objectpresence. This limits the application of WSVOS for semantic annotation ofsurgical videos where multiple surgical tools frequently move in and out of thefield of view, a problem that is more difficult than typically encountered inWSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks(VDST-Net), a framework to disentangle spatiotemporal information usingsemi-decoupled knowledge distillation to predict high-quality class activationmaps (CAMs). A teacher network designed to resolve temporal conflicts whenspecifics about object location and timing in the video are not provided workswith a student network that integrates information over time by leveragingtemporal dependencies. We demonstrate the efficacy of our framework on a publicreference dataset and on a more challenging surgical video dataset whereobjects are, on average, present in less than 60\% of annotated frames. Ourmethod outperforms state-of-the-art techniques and generates superiorsegmentation masks under video-level weak supervision.</description><author>Guiqiu Liao, Matjaz Jogan, Sai Koushik, Eric Eaton, Daniel A. Hashimoto</author><pubDate>Fri, 01 Nov 2024 14:19:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15794v4</guid></item><item><title>$\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity</title><link>http://arxiv.org/abs/2407.10691v2</link><description>Recent studies show the growing significance of document retrieval in thegeneration of LLMs, i.e., RAG, within the scientific domain by bridging theirknowledge gap. However, dense retrievers often struggle with domain-specificretrieval and complex query-document relationships, particularly when querysegments correspond to various parts of a document. To alleviate such prevalentchallenges, this paper introduces $\texttt{MixGR}$, which improves denseretrievers' awareness of query-document matching across various levels ofgranularity in queries and documents using a zero-shot approach.$\texttt{MixGR}$ fuses various metrics based on these granularities to a unitedscore that reflects a comprehensive query-document similarity. Our experimentsdemonstrate that $\texttt{MixGR}$ outperforms previous document retrieval by24.7%, 9.8%, and 6.9% on nDCG@5 with unsupervised, supervised, and LLM-basedretrievers, respectively, averaged on queries containing multiple subqueriesfrom five scientific retrieval datasets. Moreover, the efficacy of twodownstream scientific question-answering tasks highlights the advantage of$\texttt{MixGR}$ to boost the application of LLMs in the scientific domain. Thecode and experimental datasets are available.</description><author>Fengyu Cai, Xinran Zhao, Tong Chen, Sihao Chen, Hongming Zhang, Iryna Gurevych, Heinz Koeppl</author><pubDate>Fri, 01 Nov 2024 14:08:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10691v2</guid></item><item><title>ConvBKI: Real-Time Probabilistic Semantic Mapping Network with Quantifiable Uncertainty</title><link>http://arxiv.org/abs/2310.16020v3</link><description>In this paper, we develop a modular neural network for real-time{\color{black}(&gt; 10 Hz)} semantic mapping in uncertain environments, whichexplicitly updates per-voxel probabilistic distributions within a neuralnetwork layer. Our approach combines the reliability of classical probabilisticalgorithms with the performance and efficiency of modern neural networks.Although robotic perception is often divided between modern differentiablemethods and classical explicit methods, a union of both is necessary forreal-time and trustworthy performance. We introduce a novel ConvolutionalBayesian Kernel Inference (ConvBKI) layer which incorporates semanticsegmentation predictions online into a 3D map through a depthwise convolutionlayer by leveraging conjugate priors. We compare ConvBKI againststate-of-the-art deep learning approaches and probabilistic algorithms formapping to evaluate reliability and performance. We also create a RobotOperating System (ROS) package of ConvBKI and test it on real-worldperceptually challenging off-road driving data.</description><author>Joey Wilson, Yuewei Fu, Joshua Friesen, Parker Ewen, Andrew Capodieci, Paramsothy Jayakumar, Kira Barton, Maani Ghaffari</author><pubDate>Fri, 01 Nov 2024 13:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16020v3</guid></item><item><title>Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer</title><link>http://arxiv.org/abs/2406.00976v2</link><description>While recent advancements in speech language models have achieved significantprogress, they face remarkable challenges in modeling the long acousticsequences of neural audio codecs. In this paper, we introduce\textbf{G}enerative \textbf{P}re-trained \textbf{S}peech \textbf{T}ransformer(GPST), a hierarchical transformer designed for efficient speech languagemodeling. GPST quantizes audio waveforms into two distinct types of discretespeech representations and integrates them within a hierarchical transformerarchitecture, allowing for a unified one-stage generation process and enhancingHi-Res audio generation capabilities. By training on large corpora of speechesin an end-to-end unsupervised manner, GPST can generate syntacticallyconsistent speech with diverse speaker identities. Given a brief 3-secondprompt, GPST can produce natural and coherent personalized speech,demonstrating in-context learning abilities. Moreover, our approach can beeasily extended to spoken cross-lingual speech generation by incorporatingmulti-lingual semantic tokens and universal acoustic tokens. Experimentalresults indicate that GPST significantly outperforms the existing speechlanguage models in terms of word error rate, speech quality, and speakersimilarity. The code is available at \url{https://github.com/youngsheen/GPST}.</description><author>Yongxin Zhu, Dan Su, Liqiang He, Linli Xu, Dong Yu</author><pubDate>Fri, 01 Nov 2024 13:54:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00976v2</guid></item><item><title>Multimodal Fusion on Low-quality Data: A Comprehensive Survey</title><link>http://arxiv.org/abs/2404.18947v3</link><description>Multimodal fusion focuses on integrating information from multiple modalitieswith the goal of more accurate prediction, which has achieved remarkableprogress in a wide range of scenarios, including autonomous driving and medicaldiagnosis. However, the reliability of multimodal fusion remains largelyunexplored especially under low-quality data settings. This paper surveys thecommon challenges and recent advances of multimodal fusion in the wild andpresents them in a comprehensive taxonomy. From a data-centric view, weidentify four main challenges that are faced by multimodal fusion onlow-quality data, namely (1) noisy multimodal data that are contaminated withheterogeneous noises, (2) incomplete multimodal data that some modalities aremissing, (3) imbalanced multimodal data that the qualities or properties ofdifferent modalities are significantly different and (4) quality-varyingmultimodal data that the quality of each modality dynamically changes withrespect to different samples. This new taxonomy will enable researchers tounderstand the state of the field and identify several potential directions. Wealso provide discussion for the open problems in this field together withinteresting future research directions.</description><author>Qingyang Zhang, Yake Wei, Zongbo Han, Huazhu Fu, Xi Peng, Cheng Deng, Qinghua Hu, Cai Xu, Jie Wen, Di Hu, Changqing Zhang</author><pubDate>Fri, 01 Nov 2024 13:53:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18947v3</guid></item><item><title>Analysis of Bootstrap and Subsampling in High-dimensional Regularized Regression</title><link>http://arxiv.org/abs/2402.13622v2</link><description>We investigate popular resampling methods for estimating the uncertainty ofstatistical models, such as subsampling, bootstrap and the jackknife, and theirperformance in high-dimensional supervised regression tasks. We provide a tightasymptotic description of the biases and variances estimated by these methodsin the context of generalized linear models, such as ridge and logisticregression, taking the limit where the number of samples $n$ and dimension $d$of the covariates grow at a comparable fixed rate $\alpha\!=\! n/d$. Ourfindings are three-fold: i) resampling methods are fraught with problems inhigh dimensions and exhibit the double-descent-like behavior typical of thesesituations; ii) only when $\alpha$ is large enough do they provide consistentand reliable error estimations (we give convergence rates); iii) in theover-parametrized regime $\alpha\!&lt;\!1$ relevant to modern machine learningpractice, their predictions are not consistent, even with optimalregularization.</description><author>Lucas Clarté, Adrien Vandenbroucque, Guillaume Dalle, Bruno Loureiro, Florent Krzakala, Lenka Zdeborová</author><pubDate>Fri, 01 Nov 2024 13:33:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13622v2</guid></item><item><title>Theoretical Foundations of Deep Selective State-Space Models</title><link>http://arxiv.org/abs/2402.19047v3</link><description>Structured state-space models (SSMs) such as S4, stemming from the seminalwork of Gu et al., are gaining popularity as effective approaches for modelingsequential data. Deep SSMs demonstrate outstanding performance across a diverseset of domains, at a reduced training and inference cost compared toattention-based transformers. Recent developments show that if the linearrecurrence powering SSMs allows for multiplicative interactions between inputsand hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecturecan surpass in both in accuracy and efficiency attention-powered foundationmodels trained on text, at scales of billion parameters. In this paper, we givetheoretical grounding to this recent finding using tools from Rough PathTheory: we show that when random linear recurrences are equipped with simpleinput-controlled transitions (selectivity mechanism), then the hidden state isprovably a low-dimensional projection of a powerful mathematical object calledthe signature of the input -- capturing non-linear interactions between tokensat distinct timescales. Our theory not only motivates the success of modernselective state-space models such as Mamba but also provides a solid frameworkto understand the expressive power of future SSM variants.</description><author>Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi, Terry Lyons</author><pubDate>Fri, 01 Nov 2024 13:28:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19047v3</guid></item><item><title>Semantic Density: Uncertainty Quantification for Large Language Models through Confidence Measurement in Semantic Space</title><link>http://arxiv.org/abs/2405.13845v3</link><description>With the widespread application of Large Language Models (LLMs) to variousdomains, concerns regarding the trustworthiness of LLMs in safety-criticalscenarios have been raised, due to their unpredictable tendency to hallucinateand generate misinformation. Existing LLMs do not have an inherentfunctionality to provide the users with an uncertainty/confidence metric foreach response it generates, making it difficult to evaluate trustworthiness.Although several studies aim to develop uncertainty quantification methods forLLMs, they have fundamental limitations, such as being restricted toclassification tasks, requiring additional training and data, considering onlylexical instead of semantic information, and being prompt-wise but notresponse-wise. A new framework is proposed in this paper to address theseissues. Semantic density extracts uncertainty/confidence information for eachresponse from a probability distribution perspective in semantic space. It hasno restriction on task types and is "off-the-shelf" for new models and tasks.Experiments on seven state-of-the-art LLMs, including the latest Llama 3 andMixtral-8x22B models, on four free-form question-answering benchmarksdemonstrate the superior performance and robustness of semantic densitycompared to prior approaches.</description><author>Xin Qiu, Risto Miikkulainen</author><pubDate>Fri, 01 Nov 2024 13:25:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13845v3</guid></item><item><title>Accelerating Transfer Learning with Near-Data Computation on Cloud Object Stores</title><link>http://arxiv.org/abs/2210.08650v3</link><description>Storage disaggregation underlies today's cloud and is naturally complementedby pushing down some computation to storage, thus mitigating the potentialnetwork bottleneck between the storage and compute tiers. We show how MLtraining benefits from storage pushdowns by focusing on transfer learning (TL),the widespread technique that democratizes ML by reusing existing knowledge onrelated tasks. We propose HAPI, a new TL processing system centered around twocomplementary techniques that address challenges introduced by disaggregation.First, applications must carefully balance execution across tiers forperformance. HAPI judiciously splits the TL computation during the featureextraction phase yielding pushdowns that not only improve network time but alsoimprove total TL training time by overlapping the execution of consecutivetraining iterations across tiers. Second, operators want resource efficiencyfrom the storage-side computational resources. HAPI employs storage-side batchsize adaptation allowing increased storage-side pushdown concurrency withoutaffecting training accuracy. HAPI yields up to 2.5x training speed-up whilechoosing in 86.8% of cases the best performing split point or one that is atmost 5% off from the best.</description><author>Diana Petrescu, Arsany Guirguis, Do Le Quoc, Javier Picorel, Rachid Guerraoui, Florin Dinu</author><pubDate>Fri, 01 Nov 2024 13:02:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.08650v3</guid></item><item><title>Kuro Siwo: 33 billion $m^2$ under the water. A global multi-temporal satellite dataset for rapid flood mapping</title><link>http://arxiv.org/abs/2311.12056v3</link><description>Global floods, exacerbated by climate change, pose severe threats to humanlife, infrastructure, and the environment. Recent catastrophic events inPakistan and New Zealand underscore the urgent need for precise flood mappingto guide restoration efforts, understand vulnerabilities, and prepare forfuture occurrences. While Synthetic Aperture Radar (SAR) remote sensing offersday-and-night, all-weather imaging capabilities, its application in deeplearning for flood segmentation is limited by the lack of large annotateddatasets. To address this, we introduce Kuro Siwo, a manually annotatedmulti-temporal dataset, spanning 43 flood events globally. Our dataset mapsmore than 338 billion $m^2$ of land, with 33 billion designated as eitherflooded areas or permanent water bodies. Kuro Siwo includes a highly processedproduct optimized for flood mapping based on SAR Ground Range Detected, and aprimal SAR Single Look Complex product with minimal preprocessing, designed topromote research on the exploitation of both the phase and amplitudeinformation and to offer maximum flexibility for downstream task preprocessing.To leverage advances in large scale self-supervised pretraining methods forremote sensing data, we augment Kuro Siwo with a large unlabeled set of SARsamples. Finally, we provide an extensive benchmark, namely BlackBench,offering strong baselines for a diverse set of flood events from Europe,America, Africa, Asia and Australia.</description><author>Nikolaos Ioannis Bountos, Maria Sdraka, Angelos Zavras, Ilektra Karasante, Andreas Karavias, Themistocles Herekakis, Angeliki Thanasou, Dimitrios Michail, Ioannis Papoutsis</author><pubDate>Fri, 01 Nov 2024 12:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12056v3</guid></item><item><title>On-Air Deep Learning Integrated Semantic Inference Models for Enhanced Earth Observation Satellite Networks</title><link>http://arxiv.org/abs/2409.15246v3</link><description>Earth Observation (EO) systems are crucial for cartography, disastersurveillance, and resource administration. Nonetheless, they encounterconsiderable obstacles in the processing and transmission of extensive data,especially in specialized domains such as precision agriculture and real-timedisaster response. Earth observation satellites, outfitted with remote sensingtechnology, gather data from onboard sensors and IoT-enabled terrestrialobjects, delivering important information remotely. Domain-adapted LargeLanguage Models (LLMs) provide a solution by enabling the integration of rawand processed EO data. Through domain adaptation, LLMs improve the assimilationand analysis of many data sources, tackling the intricacies of specializeddatasets in agriculture and disaster response. This data synthesis, directed byLLMs, enhances the precision and pertinence of conveyed information. This studyprovides a thorough examination of using semantic inference and deep learningfor sophisticated EO systems. It presents an innovative architecture forsemantic communication in EO satellite networks, designed to improve datatransmission efficiency using semantic processing methodologies. Recentadvancements in onboard processing technologies enable dependable, adaptable,and energy-efficient data management in orbit. These improvements guaranteereliable performance in adverse space circumstances using radiation-hardenedand reconfigurable technology. Collectively, these advancements enablenext-generation satellite missions with improved processing capabilities,crucial for operational flexibility and real-time decision-making in 6Gsatellite communication.</description><author>Hong-fu Chou, Vu Nguyen Ha, Prabhu Thiruvasagam, Thanh-Dung Le, Geoffrey Eappen, Ti Ti Nguyen, Luis M. Garces-Socarras, Jorge L. Gonzalez-Rios, Juan Carlos Merlano-Duncan, Symeon Chatzinotas</author><pubDate>Fri, 01 Nov 2024 12:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15246v3</guid></item><item><title>Video Diffusion Models are Training-free Motion Interpreter and Controller</title><link>http://arxiv.org/abs/2405.14864v2</link><description>Video generation primarily aims to model authentic and customized motionacross frames, making understanding and controlling the motion a crucial topic.Most diffusion-based studies on video motion focus on motion customization withtraining-based paradigms, which, however, demands substantial trainingresources and necessitates retraining for diverse models. Crucially, theseapproaches do not explore how video diffusion models encode cross-frame motioninformation in their features, lacking interpretability and transparency intheir effectiveness. To answer this question, this paper introduces a novelperspective to understand, localize, and manipulate motion-aware features invideo diffusion models. Through analysis using Principal Component Analysis(PCA), our work discloses that robust motion-aware feature already exists invideo diffusion models. We present a new MOtion FeaTure (MOFT) by eliminatingcontent correlation information and filtering motion channels. MOFT provides adistinct set of benefits, including the ability to encode comprehensive motioninformation with clear interpretability, extraction without the need fortraining, and generalizability across diverse architectures. Leveraging MOFT,we propose a novel training-free video motion control framework. Our methoddemonstrates competitive performance in generating natural and faithful motion,providing architecture-agnostic insights and applicability in a variety ofdownstream tasks.</description><author>Zeqi Xiao, Yifan Zhou, Shuai Yang, Xingang Pan</author><pubDate>Fri, 01 Nov 2024 12:46:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14864v2</guid></item><item><title>Improving Generalization in Visual Reasoning via Self-Ensemble</title><link>http://arxiv.org/abs/2410.20883v2</link><description>The cognitive faculty of visual reasoning necessitates the integration ofmultimodal perceptual processing and commonsense and external knowledge of theworld. In recent years, a plethora of large vision-language models (LVLMs) havebeen proposed, demonstrating outstanding power and exceptional proficiency incommonsense reasoning across diverse domains and tasks. Nevertheless, trainingsuch LVLMs requires a lot of costly resources. Recent approaches, instead oftraining LVLMs from scratch on various large datasets, focus on exploring waysto take advantage of the capabilities of many different LVLMs, such as ensemblemethods. In this work, we propose self-ensemble, a novel method that improvesthe generalization and visual reasoning of the model without updating anyparameters, a training-free method. Our key insight is that we realized thatLVLM itself can ensemble without the need for any other LVLMs, which helps tounlock their internal capabilities. Extensive experiments on various benchmarksdemonstrate the effectiveness of our method in achieving state-of-the-art(SOTA) performance on SketchyVQA, Outside Knowledge VQA, andout-of-distribution VQA tasks.</description><author>Tien-Huy Nguyen, Quang-Khai Tran, Anh-Tuan Quang-Hoang</author><pubDate>Fri, 01 Nov 2024 12:42:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.20883v2</guid></item><item><title>LaCour!: Enabling Research on Argumentation in Hearings of the European Court of Human Rights</title><link>http://arxiv.org/abs/2312.05061v4</link><description>Why does an argument end up in the final court decision? Was it deliberatedor questioned during the oral hearings? Was there something in the hearingsthat triggered a particular judge to write a dissenting opinion? Despite theavailability of the final judgments of the European Court of Human Rights(ECHR), none of these legal research questions can currently be answered as theECHR's multilingual oral hearings are not transcribed, structured, orspeaker-attributed. We address this fundamental gap by presenting LaCour!, thefirst corpus of textual oral arguments of the ECHR, consisting of 154 fullhearings (2.1 million tokens from over 267 hours of video footage) in English,French, and other court languages, each linked to the corresponding finaljudgment documents. In addition to the transcribed and partially manuallycorrected text from the video, we provide sentence-level timestamps andmanually annotated role and language labels. We also showcase LaCour! in a setof preliminary experiments that explore the interplay between questions anddissenting opinions. Apart from the use cases in legal NLP, we hope that lawstudents or other interested parties will also use LaCour! as a learningresource, as it is freely available in various formats athttps://huggingface.co/datasets/TrustHLT/LaCour.</description><author>Lena Held, Ivan Habernal</author><pubDate>Fri, 01 Nov 2024 12:40:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05061v4</guid></item><item><title>Multi-Agent Large Language Models for Conversational Task-Solving</title><link>http://arxiv.org/abs/2410.22932v2</link><description>In an era where single large language models have dominated the landscape ofartificial intelligence for years, multi-agent systems arise as newprotagonists in conversational task-solving. While previous studies haveshowcased their potential in reasoning tasks and creative endeavors, ananalysis of their limitations concerning the conversational paradigms and theimpact of individual agents is missing. It remains unascertained howmulti-agent discussions perform across tasks of varying complexity and how thestructure of these conversations influences the process. To fill that gap, thiswork systematically evaluates multi-agent systems across various discussionparadigms, assessing their strengths and weaknesses in both generative tasksand question-answering tasks. Alongside the experiments, I propose a taxonomyof 20 multi-agent research studies from 2022 to 2024, followed by theintroduction of a framework for deploying multi-agent LLMs in conversationaltask-solving. I demonstrate that while multi-agent systems excel in complexreasoning tasks, outperforming a single model by leveraging expert personas,they fail on basic tasks. Concretely, I identify three challenges that arise:1) While longer discussions enhance reasoning, agents fail to maintainconformity to strict task requirements, which leads to problem drift, makingshorter conversations more effective for basic tasks. 2) Prolonged discussionsrisk alignment collapse, raising new safety concerns for these systems. 3) Ishowcase discussion monopolization through long generations, posing the problemof fairness in decision-making for tasks like summarization. This work uncoversboth the potential and challenges that arise with multi-agent interaction andvarying conversational paradigms, providing insights into how future researchcould improve the efficiency, performance, and safety of multi-agent LLMs.</description><author>Jonas Becker</author><pubDate>Fri, 01 Nov 2024 12:37:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.22932v2</guid></item><item><title>Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level</title><link>http://arxiv.org/abs/2405.16405v2</link><description>Graph Neural Networks (GNNs) excel across various applications but remainvulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs),which inject malicious nodes into the original graph and pose realisticthreats. Text-attributed graphs (TAGs), where nodes are associated with textualfeatures, are crucial due to their prevalence in real-world applications andare commonly used to evaluate these vulnerabilities. However, existing researchonly focuses on embedding-level GIAs, which inject node embeddings rather thanactual textual content, limiting their applicability and simplifying detection.In this paper, we pioneer the exploration of GIAs at the text level, presentingthree novel attack designs that inject textual content into the graph. Throughtheoretical and empirical analysis, we demonstrate that text interpretability,a factor previously overlooked at the embedding level, plays a crucial role inattack strength. Among the designs we investigate, the Word-frequency-basedText-level GIA (WTGIA) is particularly notable for its balance betweenperformance and interpretability. Despite the success of WTGIA, we discoverthat defenders can easily enhance their defenses with customized text embeddingmethods or large language model (LLM)--based predictors. These insightsunderscore the necessity for further research into the potential and practicalsignificance of text-level GIAs.</description><author>Runlin Lei, Yuwei Hu, Yuchen Ren, Zhewei Wei</author><pubDate>Fri, 01 Nov 2024 12:15:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16405v2</guid></item><item><title>FRoundation: Are Foundation Models Ready for Face Recognition?</title><link>http://arxiv.org/abs/2410.23831v2</link><description>Foundation models are predominantly trained in an unsupervised orself-supervised manner on highly diverse and large-scale datasets, making thembroadly applicable to various downstream tasks. In this work, we investigatefor the first time whether such models are suitable for the specific domain offace recognition. We further propose and demonstrate the adaptation of thesemodels for face recognition across different levels of data availability.Extensive experiments are conducted on multiple foundation models and datasetsof varying scales for training and fine-tuning, with evaluation on a wide rangeof benchmarks. Our results indicate that, despite their versatility,pre-trained foundation models underperform in face recognition compared tosimilar architectures trained specifically for this task. However, fine-tuningfoundation models yields promising results, often surpassing models trainedfrom scratch when training data is limited. Even with access to large-scaleface recognition training datasets, fine-tuned foundation models performcomparably to models trained from scratch, but with lower trainingcomputational costs and without relying on the assumption of extensive dataavailability. Our analysis also explores bias in face recognition, withslightly higher bias observed in some settings when using foundation models.</description><author>Tahar Chettaoui, Naser Damer, Fadi Boutros</author><pubDate>Fri, 01 Nov 2024 12:11:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23831v2</guid></item><item><title>Towards shutdownable agents via stochastic choice</title><link>http://arxiv.org/abs/2407.00805v2</link><description>Some worry that advanced artificial agents may resist being shut down. TheIncomplete Preferences Proposal (IPP) is an idea for ensuring that doesn'thappen. A key part of the IPP is using a novel 'Discounted REward forSame-Length Trajectories (DREST)' reward function to train agents to (1) pursuegoals effectively conditional on each trajectory-length (be 'USEFUL'), and (2)choose stochastically between different trajectory-lengths (be 'NEUTRAL' abouttrajectory-lengths). In this paper, we propose evaluation metrics forUSEFULNESS and NEUTRALITY. We use a DREST reward function to train simpleagents to navigate gridworlds, and we find that these agents learn to be USEFULand NEUTRAL. Our results thus suggest that DREST reward functions could alsotrain advanced agents to be USEFUL and NEUTRAL, and thereby make these advancedagents useful and shutdownable.</description><author>Elliott Thornley, Alexander Roman, Christos Ziakas, Leyton Ho, Louis Thomson</author><pubDate>Fri, 01 Nov 2024 12:10:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00805v2</guid></item><item><title>Tree Ensembles for Contextual Bandits</title><link>http://arxiv.org/abs/2402.06963v3</link><description>We propose a new framework for contextual multi-armed bandits based on treeensembles. Our framework adapts two widely used bandit methods, UpperConfidence Bound and Thompson Sampling, for both standard and combinatorialsettings. As part of this framework, we propose a novel method of estimatingthe uncertainty in tree ensemble predictions. We further demonstrate theeffectiveness of our framework via several experimental studies, employingXGBoost and random forests, two popular tree ensemble methods. Compared tostate-of-the-art methods based on decision trees and neural networks, ourmethods exhibit superior performance in terms of both regret minimization andcomputational runtime, when applied to benchmark datasets and the real-worldapplication of navigation over road networks.</description><author>Hannes Nilsson, Rikard Johansson, Niklas Åkerblom, Morteza Haghir Chehreghani</author><pubDate>Fri, 01 Nov 2024 11:46:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06963v3</guid></item><item><title>LongVILA: Scaling Long-Context Visual Language Models for Long Videos</title><link>http://arxiv.org/abs/2408.10188v5</link><description>Long-context capability is critical for multi-modal foundation models,especially for long video understanding. We introduce LongVILA, a full-stacksolution for long-context visual-language models by co-designing the algorithmand system. For model training, we upgrade existing VLMs to support long videounderstanding by incorporating two additional stages, i.e., long contextextension and long video supervised fine-tuning. However, training on longvideo is computationally and memory intensive. We introduce the long-contextMulti-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizeslong video training and inference, enabling 2M context length training on 256GPUs without any gradient checkpointing. LongVILA efficiently extends thenumber of video frames of VILA from 8 to 2048, improving the long videocaptioning score from 2.00 to 3.26 (out of 5), achieving 99.8% accuracy in6,000-frame (more than 1 million tokens) video needle-in-a-haystack.LongVILA-7B demonstrates strong accuracy on the VideoMME benchmark, i.e., 61.8%with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequenceparallelism and 1.1x - 1.4x faster than Megatron with a hybrid context andtensor parallelism. Moreover, it seamlessly integrates with Hugging FaceTransformers.</description><author>Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, Song Han</author><pubDate>Fri, 01 Nov 2024 10:57:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10188v5</guid></item><item><title>Bounds and Sensitivity Analysis of the Causal Effect Under Outcome-Independent MNAR Confounding</title><link>http://arxiv.org/abs/2410.06726v2</link><description>We report assumption-free bounds for any contrast between the probabilitiesof the potential outcome under exposure and non-exposure when the confoundersare missing not at random. We assume that the missingness mechanism isoutcome-independent. We also report a sensitivity analysis method to complementour bounds.</description><author>Jose M. Peña</author><pubDate>Fri, 01 Nov 2024 10:28:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06726v2</guid></item><item><title>Aligning Large Language Models with Human Opinions through Persona Selection and Value--Belief--Norm Reasoning</title><link>http://arxiv.org/abs/2311.08385v4</link><description>Reasoning and predicting human opinions with large language models (LLMs) isessential yet challenging. Current methods employ role-playing with personaebut face two major issues: LLMs are sensitive to even a single irrelevantpersona, skewing predictions by up to 30%, and LLMs fail to reasonstrategically over personae. We propose Chain-of-Opinion (COO), a simplefour-step solution modeling which and how to reason with personae, inspired bythe Value--Belief--Norm (VBN) theory. COO differentiates between explicitpersonae (demographics and ideology) and implicit personae (historicalopinions), involves: (1) filtering irrelevant attributes from explicitpersonae, (2) ranking implicit personae into a preferential list for selectingtop-k, (3) applying novel VBN reasoning to extract user environmental andpersonal value, belief, and norm variables for accurate and reliablepredictions, and (4) iterating VBN reasoning with progressively larger lists ofimplicit personae to handle potential persona insufficiency. COO efficientlyachieves new state-of-the-art opinion prediction via prompting with only 5inference calls, improving prior techniques by up to 4%. Notably, fine-tuningLMs with COO data results in significantly better opinion-aligned models, by upto 23%.</description><author>Do Xuan Long, Kenji Kawaguchi, Min-Yen Kan, Nancy F. Chen</author><pubDate>Fri, 01 Nov 2024 10:28:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08385v4</guid></item><item><title>Unity by Diversity: Improved Representation Learning in Multimodal VAEs</title><link>http://arxiv.org/abs/2403.05300v4</link><description>Variational Autoencoders for multimodal data hold promise for many tasks indata analysis, such as representation learning, conditional generation, andimputation. Current architectures either share the encoder output, decoderinput, or both across modalities to learn a shared representation. Sucharchitectures impose hard constraints on the model. In this work, we show thata better latent representation can be obtained by replacing these hardconstraints with a soft constraint. We propose a new mixture-of-experts prior,softly guiding each modality's latent representation towards a shared aggregateposterior. This approach results in a superior latent representation and allowseach encoding to preserve information better from its uncompressed originalfeatures. In extensive experiments on multiple benchmark datasets and twochallenging real-world datasets, we show improved learned latentrepresentations and imputation of missing data modalities compared to existingmethods.</description><author>Thomas M. Sutter, Yang Meng, Andrea Agostini, Daphné Chopard, Norbert Fortin, Julia E. Vogt, Bahbak Shahbaba, Stephan Mandt</author><pubDate>Fri, 01 Nov 2024 10:19:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05300v4</guid></item><item><title>Efficiency for Free: Ideal Data Are Transportable Representations</title><link>http://arxiv.org/abs/2405.14669v2</link><description>Data, the seminal opportunity and challenge in modern machine learning,currently constrains the scalability of representation learning and impedes thepace of model evolution. In this work, we investigate the efficiency propertiesof data from both optimization and generalization perspectives. Our theoreticaland empirical analysis reveals an unexpected finding: for a given task,utilizing a publicly available, task- and architecture-agnostic model (referredto as the `prior model' in this paper) can effectively produce efficient data.Building on this insight, we propose the Representation Learning Accelerator(\algopt), which promotes the formation and utilization of efficient data,thereby accelerating representation learning. Utilizing a ResNet-18 pre-trainedon CIFAR-10 as a prior model to inform ResNet-50 training on ImageNet-1Kreduces computational costs by 50% while maintaining the same accuracy as themodel trained with the original BYOL, which requires 100% cost. Our code isavailable at: \url{https://github.com/LINs-lab/ReLA}.</description><author>Peng Sun, Yi Jiang, Tao Lin</author><pubDate>Fri, 01 Nov 2024 09:56:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14669v2</guid></item><item><title>Du-IN: Discrete units-guided mask modeling for decoding speech from Intracranial Neural signals</title><link>http://arxiv.org/abs/2405.11459v3</link><description>Invasive brain-computer interfaces with Electrocorticography (ECoG) haveshown promise for high-performance speech decoding in medical applications, butless damaging methods like intracranial stereo-electroencephalography (sEEG)remain underexplored. With rapid advances in representation learning,leveraging abundant recordings to enhance speech decoding is increasinglyattractive. However, popular methods often pre-train temporal models based onbrain-level tokens, overlooking that brain activities in different regions arehighly desynchronized during tasks. Alternatively, they pre-trainspatial-temporal models based on channel-level tokens but fail to evaluate themon challenging tasks like speech decoding, which requires intricate processingin specific language-related areas. To address this issue, we collected awell-annotated Chinese word-reading sEEG dataset targeting language-relatedbrain networks from 12 subjects. Using this benchmark, we developed the Du-INmodel, which extracts contextual embeddings based on region-level tokensthrough discrete codex-guided mask modeling. Our model achievesstate-of-the-art performance on the 61-word classification task, surpassing allbaselines. Model comparisons and ablation studies reveal that our designchoices, including (i) temporal modeling based on region-level tokens byutilizing 1D depthwise convolution to fuse channels in the ventral sensorimotorcortex (vSMC) and superior temporal gyrus (STG) and (ii) self-supervisionthrough discrete codex-guided mask modeling, significantly contribute to thisperformance. Overall, our approach -- inspired by neuroscience findings andcapitalizing on region-level representations from specific brain regions -- issuitable for invasive brain modeling and represents a promising neuro-inspiredAI approach in brain-computer interfaces.</description><author>Hui Zheng, Hai-Teng Wang, Wei-Bang Jiang, Zhong-Tao Chen, Li He, Pei-Yang Lin, Peng-Hu Wei, Guo-Guang Zhao, Yun-Zhe Liu</author><pubDate>Fri, 01 Nov 2024 09:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11459v3</guid></item><item><title>Improved Generation of Adversarial Examples Against Safety-aligned LLMs</title><link>http://arxiv.org/abs/2405.20778v2</link><description>Adversarial prompts generated using gradient-based methods exhibitoutstanding performance in performing automatic jailbreak attacks againstsafety-aligned LLMs. Nevertheless, due to the discrete nature of texts, theinput gradient of LLMs struggles to precisely reflect the magnitude of losschange that results from token replacements in the prompt, leading to limitedattack success rates against safety-aligned LLMs, even in the white-boxsetting. In this paper, we explore a new perspective on this problem,suggesting that it can be alleviated by leveraging innovations inspired intransfer-based attacks that were originally proposed for attacking black-boximage classification models. For the first time, we appropriate the ideologiesof effective methods among these transfer-based attacks, i.e., Skip GradientMethod and Intermediate Level Attack, into gradient-based adversarial promptgeneration and achieve significant performance gains without introducingobvious computational cost. Meanwhile, by discussing mechanisms behind thegains, new insights are drawn, and proper combinations of these methods arealso developed. Our empirical results show that 87% of the query-specificadversarial suffixes generated by the developed combination can induceLlama-2-7B-Chat to produce the output that exactly matches the target string onAdvBench. This match rate is 33% higher than that of a very strong baselineknown as GCG, demonstrating advanced discrete optimization for adversarialprompt generation against LLMs. In addition, without introducing obvious cost,the combination achieves &gt;30% absolute increase in attack success ratescompared with GCG when generating both query-specific (38% -&gt; 68%) anduniversal adversarial prompts (26.68% -&gt; 60.32%) for attacking theLlama-2-7B-Chat model on AdvBench. Code at:https://github.com/qizhangli/Gradient-based-Jailbreak-Attacks.</description><author>Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen</author><pubDate>Fri, 01 Nov 2024 09:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20778v2</guid></item><item><title>DASH: Warm-Starting Neural Network Training in Stationary Settings without Loss of Plasticity</title><link>http://arxiv.org/abs/2410.23495v2</link><description>Warm-starting neural network training by initializing networks withpreviously learned weights is appealing, as practical neural networks are oftendeployed under a continuous influx of new data. However, it often leads to lossof plasticity, where the network loses its ability to learn new information,resulting in worse generalization than training from scratch. This occurs evenunder stationary data distributions, and its underlying mechanism is poorlyunderstood. We develop a framework emulating real-world neural network trainingand identify noise memorization as the primary cause of plasticity loss whenwarm-starting on stationary data. Motivated by this, we propose Direction-AwareSHrinking (DASH), a method aiming to mitigate plasticity loss by selectivelyforgetting memorized noise while preserving learned features. We validate ourapproach on vision tasks, demonstrating improvements in test accuracy andtraining efficiency.</description><author>Baekrok Shin, Junsoo Oh, Hanseul Cho, Chulhee Yun</author><pubDate>Fri, 01 Nov 2024 09:49:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23495v2</guid></item><item><title>TEAM: Topological Evolution-aware Framework for Traffic Forecasting--Extended Version</title><link>http://arxiv.org/abs/2410.19192v2</link><description>Due to the global trend towards urbanization, people increasingly move to andlive in cities that then continue to grow. Traffic forecasting plays animportant role in the intelligent transportation systems of cities as well asin spatio-temporal data mining. State-of-the-art forecasting is achieved bydeep-learning approaches due to their ability to contend with complexspatio-temporal dynamics. However, existing methods assume the input isfixed-topology road networks and static traffic time series. These assumptionsfail to align with urbanization, where time series are collected continuouslyand road networks evolve over time. In such settings, deep-learning modelsrequire frequent re-initialization and re-training, imposing high computationalcosts. To enable much more efficient training without jeopardizing modelaccuracy, we propose the Topological Evolution-aware Framework (TEAM) fortraffic forecasting that incorporates convolution and attention. Thiscombination of mechanisms enables better adaptation to newly collected timeseries, while being able to maintain learned knowledge from old time series.TEAM features a continual learning module based on the Wasserstein metric thatacts as a buffer that can identify the most stable and the most changingnetwork nodes. Then, only data related to stable nodes is employed forre-training when consolidating a model. Further, only data of new nodes andtheir adjacent nodes as well as data pertaining to changing nodes are used tore-train the model. Empirical studies with two real-world traffic datasetsoffer evidence that TEAM is capable of much lower re-training costs thanexisting methods are, without jeopardizing forecasting accuracy.</description><author>Duc Kieu, Tung Kieu, Peng Han, Bin Yang, Christian S. Jensen, Bac Le</author><pubDate>Fri, 01 Nov 2024 09:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19192v2</guid></item><item><title>Dynamical similarity analysis uniquely captures how computations develop in RNNs</title><link>http://arxiv.org/abs/2410.24070v2</link><description>Methods for analyzing representations in neural systems are increasinglypopular tools in neuroscience and mechanistic interpretability. Measurescomparing neural activations across conditions, architectures, and species givescalable ways to understand information transformation within different neuralnetworks. However, recent findings show that some metrics respond to spurioussignals, leading to misleading results. Establishing benchmark test cases isthus essential for identifying the most reliable metric and potentialimprovements. We propose that compositional learning in recurrent neuralnetworks (RNNs) can provide a test case for dynamical representation alignmentmetrics. Implementing this case allows us to evaluate if metrics can identifyrepresentations that develop throughout learning and determine ifrepresentations identified by metrics reflect the network's actualcomputations. Building both attractor and RNN based test cases, we show thatthe recently proposed Dynamical Similarity Analysis (DSA) is more noise robustand reliably identifies behaviorally relevant representations compared to priormetrics (Procrustes, CKA). We also demonstrate how such test cases can extendbeyond metric evaluation to study new architectures. Specifically, testing DSAin modern (Mamba) state space models suggests that these models, unlike RNNs,may not require changes in recurrent dynamics due to their expressive hiddenstates. Overall, we develop test cases that showcase how DSA's enhanced abilityto detect dynamical motifs makes it highly effective for identifying ongoingcomputations in RNNs and revealing how networks learn tasks.</description><author>Quentin Guilhot, Michał Wójcik, Jascha Achterberg, Rui Ponte Costa</author><pubDate>Fri, 01 Nov 2024 09:41:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24070v2</guid></item><item><title>A Systematic Survey on Large Language Models for Algorithm Design</title><link>http://arxiv.org/abs/2410.14716v3</link><description>Algorithm Design (AD) is crucial for effective problem-solving across variousdomains. The advent of Large Language Models (LLMs) has notably enhanced theautomation and innovation within this field, offering new perspectives andpromising solutions. Over the past three years, the integration of LLMs into AD(LLM4AD) has seen substantial progress, with applications spanningoptimization, machine learning, mathematical reasoning, and scientificdiscovery. Given the rapid advancements and expanding scope of this field, asystematic review is both timely and necessary. This paper provides asystematic review of LLM4AD. First, we offer an overview and summary ofexisting studies. Then, we introduce a taxonomy and review the literatureacross four dimensions: the roles of LLMs, search methods, prompt methods, andapplication domains with a discussion of potential and achievements of LLMs inAD. Finally, we identify current challenges and highlight several promisingdirections for future research.</description><author>Fei Liu, Yiming Yao, Ping Guo, Zhiyuan Yang, Zhe Zhao, Xi Lin, Xialiang Tong, Mingxuan Yuan, Zhichao Lu, Zhenkun Wang, Qingfu Zhang</author><pubDate>Fri, 01 Nov 2024 09:38:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14716v3</guid></item><item><title>Long-Range Feedback Spiking Network Captures Dynamic and Static Representations of the Visual Cortex under Movie Stimuli</title><link>http://arxiv.org/abs/2306.01354v2</link><description>Deep neural networks (DNNs) are widely used models for investigatingbiological visual representations. However, existing DNNs are mostly designedto analyze neural responses to static images, relying on feedforward structuresand lacking physiological neuronal mechanisms. There is limited insight intohow the visual cortex represents natural movie stimuli that containcontext-rich information. To address these problems, this work proposes thelong-range feedback spiking network (LoRaFB-SNet), which mimics top-downconnections between cortical regions and incorporates spike informationprocessing mechanisms inherent to biological neurons. Taking into account thetemporal dependence of representations under movie stimuli, we presentTime-Series Representational Similarity Analysis (TSRSA) to measure thesimilarity between model representations and visual cortical representations ofmice. LoRaFB-SNet exhibits the highest level of representational similarity,outperforming other well-known and leading alternatives across variousexperimental paradigms, especially when representing long movie stimuli. Wefurther conduct experiments to quantify how temporal structures (dynamicinformation) and static textures (static information) of the movie stimuliinfluence representational similarity, suggesting that our model benefits fromlong-range feedback to encode context-dependent representations just like thebrain. Altogether, LoRaFB-SNet is highly competent in capturing both dynamicand static representations of the mouse visual cortex and contributes to theunderstanding of movie processing mechanisms of the visual system. Our codesare available at https://github.com/Grasshlw/SNN-Neural-Similarity-Movie.</description><author>Liwei Huang, Zhengyu Ma, Liutao Yu, Huihui Zhou, Yonghong Tian</author><pubDate>Fri, 01 Nov 2024 09:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01354v2</guid></item><item><title>Conditional GAN for Enhancing Diffusion Models in Efficient and Authentic Global Gesture Generation from Audios</title><link>http://arxiv.org/abs/2410.20359v2</link><description>Audio-driven simultaneous gesture generation is vital for human-computercommunication, AI games, and film production. While previous research has shownpromise, there are still limitations. Methods based on VAEs are accompanied byissues of local jitter and global instability, whereas methods based ondiffusion models are hampered by low generation efficiency. This is because thedenoising process of DDPM in the latter relies on the assumption that the noiseadded at each step is sampled from a unimodal distribution, and the noisevalues are small. DDIM borrows the idea from the Euler method for solvingdifferential equations, disrupts the Markov chain process, and increases thenoise step size to reduce the number of denoising steps, thereby acceleratinggeneration. However, simply increasing the step size during the step-by-stepdenoising process causes the results to gradually deviate from the originaldata distribution, leading to a significant drop in the quality of thegenerated actions and the emergence of unnatural artifacts. In this paper, webreak the assumptions of DDPM and achieves breakthrough progress in denoisingspeed and fidelity. Specifically, we introduce a conditional GAN to captureaudio control signals and implicitly match the multimodal denoisingdistribution between the diffusion and denoising steps within the same samplingstep, aiming to sample larger noise values and apply fewer denoising steps forhigh-speed generation.</description><author>Yongkang Cheng, Mingjiang Liang, Shaoli Huang, Gaoge Han, Jifeng Ning, Wei Liu</author><pubDate>Fri, 01 Nov 2024 09:33:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.20359v2</guid></item><item><title>RopeTP: Global Human Motion Recovery via Integrating Robust Pose Estimation with Diffusion Trajectory Prior</title><link>http://arxiv.org/abs/2410.20358v2</link><description>We present RopeTP, a novel framework that combines Robust pose estimationwith a diffusion Trajectory Prior to reconstruct global human motion fromvideos. At the heart of RopeTP is a hierarchical attention mechanism thatsignificantly improves context awareness, which is essential for accuratelyinferring the posture of occluded body parts. This is achieved by exploitingthe relationships with visible anatomical structures, enhancing the accuracy oflocal pose estimations. The improved robustness of these local estimationsallows for the reconstruction of precise and stable global trajectories.Additionally, RopeTP incorporates a diffusion trajectory model that predictsrealistic human motion from local pose sequences. This model ensures that thegenerated trajectories are not only consistent with observed local actions butalso unfold naturally over time, thereby improving the realism and stability of3D human motion reconstruction. Extensive experimental validation shows thatRopeTP surpasses current methods on two benchmark datasets, particularlyexcelling in scenarios with occlusions. It also outperforms methods that relyon SLAM for initial camera estimates and extensive optimization, deliveringmore accurate and realistic trajectories.</description><author>Mingjiang Liang, Yongkang Cheng, Hualin Liang, Shaoli Huang, Wei Liu</author><pubDate>Fri, 01 Nov 2024 09:20:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.20358v2</guid></item><item><title>Continuous-time q-learning for mean-field control problems</title><link>http://arxiv.org/abs/2306.16208v4</link><description>This paper studies the q-learning, recently coined as the continuous timecounterpart of Q-learning by Jia and Zhou (2023), for continuous timeMckean-Vlasov control problems in the setting of entropy-regularizedreinforcement learning. In contrast to the single agent's control problem inJia and Zhou (2023), the mean-field interaction of agents renders thedefinition of the q-function more subtle, for which we reveal that two distinctq-functions naturally arise: (i) the integrated q-function (denoted by $q$) asthe first-order approximation of the integrated Q-function introduced in Gu,Guo, Wei and Xu (2023), which can be learnt by a weak martingale conditioninvolving test policies; and (ii) the essential q-function (denoted by $q_e$)that is employed in the policy improvement iterations. We show that twoq-functions are related via an integral representation under all test policies.Based on the weak martingale condition and our proposed searching method oftest policies, some model-free learning algorithms are devised. In twoexamples, one in LQ control framework and one beyond LQ control framework, wecan obtain the exact parameterization of the optimal value function andq-functions and illustrate our algorithms with simulation experiments.</description><author>Xiaoli Wei, Xiang Yu</author><pubDate>Fri, 01 Nov 2024 09:07:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16208v4</guid></item><item><title>Adversarial Purification and Fine-tuning for Robust UDC Image Restoration</title><link>http://arxiv.org/abs/2402.13629v3</link><description>This study delves into the enhancement of Under-Display Camera (UDC) imagerestoration models, focusing on their robustness against adversarial attacks.Despite its innovative approach to seamless display integration, UDC technologyfaces unique image degradation challenges exacerbated by the susceptibility toadversarial perturbations. Our research initially conducts an in-depthrobustness evaluation of deep-learning-based UDC image restoration models byemploying several white-box and black-box attacking methods. This evaluation ispivotal in understanding the vulnerabilities of current UDC image restorationtechniques. Following the assessment, we introduce a defense frameworkintegrating adversarial purification with subsequent fine-tuning processes.First, our approach employs diffusion-based adversarial purification,effectively neutralizing adversarial perturbations. Then, we apply thefine-tuning methodologies to refine the image restoration models further,ensuring that the quality and fidelity of the restored images are maintained.The effectiveness of our proposed approach is validated through extensiveexperiments, showing marked improvements in resilience against typicaladversarial attacks.</description><author>Zhenbo Song, Zhenyuan Zhang, Kaihao Zhang, Zhaoxin Fan, Jianfeng Lu</author><pubDate>Fri, 01 Nov 2024 08:56:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13629v3</guid></item><item><title>Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts</title><link>http://arxiv.org/abs/2404.05019v2</link><description>Expert parallelism has been introduced as a strategy to distribute thecomputational workload of sparsely-gated mixture-of-experts (MoE) models acrossmultiple computing devices, facilitating the execution of these increasinglylarge-scale models. However, the All-to-All communication intrinsic to expertparallelism constitutes a significant overhead, diminishing the MoE models'efficiency. Current optimization approaches offer some relief, yet they areconstrained by the sequential interdependence of communication and computationoperations. To address this limitation, we present a novel shortcut-connectedMoE (ScMoE) architecture with an overlapping parallel strategy, whicheffectively decouples communication from its conventional sequence, allowingfor a substantial overlap of 70% to 100% with computation. When compared withthe prevalent top-2 MoE architecture, ScMoE demonstrates training speedimprovements of 30% and 11%, and inference improvements of 40% and 15%, in ourdistributed environments with PCIe and NVLink hardware, respectively, wherecommunication constitutes 60% and 15% of the total MoE time consumption.Building on the ScMoE architecture, we further implement an expert offloadingstrategy to facilitate memory-limited inference, optimizing latency through theoverlap of expert migration. Additionally, extensive experiments andtheoretical analyses indicate that ScMoE not only achieves comparable but insome instances surpasses the model quality of existing approaches.</description><author>Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi Huang</author><pubDate>Fri, 01 Nov 2024 08:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05019v2</guid></item><item><title>Block Transformer: Global-to-Local Language Modeling for Fast Inference</title><link>http://arxiv.org/abs/2406.02657v2</link><description>We introduce the Block Transformer which adopts hierarchical global-to-localmodeling to autoregressive transformers to mitigate the inference bottlenecksassociated with self-attention. Self-attention requires the key-value (KV)cache of all previous sequences to be retrieved from memory at every decodingstep to retrieve context information, leading to two primary bottlenecks duringbatch inference. First, there is a significant delay in obtaining the firsttoken, as the information of the entire prompt must first be processed toprefill the KV cache. Second, computation of subsequent tokens is bottleneckedby the high memory I/O demand of fetching the entire KV cache, which growslinearly with sequence length, incurring quadratic memory reads overall. Wedesign the Block Transformer to strategically mitigate these costs, byincorporating coarsity and locality into an integrated global-to-localarchitecture. At the lower layers, we aggregate tokens into fixed size blocksto apply attention across the entire sequence at coarse-grained detail, tocapture the global context while minimizing KV cache overhead. At upper layers,we apply attention within each block to decode individual tokens, to modelfine-grained details with a lightweight local KV cache. We pretrain vanilla andBlock Transformers from scratch and demonstrate that Block Transformers reach10--20x inference throughput compared to vanilla transformers with equivalentperplexity and zero-shot task performance. Code is available athttps://github.com/itsnamgyu/block-transformer.</description><author>Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal Schuster, Adam Fisch, James Thorne, Se-Young Yun</author><pubDate>Fri, 01 Nov 2024 08:52:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02657v2</guid></item><item><title>Towards Robust Multimodal Sentiment Analysis with Incomplete Data</title><link>http://arxiv.org/abs/2409.20012v2</link><description>The field of Multimodal Sentiment Analysis (MSA) has recently witnessed anemerging direction seeking to tackle the issue of data incompleteness.Recognizing that the language modality typically contains dense sentimentinformation, we consider it as the dominant modality and present an innovativeLanguage-dominated Noise-resistant Learning Network (LNLN) to achieve robustMSA. The proposed LNLN features a dominant modality correction (DMC) module anddominant modality based multimodal learning (DMML) module, which enhances themodel's robustness across various noise scenarios by ensuring the quality ofdominant modality representations. Aside from the methodical design, we performcomprehensive experiments under random data missing scenarios, utilizingdiverse and meaningful settings on several popular datasets (\textit{e.g.,}MOSI, MOSEI, and SIMS), providing additional uniformity, transparency, andfairness compared to existing evaluations in the literature. Empirically, LNLNconsistently outperforms existing baselines, demonstrating superior performanceacross these challenging and extensive evaluation metrics.</description><author>Haoyu Zhang, Wenbin Wang, Tianshu Yu</author><pubDate>Fri, 01 Nov 2024 08:40:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20012v2</guid></item><item><title>Posture-Informed Muscular Force Learning for Robust Hand Pressure Estimation</title><link>http://arxiv.org/abs/2410.23629v2</link><description>We present PiMForce, a novel framework that enhances hand pressure estimationby leveraging 3D hand posture information to augment forearm surfaceelectromyography (sEMG) signals. Our approach utilizes detailed spatialinformation from 3D hand poses in conjunction with dynamic muscle activity fromsEMG to enable accurate and robust whole-hand pressure measurements underdiverse hand-object interactions. We also developed a multimodal datacollection system that combines a pressure glove, an sEMG armband, and amarkerless finger-tracking module. We created a comprehensive dataset from 21participants, capturing synchronized data of hand posture, sEMG signals, andexerted hand pressure across various hand postures and hand-object interactionscenarios using our collection system. Our framework enables precise handpressure estimation in complex and natural interaction scenarios. Our approachsubstantially mitigates the limitations of traditional sEMG-based orvision-based methods by integrating 3D hand posture information with sEMGsignals. Video demos, data, and code are available online.</description><author>Kyungjin Seo, Junghoon Seo, Hanseok Jeong, Sangpil Kim, Sang Ho Yoon</author><pubDate>Fri, 01 Nov 2024 08:38:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23629v2</guid></item><item><title>CrysToGraph: A Comprehensive Predictive Model for Crystal Materials Properties and the Benchmark</title><link>http://arxiv.org/abs/2407.16131v2</link><description>The ionic bonding across the lattice and ordered microscopic structures endowcrystals with unique symmetry and determine their macroscopic properties.Unconventional crystals, in particular, exhibit non-traditional latticestructures or possess exotic physical properties, making them intriguingsubjects for investigation. Therefore, to accurately predict the physical andchemical properties of crystals, it is crucial to consider long-range orders.While GNN excels at capturing the local environment of atoms in crystals, theyoften face challenges in effectively capturing longer-ranged interactions dueto their limited depth. In this paper, we propose CrysToGraph($\textbf{Crys}$tals with $\textbf{T}$ransformers $\textbf{o}$n$\textbf{Graph}$s), a novel transformer-based geometric graph network designedspecifically for unconventional crystalline systems, and UnconvBench, acomprehensive benchmark to evaluate models' predictive performance onunconventional crystal materials such as defected crystals, low-dimensioncrystals and MOF. CrysToGraph effectively captures short-range interactionswith transformer-based graph convolution blocks as well as long-rangeinteractions with graph-wise transformer blocks. CrysToGraph proofs itseffectiveness in modelling unconventional crystal materials in multiple tasks,and moreover, it outperforms most existing methods, achieving newstate-of-the-art results on the benchmarks of both unconventional crystals andtraditional crystals.</description><author>Hongyi Wang, Ji Sun, Jinzhe Liang, Li Zhai, Zitian Tang, Zijian Li, Wei Zhai, Xusheng Wang, Weihao Gao, Sheng Gong</author><pubDate>Fri, 01 Nov 2024 08:25:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16131v2</guid></item><item><title>Graph Convolutions Enrich the Self-Attention in Transformers!</title><link>http://arxiv.org/abs/2312.04234v5</link><description>Transformers, renowned for their self-attention mechanism, have achievedstate-of-the-art performance across various tasks in natural languageprocessing, computer vision, time-series modeling, etc. However, one of thechallenges with deep Transformer models is the oversmoothing problem, whererepresentations across layers converge to indistinguishable values, leading tosignificant performance degradation. We interpret the original self-attentionas a simple graph filter and redesign it from a graph signal processing (GSP)perspective. We propose a graph-filter-based self-attention (GFSA) to learn ageneral yet effective one, whose complexity, however, is slightly larger thanthat of the original self-attention mechanism. We demonstrate that GFSAimproves the performance of Transformers in various fields, including computervision, natural language processing, graph-level tasks, speech recognition, andcode classification.</description><author>Jeongwhan Choi, Hyowon Wi, Jayoung Kim, Yehjin Shin, Kookjin Lee, Nathaniel Trask, Noseong Park</author><pubDate>Fri, 01 Nov 2024 08:16:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04234v5</guid></item><item><title>MAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings</title><link>http://arxiv.org/abs/2309.08648v4</link><description>In recent years, predicting mobile app usage has become increasinglyimportant for areas like app recommendation, user behaviour analysis, andmobile resource management. Existing models, however, struggle with theheterogeneous nature of contextual data and the user cold start problem. Thisstudy introduces a novel prediction model, Mobile App Prediction LeveragingLarge Language Model Embeddings (MAPLE), which employs Large Language Models(LLMs) and installed app similarity to overcome these challenges. MAPLEutilises the power of LLMs to process contextual data and discern intricaterelationships within it effectively. Additionally, we explore the use ofinstalled app similarity to address the cold start problem, facilitating themodelling of user preferences and habits, even for new users with limitedhistorical data. In essence, our research presents MAPLE as a novel, potent,and practical approach to app usage prediction, making significant strides inresolving issues faced by existing models. MAPLE stands out as a comprehensiveand effective solution, setting a new benchmark for more precise andpersonalised app usage predictions. In tests on two real-world datasets, MAPLEsurpasses contemporary models in both standard and cold start scenarios. Theseoutcomes validate MAPLE's capacity for precise app usage predictions and itsresilience against the cold start problem. This enhanced performance stems fromthe model's proficiency in capturing complex temporal patterns and leveragingcontextual information. As a result, MAPLE can potentially improve personalisedmobile app usage predictions and user experiences markedly.</description><author>Yonchanok Khaokaew, Hao Xue, Flora D. Salim</author><pubDate>Fri, 01 Nov 2024 08:06:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08648v4</guid></item><item><title>Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using Discrete State Space Diffusion Model</title><link>http://arxiv.org/abs/2410.23994v2</link><description>Sequential recommendation (SR) aims to predict items that users may beinterested in based on their historical behavior sequences. We revisit SR froma novel information-theoretic perspective and find that conventional sequentialmodeling methods fail to adequately capture the randomness and unpredictabilityof user behavior. Inspired by fuzzy information processing theory, this paperintroduces the DDSR model, which uses fuzzy sets of interaction sequences toovercome the limitations and better capture the evolution of users' realinterests. Formally based on diffusion transition processes in discrete statespaces, which is unlike common diffusion models such as DDPM that operate incontinuous domains. It is better suited for discrete data, using structuredtransitions instead of arbitrary noise introduction to avoid information loss.Additionally, to address the inefficiency of matrix transformations due to thevast discrete space, we use semantic labels derived from quantization or RQ-VAEto replace item IDs, enhancing efficiency and improving cold start issues.Testing on three public benchmark datasets shows that DDSR outperforms existingstate-of-the-art methods in various settings, demonstrating its potential andeffectiveness in handling SR tasks.</description><author>Wenjia Xie, Hao Wang, Luankang Zhang, Rui Zhou, Defu Lian, Enhong Chen</author><pubDate>Fri, 01 Nov 2024 07:55:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23994v2</guid></item><item><title>$FastDoc$: Domain-Specific Fast Continual Pre-training Technique using Document-Level Metadata and Taxonomy</title><link>http://arxiv.org/abs/2306.06190v3</link><description>In this paper, we propose $FastDoc$ (Fast Continual Pre-training Techniqueusing Document Level Metadata and Taxonomy), a novel, compute-efficientframework that utilizes Document metadata and Domain-Specific Taxonomy assupervision signals to continually pre-train transformer encoder on adomain-specific corpus. The main innovation is that during domain-specificpretraining, an open-domain encoder is continually pre-trained usingsentence-level embeddings as inputs (to accommodate long documents), however,fine-tuning is done with token-level embeddings as inputs to this encoder. Weperform such domain-specific pre-training on three different domains namelycustomer support, scientific, and legal domains, and compare performance on 6different downstream tasks and 9 different datasets. The novel use ofdocument-level supervision along with sentence-level embedding input forpre-training reduces pre-training compute by around $1,000$, $4,500$, and $500$times compared to MLM and/or NSP in Customer Support, Scientific, and LegalDomains, respectively. The reduced training time does not lead to adeterioration in performance. In fact we show that $FastDoc$ either outperformsor performs on par with several competitive transformer-based baselines interms of character-level F1 scores and other automated metrics in the CustomerSupport, Scientific, and Legal Domains. Moreover, reduced training aids inmitigating the risk of catastrophic forgetting. Thus, unlike baselines,$FastDoc$ shows a negligible drop in performance on open domain.</description><author>Abhilash Nandy, Manav Nitin Kapadnis, Sohan Patnaik, Yash Parag Butala, Pawan Goyal, Niloy Ganguly</author><pubDate>Fri, 01 Nov 2024 07:53:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06190v3</guid></item><item><title>Adversarial Representation Engineering: A General Model Editing Framework for Large Language Models</title><link>http://arxiv.org/abs/2404.13752v3</link><description>Since the rapid development of Large Language Models (LLMs) has achievedremarkable success, understanding and rectifying their internal complexmechanisms has become an urgent issue. Recent research has attempted tointerpret their behaviors through the lens of inner representation. However,developing practical and efficient methods for applying these representationsfor general and flexible model editing remains challenging. In this work, weexplore how to leverage insights from representation engineering to guide theediting of LLMs by deploying a representation sensor as an editing oracle. Wefirst identify the importance of a robust and reliable sensor during editing,then propose an Adversarial Representation Engineering (ARE) framework toprovide a unified and interpretable approach for conceptual model editingwithout compromising baseline performance. Experiments on multiple tasksdemonstrate the effectiveness of ARE in various model editing scenarios. Ourcode and data are available athttps://github.com/Zhang-Yihao/Adversarial-Representation-Engineering.</description><author>Yihao Zhang, Zeming Wei, Jun Sun, Meng Sun</author><pubDate>Fri, 01 Nov 2024 07:51:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13752v3</guid></item><item><title>LADDER: Language Driven Slice Discovery and Error Rectification</title><link>http://arxiv.org/abs/2408.07832v4</link><description>Error slice discovery associates structured patterns with model errors.Existing methods discover error slices by clustering the error-prone sampleswith similar patterns or assigning discrete attributes to each sample forpost-hoc analysis. While these methods aim for interpretability and easiermitigation through reweighting or rebalancing, they may not capture the fullcomplexity of error patterns due to incomplete or missing attributes. Contraryto the existing approach, this paper utilizes the reasoning capabilities of theLarge Language Model (LLM) to analyze complex error patterns and generatetestable hypotheses. This paper proposes LADDER: Language Driven sliceDiscovery and Error Rectification. It first projects the model's representationinto a language-aligned feature space (eg CLIP) to preserve semantics in theoriginal model feature space. This ensures the accurate retrieval of sentencesthat highlight the model's errors. Next, the LLM utilizes the sentences andgenerates hypotheses to discover error slices. Finally, we mitigate the errorby fine-tuning the classification head by creating a group-balanced datasetusing the hypotheses. Our entire method does not require any attributeannotation, either explicitly or through external tagging models. We validateour method with \textbf{five} image classification datasets. The code isavailable (https://github.com/batmanlab/Ladder).</description><author>Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Shyam Visweswaran, Kayhan Batmanghelich</author><pubDate>Fri, 01 Nov 2024 07:41:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07832v4</guid></item><item><title>Human Action Recognition (HAR) Using Skeleton-based Spatial Temporal Relative Transformer Network: ST-RTR</title><link>http://arxiv.org/abs/2410.23806v2</link><description>Human Action Recognition (HAR) is an interesting research area inhuman-computer interaction used to monitor the activities of elderly anddisabled individuals affected by physical and mental health. In the recent era,skeleton-based HAR has received much attention because skeleton data has shownthat it can handle changes in striking, body size, camera views, and complexbackgrounds. One key characteristic of ST-GCN is automatically learning spatialand temporal patterns from skeleton sequences. It has some limitations, as thismethod only works for short-range correlation due to its limited receptivefield. Consequently, understanding human action requires long-rangeinterconnection. To address this issue, we developed a spatial-temporalrelative transformer ST-RTR model. The ST-RTR includes joint and relay nodes,which allow efficient communication and data transmission within the network.These nodes help to break the inherent spatial and temporal skeletontopologies, which enables the model to understand long-range human actionbetter. Furthermore, we combine ST-RTR with a fusion model for furtherperformance improvements. To assess the performance of the ST-RTR method, weconducted experiments on three skeleton-based HAR benchmarks: NTU RGB+D 60, NTURGB+D 120, and UAV-Human. It boosted CS and CV by 2.11 % and 1.45% on NTU RGB+D60, 1.25% and 1.05% on NTU RGB+D 120. On UAV-Human datasets, accuracy improvedby 2.54%. The experimental outcomes explain that the proposed ST-RTR modelsignificantly improves action recognition associated with the standard ST-GCNmethod.</description><author>Faisal Mehmood, Enqing Chen, Touqeer Abbas, Samah M. Alzanin</author><pubDate>Fri, 01 Nov 2024 07:25:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23806v2</guid></item><item><title>Offline Reinforcement Learning with OOD State Correction and OOD Action Suppression</title><link>http://arxiv.org/abs/2410.19400v4</link><description>In offline reinforcement learning (RL), addressing the out-of-distribution(OOD) action issue has been a focus, but we argue that there exists an OODstate issue that also impairs performance yet has been underexplored. Such anissue describes the scenario when the agent encounters states out of theoffline dataset during the test phase, leading to uncontrolled behavior andperformance degradation. To this end, we propose SCAS, a simple yet effectiveapproach that unifies OOD state correction and OOD action suppression inoffline RL. Technically, SCAS achieves value-aware OOD state correction,capable of correcting the agent from OOD states to high-value in-distributionstates. Theoretical and empirical results show that SCAS also exhibits theeffect of suppressing OOD actions. On standard offline RL benchmarks, SCASachieves excellent performance without additional hyperparameter tuning.Moreover, benefiting from its OOD state correction feature, SCAS demonstratesenhanced robustness against environmental perturbations.</description><author>Yixiu Mao, Qi Wang, Chen Chen, Yun Qu, Xiangyang Ji</author><pubDate>Fri, 01 Nov 2024 07:20:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19400v4</guid></item><item><title>Are large language models superhuman chemists?</title><link>http://arxiv.org/abs/2404.01475v2</link><description>Large language models (LLMs) have gained widespread interest due to theirability to process human language and perform tasks on which they have not beenexplicitly trained. However, we possess only a limited systematic understanding of the chemicalcapabilities of LLMs, which would be required to improve models and mitigatepotential harm. Here, we introduce "ChemBench," an automated framework forevaluating the chemical knowledge and reasoning abilities of state-of-the-artLLMs against the expertise of chemists. We curated more than 2,700 question-answer pairs, evaluated leading open- andclosed-source LLMs, and found that the best models outperformed the best humanchemists in our study on average. However, the models struggle with some basictasks and provide overconfident predictions. These findings reveal LLMs' impressive chemical capabilities whileemphasizing the need for further research to improve their safety andusefulness. They also suggest adapting chemistry education and show the valueof benchmarking frameworks for evaluating LLMs in specific domains.</description><author>Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Martiño Ríos-García, Benedict Emoekabu, Aswanth Krishnan, Tanya Gupta, Mara Schilling-Wilhelmi, Macjonathan Okereke, Anagha Aneesh, Amir Mohammad Elahi, Mehrdad Asgari, Juliane Eberhardt, Hani M. Elbeheiry, María Victoria Gil, Maximilian Greiner, Caroline T. Holick, Christina Glaubitz, Tim Hoffmann, Abdelrahman Ibrahim, Lea C. Klepsch, Yannik Köster, Fabian Alexander Kreth, Jakob Meyer, Santiago Miret, Jan Matthias Peschel, Michael Ringleb, Nicole Roesner, Johanna Schreiber, Ulrich S. Schubert, Leanne M. Stafast, Dinga Wonanke, Michael Pieler, Philippe Schwaller, Kevin Maik Jablonka</author><pubDate>Fri, 01 Nov 2024 07:05:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01475v2</guid></item></channel></rss>