<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 26 Jun 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Text-Animator: Controllable Visual Text Video Generation</title><link>http://arxiv.org/abs/2406.17777v1</link><description>Video generation is a challenging yet pivotal task in various industries,such as gaming, e-commerce, and advertising. One significant unresolved aspectwithin T2V is the effective visualization of text within generated videos.Despite the progress achieved in Text-to-Video~(T2V) generation, currentmethods still cannot effectively visualize texts in videos directly, as theymainly focus on summarizing semantic scene information, understanding, anddepicting actions. While recent advances in image-level visual text generationshow promise, transitioning these techniques into the video domain facesproblems, notably in preserving textual fidelity and motion coherence. In thispaper, we propose an innovative approach termed Text-Animator for visual textvideo generation. Text-Animator contains a text embedding injection module toprecisely depict the structures of visual text in generated videos. Besides, wedevelop a camera control module and a text refinement module to improve thestability of generated visual text by controlling the camera movement as wellas the motion of visualized text. Quantitative and qualitative experimentalresults demonstrate the superiority of our approach to the accuracy ofgenerated visual text over state-of-the-art video generation methods. Theproject page can be found at https://laulampaul.github.io/text-animator.html.</description><author>Lin Liu, Quande Liu, Shengju Qian, Yuan Zhou, Wengang Zhou, Houqiang Li, Lingxi Xie, Qi Tian</author><pubDate>Tue, 25 Jun 2024 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17777v1</guid></item><item><title>Fast and Uncertainty-Aware SVBRDF Recovery from Multi-View Capture using Frequency Domain Analysis</title><link>http://arxiv.org/abs/2406.17774v1</link><description>Relightable object acquisition is a key challenge in simplifying digitalasset creation. Complete reconstruction of an object typically requirescapturing hundreds to thousands of photographs under controlled illumination,with specialized equipment. The recent progress in differentiable renderingimproved the quality and accessibility of inverse rendering optimization.Nevertheless, under uncontrolled illumination and unstructured viewpoints,there is no guarantee that the observations contain enough information toreconstruct the appearance properties of the captured object. We thus propose to consider the acquisition process from a signal-processingperspective. Given an object's geometry and a lighting environment, we estimatethe properties of the materials on the object's surface in seconds. We do so byleveraging frequency domain analysis, considering the recovery of materialproperties as a deconvolution, enabling fast error estimation. We then quantifythe uncertainty of the estimation, based on the available data, highlightingthe areas for which priors or additional samples would be required for improvedacquisition quality. We compare our approach to previous work andquantitatively evaluate our results, showing similar quality as previous workin a fraction of the time, and providing key information about the certainty ofthe results.</description><author>Ruben Wiersma, Julien Philip, Miloš Hašan, Krishna Mullia, Fujun Luan, Elmar Eisemann, Valentin Deschaintre</author><pubDate>Tue, 25 Jun 2024 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17774v1</guid></item><item><title>CT-Bound: Robust Boundary Detection From Noisy Images Via Hybrid Convolution and Transformer Neural Networks</title><link>http://arxiv.org/abs/2403.16494v2</link><description>We present CT-Bound, a robust and fast boundary detection method for verynoisy images using a hybrid Convolution and Transformer neural network. Theproposed architecture decomposes boundary estimation into two tasks: localdetection and global regularization. During the local detection, the model usesa convolutional architecture to predict the boundary structure of each imagepatch in the form of a pre-defined local boundary representation, thefield-of-junctions (FoJ). Then, it uses a feed-forward transformer architectureto globally refine the boundary structures of each patch to generate an edgemap and a smoothed color map simultaneously. Our quantitative analysis showsthat CT-Bound outperforms the previous best algorithms in edge detection onvery noisy images. It also increases the edge detection accuracy of FoJ-basedmethods while having a 3-time speed improvement. Finally, we demonstrate thatCT-Bound can produce boundary and color maps on real captured images withoutextra fine-tuning and real-time boundary map and color map videos at ten framesper second.</description><author>Wei Xu, Junjie Luo, Qi Guo</author><pubDate>Tue, 25 Jun 2024 18:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16494v2</guid></item><item><title>DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models</title><link>http://arxiv.org/abs/2402.12289v5</link><description>A primary hurdle of autonomous driving in urban environments is understandingcomplex and long-tail scenarios, such as challenging road conditions anddelicate human behaviors. We introduce DriveVLM, an autonomous driving systemleveraging Vision-Language Models (VLMs) for enhanced scene understanding andplanning capabilities. DriveVLM integrates a unique combination of reasoningmodules for scene description, scene analysis, and hierarchical planning.Furthermore, recognizing the limitations of VLMs in spatial reasoning and heavycomputational requirements, we propose DriveVLM-Dual, a hybrid system thatsynergizes the strengths of DriveVLM with the traditional autonomous drivingpipeline. Experiments on both the nuScenes dataset and our SUP-AD datasetdemonstrate the efficacy of DriveVLM and DriveVLM-Dual in handling complex andunpredictable driving conditions. Finally, we deploy the DriveVLM-Dual on aproduction vehicle, verifying it is effective in real-world autonomous drivingenvironments.</description><author>Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, Hang Zhao</author><pubDate>Tue, 25 Jun 2024 18:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12289v5</guid></item><item><title>MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning</title><link>http://arxiv.org/abs/2406.17770v1</link><description>Multi-modal large language models (MLLMs) have made significant strides invarious visual understanding tasks. However, the majority of these models areconstrained to process low-resolution images, which limits their effectivenessin perception tasks that necessitate detailed visual information. In our study,we present MG-LLaVA, an innovative MLLM that enhances the model's visualprocessing capabilities by incorporating a multi-granularity vision flow, whichincludes low-resolution, high-resolution, and object-centric features. Wepropose the integration of an additional high-resolution visual encoder tocapture fine-grained details, which are then fused with base visual featuresthrough a Conv-Gate fusion network. To further refine the model's objectrecognition abilities, we incorporate object-level features derived frombounding boxes identified by offline detectors. Being trained solely onpublicly available multimodal data through instruction tuning, MG-LLaVAdemonstrates exceptional perception skills. We instantiate MG-LLaVA with a widevariety of language encoders, ranging from 3.8B to 34B, to evaluate the model'sperformance comprehensively. Extensive evaluations across multiple benchmarksdemonstrate that MG-LLaVA outperforms existing MLLMs of comparable parametersizes, showcasing its remarkable efficacy. The code will be available athttps://github.com/PhoenixZ810/MG-LLaVA.</description><author>Xiangyu Zhao, Xiangtai Li, Haodong Duan, Haian Huang, Yining Li, Kai Chen, Hua Yang</author><pubDate>Tue, 25 Jun 2024 18:55:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17770v1</guid></item><item><title>EXTRACT: Efficient Policy Learning by Extracting Transferrable Robot Skills from Offline Data</title><link>http://arxiv.org/abs/2406.17768v1</link><description>Most reinforcement learning (RL) methods focus on learning optimal policiesover low-level action spaces. While these methods can perform well in theirtraining environments, they lack the flexibility to transfer to new tasks.Instead, RL agents that can act over useful, temporally extended skills ratherthan low-level actions can learn new tasks more easily. Prior work inskill-based RL either requires expert supervision to define useful skills,which is hard to scale, or learns a skill-space from offline data withheuristics that limit the adaptability of the skills, making them difficult totransfer during downstream RL. Our approach, EXTRACT, instead utilizespre-trained vision language models to extract a discrete set of semanticallymeaningful skills from offline data, each of which is parameterized bycontinuous arguments, without human supervision. This skill parameterizationallows robots to learn new tasks by only needing to learn when to select aspecific skill and how to modify its arguments for the specific task. Wedemonstrate through experiments in sparse-reward, image-based, robotmanipulation environments that EXTRACT can more quickly learn new tasks thanprior works, with major gains in sample efficiency and performance over priorskill-based RL. Website at https://www.jessezhang.net/projects/extract/.</description><author>Jesse Zhang, Minho Heo, Zuxin Liu, Erdem Biyik, Joseph J Lim, Yao Liu, Rasool Fakoor</author><pubDate>Tue, 25 Jun 2024 18:50:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17768v1</guid></item><item><title>Towards Diverse Evaluation of Class Incremental Learning: A Representation Learning Perspective</title><link>http://arxiv.org/abs/2206.08101v3</link><description>Class incremental learning (CIL) algorithms aim to continually learn newobject classes from incrementally arriving data while not forgetting pastlearned classes. The common evaluation protocol for CIL algorithms is tomeasure the average test accuracy across all classes learned so far -- however,we argue that solely focusing on maximizing the test accuracy may notnecessarily lead to developing a CIL algorithm that also continually learns andupdates the representations, which may be transferred to the downstream tasks.To that end, we experimentally analyze neural network models trained by CILalgorithms using various evaluation protocols in representation learning andpropose new analysis methods. Our experiments show that most state-of-the-artalgorithms prioritize high stability and do not significantly change thelearned representation, and sometimes even learn a representation of lowerquality than a naive baseline. However, we observe that these algorithms canstill achieve high test accuracy because they enable a model to learn aclassifier that closely resembles an estimated linear classifier trained forlinear probing. Furthermore, the base model learned in the first task, whichinvolves single-task learning, exhibits varying levels of representationquality across different algorithms, and this variance impacts the finalperformance of CIL algorithms. Therefore, we suggest that therepresentation-level evaluation should be considered as an additional recipefor more diverse evaluation for CIL algorithms.</description><author>Sungmin Cha, Jihwan Kwak, Dongsub Shim, Hyunwoo Kim, Moontae Lee, Honglak Lee, Taesup Moon</author><pubDate>Tue, 25 Jun 2024 18:49:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.08101v3</guid></item><item><title>BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning</title><link>http://arxiv.org/abs/2406.17764v1</link><description>Large language models (LLMs) possess extensive parametric knowledge, but thisknowledge is difficult to update with new information because retraining isvery expensive and infeasible for closed-source models. Knowledge editing (KE)has emerged as a viable solution for updating the knowledge of LLMs withoutcompromising their overall performance. On-the-fly KE methods, inspired byin-context learning (ICL), have shown great promise and allow LLMs to betreated as black boxes. In the past, KE was primarily employed in Englishcontexts, whereas the potential for cross-lingual KE in current English-centricLLMs has not been fully explored. To foster more research in this direction, weintroduce the BMIKE-53 benchmark for evaluating cross-lingual KE on 53 diverselanguages across three KE task types. We also propose a gradient-free KE methodcalled Multilingual In-context Knowledge Editing (MIKE) and evaluate it onBMIKE-53. Our evaluation focuses on cross-lingual knowledge transfer in termsof reliability, generality, locality, and portability, offering valuableinsights and a framework for future research in cross-lingual KE. Our code anddata are publicly accessible via the anonymous repository athttps://anonymous.4open.science/r/MIKE.</description><author>Ercong Nie, Bo Shao, Zifeng Ding, Mingyang Wang, Helmut Schmid, Hinrich Schütze</author><pubDate>Tue, 25 Jun 2024 18:48:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17764v1</guid></item><item><title>DiffusionPDE: Generative PDE-Solving Under Partial Observation</title><link>http://arxiv.org/abs/2406.17763v1</link><description>We introduce a general framework for solving partial differential equations(PDEs) using generative diffusion models. In particular, we focus on thescenarios where we do not have the full knowledge of the scene necessary toapply classical solvers. Most existing forward or inverse PDE approachesperform poorly when the observations on the data or the underlying coefficientsare incomplete, which is a common assumption for real-world measurements. Inthis work, we propose DiffusionPDE that can simultaneously fill in the missinginformation and solve a PDE by modeling the joint distribution of the solutionand coefficient spaces. We show that the learned generative priors lead to aversatile framework for accurately solving a wide range of PDEs under partialobservation, significantly outperforming the state-of-the-art methods for bothforward and inverse directions.</description><author>Jiahe Huang, Guandao Yang, Zichen Wang, Jeong Joon Park</author><pubDate>Tue, 25 Jun 2024 18:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17763v1</guid></item><item><title>Solving Hard Mizar Problems with Instantiation and Strategy Invention</title><link>http://arxiv.org/abs/2406.17762v1</link><description>In this work, we prove over 3000 previously ATP-unproved Mizar/MPTP problemsby using several ATP and AI methods, raising the number of ATP-solved Mizarproblems from 75\% to above 80\%. First, we start to experiment with the cvc5SMT solver which uses several instantiation-based heuristics that differ fromthe superposition-based systems, that were previously applied to Mizar,and addmany new solutions. Then we use automated strategy invention to develop cvc5strategies that largely improve cvc5's performance on the hard problems. Inparticular, the best invented strategy solves over 14\% more problems than thebest previously available cvc5 strategy. We also show that differentclausification methods have a high impact on such instantiation-based methods,again producing many new solutions. In total, the methods solve 3021 (21.3\%)of the 14163 previously unsolved hard Mizar problems. This is a new milestoneover the Mizar large-theory benchmark and a large strengthening of the hammermethods for Mizar.</description><author>Jan Jakubův, Mikoláš Janota, Josef Urban</author><pubDate>Tue, 25 Jun 2024 18:47:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17762v1</guid></item><item><title>CaLMQA: Exploring culturally specific long-form question answering across 23 languages</title><link>http://arxiv.org/abs/2406.17761v1</link><description>Large language models (LLMs) are commonly used for long-form questionanswering, which requires them to generate paragraph-length answers to complexquestions. While long-form QA has been well-studied in English via manydifferent datasets and evaluation metrics, this research has not been extendedto cover most other languages. To bridge this gap, we introduce CaLMQA, acollection of 2.6K complex questions spanning 23 languages, includingunder-resourced, rarely-studied languages such as Fijian and Kirundi. Ourdataset includes both naturally-occurring questions collected from communityweb forums as well as questions written by native speakers, whom we hire forthis purpose. Our process yields diverse, complex questions that reflectcultural topics (e.g. traditions, laws, news) and the language usage of nativespeakers. We conduct automatic evaluation across a suite of open- andclosed-source models using our novel metric CaLMScore, which detects incorrectlanguage and token repetitions in answers, and observe that the quality ofLLM-generated answers degrades significantly for some low-resource languages.We perform human evaluation on a subset of models and see that modelperformance is significantly worse for culturally specific questions than forculturally agnostic questions. Our findings highlight the need for furtherresearch in LLM multilingual capabilities and non-English LFQA evaluation.</description><author>Shane Arora, Marzena Karpinska, Hung-Ting Chen, Ipsita Bhattacharjee, Mohit Iyyer, Eunsol Choi</author><pubDate>Tue, 25 Jun 2024 18:45:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17761v1</guid></item><item><title>Adam-mini: Use Fewer Learning Rates To Gain More</title><link>http://arxiv.org/abs/2406.16793v2</link><description>We propose Adam-mini, an optimizer that achieves on-par or better performancethan AdamW with 45% to 50% less memory footprint. Adam-mini reduces memory bycutting down the number of learning rates in Adam: Instead of assigning anindividual learning rate for each parameter using $1/\sqrt{v}$, Adam-mini usesthe average of $v$ within a pre-defined parameter block as the learning ratefor that block. Such a design is inspired by two empirical findings. First, theHessian of Transformers exhibits a near-block diagonal structure with differentsizes of dense sub-blocks. Second, for each of these dense sub-blocks, thereexists a single high-quality learning rate that can outperform Adam, providedthat sufficient resources are available to search it out. Adam-mini providesone cost-effective way to find these good learning rates and manage to cut down$\geq$ 90% $v$ in Adam. Empirically, we verify that Adam-mini performs on paror better than AdamW on various language models sized from 125M to 7B forpre-training, supervised fine-tuning, and RLHF. The reduced memory footprint ofAdam-mini also alleviates communication overheads among GPUs and CPUs, therebyincreasing throughput. For instance, Adam-mini achieves 49.6% higher throughputthan AdamW when pre-training Llama2-7B on 2x A800-80GB GPUs, which saves 33%wall-clock time for pre-training.</description><author>Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Yinyu Ye, Zhi-Quan Luo, Ruoyu Sun</author><pubDate>Tue, 25 Jun 2024 18:45:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16793v2</guid></item><item><title>Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing</title><link>http://arxiv.org/abs/2406.06582v2</link><description>Recent work on discrete speech tokenization has paved the way for models thatcan seamlessly perform multiple tasks across modalities, e.g., speechrecognition, text to speech, speech to speech translation. Moreover, largelanguage models (LLMs) pretrained from vast text corpora contain richlinguistic information that can improve accuracy in a variety of tasks. In thispaper, we present a decoder-only Discrete Multimodal Language Model (DMLM),which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) andmodalities (text, speech, vision). We explore several critical aspects ofdiscrete multi-modal models, including the loss function, weightinitialization, mixed training supervision, and codebook. Our results show thatDMLM benefits significantly, across multiple tasks and datasets, from acombination of supervised and unsupervised training. Moreover, for ASR, itbenefits from initializing DMLM from a pretrained LLM, and from a codebookderived from Whisper activations.</description><author>Viet Anh Trinh, Rosy Southwell, Yiwen Guan, Xinlu He, Zhiyong Wang, Jacob Whitehill</author><pubDate>Tue, 25 Jun 2024 18:44:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06582v2</guid></item><item><title>Interpreting Attention Layer Outputs with Sparse Autoencoders</title><link>http://arxiv.org/abs/2406.17759v1</link><description>Decomposing model activations into interpretable components is a key openproblem in mechanistic interpretability. Sparse autoencoders (SAEs) are apopular method for decomposing the internal activations of trained transformersinto sparse, interpretable features, and have been applied to MLP layers andthe residual stream. In this work we train SAEs on attention layer outputs andshow that also here SAEs find a sparse, interpretable decomposition. Wedemonstrate this on transformers from several model families and up to 2Bparameters. We perform a qualitative study of the features computed by attention layers,and find multiple families: long-range context, short-range context andinduction features. We qualitatively study the role of every head in GPT-2Small, and estimate that at least 90% of the heads are polysemantic, i.e. havemultiple unrelated roles. Further, we show that Sparse Autoencoders are a useful tool that enableresearchers to explain model behavior in greater detail than prior work. Forexample, we explore the mystery of why models have so many seemingly redundantinduction heads, use SAEs to motivate the hypothesis that some are long-prefixwhereas others are short-prefix, and confirm this with more rigorous analysis.We use our SAEs to analyze the computation performed by the Indirect ObjectIdentification circuit (Wang et al.), validating that the SAEs find causallymeaningful intermediate variables, and deepening our understanding of thesemantics of the circuit. We open-source the trained SAEs and a tool forexploring arbitrary prompts through the lens of Attention Output SAEs.</description><author>Connor Kissane, Robert Krzyzanowski, Joseph Isaac Bloom, Arthur Conmy, Neel Nanda</author><pubDate>Tue, 25 Jun 2024 18:43:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17759v1</guid></item><item><title>MotionBooth: Motion-Aware Customized Text-to-Video Generation</title><link>http://arxiv.org/abs/2406.17758v1</link><description>In this work, we present MotionBooth, an innovative framework designed foranimating customized subjects with precise control over both object and cameramovements. By leveraging a few images of a specific object, we efficientlyfine-tune a text-to-video model to capture the object's shape and attributesaccurately. Our approach presents subject region loss and video preservationloss to enhance the subject's learning performance, along with a subject tokencross-attention loss to integrate the customized subject with motion controlsignals. Additionally, we propose training-free techniques for managing subjectand camera motions during inference. In particular, we utilize cross-attentionmap manipulation to govern subject motion and introduce a novel latent shiftmodule for camera movement control as well. MotionBooth excels in preservingthe appearance of subjects while simultaneously controlling the motions ingenerated videos. Extensive quantitative and qualitative evaluationsdemonstrate the superiority and effectiveness of our method. Our project pageis at https://jianzongwu.github.io/projects/motionbooth</description><author>Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, Kai Chen</author><pubDate>Tue, 25 Jun 2024 18:42:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17758v1</guid></item><item><title>Regularization and Optimal Multiclass Learning</title><link>http://arxiv.org/abs/2309.13692v2</link><description>The quintessential learning algorithm of empirical risk minimization (ERM) isknown to fail in various settings for which uniform convergence does notcharacterize learning. It is therefore unsurprising that the practice ofmachine learning is rife with considerably richer algorithmic techniques forsuccessfully controlling model capacity. Nevertheless, no such technique orprinciple has broken away from the pack to characterize optimal learning inthese more general settings. The purpose of this work is to characterize the role of regularization inperhaps the simplest setting for which ERM fails: multiclass learning witharbitrary label sets. Using one-inclusion graphs (OIGs), we exhibit optimallearning algorithms that dovetail with tried-and-true algorithmic principles:Occam's Razor as embodied by structural risk minimization (SRM), the principleof maximum entropy, and Bayesian reasoning. Most notably, we introduce anoptimal learner which relaxes structural risk minimization on two dimensions:it allows the regularization function to be "local" to datapoints, and uses anunsupervised learning stage to learn this regularizer at the outset. We justifythese relaxations by showing that they are necessary: removing either dimensionfails to yield a near-optimal learner. We also extract from OIGs acombinatorial sequence we term the Hall complexity, which is the first tocharacterize a problem's transductive error rate exactly. Lastly, we introduce a generalization of OIGs and the transductive learningsetting to the agnostic case, where we show that optimal orientations ofHamming graphs -- judged using nodes' outdegrees minus a system ofnode-dependent credits -- characterize optimal learners exactly. We demonstratethat an agnostic version of the Hall complexity again characterizes error ratesexactly, and exhibit an optimal learner using maximum entropy programs.</description><author>Julian Asilis, Siddartha Devic, Shaddin Dughmi, Vatsal Sharan, Shang-Hua Teng</author><pubDate>Tue, 25 Jun 2024 18:42:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13692v2</guid></item><item><title>Accelerating Clinical Evidence Synthesis with Large Language Models</title><link>http://arxiv.org/abs/2406.17755v1</link><description>Automatic medical discovery by AI is a dream of many. One step toward thatgoal is to create an AI model to understand clinical studies and synthesizeclinical evidence from the literature. Clinical evidence synthesis currentlyrelies on systematic reviews of clinical trials and retrospective analyses frommedical literature. However, the rapid expansion of publications presentschallenges in efficiently identifying, summarizing, and updating evidence. Weintroduce TrialMind, a generative AI-based pipeline for conducting medicalsystematic reviews, encompassing study search, screening, and data extractionphases. We utilize large language models (LLMs) to drive each pipelinecomponent while incorporating human expert oversight to minimize errors. Tofacilitate evaluation, we also create a benchmark dataset TrialReviewBench, acustom dataset with 870 annotated clinical studies from 25 meta-analysis papersacross various medical treatments. Our results demonstrate that TrialMindsignificantly improves the literature review process, achieving high recallrates (0.897-1.000) in study searching from over 20 million PubMed studies andoutperforming traditional language model embeddings-based methods in screening(Recall@20 of 0.227-0.246 vs. 0.000-0.102). Furthermore, our approach surpassesdirect GPT-4 performance in result extraction, with accuracy ranging from 0.65to 0.84. We also support clinical evidence synthesis in forest plots, asvalidated by eight human annotators who preferred TrialMind over the GPT-4baseline with a winning rate of 62.5%-100% across the involved reviews. Ourfindings suggest that an LLM-based clinical evidence synthesis approach, suchas TrialMind, can enable reliable and high-quality clinical evidence synthesisto improve clinical research efficiency.</description><author>Zifeng Wang, Lang Cao, Benjamin Danek, Yichi Zhang, Qiao Jin, Zhiyong Lu, Jimeng Sun</author><pubDate>Tue, 25 Jun 2024 18:41:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17755v1</guid></item><item><title>Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language</title><link>http://arxiv.org/abs/2406.17753v1</link><description>We are exposed to much information trying to influence us, such as teasermessages, debates, politically framed news, and propaganda - all of which usepersuasive language. With the recent interest in Large Language Models (LLMs),we study the ability of LLMs to produce persuasive text. As opposed to priorwork which focuses on particular domains or types of persuasion, we conduct ageneral study across various domains to measure and benchmark to what degreeLLMs produce persuasive text - both when explicitly instructed to rewrite textto be more or less persuasive and when only instructed to paraphrase. To thisend, we construct a new dataset, Persuasive-Pairs, of pairs each consisting ofa short text and of a text rewritten by an LLM to amplify or diminishpersuasive language. We multi-annotate the pairs on a relative scale forpersuasive language. This data is not only a valuable resource in itself, butwe also show that it can be used to train a regression model to predict a scoreof persuasive language between text pairs. This model can score and benchmarknew LLMs across domains, thereby facilitating the comparison of different LLMs.Finally, we discuss effects observed for different system prompts. Notably, wefind that different 'personas' in the system prompt of LLaMA3 change thepersuasive language in the text substantially, even when only instructed toparaphrase. These findings underscore the importance of investigatingpersuasive language in LLM generated text.</description><author>Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent</author><pubDate>Tue, 25 Jun 2024 18:40:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17753v1</guid></item><item><title>Enhancing Active Learning for Sentinel 2 Imagery through Contrastive Learning and Uncertainty Estimation</title><link>http://arxiv.org/abs/2405.13285v2</link><description>In this paper, we introduce a novel method designed to enhance labelefficiency in satellite imagery analysis by integrating semi-supervisedlearning (SSL) with active learning strategies. Our approach utilizescontrastive learning together with uncertainty estimations via Monte CarloDropout (MC Dropout), with a particular focus on Sentinel-2 imagery analyzedusing the Eurosat dataset. We explore the effectiveness of our method inscenarios featuring both balanced and unbalanced class distributions. Ourresults show that the proposed method performs better than several otherpopular methods in this field, enabling significant savings in labeling effortwhile maintaining high classification accuracy. These findings highlight thepotential of our approach to facilitate scalable and cost-effective satelliteimage analysis, particularly advantageous for extensive environmentalmonitoring and land use classification tasks.</description><author>David Pogorzelski, Peter Arlinghaus, Wenyan Zhang</author><pubDate>Tue, 25 Jun 2024 18:40:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13285v2</guid></item><item><title>Benchmarking Deep Learning Models on NVIDIA Jetson Nano for Real-Time Systems: An Empirical Investigation</title><link>http://arxiv.org/abs/2406.17749v1</link><description>The proliferation of complex deep learning (DL) models has revolutionizedvarious applications, including computer vision-based solutions, promptingtheir integration into real-time systems. However, the resource-intensivenature of these models poses challenges for deployment on low-computationalpower and low-memory devices, like embedded and edge devices. This workempirically investigates the optimization of such complex DL models to analyzetheir functionality on an embedded device, particularly on the NVIDIA JetsonNano. It evaluates the effectiveness of the optimized models in terms of theirinference speed for image classification and video action detection. Theexperimental results reveal that, on average, optimized models exhibit a 16.11%speed improvement over their non-optimized counterparts. This not onlyemphasizes the critical need to consider hardware constraints and environmentalsustainability in model development and deployment but also underscores thepivotal role of model optimization in enabling the widespread deployment ofAI-assisted technologies on resource-constrained computational systems. It alsoserves as proof that prioritizing hardware-specific model optimization leads toefficient and scalable solutions that substantially decrease energy consumptionand carbon footprint.</description><author>Tushar Prasanna Swaminathan, Christopher Silver, Thangarajah Akilan</author><pubDate>Tue, 25 Jun 2024 18:34:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17749v1</guid></item><item><title>A New Perspective on Shampoo's Preconditioner</title><link>http://arxiv.org/abs/2406.17748v1</link><description>Shampoo, a second-order optimization algorithm which uses a Kronecker productpreconditioner, has recently garnered increasing attention from the machinelearning community. The preconditioner used by Shampoo can be viewed either asan approximation of the Gauss--Newton component of the Hessian or thecovariance matrix of the gradients maintained by Adagrad. We provide anexplicit and novel connection between the $\textit{optimal}$ Kronecker productapproximation of these matrices and the approximation made by Shampoo. Ourconnection highlights a subtle but common misconception about Shampoo'sapproximation. In particular, the $\textit{square}$ of the approximation usedby the Shampoo optimizer is equivalent to a single step of the power iterationalgorithm for computing the aforementioned optimal Kronecker productapproximation. Across a variety of datasets and architectures we empiricallydemonstrate that this is close to the optimal Kronecker product approximation.Additionally, for the Hessian approximation viewpoint, we empirically study theimpact of various practical tricks to make Shampoo more computationallyefficient (such as using the batch gradient and the empirical Fisher) on thequality of Hessian approximation.</description><author>Depen Morwani, Itai Shapira, Nikhil Vyas, Eran Malach, Sham Kakade, Lucas Janson</author><pubDate>Tue, 25 Jun 2024 18:34:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17748v1</guid></item><item><title>Probing the effects of broken symmetries in machine learning</title><link>http://arxiv.org/abs/2406.17747v1</link><description>Symmetry is one of the most central concepts in physics, and it is nosurprise that it has also been widely adopted as an inductive bias formachine-learning models applied to the physical sciences. This is especiallytrue for models targeting the properties of matter at the atomic scale. Bothestablished and state-of-the-art approaches, with almost no exceptions, arebuilt to be exactly equivariant to translations, permutations, and rotations ofthe atoms. Incorporating symmetries -- rotations in particular -- constrainsthe model design space and implies more complicated architectures that areoften also computationally demanding. There are indications that non-symmetricmodels can easily learn symmetries from data, and that doing so can even bebeneficial for the accuracy of the model. We put a model that obeys rotationalinvariance only approximately to the test, in realistic scenarios involvingsimulations of gas-phase, liquid, and solid water. We focus specifically onphysical observables that are likely to be affected -- directly or indirectly-- by symmetry breaking, finding negligible consequences when the model is usedin an interpolative, bulk, regime. Even for extrapolative gas-phasepredictions, the model remains very stable, even though symmetry artifacts arenoticeable. We also discuss strategies that can be used to systematicallyreduce the magnitude of symmetry breaking when it occurs, and assess theirimpact on the convergence of observables.</description><author>Marcel F. Langer, Sergey N. Pozdnyakov, Michele Ceriotti</author><pubDate>Tue, 25 Jun 2024 18:34:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17747v1</guid></item><item><title>Diverse Part Synthesis for 3D Shape Creation</title><link>http://arxiv.org/abs/2401.09384v2</link><description>Methods that use neural networks for synthesizing 3D shapes in the form of apart-based representation have been introduced over the last few years. Thesemethods represent shapes as a graph or hierarchy of parts and enable a varietyof applications such as shape sampling and reconstruction. However, currentmethods do not allow easily regenerating individual shape parts according touser preferences. In this paper, we investigate techniques that allow the userto generate multiple, diverse suggestions for individual parts. Specifically,we experiment with multimodal deep generative models that allow samplingdiverse suggestions for shape parts and focus on models which have not beenconsidered in previous work on shape synthesis. To provide a comparative studyof these techniques, we introduce a method for synthesizing 3D shapes in apart-based representation and evaluate all the part suggestion techniqueswithin this synthesis method. In our method, which is inspired by previouswork, shapes are represented as a set of parts in the form of implicitfunctions which are then positioned in space to form the final shape. Synthesisin this representation is enabled by a neural network architecture based on animplicit decoder and a spatial transformer. We compare the various multimodalgenerative models by evaluating their performance in generating partsuggestions. Our contribution is to show with qualitative and quantitativeevaluations which of the new techniques for multimodal part generation performthe best and that a synthesis method based on the top-performing techniquesallows the user to more finely control the parts that are generated in the 3Dshapes while maintaining high shape fidelity when reconstructing shapes.</description><author>Yanran Guan, Oliver van Kaick</author><pubDate>Tue, 25 Jun 2024 18:33:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09384v2</guid></item><item><title>Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon</title><link>http://arxiv.org/abs/2406.17746v1</link><description>Memorization in language models is typically treated as a homogenousphenomenon, neglecting the specifics of the memorized data. We instead modelmemorization as the effect of a set of complex factors that describe eachsample and relate it to the model and corpus. To build intuition around thesefactors, we break memorization down into a taxonomy: recitation of highlyduplicated sequences, reconstruction of inherently predictable sequences, andrecollection of sequences that are neither. We demonstrate the usefulness ofour taxonomy by using it to construct a predictive model for memorization. Byanalyzing dependencies and inspecting the weights of the predictive model, wefind that different factors influence the likelihood of memorizationdifferently depending on the taxonomic category.</description><author>USVSN Sai Prashanth, Alvin Deng, Kyle O'Brien, Jyothir S V, Mohammad Aflah Khan, Jaydeep Borkar, Christopher A. Choquette-Choo, Jacob Ray Fuehne, Stella Biderman, Tracy Ke, Katherine Lee, Naomi Saphra</author><pubDate>Tue, 25 Jun 2024 18:32:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17746v1</guid></item><item><title>Light-weight End-to-End Graph Interest Network for CTR Prediction in E-commerce Search</title><link>http://arxiv.org/abs/2406.17745v1</link><description>Click-through-rate (CTR) prediction has an essential impact on improving userexperience and revenue in e-commerce search. With the development of deeplearning, graph-based methods are well exploited to utilize graph structureextracted from user behaviors and other information to help embedding learning.However, most of the previous graph-based methods mainly focus onrecommendation scenarios, and therefore their graph structures highly depend onitem's sequential information from user behaviors, ignoring query's sequentialsignal and query-item correlation. In this paper, we propose a new approachnamed Light-weight End-to-End Graph Interest Network (EGIN) to effectively mineusers' search interests and tackle previous challenges. (i) EGIN utilizes queryand item's correlation and sequential information from the search system tobuild a heterogeneous graph for better CTR prediction in e-commerce search.(ii) EGIN's graph embedding learning shares the same training input and isjointly trained with CTR prediction, making the end-to-end framework effortlessto deploy in large-scale search systems. The proposed EGIN is composed of threeparts: query-item heterogeneous graph, light-weight graph sampling, andmulti-interest network. The query-item heterogeneous graph captures correlationand sequential information of query and item efficiently by the proposedlight-weight graph sampling. The multi-interest network is well designed toutilize graph embedding to capture various similarity relationships betweenquery and item to enhance the final CTR prediction. We conduct extensiveexperiments on both public and industrial datasets to demonstrate theeffectiveness of the proposed EGIN. At the same time, the training cost ofgraph learning is relatively low compared with the main CTR prediction task,ensuring efficiency in practical applications.</description><author>Pai Peng, Quanxiang Jia, Ziqiang Zhou, Shuang Hong, Zichong Xiao</author><pubDate>Tue, 25 Jun 2024 18:31:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17745v1</guid></item><item><title>Following Length Constraints in Instructions</title><link>http://arxiv.org/abs/2406.17744v1</link><description>Aligned instruction following models can better fulfill user requests thantheir unaligned counterparts. However, it has been shown that there is a lengthbias in evaluation of such models, and that training algorithms tend to exploitthis bias by learning longer responses. In this work we show how to trainmodels that can be controlled at inference time with instructions containingdesired length constraints. Such models are superior in length instructedevaluations, outperforming standard instruction following models such as GPT4,Llama 3 and Mixtral.</description><author>Weizhe Yuan, Ilia Kulikov, Ping Yu, Kyunghyun Cho, Sainbayar Sukhbaatar, Jason Weston, Jing Xu</author><pubDate>Tue, 25 Jun 2024 18:29:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17744v1</guid></item><item><title>Point-SAM: Promptable 3D Segmentation Model for Point Clouds</title><link>http://arxiv.org/abs/2406.17741v1</link><description>The development of 2D foundation models for image segmentation has beensignificantly advanced by the Segment Anything Model (SAM). However, achievingsimilar success in 3D models remains a challenge due to issues such asnon-unified data formats, lightweight models, and the scarcity of labeled datawith diverse masks. To this end, we propose a 3D promptable segmentation model(Point-SAM) focusing on point clouds. Our approach utilizes a transformer-basedmethod, extending SAM to the 3D domain. We leverage part-level and object-levelannotations and introduce a data engine to generate pseudo labels from SAM,thereby distilling 2D knowledge into our 3D model. Our model outperformsstate-of-the-art models on several indoor and outdoor benchmarks anddemonstrates a variety of applications, such as 3D annotation. Codes and democan be found at https://github.com/zyc00/Point-SAM.</description><author>Yuchen Zhou, Jiayuan Gu, Tung Yen Chiang, Fanbo Xiang, Hao Su</author><pubDate>Tue, 25 Jun 2024 18:28:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17741v1</guid></item><item><title>Structured Unrestricted-Rank Matrices for Parameter Efficient Fine-tuning</title><link>http://arxiv.org/abs/2406.17740v1</link><description>Recent efforts to scale Transformer models have demonstrated rapid progressacross a wide range of tasks (Wei et al., 2022). However, fine-tuning thesemodels for downstream tasks is expensive due to their large parameter counts.Parameter-efficient fine-tuning (PEFT) approaches have emerged as a viablealternative by allowing us to fine-tune models by updating only a small numberof parameters. In this work, we propose a general framework for parameterefficient fine-tuning (PEFT), based on structured unrestricted-rank matrices(SURM) which can serve as a drop-in replacement for popular approaches such asAdapters and LoRA. Unlike other methods like LoRA, SURMs provides moreflexibility in finding the right balance between compactness andexpressiveness. This is achieved by using low displacement rank matrices(LDRMs), which hasn't been used in this context before. SURMs remaincompetitive with baselines, often providing significant quality improvementswhile using a smaller parameter budget. SURMs achieve 5-7% accuracy gains onvarious image classification tasks while replacing low-rank matrices in LoRA.It also results in up to 12x reduction of the number of parameters in adapters(with virtually no loss in quality) on the GLUE benchmark.</description><author>Arijit Sehanobish, Avinava Dubey, Krzysztof Choromanski, Somnath Basu Roy Chowdhury, Deepali Jain, Vikas Sindhwani, Snigdha Chaturvedi</author><pubDate>Tue, 25 Jun 2024 18:26:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17740v1</guid></item><item><title>Find Parent then Label Children: A Two-stage Taxonomy Completion Method with Pre-trained Language Model</title><link>http://arxiv.org/abs/2406.17739v1</link><description>Taxonomies, which organize domain concepts into hierarchical structures, arecrucial for building knowledge systems and downstream applications. As domainknowledge evolves, taxonomies need to be continuously updated to include newconcepts. Previous approaches have mainly focused on adding concepts to theleaf nodes of the existing hierarchical tree, which does not fully utilize thetaxonomy's knowledge and is unable to update the original taxonomy structure(usually involving non-leaf nodes). In this paper, we propose a two-stagemethod called ATTEMPT for taxonomy completion. Our method inserts new conceptsinto the correct position by finding a parent node and labeling child nodes.Specifically, by combining local nodes with prompts to generate naturalsentences, we take advantage of pre-trained language models forhypernym/hyponymy recognition. Experimental results on two public datasets(including six domains) show that ATTEMPT performs best on both taxonomycompletion and extension tasks, surpassing existing methods.</description><author>Fei Xia, Yixuan Weng, Shizhu He, Kang Liu, Jun Zhao</author><pubDate>Tue, 25 Jun 2024 18:25:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17739v1</guid></item><item><title>LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users</title><link>http://arxiv.org/abs/2406.17737v1</link><description>While state-of-the-art Large Language Models (LLMs) have shown impressiveperformance on many tasks, there has been extensive research on undesirablemodel behavior such as hallucinations and bias. In this work, we investigatehow the quality of LLM responses changes in terms of information accuracy,truthfulness, and refusals depending on three user traits: English proficiency,education level, and country of origin. We present extensive experimentation onthree state-of-the-art LLMs and two different datasets targeting truthfulnessand factuality. Our findings suggest that undesirable behaviors instate-of-the-art LLMs occur disproportionately more for users with lowerEnglish proficiency, of lower education status, and originating from outsidethe US, rendering these models unreliable sources of information towards theirmost vulnerable users.</description><author>Elinor Poole-Dayan, Deb Roy, Jad Kabbara</author><pubDate>Tue, 25 Jun 2024 18:24:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17737v1</guid></item><item><title>Large Language Models in Healthcare: A Comprehensive Benchmark</title><link>http://arxiv.org/abs/2405.00716v2</link><description>The adoption of large language models (LLMs) to assist clinicians hasattracted remarkable attention. Existing works mainly adopt the close-endedquestion-answering (QA) task with answer options for evaluation. However, manyclinical decisions involve answering open-ended questions without pre-setoptions. To better understand LLMs in the clinic, we construct a benchmarkClinicBench. We first collect eleven existing datasets covering diverseclinical language generation, understanding, and reasoning tasks. Furthermore,we construct six novel datasets and complex clinical tasks that are close toreal-world practice, i.e., referral QA, treatment recommendation,hospitalization (long document) summarization, patient education, pharmacologyQA and drug interaction for emerging drugs. We conduct an extensive evaluationof twenty-two LLMs under both zero-shot and few-shot settings. Finally, weinvite medical experts to evaluate the clinical usefulness of LLMs.</description><author>Andrew Liu, Hongjian Zhou, Yining Hua, Omid Rohanian, Anshul Thakur, Lei Clifton, David A. Clifton</author><pubDate>Tue, 25 Jun 2024 18:23:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00716v2</guid></item><item><title>The Best Arm Evades: Near-optimal Multi-pass Streaming Lower Bounds for Pure Exploration in Multi-armed Bandits</title><link>http://arxiv.org/abs/2309.03145v2</link><description>We give a near-optimal sample-pass trade-off for pure exploration inmulti-armed bandits (MABs) via multi-pass streaming algorithms: any streamingalgorithm with sublinear memory that uses the optimal sample complexity of$O(\frac{n}{\Delta^2})$ requires$\Omega(\frac{\log{(1/\Delta)}}{\log\log{(1/\Delta)}})$ passes. Here, $n$ isthe number of arms and $\Delta$ is the reward gap between the best and thesecond-best arms. Our result matches the $O(\log(\frac{1}{\Delta}))$-passalgorithm of Jin et al. [ICML'21] (up to lower order terms) that only uses$O(1)$ memory and answers an open question posed by Assadi and Wang [STOC'20].</description><author>Sepehr Assadi, Chen Wang</author><pubDate>Tue, 25 Jun 2024 18:20:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03145v2</guid></item><item><title>Arboretum: A Large Multimodal Dataset Enabling AI for Biodiversity</title><link>http://arxiv.org/abs/2406.17720v1</link><description>We introduce Arboretum, the largest publicly accessible dataset designed toadvance AI for biodiversity applications. This dataset, curated from theiNaturalist community science platform and vetted by domain experts to ensureaccuracy, includes 134.6 million images, surpassing existing datasets in scaleby an order of magnitude. The dataset encompasses image-language paired datafor a diverse set of species from birds (Aves), spiders/ticks/mites(Arachnida), insects (Insecta), plants (Plantae), fungus/mushrooms (Fungi),snails (Mollusca), and snakes/lizards (Reptilia), making it a valuable resourcefor multimodal vision-language AI models for biodiversity assessment andagriculture research. Each image is annotated with scientific names, taxonomicdetails, and common names, enhancing the robustness of AI model training. We showcase the value of Arboretum by releasing a suite of CLIP modelstrained using a subset of 40 million captioned images. We introduce several newbenchmarks for rigorous assessment, report accuracy for zero-shot learning, andevaluations across life stages, rare species, confounding species, and variouslevels of the taxonomic hierarchy. We anticipate that Arboretum will spur the development of AI models that canenable a variety of digital tools ranging from pest control strategies, cropmonitoring, and worldwide biodiversity assessment and environmentalconservation. These advancements are critical for ensuring food security,preserving ecosystems, and mitigating the impacts of climate change. Arboretumis publicly available, easily accessible, and ready for immediate use. Please see the \href{https://baskargroup.github.io/Arboretum/}{projectwebsite} for links to our data, models, and code.</description><author>Chih-Hsuan Yang, Benjamin Feuer, Zaki Jubery, Zi K. Deng, Andre Nakkab, Md Zahid Hasan, Shivani Chiranjeevi, Kelly Marshall, Nirmal Baishnab, Asheesh K Singh, Arti Singh, Soumik Sarkar, Nirav Merchant, Chinmay Hegde, Baskar Ganapathysubramanian</author><pubDate>Tue, 25 Jun 2024 18:09:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17720v1</guid></item><item><title>Fast gradient-free activation maximization for neurons in spiking neural networks</title><link>http://arxiv.org/abs/2401.10748v2</link><description>Elements of neural networks, both biological and artificial, can be describedby their selectivity for specific cognitive features. Understanding thesefeatures is important for understanding the inner workings of neural networks.For a living system, such as a neuron, whose response to a stimulus is unknownand not differentiable, the only way to reveal these features is through afeedback loop that exposes it to a large set of different stimuli. Theproperties of these stimuli should be varied iteratively in order to maximizethe neuronal response. To utilize this feedback loop for a biological neuralnetwork, it is important to run it quickly and efficiently in order to reachthe stimuli that maximizes certain neurons' activation with the least number ofiterations possible. Here we present a framework with an efficient design forsuch a loop. We successfully tested it on an artificial spiking neural network(SNN), which is a model that simulates the asynchronous spiking activity ofneurons in living brains. Our optimization method for activation maximizationis based on the low-rank Tensor Train decomposition of the discrete activationfunction. The optimization space is the latent parameter space of imagesgenerated by SN-GAN or VQ-VAE generative models. To our knowledge, this is thefirst time that effective AM has been applied to SNNs. We track changes in theoptimal stimuli for artificial neurons during training and show that highlyselective neurons can form already in the early epochs of training and in theearly layers of a convolutional spiking network. This formation of refinedoptimal stimuli is associated with an increase in classification accuracy. Someneurons, especially in the deeper layers, may gradually change the conceptsthey are selective for during learning, potentially explaining their importancefor model performance.</description><author>Nikita Pospelov, Andrei Chertkov, Maxim Beketov, Ivan Oseledets, Konstantin Anokhin</author><pubDate>Tue, 25 Jun 2024 18:08:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10748v2</guid></item><item><title>When does Self-Prediction help? Understanding Auxiliary Tasks in Reinforcement Learning</title><link>http://arxiv.org/abs/2406.17718v1</link><description>We investigate the impact of auxiliary learning tasks such as observationreconstruction and latent self-prediction on the representation learningproblem in reinforcement learning. We also study how they interact withdistractions and observation functions in the MDP. We provide a theoreticalanalysis of the learning dynamics of observation reconstruction, latentself-prediction, and TD learning in the presence of distractions andobservation functions under linear model assumptions. With this formalization,we are able to explain why latent-self prediction is a helpful \emph{auxiliarytask}, while observation reconstruction can provide more useful features whenused in isolation. Our empirical analysis shows that the insights obtained fromour learning dynamics framework predicts the behavior of these loss functionsbeyond the linear model assumption in non-linear neural networks. Thisreinforces the usefulness of the linear model framework not only fortheoretical analysis, but also practical benefit for applied problems.</description><author>Claas Voelcker, Tyler Kastner, Igor Gilitschenski, Amir-massoud Farahmand</author><pubDate>Tue, 25 Jun 2024 18:06:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17718v1</guid></item><item><title>A Temporal Stochastic Bias Correction using a Machine Learning Attention model</title><link>http://arxiv.org/abs/2402.14169v5</link><description>Climate models are biased with respect to real-world observations. Theyusually need to be adjusted before being used in impact studies. The suite ofstatistical methods that enable such adjustments is called bias correction(BC). However, BC methods currently struggle to adjust temporal biases. Becausethey mostly disregard the dependence between consecutive time points. As aresult, climate statistics with long-range temporal properties, such asheatwave duration and frequency, cannot be corrected accurately. This makes itmore difficult to produce reliable impact studies on such climate statistics.This paper offers a novel BC methodology to correct temporal biases. This ismade possible by rethinking the philosophy behind BC. We will introduce BC as atime-indexed regression task with stochastic outputs. Rethinking BC enables usto adapt state-of-the-art machine learning (ML) attention models and therebylearn different types of biases, including temporal asynchronicities. With acase study of heatwave duration statistics in Abuja, Nigeria, and Tokyo, Japan,we show more accurate results than current climate model outputs andalternative BC methods.</description><author>Omer Nivron, Damon J. Wischik, Mathieu Vrac, Emily Shuckburgh, Alex T. Archibald</author><pubDate>Tue, 25 Jun 2024 18:03:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14169v5</guid></item><item><title>A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models</title><link>http://arxiv.org/abs/2402.15422v2</link><description>Patients often face difficulties in understanding their hospitalizations,while healthcare workers have limited resources to provide explanations. Inthis work, we investigate the potential of large language models to generatepatient summaries based on doctors' notes and study the effect of training dataon the faithfulness and quality of the generated summaries. To this end, werelease (i) a rigorous labeling protocol for errors in medical texts and (ii) apublicly available dataset of annotated hallucinations in 100 doctor-writtenand 100 generated summaries. We show that fine-tuning on hallucination-freedata effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama2, while preserving relevant information. We observe a similar effect on GPT-4(0.70 to 0.40), when the few-shot examples are hallucination-free. We alsoconduct a qualitative evaluation using hallucination-free and improved trainingdata. We find that common quantitative metrics do not correlate well withfaithfulness and quality. Finally, we test GPT-4 for automatic hallucinationdetection, which clearly outperforms common baselines.</description><author>Stefan Hegselmann, Shannon Zejiang Shen, Florian Gierse, Monica Agrawal, David Sontag, Xiaoyi Jiang</author><pubDate>Tue, 25 Jun 2024 18:02:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15422v2</guid></item><item><title>XCube: Large-Scale 3D Generative Modeling using Sparse Voxel Hierarchies</title><link>http://arxiv.org/abs/2312.03806v2</link><description>We present XCube (abbreviated as $\mathcal{X}^3$), a novel generative modelfor high-resolution sparse 3D voxel grids with arbitrary attributes. Our modelcan generate millions of voxels with a finest effective resolution of up to$1024^3$ in a feed-forward fashion without time-consuming test-timeoptimization. To achieve this, we employ a hierarchical voxel latent diffusionmodel which generates progressively higher resolution grids in a coarse-to-finemanner using a custom framework built on the highly efficient VDB datastructure. Apart from generating high-resolution objects, we demonstrate theeffectiveness of XCube on large outdoor scenes at scales of 100m$\times$100mwith a voxel size as small as 10cm. We observe clear qualitative andquantitative improvements over past approaches. In addition to unconditionalgeneration, we show that our model can be used to solve a variety of tasks suchas user-guided editing, scene completion from a single scan, and text-to-3D.The source code and more results can be found athttps://research.nvidia.com/labs/toronto-ai/xcube/.</description><author>Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, Francis Williams</author><pubDate>Tue, 25 Jun 2024 18:01:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03806v2</guid></item><item><title>ViANLI: Adversarial Natural Language Inference for Vietnamese</title><link>http://arxiv.org/abs/2406.17716v1</link><description>The development of Natural Language Processing (NLI) datasets and models hasbeen inspired by innovations in annotation design. With the rapid developmentof machine learning models today, the performance of existing machine learningmodels has quickly reached state-of-the-art results on a variety of tasksrelated to natural language processing, including natural language inferencetasks. By using a pre-trained model during the annotation process, it ispossible to challenge current NLI models by having humans producepremise-hypothesis combinations that the machine model cannot correctlypredict. To remain attractive and challenging in the research of naturallanguage inference for Vietnamese, in this paper, we introduce the adversarialNLI dataset to the NLP research community with the name ViANLI. This data setcontains more than 10K premise-hypothesis pairs and is built by a continuouslyadjusting process to obtain the most out of the patterns generated by theannotators. ViANLI dataset has brought many difficulties to many current SOTAmodels when the accuracy of the most powerful model on the test set onlyreached 48.4%. Additionally, the experimental results show that the modelstrained on our dataset have significantly improved the results on otherVietnamese NLI datasets.</description><author>Tin Van Huynh, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen</author><pubDate>Tue, 25 Jun 2024 17:58:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17716v1</guid></item><item><title>ID-Animator: Zero-Shot Identity-Preserving Human Video Generation</title><link>http://arxiv.org/abs/2404.15275v3</link><description>Generating high-fidelity human video with specified identities has attractedsignificant attention in the content generation community. However, existingtechniques struggle to strike a balance between training efficiency andidentity preservation, either requiring tedious case-by-case fine-tuning orusually missing identity details in the video generation process. In thisstudy, we present \textbf{ID-Animator}, a zero-shot human-video generationapproach that can perform personalized video generation given a singlereference facial image without further training. ID-Animator inherits existingdiffusion-based video generation backbones with a face adapter to encode theID-relevant embeddings from learnable facial latent queries. To facilitate theextraction of identity information in video generation, we introduce anID-oriented dataset construction pipeline that incorporates unified humanattributes and action captioning techniques from a constructed facial imagepool. Based on this pipeline, a random reference training strategy is furtherdevised to precisely capture the ID-relevant embeddings with an ID-preservingloss, thus improving the fidelity and generalization capacity of our model forID-specific video generation. Extensive experiments demonstrate the superiorityof ID-Animator to generate personalized human videos over previous models.Moreover, our method is highly compatible with popular pre-trained T2V modelslike animatediff and various community backbone models, showing highextendability in real-world applications for video generation where identitypreservation is highly desired. Our codes and checkpoints are released athttps://github.com/ID-Animator/ID-Animator.</description><author>Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, Jie Zhang</author><pubDate>Tue, 25 Jun 2024 17:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15275v3</guid></item><item><title>Compositional Models for Estimating Causal Effects</title><link>http://arxiv.org/abs/2406.17714v1</link><description>Many real-world systems can be represented as sets of interacting components.Examples of such systems include computational systems such as queryprocessors, natural systems such as cells, and social systems such as families.Many approaches have been proposed in traditional (associational) machinelearning to model such structured systems, including statistical relationalmodels and graph neural networks. Despite this prior work, existing approachesto estimating causal effects typically treat such systems as single units,represent them with a fixed set of variables and assume a homogeneousdata-generating process. We study a compositional approach for estimatingindividual treatment effects (ITE) in structured systems, where each unit isrepresented by the composition of multiple heterogeneous components. Thisapproach uses a modular architecture to model potential outcomes at eachcomponent and aggregates component-level potential outcomes to obtain theunit-level potential outcomes. We discover novel benefits of the compositionalapproach in causal inference - systematic generalization to estimatecounterfactual outcomes of unseen combinations of components and improvedoverlap guarantees between treatment and control groups compared to theclassical methods for causal effect estimation. We also introduce a set ofnovel environments for empirically evaluating the compositional approach anddemonstrate the effectiveness of our approach using both simulated andreal-world data.</description><author>Purva Pruthi, David Jensen</author><pubDate>Tue, 25 Jun 2024 17:56:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17714v1</guid></item><item><title>Reward Steering with Evolutionary Heuristics for Decoding-time Alignment</title><link>http://arxiv.org/abs/2406.15193v3</link><description>The widespread applicability and increasing omnipresence of LLMs haveinstigated a need to align LLM responses to user and stakeholder preferences.Many preference optimization approaches have been proposed that fine-tune LLMparameters to achieve good alignment. However, such parameter tuning is knownto interfere with model performance on many tasks. Moreover, keeping up withshifting user preferences is tricky in such a situation. Decoding-timealignment with reward model guidance solves these issues at the cost ofincreased inference time. However, most of such methods fail to strike theright balance between exploration and exploitation of reward -- often due tothe conflated formulation of these two aspects - to give well-alignedresponses. To remedy this we decouple these two aspects and implement them inan evolutionary fashion: exploration is enforced by decoding from mutatedinstructions and exploitation is represented as the periodic replacement ofpoorly-rewarded generations with well-rewarded ones. Empirical evidencesindicate that this strategy outperforms many preference optimization anddecode-time alignment approaches on two widely accepted alignment benchmarksAlpacaEval 2 and MT-Bench. Our implementation will be available at:https://darwin-alignment.github.io.</description><author>Chia-Yu Hung, Navonil Majumder, Ambuj Mehrish, Soujanya Poria</author><pubDate>Tue, 25 Jun 2024 17:55:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.15193v3</guid></item><item><title>Multi-objective Binary Differential Approach with Parameter Tuning for Discovering Business Process Models: MoD-ProM</title><link>http://arxiv.org/abs/2406.17713v1</link><description>Process discovery approaches analyze the business data to automaticallyuncover structured information, known as a process model. The quality of aprocess model is measured using quality dimensions -- completeness (replayfitness), preciseness, simplicity, and generalization. Traditional processdiscovery algorithms usually output a single process model. A single model maynot accurately capture the observed behavior and overfit the training data. Wehave formed the process discovery problem in a multi-objective framework thatyields several candidate solutions for the end user who can pick a suitablemodel based on the local environmental constraints (possibly varying). Weconsider the Binary Differential Evolution approach in a multi-objectiveframework for the task of process discovery. The proposed method employsdichotomous crossover/mutation operators. The parameters are tuned using Greyrelational analysis combined with the Taguchi approach. {We have compared theproposed approach with the well-known single-objective algorithms andstate-of-the-art multi-objective evolutionary algorithm -- Non-dominatedSorting Genetic Algorithm (NSGA-II).} Additional comparison via computing aweighted average of the quality dimensions is also undertaken. Results showthat the proposed algorithm is computationally efficient and producesdiversified candidate solutions that score high on the fitness functions. It isshown that the process models generated by the proposed approach are superiorto or at least as good as those generated by the state-of-the-art algorithms.</description><author>Sonia Deshmukh, Shikha Gupta, Naveen Kumar</author><pubDate>Tue, 25 Jun 2024 17:53:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17713v1</guid></item><item><title>Deep Pulse-Signal Magnification for remote Heart Rate Estimation in Compressed Videos</title><link>http://arxiv.org/abs/2405.02652v2</link><description>Recent advancements in data-driven approaches for remote photoplethysmography(rPPG) have significantly improved the accuracy of remote heart rateestimation. However, the performance of such approaches worsens considerablyunder video compression, which is nevertheless necessary to store and transmitvideo data efficiently. In this paper, we present a novel approach to addressthe impact of video compression on rPPG estimation, which leverages apulse-signal magnification transformation to adapt compressed videos to anuncompressed data domain in which the rPPG signal is magnified. We validate theeffectiveness of our model by exhaustive evaluations on two publicly availabledatasets, UCLA-rPPG and UBFC-rPPG, employing both intra- and cross-databaseperformance at several compression rates. Additionally, we assess therobustness of our approach on two additional highly compressed and widely-useddatasets, MAHNOB-HCI and COHFACE, which reveal outstanding heart rateestimation results.</description><author>Joaquim Comas, Adria Ruiz, Federico Sukno</author><pubDate>Tue, 25 Jun 2024 17:53:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02652v2</guid></item><item><title>Data curation via joint example selection further accelerates multimodal learning</title><link>http://arxiv.org/abs/2406.17711v1</link><description>Data curation is an essential component of large-scale pretraining. In thiswork, we demonstrate that jointly selecting batches of data is more effectivefor learning than selecting examples independently. Multimodal contrastiveobjectives expose the dependencies between data and thus naturally yieldcriteria for measuring the joint learnability of a batch. We derive a simpleand tractable algorithm for selecting such batches, which significantlyaccelerate training beyond individually-prioritized data points. As performanceimproves by selecting from larger super-batches, we also leverage recentadvances in model approximation to reduce the associated computationaloverhead. As a result, our approach--multimodal contrastive learning with jointexample selection (JEST)--surpasses state-of-the-art models with up to13$\times$ fewer iterations and 10$\times$ less computation. Essential to theperformance of JEST is the ability to steer the data selection process towardsthe distribution of smaller, well-curated datasets via pretrained referencemodels, exposing the level of data curation as a new dimension for neuralscaling laws.</description><author>Talfan Evans, Nikhil Parthasarathy, Hamza Merzic, Olivier J. Henaff</author><pubDate>Tue, 25 Jun 2024 17:52:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17711v1</guid></item><item><title>Mask-Guided Attention U-Net for Enhanced Neonatal Brain Extraction and Image Preprocessing</title><link>http://arxiv.org/abs/2406.17709v1</link><description>In this study, we introduce MGA-Net, a novel mask-guided attention neuralnetwork, which extends the U-net model for precision neonatal brain imaging.MGA-Net is designed to extract the brain from other structures and reconstructhigh-quality brain images. The network employs a common encoder and twodecoders: one for brain mask extraction and the other for brain regionreconstruction. A key feature of MGA-Net is its high-level mask-guidedattention module, which leverages features from the brain mask decoder toenhance image reconstruction. To enable the same encoder and decoder to processboth MRI and ultrasound (US) images, MGA-Net integrates sinusoidal positionalencoding. This encoding assigns distinct positional values to MRI and USimages, allowing the model to effectively learn from both modalities.Consequently, features learned from a single modality can aid in learning amodality with less available data, such as US. We extensively validated theproposed MGA-Net on diverse datasets from varied clinical settings and neonatalage groups. The metrics used for assessment included the DICE similaritycoefficient, recall, and accuracy for image segmentation; structural similarityfor image reconstruction; and root mean squared error for total brain volumeestimation from 3D ultrasound images. Our results demonstrate that MGA-Netsignificantly outperforms traditional methods, offering superior performance inbrain extraction and segmentation while achieving high precision in imagereconstruction and volumetric analysis. Thus, MGA-Net represents a robust andeffective preprocessing tool for MRI and 3D ultrasound images, marking asignificant advance in neuroimaging that enhances both research and clinicaldiagnostics in the neonatal period and beyond.</description><author>Bahram Jafrasteh, Simon Pedro Lubian-Lopez, Emiliano Trimarco, Macarena Roman Ruiz, Carmen Rodriguez Barrios, Yolanda Marin Almagro, Isabel Benavente-Fernandez</author><pubDate>Tue, 25 Jun 2024 17:48:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17709v1</guid></item><item><title>SurgeMOD: Translating image-space tissue motions into vision-based surgical forces</title><link>http://arxiv.org/abs/2406.17707v1</link><description>We present a new approach for vision-based force estimation in MinimallyInvasive Robotic Surgery based on frequency domain basis of motion of organsderived directly from video. Using internal movements generated by naturalprocesses like breathing or the cardiac cycle, we infer the image-space basisof the motion on the frequency domain. As we are working with thisrepresentation, we discretize the problem to a limited amount oflow-frequencies to build an image-space mechanical model of the environment. Weuse this pre-built model to define our force estimation problem as a dynamicconstraint problem. We demonstrate that this method can estimate point contactforces reliably for silicone phantom and ex-vivo experiments, matching realreadings from a force sensor. In addition, we perform qualitative experimentsin which we synthesize coherent force textures from surgical videos over acertain region of interest selected by the user. Our method demonstrates goodresults for both quantitative and qualitative analysis, providing a goodstarting point for a purely vision-based method for surgical force estimation.</description><author>Mikel De Iturrate Reyzabal, Dionysios Malas, Shuai Wang, Sebastien Ourselin, Hongbin Liu</author><pubDate>Tue, 25 Jun 2024 17:46:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17707v1</guid></item><item><title>FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model</title><link>http://arxiv.org/abs/2406.17706v1</link><description>Large language models (LLMs) show amazing performance on many domain-specifictasks after fine-tuning with some appropriate data. However, manydomain-specific data are privately distributed across multiple owners. Thus,this dilemma raises the interest in how to perform LLM fine-tuning in federatedlearning (FL). However, confronted with limited computation and communicationcapacities, FL clients struggle to fine-tune an LLM effectively. To this end,we introduce FedBiOT, a resource-efficient LLM fine-tuning approach to FL.Specifically, our method involves the server generating a compressed LLM andaligning its performance with the full model. Subsequently, the clientsfine-tune a lightweight yet important part of the compressed model, referred toas an adapter. Notice that as the server has no access to the private dataowned by the clients, the data used for alignment by the server has a differentdistribution from the one used for fine-tuning by clients. We formulate theproblem into a bi-level optimization problem to minimize the negative effect ofdata discrepancy and derive the updating rules for the server and clients. Weconduct extensive experiments on LLaMA-2, empirically showing that the adapterhas exceptional performance when reintegrated into the global LLM. The resultsalso indicate that the proposed FedBiOT significantly reduces resourceconsumption compared to existing benchmarks, all while achieving comparableperformance levels.</description><author>Feijie Wu, Zitao Li, Yaliang Li, Bolin Ding, Jing Gao</author><pubDate>Tue, 25 Jun 2024 17:45:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17706v1</guid></item><item><title>Can independent Metropolis beat crude Monte Carlo?</title><link>http://arxiv.org/abs/2406.17699v1</link><description>Assume that we would like to estimate the expected value of a function $F$with respect to a density $\pi$. We prove that if $\pi$ is close enough underKL divergence to another density $q$, an independent Metropolis samplerestimator that obtains samples from $\pi$ with proposal density $q$, enrichedwith a variance reduction computational strategy based on control variates,achieves smaller asymptotic variance than that of the crude Monte Carloestimator. The control variates construction requires no extra computationaleffort but assumes that the expected value of $F$ under $q$ is analyticallyavailable. We illustrate this result by calculating the marginal likelihood ina linear regression model with prior-likelihood conflict and a non-conjugateprior. Furthermore, we propose an adaptive independent Metropolis algorithmthat adapts the proposal density such that its KL divergence with the target isbeing reduced. We demonstrate its applicability in a Bayesian logistic andGaussian process regression problems and we rigorously justify our asymptoticarguments under easily verifiable and essentially minimal conditions.</description><author>Siran Liu, Petros Dellaportas, Michalis K. Titsias</author><pubDate>Tue, 25 Jun 2024 17:38:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17699v1</guid></item><item><title>Identifying Nonstationary Causal Structures with High-Order Markov Switching Models</title><link>http://arxiv.org/abs/2406.17698v1</link><description>Causal discovery in time series is a rapidly evolving field with a widevariety of applications in other areas such as climate science andneuroscience. Traditional approaches assume a stationary causal graph, whichcan be adapted to nonstationary time series with time-dependent effects orheterogeneous noise. In this work we address nonstationarity viaregime-dependent causal structures. We first establish identifiability forhigh-order Markov Switching Models, which provide the foundations foridentifiable regime-dependent causal discovery. Our empirical studiesdemonstrate the scalability of our proposed approach for high-orderregime-dependent structure estimation, and we illustrate its applicability onbrain activity data.</description><author>Carles Balsells-Rodas, Yixin Wang, Pedro A. M. Mediano, Yingzhen Li</author><pubDate>Tue, 25 Jun 2024 17:38:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17698v1</guid></item><item><title>HGTDP-DTA: Hybrid Graph-Transformer with Dynamic Prompt for Drug-Target Binding Affinity Prediction</title><link>http://arxiv.org/abs/2406.17697v1</link><description>Drug target binding affinity (DTA) is a key criterion for drug screening.Existing experimental methods are time-consuming and rely on limited structuraland domain information. While learning-based methods can model sequence andstructural information, they struggle to integrate contextual data and oftenlack comprehensive modeling of drug-target interactions. In this study, wepropose a novel DTA prediction method, termed HGTDP-DTA, which utilizes dynamicprompts within a hybrid Graph-Transformer framework. Our method generatescontext-specific prompts for each drug-target pair, enhancing the model'sability to capture unique interactions. The introduction of prompt tuningfurther optimizes the prediction process by filtering out irrelevant noise andemphasizing task-relevant information, dynamically adjusting the input featuresof the molecular graph. The proposed hybrid Graph-Transformer architecturecombines structural information from Graph Convolutional Networks (GCNs) withsequence information captured by Transformers, facilitating the interactionbetween global and local information. Additionally, we adopted the multi-viewfeature fusion method to project molecular graph views and affinity subgraphviews into a common feature space, effectively combining structural andcontextual information. Experiments on two widely used public datasets, Davisand KIBA, show that HGTDP-DTA outperforms state-of-the-art DTA predictionmethods in both prediction performance and generalization ability.</description><author>Xi Xiao, Wentao Wang, Jiacheng Xie, Lijing Zhu, Gaofei Chen, Zhengji Li, Tianyang Wang, Min Xu</author><pubDate>Tue, 25 Jun 2024 17:33:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17697v1</guid></item><item><title>From Distributional to Overton Pluralism: Investigating Large Language Model Alignment</title><link>http://arxiv.org/abs/2406.17692v1</link><description>The alignment process changes several properties of a large language model's(LLM's) output distribution. We analyze two aspects of post-alignmentdistributional shift of LLM responses. First, we re-examine previously reportedreductions in response diversity post-alignment. Our analysis suggests that anapparent drop in the diversity of responses is largely explained by qualitycontrol and information aggregation. Alignment suppresses irrelevant andunhelpful content while shifting the output distribution toward longerresponses that cover information spanning several responses from the base LLM,essentially presenting diverse information in a single response. Finding littleevidence that alignment suppresses useful information, it is natural to ask theopposite question: do aligned models surface information that cannot berecovered from base models? Our second investigation shows this is not the caseand the behavior of aligned models is recoverable from base models withoutfine-tuning. A combination of in-context examples and lower-resolution semantichints about response content can elicit responses from base LLMs that are assimilar to alignment-tuned LLM responses as alignment-tuned LLM responses areto each other. Taken together, these results indicate that current alignmenttechniques capture but do not extend the useful subset of assistant-like baseLLM behavior, providing further evidence for the Superficial AlignmentHypothesis. They also show that in-context alignment can go surprisingly far asa strategy for imitating aligned LLMs without fine-tuning. Our code and data isavailable at https://github.com/thomlake/investigating-alignment.</description><author>Thom Lake, Eunsol Choi, Greg Durrett</author><pubDate>Tue, 25 Jun 2024 17:32:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17692v1</guid></item><item><title>Analysis of learning a flow-based generative model from limited sample complexity</title><link>http://arxiv.org/abs/2310.03575v2</link><description>We study the problem of training a flow-based generative model, parametrizedby a two-layer autoencoder, to sample from a high-dimensional Gaussian mixture.We provide a sharp end-to-end analysis of the problem. First, we provide atight closed-form characterization of the learnt velocity field, whenparametrized by a shallow denoising auto-encoder trained on a finite number $n$of samples from the target distribution. Building on this analysis, we providea sharp description of the corresponding generative flow, which pushes the baseGaussian density forward to an approximation of the target density. Inparticular, we provide closed-form formulae for the distance between the meanof the generated mixture and the mean of the target mixture, which we showdecays as $\Theta_n(\frac{1}{n})$. Finally, this rate is shown to be in factBayes-optimal.</description><author>Hugo Cui, Florent Krzakala, Eric Vanden-Eijnden, Lenka Zdeborová</author><pubDate>Tue, 25 Jun 2024 17:32:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03575v2</guid></item><item><title>Truthful Aggregation of LLMs with an Application to Online Advertising</title><link>http://arxiv.org/abs/2405.05905v2</link><description>Online platforms generate hundreds of billions of dollars in revenue per yearby showing advertisements alongside their own content. Currently, theseplatforms are integrating Large Language Models (LLMs) into their services.This makes revenue generation from LLM-generated content the next majorchallenge in online advertising. We consider a scenario where advertisers aimto influence the responses of an LLM to align with their interests, whileplatforms seek to maximize advertiser value and ensure user satisfaction. Weintroduce an auction mechanism for this problem that operates without LLMfine-tuning or access to model weights and provably converges to the output ofthe optimally fine-tuned LLM for the platform's objective as computationalresources increase. Our mechanism ensures that truthful reporting is a dominantstrategy for advertisers and it aligns each advertiser's utility with theircontribution to social welfare - an essential feature for long-term viability.Additionally, it can incorporate contextual information about the advertisers,significantly accelerating convergence. Via experiments with a publiclyavailable LLM, we show that our mechanism significantly boosts advertiser valueand platform revenue, with low computational overhead. While our motivatingapplication is online advertising, our mechanism can be applied in any settingwith monetary transfers, making it a general-purpose solution for truthfullyaggregating the preferences of self-interested agents over LLM-generatedreplies.</description><author>Ermis Soumalias, Michael J. Curry, Sven Seuken</author><pubDate>Tue, 25 Jun 2024 17:31:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05905v2</guid></item><item><title>Image Distillation for Safe Data Sharing in Histopathology</title><link>http://arxiv.org/abs/2406.13536v2</link><description>Histopathology can help clinicians make accurate diagnoses, determine diseaseprognosis, and plan appropriate treatment strategies. As deep learningtechniques prove successful in the medical domain, the primary challengesbecome limited data availability and concerns about data sharing and privacy.Federated learning has addressed this challenge by training models locally andupdating parameters on a server. However, issues, such as domain shift andbias, persist and impact overall performance. Dataset distillation presents analternative approach to overcoming these challenges. It involves creating asmall synthetic dataset that encapsulates essential information, which can beshared without constraints. At present, this paradigm is not practicable ascurrent distillation approaches only generate non human readablerepresentations and exhibit insufficient performance for downstream learningtasks. We train a latent diffusion model and construct a new distilledsynthetic dataset with a small number of human readable synthetic images.Selection of maximally informative synthetic images is done via graph communityanalysis of the representation space. We compare downstream classificationmodels trained on our synthetic distillation data to models trained on realdata and reach performances suitable for practical application.</description><author>Zhe Li, Bernhard Kainz</author><pubDate>Tue, 25 Jun 2024 17:28:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13536v2</guid></item><item><title>Unified Auto-Encoding with Masked Diffusion</title><link>http://arxiv.org/abs/2406.17688v1</link><description>At the core of both successful generative and self-supervised representationlearning models there is a reconstruction objective that incorporates some formof image corruption. Diffusion models implement this approach through ascheduled Gaussian corruption process, while masked auto-encoder models do soby masking patches of the image. Despite their different approaches, theunderlying similarity in their methodologies suggests a promising avenue for anauto-encoder capable of both de-noising tasks. We propose a unifiedself-supervised objective, dubbed Unified Masked Diffusion (UMD), that combinespatch-based and noise-based corruption techniques within a single auto-encodingframework. Specifically, UMD modifies the diffusion transformer (DiT) trainingprocess by introducing an additional noise-free, high masking representationstep in the diffusion noising schedule, and utilizes a mixed masked and noisedimage for subsequent timesteps. By integrating features useful for diffusionmodeling and for predicting masked patch tokens, UMD achieves strongperformance in downstream generative and representation learning tasks,including linear probing and class-conditional generation. This is achievedwithout the need for heavy data augmentations, multiple views, or additionalencoders. Furthermore, UMD improves over the computational efficiency of priordiffusion based methods in total training time. We release our code athttps://github.com/philippe-eecs/small-vision.</description><author>Philippe Hansen-Estruch, Sriram Vishwanath, Amy Zhang, Manan Tomar</author><pubDate>Tue, 25 Jun 2024 17:24:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17688v1</guid></item><item><title>PiPar: Pipeline Parallelism for Collaborative Machine Learning</title><link>http://arxiv.org/abs/2302.12803v2</link><description>Collaborative machine learning (CML) techniques, such as federated learning,have been proposed to train deep learning models across multiple mobile devicesand a server. CML techniques are privacy-preserving as a local model that istrained on each device instead of the raw data from the device is shared withthe server. However, CML training is inefficient due to low resourceutilization. We identify idling resources on the server and devices due tosequential computation and communication as the principal cause of low resourceutilization. A novel framework PiPar that leverages pipeline parallelism forCML techniques is developed to substantially improve resource utilization. Anew training pipeline is designed to parallelize the computations on differenthardware resources and communication on different bandwidth resources, therebyaccelerating the training process in CML. A low overhead automated parameterselection method is proposed to optimize the pipeline, maximizing theutilization of available resources. The experimental results confirm thevalidity of the underlying approach of PiPar and highlight that when comparedto federated learning: (i) the idle time of the server can be reduced by up to64.1x, and (ii) the overall training time can be accelerated by up to 34.6xunder varying network conditions for a collection of six small and largepopular deep neural networks and four datasets without sacrificing accuracy. Itis also experimentally demonstrated that PiPar achieves performance benefitswhen incorporating differential privacy methods and operating in environmentswith heterogeneous devices and changing bandwidths.</description><author>Zihan Zhang, Philip Rodgers, Peter Kilpatrick, Ivor Spence, Blesson Varghese</author><pubDate>Tue, 25 Jun 2024 17:17:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12803v2</guid></item><item><title>Feudal Graph Reinforcement Learning</title><link>http://arxiv.org/abs/2304.05099v4</link><description>Graph-based representations and message-passing modular policies constituteprominent approaches to tackling composable control problems in ReinforcementLearning (RL). However, as shown by recent graph deep learning literature, suchlocal message-passing operators can create information bottlenecks and hinderglobal coordination. The issue becomes more serious in tasks requiringhigh-level planning. In this work, we propose a novel methodology, named FeudalGraph Reinforcement Learning (FGRL), that addresses such challenges by relyingon hierarchical RL and a pyramidal message-passing architecture. In particular,FGRL defines a hierarchy of policies where high-level commands are propagatedfrom the top of the hierarchy down through a layered graph structure. Thebottom layers mimic the morphology of the physical system, while the upperlayers correspond to higher-order sub-modules. The resulting agents are thencharacterized by a committee of policies where actions at a certain level setgoals for the level below, thus implementing a hierarchical decision-makingstructure that can naturally implement task decomposition. We evaluate theproposed framework on a graph clustering problem and MuJoCo locomotion tasks;simulation results show that FGRL compares favorably against relevantbaselines. Furthermore, an in-depth analysis of the command propagationmechanism provides evidence that the introduced message-passing scheme favorslearning hierarchical decision-making policies.</description><author>Tommaso Marzi, Arshjot Khehra, Andrea Cini, Cesare Alippi</author><pubDate>Tue, 25 Jun 2024 17:16:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05099v4</guid></item><item><title>Fine-grained Prompt Tuning: A Parameter and Memory Efficient Transfer Learning Method for High-resolution Medical Image Classification</title><link>http://arxiv.org/abs/2403.07576v3</link><description>Parameter-efficient transfer learning (PETL) is proposed as a cost-effectiveway to transfer pre-trained models to downstream tasks, avoiding the high costof updating entire large-scale pre-trained models (LPMs). In this work, wepresent Fine-grained Prompt Tuning (FPT), a novel PETL method for medical imageclassification. FPT significantly reduces memory consumption compared to otherPETL methods, especially in high-resolution input contexts. To achieve this, wefirst freeze the weights of the LPM and construct a learnable lightweight sidenetwork. The frozen LPM takes high-resolution images as input to extractfine-grained features, while the side network is fed low-resolution images toreduce memory usage. To allow the side network to access pre-trained knowledge,we introduce fine-grained prompts that summarize information from the LPMthrough a fusion module. Important tokens selection and preloading techniquesare employed to further reduce training cost and memory requirements. Weevaluate FPT on four medical datasets with varying sizes, modalities, andcomplexities. Experimental results demonstrate that FPT achieves comparableperformance to fine-tuning the entire LPM while using only 1.8% of thelearnable parameters and 13% of the memory costs of an encoder ViT-B model witha 512 x 512 input resolution.</description><author>Yijin Huang, Pujin Cheng, Roger Tam, Xiaoying Tang</author><pubDate>Tue, 25 Jun 2024 17:15:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07576v3</guid></item><item><title>VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation</title><link>http://arxiv.org/abs/2406.17681v1</link><description>As large language models achieve impressive scores on traditional benchmarks,an increasing number of researchers are becoming concerned about benchmark dataleakage during pre-training, commonly known as the data contamination problem.To ensure fair evaluation, recent benchmarks release only the training andvalidation sets, keeping the test set labels closed-source. They require anyonewishing to evaluate his language model to submit the model's predictions forcentralized processing and then publish the model's result on theirleaderboard. However, this submission process is inefficient and preventseffective error analysis. To address this issue, we propose to variabilizebenchmarks and evaluate language models dynamically. Specifically, we extractvariables from each test case and define a value range for each variable. Foreach evaluation, we sample new values from these value ranges to create uniquetest cases, thus ensuring a fresh evaluation each time. We applied thisvariable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, andTruthfulQA, which cover mathematical generation and multiple-choice tasks. Ourexperimental results demonstrate that this approach provides a more accurateassessment of the true capabilities of language models, effectively mitigatingthe contamination problem.</description><author>Kun Qian, Shunji Wan, Claudia Tang, Youzhi Wang, Xuanming Zhang, Maximillian Chen, Zhou Yu</author><pubDate>Tue, 25 Jun 2024 17:13:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17681v1</guid></item><item><title>An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek</title><link>http://arxiv.org/abs/2311.00541v5</link><description>Word meanings change over time, and word senses evolve, emerge or die out inthe process. For ancient languages, where the corpora are often small andsparse, modelling such changes accurately proves challenging, and quantifyinguncertainty in sense-change estimates consequently becomes important. GASC(Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existinggenerative models that have been used to analyse sense change for target wordsfrom an ancient Greek text corpus, using unsupervised learning without the helpof any pre-training. These models represent the senses of a given target wordsuch as "kosmos" (meaning decoration, order or world) as distributions overcontext words, and sense prevalence as a distribution over senses. The modelsare fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporalchanges in these representations. This paper introduces EDiSC, an Embedded DiSCmodel, which combines word embeddings with DiSC to provide superior modelperformance. It is shown empirically that EDiSC offers improved predictiveaccuracy, ground-truth recovery and uncertainty quantification, as well asbetter sampling efficiency and scalability properties with MCMC methods. Thechallenges of fitting these models are also discussed.</description><author>Schyan Zafar, Geoff K. Nicholls</author><pubDate>Tue, 25 Jun 2024 17:13:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00541v5</guid></item><item><title>End-to-End Autonomous Driving without Costly Modularization and 3D Manual Annotation</title><link>http://arxiv.org/abs/2406.17680v1</link><description>We propose UAD, a method for vision-based end-to-end autonomous driving(E2EAD), achieving the best open-loop evaluation performance in nuScenes,meanwhile showing robust closed-loop driving quality in CARLA. Our motivationstems from the observation that current E2EAD models still mimic the modulararchitecture in typical driving stacks, with carefully designed supervisedperception and prediction subtasks to provide environment information fororiented planning. Although achieving groundbreaking progress, such design hascertain drawbacks: 1) preceding subtasks require massive high-quality 3Dannotations as supervision, posing a significant impediment to scaling thetraining data; 2) each submodule entails substantial computation overhead inboth training and inference. To this end, we propose UAD, an E2EAD frameworkwith an unsupervised proxy to address all these issues. Firstly, we design anovel Angular Perception Pretext to eliminate the annotation requirement. Thepretext models the driving scene by predicting the angular-wise spatialobjectness and temporal dynamics, without manual annotation. Secondly, aself-supervised training strategy, which learns the consistency of thepredicted trajectories under different augment views, is proposed to enhancethe planning robustness in steering scenarios. Our UAD achieves 38.7% relativeimprovements over UniAD on the average collision rate in nuScenes and surpassesVAD for 41.32 points on the driving score in CARLA's Town05 Long benchmark.Moreover, the proposed method only consumes 44.3% training resources of UniADand runs 3.4 times faster in inference. Our innovative design not only for thefirst time demonstrates unarguable performance advantages over supervisedcounterparts, but also enjoys unprecedented efficiency in data, training, andinference. Code and models will be released athttps://github.com/KargoBot_Research/UAD.</description><author>Mingzhe Guo, Zhipeng Zhang, Yuan He, Ke Wang, Liping Jing</author><pubDate>Tue, 25 Jun 2024 17:12:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17680v1</guid></item><item><title>Local-to-Global Cross-Modal Attention-Aware Fusion for HSI-X Semantic Segmentation</title><link>http://arxiv.org/abs/2406.17679v1</link><description>Hyperspectral image (HSI) classification has recently reached its performancebottleneck. Multimodal data fusion is emerging as a promising approach toovercome this bottleneck by providing rich complementary information from thesupplementary modality (X-modality). However, achieving comprehensivecross-modal interaction and fusion that can be generalized across differentsensing modalities is challenging due to the disparity in imaging sensors,resolution, and content of different modalities. In this study, we propose aLocal-to-Global Cross-modal Attention-aware Fusion (LoGoCAF) framework forHSI-X classification that jointly considers efficiency, accuracy, andgeneralizability. LoGoCAF adopts a pixel-to-pixel two-branch semanticsegmentation architecture to learn information from HSI and X modalities. Thepipeline of LoGoCAF consists of a local-to-global encoder and a lightweightmultilayer perceptron (MLP) decoder. In the encoder, convolutions are used toencode local and high-resolution fine details in shallow layers, whiletransformers are used to integrate global and low-resolution coarse features indeeper layers. The MLP decoder aggregates information from the encoder forfeature fusion and prediction. In particular, two cross-modality modules, thefeature enhancement module (FEM) and the feature interaction and fusion module(FIFM), are introduced in each encoder stage. The FEM is used to enhancecomplementary information by combining the feature from the other modalityacross direction-aware, position-sensitive, and channel-wise dimensions. Withthe enhanced features, the FIFM is designed to promote cross-modalityinformation interaction and fusion for the final semantic prediction. Extensiveexperiments demonstrate that our LoGoCAF achieves superior performance andgeneralizes well. The code will be made publicly available.</description><author>Xuming Zhang, Naoto Yokoya, Xingfa Gu, Qingjiu Tian, Lorenzo Bruzzone</author><pubDate>Tue, 25 Jun 2024 17:12:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17679v1</guid></item><item><title>Multi-Modal Conformal Prediction Regions with Simple Structures by Optimizing Convex Shape Templates</title><link>http://arxiv.org/abs/2312.07434v2</link><description>Conformal prediction is a statistical tool for producing prediction regionsfor machine learning models that are valid with high probability. A keycomponent of conformal prediction algorithms is a \emph{non-conformity scorefunction} that quantifies how different a model's prediction is from theunknown ground truth value. Essentially, these functions determine the shapeand the size of the conformal prediction regions. While prior work has goneinto creating score functions that produce multi-model prediction regions, suchregions are generally too complex for use in downstream planning and controlproblems. We propose a method that optimizes parameterized \emph{shape templatefunctions} over calibration data, which results in non-conformity scorefunctions that produce prediction regions with minimum volume. Our approachresults in prediction regions that are \emph{multi-modal}, so they can properlycapture residuals of distributions that have multiple modes, and\emph{practical}, so each region is convex and can be easily incorporated intodownstream tasks, such as a motion planner using conformal prediction regions.Our method applies to general supervised learning tasks, while we illustrateits use in time-series prediction. We provide a toolbox and presentillustrative case studies of F16 fighter jets and autonomous vehicles, showingan up to $68\%$ reduction in prediction region area compared to a circularbaseline region.</description><author>Renukanandan Tumu, Matthew Cleaveland, Rahul Mangharam, George J. Pappas, Lars Lindemann</author><pubDate>Tue, 25 Jun 2024 17:10:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07434v2</guid></item><item><title>Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models</title><link>http://arxiv.org/abs/2406.17675v1</link><description>Large Language Models (LLMs) have demonstrated exceptional task-solvingcapabilities, increasingly adopting roles akin to human-like assistants. Thebroader integration of LLMs into society has sparked interest in whether theymanifest psychological attributes, and whether these attributes arestable-inquiries that could deepen the understanding of their behaviors.Inspired by psychometrics, this paper presents a framework for investigatingpsychology in LLMs, including psychological dimension identification,assessment dataset curation, and assessment with results validation. Followingthis framework, we introduce a comprehensive psychometrics benchmark for LLMsthat covers six psychological dimensions: personality, values, emotion, theoryof mind, motivation, and intelligence. This benchmark includes thirteendatasets featuring diverse scenarios and item types. Our findings indicate thatLLMs manifest a broad spectrum of psychological attributes. We also uncoverdiscrepancies between LLMs' self-reported traits and their behaviors inreal-world scenarios. This paper demonstrates a thorough psychometricassessment of LLMs, providing insights into reliable evaluation and potentialapplications in AI and social sciences.</description><author>Yuan Li, Yue Huang, Hongyi Wang, Xiangliang Zhang, James Zou, Lichao Sun</author><pubDate>Tue, 25 Jun 2024 17:09:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17675v1</guid></item><item><title>Locally Differentially Private Distributed Online Learning with Guaranteed Optimality</title><link>http://arxiv.org/abs/2306.14094v2</link><description>Distributed online learning is gaining increased traction due to its uniqueability to process large-scale datasets and streaming data. To address thegrowing public awareness and concern on privacy protection, plenty ofalgorithms have been proposed to enable differential privacy in distributedonline optimization and learning. However, these algorithms often face thedilemma of trading learning accuracy for privacy. By exploiting the uniquecharacteristics of online learning, this paper proposes an approach thattackles the dilemma and ensures both differential privacy and learning accuracyin distributed online learning. More specifically, while ensuring a diminishingexpected instantaneous regret, the approach can simultaneously ensure a finitecumulative privacy budget, even in the infinite time horizon. To cater for thefully distributed setting, we adopt the local differential-privacy framework,which avoids the reliance on a trusted data curator, and, hence, providesstronger protection than the classic "centralized" (global) differentialprivacy. To the best of our knowledge, this is the first algorithm thatsuccessfully ensures both rigorous local differential privacy and learningaccuracy. The effectiveness of the proposed algorithm is evaluated usingmachine learning tasks, including logistic regression on the the "mushrooms"datasets and CNN-based image classification on the "MNIST" and "CIFAR-10"datasets.</description><author>Ziqin Chen, Yongqiang Wang</author><pubDate>Tue, 25 Jun 2024 17:07:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.14094v2</guid></item><item><title>LaTable: Towards Large Tabular Models</title><link>http://arxiv.org/abs/2406.17673v1</link><description>Tabular data is one of the most ubiquitous modalities, yet the literature ontabular generative foundation models is lagging far behind its text and visioncounterparts. Creating such a model is hard, due to the heterogeneous featurespaces of different tabular datasets, tabular metadata (e.g. datasetdescription and feature headers), and tables lacking prior knowledge (e.g.feature order). In this work we propose LaTable: a novel tabular diffusionmodel that addresses these challenges and can be trained across differentdatasets. Through extensive experiments we find that LaTable outperformsbaselines on in-distribution generation, and that finetuning LaTable cangenerate out-of-distribution datasets better with fewer samples. On the otherhand, we explore the poor zero-shot performance of LaTable, and what it mayteach us about building generative tabular foundation models with better zero-and few-shot generation capabilities.</description><author>Boris van Breugel, Jonathan Crabbé, Rob Davis, Mihaela van der Schaar</author><pubDate>Tue, 25 Jun 2024 17:03:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17673v1</guid></item><item><title>Metrics for Dataset Demographic Bias: A Case Study on Facial Expression Recognition</title><link>http://arxiv.org/abs/2303.15889v2</link><description>Demographic biases in source datasets have been shown as one of the causes ofunfairness and discrimination in the predictions of Machine Learning models.One of the most prominent types of demographic bias are statistical imbalancesin the representation of demographic groups in the datasets. In this paper, westudy the measurement of these biases by reviewing the existing metrics,including those that can be borrowed from other disciplines. We develop ataxonomy for the classification of these metrics, providing a practical guidefor the selection of appropriate metrics. To illustrate the utility of ourframework, and to further understand the practical characteristics of themetrics, we conduct a case study of 20 datasets used in Facial EmotionRecognition (FER), analyzing the biases present in them. Our experimentalresults show that many metrics are redundant and that a reduced subset ofmetrics may be sufficient to measure the amount of demographic bias. The paperprovides valuable insights for researchers in AI and related fields to mitigatedataset bias and improve the fairness and accuracy of AI models. The code isavailable at https://github.com/irisdominguez/dataset_bias_metrics.</description><author>Iris Dominguez-Catena, Daniel Paternain, Mikel Galar</author><pubDate>Tue, 25 Jun 2024 17:02:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15889v2</guid></item><item><title>GLAD: Improving Latent Graph Generative Modeling with Simple Quantization</title><link>http://arxiv.org/abs/2403.16883v2</link><description>Exploring the graph latent structures has not garnered much attention in thegraph generative research field. Yet, exploiting the latent space is as crucialas working on the data space for discrete data such as graphs. However,previous methods either failed to preserve the permutation symmetry of graphsor lacked an effective approaches to model appropriately within the latentspace. To mitigate those issues, we propose a simple, yet effective discretelatent graph diffusion generative model. Our model, namely GLAD, not onlyovercomes the drawbacks of existing latent approaches, but also alleviatesinherent issues present in diffusion methods applied on the graph space. Wevalidate our generative model on the molecular benchmark datasets, on which itdemonstrates competitive performance compared with the state-of-the-artbaselines.</description><author>Van Khoa Nguyen, Yoann Boget, Frantzeska Lavda, Alexandros Kalousis</author><pubDate>Tue, 25 Jun 2024 17:01:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16883v2</guid></item><item><title>Brain Tumor Classification using Vision Transformer with Selective Cross-Attention Mechanism and Feature Calibration</title><link>http://arxiv.org/abs/2406.17670v1</link><description>Brain tumor classification is a challenging task in medical image analysis.In this paper, we propose a novel approach to brain tumor classification usinga vision transformer with a novel cross-attention mechanism. Our approachleverages the strengths of transformers in modeling long-range dependencies andmulti-scale feature fusion. We introduce two new mechanisms to improve theperformance of the cross-attention fusion module: Feature Calibration Mechanism(FCM) and Selective Cross-Attention (SCA). FCM calibrates the features fromdifferent branches to make them more compatible, while SCA selectively attendsto the most informative features. Our experiments demonstrate that the proposedapproach outperforms other state-of-the-art methods in brain tumorclassification, achieving improved accuracy and efficiency. The proposed FCMand SCA mechanisms can be easily integrated into other vision transformerarchitectures, making them a promising direction for future research in medicalimage analysis. Experimental results confirm that our approach surpassesexisting methods, achieving state-of-the-art performance in brain tumorclassification tasks.</description><author>Mohammad Ali Labbaf Khaniki, Alireza Golkarieh, Mohammad Manthouri</author><pubDate>Tue, 25 Jun 2024 16:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17670v1</guid></item><item><title>This Paper Had the Smartest Reviewers -- Flattery Detection Utilising an Audio-Textual Transformer-Based Approach</title><link>http://arxiv.org/abs/2406.17667v1</link><description>Flattery is an important aspect of human communication that facilitatessocial bonding, shapes perceptions, and influences behavior through strategiccompliments and praise, leveraging the power of speech to build rapporteffectively. Its automatic detection can thus enhance the naturalness ofhuman-AI interactions. To meet this need, we present a novel audio textualdataset comprising 20 hours of speech and train machine learning models forautomatic flattery detection. In particular, we employ pretrained AST,Wav2Vec2, and Whisper models for the speech modality, and Whisper TTS modelscombined with a RoBERTa text classifier for the textual modality. Subsequently,we build a multimodal classifier by combining text and audio representations.Evaluation on unseen test data demonstrates promising results, with UnweightedAverage Recall scores reaching 82.46% in audio-only experiments, 85.97% intext-only experiments, and 87.16% using a multimodal approach.</description><author>Lukas Christ, Shahin Amiriparian, Friederike Hawighorst, Ann-Kathrin Schill, Angelo Boutalikakis, Lorenz Graf-Vlachy, Andreas König, Björn W. Schuller</author><pubDate>Tue, 25 Jun 2024 16:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17667v1</guid></item><item><title>LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic</title><link>http://arxiv.org/abs/2406.17663v1</link><description>We introduce LLM-ARC, a neuro-symbolic framework designed to enhance thelogical reasoning capabilities of Large Language Models (LLMs), by combiningthem with an Automated Reasoning Critic (ARC). LLM-ARC employs an Actor-Criticmethod where the LLM Actor generates declarative logic programs along withtests for semantic correctness, while the Automated Reasoning Critic evaluatesthe code, runs the tests and provides feedback on test failures for iterativerefinement. Implemented using Answer Set Programming (ASP), LLM-ARC achieves anew state-of-the-art accuracy of 88.32% on the FOLIO benchmark which testscomplex logical reasoning capabilities. Our experiments demonstrate significantimprovements over LLM-only baselines, highlighting the importance of logic testgeneration and iterative self-refinement. We achieve our best result using afully automated self-supervised training loop where the Actor is trained onend-to-end dialog traces with Critic feedback. We discuss potentialenhancements and provide a detailed error analysis, showcasing the robustnessand efficacy of LLM-ARC for complex natural language reasoning tasks.</description><author>Aditya Kalyanpur, Kailash Saravanakumar, Victor Barres, Jennifer Chu-Carroll, David Melville, David Ferrucci</author><pubDate>Tue, 25 Jun 2024 16:52:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17663v1</guid></item><item><title>Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients</title><link>http://arxiv.org/abs/2406.17660v1</link><description>Large language model (LLM) training and finetuning are often bottlenecked bylimited GPU memory. While existing projection-based optimization methodsaddress this by projecting gradients into a lower-dimensional subspace toreduce optimizer state memory, they typically rely on dense projectionmatrices, which can introduce computational and memory overheads. In this work,we propose Grass (GRAdient Stuctured Sparsification), a novel approach thatleverages sparse projections to transform gradients into structured sparseupdates. This design not only significantly reduces memory usage for optimizerstates but also minimizes gradient memory footprint, computation, andcommunication costs, leading to substantial throughput improvements. Extensiveexperiments on pretraining and finetuning tasks demonstrate that Grass achievescompetitive performance to full-rank training and existing projection-basedmethods. Notably, Grass enables half-precision pretraining of a 13B parameterLLaMA model on a single 40GB A100 GPU--a feat infeasible for previousmethods--and yields up to a $2\times$ throughput improvement on an 8-GPUsystem. Code can be found at https://github.com/aashiqmuhamed/GRASS .</description><author>Aashiq Muhamed, Oscar Li, David Woodruff, Mona Diab, Virginia Smith</author><pubDate>Tue, 25 Jun 2024 16:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17660v1</guid></item><item><title>DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning</title><link>http://arxiv.org/abs/2406.17659v1</link><description>Vision-language models (VLMs) have been applied to robot task planningproblems, where the robot receives a task in natural language and generatesplans based on visual inputs. While current VLMs have demonstrated strongvision-language understanding capabilities, their performance is still far frombeing satisfactory in planning tasks. At the same time, although classical taskplanners, such as PDDL-based, are strong in planning for long-horizon tasks,they do not work well in open worlds where unforeseen situations are common. Inthis paper, we propose a novel task planning and execution framework, calledDKPROMPT, which automates VLM prompting using domain knowledge in PDDL forclassical planning in open worlds. Results from quantitative experiments showthat DKPROMPT outperforms classical planning, pure VLM-based and a few othercompetitive baselines in task completion rate.</description><author>Xiaohan Zhang, Zainab Altaweel, Yohei Hayamizu, Yan Ding, Saeid Amiri, Hao Yang, Andy Kaminski, Chad Esselink, Shiqi Zhang</author><pubDate>Tue, 25 Jun 2024 16:49:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17659v1</guid></item><item><title>MDHA: Multi-Scale Deformable Transformer with Hybrid Anchors for Multi-View 3D Object Detection</title><link>http://arxiv.org/abs/2406.17654v1</link><description>Multi-view 3D object detection is a crucial component of autonomous drivingsystems. Contemporary query-based methods primarily depend either ondataset-specific initialization of 3D anchors, introducing bias, or utilizedense attention mechanisms, which are computationally inefficient andunscalable. To overcome these issues, we present MDHA, a novel sparsequery-based framework, which constructs adaptive 3D output proposals usinghybrid anchors from multi-view, multi-scale input. Fixed 2D anchors arecombined with depth predictions to form 2.5D anchors, which are projected toobtain 3D proposals. To ensure high efficiency, our proposed Anchor Encoderperforms sparse refinement and selects the top-k anchors and features.Moreover, while existing multi-view attention mechanisms rely on projectingreference points to multiple images, our novel Circular Deformable Attentionmechanism only projects to a single image but allows reference points toseamlessly attend to adjacent images, improving efficiency without compromisingon performance. On the nuScenes val set, it achieves 46.4% mAP and 55.0% NDSwith a ResNet101 backbone. MDHA significantly outperforms the baseline, whereanchor proposals are modelled as learnable embeddings.</description><author>Michelle Adeline, Junn Yong Loo, Vishnu Monn Baskaran</author><pubDate>Tue, 25 Jun 2024 16:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17654v1</guid></item><item><title>Time-varying Extremum Graphs</title><link>http://arxiv.org/abs/2406.17652v1</link><description>We introduce time-varying extremum graph (TVEG), a topological structure tosupport visualization and analysis of a time-varying scalar field. The extremumgraph is a substructure of the Morse-Smale complex. It captures the adjacencyrelationship between cells in the Morse decomposition of a scalar field. Wedefine the TVEG as a time-varying extension of the extremum graph anddemonstrate how it captures salient feature tracks within a dynamic scalarfield. We formulate the construction of the TVEG as an optimization problem anddescribe an algorithm for computing the graph. We also demonstrate thecapabilities of \TVEG towards identification and exploration of topologicalevents such as deletion, generation, split, and merge within a dynamic scalarfield via comprehensive case studies including a viscous fingers and a 3D vonK\'arm\'an vortex street dataset.</description><author>Somenath Das, Raghavendra Sridharamurthy, Vijay Natarajan</author><pubDate>Tue, 25 Jun 2024 16:43:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17652v1</guid></item><item><title>Leveraging Large Language Models for Software Model Completion: Results from Industrial and Public Datasets</title><link>http://arxiv.org/abs/2406.17651v1</link><description>Modeling structure and behavior of software systems plays a crucial role inthe industrial practice of software engineering. As with other softwareengineering artifacts, software models are subject to evolution. Supportingmodelers in evolving software models with recommendations for model completionsis still an open problem, though. In this paper, we explore the potential oflarge language models for this task. In particular, we propose an approach,retrieval-augmented generation, leveraging large language models, modelhistories, and retrieval-augmented generation for model completion. Throughexperiments on three datasets, including an industrial application, one publicopen-source community dataset, and one controlled collection of simulated modelrepositories, we evaluate the potential of large language models for modelcompletion with retrieval-augmented generation. We found that large languagemodels are indeed a promising technology for supporting software modelevolution (62.30% semantically correct completions on real-world industrialdata and up to 86.19% type-correct completions). The general inferencecapabilities of large language models are particularly useful when dealing withconcepts for which there are few, noisy, or no examples at all.</description><author>Christof Tinnes, Alisa Welter, Sven Apel</author><pubDate>Tue, 25 Jun 2024 16:43:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17651v1</guid></item><item><title>ELIZA Reinterpreted: The world's first chatbot was not intended as a chatbot at all</title><link>http://arxiv.org/abs/2406.17650v1</link><description>ELIZA, often considered the world's first chatbot, was written by JosephWeizenbaum in the early 1960s. Weizenbaum did not intend to invent the chatbot,but rather to build a platform for research into human-machine conversation andthe important cognitive processes of interpretation and misinterpretation. Hispurpose was obscured by ELIZA's fame, resulting in large part from thefortuitous timing of it's creation, and it's escape into the wild. In thispaper I provide a rich historical context for ELIZA's creation, demonstratingthat ELIZA arose from the intersection of some of the central threads in thetechnical history of AI. I also briefly discuss how ELIZA escaped into theworld, and how its accidental escape, along with several coincidental turns ofthe programming language screws, led both to the misapprehension that ELIZA wasintended as a chatbot, and to the loss of the original ELIZA to history forover 50 years.</description><author>Jeff Shrager</author><pubDate>Tue, 25 Jun 2024 16:41:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17650v1</guid></item><item><title>Privacy Preserving Reinforcement Learning for Population Processes</title><link>http://arxiv.org/abs/2406.17649v1</link><description>We consider the problem of privacy protection in Reinforcement Learning (RL)algorithms that operate over population processes, a practical but understudiedsetting that includes, for example, the control of epidemics in largepopulations of dynamically interacting individuals. In this setting, the RLalgorithm interacts with the population over $T$ time steps by receivingpopulation-level statistics as state and performing actions which can affectthe entire population at each time step. An individual's data can be collectedacross multiple interactions and their privacy must be protected at all times.We clarify the Bayesian semantics of Differential Privacy (DP) in the presenceof correlated data in population processes through a Pufferfish Privacyanalysis. We then give a meta algorithm that can take any RL algorithm as inputand make it differentially private. This is achieved by taking an approach thatuses DP mechanisms to privatize the state and reward signal at each time stepbefore the RL algorithm receives them as input. Our main theoretical resultshows that the value-function approximation error when applying standard RLalgorithms directly to the privatized states shrinks quickly as the populationsize and privacy budget increase. This highlights that reasonableprivacy-utility trade-offs are possible for differentially private RLalgorithms in population processes. Our theoretical findings are validated byexperiments performed on a simulated epidemic control problem over largepopulation sizes.</description><author>Samuel Yang-Zhao, Kee Siong Ng</author><pubDate>Tue, 25 Jun 2024 16:41:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17649v1</guid></item><item><title>Variationist: Exploring Multifaceted Variation and Bias in Written Language Data</title><link>http://arxiv.org/abs/2406.17647v1</link><description>Exploring and understanding language data is a fundamental stage in all areasdealing with human language. It allows NLP practitioners to uncover qualityconcerns and harmful biases in data before training, and helps linguists andsocial scientists to gain insight into language use and human behavior. Yet,there is currently a lack of a unified, customizable tool to seamlessly inspectand visualize language variation and bias across multiple variables, languageunits, and diverse metrics that go beyond descriptive statistics. In thispaper, we introduce Variationist, a highly-modular, extensible, andtask-agnostic tool that fills this gap. Variationist handles at once apotentially unlimited combination of variable types and semantics acrossdiversity and association metrics with regards to the language unit of choice,and orchestrates the creation of up to five-dimensional interactive charts forover 30 variable type-semantics combinations. Through our case studies oncomputational dialectology, human label variation, and text generation, we showhow Variationist enables researchers from different disciplines to effortlesslyanswer specific research questions or unveil undesired associations in languagedata. A Python library, code, documentation, and tutorials are made publiclyavailable to the research community.</description><author>Alan Ramponi, Camilla Casula, Stefano Menini</author><pubDate>Tue, 25 Jun 2024 16:41:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17647v1</guid></item><item><title>Banishing LLM Hallucinations Requires Rethinking Generalization</title><link>http://arxiv.org/abs/2406.17642v1</link><description>Despite their powerful chat, coding, and reasoning abilities, Large LanguageModels (LLMs) frequently hallucinate. Conventional wisdom suggests thathallucinations are a consequence of a balance between creativity andfactuality, which can be mitigated, but not eliminated, by grounding the LLM inexternal knowledge sources. Through extensive systematic experiments, we showthat these traditional approaches fail to explain why LLMs hallucinate inpractice. Specifically, we show that LLMs augmented with a massive Mixture ofMemory Experts (MoME) can easily memorize large datasets of random numbers. Wecorroborate these experimental findings with a theoretical construction showingthat simple neural networks trained to predict the next token hallucinate whenthe training loss is above a threshold as it usually does in practice whentraining on internet scale data. We interpret our findings by comparing againsttraditional retrieval methods for mitigating hallucinations. We use ourfindings to design a first generation model for removing hallucinations --Lamini-1 -- that stores facts in a massive mixture of millions of memoryexperts that are retrieved dynamically.</description><author>Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui, Yuxin Ye, Nithyashree Manohar, Zhuxiaona Wei, Tian Wu, Ben Echols, Sharon Zhou, Gregory Diamos</author><pubDate>Tue, 25 Jun 2024 16:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17642v1</guid></item><item><title>BayTTA: Uncertainty-aware medical image classification with optimized test-time augmentation using Bayesian model averaging</title><link>http://arxiv.org/abs/2406.17640v1</link><description>Test-time augmentation (TTA) is a well-known technique employed during thetesting phase of computer vision tasks. It involves aggregating multipleaugmented versions of input data. Combining predictions using a simple averageformulation is a common and straightforward approach after performing TTA. Thispaper introduces a novel framework for optimizing TTA, called BayTTA(Bayesian-based TTA), which is based on Bayesian Model Averaging (BMA). First,we generate a model list associated with different variations of the input datacreated through TTA. Then, we use BMA to combine model predictions weighted bytheir respective posterior probabilities. Such an approach allows one to takeinto account model uncertainty, and thus to enhance the predictive performanceof the related machine learning or deep learning model. We evaluate theperformance of BayTTA on various public data, including three medical imagedatasets comprising skin cancer, breast cancer, and chest X-ray images and twowell-known gene editing datasets, CRISPOR and GUIDE-seq. Our experimentalresults indicate that BayTTA can be effectively integrated intostate-of-the-art deep learning models used in medical image analysis as well asinto some popular pre-trained CNN models such as VGG-16, MobileNetV2,DenseNet201, ResNet152V2, and InceptionRes-NetV2, leading to the enhancement intheir accuracy and robustness performance.</description><author>Zeinab Sherkatghanad, Moloud Abdar, Mohammadreza Bakhtyari, Vladimir Makarenkov</author><pubDate>Tue, 25 Jun 2024 16:24:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17640v1</guid></item><item><title>Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP</title><link>http://arxiv.org/abs/2406.17639v1</link><description>Contrastive Language--Image Pre-training (CLIP) has manifested remarkableimprovements in zero-shot classification and cross-modal vision-language tasks.Yet, from a geometrical point of view, the CLIP embedding space has been foundto have a pronounced modality gap. This gap renders the embedding space overlysparse and disconnected, with different modalities being densely distributed indistinct subregions of the hypersphere. In this work, we aim at answering twomain questions: 1. Does sharing the parameter space between the multi-modalencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apartthe uni-modal embeddings via intra-modality separation? We design AlignCLIP, inorder to answer these questions and show that answers to both questions arepositive. Through extensive experiments, we show that AlignCLIP achievesnoticeable enhancements in the cross-modal alignment of the embeddings, andthereby, reduces the modality gap, while maintaining the performance acrossseveral downstream evaluations, such as zero-shot image classification,zero-shot multi-modal retrieval and zero-shot semantic text similarity.</description><author>Sedigheh Eslami, Gerard de Melo</author><pubDate>Tue, 25 Jun 2024 16:24:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17639v1</guid></item><item><title>Aligning Diffusion Models with Noise-Conditioned Perception</title><link>http://arxiv.org/abs/2406.17636v1</link><description>Recent advancements in human preference optimization, initially developed forLanguage Models (LMs), have shown promise for text-to-image Diffusion Models,enhancing prompt alignment, visual appeal, and user preference. Unlike LMs,Diffusion Models typically optimize in pixel or VAE space, which does not alignwell with human perception, leading to slower and less efficient trainingduring the preference alignment stage. We propose using a perceptual objectivein the U-Net embedding space of the diffusion model to address these issues.Our approach involves fine-tuning Stable Diffusion 1.5 and XL using DirectPreference Optimization (DPO), Contrastive Preference Optimization (CPO), andsupervised fine-tuning (SFT) within this embedding space. This methodsignificantly outperforms standard latent-space implementations across variousmetrics, including quality and computational cost. For SDXL, our approachprovides 60.8\% general preference, 62.2\% visual appeal, and 52.1\% promptfollowing against original open-sourced SDXL-DPO on the PartiPrompts dataset,while significantly reducing compute. Our approach not only improves theefficiency and quality of human preference alignment for diffusion models butis also easily integrable with other optimization techniques. The training codeand LoRA weights will be available here:https://huggingface.co/alexgambashidze/SDXL\_NCP-DPO\_v0.1</description><author>Alexander Gambashidze, Anton Kulikov, Yuriy Sosnin, Ilya Makarov</author><pubDate>Tue, 25 Jun 2024 16:21:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17636v1</guid></item><item><title>Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels</title><link>http://arxiv.org/abs/2406.17633v1</link><description>Computational social science (CSS) practitioners often rely on human-labeleddata to fine-tune supervised text classifiers. We assess the potential forresearchers to augment or replace human-generated training data with surrogatetraining labels from generative large language models (LLMs). We introduce arecommended workflow and test this LLM application by replicating 14classification tasks and measuring performance. We employ a novel corpus ofEnglish-language text classification data sets from recent CSS articles inhigh-impact journals. Because these data sets are stored in password-protectedarchives, our analyses are less prone to issues of contamination. For eachtask, we compare supervised classifiers fine-tuned using GPT-4 labels againstclassifiers fine-tuned with human annotations and against labels from GPT-4 andMistral-7B with few-shot in-context learning. Our findings indicate thatsupervised classification models fine-tuned on LLM-generated labels performcomparably to models fine-tuned with labels from human annotators. Fine-tuningmodels using LLM-generated labels can be a fast, efficient and cost-effectivemethod of building supervised text classifiers.</description><author>Nicholas Pangakis, Samuel Wolken</author><pubDate>Tue, 25 Jun 2024 16:20:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17633v1</guid></item><item><title>KANQAS: Kolmogorov Arnold Network for Quantum Architecture Search</title><link>http://arxiv.org/abs/2406.17630v1</link><description>Quantum architecture search~(QAS) is a promising direction for optimizationand automated design of quantum circuits towards quantum advantage. Recenttechniques in QAS focus on machine learning-based approaches from reinforcementlearning, like deep Q-network. While multi-layer perceptron-based deepQ-networks have been applied for QAS, their interpretability remainschallenging due to the high number of parameters. In this work, we evaluate thepracticality of KANs in quantum architecture search problems, analyzing theirefficiency in terms of the probability of success, frequency of optimalsolutions and their dependencies on various degrees of freedom of the network.In a noiseless scenario, the probability of success and the number of optimalquantum circuit configurations to generate the multi-qubit maximally entangledstates are significantly higher than MLPs. Moreover in noisy scenarios, KAN canachieve a better fidelity in approximating maximally entangled state than MLPs,where the performance of the MLP significantly depends on the choice ofactivation function. Further investigation reveals that KAN requires a verysmall number of learnable parameters compared to MLPs, however, the averagetime of executing each episode for KAN is much higher.</description><author>Akash Kundu, Aritra Sarkar, Abhishek Sadhu</author><pubDate>Tue, 25 Jun 2024 16:17:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17630v1</guid></item><item><title>Controlling Moments with Kernel Stein Discrepancies</title><link>http://arxiv.org/abs/2211.05408v4</link><description>Kernel Stein discrepancies (KSDs) measure the quality of a distributionalapproximation and can be computed even when the target density has anintractable normalizing constant. Notable applications include the diagnosis ofapproximate MCMC samplers and goodness-of-fit tests for unnormalizedstatistical models. The present work analyzes the convergence controlproperties of KSDs. We first show that standard KSDs used for weak convergencecontrol fail to control moment convergence. To address this limitation, we nextprovide sufficient conditions under which alternative diffusion KSDs controlboth moment and weak convergence. As an immediate consequence we develop, foreach $q &gt; 0$, the first KSDs known to exactly characterize $q$-Wassersteinconvergence.</description><author>Heishiro Kanagawa, Alessandro Barp, Arthur Gretton, Lester Mackey</author><pubDate>Tue, 25 Jun 2024 16:16:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.05408v4</guid></item><item><title>Video Inpainting Localization with Contrastive Learning</title><link>http://arxiv.org/abs/2406.17628v1</link><description>Deep video inpainting is typically used as malicious manipulation to removeimportant objects for creating fake videos. It is significant to identify theinpainted regions blindly. This letter proposes a simple yet effective forensicscheme for Video Inpainting LOcalization with ContrAstive Learning (ViLocal).Specifically, a 3D Uniformer encoder is applied to the video noise residual forlearning effective spatiotemporal forensic features. To enhance thediscriminative power, supervised contrastive learning is adopted to capture thelocal inconsistency of inpainted videos through attracting/repelling thepositive/negative pristine and forged pixel pairs. A pixel-wise inpaintinglocalization map is yielded by a lightweight convolution decoder with aspecialized two-stage training strategy. To prepare enough training samples, webuild a video object segmentation dataset of 2500 videos with pixel-levelannotations per frame. Extensive experimental results validate the superiorityof ViLocal over state-of-the-arts. Code and dataset will be available athttps://github.com/multimediaFor/ViLocal.</description><author>Zijie Lou, Gang Cao, Man Lin</author><pubDate>Tue, 25 Jun 2024 16:15:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17628v1</guid></item><item><title>Querying Labeled Time Series Data with Scenario Programs</title><link>http://arxiv.org/abs/2406.17627v1</link><description>In order to ensure autonomous vehicles are safe for on-road deployment,simulation-based testing has become an integral complement to on-road testing.The rise in simulation testing and validation reflects a growing need to verifythat AV behavior is consistent with desired outcomes even in edge casescenarios $-$ which may seldom or never appear in on-road testing data. Thisraises a critical question: to what extent are AV failures in simulationconsistent with data collected from real-world testing? As a result of the gapbetween simulated and real sensor data (sim-to-real gap), failures insimulation can either be spurious (simulation- or simulator-specific issues) orrelevant (safety-critical AV system issues). One possible method for validatingif simulated time series failures are consistent with real world time seriessensor data could involve retrieving instances of the failure scenario from areal-world time series dataset, in order to understand AV performance in thesescenarios. Adopting this strategy, we propose a formal definition of whatconstitutes a match between a real-world labeled time series data item and asimulated scenario written from a fragment of the Scenic probabilisticprogramming language for simulation generation. With this definition of amatch, we develop a querying algorithm that identifies the subset of a labeledtime series dataset matching a given scenario. To allow this approach to beused to verify the safety of other cyber-physical systems (CPS), we present adefinition and algorithm for matching scalable beyond the autonomous vehiclesdomain. Experiments demonstrate the precision and scalability of the algorithmfor a set of challenging and uncommon time series scenarios identified from thenuScenes autonomous driving dataset. We include a full system implementation ofthe querying algorithm freely available for use across a wide range of CPS.</description><author>Devan Shanker</author><pubDate>Tue, 25 Jun 2024 16:15:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17627v1</guid></item><item><title>Insights into the Lottery Ticket Hypothesis and Iterative Magnitude Pruning</title><link>http://arxiv.org/abs/2403.15022v3</link><description>Lottery ticket hypothesis for deep neural networks emphasizes the importanceof initialization used to re-train the sparser networks obtained using theiterative magnitude pruning process. An explanation for why the specificinitialization proposed by the lottery ticket hypothesis tends to work betterin terms of generalization (and training) performance has been lacking.Moreover, the underlying principles in iterative magnitude pruning, like thepruning of smaller magnitude weights and the role of the iterative process,lack full understanding and explanation. In this work, we attempt to provideinsights into these phenomena by empirically studying the volume/geometry andloss landscape characteristics of the solutions obtained at various stages ofthe iterative magnitude pruning process.</description><author>Tausifa Jan Saleem, Ramanjit Ahuja, Surendra Prasad, Brejesh Lall</author><pubDate>Tue, 25 Jun 2024 16:14:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15022v3</guid></item><item><title>CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference</title><link>http://arxiv.org/abs/2406.17626v1</link><description>As large language models (LLMs) constantly evolve, ensuring their safetyremains a critical research problem. Previous red-teaming approaches for LLMsafety have primarily focused on single prompt attacks or goal hijacking. Tothe best of our knowledge, we are the first to study LLM safety in multi-turndialogue coreference. We created a dataset of 1,400 questions across 14categories, each featuring multi-turn coreference safety attacks. We thenconducted detailed evaluations on five widely used open-source LLMs. Theresults indicated that under multi-turn coreference safety attacks, the highestattack success rate was 56% with the LLaMA2-Chat-7b model, while the lowest was13.9% with the Mistral-7B-Instruct model. These findings highlight the safetyvulnerabilities in LLMs during dialogue coreference interactions.</description><author>Erxin Yu, Jing Li, Ming Liao, Siqi Wang, Zuchen Gao, Fei Mi, Lanqing Hong</author><pubDate>Tue, 25 Jun 2024 16:13:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17626v1</guid></item><item><title>iWISDM: Assessing instruction following in multimodal models at scale</title><link>http://arxiv.org/abs/2406.14343v3</link><description>The ability to perform complex tasks from detailed instructions is a key tomany remarkable achievements of our species. As humans, we are not only capableof performing a wide variety of tasks but also very complex ones that mayentail hundreds or thousands of steps to complete. Large language models andtheir more recent multimodal counterparts that integrate textual and visualinputs have achieved unprecedented success in performing complex tasks. Yet,most existing benchmarks are largely confined to single-modality inputs (eithertext or vision), narrowing the scope of multimodal assessments, particularlyfor instruction-following in multimodal contexts. To bridge this gap, weintroduce the instructed-Virtual VISual Decision Making (iWISDM) environmentengineered to generate a limitless array of vision-language tasks of varyingcomplexity. Using iWISDM, we compiled three distinct benchmarks of instructionfollowing visual tasks across varying complexity levels and evaluated severalnewly developed multimodal models on these benchmarks. Our findings establishiWISDM as a robust benchmark for assessing the instructional adherence of bothexisting and emergent multimodal models and highlight a large gap between thesemodels' ability to precisely follow instructions with that of humans.The codeof iWISDM is available on GitHub at https://github.com/BashivanLab/iWISDM.</description><author>Xiaoxuan Lei, Lucas Gomez, Hao Yuan Bai, Pouya Bashivan</author><pubDate>Tue, 25 Jun 2024 16:12:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14343v3</guid></item><item><title>Self-assessment, Exhibition, and Recognition: a Review of Personality in Large Language Models</title><link>http://arxiv.org/abs/2406.17624v1</link><description>As large language models (LLMs) appear to behave increasingly human-like intext-based interactions, more and more researchers become interested ininvestigating personality in LLMs. However, the diversity of psychologicalpersonality research and the rapid development of LLMs have led to a broad yetfragmented landscape of studies in this interdisciplinary field. Extensivestudies across different research focuses, different personality psychometrics,and different LLMs make it challenging to have a holistic overview and furtherpose difficulties in applying findings to real-world applications. In thispaper, we present a comprehensive review by categorizing current studies intothree research problems: self-assessment, exhibition, and recognition, based onthe intrinsic characteristics and external manifestations of personality inLLMs. For each problem, we provide a thorough analysis and conduct in-depthcomparisons of their corresponding solutions. Besides, we summarize researchfindings and open challenges from current studies and further discuss theirunderlying causes. We also collect extensive publicly available resources tofacilitate interested researchers and developers. Lastly, we discuss thepotential future research directions and application scenarios. Our paper isthe first comprehensive survey of up-to-date literature on personality in LLMs.By presenting a clear taxonomy, in-depth analysis, promising future directions,and extensive resource collections, we aim to provide a better understandingand facilitate further advancements in this emerging field.</description><author>Zhiyuan Wen, Yu Yang, Jiannong Cao, Haoming Sun, Ruosong Yang, Shuaiqi Liu</author><pubDate>Tue, 25 Jun 2024 16:08:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17624v1</guid></item><item><title>Fundamental Bounds on Online Strategic Classification</title><link>http://arxiv.org/abs/2302.12355v2</link><description>We study the problem of online binary classification where strategic agentscan manipulate their observable features in predefined ways, modeled by amanipulation graph, in order to receive a positive classification. We show thissetting differs in fundamental ways from non-strategic online classification.For instance, whereas in the non-strategic case, a mistake bound of $\ln|H|$ isachievable via the halving algorithm when the target function belongs to aknown class $H$, we show that no deterministic algorithm can achieve a mistakebound $o(\Delta)$ in the strategic setting, where $\Delta$ is the maximumdegree of the manipulation graph (even when $|H|=O(\Delta)$). We obtain analgorithm achieving mistake bound $O(\Delta\ln|H|)$. We also extend this to theagnostic setting and obtain an algorithm with a $\Delta$ multiplicative regret,and we show no deterministic algorithm can achieve $o(\Delta)$ multiplicativeregret. Next, we study two randomized models based on whether the random choices aremade before or after agents respond, and show they exhibit fundamentaldifferences. In the first model, at each round the learner deterministicallychooses a probability distribution over classifiers inducing expected values oneach vertex (probabilities of being classified as positive), which thestrategic agents respond to. We show that any learner in this model has tosuffer linear regret. On the other hand, in the second model, while theadversary who selects the next agent must respond to the learner's probabilitydistribution over classifiers, the agent then responds to the actual hypothesisclassifier drawn from this distribution. Surprisingly, we show this model ismore advantageous to the learner, and we design randomized algorithms thatachieve sublinear regret bounds against both oblivious and adaptiveadversaries.</description><author>Saba Ahmadi, Avrim Blum, Kunhe Yang</author><pubDate>Tue, 25 Jun 2024 16:06:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12355v2</guid></item><item><title>Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG</title><link>http://arxiv.org/abs/2406.13069v2</link><description>How novel are texts generated by language models (LMs) relative to theirtraining corpora? In this work, we investigate the extent to which modern LMsgenerate $n$-grams from their training data, evaluating both (i) theprobability LMs assign to complete training $n$-grams and (ii) $n$-novelty, theproportion of $n$-grams generated by an LM that did not appear in the trainingdata (for arbitrarily large $n$). To enable arbitrary-length $n$-gram searchover a corpus in constant time, we develop Rusty-DAWG, a novel search toolinspired by indexing of genomic data. We compare the novelty of LM-generatedtext to human-written text and explore factors that affect generation novelty,focusing on the Pythia models. We find that, for $n &gt; 4$, LM-generated text isless novel than human-written text, though it is more novel for smaller $n$.Larger LMs and more constrained decoding strategies both decrease novelty.Finally, we show that LMs complete $n$-grams with lower loss if they are morefrequent in the training data. Overall, our results reveal factors influencingthe novelty of LM-generated text, and we release Rusty-DAWG to facilitatefurther pretraining data research.</description><author>William Merrill, Noah A. Smith, Yanai Elazar</author><pubDate>Tue, 25 Jun 2024 16:02:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13069v2</guid></item><item><title>Towards Building an End-to-End Multilingual Automatic Lyrics Transcription Model</title><link>http://arxiv.org/abs/2406.17618v1</link><description>Multilingual automatic lyrics transcription (ALT) is a challenging task dueto the limited availability of labelled data and the challenges introduced bysinging, compared to multilingual automatic speech recognition. Although somemultilingual singing datasets have been released recently, English continues todominate these collections. Multilingual ALT remains underexplored due to thescale of data and annotation quality. In this paper, we aim to create amultilingual ALT system with available datasets. Inspired by architectures thathave been proven effective for English ALT, we adapt these techniques to themultilingual scenario by expanding the target vocabulary set. We then evaluatethe performance of the multilingual model in comparison to its monolingualcounterparts. Additionally, we explore various conditioning methods toincorporate language information into the model. We apply analysis by languageand combine it with the language classification performance. Our findingsreveal that the multilingual model performs consistently better than themonolingual models trained on the language subsets. Furthermore, we demonstratethat incorporating language information significantly enhances performance.</description><author>Jiawen Huang, Emmanouil Benetos</author><pubDate>Tue, 25 Jun 2024 16:02:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17618v1</guid></item><item><title>Embedded event based object detection with spiking neural network</title><link>http://arxiv.org/abs/2406.17617v1</link><description>The complexity of event-based object detection (OD) poses considerablechallenges. Spiking Neural Networks (SNNs) show promising results and pave theway for efficient event-based OD. Despite this success, the path to efficientSNNs on embedded devices remains a challenge. This is due to the size of thenetworks required to accomplish the task and the ability of devices to takeadvantage of SNNs benefits. Even when "edge" devices are considered, theytypically use embedded GPUs that consume tens of watts. In response to thesechallenges, our research introduces an embedded neuromorphic testbench thatutilizes the SPiking Low-power Event-based ArchiTecture (SPLEAT) accelerator.Using an extended version of the Qualia framework, we can train, evaluate,quantize, and deploy spiking neural networks on an FPGA implementation ofSPLEAT. We used this testbench to load a state-of-the-art SNN solution,estimate the performance loss associated with deploying the network ondedicated hardware, and run real-world event-based OD on neuromorphic hardwarespecifically designed for low-power spiking neural networks. Remarkably, ourembedded spiking solution, which includes a model with 1.08 million parameters,operates efficiently with 490 mJ per prediction.</description><author>Jonathan Courtois, Pierre-Emmanuel Novac, Edgar Lemaire, Alain Pegatoquet, Benoit Miramond</author><pubDate>Tue, 25 Jun 2024 16:02:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17617v1</guid></item><item><title>Aligning Programming Language and Natural Language: Exploring Design Choices in Multi-Modal Transformer-Based Embedding for Bug Localization</title><link>http://arxiv.org/abs/2406.17615v1</link><description>Bug localization refers to the identification of source code files which isin a programming language and also responsible for the unexpected behavior ofsoftware using the bug report, which is a natural language. As bug localizationis labor-intensive, bug localization models are employed to assist softwaredevelopers. Due to the domain difference between source code files and bugreports, modern bug-localization systems, based on deep learning models, relyheavily on embedding techniques that project bug reports and source code filesinto a shared vector space. The creation of an embedding involves severaldesign choices, but the impact of these choices on the quality of embedding andthe performance of bug localization models remains unexplained in currentresearch. To address this gap, our study evaluated 14 distinct embedding models to gaininsights into the effects of various design choices. Subsequently, we developedbug localization models utilizing these embedding models to assess theinfluence of these choices on the performance of the localization models. Ourfindings indicate that the pre-training strategies significantly affect thequality of the embedding. Moreover, we discovered that the familiarity of theembedding models with the data has a notable impact on the bug localizationmodel's performance. Notably, when the training and testing data are collectedfrom different projects, the performance of the bug localization modelsexhibits substantial fluctuations.</description><author>Partha Chakraborty, Venkatraman Arumugam, Meiyappan Nagappan</author><pubDate>Tue, 25 Jun 2024 16:01:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17615v1</guid></item><item><title>MSRS: Training Multimodal Speech Recognition Models from Scratch with Sparse Mask Optimization</title><link>http://arxiv.org/abs/2406.17614v1</link><description>Pre-trained models have been a foundational approach in speech recognition,albeit with associated additional costs. In this study, we propose aregularization technique that facilitates the training of visual andaudio-visual speech recognition models (VSR and AVSR) from scratch. Thisapproach, abbreviated as \textbf{MSRS} (Multimodal Speech Recognition fromScratch), introduces a sparse regularization that rapidly learns sparsestructures within the dense model at the very beginning of training, whichreceives healthier gradient flow than the dense equivalent. Once the sparsemask stabilizes, our method allows transitioning to a dense model or keeping asparse model by updating non-zero values. MSRS achieves competitive results inVSR and AVSR with 21.1% and 0.9% WER on the LRS3 benchmark, while reducingtraining time by at least 2x. We explore other sparse approaches and show thatonly MSRS enables training from scratch by implicitly masking the weightsaffected by vanishing gradients.</description><author>Adriana Fernandez-Lopez, Honglie Chen, Pingchuan Ma, Lu Yin, Qiao Xiao, Stavros Petridis, Shiwei Liu, Maja Pantic</author><pubDate>Tue, 25 Jun 2024 16:00:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17614v1</guid></item><item><title>Distributed Training of Large Graph Neural Networks with Variable Communication Rates</title><link>http://arxiv.org/abs/2406.17611v1</link><description>Training Graph Neural Networks (GNNs) on large graphs presents uniquechallenges due to the large memory and computing requirements. Distributed GNNtraining, where the graph is partitioned across multiple machines, is a commonapproach to training GNNs on large graphs. However, as the graph cannotgenerally be decomposed into small non-interacting components, datacommunication between the training machines quickly limits training speeds.Compressing the communicated node activations by a fixed amount improves thetraining speeds, but lowers the accuracy of the trained GNN. In this paper, weintroduce a variable compression scheme for reducing the communication volumein distributed GNN training without compromising the accuracy of the learnedmodel. Based on our theoretical analysis, we derive a variable compressionmethod that converges to a solution equivalent to the full communication case,for all graph partitioning schemes. Our empirical results show that our methodattains a comparable performance to the one obtained with full communication.We outperform full communication at any fixed compression ratio for anycommunication budget.</description><author>Juan Cervino, Md Asadullah Turja, Hesham Mostafa, Nageen Himayat, Alejandro Ribeiro</author><pubDate>Tue, 25 Jun 2024 15:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17611v1</guid></item></channel></rss>