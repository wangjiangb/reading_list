<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 06 Mar 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation</title><link>http://arxiv.org/abs/2403.03221v1</link><description>Estimating relative camera poses between images has been a central problem incomputer vision. Methods that find correspondences and solve for thefundamental matrix offer high precision in most cases. Conversely, methodspredicting pose directly using neural networks are more robust to limitedoverlap and can infer absolute translation scale, but at the expense of reducedprecision. We show how to combine the best of both methods; our approach yieldsresults that are both precise and robust, while also accurately inferringtranslation scales. At the heart of our model lies a Transformer that (1)learns to balance between solved and learned pose estimations, and (2) providesa prior to guide a solver. A comprehensive analysis supports our design choicesand demonstrates that our method adapts flexibly to various feature extractorsand correspondence estimators, showing state-of-the-art performance in 6DoFpose estimation on Matterport3D, InteriorNet, StreetLearn, and Map-freeRelocalization.</description><author>Chris Rockwell, Nilesh Kulkarni, Linyi Jin, Jeong Joon Park, Justin Johnson, David F. Fouhey</author><pubDate>Tue, 05 Mar 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03221v1</guid></item><item><title>LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits</title><link>http://arxiv.org/abs/2403.03219v1</link><description>This study considers the linear contextual bandit problem with independentand identically distributed (i.i.d.) contexts. In this problem, existingstudies have proposed Best-of-Both-Worlds (BoBW) algorithms whose regretssatisfy $O(\log^2(T))$ for the number of rounds $T$ in a stochastic regime witha suboptimality gap lower-bounded by a positive constant, while satisfying$O(\sqrt{T})$ in an adversarial regime. However, the dependency on $T$ has roomfor improvement, and the suboptimality-gap assumption can be relaxed. For thisissue, this study proposes an algorithm whose regret satisfies $O(\log(T))$ inthe setting when the suboptimality gap is lower-bounded. Furthermore, weintroduce a margin condition, a milder assumption on the suboptimality gap.That condition characterizes the problem difficulty linked to the suboptimalitygap using a parameter $\beta \in (0, \infty]$. We then show that thealgorithm's regret satisfies$O\left(\left\{\log(T)\right\}^{\frac{1+\beta}{2+\beta}}T^{\frac{1}{2+\beta}}\right)$.Here, $\beta= \infty$ corresponds to the case in the existing studies where alower bound exists in the suboptimality gap, and our regret satisfies$O(\log(T))$ in that case. Our proposed algorithm is based on theFollow-The-Regularized-Leader with the Tsallis entropy and referred to as the$\alpha$-Linear-Contextual (LC)-Tsallis-INF.</description><author>Masahiro Kato, Shinji Ito</author><pubDate>Tue, 05 Mar 2024 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03219v1</guid></item><item><title>The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning</title><link>http://arxiv.org/abs/2403.03218v1</link><description>The White House Executive Order on Artificial Intelligence highlights therisks of large language models (LLMs) empowering malicious actors in developingbiological, cyber, and chemical weapons. To measure these risks of malicioususe, government institutions and major AI labs are developing evaluations forhazardous capabilities in LLMs. However, current evaluations are private,preventing further research into mitigating risk. Furthermore, they focus ononly a few, highly specific pathways for malicious use. To fill these gaps, wepublicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, adataset of 4,157 multiple-choice questions that serve as a proxy measurement ofhazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDPwas developed by a consortium of academics and technical consultants, and wasstringently filtered to eliminate sensitive information prior to publicrelease. WMDP serves two roles: first, as an evaluation for hazardous knowledgein LLMs, and second, as a benchmark for unlearning methods to remove suchhazardous knowledge. To guide progress on unlearning, we develop CUT, astate-of-the-art unlearning method based on controlling model representations.CUT reduces model performance on WMDP while maintaining general capabilities inareas such as biology and computer science, suggesting that unlearning may be aconcrete path towards reducing malicious use from LLMs. We release ourbenchmark and code publicly at https://wmdp.ai</description><author>Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-Burger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Ariel Herbert-Voss, Cort B. Breuer, Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Liu, Adam A. Hunt, Justin Tienken-Harder, Kevin Y. Shih, Kemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis, Alex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen Fitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexan</author><pubDate>Tue, 05 Mar 2024 18:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03218v1</guid></item><item><title>Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion</title><link>http://arxiv.org/abs/2403.03217v1</link><description>3D patient body modeling is critical to the success of automated patientpositioning for smart medical scanning and operating rooms. Existing CNN-basedend-to-end patient modeling solutions typically require a) customized networkdesigns demanding large amount of relevant training data, covering extensiverealistic clinical scenarios (e.g., patient covered by sheets), which leads tosuboptimal generalizability in practical deployment, b) expensive 3D humanmodel annotations, i.e., requiring huge amount of manual effort, resulting insystems that scale poorly. To address these issues, we propose a genericmodularized 3D patient modeling method consists of (a) a multi-modal keypointdetection module with attentive fusion for 2D patient joint localization, tolearn complementary cross-modality patient body information, leading toimproved keypoint localization robustness and generalizability in a widevariety of imaging (e.g., CT, MRI etc.) and clinical scenarios (e.g., heavyocclusions); and (b) a self-supervised 3D mesh regression module which does notrequire expensive 3D mesh parameter annotations to train, bringing immediatecost benefits for clinical deployment. We demonstrate the efficacy of theproposed method by extensive patient positioning experiments on both public andclinical data. Our evaluation results achieve superior patient positioningperformance across various imaging modalities in real clinical scenarios.</description><author>Meng Zheng, Benjamin Planche, Xuan Gong, Fan Yang, Terrence Chen, Ziyan Wu</author><pubDate>Tue, 05 Mar 2024 18:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03217v1</guid></item><item><title>Active Statistical Inference</title><link>http://arxiv.org/abs/2403.03208v1</link><description>Inspired by the concept of active learning, we propose activeinference$\unicode{x2013}$a methodology for statistical inference withmachine-learning-assisted data collection. Assuming a budget on the number oflabels that can be collected, the methodology uses a machine learning model toidentify which data points would be most beneficial to label, thus effectivelyutilizing the budget. It operates on a simple yet powerful intuition:prioritize the collection of labels for data points where the model exhibitsuncertainty, and rely on the model's predictions where it is confident. Activeinference constructs provably valid confidence intervals and hypothesis testswhile leveraging any black-box machine learning model and handling any datadistribution. The key point is that it achieves the same level of accuracy withfar fewer samples than existing baselines relying on non-adaptively-collecteddata. This means that for the same number of collected samples, activeinference enables smaller confidence intervals and more powerful p-values. Weevaluate active inference on datasets from public opinion research, censusanalysis, and proteomics.</description><author>Tijana Zrnic, Emmanuel J. Cand√®s</author><pubDate>Tue, 05 Mar 2024 18:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03208v1</guid></item><item><title>Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</title><link>http://arxiv.org/abs/2403.03206v1</link><description>Diffusion models create data from noise by inverting the forward paths ofdata towards noise and have emerged as a powerful generative modeling techniquefor high-dimensional, perceptual data such as images and videos. Rectified flowis a recent generative model formulation that connects data and noise in astraight line. Despite its better theoretical properties and conceptualsimplicity, it is not yet decisively established as standard practice. In thiswork, we improve existing noise sampling techniques for training rectified flowmodels by biasing them towards perceptually relevant scales. Through alarge-scale study, we demonstrate the superior performance of this approachcompared to established diffusion formulations for high-resolutiontext-to-image synthesis. Additionally, we present a novel transformer-basedarchitecture for text-to-image generation that uses separate weights for thetwo modalities and enables a bidirectional flow of information between imageand text tokens, improving text comprehension, typography, and human preferenceratings. We demonstrate that this architecture follows predictable scalingtrends and correlates lower validation loss to improved text-to-image synthesisas measured by various metrics and human evaluations. Our largest modelsoutperform state-of-the-art models, and we will make our experimental data,code, and model weights publicly available.</description><author>Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M√ºller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, Robin Rombach</author><pubDate>Tue, 05 Mar 2024 18:45:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03206v1</guid></item><item><title>CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments</title><link>http://arxiv.org/abs/2403.03203v1</link><description>The integration of learning and reasoning is high on the research agenda inAI. Nevertheless, there is only a little attention to use existing backgroundknowledge for reasoning about partially observed scenes to answer questionsabout the scene. Yet, we as humans use such knowledge frequently to inferplausible answers to visual questions (by eliminating all inconsistent ones).Such knowledge often comes in the form of constraints about objects and ittends to be highly domain or environment-specific. We contribute a novelbenchmark called CLEVR-POC for reasoning-intensive visual question answering(VQA) in partially observable environments under constraints. In CLEVR-POC,knowledge in the form of logical constraints needs to be leveraged to generateplausible answers to questions about a hidden object in a given partial scene.For instance, if one has the knowledge that all cups are colored either red,green or blue and that there is only one green cup, it becomes possible todeduce the color of an occluded cup as either red or blue, provided that allother cups, including the green one, are observed. Through experiments, weobserve that the low performance of pre-trained vision language models likeCLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POCascertains the necessity for frameworks that can handle reasoning-intensivetasks where environment-specific background knowledge is available and crucial.Furthermore, our demonstration illustrates that a neuro-symbolic model, whichintegrates an LLM like GPT-4 with a visual perception network and a formallogical reasoner, exhibits exceptional performance on CLEVR-POC.</description><author>Savitha Sam Abraham, Marjan Alirezaie, Luc De Raedt</author><pubDate>Tue, 05 Mar 2024 18:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03203v1</guid></item><item><title>In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation</title><link>http://arxiv.org/abs/2403.01548v2</link><description>Large language models (LLMs) frequently hallucinate and produce factualerrors, yet our understanding of why they make these errors remains limited. Inthis study, we delve into the underlying mechanisms of LLM hallucinations fromthe perspective of inner representations, and discover a salient patternassociated with hallucinations: correct generations tend to have sharpercontext activations in the hidden states of the in-context tokens, compared tothe incorrect ones. Leveraging this insight, we propose an entropy-based metricto quantify the ``sharpness'' among the in-context hidden states andincorporate it into the decoding process to formulate a constrained decodingapproach. Experiments on various knowledge-seeking and hallucination benchmarksdemonstrate our approach's consistent effectiveness, for example, achieving upto an 8.6 point improvement on TruthfulQA. We believe this study can improveour understanding of hallucinations and serve as a practical solution forhallucination mitigation.</description><author>Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang Gao, Junxian He</author><pubDate>Tue, 05 Mar 2024 18:41:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01548v2</guid></item><item><title>MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets</title><link>http://arxiv.org/abs/2403.03194v1</link><description>Development of multimodal interactive systems is hindered by the lack ofrich, multimodal (text, images) conversational data, which is needed in largequantities for LLMs. Previous approaches augment textual dialogues withretrieved images, posing privacy, diversity, and quality constraints. In thiswork, we introduce \textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative\textbf{I}mages \textbf{D}ialogues (MAGID), a framework to augment text-onlydialogues with diverse and high-quality images. Subsequently, a diffusion modelis applied to craft corresponding images, ensuring alignment with theidentified text. Finally, MAGID incorporates an innovative feedback loopbetween an image description generation module (textual LLM) and image qualitymodules (addressing aesthetics, image-text matching, and safety), that work intandem to generate high-quality and multi-modal dialogues. We compare MAGID toother SOTA baselines on three dialogue datasets, using automated and humanevaluation. Our results show that MAGID is comparable to or better thanbaselines, with significant improvements in human evaluation, especiallyagainst retrieval baselines where the image database is small.</description><author>Hossein Aboutalebi, Hwanjun Song, Yusheng Xie, Arshit Gupta, Justin Sun, Hang Su, Igor Shalyminov, Nikolaos Pappas, Siffi Singh, Saab Mansour</author><pubDate>Tue, 05 Mar 2024 18:31:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03194v1</guid></item><item><title>Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract Reasoning process</title><link>http://arxiv.org/abs/2403.03190v1</link><description>Abstract reasoning problems pose significant challenges to artificialintelligence algorithms, demanding cognitive capabilities beyond those requiredfor perception tasks. This study introduces the Triple-CFN approach to tacklethe Bongard-Logo problem, achieving notable reasoning accuracy by implicitlyreorganizing the concept space of conflicting instances. Additionally, theTriple-CFN paradigm proves effective for the RPM problem with necessarymodifications, yielding competitive results. To further enhance performance onthe RPM issue, we develop the Meta Triple-CFN network, which explicitlystructures the problem space while maintaining interpretability on progressivepatterns. The success of Meta Triple-CFN is attributed to its paradigm ofmodeling the conceptual space, equivalent to normalizing reasoning information.Based on this ideology, we introduce the Re-space layer, enhancing theperformance of both Meta Triple-CFN and Triple-CFN. This paper aims tocontribute to advancements in machine intelligence by exploring innovativenetwork designs for addressing abstract reasoning problems, paving the way forfurther breakthroughs in this domain.</description><author>Ruizhuo Song, Beiming Yuan</author><pubDate>Tue, 05 Mar 2024 18:29:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03190v1</guid></item><item><title>Counterfactual Effect Generalization: A Combinatorial Definition</title><link>http://arxiv.org/abs/2108.04376v4</link><description>The widely used 'Counterfactual' definition of Causal Effects was derived forunbiasedness and accuracy - and not generalizability. We propose aCombinatorial definition for the External Validity (EV) of interventioneffects. We first define the concept of an effect observation 'background'. Wethen formulate conditions for effect generalization based on their sets of(observable and unobservable) backgrounds. This reveals two limits for effectgeneralization: (1) when effects are observed under all their enumerablebackgrounds, or, (2) when backgrounds have become sufficiently randomized. Weuse the resulting combinatorial framework to re-examine several issues in theoriginal counterfactual formulation: out-of-sample validity, concurrentestimation of multiple effects, bias-variance tradeoffs, statistical power, andconnections to current predictive and explaining techniques. Methodologically, the definitions also allow us to also replace theparametric estimation problems that followed the counterfactual definition bycombinatorial enumeration and randomization problems in non-experimentalsamples. We use this non-parametric framework to demonstrate (ExternalValidity, Unconfoundness and Precision) tradeoffs in the performance of popularsupervised, explaining, and causal-effect estimators. We demonstrate theapproach also allows for the use of these methods in non-i.i.d. samples. TheCOVID19 pandemic highlighted the need for learning solutions to providepredictions in severally incomplete samples. We demonstrate applications inthis pressing problem.</description><author>Andre F. Ribeiro</author><pubDate>Tue, 05 Mar 2024 18:28:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.04376v4</guid></item><item><title>NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models</title><link>http://arxiv.org/abs/2403.01777v2</link><description>Understanding the reasoning capabilities of Multimodal Large Language Models(MLLMs) is an important area of research. In this study, we introduce a dynamicbenchmark, NPHardEval4V, aimed at addressing the existing gaps in evaluatingthe pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue todisentangle the effect of various factors such as image recognition andinstruction following, from the overall performance of the models, allowing usto focus solely on evaluating their reasoning abilities. It is built byconverting textual description of questions from NPHardEval to imagerepresentations. Our findings reveal significant discrepancies in reasoningabilities across different models and highlight the relatively weak performanceof MLLMs compared to LLMs in terms of reasoning. We also investigate the impactof different prompting styles, including visual, text, and combined visual andtext prompts, on the reasoning abilities of MLLMs, demonstrating the differentimpacts of multimodal inputs in model performance. Unlike traditionalbenchmarks, which focus primarily on static evaluations, our benchmark will beupdated monthly to prevent overfitting and ensure a more authentic andfine-grained evaluation of the models. We believe that this benchmark can aidin understanding and guide the further development of reasoning abilities inMLLMs. The benchmark dataset and code are available athttps://github.com/lizhouf/NPHardEval4V</description><author>Lizhou Fan, Wenyue Hua, Xiang Li, Kaijie Zhu, Mingyu Jin, Lingyao Li, Haoyang Ling, Jinkui Chi, Jindong Wang, Xin Ma, Yongfeng Zhang</author><pubDate>Tue, 05 Mar 2024 18:26:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01777v2</guid></item><item><title>Volumetric Semantically Consistent 3D Panoptic Mapping</title><link>http://arxiv.org/abs/2309.14737v2</link><description>We introduce an online 2D-to-3D semantic instance mapping algorithm aimed atgenerating comprehensive, accurate, and efficient semantic 3D maps suitable forautonomous agents in unstructured environments. The proposed approach is basedon a Voxel-TSDF representation used in recent algorithms. It introduces novelways of integrating semantic prediction confidence during mapping, producingsemantic and instance-consistent 3D regions. Further improvements are achievedby graph optimization-based semantic labeling and instance refinement. Theproposed method achieves accuracy superior to the state of the art on publiclarge-scale datasets, improving on a number of widely used metrics. We alsohighlight a downfall in the evaluation of recent studies: using the groundtruth trajectory as input instead of a SLAM-estimated one substantially affectsthe accuracy, creating a large gap between the reported results and the actualperformance on real-world data.</description><author>Yang Miao, Iro Armeni, Marc Pollefeys, Daniel Barath</author><pubDate>Tue, 05 Mar 2024 18:25:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14737v2</guid></item><item><title>Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement</title><link>http://arxiv.org/abs/2403.03188v1</link><description>Real-time flood forecasting plays a crucial role in enabling timely andeffective emergency responses. However, a significant challenge lies inbridging the gap between complex numerical flood models and practicaldecision-making. Decision-makers often rely on experts to interpret thesemodels for optimizing flood mitigation strategies. And the public requirescomplex techniques to inquiry and understand socio-cultural and institutionalfactors, often hinders the public's understanding of flood risks. To overcomethese challenges, our study introduces an innovative solution: a customized AIAssistant powered by the GPT-4 Large Language Model. This AI Assistant isdesigned to facilitate effective communication between decision-makers, thegeneral public, and flood forecasters, without the requirement of specializedknowledge. The new framework utilizes GPT-4's advanced natural languageunderstanding and function calling capabilities to provide immediate floodalerts and respond to various flood-related inquiries. Our developed prototypeintegrates real-time flood warnings with flood maps and social vulnerabilitydata. It also effectively translates complex flood zone information intoactionable risk management advice. To assess its performance, we evaluated theprototype using six criteria within three main categories: relevance, errorresilience, and understanding of context. Our research marks a significant steptowards a more accessible and user-friendly approach in flood risk management.This study highlights the potential of advanced AI tools like GPT-4 indemocratizing information and enhancing public engagement in critical socialand environmental issues.</description><author>Rafaela Martelo, Ruo-Qian Wang</author><pubDate>Tue, 05 Mar 2024 18:24:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03188v1</guid></item><item><title>Reliable, Adaptable, and Attributable Language Models with Retrieval</title><link>http://arxiv.org/abs/2403.03187v1</link><description>Parametric language models (LMs), which are trained on vast amounts of webdata, exhibit remarkable flexibility and capability. However, they still facepractical challenges such as hallucinations, difficulty in adapting to new datadistributions, and a lack of verifiability. In this position paper, we advocatefor retrieval-augmented LMs to replace parametric LMs as the next generation ofLMs. By incorporating large-scale datastores during inference,retrieval-augmented LMs can be more reliable, adaptable, and attributable.Despite their potential, retrieval-augmented LMs have yet to be widely adopteddue to several obstacles: specifically, current retrieval-augmented LMsstruggle to leverage helpful text beyond knowledge-intensive tasks such asquestion answering, have limited interaction between retrieval and LMcomponents, and lack the infrastructure for scaling. To address these, wepropose a roadmap for developing general-purpose retrieval-augmented LMs. Thisinvolves a reconsideration of datastores and retrievers, the exploration ofpipelines with improved retriever-LM interaction, and significant investment ininfrastructure for efficient training and inference.</description><author>Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih</author><pubDate>Tue, 05 Mar 2024 18:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03187v1</guid></item><item><title>Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study</title><link>http://arxiv.org/abs/2403.03186v1</link><description>Recent studies have demonstrated the success of foundation agents in specifictasks or scenarios. However, existing agents cannot generalize across differentscenarios, mainly due to their diverse observation and action spaces andsemantic gaps, or reliance on task-specific resources. In this work, we proposethe General Computer Control (GCC) setting: building foundation agents that canmaster any computer task by taking only screen images (and possibly audio) ofthe computer as input, and producing keyboard and mouse operations as output,similar to human-computer interaction. To target GCC, we propose Cradle, anagent framework with strong reasoning abilities, including self-reflection,task inference, and skill curation, to ensure generalizability andself-improvement across various tasks. To demonstrate the capabilities ofCradle, we deploy it in the complex AAA game Red Dead Redemption II, serving asa preliminary attempt towards GCC with a challenging target. Our agent canfollow the main storyline and finish real missions in this complex AAA game,with minimal reliance on prior knowledge and application-specific resources.The project website is at https://baai-agents.github.io/Cradle/.</description><author>Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, Yifei Bi, Pengjie Gu, Xinrun Wang, B√∂rje F. Karlsson, Bo An, Zongqing Lu</author><pubDate>Tue, 05 Mar 2024 18:22:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03186v1</guid></item><item><title>Preventing Reward Hacking with Occupancy Measure Regularization</title><link>http://arxiv.org/abs/2403.03185v1</link><description>Reward hacking occurs when an agent performs very well with respect to a"proxy" reward function (which may be hand-specified or learned), but poorlywith respect to the unknown true reward. Since ensuring good alignment betweenthe proxy and true reward is extremely difficult, one approach to preventreward hacking is optimizing the proxy conservatively. Prior work hasparticularly focused on enforcing the learned policy to behave similarly to a"safe" policy by penalizing the KL divergence between their actiondistributions (AD). However, AD regularization doesn't always work well since asmall change in action distribution at a single state can lead to potentiallycalamitous outcomes, while large changes might not be indicative of anydangerous activity. Our insight is that when reward hacking, the agent visitsdrastically different states from those reached by the safe policy, causinglarge deviations in state occupancy measure (OM). Thus, we propose regularizingbased on the OM divergence between policies instead of AD divergence to preventreward hacking. We theoretically establish that OM regularization can moreeffectively avoid large drops in true reward. Then, we empirically demonstratein a variety of realistic environments that OM divergence is superior to ADdivergence for preventing reward hacking by regularizing towards a safe policy.Furthermore, we show that occupancy measure divergence can also regularizelearned policies away from reward hacking behavior. Our code and data areavailable at https://github.com/cassidylaidlaw/orpo</description><author>Cassidy Laidlaw, Shivam Singhal, Anca Dragan</author><pubDate>Tue, 05 Mar 2024 18:22:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03185v1</guid></item><item><title>How Well Can Transformers Emulate In-context Newton's Method?</title><link>http://arxiv.org/abs/2403.03183v1</link><description>Transformer-based models have demonstrated remarkable in-context learningcapabilities, prompting extensive research into its underlying mechanisms.Recent studies have suggested that Transformers can implement first-orderoptimization algorithms for in-context learning and even second order ones forthe case of linear regression. In this work, we study whether Transformers canperform higher order optimization methods, beyond the case of linearregression. We establish that linear attention Transformers with ReLU layerscan approximate second order optimization algorithms for the task of logisticregression and achieve $\epsilon$ error with only a logarithmic to the errormore layers. As a by-product we demonstrate the ability of even linearattention-only Transformers in implementing a single step of Newton's iterationfor matrix inversion with merely two layers. These results suggest the abilityof the Transformer architecture to implement complex algorithms, beyondgradient descent.</description><author>Angeliki Giannou, Liu Yang, Tianhao Wang, Dimitris Papailiopoulos, Jason D. Lee</author><pubDate>Tue, 05 Mar 2024 18:20:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03183v1</guid></item><item><title>Behavior Generation with Latent Actions</title><link>http://arxiv.org/abs/2403.03181v1</link><description>Generative modeling of complex behaviors from labeled datasets has been alongstanding problem in decision making. Unlike language or image generation,decision making requires modeling actions - continuous-valued vectors that aremultimodal in their distribution, potentially drawn from uncurated sources,where generation errors can compound in sequential prediction. A recent classof models called Behavior Transformers (BeT) addresses this by discretizingactions using k-means clustering to capture different modes. However, k-meansstruggles to scale for high-dimensional action spaces or long sequences, andlacks gradient information, and thus BeT suffers in modeling long-rangeactions. In this work, we present Vector-Quantized Behavior Transformer(VQ-BeT), a versatile model for behavior generation that handles multimodalaction prediction, conditional generation, and partial observations. VQ-BeTaugments BeT by tokenizing continuous actions with a hierarchical vectorquantization module. Across seven environments including simulatedmanipulation, autonomous driving, and robotics, VQ-BeT improves onstate-of-the-art models such as BeT and Diffusion Policies. Importantly, wedemonstrate VQ-BeT's improved ability to capture behavior modes whileaccelerating inference speed 5x over Diffusion Policies. Videos and code can befound https://sjlee.cc/vq-bet</description><author>Seungjae Lee, Yibin Wang, Haritheja Etukuru, H. Jin Kim, Nur Muhammad Mahi Shafiullah, Lerrel Pinto</author><pubDate>Tue, 05 Mar 2024 18:19:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03181v1</guid></item><item><title>Shuffling Momentum Gradient Algorithm for Convex Optimization</title><link>http://arxiv.org/abs/2403.03180v1</link><description>The Stochastic Gradient Descent method (SGD) and its stochastic variants havebecome methods of choice for solving finite-sum optimization problems arisingfrom machine learning and data science thanks to their ability to handlelarge-scale applications and big datasets. In the last decades, researchershave made substantial effort to study the theoretical performance of SGD andits shuffling variants. However, only limited work has investigated itsshuffling momentum variants, including shuffling heavy-ball momentum schemesfor non-convex problems and Nesterov's momentum for convex settings. In thiswork, we extend the analysis of the shuffling momentum gradient methoddeveloped in [Tran et al (2021)] to both finite-sum convex and strongly convexoptimization problems. We provide the first analysis of shufflingmomentum-based methods for the strongly convex setting, attaining a convergencerate of $O(1/nT^2)$, where $n$ is the number of samples and $T$ is the numberof training epochs. Our analysis is a state-of-the-art, matching the best ratesof existing shuffling stochastic gradient algorithms in the literature.</description><author>Trang H. Tran, Quoc Tran-Dinh, Lam M. Nguyen</author><pubDate>Tue, 05 Mar 2024 18:19:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03180v1</guid></item><item><title>Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models</title><link>http://arxiv.org/abs/2403.01616v2</link><description>This paper presents our contributions towards advancing the state ofVietnamese language understanding and generation through the development anddissemination of open datasets and pre-trained models for VietnameseRetrieval-Augmented Generation (RAG) and Large Language Models (LLMs).</description><author>Nguyen Quang Duc, Le Hai Son, Nguyen Duc Nhan, Nguyen Dich Nhat Minh, Le Thanh Huong, Dinh Viet Sang</author><pubDate>Tue, 05 Mar 2024 18:15:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01616v2</guid></item><item><title>Unifying and Certifying Top-Quality Planning</title><link>http://arxiv.org/abs/2403.03176v1</link><description>The growing utilization of planning tools in practical scenarios has sparkedan interest in generating multiple high-quality plans. Consequently, a range ofcomputational problems under the general umbrella of top-quality planning wereintroduced over a short time period, each with its own definition. In thiswork, we show that the existing definitions can be unified into one, based on adominance relation. The different computational problems, therefore, simplycorrespond to different dominance relations. Given the unified definition, wecan now certify the top-quality of the solutions, leveraging existingcertification of unsolvability and optimality. We show that tasktransformations found in the existing literature can be employed for theefficient certification of various top-quality planning problems and propose anovel transformation to efficiently certify loopless top-quality planning.</description><author>Michael Katz, Junkyu Lee, Shirin Sohrabi</author><pubDate>Tue, 05 Mar 2024 18:13:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03176v1</guid></item><item><title>A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models</title><link>http://arxiv.org/abs/2310.00194v2</link><description>Large language models (LLMs) demonstrate impressive performance on a widevariety of tasks, but they often struggle with tasks that require multi-stepreasoning or goal-directed planning. To address this, we take inspiration fromthe human brain, in which planning is accomplished via the recurrentinteraction of specialized modules in the prefrontal cortex (PFC). Thesemodules perform functions such as conflict monitoring, state prediction, stateevaluation, task decomposition, and task coordination. We find that LLMs aresometimes capable of carrying out these functions in isolation, but struggle toautonomously coordinate them in the service of a goal. Therefore, we propose ablack box architecture with multiple LLM-based (GPT-4) modules. Thearchitecture improves planning through the interaction of specializedPFC-inspired modules that break down a larger problem into multiple briefautomated calls to the LLM. We evaluate the combined architecture on threechallenging planning tasks -- graph traversal, Tower of Hanoi, and logistics --finding that it yields significant improvements over standard LLM methods(e.g., zero-shot prompting, in-context learning, and chain-of-thought). Theseresults demonstrate the benefit of utilizing knowledge from cognitiveneuroscience to improve planning in LLMs.</description><author>Taylor Webb, Shanka Subhra Mondal, Chi Wang, Brian Krabach, Ida Momennejad</author><pubDate>Tue, 05 Mar 2024 18:12:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00194v2</guid></item><item><title>Learning-based sound speed estimation and aberration correction in linear-array photoacoustic imaging</title><link>http://arxiv.org/abs/2306.11034v2</link><description>Photoacoustic (PA) image reconstruction involves acoustic inversion thatnecessitates the specification of the speed of sound (SoS) within the medium ofpropagation. Due to the lack of information on the spatial distribution of theSoS within heterogeneous soft tissue, a homogeneous SoS distribution (such as1540 m/s) is typically assumed in PA image reconstruction, similar to that ofultrasound (US) imaging. Failure to compensate the SoS variations leads toaberration artefacts, deteriorating the image quality. Various methods havebeen proposed to address this issue, but they usually involve complex hardwareand/or time-consuming algorithms, hindering clinical translation. In this work,we introduce a deep learning framework for SoS estimation and subsequentaberration correction in a dual-modal PA/US imaging system exploiting aclinical US probe. As the acquired PA and US images were inherentlyco-registered, the estimated SoS distribution from US channel data using a deepneural network was incorporated for accurate PA image reconstruction. Theframework comprised an initial pre-training stage based on digital phantoms,which was further enhanced through transfer learning using physical phantomdata and associated SoS maps obtained from measurements. This frameworkachieved a root mean square error of 10.2 m/s and 15.2 m/s for SoS estimationon digital and physical phantoms, respectively and structural similarity indexmeasures of up to 0.86 for PA reconstructions as compared to the conventionalapproach of 0.69. A maximum of 1.2 times improvement in signal-to-noise ratioof PA images was further demonstrated with a human volunteer study. Our resultsshow that the proposed framework could be valuable in various clinical andpreclinical applications to enhance PA image reconstruction.</description><author>Mengjie Shi, Tom Vercauteren, Wenfeng Xia</author><pubDate>Tue, 05 Mar 2024 18:11:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11034v2</guid></item><item><title>Codebook-enabled Generative End-to-end Semantic Communication Powered by Transformer</title><link>http://arxiv.org/abs/2402.16868v2</link><description>Codebook-based generative semantic communication attracts increasingattention, since only indices are required to be transmitted when the codebookis shared between transmitter and receiver. However, due to the fact that thesemantic relations among code vectors are not necessarily related to thedistance of the corresponding code indices, the performance of thecodebook-enabled semantic communication system is susceptible to the channelnoise. Thus, how to improve the system robustness against the noise requirescareful design. This paper proposes a robust codebook-assisted image semanticcommunication system, where semantic codec and codebook are first jointlyconstructed, and then vector-to-index transformer is designed guided by thecodebook to eliminate the effects of channel noise, and achieve imagegeneration. Thanks to the assistance of the high-quality codebook to theTransformer, the generated images at the receiver outperform those of thecompared methods in terms of visual perception. In the end, numerical resultsand generated images demonstrate the advantages of the generative semanticcommunication method over JPEG+LDPC and traditional joint source channel coding(JSCC) methods.</description><author>Peigen Ye, Yaping Sun, Shumin Yao, Hao Chen, Xiaodong Xu, Shuguang Cui</author><pubDate>Tue, 05 Mar 2024 18:10:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16868v2</guid></item><item><title>MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting</title><link>http://arxiv.org/abs/2403.03174v1</link><description>Open-vocabulary generalization requires robotic systems to perform tasksinvolving complex and diverse environments and task goals. While the recentadvances in vision language models (VLMs) present unprecedented opportunitiesto solve unseen problems, how to utilize their emergent capabilities to controlrobots in the physical world remains an open question. In this paper, wepresent MOKA (Marking Open-vocabulary Keypoint Affordances), an approach thatemploys VLMs to solve robotic manipulation tasks specified by free-formlanguage descriptions. At the heart of our approach is a compact point-basedrepresentation of affordance and motion that bridges the VLM's predictions onRGB images and the robot's motions in the physical world. By prompting a VLMpre-trained on Internet-scale data, our approach predicts the affordances andgenerates the corresponding motions by leveraging the concept understanding andcommonsense knowledge from broad sources. To scaffold the VLM's reasoning inzero-shot, we propose a visual prompting technique that annotates marks on theimages, converting the prediction of keypoints and waypoints into a series ofvisual question answering problems that are feasible for the VLM to solve.Using the robot experiences collected in this way, we further investigate waysto bootstrap the performance through in-context learning and policydistillation. We evaluate and analyze MOKA's performance on a variety ofmanipulation tasks specified by free-form language descriptions, such as tooluse, deformable body manipulation, and object rearrangement.</description><author>Fangchen Liu, Kuan Fang, Pieter Abbeel, Sergey Levine</author><pubDate>Tue, 05 Mar 2024 18:08:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03174v1</guid></item><item><title>Solving the bongard-logo problem by modeling a probabilistic model</title><link>http://arxiv.org/abs/2403.03173v1</link><description>Abstract reasoning problems challenge the perceptual and cognitive abilitiesof AI algorithms, demanding deeper pattern discernment and inductive reasoningbeyond explicit image features. This study introduces PMoC, a tailoredprobability model for the Bongard-Logo problem, achieving high reasoningaccuracy by constructing independent probability models. Additionally, wepresent Pose-Transformer, an enhanced Transformer-Encoder designed for complexabstract reasoning tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM.Pose-Transformer incorporates positional information learning, inspired bycapsule networks' pose matrices, enhancing its focus on local positionalrelationships in image data processing. When integrated with PMoC, it furtherimproves reasoning accuracy. Our approach effectively addresses reasoningdifficulties associated with abstract entities' positional changes,outperforming previous models on the OIG, D3$\times$3 subsets of RAVEN, and PGMdatabases. This research contributes to advancing AI's capabilities in abstractreasoning and cognitive pattern recognition.</description><author>Ruizhuo Song, Beiming Yuan</author><pubDate>Tue, 05 Mar 2024 18:08:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03173v1</guid></item><item><title>Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination</title><link>http://arxiv.org/abs/2403.03172v1</link><description>Reaching consensus is key to multi-agent coordination. To accomplish acooperative task, agents need to coherently select optimal joint actions tomaximize the team reward. However, current cooperative multi-agentreinforcement learning (MARL) methods usually do not explicitly take consensusinto consideration, which may cause miscoordination problem. In this paper, wepropose a model-based consensus mechanism to explicitly coordinate multipleagents. The proposed Multi-agent Goal Imagination (MAGI) framework guidesagents to reach consensus with an Imagined common goal. The common goal is anachievable state with high value, which is obtained by sampling from thedistribution of future states. We directly model this distribution with aself-supervised generative model, thus alleviating the "curse of dimensinality"problem induced by multi-agent multi-step policy rollout commonly used inmodel-based methods. We show that such efficient consensus mechanism can guideall agents cooperatively reaching valuable future states. Results onMulti-agent Particle-Environments and Google Research Football environmentdemonstrate the superiority of MAGI in both sample efficiency and performance.</description><author>Liangzhou Wang, Kaiwen Zhu, Fengming Zhu, Xinghu Yao, Shujie Zhang, Deheng Ye, Haobo Fu, Qiang Fu, Wei Yang</author><pubDate>Tue, 05 Mar 2024 18:07:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03172v1</guid></item><item><title>3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos</title><link>http://arxiv.org/abs/2403.01444v2</link><description>Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenesfrom multi-view videos remains a challenging endeavor. Despite the remarkableadvancements achieved by current neural rendering techniques, these methodsgenerally require complete video sequences for offline training and are notcapable of real-time rendering. To address these constraints, we introduce3DGStream, a method designed for efficient FVV streaming of real-world dynamicscenes. Our method achieves fast on-the-fly per-frame reconstruction within 12seconds and real-time rendering at 200 FPS. Specifically, we utilize 3DGaussians (3DGs) to represent the scene. Instead of the na\"ive approach ofdirectly optimizing 3DGs per-frame, we employ a compact Neural TransformationCache (NTC) to model the translations and rotations of 3DGs, markedly reducingthe training time and storage required for each FVV frame. Furthermore, wepropose an adaptive 3DG addition strategy to handle emerging objects in dynamicscenes. Experiments demonstrate that 3DGStream achieves competitive performancein terms of rendering speed, image quality, training time, and model storagewhen compared with state-of-the-art methods.</description><author>Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, Wei Xing</author><pubDate>Tue, 05 Mar 2024 18:06:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01444v2</guid></item><item><title>SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection</title><link>http://arxiv.org/abs/2403.03170v1</link><description>Misinformation is a prevalent societal issue due to its potential high risks.Out-of-context (OOC) misinformation, where authentic images are repurposed withfalse text, is one of the easiest and most effective ways to mislead audiences.Current methods focus on assessing image-text consistency but lack convincingexplanations for their judgments, which is essential for debunkingmisinformation. While Multimodal Large Language Models (MLLMs) have richknowledge and innate capability for visual reasoning and explanationgeneration, they still lack sophistication in understanding and discovering thesubtle crossmodal differences. In this paper, we introduce SNIFFER, a novelmultimodal large language model specifically engineered for OOC misinformationdetection and explanation. SNIFFER employs two-stage instruction tuning onInstructBLIP. The first stage refines the model's concept alignment of genericobjects with news-domain entities and the second stage leverages language-onlyGPT-4 generated OOC-specific instruction data to fine-tune the model'sdiscriminatory powers. Enhanced by external tools and retrieval, SNIFFER notonly detects inconsistencies between text and image but also utilizes externalknowledge for contextual verification. Our experiments show that SNIFFERsurpasses the original MLLM by over 40% and outperforms state-of-the-artmethods in detection accuracy. SNIFFER also provides accurate and persuasiveexplanations as validated by quantitative and human evaluations.</description><author>Peng Qi, Zehong Yan, Wynne Hsu, Mong Li Lee</author><pubDate>Tue, 05 Mar 2024 18:04:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03170v1</guid></item><item><title>Learning Explicitly Conditioned Sparsifying Transforms</title><link>http://arxiv.org/abs/2403.03168v1</link><description>Sparsifying transforms became in the last decades widely known tools forfinding structured sparse representations of signals in certain transformdomains. Despite the popularity of classical transforms such as DCT andWavelet, learning optimal transforms that guarantee good representations ofdata into the sparse domain has been recently analyzed in a series of papers.Typically, the conditioning number and representation ability are complementarykey features of learning square transforms that may not be explicitlycontrolled in a given optimization model. Unlike the existing approaches fromthe literature, in our paper, we consider a new sparsifying transform modelthat enforces explicit control over the data representation quality and thecondition number of the learned transforms. We confirm through numericalexperiments that our model presents better numerical behavior than thestate-of-the-art.</description><author>Andrei PƒÉtra≈ücu, Cristian Rusu, Paul Irofti</author><pubDate>Tue, 05 Mar 2024 18:03:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03168v1</guid></item><item><title>PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset</title><link>http://arxiv.org/abs/2403.03167v1</link><description>Recently, there has been growing interest within the community regardingwhether large language models are capable of planning or executing plans.However, most prior studies use LLMs to generate high-level plans forsimplified scenarios lacking linguistic complexity and domain diversity,limiting analysis of their planning abilities. These setups constrainevaluation methods (e.g., predefined action space), architectural choices(e.g., only generative models), and overlook the linguistic nuances essentialfor realistic analysis. To tackle this, we present PARADISE, an abductivereasoning task using Q\&amp;A format on practical procedural text sourced fromwikiHow. It involves warning and tip inference tasks directly associated withgoals, excluding intermediary steps, with the aim of testing the ability of themodels to infer implicit knowledge of the plan solely from the given goal. Ourexperiments, utilizing fine-tuned language models and zero-shot prompting,reveal the effectiveness of task-specific small models over large languagemodels in most scenarios. Despite advancements, all models fall short of humanperformance. Notably, our analysis uncovers intriguing insights, such asvariations in model behavior with dropped keywords, struggles of BERT-familyand GPT-4 with physical and abstract goals, and the proposed tasks offeringvaluable prior knowledge for other unseen procedural tasks. The PARADISEdataset and associated resources are publicly available for further researchexploration with https://github.com/GGLAB-KU/paradise.</description><author>Arda Uzunoƒülu, Abdalfatah Rashid Safa, G√∂zde G√ºl ≈ûahin</author><pubDate>Tue, 05 Mar 2024 18:01:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03167v1</guid></item><item><title>Leveraging Federated Learning and Edge Computing for Recommendation Systems within Cloud Computing Networks</title><link>http://arxiv.org/abs/2403.03165v1</link><description>To enable large-scale and efficient deployment of artificial intelligence(AI), the combination of AI and edge computing has spawned Edge Intelligence,which leverages the computing and communication capabilities of end devices andedge servers to process data closer to where it is generated. A key technologyfor edge intelligence is the privacy-protecting machine learning paradigm knownas Federated Learning (FL), which enables data owners to train models withouthaving to transfer raw data to third-party servers. However, FL networks areexpected to involve thousands of heterogeneous distributed devices. As aresult, communication efficiency remains a key bottleneck. To reduce nodefailures and device exits, a Hierarchical Federated Learning (HFL) framework isproposed, where a designated cluster leader supports the data owner throughintermediate model aggregation. Therefore, based on the improvement of edgeserver resource utilization, this paper can effectively make up for thelimitation of cache capacity. In order to mitigate the impact of soft clicks onthe quality of user experience (QoE), the authors model the user QoE as acomprehensive system cost. To solve the formulaic problem, the authors proposea decentralized caching algorithm with federated deep reinforcement learning(DRL) and federated learning (FL), where multiple agents learn and makedecisions independently</description><author>Yaqian Qi, Yaqian Qi, Xiangxiang Wang, Hanzhe Li, Jingxiao Tian</author><pubDate>Tue, 05 Mar 2024 17:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03165v1</guid></item><item><title>Design2Code: How Far Are We From Automating Front-End Engineering?</title><link>http://arxiv.org/abs/2403.03163v1</link><description>Generative AI has made rapid advancements in recent years, achievingunprecedented capabilities in multimodal understanding and code generation.This can enable a new paradigm of front-end development, in which multimodalLLMs might directly convert visual designs into code implementations. In thiswork, we formalize this as a Design2Code task and conduct comprehensivebenchmarking. Specifically, we manually curate a benchmark of 484 diversereal-world webpages as test cases and develop a set of automatic evaluationmetrics to assess how well current multimodal LLMs can generate the codeimplementations that directly render into the given reference webpages, giventhe screenshots as input. We also complement automatic metrics withcomprehensive human evaluations. We develop a suite of multimodal promptingmethods and show their effectiveness on GPT-4V and Gemini Pro Vision. Wefurther finetune an open-source Design2Code-18B model that successfully matchesthe performance of Gemini Pro Vision. Both human evaluation and automaticmetrics show that GPT-4V performs the best on this task compared to othermodels. Moreover, annotators think GPT-4V generated webpages can replace theoriginal reference webpages in 49% of cases in terms of visual appearance andcontent; and perhaps surprisingly, in 64% of cases GPT-4V generated webpagesare considered better than the original reference webpages. Our fine-grainedbreak-down metrics indicate that open-source models mostly lag in recallingvisual elements from the input webpages and in generating correct layoutdesigns, while aspects like text content and coloring can be drasticallyimproved with proper finetuning.</description><author>Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, Diyi Yang</author><pubDate>Tue, 05 Mar 2024 17:56:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03163v1</guid></item><item><title>A Good Feature Extractor Is All You Need for Weakly Supervised Pathology Slide Classification</title><link>http://arxiv.org/abs/2311.11772v4</link><description>Stain normalisation is thought to be a crucial preprocessing step incomputational pathology pipelines. We question this belief in the context ofweakly supervised whole slide image classification, motivated by the emergenceof powerful feature extractors trained using self-supervised learning ondiverse pathology datasets. To this end, we performed the most comprehensiveevaluation of publicly available pathology feature extractors to date,involving more than 8,000 training runs across nine tasks, five datasets, threedownstream architectures, and various preprocessing setups. Notably, we findthat omitting stain normalisation and image augmentations does not compromisedownstream slide-level classification performance, while incurring substantialsavings in memory and compute. Using a new evaluation metric that facilitatesrelative downstream performance comparison, we identify the best publiclyavailable extractors, and show that their latent spaces are remarkably robustto variations in stain and augmentations like rotation. Contrary to previouspatch-level benchmarking studies, our approach emphasises clinical relevance byfocusing on slide-level biomarker prediction tasks in a weakly supervisedsetting with external validation cohorts. Our findings stand to streamlinedigital pathology workflows by minimising preprocessing needs and informing theselection of feature extractors. Code and data are available athttps://georg.woelflein.eu/good-features.</description><author>Georg W√∂lflein, Dyke Ferber, Asier Rabasco Meneghetti, Omar S. M. El Nahhas, Daniel Truhn, Zunamys I. Carrero, David J. Harrison, Ognjen Arandjeloviƒá, Jakob N. Kather</author><pubDate>Tue, 05 Mar 2024 17:56:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11772v4</guid></item><item><title>PalmProbNet: A Probabilistic Approach to Understanding Palm Distributions in Ecuadorian Tropical Forest via Transfer Learning</title><link>http://arxiv.org/abs/2403.03161v1</link><description>Palms play an outsized role in tropical forests and are important resourcesfor humans and wildlife. A central question in tropical ecosystems isunderstanding palm distribution and abundance. However, accurately identifyingand localizing palms in geospatial imagery presents significant challenges dueto dense vegetation, overlapping canopies, and variable lighting conditions inmixed-forest landscapes. Addressing this, we introduce PalmProbNet, aprobabilistic approach utilizing transfer learning to analyze high-resolutionUAV-derived orthomosaic imagery, enabling the detection of palm trees withinthe dense canopy of the Ecuadorian Rainforest. This approach represents asubstantial advancement in automated palm detection, effectively pinpointingpalm presence and locality in mixed tropical rainforests. Our process begins bygenerating an orthomosaic image from UAV images, from which we extract andlabel palm and non-palm image patches in two distinct sizes. These patches arethen used to train models with an identical architecture, consisting of anunaltered pre-trained ResNet-18 and a Multilayer Perceptron (MLP) withspecifically trained parameters. Subsequently, PalmProbNet employs a slidingwindow technique on the landscape orthomosaic, using both small and largewindow sizes to generate a probability heatmap. This heatmap effectivelyvisualizes the distribution of palms, showcasing the scalability andadaptability of our approach in various forest densities. Despite thechallenging terrain, our method demonstrated remarkable performance, achievingan accuracy of 97.32% and a Cohen's kappa of 94.59% in testing.</description><author>Kangning Cui, Zishan Shao, Gregory Larsen, Victor Pauca, Sarra Alqahtani, David Segurado, Jo√£o Pinheiro, Manqi Wang, David Lutz, Robert Plemmons, Miles Silman</author><pubDate>Tue, 05 Mar 2024 17:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03161v1</guid></item><item><title>Momentum Benefits Non-IID Federated Learning Simply and Provably</title><link>http://arxiv.org/abs/2306.16504v3</link><description>Federated learning is a powerful paradigm for large-scale machine learning,but it faces significant challenges due to unreliable network connections, slowcommunication, and substantial data heterogeneity across clients. FedAvg andSCAFFOLD are two prominent algorithms to address these challenges. Inparticular, FedAvg employs multiple local updates before communicating with acentral server, while SCAFFOLD maintains a control variable on each client tocompensate for ``client drift'' in its local updates. Various methods have beenproposed to enhance the convergence of these two algorithms, but they eithermake impractical adjustments to the algorithmic structure or rely on theassumption of bounded data heterogeneity. This paper explores the utilization of momentum to enhance the performance ofFedAvg and SCAFFOLD. When all clients participate in the training process, wedemonstrate that incorporating momentum allows FedAvg to converge withoutrelying on the assumption of bounded data heterogeneity even using a constantlocal learning rate. This is novel and fairly surprising as existing analysesfor FedAvg require bounded data heterogeneity even with diminishing locallearning rates. In partial client participation, we show that momentum enablesSCAFFOLD to converge provably faster without imposing any additionalassumptions. Furthermore, we use momentum to develop new variance-reducedextensions of FedAvg and SCAFFOLD, which exhibit state-of-the-art convergencerates. Our experimental results support all theoretical findings.</description><author>Ziheng Cheng, Xinmeng Huang, Pengfei Wu, Kun Yuan</author><pubDate>Tue, 05 Mar 2024 17:51:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16504v3</guid></item><item><title>Rethinking Clustered Federated Learning in NOMA Enhanced Wireless Networks</title><link>http://arxiv.org/abs/2403.03157v1</link><description>This study explores the benefits of integrating the novel clustered federatedlearning (CFL) approach with non-orthogonal multiple access (NOMA) undernon-independent and identically distributed (non-IID) datasets, where multipledevices participate in the aggregation with time limitations and a finitenumber of sub-channels. A detailed theoretical analysis of the generalizationgap that measures the degree of non-IID in the data distribution is presented.Following that, solutions to address the challenges posed by non-IID conditionsare proposed with the analysis of the properties. Specifically, users' datadistributions are parameterized as concentration parameters and grouped usingspectral clustering, with Dirichlet distribution serving as the prior. Theinvestigation into the generalization gap and convergence rate guides thedesign of sub-channel assignments through the matching-based algorithm, and thepower allocation is achieved by Karush-Kuhn-Tucker (KKT) conditions with thederived closed-form solution. The extensive simulation results show that theproposed cluster-based FL framework can outperform FL baselines in terms ofboth test accuracy and convergence rate. Moreover, jointly optimizingsub-channel and power allocation in NOMA-enhanced networks can lead to asignificant improvement.</description><author>Yushen Lin, Kaidi Wang, Zhiguo Ding</author><pubDate>Tue, 05 Mar 2024 17:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03157v1</guid></item><item><title>Quantum Many-Body Physics Calculations with Large Language Models</title><link>http://arxiv.org/abs/2403.03154v1</link><description>Large language models (LLMs) have demonstrated an unprecedented ability toperform complex tasks in multiple domains, including mathematical andscientific reasoning. We demonstrate that with carefully designed prompts, LLMscan accurately carry out key calculations in research papers in theoreticalphysics. We focus on a broadly used approximation method in quantum physics:the Hartree-Fock method, requiring an analytic multi-step calculation derivingapproximate Hamiltonian and corresponding self-consistency equations. To carryout the calculations using LLMs, we design multi-step prompt templates thatbreak down the analytic calculation into standardized steps with placeholdersfor problem-specific information. We evaluate GPT-4's performance in executingthe calculation for 15 research papers from the past decade, demonstratingthat, with correction of intermediate steps, it can correctly derive the finalHartree-Fock Hamiltonian in 13 cases and makes minor errors in 2 cases.Aggregating across all research papers, we find an average score of 87.5 (outof 100) on the execution of individual calculation steps. Overall, therequisite skill for doing these calculations is at the graduate level inquantum condensed matter theory. We further use LLMs to mitigate the twoprimary bottlenecks in this evaluation process: (i) extracting information frompapers to fill in templates and (ii) automatic scoring of the calculationsteps, demonstrating good results in both cases. The strong performance is thefirst step for developing algorithms that automatically explore theoreticalhypotheses at an unprecedented scale.</description><author>Haining Pan, Nayantara Mudur, Will Taranto, Maria Tikhanovskaya, Subhashini Venugopalan, Yasaman Bahri, Michael P. Brenner, Eun-Ah Kim</author><pubDate>Tue, 05 Mar 2024 17:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03154v1</guid></item><item><title>Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources</title><link>http://arxiv.org/abs/2308.09766v2</link><description>Prediction of dynamic environmental variables in unmonitored sites remains along-standing challenge for water resources science. The majority of theworld's freshwater resources have inadequate monitoring of criticalenvironmental variables needed for management. Yet, the need to have widespreadpredictions of hydrological variables such as river flow and water quality hasbecome increasingly urgent due to climate and land use change over the pastdecades, and their associated impacts on water resources. Modern machinelearning methods increasingly outperform their process-based and empiricalmodel counterparts for hydrologic time series prediction with their ability toextract information from large, diverse data sets. We review relevantstate-of-the art applications of machine learning for streamflow, waterquality, and other water resources prediction and discuss opportunities toimprove the use of machine learning with emerging methods for incorporatingwatershed characteristics into deep learning models, transfer learning, andincorporating process knowledge into machine learning models. The analysis heresuggests most prior efforts have been focused on deep learning learningframeworks built on many sites for predictions at daily time scales in theUnited States, but that comparisons between different classes of machinelearning methods are few and inadequate. We identify several open questions fortime series predictions in unmonitored sites that include incorporating dynamicinputs and site characteristics, mechanistic understanding and spatial context,and explainable AI techniques in modern machine learning frameworks.</description><author>Jared D. Willard, Charuleka Varadharajan, Xiaowei Jia, Vipin Kumar</author><pubDate>Tue, 05 Mar 2024 17:45:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09766v2</guid></item><item><title>Deep-Learned Compression for Radio-Frequency Signal Classification</title><link>http://arxiv.org/abs/2403.03150v1</link><description>Next-generation cellular concepts rely on the processing of large quantitiesof radio-frequency (RF) samples. This includes Radio Access Networks (RAN)connecting the cellular front-end based on software defined radios (SDRs) and aframework for the AI processing of spectrum-related data. The RF data collectedby the dense RAN radio units and spectrum sensors may need to be jointlyprocessed for intelligent decision making. Moving large amounts of data to AIagents may result in significant bandwidth and latency costs. We propose a deeplearned compression (DLC) model, HQARF, based on learned vector quantization(VQ), to compress the complex-valued samples of RF signals comprised of 6modulation classes. We are assessing the effects of HQARF on the performance ofan AI model trained to infer the modulation class of the RF signal. Compressionof narrow-band RF samples for the training and off-the-site inference willallow for an efficient use of the bandwidth and storage for non-real-timeanalytics, and for a decreased delay in real-time applications. While exploringthe effectiveness of the HQARF signal reconstructions in modulationclassification tasks, we highlight the DLC optimization space and some openproblems related to the training of the VQ embedded in HQARF.</description><author>Armani Rodriguez, Yagna Kaasaragadda, Silvija Kokalj-Filipovic</author><pubDate>Tue, 05 Mar 2024 17:42:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03150v1</guid></item><item><title>Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks</title><link>http://arxiv.org/abs/2403.03149v1</link><description>Recent studies have revealed that federated learning (FL), once consideredsecure due to clients not sharing their private data with the server, isvulnerable to attacks such as client-side training data distribution inference,where a malicious client can recreate the victim's data. While variouscountermeasures exist, they are not practical, often assuming server access tosome training data or knowledge of label distribution before the attack. In this work, we bridge the gap by proposing InferGuard, a novelByzantine-robust aggregation rule aimed at defending against client-sidetraining data distribution inference attacks. In our proposed InferGuard, theserver first calculates the coordinate-wise median of all the model updates itreceives. A client's model update is considered malicious if it significantlydeviates from the computed median update. We conduct a thorough evaluation ofour proposed InferGuard on five benchmark datasets and perform a comparisonwith ten baseline methods. The results of our experiments indicate that ourdefense mechanism is highly effective in protecting against client-sidetraining data distribution inference attacks, even against strong adaptiveattacks. Furthermore, our method substantially outperforms the baseline methodsin various practical FL scenarios.</description><author>Yichang Xu, Ming Yin, Minghong Fang, Neil Zhenqiang Gong</author><pubDate>Tue, 05 Mar 2024 17:41:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03149v1</guid></item><item><title>Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization</title><link>http://arxiv.org/abs/2403.03145v1</link><description>Audio-Visual Source Localization (AVSL) aims to locate sounding objectswithin video frames given the paired audio clips. Existing methodspredominantly rely on self-supervised contrastive learning of audio-visualcorrespondence. Without any bounding-box annotations, they struggle to achieveprecise localization, especially for small objects, and suffer from blurryboundaries and false positives. Moreover, the naive semi-supervised method ispoor in fully leveraging the information of abundant unlabeled data. In thispaper, we propose a novel semi-supervised learning framework for AVSL, namelyDual Mean-Teacher (DMT), comprising two teacher-student structures tocircumvent the confirmation bias issue. Specifically, two teachers, pre-trainedon limited labeled data, are employed to filter out noisy samples via theconsensus between their predictions, and then generate high-qualitypseudo-labels by intersecting their confidence maps. The sufficient utilizationof both labeled and unlabeled data and the proposed unbiased framework enableDMT to outperform current state-of-the-art methods by a large margin, with CIoUof 90.4% and 48.8% on Flickr-SoundNet and VGG-Sound Source, obtaining 8.9%,9.6% and 4.6%, 6.4% improvements over self- and semi-supervised methodsrespectively, given only 3% positional-annotations. We also extend ourframework to some existing AVSL methods and consistently boost theirperformance.</description><author>Yuxin Guo, Shijie Ma, Hu Su, Zhiqing Wang, Yuhao Zhao, Wei Zou, Siyang Sun, Yun Zheng</author><pubDate>Tue, 05 Mar 2024 17:35:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03145v1</guid></item><item><title>RDR: the Recap, Deliberate, and Respond Method for Enhanced Language Understanding</title><link>http://arxiv.org/abs/2312.09932v2</link><description>Natural language understanding (NLU) using neural network pipelines oftenrequires additional context that is not solely present in the input data.Through Prior research, it has been evident that NLU benchmarks are susceptibleto manipulation by neural models, wherein these models exploit statisticalartifacts within the encoded external knowledge to artificially inflateperformance metrics for downstream tasks. Our proposed approach, known as theRecap, Deliberate, and Respond (RDR) paradigm, addresses this issue byincorporating three distinct objectives within the neural network pipeline.Firstly, the Recap objective involves paraphrasing the input text using aparaphrasing model in order to summarize and encapsulate its essence. Secondly,the Deliberation objective entails encoding external graph information relatedto entities mentioned in the input text, utilizing a graph embedding model.Finally, the Respond objective employs a classification head model thatutilizes representations from the Recap and Deliberation modules to generatethe final prediction. By cascading these three models and minimizing a combinedloss, we mitigate the potential for gaming the benchmark and establish a robustmethod for capturing the underlying semantic patterns, thus enabling accuratepredictions. To evaluate the effectiveness of the RDR method, we conduct testson multiple GLUE benchmark tasks. Our results demonstrate improved performancecompared to competitive baselines, with an enhancement of up to 2\% on standardmetrics. Furthermore, we analyze the observed evidence for semanticunderstanding exhibited by RDR models, emphasizing their ability to avoidgaming the benchmark and instead accurately capture the true underlyingsemantic patterns.</description><author>Yuxin Zi, Hariram Veeramani, Kaushik Roy, Amit Sheth</author><pubDate>Tue, 05 Mar 2024 17:29:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09932v2</guid></item><item><title>Soft-prompt Tuning for Large Language Models to Evaluate Bias</title><link>http://arxiv.org/abs/2306.04735v2</link><description>Prompting large language models has gained immense popularity in recent yearsdue to the advantage of producing good results even without the need forlabelled data. However, this requires prompt tuning to get optimal prompts thatlead to better model performances. In this paper, we explore the use ofsoft-prompt tuning on sentiment classification task to quantify the biases oflarge language models (LLMs) such as Open Pre-trained Transformers (OPT) andGalactica language model. Since these models are trained on real-world datathat could be prone to bias toward certain groups of populations, it isimportant to identify these underlying issues. Using soft-prompts to evaluatebias gives us the extra advantage of avoiding the human-bias injection that canbe caused by manually designed prompts. We check the model biases on differentsensitive attributes using the group fairness (bias) and find interesting biaspatterns. Since LLMs have been used in the industry in various applications, itis crucial to identify the biases before deploying these models in practice. Weopen-source our pipeline and encourage industry researchers to adapt our workto their use cases.</description><author>Jacob-Junqi Tian, David Emerson, Sevil Zanjani Miyandoab, Deval Pandya, Laleh Seyyed-Kalantari, Faiza Khan Khattak</author><pubDate>Tue, 05 Mar 2024 17:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04735v2</guid></item><item><title>Language Guided Exploration for RL Agents in Text Environments</title><link>http://arxiv.org/abs/2403.03141v1</link><description>Real-world sequential decision making is characterized by sparse rewards andlarge decision spaces, posing significant difficulty for experiential learningsystems like $\textit{tabula rasa}$ reinforcement learning (RL) agents. LargeLanguage Models (LLMs), with a wealth of world knowledge, can help RL agentslearn quickly and adapt to distribution shifts. In this work, we introduceLanguage Guided Exploration (LGE) framework, which uses a pre-trained languagemodel (called GUIDE ) to provide decision-level guidance to an RL agent (calledEXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challengingtext environment, LGE outperforms vanilla RL agents significantly and alsooutperforms other sophisticated methods like Behaviour Cloning and TextDecision Transformer.</description><author>Hitesh Golchha, Sahil Yerawar, Dhruvesh Patel, Soham Dan, Keerthiram Murugesan</author><pubDate>Tue, 05 Mar 2024 17:26:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03141v1</guid></item><item><title>Simplicity in Complexity</title><link>http://arxiv.org/abs/2403.03134v1</link><description>The complexity of visual stimuli plays an important role in many cognitivephenomena, including attention, engagement, memorability, time perception andaesthetic evaluation. Despite its importance, complexity is poorly understoodand ironically, previous models of image complexity have been quite\textit{complex}. There have been many attempts to find handcrafted featuresthat explain complexity, but these features are usually dataset specific, andhence fail to generalise. On the other hand, more recent work has employed deepneural networks to predict complexity, but these models remain difficult tointerpret, and do not guide a theoretical understanding of the problem. Here wepropose to model complexity using segment-based representations of images. Weuse state-of-the-art segmentation models, SAM and FC-CLIP, to quantify thenumber of segments at multiple granularities, and the number of classes in animage respectively. We find that complexity is well-explained by a simplelinear model with these two features across six diverse image-sets ofnaturalistic scene and art images. This suggests that the complexity of imagescan be surprisingly simple.</description><author>Kevin Shen, Surabhi S Nath, Aenne Brielmann, Peter Dayan</author><pubDate>Tue, 05 Mar 2024 17:21:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03134v1</guid></item><item><title>Nonparametric Linear Feature Learning in Regression Through Regularisation</title><link>http://arxiv.org/abs/2307.12754v3</link><description>Representation learning plays a crucial role in automated feature selection,particularly in the context of high-dimensional data, where non-parametricmethods often struggle. In this study, we focus on supervised learningscenarios where the pertinent information resides within a lower-dimensionallinear subspace of the data, namely the multi-index model. If this subspacewere known, it would greatly enhance prediction, computation, andinterpretation. To address this challenge, we propose a novel method for linearfeature learning with non-parametric prediction, which simultaneously estimatesthe prediction function and the linear subspace. Our approach employs empiricalrisk minimisation, augmented with a penalty on function derivatives, ensuringversatility. Leveraging the orthogonality and rotation invariance properties ofHermite polynomials, we introduce our estimator, named RegFeaL. By utilisingalternative minimisation, we iteratively rotate the data to improve alignmentwith leading directions and accurately estimate the relevant dimension inpractical settings. We establish that our method yields a consistent estimatorof the prediction function with explicit rates. Additionally, we provideempirical results demonstrating the performance of RegFeaL in variousexperiments.</description><author>Bertille Follain, Francis Bach</author><pubDate>Tue, 05 Mar 2024 17:19:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12754v3</guid></item><item><title>PETA: Parameter-Efficient Trojan Attacks</title><link>http://arxiv.org/abs/2310.00648v4</link><description>Parameter-efficient fine-tuning (PEFT) enables efficient adaptation ofpre-trained language models (PLMs) to specific tasks. By tuning only a minimalset of (extra) parameters, PEFT achieves performance that is comparable tostandard fine-tuning. However, despite its prevalent use, the securityimplications of PEFT remain largely unexplored. In this paper, we take theinitial steps and present PETA, a novel trojan attack that compromises theweights of PLMs by accounting for downstream adaptation through bileveloptimization: the upper-level objective embeds the backdoor into a model whilethe lower-level objective simulates PEFT to both retain the PLM's task-specificperformance and ensure that the backdoor persists after fine-tuning. Withextensive evaluation across a variety of downstream tasks and trigger designs,we demonstrate PETA's effectiveness in terms of both attack success rate andclean accuracy, even when the attacker does not have full knowledge of thevictim user's training process.</description><author>Lauren Hong, Ting Wang</author><pubDate>Tue, 05 Mar 2024 17:15:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00648v4</guid></item><item><title>CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following</title><link>http://arxiv.org/abs/2403.03129v1</link><description>With the advancement of language models (LMs), their exposure to private datais increasingly inevitable, and their deployment (especially for smaller ones)on personal devices, such as PCs and smartphones, has become a prevailingtrend. In contexts laden with user information, enabling models to bothsafeguard user privacy and execute commands efficiently emerges as an essentialresearch imperative. In this paper, we propose CoGenesis, a collaborativegeneration framework integrating large (hosted on cloud infrastructure) andsmall models (deployed on local devices) to address privacy concerns logically.Initially, we design a pipeline to create personalized writing instructiondatasets enriched with extensive context details as the testbed of thisresearch issue. Subsequently, we introduce two variants of CoGenesis based onsketch and logits respectively. Our experimental findings, based on oursynthesized dataset and two additional open-source datasets, indicate that: 1)Large-scale models perform well when provided with user context but struggle inthe absence of such context. 2) While specialized smaller models fine-tuned onthe synthetic dataset show promise, they still lag behind their largercounterparts. 3) Our CoGenesis framework, utilizing mixed-scale models,showcases competitive performance, providing a feasible solution to privacyissues.</description><author>Kaiyan Zhang, Jianyu Wang, Ermo Hua, Biqing Qi, Ning Ding, Bowen Zhou</author><pubDate>Tue, 05 Mar 2024 17:15:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03129v1</guid></item><item><title>NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors</title><link>http://arxiv.org/abs/2403.03122v1</link><description>Faithfully modeling the space of articulations is a crucial task that allowsrecovery and generation of realistic poses, and remains a notorious challenge.To this end, we introduce Neural Riemannian Distance Fields (NRDFs),data-driven priors modeling the space of plausible articulations, representedas the zero-level-set of a neural field in a high-dimensionalproduct-quaternion space. To train NRDFs only on positive examples, weintroduce a new sampling algorithm, ensuring that the geodesic distances followa desired distribution, yielding a principled distance field learning paradigm.We then devise a projection algorithm to map any random pose onto the level-setby an adaptive-step Riemannian optimizer, adhering to the product manifold ofjoint rotations at all times. NRDFs can compute the Riemannian gradient viabackpropagation and by mathematical analogy, are related to Riemannian flowmatching, a recent generative model. We conduct a comprehensive evaluation ofNRDF against other pose priors in various downstream tasks, i.e., posegeneration, image-based pose estimation, and solving inverse kinematics,highlighting NRDF's superior performance. Besides humans, NRDF's versatilityextends to hand and animal poses, as it can effectively represent anyarticulation.</description><author>Yannan He, Garvita Tiwari, Tolga Birdal, Jan Eric Lenssen, Gerard Pons-Moll</author><pubDate>Tue, 05 Mar 2024 17:07:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03122v1</guid></item><item><title>Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training</title><link>http://arxiv.org/abs/2305.14342v4</link><description>Given the massive cost of language model pre-training, a non-trivialimprovement of the optimization algorithm would lead to a material reduction onthe time and cost of training. Adam and its variants have been state-of-the-artfor years, and more sophisticated second-order (Hessian-based) optimizers oftenincur too much per-step overhead. In this paper, we propose Sophia,Second-order Clipped Stochastic Optimization, a simple scalable second-orderoptimizer that uses a light-weight estimate of the diagonal Hessian as thepre-conditioner. The update is the moving average of the gradients divided bythe moving average of the estimated Hessian, followed by element-wise clipping.The clipping controls the worst-case update size and tames the negative impactof non-convexity and rapid change of Hessian along the trajectory. Sophia onlyestimates the diagonal Hessian every handful of iterations, which hasnegligible average per-step time and memory overhead. On language modeling withGPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-upcompared to Adam in the number of steps, total compute, and wall-clock time,achieving the same perplexity with 50% fewer steps, less total compute, andreduced wall-clock time. Theoretically, we show that Sophia, in a muchsimplified setting, adapts to the heterogeneous curvatures in differentparameter dimensions, and thus has a run-time bound that does not depend on thecondition number of the loss.</description><author>Hong Liu, Zhiyuan Li, David Hall, Percy Liang, Tengyu Ma</author><pubDate>Tue, 05 Mar 2024 17:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14342v4</guid></item><item><title>Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution</title><link>http://arxiv.org/abs/2403.03121v1</link><description>Large language models (LLMs) reflect societal norms and biases, especiallyabout gender. While societal biases and stereotypes have been extensivelyresearched in various NLP applications, there is a surprising gap for emotionanalysis. However, emotion and gender are closely linked in societal discourse.E.g., women are often thought of as more empathetic, while men's anger is moresocially accepted. To fill this gap, we present the first comprehensive studyof gendered emotion attribution in five state-of-the-art LLMs (open- andclosed-source). We investigate whether emotions are gendered, and whether thesevariations are based on societal stereotypes. We prompt the models to adopt agendered persona and attribute emotions to an event like 'When I had a seriousargument with a dear person'. We then analyze the emotions generated by themodels in relation to the gender-event pairs. We find that all modelsconsistently exhibit gendered emotions, influenced by gender stereotypes. Thesefindings are in line with established research in psychology and genderstudies. Our study sheds light on the complex societal interplay betweenlanguage, gender, and emotion. The reproduction of emotion stereotypes in LLMsallows us to use those models to study the topic in detail, but raisesquestions about the predictive use of those same LLMs for emotion applications.</description><author>Flor Miriam Plaza-del-Arco, Amanda Cercas Curry, Alba Curry, Gavin Abercrombie, Dirk Hovy</author><pubDate>Tue, 05 Mar 2024 17:04:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03121v1</guid></item><item><title>Motion-Corrected Moving Average: Including Post-Hoc Temporal Information for Improved Video Segmentation</title><link>http://arxiv.org/abs/2403.03120v1</link><description>Real-time computational speed and a high degree of precision are requirementsfor computer-assisted interventions. Applying a segmentation network to amedical video processing task can introduce significant inter-frame predictionnoise. Existing approaches can reduce inconsistencies by including temporalinformation but often impose requirements on the architecture or dataset. Thispaper proposes a method to include temporal information in any segmentationmodel and, thus, a technique to improve video segmentation performance withoutalterations during training or additional labeling. With Motion-CorrectedMoving Average, we refine the exponential moving average between the currentand previous predictions. Using optical flow to estimate the movement betweenconsecutive frames, we can shift the prior term in the moving-averagecalculation to align with the geometry of the current frame. The optical flowcalculation does not require the output of the model and can therefore beperformed in parallel, leading to no significant runtime penalty for ourapproach. We evaluate our approach on two publicly available segmentationdatasets and two proprietary endoscopic datasets and show improvements over abaseline approach.</description><author>Robert Mendel, Tobias Rueckert, Dirk Wilhelm, Daniel Rueckert, Christoph Palm</author><pubDate>Tue, 05 Mar 2024 17:01:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03120v1</guid></item><item><title>Equilibria in Two-Stage Facility Location with Atomic Clients</title><link>http://arxiv.org/abs/2403.03114v1</link><description>We consider competitive facility location as a two-stage multi-agent systemwith two types of clients. For a given host graph with weighted clients on thevertices, first facility agents strategically select vertices for opening theirfacilities. Then, the clients strategically select which of the openedfacilities in their neighborhood to patronize. Facilities want to attract asmuch client weight as possible, clients want to minimize congestion on thechosen facility. All recently studied versions of this model assume that clients can splittheir weight strategically. We consider clients with unsplittable weights, butallow mixed strategies. So clients may randomize over which facility topatronize. Besides modeling a natural client behavior, this subtle changeyields drastic changes, e.g., for a given facility placement, qualitativelydifferent client equilibria are possible. As our main result, we show that pure subgame perfect equilibria always existif all client weights are identical. For this, we use a novel potentialfunction argument, employing a hierarchical classification of the clients andsophisticated rounding in each step. In contrast, for non-identical clients, weshow that deciding the existence of even approximately stable states iscomputationally intractable. On the positive side, we give a tight bound of 2on the price of anarchy which implies high social welfare of equilibria, ifthey exist.</description><author>Simon Krogmann, Pascal Lenzner, Alexander Skopalik, Marc Uetz, Marnix C. Vos</author><pubDate>Tue, 05 Mar 2024 16:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03114v1</guid></item><item><title>Improved LiDAR Odometry and Mapping using Deep Semantic Segmentation and Novel Outliers Detection</title><link>http://arxiv.org/abs/2403.03111v1</link><description>Perception is a key element for enabling intelligent autonomous navigation.Understanding the semantics of the surrounding environment and accurate vehiclepose estimation are essential capabilities for autonomous vehicles, includingself-driving cars and mobile robots that perform complex tasks. Fast movingplatforms like self-driving cars impose a hard challenge for localization andmapping algorithms. In this work, we propose a novel framework for real-timeLiDAR odometry and mapping based on LOAM architecture for fast movingplatforms. Our framework utilizes semantic information produced by a deeplearning model to improve point-to-line and point-to-plane matching betweenLiDAR scans and build a semantic map of the environment, leading to moreaccurate motion estimation using LiDAR data. We observe that including semanticinformation in the matching process introduces a new type of outlier matches tothe process, where matching occur between different objects of the samesemantic class. To this end, we propose a novel algorithm that explicitlyidentifies and discards potential outliers in the matching process. In ourexperiments, we study the effect of improving the matching process on therobustness of LiDAR odometry against high speed motion. Our experimentalevaluations on KITTI dataset demonstrate that utilizing semantic informationand rejecting outliers significantly enhance the robustness of LiDAR odometryand mapping when there are large gaps between scan acquisition poses, which istypical for fast moving platforms.</description><author>Mohamed Afifi, Mohamed ElHelw</author><pubDate>Tue, 05 Mar 2024 16:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03111v1</guid></item><item><title>Car-Following Models: A Multidisciplinary Review</title><link>http://arxiv.org/abs/2304.07143v4</link><description>Car-following (CF) algorithms are crucial components of traffic simulationsand have been integrated into many production vehicles equipped with AdvancedDriving Assistance Systems (ADAS). Insights from the model of car-followingbehavior help us understand the causes of various macro phenomena that arisefrom interactions between pairs of vehicles. Car-following models encompassmultiple disciplines, including traffic engineering, physics, dynamic systemcontrol, cognitive science, machine learning, and reinforcement learning. Thispaper presents an extensive survey that highlights the differences,complementarities, and overlaps among microscopic traffic flow and controlmodels based on their underlying principles and design logic. It reviewsrepresentative algorithms, ranging from theory-based kinematic models,Psycho-Physical Models, and Adaptive cruise control models to data-drivenalgorithms like Reinforcement Learning (RL) and Imitation Learning (IL). Themanuscript discusses the strengths and limitations of these models and explorestheir applications in different contexts. This review synthesizes existingresearches across different domains to fill knowledge gaps and offer guidancefor future research by identifying the latest trends in car following modelsand their applications.</description><author>Tianya Terry Zhang, Ph. D., Peter J. Jin, Ph. D., Sean T. McQuade, Ph. D., Alexandre Bayen, Ph. D., Benedetto Piccoli</author><pubDate>Tue, 05 Mar 2024 16:50:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07143v4</guid></item><item><title>DiffDA: a Diffusion model for weather-scale Data Assimilation</title><link>http://arxiv.org/abs/2401.05932v2</link><description>The generation of initial conditions via accurate data assimilation iscrucial for weather forecasting and climate modeling. We propose DiffDA as adenoising diffusion model capable of assimilating atmospheric variables usingpredicted states and sparse observations. Acknowledging the similarity betweena weather forecast model and a denoising diffusion model dedicated to weatherapplications, we adapt the pretrained GraphCast neural network as the backboneof the diffusion model. Through experiments based on simulated observationsfrom the ERA5 reanalysis dataset, our method can produce assimilated globalatmospheric data consistent with observations at 0.25 deg (~30km) resolutionglobally. This marks the highest resolution achieved by ML data assimilationmodels. The experiments also show that the initial conditions assimilated fromsparse observations (less than 0.77% of gridded data) and 48-hour forecast canbe used for forecast models with a loss of lead time of at most 24 hourscompared to initial conditions from state-of-the-art data assimilation in ERA5.This enables the application of the method to real-world applications, such ascreating reanalysis datasets with autoregressive data assimilation.</description><author>Langwen Huang, Lukas Gianinazzi, Yuejiang Yu, Peter D. Dueben, Torsten Hoefler</author><pubDate>Tue, 05 Mar 2024 16:48:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05932v2</guid></item><item><title>Input-gradient space particle inference for neural network ensembles</title><link>http://arxiv.org/abs/2306.02775v3</link><description>Deep Ensembles (DEs) demonstrate improved accuracy, calibration androbustness to perturbations over single neural networks partly due to theirfunctional diversity. Particle-based variational inference (ParVI) methodsenhance diversity by formalizing a repulsion term based on a network similaritykernel. However, weight-space repulsion is inefficient due toover-parameterization, while direct function-space repulsion has been found toproduce little improvement over DEs. To sidestep these difficulties, we proposeFirst-order Repulsive Deep Ensemble (FoRDE), an ensemble learning method basedon ParVI, which performs repulsion in the space of first-order input gradients.As input gradients uniquely characterize a function up to translation and aremuch smaller in dimension than the weights, this method guarantees thatensemble members are functionally different. Intuitively, diversifying theinput gradients encourages each network to learn different features, which isexpected to improve the robustness of an ensemble. Experiments on imageclassification datasets and transfer learning tasks show that FoRDEsignificantly outperforms the gold-standard DEs and other ensemble methods inaccuracy and calibration under covariate shift due to input perturbations.</description><author>Trung Trinh, Markus Heinonen, Luigi Acerbi, Samuel Kaski</author><pubDate>Tue, 05 Mar 2024 16:44:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02775v3</guid></item><item><title>Emergent Equivariance in Deep Ensembles</title><link>http://arxiv.org/abs/2403.03103v1</link><description>We demonstrate that deep ensembles are secretly equivariant models. Moreprecisely, we show that deep ensembles become equivariant for all inputs and atall training times by simply using data augmentation. Crucially, equivarianceholds off-manifold and for any architecture in the infinite width limit. Theequivariance is emergent in the sense that predictions of individual ensemblemembers are not equivariant but their collective prediction is. Neural tangentkernel theory is used to derive this result and we verify our theoreticalinsights using detailed numerical experiments.</description><author>Jan E. Gerken, Pan Kessel</author><pubDate>Tue, 05 Mar 2024 16:43:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03103v1</guid></item><item><title>"In Dialogues We Learn": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning</title><link>http://arxiv.org/abs/2403.03102v1</link><description>Personalized dialogue systems have gained significant attention in recentyears for their ability to generate responses in alignment with differentpersonas. However, most existing approaches rely on pre-defined personalprofiles, which are not only time-consuming and labor-intensive to create butalso lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuningframework that enhances the ability of pre-trained large language models toleverage dialogue history to characterize persona for completing personalizeddialogue generation tasks without pre-defined profiles. Our experiments onthree datasets demonstrate that IDL brings substantial improvements, with BLEUand ROUGE scores increasing by up to 200% and 247%, respectively. Additionally,the results of human evaluations further validate the efficacy of our proposedmethod.</description><author>Chuanqi Cheng, Quan Tu, Wei Wu, Shuo Shang, Cunli Mao, Zhengtao Yu, Rui Yan</author><pubDate>Tue, 05 Mar 2024 16:43:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03102v1</guid></item><item><title>KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents</title><link>http://arxiv.org/abs/2403.03101v1</link><description>Large Language Models (LLMs) have demonstrated great potential in complexreasoning tasks, yet they fall short when tackling more sophisticatedchallenges, especially when interacting with environments through generatingexecutable actions. This inadequacy primarily stems from the lack of built-inaction knowledge in language agents, which fails to effectively guide theplanning trajectories during task solving and results in planninghallucination. To address this issue, we introduce KnowAgent, a novel approachdesigned to enhance the planning capabilities of LLMs by incorporating explicitaction knowledge. Specifically, KnowAgent employs an action knowledge base anda knowledgeable self-learning strategy to constrain the action path duringplanning, enabling more reasonable trajectory synthesis, and thereby enhancingthe planning performance of language agents. Experimental results on HotpotQAand ALFWorld based on various backbone models demonstrate that KnowAgent canachieve comparable or superior performance to existing baselines. Furtheranalysis indicates the effectiveness of KnowAgent in terms of planninghallucinations mitigation. Code is available inhttps://github.com/zjunlp/KnowAgent.</description><author>Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Ningyu Zhang, Shiwei Lyu, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen</author><pubDate>Tue, 05 Mar 2024 16:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03101v1</guid></item><item><title>Know your exceptions: Towards an Ontology of Exceptions in Knowledge Representation</title><link>http://arxiv.org/abs/2403.00685v2</link><description>Defeasible reasoning is a kind of reasoning where some generalisations maynot be valid in all circumstances, that is general conclusions may fail in somecases. Various formalisms have been developed to model this kind of reasoning,which is characteristic of common-sense contexts. However, it is not easy for amodeller to choose among these systems the one that better fits its domain froman ontological point of view. In this paper we first propose a framework basedon the notions of exceptionality and defeasibility in order to be able tocompare formalisms and reveal their ontological commitments. Then, we applythis framework to compare four systems, showing the differences that may occurfrom an ontological perspective.</description><author>Gabriele Sacco, Loris Bozzato, Oliver Kutz</author><pubDate>Tue, 05 Mar 2024 16:35:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00685v2</guid></item><item><title>NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models</title><link>http://arxiv.org/abs/2403.03100v1</link><description>While recent large-scale text-to-speech (TTS) models have achievedsignificant progress, they still fall short in speech quality, similarity, andprosody. Considering speech intricately encompasses various attributes (e.g.,content, prosody, timbre, and acoustic details) that pose significantchallenges for generation, a natural idea is to factorize speech intoindividual subspaces representing different attributes and generate themindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system withnovel factorized diffusion models to generate natural speech in a zero-shotway. Specifically, 1) we design a neural codec with factorized vectorquantization (FVQ) to disentangle speech waveform into subspaces of content,prosody, timbre, and acoustic details; 2) we propose a factorized diffusionmodel to generate attributes in each subspace following its correspondingprompt. With this factorization design, NaturalSpeech 3 can effectively andefficiently model the intricate speech with disentangled subspaces in adivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms thestate-of-the-art TTS systems on quality, similarity, prosody, andintelligibility. Furthermore, we achieve better performance by scaling to 1Bparameters and 200K hours of training data.</description><author>Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao</author><pubDate>Tue, 05 Mar 2024 16:35:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03100v1</guid></item><item><title>Mitigating Temporal Misalignment by Discarding Outdated Facts</title><link>http://arxiv.org/abs/2305.14824v3</link><description>While large language models are able to retain vast amounts of worldknowledge seen during pretraining, such knowledge is prone to going out of dateand is nontrivial to update. Furthermore, these models are often used undertemporal misalignment, tasked with answering questions about the present,despite having only been trained on data collected in the past. To mitigate theeffects of temporal misalignment, we propose fact duration prediction: the taskof predicting how long a given fact will remain true. In our experiments, wedemonstrate that identifying which facts are prone to rapid change can helpmodels avoid reciting outdated information and determine which predictionsrequire seeking out up-to-date knowledge sources. We also show how modelingfact duration improves calibration for knowledge-intensive tasks, such asopen-retrieval question answering, under temporal misalignment, by discardingvolatile facts. Our data and code are released publicly athttps://github.com/mikejqzhang/mitigating_misalignment.</description><author>Michael J. Q. Zhang, Eunsol Choi</author><pubDate>Tue, 05 Mar 2024 16:32:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14824v3</guid></item><item><title>Neural Control System for Continuous Glucose Monitoring and Maintenance</title><link>http://arxiv.org/abs/2402.13852v2</link><description>Precise glucose level monitoring is critical for people with diabetes toavoid serious complications. While there are several methods for continuousglucose level monitoring, research on maintenance devices is limited. Tomitigate the gap, we provide a novel neural control system for continuousglucose monitoring and management that uses differential predictive control.Our approach, led by a sophisticated neural policy and differentiable modeling,constantly adjusts insulin supply in real-time, thereby improving glucose leveloptimization in the body. This end-to-end method maximizes efficiency,providing personalized care and improved health outcomes, as confirmed byempirical evidence.</description><author>Azmine Toushik Wasi</author><pubDate>Tue, 05 Mar 2024 16:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13852v2</guid></item><item><title>Attacks on Node Attributes in Graph Neural Networks</title><link>http://arxiv.org/abs/2402.12426v2</link><description>Graphs are commonly used to model complex networks prevalent in modern socialmedia and literacy applications. Our research investigates the vulnerability ofthese graphs through the application of feature based adversarial attacks,focusing on both decision time attacks and poisoning attacks. In contrast tostate of the art models like Net Attack and Meta Attack, which target nodeattributes and graph structure, our study specifically targets node attributes.For our analysis, we utilized the text dataset Hellaswag and graph datasetsCora and CiteSeer, providing a diverse basis for evaluation. Our findingsindicate that decision time attacks using Projected Gradient Descent (PGD) aremore potent compared to poisoning attacks that employ Mean Node Embeddings andGraph Contrastive Learning strategies. This provides insights for graph datasecurity, pinpointing where graph-based models are most vulnerable and therebyinforming the development of stronger defense mechanisms against such attacks.</description><author>Ying Xu, Michael Lanier, Anindya Sarkar, Yevgeniy Vorobeychik</author><pubDate>Tue, 05 Mar 2024 16:31:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12426v2</guid></item><item><title>Cross Pseudo-Labeling for Semi-Supervised Audio-Visual Source Localization</title><link>http://arxiv.org/abs/2403.03095v1</link><description>Audio-Visual Source Localization (AVSL) is the task of identifying specificsounding objects in the scene given audio cues. In our work, we focus onsemi-supervised AVSL with pseudo-labeling. To address the issues with vanillahard pseudo-labels including bias accumulation, noise sensitivity, andinstability, we propose a novel method named Cross Pseudo-Labeling (XPL),wherein two models learn from each other with the cross-refine mechanism toavoid bias accumulation. We equip XPL with two effective components. Firstly,the soft pseudo-labels with sharpening and pseudo-label exponential movingaverage mechanisms enable models to achieve gradual self-improvement and ensurestable training. Secondly, the curriculum data selection module adaptivelyselects pseudo-labels with high quality during training to mitigate potentialbias. Experimental results demonstrate that XPL significantly outperformsexisting methods, achieving state-of-the-art performance while effectivelymitigating confirmation bias and ensuring training stability.</description><author>Yuxin Guo, Shijie Ma, Yuhao Zhao, Hu Su, Wei Zou</author><pubDate>Tue, 05 Mar 2024 16:28:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03095v1</guid></item><item><title>VQSynery: Robust Drug Synergy Prediction With Vector Quantization Mechanism</title><link>http://arxiv.org/abs/2403.03089v1</link><description>The pursuit of optimizing cancer therapies is significantly advanced by theaccurate prediction of drug synergy. Traditional methods, such as clinicaltrials, are reliable yet encumbered by extensive time and financial demands.The emergence of high-throughput screening and computational innovations hasheralded a shift towards more efficient methodologies for exploring druginteractions. In this study, we present VQSynergy, a novel framework thatemploys the Vector Quantization (VQ) mechanism, integrated with gated residualsand a tailored attention mechanism, to enhance the precision andgeneralizability of drug synergy predictions. Our findings demonstrate thatVQSynergy surpasses existing models in terms of robustness, particularly underGaussian noise conditions, highlighting its superior performance and utility inthe complex and often noisy domain of drug synergy research. This studyunderscores the potential of VQSynergy in revolutionizing the field through itsadvanced predictive capabilities, thereby contributing to the optimization ofcancer treatment strategies.</description><author>Jiawei Wu, Mingyuan Yan, Dianbo Liu</author><pubDate>Tue, 05 Mar 2024 16:21:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03089v1</guid></item><item><title>Optimal Inference in Contextual Stochastic Block Models</title><link>http://arxiv.org/abs/2306.07948v2</link><description>The contextual stochastic block model (cSBM) was proposed for unsupervisedcommunity detection on attributed graphs where both the graph and thehigh-dimensional node information correlate with node labels. In the context ofmachine learning on graphs, the cSBM has been widely used as a syntheticdataset for evaluating the performance of graph-neural networks (GNNs) forsemi-supervised node classification. We consider a probabilistic Bayes-optimalformulation of the inference problem and we derive a belief-propagation-basedalgorithm for the semi-supervised cSBM; we conjecture it is optimal in theconsidered setting and we provide its implementation. We show that there can bea considerable gap between the accuracy reached by this algorithm and theperformance of the GNN architectures proposed in the literature. This suggeststhat the cSBM, along with the comparison to the performance of the optimalalgorithm, readily accessible via our implementation, can be instrumental inthe development of more performant GNN architectures.</description><author>O. Duranthon, L. Zdeborov√°</author><pubDate>Tue, 05 Mar 2024 16:09:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07948v2</guid></item><item><title>Recall-Oriented Continual Learning with Generative Adversarial Meta-Model</title><link>http://arxiv.org/abs/2403.03082v1</link><description>The stability-plasticity dilemma is a major challenge in continual learning,as it involves balancing the conflicting objectives of maintaining performanceon previous tasks while learning new tasks. In this paper, we propose therecall-oriented continual learning framework to address this challenge.Inspired by the human brain's ability to separate the mechanisms responsiblefor stability and plasticity, our framework consists of a two-levelarchitecture where an inference network effectively acquires new knowledge anda generative network recalls past knowledge when necessary. In particular, tomaximize the stability of past knowledge, we investigate the complexity ofknowledge depending on different representations, and thereby introducinggenerative adversarial meta-model (GAMM) that incrementally learnstask-specific parameters instead of input data samples of the task. Through ourexperiments, we show that our framework not only effectively learns newknowledge without any disruption but also achieves high stability of previousknowledge in both task-aware and task-agnostic learning scenarios. Our code isavailable at: https://github.com/bigdata-inha/recall-oriented-cl-framework.</description><author>Haneol Kang, Dong-Wan Choi</author><pubDate>Tue, 05 Mar 2024 16:08:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03082v1</guid></item><item><title>MiKASA: Multi-Key-Anchor &amp; Scene-Aware Transformer for 3D Visual Grounding</title><link>http://arxiv.org/abs/2403.03077v1</link><description>3D visual grounding involves matching natural language descriptions withtheir corresponding objects in 3D spaces. Existing methods often facechallenges with accuracy in object recognition and struggle in interpretingcomplex linguistic queries, particularly with descriptions that involvemultiple anchors or are view-dependent. In response, we present the MiKASA(Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained modelintegrates a self-attention-based scene-aware object encoder and an originalmulti-key-anchor technique, enhancing object recognition accuracy and theunderstanding of spatial relationships. Furthermore, MiKASA improves theexplainability of decision-making, facilitating error diagnosis. Our modelachieves the highest overall accuracy in the Referit3D challenge for both theSr3D and Nr3D datasets, particularly excelling by a large margin in categoriesthat require viewpoint-dependent descriptions. The source code and additional resources for this project are available onGitHub: https://github.com/birdy666/MiKASA-3DVG</description><author>Chun-Peng Chang, Shaoxiang Wang, Alain Pagani, Didier Stricker</author><pubDate>Tue, 05 Mar 2024 16:01:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03077v1</guid></item><item><title>Detecting Concrete Visual Tokens for Multimodal Machine Translation</title><link>http://arxiv.org/abs/2403.03075v1</link><description>The challenge of visual grounding and masking in multimodal machinetranslation (MMT) systems has encouraged varying approaches to the detectionand selection of visually-grounded text tokens for masking. We introduce newmethods for detection of visually and contextually relevant (concrete) tokensfrom source sentences, including detection with natural language processing(NLP), detection with object detection, and a joint detection-verificationtechnique. We also introduce new methods for selection of detected tokens,including shortest $n$ tokens, longest $n$ tokens, and all detected concretetokens. We utilize the GRAM MMT architecture to train models againstsynthetically collated multimodal datasets of source images with maskedsentences, showing performance improvements and improved usage of visualcontext during translation tasks over the baseline model.</description><author>Braeden Bowen, Vipin Vijayan, Scott Grigsby, Timothy Anderson, Jeremy Gwinnup</author><pubDate>Tue, 05 Mar 2024 16:01:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03075v1</guid></item><item><title>On a Neural Implementation of Brenier's Polar Factorization</title><link>http://arxiv.org/abs/2403.03071v1</link><description>In 1991, Brenier proved a theorem that generalizes the $QR$ decomposition forsquare matrices -- factored as PSD $\times$ unitary -- to any vector field$F:\mathbb{R}^d\rightarrow \mathbb{R}^d$. The theorem, known as the polarfactorization theorem, states that any field $F$ can be recovered as thecomposition of the gradient of a convex function $u$ with a measure-preservingmap $M$, namely $F=\nabla u \circ M$. We propose a practical implementation ofthis far-reaching theoretical result, and explore possible uses within machinelearning. The theorem is closely related to optimal transport (OT) theory, andwe borrow from recent advances in the field of neural optimal transport toparameterize the potential $u$ as an input convex neural network. The map $M$can be either evaluated pointwise using $u^*$, the convex conjugate of $u$,through the identity $M=\nabla u^* \circ F$, or learned as an auxiliarynetwork. Because $M$ is, in general, not injective, we consider the additionaltask of estimating the ill-posed inverse map that can approximate the pre-imagemeasure $M^{-1}$ using a stochastic generator. We illustrate possibleapplications of \citeauthor{Brenier1991PolarFA}'s polar factorization tonon-convex optimization problems, as well as sampling of densities that are notlog-concave.</description><author>Nina Vesseron, Marco Cuturi</author><pubDate>Tue, 05 Mar 2024 15:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03071v1</guid></item><item><title>Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families</title><link>http://arxiv.org/abs/2403.03069v1</link><description>We consider the task of estimating variational autoencoders (VAEs) when thetraining data is incomplete. We show that missing data increases the complexityof the model's posterior distribution over the latent variables compared to thefully-observed case. The increased complexity may adversely affect the fit ofthe model due to a mismatch between the variational and model posteriordistributions. We introduce two strategies based on (i) finitevariational-mixture and (ii) imputation-based variational-mixture distributionsto address the increased posterior complexity. Through a comprehensiveevaluation of the proposed approaches, we show that variational mixtures areeffective at improving the accuracy of VAE estimation from incomplete data.</description><author>Vaidotas Simkus, Michael U. Gutmann</author><pubDate>Tue, 05 Mar 2024 15:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03069v1</guid></item><item><title>CrackNex: a Few-shot Low-light Crack Segmentation Model Based on Retinex Theory for UAV Inspections</title><link>http://arxiv.org/abs/2403.03063v1</link><description>Routine visual inspections of concrete structures are imperative forupholding the safety and integrity of critical infrastructure. Such visualinspections sometimes happen under low-light conditions, e.g., checking forbridge health. Crack segmentation under such conditions is challenging due tothe poor contrast between cracks and their surroundings. However, most deeplearning methods are designed for well-illuminated crack images and hence theirperformance drops dramatically in low-light scenes. In addition, conventionalapproaches require many annotated low-light crack images which istime-consuming. In this paper, we address these challenges by proposingCrackNex, a framework that utilizes reflectance information based on RetinexTheory to help the model learn a unified illumination-invariant representation.Furthermore, we utilize few-shot segmentation to solve the inefficient trainingdata problem. In CrackNex, both a support prototype and a reflectance prototypeare extracted from the support set. Then, a prototype fusion module is designedto integrate the features from both prototypes. CrackNex outperforms the SOTAmethods on multiple datasets. Additionally, we present the first benchmarkdataset, LCSD, for low-light crack segmentation. LCSD consists of 102well-illuminated crack images and 41 low-light crack images. The dataset andcode are available at https://github.com/zy1296/CrackNex.</description><author>Zhen Yao, Jiawei Xu, Shuhang Hou, Mooi Choo Chuah</author><pubDate>Tue, 05 Mar 2024 15:52:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03063v1</guid></item><item><title>Mixed-Strategy Nash Equilibrium for Crowd Navigation</title><link>http://arxiv.org/abs/2403.01537v2</link><description>We address the problem of finding mixed-strategy Nash equilibrium for crowdnavigation. Mixed-strategy Nash equilibrium provides a rigorous model for therobot to anticipate uncertain yet cooperative human behavior in crowds, but thecomputation cost is often too high for scalable and real-time decision-making.Here we prove that a simple iterative Bayesian updating scheme converges to theNash equilibrium of a mixed-strategy social navigation game. Furthermore, wepropose a data-driven framework to construct the game by initializing agentstrategies as Gaussian processes learned from human datasets. Based on theproposed mixed-strategy Nash equilibrium model, we develop a sampling-basedcrowd navigation framework that can be integrated into existing navigationmethods and runs in real-time on a laptop CPU. We evaluate our framework inboth simulated environments and real-world human datasets in unstructuredenvironments. Our framework consistently outperforms both non-learning andlearning-based methods on both safety and navigation efficiency and reacheshuman-level crowd navigation performance on top of a meta-planner.</description><author>Muchen Sun, Francesca Baldini, Peter Trautman, Todd Murphey</author><pubDate>Tue, 05 Mar 2024 15:48:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01537v2</guid></item><item><title>Machine Learning Assisted Adjustment Boosts Inferential Efficiency of Randomized Controlled Trials</title><link>http://arxiv.org/abs/2403.03058v1</link><description>In this work, we proposed a novel inferential procedure assisted by machinelearning based adjustment for randomized control trials. The method wasdeveloped under the Rosenbaum's framework of exact tests in randomizedexperiments with covariate adjustments. Through extensive simulationexperiments, we showed the proposed method can robustly control the type Ierror and can boost the inference efficiency for a randomized controlled trial(RCT). This advantage was further demonstrated in a real world example. Thesimplicity and robustness of the proposed method makes it a competitivecandidate as a routine inference procedure for RCTs, especially when the numberof baseline covariates is large, and when nonlinear association or interactionamong covariates is expected. Its application may remarkably reduce therequired sample size and cost of RCTs, such as phase III clinical trials.</description><author>Han Yu, Alan D. Hutson</author><pubDate>Tue, 05 Mar 2024 15:48:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03058v1</guid></item><item><title>Distributed Policy Gradient for Linear Quadratic Networked Control with Limited Communication Range</title><link>http://arxiv.org/abs/2403.03055v1</link><description>This paper proposes a scalable distributed policy gradient method and provesits convergence to near-optimal solution in multi-agent linear quadraticnetworked systems. The agents engage within a specified network under localcommunication constraints, implying that each agent can only exchangeinformation with a limited number of neighboring agents. On the underlyinggraph of the network, each agent implements its control input depending on itsnearby neighbors' states in the linear quadratic control setting. We show thatit is possible to approximate the exact gradient only using local information.Compared with the centralized optimal controller, the performance gap decreasesto zero exponentially as the communication and control ranges increase. We alsodemonstrate how increasing the communication range enhances system stability inthe gradient descent process, thereby elucidating a critical trade-off. Thesimulation results verify our theoretical findings.</description><author>Yuzi Yan, Yuan Shen</author><pubDate>Tue, 05 Mar 2024 15:38:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03055v1</guid></item><item><title>Neural Codebook Design for Network Beam Management</title><link>http://arxiv.org/abs/2403.03053v1</link><description>Obtaining accurate and timely channel state information (CSI) is afundamental challenge for large antenna systems. Mobile systems like 5G use abeam management framework that joins the initial access, beamforming, CSIacquisition, and data transmission. The design of codebooks for these stages,however, is challenging due to their interrelationships, varying array sizes,and site-specific channel and user distributions. Furthermore, beam managementis often focused on single-sector operations while ignoring the overarchingnetwork- and system-level optimization. In this paper, we proposed anend-to-end learned codebook design algorithm, network beamspace learning (NBL),that captures and optimizes codebooks to mitigate interference while maximizingthe achievable performance with extremely large hybrid arrays. The proposedalgorithm requires limited shared information yet designs codebooks thatoutperform traditional codebooks by over 10dB in beam alignment and achievemore than 25% improvements in network spectral efficiency.</description><author>Ryan M. Dreifuerst, Robert W. Heath Jr</author><pubDate>Tue, 05 Mar 2024 15:37:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03053v1</guid></item><item><title>A Generalized Neural Diffusion Framework on Graphs</title><link>http://arxiv.org/abs/2312.08616v4</link><description>Recent studies reveal the connection between GNNs and the diffusion process,which motivates many diffusion-based GNNs to be proposed. However, since thesetwo mechanisms are closely related, one fundamental question naturally arises:Is there a general diffusion framework that can formally unify these GNNs? Theanswer to this question can not only deepen our understanding of the learningprocess of GNNs, but also may open a new door to design a broad new class ofGNNs. In this paper, we propose a general diffusion equation framework with thefidelity term, which formally establishes the relationship between thediffusion process with more GNNs. Meanwhile, with this framework, we identifyone characteristic of graph diffusion networks, i.e., the current neuraldiffusion process only corresponds to the first-order diffusion equation.However, by an experimental investigation, we show that the labels ofhigh-order neighbors actually exhibit monophily property, which induces thesimilarity based on labels among high-order neighbors without requiring thesimilarity among first-order neighbors. This discovery motives to design a newhigh-order neighbor-aware diffusion equation, and derive a new type of graphdiffusion network (HiD-Net) based on the framework. With the high-orderdiffusion equation, HiD-Net is more robust against attacks and works on bothhomophily and heterophily graphs. We not only theoretically analyze therelation between HiD-Net with high-order random walk, but also provide atheoretical convergence guarantee. Extensive experimental results welldemonstrate the effectiveness of HiD-Net over state-of-the-art graph diffusionnetworks.</description><author>Yibo Li, Xiao Wang, Hongrui Liu, Chuan Shi</author><pubDate>Tue, 05 Mar 2024 15:31:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08616v4</guid></item><item><title>Adding Multimodal Capabilities to a Text-only Translation Model</title><link>http://arxiv.org/abs/2403.03045v1</link><description>While most current work in multimodal machine translation (MMT) uses theMulti30k dataset for training and evaluation, we find that the resulting modelsoverfit to the Multi30k dataset to an extreme degree. Consequently, thesemodels perform very badly when evaluated against typical text-only testing setssuch as the WMT newstest datasets. In order to perform well on both Multi30kand typical text-only datasets, we use a performant text-only machinetranslation (MT) model as the starting point of our MMT model. We addvision-text adapter layers connected via gating mechanisms to the MT model, andincrementally transform the MT model into an MMT model by 1) pre-training usingvision-based masking of the source text and 2) fine-tuning on Multi30k.</description><author>Vipin Vijayan, Braeden Bowen, Scott Grigsby, Timothy Anderson, Jeremy Gwinnup</author><pubDate>Tue, 05 Mar 2024 15:28:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03045v1</guid></item><item><title>A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives</title><link>http://arxiv.org/abs/2403.03037v1</link><description>Human comprehension of a video stream is naturally broad: in a few instants,we are able to understand what is happening, the relevance and relationship ofobjects, and forecast what will follow in the near future, everything all atonce. We believe that - to effectively transfer such an holistic perception tointelligent machines - an important role is played by learning to correlateconcepts and to abstract knowledge coming from different tasks, tosynergistically exploit them when learning novel skills. To accomplish this, weseek for a unified approach to video understanding which combines sharedtemporal modelling of human actions with minimal overhead, to support multipledownstream tasks and enable cooperation when learning novel skills. We thenpropose EgoPack, a solution that creates a collection of task perspectives thatcan be carried across downstream tasks and used as a potential source ofadditional insights, as a backpack of skills that a robot can carry around anduse when needed. We demonstrate the effectiveness and efficiency of ourapproach on four Ego4D benchmarks, outperforming current state-of-the-artmethods.</description><author>Simone Alberto Peirone, Francesca Pistilli, Antonio Alliegro, Giuseppe Averta</author><pubDate>Tue, 05 Mar 2024 15:18:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03037v1</guid></item><item><title>Learning to Use Tools via Cooperative and Interactive Agents</title><link>http://arxiv.org/abs/2403.03031v1</link><description>Tool learning empowers large language models (LLMs) as agents to use externaltools to extend their capability. Existing methods employ one single LLM-basedagent to iteratively select and execute tools, thereafter incorporating theresult into the next action prediction. However, they still suffer frompotential performance degradation when addressing complex tasks due to: (1) thelimitation of the inherent capability of a single LLM to perform diverseactions, and (2) the struggle to adaptively correct mistakes when the taskfails. To mitigate these problems, we propose the ConAgents, a Cooperative andinteractive Agents framework, which modularizes the workflow of tool learninginto Grounding, Execution, and Observing agents. We also introduce an iterativecalibration (IterCali) method, enabling the agents to adapt themselves based onthe feedback from the tool environment. Experiments conducted on three datasetsdemonstrate the superiority of our ConAgents (e.g., 6 point improvement overthe SOTA baseline). We further provide fine-granularity analysis for theefficiency and consistency of our framework.</description><author>Zhengliang Shi, Shen Gao, Xiuyi Chen, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Pengjie Ren, Suzan Verberne, Zhaochun Ren</author><pubDate>Tue, 05 Mar 2024 15:08:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03031v1</guid></item><item><title>Unifying Controller Design for Stabilizing Nonlinear Systems with Norm-Bounded Control Inputs</title><link>http://arxiv.org/abs/2403.03030v1</link><description>This paper revisits a classical challenge in the design of stabilizingcontrollers for nonlinear systems with a norm-bounded input constraint. Byextending Lin-Sontag's universal formula and introducing a generic(state-dependent) scaling term, a unifying controller design method isproposed. The incorporation of this generic scaling term gives a unifiedcontroller and enables the derivation of alternative universal formulas withvarious favorable properties, which makes it suitable for tailored controldesigns to meet specific requirements and provides versatility across differentcontrol scenarios. Additionally, we present a constructive approach todetermine the optimal scaling term, leading to an explicit solution to anoptimization problem, named optimization-based universal formula. The resultingcontroller ensures asymptotic stability, satisfies a norm-bounded inputconstraint, and optimizes a predefined cost function. Finally, the essentialproperties of the unified controllers are analyzed, including smoothness,continuity at the origin, stability margin, and inverse optimality. Simulationsvalidate the approach, showcasing its effectiveness in addressing a challengingstabilizing control problem of a nonlinear system.</description><author>Ming Li, Zhiyong Sun, Siep Weiland</author><pubDate>Tue, 05 Mar 2024 15:06:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03030v1</guid></item><item><title>Language-EXtended Indoor SLAM (LEXIS): A Versatile System for Real-time Visual Scene Understanding</title><link>http://arxiv.org/abs/2309.15065v2</link><description>Versatile and adaptive semantic understanding would enable autonomous systemsto comprehend and interact with their surroundings. Existing fixed-class modelslimit the adaptability of indoor mobile and assistive autonomous systems. Inthis work, we introduce LEXIS, a real-time indoor Simultaneous Localization andMapping (SLAM) system that harnesses the open-vocabulary nature of LargeLanguage Models (LLMs) to create a unified approach to scene understanding andplace recognition. The approach first builds a topological SLAM graph of theenvironment (using visual-inertial odometry) and embeds ContrastiveLanguage-Image Pretraining (CLIP) features in the graph nodes. We use thisrepresentation for flexible room classification and segmentation, serving as abasis for room-centric place recognition. This allows loop closure searches tobe directed towards semantically relevant places. Our proposed system isevaluated using both public, simulated data and real-world data, coveringoffice and home environments. It successfully categorizes rooms with varyinglayouts and dimensions and outperforms the state-of-the-art (SOTA). For placerecognition and trajectory estimation tasks we achieve equivalent performanceto the SOTA, all also utilizing the same pre-trained model. Lastly, wedemonstrate the system's potential for planning.</description><author>Christina Kassab, Matias Mattamala, Lintong Zhang, Maurice Fallon</author><pubDate>Tue, 05 Mar 2024 15:05:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15065v2</guid></item><item><title>Socratic Reasoning Improves Positive Text Rewriting</title><link>http://arxiv.org/abs/2403.03029v1</link><description>Reframing a negative into a positive thought is at the crux of severalcognitive approaches to mental health and psychotherapy that could be made moreaccessible by large language model-based solutions. Such reframing is typicallynon-trivial and requires multiple rationalization steps to uncover theunderlying issue of a negative thought and transform it to be more positive.However, this rationalization process is currently neglected by both datasetsand models which reframe thoughts in one step. In this work, we address thisgap by augmenting open-source datasets for positive text rewriting withsynthetically-generated Socratic rationales using a novel framework called\textsc{SocraticReframe}. \textsc{SocraticReframe} uses a sequence ofquestion-answer pairs to rationalize the thought rewriting process. We showthat such Socratic rationales significantly improve positive text rewriting fordifferent open-source LLMs according to both automatic and human evaluationsguided by criteria from psychotherapy research.</description><author>Anmol Goel, Nico Daheim, Iryna Gurevych</author><pubDate>Tue, 05 Mar 2024 15:05:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03029v1</guid></item><item><title>Word Importance Explains How Prompts Affect Language Model Outputs</title><link>http://arxiv.org/abs/2403.03028v1</link><description>The emergence of large language models (LLMs) has revolutionized numerousapplications across industries. However, their "black box" nature often hindersthe understanding of how they make specific decisions, raising concerns abouttheir transparency, reliability, and ethical use. This study presents a methodto improve the explainability of LLMs by varying individual words in prompts touncover their statistical impact on the model outputs. This approach, inspiredby permutation importance for tabular data, masks each word in the systemprompt and evaluates its effect on the outputs based on the available textscores aggregated over multiple user inputs. Unlike classical attention, wordimportance measures the impact of prompt words on arbitrarily-defined textscores, which enables decomposing the importance of words into the specificmeasures of interest--including bias, reading level, verbosity, etc. Thisprocedure also enables measuring impact when attention weights are notavailable. To test the fidelity of this approach, we explore the effect ofadding different suffixes to multiple different system prompts and comparingsubsequent generations with different large language models. Results show thatword importance scores are closely related to the expected suffix importancesfor multiple scoring functions.</description><author>Stefan Hackmann, Haniyeh Mahmoudian, Mark Steadman, Michael Schmidt</author><pubDate>Tue, 05 Mar 2024 15:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03028v1</guid></item><item><title>Forecasting Tropical Cyclones with Cascaded Diffusion Models</title><link>http://arxiv.org/abs/2310.01690v5</link><description>As tropical cyclones become more intense due to climate change, the rise ofAl-based modelling provides a more affordable and accessible approach comparedto traditional methods based on mathematical models. This work leveragesgenerative diffusion models to forecast cyclone trajectories and precipitationpatterns by integrating satellite imaging, remote sensing, and atmosphericdata. It employs a cascaded approach that incorporates three main tasks:forecasting, super-resolution, and precipitation modelling. The trainingdataset includes 51 cyclones from six major tropical cyclone basins fromJanuary 2019 - March 2023. Experiments demonstrate that the final forecastsfrom the cascaded models show accurate predictions up to a 36-hour rollout,with excellent Structural Similarity (SSIM) and Peak-To-Noise Ratio (PSNR)values exceeding 0.5 and 20 dB, respectively, for all three tasks. The 36-hourforecasts can be produced in as little as 30 mins on a single Nvidia A30/RTX2080 Ti. This work also highlights the promising efficiency of Al methods suchas diffusion models for high-performance needs in weather forecasting, such astropical cyclone forecasting, while remaining computationally affordable,making them ideal for highly vulnerable regions with critical forecasting needsand financial limitations. Code accessible at\url{https://github.com/nathzi1505/forecast-diffmodels}.</description><author>Pritthijit Nath, Pancham Shukla, Shuai Wang, C√©sar Quilodr√°n-Casas</author><pubDate>Tue, 05 Mar 2024 15:03:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01690v5</guid></item><item><title>Attention Guidance Mechanism for Handwritten Mathematical Expression Recognition</title><link>http://arxiv.org/abs/2403.01756v2</link><description>Handwritten mathematical expression recognition (HMER) is challenging inimage-to-text tasks due to the complex layouts of mathematical expressions andsuffers from problems including over-parsing and under-parsing. To solve these,previous HMER methods improve the attention mechanism by utilizing historicalalignment information. However, this approach has limitations in addressingunder-parsing since it cannot correct the erroneous attention on image areasthat should be parsed at subsequent decoding steps. This faulty attentioncauses the attention module to incorporate future context into the currentdecoding step, thereby confusing the alignment process. To address this issue,we propose an attention guidance mechanism to explicitly suppress attentionweights in irrelevant areas and enhance the appropriate ones, therebyinhibiting access to information outside the intended context. Depending on thetype of attention guidance, we devise two complementary approaches to refineattention weights: self-guidance that coordinates attention of multiple headsand neighbor-guidance that integrates attention from adjacent time steps.Experiments show that our method outperforms existing state-of-the-art methods,achieving expression recognition rates of 60.75% / 61.81% / 63.30% on theCROHME 2014/ 2016/ 2019 datasets.</description><author>Yutian Liu, Wenjun Ke, Jianguo Wei</author><pubDate>Tue, 05 Mar 2024 15:02:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01756v2</guid></item><item><title>SE(3) Equivariant Augmented Coupling Flows</title><link>http://arxiv.org/abs/2308.10364v6</link><description>Coupling normalizing flows allow for fast sampling and density evaluation,making them the tool of choice for probabilistic modeling of physical systems.However, the standard coupling architecture precludes endowing flows thatoperate on the Cartesian coordinates of atoms with the SE(3) and permutationinvariances of physical systems. This work proposes a coupling flow thatpreserves SE(3) and permutation equivariance by performing coordinate splitsalong additional augmented dimensions. At each layer, the flow maps atoms'positions into learned SE(3) invariant bases, where we apply standard flowtransformations, such as monotonic rational-quadratic splines, before returningto the original basis. Crucially, our flow preserves fast sampling and densityevaluation, and may be used to produce unbiased estimates of expectations withrespect to the target distribution via importance sampling. When trained on theDW4, LJ13, and QM9-positional datasets, our flow is competitive withequivariant continuous normalizing flows and diffusion models, while allowingsampling more than an order of magnitude faster. Moreover, to the best of ourknowledge, we are the first to learn the full Boltzmann distribution of alaninedipeptide by only modeling the Cartesian positions of its atoms. Lastly, wedemonstrate that our flow can be trained to approximately sample from theBoltzmann distribution of the DW4 and LJ13 particle systems using only theirenergy functions.</description><author>Laurence I. Midgley, Vincent Stimper, Javier Antor√°n, Emile Mathieu, Bernhard Sch√∂lkopf, Jos√© Miguel Hern√°ndez-Lobato</author><pubDate>Tue, 05 Mar 2024 14:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10364v6</guid></item><item><title>Directed Acyclic Graph Structure Learning from Dynamic Graphs</title><link>http://arxiv.org/abs/2211.17029v2</link><description>Estimating the structure of directed acyclic graphs (DAGs) of features(variables) plays a vital role in revealing the latent data generation processand providing causal insights in various applications. Although there have beenmany studies on structure learning with various types of data, the structurelearning on the dynamic graph has not been explored yet, and thus we study thelearning problem of node feature generation mechanism on such ubiquitousdynamic graph data. In a dynamic graph, we propose to simultaneously estimatecontemporaneous relationships and time-lagged interaction relationships betweenthe node features. These two kinds of relationships form a DAG, which couldeffectively characterize the feature generation process in a concise way. Tolearn such a DAG, we cast the learning problem as a continuous score-basedoptimization problem, which consists of a differentiable score function tomeasure the validity of the learned DAGs and a smooth acyclicity constraint toensure the acyclicity of the learned DAGs. These two components are translatedinto an unconstraint augmented Lagrangian objective which could be minimized bymature continuous optimization techniques. The resulting algorithm, namedGraphNOTEARS, outperforms baselines on simulated data across a wide range ofsettings that may encounter in real-world applications. We also apply theproposed approach on two dynamic graphs constructed from the real-world Yelpdataset, demonstrating our method could learn the connections between nodefeatures, which conforms with the domain knowledge.</description><author>Shaohua Fan, Shuyang Zhang, Xiao Wang, Chuan Shi</author><pubDate>Tue, 05 Mar 2024 14:57:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.17029v2</guid></item><item><title>SplAgger: Split Aggregation for Meta-Reinforcement Learning</title><link>http://arxiv.org/abs/2403.03020v1</link><description>A core ambition of reinforcement learning (RL) is the creation of agentscapable of rapid learning in novel tasks. Meta-RL aims to achieve this bydirectly learning such agents. One category of meta-RL methods, called blackbox methods, does so by training off-the-shelf sequence models end-to-end. Incontrast, another category of methods have been developed that explicitly infera posterior distribution over the unknown task. These methods generally havedistinct objectives and sequence models designed to enable task inference, andso are known as task inference methods. However, recent evidence suggests thattask inference objectives are unnecessary in practice. Nonetheless, it remainsunclear whether task inference sequence models are beneficial even when taskinference objectives are not. In this paper, we present strong evidence thattask inference sequence models are still beneficial. In particular, weinvestigate sequence models with permutation invariant aggregation, whichexploit the fact that, due to the Markov property, the task posterior does notdepend on the order of data. We empirically confirm the advantage ofpermutation invariant sequence models without the use of task inferenceobjectives. However, we also find, surprisingly, that there are multipleconditions under which permutation variance remains useful. Therefore, wepropose SplAgger, which uses both permutation variant and invariant componentsto achieve the best of both worlds, outperforming all baselines on continuouscontrol and memory environments.</description><author>Jacob Beck, Matthew Jackson, Risto Vuorio, Zheng Xiong, Shimon Whiteson</author><pubDate>Tue, 05 Mar 2024 14:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03020v1</guid></item><item><title>Identification for Tree-shaped Structural Causal Models in Polynomial Time</title><link>http://arxiv.org/abs/2311.14058v2</link><description>Linear structural causal models (SCMs) are used to express and analyse therelationships between random variables. Direct causal effects are representedas directed edges and confounding factors as bidirected edges. Identifying thecausal parameters from correlations between the nodes is an open problem inartificial intelligence. In this paper, we study SCMs whose directed componentforms a tree. Van der Zander et al. (AISTATS'22, PLMR 151, pp. 6770--6792,2022) give a PSPACE-algorithm for the identification problem in this case,which is a significant improvement over the general Gr\"obner basis approach,which has doubly-exponential time complexity in the number of structuralparameters. In this work, we present a randomized polynomial-time algorithm,which solves the identification problem for tree-shaped SCMs. For everystructural parameter, our algorithms decides whether it is genericallyidentifiable, generically 2-identifiable, or generically unidentifiable. (Noother cases can occur.) In the first two cases, it provides one or twofractional affine square root terms of polynomials (FASTPs) for thecorresponding parameter, respectively.</description><author>Aaryan Gupta, Markus Bl√§ser</author><pubDate>Tue, 05 Mar 2024 14:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14058v2</guid></item><item><title>CRISPR: Ensemble Model</title><link>http://arxiv.org/abs/2403.03018v1</link><description>Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) is a geneediting technology that has revolutionized the fields of biology and medicine.However, one of the challenges of using CRISPR is predicting the on-targetefficacy and off-target sensitivity of single-guide RNAs (sgRNAs). This isbecause most existing methods are trained on separate datasets with differentgenes and cells, which limits their generalizability. In this paper, we proposea novel ensemble learning method for sgRNA design that is accurate andgeneralizable. Our method combines the predictions of multiple machine learningmodels to produce a single, more robust prediction. This approach allows us tolearn from a wider range of data, which improves the generalizability of ourmodel. We evaluated our method on a benchmark dataset of sgRNA designs andfound that it outperformed existing methods in terms of both accuracy andgeneralizability. Our results suggest that our method can be used to designsgRNAs with high sensitivity and specificity, even for new genes or cells. Thiscould have important implications for the clinical use of CRISPR, as it wouldallow researchers to design more effective and safer treatments for a varietyof diseases.</description><author>Mohammad Rostami, Amin Ghariyazi, Hamed Dashti, Mohammad Hossein Rohban, Hamid R. Rabiee</author><pubDate>Tue, 05 Mar 2024 14:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03018v1</guid></item><item><title>OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following</title><link>http://arxiv.org/abs/2403.03017v1</link><description>Embodied Instruction Following (EIF) is a crucial task in embodied learning,requiring agents to interact with their environment through egocentricobservations to fulfill natural language instructions. Recent advancements haveseen a surge in employing large language models (LLMs) within aframework-centric approach to enhance performance in embodied learning tasks,including EIF. Despite these efforts, there exists a lack of a unifiedunderstanding regarding the impact of various components-ranging from visualperception to action execution-on task performance. To address this gap, weintroduce OPEx, a comprehensive framework that delineates the core componentsessential for solving embodied learning tasks: Observer, Planner, and Executor.Through extensive evaluations, we provide a deep analysis of how each componentinfluences EIF task performance. Furthermore, we innovate within this space bydeploying a multi-agent dialogue strategy on a TextWorld counterpart, furtherenhancing task performance. Our findings reveal that LLM-centric designmarkedly improves EIF outcomes, identify visual perception and low-level actionexecution as critical bottlenecks, and demonstrate that augmenting LLMs with amulti-agent framework further elevates performance.</description><author>Haochen Shi, Zhiyuan Sun, Xingdi Yuan, Marc-Alexandre C√¥t√©, Bang Liu</author><pubDate>Tue, 05 Mar 2024 14:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03017v1</guid></item><item><title>A Variational Approach for Joint Image Recovery and Feature Extraction Based on Spatially-Varying Generalised Gaussian Models</title><link>http://arxiv.org/abs/2209.01375v3</link><description>The joint problem of reconstruction / feature extraction is a challengingtask in image processing. It consists in performing, in a joint manner, therestoration of an image and the extraction of its features. In this work, wefirstly propose a novel nonsmooth and non-convex variational formulation of theproblem. For this purpose, we introduce a versatile generalised Gaussian priorwhose parameters, including its exponent, are space-variant. Secondly, wedesign an alternating proximal-based optimisation algorithm that efficientlyexploits the structure of the proposed non-convex objective function. We alsoanalyse the convergence of this algorithm. As shown in numerical experimentsconducted on joint deblurring/segmentation tasks, the proposed method provideshigh-quality results.</description><author>Emilie Chouzenoux, Marie-Caroline Corbineau, Jean-Christophe Pesquet, Gabriele Scrivanti</author><pubDate>Tue, 05 Mar 2024 14:50:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.01375v3</guid></item><item><title>The Case for Evaluating Multimodal Translation Models on Text Datasets</title><link>http://arxiv.org/abs/2403.03014v1</link><description>A good evaluation framework should evaluate multimodal machine translation(MMT) models by measuring 1) their use of visual information to aid in thetranslation task and 2) their ability to translate complex sentences such asdone for text-only machine translation. However, most current work in MMT isevaluated against the Multi30k testing sets, which do not measure theseproperties. Namely, the use of visual information by the MMT model cannot beshown directly from the Multi30k test set results and the sentences in Multi30kare are image captions, i.e., short, descriptive sentences, as opposed tocomplex sentences that typical text-only machine translation models areevaluated against. Therefore, we propose that MMT models be evaluated using 1) the CoMMuTEevaluation framework, which measures the use of visual information by MMTmodels, 2) the text-only WMT news translation task test sets, which evaluatestranslation performance against complex sentences, and 3) the Multi30k testsets, for measuring MMT model performance against a real MMT dataset. Finally,we evaluate recent MMT models trained solely against the Multi30k datasetagainst our proposed evaluation framework and demonstrate the dramatic dropperformance against text-only testing sets compared to recent text-only MTmodels.</description><author>Vipin Vijayan, Braeden Bowen, Scott Grigsby, Timothy Anderson, Jeremy Gwinnup</author><pubDate>Tue, 05 Mar 2024 14:49:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03014v1</guid></item><item><title>Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations</title><link>http://arxiv.org/abs/2403.03008v1</link><description>In the era of personalized education, the provision of comprehensibleexplanations for learning recommendations is of a great value to enhance thelearner's understanding and engagement with the recommended learning content.Large language models (LLMs) and generative AI in general have recently openednew doors for generating human-like explanations, for and along learningrecommendations. However, their precision is still far away from acceptable ina sensitive field like education. To harness the abilities of LLMs, while stillensuring a high level of precision towards the intent of the learners, thispaper proposes an approach to utilize knowledge graphs (KG) as a source offactual context, for LLM prompts, reducing the risk of model hallucinations,and safeguarding against wrong or imprecise information, while maintaining anapplication-intended learning context. We utilize the semantic relations in theknowledge graph to offer curated knowledge about learning recommendations. Withdomain-experts in the loop, we design the explanation as a textual template,which is filled and completed by the LLM. Domain experts were integrated in theprompt engineering phase as part of a study, to ensure that explanationsinclude information that is relevant to the learner. We evaluate our approachquantitatively using Rouge-N and Rouge-L measures, as well as qualitativelywith experts and learners. Our results show an enhanced recall and precision ofthe generated explanations compared to those generated solely by the GPT model,with a greatly reduced risk of generating imprecise information in the finallearning explanation.</description><author>Hasan Abu-Rasheed, Christian Weber, Madjid Fathi</author><pubDate>Tue, 05 Mar 2024 14:41:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03008v1</guid></item><item><title>Sensitivity Analysis On Loss Landscape</title><link>http://arxiv.org/abs/2403.01128v2</link><description>Gradients can be employed for sensitivity analysis. Here, we leverage theadvantages of the Loss Landscape to comprehend which independent variablesimpact the dependent variable. We seek to grasp the loss landscape by utilizingfirst, second, and third derivatives through automatic differentiation. we knowthat Spearman's rank correlation coefficient can detect the monotonicrelationship between two variables. However, I have found that second-ordergradients, with certain configurations and parameters, provide information thatcan be visualized similarly to Spearman results, In this approach, weincorporate a loss function with an activation function, resulting in anon-linear pattern. Each exploration of the loss landscape through retrainingyields new valuable information. Furthermore, the first and third derivativesare also beneficial, as they indicate the extent to which independent variablesinfluence the dependent variable.</description><author>Salman Faroz</author><pubDate>Tue, 05 Mar 2024 14:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01128v2</guid></item></channel></rss>