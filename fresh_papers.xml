<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 08 Aug 2024 13:00:08 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>How Well Can Vision Language Models See Image Details?</title><link>http://arxiv.org/abs/2408.03940v1</link><description>Large Language Model-based Vision-Language Models (LLM-based VLMs) havedemonstrated impressive results in various vision-language understanding tasks.However, how well these VLMs can see image detail beyond the semantic levelremains unclear. In our study, we introduce a pixel value prediction task (PVP)to explore "How Well Can Vision Language Models See Image Details?" and toassist VLMs in perceiving more details. Typically, these models comprise afrozen CLIP visual encoder, a large language model, and a connecting module.After fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle topredict precise pixel values by only fine-tuning the connection module and LLM;and 2) prediction precision is significantly improved when the vision encoderis also adapted. Additionally, our research reveals that incorporating pixelvalue prediction as one of the VLM pre-training tasks and vision encoderadaptation markedly boosts VLM performance on downstream image-languageunderstanding tasks requiring detailed image perception, such as referringimage segmentation (with an average +10.19 cIoU improvement) and video gamedecision making (with average score improvements of +80.34 and +70.54 on twogames, respectively).</description><author>Chenhui Gou, Abdulwahab Felemban, Faizan Farooq Khan, Deyao Zhu, Jianfei Cai, Hamid Rezatofighi, Mohamed Elhoseiny</author><pubDate>Wed, 07 Aug 2024 17:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03940v1</guid></item><item><title>SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance for Mercosur Common Nomenclature</title><link>http://arxiv.org/abs/2408.03936v1</link><description>Natural language processing (NLP) has seen significant advancements with theadvent of large language models (LLMs). However, substantial improvements arestill needed for languages other than English, especially for specific domainslike the applications of Mercosur Common Nomenclature (NCM), a BrazilianHarmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, afoundational Portuguese LLM, as an LLM source to implement the NCM applicationprocessing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)technique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.This approach retains the chain-of-thought (CoT) methodology for promptdevelopment in a more concise and streamlined manner, utilizing brief andfocused documents for training. The proposed model demonstrates an efficientand cost-effective alternative for fine-tuning smaller LLMs, significantlyoutperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although theresearch focuses on NCM applications, the methodology can be easily adapted forHS applications worldwide.</description><author>Vinícius Di Oliveira, Yuri Façanha Bezerra, Li Weigang, Pedro Carvalho Brom, Victor Rafael R. Celestino</author><pubDate>Wed, 07 Aug 2024 17:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03936v1</guid></item><item><title>From Words to Worth: Newborn Article Impact Prediction with LLM</title><link>http://arxiv.org/abs/2408.03934v1</link><description>As the academic landscape expands, the challenge of efficiently identifyingpotentially high-impact articles among the vast number of newly published worksbecomes critical. This paper introduces a promising approach, leveraging thecapabilities of fine-tuned LLMs to predict the future impact of newbornarticles solely based on titles and abstracts. Moving beyond traditionalmethods heavily reliant on external information, the proposed method discernsthe shared semantic features of highly impactful papers from a large collectionof title-abstract and potential impact pairs. These semantic features arefurther utilized to regress an improved metric, TNCSI_SP, which has beenendowed with value, field, and time normalization properties. Additionally, acomprehensive dataset has been constructed and released for fine-tuning theLLM, containing over 12,000 entries with corresponding titles, abstracts, andTNCSI_SP. The quantitative results, with an NDCG@20 of 0.901, demonstrate thatthe proposed approach achieves state-of-the-art performance in predicting theimpact of newborn articles when compared to competitive counterparts. Finally,we demonstrate a real-world application for predicting the impact of newbornjournal articles to demonstrate its noteworthy practical value. Overall, ourfindings challenge existing paradigms and propose a shift towards a morecontent-focused prediction of academic impact, offering new insights forassessing newborn article impact.</description><author>Penghai Zhao, Qinghua Xing, Kairan Dou, Jinyu Tian, Ying Tai, Jian Yang, Ming-Ming Cheng, Xiang Li</author><pubDate>Wed, 07 Aug 2024 17:52:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03934v1</guid></item><item><title>Open-Set Multivariate Time-Series Anomaly Detection</title><link>http://arxiv.org/abs/2310.12294v3</link><description>Numerous methods for time-series anomaly detection (TSAD) have emerged inrecent years, most of which are unsupervised and assume that only normalsamples are available during the training phase, due to the challenge ofobtaining abnormal data in real-world scenarios. Still, limited samples ofabnormal data are often available, albeit they are far from representative ofall possible anomalies. Supervised methods can be utilized to classify normaland seen anomalies, but they tend to overfit to the seen anomalies presentduring training, hence, they fail to generalize to unseen anomalies. We proposethe first algorithm to address the open-set TSAD problem, called MultivariateOpen-Set Time-Series Anomaly Detector (MOSAD), that leverages only a few shotsof labeled anomalies during the training phase in order to achieve superioranomaly detection performance compared to both supervised and unsupervised TSADalgorithms. MOSAD is a novel multi-head TSAD framework with a sharedrepresentation space and specialized heads, including the Generative head, theDiscriminative head, and the Anomaly-Aware Contrastive head. The latterproduces a superior representation space for anomaly detection compared toconventional supervised contrastive learning. Extensive experiments on threereal-world datasets establish MOSAD as a new state-of-the-art in the TSADfield.</description><author>Thomas Lai, Thi Kieu Khanh Ho, Narges Armanfard</author><pubDate>Wed, 07 Aug 2024 17:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12294v3</guid></item><item><title>Advancing Prompt Learning through an External Layer</title><link>http://arxiv.org/abs/2407.19674v3</link><description>Prompt learning represents a promising method for adapting pre-trainedvision-language models (VLMs) to various downstream tasks by learning a set oftext embeddings. One challenge inherent to these methods is the poorgeneralization performance due to the invalidity of the learned text embeddingsfor unseen tasks. A straightforward approach to bridge this gap is to freezethe text embeddings in prompts, which results in a lack of capacity to adaptVLMs for downstream tasks. To address this dilemma, we propose a paradigmcalled EnPrompt with a novel External Layer (EnLa). Specifically, we propose atextual external layer and learnable visual embeddings for adapting VLMs todownstream tasks. The learnable external layer is built upon valid embeddingsof pre-trained CLIP. This design considers the balance of learning capabilitiesbetween the two branches. To align the textual and visual features, we proposea novel two-pronged approach: i) we introduce the optimal transport as thediscrepancy metric to align the vision and text modalities, and ii) weintroduce a novel strengthening feature to enhance the interaction betweenthese two modalities. Four representative experiments (i.e., base-to-novelgeneralization, few-shot learning, cross-dataset generalization, domain shiftsgeneralization) across 15 datasets demonstrate that our method outperforms theexisting prompt learning method.</description><author>Fangming Cui, Xun Yang, Chao Wu, Liang Xiao, Xinmei Tian</author><pubDate>Wed, 07 Aug 2024 17:45:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19674v3</guid></item><item><title>Fast Sprite Decomposition from Animated Graphics</title><link>http://arxiv.org/abs/2408.03923v1</link><description>This paper presents an approach to decomposing animated graphics intosprites, a set of basic elements or layers. Our approach builds on theoptimization of sprite parameters to fit the raster video. For efficiency, weassume static textures for sprites to reduce the search space while preventingartifacts using a texture prior model. To further speed up the optimization, weintroduce the initialization of the sprite parameters utilizing a pre-trainedvideo object segmentation model and user input of single frame annotations. Forour study, we construct the Crello Animation dataset from an online designservice and define quantitative metrics to measure the quality of the extractedsprites. Experiments show that our method significantly outperforms baselinesfor similar decomposition tasks in terms of the quality/efficiency tradeoff.</description><author>Tomoyuki Suzuki, Kotaro Kikuchi, Kota Yamaguchi</author><pubDate>Wed, 07 Aug 2024 17:30:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03923v1</guid></item><item><title>FourierMamba: Fourier Learning Integration with State Space Models for Image Deraining</title><link>http://arxiv.org/abs/2405.19450v2</link><description>Image deraining aims to remove rain streaks from rainy images and restoreclear backgrounds. Currently, some research that employs the Fourier transformhas proved to be effective for image deraining, due to it acting as aneffective frequency prior for capturing rain streaks. However, despite thereexists dependency of low frequency and high frequency in images, theseFourier-based methods rarely exploit the correlation of different frequenciesfor conjuncting their learning procedures, limiting the full utilization offrequency information for image deraining. Alternatively, the recently emergedMamba technique depicts its effectiveness and efficiency for modelingcorrelation in various domains (e.g., spatial, temporal), and we argue thatintroducing Mamba into its unexplored Fourier spaces to correlate differentfrequencies would help improve image deraining. This motivates us to propose anew framework termed FourierMamba, which performs image deraining with Mamba inthe Fourier space. Owning to the unique arrangement of frequency orders inFourier space, the core of FourierMamba lies in the scanning encoding ofdifferent frequencies, where the low-high frequency order formats exhibitdifferently in the spatial dimension (unarranged in axis) and channel dimension(arranged in axis). Therefore, we design FourierMamba that correlates Fourierspace information in the spatial and channel dimensions with distinct designs.Specifically, in the spatial dimension Fourier space, we introduce the zigzagcoding to scan the frequencies to rearrange the orders from low to highfrequencies, thereby orderly correlating the connections between frequencies;in the channel dimension Fourier space with arranged orders of frequencies inaxis, we can directly use Mamba to perform frequency correlation and improvethe channel information representation.</description><author>Dong Li, Yidi Liu, Xueyang Fu, Senyan Xu, Zheng-Jun Zha</author><pubDate>Wed, 07 Aug 2024 17:30:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19450v2</guid></item><item><title>FMiFood: Multi-modal Contrastive Learning for Food Image Classification</title><link>http://arxiv.org/abs/2408.03922v1</link><description>Food image classification is the fundamental step in image-based dietaryassessment, which aims to estimate participants' nutrient intake from eatingoccasion images. A common challenge of food images is the intra-class diversityand inter-class similarity, which can significantly hinder classificationperformance. To address this issue, we introduce a novel multi-modalcontrastive learning framework called FMiFood, which learns more discriminativefeatures by integrating additional contextual information, such as foodcategory text descriptions, to enhance classification accuracy. Specifically,we propose a flexible matching technique that improves the similarity matchingbetween text and image embeddings to focus on multiple key information.Furthermore, we incorporate the classification objectives into the frameworkand explore the use of GPT-4 to enrich the text descriptions and provide moredetailed context. Our method demonstrates improved performance on both theUPMC-101 and VFN datasets compared to existing methods.</description><author>Xinyue Pan, Jiangpeng He, Fengqing Zhu</author><pubDate>Wed, 07 Aug 2024 17:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03922v1</guid></item><item><title>Hard to Explain: On the Computational Hardness of In-Distribution Model Interpretation</title><link>http://arxiv.org/abs/2408.03915v1</link><description>The ability to interpret Machine Learning (ML) models is becomingincreasingly essential. However, despite significant progress in the field,there remains a lack of rigorous characterization regarding the innateinterpretability of different models. In an attempt to bridge this gap, recentwork has demonstrated that it is possible to formally assess interpretabilityby studying the computational complexity of explaining the decisions of variousmodels. In this setting, if explanations for a particular model can be obtainedefficiently, the model is considered interpretable (since it can be explained``easily''). However, if generating explanations over an ML model iscomputationally intractable, it is considered uninterpretable. Prior researchidentified two key factors that influence the complexity of interpreting an MLmodel: (i) the type of the model (e.g., neural networks, decision trees, etc.);and (ii) the form of explanation (e.g., contrastive explanations, Shapleyvalues, etc.). In this work, we claim that a third, important factor must alsobe considered for this analysis -- the underlying distribution over which theexplanation is obtained. Considering the underlying distribution is key inavoiding explanations that are socially misaligned, i.e., convey informationthat is biased and unhelpful to users. We demonstrate the significant influenceof the underlying distribution on the resulting overall interpretationcomplexity, in two settings: (i) prediction models paired with an externalout-of-distribution (OOD) detector; and (ii) prediction models designed toinherently generate socially aligned explanations. Our findings prove that theexpressiveness of the distribution can significantly influence the overallcomplexity of interpretation, and identify essential prerequisites that a modelmust possess to generate socially aligned explanations.</description><author>Guy Amir, Shahaf Bassan, Guy Katz</author><pubDate>Wed, 07 Aug 2024 17:20:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03915v1</guid></item><item><title>AdapMTL: Adaptive Pruning Framework for Multitask Learning Model</title><link>http://arxiv.org/abs/2408.03913v1</link><description>In the domain of multimedia and multimodal processing, the efficient handlingof diverse data streams such as images, video, and sensor data is paramount.Model compression and multitask learning (MTL) are crucial in this field,offering the potential to address the resource-intensive demands of processingand interpreting multiple forms of media simultaneously. However, effectivelycompressing a multitask model presents significant challenges due to thecomplexities of balancing sparsity allocation and accuracy performance acrossmultiple tasks. To tackle these challenges, we propose AdapMTL, an adaptivepruning framework for MTL models. AdapMTL leverages multiple learnable softthresholds independently assigned to the shared backbone and the task-specificheads to capture the nuances in different components' sensitivity to pruning.During training, it co-optimizes the soft thresholds and MTL model weights toautomatically determine the suitable sparsity level at each component toachieve both high task accuracy and high overall sparsity. It furtherincorporates an adaptive weighting mechanism that dynamically adjusts theimportance of task-specific losses based on each task's robustness to pruning.We demonstrate the effectiveness of AdapMTL through comprehensive experimentson popular multitask datasets, namely NYU-v2 and Tiny-Taskonomy, with differentarchitectures, showcasing superior performance compared to state-of-the-artpruning methods.</description><author>Mingcan Xiang, Steven Jiaxun Tang, Qizheng Yang, Hui Guan, Tongping Liu</author><pubDate>Wed, 07 Aug 2024 17:19:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03913v1</guid></item><item><title>CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases</title><link>http://arxiv.org/abs/2408.03910v1</link><description>Large Language Models (LLMs) excel in stand-alone code tasks like HumanEvaland MBPP, but struggle with handling entire code repositories. This challengehas prompted research on enhancing LLM-codebase interaction at a repositoryscale. Current solutions rely on similarity-based retrieval or manual tools andAPIs, each with notable drawbacks. Similarity-based retrieval often has lowrecall in complex tasks, while manual tools and APIs are typicallytask-specific and require expert knowledge, reducing their generalizabilityacross diverse code tasks and real-world applications. To mitigate theselimitations, we introduce \framework, a system that integrates LLM agents withgraph database interfaces extracted from code repositories. By leveraging thestructural properties of graph databases and the flexibility of the graph querylanguage, \framework enables the LLM agent to construct and execute queries,allowing for precise, code structure-aware context retrieval and codenavigation. We assess \framework using three benchmarks: CrossCodeEval,SWE-bench, and EvoCodeBench. Additionally, we develop five real-world codingapplications. With a unified graph database schema, \framework demonstratescompetitive performance and potential in both academic and real-worldenvironments, showcasing its versatility and efficacy in software engineering.Our application demo:https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.</description><author>Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Wenmeng Zhou, Fei Wang, Michael Shieh</author><pubDate>Wed, 07 Aug 2024 17:13:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03910v1</guid></item><item><title>LaFA: Latent Feature Attacks on Non-negative Matrix Factorization</title><link>http://arxiv.org/abs/2408.03909v1</link><description>As Machine Learning (ML) applications rapidly grow, concerns aboutadversarial attacks compromising their reliability have gained significantattention. One unsupervised ML method known for its resilience to such attacksis Non-negative Matrix Factorization (NMF), an algorithm that decomposes inputdata into lower-dimensional latent features. However, the introduction ofpowerful computational tools such as Pytorch enables the computation ofgradients of the latent features with respect to the original data, raisingconcerns about NMF's reliability. Interestingly, naively deriving theadversarial loss for NMF as in the case of ML would result in thereconstruction loss, which can be shown theoretically to be an ineffectiveattacking objective. In this work, we introduce a novel class of attacks in NMFtermed Latent Feature Attacks (LaFA), which aim to manipulate the latentfeatures produced by the NMF process. Our method utilizes the Feature Error(FE) loss directly on the latent features. By employing FE loss, we generateperturbations in the original data that significantly affect the extractedlatent features, revealing vulnerabilities akin to those found in other MLtechniques. To handle large peak-memory overhead from gradient back-propagationin FE attacks, we develop a method based on implicit differentiation whichenables their scaling to larger datasets. We validate NMF vulnerabilities andFE attacks effectiveness through extensive experiments on synthetic andreal-world data.</description><author>Minh Vu, Ben Nebgen, Erik Skau, Geigh Zollicoffer, Juan Castorena, Kim Rasmussen, Boian Alexandrov, Manish Bhattarai</author><pubDate>Wed, 07 Aug 2024 17:13:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03909v1</guid></item><item><title>Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models</title><link>http://arxiv.org/abs/2408.03907v1</link><description>Large Language Models (LLMs) have excelled at language understanding andgenerating human-level text. However, even with supervised training and humanalignment, these LLMs are susceptible to adversarial attacks where malicioususers can prompt the model to generate undesirable text. LLMs also inherentlyencode potential biases that can cause various harmful effects duringinteractions. Bias evaluation metrics lack standards as well as consensus andexisting methods often rely on human-generated templates and annotations whichare expensive and labor intensive. In this work, we train models toautomatically create adversarial prompts to elicit biased responses from targetLLMs. We present LLM- based bias evaluation metrics and also analyze severalexisting automatic evaluation methods and metrics. We analyze the variousnuances of model responses, identify the strengths and weaknesses of modelfamilies, and assess where evaluation methods fall short. We compare thesemetrics to human evaluation and validate that the LLM-as-a-Judge metric alignswith human judgement on bias in response generation.</description><author>Shachi H Kumar, Saurav Sahay, Sahisnu Mazumder, Eda Okur, Ramesh Manuvinakurike, Nicole Beckage, Hsuan Su, Hung-yi Lee, Lama Nachman</author><pubDate>Wed, 07 Aug 2024 17:11:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03907v1</guid></item><item><title>Generative Adversarial Models for Extreme Geospatial Downscaling</title><link>http://arxiv.org/abs/2402.14049v2</link><description>Addressing the challenges of climate change requires accurate andhigh-resolution mapping of geospatial data, especially climate and weathervariables. However, many existing geospatial datasets, such as the griddedoutputs of the state-of-the-art numerical climate models (e.g., generalcirculation models), are only available at very coarse spatial resolutions dueto the model complexity and extremely high computational demand.Deep-learning-based methods, particularly generative adversarial networks(GANs) and their variants, have proved effective for refining natural imagesand have shown great promise in improving geospatial datasets. This paperdescribes a conditional GAN-based stochastic geospatial downscaling method thatcan accommodates very high scaling factors. Compared to most existing methods,the method can generate high-resolution accurate climate datasets from verylow-resolution inputs. More importantly, the method explicitly considers theuncertainty inherent to the downscaling process that tends to be ignored inexisting methods. Given an input, the method can produce a multitude ofplausible high-resolution samples instead of one single deterministic result.These samples allow for an empirical exploration and inferences of modeluncertainty and robustness. With a case study of gridded climate datasets (windvelocity and solar irradiance), we demonstrate the performances of theframework in downscaling tasks with large scaling factors (up to $64\times$)and highlight the advantages of the framework with a comprehensive comparisonwith commonly used and most recent downscaling methods, including area-to-point(ATP) kriging, deep image prior (DIP), enhanced super-resolution generativeadversarial networks (ESRGAN), physics-informed resolution-enhancing GAN (PhIREGAN), and an efficient diffusion model for remote sensing imagesuper-resolution (EDiffSR).</description><author>Guiye Li, Guofeng Cao</author><pubDate>Wed, 07 Aug 2024 17:09:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14049v2</guid></item><item><title>Lightweight Video Denoising Using a Classic Bayesian Backbone</title><link>http://arxiv.org/abs/2408.03904v1</link><description>In recent years, state-of-the-art image and video denoising networks havebecome increasingly large, requiring millions of trainable parameters toachieve best-in-class performance. Improved denoising quality has come at thecost of denoising speed, where modern transformer networks are far slower torun than smaller denoising networks such as FastDVDnet and classic Bayesiandenoisers such as the Wiener filter. In this paper, we implement a hybrid Wiener filter which leverages smallancillary networks to increase the original denoiser performance, whileretaining fast denoising speeds. These networks are used to refine the Wienercoring estimate, optimise windowing functions and estimate the unknown noiseprofile. Using these methods, we outperform several popular denoisers andremain within 0.2 dB, on average, of the popular VRT transformer. Our methodwas found to be over x10 faster than the transformer method, with a far lowerparameter cost.</description><author>Clément Bled, François Pitié</author><pubDate>Wed, 07 Aug 2024 17:08:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03904v1</guid></item><item><title>ESP-MedSAM: Efficient Self-Prompting SAM for Universal Image Segmentation</title><link>http://arxiv.org/abs/2407.14153v2</link><description>The Segment Anything Model (SAM) has demonstrated outstanding adaptation tomedical image segmentation but still faces three major challenges. Firstly, thehuge computational costs of SAM limit its real-world applicability. Secondly,SAM depends on manual annotations (e.g., points, boxes) as prompts, which arelaborious and impractical in clinical scenarios. Thirdly, SAM handles allsegmentation targets equally, which is suboptimal for diverse medicalmodalities with inherent heterogeneity. To address these issues, we propose anEfficient Self-Prompting SAM for universal medical image segmentation, namedESP-MedSAM. We devise a Multi-Modal Decoupled Knowledge Distillation (MMDKD)strategy to distil common image knowledge and domain-specific medical knowledgefrom the foundation model to train a lightweight image encoder and a modalitycontroller. Further, they combine with the additionally introduced Self-PatchPrompt Generator (SPPG) and Query-Decoupled Modality Decoder (QDMD) toconstruct ESP-MedSAM. Specifically, SPPG aims to generate a set of patchprompts automatically and QDMD leverages a one-to-one strategy to provide anindependent decoding channel for every modality. Extensive experiments indicatethat ESP-MedSAM outperforms state-of-the-arts in diverse medical imagingsegmentation takes, displaying superior zero-shot learning and modalitytransfer ability. Especially, our framework uses only 31.4% parameters comparedto SAM-Base.</description><author>Qing Xu, Jiaxuan Li, Xiangjian He, Ziyu Liu, Zhen Chen, Wenting Duan, Chenxin Li, Maggie M. He, Fiseha B. Tesema, Wooi P. Cheah, Yi Wang, Rong Qu, Jonathan M. Garibaldi</author><pubDate>Wed, 07 Aug 2024 17:04:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14153v2</guid></item><item><title>UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction</title><link>http://arxiv.org/abs/2403.15098v3</link><description>Vehicle trajectory prediction has increasingly relied on data-drivensolutions, but their ability to scale to different data domains and the impactof larger dataset sizes on their generalization remain under-explored. Whilethese questions can be studied by employing multiple datasets, it ischallenging due to several discrepancies, e.g., in data formats, mapresolution, and semantic annotation types. To address these challenges, weintroduce UniTraj, a comprehensive framework that unifies various datasets,models, and evaluation criteria, presenting new opportunities for the vehicletrajectory prediction field. In particular, using UniTraj, we conduct extensiveexperiments and find that model performance significantly drops whentransferred to other datasets. However, enlarging data size and diversity cansubstantially improve performance, leading to a new state-of-the-art result forthe nuScenes dataset. We provide insights into dataset characteristics toexplain these findings. The code can be found here:https://github.com/vita-epfl/UniTraj</description><author>Lan Feng, Mohammadhossein Bahari, Kaouther Messaoud Ben Amor, Éloi Zablocki, Matthieu Cord, Alexandre Alahi</author><pubDate>Wed, 07 Aug 2024 17:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15098v3</guid></item><item><title>Subjective-Aligned Dataset and Metric for Text-to-Video Quality Assessment</title><link>http://arxiv.org/abs/2403.11956v5</link><description>With the rapid development of generative models, ArtificialIntelligence-Generated Contents (AIGC) have exponentially increased in dailylives. Among them, Text-to-Video (T2V) generation has received widespreadattention. Though many T2V models have been released for generating highperceptual quality videos, there is still lack of a method to evaluate thequality of these videos quantitatively. To solve this issue, we establish thelargest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. Thedataset is composed of 10,000 videos generated by 9 different T2V models. Wealso conduct a subjective study to obtain each video's corresponding meanopinion score. Based on T2VQA-DB, we propose a novel transformer-based modelfor subjective-aligned Text-to-Video Quality Assessment (T2VQA). The modelextracts features from text-video alignment and video fidelity perspectives,then it leverages the ability of a large language model to give the predictionscore. Experimental results show that T2VQA outperforms existing T2V metricsand SOTA video quality assessment models. Quantitative analysis indicates thatT2VQA is capable of giving subjective-align predictions, validating itseffectiveness. The dataset and code will be released athttps://github.com/QMME/T2VQA.</description><author>Tengchuan Kou, Xiaohong Liu, Zicheng Zhang, Chunyi Li, Haoning Wu, Xiongkuo Min, Guangtao Zhai, Ning Liu</author><pubDate>Wed, 07 Aug 2024 17:02:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11956v5</guid></item><item><title>LiNR: Model Based Neural Retrieval on GPUs at LinkedIn</title><link>http://arxiv.org/abs/2407.13218v3</link><description>This paper introduces LiNR, LinkedIn's large-scale, GPU-based retrievalsystem. LiNR supports a billion-sized index on GPU models. We discuss ourexperiences and challenges in creating scalable, differentiable search indexesusing TensorFlow and PyTorch at production scale. In LiNR, both items and modelweights are integrated into the model binary. Viewing index construction as aform of model training, we describe scaling our system for large indexes,incorporating full scans and efficient filtering. A key focus is on enablingattribute-based pre-filtering for exhaustive GPU searches, addressing thecommon challenge of post-filtering in KNN searches that often reduces systemquality. We further provide multi-embedding retrieval algorithms and strategiesfor tackling cold start issues in retrieval. Our advancements in supportinglarger indexes through quantization are also discussed. We believe LiNRrepresents one of the industry's first Live-updated model-based retrievalindexes. Applied to out-of-network post recommendations on LinkedIn Feed, LiNRhas contributed to a 3% relative increase in professional daily active users.We envisage LiNR as a step towards integrating retrieval and ranking into asingle GPU model, simplifying complex infrastructures and enabling end-to-endoptimization of the entire differentiable infrastructure through gradientdescent.</description><author>Fedor Borisyuk, Qingquan Song, Mingzhou Zhou, Ganesh Parameswaran, Madhu Arun, Siva Popuri, Tugrul Bingol, Zhuotao Pei, Kuang-Hsuan Lee, Lu Zheng, Qizhan Shao, Ali Naqvi, Sen Zhou, Aman Gupta</author><pubDate>Wed, 07 Aug 2024 16:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13218v3</guid></item><item><title>Speech-MASSIVE: A Multilingual Speech Dataset for SLU and Beyond</title><link>http://arxiv.org/abs/2408.03900v1</link><description>We present Speech-MASSIVE, a multilingual Spoken Language Understanding (SLU)dataset comprising the speech counterpart for a portion of the MASSIVE textualcorpus. Speech-MASSIVE covers 12 languages from different families and inheritsfrom MASSIVE the annotations for the intent prediction and slot-filling tasks.Our extension is prompted by the scarcity of massively multilingual SLUdatasets and the growing need for versatile speech datasets to assessfoundation models (LLMs, speech encoders) across languages and tasks. Weprovide a multimodal, multitask, multilingual dataset and report SLU baselinesusing both cascaded and end-to-end architectures in various training scenarios(zero-shot, few-shot, and full fine-tune). Furthermore, we demonstrate thesuitability of Speech-MASSIVE for benchmarking other tasks such as speechtranscription, language identification, and speech translation. The dataset,models, and code are publicly available at:https://github.com/hlt-mt/Speech-MASSIVE</description><author>Beomseok Lee, Ioan Calapodescu, Marco Gaido, Matteo Negri, Laurent Besacier</author><pubDate>Wed, 07 Aug 2024 16:55:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03900v1</guid></item><item><title>Simplifying Scholarly Abstracts for Accessible Digital Libraries</title><link>http://arxiv.org/abs/2408.03899v1</link><description>Standing at the forefront of knowledge dissemination, digital librariescurate vast collections of scientific literature. However, these scholarlywritings are often laden with jargon and tailored for domain experts ratherthan the general public. As librarians, we strive to offer services to adiverse audience, including those with lower reading levels. To extend ourservices beyond mere access, we propose fine-tuning a language model to rewritescholarly abstracts into more comprehensible versions, thereby making scholarlyliterature more accessible when requested. We began by introducing a corpusspecifically designed for training models to simplify scholarly abstracts. Thiscorpus consists of over three thousand pairs of abstracts and significancestatements from diverse disciplines. We then fine-tuned four language modelsusing this corpus. The outputs from the models were subsequently examined bothquantitatively for accessibility and semantic coherence, and qualitatively forlanguage quality, faithfulness, and completeness. Our findings show that theresulting models can improve readability by over three grade levels, whilemaintaining fidelity to the original content. Although commercialstate-of-the-art models still hold an edge, our models are much more compact,can be deployed locally in an affordable manner, and alleviate the privacyconcerns associated with using commercial models. We envision this work as astep toward more inclusive and accessible libraries, improving our services foryoung readers and those without a college degree.</description><author>Haining Wang, Jason Clark</author><pubDate>Wed, 07 Aug 2024 16:55:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03899v1</guid></item><item><title>Algorithmic Collective Action in Machine Learning</title><link>http://arxiv.org/abs/2302.04262v3</link><description>We initiate a principled study of algorithmic collective action on digitalplatforms that deploy machine learning algorithms. We propose a simpletheoretical model of a collective interacting with a firm's learning algorithm.The collective pools the data of participating individuals and executes analgorithmic strategy by instructing participants how to modify their own datato achieve a collective goal. We investigate the consequences of this model inthree fundamental learning-theoretic settings: the case of a nonparametricoptimal learning algorithm, a parametric risk minimizer, and gradient-basedoptimization. In each setting, we come up with coordinated algorithmicstrategies and characterize natural success criteria as a function of thecollective's size. Complementing our theory, we conduct systematic experimentson a skill classification task involving tens of thousands of resumes from agig platform for freelancers. Through more than two thousand model trainingruns of a BERT-like language model, we see a striking correspondence emergebetween our empirical observations and the predictions made by our theory.Taken together, our theory and experiments broadly support the conclusion thatalgorithmic collectives of exceedingly small fractional size can exertsignificant control over a platform's learning algorithm.</description><author>Moritz Hardt, Eric Mazumdar, Celestine Mendler-Dünner, Tijana Zrnic</author><pubDate>Wed, 07 Aug 2024 16:54:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04262v3</guid></item><item><title>LiRank: Industrial Large Scale Ranking Models at LinkedIn</title><link>http://arxiv.org/abs/2402.06859v2</link><description>We present LiRank, a large-scale ranking framework at LinkedIn that brings toproduction state-of-the-art modeling architectures and optimization methods. Weunveil several modeling improvements, including Residual DCN, which addsattention and residual connections to the famous DCNv2 architecture. We shareinsights into combining and tuning SOTA architectures to create a unifiedmodel, including Dense Gating, Transformers and Residual DCN. We also proposenovel techniques for calibration and describe how we productionalized deeplearning based explore/exploit methods. To enable effective, production-gradeserving of large ranking models, we detail how to train and compress modelsusing quantization and vocabulary compression. We provide details about thedeployment setup for large-scale use cases of Feed ranking, JobsRecommendations, and Ads click-through rate (CTR) prediction. We summarize ourlearnings from various A/B tests by elucidating the most effective technicalapproaches. These ideas have contributed to relative metrics improvementsacross the board at LinkedIn: +0.5% member sessions in the Feed, +1.76%qualified job applications for Jobs search and recommendations, and +4.3% forAds CTR. We hope this work can provide practical insights and solutions forpractitioners interested in leveraging large-scale deep ranking systems.</description><author>Fedor Borisyuk, Mingzhou Zhou, Qingquan Song, Siyu Zhu, Birjodh Tiwana, Ganesh Parameswaran, Siddharth Dangi, Lars Hertel, Qiang Xiao, Xiaochen Hou, Yunbo Ouyang, Aman Gupta, Sheallika Singh, Dan Liu, Hailing Cheng, Lei Le, Jonathan Hung, Sathiya Keerthi, Ruoyan Wang, Fengyu Zhang, Mohit Kothari, Chen Zhu, Daqi Sun, Yun Dai, Xun Luan, Sirou Zhu, Zhiwei Wang, Neil Daftary, Qianqi Shen, Chengming Jiang, Haichao Wei, Maneesh Varshney, Amol Ghoting, Souvik Ghosh</author><pubDate>Wed, 07 Aug 2024 16:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06859v2</guid></item><item><title>MORTAR: A Model-based Runtime Action Repair Framework for AI-enabled Cyber-Physical Systems</title><link>http://arxiv.org/abs/2408.03892v1</link><description>Cyber-Physical Systems (CPSs) are increasingly prevalent across variousindustrial and daily-life domains, with applications ranging from roboticoperations to autonomous driving. With recent advancements in artificialintelligence (AI), learning-based components, especially AI controllers, havebecome essential in enhancing the functionality and efficiency of CPSs.However, the lack of interpretability in these AI controllers presentschallenges to the safety and quality assurance of AI-enabled CPSs (AI-CPSs).Existing methods for improving the safety of AI controllers often involveneural network repair, which requires retraining with additional adversarialexamples or access to detailed internal information of the neural network.Hence, these approaches have limited applicability for black-box policies,where only the inputs and outputs are accessible during operation. To overcomethis, we propose MORTAR, a runtime action repair framework designed for AI-CPSsin this work. MORTAR begins by constructing a prediction model that forecaststhe quality of actions proposed by the AI controller. If an unsafe action isdetected, MORTAR then initiates a repair process to correct it. The generationof repaired actions is achieved through an optimization process guided by thesafety estimates from the prediction model. We evaluate the effectiveness ofMORTAR across various CPS tasks and AI controllers. The results demonstratethat MORTAR can efficiently improve task completion rates of AI controllersunder specified safety specifications. Meanwhile, it also maintains minimalcomputational overhead, ensuring real-time operation of the AI-CPSs.</description><author>Renzhi Wang, Zhehua Zhou, Jiayang Song, Xuan Xie, Xiaofei Xie, Lei Ma</author><pubDate>Wed, 07 Aug 2024 16:44:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03892v1</guid></item><item><title>Dual-Modeling Decouple Distillation for Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2408.03888v1</link><description>Knowledge distillation based on student-teacher network is one of themainstream solution paradigms for the challenging unsupervised AnomalyDetection task, utilizing the difference in representation capabilities of theteacher and student networks to implement anomaly localization. However,over-generalization of the student network to the teacher network may lead tonegligible differences in representation capabilities of anomaly, thusaffecting the detection effectiveness. Existing methods address the possibleover-generalization by using differentiated students and teachers from thestructural perspective or explicitly expanding distilled information from thecontent perspective, which inevitably result in an increased likelihood ofunderfitting of the student network and poor anomaly detection capabilities inanomaly center or edge. In this paper, we propose Dual-Modeling DecoupleDistillation (DMDD) for the unsupervised anomaly detection. In DMDD, a DecoupleStudent-Teacher Network is proposed to decouple the initial student featuresinto normality and abnormality features. We further introduce Dual-ModelingDistillation based on normal-anomaly image pairs, fitting normality features ofanomalous image and the teacher features of the corresponding normal image,widening the distance between abnormality features and the teacher features inanomalous regions. Synthesizing these two distillation ideas, we achieveanomaly detection which focuses on both edge and center of anomaly. Finally, aMulti-perception Segmentation Network is proposed to achieve focused anomalymap fusion based on multiple attention. Experimental results on MVTec AD showthat DMDD surpasses SOTA localization performance of previous knowledgedistillation-based methods, reaching 98.85% on pixel-level AUC and 96.13% onPRO.</description><author>Xinyue Liu, Jianyuan Wang, Biao Leng, Shuo Zhang</author><pubDate>Wed, 07 Aug 2024 16:39:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03888v1</guid></item><item><title>Global-Local Progressive Integration Network for Blind Image Quality Assessment</title><link>http://arxiv.org/abs/2408.03885v1</link><description>Vision transformers (ViTs) excel in computer vision for modeling long-termdependencies, yet face two key challenges for image quality assessment (IQA):discarding fine details during patch embedding, and requiring extensivetraining data due to lack of inductive biases. In this study, we propose aGlobal-Local progressive INTegration network for IQA, called GlintIQA, toaddress these issues through three key components: 1) Hybrid feature extractioncombines ViT-based global feature extractor (VGFE) and convolutional neuralnetworks (CNNs)-based local feature extractor (CLFE) to capture globalcoarse-grained features and local fine-grained features, respectively. Theincorporation of CNNs mitigates the patch-level information loss and inductivebias constraints inherent to ViT architectures. 2) Progressive featureintegration leverages diverse kernel sizes in embedding to spatially aligncoarse- and fine-grained features, and progressively aggregate these featuresby interactively stacking channel-wise attention and spatial enhancementmodules to build effective quality-aware representations. 3) Contentsimilarity-based labeling approach is proposed that automatically assignsquality labels to images with diverse content based on subjective qualityscores. This addresses the scarcity of labeled training data in syntheticdatasets and bolsters model generalization. The experimental resultsdemonstrate the efficacy of our approach, yielding 5.04% average SROCC gains oncross-authentic dataset evaluations. Moreover, our model and its counterpartpre-trained on the proposed dataset respectively exhibited 5.40% and 13.23%improvements on across-synthetic datasets evaluation. The codes and proposeddataset will be released at https://github.com/XiaoqiWang/GlintIQA.</description><author>Xiaoqi Wang, Yun Zhang</author><pubDate>Wed, 07 Aug 2024 16:34:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03885v1</guid></item><item><title>Every Dataset Counts: Scaling up Monocular 3D Object Detection with Joint Datasets Training</title><link>http://arxiv.org/abs/2310.00920v3</link><description>Monocular 3D object detection plays a crucial role in autonomous driving.However, existing monocular 3D detection algorithms depend on 3D labels derivedfrom LiDAR measurements, which are costly to acquire for new datasets andchallenging to deploy in novel environments. Specifically, this studyinvestigates the pipeline for training a monocular 3D object detection model ona diverse collection of 3D and 2D datasets. The proposed framework comprisesthree components: (1) a robust monocular 3D model capable of functioning acrossvarious camera settings, (2) a selective-training strategy to accommodatedatasets with differing class annotations, and (3) a pseudo 3D trainingapproach using 2D labels to enhance detection performance in scenes containingonly 2D labels. With this framework, we could train models on a joint set ofvarious open 3D/2D datasets to obtain models with significantly strongergeneralization capability and enhanced performance on new dataset with only 2Dlabels. We conduct extensive experiments onKITTI/nuScenes/ONCE/Cityscapes/BDD100K datasets to demonstrate the scalingability of the proposed method.</description><author>Fulong Ma, Xiaoyang Yan, Guoyang Zhao, Xiaojie Xu, Yuxuan Liu, Ming Liu</author><pubDate>Wed, 07 Aug 2024 16:30:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00920v3</guid></item><item><title>Knowledge Probing for Graph Representation Learning</title><link>http://arxiv.org/abs/2408.03877v1</link><description>Graph learning methods have been extensively applied in diverse applicationareas. However, what kind of inherent graph properties e.g. graph proximity,graph structural information has been encoded into graph representationlearning for downstream tasks is still under-explored. In this paper, wepropose a novel graph probing framework (GraphProbe) to investigate andinterpret whether the family of graph learning methods has encoded differentlevels of knowledge in graph representation learning. Based on the intrinsicproperties of graphs, we design three probes to systematically investigate thegraph representation learning process from different perspectives, respectivelythe node-wise level, the path-wise level, and the structural level. Weconstruct a thorough evaluation benchmark with nine representative graphlearning methods from random walk based approaches, basic graph neural networksand self-supervised graph methods, and probe them on six benchmark datasets fornode classification, link prediction and graph classification. The experimentalevaluation verify that GraphProbe can estimate the capability of graphrepresentation learning. Remaking results have been concluded: GCN andWeightedGCN methods are relatively versatile methods achieving better resultswith respect to different tasks.</description><author>Mingyu Zhao, Xingyu Huang, Ziyu Lyu, Yanlin Wang, Lixin Cui, Lu Bai</author><pubDate>Wed, 07 Aug 2024 16:27:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03877v1</guid></item><item><title>Personalized Clinical Note Generation from Doctor-Patient Conversations</title><link>http://arxiv.org/abs/2408.03874v1</link><description>In this work, we present a novel technique to improve the quality of draftclinical notes for physicians. This technique is concentrated on the ability tomodel implicit physician conversation styles and note preferences. We alsointroduce a novel technique for the enrollment of new physicians when a limitednumber of clinical notes paired with conversations are available for thatphysician, without the need to re-train a model to support them. We show thatour technique outperforms the baseline model by improving the ROUGE-2 score ofthe History of Present Illness section by 13.8%, the Physical Examinationsection by 88.6%, and the Assessment &amp; Plan section by 50.8%.</description><author>Nathan Brake, Thomas Schaaf</author><pubDate>Wed, 07 Aug 2024 16:24:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03874v1</guid></item><item><title>Inter-Series Transformer: Attending to Products in Time Series Forecasting</title><link>http://arxiv.org/abs/2408.03872v1</link><description>Time series forecasting is an important task in many fields ranging fromsupply chain management to weather forecasting. Recently, Transformer neuralnetwork architectures have shown promising results in forecasting on commontime series benchmark datasets. However, application to supply chain demandforecasting, which can have challenging characteristics such as sparsity andcross-series effects, has been limited. In this work, we explore the application of Transformer-based models tosupply chain demand forecasting. In particular, we develop a newTransformer-based forecasting approach using a shared, multi-task per-timeseries network with an initial component applying attention across time series,to capture interactions and help address sparsity. We provide a case studyapplying our approach to successfully improve demand prediction for a medicaldevice manufacturing company. To further validate our approach, we also applyit to public demand forecasting datasets as well and demonstrate competitive tosuperior performance compared to a variety of baseline and state-of-the-artforecast methods across the private and public datasets.</description><author>Rares Cristian, Pavithra Harsha, Clemente Ocejo, Georgia Perakis, Brian Quanz, Ioannis Spantidakis, Hamza Zerhouni</author><pubDate>Wed, 07 Aug 2024 16:22:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03872v1</guid></item><item><title>BeeManc at the PLABA Track of TAC-2023: Investigating LLMs and Controllable Attributes for Improving Biomedical Text Readability</title><link>http://arxiv.org/abs/2408.03871v1</link><description>In this system report, we describe the models and methods we used for ourparticipation in the PLABA2023 task on biomedical abstract simplification, partof the TAC 2023 tracks. The system outputs we submitted come from the followingthree categories: 1) domain fine-tuned T5-like models including Biomedical-T5and Lay-SciFive; 2) fine-tuned BARTLarge model with controllable attributes(via tokens) BART-w-CTs; 3) ChatGPTprompting. We also present the work wecarried out for this task on BioGPT finetuning. In the official automaticevaluation using SARI scores, BeeManc ranks 2nd among all teams and our modelLaySciFive ranks 3rd among all 13 evaluated systems. In the official humanevaluation, our model BART-w-CTs ranks 2nd on Sentence-Simplicity (score92.84), 3rd on Term-Simplicity (score 82.33) among all 7 evaluated systems; Italso produced a high score 91.57 on Fluency in comparison to the highest score93.53. In the second round of submissions, our team using ChatGPT-promptingranks the 2nd in several categories including simplified term accuracy score92.26 and completeness score 96.58, and a very similar score on faithfulnessscore 95.3 to re-evaluated PLABA-base-1 (95.73) via human evaluations. Ourcodes, fine-tuned models, prompts, and data splits from the system developmentstage will be available at https://github.com/ HECTA-UoM/PLABA-MU</description><author>Zihao Li, Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Matthew Shardlow, Goran Nenadic</author><pubDate>Wed, 07 Aug 2024 16:21:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03871v1</guid></item><item><title>PraFFL: A Preference-Aware Scheme in Fair Federated Learning</title><link>http://arxiv.org/abs/2404.08973v2</link><description>Fairness in federated learning has emerged as a critical concern, aiming todevelop an unbiased model for any special group (e.g., male or female) ofsensitive features. However, there is a trade-off between model performance andfairness, i.e., improving model fairness will decrease model performance.Existing approaches have characterized such a trade-off by introducinghyperparameters to quantify client's preferences for model fairness and modelperformance. Nevertheless, these approaches are limited to scenarios where eachclient has only a single pre-defined preference, and fail to work in practicalsystems where each client generally have multiple preferences. The keychallenge is to design a method that allows the model to adapt to diversepreferences of each client in real time. To this end, we propose aPreference-aware scheme in Fair Federated Learning paradigm (called PraFFL) togenerate preference-wise model in real time. PraFFL can adaptively adjust themodel based on each client's preferences to meet their needs. We theoreticallyprove that PraFFL can offer the optimal model tailored to an arbitrarypreference of each client, and show its linear convergence. Experimentalresults show that our proposed PraFFL outperforms five fair federated learningalgorithms in terms of the model's capability of adapting to clients' differentpreferences.</description><author>Rongguang Ye, Wei-Bin Kou, Ming Tang</author><pubDate>Wed, 07 Aug 2024 16:21:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08973v2</guid></item><item><title>Digital Twins and Civil Engineering Phases: Reorienting Adoption Strategies</title><link>http://arxiv.org/abs/2403.02426v2</link><description>Digital twin (DT) technology has received immense attention over the yearsdue to the promises it presents to various stakeholders in science andengineering. As a result, different thematic areas of DT have been explored.This is no different in specific fields such as manufacturing, automation, oiland gas, and civil engineering, leading to fragmented approaches forfield-specific applications. The civil engineering industry is furtherdisadvantaged in this regard as it relies on external techniques by otherengineering fields for its DT adoption. A rising consequence of theseextensions is a concentrated application of DT to the operations andmaintenance phase. On another spectrum, Building Information Modeling (BIM) ispervasively utilized in the planning/design phase, and the transient nature ofthe construction phase remains a challenge for its DT adoption. In this paper,we present a phase-based development of DT in the Architecture, Engineering,and Construction industry. We commence by presenting succinct expositions on DTas a concept and as a service, and establish a five-level scale system.Furthermore, we present separately a systematic literature review of theconventional techniques employed at each civil engineering phase. In thisregard, we identified enabling technologies such as computer vision forextended sensing and the Internet of Things for reliable integration.Ultimately, we attempt to reveal DT as an important tool across the entire lifecycle of civil engineering projects, and nudge researchers to think moreholistically in their quest for the integration of DT for civil engineeringapplications.</description><author>Taiwo A. Adebiyi, Nafeezat A. Ajenifuja, Ruda Zhang</author><pubDate>Wed, 07 Aug 2024 16:20:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02426v2</guid></item><item><title>ORBIT: Oak Ridge Base Foundation Model for Earth System Predictability</title><link>http://arxiv.org/abs/2404.14712v3</link><description>Earth system predictability is challenged by the complexity of environmentaldynamics and the multitude of variables involved. Current AI foundation models,although advanced by leveraging large and heterogeneous data, are oftenconstrained by their size and data integration, limiting their effectiveness inaddressing the full range of Earth system prediction challenges. To overcomethese limitations, we introduce the Oak Ridge Base Foundation Model for EarthSystem Predictability (ORBIT), an advanced vision transformer model that scalesup to 113 billion parameters using a novel hybrid tensor-data orthogonalparallelism technique. As the largest model of its kind, ORBIT surpasses thecurrent climate AI foundation model size by a thousandfold. Performance scalingtests conducted on the Frontier supercomputer have demonstrated that ORBITachieves 684 petaFLOPS to 1.6 exaFLOPS sustained throughput, with scalingefficiency maintained at 41% to 85% across 49,152 AMD GPUs. These breakthroughsestablish new advances in AI-driven climate modeling and demonstrate promise tosignificantly improve the Earth system predictability.</description><author>Xiao Wang, Siyan Liu, Aristeidis Tsaris, Jong-Youl Choi, Ashwin Aji, Ming Fan, Wei Zhang, Junqi Yin, Moetasim Ashfaq, Dan Lu, Prasanna Balaprakash</author><pubDate>Wed, 07 Aug 2024 16:19:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14712v3</guid></item><item><title>Surgformer: Surgical Transformer with Hierarchical Temporal Attention for Surgical Phase Recognition</title><link>http://arxiv.org/abs/2408.03867v1</link><description>Existing state-of-the-art methods for surgical phase recognition either relyon the extraction of spatial-temporal features at a short-range temporalresolution or adopt the sequential extraction of the spatial and temporalfeatures across the entire temporal resolution. However, these methods havelimitations in modeling spatial-temporal dependency and addressingspatial-temporal redundancy: 1) These methods fail to effectively modelspatial-temporal dependency, due to the lack of long-range information or jointspatial-temporal modeling. 2) These methods utilize dense spatial featuresacross the entire temporal resolution, resulting in significantspatial-temporal redundancy. In this paper, we propose the Surgical Transformer(Surgformer) to address the issues of spatial-temporal modeling and redundancyin an end-to-end manner, which employs divided spatial-temporal attention andtakes a limited set of sparse frames as input. Moreover, we propose a novelHierarchical Temporal Attention (HTA) to capture both global and localinformation within varied temporal resolutions from a target frame-centricperspective. Distinct from conventional temporal attention that primarilyemphasizes dense long-range similarity, HTA not only captures long-terminformation but also considers local latent consistency among informativeframes. HTA then employs pyramid feature aggregation to effectively utilizetemporal information across diverse temporal resolutions, thereby enhancing theoverall temporal representation. Extensive experiments on two challengingbenchmark datasets verify that our proposed Surgformer performs favorablyagainst the state-of-the-art methods. The code is released athttps://github.com/isyangshu/Surgformer.</description><author>Shu Yang, Luyang Luo, Qiong Wang, Hao Chen</author><pubDate>Wed, 07 Aug 2024 16:16:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03867v1</guid></item><item><title>PackMamba: Efficient Processing of Variable-Length Sequences in Mamba training</title><link>http://arxiv.org/abs/2408.03865v1</link><description>With the evolution of large language models, traditional Transformer modelsbecome computationally demanding for lengthy sequences due to the quadraticgrowth in computation with respect to the sequence length. Mamba, emerging as agroundbreaking architecture in the field of generative AI, demonstratesremarkable proficiency in handling elongated sequences with reducedcomputational and memory complexity. Nevertheless, the existing trainingframework of Mamba presents inefficiency with variable-length sequence inputs.Either single-sequence training results in low GPU utilization, or batchedprocessing of variable-length sequences to a maximum length incurs considerablememory and computational overhead. To address this problem, we analyze theperformance of bottleneck operators in Mamba under diverse tensor shapes andproposed PackMamba, a high-throughput Mamba that efficiently handlesvariable-length sequences. Diving deep into state-space models (SSMs), wemodify the parallel operators to avoid passing information between individualsequences while maintaining high performance. Experimental results on an NVIDIAA100 GPU demonstrate throughput exceeding the baseline single-sequenceprocessing scheme: 3.06x speedup on the 1.4B model and 2.62x on the 2.8B model.</description><author>Haoran Xu, Ziqian Liu, Rong Fu, Zhongling Su, Zerui Wang, Zheng Cai, Zhilin Pei, Xingcheng Zhang</author><pubDate>Wed, 07 Aug 2024 16:13:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03865v1</guid></item><item><title>Visualize and Paint GAN Activations</title><link>http://arxiv.org/abs/2405.15636v3</link><description>We investigate how generated structures of GANs correlate with theiractivations in hidden layers, with the purpose of better understanding theinner workings of those models and being able to paint structures withunconditionally trained GANs. This gives us more control over the generatedimages, allowing to generate them from a semantic segmentation map while notrequiring such a segmentation in the training data. To this end we introducethe concept of tileable features, allowing us to identify activations that workwell for painting.</description><author>Rudolf Herdt, Peter Maass</author><pubDate>Wed, 07 Aug 2024 16:07:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15636v3</guid></item><item><title>Out-of-Distribution-Aware Electric Vehicle Charging</title><link>http://arxiv.org/abs/2311.05941v3</link><description>We tackle the challenge of learning to charge Electric Vehicles (EVs) withOut-of-Distribution (OOD) data. Traditional scheduling algorithms typicallyfail to balance near-optimal average performance with worst-case guarantees,particularly with OOD data. Model Predictive Control (MPC) is often tooconservative and data-independent, whereas Reinforcement Learning (RL) tends tobe overly aggressive and fully trusts the data, hindering their ability toconsistently achieve the best-of-both-worlds. To bridge this gap, we introducea novel OOD-aware scheduling algorithm, denoted OOD-Charging. This algorithmemploys a dynamic "awareness radius", which updates in real-time based on theTemporal Difference (TD)-error that reflects the severity of OOD. TheOOD-Charging algorithm allows for a more effective balance between consistencyand robustness in EV charging schedules, thereby significantly enhancingadaptability and efficiency in real-world charging environments. Our resultsdemonstrate that this approach improves the scheduling reward reliably underreal OOD scenarios with remarkable shifts of EV charging behaviors caused byCOVID-19 in the Caltech ACN-Data.</description><author>Tongxin Li, Chenxi Sun</author><pubDate>Wed, 07 Aug 2024 15:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05941v3</guid></item><item><title>Sólo Escúchame: Spanish Emotional Accompaniment Chatbot</title><link>http://arxiv.org/abs/2408.01852v2</link><description>According to the World Health Organization (WHO), suicide was the fourthleading cause of death in the world for individuals aged 15 to 29 in 2019.Given the rapid increase in mental health issues, providing psychologicalsupport is both crucial and urgent. In this paper: (1) we propose S\'oloEsc\'uchame, the first open-source Spanish emotional assistance chatbot, basedon LLaMA-2-7b-Chat. (2) We introduced the HEAR (Hispanic EmotionalAccompaniment Responses) dataset, compiled from multiple English sourcestranslated into Spanish, as well as generic data generated usingChatGPT-3.5-Turbo. Finally, (3) we propose an evaluation metric based on twosemi-automatic assessment methods. Our system outperforms a range ofstate-of-the-art models in providing psychological assistance in Spanish. Ourmodels and datasets are publicly available to facilitate reproducibility.</description><author>Bruno Gil Ramírez, Jessica López Espejel, María del Carmen Santiago Díaz, Gustavo Trinidad Rubín Linares</author><pubDate>Wed, 07 Aug 2024 15:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01852v2</guid></item><item><title>Why transformers are obviously good models of language</title><link>http://arxiv.org/abs/2408.03855v1</link><description>Nobody knows how language works, but many theories abound. Transformers are aclass of neural networks that process language automatically with more successthan alternatives, both those based on neural computations and those that relyon other (e.g. more symbolic) mechanisms. Here, I highlight direct connectionsbetween the transformer architecture and certain theoretical perspectives onlanguage. The empirical success of transformers relative to alternative modelsprovides circumstantial evidence that the linguistic approaches thattransformers embody should be, at least, evaluated with greater scrutiny by thelinguistics community and, at best, considered to be the currently bestavailable theories.</description><author>Felix Hill</author><pubDate>Wed, 07 Aug 2024 15:52:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03855v1</guid></item><item><title>Hate Speech Detection and Classification in Amharic Text with Deep Learning</title><link>http://arxiv.org/abs/2408.03849v1</link><description>Hate speech is a growing problem on social media. It can seriously impactsociety, especially in countries like Ethiopia, where it can trigger conflictsamong diverse ethnic and religious groups. While hate speech detection inresource rich languages are progressing, for low resource languages such asAmharic are lacking. To address this gap, we develop Amharic hate speech dataand SBi-LSTM deep learning model that can detect and classify text into fourcategories of hate speech: racial, religious, gender, and non-hate speech. Wehave annotated 5k Amharic social media post and comment data into fourcategories. The data is annotated using a custom annotation tool by a total of100 native Amharic speakers. The model achieves a 94.8 F1-score performance.Future improvements will include expanding the dataset and develop state-of-theart models. Keywords: Amharic hate speech detection, classification, Amharic dataset,Deep Learning, SBi-LSTM</description><author>Samuel Minale Gashe, Seid Muhie Yimam, Yaregal Assabie</author><pubDate>Wed, 07 Aug 2024 15:46:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03849v1</guid></item><item><title>G-invariant diffusion maps</title><link>http://arxiv.org/abs/2306.07350v3</link><description>The diffusion maps embedding of data lying on a manifold has shown success intasks such as dimensionality reduction, clustering, and data visualization. Inthis work, we consider embedding data sets that were sampled from a manifoldwhich is closed under the action of a continuous matrix group. An example ofsuch a data set is images whose planar rotations are arbitrary. The G-invariantgraph Laplacian, introduced in Part I of this work, admits eigenfunctions inthe form of tensor products between the elements of the irreducible unitaryrepresentations of the group and eigenvectors of certain matrices. We employthese eigenfunctions to derive diffusion maps that intrinsically account forthe group action on the data. In particular, we construct both equivariant andinvariant embeddings, which can be used to cluster and align the data points.We demonstrate the utility of our construction in the problem of randomcomputerized tomography.</description><author>Eitan Rosen, Xiuyuan Cheng, Yoel Shkolnisky</author><pubDate>Wed, 07 Aug 2024 15:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07350v3</guid></item><item><title>Bi-Level Spatial and Channel-aware Transformer for Learned Image Compression</title><link>http://arxiv.org/abs/2408.03842v1</link><description>Recent advancements in learned image compression (LIC) methods havedemonstrated superior performance over traditional hand-crafted codecs. Theselearning-based methods often employ convolutional neural networks (CNNs) orTransformer-based architectures. However, these nonlinear approaches frequentlyoverlook the frequency characteristics of images, which limits theircompression efficiency. To address this issue, we propose a novelTransformer-based image compression method that enhances the transformationstage by considering frequency components within the feature map. Our methodintegrates a novel Hybrid Spatial-Channel Attention Transformer Block (HSCATB),where a spatial-based branch independently handles high and low frequencies atthe attention layer, and a Channel-aware Self-Attention (CaSA) module capturesinformation across channels, significantly improving compression performance.Additionally, we introduce a Mixed Local-Global Feed Forward Network (MLGFFN)within the Transformer block to enhance the extraction of diverse and richinformation, which is crucial for effective compression. These innovationscollectively improve the transformation's ability to project data into a moredecorrelated latent space, thereby boosting overall compression efficiency.Experimental results demonstrate that our framework surpasses state-of-the-artLIC methods in rate-distortion performance.</description><author>Hamidreza Soltani, Erfan Ghasemi</author><pubDate>Wed, 07 Aug 2024 15:35:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03842v1</guid></item><item><title>MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models</title><link>http://arxiv.org/abs/2408.03841v1</link><description>The application of large language models to facilitate automated softwareoperations and tool generation (SOTG), thus augmenting software productivity,mirrors the early stages of human evolution when the ability to create and usetools accelerated the progress of civilization. These complex tasks require AIto continuously summarize and improve. Current research often overlooks theimportance of converting real-time task experiences into system memory anddifferentiating the value of existing knowledge for future reference. Thispaper addresses these issues by evolving external memory models intoMemory-Loop Networks for timely memorization and experience referencing. Wealso enhance a RAG mechanism with knowledge precision segmentation to utilizememory based on value differentiation, and design the MaxMind model for SOTGaccordingly.To demonstrate our approach, we developed MaxMind4Sheet, anelectronic spreadsheet processing system aligned with the MaxMind philosophy.Comparative experiments with SheetCopilot have demonstrated that theaccumulation and recycling of task memories lead to a steady enhancement intask success rate, with an improvement rate of approximately 3%-6% per round inthis implementation example. Note that as the memories continue to grow, thiscumulative improvement may be substantial. The inclusion of memory recyclingcan also boost the system's task execution efficiency by up to 25%, and it canaddress the retraining issue faced by LLMs when handling specialized tasksthrough memories transfer.These suggest that MaxMind has significant potentialto enhance the capabilities and productivity of LLM systems in SOTG.</description><author>Yuchen Dong, XiaoXiang Fang, Yuchen Hu, Renshuang Jiang, Zhe Jiang</author><pubDate>Wed, 07 Aug 2024 15:27:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03841v1</guid></item><item><title>Using a Distance Sensor to Detect Deviations in a Planar Surface</title><link>http://arxiv.org/abs/2408.03838v1</link><description>We investigate methods for determining if a planar surface contains geometricdeviations (e.g., protrusions, objects, divots, or cliffs) using only aninstantaneous measurement from a miniature optical time-of-flight sensor. Thekey to our method is to utilize the entirety of information encoded in rawtime-of-flight data captured by off-the-shelf distance sensors. We provide ananalysis of the problem in which we identify the key ambiguity between geometryand surface photometrics. To overcome this challenging ambiguity, we fit aGaussian mixture model to a small dataset of planar surface measurements. Thismodel implicitly captures the expected geometry and distribution ofphotometrics of the planar surface and is used to identify measurements thatare likely to contain deviations. We characterize our method on a variety ofsurfaces and planar deviations across a range of scenarios. We find that ourmethod utilizing raw time-of-flight data outperforms baselines which use onlyderived distance estimates. We build an example application in which our methodenables mobile robot obstacle and cliff avoidance over a wide field-of-view.</description><author>Carter Sifferman, William Sun, Mohit Gupta, Michael Gleicher</author><pubDate>Wed, 07 Aug 2024 15:24:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03838v1</guid></item><item><title>WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models</title><link>http://arxiv.org/abs/2408.03837v1</link><description>WalledEval is a comprehensive AI safety testing toolkit designed to evaluatelarge language models (LLMs). It accommodates a diverse range of models,including both open-weight and API-based ones, and features over 35 safetybenchmarks covering areas such as multilingual safety, exaggerated safety, andprompt injections. The framework supports both LLM and judge benchmarking, andincorporates custom mutators to test safety against various text-stylemutations such as future tense and paraphrasing. Additionally, WalledEvalintroduces WalledGuard, a new, small and performant content moderation tool,and SGXSTest, a benchmark for assessing exaggerated safety in culturalcontexts. We make WalledEval publicly available athttps://github.com/walledai/walledevalA.</description><author>Prannaya Gupta, Le Qi Yau, Hao Han Low, I-Shiang Lee, Hugo Maximus Lim, Yu Xin Teoh, Jia Hng Koh, Dar Win Liew, Rishabh Bhardwaj, Rajat Bhardwaj, Soujanya Poria</author><pubDate>Wed, 07 Aug 2024 15:22:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03837v1</guid></item><item><title>Target Prompting for Information Extraction with Vision Language Model</title><link>http://arxiv.org/abs/2408.03834v1</link><description>The recent trend in the Large Vision and Language model has brought a newchange in how information extraction systems are built. VLMs have set a newbenchmark with their State-of-the-art techniques in understanding documents andbuilding question-answering systems across various industries. They aresignificantly better at generating text from document images and providingaccurate answers to questions. However, there are still some challenges ineffectively utilizing these models to build a precise conversational system.General prompting techniques used with large language models are often notsuitable for these specially designed vision language models. The outputgenerated by such generic input prompts is ordinary and may contain informationgaps when compared with the actual content of the document. To obtain moreaccurate and specific answers, a well-targeted prompt is required by the visionlanguage model, along with the document image. In this paper, a technique isdiscussed called Target prompting, which focuses on explicitly targeting partsof document images and generating related answers from those specific regionsonly. The paper also covers the evaluation of response for each promptingtechnique using different user queries and input prompts.</description><author>Dipankar Medhi</author><pubDate>Wed, 07 Aug 2024 15:17:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03834v1</guid></item><item><title>An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases</title><link>http://arxiv.org/abs/2407.10853v2</link><description>Large language models (LLMs) can exhibit bias in a variety of ways. Suchbiases can create or exacerbate unfair outcomes for certain groups within aprotected attribute, including, but not limited to sex, race, sexualorientation, or age. This paper aims to provide a technical guide forpractitioners to assess bias and fairness risks in LLM use cases. The maincontribution of this work is a decision framework that allows practitioners todetermine which metrics to use for a specific LLM use case. To achieve this,this study categorizes LLM bias and fairness risks, maps those risks to ataxonomy of LLM use cases, and then formally defines various metrics to assesseach type of risk. As part of this work, several new bias and fairness metricsare introduced, including innovative counterfactual metrics as well as metricsbased on stereotype classifiers. Instead of focusing solely on the modelitself, the sensitivity of both prompt-risk and model-risk are taken intoaccount by defining evaluations at the level of an LLM use case, characterizedby a model and a population of prompts. Furthermore, because all of theevaluation metrics are calculated solely using the LLM output, the proposedframework is highly practical and easily actionable for practitioners.</description><author>Dylan Bouchard</author><pubDate>Wed, 07 Aug 2024 15:12:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10853v2</guid></item><item><title>DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs</title><link>http://arxiv.org/abs/2403.19588v2</link><description>This paper revives Densely Connected Convolutional Networks (DenseNets) andreveals the underrated effectiveness over predominant ResNet-stylearchitectures. We believe DenseNets' potential was overlooked due to untouchedtraining methods and traditional design elements not fully revealing theircapabilities. Our pilot study shows dense connections through concatenation arestrong, demonstrating that DenseNets can be revitalized to compete with modernarchitectures. We methodically refine suboptimal components - architecturaladjustments, block redesign, and improved training recipes towards wideningDenseNets and boosting memory efficiency while keeping concatenation shortcuts.Our models, employing simple architectural elements, ultimately surpass SwinTransformer, ConvNeXt, and DeiT-III - key architectures in the residuallearning lineage. Furthermore, our models exhibit near state-of-the-artperformance on ImageNet-1K, competing with the very recent models anddownstream tasks, ADE20k semantic segmentation, and COCO objectdetection/instance segmentation. Finally, we provide empirical analyses thatuncover the merits of the concatenation over additive shortcuts, steering arenewed preference towards DenseNet-style designs. Our code is available athttps://github.com/naver-ai/rdnet.</description><author>Donghyun Kim, Byeongho Heo, Dongyoon Han</author><pubDate>Wed, 07 Aug 2024 15:11:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19588v2</guid></item><item><title>New Job, New Gender? Measuring the Social Bias in Image Generation Models</title><link>http://arxiv.org/abs/2401.00763v2</link><description>Image generation models can generate or edit images from a given text. Recentadvancements in image generation technology, exemplified by DALL-E andMidjourney, have been groundbreaking. These advanced models, despite theirimpressive capabilities, are often trained on massive Internet datasets, makingthem susceptible to generating content that perpetuates social stereotypes andbiases, which can lead to severe consequences. Prior research on assessing biaswithin image generation models suffers from several shortcomings, includinglimited accuracy, reliance on extensive human labor, and lack of comprehensiveanalysis. In this paper, we propose BiasPainter, a novel evaluation frameworkthat can accurately, automatically and comprehensively trigger social bias inimage generation models. BiasPainter uses a diverse range of seed images ofindividuals and prompts the image generation models to edit these images usinggender, race, and age-neutral queries. These queries span 62 professions, 39activities, 57 types of objects, and 70 personality traits. The framework thencompares the edited images to the original seed images, focusing on thesignificant changes related to gender, race, and age. BiasPainter adopts a keyinsight that these characteristics should not be modified when subjected toneutral prompts. Built upon this design, BiasPainter can trigger the socialbias and evaluate the fairness of image generation models. We use BiasPainterto evaluate six widely-used image generation models, such as stable diffusionand Midjourney. Experimental results show that BiasPainter can successfullytrigger social bias in image generation models. According to our humanevaluation, BiasPainter can achieve 90.8% accuracy on automatic bias detection,which is significantly higher than the results reported in previous work.</description><author>Wenxuan Wang, Haonan Bai, Jen-tse Huang, Yuxuan Wan, Youliang Yuan, Haoyi Qiu, Nanyun Peng, Michael R. Lyu</author><pubDate>Wed, 07 Aug 2024 15:10:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00763v2</guid></item><item><title>Anomalies, Representations, and Self-Supervision</title><link>http://arxiv.org/abs/2301.04660v2</link><description>We develop a self-supervised method for density-based anomaly detection usingcontrastive learning, and test it using event-level anomaly data from CMSADC2021. The AnomalyCLR technique is data-driven and uses augmentations of thebackground data to mimic non-Standard-Model events in a model-agnostic way. Ituses a permutation-invariant Transformer Encoder architecture to map theobjects measured in a collider event to the representation space, where thedata augmentations define a representation space which is sensitive topotential anomalous features. An AutoEncoder trained on backgroundrepresentations then computes anomaly scores for a variety of signals in therepresentation space. With AnomalyCLR we find significant improvements onperformance metrics for all signals when compared to the raw data baseline.</description><author>Barry M. Dillon, Luigi Favaro, Friedrich Feiden, Tanmoy Modak, Tilman Plehn</author><pubDate>Wed, 07 Aug 2024 15:07:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.04660v2</guid></item><item><title>Automated Code Fix Suggestions for Accessibility Issues in Mobile Apps</title><link>http://arxiv.org/abs/2408.03827v1</link><description>Accessibility is crucial for inclusive app usability, yet developers oftenstruggle to identify and fix app accessibility issues due to a lack ofawareness, expertise, and inadequate tools. Current accessibility testing toolscan identify accessibility issues but may not always provide guidance on how toaddress them. We introduce FixAlly, an automated tool designed to suggestsource code fixes for accessibility issues detected by automated accessibilityscanners. FixAlly employs a multi-agent LLM architecture to generate fixstrategies, localize issues within the source code, and propose codemodification suggestions to fix the accessibility issue. Our empirical studydemonstrates FixAlly's capability in suggesting fixes that resolve issues foundby accessibility scanners -- with an effectiveness of 77% in generatingplausible fix suggestions -- and our survey of 12 iOS developers finds theywould be willing to accept 69.4% of evaluated fix suggestions.</description><author>Forough Mehralian, Titus Barik, Jeff Nichols, Amanda Swearngin</author><pubDate>Wed, 07 Aug 2024 15:06:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03827v1</guid></item><item><title>Towards Real-Time Gaussian Splatting: Accelerating 3DGS through Photometric SLAM</title><link>http://arxiv.org/abs/2408.03825v1</link><description>Initial applications of 3D Gaussian Splatting (3DGS) in Visual SimultaneousLocalization and Mapping (VSLAM) demonstrate the generation of high-qualityvolumetric reconstructions from monocular video streams. However, despite thesepromising advancements, current 3DGS integrations have reduced trackingperformance and lower operating speeds compared to traditional VSLAM. Toaddress these issues, we propose integrating 3DGS with Direct Sparse Odometry,a monocular photometric SLAM system. We have done preliminary experimentsshowing that using Direct Sparse Odometry point cloud outputs, as opposed tostandard structure-from-motion methods, significantly shortens the trainingtime needed to achieve high-quality renders. Reducing 3DGS training timeenables the development of 3DGS-integrated SLAM systems that operate inreal-time on mobile hardware. These promising initial findings suggest furtherexploration is warranted in combining traditional VSLAM systems with 3DGS.</description><author>Yan Song Hu, Dayou Mao, Yuhao Chen, John Zelek</author><pubDate>Wed, 07 Aug 2024 15:01:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03825v1</guid></item><item><title>Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields</title><link>http://arxiv.org/abs/2408.03822v1</link><description>3D Gaussian splatting (3DGS) has recently emerged as an alternativerepresentation that leverages a 3D Gaussian-based representation and introducesan approximated volumetric rendering, achieving very fast rendering speed andpromising image quality. Furthermore, subsequent studies have successfullyextended 3DGS to dynamic 3D scenes, demonstrating its wide range ofapplications. However, a significant drawback arises as 3DGS and its followingmethods entail a substantial number of Gaussians to maintain the high fidelityof the rendered images, which requires a large amount of memory and storage. Toaddress this critical issue, we place a specific emphasis on two keyobjectives: reducing the number of Gaussian points without sacrificingperformance and compressing the Gaussian attributes, such as view-dependentcolor and covariance. To this end, we propose a learnable mask strategy thatsignificantly reduces the number of Gaussians while preserving highperformance. In addition, we propose a compact but effective representation ofview-dependent color by employing a grid-based neural field rather than relyingon spherical harmonics. Finally, we learn codebooks to compactly represent thegeometric and temporal attributes by residual vector quantization. With modelcompression techniques such as quantization and entropy coding, we consistentlyshow over 25x reduced storage and enhanced rendering speed compared to 3DGS forstatic scenes, while maintaining the quality of the scene representation. Fordynamic scenes, our approach achieves more than 12x storage efficiency andretains a high-quality reconstruction compared to the existing state-of-the-artmethods. Our work provides a comprehensive framework for 3D scenerepresentation, achieving high performance, fast training, compactness, andreal-time rendering. Our project page is available athttps://maincold2.github.io/c3dgs/.</description><author>Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, Eunbyung Park</author><pubDate>Wed, 07 Aug 2024 14:56:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03822v1</guid></item><item><title>Leveraging Variation Theory in Counterfactual Data Augmentation for Optimized Active Learning</title><link>http://arxiv.org/abs/2408.03819v1</link><description>Active Learning (AL) allows models to learn interactively from user feedback.This paper introduces a counterfactual data augmentation approach to AL,particularly addressing the selection of datapoints for user querying, apivotal concern in enhancing data efficiency. Our approach is inspired byVariation Theory, a theory of human concept learning that emphasizes theessential features of a concept by focusing on what stays the same and whatchanges. Instead of just querying with existing datapoints, our approachsynthesizes artificial datapoints that highlight potential key similarities anddifferences among labels using a neuro-symbolic pipeline combining largelanguage models (LLMs) and rule-based models. Through an experiment in theexample domain of text classification, we show that our approach achievessignificantly higher performance when there are fewer annotated data. As theannotated training data gets larger the impact of the generated data starts todiminish showing its capability to address the cold start problem in AL. Thisresearch sheds light on integrating theories of human learning into theoptimization of AL.</description><author>Simret Araya Gebreegziabher, Kuangshi Ai, Zheng Zhang, Elena L. Glassman, Toby Jia-Jun Li</author><pubDate>Wed, 07 Aug 2024 14:55:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03819v1</guid></item><item><title>SpecRover: Code Intent Extraction via LLMs</title><link>http://arxiv.org/abs/2408.02232v2</link><description>Autonomous program improvement typically involves automatically producing bugfixes and feature additions. Such program improvement can be accomplished by acombination of large language model (LLM) and program analysis capabilities, inthe form of an LLM agent. Since program repair or program improvement typicallyrequires a specification of intended behavior - specification inference can beuseful for producing high quality program patches. In this work, we examineefficient and low-cost workflows for iterative specification inference withinan LLM agent. Given a GitHub issue to be resolved in a software project, ourgoal is to conduct iterative code search accompanied by specification inference- thereby inferring intent from both the project structure and behavior. Theintent thus captured is examined by a reviewer agent with the goal of vettingthe patches as well as providing a measure of confidence in the vetted patches.Our approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agentAutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHubissues, it shows more than 50% improvement in efficacy over AutoCodeRover.Compared to the open-source agents available, our work shows modest cost ($0.65per issue) in resolving an average GitHub issue in SWE-Bench lite. Theproduction of explanation by SpecRover allows for a better "signal" to be givento the developer, on when the suggested patches can be accepted withconfidence. SpecRover also seeks to demonstrate the continued importance ofspecification inference in automated program repair, even as program repairtechnologies enter the LLM era.</description><author>Haifeng Ruan, Yuntong Zhang, Abhik Roychoudhury</author><pubDate>Wed, 07 Aug 2024 14:52:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02232v2</guid></item><item><title>Early Prediction of Causes (not Effects) in Healthcare by Long-Term Clinical Time Series Forecasting</title><link>http://arxiv.org/abs/2408.03816v1</link><description>Machine learning for early syndrome diagnosis aims to solve the intricatetask of predicting a ground truth label that most often is the outcome (effect)of a medical consensus definition applied to observed clinical measurements(causes), given clinical measurements observed several hours before. Instead offocusing on the prediction of the future effect, we propose to directly predictthe causes via time series forecasting (TSF) of clinical variables anddetermine the effect by applying the gold standard consensus definition to theforecasted values. This method has the invaluable advantage of beingstraightforwardly interpretable to clinical practitioners, and because modeltraining does not rely on a particular label anymore, the forecasted data canbe used to predict any consensus-based label. We exemplify our method by meansof long-term TSF with Transformer models, with a focus on accurate predictionof sparse clinical variables involved in the SOFA-based Sepsis-3 definition andthe new Simplified Acute Physiology Score (SAPS-II) definition. Our experimentsare conducted on two datasets and show that contrary to recent proposals whichadvocate set function encoders for time series and direct multi-step decoders,best results are achieved by a combination of standard dense encoders withiterative multi-step decoders. The key for success of iterative multi-stepdecoding can be attributed to its ability to capture cross-variate dependenciesand to a student forcing training strategy that teaches the model to rely onits own previous time step predictions for the next time step prediction.</description><author>Michael Staniek, Marius Fracarolli, Michael Hagmann, Stefan Riezler</author><pubDate>Wed, 07 Aug 2024 14:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03816v1</guid></item><item><title>Generative Language Models with Retrieval Augmented Generation for Automated Short Answer Scoring</title><link>http://arxiv.org/abs/2408.03811v1</link><description>Automated Short Answer Scoring (ASAS) is a critical component in educationalassessment. While traditional ASAS systems relied on rule-based algorithms orcomplex deep learning methods, recent advancements in Generative LanguageModels (GLMs) offer new opportunities for improvement. This study explores theapplication of GLMs to ASAS, leveraging their off-the-shelf capabilities andperformance in various domains. We propose a novel pipeline that combinesvector databases, transformer-based encoders, and GLMs to enhance short answerscoring accuracy. Our approach stores training responses in a vector database,retrieves semantically similar responses during inference, and employs a GLM toanalyze these responses and determine appropriate scores. We further optimizethe system through fine-tuned retrieval processes and prompt engineering.Evaluation on the SemEval 2013 dataset demonstrates a significant improvementon the SCIENTSBANK 3-way and 2-way tasks compared to existing methods,highlighting the potential of GLMs in advancing ASAS technology.</description><author>Zifan Wang, Christopher Ormerod</author><pubDate>Wed, 07 Aug 2024 14:42:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03811v1</guid></item><item><title>Compression-Realized Deep Structural Network for Video Quality Enhancement</title><link>http://arxiv.org/abs/2405.06342v2</link><description>This paper focuses on the task of quality enhancement for compressed videos.Although deep network-based video restorers achieve impressive progress, mostof the existing methods lack a structured design to optimally leverage thepriors within compression codecs. Since the quality degradation of the video isprimarily induced by the compression algorithm, a new paradigm is urgentlyneeded for a more ``conscious'' process of quality enhancement. As a result, wepropose the Compression-Realized Deep Structural Network (CRDS), introducingthree inductive biases aligned with the three primary processes in the classiccompression codec, merging the strengths of classical encoder architecture withdeep network capabilities. Inspired by the residual extraction and domaintransformation process in the codec, a pre-trained Latent Degradation ResidualAuto-Encoder is proposed to transform video frames into a latent feature space,and the mutual neighborhood attention mechanism is integrated for precisemotion estimation and residual extraction. Furthermore, drawing inspirationfrom the quantization noise distribution of the codec, CRDS proposes a novelProgressive Denoising framework with intermediate supervision that decomposesthe quality enhancement into a series of simpler denoising sub-tasks.Experimental results on datasets like LDV 2.0 and MFQE 2.0 indicate ourapproach surpasses state-of-the-art models. Codes are available athttps://github.com/shc15522/CRDS.</description><author>Hanchi Sun, Xiaohong Liu, Xinyang Jiang, Yifei Shen, Dongsheng Li, Xiongkuo Min, Guangtao Zhai</author><pubDate>Wed, 07 Aug 2024 14:33:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06342v2</guid></item><item><title>Text-Region Matching for Multi-Label Image Recognition with Missing Labels</title><link>http://arxiv.org/abs/2407.18520v2</link><description>Recently, large-scale visual language pre-trained (VLP) models havedemonstrated impressive performance across various downstream tasks. Motivatedby these advancements, pioneering efforts have emerged in multi-label imagerecognition with missing labels, leveraging VLP prompt-tuning technology.However, they usually cannot match text and vision features well, due tocomplicated semantics gaps and missing labels in a multi-label image. To tacklethis challenge, we propose \textbf{T}ext-\textbf{R}egion \textbf{M}atching foroptimizing \textbf{M}ulti-\textbf{L}abel prompt tuning, namely TRM-ML, a novelmethod for enhancing meaningful cross-modal matching. Compared to existingmethods, we advocate exploring the information of category-aware regions ratherthan the entire image or pixels, which contributes to bridging the semantic gapbetween textual and visual representations in a one-to-one matching manner.Concurrently, we further introduce multimodal contrastive learning to narrowthe semantic gap between textual and visual modalities and establishintra-class and inter-class relationships. Additionally, to deal with missinglabels, we propose a multimodal category prototype that leverages intra- andinter-category semantic relationships to estimate unknown labels, facilitatingpseudo-label generation. Extensive experiments on the MS-COCO, PASCAL VOC,Visual Genome, NUS-WIDE, and CUB-200-211 benchmark datasets demonstrate thatour proposed framework outperforms the state-of-the-art methods by asignificant margin. Our code is availablehere\href{https://github.com/yu-gi-oh-leilei/TRM-ML}{\raisebox{-1pt}{\faGithub}}.</description><author>Leilei Ma, Hongxing Xie, Lei Wang, Yanping Fu, Dengdi Sun, Haifeng Zhao</author><pubDate>Wed, 07 Aug 2024 14:33:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18520v2</guid></item><item><title>Navigating the Human Maze: Real-Time Robot Pathfinding with Generative Imitation Learning</title><link>http://arxiv.org/abs/2408.03807v1</link><description>This paper addresses navigation in crowded environments by integratinggoal-conditioned generative models with Sampling-based Model Predictive Control(SMPC). We introduce goal-conditioned autoregressive models to generate crowdbehaviors, capturing intricate interactions among individuals. The modelprocesses potential robot trajectory samples and predicts the reactions ofsurrounding individuals, enabling proactive robotic navigation in complexscenarios. Extensive experiments show that this algorithm enables real-timenavigation, significantly reducing collision rates and path lengths, andoutperforming selected baseline methods. The practical effectiveness of thisalgorithm is validated on an actual robotic platform, demonstrating itscapability in dynamic settings.</description><author>Martin Moder, Stephen Adhisaputra, Josef Pauli</author><pubDate>Wed, 07 Aug 2024 14:32:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03807v1</guid></item><item><title>Trustworthy Image Semantic Communication with GenAI: Explainablity, Controllability, and Efficiency</title><link>http://arxiv.org/abs/2408.03806v1</link><description>Image semantic communication (ISC) has garnered significant attention for itspotential to achieve high efficiency in visual content transmission. However,existing ISC systems based on joint source-channel coding face challenges ininterpretability, operability, and compatibility. To address these limitations,we propose a novel trustworthy ISC framework. This approach leverages textextraction and segmentation mapping techniques to convert images intoexplainable semantics, while employing Generative Artificial Intelligence(GenAI) for multiple downstream inference tasks. We also introduce a multi-rateISC transmission protocol that dynamically adapts to both the receivedexplainable semantic content and specific task requirements at the receiver.Simulation results demonstrate that our framework achieves explainablelearning, decoupled training, and compatible transmission in variousapplication scenarios. Finally, some intriguing research directions andapplication scenarios are identified.</description><author>Xijun Wang, Dongshan Ye, Chenyuan Feng, Howard H. Yang, Xiang Chen, Tony Q. S. Quek</author><pubDate>Wed, 07 Aug 2024 14:32:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03806v1</guid></item><item><title>Frank's triangular norms in Piaget's logical proportions</title><link>http://arxiv.org/abs/2408.03795v1</link><description>Starting from the Boolean notion of logical proportion in Piaget's sense,which turns out to be equivalent to analogical proportion, this note proposes adefinition of analogical proportion between numerical values based ontriangular norms (and dual co-norms). Frank's family of triangular norms isparticularly interesting from this perspective. The article concludes with acomparative discussion with another very recent proposal for defininganalogical proportions between numerical values based on the family ofgeneralized means.</description><author>Henri Prade, Gilles Richard</author><pubDate>Wed, 07 Aug 2024 14:20:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03795v1</guid></item><item><title>Dynamic Language Group-Based MoE: Enhancing Code-Switching Speech Recognition with Hierarchical Routing</title><link>http://arxiv.org/abs/2407.18581v2</link><description>The Mixture of Experts (MoE) approach is well-suited for multilingual andcode-switching (CS) tasks due to its multi-expert architecture. This workintroduces the DLG-MoE, a Dynamic Language Group-based MoE optimized forbilingual and CS scenarios. DLG-MoE operates based on a hierarchical routingmechanism. First, the language router explicitly models the language anddispatches the representations to the corresponding language expert groups.Subsequently, the unsupervised router within each language group implicitlymodels attributes beyond language, and coordinates expert routing andcollaboration. The model achieves state-of-the-art (SOTA) performance whilealso having unparalleled flexibility. It supports different top-k inference andstreaming capabilities, and can also prune the model parameters to obtain amonolingual sub-model. The Code will be released.</description><author>Hukai Huang, Shenghui Lu, Yahui Shan, He Qu, Wenhao Guan, Qingyang Hong, Lin Li</author><pubDate>Wed, 07 Aug 2024 14:19:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18581v2</guid></item><item><title>Vision-Language Guidance for LiDAR-based Unsupervised 3D Object Detection</title><link>http://arxiv.org/abs/2408.03790v1</link><description>Accurate 3D object detection in LiDAR point clouds is crucial for autonomousdriving systems. To achieve state-of-the-art performance, the supervisedtraining of detectors requires large amounts of human-annotated data, which isexpensive to obtain and restricted to predefined object categories. To mitigatemanual labeling efforts, recent unsupervised object detection approachesgenerate class-agnostic pseudo-labels for moving objects, subsequently servingas supervision signal to bootstrap a detector. Despite promising results, theseapproaches do not provide class labels or generalize well to static objects.Furthermore, they are mostly restricted to data containing multiple drives fromthe same scene or images from a precisely calibrated and synchronized camerasetup. To overcome these limitations, we propose a vision-language-guidedunsupervised 3D detection approach that operates exclusively on LiDAR pointclouds. We transfer CLIP knowledge to classify point clusters of static andmoving objects, which we discover by exploiting the inherent spatio-temporalinformation of LiDAR point clouds for clustering, tracking, as well as box andlabel refinement. Our approach outperforms state-of-the-art unsupervised 3Dobject detectors on the Waymo Open Dataset ($+23~\text{AP}_{3D}$) and Argoverse2 ($+7.9~\text{AP}_{3D}$) and provides class labels not solely based on objectsize assumptions, marking a significant advancement in the field.</description><author>Christian Fruhwirth-Reisinger, Wei Lin, Dušan Malić, Horst Bischof, Horst Possegger</author><pubDate>Wed, 07 Aug 2024 14:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03790v1</guid></item><item><title>Counterfactuals and Uncertainty-Based Explainable Paradigm for the Automated Detection and Segmentation of Renal Cysts in Computed Tomography Images: A Multi-Center Study</title><link>http://arxiv.org/abs/2408.03789v1</link><description>Routine computed tomography (CT) scans often detect a wide range of renalcysts, some of which may be malignant. Early and precise localization of thesecysts can significantly aid quantitative image analysis. Current segmentationmethods, however, do not offer sufficient interpretability at the feature andpixel levels, emphasizing the necessity for an explainable framework that candetect and rectify model inaccuracies. We developed an interpretablesegmentation framework and validated it on a multi-centric dataset. AVariational Autoencoder Generative Adversarial Network (VAE-GAN) was employedto learn the latent representation of 3D input patches and reconstruct inputimages. Modifications in the latent representation using the gradient of thesegmentation model generated counterfactual explanations for varying dicesimilarity coefficients (DSC). Radiomics features extracted from thesecounterfactual images, using a ground truth cyst mask, were analyzed todetermine their correlation with segmentation performance. The DSCs for theoriginal and VAE-GAN reconstructed images for counterfactual image generationshowed no significant differences. Counterfactual explanations highlighted howvariations in cyst image features influence segmentation outcomes and showedmodel discrepancies. Radiomics features correlating positively and negativelywith dice scores were identified. The uncertainty of the predicted segmentationmasks was estimated using posterior sampling of the weight space. Thecombination of counterfactual explanations and uncertainty maps provided adeeper understanding of the image features within the segmented renal cyststhat lead to high uncertainty. The proposed segmentation framework not onlyachieved high segmentation accuracy but also increased interpretabilityregarding how image features impact segmentation performance.</description><author>Zohaib Salahuddin, Abdalla Ibrahim, Sheng Kuang, Yousif Widaatalla, Razvan L. Miclea, Oliver Morin, Spencer Behr, Marnix P. M. Kop, Tom Marcelissen, Patricia Zondervan, Auke Jager, Philippe Lambin, Henry C Woodruff</author><pubDate>Wed, 07 Aug 2024 14:14:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03789v1</guid></item><item><title>Diffusion-based Human Motion Style Transfer with Semantic Guidance</title><link>http://arxiv.org/abs/2405.06646v2</link><description>3D Human motion style transfer is a fundamental problem in computer graphicand animation processing. Existing AdaIN- based methods necessitate datasetswith balanced style distribution and content/style labels to train theclustered latent space. However, we may encounter a single unseen style examplein practical scenarios, but not in sufficient quantity to constitute a stylecluster for AdaIN-based methods. Therefore, in this paper, we propose a noveltwo-stage framework for few-shot style transfer learning based on the diffusionmodel. Specifically, in the first stage, we pre-train a diffusion-basedtext-to-motion model as a generative prior so that it can cope with variouscontent motion inputs. In the second stage, based on the single style example,we fine-tune the pre-trained diffusion model in a few-shot manner to make itcapable of style transfer. The key idea is regarding the reverse process ofdiffusion as a motion-style translation process since the motion styles can beviewed as special motion variations. During the fine-tuning for style transfer,a simple yet effective semantic-guided style transfer loss coordinated withstyle example reconstruction loss is introduced to supervise the style transferin CLIP semantic space. The qualitative and quantitative evaluationsdemonstrate that our method can achieve state-of-the-art performance and haspractical applications.</description><author>Lei Hu, Zihao Zhang, Yongjing Ye, Yiwen Xu, Shihong Xia</author><pubDate>Wed, 07 Aug 2024 14:06:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06646v2</guid></item><item><title>A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)</title><link>http://arxiv.org/abs/2407.11075v2</link><description>Through this comprehensive survey of Kolmogorov-Arnold Networks(KAN), we havegained a thorough understanding of its theoretical foundation, architecturaldesign, application scenarios, and current research progress. KAN, with itsunique architecture and flexible activation functions, excels in handlingcomplex data patterns and nonlinear relationships, demonstrating wide-rangingapplication potential. While challenges remain, KAN is poised to pave the wayfor innovative solutions in various fields, potentially revolutionizing how weapproach complex computational problems.</description><author>Yuntian Hou, Di Zhang</author><pubDate>Wed, 07 Aug 2024 14:05:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11075v2</guid></item><item><title>Multi-times Monte Carlo Rendering for Inter-reflection Reconstruction</title><link>http://arxiv.org/abs/2407.05771v2</link><description>Inverse rendering methods have achieved remarkable performance inreconstructing high-fidelity 3D objects with disentangled geometries,materials, and environmental light. However, they still face huge challenges inreflective surface reconstruction. Although recent methods model the lighttrace to learn specularity, the ignorance of indirect illumination makes ithard to handle inter-reflections among multiple smooth objects. In this work,we propose Ref-MC2 that introduces the multi-time Monte Carlo sampling whichcomprehensively computes the environmental illumination and meanwhile considersthe reflective light from object surfaces. To address the computation challengeas the times of Monte Carlo sampling grow, we propose a specularity-adaptivesampling strategy, significantly reducing the computational complexity. Besidesthe computational resource, higher geometry accuracy is also required becausegeometric errors accumulate multiple times. Therefore, we further introduce areflection-aware surface model to initialize the geometry and refine it duringinverse rendering. We construct a challenging dataset containing scenes withmultiple objects and inter-reflections. Experiments show that our methodoutperforms other inverse rendering methods on various object groups. We alsoshow downstream applications, e.g., relighting and material editing, toillustrate the disentanglement ability of our method.</description><author>Tengjie Zhu, Zhuo Chen, Jingnan Gao, Yichao Yan, Xiaokang Yang</author><pubDate>Wed, 07 Aug 2024 14:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05771v2</guid></item><item><title>CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning</title><link>http://arxiv.org/abs/2407.15793v2</link><description>With the emergence of Transformers and Vision-Language Models (VLMs) such asCLIP, large pre-trained models have become a common strategy to enhanceperformance in Continual Learning scenarios. This led to the development ofnumerous prompting strategies to effectively fine-tune transformer-based modelswithout succumbing to catastrophic forgetting. However, these methods struggleto specialize the model on domains significantly deviating from thepre-training and preserving its zero-shot capabilities. In this work, wepropose Continual Generative training for Incremental prompt-Learning, a novelapproach to mitigate forgetting while adapting a VLM, which exploits generativereplay to align prompts to tasks. We also introduce a new metric to evaluatezero-shot capabilities within CL benchmarks. Through extensive experiments ondifferent domains, we demonstrate the effectiveness of our framework inadapting to new tasks while improving zero-shot capabilities. Further analysisreveals that our approach can bridge the gap with joint prompt tuning. Thecodebase is available at https://github.com/aimagelab/mammoth.</description><author>Emanuele Frascaroli, Aniello Panariello, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara</author><pubDate>Wed, 07 Aug 2024 13:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15793v2</guid></item><item><title>Relevance meets Diversity: A User-Centric Framework for Knowledge Exploration through Recommendations</title><link>http://arxiv.org/abs/2408.03772v1</link><description>Providing recommendations that are both relevant and diverse is a keyconsideration of modern recommender systems. Optimizing both of these measurespresents a fundamental trade-off, as higher diversity typically comes at thecost of relevance, resulting in lower user engagement. Existing recommendationalgorithms try to resolve this trade-off by combining the two measures,relevance and diversity, into one aim and then seeking recommendations thatoptimize the combined objective, for a given number of items to recommend.Traditional approaches, however, do not consider the user interaction with therecommended items. In this paper, we put the user at the central stage, and build on theinterplay between relevance, diversity, and user behavior. In contrast toapplications where the goal is solely to maximize engagement, we focus onscenarios aiming at maximizing the total amount of knowledge encountered by theuser. We use diversity as a surrogate of the amount of knowledge obtained bythe user while interacting with the system, and we seek to maximize diversity.We propose a probabilistic user-behavior model in which users keep interactingwith the recommender system as long as they receive relevant recommendations,but they may stop if the relevance of the recommended items drops. Thus, for arecommender system to achieve a high-diversity measure, it will need to producerecommendations that are both relevant and diverse. Finally, we propose a novel recommendation strategy that combines relevanceand diversity by a copula function. We conduct an extensive evaluation of theproposed methodology over multiple datasets, and we show that our strategyoutperforms several state-of-the-art competitors. Our implementation ispublicly available at https://github.com/EricaCoppolillo/EXPLORE.</description><author>Erica Coppolillo, Giuseppe Manco, Aristides Gionis</author><pubDate>Wed, 07 Aug 2024 13:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03772v1</guid></item><item><title>Methodological Explainability Evaluation of an Interpretable Deep Learning Model for Post-Hepatectomy Liver Failure Prediction Incorporating Counterfactual Explanations and Layerwise Relevance Propagation: A Prospective In Silico Trial</title><link>http://arxiv.org/abs/2408.03771v1</link><description>Artificial intelligence (AI)-based decision support systems have demonstratedvalue in predicting post-hepatectomy liver failure (PHLF) in hepatocellularcarcinoma (HCC). However, they often lack transparency, and the impact of modelexplanations on clinicians' decisions has not been thoroughly evaluated.Building on prior research, we developed a variational autoencoder-multilayerperceptron (VAE-MLP) model for preoperative PHLF prediction. This modelintegrated counterfactuals and layerwise relevance propagation (LRP) to provideinsights into its decision-making mechanism. Additionally, we proposed amethodological framework for evaluating the explainability of AI systems. Thisframework includes qualitative and quantitative assessments of explanationsagainst recognized biomarkers, usability evaluations, and an in silico clinicaltrial. Our evaluations demonstrated that the model's explanation correlatedwith established biomarkers and exhibited high usability at both the case andsystem levels. Furthermore, results from the three-track in silico clinicaltrial showed that clinicians' prediction accuracy and confidence increased whenAI explanations were provided.</description><author>Xian Zhong, Zohaib Salahuddin, Yi Chen, Henry C Woodruff, Haiyi Long, Jianyun Peng, Nuwan Udawatte, Roberto Casale, Ayoub Mokhtari, Xiaoer Zhang, Jiayao Huang, Qingyu Wu, Li Tan, Lili Chen, Dongming Li, Xiaoyan Xie, Manxia Lin, Philippe Lambin</author><pubDate>Wed, 07 Aug 2024 13:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03771v1</guid></item><item><title>RCA: Region Conditioned Adaptation for Visual Abductive Reasoning</title><link>http://arxiv.org/abs/2303.10428v5</link><description>Visual abductive reasoning aims to make likely explanations for visualobservations. We propose a simple yet effective Region Conditioned Adaptation,a hybrid parameter-efficient fine-tuning method that equips the frozen CLIPwith the ability to infer explanations from local visual cues. We encode``local hints'' and ``global contexts'' into visual prompts of the CLIP modelseparately at fine and coarse-grained levels. Adapters are used for fine-tuningCLIP models for downstream tasks and we design a new attention adapter, thatdirectly steers the focus of the attention map with trainable query and keyprojections of a frozen CLIP model. Finally, we train our new model with amodified contrastive loss to regress the visual feature simultaneously towardfeatures of literal description and plausible explanations. The loss enablesCLIP to maintain both perception and reasoning abilities. Experiments on theSherlock visual abductive reasoning benchmark show that the RCA significantlyoutstands previous SOTAs, ranking the \nth{1} on the leaderboards (e.g., HumanAcc: RCA 31.74 \textit{vs} CPT-CLIP 29.58, higher =better). We also validatethe RCA is generalizable to local perception benchmarks like RefCOCO. Weopen-source our project at\textit{\color{magenta}{\url{https://github.com/LUNAProject22/RPA}}}.</description><author>Hao Zhang, Yeo Keat Ee, Basura Fernando</author><pubDate>Wed, 07 Aug 2024 13:44:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10428v5</guid></item><item><title>Sampling for Model Predictive Trajectory Planning in Autonomous Driving using Normalizing Flows</title><link>http://arxiv.org/abs/2404.09657v3</link><description>Alongside optimization-based planners, sampling-based approaches are oftenused in trajectory planning for autonomous driving due to their simplicity.Model predictive path integral control is a framework that builds uponoptimization principles while incorporating stochastic sampling of inputtrajectories. This paper investigates several sampling approaches fortrajectory generation. In this context, normalizing flows originating from thefield of variational inference are considered for the generation of samplingdistributions, as they model transformations of simple to more complexdistributions. Accordingly, learning-based normalizing flow models are trainedfor a more efficient exploration of the input domain for the task at hand. Thedeveloped algorithm and the proposed sampling distributions are evaluated intwo simulation scenarios.</description><author>Georg Rabenstein, Lars Ullrich, Knut Graichen</author><pubDate>Wed, 07 Aug 2024 13:44:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09657v3</guid></item><item><title>CA-LoRA: Adapting Existing LoRA for Compressed LLMs to Enable Efficient Multi-Tasking on Personal Devices</title><link>http://arxiv.org/abs/2307.07705v3</link><description>Recently, there has been a demand to deploy Large Language Models (LLMs) onpersonal devices such as laptops and smartphones. These LLMs have differentmodel variants when handling different tasks. However, personal devices havelimited resources and require reduced storage overhead. To address this, thereare two key methods available: the first is model compression, which compressesLLMs into smaller sizes; the second is LoRA, which can transfer an LLM to othertasks with very few parameters, avoiding the storage of multiple model variantsin multi-task scenarios by only preserving LoRAs. However, our experiments showthat directly combining these two methods yields sub-optimal performance.Considering that the open-source community has already contributed many LoRAsto LLMs, we propose to adapt these existing LoRAs from the LLMs to theircompressed version and introduce a Compression-Aware LoRA (CA-LoRA) framework.We incorporate knowledge inheritance and recovery strategies to recover thelost knowledge caused by model compression. Experiment results demonstrate thatCA-LoRA outperforms the vanilla LoRA methods applied to a compressed LLM andachieves comparable performance to the non-compressed LLM with existing LoRAmodules. The source code of CA-LoRA is available athttps://github.com/thunlp/CA-LoRA.</description><author>Weilin Zhao, Yuxiang Huang, Xu Han, Zhiyuan Liu, Zhengyan Zhang, Kuai Li, Chen Chen, Tao Yang, Maosong Sun</author><pubDate>Wed, 07 Aug 2024 13:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07705v3</guid></item><item><title>Nadaraya-Watson kernel smoothing as a random energy model</title><link>http://arxiv.org/abs/2408.03769v1</link><description>We investigate the behavior of the Nadaraya-Watson kernel smoothing estimatorin high dimensions using its relationship to the random energy model and todense associative memories.</description><author>Jacob A. Zavatone-Veth, Cengiz Pehlevan</author><pubDate>Wed, 07 Aug 2024 13:43:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03769v1</guid></item><item><title>TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities</title><link>http://arxiv.org/abs/2407.21693v2</link><description>Task-oriented dialogue (TOD) systems aim to efficiently handle task-orientedconversations, including information collection. How to utilize TOD accurately,efficiently and effectively for information collection has always been acritical and challenging task. Recent studies have demonstrated that LargeLanguage Models (LLMs) excel in dialogue, instruction generation, andreasoning, and can significantly enhance the performance of TOD throughfine-tuning. However, current datasets primarily cater to user-led systems andare limited to predefined specific scenarios and slots, thereby necessitatingimprovements in the proactiveness, diversity, and capabilities of TOD. In thisstudy, we present a detailed multi-domain task-oriented data constructionprocess for conversations, and a Chinese dialogue dataset generated based onthis process, TransferTOD, which authentically simulates human-computerdialogues in 30 popular life service scenarios. Leveraging this dataset, wetrained a model called TransferTOD-7B using full-parameter fine-tuning,showcasing notable abilities in slot filling and questioning. Our work hasdemonstrated its strong generalization capabilities in various downstreamscenarios, significantly enhancing both data utilization efficiency and systemperformance. The data is released inhttps://github.com/KongLongGeFDU/TransferTOD.</description><author>Ming Zhang, Caishuang Huang, Yilong Wu, Shichun Liu, Huiyuan Zheng, Yurui Dong, Yujiong Shen, Shihan Dou, Jun Zhao, Junjie Ye, Qi Zhang, Tao Gui, Xuanjing Huang</author><pubDate>Wed, 07 Aug 2024 13:42:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21693v2</guid></item><item><title>Investigating and Defending Shortcut Learning in Personalized Diffusion Models</title><link>http://arxiv.org/abs/2406.18944v3</link><description>Personalized diffusion models have gained popularity for adapting pre-trainedtext-to-image models to generate images of specific topics with minimaltraining data. However, these models are vulnerable to minor adversarialperturbations, leading to degraded performance on corrupted datasets. Suchvulnerabilities are further exploited to craft protective perturbations onsensitive images like portraits that prevent unauthorized generation. Inresponse, diffusion-based purification methods have been proposed to removethese perturbations and retain generation performance. However, existing worksturn to over-purifying the images, which causes information loss. In thispaper, we take a closer look at the fine-tuning process of personalizeddiffusion models through the lens of shortcut learning. And we propose ahypothesis explaining the manipulation mechanisms of existing perturbationmethods, demonstrating that perturbed images significantly deviate from theiroriginal prompts in the CLIP-based latent space. This misalignment duringfine-tuning causes models to associate noisy patterns with identifiers,resulting in performance degradation. Based on these insights, we introduce asystematic approach to maintain training performance through purification. Ourmethod first purifies the images to realign them with their original semanticmeanings in latent space. Then, we introduce contrastive learning with negativetokens to decouple the learning of clean identities from noisy patterns, whichshows a strong potential capacity against adaptive perturbation. Our studyuncovers shortcut learning vulnerabilities in personalized diffusion models andprovides a firm evaluation framework for future protective perturbationresearch. Code is available at https://github.com/liuyixin-louis/DiffShortcut.</description><author>Yixin Liu, Ruoxi Chen, Lichao Sun</author><pubDate>Wed, 07 Aug 2024 13:37:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18944v3</guid></item><item><title>Reliable Node Similarity Matrix Guided Contrastive Graph Clustering</title><link>http://arxiv.org/abs/2408.03765v1</link><description>Graph clustering, which involves the partitioning of nodes within a graphinto disjoint clusters, holds significant importance for numerous subsequentapplications. Recently, contrastive learning, known for utilizing supervisoryinformation, has demonstrated encouraging results in deep graph clustering.This methodology facilitates the learning of favorable node representations forclustering by attracting positively correlated node pairs and distancingnegatively correlated pairs within the representation space. Nevertheless, asignificant limitation of existing methods is their inadequacy in thoroughlyexploring node-wise similarity. For instance, some hypothesize that the nodesimilarity matrix within the representation space is identical, ignoring theinherent semantic relationships among nodes. Given the fundamental role ofinstance similarity in clustering, our research investigates contrastive graphclustering from the perspective of the node similarity matrix. We argue that anideal node similarity matrix within the representation space should accuratelyreflect the inherent semantic relationships among nodes, ensuring thepreservation of semantic similarities in the learned representations. Inresponse to this, we introduce a new framework, Reliable Node Similarity MatrixGuided Contrastive Graph Clustering (NS4GC), which estimates an approximatelyideal node similarity matrix within the representation space to guiderepresentation learning. Our method introduces node-neighbor alignment andsemantic-aware sparsification, ensuring the node similarity matrix is bothaccurate and efficiently sparse. Comprehensive experiments conducted on $8$real-world datasets affirm the efficacy of learning the node similarity matrixand the superior performance of NS4GC.</description><author>Yunhui Liu, Xinyi Gao, Tieke He, Tao Zheng, Jianhua Zhao, Hongzhi Yin</author><pubDate>Wed, 07 Aug 2024 13:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03765v1</guid></item><item><title>'Finance Wizard' at the FinLLM Challenge Task: Financial Text Summarization</title><link>http://arxiv.org/abs/2408.03762v1</link><description>This paper presents our participation under the team name `Finance Wizard' inthe FinNLP-AgentScen 2024 shared task #2: Financial Text Summarization. Itdocuments our pipeline approach of fine-tuning a foundation model into atask-specific model for Financial Text Summarization. It involves (1) adaptingLlama3 8B, a foundation model, to the Finance domain via continuedpre-training, (2) multi-task instruction-tuning to further equip the model withmore finance-related capabilities, (3) finally fine-tuning the model into atask-specific `expert'. Our model, FinLlama3\_sum, yielded commendable results,securing the third position in its category with a ROUGE-1 score of 0.521.</description><author>Meisin Lee, Soon Lay-Ki</author><pubDate>Wed, 07 Aug 2024 13:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03762v1</guid></item><item><title>MMSummary: Multimodal Summary Generation for Fetal Ultrasound Video</title><link>http://arxiv.org/abs/2408.03761v1</link><description>We present the first automated multimodal summary generation system,MMSummary, for medical imaging video, particularly with a focus on fetalultrasound analysis. Imitating the examination process performed by a humansonographer, MMSummary is designed as a three-stage pipeline, progressing fromkeyframe detection to keyframe captioning and finally anatomy segmentation andmeasurement. In the keyframe detection stage, an innovative automated workflowis proposed to progressively select a concise set of keyframes, preservingsufficient video information without redundancy. Subsequently, we adapt a largelanguage model to generate meaningful captions for fetal ultrasound keyframesin the keyframe captioning stage. If a keyframe is captioned as fetal biometry,the segmentation and measurement stage estimates biometric parameters bysegmenting the region of interest according to the textual prior. The MMSummarysystem provides comprehensive summaries for fetal ultrasound examinations andbased on reported experiments is estimated to reduce scanning time byapproximately 31.5%, thereby suggesting the potential to enhance clinicalworkflow efficiency.</description><author>Xiaoqing Guo, Qianhui Men, J. Alison Noble</author><pubDate>Wed, 07 Aug 2024 13:30:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03761v1</guid></item><item><title>Improved Monte Carlo tree search formulation with multiple root nodes for discrete sizing optimization of truss structures</title><link>http://arxiv.org/abs/2309.06045v4</link><description>This paper proposes a novel reinforcement learning (RL) algorithm usingimproved Monte Carlo tree search (IMCTS) formulation for discrete optimumdesign of truss structures. IMCTS with multiple root nodes includes updateprocess, the best reward, accelerating technique, and terminal condition.Update process means that once a final solution is found, it is used as theinitial solution for next search tree. The best reward is used in thebackpropagation step. Accelerating technique is introduced by decreasing thewidth of search tree and reducing maximum number of iterations. The agent istrained to minimize the total structural weight under various constraints untilthe terminal condition is satisfied. Then, optimal solution is the minimumvalue of all solutions found by search trees. These numerical examples showthat the agent can find optimal solution with low computational cost, stablyproduces an optimal design, and is suitable for multi-objective structuraloptimization and large-scale structures.</description><author>Fu-Yao Ko, Katsuyuki Suzuki, Kazuo Yonekura</author><pubDate>Wed, 07 Aug 2024 13:29:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06045v4</guid></item><item><title>EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning</title><link>http://arxiv.org/abs/2408.01953v2</link><description>Humans perceive and interact with the world with the awareness ofequivariance, facilitating us in manipulating different objects in diverseposes. For robotic manipulation, such equivariance also exists in manyscenarios. For example, no matter what the pose of a drawer is (translation,rotation and tilt), the manipulation strategy is consistent (grasp the handleand pull in a line). While traditional models usually do not have the awarenessof equivariance for robotic manipulation, which might result in more data fortraining and poor performance in novel object poses, we propose our EqvAffordframework, with novel designs to guarantee the equivariance in point-levelaffordance learning for downstream robotic manipulation, with great performanceand generalization ability on representative tasks on objects in diverse poses.</description><author>Yue Chen, Chenrui Tie, Ruihai Wu, Hao Dong</author><pubDate>Wed, 07 Aug 2024 13:24:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01953v2</guid></item><item><title>Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives and Negatives</title><link>http://arxiv.org/abs/2404.11317v2</link><description>The Composed Image Retrieval (CIR) task aims to retrieve target images usinga composed query consisting of a reference image and a modified text. Advancedmethods often utilize contrastive learning as the optimization objective, whichbenefits from adequate positive and negative examples. However, the triplet forCIR incurs high manual annotation costs, resulting in limited positiveexamples. Furthermore, existing methods commonly use in-batch negativesampling, which reduces the negative number available for the model. To addressthe problem of lack of positives, we propose a data generation method byleveraging a multi-modal large language model to construct triplets for CIR. Tointroduce more negatives during fine-tuning, we design a two-stage fine-tuningframework for CIR, whose second stage introduces plenty of staticrepresentations of negatives to optimize the representation space rapidly. Theabove two improvements can be effectively stacked and designed to beplug-and-play, easily applied to existing CIR models without changing theiroriginal architectures. Extensive experiments and ablation analysis demonstratethat our method effectively scales positives and negatives and achievesstate-of-the-art results on both FashionIQ and CIRR datasets. In addition, ourmethod also performs well in zero-shot composed image retrieval, providing anew CIR solution for the low-resources scenario. Our code and data are releasedat https://github.com/BUAADreamer/SPN4CIR.</description><author>Zhangchi Feng, Richong Zhang, Zhijie Nie</author><pubDate>Wed, 07 Aug 2024 13:20:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11317v2</guid></item><item><title>BAST: Binaural Audio Spectrogram Transformer for Binaural Sound Localization</title><link>http://arxiv.org/abs/2207.03927v2</link><description>Accurate sound localization in a reverberation environment is essential forhuman auditory perception. Recently, Convolutional Neural Networks (CNNs) havebeen utilized to model the binaural human auditory pathway. However, CNN showsbarriers in capturing the global acoustic features. To address this issue, wepropose a novel end-to-end Binaural Audio Spectrogram Transformer (BAST) modelto predict the sound azimuth in both anechoic and reverberation environments.Two modes of implementation, i.e. BAST-SP and BAST-NSP corresponding to BASTmodel with shared and non-shared parameters respectively, are explored. Ourmodel with subtraction interaural integration and hybrid loss achieves anangular distance of 1.29 degrees and a Mean Square Error of 1e-3 at allazimuths, significantly surpassing CNN based model. The exploratory analysis ofthe BAST's performance on the left-right hemifields and anechoic andreverberation environments shows its generalization ability as well as thefeasibility of binaural Transformers in sound localization. Furthermore, theanalysis of the attention maps is provided to give additional insights on theinterpretation of the localization process in a natural reverberantenvironment.</description><author>Sheng Kuang, Jie Shi, Kiki van der Heijden, Siamak Mehrkanoon</author><pubDate>Wed, 07 Aug 2024 13:15:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.03927v2</guid></item><item><title>Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation</title><link>http://arxiv.org/abs/2402.07118v2</link><description>Blindness and other eye diseases are a global health concern, particularly inlow- and middle-income countries like India. In this regard, during theCOVID-19 pandemic, teleophthalmology became a lifeline, and the Grabiattachment for smartphone-based eye imaging gained in use. However, quality ofuser-captured image often remained inadequate, requiring clinician vetting anddelays. In this backdrop, we propose an AI-based quality assessment system withinstant feedback mimicking clinicians' judgments and tested on patient-capturedimages. Dividing the complex problem hierarchically, here we tackle anontrivial part, and demonstrate a proof of the concept.</description><author>Dhruv Srikanth, Jayang Gurung, N Satya Deepika, Vineet Joshi, Lopamudra Giri, Pravin Vaddavalli, Soumya Jana</author><pubDate>Wed, 07 Aug 2024 13:14:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07118v2</guid></item><item><title>Automatic Classification of Subjective Time Perception Using Multi-modal Physiological Data of Air Traffic Controllers</title><link>http://arxiv.org/abs/2404.15213v2</link><description>In high-pressure environments where human individuals must simultaneouslymonitor multiple entities, communicate effectively, and maintain intense focus,the perception of time becomes a critical factor influencing performance andwell-being. One indicator of well-being can be the person's subjective timeperception. In our project $ChronoPilot$, we aim to develop a device thatmodulates human subjective time perception. In this study, we present a methodto automatically assess the subjective time perception of air trafficcontrollers, a group often faced with demanding conditions, using theirphysiological data and eleven state-of-the-art machine learning classifiers.The physiological data consist of photoplethysmogram, electrodermal activity,and temperature data. We find that the support vector classifier works bestwith an accuracy of 79 % and electrodermal activity provides the mostdescriptive biomarker. These findings are an important step towards closing thefeedback loop of our $ChronoPilot$-device to automatically modulate the user'ssubjective time perception. This technological advancement may promiseimprovements in task management, stress reduction, and overall productivity inhigh-stakes professions.</description><author>Till Aust, Eirini Balta, Argiro Vatakis, Heiko Hamann</author><pubDate>Wed, 07 Aug 2024 13:11:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15213v2</guid></item><item><title>3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2408.03753v1</link><description>The use of 3D Gaussians as representation of radiance fields has enabled highquality novel view synthesis at real-time rendering speed. However, the choiceof optimising the outgoing radiance of each Gaussian independently as sphericalharmonics results in unsatisfactory view dependent effects. In response tothese limitations, our work, Factorised Tensorial Illumination for 3D GaussianSplatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) renderingquality. Instead of optimising a single outgoing radiance parameter, 3iGSenhances 3DGS view-dependent effects by expressing the outgoing radiance as afunction of a local illumination field and Bidirectional ReflectanceDistribution Function (BRDF) features. We optimise a continuous incidentillumination field through a Tensorial Factorisation representation, whileseparately fine-tuning the BRDF features of each 3D Gaussian relative to thisillumination field. Our methodology significantly enhances the renderingquality of specular view-dependent effects of 3DGS, while maintaining rapidtraining and rendering speeds.</description><author>Zhe Jun Tang, Tat-Jen Cham</author><pubDate>Wed, 07 Aug 2024 13:06:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03753v1</guid></item><item><title>Data Generation Scheme for Thermal Modality with Edge-Guided Adversarial Conditional Diffusion Model</title><link>http://arxiv.org/abs/2408.03748v1</link><description>In challenging low light and adverse weather conditions,thermal visionalgorithms,especially object detection,have exhibited remarkablepotential,contrasting with the frequent struggles encountered by visible visionalgorithms. Nevertheless,the efficacy of thermal vision algorithms driven bydeep learning models remains constrained by the paucity of available trainingdata samples. To this end,this paper introduces a novel approach termed theedge guided conditional diffusion model. This framework aims to producemeticulously aligned pseudo thermal images at the pixel level,leveraging edgeinformation extracted from visible images. By utilizing edges as contextualcues from the visible domain,the diffusion model achieves meticulous controlover the delineation of objects within the generated images. To alleviate theimpacts of those visible-specific edge information that should not appear inthe thermal domain,a two-stage modality adversarial training strategy isproposed to filter them out from the generated images by differentiating thevisible and thermal modality. Extensive experiments on LLVIP demonstrate ECDM ssuperiority over existing state-of-the-art approaches in terms of imagegeneration quality.</description><author>Guoqing Zhu, Honghu Pan, Qiang Wang, Chao Tian, Chao Yang, Zhenyu He</author><pubDate>Wed, 07 Aug 2024 13:01:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03748v1</guid></item><item><title>Online Model-based Anomaly Detection in Multivariate Time Series: Taxonomy, Survey, Research Challenges and Future Directions</title><link>http://arxiv.org/abs/2408.03747v1</link><description>Time-series anomaly detection plays an important role in engineeringprocesses, like development, manufacturing and other operations involvingdynamic systems. These processes can greatly benefit from advances in thefield, as state-of-the-art approaches may aid in cases involving, for example,highly dimensional data. To provide the reader with understanding of theterminology, this survey introduces a novel taxonomy where a distinctionbetween online and offline, and training and inference is made. Additionally,it presents the most popular data sets and evaluation metrics used in theliterature, as well as a detailed analysis. Furthermore, this survey providesan extensive overview of the state-of-the-art model-based online semi- andunsupervised anomaly detection approaches for multivariate time-series data,categorising them into different model families and other properties. Thebiggest research challenge revolves around benchmarking, as currently there isno reliable way to compare different approaches against one another. Thisproblem is two-fold: on the one hand, public data sets suffers from at leastone fundamental flaw, while on the other hand, there is a lack of intuitive andrepresentative evaluation metrics in the field. Moreover, the way mostpublications choose a detection threshold disregards real-world conditions,which hinders the application in the real world. To allow for tangible advancesin the field, these issues must be addressed in future work.</description><author>Lucas Correia, Jan-Christoph Goos, Philipp Klein, Thomas Bäck, Anna V. Kononova</author><pubDate>Wed, 07 Aug 2024 13:01:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03747v1</guid></item><item><title>Flexible Bayesian Last Layer Models Using Implicit Priors and Diffusion Posterior Sampling</title><link>http://arxiv.org/abs/2408.03746v1</link><description>Bayesian Last Layer (BLL) models focus solely on uncertainty in the outputlayer of neural networks, demonstrating comparable performance to more complexBayesian models. However, the use of Gaussian priors for last layer weights inBayesian Last Layer (BLL) models limits their expressive capacity when facedwith non-Gaussian, outlier-rich, or high-dimensional datasets. To address thisshortfall, we introduce a novel approach that combines diffusion techniques andimplicit priors for variational learning of Bayesian last layer weights. Thismethod leverages implicit distributions for modeling weight priors in BLL,coupled with diffusion samplers for approximating true posterior predictions,thereby establishing a comprehensive Bayesian prior and posterior estimationstrategy. By delivering an explicit and computationally efficient variationallower bound, our method aims to augment the expressive abilities of BLL models,enhancing model accuracy, calibration, and out-of-distribution detectionproficiency. Through detailed exploration and experimental validation, Weshowcase the method's potential for improving predictive accuracy anduncertainty quantification while ensuring computational efficiency.</description><author>Jian Xu, Zhiqi Lin, Shigui Li, Min Chen, Junmei Yang, Delu Zeng, John Paisley</author><pubDate>Wed, 07 Aug 2024 12:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03746v1</guid></item><item><title>Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient Dataset Distillation</title><link>http://arxiv.org/abs/2305.18381v4</link><description>Data-efficient learning has garnered significant attention, especially giventhe current trend of large multi-modal models. Recently, dataset distillationhas become an effective approach by synthesizing data samples that areessential for network training. However, it remains to be explored whichsamples are essential for the dataset distillation process itself. In thiswork, we study the data efficiency and selection for the dataset distillationtask. By re-formulating the dynamics of distillation, we provide insight intothe inherent redundancy in the real dataset, both theoretically andempirically. We propose to use the empirical loss value as a static datapruning criterion. To further compensate for the variation of the data value intraining, we find the most contributing samples based on their causal effectson the distillation. The proposed selection strategy can efficiently exploitthe training dataset, outperform the previous SOTA distillation algorithms, andconsistently enhance the distillation algorithms, even on much larger-scale andmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. Webelieve this paradigm will open up new avenues in the dynamics of distillationand pave the way for efficient dataset distillation. Our code is available onhttps://github.com/silicx/GoldFromOres-BiLP.</description><author>Yue Xu, Yong-Lu Li, Kaitong Cui, Ziyu Wang, Cewu Lu, Yu-Wing Tai, Chi-Keung Tang</author><pubDate>Wed, 07 Aug 2024 12:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18381v4</guid></item><item><title>Intuitionistic Fuzzy Cognitive Maps for Interpretable Image Classification</title><link>http://arxiv.org/abs/2408.03745v1</link><description>The interpretability of machine learning models is critical, as users may bereluctant to rely on their inferences. Intuitionistic FCMs (iFCMs) have beenproposed as an extension of FCMs offering a natural mechanism to assess thequality of their output through the estimation of hesitancy, a conceptresembling to human hesitation in decision making. To address the challenge ofinterpretable image classification, this paper introduces a novel framework,named Interpretable Intuitionistic FCM (I2FCM) which is domain-independent,simple to implement, and can be applied on Convolutional Neural Network (CNN)models, rendering them interpretable. To the best of our knowledge this is thefirst time iFCMs are applied for image classification. Further novelcontributions include: a feature extraction process focusing on the mostinformative image regions; a learning algorithm for data-driven determinationof the intuitionistic fuzzy interconnections of the iFCM; an inherentlyinterpretable classification approach based on image contents. In the contextof image classification, hesitancy is considered as a degree of inconfidencewith which an image is categorized to a class. The constructed iFCM modeldistinguishes the most representative image semantics and analyses themutilizing cause-and-effect relations. The effectiveness of the introducedframework is evaluated on publicly available datasets, and the experimentalresults confirm that it can provide enhanced classification performance, whileproviding interpretable inferences.</description><author>Georgia Sovatzidi, Michael D. Vasilakakis, Dimitris K. Iakovidis</author><pubDate>Wed, 07 Aug 2024 12:58:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03745v1</guid></item><item><title>Nonparametric Linear Feature Learning in Regression Through Regularisation</title><link>http://arxiv.org/abs/2307.12754v4</link><description>Representation learning plays a crucial role in automated feature selection,particularly in the context of high-dimensional data, where non-parametricmethods often struggle. In this study, we focus on supervised learningscenarios where the pertinent information resides within a lower-dimensionallinear subspace of the data, namely the multi-index model. If this subspacewere known, it would greatly enhance prediction, computation, andinterpretation. To address this challenge, we propose a novel method for jointlinear feature learning and non-parametric function estimation, aimed at moreeffectively leveraging hidden features for learning. Our approach employsempirical risk minimisation, augmented with a penalty on function derivatives,ensuring versatility. Leveraging the orthogonality and rotation invarianceproperties of Hermite polynomials, we introduce our estimator, named RegFeaL.By using alternative minimisation, we iteratively rotate the data to improvealignment with leading directions. We establish that the expected risk of ourmethod converges in high-probability to the minimal risk under minimalassumptions and with explicit rates. Additionally, we provide empirical resultsdemonstrating the performance of RegFeaL in various experiments.</description><author>Bertille Follain, Francis Bach</author><pubDate>Wed, 07 Aug 2024 12:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12754v4</guid></item><item><title>Advancing Multimodal Large Language Models with Quantization-Aware Scale Learning for Efficient Adaptation</title><link>http://arxiv.org/abs/2408.03735v1</link><description>This paper presents the first study to explore the potential of parameterquantization for multimodal large language models to alleviate the significantresource constraint encountered during vision-language instruction tuning. Weintroduce a Quantization-aware Scale LeArning method based on multimodalWarmup, termed QSLAW. This method is grounded in two key innovations: (1) Thelearning of group-wise scale factors for quantized LLM weights to mitigate thequantization error arising from activation outliers and achieve more effectivevision-language instruction tuning; (2) The implementation of a multimodalwarmup that progressively integrates linguistic and multimodal trainingsamples, thereby preventing overfitting of the quantized model to multimodaldata while ensuring stable adaptation of multimodal large language models todownstream vision-language tasks. Extensive experiments demonstrate that modelsquantized by QSLAW perform on par with, or even surpass, their full-precisioncounterparts, while facilitating up to 1.4 times reduction in VL tuning timeand GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.</description><author>Jingjing Xie, Yuxin Zhang, Mingbao Lin, Liujuan Cao, Rongrong Ji</author><pubDate>Wed, 07 Aug 2024 12:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03735v1</guid></item><item><title>Soft-Hard Attention U-Net Model and Benchmark Dataset for Multiscale Image Shadow Removal</title><link>http://arxiv.org/abs/2408.03734v1</link><description>Effective shadow removal is pivotal in enhancing the visual quality of imagesin various applications, ranging from computer vision to digital photography.During the last decades physics and machine learning -based methodologies havebeen proposed; however, most of them have limited capacity in capturing complexshadow patterns due to restrictive model assumptions, neglecting the fact thatshadows usually appear at different scales. Also, current datasets used forbenchmarking shadow removal are composed of a limited number of images withsimple scenes containing mainly uniform shadows cast by single objects, whereasonly a few of them include both manual shadow annotations and pairedshadow-free images. Aiming to address all these limitations in the context ofnatural scene imaging, including urban environments with complex scenes, thecontribution of this study is twofold: a) it proposes a novel deep learningarchitecture, named Soft-Hard Attention U-net (SHAU), focusing on multiscaleshadow removal; b) it provides a novel synthetic dataset, named MultiscaleShadow Removal Dataset (MSRD), containing complex shadow patterns of multiplescales, aiming to serve as a privacy-preserving dataset for a morecomprehensive benchmarking of future shadow removal methodologies. Keyarchitectural components of SHAU are the soft and hard attention modules, whichalong with multiscale feature extraction blocks enable effective shadow removalof different scales and intensities. The results demonstrate the effectivenessof SHAU over the relevant state-of-the-art shadow removal methods acrossvarious benchmark datasets, improving the Peak Signal-to-Noise Ratio and RootMean Square Error for the shadow area by 25.1% and 61.3%, respectively.</description><author>Eirini Cholopoulou, Dimitrios E. Diamantis, Dimitra-Christina C. Koutsiou, Dimitris K. Iakovidis</author><pubDate>Wed, 07 Aug 2024 12:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03734v1</guid></item><item><title>Bayes-optimal learning of an extensive-width neural network from quadratically many samples</title><link>http://arxiv.org/abs/2408.03733v1</link><description>We consider the problem of learning a target function corresponding to asingle hidden layer neural network, with a quadratic activation function afterthe first layer, and random weights. We consider the asymptotic limit where theinput dimension and the network width are proportionally large. Recent work[Cui &amp; al '23] established that linear regression provides Bayes-optimal testerror to learn such a function when the number of available samples is onlylinear in the dimension. That work stressed the open challenge of theoreticallyanalyzing the optimal test error in the more interesting regime where thenumber of samples is quadratic in the dimension. In this paper, we solve thischallenge for quadratic activations and derive a closed-form expression for theBayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE,which combines approximate message passing with rotationally invariant matrixdenoising, and that asymptotically achieves the optimal performance.Technically, our result is enabled by establishing a link with recent works onoptimal denoising of extensive-rank matrices and on the ellipsoid fittingproblem. We further show empirically that, in the absence of noise,randomly-initialized gradient descent seems to sample the space of weights,leading to zero training loss, and averaging over initialization leads to atest error equal to the Bayes-optimal one.</description><author>Antoine Maillard, Emanuele Troiani, Simon Martin, Florent Krzakala, Lenka Zdeborová</author><pubDate>Wed, 07 Aug 2024 12:41:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03733v1</guid></item><item><title>Question Rephrasing for Quantifying Uncertainty in Large Language Models: Applications in Molecular Chemistry Tasks</title><link>http://arxiv.org/abs/2408.03732v1</link><description>Uncertainty quantification enables users to assess the reliability ofresponses generated by large language models (LLMs). We present a novelQuestion Rephrasing technique to evaluate the input uncertainty of LLMs, whichrefers to the uncertainty arising from equivalent variations of the inputsprovided to LLMs. This technique is integrated with sampling methods thatmeasure the output uncertainty of LLMs, thereby offering a more comprehensiveuncertainty assessment. We validated our approach on property prediction andreaction prediction for molecular chemistry tasks.</description><author>Zizhang Chen, Pengyu Hong, Sandeep Madireddy</author><pubDate>Wed, 07 Aug 2024 12:38:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03732v1</guid></item><item><title>A Convex-optimization-based Layer-wise Post-training Pruner for Large Language Models</title><link>http://arxiv.org/abs/2408.03728v1</link><description>Pruning is a critical strategy for compressing trained large language models(LLMs), aiming at substantial memory conservation and computationalacceleration without compromising performance. However, existing pruningmethods often necessitate inefficient retraining for billion-scale LLMs or relyon heuristic methods such as the optimal brain surgeon framework, which degradeperformance. In this paper, we introduce FISTAPruner, the first post-trainingpruner based on convex optimization models and algorithms. Specifically, wepropose a convex optimization model incorporating $\ell_1$ norm to inducesparsity and utilize the FISTA solver for optimization. FISTAPrunerincorporates an intra-layer cumulative error correction mechanism and supportsparallel pruning. We comprehensively evaluate FISTAPruner on models such asOPT, LLaMA, LLaMA-2, and LLaMA-3 with 125M to 70B parameters under unstructuredand 2:4 semi-structured sparsity, demonstrating superior performance overexisting state-of-the-art methods across various language benchmarks.</description><author>Pengxiang Zhao, Hanyu Hu, Ping Li, Yi Zheng, Zhefeng Wang, Xiaoming Yuan</author><pubDate>Wed, 07 Aug 2024 12:33:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03728v1</guid></item><item><title>SentenceVAE: Enable Next-sentence Prediction for Large Language Models with Faster Speed, Higher Accuracy and Longer Context</title><link>http://arxiv.org/abs/2408.00655v4</link><description>Current large language models (LLMs) primarily utilize next-token predictionmethod for inference, which significantly impedes their processing speed. Inthis paper, we introduce a novel inference methodology termed next-sentenceprediction, aimed at enhancing the inference efficiency of LLMs. We presentSentence Variational Autoencoder (SentenceVAE), a tiny model consisting of aSentence Encoder and a Sentence Decoder. The Sentence Encoder can effectivelycondense the information within a sentence into a singular token, while theSentence Decoder can reconstruct this compressed token back into sentence. Byintegrating SentenceVAE into the input and output layers of LLMs, we developSentence-level LLMs (SLLMs) that employ a sentence-by-sentence inferencemethod. In addition, the SentenceVAE module of SLLMS can maintain the integrityof the original semantic content by segmenting the context into sentences,thereby improving accuracy while boosting inference speed. Moreover, comparedto previous LLMs, SLLMs process fewer tokens over equivalent context length,significantly reducing memory demands for self-attention computation andfacilitating the handling of longer context. Extensive experiments on Wanjuandataset have reveal that the proposed method can accelerate inference speed by204~365%, reduce perplexity (PPL) to 46~75% of its original metric, anddecrease memory overhead by 86~91% for the equivalent context length, comparedto the token-by-token method.</description><author>Hongjun An, Yifan Chen, Zhe Sun, Xuelong Li</author><pubDate>Wed, 07 Aug 2024 12:23:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00655v4</guid></item></channel></rss>