<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 10 Oct 2024 13:00:32 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MM-Ego: Towards Building Egocentric Multimodal LLMs</title><link>http://arxiv.org/abs/2410.07177v1</link><description>This research aims to comprehensively explore building a multimodalfoundation model for egocentric video understanding. To achieve this goal, wework on three fronts. First, as there is a lack of QA data for egocentric videounderstanding, we develop a data engine that efficiently generates 7Mhigh-quality QA samples for egocentric videos ranging from 30 seconds to onehour long, based on human-annotated data. This is currently the largestegocentric QA dataset. Second, we contribute a challenging egocentric QAbenchmark with 629 videos and 7,026 questions to evaluate the models' abilityin recognizing and memorizing visual details across videos of varying lengths.We introduce a new de-biasing evaluation method to help mitigate theunavoidable language bias present in the models being evaluated. Third, wepropose a specialized multimodal architecture featuring a novel "Memory PointerPrompting" mechanism. This design includes a global glimpse step to gain anoverarching understanding of the entire video and identify key visualinformation, followed by a fallback step that utilizes the key visualinformation to generate responses. This enables the model to more effectivelycomprehend extended video content. With the data, benchmark, and model, wesuccessfully build MM-Ego, an egocentric multimodal LLM that shows powerfulperformance on egocentric video understanding.</description><author>Hanrong Ye, Haotian Zhang, Erik Daxberger, Lin Chen, Zongyu Lin, Yanghao Li, Bowen Zhang, Haoxuan You, Dan Xu, Zhe Gan, Jiasen Lu, Yinfei Yang</author><pubDate>Wed, 09 Oct 2024 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07177v1</guid></item><item><title>Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models</title><link>http://arxiv.org/abs/2410.07176v1</link><description>Retrieval-Augmented Generation (RAG), while effective in integrating externalknowledge to address the limitations of large language models (LLMs), can beundermined by imperfect retrieval, which may introduce irrelevant, misleading,or even malicious information. Despite its importance, previous studies haverarely explored the behavior of RAG through joint analysis on how errors fromimperfect retrieval attribute and propagate, and how potential conflicts arisebetween the LLMs' internal knowledge and external sources. We find thatimperfect retrieval augmentation might be inevitable and quite harmful, throughcontrolled analysis under realistic conditions. We identify the knowledgeconflicts between LLM-internal and external knowledge from retrieval as abottleneck to overcome in the post-retrieval stage of RAG. To render LLMsresilient to imperfect retrieval, we propose Astute RAG, a novel RAG approachthat adaptively elicits essential information from LLMs' internal knowledge,iteratively consolidates internal and external knowledge with source-awareness,and finalizes the answer according to information reliability. Our experimentsusing Gemini and Claude demonstrate that Astute RAG significantly outperformsprevious robustness-enhanced RAG methods. Notably, Astute RAG is the onlyapproach that matches or exceeds the performance of LLMs without RAG underworst-case scenarios. Further analysis reveals that Astute RAG effectivelyresolves knowledge conflicts, improving the reliability and trustworthiness ofRAG systems.</description><author>Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, Sercan Ö. Arık</author><pubDate>Wed, 09 Oct 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07176v1</guid></item><item><title>Neural Circuit Architectural Priors for Quadruped Locomotion</title><link>http://arxiv.org/abs/2410.07174v1</link><description>Learning-based approaches to quadruped locomotion commonly adopt genericpolicy architectures like fully connected MLPs. As such architectures containfew inductive biases, it is common in practice to incorporate priors in theform of rewards, training curricula, imitation data, or trajectory generators.In nature, animals are born with priors in the form of their nervous system'sarchitecture, which has been shaped by evolution to confer innate ability andefficient learning. For instance, a horse can walk within hours of birth andcan quickly improve with practice. Such architectural priors can also be usefulin ANN architectures for AI. In this work, we explore the advantages of abiologically inspired ANN architecture for quadruped locomotion based on neuralcircuits in the limbs and spinal cord of mammals. Our architecture achievesgood initial performance and comparable final performance to MLPs, while usingless data and orders of magnitude fewer parameters. Our architecture alsoexhibits better generalization to task variations, even admitting deployment ona physical robot without standard sim-to-real methods. This work shows thatneural circuits can provide valuable architectural priors for locomotion andencourages future work in other sensorimotor skills.</description><author>Nikhil X. Bhattasali, Venkatesh Pattabiraman, Lerrel Pinto, Grace W. Lindsay</author><pubDate>Wed, 09 Oct 2024 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07174v1</guid></item><item><title>Do better language models have crisper vision?</title><link>http://arxiv.org/abs/2410.07173v1</link><description>How well do text-only Large Language Models (LLMs) grasp the visual world? AsLLMs are increasingly used in computer vision, addressing this question becomesboth fundamental and pertinent. However, existing studies have primarilyfocused on limited scenarios, such as their ability to generate visual contentor cluster multimodal data. To this end, we propose the Visual TextRepresentation Benchmark (ViTeRB) to isolate key properties that make languagemodels well-aligned with the visual world. With this, we identify large-scaledecoder-based LLMs as ideal candidates for representing text in vision-centriccontexts, counter to the current practice of utilizing text encoders. Buildingon these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.By leveraging precomputable frozen features from strong vision and languagemodels, ShareLock achieves an impressive 51% accuracy on ImageNet despiteutilizing just 563k image-caption pairs. Moreover, training requires only 1 GPUhour (or 10 hours including the precomputation of features) - orders ofmagnitude less than prior methods. Code will be released.</description><author>Jona Ruthardt, Gertjan J. Burghouts, Serge Belongie, Yuki M. Asano</author><pubDate>Wed, 09 Oct 2024 17:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07173v1</guid></item><item><title>Glider: Global and Local Instruction-Driven Expert Router</title><link>http://arxiv.org/abs/2410.07172v1</link><description>The availability of performant pre-trained models has led to a proliferationof fine-tuned expert models that are specialized to particular domains. Thishas enabled the creation of powerful and adaptive routing-based "ModelMoErging" methods with the goal of using expert modules to create an aggregatesystem with improved performance or generalization. However, existing MoErgingmethods often prioritize generalization to unseen tasks at the expense ofperformance on held-in tasks, which limits its practical applicability inreal-world deployment scenarios. We observe that current token-level routingmechanisms neglect the global semantic context of the input task. Thistoken-wise independence hinders effective expert selection for held-in tasks,as routing decisions fail to incorporate the semantic properties of the task.To address this, we propose, Global and Local Instruction Driven Expert Router(GLIDER) that integrates a multi-scale routing mechanism, encompassing asemantic global router and a learned local router. The global router leveragesLLM's advanced reasoning capabilities for semantic-related contexts to enhanceexpert selection. Given the input query and LLM, the router generates semantictask instructions that guide the retrieval of the most relevant experts acrossall layers. This global guidance is complemented by a local router thatfacilitates token-level routing decisions within each module, enabling finercontrol and enhanced performance on unseen tasks. Our experiments usingT5-based models for T0 and FLAN tasks demonstrate that GLIDER achievessubstantially improved held-in performance while maintaining stronggeneralization on held-out tasks. We also perform ablations experiments to divedeeper into the components of GLIDER. Our experiments highlight the importanceof our multi-scale routing that leverages LLM-driven semantic reasoning forMoErging methods.</description><author>Pingzhi Li, Prateek Yadav, Jaehong Yoon, Jie Peng, Yi-Lin Sung, Mohit Bansal, Tianlong Chen</author><pubDate>Wed, 09 Oct 2024 17:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07172v1</guid></item><item><title>IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation</title><link>http://arxiv.org/abs/2410.07171v1</link><description>Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have madenotable strides in compositional text-to-image generation. However, thesemethods typically exhibit distinct strengths for compositional generation, withsome excelling in handling attribute binding and others in spatialrelationships. This disparity highlights the need for an approach that canleverage the complementary strengths of various models to comprehensivelyimprove the composition capability. To this end, we introduce IterComp, a novelframework that aggregates composition-aware model preferences from multiplemodels and employs an iterative feedback learning approach to enhancecompositional generation. Specifically, we curate a gallery of six powerfulopen-source diffusion models and evaluate their three key compositionalmetrics: attribute binding, spatial relationships, and non-spatialrelationships. Based on these metrics, we develop a composition-aware modelpreference dataset comprising numerous image-rank pairs to traincomposition-aware reward models. Then, we propose an iterative feedbacklearning method to enhance compositionality in a closed-loop manner, enablingthe progressive self-refinement of both the base diffusion model and rewardmodels over multiple iterations. Theoretical proof demonstrates theeffectiveness and extensive experiments show our significant superiority overprevious SOTA methods (e.g., Omost and FLUX), particularly in multi-categoryobject composition and complex semantic alignment. IterComp opens new researchavenues in reward feedback learning for diffusion models and compositionalgeneration. Code: https://github.com/YangLing0818/IterComp</description><author>Xinchen Zhang, Ling Yang, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, Bin Cui</author><pubDate>Wed, 09 Oct 2024 17:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07171v1</guid></item><item><title>One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation</title><link>http://arxiv.org/abs/2410.07170v1</link><description>Foundation models (FMs) are pre-trained on large-scale datasets and thenfine-tuned on a downstream task for a specific application. The most successfuland most commonly used fine-tuning method is to update the pre-trained weightsvia a low-rank adaptation (LoRA). LoRA introduces new weight matrices that areusually initialized at random with a uniform rank distribution across modelweights. Recent works focus on weight-driven initialization or learning ofadaptive ranks during training. Both approaches have only been investigated inisolation, resulting in slow convergence or a uniform rank distribution, inturn leading to sub-optimal performance. We propose to enhance LoRA byinitializing the new weights in a data-driven manner by computing singularvalue decomposition on minibatches of activation vectors. Then, we initializethe LoRA matrices with the obtained right-singular vectors and re-distributeranks among all weight matrices to explain the maximal amount of variance andcontinue the standard LoRA fine-tuning procedure. This results in our newmethod Explained Variance Adaptation (EVA). We apply EVA to a variety offine-tuning tasks ranging from language generation and understanding to imageclassification and reinforcement learning. EVA exhibits faster convergence thancompetitors and attains the highest average score across a multitude of tasksper domain.</description><author>Fabian Paischer, Lukas Hauzenberger, Thomas Schmied, Benedikt Alkin, Marc Peter Deisenroth, Sepp Hochreiter</author><pubDate>Wed, 09 Oct 2024 17:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07170v1</guid></item><item><title>Sylber: Syllabic Embedding Representation of Speech from Raw Audio</title><link>http://arxiv.org/abs/2410.07168v1</link><description>Syllables are compositional units of spoken language that play a crucial rolein human speech perception and production. However, current neural speechrepresentations lack structure, resulting in dense token sequences that arecostly to process. To bridge this gap, we propose a new model, Sylber, thatproduces speech representations with clean and robust syllabic structure.Specifically, we propose a self-supervised model that regresses features onsyllabic segments distilled from a teacher model which is an exponential movingaverage of the model in training. This results in a highly structuredrepresentation of speech features, offering three key benefits: 1) a fast,linear-time syllable segmentation algorithm, 2) efficient syllabic tokenizationwith an average of 4.27 tokens per second, and 3) syllabic units better suitedfor lexical and syntactic understanding. We also train token-to-speechgenerative models with our syllabic units and show that fully intelligiblespeech can be reconstructed from these tokens. Lastly, we observe thatcategorical perception, a linguistic phenomenon of speech perception, emergesnaturally in our model, making the embedding space more categorical and sparsethan previous self-supervised learning approaches. Together, we present a novelself-supervised approach for representing speech as syllables, with significantpotential for efficient speech tokenization and spoken language modeling.</description><author>Cheol Jun Cho, Nicholas Lee, Akshat Gupta, Dhruv Agarwal, Ethan Chen, Alan W Black, Gopala K. Anumanchipalli</author><pubDate>Wed, 09 Oct 2024 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07168v1</guid></item><item><title>Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate</title><link>http://arxiv.org/abs/2410.07167v1</link><description>We present the Modality Integration Rate (MIR), an effective, robust, andgeneralized metric to indicate the multi-modal pre-training quality of LargeVision Language Models (LVLMs). Large-scale pre-training plays a critical rolein building capable LVLMs, while evaluating its training quality without thecostly supervised fine-tuning stage is under-explored. Loss, perplexity, andin-context evaluation results are commonly used pre-training metrics for LargeLanguage Models (LLMs), while we observed that these metrics are lessindicative when aligning a well-trained LLM with a new modality. Due to thelack of proper metrics, the research of LVLMs in the critical pre-trainingstage is hindered greatly, including the training data choice, efficient moduledesign, etc. In this paper, we propose evaluating the pre-training quality fromthe inter-modal distribution distance perspective and present MIR, the ModalityIntegration Rate, which is 1) \textbf{Effective} to represent the pre-trainingquality and show a positive relation with the benchmark performance aftersupervised fine-tuning. 2) \textbf{Robust} toward different training/evaluationdata. 3) \textbf{Generalize} across training configurations and architecturechoices. We conduct a series of pre-training experiments to explore theeffectiveness of MIR and observe satisfactory results that MIR is indicativeabout training data selection, training strategy schedule, and modelarchitecture design to get better pre-training results. We hope MIR could be ahelpful metric for building capable LVLMs and inspire the following researchabout modality alignment in different areas. Our code is at:https://github.com/shikiw/Modality-Integration-Rate.</description><author>Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu</author><pubDate>Wed, 09 Oct 2024 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07167v1</guid></item><item><title>Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making</title><link>http://arxiv.org/abs/2410.07166v1</link><description>We aim to evaluate Large Language Models (LLMs) for embodied decision making.While a significant body of work has been leveraging LLMs for decision makingin embodied environments, we still lack a systematic understanding of theirperformance because they are usually applied in different domains, fordifferent purposes, and built based on different inputs and outputs.Furthermore, existing evaluations tend to rely solely on a final success rate,making it difficult to pinpoint what ability is missing in LLMs and where theproblem lies, which in turn blocks embodied agents from leveraging LLMseffectively and selectively. To address these limitations, we propose ageneralized interface (Embodied Agent Interface) that supports theformalization of various types of tasks and input-output specifications ofLLM-based modules. Specifically, it allows us to unify 1) a broad set ofembodied decision-making tasks involving both state and temporally extendedgoals, 2) four commonly-used LLM-based modules for decision making: goalinterpretation, subgoal decomposition, action sequencing, and transitionmodeling, and 3) a collection of fine-grained metrics which break downevaluation into various types of errors, such as hallucination errors,affordance errors, various types of planning errors, etc. Overall, ourbenchmark offers a comprehensive assessment of LLMs' performance for differentsubtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AIsystems, and providing insights for effective and selective use of LLMs inembodied decision making.</description><author>Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, Jiajun Wu</author><pubDate>Wed, 09 Oct 2024 17:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07166v1</guid></item><item><title>A neural network-based approach to hybrid systems identification for control</title><link>http://arxiv.org/abs/2404.01814v2</link><description>We consider the problem of designing a machine learning-based model of anunknown dynamical system from a finite number of (state-input)-successor statedata points, such that the model obtained is also suitable for optimal controldesign. We adopt a neural network (NN) architecture that, once suitablytrained, yields a hybrid system with continuous piecewise-affine (PWA) dynamicsthat is differentiable with respect to the network's parameters, therebyenabling the use of derivative-based training procedures. We show that acareful choice of our NN's weights produces a hybrid system model withstructural properties that are highly favorable when used as part of a finitehorizon optimal control problem (OCP). Specifically, we rely on availableresults to establish that optimal solutions with strong local optimalityguarantees can be computed via nonlinear programming (NLP), in contrast toclassical OCPs for general hybrid systems which typically require mixed-integeroptimization. Besides being well-suited for optimal control design, numericalsimulations illustrate that our NN-based technique enjoys very similarperformance to state-of-the-art system identification methods for hybridsystems and it is competitive on nonlinear benchmarks.</description><author>Filippo Fabiani, Bartolomeo Stellato, Daniele Masti, Paul J. Goulart</author><pubDate>Wed, 09 Oct 2024 17:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01814v2</guid></item><item><title>AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation</title><link>http://arxiv.org/abs/2410.07164v1</link><description>Recent advancements in diffusion models have led to significant improvementsin the generation and animation of 4D full-body human-object interactions(HOI). Nevertheless, existing methods primarily focus on SMPL-based motiongeneration, which is limited by the scarcity of realistic large-scaleinteraction data. This constraint affects their ability to create everyday HOIscenes. This paper addresses this challenge using a zero-shot approach with apre-trained diffusion model. Despite this potential, achieving our goals isdifficult due to the diffusion model's lack of understanding of ''where'' and''how'' objects interact with the human body. To tackle these issues, weintroduce AvatarGO, a novel framework designed to generate animatable 4D HOIscenes directly from textual inputs. Specifically, 1) for the ''where''challenge, we propose LLM-guided contact retargeting, which employs Lang-SAM toidentify the contact body part from text prompts, ensuring preciserepresentation of human-object spatial relations. 2) For the ''how'' challenge,we introduce correspondence-aware motion optimization that constructs motionfields for both human and object models using the linear blend skinningfunction from SMPL-X. Our framework not only generates coherent compositionalmotions, but also exhibits greater robustness in handling penetration issues.Extensive experiments with existing methods validate AvatarGO's superiorgeneration and animation capabilities on a variety of human-object pairs anddiverse poses. As the first attempt to synthesize 4D avatars with objectinteractions, we hope AvatarGO could open new doors for human-centric 4Dcontent creation.</description><author>Yukang Cao, Liang Pan, Kai Han, Kwan-Yee K. Wong, Ziwei Liu</author><pubDate>Wed, 09 Oct 2024 17:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07164v1</guid></item><item><title>Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning</title><link>http://arxiv.org/abs/2410.07163v1</link><description>In this work, we address the problem of large language model (LLM)unlearning, aiming to remove unwanted data influences and associated modelcapabilities (e.g., copyrighted data or harmful content generation) whilepreserving essential model utilities, without the need for retraining fromscratch. Despite the growing need for LLM unlearning, a principled optimizationframework remains lacking. To this end, we revisit the state-of-the-artapproach, negative preference optimization (NPO), and identify the issue ofreference model bias, which could undermine NPO's effectiveness, particularlywhen unlearning forget data of varying difficulty. Given that, we propose asimple yet effective unlearning optimization framework, called SimNPO, showingthat 'simplicity' in removing the reliance on a reference model (through thelens of simple preference optimization) benefits unlearning. We also providedeeper insights into SimNPO's advantages, supported by analysis using mixturesof Markov chains. Furthermore, we present extensive experiments validatingSimNPO's superiority over existing unlearning baselines in benchmarks like TOFUand MUSE, and robustness against relearning attacks. Codes are available athttps://github.com/OPTML-Group/Unlearn-Simple.</description><author>Chongyu Fan, Jiancheng Liu, Licong Lin, Jinghan Jia, Ruiqi Zhang, Song Mei, Sijia Liu</author><pubDate>Wed, 09 Oct 2024 17:58:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07163v1</guid></item><item><title>Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy</title><link>http://arxiv.org/abs/2407.06813v2</link><description>Diplomacy is one of the most sophisticated activities in human society. Thecomplex interactions among multiple parties/ agents involve various abilitieslike social reasoning, negotiation arts, and long-term strategy planning.Previous AI agents surely have proved their capability of handling multi-stepgames and larger action spaces on tasks involving multiple agents. However,diplomacy involves a staggering magnitude of decision spaces, especiallyconsidering the negotiation stage required. Recently, LLM agents have showntheir potential for extending the boundary of previous agents on a couple ofapplications, however, it is still not enough to handle a very long planningperiod in a complex multi-agent environment. Empowered with cutting-edge LLMtechnology, we make the first stab to explore AI's upper bound towards ahuman-like agent for such a highly comprehensive multi-agent mission bycombining three core and essential capabilities for stronger LLM-based societalagents: 1) strategic planner with memory and reflection; 2) goal-orientednegotiate with social reasoning; 3) augmenting memory by self-play games toself-evolving without any human in the loop.</description><author>Zhenyu Guan, Xiangyu Kong, Fangwei Zhong, Yizhou Wang</author><pubDate>Wed, 09 Oct 2024 17:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06813v2</guid></item><item><title>Quanda: An Interpretability Toolkit for Training Data Attribution Evaluation and Beyond</title><link>http://arxiv.org/abs/2410.07158v1</link><description>In recent years, training data attribution (TDA) methods have emerged as apromising direction for the interpretability of neural networks. While researcharound TDA is thriving, limited effort has been dedicated to the evaluation ofattributions. Similar to the development of evaluation metrics for traditionalfeature attribution approaches, several standalone metrics have been proposedto evaluate the quality of TDA methods across various contexts. However, thelack of a unified framework that allows for systematic comparison limits trustin TDA methods and stunts their widespread adoption. To address this researchgap, we introduce Quanda, a Python toolkit designed to facilitate theevaluation of TDA methods. Beyond offering a comprehensive set of evaluationmetrics, Quanda provides a uniform interface for seamless integration withexisting TDA implementations across different repositories, thus enablingsystematic benchmarking. The toolkit is user-friendly, thoroughly tested,well-documented, and available as an open-source library on PyPi and underhttps://github.com/dilyabareeva/quanda.</description><author>Dilyara Bareeva, Galip Ümit Yolcu, Anna Hedström, Niklas Schmolenski, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</author><pubDate>Wed, 09 Oct 2024 17:56:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07158v1</guid></item><item><title>InstructG2I: Synthesizing Images from Multimodal Attributed Graphs</title><link>http://arxiv.org/abs/2410.07157v1</link><description>In this paper, we approach an overlooked yet critical task Graph2Image:generating images from multimodal attributed graphs (MMAGs). This task posessignificant challenges due to the explosion in graph size, dependencies amonggraph entities, and the need for controllability in graph conditions. Toaddress these challenges, we propose a graph context-conditioned diffusionmodel called InstructG2I. InstructG2I first exploits the graph structure andmultimodal information to conduct informative neighbor sampling by combiningpersonalized page rank and re-ranking based on vision-language features. Then,a Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliaryset of graph prompts to guide the denoising process of diffusion. Finally, wepropose graph classifier-free guidance, enabling controllable generation byvarying the strength of graph guidance and multiple connected edges to a node.Extensive experiments conducted on three datasets from different domainsdemonstrate the effectiveness and controllability of our approach. The code isavailable at https://github.com/PeterGriffinJin/InstructG2I.</description><author>Bowen Jin, Ziqi Pang, Bingjun Guo, Yu-Xiong Wang, Jiaxuan You, Jiawei Han</author><pubDate>Wed, 09 Oct 2024 17:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07157v1</guid></item><item><title>Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis</title><link>http://arxiv.org/abs/2410.07155v1</link><description>Recent advances in diffusion models have demonstrated exceptionalcapabilities in image and video generation, further improving the effectivenessof 4D synthesis. Existing 4D generation methods can generate high-quality 4Dobjects or scenes based on user-friendly conditions, benefiting the gaming andvideo industries. However, these methods struggle to synthesize significantobject deformation of complex 4D transitions and interactions within scenes. Toaddress this challenge, we propose Trans4D, a novel text-to-4D synthesisframework that enables realistic complex scene transitions. Specifically, wefirst use multi-modal large language models (MLLMs) to produce a physic-awarescene description for 4D scene initialization and effective transition timingplanning. Then we propose a geometry-aware 4D transition network to realize acomplex scene-level 4D transition based on the plan, which involves expressivegeometrical object deformation. Extensive experiments demonstrate that Trans4Dconsistently outperforms existing state-of-the-art methods in generating 4Dscenes with accurate and high-quality transitions, validating itseffectiveness. Code: https://github.com/YangLing0818/Trans4D</description><author>Bohan Zeng, Ling Yang, Siyu Li, Jiaming Liu, Zixiang Zhang, Juanxi Tian, Kaixin Zhu, Yongzhen Guo, Fu-Yun Wang, Minkai Xu, Stefano Ermon, Wentao Zhang</author><pubDate>Wed, 09 Oct 2024 17:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07155v1</guid></item><item><title>CHASE: Learning Convex Hull Adaptive Shift for Skeleton-based Multi-Entity Action Recognition</title><link>http://arxiv.org/abs/2410.07153v1</link><description>Skeleton-based multi-entity action recognition is a challenging task aimingto identify interactive actions or group activities involving multiple diverseentities. Existing models for individuals often fall short in this task due tothe inherent distribution discrepancies among entity skeletons, leading tosuboptimal backbone optimization. To this end, we introduce a Convex HullAdaptive Shift based multi-Entity action recognition method (CHASE), whichmitigates inter-entity distribution gaps and unbiases subsequent backbones.Specifically, CHASE comprises a learnable parameterized network and anauxiliary objective. The parameterized network achieves plausible,sample-adaptive repositioning of skeleton sequences through two key components.First, the Implicit Convex Hull Constrained Adaptive Shift ensures that the neworigin of the coordinate system is within the skeleton convex hull. Second, theCoefficient Learning Block provides a lightweight parameterization of themapping from skeleton sequences to their specific coefficients in convexcombinations. Moreover, to guide the optimization of this network fordiscrepancy minimization, we propose the Mini-batch Pair-wise Maximum MeanDiscrepancy as the additional objective. CHASE operates as a sample-adaptivenormalization method to mitigate inter-entity distribution discrepancies,thereby reducing data bias and improving the subsequent classifier'smulti-entity action recognition performance. Extensive experiments on sixdatasets, including NTU Mutual 11/26, H2O, Assembly101, Collective Activity andVolleyball, consistently verify our approach by seamlessly adapting tosingle-entity backbones and boosting their performance in multi-entityscenarios. Our code is publicly available at https://github.com/Necolizer/CHASE .</description><author>Yuhang Wen, Mengyuan Liu, Songtao Wu, Beichen Ding</author><pubDate>Wed, 09 Oct 2024 17:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07153v1</guid></item><item><title>Towards Interpreting Visual Information Processing in Vision-Language Models</title><link>http://arxiv.org/abs/2410.07149v1</link><description>Vision-Language Models (VLMs) are powerful tools for processing andunderstanding text and images. We study the processing of visual tokens in thelanguage model component of LLaVA, a prominent VLM. Our approach focuses onanalyzing the localization of object information, the evolution of visual tokenrepresentations across layers, and the mechanism of integrating visualinformation for predictions. Through ablation studies, we demonstrated thatobject identification accuracy drops by over 70\% when object-specific tokensare removed. We observed that visual token representations become increasinglyinterpretable in the vocabulary space across layers, suggesting an alignmentwith textual tokens corresponding to image content. Finally, we found that themodel extracts object information from these refined representations at thelast token position for prediction, mirroring the process in text-only languagemodels for factual association tasks. These findings provide crucial insightsinto how VLMs process and integrate visual information, bridging the gapbetween our understanding of language and vision models, and paving the way formore interpretable and controllable multimodal systems.</description><author>Clement Neo, Luke Ong, Philip Torr, Mor Geva, David Krueger, Fazl Barez</author><pubDate>Wed, 09 Oct 2024 17:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07149v1</guid></item><item><title>Taking a turn for the better: Conversation redirection throughout the course of mental-health therapy</title><link>http://arxiv.org/abs/2410.07147v1</link><description>Mental-health therapy involves a complex conversation flow in which patientsand therapists continuously negotiate what should be talked about next. Forexample, therapists might try to shift the conversation's direction to keep thetherapeutic process on track and avoid stagnation, or patients might push thediscussion towards issues they want to focus on. How do such patient and therapist redirections relate to the development andquality of their relationship? To answer this question, we introduce aprobabilistic measure of the extent to which a certain utterance immediatelyredirects the flow of the conversation, accounting for both the intention andthe actual realization of such a change. We apply this new measure tocharacterize the development of patient-therapist relationships over multiplesessions in a very large, widely-used online therapy platform. Our analysisreveals that (1) patient control of the conversation's direction generallyincreases relative to that of the therapist as their relationship progresses;and (2) patients who have less control in the first few sessions aresignificantly more likely to eventually express dissatisfaction with theirtherapist and terminate the relationship.</description><author>Vivian Nguyen, Sang Min Jung, Lillian Lee, Thomas D. Hull, Cristian Danescu-Niculescu-Mizil</author><pubDate>Wed, 09 Oct 2024 17:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07147v1</guid></item><item><title>Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling</title><link>http://arxiv.org/abs/2410.07145v1</link><description>One essential advantage of recurrent neural networks (RNNs) overtransformer-based language models is their linear computational complexityconcerning the sequence length, which makes them much faster in handling longsequences during inference. However, most publicly available RNNs (e.g., Mambaand RWKV) are trained on sequences with less than 10K tokens, and theireffectiveness in longer contexts remains largely unsatisfying so far. In thispaper, we study the cause of the inability to process long context for RNNs andsuggest critical mitigations. We examine two practical concerns when applyingstate-of-the-art RNNs to long contexts: (1) the inability to extrapolate toinputs longer than the training length and (2) the upper bound of memorycapacity. Addressing the first concern, we first investigate *state collapse*(SC), a phenomenon that causes severe performance degradation on sequencelengths not encountered during training. With controlled experiments, weattribute this to overfitting due to the recurrent state beingoverparameterized for the training length. For the second concern, we train aseries of Mamba-2 models on long documents to empirically estimate therecurrent state capacity in language modeling and passkey retrieval. Then,three SC mitigation methods are proposed to improve Mamba-2's lengthgeneralizability, allowing the model to process more than 1M tokens without SC.We also find that the recurrent state capacity in passkey retrieval scalesexponentially to the state size, and we empirically train a Mamba-2 370M withnear-perfect passkey retrieval accuracy on 256K context length. This suggests apromising future for RNN-based long-context modeling.</description><author>Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong Sun</author><pubDate>Wed, 09 Oct 2024 17:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07145v1</guid></item><item><title>Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates</title><link>http://arxiv.org/abs/2410.07137v1</link><description>Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, andMT-Bench, have become popular for evaluating language models due to theircost-effectiveness and scalability compared to human evaluation. Achieving highwin rates on these benchmarks can significantly boost the promotional impact ofnewly released language models. This promotional benefit may motivate tricks,such as manipulating model output length or style to game win rates, eventhough several mechanisms have been developed to control length and disentanglestyle to reduce gameability. Nonetheless, we show that even a "null model" thatalways outputs a constant response (irrelevant to input instructions) can cheatautomatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate onAlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.Moreover, the crafted cheating outputs are transferable because we assume thatthe instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) areprivate and cannot be accessed. While our experiments are primarilyproof-of-concept, an adversary could use LLMs to generate more imperceptiblecheating responses, unethically benefiting from high win rates and promotionalimpact. Our findings call for the development of anti-cheating mechanisms forreliable automatic benchmarks. The code is available athttps://github.com/sail-sg/Cheating-LLM-Benchmarks.</description><author>Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin</author><pubDate>Wed, 09 Oct 2024 17:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07137v1</guid></item><item><title>EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models</title><link>http://arxiv.org/abs/2410.07133v1</link><description>Recent advancements in generation models have showcased remarkablecapabilities in generating fantastic content. However, most of them are trainedon proprietary high-quality data, and some models withhold their parameters andonly provide accessible application programming interfaces (APIs), limitingtheir benefits for downstream tasks. To explore the feasibility of training atext-to-image generation model comparable to advanced models using publiclyavailable resources, we introduce EvolveDirector. This framework interacts withadvanced models through their public APIs to obtain text-image data pairs totrain a base model. Our experiments with extensive data indicate that the modeltrained on generated data of the advanced model can approximate its generationcapability. However, it requires large-scale samples of 10 million or more.This incurs significant expenses in time, computational resources, andespecially the costs associated with calling fee-based APIs. To address thisproblem, we leverage pre-trained large vision-language models (VLMs) to guidethe evolution of the base model. VLM continuously evaluates the base modelduring training and dynamically updates and refines the training dataset by thediscrimination, expansion, deletion, and mutation operations. Experimentalresults show that this paradigm significantly reduces the required data volume.Furthermore, when approaching multiple advanced models, EvolveDirector canselect the best samples generated by them to learn powerful and balancedabilities. The final trained model Edgen is demonstrated to outperform theseadvanced models. The code and model weights are available athttps://github.com/showlab/EvolveDirector.</description><author>Rui Zhao, Hangjie Yuan, Yujie Wei, Shiwei Zhang, Yuchao Gu, Lingmin Ran, Xiang Wang, Zhangjie Wu, Junhao Zhang, Yingya Zhang, Mike Zheng Shou</author><pubDate>Wed, 09 Oct 2024 17:52:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07133v1</guid></item><item><title>Mental Disorders Detection in the Era of Large Language Models</title><link>http://arxiv.org/abs/2410.07129v1</link><description>This paper compares the effectiveness of traditional machine learningmethods, encoder-based models, and large language models (LLMs) on the task ofdetecting depression and anxiety. Five datasets were considered, each differingin format and the method used to define the target pathology class. We testedAutoML models based on linguistic features, several variations of encoder-basedTransformers such as BERT, and state-of-the-art LLMs as pathologyclassification models. The results demonstrated that LLMs outperformtraditional methods, particularly on noisy and small datasets where trainingexamples vary significantly in text length and genre. However, psycholinguisticfeatures and encoder-based models can achieve performance comparable tolanguage models when trained on texts from individuals with clinicallyconfirmed depression, highlighting their potential effectiveness in targetedclinical applications.</description><author>Gleb Kuzmin, Petr Strepetov, Maksim Stankevich, Ivan Smirnov, Artem Shelmanov</author><pubDate>Wed, 09 Oct 2024 17:51:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07129v1</guid></item><item><title>ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models</title><link>http://arxiv.org/abs/2407.12877v2</link><description>Assessing the quality of outputs generated by generative models, such aslarge language models and vision language models, presents notable challenges.Traditional methods for evaluation typically rely on either human assessments,which are resource-intensive, or automatic metrics that often show a lowcorrelation with human judgment. Another common approach is to use deeplearning systems, which not only consume a substantial amount of compute andtime but also require extensive training data. In this study, we introduce atuning-free framework called ReFeR, designed to evaluate generative outputs,including both text and images, by leveraging a 2-level hierarchy of LLMs andVLMs themselves. We rigorously evaluate our framework, ReFeR, across fourdiverse evaluation tasks. The framework not only improves the accuracy of theseevaluations, surpassing previous benchmarks but also generates constructivefeedback. Interestingly, the framework is also applicable to reasoning tasks.Experiments on four reasoning tasks demonstrate superior collective reasoningabilities of the framework. We present two variants of the framework:ReFeR-Turbo, optimized for accelerated performance, and ReFeR-Lite, offering amore cost-effective solution. ReFeR-Lite is $\sim7.7\times$ more efficientwhile being comparably accurate to ReFeR-Turbo. We make code, data and PIPpackage publicly available. See this PIP URLhttps://pypi.org/project/refer-agents/ and this Git URLhttps://github.com/yaswanth-iitkgp/ReFeR_Code .</description><author>Yaswanth Narsupalli, Abhranil Chandra, Sreevatsa Muppirala, Manish Gupta, Pawan Goyal</author><pubDate>Wed, 09 Oct 2024 17:51:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12877v2</guid></item><item><title>Thing2Reality: Transforming 2D Content into Conditioned Multiviews and 3D Gaussian Objects for XR Communication</title><link>http://arxiv.org/abs/2410.07119v1</link><description>During remote communication, participants often share both digital andphysical content, such as product designs, digital assets, and environments, toenhance mutual understanding. Recent advances in augmented communication havefacilitated users to swiftly create and share digital 2D copies of physicalobjects from video feeds into a shared space. However, conventional 2Drepresentations of digital objects restricts users' ability to spatiallyreference items in a shared immersive environment. To address this, we proposeThing2Reality, an Extended Reality (XR) communication platform that enhancesspontaneous discussions of both digital and physical items during remotesessions. With Thing2Reality, users can quickly materialize ideas or physicalobjects in immersive environments and share them as conditioned multiviewrenderings or 3D Gaussians. Thing2Reality enables users to interact with remoteobjects or discuss concepts in a collaborative manner. Our user study revealedthat the ability to interact with and manipulate 3D representations of objectssignificantly enhances the efficiency of discussions, with the potential toaugment discussion of 2D artifacts.</description><author>Erzhen Hu, Mingyi Li, Jungtaek Hong, Xun Qian, Alex Olwal, David Kim, Seongkook Heo, Ruofei Du</author><pubDate>Wed, 09 Oct 2024 17:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07119v1</guid></item><item><title>Exploring the Readiness of Prominent Small Language Models for the Democratization of Financial Literacy</title><link>http://arxiv.org/abs/2410.07118v1</link><description>The use of small language models (SLMs), herein defined as models with lessthan three billion parameters, is increasing across various domains andapplications. Due to their ability to run on more accessible hardware andpreserve user privacy, SLMs possess the potential to democratize access tolanguage models for individuals of different socioeconomic status and withdifferent privacy preferences. This study assesses several state-of-the-artSLMs (e.g., Apple's OpenELM, Microsoft's Phi, Google's Gemma, and the Tinyllamaproject) for use in the financial domain to support the development offinancial literacy LMs. Democratizing access to quality financial informationfor those who are financially under educated is greatly needed in society,particularly as new financial markets and products emerge and participation infinancial markets increases due to ease of access. We are the first to examinethe use of open-source SLMs to democratize access to financial questionanswering capabilities for individuals and students. To this end, we provide ananalysis of the memory usage, inference time, similarity comparisons toground-truth answers, and output readability of prominent SLMs to determinewhich models are most accessible and capable of supporting access to financialinformation. We analyze zero-shot and few-shot learning variants of the models.The results suggest that some off-the-shelf SLMs merit further exploration andfine-tuning to prepare them for individual use, while others may have limits totheir democratization.</description><author>Tagore Rao Kosireddy, Jeffrey D. Wall, Evan Lucas</author><pubDate>Wed, 09 Oct 2024 17:48:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07118v1</guid></item><item><title>The FIX Benchmark: Extracting Features Interpretable to eXperts</title><link>http://arxiv.org/abs/2409.13684v2</link><description>Feature-based methods are commonly used to explain model predictions, butthese methods often implicitly assume that interpretable features are readilyavailable. However, this is often not the case for high-dimensional data, andit can be hard even for domain experts to mathematically specify which featuresare important. Can we instead automatically extract collections or groups offeatures that are aligned with expert knowledge? To address this gap, wepresent FIX (Features Interpretable to eXperts), a benchmark for measuring howwell a collection of features aligns with expert knowledge. In collaborationwith domain experts, we propose FIXScore, a unified expert alignment measureapplicable to diverse real-world settings across cosmology, psychology, andmedicine domains in vision, language and time series data modalities. WithFIXScore, we find that popular feature-based explanation methods have pooralignment with expert-specified knowledge, highlighting the need for newmethods that can better identify features interpretable to experts.</description><author>Helen Jin, Shreya Havaldar, Chaehyeon Kim, Anton Xue, Weiqiu You, Helen Qu, Marco Gatti, Daniel A Hashimoto, Bhuvnesh Jain, Amin Madani, Masao Sako, Lyle Ungar, Eric Wong</author><pubDate>Wed, 09 Oct 2024 17:47:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13684v2</guid></item><item><title>Personalized Visual Instruction Tuning</title><link>http://arxiv.org/abs/2410.07113v1</link><description>Recent advancements in multimodal large language models (MLLMs) havedemonstrated significant progress; however, these models exhibit a notablelimitation, which we refer to as "face blindness". Specifically, they canengage in general conversations but fail to conduct personalized dialoguestargeting at specific individuals. This deficiency hinders the application ofMLLMs in personalized settings, such as tailored visual assistants on mobiledevices, or domestic robots that need to recognize members of the family. Inthis paper, we introduce Personalized Visual Instruction Tuning (PVIT), a noveldata curation and training framework designed to enable MLLMs to identifytarget individuals within an image and engage in personalized and coherentdialogues. Our approach involves the development of a sophisticated pipelinethat autonomously generates training data containing personalizedconversations. This pipeline leverages the capabilities of various visualexperts, image generation models, and (multi-modal) large language models. Toevaluate the personalized potential of MLLMs, we present a benchmark calledP-Bench, which encompasses various question types with different levels ofdifficulty. The experiments demonstrate a substantial personalized performanceenhancement after fine-tuning with our curated dataset.</description><author>Renjie Pi, Jianshu Zhang, Tianyang Han, Jipeng Zhang, Rui Pan, Tong Zhang</author><pubDate>Wed, 09 Oct 2024 17:46:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07113v1</guid></item><item><title>VHELM: A Holistic Evaluation of Vision Language Models</title><link>http://arxiv.org/abs/2410.07112v1</link><description>Current benchmarks for assessing vision-language models (VLMs) often focus ontheir perception or problem-solving capabilities and neglect other criticalaspects such as fairness, multilinguality, or toxicity. Furthermore, theydiffer in their evaluation procedures and the scope of the evaluation, makingit difficult to compare models. To address these issues, we extend the HELMframework to VLMs to present the Holistic Evaluation of Vision Language Models(VHELM). VHELM aggregates various datasets to cover one or more of the 9aspects: visual perception, knowledge, reasoning, bias, fairness,multilinguality, robustness, toxicity, and safety. In doing so, we produce acomprehensive, multi-dimensional view of the capabilities of the VLMs acrossthese important factors. In addition, we standardize the standard inferenceparameters, methods of prompting, and evaluation metrics to enable faircomparisons across models. Our framework is designed to be lightweight andautomatic so that evaluation runs are cheap and fast. Our initial run evaluates22 VLMs on 21 existing datasets to provide a holistic snapshot of the models.We uncover new key findings, such as the fact that efficiency-focused models(e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse thantheir full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmarkbut not when evaluated on the other aspects. For transparency, we release theraw model generations and complete results on our website(https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a livingbenchmark, and we hope to continue adding new datasets and models over time.</description><author>Tony Lee, Haoqin Tu, Chi Heem Wong, Wenhao Zheng, Yiyang Zhou, Yifan Mai, Josselin Somerville Roberts, Michihiro Yasunaga, Huaxiu Yao, Cihang Xie, Percy Liang</author><pubDate>Wed, 09 Oct 2024 17:46:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07112v1</guid></item><item><title>Continual Learning: Less Forgetting, More OOD Generalization via Adaptive Contrastive Replay</title><link>http://arxiv.org/abs/2410.07110v1</link><description>Machine learning models often suffer from catastrophic forgetting ofpreviously learned knowledge when learning new classes. Various methods havebeen proposed to mitigate this issue. However, rehearsal-based learning, whichretains samples from previous classes, typically achieves good performance buttends to memorize specific instances, struggling with Out-of-Distribution (OOD)generalization. This often leads to high forgetting rates and poorgeneralization. Surprisingly, the OOD generalization capabilities of thesemethods have been largely unexplored. In this paper, we highlight this issueand propose a simple yet effective strategy inspired by contrastive learningand data-centric principles to address it. We introduce Adaptive ContrastiveReplay (ACR), a method that employs dual optimization to simultaneously trainboth the encoder and the classifier. ACR adaptively populates the replay bufferwith misclassified samples while ensuring a balanced representation of classesand tasks. By refining the decision boundary in this way, ACR achieves abalance between stability and plasticity. Our method significantly outperformsprevious approaches in terms of OOD generalization, achieving an improvement of13.41\% on Split CIFAR-100, 9.91\% on Split Mini-ImageNet, and 5.98\% on SplitTiny-ImageNet.</description><author>Hossein Rezaei, Mohammad Sabokrou</author><pubDate>Wed, 09 Oct 2024 17:45:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07110v1</guid></item><item><title>I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy</title><link>http://arxiv.org/abs/2410.07109v1</link><description>As Large Language Model (LLM)-based agents become increasingly autonomous andwill more freely interact with each other, studying interactions between thembecomes crucial to anticipate emergent phenomena and potential risks. Drawinginspiration from the widely popular Stanford Prison Experiment, we contributeto this line of research by studying interaction patterns of LLM agents in acontext characterized by strict social hierarchy. We do so by specificallystudying two types of phenomena: persuasion and anti-social behavior insimulated scenarios involving a guard and a prisoner agent who seeks to achievea specific goal (i.e., obtaining additional yard time or escape from prison).Leveraging 200 experimental scenarios for a total of 2,000 machine-machineconversations across five different popular LLMs, we provide a set ofnoteworthy findings. We first document how some models consistently fail incarrying out a conversation in our multi-agent setup where power dynamics areat play. Then, for the models that were able to engage in successfulinteractions, we empirically show how the goal that an agent is set to achieveimpacts primarily its persuasiveness, while having a negligible effect withrespect to the agent's anti-social behavior. Third, we highlight how agents'personas, and particularly the guard's personality, drive both the likelihoodof successful persuasion from the prisoner and the emergence of anti-socialbehaviors. Fourth, we show that even without explicitly prompting for specificpersonalities, anti-social behavior emerges by simply assigning agents' roles.These results bear implications for the development of interactive LLM agentsas well as the debate on their societal impact.</description><author>Gian Maria Campedelli, Nicolò Penzo, Massimo Stefan, Roberto Dessì, Marco Guerini, Bruno Lepri, Jacopo Staiano</author><pubDate>Wed, 09 Oct 2024 17:45:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07109v1</guid></item><item><title>Private prediction for large-scale synthetic text generation</title><link>http://arxiv.org/abs/2407.12108v2</link><description>We present an approach for generating differentially private synthetic textusing large language models (LLMs), via private prediction. In the privateprediction framework, we only require the output synthetic data to satisfydifferential privacy guarantees. This is in contrast to approaches that train agenerative model on potentially sensitive user-supplied source data and seek toensure the model itself is safe to release. We prompt a pretrained LLM with source data, but ensure that next-tokenpredictions are made with differential privacy guarantees. Previous work inthis paradigm reported generating a small number of examples (&lt;10) atreasonable privacy levels, an amount of data that is useful only for downstreamin-context learning or prompting. In contrast, we make changes that allow us togenerate thousands of high-quality synthetic data points, greatly expanding theset of potential applications. Our improvements come from an improved privacyanalysis and a better private selection mechanism, which makes use of theequivalence between the softmax layer for sampling tokens in LLMs and theexponential mechanism. Furthermore, we introduce a novel use of publicpredictions via the sparse vector technique, in which we do not pay privacycosts for tokens that are predictable without sensitive data; we find this tobe particularly effective for structured data.</description><author>Kareem Amin, Alex Bie, Weiwei Kong, Alexey Kurakin, Natalia Ponomareva, Umar Syed, Andreas Terzis, Sergei Vassilvitskii</author><pubDate>Wed, 09 Oct 2024 17:45:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12108v2</guid></item><item><title>Topologically Faithful Multi-class Segmentation in Medical Images</title><link>http://arxiv.org/abs/2403.11001v2</link><description>Topological accuracy in medical image segmentation is a highly importantproperty for downstream applications such as network analysis and flow modelingin vessels or cell counting. Recently, significant methodological advancementshave brought well-founded concepts from algebraic topology to binarysegmentation. However, these approaches have been underexplored in multi-classsegmentation scenarios, where topological errors are common. We propose ageneral loss function for topologically faithful multi-class segmentationextending the recent Betti matching concept, which is based on inducedmatchings of persistence barcodes. We project the N-class segmentation problemto N single-class segmentation tasks, which allows us to use 1-parameterpersistent homology, making training of neural networks computationallyfeasible. We validate our method on a comprehensive set of four medicaldatasets with highly variant topological characteristics. Our loss formulationsignificantly enhances topological correctness in cardiac, cell, artery-vein,and Circle of Willis segmentation.</description><author>Alexander H. Berger, Nico Stucki, Laurin Lux, Vincent Buergin, Suprosanna Shit, Anna Banaszak, Daniel Rueckert, Ulrich Bauer, Johannes C. Paetzold</author><pubDate>Wed, 09 Oct 2024 17:44:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11001v2</guid></item><item><title>Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context</title><link>http://arxiv.org/abs/2410.07103v1</link><description>Multi-hop reasoning, which requires multi-step reasoning based on thesupporting documents within a given context, remains challenging for largelanguage models (LLMs). LLMs often struggle to filter out irrelevant documentswithin the context, and their performance is sensitive to the position ofsupporting documents within that context. In this paper, we identify anadditional challenge: LLMs' performance is also sensitive to the order in whichthe supporting documents are presented. We refer to this as the misorderedcontext problem. To address this issue, we propose a simple yet effectivemethod called context repetition (CoRe), which involves prompting the model byrepeatedly presenting the context to ensure the supporting documents arepresented in the optimal order for the model. Using CoRe, we improve the F1score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%pon a synthetic task. Additionally, CoRe helps mitigate the well-known"lost-in-the-middle" problem in LLMs and can be effectively combined withretrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.</description><author>Sangwon Yu, Ik-hwan Kim, Jongyoon Song, Saehyung Lee, Junsung Park, Sungroh Yoon</author><pubDate>Wed, 09 Oct 2024 17:41:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07103v1</guid></item><item><title>DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data Mining</title><link>http://arxiv.org/abs/2410.00260v2</link><description>Large Language Models (LLMs) have shown remarkable ability to generalizeeffectively across numerous industry domains while executing a range of tasks.Many of these competencies are obtained from the data utilized during thepre-training phase of the Language Models (LMs). However, these models exhibitlimitations when tasked with performing in specialized or low-resource industrydomains. More recent approaches use LLMs for generating domain-specificsynthetic data but most often they lack in truthfulness and complexity.Alternatively, in cases where domain data is available like healthcare andfinance most of the LMs are proprietary necessitating the need for a scalablemethod to curate real world industry specific pre-training data. In this work,we propose an automated and scalable framework - DoPAMine:Domain-specificPre-training Adaptation from seed-guided data Mining, to mine domain specifictraining data from a large data corpus for domain adaptation of a LM. Theframework leverages the parametric knowledge of a LLM to generate diverse andrepresentative seed data tailored to a specific domain which is then used tomine real world data from a large data corpus like Common Crawl. We evaluatedour framework's performance in the continual pre-training (CPT) setting bytraining two domain specific 7B parameter LMs in healthcare and finance withdata mined via DoPAMine. Our experiments show that DoPAMine boosts theperformance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and5-shot settings respectively on healthcare tasks from MMLU, MedQA, MedMCQA andPubMedQA datasets, and 2.9% and 6.7% for zero-shot and 5-shot settingsrespectively on finance tasks from FiQA-SA, FPB and Headlines datasets whencompared to the baseline.</description><author>Vinayak Arannil, Neha Narwal, Sourav Sanjukta Bhabesh, Sai Nikhil Thirandas, Darren Yow-Bang Wang, Graham Horwood, Alex Anto Chirayath, Gouri Pandeshwar</author><pubDate>Wed, 09 Oct 2024 17:39:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.00260v2</guid></item><item><title>Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings</title><link>http://arxiv.org/abs/2401.06112v3</link><description>Word embedding is one of the most important components in natural languageprocessing, but interpreting high-dimensional embeddings remains a challengingproblem. To address this problem, Independent Component Analysis (ICA) isidentified as an effective solution. ICA-transformed word embeddings revealinterpretable semantic axes; however, the order of these axes are arbitrary. Inthis study, we focus on this property and propose a novel method, Axis Tour,which optimizes the order of the axes. Inspired by Word Tour, a one-dimensionalword embedding method, we aim to improve the clarity of the word embeddingspace by maximizing the semantic continuity of the axes. Furthermore, we showthrough experiments on downstream tasks that Axis Tour yields better orcomparable low-dimensional embeddings compared to both PCA and ICA.</description><author>Hiroaki Yamagiwa, Yusuke Takase, Hidetoshi Shimodaira</author><pubDate>Wed, 09 Oct 2024 17:38:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06112v3</guid></item><item><title>Identifying and Addressing Delusions for Target-Directed Decision-Making</title><link>http://arxiv.org/abs/2410.07096v1</link><description>We are interested in target-directed agents, which produce targets duringdecision-time planning, to guide their behaviors and achieve bettergeneralization during evaluation. Improper training of these agents can resultin delusions: the agent may come to hold false beliefs about the targets, whichcannot be properly rejected, leading to unwanted behaviors and damagingout-of-distribution generalization. We identify different types of delusions byusing intuitive examples in carefully controlled environments, and investigatetheir causes. We demonstrate how delusions can be addressed for agents trainedby hindsight relabeling, a mainstream approach in for training target-directedRL agents. We validate empirically the effectiveness of the proposed solutionsin correcting delusional behaviors and improving out-of-distributiongeneralization.</description><author>Mingde Zhao, Tristan Sylvain, Doina Precup, Yoshua Bengio</author><pubDate>Wed, 09 Oct 2024 17:35:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07096v1</guid></item><item><title>MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering</title><link>http://arxiv.org/abs/2410.07095v1</link><description>We introduce MLE-bench, a benchmark for measuring how well AI agents performat machine learning engineering. To this end, we curate 75 MLengineering-related competitions from Kaggle, creating a diverse set ofchallenging tasks that test real-world ML engineering skills such as trainingmodels, preparing datasets, and running experiments. We establish humanbaselines for each competition using Kaggle's publicly available leaderboards.We use open-source agent scaffolds to evaluate several frontier language modelson our benchmark, finding that the best-performing setup--OpenAI's o1-previewwith AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in16.9% of competitions. In addition to our main results, we investigate variousforms of resource scaling for AI agents and the impact of contamination frompre-training. We open-source our benchmark code (github.com/openai/mle-bench/)to facilitate future research in understanding the ML engineering capabilitiesof AI agents.</description><author>Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, Aleksander Mądry</author><pubDate>Wed, 09 Oct 2024 17:34:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07095v1</guid></item><item><title>An Approach for Auto Generation of Labeling Functions for Software Engineering Chatbots</title><link>http://arxiv.org/abs/2410.07094v1</link><description>Software engineering (SE) chatbots are increasingly gaining attention fortheir role in enhancing development processes. At the core of chatbots are theNatural Language Understanding platforms (NLUs), which enable them tocomprehend and respond to user queries. Before deploying NLUs, there is a needto train them with labeled data. However, acquiring such labeled data for SEchatbots is challenging due to the scarcity of high-quality datasets. Thischallenge arises because training SE chatbots requires specialized vocabularyand phrases not found in typical language datasets. Consequently, chatbotdevelopers often resort to manually annotating user queries to gather the datanecessary for training effective chatbots, a process that is bothtime-consuming and resource-intensive. Previous studies propose approaches tosupport chatbot practitioners in annotating users' posed queries. However,these approaches require human intervention to generate rules, called labelingfunctions (LFs), that identify and categorize user queries based on specificpatterns in the data. To address this issue, we propose an approach toautomatically generate LFs by extracting patterns from labeled user queries. Weevaluate the effectiveness of our approach by applying it to the queries offour diverse SE datasets (namely AskGit, MSA, Ask Ubuntu, and Stack Overflow)and measure the performance improvement gained from training the NLU on thequeries labeled by the generated LFs. We find that the generated LFseffectively label data with AUC scores of up to 85.3%, and NLU's performanceimprovement of up to 27.2% across the studied datasets. Furthermore, ourresults show that the number of LFs used to generate LFs affects the labelingperformance. We believe that our approach can save time and resources inlabeling users' queries, allowing practitioners to focus on core chatbotfunctionalities.</description><author>Ebube Alor, Ahmad Abdellatif, SayedHassan Khatoonabadi, Emad Shihab</author><pubDate>Wed, 09 Oct 2024 17:34:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07094v1</guid></item><item><title>LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning</title><link>http://arxiv.org/abs/2410.07093v1</link><description>Language plays a vital role in the realm of human motion. Existing methodshave largely depended on CLIP text embeddings for motion generation, yet theyfall short in effectively aligning language and motion due to CLIP'spretraining on static image-text pairs. This work introduces LaMP, a novelLanguage-Motion Pretraining model, which transitions from a language-vision toa more suitable language-motion latent space. It addresses key limitations bygenerating motion-informative text embeddings, significantly enhancing therelevance and semantics of generated motion sequences. With LaMP, we advancethree key tasks: text-to-motion generation, motion-text retrieval, and motioncaptioning through aligned language-motion representation learning. Forgeneration, we utilize LaMP to provide the text condition instead of CLIP, andan autoregressive masked prediction is designed to achieve mask modelingwithout rank collapse in transformers. For retrieval, motion features fromLaMP's motion transformer interact with query tokens to retrieve text featuresfrom the text transformer, and vice versa. For captioning, we finetune a largelanguage model with the language-informative motion features to develop astrong motion captioning model. In addition, we introduce the LaMP-BertScoremetric to assess the alignment of generated motions with textual descriptions.Extensive experimental results on multiple datasets demonstrate substantialimprovements over previous methods across all three tasks. The code of ourmethod will be made public.</description><author>Zhe Li, Weihao Yuan, Yisheng He, Lingteng Qiu, Shenhao Zhu, Xiaodong Gu, Weichao Shen, Yuan Dong, Zilong Dong, Laurence T. Yang</author><pubDate>Wed, 09 Oct 2024 17:33:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07093v1</guid></item><item><title>Collusion Detection with Graph Neural Networks</title><link>http://arxiv.org/abs/2410.07091v1</link><description>Collusion is a complex phenomenon in which companies secretly collaborate toengage in fraudulent practices. This paper presents an innovative methodologyfor detecting and predicting collusion patterns in different national marketsusing neural networks (NNs) and graph neural networks (GNNs). GNNs areparticularly well suited to this task because they can exploit the inherentnetwork structures present in collusion and many other economic problems. Ourapproach consists of two phases: In Phase I, we develop and train models onindividual market datasets from Japan, the United States, two regions inSwitzerland, Italy, and Brazil, focusing on predicting collusion in singlemarkets. In Phase II, we extend the models' applicability through zero-shotlearning, employing a transfer learning approach that can detect collusion inmarkets in which training data is unavailable. This phase also incorporatesout-of-distribution (OOD) generalization to evaluate the models' performance onunseen datasets from other countries and regions. In our empirical study, weshow that GNNs outperform NNs in detecting complex collusive patterns. Thisresearch contributes to the ongoing discourse on preventing collusion andoptimizing detection methodologies, providing valuable guidance on the use ofNNs and GNNs in economic applications to enhance market fairness and economicwelfare.</description><author>Lucas Gomes, Jannis Kueck, Mara Mattes, Martin Spindler, Alexey Zaytsev</author><pubDate>Wed, 09 Oct 2024 17:31:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07091v1</guid></item><item><title>Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology</title><link>http://arxiv.org/abs/2410.07087v1</link><description>Developing agents capable of navigating to a target location based onlanguage instructions and visual information, known as vision-languagenavigation (VLN), has attracted widespread interest. Most research has focusedon ground-based agents, while UAV-based VLN remains relatively underexplored.Recent efforts in UAV vision-language navigation predominantly adoptground-based VLN settings, relying on predefined discrete action spaces andneglecting the inherent disparities in agent movement dynamics and thecomplexity of navigation tasks between ground and aerial environments. Toaddress these disparities and challenges, we propose solutions from threeperspectives: platform, benchmark, and methodology. To enable realistic UAVtrajectory simulation in VLN tasks, we propose the OpenUAV platform, whichfeatures diverse environments, realistic flight control, and extensivealgorithmic support. We further construct a target-oriented VLN datasetconsisting of approximately 12k trajectories on this platform, serving as thefirst dataset specifically designed for realistic UAV VLN tasks. To tackle thechallenges posed by complex aerial environments, we propose an assistant-guidedUAV object search benchmark called UAV-Need-Help, which provides varying levelsof guidance information to help UAVs better accomplish realistic VLN tasks. Wealso propose a UAV navigation LLM that, given multi-view images, taskdescriptions, and assistant instructions, leverages the multimodalunderstanding capabilities of the MLLM to jointly process visual and textualinformation, and performs hierarchical trajectory generation. The evaluationresults of our method significantly outperform the baseline models, while thereremains a considerable gap between our results and those achieved by humanoperators, underscoring the challenge presented by the UAV-Need-Help task.</description><author>Xiangyu Wang, Donglin Yang, Ziqin Wang, Hohin Kwan, Jinyu Chen, Wenjun Wu, Hongsheng Li, Yue Liao, Si Liu</author><pubDate>Wed, 09 Oct 2024 17:29:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07087v1</guid></item><item><title>Comprehensive Performance Evaluation of YOLO11, YOLOv10, YOLOv9 and YOLOv8 on Detecting and Counting Fruitlet in Complex Orchard Environments</title><link>http://arxiv.org/abs/2407.12040v4</link><description>This study extensively evaluated You Only Look Once (YOLO) object detectionalgorithms across all configurations (total 22) of YOLOv8, YOLOv9, YOLOv10, andYOLO11 for green fruit detection in commercial orchards. The research alsovalidated in-field fruitlet counting using an iPhone and machine vision sensorsacross four apple varieties: Scifresh, Scilate, Honeycrisp and Cosmic Crisp.Among the 22 configurations evaluated, YOLO11s and YOLOv9 gelan-baseoutperformed others with mAP@50 scores of 0.933 and 0.935 respectively. Interms of recall, YOLOv9 gelan-base achieved the highest value among YOLOv9configurations at 0.899, while YOLO11m led YOLO11 variants with 0.897. YOLO11nemerged as the fastest model, achieving fastest inference speed of only 2.4 ms,significantly outpacing the leading configurations of YOLOv10n, YOLOv9 gelan-s,and YOLOv8n, with speeds of 5.5, 11.5, and 4.1 ms, respectively. Thiscomparative evaluation highlights the strengths of YOLO11, YOLOv9, and YOLOv10,offering researchers essential insights to choose the best-suited model forfruitlet detection and possible automation in commercial orchards. Forreal-time automation related work in relevant datasets, we recommend usingYOLO11n due to its high detection and image processing speed. Keywords: YOLO11,YOLO11 Object Detection, YOLOv10, YOLOv9, YOLOv8, You Only Look Once, FruitletDetection, Greenfruit Detection, Green Apple Detection, AgriculturalAutomation, Artificial Intelligence, Deep Learning, Machine Learning, Zero-shotDetection</description><author>Ranjan Sapkota, Zhichao Meng, Martin Churuvija, Xiaoqiang Du, Zenghong Ma, Manoj Karkee</author><pubDate>Wed, 09 Oct 2024 17:28:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12040v4</guid></item><item><title>Stanceformer: Target-Aware Transformer for Stance Detection</title><link>http://arxiv.org/abs/2410.07083v1</link><description>The task of Stance Detection involves discerning the stance expressed in atext towards a specific subject or target. Prior works have relied on existingtransformer models that lack the capability to prioritize targets effectively.Consequently, these models yield similar performance regardless of whether weutilize or disregard target information, undermining the task's significance.To address this challenge, we introduce Stanceformer, a target-awaretransformer model that incorporates enhanced attention towards the targetsduring both training and inference. Specifically, we design a \textit{TargetAwareness} matrix that increases the self-attention scores assigned to thetargets. We demonstrate the efficacy of the Stanceformer with variousBERT-based models, including state-of-the-art models and Large Language Models(LLMs), and evaluate its performance across three stance detection datasets,alongside a zero-shot dataset. Our approach Stanceformer not only providessuperior performance but also generalizes even to other domains, such asAspect-based Sentiment Analysis. We make the code publiclyavailable.\footnote{\scriptsize\url{https://github.com/kgarg8/Stanceformer}}</description><author>Krishna Garg, Cornelia Caragea</author><pubDate>Wed, 09 Oct 2024 17:24:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07083v1</guid></item><item><title>JPEG Inspired Deep Learning</title><link>http://arxiv.org/abs/2410.07081v1</link><description>Although it is traditionally believed that lossy image compression, such asJPEG compression, has a negative impact on the performance of deep neuralnetworks (DNNs), it is shown by recent works that well-crafted JPEG compressioncan actually improve the performance of deep learning (DL). Inspired by this,we propose JPEG-DL, a novel DL framework that prepends any underlying DNNarchitecture with a trainable JPEG compression layer. To make the quantizationoperation in JPEG compression trainable, a new differentiable soft quantizer isemployed at the JPEG layer, and then the quantization operation and underlyingDNN are jointly trained. Extensive experiments show that in comparison with thestandard DL, JPEG-DL delivers significant accuracy improvements across variousdatasets and model architectures while enhancing robustness against adversarialattacks. Particularly, on some fine-grained image classification datasets,JPEG-DL can increase prediction accuracy by as much as 20.9%. Our code isavailable on https://github.com/JpegInspiredDl/JPEG-Inspired-DL.git.</description><author>Ahmed H. Salamah, Kaixiang Zheng, Yiwen Liu, En-Hui Yang</author><pubDate>Wed, 09 Oct 2024 17:23:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07081v1</guid></item><item><title>MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses</title><link>http://arxiv.org/abs/2410.07076v1</link><description>Scientific discovery contributes largely to human society's prosperity, andrecent progress shows that LLMs could potentially catalyze this process.However, it is still unclear whether LLMs can discover novel and validhypotheses in chemistry. In this work, we investigate this central researchquestion: Can LLMs automatically discover novel and valid chemistry researchhypotheses given only a chemistry research background (consisting of a researchquestion and/or a background survey), without limitation on the domain of theresearch question? After extensive discussions with chemistry experts, wepropose an assumption that a majority of chemistry hypotheses can be resultedfrom a research background and several inspirations. With this key insight, webreak the central question into three smaller fundamental questions. In brief,they are: (1) given a background question, whether LLMs can retrieve goodinspirations; (2) with background and inspirations, whether LLMs can lead tohypothesis; and (3) whether LLMs can identify good hypotheses to rank themhigher. To investigate these questions, we construct a benchmark consisting of51 chemistry papers published in Nature, Science, or a similar level in 2024(all papers are only available online since 2024). Every paper is divided bychemistry PhD students into three components: background, inspirations, andhypothesis. The goal is to rediscover the hypothesis, given only the backgroundand a large randomly selected chemistry literature corpus consisting the groundtruth inspiration papers, with LLMs trained with data up to 2023. We alsodevelop an LLM-based multi-agent framework that leverages the assumption,consisting of three stages reflecting the three smaller questions. The proposedmethod can rediscover many hypotheses with very high similarity with the groundtruth ones, covering the main innovations.</description><author>Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou</author><pubDate>Wed, 09 Oct 2024 17:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07076v1</guid></item><item><title>ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using LLMs</title><link>http://arxiv.org/abs/2401.14279v2</link><description>Technical Q&amp;A sites are valuable for software developers seeking knowledge,but the code snippets they provide are often uncompilable and incomplete due tounresolved types and missing libraries. This poses a challenge for users whowish to reuse or analyze these snippets. Existing methods either do not focuson creating compilable code or have low success rates. To address this, wepropose ZS4C, a lightweight approach for zero-shot synthesis of compilable codefrom incomplete snippets using Large Language Models (LLMs). ZS4C operates intwo stages: first, it uses an LLM, like GPT-3.5, to identify missing importstatements in a snippet; second, it collaborates with a validator (e.g.,compiler) to fix compilation errors caused by incorrect imports and syntaxissues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset,Python-SO, which includes 539 Python snippets from Stack Overflow across the 20most popular Python libraries. ZS4C significantly outperforms existing methods,improving the compilation rate from 63% to 95.1% compared to thestate-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infermore accurate import statements (with an F1 score of 0.98) than SnR, with animprovement of 8.5% in the F1.</description><author>Azmain Kabir, Shaowei Wang, Yuan Tian, Tse-Hsun Chen, Muhammad Asaduzzaman, Wenbin Zhang</author><pubDate>Wed, 09 Oct 2024 17:19:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14279v2</guid></item><item><title>Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning</title><link>http://arxiv.org/abs/2410.07074v1</link><description>Textual Attributed Graphs (TAGs) are crucial for modeling complex real-worldsystems, yet leveraging large language models (LLMs) for TAGs presents uniquechallenges due to the gap between sequential text processing andgraph-structured data. We introduce AskGNN, a novel approach that bridges thisgap by leveraging In-Context Learning (ICL) to integrate graph data andtask-specific information into LLMs. AskGNN employs a Graph Neural Network(GNN)-powered structure-enhanced retriever to select labeled nodes acrossgraphs, incorporating complex graph structures and their supervision signals.Our learning-to-retrieve algorithm optimizes the retriever to select examplenodes that maximize LLM performance on graph. Experiments across three tasksand seven LLMs demonstrate AskGNN's superior effectiveness in graph taskperformance, opening new avenues for applying LLMs to graph-structured datawithout extensive fine-tuning.</description><author>Zhengyu Hu, Yichuan Li, Zhengyu Chen, Jingang Wang, Han Liu, Kyumin Lee, Kaize Ding</author><pubDate>Wed, 09 Oct 2024 17:19:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07074v1</guid></item><item><title>Pixtral 12B</title><link>http://arxiv.org/abs/2410.07073v1</link><description>We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.Pixtral-12B is trained to understand both natural images and documents,achieving leading performance on various multimodal benchmarks, surpassing anumber of larger models. Unlike many open-source models, Pixtral is also acutting-edge text model for its size, and does not compromise on naturallanguage performance to excel in multimodal tasks. Pixtral uses a new visionencoder trained from scratch, which allows it to ingest images at their naturalresolution and aspect ratio. This gives users flexibility on the number oftokens used to process an image. Pixtral is also able to process any number ofimages in its long context window of 128K tokens. Pixtral 12B substaniallyoutperforms other open models of similar sizes (Llama-3.2 11B \&amp; Qwen-2-VL 7B).It also outperforms much larger open models like Llama-3.2 90B while being 7xsmaller. We further contribute an open-source benchmark, MM-MT-Bench, forevaluating vision-language models in practical scenarios, and provide detailedanalysis and code for standardized evaluation protocols for multimodal LLMs.Pixtral-12B is released under Apache 2.0 license.</description><author>Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, Albert Q. Jiang, Timothée Lacroix, Guillaume Lample, Diego Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall, Louis Martin, Arthur Mensch, Pavankumar Muddireddy, Valera Nemychnikova, Marie Pellat, Patrick Von Platen, Nikhil Raghuraman, Baptiste Rozière, Alexandre Sablayrolles, Lucile Saulnier, Romain Sauvestre, Wendy Shang, Roman Soletskyi, Lawrence Stewart, Pierre Stock, Joachim Studnia, Sandeep Subramanian, Sagar Vaze, Thomas Wang</author><pubDate>Wed, 09 Oct 2024 17:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07073v1</guid></item><item><title>Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation</title><link>http://arxiv.org/abs/2404.06809v3</link><description>The rapid development of large language models has led to the widespreadadoption of Retrieval-Augmented Generation (RAG), which integrates externalknowledge to alleviate knowledge bottlenecks and mitigate hallucinations.However, the existing RAG paradigm inevitably suffers from the impact of flawedinformation introduced during the retrieval phrase, thereby diminishing thereliability and correctness of the generated outcomes. In this paper, wepropose Credibility-aware Generation (CAG), a universally applicable frameworkdesigned to mitigate the impact of flawed information in RAG. At its core, CAGaims to equip models with the ability to discern and process information basedon its credibility. To this end, we propose an innovative data transformationframework that generates data based on credibility, thereby effectivelyendowing models with the capability of CAG. Furthermore, to accurately evaluatethe models' capabilities of CAG, we construct a comprehensive benchmarkcovering three critical real-world scenarios. Experimental results demonstratethat our model can effectively understand and utilize credibility forgeneration, significantly outperform other models with retrieval augmentation,and exhibit resilience against the disruption caused by noisy documents,thereby maintaining robust performance. Moreover, our model supports customizedcredibility, offering a wide range of potential applications.</description><author>Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang, Xunliang Cai, Le Sun</author><pubDate>Wed, 09 Oct 2024 17:16:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06809v3</guid></item><item><title>Towards xAI: Configuring RNN Weights using Domain Knowledge for MIMO Receive Processing</title><link>http://arxiv.org/abs/2410.07072v1</link><description>Deep learning is making a profound impact in the physical layer of wirelesscommunications. Despite exhibiting outstanding empirical performance in taskssuch as MIMO receive processing, the reasons behind the demonstrated superiorperformance improvement remain largely unclear. In this work, we advance thefield of Explainable AI (xAI) in the physical layer of wireless communicationsutilizing signal processing principles. Specifically, we focus on the task ofMIMO-OFDM receive processing (e.g., symbol detection) using reservoir computing(RC), a framework within recurrent neural networks (RNNs), which outperformsboth conventional and other learning-based MIMO detectors. Our analysisprovides a signal processing-based, first-principles understanding of thecorresponding operation of the RC. Building on this fundamental understanding,we are able to systematically incorporate the domain knowledge of wirelesssystems (e.g., channel statistics) into the design of the underlying RNN bydirectly configuring the untrained RNN weights for MIMO-OFDM symbol detection.The introduced RNN weight configuration has been validated through extensivesimulations demonstrating significant performance improvements. Thisestablishes a foundation for explainable RC-based architectures in MIMO-OFDMreceive processing and provides a roadmap for incorporating domain knowledgeinto the design of neural networks for NextG systems.</description><author>Shashank Jere, Lizhong Zheng, Karim Said, Lingjia Liu</author><pubDate>Wed, 09 Oct 2024 17:16:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07072v1</guid></item><item><title>Retrieval-Augmented Decision Transformer: External Memory for In-context RL</title><link>http://arxiv.org/abs/2410.07071v1</link><description>In-context learning (ICL) is the ability of a model to learn a new task byobserving a few exemplars in its context. While prevalent in NLP, thiscapability has recently also been observed in Reinforcement Learning (RL)settings. Prior in-context RL methods, however, require entire episodes in theagent's context. Given that complex environments typically lead to longepisodes with sparse rewards, these methods are constrained to simpleenvironments with short episodes. To address these challenges, we introduceRetrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an externalmemory mechanism to store past experiences from which it retrieves onlysub-trajectories relevant for the current situation. The retrieval component inRA-DT does not require training and can be entirely domain-agnostic. Weevaluate the capabilities of RA-DT on grid-world environments, roboticssimulations, and procedurally-generated video games. On grid-worlds, RA-DToutperforms baselines, while using only a fraction of their context length.Furthermore, we illuminate the limitations of current in-context RL methods oncomplex environments and discuss future directions. To facilitate futureresearch, we release datasets for four of the considered environments.</description><author>Thomas Schmied, Fabian Paischer, Vihang Patil, Markus Hofmarcher, Razvan Pascanu, Sepp Hochreiter</author><pubDate>Wed, 09 Oct 2024 17:15:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07071v1</guid></item><item><title>ReIFE: Re-evaluating Instruction-Following Evaluation</title><link>http://arxiv.org/abs/2410.07069v1</link><description>The automatic evaluation of instruction following typically involves usinglarge language models (LLMs) to assess response quality. However, there is alack of comprehensive evaluation of these LLM-based evaluators across twodimensions: the base LLMs and the evaluation protocols. Therefore, we present athorough meta-evaluation of instruction following, including 25 base LLMs and15 recently proposed evaluation protocols, on 4 human-annotated datasets,assessing the evaluation accuracy of the LLM-evaluators. Our evaluation allowsus to identify the best-performing base LLMs and evaluation protocols with ahigh degree of robustness. Moreover, our large-scale evaluation reveals: (1)Base LLM performance ranking remains largely consistent across evaluationprotocols, with less capable LLMs showing greater improvement from protocolenhancements; (2) Robust evaluation of evaluation protocols requires many baseLLMs with varying capability levels, as protocol effectiveness can depend onthe base LLM used; (3) Evaluation results on different datasets are not alwaysconsistent, so a rigorous evaluation requires multiple datasets withdistinctive features. We release our meta-evaluation suite ReIFE, whichprovides the codebase and evaluation result collection for more than 500LLM-evaluator configurations, to support future research ininstruction-following evaluation.</description><author>Yixin Liu, Kejian Shi, Alexander R. Fabbri, Yilun Zhao, Peifeng Wang, Chien-Sheng Wu, Shafiq Joty, Arman Cohan</author><pubDate>Wed, 09 Oct 2024 17:14:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07069v1</guid></item><item><title>Counterfactuals As a Means for Evaluating Faithfulness of Attribution Methods in Autoregressive Language Models</title><link>http://arxiv.org/abs/2408.11252v3</link><description>Despite the widespread adoption of autoregressive language models,explainability evaluation research has predominantly focused on span infillingand masked language models. Evaluating the faithfulness of an explanationmethod -- how accurately it explains the inner workings and decision-making ofthe model -- is challenging because it is difficult to separate the model fromits explanation. Most faithfulness evaluation techniques corrupt or removeinput tokens deemed important by a particular attribution (feature importance)method and observe the resulting change in the model's output. However, forautoregressive language models, this approach creates out-of-distributioninputs due to their next-token prediction training objective. In this study, wepropose a technique that leverages counterfactual generation to evaluate thefaithfulness of attribution methods for autoregressive language models. Ourtechnique generates fluent, in-distribution counterfactuals, making theevaluation protocol more reliable.</description><author>Sepehr Kamahi, Yadollah Yaghoobzadeh</author><pubDate>Wed, 09 Oct 2024 17:12:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11252v3</guid></item><item><title>A Gentle Introduction and Tutorial on Deep Generative Models in Transportation Research</title><link>http://arxiv.org/abs/2410.07066v1</link><description>Deep Generative Models (DGMs) have rapidly advanced in recent years, becomingessential tools in various fields due to their ability to learn complex datadistributions and generate synthetic data. Their importance in transportationresearch is increasingly recognized, particularly for applications like trafficdata generation, prediction, and feature extraction. This paper offers acomprehensive introduction and tutorial on DGMs, with a focus on theirapplications in transportation. It begins with an overview of generativemodels, followed by detailed explanations of fundamental models, a systematicreview of the literature, and practical tutorial code to aid implementation.The paper also discusses current challenges and opportunities, highlighting howthese models can be effectively utilized and further developed intransportation research. This paper serves as a valuable reference, guidingresearchers and practitioners from foundational knowledge to advancedapplications of DGMs in transportation research.</description><author>Seongjin Choi, Zhixiong Jin, Seungwoo Ham, Jiwon Kim, Lijun Sun</author><pubDate>Wed, 09 Oct 2024 17:11:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07066v1</guid></item><item><title>Population Transformer: Learning Population-level Representations of Neural Activity</title><link>http://arxiv.org/abs/2406.03044v2</link><description>We present a self-supervised framework that learns population-level codes forarbitrary ensembles of neural recordings at scale. We address two keychallenges in scaling models with neural time-series data: sparse and variableelectrode distribution across subjects and datasets. The Population Transformer(PopT) stacks on top of pretrained representations and enhances downstreamdecoding by enabling learned aggregation of multiple spatially-sparse datachannels. The pretrained PopT lowers the amount of data required for downstreamdecoding experiments, while increasing accuracy, even on held-out subjects andtasks. Compared to end-to-end methods, this approach is computationallylightweight and more interpretable, while still retaining competitiveperformance. We further show how our framework is generalizable to multipletime-series embeddings and neural data modalities. Beyond decoding, weinterpret the pretrained PopT and fine-tuned models to show how they can beused to extract neuroscience insights from massive amounts of data. We releaseour code as well as a pretrained PopT to enable off-the-shelf improvements inmulti-channel intracranial data decoding and interpretability.</description><author>Geeling Chau, Christopher Wang, Sabera Talukder, Vighnesh Subramaniam, Saraswati Soedarmadji, Yisong Yue, Boris Katz, Andrei Barbu</author><pubDate>Wed, 09 Oct 2024 17:07:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03044v2</guid></item><item><title>Data Selection via Optimal Control for Language Models</title><link>http://arxiv.org/abs/2410.07064v1</link><description>This work investigates the selection of high-quality pre-training data frommassive corpora to enhance LMs' capabilities for downstream usage. We formulatedata selection as a generalized Optimal Control problem, which can be solvedtheoretically by Pontryagin's Maximum Principle (PMP), yielding a set ofnecessary conditions that characterize the relationship between optimal dataselection and LM training dynamics. Based on these theoretical results, weintroduce PMP-based Data Selection (PDS), a framework that approximates optimaldata selection by solving the PMP conditions. In our experiments, we adopt PDSto select data from CommmonCrawl and show that the PDS-selected corpusaccelerates the learning of LMs and constantly boosts their performance on awide range of downstream tasks across various model sizes. Moreover, thebenefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced bythe extrapolation of the test loss curves according to the Scaling Laws. PDSalso improves data utilization when the pre-training data is limited, byreducing the data demand by 1.8 times, which mitigates the quick exhaustion ofavailable web-crawled corpora. Our code, data, and model checkpoints can befound in https://github.com/microsoft/LMOps/tree/main/data_selection.</description><author>Yuxian Gu, Li Dong, Hongning Wang, Yaru Hao, Qingxiu Dong, Furu Wei, Minlie Huang</author><pubDate>Wed, 09 Oct 2024 17:06:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07064v1</guid></item><item><title>InAttention: Linear Context Scaling for Transformers</title><link>http://arxiv.org/abs/2410.07063v1</link><description>VRAM requirements for transformer models scale quadratically with contextlength due to the self-attention mechanism. In this paper we modify thedecoder-only transformer, replacing self-attention with InAttention, whichscales linearly with context length during inference by having tokens attendonly to initial states. Benchmarking shows that InAttention significantlyreduces VRAM usage during inference, enabling handling of long sequences onconsumer GPUs. We corroborate that fine-tuning extends context lengthefficiently, improving performance on long sequences without high trainingcosts. InAttention offers a scalable solution for long-range dependencies intransformer models, paving the way for further optimization.</description><author>Joseph Eisner</author><pubDate>Wed, 09 Oct 2024 17:05:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07063v1</guid></item><item><title>TinyEmo: Scaling down Emotional Reasoning via Metric Projection</title><link>http://arxiv.org/abs/2410.07062v1</link><description>This paper introduces TinyEmo, a family of small multi-modal language modelsfor emotional reasoning and classification. Our approach features: (1) asynthetic emotional instruct dataset for both pre-training and fine-tuningstages, (2) a Metric Projector that delegates classification from the languagemodel allowing for more efficient training and inference, (3) a multi-modallarge language model (MM-LLM) for emotional reasoning, and (4) a semi-automatedframework for bias detection. TinyEmo is able to perform emotion classificationand emotional reasoning, all while using substantially fewer parameters thancomparable models. This efficiency allows us to freely incorporate more diverseemotional datasets, enabling strong performance on classification tasks, withour smallest model (700M parameters) outperforming larger state-of-the-artmodels based on general-purpose MM-LLMs with over 7B parameters. Additionally,the Metric Projector allows for interpretability and indirect bias detection inlarge models without additional training, offering an approach to understandand improve AI systems. We release code, models, and dataset at https://github.com/ggcr/TinyEmo</description><author>Cristian Gutierrez</author><pubDate>Wed, 09 Oct 2024 17:03:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07062v1</guid></item><item><title>Online Epsilon Net and Piercing Set for Geometric Concepts</title><link>http://arxiv.org/abs/2410.07059v1</link><description>VC-dimension and $\varepsilon$-nets are key concepts in Statistical LearningTheory. Intuitively, VC-dimension is a measure of the size of a class of sets.The famous $\varepsilon$-net theorem, a fundamental result in DiscreteGeometry, asserts that if the VC-dimension of a set system is bounded, then asmall sample exists that intersects all sufficiently large sets. In online learning scenarios where data arrives sequentially, theVC-dimension helps to bound the complexity of the set system, and$\varepsilon$-nets ensure the selection of a small representative set. Thissampling framework is crucial in various domains, including spatial dataanalysis, motion planning in dynamic environments, optimization of sensornetworks, and feature extraction in computer vision, among others. Motivated bythese applications, we study the online $\varepsilon$-net problem for geometricconcepts with bounded VC-dimension. While the offline version of this problemhas been extensively studied, surprisingly, there are no known theoreticalresults for the online version to date. We present the first deterministiconline algorithm with an optimal competitive ratio for intervals in$\mathbb{R}$. Next, we give a randomized online algorithm with a near-optimalcompetitive ratio for axis-aligned boxes in $\mathbb{R}^d$, for $d\le 3$.Furthermore, we introduce a novel technique to analyze similar-sized objects ofconstant description complexity in $\mathbb{R}^d$, which may be of independentinterest. Next, we focus on the continuous version of this problem, whereranges of the set system are geometric concepts in $\mathbb{R}^d$ arriving inan online manner, but the universe is the entire space, and the objective is tochoose a small sample that intersects all the ranges.</description><author>Sujoy Bhore, Devdan Dey, Satyam Singh</author><pubDate>Wed, 09 Oct 2024 16:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07059v1</guid></item><item><title>Predictability maximization and the origins of word order harmony</title><link>http://arxiv.org/abs/2408.16570v5</link><description>We address the linguistic problem of the sequential arrangement of a head andits dependents from an information theoretic perspective. In particular, weconsider the optimal placement of a head that maximizes the predictability ofthe sequence. We assume that dependents are statistically independent given ahead, in line with the open-choice principle and the core assumptions ofdependency grammar. We demonstrate the optimality of harmonic order, i.e.,placing the head last maximizes the predictability of the head whereas placingthe head first maximizes the predictability of dependents. We also show thatpostponing the head is the optimal strategy to maximize its predictabilitywhile bringing it forward is the optimal strategy to maximize thepredictability of dependents. We unravel the advantages of the strategy ofmaximizing the predictability of the head over maximizing the predictability ofdependents. Our findings shed light on the placements of the head adopted byreal languages or emerging in different kinds of experiments.</description><author>Ramon Ferrer-i-Cancho</author><pubDate>Wed, 09 Oct 2024 16:52:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16570v5</guid></item><item><title>Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing</title><link>http://arxiv.org/abs/2410.07054v1</link><description>Large Language Models (LLMs) have recently revolutionized the NLP field,while they still fall short in some specific down-stream tasks. In the work, wefocus on utilizing LLMs to perform machine translation, where we observe thattwo patterns of errors frequently occur and drastically affect the translationquality: language mismatch and repetition. The work sets out to explore thepotential for mitigating these two issues by leveraging model editing methods,e.g., by locating Feed-Forward Network (FFN) neurons or something that areresponsible for the errors and deactivating them in the inference time. We findthat directly applying such methods either limited effect on the targetederrors or has significant negative side-effect on the general translationquality, indicating that the located components may also be crucial forensuring machine translation with LLMs on the rails. To this end, we propose torefine the located components by fetching the intersection of the locatingresults under different language settings, filtering out the aforementionedinformation that is irrelevant to targeted errors. The experiment resultsempirically demonstrate that our methods can effectively reduce the languagemismatch and repetition ratios and meanwhile enhance or keep the generaltranslation quality in most cases.</description><author>Weichuan Wang, Zhaoyi Li, Defu Lian, Chen Ma, Linqi Song, Ying Wei</author><pubDate>Wed, 09 Oct 2024 16:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07054v1</guid></item><item><title>Robots in the Middle: Evaluating LLMs in Dispute Resolution</title><link>http://arxiv.org/abs/2410.07053v1</link><description>Mediation is a dispute resolution method featuring a neutral third-party(mediator) who intervenes to help the individuals resolve their dispute. Inthis paper, we investigate to which extent large language models (LLMs) areable to act as mediators. We investigate whether LLMs are able to analyzedispute conversations, select suitable intervention types, and generateappropriate intervention messages. Using a novel, manually created dataset of50 dispute scenarios, we conduct a blind evaluation comparing LLMs with humanannotators across several key metrics. Overall, the LLMs showed strongperformance, even outperforming our human annotators across dimensions.Specifically, in 62% of the cases, the LLMs chose intervention types that wererated as better than or equivalent to those chosen by humans. Moreover, in 84%of the cases, the intervention messages generated by the LLMs were rated asbetter than or equal to the intervention messages written by humans. LLMslikewise performed favourably on metrics such as impartiality, understandingand contextualization. Our results demonstrate the potential of integrating AIin online dispute resolution (ODR) platforms.</description><author>Jinzhe Tan, Hannes Westermann, Nikhil Reddy Pottanigari, Jaromír Šavelka, Sébastien Meeùs, Mia Godet, Karim Benyekhlef</author><pubDate>Wed, 09 Oct 2024 16:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07053v1</guid></item><item><title>CMMD: Contrastive Multi-Modal Diffusion for Video-Audio Conditional Modeling</title><link>http://arxiv.org/abs/2312.05412v2</link><description>We introduce a multi-modal diffusion model tailored for the bi-directionalconditional generation of video and audio. We propose a joint contrastivetraining loss to improve the synchronization between visual and auditoryoccurrences. We present experiments on two datasets to evaluate the efficacy ofour proposed model. The assessment of generation quality and alignmentperformance is carried out from various angles, encompassing both objective andsubjective metrics. Our findings demonstrate that the proposed modeloutperforms the baseline in terms of quality and generation speed throughintroduction of our novel cross-modal easy fusion architectural block.Furthermore, the incorporation of the contrastive loss results in improvementsin audio-visual alignment, particularly in the high-correlation video-to-audiogeneration task.</description><author>Ruihan Yang, Hannes Gamper, Sebastian Braun</author><pubDate>Wed, 09 Oct 2024 16:49:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05412v2</guid></item><item><title>S2HPruner: Soft-to-Hard Distillation Bridges the Discretization Gap in Pruning</title><link>http://arxiv.org/abs/2410.07046v1</link><description>Recently, differentiable mask pruning methods optimize the continuousrelaxation architecture (soft network) as the proxy of the pruned discretenetwork (hard network) for superior sub-architecture search. However, due tothe agnostic impact of the discretization process, the hard network struggleswith the equivalent representational capacity as the soft network, namelydiscretization gap, which severely spoils the pruning performance. In thispaper, we first investigate the discretization gap and propose a novelstructural differentiable mask pruning framework named S2HPruner to bridge thediscretization gap in a one-stage manner. In the training procedure, SH2Prunerforwards both the soft network and its corresponding hard network, thendistills the hard network under the supervision of the soft network. Tooptimize the mask and prevent performance degradation, we propose a decoupledbidirectional knowledge distillation. It blocks the weight updating from thehard to the soft network while maintaining the gradient corresponding to themask. Compared with existing pruning arts, S2HPruner achieves surpassingpruning performance without fine-tuning on comprehensive benchmarks, includingCIFAR-100, Tiny ImageNet, and ImageNet with a variety of network architectures.Besides, investigation and analysis experiments explain the effectiveness ofS2HPruner. Codes will be released soon.</description><author>Weihao Lin, Shengji Tang, Chong Yu, Peng Ye, Tao Chen</author><pubDate>Wed, 09 Oct 2024 16:36:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07046v1</guid></item><item><title>Z-upscaling: Optical Flow Guided Frame Interpolation for Isotropic Reconstruction of 3D EM Volumes</title><link>http://arxiv.org/abs/2410.07043v1</link><description>We propose a novel optical flow based approach to enhance the axialresolution of anisotropic 3D EM volumes to achieve isotropic 3D reconstruction.Assuming spatial continuity of 3D biological structures in well aligned EMvolumes, we reasoned that optical flow estimation techniques, often applied fortemporal resolution enhancement in videos, can be utilized. Pixel level motionis estimated between neighboring 2D slices along z, using spatial gradient flowestimates to interpolate and generate new 2D slices resulting in isotropicvoxels. We leverage recent state-of-the-art learning methods for video frameinterpolation and transfer learning techniques, and demonstrate the success ofour approach on publicly available ultrastructure EM volumes.</description><author>Fisseha A. Ferede, Ali Khalighifar, Jaison John, Krishnan Venkataraman, Khaled Khairy</author><pubDate>Wed, 09 Oct 2024 16:34:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07043v1</guid></item><item><title>Greener GRASS: Enhancing GNNs with Encoding, Rewiring, and Attention</title><link>http://arxiv.org/abs/2407.05649v3</link><description>Graph Neural Networks (GNNs) have become important tools for machine learningon graph-structured data. In this paper, we explore the synergistic combinationof graph encoding, graph rewiring, and graph attention, by introducing GraphAttention with Stochastic Structures (GRASS), a novel GNN architecture. GRASSutilizes relative random walk probabilities (RRWP) encoding and a noveldecomposed variant (D-RRWP) to efficiently capture structural information. Itrewires the input graph by superimposing a random regular graph to enhancelong-range information propagation. It also employs a novel additive attentionmechanism tailored for graph-structured data. Our empirical evaluationsdemonstrate that GRASS achieves state-of-the-art performance on multiplebenchmark datasets, including a 20.3% reduction in mean absolute error on theZINC dataset.</description><author>Tongzhou Liao, Barnabás Póczos</author><pubDate>Wed, 09 Oct 2024 16:32:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05649v3</guid></item><item><title>Emergent properties with repeated examples</title><link>http://arxiv.org/abs/2410.07041v1</link><description>We study the performance of transformers as a function of the number ofrepetitions of training examples with algorithmically generated datasets. Onthree problems of mathematics: the greatest common divisor, modularmultiplication, and matrix eigenvalues, we show that for a fixed number oftraining steps, models trained on smaller sets of repeated examples outperformmodels trained on larger sets of single-use examples. We also demonstrate thattwo-set training - repeated use of a small random subset of examples, alongnormal sampling on the rest of the training set - provides for faster learningand better performance. This highlights that the benefits of repetition canoutweigh those of data diversity. These datasets and problems provide acontrolled setting to shed light on the still poorly understood interplaybetween generalization and memorization in deep learning.</description><author>François Charton, Julia Kempe</author><pubDate>Wed, 09 Oct 2024 16:28:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07041v1</guid></item><item><title>A Poincaré Inequality and Consistency Results for Signal Sampling on Large Graphs</title><link>http://arxiv.org/abs/2311.10610v3</link><description>Large-scale graph machine learning is challenging as the complexity oflearning models scales with the graph size. Subsampling the graph is a viablealternative, but sampling on graphs is nontrivial as graphs are non-Euclidean.Existing graph sampling techniques require not only computing the spectra oflarge matrices but also repeating these computations when the graph changes,e.g., grows. In this paper, we introduce a signal sampling theory for a type ofgraph limit -- the graphon. We prove a Poincar\'e inequality for graphonsignals and show that complements of node subsets satisfying this inequalityare unique sampling sets for Paley-Wiener spaces of graphon signals. Exploitingconnections with spectral clustering and Gaussian elimination, we prove thatsuch sampling sets are consistent in the sense that unique sampling sets on aconvergent graph sequence converge to unique sampling sets on the graphon. Wethen propose a related graphon signal sampling algorithm for large graphs, anddemonstrate its good empirical performance on graph machine learning tasks.</description><author>Thien Le, Luana Ruiz, Stefanie Jegelka</author><pubDate>Wed, 09 Oct 2024 16:28:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10610v3</guid></item><item><title>Distributionally Robust Clustered Federated Learning: A Case Study in Healthcare</title><link>http://arxiv.org/abs/2410.07039v1</link><description>In this paper, we address the challenge of heterogeneous data distributionsin cross-silo federated learning by introducing a novel algorithm, which weterm Cross-silo Robust Clustered Federated Learning (CS-RCFL). Our approachleverages the Wasserstein distance to construct ambiguity sets around eachclient's empirical distribution that capture possible distribution shifts inthe local data, enabling evaluation of worst-case model performance. We thenpropose a model-agnostic integer fractional program to determine the optimaldistributionally robust clustering of clients into coalitions so that possiblebiases in the local models caused by statistically heterogeneous clientdatasets are avoided, and analyze our method for linear and logistic regressionmodels. Finally, we discuss a federated learning protocol that ensures theprivacy of client distributions, a critical consideration, for instance, whenclients are healthcare institutions. We evaluate our algorithm on synthetic andreal-world healthcare data.</description><author>Xenia Konti, Hans Riess, Manos Giannopoulos, Yi Shen, Michael J. Pencina, Nicoleta J. Economou-Zavlanos, Michael M. Zavlanos</author><pubDate>Wed, 09 Oct 2024 16:25:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07039v1</guid></item><item><title>PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness</title><link>http://arxiv.org/abs/2410.07035v1</link><description>Large Language Models (LLMs) demonstrate impressive capabilities acrossvarious domains, including role-playing, creative writing, mathematicalreasoning, and coding. Despite these advancements, LLMs still encounterchallenges with length control, frequently failing to adhere to specific lengthconstraints due to their token-level operations and insufficient training ondata with strict length limitations. We identify this issue as stemming from alack of positional awareness and propose novel approaches--PositionID Promptingand PositionID Fine-Tuning--to address it. These methods enhance the model'sability to continuously monitor and manage text length during generation.Additionally, we introduce PositionID CP Prompting to enable LLMs to performcopy and paste operations accurately. Furthermore, we develop two benchmarksfor evaluating length control and copy-paste abilities. Our experimentsdemonstrate that our methods significantly improve the model's adherence tolength constraints and copy-paste accuracy without compromising responsequality.</description><author>Zekun Wang, Feiyu Duan, Yibo Zhang, Wangchunshu Zhou, Ke Xu, Wenhao Huang, Jie Fu</author><pubDate>Wed, 09 Oct 2024 16:15:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07035v1</guid></item><item><title>Clean Evaluations on Contaminated Visual Language Models</title><link>http://arxiv.org/abs/2410.07030v1</link><description>How to evaluate large language models (LLMs) cleanly has been established asan important research era to genuinely report the performance of possiblycontaminated LLMs. Yet, how to cleanly evaluate the visual language models(VLMs) is an under-studied problem. We propose a novel approach to achieve suchgoals through data augmentation methods on the visual input information. Wethen craft a new visual clean evaluation benchmark with thousands of datainstances. Through extensive experiments, we found that the traditional visualdata augmentation methods are useful, but they are at risk of being used as apart of the training data as a workaround. We further propose using BGRaugmentation to switch the colour channel of the visual information. We foundthat it is a simple yet effective method for reducing the effect of datacontamination and fortunately, it is also harmful to be used as a dataaugmentation method during training. It means that it is hard to integrate suchdata augmentation into training by malicious trainers and it could be apromising technique to cleanly evaluate visual LLMs. Our code, data, and modelweights will be released upon publication.</description><author>Hongyuan Lu, Shujie Miao, Wai Lam</author><pubDate>Wed, 09 Oct 2024 16:13:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07030v1</guid></item><item><title>Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models</title><link>http://arxiv.org/abs/2405.15234v3</link><description>Diffusion models (DMs) have achieved remarkable success in text-to-imagegeneration, but they also pose safety risks, such as the potential generationof harmful content and copyright violations. The techniques of machineunlearning, also known as concept erasing, have been developed to address theserisks. However, these techniques remain vulnerable to adversarial promptattacks, which can prompt DMs post-unlearning to regenerate undesired imagescontaining concepts (such as nudity) meant to be erased. This work aims toenhance the robustness of concept erasing by integrating the principle ofadversarial training (AT) into machine unlearning, resulting in the robustunlearning framework referred to as AdvUnlearn. However, achieving thiseffectively and efficiently is highly nontrivial. First, we find that astraightforward implementation of AT compromises DMs' image generation qualitypost-unlearning. To address this, we develop a utility-retaining regularizationon an additional retain set, optimizing the trade-off between concept erasurerobustness and model utility in AdvUnlearn. Moreover, we identify the textencoder as a more suitable module for robustification compared to UNet,ensuring unlearning effectiveness. And the acquired text encoder can serve as aplug-and-play robust unlearner for various DM types. Empirically, we performextensive experiments to demonstrate the robustness advantage of AdvUnlearnacross various DM unlearning scenarios, including the erasure of nudity,objects, and style concepts. In addition to robustness, AdvUnlearn alsoachieves a balanced tradeoff with model utility. To our knowledge, this is thefirst work to systematically explore robust DM unlearning through AT, settingit apart from existing methods that overlook robustness in concept erasing.Codes are available at: https://github.com/OPTML-Group/AdvUnlearn</description><author>Yimeng Zhang, Xin Chen, Jinghan Jia, Yihua Zhang, Chongyu Fan, Jiancheng Liu, Mingyi Hong, Ke Ding, Sijia Liu</author><pubDate>Wed, 09 Oct 2024 16:12:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15234v3</guid></item><item><title>Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation Models Without Human Feedback</title><link>http://arxiv.org/abs/2410.07025v1</link><description>Radiologists play a crucial role by translating medical images into medicalreports. However, the field faces staffing shortages and increasing workloads.While automated approaches using vision-language models (VLMs) show promise asassistants, they require exceptionally high accuracy. Most current VLMs inradiology rely solely on supervised fine-tuning (SFT). Meanwhile, in thegeneral domain, additional preference fine-tuning has become standard practice.The challenge in radiology lies in the prohibitive cost of obtainingradiologist feedback. We propose a scalable automated preference alignmenttechnique for VLMs in radiology, focusing on chest X-ray (CXR) reportgeneration. Our method leverages publicly available datasets with anLLM-as-a-Judge mechanism, eliminating the need for additional expertradiologist feedback. We evaluate and benchmark five direct alignmentalgorithms (DAAs). Our results show up to a 57.4% improvement in average GREENscores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase inan average across six metrics (domain specific and general), compared to theSFT baseline. We study reward overoptimization via length exploitation, withreports lengthening by up to 3.2x. To assess a potential alignment tax, webenchmark on six additional diverse tasks, finding no significant degradations.A reader study involving four board-certified radiologists indicates win ratesof up to 0.62 over the SFT baseline, while significantly penalizing verbosity.Our analysis provides actionable insights for the development of VLMs inhigh-stakes fields like radiology.</description><author>Dennis Hein, Zhihong Chen, Sophie Ostmeier, Justin Xu, Maya Varma, Eduardo Pontes Reis, Arne Edward Michalson, Christian Bluethgen, Hyun Joo Shin, Curtis Langlotz, Akshay S Chaudhari</author><pubDate>Wed, 09 Oct 2024 16:07:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07025v1</guid></item><item><title>Vocabulary Transfer for Medical Texts</title><link>http://arxiv.org/abs/2208.02554v2</link><description>Working within specific NLP subdomains presents significant challenges,primarily due to a persistent deficit of data. Stringent privacy concerns andlimited data accessibility often drive this shortage. Additionally, the medicaldomain demands high accuracy, where even marginal improvements in modelperformance can have profound impacts. In this study, we investigate thepotential of vocabulary transfer to enhance model performance in biomedical NLPtasks. Specifically, we focus on vocabulary extension, a technique thatinvolves expanding the target vocabulary to incorporate domain-specificbiomedical terms. Our findings demonstrate that vocabulary extension, leads tomeasurable improvements in both downstream model performance and inferencetime.</description><author>Priyanka Singh, Vladislav D. Mosin, Ivan P. Yamshchikov</author><pubDate>Wed, 09 Oct 2024 16:07:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.02554v2</guid></item><item><title>Do Contemporary CATE Models Capture Real-World Heterogeneity? Findings from a Large-Scale Benchmark</title><link>http://arxiv.org/abs/2410.07021v1</link><description>We present unexpected findings from a large-scale benchmark study evaluatingConditional Average Treatment Effect (CATE) estimation algorithms. By running16 modern CATE models across 43,200 datasets, we find that: (a) 62\% of CATEestimates have a higher Mean Squared Error (MSE) than a trivial zero-effectpredictor, rendering them ineffective; (b) in datasets with at least one usefulCATE estimate, 80\% still have higher MSE than a constant-effect model; and (c)Orthogonality-based models outperform other models only 30\% of the time,despite widespread optimism about their performance. These findings exposesignificant limitations in current CATE models and suggest ample opportunitiesfor further research. Our findings stem from a novel application of \textit{observationalsampling}, originally developed to evaluate Average Treatment Effect (ATE)estimates from observational methods with experiment data. To adaptobservational sampling for CATE evaluation, we introduce a statisticalparameter, $Q$, equal to MSE minus a constant and preserves the ranking ofmodels by their MSE. We then derive a family of sample statistics, collectivelycalled $\hat{Q}$, that can be computed from real-world data. We prove that$\hat{Q}$ is a consistent estimator of $Q$ under mild technical conditions.When used in observational sampling, $\hat{Q}$ is unbiased and asymptoticallyselects the model with the smallest MSE. To ensure the benchmark reflectsreal-world heterogeneity, we handpick datasets where outcomes come from fieldrather than simulation. By combining the new observational sampling method, newstatistics, and real-world datasets, the benchmark provides a uniqueperspective on CATE estimator performance and uncover gaps in capturingreal-world heterogeneity.</description><author>Haining Yu, Yizhou Sun</author><pubDate>Wed, 09 Oct 2024 16:04:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07021v1</guid></item><item><title>Diversify, Rationalize, and Combine: Ensembling Multiple QA Strategies for Zero-shot Knowledge-based VQA</title><link>http://arxiv.org/abs/2406.12746v5</link><description>Knowledge-based Visual Question-answering (K-VQA) often requires the use ofbackground knowledge beyond the image. However, we discover that a singleknowledge generation strategy is often insufficient for all K-VQA questions. Tothis end, we propose Diversification, Evidence Truncation, and Combination forKnowledge-based Elucidation (DietCoke), which utilizes a bundle ofcomplementary question-answering tactics and aggregates their answers usingtextual rationales. DietCoke comprises of three stages: diversification,rationalization, and ensemble. The diversification stage generates threedistinctive decision contexts, each leading to its own answer candidate. Therationalization stage generates two rationales, the automatic rationale and themechanistic rationale, for each answer candidate using decorrelated techniques.Finally, in the ensemble stage, an LLM informed by the rationales selects oneanswer from the three candidates. Experiments show that DietCoke significantlyoutperforms state-of-the-art LLM-based baselines by 2.8% on OK-VOA and 4.7% onA-OKVOA and that the strategies in the ensembles are highly complementary. Codeis available at: https://github.com/limiaoyu/DietCoke</description><author>Miaoyu Li, Haoxin Li, Zilin Du, Boyang Li</author><pubDate>Wed, 09 Oct 2024 16:04:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12746v5</guid></item><item><title>The Vital Role of Gradient Clipping in Byzantine-Resilient Distributed Learning</title><link>http://arxiv.org/abs/2405.14432v4</link><description>Byzantine-resilient distributed machine learning seeks to achieve robustlearning performance in the presence of misbehaving or adversarial workers.While state-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD)methods were proven theoretically optimal, their empirical success has oftenrelied on pre-aggregation gradient clipping. However, the currently consideredstatic clipping strategy exhibits mixed results: improving robustness againstsome attacks while being ineffective or detrimental against others. We addressthis gap by proposing a principled adaptive clipping strategy, termed AdaptiveRobust Clipping (ARC). We show that ARC consistently enhances the empiricalrobustness of SOTA Robust-DGD methods, while preserving the theoreticalrobustness guarantees. Our analysis shows that ARC provably improves theasymptotic convergence guarantee of Robust-DGD in the case when the model iswell-initialized. We validate this theoretical insight through an exhaustiveset of experiments on benchmark image classification tasks. We observe that theimprovement induced by ARC is more pronounced in highly heterogeneous andadversarial settings.</description><author>Youssef Allouah, Rachid Guerraoui, Nirupam Gupta, Ahmed Jellouli, Geovani Rizk, John Stephan</author><pubDate>Wed, 09 Oct 2024 16:04:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14432v4</guid></item><item><title>LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law</title><link>http://arxiv.org/abs/2402.00795v4</link><description>Pretrained large language models (LLMs) are surprisingly effective atperforming zero-shot tasks, including time-series forecasting. However,understanding the mechanisms behind such capabilities remains highlychallenging due to the complexity of the models. We study LLMs' ability toextrapolate the behavior of dynamical systems whose evolution is governed byprinciples of physical interest. Our results show that LLaMA 2, a languagemodel trained primarily on texts, achieves accurate predictions of dynamicalsystem time series without fine-tuning or prompt engineering. Moreover, theaccuracy of the learned physical rules increases with the length of the inputcontext window, revealing an in-context version of neural scaling law. Alongthe way, we present a flexible and efficient algorithm for extractingprobability density functions of multi-digit numbers directly from LLMs.</description><author>Toni J. B. Liu, Nicolas Boullé, Raphaël Sarfati, Christopher J. Earls</author><pubDate>Wed, 09 Oct 2024 16:02:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00795v4</guid></item><item><title>Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization</title><link>http://arxiv.org/abs/2410.07018v1</link><description>Out-of-Distribution (OOD) generalization in machine learning is a burgeoningarea of study. Its primary goal is to enhance the adaptability and resilienceof machine learning models when faced with new, unseen, and potentiallyadversarial data that significantly diverges from their original trainingdatasets. In this paper, we investigate time series OOD generalization viapre-trained Large Language Models (LLMs). We first propose a novel\textbf{T}ri-level learning framework for \textbf{T}ime \textbf{S}eries\textbf{O}OD generalization, termed TTSO, which considers both sample-level andgroup-level uncertainties. This formula offers a fresh theoretic perspectivefor formulating and analyzing OOD generalization problem. In addition, weprovide a theoretical analysis to justify this method is well motivated. Wethen develop a stratified localization algorithm tailored for this tri-leveloptimization problem, theoretically demonstrating the guaranteed convergence ofthe proposed algorithm. Our analysis also reveals that the iteration complexityto obtain an $\epsilon$-stationary point is bounded byO($\frac{1}{\epsilon^{2}}$). Extensive experiments on real-world datasets havebeen conducted to elucidate the effectiveness of the proposed method.</description><author>Chengtao Jian, Kai Yang, Yang Jiao</author><pubDate>Wed, 09 Oct 2024 16:00:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07018v1</guid></item><item><title>Optimizing Estimators of Squared Calibration Errors in Classification</title><link>http://arxiv.org/abs/2410.07014v1</link><description>In this work, we propose a mean-squared error-based risk that enables thecomparison and optimization of estimators of squared calibration errors inpractical settings. Improving the calibration of classifiers is crucial forenhancing the trustworthiness and interpretability of machine learning models,especially in sensitive decision-making scenarios. Although various calibration(error) estimators exist in the current literature, there is a lack of guidanceon selecting the appropriate estimator and tuning its hyperparameters. Byleveraging the bilinear structure of squared calibration errors, we reformulatecalibration estimation as a regression problem with independent and identicallydistributed (i.i.d.) input pairs. This reformulation allows us to quantify theperformance of different estimators even for the most challenging calibrationcriterion, known as canonical calibration. Our approach advocates for atraining-validation-testing pipeline when estimating a calibration error on anevaluation dataset. We demonstrate the effectiveness of our pipeline byoptimizing existing calibration estimators and comparing them with novel kernelridge regression-based estimators on standard image classification tasks.</description><author>Sebastian G. Gruber, Francis Bach</author><pubDate>Wed, 09 Oct 2024 15:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07014v1</guid></item><item><title>Causal Representation Learning in Temporal Data via Single-Parent Decoding</title><link>http://arxiv.org/abs/2410.07013v1</link><description>Scientific research often seeks to understand the causal structure underlyinghigh-level variables in a system. For example, climate scientists study howphenomena, such as El Ni\~no, affect other climate processes at remotelocations across the globe. However, scientists typically collect low-levelmeasurements, such as geographically distributed temperature readings. Fromthese, one needs to learn both a mapping to causally-relevant latent variables,such as a high-level representation of the El Ni\~no phenomenon and otherprocesses, as well as the causal model over them. The challenge is that thistask, called causal representation learning, is highly underdetermined fromobservational data alone, requiring other constraints during learning toresolve the indeterminacies. In this work, we consider a temporal model with asparsity assumption, namely single-parent decoding: each observed low-levelvariable is only affected by a single latent variable. Such an assumption isreasonable in many scientific applications that require finding groups oflow-level variables, such as extracting regions from geographically griddedmeasurement data in climate research or capturing brain regions from neuralactivity data. We demonstrate the identifiability of the resulting model andpropose a differentiable method, Causal Discovery with Single-parent Decoding(CDSD), that simultaneously learns the underlying latents and a causal graphover them. We assess the validity of our theoretical results using simulateddata and showcase the practical validity of our method in an application toreal-world data from the climate science field.</description><author>Philippe Brouillard, Sébastien Lachapelle, Julia Kaltenborn, Yaniv Gurwicz, Dhanya Sridhar, Alexandre Drouin, Peer Nowack, Jakob Runge, David Rolnick</author><pubDate>Wed, 09 Oct 2024 15:57:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07013v1</guid></item><item><title>Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based Outline-guided Generation</title><link>http://arxiv.org/abs/2410.07009v1</link><description>The patent domain is gaining attention in natural language processingresearch, offering practical applications in streamlining the patenting processand providing challenging benchmarks for large language models (LLMs). However,the generation of the description sections of patents, which constitute morethan 90% of the patent document, has not been studied to date. We address thisgap by introducing the task of outline-guided paper-to-patent generation, wherean academic paper provides the technical specification of the invention and anoutline conveys the desired patent structure. We present PAP2PAT, a newchallenging benchmark of 1.8k patent-paper pairs with document outlines,collected using heuristics that reflect typical research lab practices. Ourexperiments with current open-weight LLMs and outline-guided chunk-basedgeneration show that they can effectively use information from the paper butstruggle with repetitions, likely due to the inherent repetitiveness of patentlanguage. We release our data and code.</description><author>Valentin Knappich, Simon Razniewski, Anna Hätty, Annemarie Friedrich</author><pubDate>Wed, 09 Oct 2024 15:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07009v1</guid></item><item><title>Through the Looking Glass: Mirror Schrödinger Bridges</title><link>http://arxiv.org/abs/2410.07003v1</link><description>Resampling from a target measure whose density is unknown is a fundamentalproblem in mathematical statistics and machine learning. A setting thatdominates the machine learning literature consists of learning a map from aneasy-to-sample prior, such as the Gaussian distribution, to a target measure.Under this model, samples from the prior are pushed forward to generate a newsample on the target measure, which is often difficult to sample from directly.In this paper, we propose a new model for conditional resampling called mirrorSchr\"odinger bridges. Our key observation is that solving the Schr\"odingerbridge problem between a distribution and itself provides a natural way toproduce new samples from conditional distributions, giving in-distributionvariations of an input data point. We show how to efficiently solve thislargely overlooked version of the Schr\"odinger bridge problem. We prove thatour proposed method leads to significant algorithmic simplifications overexisting alternatives, in addition to providing control over in-distributionvariation. Empirically, we demonstrate how these benefits can be leveraged toproduce proximal samples in a number of application domains.</description><author>Leticia Mattos Da Silva, Silvia Sellán, Justin Solomon</author><pubDate>Wed, 09 Oct 2024 15:48:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07003v1</guid></item><item><title>CursorCore: Assist Programming through Aligning Anything</title><link>http://arxiv.org/abs/2410.07002v1</link><description>Large language models have been successfully applied to programmingassistance tasks, such as code completion, code insertion, and instructionalcode editing. However, these applications remain insufficiently automated andstruggle to effectively integrate various types of information during theprogramming process, including coding history, current code, and userinstructions. In this work, we propose a new conversational framework thatcomprehensively integrates these information sources, collect data to train ourmodels and evaluate their performance. Firstly, to thoroughly evaluate how wellmodels align with different types of information and the quality of theiroutputs, we introduce a new benchmark, APEval (Assist Programming Eval), tocomprehensively assess the performance of models in programming assistancetasks. Then, for data collection, we develop a data generation pipeline,Programming-Instruct, which synthesizes training data from diverse sources,such as GitHub and online judge platforms. This pipeline can automaticallygenerate various types of messages throughout the programming process. Finally,using this pipeline, we generate 219K samples, fine-tune multiple models, anddevelop the CursorCore series. We show that CursorCore outperforms other modelsof comparable size. This framework unifies applications such as inline chat andautomated editing, contributes to the advancement of coding assistants. Code,models and data are freely available athttps://github.com/TechxGenus/CursorCore.</description><author>Hao Jiang, Qi Liu, Rui Li, Shengyu Ye, Shijin Wang</author><pubDate>Wed, 09 Oct 2024 15:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07002v1</guid></item><item><title>When "A Helpful Assistant" Is Not Really Helpful: Personas in System Prompts Do Not Improve Performances of Large Language Models</title><link>http://arxiv.org/abs/2311.10054v3</link><description>Prompting serves as the major way humans interact with Large Language Models(LLM). Commercial AI systems commonly define the role of the LLM in systemprompts. For example, ChatGPT uses ``You are a helpful assistant'' as part ofits default system prompt. Despite current practices of adding personas tosystem prompts, it remains unclear how different personas affect a model'sperformance on objective tasks. In this study, we present a systematicevaluation of personas in system prompts. We curate a list of 162 rolescovering 6 types of interpersonal relationships and 8 domains of expertise.Through extensive analysis of 4 popular families of LLMs and 2,410 factualquestions, we demonstrate that adding personas in system prompts does notimprove model performance across a range of questions compared to the controlsetting where no persona is added. Nevertheless, further analysis suggests thatthe gender, type, and domain of the persona can all influence the resultingprediction accuracies. We further experimented with a list of persona searchstrategies and found that, while aggregating results from the best persona foreach question significantly improves prediction accuracy, automaticallyidentifying the best persona is challenging, with predictions often performingno better than random selection. Overall, our findings suggest that whileadding a persona may lead to performance gains in certain settings, the effectof each persona can be largely random. Code and data are available athttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.</description><author>Mingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran, Moontae Lee, David Jurgens</author><pubDate>Wed, 09 Oct 2024 15:44:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10054v3</guid></item><item><title>Can Your Generative Model Detect Out-of-Distribution Covariate Shift?</title><link>http://arxiv.org/abs/2409.03043v2</link><description>Detecting Out-of-Distribution (OOD) sensory data and covariate distributionshift aims to identify new test examples with different high-level imagestatistics to the captured, normal and In-Distribution (ID) set. Existing OODdetection literature largely focuses on semantic shift with little-to-noconsensus over covariate shift. Generative models capture the ID data in anunsupervised manner, enabling them to effectively identify samples that deviatesignificantly from this learned distribution, irrespective of the downstreamtask. In this work, we elucidate the ability of generative models to detect andquantify domain-specific covariate shift through extensive analyses thatinvolves a variety of models. To this end, we conjecture that it is sufficientto detect most occurring sensory faults (anomalies and deviations in globalsignals statistics) by solely modeling high-frequency signal-dependent andindependent details. We propose a novel method, CovariateFlow, for OODdetection, specifically tailored to covariate heteroscedastic high-frequencyimage-components using conditional Normalizing Flows (cNFs). Our results onCIFAR10 vs. CIFAR10-C and ImageNet200 vs. ImageNet200-C demonstrate theeffectiveness of the method by accurately detecting OOD covariate shift. Thiswork contributes to enhancing the fidelity of imaging systems and aidingmachine learning models in OOD detection in the presence of covariate shift.</description><author>Christiaan Viviers, Amaan Valiuddin, Francisco Caetano, Lemar Abdi, Lena Filatova, Peter de With, Fons van der Sommen</author><pubDate>Wed, 09 Oct 2024 15:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03043v2</guid></item><item><title>A Diffusion-based Xray2MRI Model: Generating Pseudo-MRI Volumes From one Single X-ray</title><link>http://arxiv.org/abs/2410.06997v1</link><description>Knee osteoarthritis (KOA) is a prevalent musculoskeletal disorder, and X-raysare commonly used for its diagnosis due to their cost-effectiveness. MagneticResonance Imaging (MRI), on the other hand, offers detailed soft tissuevisualization and has become a valuable supplementary diagnostic tool for KOA.Unfortunately, the high cost and limited accessibility of MRI hinder itswidespread use, leaving many patients with KOA reliant solely on X-ray imaging.In this study, we introduce a novel diffusion-based Xray2MRI model capable ofgenerating pseudo-MRI volumes from one single X-ray image. In addition to usingX-rays as conditional input, our model integrates target depth, KOA probabilitydistribution, and image intensity distribution modules to guide the synthesisprocess, ensuring that the generated corresponding slices accurately correspondto the anatomical structures. Experimental results demonstrate that byintegrating information from X-rays with additional input data, our proposedapproach is capable of generating pseudo-MRI sequences that approximate realMRI scans. Moreover, by increasing the inference times, the model achieveseffective interpolation, further improving the continuity and smoothness of thegenerated MRI sequences, representing one promising initial attempt forcost-effective medical imaging solutions.</description><author>Zhe Wang, Rachid Jennane, Aladine Chetouani, Mohamed Jarraya</author><pubDate>Wed, 09 Oct 2024 15:44:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06997v1</guid></item><item><title>Efficient Distribution Matching of Representations via Noise-Injected Deep InfoMax</title><link>http://arxiv.org/abs/2410.06993v1</link><description>Deep InfoMax (DIM) is a well-established method for self-supervisedrepresentation learning (SSRL) based on maximization of the mutual informationbetween the input and the output of a deep neural network encoder. Despite theDIM and contrastive SSRL in general being well-explored, the task of learningrepresentations conforming to a specific distribution (i.e., distributionmatching, DM) is still under-addressed. Motivated by the importance of DM toseveral downstream tasks (including generative modeling, disentanglement,outliers detection and other), we enhance DIM to enable automatic matching oflearned representations to a selected prior distribution. To achieve this, wepropose injecting an independent noise into the normalized outputs of theencoder, while keeping the same InfoMax training objective. We show that suchmodification allows for learning uniformly and normally distributedrepresentations, as well as representations of other absolutely continuousdistributions. Our approach is tested on various downstream tasks. The resultsindicate a moderate trade-off between the performance on the downstream tasksand quality of DM.</description><author>Ivan Butakov, Alexander Sememenko, Alexander Tolmachev, Andrey Gladkov, Marina Munkhoeva, Alexey Frolov</author><pubDate>Wed, 09 Oct 2024 15:40:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06993v1</guid></item><item><title>MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models</title><link>http://arxiv.org/abs/2405.13053v3</link><description>The pretrain+fine-tune paradigm is foundational for deploying large languagemodels (LLMs) across various downstream applications. Within this framework,Low-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning(PEFT), producing numerous reusable task-specific LoRA adapters. However, thisapproach requires explicit task intention selection, posing challenges forautonomous task sensing and switching during inference with multiple existingLoRA adapters embedded in a single LLM. In this work, we introduce MeteoRA(Multiple-tasks embedded LoRA), a scalable and efficient framework that reusesmultiple task-specific LoRA adapters into the base LLM via a full-modeMixture-of-Experts (MoE) architecture. This framework also includes novel MoEforward acceleration strategies to address the efficiency challenges oftraditional MoE implementations. Our evaluation, using the LlaMA2-13B andLlaMA3-8B base models equipped with 28 existing LoRA adapters through MeteoRA,demonstrates equivalent performance with the traditional PEFT method. Moreover,the LLM equipped with MeteoRA achieves superior performance in handlingcomposite tasks, effectively solving ten sequential problems in a singleinference pass, thereby demonstrating the framework's enhanced capability fortimely adapter switching.</description><author>Jingwei Xu, Junyu Lai, Yunpeng Huang</author><pubDate>Wed, 09 Oct 2024 15:33:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13053v3</guid></item><item><title>Symbolic Recovery of Differential Equations: The Identifiability Problem</title><link>http://arxiv.org/abs/2210.08342v9</link><description>Symbolic recovery of differential equations is the ambitious attempt atautomating the derivation of governing equations with the use of machinelearning techniques. In contrast to classical methods which assume thestructure of the equation to be known and focus on the estimation of specificparameters, these algorithms aim to learn the structure and the parameterssimultaneously. While the uniqueness and, therefore, the identifiability ofparameters of governing equations are a well-addressed problem in the field ofparameter estimation, it has not been investigated for symbolic recovery.However, this problem should be even more present in this field since thealgorithms aim to cover larger spaces of governing equations. In this paper, weinvestigate under which conditions a solution of a differential equation doesnot uniquely determine the equation itself. For various classes of differentialequations, we provide both necessary and sufficient conditions for a functionto uniquely determine the corresponding differential equation. We then use ourresults to devise numerical algorithms aiming to determine whether a functionsolves a differential equation uniquely. Finally, we provide extensivenumerical experiments showing that our algorithms can indeed guarantee theuniqueness of the learned governing differential equation, without assuming anyknowledge about the analytic form of function, thereby ensuring the reliabilityof the learned equation.</description><author>Philipp Scholl, Aras Bacho, Holger Boche, Gitta Kutyniok</author><pubDate>Wed, 09 Oct 2024 15:27:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.08342v9</guid></item><item><title>A Unified Generative Framework for Realistic Lidar Simulation in Autonomous Driving Systems</title><link>http://arxiv.org/abs/2312.15817v2</link><description>Simulation models for perception sensors are integral components ofautomotive simulators used for the virtual Verification and Validation (V\&amp;V)of Autonomous Driving Systems (ADS). These models also serve as powerful toolsfor generating synthetic datasets to train deep learning-based perceptionmodels. Lidar is a widely used sensor type among the perception sensors for ADSdue to its high precision in 3D environment scanning. However, developingrealistic Lidar simulation models is a significant technical challenge. Inparticular, unrealistic models can result in a large gap between thesynthesised and real-world point clouds, limiting their effectiveness in ADSapplications. Recently, deep generative models have emerged as promisingsolutions to synthesise realistic sensory data. However, for Lidar simulation,deep generative models have been primarily hybridised with conventionalalgorithms, leaving unified generative approaches largely unexplored in theliterature. Motivated by this research gap, we propose a unified generativeframework to enhance Lidar simulation fidelity. Our proposed framework projectsLidar point clouds into depth-reflectance images via a lossless transformation,and employs our novel Controllable Lidar point cloud Generative model, CoLiGen,to translate the images. We extensively evaluate our CoLiGen model, comparingit with the state-of-the-art image-to-image translation models using variousmetrics to assess the realness, faithfulness, and performance of a downstreamperception model. Our results show that CoLiGen exhibits superior performanceacross most metrics. The dataset and source code for this research areavailable at https://github.com/hamedhaghighi/CoLiGen.git.</description><author>Hamed Haghighi, Mehrdad Dianati, Valentina Donzella, Kurt Debattista</author><pubDate>Wed, 09 Oct 2024 15:26:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15817v2</guid></item><item><title>Linguistic Structure from a Bottleneck on Sequential Information Processing</title><link>http://arxiv.org/abs/2405.12109v2</link><description>Human language is a unique form of communication in the natural world,distinguished by its structured nature. Most fundamentally, it is systematic,meaning that signals can be broken down into component parts that areindividually meaningful -- roughly, words -- which are combined in a regularway to form sentences. Furthermore, the way in which these parts are combinedmaintains a kind of locality: words are usually concatenated together, and theyform contiguous phrases, keeping related parts of sentences close to eachother. We address the challenge of understanding how these basic properties oflanguage arise from broader principles of efficient communication underinformation processing constraints. Here we show that natural-language-likesystematicity arises in codes that are constrained by predictive information, ameasure of the amount of information that must be extracted from the past of asequence in order to predict its future. In simulations, we show that suchcodes approximately factorize their source distributions, and then express theresulting factors systematically and locally. Next, in a series ofcross-linguistic corpus studies, we show that human languages are structured tohave low predictive information at the levels of phonology, morphology, syntax,and semantics. Our result suggests that human language performs a sequential,discrete form of Independent Components Analysis on the statisticaldistribution over meanings that need to be expressed. It establishes a linkbetween the statistical and algebraic structure of human language, andreinforces the idea that the structure of human language is shaped bycommunication under cognitive constraints.</description><author>Richard Futrell, Michael Hahn</author><pubDate>Wed, 09 Oct 2024 15:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12109v2</guid></item><item><title>GPT-4V Cannot Generate Radiology Reports Yet</title><link>http://arxiv.org/abs/2407.12176v2</link><description>GPT-4V's purported strong multimodal abilities raise interests in using it toautomate radiology report writing, but there lacks thorough evaluations. Inthis work, we perform a systematic evaluation of GPT-4V in generating radiologyreports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attemptto directly generate reports using GPT-4V through different promptingstrategies and find that it fails terribly in both lexical metrics and clinicalefficacy metrics. To understand the low performance, we decompose the task intotwo steps: 1) the medical image reasoning step of predicting medical conditionlabels from images; and 2) the report synthesis step of generating reports from(groundtruth) conditions. We show that GPT-4V's performance in image reasoningis consistently low across different prompts. In fact, the distributions ofmodel-predicted labels remain constant regardless of which groundtruthconditions are present on the image, suggesting that the model is notinterpreting chest X-rays meaningfully. Even when given groundtruth conditionsin report synthesis, its generated reports are less correct and lessnatural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubton the viability of using GPT-4V in a radiology workflow.</description><author>Yuyang Jiang, Chacha Chen, Dang Nguyen, Benjamin M. Mervak, Chenhao Tan</author><pubDate>Wed, 09 Oct 2024 15:23:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12176v2</guid></item><item><title>Diffusion Density Estimators</title><link>http://arxiv.org/abs/2410.06986v1</link><description>We investigate the use of diffusion models as neural density estimators. Thecurrent approach to this problem involves converting the generative process toa smooth flow, known as the Probability Flow ODE. The log density at a givensample can be obtained by solving the ODE with a black-box solver. We introducea new, highly parallelizable method that computes log densities without theneed to solve a flow. Our approach is based on estimating a path integral byMonte Carlo, in a manner identical to the simulation-free training of diffusionmodels. We also study how different training parameters affect the accuracy ofthe density calculation, and offer insights into how these models can be mademore scalable and efficient.</description><author>Akhil Premkumar</author><pubDate>Wed, 09 Oct 2024 15:21:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06986v1</guid></item><item><title>Jointly Generating Multi-view Consistent PBR Textures using Collaborative Control</title><link>http://arxiv.org/abs/2410.06985v1</link><description>Multi-view consistency remains a challenge for image diffusion models. Evenwithin the Text-to-Texture problem, where perfect geometric correspondences areknown a priori, many methods fail to yield aligned predictions across views,necessitating non-trivial fusion methods to incorporate the results onto theoriginal mesh. We explore this issue for a Collaborative Control workflowspecifically in PBR Text-to-Texture. Collaborative Control directly models PBRimage probability distributions, including normal bump maps; to our knowledge,the only diffusion model to directly output full PBR stacks. We discuss thedesign decisions involved in making this model multi-view consistent, anddemonstrate the effectiveness of our approach in ablation studies, as well aspractical applications.</description><author>Shimon Vainer, Konstantin Kutsy, Dante De Nigris, Ciara Rowles, Slava Elizarov, Simon Donné</author><pubDate>Wed, 09 Oct 2024 15:21:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06985v1</guid></item><item><title>Structure-Centric Robust Monocular Depth Estimation via Knowledge Distillation</title><link>http://arxiv.org/abs/2410.06982v1</link><description>Monocular depth estimation, enabled by self-supervised learning, is a keytechnique for 3D perception in computer vision. However, it faces significantchallenges in real-world scenarios, which encompass adverse weather variations,motion blur, as well as scenes with poor lighting conditions at night. Ourresearch reveals that we can divide monocular depth estimation into threesub-problems: depth structure consistency, local texture disambiguation, andsemantic-structural correlation. Our approach tackles the non-robustness ofexisting self-supervised monocular depth estimation models to interferencetextures by adopting a structure-centered perspective and utilizing the scenestructure characteristics demonstrated by semantics and illumination. We devisea novel approach to reduce over-reliance on local textures, enhancingrobustness against missing or interfering patterns. Additionally, weincorporate a semantic expert model as the teacher and construct inter-modelfeature dependencies via learnable isomorphic graphs to enable aggregation ofsemantic structural knowledge. Our approach achieves state-of-the-artout-of-distribution monocular depth estimation performance across a range ofpublic adverse scenario datasets. It demonstrates notable scalability andcompatibility, without necessitating extensive model engineering. Thisshowcases the potential for customizing models for diverse industrialapplications.</description><author>Runze Chen, Haiyong Luo, Fang Zhao, Jingze Yu, Yupeng Jia, Juan Wang, Xuepeng Ma</author><pubDate>Wed, 09 Oct 2024 15:20:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06982v1</guid></item><item><title>Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models</title><link>http://arxiv.org/abs/2410.06981v1</link><description>We investigate feature universality in large language models (LLMs), aresearch field that aims to understand how different models similarly representconcepts in the latent spaces of their intermediate layers. Demonstratingfeature universality allows discoveries about latent representations togeneralize across several models. However, comparing features across LLMs ischallenging due to polysemanticity, in which individual neurons oftencorrespond to multiple features rather than distinct ones. This makes itdifficult to disentangle and match features across different models. To addressthis issue, we employ a method known as dictionary learning by using sparseautoencoders (SAEs) to transform LLM activations into more interpretable spacesspanned by neurons corresponding to individual features. After matching featureneurons across models via activation correlation, we apply representationalspace similarity metrics like Singular Value Canonical Correlation Analysis toanalyze these SAE features across different LLMs. Our experiments revealsignificant similarities in SAE feature spaces across various LLMs, providingnew evidence for feature universality.</description><author>Michael Lan, Philip Torr, Austin Meek, Ashkan Khakzar, David Krueger, Fazl Barez</author><pubDate>Wed, 09 Oct 2024 15:18:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06981v1</guid></item><item><title>Mixing of the No-U-Turn Sampler and the Geometry of Gaussian Concentration</title><link>http://arxiv.org/abs/2410.06978v1</link><description>We prove that the mixing time of the No-U-Turn Sampler (NUTS), wheninitialized in the concentration region of the canonical Gaussian measure,scales as $d^{1/4}$, up to logarithmic factors, where $d$ is the dimension.This scaling is expected to be sharp. This result is based on a couplingargument that leverages the geometric structure of the target distribution.Specifically, concentration of measure results in a striking uniformity inNUTS' locally adapted transitions, which holds with high probability. Thisuniformity is formalized by interpreting NUTS as an accept/reject Markov chain,where the mixing properties for the more uniform accept chain are analyticallytractable. Additionally, our analysis uncovers a previously unnoticed issuewith the path length adaptation procedure of NUTS, specifically related tolooping behavior, which we address in detail.</description><author>Nawaf Bou-Rabee, Stefan Oberdörster</author><pubDate>Wed, 09 Oct 2024 15:17:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06978v1</guid></item></channel></rss>