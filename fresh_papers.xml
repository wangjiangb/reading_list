<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 28 May 2023 06:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2305.16322v1</link><description>Text-to-Image diffusion models have made tremendous progress over the pasttwo years, enabling the generation of highly realistic images based onopen-domain text descriptions. However, despite their success, textdescriptions often struggle to adequately convey detailed controls, even whencomposed of long and complex texts. Moreover, recent studies have also shownthat these models face challenges in understanding such complex texts andgenerating the corresponding images. Therefore, there is a growing need toenable more control modes beyond text description. In this paper, we introduceUni-ControlNet, a novel approach that allows for the simultaneous utilizationof different local controls (e.g., edge maps, depth map, segmentation masks)and global controls (e.g., CLIP image embeddings) in a flexible and composablemanner within one model. Unlike existing methods, Uni-ControlNet only requiresthe fine-tuning of two additional adapters upon frozen pre-trainedtext-to-image diffusion models, eliminating the huge cost of training fromscratch. Moreover, thanks to some dedicated adapter designs, Uni-ControlNetonly necessitates a constant number (i.e., 2) of adapters, regardless of thenumber of local or global controls used. This not only reduces the fine-tuningcosts and model size, making it more suitable for real-world deployment, butalso facilitate composability of different conditions. Through bothquantitative and qualitative comparisons, Uni-ControlNet demonstrates itssuperiority over existing methods in terms of controllability, generationquality and composability. Code is available at\url{https://github.com/ShihaoZhaoZSH/Uni-ControlNet}.</description><author>Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, Kwan-Yee K. Wong</author><pubDate>Thu, 25 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16322v1</guid></item><item><title>Eclipse: Disambiguating Illumination and Materials using Unintended Shadows</title><link>http://arxiv.org/abs/2305.16321v1</link><description>Decomposing an object's appearance into representations of its materials andthe surrounding illumination is difficult, even when the object's 3D shape isknown beforehand. This problem is ill-conditioned because diffuse materialsseverely blur incoming light, and is ill-posed because diffuse materials underhigh-frequency lighting can be indistinguishable from shiny materials underlow-frequency lighting. We show that it is possible to recover precisematerials and illumination -- even from diffuse objects -- by exploitingunintended shadows, like the ones cast onto an object by the photographer whomoves around it. These shadows are a nuisance in most previous inverserendering pipelines, but here we exploit them as signals that improveconditioning and help resolve material-lighting ambiguities. We present amethod based on differentiable Monte Carlo ray tracing that uses images of anobject to jointly recover its spatially-varying materials, the surroundingillumination environment, and the shapes of the unseen light occluders whoinadvertently cast shadows upon it.</description><author>Dor Verbin, Ben Mildenhall, Peter Hedman, Jonathan T. Barron, Todd Zickler, Pratul P. Srinivasan</author><pubDate>Thu, 25 May 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16321v1</guid></item><item><title>Image is First-order Norm+Linear Autoregressive</title><link>http://arxiv.org/abs/2305.16319v1</link><description>This paper reveals that every image can be understood as a first-ordernorm+linear autoregressive process, referred to as FINOLA, where norm+lineardenotes the use of normalization before the linear model. We demonstrate thatimages of size 256$\times$256 can be reconstructed from a compressed vectorusing autoregression up to a 16$\times$16 feature map, followed by upsamplingand convolution. This discovery sheds light on the underlying partialdifferential equations (PDEs) governing the latent feature space. Additionally,we investigate the application of FINOLA for self-supervised learning through asimple masked prediction technique. By encoding a single unmasked quadrantblock, we can autoregressively predict the surrounding masked region.Remarkably, this pre-trained representation proves effective for imageclassification and object detection tasks, even in lightweight networks,without requiring fine-tuning. The code will be made publicly available.</description><author>Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Lu Yuan, Zicheng Liu, Youzuo Lin</author><pubDate>Thu, 25 May 2023 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16319v1</guid></item><item><title>Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation</title><link>http://arxiv.org/abs/2305.16318v1</link><description>Recently, video object segmentation (VOS) referred by multi-modal signals,e.g., language and audio, has evoked increasing attention in both industry andacademia. It is challenging for exploring the semantic alignment withinmodalities and the visual correspondence across frames. However, existingmethods adopt separate network architectures for different modalities, andneglect the inter-frame temporal interaction with references. In this paper, wepropose MUTR, a Multi-modal Unified Temporal transformer for Referring videoobject segmentation. With a unified framework for the first time, MUTR adopts aDETR-style transformer and is capable of segmenting video objects designated byeither text or audio reference. Specifically, we introduce two strategies tofully explore the temporal relations between videos and multi-modal signals.Firstly, for low-level temporal aggregation before the transformer, we enablethe multi-modal references to capture multi-scale visual cues from consecutivevideo frames. This effectively endows the text or audio signals with temporalknowledge and boosts the semantic alignment between modalities. Secondly, forhigh-level temporal interaction after the transformer, we conduct inter-framefeature communication for different object embeddings, contributing to betterobject-wise correspondence for tracking along the video. On Ref-YouTube-VOS andAVSBench datasets with respective text and audio references, MUTR achieves+4.2% and +4.2% J&amp;F improvements to state-of-the-art methods, demonstrating oursignificance for unified multi-modal VOS. Code is released athttps://github.com/OpenGVLab/MUTR.</description><author>Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, Wei Zhang, Hongyang Li, Yu Qiao, Zhongjiang He, Peng Gao</author><pubDate>Thu, 25 May 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16318v1</guid></item><item><title>Parallel Sampling of Diffusion Models</title><link>http://arxiv.org/abs/2305.16317v1</link><description>Diffusion models are powerful generative models but suffer from slowsampling, often taking 1000 sequential denoising steps for one sample. As aresult, considerable efforts have been directed toward reducing the number ofdenoising steps, but these methods hurt sample quality. Instead of reducing thenumber of denoising steps (trading quality for speed), in this paper we explorean orthogonal approach: can we run the denoising steps in parallel (tradingcompute for speed)? In spite of the sequential nature of the denoising steps,we show that surprisingly it is possible to parallelize sampling via Picarditerations, by guessing the solution of future denoising steps and iterativelyrefining until convergence. With this insight, we present ParaDiGMS, a novelmethod to accelerate the sampling of pretrained diffusion models by denoisingmultiple steps in parallel. ParaDiGMS is the first diffusion sampling methodthat enables trading compute for speed and is even compatible with existingfast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, weimprove sampling speed by 2-4x across a range of robotics and image generationmodels, giving state-of-the-art sampling speeds of 0.2s on 100-stepDiffusionPolicy and 16s on 1000-step StableDiffusion-v2 with no measurabledegradation of task reward, FID score, or CLIP score.</description><author>Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, Nima Anari</author><pubDate>Thu, 25 May 2023 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16317v1</guid></item><item><title>Making Vision Transformers Truly Shift-Equivariant</title><link>http://arxiv.org/abs/2305.16316v1</link><description>For computer vision tasks, Vision Transformers (ViTs) have become one of thego-to deep net architectures. Despite being inspired by Convolutional NeuralNetworks (CNNs), ViTs remain sensitive to small shifts in the input image. Toaddress this, we introduce novel designs for each of the modules in ViTs, suchas tokenization, self-attention, patch merging, and positional encoding. Withour proposed modules, we achieve truly shift-equivariant ViTs on fourwell-established models, namely, Swin, SwinV2, MViTv2, and CvT, both in theoryand practice. Empirically, we tested these models on image classification andsemantic segmentation, achieving competitive performance across three differentdatasets while maintaining 100% shift consistency.</description><author>Renan A. Rojas-Gomez, Teck-Yian Lim, Minh N. Do, Raymond A. Yeh</author><pubDate>Thu, 25 May 2023 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16316v1</guid></item><item><title>NAP: Neural 3D Articulation Prior</title><link>http://arxiv.org/abs/2305.16315v1</link><description>We propose Neural 3D Articulation Prior (NAP), the first 3D deep generativemodel to synthesize 3D articulated object models. Despite the extensiveresearch on generating 3D objects, compositions, or scenes, there remains alack of focus on capturing the distribution of articulated objects, a commonobject category for human and robot interaction. To generate articulatedobjects, we first design a novel articulation tree/graph parameterization andthen apply a diffusion-denoising probabilistic model over this representationwhere articulated objects can be generated via denoising from random completegraphs. In order to capture both the geometry and the motion structure whosedistribution will affect each other, we design a graph-attention denoisingnetwork for learning the reverse diffusion process. We propose a novel distancethat adapts widely used 3D generation metrics to our novel task to evaluategeneration quality, and experiments demonstrate our high performance inarticulated object generation. We also demonstrate several conditionedgeneration applications, including Part2Motion, PartNet-Imagination,Motion2Part, and GAPart2Object.</description><author>Jiahui Lei, Congyue Deng, Bokui Shen, Leonidas Guibas, Kostas Daniilidis</author><pubDate>Thu, 25 May 2023 18:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16315v1</guid></item><item><title>Banana: Banach Fixed-Point Network for Pointcloud Segmentation with Inter-Part Equivariance</title><link>http://arxiv.org/abs/2305.16314v1</link><description>Equivariance has gained strong interest as a desirable network property thatinherently ensures robust generalization. However, when dealing with complexsystems such as articulated objects or multi-object scenes, effectivelycapturing inter-part transformations poses a challenge, as it becomes entangledwith the overall structure and local transformations. The interdependence ofpart assignment and per-part group action necessitates a novel equivarianceformulation that allows for their co-evolution. In this paper, we presentBanana, a Banach fixed-point network for equivariant segmentation withinter-part equivariance by construction. Our key insight is to iterativelysolve a fixed-point problem, where point-part assignment labels and per-partSE(3)-equivariance co-evolve simultaneously. We provide theoretical derivationsof both per-step equivariance and global convergence, which induces anequivariant final convergent state. Our formulation naturally provides a strictdefinition of inter-part equivariance that generalizes to unseen inter-partconfigurations. Through experiments conducted on both articulated objects andmulti-object scans, we demonstrate the efficacy of our approach in achievingstrong generalization under inter-part transformations, even when confrontedwith substantial changes in pointcloud geometry and topology.</description><author>Congyue Deng, Jiahui Lei, Bokui Shen, Kostas Daniilidis, Leonidas Guibas</author><pubDate>Thu, 25 May 2023 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16314v1</guid></item><item><title>A Small Gain Analysis of Single Timescale Actor Critic</title><link>http://arxiv.org/abs/2203.02591v4</link><description>We consider a version of actor-critic which uses proportional step-sizes andonly one critic update with a single sample from the stationary distributionper actor step. We provide an analysis of this method using the small-gaintheorem. Specifically, we prove that this method can be used to find astationary point, and that the resulting sample complexity improves the stateof the art for actor-critic methods to $O \left(\mu^{-2} \epsilon^{-2} \right)$to find an $\epsilon$-approximate stationary point where $\mu$ is the conditionnumber associated with the critic.</description><author>Alex Olshevsky, Bahman Gharesifard</author><pubDate>Thu, 25 May 2023 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.02591v4</guid></item><item><title>Break-A-Scene: Extracting Multiple Concepts from a Single Image</title><link>http://arxiv.org/abs/2305.16311v1</link><description>Text-to-image model personalization aims to introduce a user-provided conceptto the model, allowing its synthesis in diverse contexts. However, currentmethods primarily focus on the case of learning a single concept from multipleimages with variations in backgrounds and poses, and struggle when adapted to adifferent scenario. In this work, we introduce the task of textual scenedecomposition: given a single image of a scene that may contain severalconcepts, we aim to extract a distinct text token for each concept, enablingfine-grained control over the generated scenes. To this end, we proposeaugmenting the input image with masks that indicate the presence of targetconcepts. These masks can be provided by the user or generated automatically bya pre-trained segmentation model. We then present a novel two-phasecustomization process that optimizes a set of dedicated textual embeddings(handles), as well as the model weights, striking a delicate balance betweenaccurately capturing the concepts and avoiding overfitting. We employ a maskeddiffusion loss to enable handles to generate their assigned concepts,complemented by a novel loss on cross-attention maps to prevent entanglement.We also introduce union-sampling, a training strategy aimed to improve theability of combining multiple concepts in generated images. We use severalautomatic metrics to quantitatively compare our method against severalbaselines, and further affirm the results using a user study. Finally, weshowcase several applications of our method. Project page is available at:https://omriavrahami.com/break-a-scene/</description><author>Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, Dani Lischinski</author><pubDate>Thu, 25 May 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16311v1</guid></item><item><title>UMat: Uncertainty-Aware Single Image High Resolution Material Capture</title><link>http://arxiv.org/abs/2305.16312v1</link><description>We propose a learning-based method to recover normals, specularity, androughness from a single diffuse image of a material, using microgeometryappearance as our primary cue. Previous methods that work on single images tendto produce over-smooth outputs with artifacts, operate at limited resolution,or train one model per class with little room for generalization. Previousmethods that work on single images tend to produce over-smooth outputs withartifacts, operate at limited resolution, or train one model per class withlittle room for generalization. In contrast, in this work, we propose a novelcapture approach that leverages a generative network with attention and a U-Netdiscriminator, which shows outstanding performance integrating globalinformation at reduced computational complexity. We showcase the performance ofour method with a real dataset of digitized textile materials and show that acommodity flatbed scanner can produce the type of diffuse illumination requiredas input to our method. Additionally, because the problem might be illposed-more than a single diffuse image might be needed to disambiguate the specularreflection- or because the training dataset is not representative enough of thereal distribution, we propose a novel framework to quantify the model'sconfidence about its prediction at test time. Our method is the first one todeal with the problem of modeling uncertainty in material digitization,increasing the trustworthiness of the process and enabling more intelligentstrategies for dataset creation, as we demonstrate with an active learningexperiment.</description><author>Carlos Rodriguez-Pardo, Henar Dominguez-Elvira, David Pascual-Hernandez, Elena Garces</author><pubDate>Thu, 25 May 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16312v1</guid></item><item><title>Securing Deep Generative Models with Universal Adversarial Signature</title><link>http://arxiv.org/abs/2305.16310v1</link><description>Recent advances in deep generative models have led to the development ofmethods capable of synthesizing high-quality, realistic images. These modelspose threats to society due to their potential misuse. Prior research attemptedto mitigate these threats by detecting generated images, but the varying tracesleft by different generative models make it challenging to create a universaldetector capable of generalizing to new, unseen generative models. In thispaper, we propose to inject a universal adversarial signature into an arbitrarypre-trained generative model, in order to make its generated contents moredetectable and traceable. First, the imperceptible optimal signature for eachimage can be found by a signature injector through adversarial training.Subsequently, the signature can be incorporated into an arbitrary generator byfine-tuning it with the images processed by the signature injector. In thisway, the detector corresponding to the signature can be reused for anyfine-tuned generator for tracking the generator identity. The proposed methodis validated on the FFHQ and ImageNet datasets with various state-of-the-artgenerative models, consistently showing a promising detection rate. Code willbe made publicly available at \url{https://github.com/zengxianyu/genwm}.</description><author>Yu Zeng, Mo Zhou, Yuan Xue, Vishal M. Patel</author><pubDate>Thu, 25 May 2023 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16310v1</guid></item><item><title>Imitating Task and Motion Planning with Visuomotor Transformers</title><link>http://arxiv.org/abs/2305.16309v1</link><description>Imitation learning is a powerful tool for training robot manipulationpolicies, allowing them to learn from expert demonstrations without manualprogramming or trial-and-error. However, common methods of data collection,such as human supervision, scale poorly, as they are time-consuming andlabor-intensive. In contrast, Task and Motion Planning (TAMP) can autonomouslygenerate large-scale datasets of diverse demonstrations. In this work, we showthat the combination of large-scale datasets generated by TAMP supervisors andflexible Transformer models to fit them is a powerful paradigm for robotmanipulation. To that end, we present a novel imitation learning system calledOPTIMUS that trains large-scale visuomotor Transformer policies by imitating aTAMP agent. OPTIMUS introduces a pipeline for generating TAMP data that isspecifically curated for imitation learning and can be used to train performanttransformer-based policies. In this paper, we present a thorough study of thedesign decisions required to imitate TAMP and demonstrate that OPTIMUS cansolve a wide variety of challenging vision-based manipulation tasks with over70 different objects, ranging from long-horizon pick-and-place tasks, to shelfand articulated object manipulation, achieving 70 to 80% success rates. Videoresults at https://mihdalal.github.io/optimus/</description><author>Murtaza Dalal, Ajay Mandlekar, Caelan Garrett, Ankur Handa, Ruslan Salakhutdinov, Dieter Fox</author><pubDate>Thu, 25 May 2023 18:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16309v1</guid></item><item><title>Rectifying Group Irregularities in Explanations for Distribution Shift</title><link>http://arxiv.org/abs/2305.16308v1</link><description>It is well-known that real-world changes constituting distribution shiftadversely affect model performance. How to characterize those changes in aninterpretable manner is poorly understood. Existing techniques to address thisproblem take the form of shift explanations that elucidate how to map samplesfrom the original distribution toward the shifted one by reducing the disparitybetween these two distributions. However, these methods can introduce groupirregularities, leading to explanations that are less feasible and robust. Toaddress these issues, we propose Group-aware Shift Explanations (GSE), a methodthat produces interpretable explanations by leveraging worst-group optimizationto rectify group irregularities. We demonstrate how GSE not only maintainsgroup structures, such as demographic and hierarchical subpopulations, but alsoenhances feasibility and robustness in the resulting explanations in a widerange of tabular, language, and image settings.</description><author>Adam Stein, Yinjun Wu, Eric Wong, Mayur Naik</author><pubDate>Thu, 25 May 2023 18:57:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16308v1</guid></item><item><title>IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages</title><link>http://arxiv.org/abs/2305.16307v1</link><description>India has a rich linguistic landscape with languages from 4 major languagefamilies spoken by over a billion people. 22 of these languages are listed inthe Constitution of India (referred to as scheduled languages) are the focus ofthis work. Given the linguistic diversity, high-quality and accessible MachineTranslation (MT) systems are essential in a country like India. Prior to thiswork, there was (i) no parallel training data spanning all the 22 languages,(ii) no robust benchmarks covering all these languages and containing contentrelevant to India, and (iii) no existing translation models which support allthe 22 scheduled languages of India. In this work, we aim to address this gapby focusing on the missing pieces required for enabling wide, easy, and openaccess to good machine translation systems for all 22 scheduled Indianlanguages. We identify four key areas of improvement: curating and creatinglarger training datasets, creating diverse and high-quality benchmarks,training multilingual models, and releasing models with open access. Our firstcontribution is the release of the Bharat Parallel Corpus Collection (BPCC),the largest publicly available parallel corpora for Indic languages. BPCCcontains a total of 230M bitext pairs, of which a total of 126M were newlyadded, including 644K manually translated sentence pairs created as part ofthis work. Our second contribution is the release of the first n-way parallelbenchmark covering all 22 Indian languages, featuring diverse domains,Indian-origin content, and source-original test sets. Next, we presentIndicTrans2, the first model to support all 22 languages, surpassing existingmodels on multiple existing and new benchmarks created as a part of this work.Lastly, to promote accessibility and collaboration, we release our models andassociated data with permissive licenses athttps://github.com/ai4bharat/IndicTrans2.</description><author>AI4Bharat, Jay Gala, Pranjal A. Chitale, Raghavan AK, Sumanth Doddapaneni, Varun Gumma, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh M. Khapra, Raj Dabre, Anoop Kunchukuttan</author><pubDate>Thu, 25 May 2023 18:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16307v1</guid></item><item><title>Sequential Underspecified Instrument Selection for Cause-Effect Estimation</title><link>http://arxiv.org/abs/2302.05684v2</link><description>Instrumental variable (IV) methods are used to estimate causal effects insettings with unobserved confounding, where we cannot directly experiment onthe treatment variable. Instruments are variables which only affect the outcomeindirectly via the treatment variable(s). Most IV applications focus onlow-dimensional treatments and crucially require at least as many instrumentsas treatments. This assumption is restrictive: in the natural sciences we oftenseek to infer causal effects of high-dimensional treatments (e.g., the effectof gene expressions or microbiota on health and disease), but can only run fewexperiments with a limited number of instruments (e.g., drugs or antibiotics).In such underspecified problems, the full treatment effect is not identifiablein a single experiment even in the linear case. We show that one can stillreliably recover the projection of the treatment effect onto the instrumentedsubspace and develop techniques to consistently combine such partial estimatesfrom different sets of instruments. We then leverage our combined estimators inan algorithm that iteratively proposes the most informative instruments at eachround of experimentation to maximize the overall information about the fullcausal effect.</description><author>Elisabeth Ailer, Jason Hartford, Niki Kilbertus</author><pubDate>Thu, 25 May 2023 18:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05684v2</guid></item><item><title>Fine-Grained Complexity Analysis of Multi-Agent Path Finding on 2D Grids</title><link>http://arxiv.org/abs/2305.16303v1</link><description>Multi-Agent Path Finding (MAPF) is a fundamental motion coordination problemarising in multi-agent systems with a wide range of applications. The problem'sintractability has led to extensive research on improving the scalability ofsolvers for it. Since optimal solvers can struggle to scale, a major challengethat arises is understanding what makes MAPF hard. We tackle this challengethrough a fine-grained complexity analysis of time-optimal MAPF on 2D grids,thereby closing two gaps and identifying a new tractability frontier. First, weshow that 2-colored MAPF, i.e., where the agents are divided into two teams,each with its own set of targets, remains NP-hard. Second, for the flowtimeobjective (also called sum-of-costs), we show that it remains NP-hard to find asolution in which agents have an individually optimal cost, which we call anindividually optimal solution. The previously tightest results for these MAPFvariants are for (non-grid) planar graphs. We use a single hardnessconstruction that replaces, strengthens, and unifies previous proofs. Webelieve that it is also simpler than previous proofs for the planar case as itemploys minimal gadgets that enable its full visualization in one figure.Finally, for the flowtime objective, we establish a tractability frontier basedon the number of directions agents can move in. Namely, we complement ourhardness result, which holds for three directions, with an efficient algorithmfor finding an individually optimal solution if only two directions areallowed. This result sheds new light on the structure of optimal solutions,which may help guide algorithm design for the general problem.</description><author>Tzvika Geft</author><pubDate>Thu, 25 May 2023 18:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16303v1</guid></item><item><title>Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder</title><link>http://arxiv.org/abs/2305.16304v1</link><description>Composed image retrieval aims to find an image that best matches a givenmulti-modal user query consisting of a reference image and text pair. Existingmethods commonly pre-compute image embeddings over the entire corpus andcompare these to a reference image embedding modified by the query text at testtime. Such a pipeline is very efficient at test time since fast vectordistances can be used to evaluate candidates, but modifying the reference imageembedding guided only by a short textual description can be difficult,especially independent of potential candidates. An alternative approach is toallow interactions between the query and every possible candidate, i.e.,reference-text-candidate triplets, and pick the best from the entire set.Though this approach is more discriminative, for large-scale datasets thecomputational cost is prohibitive since pre-computation of candidate embeddingsis no longer possible. We propose to combine the merits of both schemes using atwo-stage model. Our first stage adopts the conventional vector distancingmetric and performs a fast pruning among candidates. Meanwhile, our secondstage employs a dual-encoder architecture, which effectively attends to theinput triplet of reference-text-candidate and re-ranks the candidates. Bothstages utilize a vision-and-language pre-trained network, which has provenbeneficial for various downstream tasks. Our method consistently outperformsstate-of-the-art approaches on standard benchmarks for the task.</description><author>Zheyuan Liu, Weixuan Sun, Damien Teney, Stephen Gould</author><pubDate>Thu, 25 May 2023 18:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16304v1</guid></item><item><title>Cross-Lingual Knowledge Distillation for Answer Sentence Selection in Low-Resource Languages</title><link>http://arxiv.org/abs/2305.16302v1</link><description>While impressive performance has been achieved on the task of Answer SentenceSelection (AS2) for English, the same does not hold for languages that lacklarge labeled datasets. In this work, we propose Cross-Lingual KnowledgeDistillation (CLKD) from a strong English AS2 teacher as a method to train AS2models for low-resource languages in the tasks without the need of labeled datafor the target language. To evaluate our method, we introduce 1) Xtr-WikiQA, atranslation-based WikiQA dataset for 9 additional languages, and 2) TyDi-AS2, amultilingual AS2 dataset with over 70K questions spanning 8 typologicallydiverse languages. We conduct extensive experiments on Xtr-WikiQA and TyDi-AS2with multiple teachers, diverse monolingual and multilingual pretrainedlanguage models (PLMs) as students, and both monolingual and multilingualtraining. The results demonstrate that CLKD either outperforms or rivals evensupervised fine-tuning with the same amount of labeled data and a combinationof machine translation and the teacher model. Our method can potentially enablestronger AS2 models for low-resource languages, while TyDi-AS2 can serve as thelargest multilingual AS2 dataset for further studies in the research community.</description><author>Shivanshu Gupta, Yoshitomo Matsubara, Ankit Chadha, Alessandro Moschitti</author><pubDate>Thu, 25 May 2023 18:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16302v1</guid></item><item><title>Look Ma, No Hands! Agent-Environment Factorization of Egocentric Videos</title><link>http://arxiv.org/abs/2305.16301v1</link><description>The analysis and use of egocentric videos for robotic tasks is madechallenging by occlusion due to the hand and the visual mismatch between thehuman hand and a robot end-effector. In this sense, the human hand presents anuisance. However, often hands also provide a valuable signal, e.g. the handpose may suggest what kind of object is being held. In this work, we propose toextract a factored representation of the scene that separates the agent (humanhand) and the environment. This alleviates both occlusion and mismatch whilepreserving the signal, thereby easing the design of models for downstreamrobotics tasks. At the heart of this factorization is our proposed VideoInpainting via Diffusion Model (VIDM) that leverages both a prior on real-worldimages (through a large-scale pre-trained diffusion model) and the appearanceof the object in earlier frames of the video (through attention). Ourexperiments demonstrate the effectiveness of VIDM at improving inpaintingquality on egocentric videos and the power of our factored representation fornumerous tasks: object detection, 3D reconstruction of manipulated objects, andlearning of reward functions, policies, and affordances from videos.</description><author>Matthew Chang, Aditya Prakash, Saurabh Gupta</author><pubDate>Thu, 25 May 2023 18:55:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16301v1</guid></item><item><title>SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2)</title><link>http://arxiv.org/abs/2305.06586v2</link><description>We present the findings of SemEval-2023 Task 2 on Fine-grained MultilingualNamed Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the taskfocused on methods to identify complex fine-grained named entities (likeWRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual andmultilingual scenarios, as well as noisy settings. The task used the MultiCoNERV2 dataset, composed of 2.2 million instances in Bangla, Chinese, English,Farsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, andUkrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. Itattracted 842 submissions from 47 teams, and 34 teams submitted system papers.Results showed that complex entity types such as media titles and product nameswere the most challenging. Methods fusing external knowledge into transformermodels achieved the best performance, and the largest gains were on theCreative Work and Group classes, which are still challenging even with externalknowledge. Some fine-grained classes proved to be more challenging than others,such as SCIENTIST, ARTWORK, and PRIVATECORP. We also observed that noisy datahas a significant impact on model performance, with an average drop of 10% onthe noisy subset. The task highlights the need for future research on improvingNER robustness on noisy data containing complex entities.</description><author>Besnik Fetahu, Sudipta Kar, Zhiyu Chen, Oleg Rokhlenko, Shervin Malmasi</author><pubDate>Thu, 25 May 2023 18:54:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06586v2</guid></item><item><title>Landmark Attention: Random-Access Infinite Context Length for Transformers</title><link>http://arxiv.org/abs/2305.16300v1</link><description>While transformers have shown remarkable success in natural languageprocessing, their attention mechanism's large memory requirements have limitedtheir ability to handle longer contexts. Prior approaches, such as recurrentmemory or retrieval-based augmentation, have either compromised therandom-access flexibility of attention (i.e., the capability to select anytoken in the entire context) or relied on separate mechanisms for relevantcontext retrieval, which may not be compatible with the model's attention. Inthis paper, we present a novel approach that allows access to the completecontext while retaining random-access flexibility, closely resembling runningattention on the entire context. Our method uses a landmark token to representeach block of the input and trains the attention to use it for selectingrelevant blocks, enabling retrieval of blocks directly through the attentionmechanism instead of by relying on a separate mechanism. Our approachseamlessly integrates with specialized data structures and the system's memoryhierarchy, enabling processing of arbitrarily long context lengths. Wedemonstrate that our method can obtain comparable performance withTransformer-XL while significantly reducing the number of retrieved tokens ineach step. Finally, we show that fine-tuning LLaMA 7B with our methodsuccessfully extends its context length capacity up to 32k tokens, allowing forinference at the context lengths of GPT-4.</description><author>Amirkeivan Mohtashami, Martin Jaggi</author><pubDate>Thu, 25 May 2023 18:53:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16300v1</guid></item><item><title>Memory-Based Meta-Learning on Non-Stationary Distributions</title><link>http://arxiv.org/abs/2302.03067v2</link><description>Memory-based meta-learning is a technique for approximating Bayes-optimalpredictors. Under fairly general conditions, minimizing sequential predictionerror, measured by the log loss, leads to implicit meta-learning. The goal ofthis work is to investigate how far this interpretation can be realized bycurrent sequence prediction models and training regimes. The focus is onpiecewise stationary sources with unobserved switching-points, which arguablycapture an important characteristic of natural language and action-observationsequences in partially observable environments. We show that various types ofmemory-based neural models, including Transformers, LSTMs, and RNNs can learnto accurately approximate known Bayes-optimal algorithms and behave as ifperforming Bayesian inference over the latent switching-points and the latentparameters governing the data distribution within each segment.</description><author>Tim Genewein, Grégoire Delétang, Anian Ruoss, Li Kevin Wenliang, Elliot Catt, Vincent Dutordoir, Jordi Grau-Moya, Laurent Orseau, Marcus Hutter, Joel Veness</author><pubDate>Thu, 25 May 2023 18:53:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03067v2</guid></item><item><title>Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?</title><link>http://arxiv.org/abs/2305.16297v1</link><description>Communication compression is a common technique in distributed optimizationthat can alleviate communication overhead by transmitting compressed gradientsand model parameters. However, compression can introduce informationdistortion, which slows down convergence and incurs more communication roundsto achieve desired solutions. Given the trade-off between lower per-roundcommunication costs and additional rounds of communication, it is unclearwhether communication compression reduces the total communication cost. This paper explores the conditions under which unbiased compression, a widelyused form of compression, can reduce the total communication cost, as well asthe extent to which it can do so. To this end, we present the first theoreticalformulation for characterizing the total communication cost in distributedoptimization with communication compression. We demonstrate that unbiasedcompression alone does not necessarily save the total communication cost, butthis outcome can be achieved if the compressors used by all workers are furtherassumed independent. We establish lower bounds on the communication roundsrequired by algorithms using independent unbiased compressors to minimizesmooth convex functions, and show that these lower bounds are tight by refiningthe analysis for ADIANA. Our results reveal that using independent unbiasedcompression can reduce the total communication cost by a factor of up to$\Theta(\sqrt{\min\{n, \kappa\}})$, where $n$ is the number of workers and$\kappa$ is the condition number of the functions being minimized. Thesetheoretical findings are supported by experimental results.</description><author>Yutong He, Xinmeng Huang, Kun Yuan</author><pubDate>Thu, 25 May 2023 18:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16297v1</guid></item><item><title>A Guide Through the Zoo of Biased SGD</title><link>http://arxiv.org/abs/2305.16296v1</link><description>Stochastic Gradient Descent (SGD) is arguably the most important singlealgorithm in modern machine learning. Although SGD with unbiased gradientestimators has been studied extensively over at least half a century, SGDvariants relying on biased estimators are rare. Nevertheless, there has been anincreased interest in this topic in recent years. However, existing literatureon SGD with biased estimators (BiasedSGD) lacks coherence since each new paperrelies on a different set of assumptions, without any clear understanding ofhow they are connected, which may lead to confusion. We address this gap byestablishing connections among the existing assumptions, and presenting acomprehensive map of the underlying relationships. Additionally, we introduce anew set of assumptions that is provably weaker than all previous assumptions,and use it to present a thorough analysis of BiasedSGD in both convex andnon-convex settings, offering advantages over previous results. We also provideexamples where biased estimators outperform their unbiased counterparts orwhere unbiased versions are simply not available. Finally, we demonstrate theeffectiveness of our framework through experimental results that validate ourtheoretical findings.</description><author>Yury Demidovich, Grigory Malinovsky, Igor Sokolov, Peter Richtárik</author><pubDate>Thu, 25 May 2023 18:50:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16296v1</guid></item><item><title>HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning</title><link>http://arxiv.org/abs/2305.16295v1</link><description>A great deal of progress has been made in image captioning, driven byresearch into how to encode the image using pre-trained models. This includesvisual encodings (e.g. image grid features or detected objects) and morerecently textual encodings (e.g. image tags or text descriptions of imageregions). As more advanced encodings are available and incorporated, it isnatural to ask: how to efficiently and effectively leverage the heterogeneousset of encodings? In this paper, we propose to regard the encodings asaugmented views of the input image. The image captioning model encodes eachview independently with a shared encoder efficiently, and a contrastive loss isincorporated across the encoded views in a novel way to improve theirrepresentation quality and the model's data efficiency. Our proposedhierarchical decoder then adaptively weighs the encoded views according totheir effectiveness for caption generation by first aggregating within eachview at the token level, and then across views at the view level. Wedemonstrate significant performance improvements of +5.6% CIDEr on MS-COCO and+12.9% CIDEr on Flickr30k compared to state of the arts, and conduct rigorousanalyses to demonstrate the importance of each part of our design.</description><author>Chia-Wen Kuo, Zsolt Kira</author><pubDate>Thu, 25 May 2023 18:50:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16295v1</guid></item><item><title>Sharpness-Aware Minimization Leads to Low-Rank Features</title><link>http://arxiv.org/abs/2305.16292v1</link><description>Sharpness-aware minimization (SAM) is a recently proposed method thatminimizes the sharpness of the training loss of a neural network. While itsgeneralization improvement is well-known and is the primary motivation, weuncover an additional intriguing effect of SAM: reduction of the feature rankwhich happens at different layers of a neural network. We show that thislow-rank effect occurs very broadly: for different architectures such asfully-connected networks, convolutional networks, vision transformers and fordifferent objectives such as regression, classification, language-imagecontrastive training. To better understand this phenomenon, we provide amechanistic understanding of how low-rank features arise in a simple two-layernetwork. We observe that a significant number of activations gets entirelypruned by SAM which directly contributes to the rank reduction. We confirm thiseffect theoretically and check that it can also occur in deep networks,although the overall rank reduction mechanism can be more complex, especiallyfor deep networks with pre-activation skip connections and self-attentionlayers. We make our code available athttps://github.com/tml-epfl/sam-low-rank-features.</description><author>Maksym Andriushchenko, Dara Bahri, Hossein Mobahi, Nicolas Flammarion</author><pubDate>Thu, 25 May 2023 18:46:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16292v1</guid></item><item><title>Voyager: An Open-Ended Embodied Agent with Large Language Models</title><link>http://arxiv.org/abs/2305.16291v1</link><description>We introduce Voyager, the first LLM-powered embodied lifelong learning agentin Minecraft that continuously explores the world, acquires diverse skills, andmakes novel discoveries without human intervention. Voyager consists of threekey components: 1) an automatic curriculum that maximizes exploration, 2) anever-growing skill library of executable code for storing and retrievingcomplex behaviors, and 3) a new iterative prompting mechanism that incorporatesenvironment feedback, execution errors, and self-verification for programimprovement. Voyager interacts with GPT-4 via blackbox queries, which bypassesthe need for model parameter fine-tuning. The skills developed by Voyager aretemporally extended, interpretable, and compositional, which compounds theagent's abilities rapidly and alleviates catastrophic forgetting. Empirically,Voyager shows strong in-context lifelong learning capability and exhibitsexceptional proficiency in playing Minecraft. It obtains 3.3x more uniqueitems, travels 2.3x longer distances, and unlocks key tech tree milestones upto 15.3x faster than prior SOTA. Voyager is able to utilize the learned skilllibrary in a new Minecraft world to solve novel tasks from scratch, while othertechniques struggle to generalize. We open-source our full codebase and promptsat https://voyager.minedojo.org/.</description><author>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar</author><pubDate>Thu, 25 May 2023 18:46:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16291v1</guid></item><item><title>Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation</title><link>http://arxiv.org/abs/2305.16289v1</link><description>Many fine-grained classification tasks, like rare animal identification, havelimited training data and consequently classifiers trained on these datasetsoften fail to generalize to variations in the domain like changes in weather orlocation. As such, we explore how natural language descriptions of the domainsseen in training data can be used with large vision models trained on diversepretraining datasets to generate useful variations of the training data. Weintroduce ALIA (Automated Language-guided Image Augmentation), a method whichutilizes large vision and language models to automatically generate naturallanguage descriptions of a dataset's domains and augment the training data vialanguage-guided image editing. To maintain data integrity, a model trained onthe original dataset filters out minimal image edits and those which corruptclass-relevant information. The resulting dataset is visually consistent withthe original training data and offers significantly enhanced diversity. Onfine-grained and cluttered datasets for classification and detection, ALIAsurpasses traditional data augmentation and text-to-image generated data by upto 15\%, often even outperforming equivalent additions of real data. Code isavilable at https://github.com/lisadunlap/ALIA.</description><author>Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E. Gonzalez, Trevor Darrell</author><pubDate>Thu, 25 May 2023 18:43:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16289v1</guid></item><item><title>Reversible Graph Neural Network-based Reaction Distribution Learning for Multiple Appropriate Facial Reactions Generation</title><link>http://arxiv.org/abs/2305.15270v2</link><description>Generating facial reactions in a human-human dyadic interaction is complexand highly dependent on the context since more than one facial reactions can beappropriate for the speaker's behaviour. This has challenged existing machinelearning (ML) methods, whose training strategies enforce models to reproduce aspecific (not multiple) facial reaction from each input speaker behaviour. Thispaper proposes the first multiple appropriate facial reaction generationframework that re-formulates the one-to-many mapping facial reaction generationproblem as a one-to-one mapping problem. This means that we approach thisproblem by considering the generation of a distribution of the listener'sappropriate facial reactions instead of multiple different appropriate facialreactions, i.e., 'many' appropriate facial reaction labels are summarised as'one' distribution label during training. Our model consists of a perceptualprocessor, a cognitive processor, and a motor processor. The motor processor isimplemented with a novel Reversible Multi-dimensional Edge Graph Neural Network(REGNN). This allows us to obtain a distribution of appropriate real facialreactions during the training process, enabling the cognitive processor to betrained to predict the appropriate facial reaction distribution. At theinference stage, the REGNN decodes an appropriate facial reaction by using thisdistribution as input. Experimental results demonstrate that our approachoutperforms existing models in generating more appropriate, realistic, andsynchronized facial reactions. The improved performance is largely attributedto the proposed appropriate facial reaction distribution learning strategy andthe use of a REGNN. The code is available athttps://github.com/TongXu-05/REGNN-Multiple-Appropriate-Facial-Reaction-Generation.</description><author>Tong Xu, Micol Spitale, Hao Tang, Lu Liu, Hatice Gunes, Siyang Song</author><pubDate>Thu, 25 May 2023 18:41:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15270v2</guid></item><item><title>UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild</title><link>http://arxiv.org/abs/2305.11147v2</link><description>Achieving machine autonomy and human control often represent divergentobjectives in the design of interactive AI systems. Visual generativefoundation models such as Stable Diffusion show promise in navigating thesegoals, especially when prompted with arbitrary languages. However, they oftenfall short in generating images with spatial, structural, or geometriccontrols. The integration of such controls, which can accommodate variousvisual conditions in a single unified model, remains an unaddressed challenge.In response, we introduce UniControl, a new generative foundation model thatconsolidates a wide array of controllable condition-to-image (C2I) tasks withina singular framework, while still allowing for arbitrary language prompts.UniControl enables pixel-level-precise image generation, where visualconditions primarily influence the generated structures and language promptsguide the style and context. To equip UniControl with the capacity to handlediverse visual conditions, we augment pretrained text-to-image diffusion modelsand introduce a task-aware HyperNet to modulate the diffusion models, enablingthe adaptation to different C2I tasks simultaneously. Trained on nine uniqueC2I tasks, UniControl demonstrates impressive zero-shot generation abilitieswith unseen visual conditions. Experimental results show that UniControl oftensurpasses the performance of single-task-controlled methods of comparable modelsizes. This control versatility positions UniControl as a significantadvancement in the realm of controllable visual generation.</description><author>Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, Ran Xu</author><pubDate>Thu, 25 May 2023 18:41:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11147v2</guid></item><item><title>DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method</title><link>http://arxiv.org/abs/2305.16284v1</link><description>This paper proposes a new easy-to-implement parameter-free gradient-basedoptimizer: DoWG (Distance over Weighted Gradients). We prove that DoWG isefficient -- matching the convergence rate of optimally tuned gradient descentin convex optimization up to a logarithmic factor without tuning anyparameters, and universal -- automatically adapting to both smooth andnonsmooth problems. While popular algorithms such as AdaGrad, Adam, or DoGcompute a running average of the squared gradients, DoWG maintains a newdistance-based weighted version of the running average, which is crucial toachieve the desired properties. To our best knowledge, DoWG is the firstparameter-free, efficient, and universal algorithm that does not requirebacktracking search procedures. It is also the first parameter-free AdaGradstyle algorithm that adapts to smooth optimization. To complement our theory,we also show empirically that DoWG trains at the edge of stability, andvalidate its effectiveness on practical machine learning tasks. This paper further uncovers the underlying principle behind the success ofthe AdaGrad family of algorithms by presenting a novel analysis of NormalizedGradient Descent (NGD), that shows NGD adapts to smoothness when it exists,with no change to the stepsize. This establishes the universality of NGD andpartially explains the empirical observation that it trains at the edge ofstability in a much more general setup compared to standard gradient descent.The latter might be of independent interest to the community.</description><author>Ahmed Khaled, Konstantin Mishchenko, Chi Jin</author><pubDate>Thu, 25 May 2023 18:40:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16284v1</guid></item><item><title>CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graphs</title><link>http://arxiv.org/abs/2305.16283v1</link><description>Controllable scene synthesis aims to create interactive environments forvarious industrial use cases. Scene graphs provide a highly suitable interfaceto facilitate these applications by abstracting the scene context in a compactmanner. Existing methods, reliant on retrieval from extensive databases orpre-trained shape embeddings, often overlook scene-object and object-objectrelationships, leading to inconsistent results due to their limited generationcapacity. To address this issue, we present CommonScenes, a fully generativemodel that converts scene graphs into corresponding controllable 3D scenes,which are semantically realistic and conform to commonsense. Our pipelineconsists of two branches, one predicting the overall scene layout via avariational auto-encoder and the other generating compatible shapes via latentdiffusion, capturing global scene-object and local inter-object relationshipswhile preserving shape diversity. The generated scenes can be manipulated byediting the input scene graph and sampling the noise in the diffusion model.Due to lacking a scene graph dataset offering high-quality object-level mesheswith relations, we also construct SG-FRONT, enriching the off-the-shelf indoordataset 3D-FRONT with additional scene graph labels. Extensive experiments areconducted on SG-FRONT where CommonScenes shows clear advantages over othermethods regarding generation consistency, quality, and diversity. Codes and thedataset will be released upon acceptance.</description><author>Guangyao Zhai, Evin Pinar Örnek, Shun-Cheng Wu, Yan Di, Federico Tombari, Nassir Navab, Benjamin Busam</author><pubDate>Thu, 25 May 2023 18:39:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16283v1</guid></item><item><title>An Analysis of Quantile Temporal-Difference Learning</title><link>http://arxiv.org/abs/2301.04462v2</link><description>We analyse quantile temporal-difference learning (QTD), a distributionalreinforcement learning algorithm that has proven to be a key component inseveral successful large-scale applications of reinforcement learning. Despitethese empirical successes, a theoretical understanding of QTD has provenelusive until now. Unlike classical TD learning, which can be analysed withstandard stochastic approximation tools, QTD updates do not approximatecontraction mappings, are highly non-linear, and may have multiple fixedpoints. The core result of this paper is a proof of convergence to the fixedpoints of a related family of dynamic programming procedures with probability1, putting QTD on firm theoretical footing. The proof establishes connectionsbetween QTD and non-linear differential inclusions through stochasticapproximation theory and non-smooth analysis.</description><author>Mark Rowland, Rémi Munos, Mohammad Gheshlaghi Azar, Yunhao Tang, Georg Ostrovski, Anna Harutyunyan, Karl Tuyls, Marc G. Bellemare, Will Dabney</author><pubDate>Thu, 25 May 2023 18:37:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.04462v2</guid></item><item><title>CENSUS-HWR: a large training dataset for offline handwriting recognition</title><link>http://arxiv.org/abs/2305.16275v1</link><description>Progress in Automated Handwriting Recognition has been hampered by the lackof large training datasets. Nearly all research uses a set of small datasetsthat often cause models to overfit. We present CENSUS-HWR, a new datasetconsisting of full English handwritten words in 1,812,014 gray scale images. Atotal of 1,865,134 handwritten texts from a vocabulary of 10,711 words in theEnglish language are present in this collection. This dataset is intended toserve handwriting models as a benchmark for deep learning algorithms. This hugeEnglish handwriting recognition dataset has been extracted from the US 1930 and1940 censuses taken by approximately 70,000 enumerators each year. The datasetand the trained model with their weights are freely available to download athttps://censustree.org/data.html.</description><author>Chetan Joshi, Lawry Sorenson, Ammon Wolfert, Dr. Mark Clement, Dr. Joseph Price, Dr. Kasey Buckles</author><pubDate>Thu, 25 May 2023 18:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16275v1</guid></item><item><title>Non-adversarial training of Neural SDEs with signature kernel scores</title><link>http://arxiv.org/abs/2305.16274v1</link><description>Neural SDEs are continuous-time generative models for sequential data.State-of-the-art performance for irregular time series generation has beenpreviously obtained by training these models adversarially as GANs. However, astypical for GAN architectures, training is notoriously unstable, often suffersfrom mode collapse, and requires specialised techniques such as weight clippingand gradient penalty to mitigate these issues. In this paper, we introduce anovel class of scoring rules on pathspace based on signature kernels and usethem as objective for training Neural SDEs non-adversarially. By showing strictproperness of such kernel scores and consistency of the correspondingestimators, we provide existence and uniqueness guarantees for the minimiser.With this formulation, evaluating the generator-discriminator pair amounts tosolving a system of linear path-dependent PDEs which allows formemory-efficient adjoint-based backpropagation. Moreover, because the proposedkernel scores are well-defined for paths with values in infinite dimensionalspaces of functions, our framework can be easily extended to generatespatiotemporal data. Our procedure permits conditioning on a rich variety ofmarket conditions and significantly outperforms alternative ways of trainingNeural SDEs on a variety of tasks including the simulation of rough volatilitymodels, the conditional probabilistic forecasts of real-world forex pairs wherethe conditioning variable is an observed past trajectory, and the mesh-freegeneration of limit order book dynamics.</description><author>Zacharia Issa, Blanka Horvath, Maud Lemercier, Cristopher Salvi</author><pubDate>Thu, 25 May 2023 18:31:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16274v1</guid></item><item><title>Incentivizing Honesty among Competitors in Collaborative Learning and Optimization</title><link>http://arxiv.org/abs/2305.16272v1</link><description>Collaborative learning techniques have the potential to enable trainingmachine learning models that are superior to models trained on a singleentity's data. However, in many cases, potential participants in suchcollaborative schemes are competitors on a downstream task, such as firms thateach aim to attract customers by providing the best recommendations. This canincentivize dishonest updates that damage other participants' models,potentially undermining the benefits of collaboration. In this work, weformulate a game that models such interactions and study two learning taskswithin this framework: single-round mean estimation and multi-round SGD onstrongly-convex objectives. For a natural class of player actions, we show thatrational clients are incentivized to strongly manipulate their updates,preventing learning. We then propose mechanisms that incentivize honestcommunication and ensure learning quality comparable to full cooperation.Lastly, we empirically demonstrate the effectiveness of our incentive scheme ona standard non-convex federated learning benchmark. Our work shows thatexplicitly modeling the incentives and actions of dishonest clients, ratherthan assuming them malicious, can enable strong robustness guarantees forcollaborative learning.</description><author>Florian E. Dorner, Nikola Konstantinov, Georgi Pashaliev, Martin Vechev</author><pubDate>Thu, 25 May 2023 18:28:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16272v1</guid></item><item><title>UDPM: Upsampling Diffusion Probabilistic Models</title><link>http://arxiv.org/abs/2305.16269v1</link><description>In recent years, Denoising Diffusion Probabilistic Models (DDPM) have caughtsignificant attention. By composing a Markovian process that starts in the datadomain and then gradually adds noise until reaching pure white noise, theyachieve superior performance in learning data distributions. Yet, these modelsrequire a large number of diffusion steps to produce aesthetically pleasingsamples, which is inefficient. In addition, unlike common generativeadversarial networks, the latent space of diffusion models is notinterpretable. In this work, we propose to generalize the denoising diffusionprocess into an Upsampling Diffusion Probabilistic Model (UDPM), in which wereduce the latent variable dimension in addition to the traditional noise leveladdition. As a result, we are able to sample images of size $256\times 256$with only 7 diffusion steps, which is less than two orders of magnitudecompared to standard DDPMs. We formally develop the Markovian diffusionprocesses of the UDPM, and demonstrate its generation capabilities on thepopular FFHQ, LSUN horses, ImageNet, and AFHQv2 datasets. Another favorableproperty of UDPM is that it is very easy to interpolate its latent space, whichis not the case with standard diffusion models. Our code is available online\url{https://github.com/shadyabh/UDPM}</description><author>Shady Abu-Hussein, Raja Giryes</author><pubDate>Thu, 25 May 2023 18:25:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16269v1</guid></item><item><title>Supervised learning with probabilistic morphisms and kernel mean embeddings</title><link>http://arxiv.org/abs/2305.06348v2</link><description>In this paper I propose a concept of a correct loss function in a generativemodel of supervised learning for an input space $\mathcal{X}$ and a label space$\mathcal{Y}$, which are measurable spaces. A correct loss function in agenerative model of supervised learning must correctly measure the discrepancybetween elements of a hypothesis space $\mathcal{H}$ of possible predictors andthe supervisor operator, which may not belong to $\mathcal{H}$. To definecorrect loss functions, I propose a characterization of a regular conditionalprobability measure $\mu_{\mathcal{Y}|\mathcal{X}}$ for a probability measure$\mu$ on $\mathcal{X} \times \mathcal{Y}$ relative to the projection$\Pi_{\mathcal{X}}: \mathcal{X}\times\mathcal{Y}\to \mathcal{X}$ as a solutionof a linear operator equation. If $\mathcal{Y}$ is a separable metrizabletopological space with the Borel $\sigma$-algebra $ \mathcal{B} (\mathcal{Y})$,I propose another characterization of a regular conditional probability measure$\mu_{\mathcal{Y}|\mathcal{X}}$ as a minimizer of a mean square error on thespace of Markov kernels, called probabilistic morphisms, from $\mathcal{X}$ to$\mathcal{Y}$, using kernel mean embeddings. Using these results and usinginner measure to quantify generalizability of a learning algorithm, I give ageneralization of a result due to Cucker-Smale, which concerns the learnabilityof a regression model, to a setting of a conditional probability estimationproblem. I also give a variant of Vapnik's regularization method for solvingstochastic ill-posed problems, using inner measure, and present itsapplications.</description><author>Hông Vân Lê</author><pubDate>Thu, 25 May 2023 18:24:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06348v2</guid></item><item><title>Detecting the Severity of Major Depressive Disorder from Speech: A Novel HARD-Training Methodology</title><link>http://arxiv.org/abs/2206.01542v2</link><description>Major Depressive Disorder (MDD) is a common worldwide mental health issuewith high associated socioeconomic costs. The prediction and automaticdetection of MDD can, therefore, make a huge impact on society. Speech, as anon-invasive, easy to collect signal, is a promising marker to aid thediagnosis and assessment of MDD. In this regard, speech samples were collectedas part of the Remote Assessment of Disease and Relapse in Major DepressiveDisorder (RADAR-MDD) research programme. RADAR-MDD was an observational cohortstudy in which speech and other digital biomarkers were collected from a cohortof individuals with a history of MDD in Spain, United Kingdom and theNetherlands. In this paper, the RADAR-MDD speech corpus was taken as anexperimental framework to test the efficacy of a Sequence-to-Sequence modelwith a local attention mechanism in a two-class depression severityclassification paradigm. Additionally, a novel training method, HARD-Training,is proposed. It is a methodology based on the selection of more ambiguoussamples for the model training, and inspired by the curriculum learningparadigm. HARD-Training was found to consistently improve - with an averageincrement of 8.6% - the performance of our classifiers for both of two speechelicitation tasks used and each collection site of the RADAR-MDD speech corpus.With this novel methodology, our Sequence-to-Sequence model was able toeffectively detect MDD severity regardless of language. Finally, recognisingthe need for greater awareness of potential algorithmic bias, we conduct anadditional analysis of our results separately for each gender.</description><author>Edward L. Campbell, Judith Dineley, Pauline Conde, Faith Matcham, Femke Lamers, Sara Siddi, Laura Docio-Fernandez, Carmen Garcia-Mateo, Nicholas Cummins, the RADAR-CNS Consortium</author><pubDate>Thu, 25 May 2023 18:24:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.01542v2</guid></item><item><title>UNITE: A Unified Benchmark for Text-to-SQL Evaluation</title><link>http://arxiv.org/abs/2305.16265v1</link><description>A practical text-to-SQL system should generalize well on a wide variety ofnatural language questions, unseen database schemas, and novel SQL querystructures. To comprehensively evaluate text-to-SQL systems, we introduce a\textbf{UNI}fied benchmark for \textbf{T}ext-to-SQL \textbf{E}valuation(UNITE). It is composed of publicly available text-to-SQL datasets, containingnatural language questions from more than 12 domains, SQL queries from morethan 3.9K patterns, and 29K databases. Compared to the widely used Spiderbenchmark \cite{yu-etal-2018-spider}, we introduce $\sim$120K additionalexamples and a threefold increase in SQL patterns, such as comparative andboolean questions. We conduct a systematic study of six state-of-the-art (SOTA)text-to-SQL parsers on our new benchmark and show that: 1) Codex performssurprisingly well on out-of-domain datasets; 2) specially designed decodingmethods (e.g. constrained beam search) can improve performance for bothin-domain and out-of-domain settings; 3) explicitly modeling the relationshipbetween questions and schemas further improves the Seq2Seq models. Moreimportantly, our benchmark presents key challenges towards compositionalgeneralization and robustness issues -- which these SOTA models cannot addresswell.</description><author>Wuwei Lan, Zhiguo Wang, Anuj Chauhan, Henghui Zhu, Alexander Li, Jiang Guo, Sheng Zhang, Chung-Wei Hang, Joseph Lilien, Yiqun Hu, Lin Pan, Mingwen Dong, Jun Wang, Jiarong Jiang, Stephen Ash, Vittorio Castelli, Patrick Ng, Bing Xiang</author><pubDate>Thu, 25 May 2023 18:19:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16265v1</guid></item><item><title>Scaling Data-Constrained Language Models</title><link>http://arxiv.org/abs/2305.16264v1</link><description>The current trend of scaling language models involves increasing bothparameter count and training dataset size. Extrapolating this trend suggeststhat training dataset size may soon be limited by the amount of text dataavailable on the internet. Motivated by this limit, we investigate scalinglanguage models in data-constrained regimes. Specifically, we run a large setof experiments varying the extent of data repetition and compute budget,ranging up to 900 billion training tokens and 9 billion parameter models. Wefind that with constrained data for a fixed compute budget, training with up to4 epochs of repeated data yields negligible changes to loss compared to havingunique data. However, with more repetition, the value of adding computeeventually decays to zero. We propose and empirically validate a scaling lawfor compute optimality that accounts for the decreasing value of repeatedtokens and excess parameters. Finally, we experiment with approaches mitigatingdata scarcity, including augmenting the training dataset with code data orremoving commonly used filters. Models and datasets from our 400 training runsare publicly available at https://github.com/huggingface/datablations.</description><author>Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, Colin Raffel</author><pubDate>Thu, 25 May 2023 18:18:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16264v1</guid></item><item><title>Unified Modeling of Multi-Talker Overlapped Speech Recognition and Diarization with a Sidecar Separator</title><link>http://arxiv.org/abs/2305.16263v1</link><description>Multi-talker overlapped speech poses a significant challenge for speechrecognition and diarization. Recent research indicated that these two tasks areinter-dependent and complementary, motivating us to explore a unified modelingmethod to address them in the context of overlapped speech. A recent studyproposed a cost-effective method to convert a single-talker automatic speechrecognition (ASR) system into a multi-talker one, by inserting a Sidecarseparator into the frozen well-trained ASR model. Extending on this, weincorporate a diarization branch into the Sidecar, allowing for unifiedmodeling of both ASR and diarization with a negligible overhead of only 768parameters. The proposed method yields better ASR results compared to thebaseline on LibriMix and LibriSpeechMix datasets. Moreover, withoutsophisticated customization on the diarization task, our method achievesacceptable diarization results on the two-speaker subset of CALLHOME with onlya few adaptation steps.</description><author>Lingwei Meng, Jiawen Kang, Mingyu Cui, Haibin Wu, Xixin Wu, Helen Meng</author><pubDate>Thu, 25 May 2023 18:18:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16263v1</guid></item><item><title>Trans-Dimensional Generative Modeling via Jump Diffusion Models</title><link>http://arxiv.org/abs/2305.16261v1</link><description>We propose a new class of generative models that naturally handle data ofvarying dimensionality by jointly modeling the state and dimension of eachdatapoint. The generative process is formulated as a jump diffusion processthat makes jumps between different dimensional spaces. We first define adimension destroying forward noising process, before deriving the dimensioncreating time-reversed generative process along with a novel evidence lowerbound training objective for learning to approximate it. Simulating our learnedapproximation to the time-reversed generative process then provides aneffective way of sampling data of varying dimensionality by jointly generatingstate values and dimensions. We demonstrate our approach on molecular and videodatasets of varying dimensionality, reporting better compatibility withtest-time diffusion guidance imputation tasks and improved interpolationcapabilities versus fixed dimensional models that generate state values anddimensions separately.</description><author>Andrew Campbell, William Harvey, Christian Weilbach, Valentin De Bortoli, Tom Rainforth, Arnaud Doucet</author><pubDate>Thu, 25 May 2023 18:15:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16261v1</guid></item><item><title>Improving Zero-shot Generalization and Robustness of Multi-modal Models</title><link>http://arxiv.org/abs/2212.01758v2</link><description>Multi-modal image-text models such as CLIP and LiT have demonstratedimpressive performance on image classification benchmarks and their zero-shotgeneralization ability is particularly exciting. While the top-5 zero-shotaccuracies of these models are very high, the top-1 accuracies are much lower(over 25% gap in some cases). We investigate the reasons for this performancegap and find that many of the failure cases are caused by ambiguity in the textprompts. First, we develop a simple and efficient zero-shot post-hoc method toidentify images whose top-1 prediction is likely to be incorrect, by measuringconsistency of the predictions w.r.t. multiple prompts and imagetransformations. We show that our procedure better predicts mistakes,outperforming the popular max logit baseline on selective prediction tasks.Next, we propose a simple and efficient way to improve accuracy on suchuncertain images by making use of the WordNet hierarchy; specifically weaugment the original class by incorporating its parent and children from thesemantic label hierarchy, and plug the augmentation into text prompts. Weconduct experiments on both CLIP and LiT models with five differentImageNet-based datasets. For CLIP, our method improves the top-1 accuracy by17.13% on the uncertain subset and 3.6% on the entire ImageNet validation set.We also show that our method improves across ImageNet shifted datasets, fourother datasets, and other model architectures such as LiT. The proposed methodis hyperparameter-free, requires no additional model training and can be easilyscaled to other large multi-modal architectures. Code is available athttps://github.com/gyhandy/Hierarchy-CLIP.</description><author>Yunhao Ge, Jie Ren, Andrew Gallagher, Yuxiao Wang, Ming-Hsuan Yang, Hartwig Adam, Laurent Itti, Balaji Lakshminarayanan, Jiaping Zhao</author><pubDate>Thu, 25 May 2023 18:14:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.01758v2</guid></item><item><title>Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art</title><link>http://arxiv.org/abs/2305.16259v1</link><description>The adoption of Deep Neural Networks (DNNs) has greatly benefited NaturalLanguage Processing (NLP) during the past decade. However, the demands of longdocuments analysis are quite different from those of shorter texts, with theever increasing size of documents uploaded online rendering NLP on longdocuments a critical area of research. This paper surveys the currentstate-of-the-art in the domain, overviewing the relevant neural building blocksand subsequently focusing on two main NLP tasks: Document Classification,Summarization as well as mentioning uses in Sentiment Analysis. We detail thechallenges, issues and current solutions related to long-document NLP. We alsolist publicly available, labelled, long-document datasets used in currentresearch.</description><author>Dimitrios Tsirmpas, Ioannis Gkionis, Ioannis Mademlis</author><pubDate>Thu, 25 May 2023 18:13:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16259v1</guid></item><item><title>Fast Online Node Labeling for Very Large Graphs</title><link>http://arxiv.org/abs/2305.16257v1</link><description>This paper studies the online node classification problem under atransductive learning setting. Current methods either invert a graph kernelmatrix with $\mathcal{O}(n^3)$ runtime and $\mathcal{O}(n^2)$ space complexityor sample a large volume of random spanning trees, thus are difficult to scaleto large graphs. In this work, we propose an improvement based on the\textit{online relaxation} technique introduced by a series of works (Rakhlinet al.,2012; Rakhlin and Sridharan, 2015; 2017). We first prove an effectiveregret $\mathcal{O}(\sqrt{n^{1+\gamma}})$ when suitable parameterized graphkernels are chosen, then propose an approximate algorithm FastONL enjoying$\mathcal{O}(k\sqrt{n^{1+\gamma}})$ regret based on this relaxation. The key ofFastONL is a \textit{generalized local push} method that effectivelyapproximates inverse matrix columns and applies to a series of popular kernels.Furthermore, the per-prediction cost is$\mathcal{O}(\text{vol}({\mathcal{S}})\log 1/\epsilon)$ locally dependent onthe graph with linear memory cost. Experiments show that our scalable methodenjoys a better tradeoff between local and global consistency.</description><author>Baojian Zhou, Yifan Sun, Reza Babanezhad</author><pubDate>Thu, 25 May 2023 18:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16257v1</guid></item><item><title>Uncovering and Categorizing Social Biases in Text-to-SQL</title><link>http://arxiv.org/abs/2305.16253v1</link><description>Content Warning: This work contains examples that potentially implicatestereotypes, associations, and other harms that could be offensive toindividuals in certain social groups.} Large pre-trained language models areacknowledged to carry social biases towards different demographics, which canfurther amplify existing stereotypes in our society and cause even more harm.Text-to-SQL is an important task, models of which are mainly adopted byadministrative industries, where unfair decisions may lead to catastrophicconsequences. However, existing Text-to-SQL models are trained on clean,neutral datasets, such as Spider and WikiSQL. This, to some extent, cover upsocial bias in models under ideal conditions, which nevertheless may emerge inreal application scenarios. In this work, we aim to uncover and categorizesocial biases in Text-to-SQL models. We summarize the categories of socialbiases that may occur in structured data for Text-to-SQL models. We build testbenchmarks and reveal that models with similar task accuracy can contain socialbiases at very different rates. We show how to take advantage of ourmethodology to uncover and assess social biases in the downstream Text-to-SQLtask. We will release our code and data.</description><author>Yan Liu, Yan Gao, Zhe Su, Xiaokang Chen, Elliott Ash, Jian-Guang Lou</author><pubDate>Thu, 25 May 2023 18:08:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16253v1</guid></item><item><title>Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning</title><link>http://arxiv.org/abs/2305.16252v1</link><description>Real-life multilingual systems should be able to efficiently incorporate newlanguages as data distributions fed to the system evolve and shift over time.To do this, systems need to handle the issue of catastrophic forgetting, wherethe model performance drops for languages or tasks seen further in its past. Inthis paper, we study catastrophic forgetting, as well as methods to minimizethis, in a massively multilingual continual learning framework involving up to51 languages and covering both classification and sequence labeling tasks. Wepresent LR ADJUST, a learning rate scheduling method that is simple, yeteffective in preserving new information without strongly overwriting pastknowledge. Furthermore, we show that this method is effective across multiplecontinual learning approaches. Finally, we provide further insights into thedynamics of catastrophic forgetting in this massively multilingual setup.</description><author>Genta Indra Winata, Lingjue Xie, Karthik Radhakrishnan, Shijie Wu, Xisen Jin, Pengxiang Cheng, Mayank Kulkarni, Daniel Preotiuc-Pietro</author><pubDate>Thu, 25 May 2023 18:06:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16252v1</guid></item><item><title>High-Speed and Energy-Efficient Non-Volatile Silicon Photonic Memory Based on Heterogeneously Integrated Memresonator</title><link>http://arxiv.org/abs/2303.05644v2</link><description>Recently, interest in programmable photonics integrated circuits has grown asa potential hardware framework for deep neural networks, quantum computing, andfield programmable arrays (FPGAs). However, these circuits are constrained bythe limited tuning speed and large power consumption of the phase shiftersused. In this paper, introduced for the first time are memresonators, ormemristors heterogeneously integrated with silicon photonic microringresonators, as phase shifters with non-volatile memory. These devices arecapable of retention times of 12 hours, switching voltages lower than 5 V, anendurance of 1,000 switching cycles. Also, these memresonators have beenswitched using voltage pulses as short as 300 ps with a record low switchingenergy of 0.15 pJ. Furthermore, these memresonators are fabricated on aheterogeneous III-V/Si platform capable of integrating a rich family of active,passive, and non-linear optoelectronic devices, such as lasers and detectors,directly on-chip to enable in-memory photonic computing and further advance thescalability of integrated photonic processor circuits.</description><author>Bassem Tossoun, Di Liang, Stanley Cheung, Zhuoran Fang, Xia Sheng, John Paul Strachan, Raymond G. Beausoleil</author><pubDate>Thu, 25 May 2023 18:06:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05644v2</guid></item><item><title>Investigating the Role of Feed-Forward Networks in Transformers Using Parallel Attention and Feed-Forward Net Design</title><link>http://arxiv.org/abs/2305.13297v2</link><description>This paper investigates the key role of Feed-Forward Networks (FFNs) intransformer models by utilizing the Parallel Attention and Feed-Forward NetDesign (PAF) architecture, and comparing it to their Series Attention andFeed-Forward Net Design (SAF) counterparts. Central to the effectiveness of PAFare two main assumptions regarding the FFN block and the attention block withina layer: 1) the primary function of the FFN block is to maintain isotropy amongtoken embeddings and prevent their degeneration, and 2) the residual normcomputed in the attention block is substantially smaller than the input tokenembedding norm. To empirically validate these assumptions, we train PAFvariants of two large language models (RoBERTa-large and bert-large-uncased).Our results demonstrate that both assumptions hold true in the PAF design. Thisstudy contributes to a deeper understanding of the roles and interactionsbetween FFNs and self-attention mechanisms in transformer architectures.</description><author>Shashank Sonkar, Richard G. Baraniuk</author><pubDate>Thu, 25 May 2023 18:01:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13297v2</guid></item><item><title>Distributed TD(0) with Almost No Communication</title><link>http://arxiv.org/abs/2305.16246v1</link><description>We provide a new non-asymptotic analysis of distributed temporal differencelearning with linear function approximation. Our approach relies on ``one-shotaveraging,'' where $N$ agents run identical local copies of the TD(0) methodand average the outcomes only once at the very end. We demonstrate a version ofthe linear time speedup phenomenon, where the convergence time of thedistributed process is a factor of $N$ faster than the convergence time ofTD(0). This is the first result proving benefits from parallelism for temporaldifference methods.</description><author>Rui Liu, Alex Olshevsky</author><pubDate>Thu, 25 May 2023 18:00:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16246v1</guid></item><item><title>Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models</title><link>http://arxiv.org/abs/2305.16243v1</link><description>Augmenting language models with a retrieval mechanism has been shown tosignificantly improve their performance while keeping the number of parameterslow. Retrieval-augmented models commonly rely on a semantic retrieval mechanismbased on the similarity between dense representations of the query chunk andpotential neighbors. In this paper, we study the state-of-the-art Retro modeland observe that its performance gain is better explained by surface-levelsimilarities, such as token overlap. Inspired by this, we replace the semanticretrieval in Retro with a surface-level method based on BM25, obtaining asignificant reduction in perplexity. As full BM25 retrieval can becomputationally costly for large datasets, we also apply it in a re-rankingscenario, gaining part of the perplexity reduction with minimal computationaloverhead.</description><author>Ehsan Doostmohammadi, Tobias Norlund, Marco Kuhlmann, Richard Johansson</author><pubDate>Thu, 25 May 2023 17:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16243v1</guid></item><item><title>Two-timescale Extragradient for Finding Local Minimax Points</title><link>http://arxiv.org/abs/2305.16242v1</link><description>Minimax problems are notoriously challenging to optimize. However, wedemonstrate that the two-timescale extragradient can be a viable solution. Byutilizing dynamical systems theory, we show that it converges to points thatsatisfy the second-order necessary condition of local minimax points, under amild condition. This work surpasses all previous results as we eliminate acrucial assumption that the Hessian, with respect to the maximization variable,is nondegenerate.</description><author>Jiseok Chae, Kyuwon Kim, Donghwan Kim</author><pubDate>Thu, 25 May 2023 17:52:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16242v1</guid></item><item><title>Persistent Laplacian-enhanced Algorithm for Scarcely Labeled Data Classification</title><link>http://arxiv.org/abs/2305.16239v1</link><description>The success of many machine learning (ML) methods depends crucially on havinglarge amounts of labeled data. However, obtaining enough labeled data can beexpensive, time-consuming, and subject to ethical constraints for manyapplications. One approach that has shown tremendous value in addressing thischallenge is semi-supervised learning (SSL); this technique utilizes bothlabeled and unlabeled data during training, often with much less labeled datathan unlabeled data, which is often relatively easy and inexpensive to obtain.In fact, SSL methods are particularly useful in applications where the cost oflabeling data is especially expensive, such as medical analysis, naturallanguage processing (NLP), or speech recognition. A subset of SSL methods thathave achieved great success in various domains involves algorithms thatintegrate graph-based techniques. These procedures are popular due to the vastamount of information provided by the graphical framework and the versatilityof their applications. In this work, we propose an algebraic topology-basedsemi-supervised method called persistent Laplacian-enhanced graph MBO (PL-MBO)by integrating persistent spectral graph theory with the classicalMerriman-Bence- Osher (MBO) scheme. Specifically, we use a filtration procedureto generate a sequence of chain complexes and associated families of simplicialcomplexes, from which we construct a family of persistent Laplacians. Overall,it is a very efficient procedure that requires much less labeled data toperform well compared to many ML techniques, and it can be adapted for bothsmall and large datasets. We evaluate the performance of the proposed method ondata classification, and the results indicate that the proposed techniqueoutperforms other existing semi-supervised algorithms.</description><author>Gokul Bhusal, Ekaterina Merkurjev, Guo-Wei Wei</author><pubDate>Thu, 25 May 2023 17:49:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16239v1</guid></item><item><title>Interactive Segment Anything NeRF with Feature Imitation</title><link>http://arxiv.org/abs/2305.16233v1</link><description>This paper investigates the potential of enhancing Neural Radiance Fields(NeRF) with semantics to expand their applications. Although NeRF has beenproven useful in real-world applications like VR and digital creation, the lackof semantics hinders interaction with objects in complex scenes. We propose toimitate the backbone feature of off-the-shelf perception models to achievezero-shot semantic segmentation with NeRF. Our framework reformulates thesegmentation process by directly rendering semantic features and only applyingthe decoder from perception models. This eliminates the need for expensivebackbones and benefits 3D consistency. Furthermore, we can project the learnedsemantics onto extracted mesh surfaces for real-time interaction. With thestate-of-the-art Segment Anything Model (SAM), our framework acceleratessegmentation by 16 times with comparable mask quality. The experimental resultsdemonstrate the efficacy and computational advantages of our approach. Projectpage: \url{https://me.kiui.moe/san/}.</description><author>Xiaokang Chen, Jiaxiang Tang, Diwen Wan, Jingbo Wang, Gang Zeng</author><pubDate>Thu, 25 May 2023 17:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16233v1</guid></item><item><title>Image-based Treatment Effect Heterogeneity</title><link>http://arxiv.org/abs/2206.06417v5</link><description>Randomized controlled trials (RCTs) are considered the gold standard forestimating the average treatment effect (ATE) of interventions. One use of RCTsis to study the causes of global poverty -- a subject explicitly cited in the2019 Nobel Memorial Prize awarded to Duflo, Banerjee, and Kremer "for theirexperimental approach to alleviating global poverty." Because the ATE is apopulation summary, anti-poverty experiments often seek to unpack the effectvariation around the ATE by conditioning (CATE) on tabular variables such asage and ethnicity that were measured during the RCT data collection. Althoughsuch variables are key to unpacking CATE, using only such variables may fail tocapture historical, geographical, or neighborhood-specific contributors toeffect variation, as tabular RCT data are often only observed near the time ofthe experiment. In global poverty research, when the location of the experimentunits is approximately known, satellite imagery can provide a window into suchfactors important for understanding heterogeneity. However, there is no methodthat specifically enables applied researchers to analyze CATE from images. Inthis paper, using a deep probabilistic modeling framework, we develop such amethod that estimates latent clusters of images by identifying images withsimilar treatment effects distributions. Our interpretable image CATE modelalso includes a sensitivity factor that quantifies the importance of imagesegments contributing to the effect cluster prediction. We compare the proposedmethods against alternatives in simulation; also, we show how the model worksin an actual RCT, estimating the effects of an anti-poverty intervention innorthern Uganda and obtaining a posterior predictive distribution over effectsfor the rest of the country where no experimental data was collected. We makeall models available in open-source software.</description><author>Connor T. Jerzak, Fredrik Johansson, Adel Daoud</author><pubDate>Thu, 25 May 2023 17:42:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.06417v5</guid></item><item><title>Topological gap protocol based machine learning optimization of Majorana hybrid wires</title><link>http://arxiv.org/abs/2305.16230v1</link><description>Majorana zero modes in superconductor-nanowire hybrid structures are apromising candidate for topologically protected qubits with the potential to beused in scalable structures. Currently, disorder in such Majorana wires is amajor challenge, as it can destroy the topological phase and thus reduce theyield in the fabrication of Majorana devices. We study machine learningoptimization of a gate array in proximity to a grounded Majorana wire, whichallows us to reliably compensate even strong disorder. We propose a metric foroptimization that is inspired by the topological gap protocol, and which can beimplemented based on measurements of the non-local conductance through thewire.</description><author>Matthias Thamm, Bernd Rosenow</author><pubDate>Thu, 25 May 2023 17:37:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16230v1</guid></item><item><title>Gaussian Processes with State-Dependent Noise for Stochastic Control</title><link>http://arxiv.org/abs/2305.16229v1</link><description>This paper considers a stochastic control framework, in which the residualmodel uncertainty of the dynamical system is learned using a Gaussian Process(GP). In the proposed formulation, the residual model uncertainty consists of anonlinear function and state-dependent noise. The proposed formulation uses aposterior-GP to approximate the residual model uncertainty and a prior-GP toaccount for state-dependent noise. The two GPs are interdependent and are thuslearned jointly using an iterative algorithm. Theoretical properties of theiterative algorithm are established. Advantages of the proposed state-dependentformulation include (i) faster convergence of the GP estimate to the unknownfunction as the GP learns which data samples are more trustworthy and (ii) anaccurate estimate of state-dependent noise, which can, e.g., be useful for acontroller or decision-maker to determine the uncertainty of an action.Simulation studies highlight these two advantages.</description><author>Marcel Menner, Karl Berntorp</author><pubDate>Thu, 25 May 2023 17:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16229v1</guid></item><item><title>Q-learning Decision Transformer: Leveraging Dynamic Programming for Conditional Sequence Modelling in Offline RL</title><link>http://arxiv.org/abs/2209.03993v4</link><description>Recent works have shown that tackling offline reinforcement learning (RL)with a conditional policy produces promising results. The Decision Transformer(DT) combines the conditional policy approach and a transformer architecture,showing competitive performance against several benchmarks. However, DT lacksstitching ability -- one of the critical abilities for offline RL to learn theoptimal policy from sub-optimal trajectories. This issue becomes particularlysignificant when the offline dataset only contains sub-optimal trajectories. Onthe other hand, the conventional RL approaches based on Dynamic Programming(such as Q-learning) do not have the same limitation; however, they suffer fromunstable learning behaviours, especially when they rely on functionapproximation in an off-policy learning setting. In this paper, we propose theQ-learning Decision Transformer (QDT) to address the shortcomings of DT byleveraging the benefits of Dynamic Programming (Q-learning). It utilises theDynamic Programming results to relabel the return-to-go in the training data tothen train the DT with the relabelled data. Our approach efficiently exploitsthe benefits of these two approaches and compensates for each other'sshortcomings to achieve better performance. We empirically show these in bothsimple toy environments and the more complex D4RL benchmark, showingcompetitive performance gains.</description><author>Taku Yamagata, Ahmed Khalil, Raul Santos-Rodriguez</author><pubDate>Thu, 25 May 2023 17:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.03993v4</guid></item><item><title>ProSpect: Expanded Conditioning for the Personalization of Attribute-aware Image Generation</title><link>http://arxiv.org/abs/2305.16225v1</link><description>Personalizing generative models offers a way to guide image generation withuser-provided references. Current personalization methods can invert an objector concept into the textual conditioning space and compose new naturalsentences for text-to-image diffusion models. However, representing and editingspecific visual attributes like material, style, layout, etc. remains achallenge, leading to a lack of disentanglement and editability. To addressthis, we propose a novel approach that leverages the step-by-step generationprocess of diffusion models, which generate images from low- to high-frequencyinformation, providing a new perspective on representing, generating, andediting images. We develop Prompt Spectrum Space P*, an expanded textualconditioning space, and a new image representation method called ProSpect.ProSpect represents an image as a collection of inverted textual tokenembeddings encoded from per-stage prompts, where each prompt corresponds to aspecific generation stage (i.e., a group of consecutive steps) of the diffusionmodel. Experimental results demonstrate that P* and ProSpect offer strongerdisentanglement and controllability compared to existing methods. We applyProSpect in various personalized attribute-aware image generation applications,such as image/text-guided material/style/layout transfer/editing, achievingpreviously unattainable results with a single image input without fine-tuningthe diffusion models.</description><author>Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver Deussen, Changsheng Xu</author><pubDate>Thu, 25 May 2023 17:32:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16225v1</guid></item><item><title>Prompt-Free Diffusion: Taking "Text" out of Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2305.16223v1</link><description>Text-to-image (T2I) research has grown explosively in the past year, owing tothe large-scale pre-trained diffusion models and many emerging personalizationand editing approaches. Yet, one pain point persists: the text promptengineering, and searching high-quality text prompts for customized results ismore art than science. Moreover, as commonly argued: "an image is worth athousand words" - the attempt to describe a desired image with texts often endsup being ambiguous and cannot comprehensively cover delicate visual details,hence necessitating more additional controls from the visual domain. In thispaper, we take a bold step forward: taking "Text" out of a pre-trained T2Idiffusion model, to reduce the burdensome prompt engineering efforts for users.Our proposed framework, Prompt-Free Diffusion, relies on only visual inputs togenerate new images: it takes a reference image as "context", an optional imagestructural conditioning, and an initial noise, with absolutely no text prompt.The core architecture behind the scene is Semantic Context Encoder (SeeCoder),substituting the commonly used CLIP-based or LLM-based text encoder. Thereusability of SeeCoder also makes it a convenient drop-in component: one canalso pre-train a SeeCoder in one T2I model and reuse it for another. Throughextensive experiments, Prompt-Free Diffusion is experimentally found to (i)outperform prior exemplar-based image synthesis approaches; (ii) perform on parwith state-of-the-art T2I models using prompts following the best practice; and(iii) be naturally extensible to other downstream applications such as animefigure generation and virtual try-on, with promising quality. Our code andmodels are open-sourced at https://github.com/SHI-Labs/Prompt-Free-Diffusion.</description><author>Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan Essa, Humphrey Shi</author><pubDate>Thu, 25 May 2023 17:30:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16223v1</guid></item><item><title>Incomplete Multimodal Learning for Complex Brain Disorders Prediction</title><link>http://arxiv.org/abs/2305.16222v1</link><description>Recent advancements in the acquisition of various brain data sources havecreated new opportunities for integrating multimodal brain data to assist inearly detection of complex brain disorders. However, current data integrationapproaches typically need a complete set of biomedical data modalities, whichmay not always be feasible, as some modalities are only available inlarge-scale research cohorts and are prohibitive to collect in routine clinicalpractice. Especially in studies of brain diseases, research cohorts may includeboth neuroimaging data and genetic data, but for practical clinical diagnosis,we often need to make disease predictions only based on neuroimages. As aresult, it is desired to design machine learning models which can use allavailable data (different data could provide complementary information) duringtraining but conduct inference using only the most common data modality. Wepropose a new incomplete multimodal data integration approach that employstransformers and generative adversarial networks to effectively exploitauxiliary modalities available during training in order to improve theperformance of a unimodal model at inference. We apply our new method topredict cognitive degeneration and disease outcomes using the multimodalimaging genetic data from Alzheimer's Disease Neuroimaging Initiative (ADNI)cohort. Experimental results demonstrate that our approach outperforms therelated machine learning and deep learning methods by a significant margin.</description><author>Reza Shirkavand, Liang Zhan, Heng Huang, Li Shen, Paul M. Thompson</author><pubDate>Thu, 25 May 2023 17:29:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16222v1</guid></item><item><title>On the Robustness of Segment Anything</title><link>http://arxiv.org/abs/2305.16220v1</link><description>Segment anything model (SAM) has presented impressive objectnessidentification capability with the idea of prompt learning and a new collectedlarge-scale dataset. Given a prompt (e.g., points, bounding boxes, or masks)and an input image, SAM is able to generate valid segment masks for all objectsindicated by the prompts, presenting high generalization across diversescenarios and being a general method for zero-shot transfer to downstreamvision tasks. Nevertheless, it remains unclear whether SAM may introduce errorsin certain threatening scenarios. Clarifying this is of significant importancefor applications that require robustness, such as autonomous vehicles. In thispaper, we aim to study the testing-time robustness of SAM under adversarialscenarios and common corruptions. To this end, we first build a testing-timerobustness evaluation benchmark for SAM by integrating existing publicdatasets. Second, we extend representative adversarial attacks against SAM andstudy the influence of different prompts on robustness. Third, we study therobustness of SAM under diverse corruption types by evaluating SAM on corrupteddatasets with different prompts. With experiments conducted on SA-1B and KITTIdatasets, we find that SAM exhibits remarkable robustness against variouscorruptions, except for blur-related corruption. Furthermore, SAM remainssusceptible to adversarial attacks, particularly when subjected to PGD and BIMattacks. We think such a comprehensive study could highlight the importance ofthe robustness issues of SAM and trigger a series of new tasks for SAM as wellas downstream vision tasks.</description><author>Yihao Huang, Yue Cao, Tianlin Li, Felix Juefei-Xu, Di Lin, Ivor W. Tsang, Yang Liu, Qing Guo</author><pubDate>Thu, 25 May 2023 17:28:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16220v1</guid></item><item><title>FIT: Far-reaching Interleaved Transformers</title><link>http://arxiv.org/abs/2305.12689v2</link><description>We present FIT: a transformer-based architecture with efficientself-attention and adaptive computation. Unlike original transformers, whichoperate on a single sequence of data tokens, we divide the data tokens intogroups, with each group being a shorter sequence of tokens. We employ two typesof transformer layers: local layers operate on data tokens within each group,while global layers operate on a smaller set of introduced latent tokens. Theselayers, comprising the same set of self-attention and feed-forward layers asstandard transformers, are interleaved, and cross-attention is used tofacilitate information exchange between data and latent tokens within the samegroup. The attention complexity is $O(n^2)$ locally within each group of size$n$, but can reach $O(L^{{4}/{3}})$ globally for sequence length of $L$. Theefficiency can be further enhanced by relying more on global layers thatperform adaptive computation using a smaller set of latent tokens. FIT is aversatile architecture and can function as an encoder, diffusion decoder, orautoregressive decoder. We provide initial evidence demonstrating itseffectiveness in high-resolution image understanding and generation tasks.Notably, FIT exhibits potential in performing end-to-end training ongigabit-scale data, such as 6400$\times$6400 images, or 160K tokens (afterpatch tokenization), within a memory capacity of 16GB, without requiringspecific optimizations or model parallelism.</description><author>Ting Chen, Lala Li</author><pubDate>Thu, 25 May 2023 17:27:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12689v2</guid></item><item><title>Beyond Reward: Offline Preference-guided Policy Optimization</title><link>http://arxiv.org/abs/2305.16217v1</link><description>This study focuses on the topic of offline preference-based reinforcementlearning (PbRL), a variant of conventional reinforcement learning thatdispenses with the need for online interaction or specification of rewardfunctions. Instead, the agent is provided with pre-existing offlinetrajectories and human preferences between pairs of trajectories to extract thedynamics and task information, respectively. Since the dynamics and taskinformation are orthogonal, a naive approach would involve usingpreference-based reward learning followed by an off-the-shelf offline RLalgorithm. However, this requires the separate learning of a scalar rewardfunction, which is assumed to be an information bottleneck. To address thisissue, we propose the offline preference-guided policy optimization (OPPO)paradigm, which models offline trajectories and preferences in a one-stepprocess, eliminating the need for separately learning a reward function. OPPOachieves this by introducing an offline hindsight information matchingobjective for optimizing a contextual policy and a preference modelingobjective for finding the optimal context. OPPO further integrates awell-performing decision policy by optimizing the two objectives iteratively.Our empirical results demonstrate that OPPO effectively models offlinepreferences and outperforms prior competing baselines, including offline RLalgorithms performed over either true or pseudo reward function specifications.Our code is available at https://github.com/bkkgbkjb/OPPO .</description><author>Yachen Kang, Diyuan Shi, Jinxin Liu, Li He, Donglin Wang</author><pubDate>Thu, 25 May 2023 17:24:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16217v1</guid></item><item><title>Cross-supervised Dual Classifiers for Semi-supervised Medical Image Segmentation</title><link>http://arxiv.org/abs/2305.16216v1</link><description>Semi-supervised medical image segmentation offers a promising solution forlarge-scale medical image analysis by significantly reducing the annotationburden while achieving comparable performance. Employing this method exhibits ahigh degree of potential for optimizing the segmentation process and increasingits feasibility in clinical settings during translational investigations.Recently, cross-supervised training based on different co-training sub-networkshas become a standard paradigm for this task. Still, the critical issues ofsub-network disagreement and label-noise suppression require further attentionand progress in cross-supervised training. This paper proposes across-supervised learning framework based on dual classifiers (DC-Net),including an evidential classifier and a vanilla classifier. The twoclassifiers exhibit complementary characteristics, enabling them to handledisagreement effectively and generate more robust and accurate pseudo-labelsfor unlabeled data. We also incorporate the uncertainty estimation from theevidential classifier into cross-supervised training to alleviate the negativeeffect of the error supervision signal. The extensive experiments on LA andPancreas-CT dataset illustrate that DC-Net outperforms other state-of-the-artmethods for semi-supervised segmentation. The code will be released soon.</description><author>Zhenxi Zhang, Ran Ran, Chunna Tian, Heng Zhou, Fan Yang, Xin Li, Zhicheng Jiao</author><pubDate>Thu, 25 May 2023 17:23:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16216v1</guid></item><item><title>Koopman Kernel Regression</title><link>http://arxiv.org/abs/2305.16215v1</link><description>Many machine learning approaches for decision making, such as reinforcementlearning, rely on simulators or predictive models to forecast thetime-evolution of quantities of interest, e.g., the state of an agent or thereward of a policy. Forecasts of such complex phenomena are commonly describedby highly nonlinear dynamical systems, making their use in optimization-baseddecision-making challenging. Koopman operator theory offers a beneficialparadigm for addressing this problem by characterizing forecasts via lineardynamical systems. This makes system analysis and long-term predictions simple-- involving only matrix multiplications. However, the transformation to alinear system is generally non-trivial and unknown, requiring learning-basedapproaches. While there exists a variety of approaches, they usually lackcrucial learning-theoretic guarantees, such that the behavior of the obtainedmodels with increasing data and dimensionality is often unclear. We address theaforementioned by deriving a novel reproducing kernel Hilbert space (RKHS) thatsolely spans transformations into linear dynamical systems. The resultingKoopman Kernel Regression (KKR) framework enables the use of statisticallearning tools from function approximation for novel convergence results andgeneralization risk bounds under weaker assumptions than existing work. Ournumerical experiments indicate advantages over state-of-the-art statisticallearning approaches for Koopman-based predictors.</description><author>Petar Bevanda, Max Beier, Armin Lederer, Stefan Sosnowski, Eyke Hüllermeier, Sandra Hirche</author><pubDate>Thu, 25 May 2023 17:22:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16215v1</guid></item><item><title>Self-aware and Cross-sample Prototypical Learning for Semi-supervised Medical Image Segmentation</title><link>http://arxiv.org/abs/2305.16214v1</link><description>Consistency learning plays a crucial role in semi-supervised medical imagesegmentation as it enables the effective utilization of limited annotated datawhile leveraging the abundance of unannotated data. The effectiveness andefficiency of consistency learning are challenged by prediction diversity andtraining stability, which are often overlooked by existing studies. Meanwhile,the limited quantity of labeled data for training often proves inadequate forformulating intra-class compactness and inter-class discrepancy of pseudolabels. To address these issues, we propose a self-aware and cross-sampleprototypical learning method (SCP-Net) to enhance the diversity of predictionin consistency learning by utilizing a broader range of semantic informationderived from multiple inputs. Furthermore, we introduce a self-awareconsistency learning method that exploits unlabeled data to improve thecompactness of pseudo labels within each class. Moreover, a dual lossre-weighting method is integrated into the cross-sample prototypicalconsistency learning method to improve the reliability and stability of ourmodel. Extensive experiments on ACDC dataset and PROMISE12 dataset validatethat SCP-Net outperforms other state-of-the-art semi-supervised segmentationmethods and achieves significant performance gains compared to the limitedsupervised training. Our code will come soon.</description><author>Zhenxi Zhang, Ran Ran, Chunna Tian, Heng Zhou, Xin Li, Fan Yang, Zhicheng Jiao</author><pubDate>Thu, 25 May 2023 17:22:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16214v1</guid></item><item><title>ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation</title><link>http://arxiv.org/abs/2305.16213v1</link><description>Score distillation sampling (SDS) has shown great promise in text-to-3Dgeneration by distilling pretrained large-scale text-to-image diffusion models,but suffers from over-saturation, over-smoothing, and low-diversity problems.In this work, we propose to model the 3D parameter as a random variable insteadof a constant as in SDS and present variational score distillation (VSD), aprincipled particle-based variational framework to explain and address theaforementioned issues in text-to-3D generation. We show that SDS is a specialcase of VSD and leads to poor samples with both small and large CFG weights. Incomparison, VSD works well with various CFG weights as ancestral sampling fromdiffusion models and simultaneously improves the diversity and sample qualitywith a common CFG weight (i.e., $7.5$). We further present various improvementsin the design space for text-to-3D such as distillation time schedule anddensity initialization, which are orthogonal to the distillation algorithm yetnot well explored. Our overall approach, dubbed ProlificDreamer, can generatehigh rendering resolution (i.e., $512\times512$) and high-fidelity NeRF withrich structure and complex effects (e.g., smoke and drops). Further,initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed andphoto-realistic. Project page: https://ml.cs.tsinghua.edu.cn/prolificdreamer/</description><author>Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu</author><pubDate>Thu, 25 May 2023 17:19:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16213v1</guid></item><item><title>EMNS /Imz/ Corpus: An emotive single-speaker dataset for narrative storytelling in games, television and graphic novels</title><link>http://arxiv.org/abs/2305.13137v2</link><description>The increasing adoption of text-to-speech technologies has led to a growingdemand for natural and emotive voices that adapt to a conversation's contextand emotional tone. The Emotive Narrative Storytelling (EMNS) corpus is aunique speech dataset created to enhance conversations' expressiveness andemotive quality in interactive narrative-driven systems. The corpus consists ofa 2.3-hour recording featuring a female speaker delivering labelled utterances.It encompasses eight acted emotional states, evenly distributed with a varianceof 0.68%, along with expressiveness levels and natural language descriptionswith word emphasis labels. The evaluation of audio samples from differentdatasets revealed that the EMNS corpus achieved the highest average scores inaccurately conveying emotions and demonstrating expressiveness. It outperformedother datasets in conveying shared emotions and achieved comparable levels ofgenuineness. A classification task confirmed the accurate representation ofintended emotions in the corpus, with participants recognising the recordingsas genuine and expressive. Additionally, the availability of the datasetcollection tool under the Apache 2.0 License simplifies remote speech datacollection for researchers.</description><author>Kari Ali Noriy, Xiaosong Yang, Jian Jun Zhang</author><pubDate>Thu, 25 May 2023 17:17:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13137v2</guid></item><item><title>When are Post-hoc Conceptual Explanations Identifiable?</title><link>http://arxiv.org/abs/2206.13872v4</link><description>Interest in understanding and factorizing learned embedding spaces throughconceptual explanations is steadily growing. When no human concept labels areavailable, concept discovery methods search trained embedding spaces forinterpretable concepts like object shape or color that can be used to providepost-hoc explanations for decisions. Unlike previous work, we argue thatconcept discovery should be identifiable, meaning that a number of knownconcepts can be provably recovered to guarantee reliability of theexplanations. As a starting point, we explicitly make the connection betweenconcept discovery and classical methods like Principal Component Analysis andIndependent Component Analysis by showing that they can recover independentconcepts with non-Gaussian distributions. For dependent concepts, we proposetwo novel approaches that exploit functional compositionality properties ofimage-generating processes. Our provably identifiable concept discovery methodssubstantially outperform competitors on a battery of experiments includinghundreds of trained models and dependent concepts, where they exhibit up to 29% better alignment with the ground truth. Our results provide a rigorousfoundation for reliable concept discovery without human labels.</description><author>Tobias Leemann, Michael Kirchhof, Yao Rong, Enkelejda Kasneci, Gjergji Kasneci</author><pubDate>Thu, 25 May 2023 17:10:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.13872v4</guid></item><item><title>C-MCTS: Safe Planning with Monte Carlo Tree Search</title><link>http://arxiv.org/abs/2305.16209v1</link><description>Many real-world decision-making tasks, such as safety-critical scenarios,cannot be fully described in a single-objective setting using the MarkovDecision Process (MDP) framework, as they include hard constraints. These caninstead be modeled with additional cost functions within the Constrained MarkovDecision Process (CMDP) framework. Even though CMDPs have been extensivelystudied in the Reinforcement Learning literature, little attention has beengiven to sampling-based planning algorithms such as MCTS for solving them.Previous approaches use Monte Carlo cost estimates to avoid constraintviolations. However, these suffer from high variance which results inconservative performance with respect to costs. We propose Constrained MCTS(C-MCTS), an algorithm that estimates cost using a safety critic. The safetycritic training is based on Temporal Difference learning in an offline phaseprior to agent deployment. This critic limits the exploration of the searchtree and removes unsafe trajectories within MCTS during deployment. C-MCTSsatisfies cost constraints but operates closer to the constraint boundary,achieving higher rewards compared to previous work. As a nice byproduct, theplanner is more efficient requiring fewer planning steps. Most importantly, weshow that under model mismatch between the planner and the real world, ourapproach is less susceptible to cost violations than previous work.</description><author>Dinesh Parthasarathy, Georgios Kontes, Axel Plinge, Christopher Mutschler</author><pubDate>Thu, 25 May 2023 17:08:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16209v1</guid></item><item><title>On Computing Universal Plans for Partially Observable Multi-Agent Path Finding</title><link>http://arxiv.org/abs/2305.16203v1</link><description>Multi-agent routing problems have drawn significant attention nowadays due totheir broad industrial applications in, e.g., warehouse robots, logisticsautomation, and traffic control. Conventionally, they are modelled as classicalplanning problems. In this paper, we argue that it is beneficial to formulatethem as universal planning problems. We therefore propose universal plans, alsoknown as policies, as the solution concepts, and implement a system calledASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) forcomputing them. Given an arbitrary two-dimensional map and a profile of goalsfor the agents, the system finds a feasible universal plan for each agent thatensures no collision with others. We use the system to conduct someexperiments, and make some observations on the types of goal profiles andenvironments that will have feasible policies, and how they may depend onagents' sensors. We also demonstrate how users can customize action preferencesto compute more efficient policies, even (near-)optimal ones.</description><author>Fengming Zhu, Fangzhen Lin</author><pubDate>Thu, 25 May 2023 17:06:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16203v1</guid></item><item><title>DP-SGD Without Clipping: The Lipschitz Neural Network Way</title><link>http://arxiv.org/abs/2305.16202v1</link><description>State-of-the-art approaches for training Differentially Private (DP) DeepNeural Networks (DNN) faces difficulties to estimate tight bounds on thesensitivity of the network's layers, and instead rely on a process ofper-sample gradient clipping. This clipping process not only biases thedirection of gradients but also proves costly both in memory consumption and incomputation. To provide sensitivity bounds and bypass the drawbacks of theclipping process, our theoretical analysis of Lipschitz constrained networksreveals an unexplored link between the Lipschitz constant with respect to theirinput and the one with respect to their parameters. By bounding the Lipschitzconstant of each layer with respect to its parameters we guarantee DP trainingof these networks. This analysis not only allows the computation of theaforementioned sensitivities at scale but also provides leads on to howmaximize the gradient-to-noise ratio for fixed privacy guarantees. Tofacilitate the application of Lipschitz networks and foster robust andcertifiable learning under privacy guarantees, we provide a Python package thatimplements building blocks allowing the construction and private training ofsuch networks.</description><author>Louis Bethune, Thomas Massena, Thibaut Boissin, Yannick Prudent, Corentin Friedrich, Franck Mamalet, Aurelien Bellet, Mathieu Serrurier, David Vigouroux</author><pubDate>Thu, 25 May 2023 17:05:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16202v1</guid></item><item><title>Diversity-Aware Coherence Loss for Improving Neural Topic Models</title><link>http://arxiv.org/abs/2305.16199v1</link><description>The standard approach for neural topic modeling uses a variationalautoencoder (VAE) framework that jointly minimizes the KL divergence betweenthe estimated posterior and prior, in addition to the reconstruction loss.Since neural topic models are trained by recreating individual input documents,they do not explicitly capture the coherence between topic words on the corpuslevel. In this work, we propose a novel diversity-aware coherence loss thatencourages the model to learn corpus-level coherence scores while maintaining ahigh diversity between topics. Experimental results on multiple datasets showthat our method significantly improves the performance of neural topic modelswithout requiring any pretraining or additional parameters.</description><author>Raymond Li, Felipe González-Pizarro, Linzi Xing, Gabriel Murray, Giuseppe Carenini</author><pubDate>Thu, 25 May 2023 17:01:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16199v1</guid></item><item><title>Approximating Energy Market Clearing and Bidding With Model-Based Reinforcement Learning</title><link>http://arxiv.org/abs/2303.01772v2</link><description>Energy markets can provide incentives for undesired behavior of marketparticipants. Multi-agent Reinforcement learning (MARL) is a promising newapproach to predicting the expected behavior of energy market participants.However, reinforcement learning requires many interactions with the system toconverge, and the power system environment often consists of extensivecomputations, e.g., optimal power flow (OPF) calculation for market clearing.To tackle this complexity, we provide a model of the energy market to a basicMARL algorithm in the form of a learned OPF approximation and explicit marketrules. The learned OPF surrogate model makes an explicit solving of the OPFcompletely unnecessary. Our experiments demonstrate that the model additionallyreduces training time by about one order of magnitude but at the cost of aslightly worse approximation of the Nash equilibrium. Potential applications ofour method are market design, more realistic modeling of market participants,and analysis of manipulative behavior.</description><author>Thomas Wolgast, Astrid Nieße</author><pubDate>Thu, 25 May 2023 16:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01772v2</guid></item><item><title>FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs</title><link>http://arxiv.org/abs/2301.06719v2</link><description>Efficient detectors for edge devices are often optimized for metrics likeparameters or speed counts, which remain weak correlation with the energy ofdetectors. However, among vision applications of convolutional neural networks(CNNs), some, such as always-on surveillance cameras, are critical for energyconstraints. This paper aims to serve as a baseline by designing detectors toreach tradeoffs between energy and performance from two perspectives: 1) Weextensively analyze various CNNs to identify low-energy architectures,including the selection of activation functions, convolutions operators, andfeature fusion structures on necks. These underappreciated details in pastworks seriously affect the energy consumption of detectors; 2) To break throughthe dilemmatic energy-performance problem, we propose a balanced detectordriven by energy using discovered low-energy components named\textit{FemtoDet}. In addition to the novel construction, we further improveFemtoDet by considering convolutions and training strategy optimizations.Specifically, we develop a new instance boundary enhancement (IBE) module forconvolution optimization to overcome the contradiction between the limitedcapacity of CNNs and detection tasks in diverse spatial representations, andpropose a recursive warm-restart (RecWR) for optimizing training strategy toescape the sub-optimization of light-weight detectors, considering the datashift produced in popular augmentations. As a result, FemtoDet with only 68.77kparameters achieves a competitive score of 46.3 AP50 on PASCAL VOC and power of7.83W on RTX 3090. Extensive experiments on COCO and TJU-DHD datasets indicatethat the proposed method achieves competitive results in diverse scenes.</description><author>Peng Tu, Xu Xie, Guo AI, Yuexiang Li, Yawen Huang, Yefeng Zheng</author><pubDate>Thu, 25 May 2023 16:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.06719v2</guid></item><item><title>Optimization and Interpretability of Graph Attention Networks for Small Sparse Graph Structures in Automotive Applications</title><link>http://arxiv.org/abs/2305.16196v1</link><description>For automotive applications, the Graph Attention Network (GAT) is aprominently used architecture to include relational information of a trafficscenario during feature embedding. As shown in this work, however, one of themost popular GAT realizations, namely GATv2, has potential pitfalls that hinderan optimal parameter learning. Especially for small and sparse graph structuresa proper optimization is problematic. To surpass limitations, this workproposes architectural modifications of GATv2. In controlled experiments, it isshown that the proposed model adaptions improve prediction performance in anode-level regression task and make it more robust to parameter initialization.This work aims for a better understanding of the attention mechanism andanalyzes its interpretability of identifying causal importance.</description><author>Marion Neumeier, Andreas Tollkühn, Sebastian Dorn, Michael Botsch, Wolfgang Utschick</author><pubDate>Thu, 25 May 2023 16:55:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16196v1</guid></item><item><title>Abstractive Summary Generation for the Urdu Language</title><link>http://arxiv.org/abs/2305.16195v1</link><description>Abstractive summary generation is a challenging task that requires the modelto comprehend the source text and generate a concise and coherent summary thatcaptures the essential information. In this paper, we explore the use of anencoder/decoder approach for abstractive summary generation in the Urdulanguage. We employ a transformer-based model that utilizes self-attentionmechanisms to encode the input text and generate a summary. Our experimentsshow that our model can produce summaries that are grammatically correct andsemantically meaningful. We evaluate our model on a publicly available datasetand achieve state-of-the-art results in terms of Rouge scores. We also conducta qualitative analysis of our model's output to assess its effectiveness andlimitations. Our findings suggest that the encoder/decoder approach is apromising method for abstractive summary generation in Urdu and can be extendedto other languages with suitable modifications.</description><author>Ali Raza, Hadia Sultan Raja, Usman Maratib</author><pubDate>Thu, 25 May 2023 16:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16195v1</guid></item><item><title>EXACT: Extensive Attack for Split Learning</title><link>http://arxiv.org/abs/2305.12997v2</link><description>Privacy-Preserving machine learning (PPML) can help us train and deploymodels that utilize private information. In particular, on-device MachineLearning allows us to completely avoid sharing information with a third-partyserver during inference. However, on-device models are typically less accuratewhen compared to the server counterparts due to the fact that (1) theytypically only rely on a small set of on-device features and (2) they need tobe small enough to run efficiently on end-user devices. Split Learning (SL) isa promising approach that can overcome these limitations. In SL, a largemachine learning model is divided into two parts, with the bigger part residingon the server-side and a smaller part executing on-device, aiming toincorporate the private features. However, end-to-end training of such modelsrequires exchanging gradients at the cut layer, which might encode privatefeatures or labels. In this paper, we provide insights into potential privacyrisks associated with SL and introduce a novel attack method, EXACT, toreconstruct private information. Furthermore, we also investigate theeffectiveness of various mitigation strategies. Our results indicate that thegradients significantly improve the attacker's effectiveness in all threedatasets reaching almost 100% reconstruction accuracy for some features.However, a small amount of differential privacy (DP) is quite effective inmitigating this risk without causing significant training degradation.</description><author>Xinchi Qiu, Ilias Leontiadis, Luca Melis, Alex Sablayrolles, Pierre Stock</author><pubDate>Thu, 25 May 2023 16:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12997v2</guid></item><item><title>Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers</title><link>http://arxiv.org/abs/2212.04325v3</link><description>Recently, RNN-Transducers have achieved remarkable results on variousautomatic speech recognition tasks. However, lattice-free sequencediscriminative training methods, which obtain superior performance in hybridmodels, are rarely investigated in RNN-Transducers. In this work, we proposethree lattice-free training objectives, namely lattice-free maximum mutualinformation, lattice-free segment-level minimum Bayes risk, and lattice-freeminimum Bayes risk, which are used for the final posterior output of thephoneme-based neural transducer with a limited context dependency. Compared tocriteria using N-best lists, lattice-free methods eliminate the decoding stepfor hypotheses generation during training, which leads to more efficienttraining. Experimental results show that lattice-free methods gain up to 6.5%relative improvement in word error rate compared to a sequence-levelcross-entropy trained model. Compared to the N-best-list based minimum Bayesrisk objectives, lattice-free methods gain 40% - 70% relative training timespeedup with a small degradation in performance.</description><author>Zijian Yang, Wei Zhou, Ralf Schlüter, Hermann Ney</author><pubDate>Thu, 25 May 2023 16:54:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04325v3</guid></item><item><title>Explainability Techniques for Chemical Language Models</title><link>http://arxiv.org/abs/2305.16192v1</link><description>Explainability techniques are crucial in gaining insights into the reasonsbehind the predictions of deep learning models, which have not yet been appliedto chemical language models. We propose an explainable AI technique thatattributes the importance of individual atoms towards the predictions made bythese models. Our method backpropagates the relevance information towards thechemical input string and visualizes the importance of individual atoms. Wefocus on self-attention Transformers operating on molecular stringrepresentations and leverage a pretrained encoder for finetuning. We showcasethe method by predicting and visualizing solubility in water and organicsolvents. We achieve competitive model performance while obtaininginterpretable predictions, which we use to inspect the pretrained model.</description><author>Stefan Hödl, William Robinson, Yoram Bachrach, Wilhelm Huck, Tal Kachman</author><pubDate>Thu, 25 May 2023 16:52:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16192v1</guid></item><item><title>Colloquium: Advances in automation of quantum dot devices control</title><link>http://arxiv.org/abs/2112.09362v3</link><description>Arrays of quantum dots (QDs) are a promising candidate system to realizescalable, coupled qubit systems and serve as a fundamental building block forquantum computers. In such semiconductor quantum systems, devices now have tensof individual electrostatic and dynamical voltages that must be carefully setto localize the system into the single-electron regime and to realize goodqubit operational performance. The mapping of requisite QD locations andcharges to gate voltages presents a challenging classical control problem. Withan increasing number of QD qubits, the relevant parameter space growssufficiently to make heuristic control unfeasible. In recent years, there hasbeen considerable effort to automate device control that combines script-basedalgorithms with machine learning (ML) techniques. In this Colloquium, acomprehensive overview of the recent progress in the automation of QD devicecontrol is presented, with a particular emphasis on silicon- and GaAs-based QDsformed in two-dimensional electron gases. Combining physics-based modeling withmodern numerical optimization and ML has proven effective in yieldingefficient, scalable control. Further integration of theoretical, computational,and experimental efforts with computer science and ML holds vast potential inadvancing semiconductor and other platforms for quantum computing.</description><author>Justyna P. Zwolak, Jacob M. Taylor</author><pubDate>Thu, 25 May 2023 16:52:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.09362v3</guid></item><item><title>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</title><link>http://arxiv.org/abs/2303.17580v3</link><description>Solving complicated AI tasks with different domains and modalities is a keystep toward artificial general intelligence. While there are abundant AI modelsavailable for different domains and modalities, they cannot handle complicatedAI tasks. Considering large language models (LLMs) have exhibited exceptionalability in language understanding, generation, interaction, and reasoning, weadvocate that LLMs could act as a controller to manage existing AI models tosolve complicated AI tasks and language could be a generic interface to empowerthis. Based on this philosophy, we present HuggingGPT, a framework thatleverages LLMs (e.g., ChatGPT) to connect various AI models in machine learningcommunities (e.g., Hugging Face) to solve AI tasks. Specifically, we useChatGPT to conduct task planning when receiving a user request, select modelsaccording to their function descriptions available in Hugging Face, executeeach subtask with the selected AI model, and summarize the response accordingto the execution results. By leveraging the strong language capability ofChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to covernumerous sophisticated AI tasks in different modalities and domains and achieveimpressive results in language, vision, speech, and other challenging tasks,which paves a new way towards artificial general intelligence.</description><author>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang</author><pubDate>Thu, 25 May 2023 16:50:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17580v3</guid></item><item><title>UpMax: User partitioning for MaxSAT</title><link>http://arxiv.org/abs/2305.16191v1</link><description>It has been shown that Maximum Satisfiability (MaxSAT) problem instances canbe effectively solved by partitioning the set of soft clauses into severaldisjoint sets. The partitioning methods can be based on clause weights (e.g.,stratification) or based on graph representations of the formula. Afterwards, amerge procedure is applied to guarantee that an optimal solution is found. This paper proposes a new framework called UpMax that decouples thepartitioning procedure from the MaxSAT solving algorithms. As a result, newpartitioning procedures can be defined independently of the MaxSAT algorithm tobe used. Moreover, this decoupling also allows users that build new MaxSATformulas to propose partition schemes based on knowledge of the problem to besolved. We illustrate this approach using several problems and show thatpartitioning has a large impact on the performance of unsatisfiability-basedMaxSAT algorithms.</description><author>Pedro Orvalho, Vasco Manquinho, Ruben Martins</author><pubDate>Thu, 25 May 2023 16:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16191v1</guid></item><item><title>Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders</title><link>http://arxiv.org/abs/2305.16189v1</link><description>Unsupervised source separation involves unraveling an unknown set of sourcesignals recorded through a mixing operator, with limited prior knowledge aboutthe sources, and only access to a dataset of signal mixtures. This problem isinherently ill-posed and is further challenged by the variety of time-scalesexhibited by sources in time series data. Existing methods typically rely on apreselected window size that limits their capacity to handle multi-scalesources. To address this issue, instead of operating in the time domain, wepropose an unsupervised multi-scale clustering and source separation frameworkby leveraging wavelet scattering covariances that provide a low-dimensionalrepresentation of stochastic processes, capable of distinguishing betweendifferent non-Gaussian stochastic processes. Nested within this representationspace, we develop a factorial Gaussian-mixture variational autoencoder that istrained to (1) probabilistically cluster sources at different time-scales and(2) independently sample scattering covariance representations associated witheach cluster. Using samples from each cluster as prior information, weformulate source separation as an optimization problem in the waveletscattering covariance representation space, resulting in separated sources inthe time domain. When applied to seismic data recorded during the NASA InSightmission on Mars, our multi-scale nested approach proves to be a powerful toolfor discriminating between sources varying greatly in time-scale, e.g.,minute-long transient one-sided pulses (known as ``glitches'') and structuredambient noises resulting from atmospheric activities that typically last fortens of minutes. These results provide an opportunity to conduct furtherinvestigations into the isolated sources related to atmospheric-surfaceinteractions, thermal relaxations, and other complex phenomena.</description><author>Ali Siahkoohi, Rudy Morel, Randall Balestriero, Erwan Allys, Grégory Sainton, Taichi Kawamura, Maarten V. de Hoop</author><pubDate>Thu, 25 May 2023 16:49:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16189v1</guid></item><item><title>MEGAN: Multi-Explanation Graph Attention Network</title><link>http://arxiv.org/abs/2211.13236v2</link><description>We propose a multi-explanation graph attention network (MEGAN). Unlikeexisting graph explainability methods, our network can produce node and edgeattributional explanations along multiple channels, the number of which isindependent of task specifications. This proves crucial to improve theinterpretability of graph regression predictions, as explanations can be splitinto positive and negative evidence w.r.t to a reference value. Additionally,our attention-based network is fully differentiable and explanations canactively be trained in an explanation-supervised manner. We first validate ourmodel on a synthetic graph regression dataset with known ground-truthexplanations. Our network outperforms existing baseline explainability methodsfor the single- as well as the multi-explanation case, achieving near-perfectexplanation accuracy during explanation supervision. Finally, we demonstrateour model's capabilities on multiple real-world datasets. We find that ourmodel produces sparse high-fidelity explanations consistent with humanintuition about those tasks.</description><author>Jonas Teufel, Luca Torresi, Patrick Reiser, Pascal Friederich</author><pubDate>Thu, 25 May 2023 16:48:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13236v2</guid></item><item><title>Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction</title><link>http://arxiv.org/abs/2207.14116v2</link><description>We present Claim-Dissector: a novel latent variable model for fact-checkingand analysis, which given a claim and a set of retrieved evidences jointlylearns to identify: (i) the relevant evidences to the given claim, (ii) theveracity of the claim. We propose to disentangle the per-evidence relevanceprobability and its contribution to the final veracity probability in aninterpretable way -- the final veracity probability is proportional to a linearensemble of per-evidence relevance probabilities. In this way, the individualcontributions of evidences towards the final predicted probability can beidentified. In per-evidence relevance probability, our model can furtherdistinguish whether each relevant evidence is supporting (S) or refuting (R)the claim. This allows to quantify how much the S/R probability contributes tothe final verdict or to detect disagreeing evidence. Despite its interpretable nature, our system achieves results competitivewith state-of-the-art on the FEVER dataset, as compared to typical two-stagesystem pipelines, while using significantly fewer parameters. It also sets newstate-of-the-art on FAVIQ and RealFC datasets. Furthermore, our analysis showsthat our model can learn fine-grained relevance cues while using coarse-grainedsupervision, and we demonstrate it in 2 ways. (i) We show that our model canachieve competitive sentence recall while using only paragraph-level relevancesupervision. (ii) Traversing towards the finest granularity of relevance, weshow that our model is capable of identifying relevance at the token level. Todo this, we present a new benchmark TLR-FEVER focusing on token-levelinterpretability -- humans annotate tokens in relevant evidences theyconsidered essential when making their judgment. Then we measure how similarare these annotations to the tokens our model is focusing on.</description><author>Martin Fajcik, Petr Motlicek, Pavel Smrz</author><pubDate>Thu, 25 May 2023 16:44:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.14116v2</guid></item><item><title>Accelerated Methods for Riemannian Min-Max Optimization Ensuring Bounded Geometric Penalties</title><link>http://arxiv.org/abs/2305.16186v1</link><description>In this work, we study optimization problems of the form $\min_x \max_y f(x,y)$, where $f(x, y)$ is defined on a product Riemannian manifold $\mathcal{M}\times \mathcal{N}$ and is $\mu_x$-strongly geodesically convex (g-convex) in$x$ and $\mu_y$-strongly g-concave in $y$, for $\mu_x, \mu_y \geq 0$. We designaccelerated methods when $f$ is $(L_x, L_y, L_{xy})$-smooth and $\mathcal{M}$,$\mathcal{N}$ are Hadamard. To that aim we introduce new g-convex optimizationresults, of independent interest: we show global linear convergence formetric-projected Riemannian gradient descent and improve existing acceleratedmethods by reducing geometric constants. Additionally, we complete the analysisof two previous works applying to the Riemannian min-max case by removing anassumption about iterates staying in a pre-specified compact set.</description><author>David Martínez-Rubio, Christophe Roux, Christopher Criscitiello, Sebastian Pokutta</author><pubDate>Thu, 25 May 2023 16:43:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16186v1</guid></item><item><title>Passive learning of active causal strategies in agents and language models</title><link>http://arxiv.org/abs/2305.16183v1</link><description>What can be learned about causality and experimentation from passive data?This question is salient given recent successes of passively-trained languagemodels in interactive domains such as tool use. Passive learning is inherentlylimited. However, we show that purely passive learning can in fact allow anagent to learn generalizable strategies for determining and using causalstructures, as long as the agent can intervene at test time. We formallyillustrate that learning a strategy of first experimenting, then seeking goals,can allow generalization from passive learning in principle. We then showempirically that agents trained via imitation on expert data can indeedgeneralize at test time to infer and use causal links which are never presentin the training data; these agents can also generalize experimentationstrategies to novel variable sets never observed in training. We then show thatstrategies for causal intervention and exploitation can be generalized frompassive data even in a more complex environment with high-dimensionalobservations, with the support of natural language explanations. Explanationscan even allow passive learners to generalize out-of-distribution fromperfectly-confounded training data. Finally, we show that language models,trained only on passive next-word prediction, can generalize causalintervention strategies from a few-shot prompt containing examples ofexperimentation, together with explanations and reasoning. These resultshighlight the surprising power of passive learning of active causal strategies,and may help to understand the behaviors and capabilities of language models.</description><author>Andrew Kyle Lampinen, Stephanie C Y Chan, Ishita Dasgupta, Andrew J Nam, Jane X Wang</author><pubDate>Thu, 25 May 2023 16:39:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16183v1</guid></item><item><title>Dropout Drops Double Descent</title><link>http://arxiv.org/abs/2305.16179v1</link><description>In this paper, we find and analyze that we can easily drop the double descentby only adding one dropout layer before the fully-connected linear layer. Thesurprising double-descent phenomenon has drawn public attention in recentyears, making the prediction error rise and drop as we increase either sampleor model size. The current paper shows that it is possible to alleviate thesephenomena by using optimal dropout in the linear regression model and thenonlinear random feature regression, both theoretically and empirically. %${y}=X{\beta}^0+{\epsilon}$ with $X\in\mathbb{R}^{n\times p}$. We obtain theoptimal dropout hyperparameter by estimating the ground truth ${\beta}^0$ withgeneralized ridge typed estimator$\hat{{\beta}}=(X^TX+\alpha\cdot\mathrm{diag}(X^TX))^{-1}X^T{y}$. Moreover, weempirically show that optimal dropout can achieve a monotonic test error curvein nonlinear neural networks using Fashion-MNIST and CIFAR-10. Our resultssuggest considering dropout for risk curve scaling when meeting the peakphenomenon. In addition, we figure out why previous deep learning models do notencounter double-descent scenarios -- because we already apply a usualregularization approach like the dropout in our models. To our best knowledge,this paper is the first to analyze the relationship between dropout and doubledescent.</description><author>Tian-Le Yang, Joe Suzuki</author><pubDate>Thu, 25 May 2023 16:35:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16179v1</guid></item><item><title>Federated Multi-organ Segmentation with Inconsistent Labels</title><link>http://arxiv.org/abs/2206.07156v2</link><description>Federated learning is an emerging paradigm allowing large-scale decentralizedlearning without sharing data across different data owners, which helps addressthe concern of data privacy in medical image analysis. However, the requirementfor label consistency across clients by the existing methods largely narrowsits application scope. In practice, each clinical site may only annotatecertain organs of interest with partial or no overlap with other sites.Incorporating such partially labeled data into a unified federation is anunexplored problem with clinical significance and urgency. This work tacklesthe challenge by using a novel federated multi-encoding U-Net (Fed-MENU) methodfor multi-organ segmentation. In our method, a multi-encoding U-Net (MENU-Net)is proposed to extract organ-specific features through different encodingsub-networks. Each sub-network can be seen as an expert of a specific organ andtrained for that client. Moreover, to encourage the organ-specific featuresextracted by different sub-networks to be informative and distinctive, weregularize the training of the MENU-Net by designing an auxiliary genericdecoder (AGD). Extensive experiments on six public abdominal CT datasets showthat our Fed-MENU method can effectively obtain a federated learning modelusing the partially labeled datasets with superior performance to other modelstrained by either localized or centralized learning methods. Source code ispublicly available at https://github.com/DIAL-RPI/Fed-MENU.</description><author>Xuanang Xu, Hannah H. Deng, Jaime Gateno, Pingkun Yan</author><pubDate>Thu, 25 May 2023 16:34:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.07156v2</guid></item><item><title>From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module</title><link>http://arxiv.org/abs/2305.16174v1</link><description>Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks(GNNs) on a given graph topology by dynamically learning it. However, most ofLGI methods assume to have a (noisy, incomplete, improvable, ...) input graphto rewire and can solely learn regular graph topologies. In the wake of thesuccess of Topological Deep Learning (TDL), we study Latent Topology Inference(LTI) for learning higher-order cell complexes (with sparse and not regulartopology) describing multi-way interactions between data points. To this aim,we introduce the Differentiable Cell Complex Module (DCM), a novel learnablefunction that computes cell probabilities in the complex to improve thedownstream task. We show how to integrate DCM with cell complex message passingnetworks layers and train it in a end-to-end fashion, thanks to a two-stepinference procedure that avoids an exhaustive search across all possible cellsin the input, thus maintaining scalability. Our model is tested on severalhomophilic and heterophilic graph datasets and it is shown to outperform otherstate-of-the-art techniques, offering significant improvements especially incases where an input graph is not provided.</description><author>Claudio Battiloro, Indro Spinelli, Lev Telyatnikov, Michael Bronstein, Simone Scardapane, Paolo Di Lorenzo</author><pubDate>Thu, 25 May 2023 16:33:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16174v1</guid></item><item><title>Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration</title><link>http://arxiv.org/abs/2305.16173v1</link><description>Since the control of the Lipschitz constant has a great impact on thetraining stability, generalization, and robustness of neural networks, theestimation of this value is nowadays a real scientific challenge. In this paperwe introduce a precise, fast, and differentiable upper bound for the spectralnorm of convolutional layers using circulant matrix theory and a newalternative to the Power iteration. Called the Gram iteration, our approachexhibits a superlinear convergence. First, we show through a comprehensive setof experiments that our approach outperforms other state-of-the-art methods interms of precision, computational cost, and scalability. Then, it proves highlyeffective for the Lipschitz regularization of convolutional neural networks,with competitive results against concurrent approaches.</description><author>Blaise Delattre, Quentin Barthélemy, Alexandre Araujo, Alexandre Allauzen</author><pubDate>Thu, 25 May 2023 16:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16173v1</guid></item><item><title>Masked and Permuted Implicit Context Learning for Scene Text Recognition</title><link>http://arxiv.org/abs/2305.16172v1</link><description>Scene Text Recognition (STR) is a challenging task due to variations in textstyle, shape, and background. Incorporating linguistic information is aneffective way to enhance the robustness of STR models. Existing methods rely onpermuted language modeling (PLM) or masked language modeling (MLM) to learncontextual information implicitly, either through an ensemble of permutedautoregressive (AR) LMs training or iterative non-autoregressive (NAR) decodingprocedure. However, these methods exhibit limitations: PLM's AR decodingresults in the lack of information about future characters, while MLM providesglobal information of the entire text but neglects dependencies among eachpredicted character. In this paper, we propose a Masked and Permuted ImplicitContext Learning Network for STR, which unifies PLM and MLM within a singledecoding architecture, inheriting the advantages of both approaches. We utilizethe training procedure of PLM, and to integrate MLM, we incorporate word lengthinformation into the decoding process by introducing specific numbers of masktokens. Experimental results demonstrate that our proposed model achievesstate-of-the-art performance on standard benchmarks using both AR and NARdecoding procedures.</description><author>Xiaomeng Yang, Zhi Qiao, Jin Wei, Yu Zhou, Ye Yuan, Zhilong Ji, Dongbao Yang, Weiping Wang</author><pubDate>Thu, 25 May 2023 16:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16172v1</guid></item><item><title>Multi-lingual and Multi-cultural Figurative Language Understanding</title><link>http://arxiv.org/abs/2305.16171v1</link><description>Figurative language permeates human communication, but at the same time isrelatively understudied in NLP. Datasets have been created in English toaccelerate progress towards measuring and improving figurative languageprocessing in language models (LMs). However, the use of figurative language isan expression of our cultural and societal experiences, making it difficult forthese phrases to be universally applicable. In this work, we create afigurative language inference dataset, \datasetname, for seven diverselanguages associated with a variety of cultures: Hindi, Indonesian, Javanese,Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each languagerelies on cultural and regional concepts for figurative expressions, with thehighest overlap between languages originating from the same region. We assessmultilingual LMs' abilities to interpret figurative language in zero-shot andfew-shot settings. All languages exhibit a significant deficiency compared toEnglish, with variations in performance reflecting the availability ofpre-training and fine-tuning data, emphasizing the need for LMs to be exposedto a broader range of linguistic and cultural variation during training.</description><author>Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Indra Winata, Samuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, Graham Neubig</author><pubDate>Thu, 25 May 2023 16:30:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16171v1</guid></item><item><title>Archetypal Analysis++: Rethinking the Initialization Strategy</title><link>http://arxiv.org/abs/2301.13748v2</link><description>Archetypal analysis is a matrix factorization method with convexityconstraints. Due to local minima, a good initialization is essential, butfrequently used initialization methods yield either sub-optimal starting pointsor are prone to get stuck in poor local minima. In this paper, we proposearchetypal analysis++ (AA++), a probabilistic initialization strategy forarchetypal analysis that sequentially samples points based on their influenceon the objective, similar to $k$-means++. In fact, we argue that $k$-means++already approximates the proposed initialization method. Furthermore, wesuggest to adapt an efficient Monte Carlo approximation of $k$-means++ to AA++.In an extensive empirical evaluation of 13 real-world data sets of varyingsizes and dimensionalities and considering two pre-processing strategies, weshow that AA++ nearly always outperforms all baselines, including the mostfrequently used ones.</description><author>Sebastian Mair, Jens Sjölund</author><pubDate>Thu, 25 May 2023 16:29:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13748v2</guid></item><item><title>Multimodal Relation Extraction with Cross-Modal Retrieval and Synthesis</title><link>http://arxiv.org/abs/2305.16166v1</link><description>Multimodal relation extraction (MRE) is the task of identifying the semanticrelationships between two entities based on the context of the sentence imagepair. Existing retrieval-augmented approaches mainly focused on modeling theretrieved textual knowledge, but this may not be able to accurately identifycomplex relations. To improve the prediction, this research proposes toretrieve textual and visual evidence based on the object, sentence, and wholeimage. We further develop a novel approach to synthesize the object-level,image-level, and sentence-level information for better reasoning between thesame and different modalities. Extensive experiments and analyses show that theproposed method is able to effectively select and compare evidence acrossmodalities and significantly outperforms state-of-the-art models.</description><author>Xuming Hu, Zhijiang Guo, Zhiyang Teng, Irwin King, Philip S. Yu</author><pubDate>Thu, 25 May 2023 16:26:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16166v1</guid></item><item><title>Feature Collapse</title><link>http://arxiv.org/abs/2305.16162v1</link><description>We formalize and study a phenomenon called feature collapse that makesprecise the intuitive idea that entities playing a similar role in a learningtask receive similar representations. As feature collapse requires a notion oftask, we leverage a simple but prototypical NLP task to study it. We start byshowing experimentally that feature collapse goes hand in hand withgeneralization. We then prove that, in the large sample limit, distinct wordsthat play identical roles in this NLP task receive identical local featurerepresentations in a neural network. This analysis reveals the crucial rolethat normalization mechanisms, such as LayerNorm, play in feature collapse andin generalization.</description><author>Thomas Laurent, James H. von Brecht, Xavier Bresson</author><pubDate>Thu, 25 May 2023 16:25:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16162v1</guid></item></channel></rss>