<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 22 Aug 2023 06:00:18 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>CamP: Camera Preconditioning for Neural Radiance Fields</title><link>http://arxiv.org/abs/2308.10902v1</link><description>Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3Dscene reconstructions of objects and large-scale scenes. However, NeRFs requireaccurate camera parameters as input -- inaccurate camera parameters result inblurry renderings. Extrinsic and intrinsic camera parameters are usuallyestimated using Structure-from-Motion (SfM) methods as a pre-processing step toNeRF, but these techniques rarely yield perfect estimates. Thus, prior workshave proposed jointly optimizing camera parameters alongside a NeRF, but thesemethods are prone to local minima in challenging settings. In this work, weanalyze how different camera parameterizations affect this joint optimizationproblem, and observe that standard parameterizations exhibit large differencesin magnitude with respect to small perturbations, which can lead to anill-conditioned optimization problem. We propose using a proxy problem tocompute a whitening transform that eliminates the correlation between cameraparameters and normalizes their effects, and we propose to use this transformas a preconditioner for the camera parameters during joint optimization. Ourpreconditioned camera optimization significantly improves reconstructionquality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE)by 67% compared to state-of-the-art NeRF approaches that do not optimize forcameras like Zip-NeRF, and by 29% relative to state-of-the-art jointoptimization approaches using the camera parameterization of SCNeRF. Ourapproach is easy to implement, does not significantly increase runtime, can beapplied to a wide variety of camera parameterizations, and canstraightforwardly be incorporated into other NeRF-like models.</description><author>Keunhong Park, Philipp Henzler, Ben Mildenhall, Jonathan T. Barron, Ricardo Martin-Brualla</author><pubDate>Mon, 21 Aug 2023 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10902v1</guid></item><item><title>Structured World Models from Human Videos</title><link>http://arxiv.org/abs/2308.10901v1</link><description>We tackle the problem of learning complex, general behaviors directly in thereal world. We propose an approach for robots to efficiently learn manipulationskills using only a handful of real-world interaction trajectories from manydifferent settings. Inspired by the success of learning from large-scaledatasets in the fields of computer vision and natural language, our belief isthat in order to efficiently learn, a robot must be able to leverageinternet-scale, human video data. Humans interact with the world in manyinteresting ways, which can allow a robot to not only build an understanding ofuseful actions and affordances but also how these actions affect the world formanipulation. Our approach builds a structured, human-centric action spacegrounded in visual affordances learned from human videos. Further, we train aworld model on human videos and fine-tune on a small amount of robotinteraction data without any task supervision. We show that this approach ofaffordance-space world models enables different robots to learn variousmanipulation skills in complex settings, in under 30 minutes of interaction.Videos can be found at https://human-world-model.github.io</description><author>Russell Mendonca, Shikhar Bahl, Deepak Pathak</author><pubDate>Mon, 21 Aug 2023 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10901v1</guid></item><item><title>TADA! Text to Animatable Digital Avatars</title><link>http://arxiv.org/abs/2308.10899v1</link><description>We introduce TADA, a simple-yet-effective approach that takes textualdescriptions and produces expressive 3D avatars with high-quality geometry andlifelike textures, that can be animated and rendered with traditional graphicspipelines. Existing text-based character generation methods are limited interms of geometry and texture quality, and cannot be realistically animated dueto inconsistent alignment between the geometry and the texture, particularly inthe face region. To overcome these limitations, TADA leverages the synergy of a2D diffusion model and an animatable parametric body model. Specifically, wederive an optimizable high-resolution body model from SMPL-X with 3Ddisplacements and a texture map, and use hierarchical rendering with scoredistillation sampling (SDS) to create high-quality, detailed, holistic 3Davatars from text. To ensure alignment between the geometry and texture, werender normals and RGB images of the generated character and exploit theirlatent embeddings in the SDS training process. We further introduce variousexpression parameters to deform the generated character during training,ensuring that the semantics of our generated character remain consistent withthe original SMPL-X model, resulting in an animatable character. Comprehensiveevaluations demonstrate that TADA significantly surpasses existing approacheson both qualitative and quantitative measures. TADA enables creation oflarge-scale digital character assets that are ready for animation andrendering, while also being easily editable through natural language. The codewill be public for research purposes.</description><author>Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang, Yangyi Huang, Justus Thies, Michael J. Black</author><pubDate>Mon, 21 Aug 2023 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10899v1</guid></item><item><title>Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation</title><link>http://arxiv.org/abs/2308.10898v1</link><description>We study the problem of few-shot physically-aware articulated meshgeneration. By observing an articulated object dataset containing only a fewexamples, we wish to learn a model that can generate diverse meshes with highvisual fidelity and physical validity. Previous mesh generative models eitherhave difficulties in depicting a diverse data space from only a few examples orfail to ensure physical validity of their samples. Regarding the abovechallenges, we propose two key innovations, including 1) a hierarchical meshdeformation-based generative model based upon the divide-and-conquer philosophyto alleviate the few-shot challenge by borrowing transferrable deformationpatterns from large scale rigid meshes and 2) a physics-aware deformationcorrection scheme to encourage physically plausible generations. We conductextensive experiments on 6 articulated categories to demonstrate thesuperiority of our method in generating articulated meshes with betterdiversity, higher visual fidelity, and better physical validity over previousmethods in the few-shot setting. Further, we validate solid contributions ofour two innovations in the ablation study. Project page with code is availableat https://meowuu7.github.io/few-arti-obj-gen.</description><author>Xueyi Liu, Bin Wang, He Wang, Li Yi</author><pubDate>Mon, 21 Aug 2023 18:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10898v1</guid></item><item><title>Can Language Models Learn to Listen?</title><link>http://arxiv.org/abs/2308.10897v1</link><description>We present a framework for generating appropriate facial responses from alistener in dyadic social interactions based on the speaker's words. Given aninput transcription of the speaker's words with their timestamps, our approachautoregressively predicts a response of a listener: a sequence of listenerfacial gestures, quantized using a VQ-VAE. Since gesture is a languagecomponent, we propose treating the quantized atomic motion elements asadditional language token inputs to a transformer-based large language model.Initializing our transformer with the weights of a language model pre-trainedonly on text results in significantly higher quality listener responses thantraining a transformer from scratch. We show that our generated listener motionis fluent and reflective of language semantics through quantitative metrics anda qualitative user study. In our evaluation, we analyze the model's ability toutilize temporal and semantic aspects of spoken text. Project page:https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/</description><author>Evonne Ng, Sanjay Subramanian, Dan Klein, Angjoo Kanazawa, Trevor Darrell, Shiry Ginosar</author><pubDate>Mon, 21 Aug 2023 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10897v1</guid></item><item><title>Differentiable Shadow Mapping for Efficient Inverse Graphics</title><link>http://arxiv.org/abs/2308.10896v1</link><description>We show how shadows can be efficiently generated in differentiable renderingof triangle meshes. Our central observation is that pre-filtered shadowmapping, a technique for approximating shadows based on rendering from theperspective of a light, can be combined with existing differentiablerasterizers to yield differentiable visibility information. We demonstrate atseveral inverse graphics problems that differentiable shadow maps are orders ofmagnitude faster than differentiable light transport simulation with similaraccuracy -- while differentiable rasterization without shadows often fails toconverge.</description><author>Markus Worchel, Marc Alexa</author><pubDate>Mon, 21 Aug 2023 18:58:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10896v1</guid></item><item><title>YOLOBench: Benchmarking Efficient Object Detectors on Embedded Systems</title><link>http://arxiv.org/abs/2307.13901v2</link><description>We present YOLOBench, a benchmark comprised of 550+ YOLO-based objectdetection models on 4 different datasets and 4 different embedded hardwareplatforms (x86 CPU, ARM CPU, Nvidia GPU, NPU). We collect accuracy and latencynumbers for a variety of YOLO-based one-stage detectors at different modelscales by performing a fair, controlled comparison of these detectors with afixed training environment (code and training hyperparameters).Pareto-optimality analysis of the collected data reveals that, if moderndetection heads and training techniques are incorporated into the learningprocess, multiple architectures of the YOLO series achieve a goodaccuracy-latency trade-off, including older models like YOLOv3 and YOLOv4. Wealso evaluate training-free accuracy estimators used in neural architecturesearch on YOLOBench and demonstrate that, while most state-of-the-art zero-costaccuracy estimators are outperformed by a simple baseline like MAC count, someof them can be effectively used to predict Pareto-optimal detection models. Weshowcase that by using a zero-cost proxy to identify a YOLO architecturecompetitive against a state-of-the-art YOLOv8 model on a Raspberry Pi 4 CPU.The code and data are available athttps://github.com/Deeplite/deeplite-torch-zoo</description><author>Ivan Lazarevich, Matteo Grimaldi, Ravish Kumar, Saptarshi Mitra, Shahrukh Khan, Sudhakar Sah</author><pubDate>Mon, 21 Aug 2023 18:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13901v2</guid></item><item><title>A New Pathway to Approximate Energy Expenditure and Recovery of an Athlete</title><link>http://arxiv.org/abs/2104.07903v3</link><description>This work proposes to use evolutionary computation as a pathway to allow anew perspective on the modeling of energy expenditure and recovery of anindividual athlete during exercise. We revisit a theoretical concept called the"three component hydraulic model" which is designed to simulate metabolicsystems during exercise and which is able to address recently highlightedshortcomings of currently applied performance models. This hydraulic model hasnot been entirely validated on individual athletes because it depends onphysiological measures that cannot be acquired in the required precision orquantity. This paper introduces a generalized interpretation and formalizationof the three component hydraulic model that removes its ties to concretemetabolic measures and allows to use evolutionary computation to fit itsparameters to an athlete.</description><author>Fabian Clemens Weigend, Jason Siegler, Oliver Obst</author><pubDate>Mon, 21 Aug 2023 18:47:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.07903v3</guid></item><item><title>A Gated Attention Transformer for Multi-Person Pose Tracking</title><link>http://arxiv.org/abs/2306.05807v2</link><description>Multi-person pose tracking is an important element for many applications andrequires to estimate the human poses of all persons in a video and to trackthem over time. The association of poses across frames remains an open researchproblem, in particular for online tracking methods, due to motion blur, crowdedscenes and occlusions. To tackle the association challenge, we propose a GatedAttention Transformer. The core aspect of our model is the gating mechanismthat automatically adapts the impact of appearance embeddings and embeddingsbased on temporal pose similarity in the attention layers. In order tore-identify persons that have been occluded, we incorporate a pose-conditionedre-identification network that provides initial embeddings and allows to matchpersons even if the number of visible joints differ between frames. We furtherpropose a matching layer based on gated attention for pose-to-track associationand duplicate removal. We evaluate our approach on PoseTrack 2018 andPoseTrack21.</description><author>Andreas Doering, Juergen Gall</author><pubDate>Mon, 21 Aug 2023 18:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05807v2</guid></item><item><title>Unlocking Accuracy and Fairness in Differentially Private Image Classification</title><link>http://arxiv.org/abs/2308.10888v1</link><description>Privacy-preserving machine learning aims to train models on private datawithout leaking sensitive information. Differential privacy (DP) is consideredthe gold standard framework for privacy-preserving training, as it providesformal privacy guarantees. However, compared to their non-private counterparts,models trained with DP often have significantly reduced accuracy. Privateclassifiers are also believed to exhibit larger performance disparities acrosssubpopulations, raising fairness concerns. The poor performance of classifierstrained with DP has prevented the widespread adoption of privacy preservingmachine learning in industry. Here we show that pre-trained foundation modelsfine-tuned with DP can achieve similar accuracy to non-private classifiers,even in the presence of significant distribution shifts between pre-trainingdata and downstream tasks. We achieve private accuracies within a few percentof the non-private state of the art across four datasets, including two medicalimaging benchmarks. Furthermore, our private medical classifiers do not exhibitlarger performance disparities across demographic groups than non-privatemodels. This milestone to make DP training a practical and reliable technologyhas the potential to widely enable machine learning practitioners to trainsafely on sensitive datasets while protecting individuals' privacy.</description><author>Leonard Berrada, Soham De, Judy Hanwen Shen, Jamie Hayes, Robert Stanforth, David Stutz, Pushmeet Kohli, Samuel L. Smith, Borja Balle</author><pubDate>Mon, 21 Aug 2023 18:42:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10888v1</guid></item><item><title>HLSDataset: Open-Source Dataset for ML-Assisted FPGA Design using High Level Synthesis</title><link>http://arxiv.org/abs/2302.10977v2</link><description>Machine Learning (ML) has been widely adopted in design exploration usinghigh level synthesis (HLS) to give a better and faster performance, andresource and power estimation at very early stages for FPGA-based design. Toperform prediction accurately, high-quality and large-volume datasets arerequired for training ML models.This paper presents a dataset for ML-assistedFPGA design using HLS, called HLSDataset. The dataset is generated from widelyused HLS C benchmarks including Polybench, Machsuite, CHStone and Rossetta. TheVerilog samples are generated with a variety of directives including loopunroll, loop pipeline and array partition to make sure optimized and realisticdesigns are covered. The total number of generated Verilog samples is nearly9,000 per FPGA type. To demonstrate the effectiveness of our dataset, weundertake case studies to perform power estimation and resource usageestimation with ML models trained with our dataset. All the codes and datasetare public at the github repo.We believe that HLSDataset can save valuable timefor researchers by avoiding the tedious process of running tools, scripting andparsing files to generate the dataset, and enable them to spend more time whereit counts, that is, in training ML models.</description><author>Zhigang Wei, Aman Arora, Ruihao Li, Lizy K. John</author><pubDate>Mon, 21 Aug 2023 18:36:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10977v2</guid></item><item><title>Giraffe: Adventures in Expanding Context Lengths in LLMs</title><link>http://arxiv.org/abs/2308.10882v1</link><description>Modern large language models (LLMs) that rely on attention mechanisms aretypically trained with fixed context lengths which enforce upper limits on thelength of input sequences that they can handle at evaluation time. To use thesemodels on sequences longer than the train-time context length, one might employtechniques from the growing family of context length extrapolation methods --most of which focus on modifying the system of positional encodings used in theattention mechanism to indicate where tokens or activations are located in theinput sequence. We conduct a wide survey of existing methods of context lengthextrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our owndesign as well -- in particular, a new truncation strategy for modifying thebasis for the position encoding. We test these methods using three new evaluation tasks (FreeFormQA,AlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find tobe less fine-grained as a measure of long context performance of LLMs. Werelease the three tasks publicly as datasets on HuggingFace. We discover thatlinear scaling is the best method for extending context length, and show thatfurther gains can be achieved by using longer scales at evaluation time. Wealso discover promising extrapolation capabilities in the truncated basis. Tosupport further research in this area, we release three new 13B parameterlong-context models which we call Giraffe: 4k and 16k context models trainedfrom base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. Wealso release the code to replicate our results.</description><author>Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, Siddartha Naidu</author><pubDate>Mon, 21 Aug 2023 18:30:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10882v1</guid></item><item><title>Bias and Extrapolation in Markovian Linear Stochastic Approximation with Constant Stepsizes</title><link>http://arxiv.org/abs/2210.00953v3</link><description>We consider Linear Stochastic Approximation (LSA) with a constant stepsizeand Markovian data. Viewing the joint process of the data and LSA iterate as atime-homogeneous Markov chain, we prove its convergence to a unique limitingand stationary distribution in Wasserstein distance and establishnon-asymptotic, geometric convergence rates. Furthermore, we show that the biasvector of this limit admits an infinite series expansion with respect to thestepsize. Consequently, the bias is proportional to the stepsize up to higherorder terms. This result stands in contrast with LSA under i.i.d. data, forwhich the bias vanishes. In the reversible chain setting, we provide a generalcharacterization of the relationship between the bias and the mixing time ofthe Markovian data, establishing that they are roughly proportional to eachother. While Polyak-Ruppert tail-averaging reduces the variance of the LSA iterates,it does not affect the bias. The above characterization allows us to show thatthe bias can be reduced using Richardson-Romberg extrapolation with $m\ge 2$stepsizes, which eliminates the $m-1$ leading terms in the bias expansion. Thisextrapolation scheme leads to an exponentially smaller bias and an improvedmean squared error, both in theory and empirically. Our results immediatelyapply to the Temporal Difference learning algorithm with linear functionapproximation, Markovian data, and constant stepsizes.</description><author>Dongyan Huo, Yudong Chen, Qiaomin Xie</author><pubDate>Mon, 21 Aug 2023 18:23:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.00953v3</guid></item><item><title>Analyzing Transformer Dynamics as Movement through Embedding Space</title><link>http://arxiv.org/abs/2308.10874v1</link><description>Transformer language models exhibit intelligent behaviors such asunderstanding natural language, recognizing patterns, acquiring knowledge,reasoning, planning, reflecting and using tools. This paper explores how theirunderlying mechanics give rise to intelligent behaviors. We adopt a systemsapproach to analyze Transformers in detail and develop a mathematical frameworkthat frames their dynamics as movement through embedding space. This novelperspective provides a principled way of thinking about the problem and revealsimportant insights related to the emergence of intelligence: 1. At its core the Transformer is a Embedding Space walker, mappingintelligent behavior to trajectories in this vector space. 2. At each step of the walk, it composes context into a single compositevector whose location in Embedding Space defines the next step. 3. No learning actually occurs during decoding; in-context learning andgeneralization are simply the result of different contexts composing intodifferent vectors. 4. Ultimately the knowledge, intelligence and skills exhibited by the modelare embodied in the organization of vectors in Embedding Space rather than inspecific neurons or layers. These abilities are properties of thisorganization. 5. Attention's contribution boils down to the association-bias it lends tovector composition and which influences the aforementioned organization.However, more investigation is needed to ascertain its significance. 6. The entire model is composed from two principal operations: dataindependent filtering and data dependent aggregation. This generalizationunifies Transformers with other sequence models and across modalities. Building upon this foundation we formalize and test a semantic space theorywhich posits that embedding vectors represent semantic concepts and find someevidence of its validity.</description><author>Sumeet S. Singh</author><pubDate>Mon, 21 Aug 2023 18:21:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10874v1</guid></item><item><title>SpikingBERT: Distilling BERT to Train Spiking Language Models Using Implicit Differentiation</title><link>http://arxiv.org/abs/2308.10873v1</link><description>Large language Models (LLMs), though growing exceedingly powerful, comprisesof orders of magnitude less neurons and synapses than the human brain. However,it requires significantly more power/energy to operate. In this work, wepropose a novel bio-inspired spiking language model (LM) which aims to reducethe computational cost of conventional LMs by drawing motivation from thesynaptic information flow in the brain. In this paper, we demonstrate aframework that leverages the average spiking rate of neurons at equilibrium totrain a neuromorphic spiking LM using implicit differentiation technique,thereby overcoming the non-differentiability problem of spiking neural network(SNN) based algorithms without using any type of surrogate gradient. Thesteady-state convergence of the spiking neurons also allows us to design aspiking attention mechanism, which is critical in developing a scalable spikingLM. Moreover, the convergence of average spiking rate of neurons at equilibriumis utilized to develop a novel ANN-SNN knowledge distillation based techniquewherein we use a pre-trained BERT model as "teacher" to train our "student"spiking architecture. While the primary architecture proposed in this paper ismotivated by BERT, the technique can be potentially extended to different kindsof LLMs. Our work is the first one to demonstrate the performance of anoperational spiking LM architecture on multiple different tasks in the GLUEbenchmark.</description><author>Malyaban Bal, Abhronil Sengupta</author><pubDate>Mon, 21 Aug 2023 18:20:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10873v1</guid></item><item><title>Low-Variance Forward Gradients using Direct Feedback Alignment and Momentum</title><link>http://arxiv.org/abs/2212.07282v4</link><description>Supervised learning in deep neural networks is commonly performed using errorbackpropagation. However, the sequential propagation of errors during thebackward pass limits its scalability and applicability to low-poweredneuromorphic hardware. Therefore, there is growing interest in finding localalternatives to backpropagation. Recently proposed methods based onforward-mode automatic differentiation suffer from high variance in large deepneural networks, which affects convergence. In this paper, we propose theForward Direct Feedback Alignment algorithm that combines Activity-PerturbedForward Gradients with Direct Feedback Alignment and momentum. We provide boththeoretical proofs and empirical evidence that our proposed method achieveslower variance than forward gradient techniques. In this way, our approachenables faster convergence and better performance when compared to other localalternatives to backpropagation and opens a new perspective for the developmentof online learning algorithms compatible with neuromorphic systems.</description><author>Florian Bacho, Dominique Chu</author><pubDate>Mon, 21 Aug 2023 18:17:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07282v4</guid></item><item><title>Multi-Directional Subspace Editing in Style-Space</title><link>http://arxiv.org/abs/2211.11825v2</link><description>This paper describes a new technique for finding disentangled semanticdirections in the latent space of StyleGAN. Our method identifies meaningfulorthogonal subspaces that allow editing of one human face attribute, whileminimizing undesired changes in other attributes. Our model is capable ofediting a single attribute in multiple directions, resulting in a range ofpossible generated images. We compare our scheme with three state-of-the-artmodels and show that our method outperforms them in terms of face editing anddisentanglement capabilities. Additionally, we suggest quantitative measuresfor evaluating attribute separation and disentanglement, and exhibit thesuperiority of our model with respect to those measures.</description><author>Chen Naveh, Yacov Hel-Or</author><pubDate>Mon, 21 Aug 2023 18:11:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11825v2</guid></item><item><title>A deep complementary energy method for solid mechanics using minimum complementary energy principle</title><link>http://arxiv.org/abs/2302.01538v5</link><description>In recent years, the rapid advancement of deep learning has significantlyimpacted various fields, particularly in solving partial differential equations(PDEs) in the realm of solid mechanics, benefiting greatly from the remarkableapproximation capabilities of neural networks. In solving PDEs,Physics-Informed Neural Networks (PINNs) and the Deep Energy Method (DEM) havegarnered substantial attention. The principle of minimum potential energy andcomplementary energy are two important variational principles in solidmechanics. However, the well-known Deep Energy Method (DEM) is based on theprinciple of minimum potential energy, but there lacks the important form ofminimum complementary energy. To bridge this gap, we propose the deepcomplementary energy method (DCEM) based on the principle of minimumcomplementary energy. The output function of DCEM is the stress function, whichinherently satisfies the equilibrium equation. We present numerical resultsusing the Prandtl and Airy stress functions, and compare DCEM with existingPINNs and DEM algorithms when modeling representative mechanical problems. Theresults demonstrate that DCEM outperforms DEM in terms of stress accuracy andefficiency and has an advantage in dealing with complex displacement boundaryconditions, which is supported by theoretical analyses and numericalsimulations. We extend DCEM to DCEM-Plus (DCEM-P), adding terms that satisfypartial differential equations. Furthermore, we propose a deep complementaryenergy operator method (DCEM-O) by combining operator learning with physicalequations. Initially, we train DCEM-O using high-fidelity numerical results andthen incorporate complementary energy. DCEM-P and DCEM-O further enhance theaccuracy and efficiency of DCEM.</description><author>Yizheng Wang, Jia Sun, Timon Rabczuk, Yinghua Liu</author><pubDate>Mon, 21 Aug 2023 18:00:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01538v5</guid></item><item><title>Diffusion Models for Black-Box Optimization</title><link>http://arxiv.org/abs/2306.07180v2</link><description>The goal of offline black-box optimization (BBO) is to optimize an expensiveblack-box function using a fixed dataset of function evaluations. Prior worksconsider forward approaches that learn surrogates to the black-box function andinverse approaches that directly map function values to corresponding points inthe input domain of the black-box function. These approaches are limited by thequality of the offline dataset and the difficulty in learning one-to-manymappings in high dimensions, respectively. We propose Denoising DiffusionOptimization Models (DDOM), a new inverse approach for offline black-boxoptimization based on diffusion models. Given an offline dataset, DDOM learns aconditional generative model over the domain of the black-box functionconditioned on the function values. We investigate several design choices inDDOM, such as re-weighting the dataset to focus on high function values and theuse of classifier-free guidance at test-time to enable generalization tofunction values that can even exceed the dataset maxima. Empirically, weconduct experiments on the Design-Bench benchmark and show that DDOM achievesresults competitive with state-of-the-art baselines.</description><author>Siddarth Krishnamoorthy, Satvik Mehul Mashkaria, Aditya Grover</author><pubDate>Mon, 21 Aug 2023 17:55:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07180v2</guid></item><item><title>VAPI: Vectorization of Algorithm for Performance Improvement</title><link>http://arxiv.org/abs/2308.01269v2</link><description>This study presents the vectorization of metaheuristic algorithms as thefirst stage of vectorized optimization implementation. Vectorization is atechnique for converting an algorithm, which operates on a single value at atime to one that operates on a collection of values at a time to executerapidly. The vectorization technique also operates by replacing multipleiterations into a single operation, which improves the algorithm's performancein speed and makes the algorithm simpler and easier to be implemented. It isimportant to optimize the algorithm by implementing the vectorizationtechnique, which improves the program's performance, which requires less timeand can run long-running test functions faster, also execute test functionsthat cannot be implemented in non-vectorized algorithms and reduces iterationsand time complexity. Converting to vectorization to operate several values atonce and enhance algorithms' speed and efficiency is a solution for longrunning times and complicated algorithms. The objective of this study is to usethe vectorization technique on one of the metaheuristic algorithms and comparethe results of the vectorized algorithm with the algorithm which isnon-vectorized.</description><author>Mahmood Yashar, Tarik A. Rashid</author><pubDate>Mon, 21 Aug 2023 17:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01269v2</guid></item><item><title>Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs</title><link>http://arxiv.org/abs/2305.00948v2</link><description>The performance of large language models (LLMs) has recently improved to thepoint where the models can perform well on many language tasks. We show herethat for the first time, the models can also generate coherent and valid formalanalyses of linguistic data and illustrate the vast potential of large languagemodels for analyses of their metalinguistic abilities. LLMs are primarilytrained on language data in the form of text; analyzing and evaluating theirmetalinguistic abilities improves our understanding of their generalcapabilities and sheds new light on theoretical models in linguistics. In thispaper, we probe into GPT-4's metalinguistic capabilities by focusing on threesubfields of formal linguistics: syntax, phonology, and semantics. We outline aresearch program for metalinguistic analyses of large language models, proposeexperimental designs, provide general guidelines, discuss limitations, andoffer future directions for this line of research. This line of inquiry alsoexemplifies behavioral interpretability of deep learning, where models'representations are accessed by explicit prompting rather than internalrepresentations.</description><author>Gašper Beguš, Maksymilian Dąbkowski, Ryan Rhodes</author><pubDate>Mon, 21 Aug 2023 17:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00948v2</guid></item><item><title>Majorana Demonstrator Data Release for AI/ML Applications</title><link>http://arxiv.org/abs/2308.10856v1</link><description>The enclosed data release consists of a subset of the calibration data fromthe Majorana Demonstrator experiment. Each Majorana event is accompanied by rawGermanium detector waveforms, pulse shape discrimination cuts, and calibratedfinal energies, all shared in an HDF5 file format along with relevant metadata.This release is specifically designed to support the training and testing ofArtificial Intelligence (AI) and Machine Learning (ML) algorithms upon ourdata. This document is structured as follows. Section I provides an overview ofthe dataset's content and format; Section II outlines the location of thisdataset and the method for accessing it; Section III presents the NPML MachineLearning Challenge associated with this dataset; Section IV contains adisclaimer from the Majorana collaboration regarding the use of this dataset;Appendix A contains technical details of this data release. Please directquestions about the material provided within this release to liaobo77@ucsd.edu(A. Li).</description><author>I. J. Arnquist, F. T. Avignone III, A. S. Barabash, C. J. Barton, K. H. Bhimani, E. Blalock, B. Bos, M. Busch, M. Buuck, T. S. Caldwell, Y. -D. Chan, C. D. Christofferson, P. -H. Chu, M. L. Clark, C. Cuesta, J. A. Detwiler, Yu. Efremenko, H. Ejiri, S. R. Elliott, N. Fuad, G. K. Giovanetti, M. P. Green, J. Gruszko, I. S. Guinn, V. E. Guiseppe, C. R. Haufe, R. Henning, D. Hervas Aguilar, E. W. Hoppe, A. Hostiuc, M. F. Kidd, I. Kim, R. T. Kouzes, T. E. Lannen V, A. Li, J. M. Lopez-Castano, R. D. Martin, R. Massarczyk, S. J. Meijer, S. Mertens, T. K. Oli, L. S. Paudel, W. Pettus, A. W. P. Poon, B. Quenallata, D. C. Radford, A. L. Reine, K. Rielage, N. W. Ruof, D. C. Schaper, S. J. Schleich, D. Tedeschi, R. L. Varner, S. Vasilyev, S. L. Watkins, J. F. Wilkerson, C. Wiseman, W. Xu, C. -H. Yu, B.</author><pubDate>Mon, 21 Aug 2023 17:50:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10856v1</guid></item><item><title>LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles</title><link>http://arxiv.org/abs/2308.10855v1</link><description>With the continuous evolution and refinement of LLMs, they are endowed withimpressive logical reasoning or vertical thinking capabilities. But can theythink out of the box? Do they possess proficient lateral thinking abilities?Following the setup of Lateral Thinking Puzzles, we propose a novel evaluationbenchmark, LatEval, which assesses the model's lateral thinking within aninteractive framework. In our benchmark, we challenge LLMs with 2 aspects: thequality of questions posed by the model and the model's capability to integrateinformation for problem-solving. We find that nearly all LLMs struggle withemploying lateral thinking during interactions. For example, even the mostadvanced model, GPT-4, exhibits the advantage to some extent, yet stillmaintain a noticeable gap when compared to human. This evaluation benchmarkprovides LLMs with a highly challenging and distinctive task that is crucial toan effective AI assistant.</description><author>Shulin Huang, Shirong Ma, Yinghui Li, Mengzuo Huang, Wuhe Zou, Weidong Zhang, Hai-Tao Zheng</author><pubDate>Mon, 21 Aug 2023 17:49:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10855v1</guid></item><item><title>Generative Pretraining for Black-Box Optimization</title><link>http://arxiv.org/abs/2206.10786v4</link><description>Many problems in science and engineering involve optimizing an expensiveblack-box function over a high-dimensional space. For such black-boxoptimization (BBO) problems, we typically assume a small budget for onlinefunction evaluations, but also often have access to a fixed, offline datasetfor pretraining. Prior approaches seek to utilize the offline data toapproximate the function or its inverse but are not sufficiently accurate farfrom the data distribution. We propose BONET, a generative framework forpretraining a novel black-box optimizer using offline datasets. In BONET, wetrain an autoregressive model on fixed-length trajectories derived from anoffline dataset. We design a sampling strategy to synthesize trajectories fromoffline data using a simple heuristic of rolling out monotonic transitions fromlow-fidelity to high-fidelity samples. Empirically, we instantiate BONET usinga causally masked Transformer and evaluate it on Design-Bench, where we rankthe best on average, outperforming state-of-the-art baselines.</description><author>Siddarth Krishnamoorthy, Satvik Mehul Mashkaria, Aditya Grover</author><pubDate>Mon, 21 Aug 2023 17:49:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.10786v4</guid></item><item><title>AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents</title><link>http://arxiv.org/abs/2308.10848v1</link><description>Autonomous agents empowered by Large Language Models (LLMs) have undergonesignificant improvements, enabling them to generalize across a broad spectrumof tasks. However, in real-world scenarios, cooperation among individuals isoften required to enhance the efficiency and effectiveness of taskaccomplishment. Hence, inspired by human group dynamics, we propose amulti-agent framework \framework that can collaboratively and dynamicallyadjust its composition as a greater-than-the-sum-of-its-parts system. Ourexperiments demonstrate that \framework framework can effectively deploymulti-agent groups that outperform a single agent. Furthermore, we delve intothe emergence of social behaviors among individual agents within a group duringcollaborative task accomplishment. In view of these behaviors, we discuss somepossible strategies to leverage positive ones and mitigate negative ones forimproving the collaborative potential of multi-agent groups. Our codes for\framework will soon be released at\url{https://github.com/OpenBMB/AgentVerse}.</description><author>Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou</author><pubDate>Mon, 21 Aug 2023 17:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10848v1</guid></item><item><title>Evaluating quantum generative models via imbalanced data classification benchmarks</title><link>http://arxiv.org/abs/2308.10847v1</link><description>A limited set of tools exist for assessing whether the behavior of quantummachine learning models diverges from conventional models, outside of abstractor theoretical settings. We present a systematic application of explainableartificial intelligence techniques to analyze synthetic data generated from ahybrid quantum-classical neural network adapted from twenty differentreal-world data sets, including solar flares, cardiac arrhythmia, and speechdata. Each of these data sets exhibits varying degrees of complexity and classimbalance. We benchmark the quantum-generated data relative to state-of-the-artmethods for mitigating class imbalance for associated classification tasks. Weleverage this approach to elucidate the qualities of a problem that make itmore or less likely to be amenable to a hybrid quantum-classical generativemodel.</description><author>Graham R. Enos, Matthew J. Reagor, Eric Hulburd</author><pubDate>Mon, 21 Aug 2023 17:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10847v1</guid></item><item><title>Real World Time Series Benchmark Datasets with Distribution Shifts: Global Crude Oil Price and Volatility</title><link>http://arxiv.org/abs/2308.10846v1</link><description>The scarcity of task-labeled time-series benchmarks in the financial domainhinders progress in continual learning. Addressing this deficit would fosterinnovation in this area. Therefore, we present COB, Crude Oil Benchmarkdatasets. COB includes 30 years of asset prices that exhibit significantdistribution shifts and optimally generates corresponding task (i.e., regime)labels based on these distribution shifts for the three most important crudeoils in the world. Our contributions include creating real-world benchmarkdatasets by transforming asset price data into volatility proxies, fittingmodels using expectation-maximization (EM), generating contextual task labelsthat align with real-world events, and providing these labels as well as thegeneral algorithm to the public. We show that the inclusion of these tasklabels universally improves performance on four continual learning algorithms,some state-of-the-art, over multiple forecasting horizons. We hope thesebenchmarks accelerate research in handling distribution shifts in real-worlddata, especially due to the global importance of the assets considered. We'vemade the (1) raw price data, (2) task labels generated by our approach, (3) andcode for our algorithm available at https://oilpricebenchmarks.github.io.</description><author>Pranay Pasula</author><pubDate>Mon, 21 Aug 2023 17:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10846v1</guid></item><item><title>Vision Transformer Pruning Via Matrix Decomposition</title><link>http://arxiv.org/abs/2308.10839v1</link><description>This is a further development of Vision Transformer Pruning via matrixdecomposition. The purpose of the Vision Transformer Pruning is to prune thedimension of the linear projection of the dataset by learning their associatedimportance score in order to reduce the storage, run-time memory, andcomputational demands. In this paper we further reduce dimension and complexityof the linear projection by implementing and comparing several matrixdecomposition methods while preserving the generated important features. We endup selected the Singular Value Decomposition as the method to achieve our goalby comparing the original accuracy scores in the original Github repository andthe accuracy scores of using those matrix decomposition methods, includingSingular Value Decomposition, four versions of QR Decomposition, and LUfactorization.</description><author>Tianyi Sun</author><pubDate>Mon, 21 Aug 2023 17:40:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10839v1</guid></item><item><title>TACOformer:Token-channel compounded Cross Attention for Multimodal Emotion Recognition</title><link>http://arxiv.org/abs/2306.13592v2</link><description>Recently, emotion recognition based on physiological signals has emerged as afield with intensive research. The utilization of multi-modal, multi-channelphysiological signals has significantly improved the performance of emotionrecognition systems, due to their complementarity. However, effectivelyintegrating emotion-related semantic information from different modalities andcapturing inter-modal dependencies remains a challenging issue. Many existingmultimodal fusion methods ignore either token-to-token or channel-to-channelcorrelations of multichannel signals from different modalities, which limitsthe classification capability of the models to some extent. In this paper, wepropose a comprehensive perspective of multimodal fusion that integrateschannel-level and token-level cross-modal interactions. Specifically, weintroduce a unified cross attention module called Token-chAnnel COmpound (TACO)Cross Attention to perform multimodal fusion, which simultaneously modelschannel-level and token-level dependencies between modalities. Additionally, wepropose a 2D position encoding method to preserve information about the spatialdistribution of EEG signal channels, then we use two transformer encoders aheadof the fusion module to capture long-term temporal dependencies from the EEGsignal and the peripheral physiological signal, respectively.Subject-independent experiments on emotional dataset DEAP and Dreamerdemonstrate that the proposed model achieves state-of-the-art performance.</description><author>Xinda Li</author><pubDate>Mon, 21 Aug 2023 17:37:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13592v2</guid></item><item><title>Polynomial-Time Algorithms for Counting and Sampling Markov Equivalent DAGs with Applications</title><link>http://arxiv.org/abs/2205.02654v3</link><description>Counting and sampling directed acyclic graphs from a Markov equivalence classare fundamental tasks in graphical causal analysis. In this paper we show thatthese tasks can be performed in polynomial time, solving a long-standing openproblem in this area. Our algorithms are effective and easily implementable. Aswe show in experiments, these breakthroughs make thought-to-be-infeasiblestrategies in active learning of causal structures and causal effectidentification with regard to a Markov equivalence class practicallyapplicable.</description><author>Marcel Wienöbst, Max Bannach, Maciej Liśkiewicz</author><pubDate>Mon, 21 Aug 2023 17:31:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.02654v3</guid></item><item><title>Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction</title><link>http://arxiv.org/abs/2308.06058v2</link><description>The recently proposed stochastic Polyak stepsize (SPS) and stochasticline-search (SLS) for SGD have shown remarkable effectiveness when trainingover-parameterized models. However, in non-interpolation settings, bothalgorithms only guarantee convergence to a neighborhood of a solution which mayresult in a worse output than the initial guess. While artificially decreasingthe adaptive stepsize has been proposed to address this issue (Orvieto et al.[2022]), this approach results in slower convergence rates for convex andover-parameterized models. In this work, we make two contributions: Firstly, wepropose two new variants of SPS and SLS, called AdaSPS and AdaSLS, whichguarantee convergence in non-interpolation settings and maintain sub-linear andlinear convergence rates for convex and strongly convex functions when trainingover-parameterized models. AdaSLS requires no knowledge of problem-dependentparameters, and AdaSPS requires only a lower bound of the optimal functionvalue as input. Secondly, we equip AdaSPS and AdaSLS with a novel variancereduction technique and obtain algorithms that require$\smash{\widetilde{\mathcal{O}}}(n+1/\epsilon)$ gradient evaluations to achievean $\mathcal{O}(\epsilon)$-suboptimality for convex functions, which improvesupon the slower $\mathcal{O}(1/\epsilon^2)$ rates of AdaSPS and AdaSLS withoutvariance reduction in the non-interpolation regimes. Moreover, our resultmatches the fast rates of AdaSVRG but removes the inner-outer-loop structure,which is easier to implement and analyze. Finally, numerical experiments onsynthetic and real datasets validate our theory and demonstrate theeffectiveness and robustness of our algorithms.</description><author>Xiaowen Jiang, Sebastian U. Stich</author><pubDate>Mon, 21 Aug 2023 17:28:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06058v2</guid></item><item><title>EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition</title><link>http://arxiv.org/abs/2308.10832v1</link><description>Visual Place Recognition is a task that aims to predict the place of an image(called query) based solely on its visual features. This is typically donethrough image retrieval, where the query is matched to the most similar imagesfrom a large database of geotagged photos, using learned global descriptors. Amajor challenge in this task is recognizing places seen from differentviewpoints. To overcome this limitation, we propose a new method, calledEigenPlaces, to train our neural network on images from different point ofviews, which embeds viewpoint robustness into the learned global descriptors.The underlying idea is to cluster the training data so as to explicitly presentthe model with different views of the same points of interest. The selection ofthis points of interest is done without the need for extra supervision. We thenpresent experiments on the most comprehensive set of datasets in literature,finding that EigenPlaces is able to outperform previous state of the art on themajority of datasets, while requiring 60\% less GPU memory for training andusing 50\% smaller descriptors. The code and trained models for EigenPlaces areavailable at {\small{\url{https://github.com/gmberton/EigenPlaces}}}, whileresults with any other baseline can be computed with the codebase at{\small{\url{https://github.com/gmberton/auto_VPR}}}.</description><author>Gabriele Berton, Gabriele Trivigno, Barbara Caputo, Carlo Masone</author><pubDate>Mon, 21 Aug 2023 17:27:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10832v1</guid></item><item><title>Rethinking Data Distillation: Do Not Overlook Calibration</title><link>http://arxiv.org/abs/2307.12463v2</link><description>Neural networks trained on distilled data often produce over-confident outputand require correction by calibration methods. Existing calibration methodssuch as temperature scaling and mixup work well for networks trained onoriginal large-scale data. However, we find that these methods fail tocalibrate networks trained on data distilled from large source datasets. Inthis paper, we show that distilled data lead to networks that are notcalibratable due to (i) a more concentrated distribution of the maximum logitsand (ii) the loss of information that is semantically meaningful but unrelatedto classification tasks. To address this problem, we propose Masked TemperatureScaling (MTS) and Masked Distillation Training (MDT) which mitigate thelimitations of distilled data and achieve better calibration results whilemaintaining the efficiency of dataset distillation.</description><author>Dongyao Zhu, Bowen Lei, Jie Zhang, Yanbo Fang, Ruqi Zhang, Yiqun Xie, Dongkuan Xu</author><pubDate>Mon, 21 Aug 2023 17:16:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12463v2</guid></item><item><title>Neural Networks Optimizations Against Concept and Data Drift in Malware Detection</title><link>http://arxiv.org/abs/2308.10821v1</link><description>Despite the promising results of machine learning models in malwaredetection, they face the problem of concept drift due to malware constantevolution. This leads to a decline in performance over time, as the datadistribution of the new files differs from the training one, requiring regularmodel update. In this work, we propose a model-agnostic protocol to improve abaseline neural network to handle with the drift problem. We show theimportance of feature reduction and training with the most recent validationset possible, and propose a loss function named Drift-Resilient BinaryCross-Entropy, an improvement to the classical Binary Cross-Entropy moreeffective against drift. We train our model on the EMBER dataset (2018) andevaluate it on a dataset of recent malicious files, collected between 2020 and2023. Our improved model shows promising results, detecting 15.2% more malwarethan a baseline model.</description><author>William Maillet, Benjamin Marais</author><pubDate>Mon, 21 Aug 2023 17:13:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10821v1</guid></item><item><title>Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction</title><link>http://arxiv.org/abs/2308.10820v1</link><description>Hyperspectral Image (HSI) reconstruction has made gratifying progress withthe deep unfolding framework by formulating the problem into a data module anda prior module. Nevertheless, existing methods still face the problem ofinsufficient matching with HSI data. The issues lie in three aspects: 1) fixedgradient descent step in the data module while the degradation of HSI isagnostic in the pixel-level. 2) inadequate prior module for 3D HSI cube. 3)stage interaction ignoring the differences in features at different stages. Toaddress these issues, in this work, we propose a Pixel Adaptive Deep UnfoldingTransformer (PADUT) for HSI reconstruction. In the data module, a pixeladaptive descent step is employed to focus on pixel-level agnostic degradation.In the prior module, we introduce the Non-local Spectral Transformer (NST) toemphasize the 3D characteristics of HSI for recovering. Moreover, inspired bythe diverse expression of features in different stages and depths, the stageinteraction is improved by the Fast Fourier Transform (FFT). Experimentalresults on both simulated and real scenes exhibit the superior performance ofour method compared to state-of-the-art HSI reconstruction methods. The code isreleased at: https://github.com/MyuLi/PADUT.</description><author>Miaoyu Li, Ying Fu, Ji Liu, Yulun Zhang</author><pubDate>Mon, 21 Aug 2023 17:12:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10820v1</guid></item><item><title>What's the Problem, Linda? The Conjunction Fallacy as a Fairness Problem</title><link>http://arxiv.org/abs/2305.09535v2</link><description>The field of Artificial Intelligence (AI) is focusing on creating automateddecision-making (ADM) systems that operate as close as possible to human-likeintelligence. This effort has pushed AI researchers into exploring cognitivefields like psychology. The work of Daniel Kahneman and the late Amos Tverskyon biased human decision-making, including the study of the conjunctionfallacy, has experienced a second revival because of this. Under theconjunction fallacy a human decision-maker will go against basic probabilitylaws and rank as more likely a conjunction over one of its parts. It has beenproven overtime through a set of experiments with the Linda Problem being themost famous one. Although this interdisciplinary effort is welcomed, we fearthat AI researchers ignore the driving force behind the conjunction fallacy ascaptured by the Linda Problem: the fact that Linda must be stereotypicallydescribed as a woman. In this paper we revisit the Linda Problem and formulateit as a fairness problem. In doing so we introduce perception as a parameter ofinterest through the structural causal perception framework. Using anillustrative decision-making example, we showcase the proposed conceptualframework and its potential impact for developing fair ADM systems.</description><author>Jose Alvarez Colmenares</author><pubDate>Mon, 21 Aug 2023 17:04:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09535v2</guid></item><item><title>Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers</title><link>http://arxiv.org/abs/2308.10814v1</link><description>Quantization scale and bit-width are the most important parameters whenconsidering how to quantize a neural network. Prior work focuses on optimizingquantization scales in a global manner through gradient methods (gradientdescent \&amp; Hessian analysis). Yet, when applying perturbations to quantizationscales, we observe a very jagged, highly non-smooth test loss landscape. Infact, small perturbations in quantization scale can greatly affect accuracy,yielding a $0.5-0.8\%$ accuracy boost in 4-bit quantized vision transformers(ViTs). In this regime, gradient methods break down, since they cannot reliablyreach local minima. In our work, dubbed Evol-Q, we use evolutionary search toeffectively traverse the non-smooth landscape. Additionally, we propose usingan infoNCE loss, which not only helps combat overfitting on the smallcalibration dataset ($1,000$ images) but also makes traversing such a highlynon-smooth surface easier. Evol-Q improves the top-1 accuracy of a fullyquantized ViT-Base by $10.30\%$, $0.78\%$, and $0.15\%$ for $3$-bit, $4$-bit,and $8$-bit weight quantization levels. Extensive experiments on a variety ofCNN and ViT architectures further demonstrate its robustness in extremequantization scenarios. Our code is available athttps://github.com/enyac-group/evol-q</description><author>Natalia Frumkin, Dibakar Gope, Diana Marculescu</author><pubDate>Mon, 21 Aug 2023 17:03:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10814v1</guid></item><item><title>Improving Continuous Sign Language Recognition with Cross-Lingual Signs</title><link>http://arxiv.org/abs/2308.10809v1</link><description>This work dedicates to continuous sign language recognition (CSLR), which isa weakly supervised task dealing with the recognition of continuous signs fromvideos, without any prior knowledge about the temporal boundaries betweenconsecutive signs. Data scarcity heavily impedes the progress of CSLR. Existingapproaches typically train CSLR models on a monolingual corpus, which is ordersof magnitude smaller than that of speech recognition. In this work, we explorethe feasibility of utilizing multilingual sign language corpora to facilitatemonolingual CSLR. Our work is built upon the observation of cross-lingualsigns, which originate from different sign languages but have similar visualsignals (e.g., hand shape and motion). The underlying idea of our approach isto identify the cross-lingual signs in one sign language and properly leveragethem as auxiliary training data to improve the recognition capability ofanother. To achieve the goal, we first build two sign language dictionariescontaining isolated signs that appear in two datasets. Then we identify thesign-to-sign mappings between two sign languages via a well-optimized isolatedsign language recognition model. At last, we train a CSLR model on thecombination of the target data with original labels and the auxiliary data withmapped labels. Experimentally, our approach achieves state-of-the-artperformance on two widely-used CSLR datasets: Phoenix-2014 and Phoenix-2014T.</description><author>Fangyun Wei, Yutong Chen</author><pubDate>Mon, 21 Aug 2023 16:58:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10809v1</guid></item><item><title>Graph Neural Bandits</title><link>http://arxiv.org/abs/2308.10808v1</link><description>Contextual bandits algorithms aim to choose the optimal arm with the highestreward out of a set of candidates based on the contextual information. Variousbandit algorithms have been applied to real-world applications due to theirability of tackling the exploitation-exploration dilemma. Motivated by onlinerecommendation scenarios, in this paper, we propose a framework named GraphNeural Bandits (GNB) to leverage the collaborative nature among users empoweredby graph neural networks (GNNs). Instead of estimating rigid user clusters asin existing works, we model the "fine-grained" collaborative effects throughestimated user graphs in terms of exploitation and exploration respectively.Then, to refine the recommendation strategy, we utilize separate GNN-basedmodels on estimated user graphs for exploitation and adaptive exploration.Theoretical analysis and experimental results on multiple real data sets incomparison with state-of-the-art baselines are provided to demonstrate theeffectiveness of our proposed framework.</description><author>Yunzhe Qi, Yikun Ban, Jingrui He</author><pubDate>Mon, 21 Aug 2023 16:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10808v1</guid></item><item><title>DynED: Dynamic Ensemble Diversification in Data Stream Classification</title><link>http://arxiv.org/abs/2308.10807v1</link><description>Ensemble methods are commonly used in classification due to their remarkableperformance. Achieving high accuracy in a data stream environment is achallenging task considering disruptive changes in the data distribution, alsoknown as concept drift. A greater diversity of ensemble components is known toenhance prediction accuracy in such settings. Despite the diversity ofcomponents within an ensemble, not all contribute as expected to its overallperformance. This necessitates a method for selecting components that exhibithigh performance and diversity. We present a novel ensemble construction andmaintenance approach based on MMR (Maximal Marginal Relevance) that dynamicallycombines the diversity and prediction accuracy of components during the processof structuring an ensemble. The experimental results on both four real and 11synthetic datasets demonstrate that the proposed approach (DynED) provides ahigher average mean accuracy compared to the five state-of-the-art baselines.</description><author>Soheil Abadifard, Sepehr Bakhshi, Sanaz Gheibuni, Fazli Can</author><pubDate>Mon, 21 Aug 2023 16:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10807v1</guid></item><item><title>Differentiable Frank-Wolfe Optimization Layer</title><link>http://arxiv.org/abs/2308.10806v1</link><description>Differentiable optimization has received a significant amount of attentiondue to its foundational role in the domain of machine learning based on neuralnetworks. The existing methods leverages the optimality conditions and implicitfunction theorem to obtain the Jacobian matrix of the output, which increasesthe computational cost and limits the application of differentiableoptimization. In addition, some non-differentiable constraints lead to morechallenges when using prior differentiable optimization layers. This paperproposes a differentiable layer, named Differentiable Frank-Wolfe Layer(DFWLayer), by rolling out the Frank-Wolfe method, a well-known optimizationalgorithm which can solve constrained optimization problems without projectionsand Hessian matrix computations, thus leading to a efficient way of dealingwith large-scale problems. Theoretically, we establish a bound on thesuboptimality gap of the DFWLayer in the context of l1-norm constraints.Experimental assessments demonstrate that the DFWLayer not only attainscompetitive accuracy in solutions and gradients but also consistently adheresto constraints. Moreover, it surpasses the baselines in both forward andbackward computational speeds.</description><author>Zixuan Liu, Liu Liu, Xueqian Wang, Peilin Zhao</author><pubDate>Mon, 21 Aug 2023 16:53:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10806v1</guid></item><item><title>Artificial intelligence is ineffective and potentially harmful for fact checking</title><link>http://arxiv.org/abs/2308.10800v1</link><description>Fact checking can be an effective strategy against misinformation, but itsimplementation at scale is impeded by the overwhelming volume of informationonline. Recent artificial intelligence (AI) language models have shownimpressive ability in fact-checking tasks, but how humans interact withfact-checking information provided by these models is unclear. Here weinvestigate the impact of fact checks generated by a popular AI model on beliefin, and sharing intent of, political news in a preregistered randomized controlexperiment. Although the AI performs reasonably well in debunking falseheadlines, we find that it does not significantly affect participants' abilityto discern headline accuracy or share accurate news. However, the AIfact-checker is harmful in specific cases: it decreases beliefs in trueheadlines that it mislabels as false and increases beliefs for false headlinesthat it is unsure about. On the positive side, the AI increases sharing intentsfor correctly labeled true headlines. When participants are given the option toview AI fact checks and choose to do so, they are significantly more likely toshare both true and false news but only more likely to believe false news. Ourfindings highlight an important source of potential harm stemming from AIapplications and underscore the critical need for policies to prevent ormitigate such unintended consequences.</description><author>Matthew R. DeVerna, Harry Yaojun Yan, Kai-Cheng Yang, Filippo Menczer</author><pubDate>Mon, 21 Aug 2023 16:47:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10800v1</guid></item><item><title>Stabilizing Unsupervised Environment Design with a Learned Adversary</title><link>http://arxiv.org/abs/2308.10797v1</link><description>A key challenge in training generally-capable agents is the design oftraining tasks that facilitate broad generalization and robustness toenvironment variations. This challenge motivates the problem setting ofUnsupervised Environment Design (UED), whereby a student agent trains on anadaptive distribution of tasks proposed by a teacher agent. A pioneeringapproach for UED is PAIRED, which uses reinforcement learning (RL) to train ateacher policy to design tasks from scratch, making it possible to directlygenerate tasks that are adapted to the agent's current capabilities. Despiteits strong theoretical backing, PAIRED suffers from a variety of challengesthat hinder its practical performance. Thus, state-of-the-art methods currentlyrely on curation and mutation rather than generation of new tasks. In thiswork, we investigate several key shortcomings of PAIRED and propose solutionsfor each shortcoming. As a result, we make it possible for PAIRED to match orexceed state-of-the-art methods, producing robust agents in several establishedchallenging procedurally-generated environments, including a partially-observedmaze navigation task and a continuous-control car racing environment. Webelieve this work motivates a renewed emphasis on UED methods based on learnedmodels that directly generate challenging environments, potentially unlockingmore open-ended RL training and, as a result, more general agents.</description><author>Ishita Mediratta, Minqi Jiang, Jack Parker-Holder, Michael Dennis, Eugene Vinitsky, Tim Rocktäschel</author><pubDate>Mon, 21 Aug 2023 16:42:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10797v1</guid></item><item><title>MGMAE: Motion Guided Masking for Video Masked Autoencoding</title><link>http://arxiv.org/abs/2308.10794v1</link><description>Masked autoencoding has shown excellent performance on self-supervised videorepresentation learning. Temporal redundancy has led to a high masking ratioand customized masking strategy in VideoMAE. In this paper, we aim to furtherimprove the performance of video masked autoencoding by introducing a motionguided masking strategy. Our key insight is that motion is a general and uniqueprior in video, which should be taken into account during masked pre-training.Our motion guided masking explicitly incorporates motion information to buildtemporal consistent masking volume. Based on this masking volume, we can trackthe unmasked tokens in time and sample a set of temporal consistent cubes fromvideos. These temporal aligned unmasked tokens will further relieve theinformation leakage issue in time and encourage the MGMAE to learn more usefulstructure information. We implement our MGMAE with an online efficient opticalflow estimator and backward masking map warping strategy. We performexperiments on the datasets of Something-Something V2 and Kinetics-400,demonstrating the superior performance of our MGMAE to the original VideoMAE.In addition, we provide the visualization analysis to illustrate that our MGMAEcan sample temporal consistent cubes in a motion-adaptive manner for moreeffective video pre-training.</description><author>Bingkun Huang, Zhiyu Zhao, Guozhen Zhang, Yu Qiao, Limin Wang</author><pubDate>Mon, 21 Aug 2023 16:39:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10794v1</guid></item><item><title>Simple Cycle Reservoirs are Universal</title><link>http://arxiv.org/abs/2308.10793v1</link><description>Reservoir computation models form a subclass of recurrent neural networkswith fixed non-trainable input and dynamic coupling weights. Only the staticreadout from the state space (reservoir) is trainable, thus avoiding the knownproblems with propagation of gradient information backwards through time.Reservoir models have been successfully applied in a variety of tasks and wereshown to be universal approximators of time-invariant fading memory dynamicfilters under various settings. Simple cycle reservoirs (SCR) have beensuggested as severely restricted reservoir architecture, with equal weight ringconnectivity of the reservoir units and input-to-reservoir weights of binarynature with the same absolute value. Such architectures are well suited forhardware implementations without performance degradation in many practicaltasks. In this contribution, we rigorously study the expressive power of SCR inthe complex domain and show that they are capable of universal approximation ofany unrestricted linear reservoir system (with continuous readout) and henceany time-invariant fading memory filter over uniformly bounded input streams.</description><author>Boyu Li, Robert Simon Fong, Peter Tiňo</author><pubDate>Mon, 21 Aug 2023 16:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10793v1</guid></item><item><title>Instruction Tuning for Large Language Models: A Survey</title><link>http://arxiv.org/abs/2308.10792v1</link><description>This paper surveys research works in the quickly advancing field ofinstruction tuning (IT), a crucial technique to enhance the capabilities andcontrollability of large language models (LLMs). Instruction tuning refers tothe process of further training LLMs on a dataset consisting of\textsc{(instruction, output)} pairs in a supervised fashion, which bridges thegap between the next-word prediction objective of LLMs and the users' objectiveof having LLMs adhere to human instructions. In this work, we make a systematicreview of the literature, including the general methodology of IT, theconstruction of IT datasets, the training of IT models, and applications todifferent modalities, domains and applications, along with an analysis onaspects that influence the outcome of IT (e.g., generation of instructionoutputs, size of the instruction dataset, etc). We also review the potentialpitfalls of IT along with criticism against it, along with efforts pointing outcurrent deficiencies of existing strategies and suggest some avenues forfruitful research.</description><author>Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang</author><pubDate>Mon, 21 Aug 2023 16:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10792v1</guid></item><item><title>A Block-Ring connected Topology of Parameterized Quantum Circuits</title><link>http://arxiv.org/abs/2308.10791v1</link><description>It is essential to select efficient topology of parameterized quantumcircuits (PQCs) in variational quantum algorithms (VQAs). However, there areproblems in current circuits, i.e. optimization difficulties caused by too manyparameters or performance is hard to guarantee. How to reduce the number ofparameters (number of single-qubit rotation gates and 2-qubit gates) in PQCswithout reducing the performance has become a new challenge. To solve thisproblem, we propose a novel topology, called Block-Ring (BR) topology, toconstruct the PQCs. This topology allocate all qubits to several blocks,all-to-all mode is adopt inside each block and ring mode is applied to connectdifferent blocks. Compared with the pure all-to-all topology circuits which ownthe best power, BR topology have similar performance and the number ofparameters and 2-qubit gate reduced from 0(n^2) to 0(mn) , m is ahyperparameter set by ourselves. Besides, we compared BR topology with othertopology circuits in terms of expressibility and entangling capability.Considering the effects of different 2-qubit gates on circuits, we also make adistinction between controlled X-rotation gates and controlled Z-rotationgates. Finally, the 1- and 2-layer configurations of PQCs are taken intoconsideration as well, which shows the BR's performance improvement in thecondition of multilayer circuits.</description><author>Wenjie Liu, Qingshan Wu</author><pubDate>Mon, 21 Aug 2023 16:35:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10791v1</guid></item><item><title>Extraction of Text from Optic Nerve Optical Coherence Tomography Reports</title><link>http://arxiv.org/abs/2308.10790v1</link><description>Purpose: The purpose of this study was to develop and evaluate rule-basedalgorithms to enhance the extraction of text data, including retinal nervefiber layer (RNFL) values and other ganglion cell count (GCC) data, from ZeissCirrus optical coherence tomography (OCT) scan reports. Methods: DICOM filesthat contained encapsulated PDF reports with RNFL or Ganglion Cell in theirdocument titles were identified from a clinical imaging repository at a singleacademic ophthalmic center. PDF reports were then converted into image filesand processed using the PaddleOCR Python package for optical characterrecognition. Rule-based algorithms were designed and iteratively optimized forimproved performance in extracting RNFL and GCC data. Evaluation of thealgorithms was conducted through manual review of a set of RNFL and GCCreports. Results: The developed algorithms demonstrated high precision inextracting data from both RNFL and GCC scans. Precision was slightly better forthe right eye in RNFL extraction (OD: 0.9803 vs. OS: 0.9046), and for the lefteye in GCC extraction (OD: 0.9567 vs. OS: 0.9677). Some values presented morechallenges in extraction, particularly clock hours 5 and 6 for RNFL thickness,and signal strength for GCC. Conclusions: A customized optical characterrecognition algorithm can identify numeric results from optical coherence scanreports with high precision. Automated processing of PDF reports can greatlyreduce the time to extract OCT results on a large scale.</description><author>Iyad Majid, Youchen Victor Zhang, Robert Chang, Sophia Y. Wang</author><pubDate>Mon, 21 Aug 2023 16:34:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10790v1</guid></item><item><title>WeditGAN: Few-shot Image Generation via Latent Space Relocation</title><link>http://arxiv.org/abs/2305.06671v2</link><description>In few-shot image generation, directly training GAN models on just a handfulof images faces the risk of overfitting. A popular solution is to transfer themodels pretrained on large source domains to small target ones. In this work,we introduce WeditGAN, which realizes model transfer by editing theintermediate latent codes $w$ in StyleGANs with learned constant offsets($\Delta w$), discovering and constructing target latent spaces via simplyrelocating the distribution of source latent spaces. The established one-to-onemapping between latent spaces can naturally prevents mode collapse andoverfitting. Besides, we also propose variants of WeditGAN to further enhancethe relocation process by regularizing the direction or finetuning theintensity of $\Delta w$. Experiments on a collection of widely usedsource/target datasets manifest the capability of WeditGAN in generatingrealistic and diverse images, which is simple yet highly effective in theresearch area of few-shot image generation.</description><author>Yuxuan Duan, Li Niu, Yan Hong, Liqing Zhang</author><pubDate>Mon, 21 Aug 2023 16:29:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06671v2</guid></item><item><title>One-Vote Veto: Semi-Supervised Learning for Low-Shot Glaucoma Diagnosis</title><link>http://arxiv.org/abs/2012.04841v4</link><description>Convolutional neural networks (CNNs) are a promising technique for automatedglaucoma diagnosis from images of the fundus, and these images are routinelyacquired as part of an ophthalmic exam. Nevertheless, CNNs typically require alarge amount of well-labeled data for training, which may not be available inmany biomedical image classification applications, especially when diseases arerare and where labeling by experts is costly. This article makes twocontributions to address this issue: (1) It extends the conventional Siamesenetwork and introduces a training method for low-shot learning when labeleddata are limited and imbalanced, and (2) it introduces a novel semi-supervisedlearning strategy that uses additional unlabeled training data to achievegreater accuracy. Our proposed multi-task Siamese network (MTSN) can employ anybackbone CNN, and we demonstrate with four backbone CNNs that its accuracy withlimited training data approaches the accuracy of backbone CNNs trained with adataset that is 50 times larger. We also introduce One-Vote Veto (OVV)self-training, a semi-supervised learning strategy that is designedspecifically for MTSNs. By taking both self-predictions and contrastivepredictions of the unlabeled training data into account, OVV self-trainingprovides additional pseudo labels for fine-tuning a pre-trained MTSN. Using alarge (imbalanced) dataset with 66,715 fundus photographs acquired over 15years, extensive experimental results demonstrate the effectiveness of low-shotlearning with MTSN and semi-supervised learning with OVV self-training. Threeadditional, smaller clinical datasets of fundus images acquired under differentconditions (cameras, instruments, locations, populations) are used todemonstrate the generalizability of the proposed methods.</description><author>Rui Fan, Christopher Bowd, Nicole Brye, Mark Christopher, Robert N. Weinreb, David Kriegman, Linda M. Zangwill</author><pubDate>Mon, 21 Aug 2023 16:23:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.04841v4</guid></item><item><title>MMD Aggregated Two-Sample Test</title><link>http://arxiv.org/abs/2110.15073v4</link><description>We propose two novel nonparametric two-sample kernel tests based on theMaximum Mean Discrepancy (MMD). First, for a fixed kernel, we construct an MMDtest using either permutations or a wild bootstrap, two popular numericalprocedures to determine the test threshold. We prove that this test controlsthe probability of type I error non-asymptotically. Hence, it can be usedreliably even in settings with small sample sizes as it remainswell-calibrated, which differs from previous MMD tests which only guaranteecorrect test level asymptotically. When the difference in densities lies in aSobolev ball, we prove minimax optimality of our MMD test with a specifickernel depending on the smoothness parameter of the Sobolev ball. In practice,this parameter is unknown and, hence, the optimal MMD test with this particularkernel cannot be used. To overcome this issue, we construct an aggregated test,called MMDAgg, which is adaptive to the smoothness parameter. The test power ismaximised over the collection of kernels used, without requiring held-out datafor kernel selection (which results in a loss of test power), or arbitrarykernel choices such as the median heuristic. We prove that MMDAgg stillcontrols the level non-asymptotically, and achieves the minimax rate overSobolev balls, up to an iterated logarithmic term. Our guarantees are notrestricted to a specific type of kernel, but hold for any product ofone-dimensional translation invariant characteristic kernels. We provide auser-friendly parameter-free implementation of MMDAgg using an adaptivecollection of bandwidths. We demonstrate that MMDAgg significantly outperformsalternative state-of-the-art MMD-based two-sample tests on synthetic datasatisfying the Sobolev smoothness assumption, and that, on real-world imagedata, MMDAgg closely matches the power of tests leveraging the use of modelssuch as neural networks.</description><author>Antonin Schrab, Ilmun Kim, Mélisande Albert, Béatrice Laurent, Benjamin Guedj, Arthur Gretton</author><pubDate>Mon, 21 Aug 2023 16:20:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.15073v4</guid></item><item><title>Dense Error Map Estimation for MRI-Ultrasound Registration in Brain Tumor Surgery Using Swin UNETR</title><link>http://arxiv.org/abs/2308.10784v1</link><description>Early surgical treatment of brain tumors is crucial in reducing patientmortality rates. However, brain tissue deformation (called brain shift) occursduring the surgery, rendering pre-operative images invalid. As a cost-effectiveand portable tool, intra-operative ultrasound (iUS) can track brain shift, andaccurate MRI-iUS registration techniques can update pre-surgical plans andfacilitate the interpretation of iUS. This can boost surgical safety andoutcomes by maximizing tumor removal while avoiding eloquent regions. However,manual assessment of MRI-iUS registration results in real-time is difficult andprone to errors due to the 3D nature of the data. Automatic algorithms that canquantify the quality of inter-modal medical image registration outcomes can behighly beneficial. Therefore, we propose a novel deep-learning (DL) basedframework with the Swin UNETR to automatically assess 3D-patch-wise dense errormaps for MRI-iUS registration in iUS-guided brain tumor resection and show itsperformance with real clinical data for the first time.</description><author>Soorena Salari, Amirhossein Rasoulian, Hassan Rivaz, Yiming Xiao</author><pubDate>Mon, 21 Aug 2023 16:19:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10784v1</guid></item><item><title>Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis</title><link>http://arxiv.org/abs/2308.10783v1</link><description>The rapid expansion of the digital world has propelled sentiment analysisinto a critical tool across diverse sectors such as marketing, politics,customer service, and healthcare. While there have been significantadvancements in sentiment analysis for widely spoken languages, low-resourcelanguages, such as Bangla, remain largely under-researched due to resourceconstraints. Furthermore, the recent unprecedented performance of LargeLanguage Models (LLMs) in various applications highlights the need to evaluatethem in the context of low-resource languages. In this study, we present asizeable manually annotated dataset encompassing 33,605 Bangla news tweets andFacebook comments. We also investigate zero- and few-shot in-context learningwith several language models, including Flan-T5, GPT-4, and Bloomz, offering acomparative analysis against fine-tuned models. Our findings suggest thatmonolingual transformer-based models consistently outperform other models, evenin zero and few-shot scenarios. To foster continued exploration, we intend tomake this dataset and our research tools publicly available to the broaderresearch community. In the spirit of further research, we plan to make thisdataset and our experimental resources publicly accessible to the widerresearch community.</description><author>Md. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj Alam, Anika Anjum, Avijit Sarker, Sheak Rashed Haider Noori</author><pubDate>Mon, 21 Aug 2023 16:19:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10783v1</guid></item><item><title>Sparse Linear Concept Discovery Models</title><link>http://arxiv.org/abs/2308.10782v1</link><description>The recent mass adoption of DNNs, even in safety-critical scenarios, hasshifted the focus of the research community towards the creation of inherentlyintrepretable models. Concept Bottleneck Models (CBMs) constitute a popularapproach where hidden layers are tied to human understandable concepts allowingfor investigation and correction of the network's decisions. However, CBMsusually suffer from: (i) performance degradation and (ii) lowerinterpretability than intended due to the sheer amount of concepts contributingto each decision. In this work, we propose a simple yet highly intuitiveinterpretable framework based on Contrastive Language Image models and a singlesparse linear layer. In stark contrast to related approaches, the sparsity inour framework is achieved via principled Bayesian arguments by inferringconcept presence via a data-driven Bernoulli distribution. As we experimentallyshow, our framework not only outperforms recent CBM approaches accuracy-wise,but it also yields high per example concept sparsity, facilitating theindividual investigation of the emerging concepts.</description><author>Konstantinos P. Panousis, Dino Ienco, Diego Marcos</author><pubDate>Mon, 21 Aug 2023 16:16:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10782v1</guid></item><item><title>Mixed-Integer Projections for Automated Data Correction of EMRs Improve Predictions of Sepsis among Hospitalized Patients</title><link>http://arxiv.org/abs/2308.10781v1</link><description>Machine learning (ML) models are increasingly pivotal in automating clinicaldecisions. Yet, a glaring oversight in prior research has been the lack ofproper processing of Electronic Medical Record (EMR) data in the clinicalcontext for errors and outliers. Addressing this oversight, we introduce aninnovative projections-based method that seamlessly integrates clinicalexpertise as domain constraints, generating important meta-data that can beused in ML workflows. In particular, by using high-dimensional mixed-integerprograms that capture physiological and biological constraints on patientvitals and lab values, we can harness the power of mathematical "projections"for the EMR data to correct patient data. Consequently, we measure the distanceof corrected data from the constraints defining a healthy range of patientdata, resulting in a unique predictive metric we term as "trust-scores". Thesescores provide insight into the patient's health status and significantly boostthe performance of ML classifiers in real-life clinical settings. We validatethe impact of our framework in the context of early detection of sepsis usingML. We show an AUROC of 0.865 and a precision of 0.922, that surpassesconventional ML models without such projections.</description><author>Mehak Arora, Hassan Mortagy, Nathan Dwarshius, Swati Gupta, Andre L. Holder, Rishikesan Kamaleswaran</author><pubDate>Mon, 21 Aug 2023 16:14:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10781v1</guid></item><item><title>Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based Link Prediction on Continuous-Time Dynamic Graphs</title><link>http://arxiv.org/abs/2308.10779v1</link><description>Real-world graphs are dynamic, constantly evolving with new interactions,such as financial transactions in financial networks. Temporal Graph NeuralNetworks (TGNNs) have been developed to effectively capture the evolvingpatterns in dynamic graphs. While these models have demonstrated theirsuperiority, being widely adopted in various important fields, theirvulnerabilities against adversarial attacks remain largely unexplored. In thispaper, we propose T-SPEAR, a simple and effective adversarial attack method forlink prediction on continuous-time dynamic graphs, focusing on investigatingthe vulnerabilities of TGNNs. Specifically, before the training procedure of avictim model, which is a TGNN for link prediction, we inject edge perturbationsto the data that are unnoticeable in terms of the four constraints we propose,and yet effective enough to cause malfunction of the victim model. Moreover, wepropose a robust training approach T-SHIELD to mitigate the impact ofadversarial attacks. By using edge filtering and enforcing temporal smoothnessto node embeddings, we enhance the robustness of the victim model. Ourexperimental study shows that T-SPEAR significantly degrades the victim model'sperformance on link prediction tasks, and even more, our attacks aretransferable to other TGNNs, which differ from the victim model assumed by theattacker. Moreover, we demonstrate that T-SHIELD effectively filters outadversarial edges and exhibits robustness against adversarial attacks,surpassing the link prediction performance of the naive TGNN by up to 11.2%under T-SPEAR.</description><author>Dongjin Lee, Juho Lee, Kijung Shin</author><pubDate>Mon, 21 Aug 2023 16:09:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10779v1</guid></item><item><title>SatlasPretrain: A Large-Scale Dataset for Remote Sensing Image Understanding</title><link>http://arxiv.org/abs/2211.15660v3</link><description>Remote sensing images are useful for a wide variety of planet monitoringapplications, from tracking deforestation to tackling illegal fishing. TheEarth is extremely diverse -- the amount of potential tasks in remote sensingimages is massive, and the sizes of features range from several kilometers tojust tens of centimeters. However, creating generalizable computer visionmethods is a challenge in part due to the lack of a large-scale dataset thatcaptures these diverse features for many tasks. In this paper, we presentSatlasPretrain, a remote sensing dataset that is large in both breadth andscale, combining Sentinel-2 and NAIP images with 302M labels under 137categories and seven label types. We evaluate eight baselines and a proposedmethod on SatlasPretrain, and find that there is substantial room forimprovement in addressing research challenges specific to remote sensing,including processing image time series that consist of images from verydifferent types of sensors, and taking advantage of long-range spatial context.Moreover, we find that pre-training on SatlasPretrain substantially improvesperformance on downstream tasks, increasing average accuracy by 18% overImageNet and 6% over the next best baseline. The dataset, pre-trained modelweights, and code are available at https://satlas-pretrain.allen.ai/.</description><author>Favyen Bastani, Piper Wolters, Ritwik Gupta, Joe Ferdinando, Aniruddha Kembhavi</author><pubDate>Mon, 21 Aug 2023 16:09:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15660v3</guid></item><item><title>A Modular and Adaptive System for Business Email Compromise Detection</title><link>http://arxiv.org/abs/2308.10776v1</link><description>The growing sophistication of Business Email Compromise (BEC) and spearphishing attacks poses significant challenges to organizations worldwide. Thetechniques featured in traditional spam and phishing detection are insufficientdue to the tailored nature of modern BEC attacks as they often blend in withthe regular benign traffic. Recent advances in machine learning, particularlyin Natural Language Understanding (NLU), offer a promising avenue for combatingsuch attacks but in a practical system, due to limitations such as dataavailability, operational costs, verdict explainability requirements or a needto robustly evolve the system, it is essential to combine multiple approachestogether. We present CAPE, a comprehensive and efficient system for BECdetection that has been proven in a production environment for a period of overtwo years. Rather than being a single model, CAPE is a system that combinesindependent ML models and algorithms detecting BEC-related behaviors acrossvarious email modalities such as text, images, metadata and the email'scommunication context. This decomposition makes CAPE's verdicts naturallyexplainable. In the paper, we describe the design principles and constraintsbehind its architecture, as well as the challenges of model design, evaluationand adapting the system continuously through a Bayesian approach that combineslimited data with domain knowledge. Furthermore, we elaborate on severalspecific behavioral detectors, such as those based on Transformer neuralarchitectures.</description><author>Jan Brabec, Filip Šrajer, Radek Starosta, Tomáš Sixta, Marc Dupont, Miloš Lenoch, Jiří Menšík, Florian Becker, Jakub Boros, Tomáš Pop, Pavel Novák</author><pubDate>Mon, 21 Aug 2023 16:06:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10776v1</guid></item><item><title>Adversarial Attacks and Defenses for Semantic Communication in Vehicular Metaverses</title><link>http://arxiv.org/abs/2306.03528v2</link><description>For vehicular metaverses, one of the ultimate user-centric goals is tooptimize the immersive experience and Quality of Service (QoS) for users onboard. Semantic Communication (SemCom) has been introduced as a revolutionaryparadigm that significantly eases communication resource pressure for vehicularmetaverse applications to achieve this goal. SemCom enables high-quality andultra-efficient vehicular communication, even with explosively increasing datatraffic among vehicles. In this article, we propose a hierarchicalSemCom-enabled vehicular metaverses framework consisting of the globalmetaverse, local metaverses, SemCom module, and resource pool. The global andlocal metaverses are brand-new concepts from the metaverse's distributionstandpoint. Considering the QoS of users, this article explores the potentialsecurity vulnerabilities of the proposed framework. To that purpose, this studyhighlights a specific security risk to the framework's SemCom module and offersa viable defense solution, so encouraging community researchers to focus moreon vehicular metaverse security. Finally, we provide an overview of the openissues of secure SemCom in the vehicular metaverses, notably pointing outpotential future research directions.</description><author>Jiawen Kang, Jiayi He, Hongyang Du, Zehui Xiong, Zhaohui Yang, Xumin Huang, Shengli Xie</author><pubDate>Mon, 21 Aug 2023 16:03:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03528v2</guid></item><item><title>AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks</title><link>http://arxiv.org/abs/2306.08107v2</link><description>The fields of both Natural Language Processing (NLP) and Automated MachineLearning (AutoML) have achieved remarkable results over the past years. In NLP,especially Large Language Models (LLMs) have experienced a rapid series ofbreakthroughs very recently. We envision that the two fields can radically pushthe boundaries of each other through tight integration. To showcase thisvision, we explore the potential of a symbiotic relationship between AutoML andLLMs, shedding light on how they can benefit each other. In particular, weinvestigate both the opportunities to enhance AutoML approaches with LLMs fromdifferent perspectives and the challenges of leveraging AutoML to furtherimprove LLMs. To this end, we survey existing work, and we critically assessrisks. We strongly believe that the integration of the two fields has thepotential to disrupt both fields, NLP and AutoML. By highlighting conceivablesynergies, but also risks, we aim to foster further exploration at theintersection of AutoML and LLMs.</description><author>Alexander Tornede, Difan Deng, Theresa Eimer, Joseph Giovanelli, Aditya Mohan, Tim Ruhkopf, Sarah Segel, Daphne Theodorakopoulos, Tanja Tornede, Henning Wachsmuth, Marius Lindauer</author><pubDate>Mon, 21 Aug 2023 16:01:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08107v2</guid></item><item><title>The prediction of the quality of results in Logic Synthesis using Transformer and Graph Neural Networks</title><link>http://arxiv.org/abs/2207.11437v2</link><description>In the logic synthesis stage, structure transformations in the synthesis toolneed to be combined into optimization sequences and act on the circuit to meetthe specified circuit area and delay. However, logic synthesis optimizationsequences are time-consuming to run, and predicting the quality of the results(QoR) against the synthesis optimization sequence for a circuit can helpengineers find a better optimization sequence faster. In this work, we proposea deep learning method to predict the QoR of unseen circuit-optimizationsequences pairs. Specifically, the structure transformations are translatedinto vectors by embedding methods and advanced natural language processing(NLP) technology (Transformer) is used to extract the features of theoptimization sequences. In addition, to enable the prediction process of themodel to be generalized from circuit to circuit, the graph representation ofthe circuit is represented as an adjacency matrix and a feature matrix. Graphneural networks(GNN) are used to extract the structural features of thecircuits. For this problem, the Transformer and three typical GNNs are used.Furthermore, the Transformer and GNNs are adopted as a joint learning policyfor the QoR prediction of the unseen circuit-optimization sequences. Themethods resulting from the combination of Transformer and GNNs are benchmarked.The experimental results show that the joint learning of Transformer andGraphSage gives the best results. The Mean Absolute Error (MAE) of thepredicted result is 0.412.</description><author>Chenghao Yang, Zhongda Wang, Yinshui Xia, Zhufei Chu</author><pubDate>Mon, 21 Aug 2023 16:00:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.11437v2</guid></item><item><title>Sound Localization from Motion: Jointly Learning Sound Direction and Camera Rotation</title><link>http://arxiv.org/abs/2303.11329v2</link><description>The images and sounds that we perceive undergo subtle but geometricallyconsistent changes as we rotate our heads. In this paper, we use these cues tosolve a problem we call Sound Localization from Motion (SLfM): jointlyestimating camera rotation and localizing sound sources. We learn to solvethese tasks solely through self-supervision. A visual model predicts camerarotation from a pair of images, while an audio model predicts the direction ofsound sources from binaural sounds. We train these models to generatepredictions that agree with one another. At test time, the models can bedeployed independently. To obtain a feature representation that is well-suitedto solving this challenging problem, we also propose a method for learning anaudio-visual representation through cross-view binauralization: estimatingbinaural sound from one view, given images and sound from another. Our modelcan successfully estimate accurate rotations on both real and synthetic scenes,and localize sound sources with accuracy competitive with state-of-the-artself-supervised approaches. Project site: https://ificl.github.io/SLfM/</description><author>Ziyang Chen, Shengyi Qian, Andrew Owens</author><pubDate>Mon, 21 Aug 2023 15:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11329v2</guid></item><item><title>GBM-based Bregman Proximal Algorithms for Constrained Learning</title><link>http://arxiv.org/abs/2308.10767v1</link><description>As the complexity of learning tasks surges, modern machine learningencounters a new constrained learning paradigm characterized by more intricateand data-driven function constraints. Prominent applications includeNeyman-Pearson classification (NPC) and fairness classification, which entailspecific risk constraints that render standard projection-based trainingalgorithms unsuitable. Gradient boosting machines (GBMs) are among the mostpopular algorithms for supervised learning; however, they are generally limitedto unconstrained settings. In this paper, we adapt the GBM for constrainedlearning tasks within the framework of Bregman proximal algorithms. Weintroduce a new Bregman primal-dual method with a global optimality guaranteewhen the learning objective and constraint functions are convex. In cases ofnonconvex functions, we demonstrate how our algorithm remains effective under aBregman proximal point framework. Distinct from existing constrained learningalgorithms, ours possess a unique advantage in their ability to seamlesslyintegrate with publicly available GBM implementations such as XGBoost (Chen andGuestrin, 2016) and LightGBM (Ke et al., 2017), exclusively relying on theirpublic interfaces. We provide substantial experimental evidence to showcase theeffectiveness of the Bregman algorithm framework. While our primary focus is onNPC and fairness ML, our framework holds significant potential for a broaderrange of constrained learning applications. The source code is currently freelyavailable athttps://github.com/zhenweilin/ConstrainedGBM}{https://github.com/zhenweilin/ConstrainedGBM.</description><author>Zhenwei Lin, Qi Deng</author><pubDate>Mon, 21 Aug 2023 15:56:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10767v1</guid></item><item><title>CoNe: Contrast Your Neighbours for Supervised Image Classification</title><link>http://arxiv.org/abs/2308.10761v1</link><description>Image classification is a longstanding problem in computer vision and machinelearning research. Most recent works (e.g. SupCon , Triplet, and max-margin)mainly focus on grouping the intra-class samples aggressively and compactly,with the assumption that all intra-class samples should be pulled tightlytowards their class centers. However, such an objective will be very hard toachieve since it ignores the intra-class variance in the dataset. (i.e.different instances from the same class can have significant differences).Thus, such a monotonous objective is not sufficient. To provide a moreinformative objective, we introduce Contrast Your Neighbours (CoNe) - a simpleyet practical learning framework for supervised image classification.Specifically, in CoNe, each sample is not only supervised by its class centerbut also directly employs the features of its similar neighbors as anchors togenerate more adaptive and refined targets. Moreover, to further boost theperformance, we propose ``distributional consistency" as a more informativeregularization to enable similar instances to have a similar probabilitydistribution. Extensive experimental results demonstrate that CoNe achievesstate-of-the-art performance across different benchmark datasets, networkarchitectures, and settings. Notably, even without a complicated trainingrecipe, our CoNe achieves 80.8\% Top-1 accuracy on ImageNet with ResNet-50,which surpasses the recent Timm training recipe (80.4\%). Code and pre-trainedmodels are available at\href{https://github.com/mingkai-zheng/CoNe}{https://github.com/mingkai-zheng/CoNe}.</description><author>Mingkai Zheng, Shan You, Lang Huang, Xiu Su, Fei Wang, Chen Qian, Xiaogang Wang, Chang Xu</author><pubDate>Mon, 21 Aug 2023 15:49:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10761v1</guid></item><item><title>Self-supervised Hypergraphs for Learning Multiple World Interpretations</title><link>http://arxiv.org/abs/2308.07615v2</link><description>We present a method for learning multiple scene representations given a smalllabeled set, by exploiting the relationships between such representations inthe form of a multi-task hypergraph. We also show how we can use the hypergraphto improve a powerful pretrained VisTransformer model without any additionallabeled data. In our hypergraph, each node is an interpretation layer (e.g.,depth or segmentation) of the scene. Within each hyperedge, one or severalinput nodes predict the layer at the output node. Thus, each node could be aninput node in some hyperedges and an output node in others. In this way,multiple paths can reach the same node, to form ensembles from which we obtainrobust pseudolabels, which allow self-supervised learning in the hypergraph. Wetest different ensemble models and different types of hyperedges and showsuperior performance to other multi-task graph models in the field. We alsointroduce Dronescapes, a large video dataset captured with UAVs in differentcomplex real-world scenes, with multiple representations, suitable formulti-task learning.</description><author>Alina Marcu, Mihai Pirvu, Dragos Costea, Emanuela Haller, Emil Slusanschi, Ahmed Nabil Belbachir, Rahul Sukthankar, Marius Leordeanu</author><pubDate>Mon, 21 Aug 2023 15:48:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07615v2</guid></item><item><title>DepreSym: A Depression Symptom Annotated Corpus and the Role of LLMs as Assessors of Psychological Markers</title><link>http://arxiv.org/abs/2308.10758v1</link><description>Computational methods for depression detection aim to mine traces ofdepression from online publications posted by Internet users. However,solutions trained on existing collections exhibit limited generalisation andinterpretability. To tackle these issues, recent studies have shown thatidentifying depressive symptoms can lead to more robust models. The eRiskinitiative fosters research on this area and has recently proposed a newranking task focused on developing search methods to find sentences related todepressive symptoms. This search challenge relies on the symptoms specified bythe Beck Depression Inventory-II (BDI-II), a questionnaire widely used inclinical practice. Based on the participant systems' results, we present theDepreSym dataset, consisting of 21580 sentences annotated according to theirrelevance to the 21 BDI-II symptoms. The labelled sentences come from a pool ofdiverse ranking methods, and the final dataset serves as a valuable resourcefor advancing the development of models that incorporate depressive markerssuch as clinical symptoms. Due to the complex nature of this relevanceannotation, we designed a robust assessment methodology carried out by threeexpert assessors (including an expert psychologist). Additionally, we explorehere the feasibility of employing recent Large Language Models (ChatGPT andGPT4) as potential assessors in this complex task. We undertake a comprehensiveexamination of their performance, determine their main limitations and analyzetheir role as a complement or replacement for human annotators.</description><author>Anxo Pérez, Marcos Fernández-Pichel, Javier Parapar, David E. Losada</author><pubDate>Mon, 21 Aug 2023 15:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10758v1</guid></item><item><title>Discriminative Bayesian filtering lends momentum to the stochastic Newton method for minimizing log-convex functions</title><link>http://arxiv.org/abs/2104.12949v3</link><description>To minimize the average of a set of log-convex functions, the stochasticNewton method iteratively updates its estimate using subsampled versions of thefull objective's gradient and Hessian. We contextualize this optimizationproblem as sequential Bayesian inference on a latent state-space model with adiscriminatively-specified observation process. Applying Bayesian filteringthen yields a novel optimization algorithm that considers the entire history ofgradients and Hessians when forming an update. We establish matrix-basedconditions under which the effect of older observations diminishes over time,in a manner analogous to Polyak's heavy ball momentum. We illustrate variousaspects of our approach with an example and review other relevant innovationsfor the stochastic Newton method.</description><author>Michael C. Burkhart</author><pubDate>Mon, 21 Aug 2023 15:44:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.12949v3</guid></item><item><title>To Whom are You Talking? A Deep Learning Model to Endow Social Robots with Addressee Estimation Skills</title><link>http://arxiv.org/abs/2308.10757v1</link><description>Communicating shapes our social word. For a robot to be considered social andbeing consequently integrated in our social environment it is fundamental tounderstand some of the dynamics that rule human-human communication. In thiswork, we tackle the problem of Addressee Estimation, the ability to understandan utterance's addressee, by interpreting and exploiting non-verbal bodily cuesfrom the speaker. We do so by implementing an hybrid deep learning modelcomposed of convolutional layers and LSTM cells taking as input imagesportraying the face of the speaker and 2D vectors of the speaker's bodyposture. Our implementation choices were guided by the aim to develop a modelthat could be deployed on social robots and be efficient in ecologicalscenarios. We demonstrate that our model is able to solve the AddresseeEstimation problem in terms of addressee localisation in space, from a robotego-centric point of view.</description><author>Carlo Mazzola, Marta Romeo, Francesco Rea, Alessandra Sciutti, Angelo Cangelosi</author><pubDate>Mon, 21 Aug 2023 15:43:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10757v1</guid></item><item><title>WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models</title><link>http://arxiv.org/abs/2308.10755v1</link><description>The rise in popularity of ChatGPT and GPT-4 has significantly accelerated thedevelopment of large models, leading to the creation of numerous impressivelarge language models(LLMs) and multimodal large language models (MLLMs). Thesecutting-edge models owe their remarkable performance to high-quality data.However, the details of the training data used in leading paradigms are oftenkept confidential. This lack of transparency, coupled with the scarcity ofopen-source data, impedes further developments within the community. As aresponse, this paper presents "Wan Juan", a large-scale multimodal datasetcomposed of both Chinese and English data, collected from a wide range of websources. The dataset incorporates text, image-text, and video modalities, witha total volume exceeding 2TB. It was utilized in the training of InternLM, amodel that demonstrated significant advantages in multi-dimensional evaluationswhen compared to models of a similar scale. All data can be accessed athttps://opendatalab.org.cn/WanJuan1.0.</description><author>Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, JiaQi Wang, Dahua Lin</author><pubDate>Mon, 21 Aug 2023 15:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10755v1</guid></item><item><title>Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis</title><link>http://arxiv.org/abs/2109.02081v2</link><description>Deep person generation has attracted extensive research attention due to itswide applications in virtual agents, video conferencing, online shopping andart/movie production. With the advancement of deep learning, visual appearances(face, pose, cloth) of a person image can be easily generated or manipulated ondemand. In this survey, we first summarize the scope of person generation, andthen systematically review recent progress and technical trends in deep persongeneration, covering three major tasks: talking-head generation (face),pose-guided person generation (pose) and garment-oriented person generation(cloth). More than two hundred papers are covered for a thorough overview, andthe milestone works are highlighted to witness the major technicalbreakthrough. Based on these fundamental tasks, a number of applications areinvestigated, e.g., virtual fitting, digital human, generative dataaugmentation. We hope this survey could shed some light on the future prospectsof deep person generation, and provide a helpful foundation for fullapplications towards digital human.</description><author>Tong Sha, Wei Zhang, Tong Shen, Zhoujun Li, Tao Mei</author><pubDate>Mon, 21 Aug 2023 15:36:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.02081v2</guid></item><item><title>Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?</title><link>http://arxiv.org/abs/2308.00158v3</link><description>Translation Quality Estimation (TQE) is an essential step before deployingthe output translation into usage. TQE is also critical in assessing machinetranslation (MT) and human translation (HT) quality without seeing thereference translations. This work examines whether the state-of-the-art largelanguage models (LLMs) can be fine-tuned for the TQE task and their capability.We take ChatGPT as one example and approach TQE as a binary classificationtask. Using \textbf{eight language pairs} including English to Italian, German,French, Japanese, Dutch, Portuguese, Turkish, and Chinese training corpora, ourexperimental results show that fine-tuned ChatGPT via its API can achieve arelatively high score on predicting translation quality, i.e. \textit{if thetranslation needs to be edited}. However, there is definitely much space toimprove the model accuracy, e.g. they are 82.42\% and 83.69\% forEnglish-Italian and English-German respectively using our experimentalsettings. English-Italiano bilingual Abstract is available in the paper.</description><author>Serge Gladkoff, Gleb Erofeev, Lifeng Han, Goran Nenadic</author><pubDate>Mon, 21 Aug 2023 15:23:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00158v3</guid></item><item><title>Differentially Private Partial Set Cover with Applications to Facility Location</title><link>http://arxiv.org/abs/2207.10240v2</link><description>It was observed in \citet{gupta2009differentially} that the Set Cover problemhas strong impossibility results under differential privacy. In our work, weobserve that these hardness results dissolve when we turn to the Partial SetCover problem, where we only need to cover a $\rho$-fraction of the elements inthe universe, for some $\rho\in(0,1)$. We show that this relaxation enables usto avoid the impossibility results: under loose conditions on the input setsystem, we give differentially private algorithms which output an explicit setcover with non-trivial approximation guarantees. In particular, this is thefirst differentially private algorithm which outputs an explicit set cover. Using our algorithm for Partial Set Cover as a subroutine, we give adifferentially private (bicriteria) approximation algorithm for a facilitylocation problem which generalizes $k$-center/$k$-supplier with outliers. Likewith the Set Cover problem, no algorithm has been able to give non-trivialguarantees for $k$-center/$k$-supplier-type facility location problems due tothe high sensitivity and impossibility results. Our algorithm shows thatrelaxing the covering requirement to serving only a $\rho$-fraction of thepopulation, for $\rho\in(0,1)$, enables us to circumvent the inherent hardness.Overall, our work is an important step in tackling and understandingimpossibility results in private combinatorial optimization.</description><author>George Z. Li, Dung Nguyen, Anil Vullikanti</author><pubDate>Mon, 21 Aug 2023 15:22:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.10240v2</guid></item><item><title>Real-time Multi-person Eyeblink Detection in the Wild for Untrimmed Video</title><link>http://arxiv.org/abs/2303.16053v2</link><description>Real-time eyeblink detection in the wild can widely serve for fatiguedetection, face anti-spoofing, emotion analysis, etc. The existing researchefforts generally focus on single-person cases towards trimmed video. However,multi-person scenario within untrimmed videos is also important for practicalapplications, which has not been well concerned yet. To address this, we shedlight on this research field for the first time with essential contributions ondataset, theory, and practices. In particular, a large-scale dataset termedMPEblink that involves 686 untrimmed videos with 8748 eyeblink events isproposed under multi-person conditions. The samples are captured fromunconstrained films to reveal "in the wild" characteristics. Meanwhile, areal-time multi-person eyeblink detection method is also proposed. Beingdifferent from the existing counterparts, our proposition runs in a one-stagespatio-temporal way with end-to-end learning capacity. Specifically, itsimultaneously addresses the sub-tasks of face detection, face tracking, andhuman instance-level eyeblink detection. This paradigm holds 2 main advantages:(1) eyeblink features can be facilitated via the face's global context (e.g.,head pose and illumination condition) with joint optimization and interaction,and (2) addressing these sub-tasks in parallel instead of sequential manner cansave time remarkably to meet the real-time running requirement. Experiments onMPEblink verify the essential challenges of real-time multi-person eyeblinkdetection in the wild for untrimmed video. Our method also outperforms existingapproaches by large margins and with a high inference speed.</description><author>Wenzheng Zeng, Yang Xiao, Sicheng Wei, Jinfang Gan, Xintao Zhang, Zhiguo Cao, Zhiwen Fang, Joey Tianyi Zhou</author><pubDate>Mon, 21 Aug 2023 15:18:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16053v2</guid></item><item><title>Boosting Adversarial Attack with Similar Target</title><link>http://arxiv.org/abs/2308.10743v1</link><description>Deep neural networks are vulnerable to adversarial examples, posing a threatto the models' applications and raising security concerns. An intriguingproperty of adversarial examples is their strong transferability. Severalmethods have been proposed to enhance transferability, including ensembleattacks which have demonstrated their efficacy. However, prior approachessimply average logits, probabilities, or losses for model ensembling, lacking acomprehensive analysis of how and why model ensembling significantly improvestransferability. In this paper, we propose a similar targeted attack methodnamed Similar Target~(ST). By promoting cosine similarity between the gradientsof each model, our method regularizes the optimization direction tosimultaneously attack all surrogate models. This strategy has been proven toenhance generalization ability. Experimental results on ImageNet validate theeffectiveness of our approach in improving adversarial transferability. Ourmethod outperforms state-of-the-art attackers on 18 discriminative classifiersand adversarially trained models.</description><author>Shuo Zhang, Ziruo Wang, Zikai Zhou, Huanran Chen</author><pubDate>Mon, 21 Aug 2023 15:16:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10743v1</guid></item><item><title>Stability of Q-Learning Through Design and Optimism</title><link>http://arxiv.org/abs/2307.02632v2</link><description>Q-learning has become an important part of the reinforcement learning toolkitsince its introduction in the dissertation of Chris Watkins in the 1980s. Thepurpose of this paper is in part a tutorial on stochastic approximation andQ-learning, providing details regarding the INFORMS APS inaugural AppliedProbability Trust Plenary Lecture, presented in Nancy France, June 2023. The paper also presents new approaches to ensure stability and potentiallyaccelerated convergence for these algorithms, and stochastic approximation inother settings. Two contributions are entirely new: 1. Stability of Q-learning with linear function approximation has been anopen topic for research for over three decades. It is shown that withappropriate optimistic training in the form of a modified Gibbs policy, thereexists a solution to the projected Bellman equation, and the algorithm isstable (in terms of bounded parameter estimates). Convergence remains one ofmany open topics for research. 2. The new Zap Zero algorithm is designed to approximate the Newton-Raphsonflow without matrix inversion. It is stable and convergent under mildassumptions on the mean flow vector field for the algorithm, and compatiblestatistical assumption on an underlying Markov chain. The algorithm is ageneral approach to stochastic approximation which in particular applies toQ-learning with "oblivious" training even with non-linear functionapproximation.</description><author>Sean Meyn</author><pubDate>Mon, 21 Aug 2023 15:14:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02632v2</guid></item><item><title>On the Adversarial Robustness of Multi-Modal Foundation Models</title><link>http://arxiv.org/abs/2308.10741v1</link><description>Multi-modal foundation models combining vision and language models such asFlamingo or GPT-4 have recently gained enormous interest. Alignment offoundation models is used to prevent models from providing toxic or harmfuloutput. While malicious users have successfully tried to jailbreak foundationmodels, an equally important question is if honest users could be harmed bymalicious third-party content. In this paper we show that imperceivable attackson images in order to change the caption output of a multi-modal foundationmodel can be used by malicious content providers to harm honest users e.g. byguiding them to malicious websites or broadcast fake information. Thisindicates that countermeasures to adversarial attacks should be used by anydeployed multi-modal foundation model.</description><author>Christian Schlarmann, Matthias Hein</author><pubDate>Mon, 21 Aug 2023 15:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10741v1</guid></item><item><title>We Don't Need No Adam, All We Need Is EVE: On The Variance of Dual Learning Rate And Beyond</title><link>http://arxiv.org/abs/2308.10740v1</link><description>In the rapidly advancing field of deep learning, optimising deep neuralnetworks is paramount. This paper introduces a novel method, Enhanced VelocityEstimation (EVE), which innovatively applies different learning rates todistinct components of the gradients. By bifurcating the learning rate, EVEenables more nuanced control and faster convergence, addressing the challengesassociated with traditional single learning rate approaches. Utilising amomentum term that adapts to the learning landscape, the method achieves a moreefficient navigation of the complex loss surface, resulting in enhancedperformance and stability. Extensive experiments demonstrate that EVEsignificantly outperforms existing optimisation techniques across variousbenchmark datasets and architectures.</description><author>Afshin Khadangi</author><pubDate>Mon, 21 Aug 2023 15:08:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10740v1</guid></item><item><title>UGSL: A Unified Framework for Benchmarking Graph Structure Learning</title><link>http://arxiv.org/abs/2308.10737v1</link><description>Graph neural networks (GNNs) demonstrate outstanding performance in a broadrange of applications. While the majority of GNN applications assume that agraph structure is given, some recent methods substantially expanded theapplicability of GNNs by showing that they may be effective even when no graphstructure is explicitly provided. The GNN parameters and a graph structure arejointly learned. Previous studies adopt different experimentation setups,making it difficult to compare their merits. In this paper, we propose abenchmarking strategy for graph structure learning using a unified framework.Our framework, called Unified Graph Structure Learning (UGSL), reformulatesexisting models into a single model. We implement a wide range of existingmodels in our framework and conduct extensive analyses of the effectiveness ofdifferent components in the framework. Our results provide a clear and conciseunderstanding of the different methods in this area as well as their strengthsand weaknesses. The benchmark code is available athttps://github.com/google-research/google-research/tree/master/ugsl.</description><author>Bahare Fatemi, Sami Abu-El-Haija, Anton Tsitsulin, Mehran Kazemi, Dustin Zelle, Neslihan Bulut, Jonathan Halcrow, Bryan Perozzi</author><pubDate>Mon, 21 Aug 2023 15:05:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10737v1</guid></item><item><title>Towards Fair Graph Neural Networks via Graph Counterfactual</title><link>http://arxiv.org/abs/2307.04937v2</link><description>Graph neural networks have shown great ability in representation (GNNs)learning on graphs, facilitating various tasks. Despite their great performancein modeling graphs, recent works show that GNNs tend to inherit and amplify thebias from training data, causing concerns of the adoption of GNNs in high-stakescenarios. Hence, many efforts have been taken for fairness-aware GNNs.However, most existing fair GNNs learn fair node representations by adoptingstatistical fairness notions, which may fail to alleviate bias in the presenceof statistical anomalies. Motivated by causal theory, there are severalattempts utilizing graph counterfactual fairness to mitigate root causes ofunfairness. However, these methods suffer from non-realistic counterfactualsobtained by perturbation or generation. In this paper, we take a causal view onfair graph learning problem. Guided by the casual analysis, we propose a novelframework CAF, which can select counterfactuals from training data to avoidnon-realistic counterfactuals and adopt selected counterfactuals to learn fairnode representations for node classification task. Extensive experiments onsynthetic and real-world datasets show the effectiveness of CAF. Our code isavailable at https://github.com/TimeLovercc/CAF-GNN.</description><author>Zhimeng Guo, Jialiang Li, Teng Xiao, Yao Ma, Suhang Wang</author><pubDate>Mon, 21 Aug 2023 15:05:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04937v2</guid></item><item><title>Patch Is Not All You Need</title><link>http://arxiv.org/abs/2308.10729v1</link><description>Vision Transformers have achieved great success in computer visions,delivering exceptional performance across various tasks. However, theirinherent reliance on sequential input enforces the manual partitioning ofimages into patch sequences, which disrupts the image's inherent structural andsemantic continuity. To handle this, we propose a novel Pattern Transformer(Patternformer) to adaptively convert images to pattern sequences forTransformer input. Specifically, we employ the Convolutional Neural Network toextract various patterns from the input image, with each channel representing aunique pattern that is fed into the succeeding Transformer as a visual token.By enabling the network to optimize these patterns, each pattern concentrateson its local region of interest, thereby preserving its intrinsic structuraland semantic information. Only employing the vanilla ResNet and Transformer, wehave accomplished state-of-the-art performance on CIFAR-10 and CIFAR-100, andhave achieved competitive results on ImageNet.</description><author>Changzhen Li, Jie Zhang, Yang Wei, Zhilong Ji, Jinfeng Bai, Shiguang Shan</author><pubDate>Mon, 21 Aug 2023 14:54:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10729v1</guid></item><item><title>Test-time augmentation-based active learning and self-training for label-efficient segmentation</title><link>http://arxiv.org/abs/2308.10727v1</link><description>Deep learning techniques depend on large datasets whose annotation istime-consuming. To reduce annotation burden, the self-training (ST) andactive-learning (AL) methods have been developed as well as methods thatcombine them in an iterative fashion. However, it remains unclear when eachmethod is the most useful, and when it is advantageous to combine them. In thispaper, we propose a new method that combines ST with AL using Test-TimeAugmentations (TTA). First, TTA is performed on an initial teacher network.Then, cases for annotation are selected based on the lowest estimated Dicescore. Cases with high estimated scores are used as soft pseudo-labels for ST.The selected annotated cases are trained with existing annotated cases and STcases with border slices annotations. We demonstrate the method on MRI fetalbody and placenta segmentation tasks with different data variabilitycharacteristics. Our results indicate that ST is highly effective for bothtasks, boosting performance for in-distribution (ID) and out-of-distribution(OOD) data. However, while self-training improved the performance ofsingle-sequence fetal body segmentation when combined with AL, it slightlydeteriorated performance of multi-sequence placenta segmentation on ID data. ALwas helpful for the high variability placenta data, but did not improve uponrandom selection for the single-sequence body data. For fetal body segmentationsequence transfer, combining AL with ST following ST iteration yielded a Diceof 0.961 with only 6 original scans and 2 new sequence scans. Results usingonly 15 high-variability placenta cases were similar to those using 50 cases.Code is available at: https://github.com/Bella31/TTA-quality-estimation-ST-AL</description><author>Bella Specktor-Fadida, Anna Levchakov, Dana Schonberger, Liat Ben-Sira, Dafna Ben-Bashat, Leo Joskowicz</author><pubDate>Mon, 21 Aug 2023 14:50:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10727v1</guid></item><item><title>Clustered Linear Contextual Bandits with Knapsacks</title><link>http://arxiv.org/abs/2308.10722v1</link><description>In this work, we study clustered contextual bandits where rewards andresource consumption are the outcomes of cluster-specific linear models. Thearms are divided in clusters, with the cluster memberships being unknown to analgorithm. Pulling an arm in a time period results in a reward and inconsumption for each one of multiple resources, and with the total consumptionof any resource exceeding a constraint implying the termination of thealgorithm. Thus, maximizing the total reward requires learning not only modelsabout the reward and the resource consumption, but also cluster memberships. Weprovide an algorithm that achieves regret sublinear in the number of timeperiods, without requiring access to all of the arms. In particular, we showthat it suffices to perform clustering only once to a randomly selected subsetof the arms. To achieve this result, we provide a sophisticated combination oftechniques from the literature of econometrics and of bandits with constraints.</description><author>Yichuan Deng, Michalis Mamakos, Zhao Song</author><pubDate>Mon, 21 Aug 2023 14:47:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10722v1</guid></item><item><title>Vox-E: Text-guided Voxel Editing of 3D Objects</title><link>http://arxiv.org/abs/2303.12048v2</link><description>Large scale text-guided diffusion models have garnered significant attentiondue to their ability to synthesize diverse images that convey complex visualconcepts. This generative power has more recently been leveraged to performtext-to-3D synthesis. In this work, we present a technique that harnesses thepower of latent diffusion models for editing existing 3D objects. Our methodtakes oriented 2D images of a 3D object as input and learns a grid-basedvolumetric representation of it. To guide the volumetric representation toconform to a target text prompt, we follow unconditional text-to-3D methods andoptimize a Score Distillation Sampling (SDS) loss. However, we observe thatcombining this diffusion-guided loss with an image-based regularization lossthat encourages the representation not to deviate too strongly from the inputobject is challenging, as it requires achieving two conflicting goals whileviewing only structure-and-appearance coupled 2D projections. Thus, weintroduce a novel volumetric regularization loss that operates directly in 3Dspace, utilizing the explicit nature of our 3D representation to enforcecorrelation between the global structure of the original and edited object.Furthermore, we present a technique that optimizes cross-attention volumetricgrids to refine the spatial extent of the edits. Extensive experiments andcomparisons demonstrate the effectiveness of our approach in creating a myriadof edits which cannot be achieved by prior works.</description><author>Etai Sella, Gal Fiebelman, Peter Hedman, Hadar Averbuch-Elor</author><pubDate>Mon, 21 Aug 2023 14:45:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12048v2</guid></item><item><title>CoMIX: A Multi-agent Reinforcement Learning Training Architecture for Efficient Decentralized Coordination and Independent Decision Making</title><link>http://arxiv.org/abs/2308.10721v1</link><description>Robust coordination skills enable agents to operate cohesively in sharedenvironments, together towards a common goal and, ideally, individually withouthindering each other's progress. To this end, this paper presents CoordinatedQMIX (CoMIX), a novel training framework for decentralized agents that enablesemergent coordination through flexible policies, allowing at the same timeindependent decision-making at individual level. CoMIX models selfish andcollaborative behavior as incremental steps in each agent's decision process.This allows agents to dynamically adapt their behavior to different situationsbalancing independence and collaboration. Experiments using a variety ofsimulation environments demonstrate that CoMIX outperforms baselines oncollaborative tasks. The results validate our incremental policy approach aseffective technique for improving coordination in multi-agent systems.</description><author>Giovanni Minelli, Mirco Musolesi</author><pubDate>Mon, 21 Aug 2023 14:45:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10721v1</guid></item><item><title>On the accuracy of interpolation based on single-layer artificial neural networks</title><link>http://arxiv.org/abs/2308.10720v1</link><description>In the present paper, we consider one-hidden layer ANNs with a feedforwardarchitecture, also referred to as shallow or two-layer networks, so that thestructure is determined by the number and types of neurons. The determinationof the parameters that define the function, called training, is done via theresolution of the approximation problem, so by imposing the interpolationthrough a set of specific nodes. We present the case where the parameters aretrained using a procedure that is referred to as Extreme Learning Machine (ELM)that leads to a linear interpolation problem. In such hypotheses, the existenceof an ANN interpolating function is guaranteed. The focus is then on theaccuracy of the interpolation outside of the given sampling interpolation nodeswhen they are the equispaced, the Chebychev, and the randomly selected ones.The study is motivated by the well-known bell-shaped Runge example, which makesit clear that the construction of a global interpolating polynomial is accurateonly if trained on suitably chosen nodes, ad example the Chebychev ones. Inorder to evaluate the behavior when growing the number of interpolation nodes,we raise the number of neurons in our network and compare it with theinterpolating polynomial. We test using Runge's function and other well-knownexamples with different regularities. As expected, the accuracy of theapproximation with a global polynomial increases only if the Chebychev nodesare considered. Instead, the error for the ANN interpolating function alwaysdecays and in most cases we observe that the convergence follows what isobserved in the polynomial case on Chebychev nodes, despite the set of nodesused for training.</description><author>Ferdinando Auricchio, Maria Roberta Belardo, Francesco Calabrò, Ariel F. Pascaner</author><pubDate>Mon, 21 Aug 2023 14:40:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10720v1</guid></item><item><title>Backdooring Textual Inversion for Concept Censorship</title><link>http://arxiv.org/abs/2308.10718v1</link><description>Recent years have witnessed success in AIGC (AI Generated Content). Peoplecan make use of a pre-trained diffusion model to generate images of highquality or freely modify existing pictures with only prompts in naturelanguage. More excitingly, the emerging personalization techniques make itfeasible to create specific-desired images with only a few images asreferences. However, this induces severe threats if such advanced techniquesare misused by malicious users, such as spreading fake news or defamingindividual reputations. Thus, it is necessary to regulate personalizationmodels (i.e., concept censorship) for their development and advancement. In this paper, we focus on the personalization technique dubbed TextualInversion (TI), which is becoming prevailing for its lightweight nature andexcellent performance. TI crafts the word embedding that contains detailedinformation about a specific object. Users can easily download the wordembedding from public websites like Civitai and add it to their own stablediffusion model without fine-tuning for personalization. To achieve the conceptcensorship of a TI model, we propose leveraging the backdoor technique for goodby injecting backdoors into the Textual Inversion embeddings. Briefly, weselect some sensitive words as triggers during the training of TI, which willbe censored for normal use. In the subsequent generation stage, if the triggersare combined with personalized embeddings as final prompts, the model willoutput a pre-defined target image rather than images including the desiredmalicious concept. To demonstrate the effectiveness of our approach, we conduct extensiveexperiments on Stable Diffusion, a prevailing open-sourced text-to-image model.Our code, data, and results are available athttps://concept-censorship.github.io.</description><author>Yutong wu, Jie Zhang, Florian Kerschbaum, Tianwei Zhang</author><pubDate>Mon, 21 Aug 2023 14:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10718v1</guid></item><item><title>Rethinking Person Re-identification from a Projection-on-Prototypes Perspective</title><link>http://arxiv.org/abs/2308.10717v1</link><description>Person Re-IDentification (Re-ID) as a retrieval task, has achieved tremendousdevelopment over the past decade. Existing state-of-the-art methods follow ananalogous framework to first extract features from the input images and thencategorize them with a classifier. However, since there is no identity overlapbetween training and testing sets, the classifier is often discarded duringinference. Only the extracted features are used for person retrieval viadistance metrics. In this paper, we rethink the role of the classifier inperson Re-ID, and advocate a new perspective to conceive the classifier as aprojection from image features to class prototypes. These prototypes areexactly the learned parameters of the classifier. In this light, we describethe identity of input images as similarities to all prototypes, which are thenutilized as more discriminative features to perform person Re-ID. We therebypropose a new baseline ProNet, which innovatively reserves the function of theclassifier at the inference stage. To facilitate the learning of classprototypes, both triplet loss and identity classification loss are applied tofeatures that undergo the projection by the classifier. An improved version ofProNet++ is presented by further incorporating multi-granularity designs.Experiments on four benchmarks demonstrate that our proposed ProNet is simpleyet effective, and significantly beats previous baselines. ProNet++ alsoachieves competitive or even better results than transformer-based competitors.</description><author>Qizao Wang, Xuelin Qian, Bin Li, Yanwei Fu, Xiangyang Xue</author><pubDate>Mon, 21 Aug 2023 14:38:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10717v1</guid></item><item><title>Color Prompting for Data-Free Continual Unsupervised Domain Adaptive Person Re-Identification</title><link>http://arxiv.org/abs/2308.10716v1</link><description>Unsupervised domain adaptive person re-identification (Re-ID) methodsalleviate the burden of data annotation through generating pseudo supervisionmessages. However, real-world Re-ID systems, with continuously accumulatingdata streams, simultaneously demand more robust adaptation and anti-forgettingcapabilities. Methods based on image rehearsal addresses the forgetting issuewith limited extra storage but carry the risk of privacy leakage. In this work,we propose a Color Prompting (CoP) method for data-free continual unsuperviseddomain adaptive person Re-ID. Specifically, we employ a light-weighted prompternetwork to fit the color distribution of the current task together with Re-IDtraining. Then for the incoming new tasks, the learned color distributionserves as color style transfer guidance to transfer the images into paststyles. CoP achieves accurate color style recovery for past tasks with adequatedata diversity, leading to superior anti-forgetting effects compared with imagerehearsal methods. Moreover, CoP demonstrates strong generalization performancefor fast adaptation into new domains, given only a small amount of unlabeledimages. Extensive experiments demonstrate that after the continual trainingpipeline the proposed CoP achieves 6.7% and 8.1% average rank-1 improvementsover the replay method on seen and unseen domains, respectively. The sourcecode for this work is publicly available inhttps://github.com/vimar-gu/ColorPromptReID.</description><author>Jianyang Gu, Hao Luo, Kai Wang, Wei Jiang, Yang You, Jian Zhao</author><pubDate>Mon, 21 Aug 2023 14:38:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10716v1</guid></item><item><title>Relax and penalize: a new bilevel approach to mixed-binary hyperparameter optimization</title><link>http://arxiv.org/abs/2308.10711v1</link><description>In recent years, bilevel approaches have become very popular to efficientlyestimate high-dimensional hyperparameters of machine learning models. However,to date, binary parameters are handled by continuous relaxation and roundingstrategies, which could lead to inconsistent solutions. In this context, wetackle the challenging optimization of mixed-binary hyperparameters byresorting to an equivalent continuous bilevel reformulation based on anappropriate penalty term. We propose an algorithmic framework that, undersuitable assumptions, is guaranteed to provide mixed-binary solutions.Moreover, the generality of the method allows to safely use existing continuousbilevel solvers within the proposed framework. We evaluate the performance ofour approach for a specific machine learning problem, i.e., the estimation ofthe group-sparsity structure in regression problems. Reported results clearlyshow that our method outperforms state-of-the-art approaches based onrelaxation and rounding</description><author>Marianna de Santis, Jordan Frecon, Francesco Rinaldi, Saverio Salzo, Martin Schmidt</author><pubDate>Mon, 21 Aug 2023 14:24:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10711v1</guid></item><item><title>Measuring the Effect of Causal Disentanglement on the Adversarial Robustness of Neural Network Models</title><link>http://arxiv.org/abs/2308.10708v1</link><description>Causal Neural Network models have shown high levels of robustness toadversarial attacks as well as an increased capacity for generalisation taskssuch as few-shot learning and rare-context classification compared totraditional Neural Networks. This robustness is argued to stem from thedisentanglement of causal and confounder input signals. However, noquantitative study has yet measured the level of disentanglement achieved bythese types of causal models or assessed how this relates to their adversarialrobustness. Existing causal disentanglement metrics are not applicable to deterministicmodels trained on real-world datasets. We, therefore, utilise metrics ofcontent/style disentanglement from the field of Computer Vision to measuredifferent aspects of the causal disentanglement for four state-of-the-artcausal Neural Network models. By re-implementing these models with a commonResNet18 architecture we are able to fairly measure their adversarialrobustness on three standard image classification benchmarking datasets underseven common white-box attacks. We find a strong association (r=0.820, p=0.001)between the degree to which models decorrelate causal and confounder signalsand their adversarial robustness. Additionally, we find a moderate negativeassociation between the pixel-level information content of the confoundersignal and adversarial robustness (r=-0.597, p=0.040).</description><author>Preben M. Ness, Dusica Marijan, Sunanda Bose</author><pubDate>Mon, 21 Aug 2023 14:22:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10708v1</guid></item><item><title>Sampling From Autoencoders' Latent Space via Quantization And Probability Mass Function Concepts</title><link>http://arxiv.org/abs/2308.10704v1</link><description>In this study, we focus on sampling from the latent space of generativemodels built upon autoencoders so as the reconstructed samples are lifelikeimages. To do to, we introduce a novel post-training sampling algorithm rootedin the concept of probability mass functions, coupled with a quantizationprocess. Our proposed algorithm establishes a vicinity around each latentvector from the input data and then proceeds to draw samples from these definedneighborhoods. This strategic approach ensures that the sampled latent vectorspredominantly inhabit high-probability regions, which, in turn, can beeffectively transformed into authentic real-world images. A noteworthy point ofcomparison for our sampling algorithm is the sampling technique based onGaussian mixture models (GMM), owing to its inherent capability to representclusters. Remarkably, we manage to improve the time complexity from theprevious $\mathcal{O}(n\times d \times k \times i)$ associated with GMMsampling to a much more streamlined $\mathcal{O}(n\times d)$, thereby resultingin substantial speedup during runtime. Moreover, our experimental results,gauged through the Fr\'echet inception distance (FID) for image generation,underscore the superior performance of our sampling algorithm across a diverserange of models and datasets. On the MNIST benchmark dataset, our approachoutperforms GMM sampling by yielding a noteworthy improvement of up to $0.89$in FID value. Furthermore, when it comes to generating images of faces andocular images, our approach showcases substantial enhancements with FIDimprovements of $1.69$ and $0.87$ respectively, as compared to GMM sampling, asevidenced on the CelebA and MOBIUS datasets. Lastly, we substantiate ourmethodology's efficacy in estimating latent space distributions in contrast toGMM sampling, particularly through the lens of the Wasserstein distance.</description><author>Aymene Mohammed Bouayed, Adrian Iaccovelli, David Naccache</author><pubDate>Mon, 21 Aug 2023 14:18:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10704v1</guid></item><item><title>Hierarchical Integration Diffusion Model for Realistic Image Deblurring</title><link>http://arxiv.org/abs/2305.12966v3</link><description>Diffusion models (DMs) have recently been introduced in image deblurring andexhibited promising performance, particularly in terms of detailsreconstruction. However, the diffusion model requires a large number ofinference iterations to recover the clean image from pure Gaussian noise, whichconsumes massive computational resources. Moreover, the distributionsynthesized by the diffusion model is often misaligned with the target results,leading to restrictions in distortion-based metrics. To address the aboveissues, we propose the Hierarchical Integration Diffusion Model (HI-Diff), forrealistic image deblurring. Specifically, we perform the DM in a highlycompacted latent space to generate the prior feature for the deblurringprocess. The deblurring process is implemented by a regression-based method toobtain better distortion accuracy. Meanwhile, the highly compact latent spaceensures the efficiency of the DM. Furthermore, we design the hierarchicalintegration module to fuse the prior into the regression-based model frommultiple scales, enabling better generalization in complex blurry scenarios.Comprehensive experiments on synthetic and real-world blur datasets demonstratethat our HI-Diff outperforms state-of-the-art methods. Code and trained modelsare available at https://github.com/zhengchen1999/HI-Diff.</description><author>Zheng Chen, Yulun Zhang, Ding Liu, Bin Xia, Jinjin Gu, Linghe Kong, Xin Yuan</author><pubDate>Mon, 21 Aug 2023 14:16:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12966v3</guid></item><item><title>Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach</title><link>http://arxiv.org/abs/2308.10699v1</link><description>Online decision making plays a crucial role in numerous real-worldapplications. In many scenarios, the decision is made based on performing asequence of tests on the incoming data points. However, performing all testscan be expensive and is not always possible. In this paper, we provide a novelformulation of the online decision making problem based on combinatorialmulti-armed bandits and take the cost of performing tests into account. Basedon this formulation, we provide a new framework for cost-efficient onlinedecision making which can utilize posterior sampling or BayesUCB forexploration. We provide a rigorous theoretical analysis for our framework andpresent various experimental results that demonstrate its applicability toreal-world problems.</description><author>Arman Rahbar, Niklas Åkerblom, Morteza Haghir Chehreghani</author><pubDate>Mon, 21 Aug 2023 14:09:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10699v1</guid></item><item><title>BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion</title><link>http://arxiv.org/abs/2307.10816v4</link><description>Recent text-to-image diffusion models have demonstrated an astonishingcapacity to generate high-quality images. However, researchers mainly studiedthe way of synthesizing images with only text prompts. While some works haveexplored using other modalities as conditions, considerable paired data, e.g.,box/mask-image pairs, and fine-tuning time are required for nurturing models.As such paired data is time-consuming and labor-intensive to acquire andrestricted to a closed set, this potentially becomes the bottleneck forapplications in an open world. This paper focuses on the simplest form ofuser-provided conditions, e.g., box or scribble. To mitigate the aforementionedproblem, we propose a training-free method to control objects and contexts inthe synthesized images adhering to the given spatial conditions. Specifically,three spatial constraints, i.e., Inner-Box, Outer-Box, and Corner Constraints,are designed and seamlessly integrated into the denoising step of diffusionmodels, requiring no additional training and massive annotated layout data.Extensive experimental results demonstrate that the proposed constraints cancontrol what and where to present in the images while retaining the ability ofDiffusion models to synthesize with high fidelity and diverse concept coverage.The code is publicly available at https://github.com/showlab/BoxDiff.</description><author>Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, Mike Zheng Shou</author><pubDate>Mon, 21 Aug 2023 14:07:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10816v4</guid></item><item><title>Beyond expectations: Residual Dynamic Mode Decomposition and Variance for Stochastic Dynamical Systems</title><link>http://arxiv.org/abs/2308.10697v1</link><description>Koopman operators linearize nonlinear dynamical systems, making theirspectral information of crucial interest. Numerous algorithms have beendeveloped to approximate these spectral properties, and Dynamic ModeDecomposition (DMD) stands out as the poster child of projection-based methods.Although the Koopman operator itself is linear, the fact that it acts in aninfinite-dimensional space of observables poses various challenges. Theseinclude spurious modes, essential spectra, and the verification of Koopman modedecompositions. While recent work has addressed these challenges fordeterministic systems, there remains a notable gap in verified DMD methodstailored for stochastic systems, where the Koopman operator measures theexpectation of observables. We show that it is necessary to go beyondexpectations to address these issues. By incorporating variance into theKoopman framework, we address these challenges. Through an additional DMD-typematrix, we approximate the sum of a squared residual and a variance term, eachof which can be approximated individually using batched snapshot data. Thisallows verified computation of the spectral properties of stochastic Koopmanoperators, controlling the projection error. We also introduce the concept ofvariance-pseudospectra to gauge statistical coherency. Finally, we present asuite of convergence results for the spectral quantities of stochastic Koopmanoperators. Our study concludes with practical applications using both simulatedand experimental data. In neural recordings from awake mice, we demonstrate howvariance-pseudospectra can reveal physiologically significant informationunavailable to standard expectation-based dynamical models.</description><author>Matthew J. Colbrook, Qin Li, Ryan V. Raut, Alex Townsend</author><pubDate>Mon, 21 Aug 2023 14:05:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10697v1</guid></item><item><title>Vanishing Point Estimation in Uncalibrated Images with Prior Gravity Direction</title><link>http://arxiv.org/abs/2308.10694v1</link><description>We tackle the problem of estimating a Manhattan frame, i.e. three orthogonalvanishing points, and the unknown focal length of the camera, leveraging aprior vertical direction. The direction can come from an Inertial MeasurementUnit that is a standard component of recent consumer devices, e.g.,smartphones. We provide an exhaustive analysis of minimal line configurationsand derive two new 2-line solvers, one of which does not suffer fromsingularities affecting existing solvers. Additionally, we design a newnon-minimal method, running on an arbitrary number of lines, to boost theperformance in local optimization. Combining all solvers in a hybrid robustestimator, our method achieves increased accuracy even with a rough prior.Experiments on synthetic and real-world datasets demonstrate the superioraccuracy of our method compared to the state of the art, while havingcomparable runtimes. We further demonstrate the applicability of our solversfor relative rotation estimation. The code is available athttps://github.com/cvg/VP-Estimation-with-Prior-Gravity.</description><author>Rémi Pautrat, Shaohui Liu, Petr Hruby, Marc Pollefeys, Daniel Barath</author><pubDate>Mon, 21 Aug 2023 14:03:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10694v1</guid></item><item><title>Exploring Fine-Grained Representation and Recomposition for Cloth-Changing Person Re-Identification</title><link>http://arxiv.org/abs/2308.10692v1</link><description>Cloth-changing person Re-IDentification (Re-ID) is a particularly challengingtask, suffering from two limitations of inferior identity-relevant features andlimited training samples. Existing methods mainly leverage auxiliaryinformation to facilitate discriminative feature learning, includingsoft-biometrics features of shapes and gaits, and additional labels ofclothing. However, these information may be unavailable in real-worldapplications. In this paper, we propose a novel FIne-grained Representation andRecomposition (FIRe$^{2}$) framework to tackle both limitations without anyauxiliary information. Specifically, we first design a Fine-grained FeatureMining (FFM) module to separately cluster images of each person. Images withsimilar so-called fine-grained attributes (e.g., clothes and viewpoints) areencouraged to cluster together. An attribute-aware classification loss isintroduced to perform fine-grained learning based on cluster labels, which arenot shared among different people, promoting the model to learnidentity-relevant features. Furthermore, by taking full advantage of theclustered fine-grained attributes, we present a Fine-grained AttributeRecomposition (FAR) module to recompose image features with differentattributes in the latent space. It can significantly enhance representationsfor robust feature learning. Extensive experiments demonstrate that FIRe$^{2}$can achieve state-of-the-art performance on five widely-used cloth-changingperson Re-ID benchmarks.</description><author>Qizao Wang, Xuelin Qian, Bin Li, Ying Fu, Yanwei Fu, Xiangyang Xue</author><pubDate>Mon, 21 Aug 2023 13:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10692v1</guid></item><item><title>SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input Images</title><link>http://arxiv.org/abs/2212.09100v3</link><description>Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novelview synthesis as Sparse Radiance Field (SRF) optimization using sparse voxelsfor efficient and fast rendering (plenoxels,InstantNGP). In order to leveragemachine learning and adoption of SRFs as a 3D representation, we present SPARF,a large-scale ShapeNet-based synthetic dataset for novel view synthesisconsisting of $\sim$ 17 million images rendered from nearly 40,000 shapes athigh resolution (400 X 400 pixels). The dataset is orders of magnitude largerthan existing synthetic datasets for novel view synthesis and includes morethan one million 3D-optimized radiance fields with multiple voxel resolutions.Furthermore, we propose a novel pipeline (SuRFNet) that learns to generatesparse voxel radiance fields from only few views. This is done by using thedensely collected SPARF dataset and 3D sparse convolutions. SuRFNet employspartial SRFs from few/one images and a specialized SRF loss to learn togenerate high-quality sparse voxel radiance fields that can be rendered fromnovel views. Our approach achieves state-of-the-art results in the task ofunconstrained novel view synthesis based on few views on ShapeNet as comparedto recent baselines. The SPARF dataset is made public with the code and modelson the project website https://abdullahamdi.com/sparf/ .</description><author>Abdullah Hamdi, Bernard Ghanem, Matthias Nießner</author><pubDate>Mon, 21 Aug 2023 13:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09100v3</guid></item><item><title>Reliable Detection and Quantification of Selective Forces in Language Change</title><link>http://arxiv.org/abs/2305.15914v2</link><description>Language change is a cultural evolutionary process in which variants oflinguistic variables change in frequency through processes analogous tomutation, selection and genetic drift. In this work, we apply arecently-introduced method to corpus data to quantify the strength of selectionin specific instances of historical language change. We first demonstrate, inthe context of English irregular verbs, that this method is more reliable andinterpretable than similar methods that have previously been applied. Wefurther extend this study to demonstrate that a bias towards phonologicalsimplicity overrides that favouring grammatical simplicity when these are inconflict. Finally, with reference to Spanish spelling reforms, we show that themethod can also detect points in time at which selection strengths change, afeature that is generically expected for socially-motivated language change.Together, these results indicate how hypotheses for mechanisms of languagechange can be tested quantitatively using historical corpus data.</description><author>Juan Guerrero Montero, Andres Karjus, Kenny Smith, Richard A. Blythe</author><pubDate>Mon, 21 Aug 2023 13:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15914v2</guid></item><item><title>Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics</title><link>http://arxiv.org/abs/2208.10533v3</link><description>An inherent problem of reinforcement learning is performing exploration of anenvironment through random actions, of which a large portion can beunproductive. Instead, exploration can be improved by initializing the learningpolicy with an existing (previously learned or hard-coded) oracle policy,offline data, or demonstrations. In the case of using an oracle policy, it canbe unclear how best to incorporate the oracle policy's experience into thelearning policy in a way that maximizes learning sample efficiency. In thispaper, we propose a method termed Critic Confidence Guided Exploration (CCGE)for incorporating such an oracle policy into standard actor-criticreinforcement learning algorithms. More specifically, CCGE takes in the oraclepolicy's actions as suggestions and incorporates this information into thelearning scheme when uncertainty is high, while ignoring it when theuncertainty is low. CCGE is agnostic to methods of estimating uncertainty, andwe show that it is equally effective with two different techniques.Empirically, we evaluate the effect of CCGE on various benchmark reinforcementlearning tasks, and show that this idea can lead to improved sample efficiencyand final performance. Furthermore, when evaluated on sparse rewardenvironments, CCGE is able to perform competitively against adjacent algorithmsthat also leverage an oracle policy. Our experiments show that it is possibleto utilize uncertainty as a heuristic to guide exploration using an oracle inreinforcement learning. We expect that this will inspire more research in thisdirection, where various heuristics are used to determine the direction ofguidance provided to learning.</description><author>Jun Jet Tai, Jordan K. Terry, Mauro S. Innocente, James Brusey, Nadjim Horri</author><pubDate>Mon, 21 Aug 2023 13:49:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.10533v3</guid></item></channel></rss>