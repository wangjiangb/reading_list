<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 21 May 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Images that Sound: Composing Images and Sounds on a Single Canvas</title><link>http://arxiv.org/abs/2405.12221v1</link><description>Spectrograms are 2D representations of sound that look very different fromthe images found in our visual world. And natural images, when played asspectrograms, make unnatural sounds. In this paper, we show that it is possibleto synthesize spectrograms that simultaneously look like natural images andsound like natural audio. We call these spectrograms images that sound. Ourapproach is simple and zero-shot, and it leverages pre-trained text-to-imageand text-to-spectrogram diffusion models that operate in a shared latent space.During the reverse process, we denoise noisy latents with both the audio andimage diffusion models in parallel, resulting in a sample that is likely underboth models. Through quantitative evaluations and perceptual studies, we findthat our method successfully generates spectrograms that align with a desiredaudio prompt while also taking the visual appearance of a desired image prompt.Please see our project page for video results:https://ificl.github.io/images-that-sound/</description><author>Ziyang Chen, Daniel Geng, Andrew Owens</author><pubDate>Mon, 20 May 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12221v1</guid></item><item><title>Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo</title><link>http://arxiv.org/abs/2405.12218v1</link><description>We present MVSGaussian, a new generalizable 3D Gaussian representationapproach derived from Multi-View Stereo (MVS) that can efficiently reconstructunseen scenes. Specifically, 1) we leverage MVS to encode geometry-awareGaussian representations and decode them into Gaussian parameters. 2) Tofurther enhance performance, we propose a hybrid Gaussian rendering thatintegrates an efficient volume rendering design for novel view synthesis. 3) Tosupport fast fine-tuning for specific scenes, we introduce a multi-viewgeometric consistent aggregation strategy to effectively aggregate the pointclouds generated by the generalizable model, serving as the initialization forper-scene optimization. Compared with previous generalizable NeRF-basedmethods, which typically require minutes of fine-tuning and seconds ofrendering per image, MVSGaussian achieves real-time rendering with bettersynthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussianachieves better view synthesis with less training computational cost. Extensiveexperiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Templesdatasets validate that MVSGaussian attains state-of-the-art performance withconvincing generalizability, real-time rendering speed, and fast per-sceneoptimization.</description><author>Tianqi Liu, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu</author><pubDate>Mon, 20 May 2024 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12218v1</guid></item><item><title>Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning</title><link>http://arxiv.org/abs/2405.12217v1</link><description>Recent studies indicate that large multimodal models (LMMs) are highly robustagainst natural distribution shifts, often surpassing previous baselines.Despite this, domain-specific adaptation is still necessary, particularly inspecialized areas like healthcare. Due to the impracticality of fine-tuningLMMs given their vast parameter space, this work investigates in-contextlearning (ICL) as an effective alternative for enhancing LMMs' adaptability. Wefind that the success of ICL heavily relies on the choice of demonstration,mirroring challenges seen in large language models but introducing uniquecomplexities for LMMs facing distribution shifts. Our study addresses this byevaluating an unsupervised ICL method, TopKNearestPR, which selects in-contextexamples through a nearest example search based on feature similarity. Weuncover that its effectiveness is limited by the deficiencies of pre-trainedvision encoders under distribution shift scenarios. To address thesechallenges, we propose InvariantSelectPR, a novel method leveragingClass-conditioned Contrastive Invariance (CCI) for more robust demonstrationselection. Specifically, CCI enhances pre-trained vision encoders by improvingtheir discriminative capabilities across different classes and ensuringinvariance to domain-specific variations. This enhancement allows the encodersto effectively identify and retrieve the most informative examples, which arethen used to guide LMMs in adapting to new query samples under varyingdistributions. Our experiments show that InvariantSelectPR substantiallyimproves the adaptability of LMMs, achieving significant performance gains onbenchmark datasets, with a 34.2%$\uparrow$ accuracy increase in 7-shot onCamelyon17 and 16.9%$\uparrow$ increase in 7-shot on HAM10000 compared to thebaseline zero-shot performance.</description><author>Guanglin Zhou, Zhongyi Han, Shiming Chen, Biwei Huang, Liming Zhu, Salman Khan, Xin Gao, Lina Yao</author><pubDate>Mon, 20 May 2024 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12217v1</guid></item><item><title>Octo: An Open-Source Generalist Robot Policy</title><link>http://arxiv.org/abs/2405.12213v1</link><description>Large policies pretrained on diverse robot datasets have the potential totransform robotic learning: instead of training new policies from scratch, suchgeneralist robot policies may be finetuned with only a little in-domain data,yet generalize broadly. However, to be widely applicable across a range ofrobotic learning scenarios, environments, and tasks, such policies need tohandle diverse sensors and action spaces, accommodate a variety of commonlyused robotic platforms, and finetune readily and efficiently to new domains. Inthis work, we aim to lay the groundwork for developing open-source, widelyapplicable, generalist policies for robotic manipulation. As a first step, weintroduce Octo, a large transformer-based policy trained on 800k trajectoriesfrom the Open X-Embodiment dataset, the largest robot manipulation dataset todate. It can be instructed via language commands or goal images and can beeffectively finetuned to robot setups with new sensory inputs and action spaceswithin a few hours on standard consumer GPUs. In experiments across 9 roboticplatforms, we demonstrate that Octo serves as a versatile policy initializationthat can be effectively finetuned to new observation and action spaces. We alsoperform detailed ablations of design decisions for the Octo model, fromarchitecture to training data, to guide future research on building generalistrobot models.</description><author>Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, Sergey Levine</author><pubDate>Mon, 20 May 2024 18:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12213v1</guid></item><item><title>Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices</title><link>http://arxiv.org/abs/2405.12211v1</link><description>Text-to-image (T2I) diffusion models achieve state-of-the-art results inimage synthesis and editing. However, leveraging such pretrained models forvideo editing is considered a major challenge. Many existing works attempt toenforce temporal consistency in the edited video through explicitcorrespondence mechanisms, either in pixel space or between deep features.These methods, however, struggle with strong nonrigid motion. In this paper, weintroduce a fundamentally different approach, which is based on the observationthat spatiotemporal slices of natural videos exhibit similar characteristics tonatural images. Thus, the same T2I diffusion model that is normally used onlyas a prior on video frames, can also serve as a strong prior for enhancingtemporal consistency by applying it on spatiotemporal slices. Based on thisobservation, we present Slicedit, a method for text-based video editing thatutilizes a pretrained T2I diffusion model to process both spatial andspatiotemporal slices. Our method generates videos that retain the structureand motion of the original video while adhering to the target text. Throughextensive experiments, we demonstrate Slicedit's ability to edit a wide rangeof real-world videos, confirming its clear advantages compared to existingcompeting methods. Webpage: https://matankleiner.github.io/slicedit/</description><author>Nathaniel Cohen, Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, Tomer Michaeli</author><pubDate>Mon, 20 May 2024 18:55:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12211v1</guid></item><item><title>MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark</title><link>http://arxiv.org/abs/2405.12209v1</link><description>Recent advancements in large language models (LLMs) have showcasedsignificant improvements in mathematics. However, traditional math benchmarkslike GSM8k offer a unidimensional perspective, falling short in providing aholistic assessment of the LLMs' math capabilities. To address this gap, weintroduce MathBench, a new benchmark that rigorously assesses the mathematicalcapabilities of large language models. MathBench spans a wide range ofmathematical disciplines, offering a detailed evaluation of both theoreticalunderstanding and practical problem-solving skills. The benchmark progressesthrough five distinct stages, from basic arithmetic to college mathematics, andis structured to evaluate models at various depths of knowledge. Each stageincludes theoretical questions and application problems, allowing us to measurea model's mathematical proficiency and its ability to apply concepts inpractical scenarios. MathBench aims to enhance the evaluation of LLMs'mathematical abilities, providing a nuanced view of their knowledgeunderstanding levels and problem solving skills in a bilingual context. Theproject is released at https://github.com/open-compass/MathBench .</description><author>Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, Kai Chen</author><pubDate>Mon, 20 May 2024 18:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12209v1</guid></item><item><title>Optimistic Query Routing in Clustering-based Approximate Maximum Inner Product Search</title><link>http://arxiv.org/abs/2405.12207v1</link><description>Clustering-based nearest neighbor search is a simple yet effective method inwhich data points are partitioned into geometric shards to form an index, andonly a few shards are searched during query processing to find an approximateset of top-$k$ vectors. Even though the search efficacy is heavily influencedby the algorithm that identifies the set of shards to probe, it has receivedlittle attention in the literature. This work attempts to bridge that gap bystudying the problem of routing in clustering-based maximum inner productsearch (MIPS). We begin by unpacking existing routing protocols and notice thesurprising contribution of optimism. We then take a page from the sequentialdecision making literature and formalize that insight following the principleof ``optimism in the face of uncertainty.'' In particular, we present a newframework that incorporates the moments of the distribution of inner productswithin each shard to optimistically estimate the maximum inner product. We thenpresent a simple instance of our algorithm that uses only the first two momentsto reach the same accuracy as state-of-the-art routers such as \scann byprobing up to $50%$ fewer points on a suite of benchmark MIPS datasets. Ouralgorithm is also space-efficient: we design a sketch of the second momentwhose size is independent of the number of points and in practice requiresstoring only $O(1)$ additional vectors per shard.</description><author>Sebastian Bruch, Aditya Krishnan, Franco Maria Nardini</author><pubDate>Mon, 20 May 2024 18:47:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12207v1</guid></item><item><title>Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control</title><link>http://arxiv.org/abs/2405.08366v3</link><description>Disentangling model activations into meaningful features is a central problemin interpretability. However, the absence of ground-truth for these features inrealistic scenarios makes validating recent approaches, such as sparsedictionary learning, elusive. To address this challenge, we propose a frameworkfor evaluating feature dictionaries in the context of specific tasks, bycomparing them against \emph{supervised} feature dictionaries. First, wedemonstrate that supervised dictionaries achieve excellent approximation,control, and interpretability of model computations on the task. Second, we usethe supervised dictionaries to develop and contextualize evaluations ofunsupervised dictionaries along the same three axes. We apply this framework to the indirect object identification (IOI) taskusing GPT-2 Small, with sparse autoencoders (SAEs) trained on either the IOI orOpenWebText datasets. We find that these SAEs capture interpretable featuresfor the IOI task, but they are less successful than supervised features incontrolling the model. Finally, we observe two qualitative phenomena in SAEtraining: feature occlusion (where a causally relevant concept is robustlyovershadowed by even slightly higher-magnitude ones in the learned features),and feature over-splitting (where binary features split into many smaller, lessinterpretable features). We hope that our framework will provide a useful steptowards more objective and grounded evaluations of sparse dictionary learningmethods.</description><author>Aleksandar Makelov, George Lange, Neel Nanda</author><pubDate>Mon, 20 May 2024 18:46:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08366v3</guid></item><item><title>Modeling citation worthiness by using attention-based bidirectional long short-term memory networks and interpretable models</title><link>http://arxiv.org/abs/2405.12206v1</link><description>Scientist learn early on how to cite scientific sources to support theirclaims. Sometimes, however, scientists have challenges determining where acitation should be situated -- or, even worse, fail to cite a sourcealtogether. Automatically detecting sentences that need a citation (i.e.,citation worthiness) could solve both of these issues, leading to more robustand well-constructed scientific arguments. Previous researchers have appliedmachine learning to this task but have used small datasets and models that donot take advantage of recent algorithmic developments such as attentionmechanisms in deep learning. We hypothesize that we can develop significantlyaccurate deep learning architectures that learn from large supervised datasetsconstructed from open access publications. In this work, we propose aBidirectional Long Short-Term Memory (BiLSTM) network with attention mechanismand contextual information to detect sentences that need citations. We alsoproduce a new, large dataset (PMOA-CITE) based on PubMed Open Access Subset,which is orders of magnitude larger than previous datasets. Our experimentsshow that our architecture achieves state of the art performance on thestandard ACL-ARC dataset ($F_{1}=0.507$) and exhibits high performance($F_{1}=0.856$) on the new PMOA-CITE. Moreover, we show that it can transferlearning across these datasets. We further use interpretable models toilluminate how specific language is used to promote and inhibit citations. Wediscover that sections and surrounding sentences are crucial for our improvedpredictions. We further examined purported mispredictions of the model, anduncovered systematic human mistakes in citation behavior and source data. Thisopens the door for our model to check documents during pre-submission andpre-archival procedures. We make this new dataset, the code, and a web-basedtool available to the community.</description><author>Tong Zeng, Daniel E. Acuna</author><pubDate>Mon, 20 May 2024 18:45:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12206v1</guid></item><item><title>Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving</title><link>http://arxiv.org/abs/2405.12205v1</link><description>Metacognitive knowledge refers to humans' intuitive knowledge of their ownthinking and reasoning processes. Today's best LLMs clearly possess somereasoning processes. The paper gives evidence that they also have metacognitiveknowledge, including ability to name skills and procedures to apply given atask. We explore this primarily in context of math reasoning, developing aprompt-guided interaction procedure to get a powerful LLM to assign sensibleskill labels to math questions, followed by having it perform semanticclustering to obtain coarser families of skill labels. These coarse skilllabels look interpretable to humans. To validate that these skill labels are meaningful and relevant to the LLM'sreasoning processes we perform the following experiments. (a) We ask GPT-4 toassign skill labels to training questions in math datasets GSM8K and MATH. (b)When using an LLM to solve the test questions, we present it with the full listof skill labels and ask it to identify the skill needed. Then it is presentedwith randomly selected exemplar solved questions associated with that skilllabel. This improves accuracy on GSM8k and MATH for several strong LLMs,including code-assisted models. The methodology presented is domain-agnostic,even though this article applies it to math problems.</description><author>Aniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap, Danilo Rezende, Yoshua Bengio, Michael Mozer, Sanjeev Arora</author><pubDate>Mon, 20 May 2024 18:45:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12205v1</guid></item><item><title>Accelerating Relative Entropy Coding with Space Partitioning</title><link>http://arxiv.org/abs/2405.12203v1</link><description>Relative entropy coding (REC) algorithms encode a random sample following atarget distribution $Q$, using a coding distribution $P$ shared between thesender and receiver. Sadly, general REC algorithms suffer from prohibitiveencoding times, at least on the order of $2^{D_{\text{KL}}[Q||P]}$, and fasteralgorithms are limited to very specific settings. This work addresses thisissue by introducing a REC scheme utilizing space partitioning to reduceruntime in practical scenarios. We provide theoretical analyses of our methodand demonstrate its effectiveness with both toy examples and practicalapplications. Notably, our method successfully handles REC tasks with$D_{\text{KL}}[Q||P]$ about three times what previous methods can manage andreduces the compression rate by approximately 5-15\% in VAE-based losslesscompression on MNIST and INR-based lossy compression on CIFAR-10 compared toprevious methods, significantly improving the practicality of REC for neuralcompression.</description><author>Jiajun He, Gergely Flamich, José Miguel Hernández-Lobato</author><pubDate>Mon, 20 May 2024 18:41:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12203v1</guid></item><item><title>Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution</title><link>http://arxiv.org/abs/2405.12202v1</link><description>In this work, we present an arbitrary-scale super-resolution (SR) method toenhance the resolution of scientific data, which often involves complexchallenges such as continuity, multi-scale physics, and the intricacies ofhigh-frequency signals. Grounded in operator learning, the proposed method isresolution-invariant. The core of our model is a hierarchical neural operatorthat leverages a Galerkin-type self-attention mechanism, enabling efficientlearning of mappings between function spaces. Sinc filters are used tofacilitate the information transfer across different levels in the hierarchy,thereby ensuring representation equivalence in the proposed neural operator.Additionally, we introduce a learnable prior structure that is derived from thespectral resizing of the input data. This loss prior is model-agnostic and isdesigned to dynamically adjust the weighting of pixel contributions, therebybalancing gradients effectively across the model. We conduct extensiveexperiments on diverse datasets from different domains and demonstrateconsistent improvements compared to strong baselines, which consist of variousstate-of-the-art SR methods.</description><author>Xihaier Luo, Xiaoning Qian, Byung-Jun Yoon</author><pubDate>Mon, 20 May 2024 18:39:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12202v1</guid></item><item><title>Multi-View Attentive Contextualization for Multi-View 3D Object Detection</title><link>http://arxiv.org/abs/2405.12200v1</link><description>We present Multi-View Attentive Contextualization (MvACon), a simple yeteffective method for improving 2D-to-3D feature lifting in query-basedmulti-view 3D (MV3D) object detection. Despite remarkable progress witnessed inthe field of query-based MV3D object detection, prior art often suffers fromeither the lack of exploiting high-resolution 2D features in denseattention-based lifting, due to high computational costs, or frominsufficiently dense grounding of 3D queries to multi-scale 2D features insparse attention-based lifting. Our proposed MvACon hits the two birds with onestone using a representationally dense yet computationally sparse attentivefeature contextualization scheme that is agnostic to specific 2D-to-3D featurelifting approaches. In experiments, the proposed MvACon is thoroughly tested onthe nuScenes benchmark, using both the BEVFormer and its recent 3D deformableattention (DFA3D) variant, as well as the PETR, showing consistent detectionperformance improvement, especially in enhancing performance in location,orientation, and velocity prediction. It is also tested on the Waymo-minibenchmark using BEVFormer with similar improvement. We qualitatively andquantitatively show that global cluster-based contexts effectively encode densescene-level contexts for MV3D object detection. The promising results of ourproposed MvACon reinforces the adage in computer vision -- ``(contextualized)feature matters".</description><author>Xianpeng Liu, Ce Zheng, Ming Qian, Nan Xue, Chen Chen, Zhebin Zhang, Chen Li, Tianfu Wu</author><pubDate>Mon, 20 May 2024 18:37:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12200v1</guid></item><item><title>Metric Entropy-Free Sample Complexity Bounds for Sample Average Approximation in Convex Stochastic Programming</title><link>http://arxiv.org/abs/2401.00664v2</link><description>This paper studies the sample average approximation (SAA) in solving convexor strongly convex stochastic programming problems. Under some commonregularity conditions, we show -- perhaps for the first time -- that the SAA'ssample complexity can be completely free from any quantification of metricentropy (such as the logarithm of the covering number), leading to asignificantly more efficient rate with dimensionality $d$ than most existingresults. From the newly established complexity bounds, an important revelationis that the SAA and the canonical stochastic mirror descent (SMD) method, twomainstream solution approaches to SP, entail almost identical rates of sampleefficiency, rectifying a long-standing theoretical discrepancy of the SAA fromthe SMD by the order of $O(d)$. Furthermore, this paper exploresnon-Lipschitzian scenarios where the SAA maintains provable efficacy, whereascorresponding results for the SMD remain unexplored, indicating the potentialof the SAA's better applicability in some irregular settings.</description><author>Hongcheng Liu, Jindong Tong</author><pubDate>Mon, 20 May 2024 18:28:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00664v2</guid></item><item><title>FashionEngine: Interactive 3D Human Generation and Editing via Multimodal Controls</title><link>http://arxiv.org/abs/2404.01655v3</link><description>We present FashionEngine, an interactive 3D human generation and editingsystem that creates 3D digital humans via user-friendly multimodal controlssuch as natural languages, visual perceptions, and hand-drawing sketches.FashionEngine automates the 3D human production with three key components: 1) Apre-trained 3D human diffusion model that learns to model 3D humans in asemantic UV latent space from 2D image training data, which provides strongpriors for diverse generation and editing tasks. 2) Multimodality-UV Spaceencoding the texture appearance, shape topology, and textual semantics of humanclothing in a canonical UV-aligned space, which faithfully aligns the usermultimodal inputs with the implicit UV latent space for controllable 3D humanediting. The multimodality-UV space is shared across different user inputs,such as texts, images, and sketches, which enables various joint multimodalediting tasks. 3) Multimodality-UV Aligned Sampler learns to samplehigh-quality and diverse 3D humans from the diffusion prior. Extensiveexperiments validate FashionEngine's state-of-the-art performance forconditional generation/editing tasks. In addition, we present an interactiveuser interface for our FashionEngine that enables both conditional andunconditional generation tasks, and editing tasks including pose/view/shapecontrol, text-, image-, and sketch-driven 3D human editing and 3D virtualtry-on, in a unified framework. Our project page is at:https://taohuumd.github.io/projects/FashionEngine.</description><author>Tao Hu, Fangzhou Hong, Zhaoxi Chen, Ziwei Liu</author><pubDate>Mon, 20 May 2024 18:25:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01655v3</guid></item><item><title>Training Data Attribution via Approximate Unrolled Differentation</title><link>http://arxiv.org/abs/2405.12186v1</link><description>Many training data attribution (TDA) methods aim to estimate how a model'sbehavior would change if one or more data points were removed from the trainingset. Methods based on implicit differentiation, such as influence functions,can be made computationally efficient, but fail to account forunderspecification, the implicit bias of the optimization algorithm, ormulti-stage training pipelines. By contrast, methods based on unrolling addressthese issues but face scalability challenges. In this work, we connect theimplicit-differentiation-based and unrolling-based approaches and combine theirbenefits by introducing Source, an approximate unrolling-based TDA method thatis computed using an influence-function-like formula. While beingcomputationally efficient compared to unrolling-based approaches, Source issuitable in cases where implicit-differentiation-based approaches struggle,such as in non-converged models and multi-stage training pipelines.Empirically, Source outperforms existing TDA techniques in counterfactualprediction, especially in settings where implicit-differentiation-basedapproaches fall short.</description><author>Juhan Bae, Wu Lin, Jonathan Lorraine, Roger Grosse</author><pubDate>Mon, 20 May 2024 18:17:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12186v1</guid></item><item><title>Multi-order Graph Clustering with Adaptive Node-level Weight Learning</title><link>http://arxiv.org/abs/2405.12183v1</link><description>Current graph clustering methods emphasize individual node and edge connections, while ignoring higher-order organization at the level of motif. Recently, higher-order graph clustering approaches have been designed by motifbased hypergraphs. However, these approaches often suffer from hypergraphfragmentation issue seriously, which degrades the clustering performancegreatly. Moreover, real-world graphs usually contain diverse motifs, with nodesparticipating in multiple motifs. A key challenge is how to achieve preciseclustering results by integrating information from multiple motifs at the nodelevel. In this paper, we propose a multi-order graph clustering model (MOGC) tointegrate multiple higher-order structures and edge connections at node level.MOGC employs an adaptive weight learning mechanism to au tomatically adjust thecontributions of different motifs for each node. This not only tackleshypergraph fragmentation issue but enhances clustering accuracy. MOGC isefficiently solved by an alternating minimization algo rithm. Experiments onseven real-world datasets illustrate the effectiveness of MOGC.</description><author>Ye Liu, Xuelei Lin, Yejia Chen, Reynold Cheng</author><pubDate>Mon, 20 May 2024 18:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12183v1</guid></item><item><title>Continual Learning of Diffusion Models with Generative Distillation</title><link>http://arxiv.org/abs/2311.14028v2</link><description>Diffusion models are powerful generative models that achieve state-of-the-artperformance in image synthesis. However, training them demands substantialamounts of data and computational resources. Continual learning would allow forincrementally learning new tasks and accumulating knowledge, thus enabling thereuse of trained models for further learning. One potentially suitablecontinual learning approach is generative replay, where a copy of a generativemodel trained on previous tasks produces synthetic data that are interleavedwith data from the current task. However, standard generative replay applied todiffusion models results in a catastrophic loss in denoising capabilities. Inthis paper, we propose generative distillation, an approach that distils theentire reverse process of a diffusion model. We demonstrate that our approachsubstantially improves the continual learning performance of generative replaywith only a modest increase in the computational costs.</description><author>Sergi Masip, Pau Rodriguez, Tinne Tuytelaars, Gido M. van de Ven</author><pubDate>Mon, 20 May 2024 18:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14028v2</guid></item><item><title>Building Temporal Kernels with Orthogonal Polynomials</title><link>http://arxiv.org/abs/2405.12179v1</link><description>We introduce a class of models named PLEIADES (PoLynomial Expansion InAdaptive Distributed Event-based Systems), which contains temporal convolutionkernels generated from orthogonal polynomial basis functions. We focus oninterfacing these networks with event-based data to perform onlinespatiotemporal classification and detection with low latency. By virtue ofusing structured temporal kernels and event-based data, we have the freedom tovary the sample rate of the data along with the discretization step-size of thenetwork without additional finetuning. We experimented with three event-basedbenchmarks and obtained state-of-the-art results on all three by large marginswith significantly smaller memory and compute costs. We achieved: 1) 99.59%accuracy with 192K parameters on the DVS128 hand gesture recognition datasetand 100% with a small additional output filter; 2) 99.58% test accuracy with277K parameters on the AIS 2024 eye tracking challenge; and 3) 0.556 mAP with576k parameters on the PROPHESEE 1 Megapixel Automotive Detection Dataset.</description><author>Yan Ru Pei, Olivier Coenen</author><pubDate>Mon, 20 May 2024 18:06:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12179v1</guid></item><item><title>Enhancing Explainable AI: A Hybrid Approach Combining GradCAM and LRP for CNN Interpretability</title><link>http://arxiv.org/abs/2405.12175v1</link><description>We present a new technique that explains the output of a CNN-based modelusing a combination of GradCAM and LRP methods. Both of these methods producevisual explanations by highlighting input regions that are important forpredictions. In the new method, the explanation produced by GradCAM is firstprocessed to remove noises. The processed output is then multiplied elementwisewith the output of LRP. Finally, a Gaussian blur is applied on the product. Wecompared the proposed method with GradCAM and LRP on the metrics ofFaithfulness, Robustness, Complexity, Localisation and Randomisation. It wasobserved that this method performs better on Complexity than both GradCAM andLRP and is better than atleast one of them in the other metrics.</description><author>Vaibhav Dhore, Achintya Bhat, Viraj Nerlekar, Kashyap Chavhan, Aniket Umare</author><pubDate>Mon, 20 May 2024 17:58:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12175v1</guid></item><item><title>CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models</title><link>http://arxiv.org/abs/2405.12174v1</link><description>Text-to-Table aims to generate structured tables to convey the keyinformation from unstructured documents. Existing text-to-table datasets aretypically oriented English, limiting the research in non-English languages.Meanwhile, the emergence of large language models (LLMs) has shown greatsuccess as general task solvers in multi-lingual settings (e.g., ChatGPT),theoretically enabling text-to-table in other languages. In this paper, wepropose a Chinese text-to-table dataset, CT-Eval, to benchmark LLMs on thistask. Our preliminary analysis of English text-to-table datasets highlights twokey factors for dataset construction: data diversity and data hallucination.Inspired by this, the CT-Eval dataset selects a popular Chinesemultidisciplinary online encyclopedia as the source and covers 28 domains toensure data diversity. To minimize data hallucination, we first train an LLM tojudge and filter out the task samples with hallucination, then employ humanannotators to clean the hallucinations in the validation and testing sets.After this process, CT-Eval contains 88.6K task samples. Using CT-Eval, weevaluate the performance of open-source and closed-source LLMs. Our resultsreveal that zero-shot LLMs (including GPT-4) still have a significantperformance gap compared with human judgment. Furthermore, after fine-tuning,open-source LLMs can significantly improve their text-to-table ability,outperforming GPT-4 by a large margin. In short, CT-Eval not only helpsresearchers evaluate and quickly understand the Chinese text-to-table abilityof existing LLMs but also serves as a valuable resource to significantlyimprove the text-to-table performance of LLMs.</description><author>Haoxiang Shi, Jiaan Wang, Jiarong Xu, Cen Wang, Tetsuya Sakai</author><pubDate>Mon, 20 May 2024 17:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12174v1</guid></item><item><title>Text-to-Vector Generation with Neural Path Representation</title><link>http://arxiv.org/abs/2405.10317v2</link><description>Vector graphics are widely used in digital art and highly favored bydesigners due to their scalability and layer-wise properties. However, theprocess of creating and editing vector graphics requires creativity and designexpertise, making it a time-consuming task. Recent advancements intext-to-vector (T2V) generation have aimed to make this process moreaccessible. However, existing T2V methods directly optimize control points ofvector graphics paths, often resulting in intersecting or jagged paths due tothe lack of geometry constraints. To overcome these limitations, we propose anovel neural path representation by designing a dual-branch VariationalAutoencoder (VAE) that learns the path latent space from both sequence andimage modalities. By optimizing the combination of neural paths, we canincorporate geometric constraints while preserving expressivity in generatedSVGs. Furthermore, we introduce a two-stage path optimization method to improvethe visual and topological quality of generated SVGs. In the first stage, apre-trained text-to-image diffusion model guides the initial generation ofcomplex vector graphics through the Variational Score Distillation (VSD)process. In the second stage, we refine the graphics using a layer-wise imagevectorization strategy to achieve clearer elements and structure. Wedemonstrate the effectiveness of our method through extensive experiments andshowcase various applications. The project page ishttps://intchous.github.io/T2V-NPR.</description><author>Peiying Zhang, Nanxuan Zhao, Jing Liao</author><pubDate>Mon, 20 May 2024 17:56:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10317v2</guid></item><item><title>A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe</title><link>http://arxiv.org/abs/2307.14361v3</link><description>In our study, we introduce a novel hybrid ensemble model that synergisticallycombines LSTM, BiLSTM, CNN, GRU, and GloVe embeddings for the classification ofgene mutations in cancer. This model was rigorously tested using Kaggle'sPersonalized Medicine: Redefining Cancer Treatment dataset, demonstratingexceptional performance across all evaluation metrics. Notably, our approachachieved a training accuracy of 80.6%, precision of 81.6%, recall of 80.6%, andan F1 score of 83.1%, alongside a significantly reduced Mean Squared Error(MSE) of 2.596. These results surpass those of advanced transformer models andtheir ensembles, showcasing our model's superior capability in handling thecomplexities of gene mutation classification. The accuracy and efficiency ofgene mutation classification are paramount in the era of precision medicine,where tailored treatment plans based on individual genetic profiles candramatically improve patient outcomes and save lives. Our model's remarkableperformance highlights its potential in enhancing the precision of cancerdiagnoses and treatments, thereby contributing significantly to the advancementof personalized healthcare.</description><author>Sanad Aburass, Osama Dorgham, Jamil Al Shaqsi</author><pubDate>Mon, 20 May 2024 17:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14361v3</guid></item><item><title>State of the Practice for Medical Imaging Software</title><link>http://arxiv.org/abs/2405.12171v1</link><description>We selected 29 medical imaging projects from 48 candidates, assessed 10software qualities by answering 108 questions for each software project, andinterviewed 8 of the 29 development teams. Based on the quantitative data, weranked the MI software with the Analytic Hierarchy Process (AHP). The fourtop-ranked software products are 3D Slicer, ImageJ, Fiji, and OHIF Viewer.Generally, MI software is in a healthy state as shown by the following: weobserved 88% of the documentation artifacts recommended by research softwaredevelopment guidelines, 100% of MI projects use version control tools, anddevelopers appear to use the common quasi-agile research software developmentprocess. However, the current state of the practice deviates from the existingguidelines because of the rarity of some recommended artifacts, low usage ofcontinuous integration (17% of the projects), low use of unit testing (about50% of projects), and room for improvement with documentation (six of ninedevelopers felt their documentation was not clear enough). From interviewingthe developers, we identified five pain points and two qualities of potentialconcern: lack of development time, lack of funding, technology hurdles,ensuring correctness, usability, maintainability, and reproducibility. Theinterviewees proposed strategies to improve the state of the practice, toaddress the identified pain points, and to improve software quality. Combiningtheir ideas with ours, we have the following list of recommendations: increasedocumentation, increase testing by enriching datasets, increase continuousintegration usage, move to web applications, employ linters, use peer reviews,design for change, add assurance cases, and incorporate a "Generate All Things"approach.</description><author>W. Spencer Smith, Ao Dong, Jacques Carette, Michael D. Noseworthy</author><pubDate>Mon, 20 May 2024 17:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12171v1</guid></item><item><title>Using Degeneracy in the Loss Landscape for Mechanistic Interpretability</title><link>http://arxiv.org/abs/2405.10927v2</link><description>Mechanistic Interpretability aims to reverse engineer the algorithmsimplemented by neural networks by studying their weights and activations. Anobstacle to reverse engineering neural networks is that many of the parametersinside a network are not involved in the computation being implemented by thenetwork. These degenerate parameters may obfuscate internal structure. Singularlearning theory teaches us that neural network parameterizations are biasedtowards being more degenerate, and parameterizations with more degeneracy arelikely to generalize further. We identify 3 ways that network parameters can bedegenerate: linear dependence between activations in a layer; linear dependencebetween gradients passed back to a layer; ReLUs which fire on the same subsetof datapoints. We also present a heuristic argument that modular networks arelikely to be more degenerate, and we develop a metric for identifying modulesin a network that is based on this argument. We propose that if we canrepresent a neural network in a way that is invariant to reparameterizationsthat exploit the degeneracies, then this representation is likely to be moreinterpretable, and we provide some evidence that such a representation islikely to have sparser interactions. We introduce the Interaction Basis, atractable technique to obtain a representation that is invariant todegeneracies from linear dependence of activations or Jacobians.</description><author>Lucius Bushnaq, Jake Mendel, Stefan Heimersheim, Dan Braun, Nicholas Goldowsky-Dill, Kaarel Hänni, Cindy Wu, Marius Hobbhahn</author><pubDate>Mon, 20 May 2024 17:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10927v2</guid></item><item><title>Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging</title><link>http://arxiv.org/abs/2405.12163v1</link><description>The rapid advancement of large language models has given rise to a plethoraof applications across a myriad of real-world tasks, mainly centered onaligning with human intent. However, the complexities inherent in human intentnecessitate a dependence on labor-intensive and time-consuming humanevaluation. To alleviate this constraint, we delve into the paradigm ofemploying open-source large language models as evaluators, aligning with theprevailing trend of utilizing GPT-4. Particularly, we present a step-by-stepevaluation framework: \textbf{Fennec}, capable of \textbf{F}ine-grained\textbf{E}valuatio\textbf{N} and correctio\textbf{N} \textbf{E}xtended throughbran\textbf{C}hing and bridging. Specifically, the branching operation dissectsthe evaluation task into various dimensions and granularities, therebyalleviating the challenges associated with evaluation. Concurrently, thebridging operation amalgamates diverse training datasets, augmenting thevariety of evaluation tasks. In experimental trials, our 7B model consistentlyoutperforms open-source larger-scale evaluation models across various widelyadopted benchmarks in terms of both \textit{Agreement} and\textit{Consistency}, closely approaching the capabilities of GPT-4. We employthe fine-grained correction capabilities induced by the evaluation model torefine multiple model responses, and the results show that the refinementelevates the quality of responses, leading to an improvement of 1-2 points onthe MT-Bench. Our code is available atGithub\footnote{\url{https://github.com/dropreg/Fennec}}.</description><author>Xiaobo Liang, Haoke Zhang, Helan hu, Juntao Li, Jun Xu, Min Zhang</author><pubDate>Mon, 20 May 2024 17:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12163v1</guid></item><item><title>SIAM: A Simple Alternating Mixer for Video Prediction</title><link>http://arxiv.org/abs/2311.11683v2</link><description>Video prediction, predicting future frames from the previous ones, has broadapplications such as autonomous driving and weather forecasting. Existingstate-of-the-art methods typically focus on extracting either spatial,temporal, or spatiotemporal features from videos. Different feature focuses,resulting from different network architectures, may make the resultant modelsexcel at some video prediction tasks but perform poorly on others. Towards amore generic video prediction solution, we explicitly model these features in aunified encoder-decoder framework and propose a novel simple alternating Mixer(SIAM). The novelty of SIAM lies in the design of dimension alternating mixing(DaMi) blocks, which can model spatial, temporal, and spatiotemporal featuresthrough alternating the dimensions of the feature maps. Extensive experimentalresults demonstrate the superior performance of the proposed SIAM on fourbenchmark video datasets covering both synthetic and real-world scenarios.</description><author>Xin Zheng, Ziang Peng, Yuan Cao, Hongming Shan, Junping Zhang</author><pubDate>Mon, 20 May 2024 17:46:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11683v2</guid></item><item><title>MambaOut: Do We Really Need Mamba for Vision?</title><link>http://arxiv.org/abs/2405.07992v3</link><description>Mamba, an architecture with RNN-like token mixer of state space model (SSM),was recently introduced to address the quadratic complexity of the attentionmechanism and subsequently applied to vision tasks. Nevertheless, theperformance of Mamba for vision is often underwhelming when compared withconvolutional and attention-based models. In this paper, we delve into theessence of Mamba, and conceptually conclude that Mamba is ideally suited fortasks with long-sequence and autoregressive characteristics. For vision tasks,as image classification does not align with either characteristic, wehypothesize that Mamba is not necessary for this task; Detection andsegmentation tasks are also not autoregressive, yet they adhere to thelong-sequence characteristic, so we believe it is still worthwhile to exploreMamba's potential for these tasks. To empirically verify our hypotheses, weconstruct a series of models named MambaOut through stacking Mamba blocks whileremoving their core token mixer, SSM. Experimental results strongly support ourhypotheses. Specifically, our MambaOut model surpasses all visual Mamba modelson ImageNet image classification, indicating that Mamba is indeed unnecessaryfor this task. As for detection and segmentation, MambaOut cannot match theperformance of state-of-the-art visual Mamba models, demonstrating thepotential of Mamba for long-sequence visual tasks. The code is available athttps://github.com/yuweihao/MambaOut</description><author>Weihao Yu, Xinchao Wang</author><pubDate>Mon, 20 May 2024 17:36:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07992v3</guid></item><item><title>The Local Interaction Basis: Identifying Computationally-Relevant and Sparsely Interacting Features in Neural Networks</title><link>http://arxiv.org/abs/2405.10928v2</link><description>Mechanistic interpretability aims to understand the behavior of neuralnetworks by reverse-engineering their internal computations. However, currentmethods struggle to find clear interpretations of neural network activationsbecause a decomposition of activations into computational features is missing.Individual neurons or model components do not cleanly correspond to distinctfeatures or functions. We present a novel interpretability method that aims toovercome this limitation by transforming the activations of the network into anew basis - the Local Interaction Basis (LIB). LIB aims to identifycomputational features by removing irrelevant activations and interactions. Ourmethod drops irrelevant activation directions and aligns the basis with thesingular vectors of the Jacobian matrix between adjacent layers. It also scalesfeatures based on their importance for downstream computation, producing aninteraction graph that shows all computationally-relevant features andinteractions in a model. We evaluate the effectiveness of LIB on modularaddition and CIFAR-10 models, finding that it identifies morecomputationally-relevant features that interact more sparsely, compared toprincipal component analysis. However, LIB does not yield substantialimprovements in interpretability or interaction sparsity when applied tolanguage models. We conclude that LIB is a promising theory-driven approach foranalyzing neural networks, but in its current form is not applicable to largelanguage models.</description><author>Lucius Bushnaq, Stefan Heimersheim, Nicholas Goldowsky-Dill, Dan Braun, Jake Mendel, Kaarel Hänni, Avery Griffin, Jörn Stöhler, Magdalena Wache, Marius Hobbhahn</author><pubDate>Mon, 20 May 2024 17:34:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10928v2</guid></item><item><title>On the sample complexity of parameter estimation in logistic regression with normal design</title><link>http://arxiv.org/abs/2307.04191v3</link><description>The logistic regression model is one of the most popular data generationmodel in noisy binary classification problems. In this work, we study thesample complexity of estimating the parameters of the logistic regression modelup to a given $\ell_2$ error, in terms of the dimension and the inversetemperature, with standard normal covariates. The inverse temperature controlsthe signal-to-noise ratio of the data generation process. While bothgeneralization bounds and asymptotic performance of the maximum-likelihoodestimator for logistic regression are well-studied, the non-asymptotic samplecomplexity that shows the dependence on error and the inverse temperature forparameter estimation is absent from previous analyses. We show that the samplecomplexity curve has two change-points in terms of the inverse temperature,clearly separating the low, moderate, and high temperature regimes.</description><author>Daniel Hsu, Arya Mazumdar</author><pubDate>Mon, 20 May 2024 17:29:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04191v3</guid></item><item><title>Using Natural Language Explanations to Improve Robustness of In-context Learning</title><link>http://arxiv.org/abs/2311.07556v2</link><description>Recent studies demonstrated that large language models (LLMs) can excel inmany tasks via in-context learning (ICL). However, recent works show thatICL-prompted models tend to produce inaccurate results when presented withadversarial inputs. In this work, we investigate whether augmenting ICL withnatural language explanations (NLEs) improves the robustness of LLMs onadversarial datasets covering natural language inference and paraphrasingidentification. We prompt LLMs with a small set of human-generated NLEs toproduce further NLEs, yielding more accurate results than both a zero-shot-ICLsetting and using only human-generated NLEs. Our results on five popular LLMs(GPT3.5-turbo, Llama2, Vicuna, Zephyr, and Mistral) show that our approachyields over 6% improvement over baseline approaches for eight adversarialdatasets: HANS, ISCS, NaN, ST, PICD, PISP, ANLI, and PAWS. Furthermore,previous studies have demonstrated that prompt selection strategiessignificantly enhance ICL on in-distribution test sets. However, our findingsreveal that these strategies do not match the efficacy of our approach forrobustness evaluations, resulting in an accuracy drop of 8% compared to theproposed approach.</description><author>Xuanli He, Yuxiang Wu, Oana-Maria Camburu, Pasquale Minervini, Pontus Stenetorp</author><pubDate>Mon, 20 May 2024 17:24:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07556v2</guid></item><item><title>Bangladeshi Native Vehicle Detection in Wild</title><link>http://arxiv.org/abs/2405.12150v1</link><description>The success of autonomous navigation relies on robust and precise vehiclerecognition, hindered by the scarcity of region-specific vehicle detectiondatasets, impeding the development of context-aware systems. To advanceterrestrial object detection research, this paper proposes a native vehicledetection dataset for the most commonly appeared vehicle classes in Bangladesh.17 distinct vehicle classes have been taken into account, with fully annotated81542 instances of 17326 images. Each image width is set to at least 1280px.The dataset's average vehicle bounding box-to-image ratio is 4.7036. ThisBangladesh Native Vehicle Dataset (BNVD) has accounted for severalgeographical, illumination, variety of vehicle sizes, and orientations to bemore robust on surprised scenarios. In the context of examining the BNVDdataset, this work provides a thorough assessment with four successive You OnlyLook Once (YOLO) models, namely YOLO v5, v6, v7, and v8. These dataset'seffectiveness is methodically evaluated and contrasted with other vehicledatasets already in use. The BNVD dataset exhibits mean average precision(mAP)at 50% intersection over union (IoU) is 0.848 corresponding precision andrecall values of 0.841 and 0.774. The research findings indicate a mAP of 0.643at an IoU range of 0.5 to 0.95. The experiments show that the BNVD datasetserves as a reliable representation of vehicle distribution and presentsconsiderable complexities.</description><author>Bipin Saha, Md. Johirul Islam, Shaikh Khaled Mostaque, Aditya Bhowmik, Tapodhir Karmakar Taton, Md. Nakib Hayat Chowdhury, Mamun Bin Ibne Reaz</author><pubDate>Mon, 20 May 2024 17:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12150v1</guid></item><item><title>Eliciting Problem Specifications via Large Language Models</title><link>http://arxiv.org/abs/2405.12147v1</link><description>Cognitive systems generally require a human to translate a problem definitioninto some specification that the cognitive system can use to attempt to solvethe problem or perform the task. In this paper, we illustrate that largelanguage models (LLMs) can be utilized to map a problem class, defined innatural language, into a semi-formal specification that can then be utilized byan existing reasoning and learning system to solve instances from the problemclass. We present the design of LLM-enabled cognitive task analyst agent(s).Implemented with LLM agents, this system produces a definition of problemspaces for tasks specified in natural language. LLM prompts are derived fromthe definition of problem spaces in the AI literature and generalproblem-solving strategies (Polya's How to Solve It). A cognitive system canthen use the problem-space specification, applying domain-general problemsolving strategies ("weak methods" such as search), to solve multiple instancesof problems from the problem class. This result, while preliminary, suggeststhe potential for speeding cognitive systems research via disintermediation ofproblem formulation while also retaining core capabilities of cognitivesystems, such as robust inference and online learning.</description><author>Robert E. Wray, James R. Kirk, John E. Laird</author><pubDate>Mon, 20 May 2024 17:19:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12147v1</guid></item><item><title>DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on LLM</title><link>http://arxiv.org/abs/2405.12139v1</link><description>Visual Language Tracking (VLT) enhances single object tracking (SOT) byintegrating natural language descriptions from a video, for the precisetracking of a specified object. By leveraging high-level semantic information,VLT guides object tracking, alleviating the constraints associated with relyingon a visual modality. Nevertheless, most VLT benchmarks are annotated in asingle granularity and lack a coherent semantic framework to provide scientificguidance. Moreover, coordinating human annotators for high-quality annotationsis laborious and time-consuming. To address these challenges, we introduceDTLLM-VLT, which automatically generates extensive and multi-granularity textto enhance environmental diversity. (1) DTLLM-VLT generates scientific andmulti-granularity text descriptions using a cohesive prompt framework. Itssuccinct and highly adaptable design allows seamless integration into variousvisual tracking benchmarks. (2) We select three prominent benchmarks to deployour approach: short-term tracking, long-term tracking, and global instancetracking. We offer four granularity combinations for these benchmarks,considering the extent and density of semantic information, thereby showcasingthe practicality and versatility of DTLLM-VLT. (3) We conduct comparativeexperiments on VLT benchmarks with different text granularities, evaluating andanalyzing the impact of diverse text on tracking performance. Conclusionally,this work leverages LLM to provide multi-granularity semantic information forVLT task from efficient and diverse perspectives, enabling fine-grainedevaluation of multi-modal trackers. In the future, we believe this work can beextended to more datasets to support vision datasets understanding.</description><author>Xuchen Li, Xiaokun Feng, Shiyu Hu, Meiqi Wu, Dailing Zhang, Jing Zhang, Kaiqi Huang</author><pubDate>Mon, 20 May 2024 17:01:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12139v1</guid></item><item><title>DreamPropeller: Supercharge Text-to-3D Generation with Parallel Sampling</title><link>http://arxiv.org/abs/2311.17082v3</link><description>Recent methods such as Score Distillation Sampling (SDS) and VariationalScore Distillation (VSD) using 2D diffusion models for text-to-3D generationhave demonstrated impressive generation quality. However, the long generationtime of such algorithms significantly degrades the user experience. To tacklethis problem, we propose DreamPropeller, a drop-in acceleration algorithm thatcan be wrapped around any existing text-to-3D generation pipeline based onscore distillation. Our framework generalizes Picard iterations, a classicalalgorithm for parallel sampling an ODE path, and can account for non-ODE pathssuch as momentum-based gradient updates and changes in dimensions during theoptimization process as in many cases of 3D generation. We show that ouralgorithm trades parallel compute for wallclock time and empirically achievesup to 4.7x speedup with a negligible drop in generation quality for all testedframeworks.</description><author>Linqi Zhou, Andy Shih, Chenlin Meng, Stefano Ermon</author><pubDate>Mon, 20 May 2024 16:53:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17082v3</guid></item><item><title>RSCNet: Dynamic CSI Compression for Cloud-based WiFi Sensing</title><link>http://arxiv.org/abs/2402.04888v2</link><description>WiFi-enabled Internet-of-Things (IoT) devices are evolving from merecommunication devices to sensing instruments, leveraging Channel StateInformation (CSI) extraction capabilities. Nevertheless, resource-constrainedIoT devices and the intricacies of deep neural networks necessitatetransmitting CSI to cloud servers for sensing. Although feasible, this leads toconsiderable communication overhead. In this context, this paper develops anovel Real-time Sensing and Compression Network (RSCNet) which enables sensingwith compressed CSI; thereby reducing the communication overheads. RSCNetfacilitates optimization across CSI windows composed of a few CSI frames. Oncetransmitted to cloud servers, it employs Long Short-Term Memory (LSTM) units toharness data from prior windows, thus bolstering both the sensing accuracy andCSI reconstruction. RSCNet adeptly balances the trade-off between CSIcompression and sensing precision, thus streamlining real-time cloud-based WiFisensing with reduced communication costs. Numerical findings demonstrate thegains of RSCNet over the existing benchmarks like SenseFi, showcasing a sensingaccuracy of 97.4% with minimal CSI reconstruction error. Numerical results alsoshow a computational analysis of the proposed RSCNet as a function of thenumber of CSI frames.</description><author>Borna Barahimi, Hakam Singh, Hina Tabassum, Omer Waqar, Mohammad Omer</author><pubDate>Mon, 20 May 2024 16:48:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04888v2</guid></item><item><title>MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning</title><link>http://arxiv.org/abs/2405.12130v1</link><description>Low-rank adaptation is a popular parameter-efficient fine-tuning method forlarge language models. In this paper, we analyze the impact of low-rankupdating, as implemented in LoRA. Our findings suggest that the low-rankupdating mechanism may limit the ability of LLMs to effectively learn andmemorize new knowledge. Inspired by this observation, we propose a new methodcalled MoRA, which employs a square matrix to achieve high-rank updating whilemaintaining the same number of trainable parameters. To achieve it, weintroduce the corresponding non-parameter operators to reduce the inputdimension and increase the output dimension for the square matrix. Furthermore,these operators ensure that the weight can be merged back into LLMs, whichmakes our method can be deployed like LoRA. We perform a comprehensiveevaluation of our method across five tasks: instruction tuning, mathematicalreasoning, continual pretraining, memory and pretraining. Our methodoutperforms LoRA on memory-intensive tasks and achieves comparable performanceon other tasks.</description><author>Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, Fuzhen Zhuang</author><pubDate>Mon, 20 May 2024 16:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12130v1</guid></item><item><title>Fair Active Learning: Solving the Labeling Problem in Insurance</title><link>http://arxiv.org/abs/2112.09466v4</link><description>This paper addresses significant obstacles that arise from the widespread useof machine learning models in the insurance industry, with a specific focus onpromoting fairness. The initial challenge lies in effectively leveragingunlabeled data in insurance while reducing the labeling effort and emphasizingdata relevance through active learning techniques. The paper explores variousactive learning sampling methodologies and evaluates their impact on bothsynthetic and real insurance datasets. This analysis highlights the difficultyof achieving fair model inferences, as machine learning models may replicatebiases and discrimination found in the underlying data. To tackle theseinterconnected challenges, the paper introduces an innovative fair activelearning method. The proposed approach samples informative and fair instances,achieving a good balance between model predictive performance and fairness, asconfirmed by numerical experiments on insurance datasets.</description><author>Romuald Elie, Caroline Hillairet, François Hu, Marc Juillard</author><pubDate>Mon, 20 May 2024 16:46:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.09466v4</guid></item><item><title>The effect of diversity on group decision-making</title><link>http://arxiv.org/abs/2402.01427v2</link><description>We explore different aspects of cognitive diversity and its effect on thesuccess of group deliberation. To evaluate this, we use 500 dialogues fromsmall, online groups discussing the Wason Card Selection task - the DeliDatacorpus. Leveraging the corpus, we perform quantitative analysis evaluatingthree different measures of cognitive diversity. First, we analyse the effectof group size as a proxy measure for diversity. Second, we evaluate the effectof the size of the initial idea pool. Finally, we look into the content of thediscussion by analysing discussed solutions, discussion patterns, and howconversational probing can improve those characteristics. Despite thereputation of groups for compounding bias, we show that small groups can,through dialogue, overcome intuitive biases and improve individualdecision-making. Across a large sample and different operationalisations, weconsistently find that greater cognitive diversity is associated with moresuccessful group deliberation. Code and data used for the analysis areavailable in the repository:https://github.com/gkaradzhov/cognitive-diversity-groups-cogsci24.</description><author>Georgi Karadzhov, Andreas Vlachos, Tom Stafford</author><pubDate>Mon, 20 May 2024 16:46:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01427v2</guid></item><item><title>Alzheimer's Magnetic Resonance Imaging Classification Using Deep and Meta-Learning Models</title><link>http://arxiv.org/abs/2405.12126v1</link><description>Deep learning, a cutting-edge machine learning approach, outperformstraditional machine learning in identifying intricate structures in complexhigh-dimensional data, particularly in the domain of healthcare. This studyfocuses on classifying Magnetic Resonance Imaging (MRI) data for Alzheimer'sdisease (AD) by leveraging deep learning techniques characterized bystate-of-the-art CNNs. Brain imaging techniques such as MRI have enabled themeasurement of pathophysiological brain changes related to Alzheimer's disease.Alzheimer's disease is the leading cause of dementia in the elderly, and it isan irreversible brain illness that causes gradual cognitive function disorder.In this paper, we train some benchmark deep models individually for theapproach of the solution and later use an ensembling approach to combine theeffect of multiple CNNs towards the observation of higher recall and accuracy.Here, the model's effectiveness is evaluated using various methods, includingstacking, majority voting, and the combination of models with high recallvalues. The majority voting performs better than the alternative modellingapproach as the majority voting approach typically reduces the variance in thepredictions. We report a test accuracy of 90% with a precision score of 0.90and a recall score of 0.89 in our proposed approach. In future, this study canbe extended to incorporate other types of medical data, including signals,images, and other data. The same or alternative datasets can be used withadditional classifiers, neural networks, and AI techniques to enhanceAlzheimer's detection.</description><author>Nida Nasir, Muneeb Ahmed, Neda Afreen, Mustafa Sameer</author><pubDate>Mon, 20 May 2024 16:44:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12126v1</guid></item><item><title>Jury: A Comprehensive Evaluation Toolkit</title><link>http://arxiv.org/abs/2310.02040v2</link><description>Evaluation plays a critical role in deep learning as a fundamental block ofany prediction-based system. However, the vast number of Natural LanguageProcessing (NLP) tasks and the development of various metrics have led tochallenges in evaluating different systems with different metrics. To addressthese challenges, we introduce jury, a toolkit that provides a unifiedevaluation framework with standardized structures for performing evaluationacross different tasks and metrics. The objective of jury is to standardize andimprove metric evaluation for all systems and aid the community in overcomingthe challenges in evaluation. Since its open-source release, jury has reached awide audience and is available at https://github.com/obss/jury.</description><author>Devrim Cavusoglu, Secil Sen, Ulas Sert, Sinan Altinuc</author><pubDate>Mon, 20 May 2024 16:40:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02040v2</guid></item><item><title>An Active Learning Framework with a Class Balancing Strategy for Time Series Classification</title><link>http://arxiv.org/abs/2405.12122v1</link><description>Training machine learning models for classification tasks often requireslabeling numerous samples, which is costly and time-consuming, especially intime series analysis. This research investigates Active Learning (AL)strategies to reduce the amount of labeled data needed for effective timeseries classification. Traditional AL techniques cannot control the selectionof instances per class for labeling, leading to potential bias inclassification performance and instance selection, particularly in imbalancedtime series datasets. To address this, we propose a novel class-balancinginstance selection algorithm integrated with standard AL strategies. Ourapproach aims to select more instances from classes with fewer labeledexamples, thereby addressing imbalance in time series datasets. We demonstratethe effectiveness of our AL framework in selecting informative data samples fortwo distinct domains of tactile texture recognition and industrial faultdetection. In robotics, our method achieves high-performance texturecategorization while significantly reducing labeled training data requirementsto 70%. We also evaluate the impact of different sliding window time intervalson robotic texture classification using AL strategies. In synthetic fibermanufacturing, we adapt AL techniques to address the challenge of faultclassification, aiming to minimize data annotation cost and time forindustries. We also address real-life class imbalances in the multiclassindustrial anomalous dataset using our class-balancing instance algorithmintegrated with AL strategies. Overall, this thesis highlights the potential ofour AL framework across these two distinct domains.</description><author>Shemonto Das</author><pubDate>Mon, 20 May 2024 16:39:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12122v1</guid></item><item><title>PECAN: A Deterministic Certified Defense Against Backdoor Attacks</title><link>http://arxiv.org/abs/2301.11824v4</link><description>Neural networks are vulnerable to backdoor poisoning attacks, where theattackers maliciously poison the training set and insert triggers into the testinput to change the prediction of the victim model. Existing defenses forbackdoor attacks either provide no formal guarantees or come withexpensive-to-compute and ineffective probabilistic guarantees. We presentPECAN, an efficient and certified approach for defending against backdoorattacks. The key insight powering PECAN is to apply off-the-shelf test-timeevasion certification techniques on a set of neural networks trained ondisjoint partitions of the data. We evaluate PECAN on image classification andmalware detection datasets. Our results demonstrate that PECAN can (1)significantly outperform the state-of-the-art certified backdoor defense, bothin defense strength and efficiency, and (2) on real back-door attacks, PECANcan reduce attack success rate by order of magnitude when compared to a rangeof baselines from the literature.</description><author>Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni</author><pubDate>Mon, 20 May 2024 16:38:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11824v4</guid></item><item><title>Deception Detection from Linguistic and Physiological Data Streams Using Bimodal Convolutional Neural Networks</title><link>http://arxiv.org/abs/2311.10944v3</link><description>Deception detection is gaining increasing interest due to ethical andsecurity concerns. This paper explores the application of convolutional neuralnetworks for the purpose of multimodal deception detection. We use a datasetbuilt by interviewing 104 subjects about two topics, with one truthful and onefalsified response from each subject about each topic. In particular, we makethree main contributions. First, we extract linguistic and physiologicalfeatures from this data to train and construct the neural network models.Second, we propose a fused convolutional neural network model using bothmodalities in order to achieve an improved overall performance. Third, wecompare our new approach with earlier methods designed for multimodal deceptiondetection. We find that our system outperforms regular classification methods;our results indicate the feasibility of using neural networks for deceptiondetection even in the presence of limited amounts of data.</description><author>Panfeng Li, Mohamed Abouelenien, Rada Mihalcea, Zhicheng Ding, Qikai Yang, Yiming Zhou</author><pubDate>Mon, 20 May 2024 16:38:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10944v3</guid></item><item><title>Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation</title><link>http://arxiv.org/abs/2405.12119v1</link><description>Large language models (LLMs) are revolutionizing conversational recommendersystems by adeptly indexing item content, understanding complex conversationalcontexts, and generating relevant item titles. However, controlling thedistribution of recommended items remains a challenge. This leads to suboptimalperformance due to the failure to capture rapidly changing data distributions,such as item popularity, on targeted conversational recommendation platforms.In conversational recommendation, LLMs recommend items by generating the titles(as multiple tokens) autoregressively, making it difficult to obtain andcontrol the recommendations over all items. Thus, we propose aReindex-Then-Adapt (RTA) framework, which converts multi-token item titles intosingle tokens within LLMs, and then adjusts the probability distributions overthese single-token item titles accordingly. The RTA framework marries thebenefits of both LLMs and traditional recommender systems (RecSys):understanding complex queries as LLMs do; while efficiently controlling therecommended item distributions in conversational recommendations as traditionalRecSys do. Our framework demonstrates improved accuracy metrics across threedifferent conversational recommendation datasets and two adaptation settings</description><author>Zhankui He, Zhouhang Xie, Harald Steck, Dawen Liang, Rahul Jha, Nathan Kallus, Julian McAuley</author><pubDate>Mon, 20 May 2024 16:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12119v1</guid></item><item><title>A New Cross-Space Total Variation Regularization Model for Color Image Restoration with Quaternion Blur Operator</title><link>http://arxiv.org/abs/2405.12114v1</link><description>The cross-channel deblurring problem in color image processing is difficultto solve due to the complex coupling and structural blurring of color pixels.Until now, there are few efficient algorithms that can reduce color infectionin deblurring process. To solve this challenging problem, we present a novelcross-space total variation (CSTV) regularization model for color imagedeblurring by introducing a quaternion blur operator and a cross-color spaceregularization functional. The existence and uniqueness of the solution isproved and a new L-curve method is proposed to find a sweet balance ofregularization functionals on different color spaces. The Euler-Lagrange equation is derived to show that CSTV has taken intoaccount the coupling of all color channels and the local smoothing within eachcolor channel. A quaternion operator splitting method is firstly proposed toenhance the ability of color infection reduction of the CSTV regularizationmodel. This strategy also applies to the well-known color deblurring models.Numerical experiments on color image databases illustrate the efficiency andmanoeuvrability of the new model and algorithms. The color images restored bythem successfully maintain the color and spatial information and are of higherquality in terms of PSNR, SSIM, MSE and CIEde2000 than the restorations ofthe-state-of-the-art methods.</description><author>Zhigang Jia, Yuelian Xiang, Meixiang Zhao, Tingting Wu, Michael K. Ng</author><pubDate>Mon, 20 May 2024 16:29:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12114v1</guid></item><item><title>CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization</title><link>http://arxiv.org/abs/2405.12110v1</link><description>3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3DGaussians to represent a scene. With sparse training views, 3DGS easily suffersfrom overfitting, negatively impacting the reconstruction quality. This paperintroduces a new co-regularization perspective for improving sparse-view 3DGS.When training two 3D Gaussian radiance fields with the same sparse views of ascene, we observe that the two radiance fields exhibit \textit{pointdisagreement} and \textit{rendering disagreement} that can unsupervisedlypredict reconstruction quality, stemming from the sampling implementation indensification. We further quantify the point disagreement and renderingdisagreement by evaluating the registration between Gaussians' pointrepresentations and calculating differences in their rendered pixels. Theempirical study demonstrates the negative correlation between the twodisagreements and accurate reconstruction, which allows us to identifyinaccurate reconstruction without accessing ground-truth information. Based onthe study, we propose CoR-GS, which identifies and suppresses inaccuratereconstruction based on the two disagreements: (\romannumeral1) Co-pruningconsiders Gaussians that exhibit high point disagreement in inaccuratepositions and prunes them. (\romannumeral2) Pseudo-view co-regularizationconsiders pixels that exhibit high rendering disagreement are inaccuratelyrendered and suppress the disagreement. Results on LLFF, Mip-NeRF360, DTU, andBlender demonstrate that CoR-GS effectively regularizes the scene geometry,reconstructs the compact representations, and achieves state-of-the-art novelview synthesis quality under sparse training views.</description><author>Jiawei Zhang, Jiahe Li, Xiaohan Yu, Lei Huang, Lin Gu, Jin Zheng, Xiao Bai</author><pubDate>Mon, 20 May 2024 16:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12110v1</guid></item><item><title>Linguistic Structure from a Bottleneck on Sequential Information Processing</title><link>http://arxiv.org/abs/2405.12109v1</link><description>Human language is a unique form of communication in the natural world,distinguished by its structured nature. Most fundamentally, it is systematic,meaning that signals can be broken down into component parts that areindividually meaningful -- roughly, words -- which are combined in a regularway to form sentences. Furthermore, the way in which these parts are combinedmaintains a kind of locality: words are usually concatenated together, and theyform contiguous phrases, keeping related parts of sentences close to eachother. We address the challenge of understanding how these basic properties oflanguage arise from broader principles of efficient communication underinformation processing constraints. Here we show that natural-language-likesystematicity arises from minimization of excess entropy, a measure ofstatistical complexity that represents the minimum amount of informationnecessary for predicting the future of a sequence based on its past. Insimulations, we show that codes that minimize excess entropy factorize theirsource distributions into approximately independent components, and thenexpress those components systematically and locally. Next, in a series ofmassively cross-linguistic corpus studies, we show that human languages arestructured to have low excess entropy at the level of phonology, morphology,syntax, and semantics. Our result suggests that human language performs asequential generalization of Independent Components Analysis on the statisticaldistribution over meanings that need to be expressed. It establishes a linkbetween the statistical and algebraic structure of human language, andreinforces the idea that the structure of human language may have evolved tominimize cognitive load while maximizing communicative expressiveness.</description><author>Richard Futrell, Michael Hahn</author><pubDate>Mon, 20 May 2024 16:25:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12109v1</guid></item><item><title>Imp: Highly Capable Large Multimodal Models for Mobile Devices</title><link>http://arxiv.org/abs/2405.12107v1</link><description>By harnessing the capabilities of large language models (LLMs), recent largemultimodal models (LMMs) have shown remarkable versatility in open-worldmultimodal understanding. Nevertheless, they are usually parameter-heavy andcomputation-intensive, thus hindering their applicability inresource-constrained scenarios. To this end, several lightweight LMMs have beenproposed successively to maximize the capabilities under constrained scale(e.g., 3B). Despite the encouraging results achieved by these methods, most ofthem only focus on one or two aspects of the design space, and the key designchoices that influence model capability have not yet been thoroughlyinvestigated. In this paper, we conduct a systematic study for lightweight LMMsfrom the aspects of model architecture, training strategy, and training data.Based on our findings, we obtain Imp -- a family of highly capable LMMs at the2B-4B scales. Notably, our Imp-3B model steadily outperforms all the existinglightweight LMMs of similar size, and even surpasses the state-of-the-art LMMsat the 13B scale. With low-bit quantization and resolution reductiontechniques, our Imp model can be deployed on a Qualcomm Snapdragon 8Gen3 mobilechip with a high inference speed of about 13 tokens/s.</description><author>Zhenwei Shao, Zhou Yu, Jun Yu, Xuecheng Ouyang, Lihao Zheng, Zhenbiao Gai, Mingyang Wang, Jiajun Ding</author><pubDate>Mon, 20 May 2024 16:23:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12107v1</guid></item><item><title>Sheet Music Transformer ++: End-to-End Full-Page Optical Music Recognition for Pianoform Sheet Music</title><link>http://arxiv.org/abs/2405.12105v1</link><description>Optical Music Recognition is a field that has progressed significantly,bringing accurate systems that transcribe effectively music scores into digitalformats. Despite this, there are still several limitations that hinder OMR fromachieving its full potential. Specifically, state of the art OMR still dependson multi-stage pipelines for performing full-page transcription, as well as ithas only been demonstrated in monophonic cases, leaving behind very relevantengravings. In this work, we present the Sheet Music Transformer++, anend-to-end model that is able to transcribe full-page polyphonic music scoreswithout the need of a previous Layout Analysis step. This is done thanks to anextensive curriculum learning-based pretraining with synthetic data generation.We conduct several experiments on a full-page extension of a public polyphonictranscription dataset. The experimental outcomes confirm that the model iscompetent at transcribing full-page pianoform scores, marking a noteworthymilestone in end-to-end OMR transcription.</description><author>Antonio Ríos-Vila, Jorge Calvo-Zaragoza, David Rizo, Thierry Paquet</author><pubDate>Mon, 20 May 2024 16:21:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12105v1</guid></item><item><title>EduceLab-Scrolls: Verifiable Recovery of Text from Herculaneum Papyri using X-ray CT</title><link>http://arxiv.org/abs/2304.02084v4</link><description>We present a complete software pipeline for revealing the hidden texts of theHerculaneum papyri using X-ray CT images. This enhanced virtual unwrappingpipeline combines machine learning with a novel geometric framework linking 3Dand 2D images. We also present EduceLab-Scrolls, a comprehensive open datasetrepresenting two decades of research effort on this problem. EduceLab-Scrollscontains a set of volumetric X-ray CT images of both small fragments andintact, rolled scrolls. The dataset also contains 2D image labels that are usedin the supervised training of an ink detection model. Labeling is enabled byaligning spectral photography of scroll fragments with X-ray CT images of thesame fragments, thus creating a machine-learnable mapping between image spacesand modalities. This alignment permits supervised learning for the detection of"invisible" carbon ink in X-ray CT, a task that is "impossible" even for humanexpert labelers. To our knowledge, this is the first aligned dataset of itskind and is the largest dataset ever released in the heritage domain. Ourmethod is capable of revealing accurate lines of text on scroll fragments withknown ground truth. Revealed text is verified using visual confirmation,quantitative image metrics, and scholarly review. EduceLab-Scrolls has alsoenabled the discovery, for the first time, of hidden texts from the Herculaneumpapyri, which we present here. We anticipate that the EduceLab-Scrolls datasetwill generate more textual discovery as research continues.</description><author>Stephen Parsons, C. Seth Parker, Christy Chapman, Mami Hayashida, W. Brent Seales</author><pubDate>Mon, 20 May 2024 16:20:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02084v4</guid></item><item><title>DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction</title><link>http://arxiv.org/abs/2405.12100v1</link><description>Math world problems correction(MWPC) is a novel task dedicated to rectifyingreasoning errors in the process of solving mathematical problems. In thispaper, leveraging the advancements in large language models (LLMs), we addresstwo key objectives:(1) Distinguishing between mathematical reasoning and errorcorrection; (2) Exploring strategies to enhance the error correctioncapabilities of LLMs in mathematics to solve MWPC task. We noticed that, inreal-time education,assisting students in recognizing their mistakes is morecrucial than simply providing correct answers. However, current research tendsto prioritize obtaining accurate solutions to math problems rather thancorrecting potentially incorrect ones. Therefore, we modify the researchparadigm, demonstrating that improving mathematical reasoning abilities doesnot equate to mastery in error correction. Meanwhile, we propose a novel methodcalled diagnostic-oriented promping(DOP) aimed at facilitating LLMs to excel inerror correction. In experiments, DOP has shown outstanding performance,highlighting its significant impact. We argue that in mathematical education,the demand for outstanding correctors surpasses that for proficient reasoners.Codes and data are available onhttps://github.com/ChenhaoEcnuCS/Reason-Correct.</description><author>Hao Chen, Biaojie Zeng, Xin Lin, Liang He, Aimin Zhou</author><pubDate>Mon, 20 May 2024 16:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12100v1</guid></item><item><title>PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models</title><link>http://arxiv.org/abs/2405.09373v2</link><description>Recent advances in large language models (LLMs) have led to their extensiveglobal deployment, and ensuring their safety calls for comprehensive andmultilingual toxicity evaluations. However, existing toxicity benchmarks areoverwhelmingly focused on English, posing serious risks to deploying LLMs inother languages. We address this by introducing PolygloToxicityPrompts (PTP),the first large-scale multilingual toxicity evaluation benchmark of 425Knaturally occurring prompts spanning 17 languages. We overcome the scarcity ofnaturally occurring toxicity in web-text and ensure coverage across languageswith varying resources by automatically scraping over 100M web-text documents.Using PTP, we investigate research questions to study the impact of model size,prompt language, and instruction and preference-tuning methods on toxicity bybenchmarking over 60 LLMs. Notably, we find that toxicity increases as languageresources decrease or model size increases. Although instruction- andpreference-tuning reduce toxicity, the choice of preference-tuning method doesnot have any significant impact. Our findings shed light on crucialshortcomings of LLM safeguarding and highlight areas for future research.</description><author>Devansh Jain, Priyanshu Kumar, Samuel Gehman, Xuhui Zhou, Thomas Hartvigsen, Maarten Sap</author><pubDate>Mon, 20 May 2024 16:07:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09373v2</guid></item><item><title>PATE: Proximity-Aware Time series anomaly Evaluation</title><link>http://arxiv.org/abs/2405.12096v1</link><description>Evaluating anomaly detection algorithms in time series data is critical asinaccuracies can lead to flawed decision-making in various domains wherereal-time analytics and data-driven strategies are essential. Traditionalperformance metrics assume iid data and fail to capture the complex temporaldynamics and specific characteristics of time series anomalies, such as earlyand delayed detections. We introduce Proximity-Aware Time series anomalyEvaluation (PATE), a novel evaluation metric that incorporates the temporalrelationship between prediction and anomaly intervals. PATE usesproximity-based weighting considering buffer zones around anomaly intervals,enabling a more detailed and informed assessment of a detection. Using theseweights, PATE computes a weighted version of the area under the Precision andRecall curve. Our experiments with synthetic and real-world datasets show thesuperiority of PATE in providing more sensible and accurate evaluations thanother evaluation metrics. We also tested several state-of-the-art anomalydetectors across various benchmark datasets using the PATE evaluation scheme.The results show that a common metric like Point-Adjusted F1 Score fails tocharacterize the detection performances well, and that PATE is able to providea more fair model comparison. By introducing PATE, we redefine theunderstanding of model efficacy that steers future studies toward developingmore effective and accurate detection models.</description><author>Ramin Ghorbani, Marcel J. T. Reinders, David M. J. Tax</author><pubDate>Mon, 20 May 2024 16:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12096v1</guid></item><item><title>DIRECT: Deep Active Learning under Imbalance and Label Noise</title><link>http://arxiv.org/abs/2312.09196v3</link><description>Class imbalance is a prevalent issue in real world machine learningapplications, often leading to poor performance in rare and minority classes.With an abundance of wild unlabeled data, active learning is perhaps the mosteffective technique in solving the problem at its root -- collecting a morebalanced and informative set of labeled examples during annotation. Label noiseis another common issue in data annotation jobs, which is especiallychallenging for active learning methods. In this work, we conduct the firststudy of active learning under both class imbalance and label noise. We proposea novel algorithm that robustly identifies the class separation threshold andannotates the most uncertain examples that are closest from it. Through a novelreduction to one-dimensional active learning, our algorithm DIRECT is able toleverage the classic active learning literature to address issues such as batchlabeling and tolerance towards label noise. We present extensive experiments onimbalanced datasets with and without label noise. Our results demonstrate thatDIRECT can save more than 60% of the annotation budget compared to state-of-artactive learning algorithms and more than 80% of annotation budget compared torandom sampling.</description><author>Shyam Nuggehalli, Jifan Zhang, Lalit Jain, Robert Nowak</author><pubDate>Mon, 20 May 2024 16:06:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09196v3</guid></item><item><title>Is Mamba Compatible with Trajectory Optimization in Offline Reinforcement Learning?</title><link>http://arxiv.org/abs/2405.12094v1</link><description>Transformer-based trajectory optimization methods have demonstratedexceptional performance in offline Reinforcement Learning (offline RL), yet itposes challenges due to substantial parameter size and limited scalability,which is particularly critical in sequential decision-making scenarios whereresources are constrained such as in robots and drones with limitedcomputational power. Mamba, a promising new linear-time sequence model, offersperformance on par with transformers while delivering substantially fewerparameters on long sequences. As it remains unclear whether Mamba is compatiblewith trajectory optimization, this work aims to conduct comprehensiveexperiments to explore the potential of Decision Mamba in offline RL (dubbedDeMa) from the aspect of data structures and network architectures with thefollowing insights: (1) Long sequences impose a significant computationalburden without contributing to performance improvements due to the fact thatDeMa's focus on sequences diminishes approximately exponentially. Consequently,we introduce a Transformer-like DeMa as opposed to an RNN-like DeMa. (2) Forthe components of DeMa, we identify that the hidden attention mechanism is keyto its success, which can also work well with other residual structures anddoes not require position embedding. Extensive evaluations from eight Atarigames demonstrate that our specially designed DeMa is compatible withtrajectory optimization and surpasses previous state-of-the-art methods,outdoing Decision Transformer (DT) by 80\% with 30\% fewer parameters, andexceeds DT in MuJoCo with only a quarter of the parameters.</description><author>Yang Dai, Oubo Ma, Longfei Zhang, Xingxing Liang, Shengchao Hu, Mengzhu Wang, Shouling Ji, Jincai Huang, Li Shen</author><pubDate>Mon, 20 May 2024 16:05:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12094v1</guid></item><item><title>Channel Balance Interpolation in the Lightning Network via Machine Learning</title><link>http://arxiv.org/abs/2405.12087v1</link><description>The Bitcoin Lightning Network is a Layer 2 payment protocol that addressesBitcoin's scalability by facilitating quick and cost effective transactionsthrough payment channels. This research explores the feasibility of usingmachine learning models to interpolate channel balances within the network,which can be used for optimizing the network's pathfinding algorithms. Whilethere has been much exploration in balance probing and multipath paymentprotocols, predicting channel balances using solely node and channel featuresremains an uncharted area. This paper evaluates the performance of severalmachine learning models against two heuristic baselines and investigates thepredictive capabilities of various features. Our model performs favorably inexperimental evaluation, outperforming by 10% against an equal split baselinewhere both edges are assigned half of the channel capacity.</description><author>Vincent, Emanuele Rossi, Vikash Singh</author><pubDate>Mon, 20 May 2024 15:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12087v1</guid></item><item><title>Noise-tolerant learnability of shallow quantum circuits from statistics and the cost of quantum pseudorandomness</title><link>http://arxiv.org/abs/2405.12085v1</link><description>This work studies the learnability of unknown quantum circuits in the nearterm. We prove the natural robustness of quantum statistical queries forlearning quantum processes and provide an efficient way to benchmark variousclasses of noise from statistics, which gives us a powerful framework fordeveloping noise-tolerant algorithms. We adapt a learning algorithm forconstant-depth quantum circuits to the quantum statistical query setting with asmall overhead in the query complexity. We prove average-case lower bounds forlearning random quantum circuits of logarithmic and higher depths withindiamond distance with statistical queries. Additionally, we show the hardnessof the quantum threshold search problem from quantum statistical queries anddiscuss its implications for the learnability of shallow quantum circuits.Finally, we prove that pseudorandom unitaries (PRUs) cannot be constructedusing circuits of constant depth by constructing an efficient distinguisher andproving a new variation of the quantum no-free lunch theorem.</description><author>Chirag Wadhwa, Mina Doosti</author><pubDate>Mon, 20 May 2024 15:55:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12085v1</guid></item><item><title>Distributional Semantics, Holism, and the Instability of Meaning</title><link>http://arxiv.org/abs/2405.12084v1</link><description>Current language models are built on the so-called distributional semanticapproach to linguistic meaning that has the distributional hypothesis at itscore. The distributional hypothesis involves a holistic conception of wordmeaning: the meaning of a word depends upon its relations to other words in themodel. A standard objection to meaning holism is the charge of instability: anychange in the meaning properties of a linguistic system (a human speaker, forexample) would lead to many changes or possibly a complete change in the entiresystem. When the systems in question are trying to communicate with each other,it has been argued that instability of this kind makes communication impossible(Fodor and Lepore 1992, 1996, 1999). In this article, we examine whether theinstability objection poses a problem for distributional models of meaning.First, we distinguish between distinct forms of instability that these modelscould exhibit, and we argue that only one such form is relevant forunderstanding the relation between instability and communication: what we calldifferential instability. Differential instability is variation in the relativedistances between points in a space, rather than variation in the absoluteposition of those points. We distinguish differential and absolute instabilityby constructing two of our own models, a toy model constructed from the text oftwo novels, and a more sophisticated model constructed using the Word2vecalgorithm from a combination of Wikipedia and SEP articles. We demonstrate thetwo forms of instability by showing how these models change as the corpora theyare constructed from increase in size.</description><author>Jumbly Grindrod, J. D. Porter, Nat Hansen</author><pubDate>Mon, 20 May 2024 15:53:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12084v1</guid></item><item><title>API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs</title><link>http://arxiv.org/abs/2402.15491v2</link><description>There is a growing need for Large Language Models (LLMs) to effectively usetools and external Application Programming Interfaces (APIs) to plan andcomplete tasks. As such, there is tremendous interest in methods that canacquire sufficient quantities of train and test data that involve calls totools / APIs. Two lines of research have emerged as the predominant strategiesfor addressing this challenge. The first has focused on synthetic datageneration techniques, while the second has involved curating task-adjacentdatasets which can be transformed into API / Tool-based tasks. In this paper,we focus on the task of identifying, curating, and transforming existingdatasets and, in turn, introduce API-BLEND, a large corpora for training andsystematic testing of tool-augmented LLMs. The datasets mimic real-worldscenarios involving API-tasks such as API / tool detection, slot filling, andsequencing of the detected APIs. We demonstrate the utility of the API-BLENDdataset for both training and benchmarking purposes.</description><author>Kinjal Basu, Ibrahim Abdelaziz, Subhajit Chaudhury, Soham Dan, Maxwell Crouse, Asim Munawar, Sadhana Kumaravel, Vinod Muthusamy, Pavan Kapanipathi, Luis A. Lastras</author><pubDate>Mon, 20 May 2024 15:52:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15491v2</guid></item><item><title>Selective Annotation via Data Allocation: These Data Should Be Triaged to Experts for Annotation Rather Than the Model</title><link>http://arxiv.org/abs/2405.12081v1</link><description>To obtain high-quality annotations under limited budget, semi-automaticannotation methods are commonly used, where a portion of the data is annotatedby experts and a model is then trained to complete the annotations for theremaining data. However, these methods mainly focus on selecting informativedata for expert annotations to improve the model predictive ability (i.e.,triage-to-human data), while the rest of the data is indiscriminately assignedto model annotation (i.e., triage-to-model data). This may lead toinefficiencies in budget allocation for annotations, as easy data that themodel could accurately annotate may be unnecessarily assigned to the expert,and hard data may be misclassified by the model. As a result, the overallannotation quality may be compromised. To address this issue, we propose aselective annotation framework called SANT. It effectively takes advantage ofboth the triage-to-human and triage-to-model data through the proposederror-aware triage and bi-weighting mechanisms. As such, informative or harddata is assigned to the expert for annotation, while easy data is handled bythe model. Experimental results show that SANT consistently outperforms otherbaselines, leading to higher-quality annotation through its proper allocationof data to both expert and model workers. We provide pioneering work on dataannotation within budget constraints, establishing a landmark for futuretriage-based annotation studies.</description><author>Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Ido Dagan</author><pubDate>Mon, 20 May 2024 15:52:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12081v1</guid></item><item><title>A Framework for Inference Inspired by Human Memory Mechanisms</title><link>http://arxiv.org/abs/2310.09297v2</link><description>How humans and machines make sense of current inputs for relation reasoningand question-answering while putting the perceived information into context ofour past memories, has been a challenging conundrum in cognitive science andartificial intelligence. Inspired by human brain's memory system and cognitivearchitectures, we propose a PMI framework that consists of perception, memoryand inference components. Notably, the memory module comprises working andlong-term memory, with the latter endowed with a higher-order structure toretain extensive and complex relational knowledge and experience. Through adifferentiable competitive write access, current perceptions update workingmemory, which is later merged with long-term memory via outer productassociations, reducing information conflicts and averting memory overflow. Inthe inference module, relevant information is retrieved from two separatememory origins and associatively integrated to attain a more comprehensive andprecise interpretation of current perceptions. We exploratively apply our PMIto improve prevailing Transformers and CNN models on question-answering taskslike bAbI-20k and Sort-of-CLEVR datasets, as well as detecting equilateraltriangles, language modeling and image classification tasks, and in each case,our PMI enhancements consistently outshine their original counterpartssignificantly. Visualization analyses reveal that relational memoryconsolidation, along with the interaction and integration of information fromdiverse memory sources, substantially contributes to the model effectiveness oninference tasks.</description><author>Xiangyu Zeng, Jie Lin, Piao Hu, Ruizheng Huang, Zhicheng Zhang</author><pubDate>Mon, 20 May 2024 15:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09297v2</guid></item><item><title>Distributed agency in second language learning and teaching through generative AI</title><link>http://arxiv.org/abs/2403.20216v2</link><description>Generative AI offers significant opportunities for language learning. Toolslike ChatGPT can provide informal second language practice through chats inwritten or voice forms, with the learner specifying through promptsconversational parameters such as proficiency level, language register, anddiscussion topics. AI can be instructed to give corrective feedback, createpractice exercises, or develop an extended study plan. Instructors can use AIto build learning and assessment materials in a variety of media. AI is likelyto make immersive technologies more powerful and versatile, moving away fromscripted interactions. For both learners and teachers, it is important tounderstand the limitations of AI systems that arise from their purelystatistical model of human language, which limits their ability to deal withnuanced social and cultural aspects of language use. Additionally, there areethical concerns over how AI systems are created as well as practicalconstraints in their use, especially for less privileged populations. The powerand versatility of AI tools are likely to turn them into valuable and constantcompanions in many peoples lives (akin to smartphones), creating a closeconnection that goes beyond simple tool use. Ecological theories such associomaterialism are helpful in examining the shared agency that developsthrough close user-AI interactions, as are the perspectives on human-objectrelations from Indigenous cultures.</description><author>Robert Godwin-Jones</author><pubDate>Mon, 20 May 2024 15:43:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20216v2</guid></item><item><title>Triple-CFN: Restructuring Concept Spaces for Enhancing Abstract Reasoning Process</title><link>http://arxiv.org/abs/2403.03190v8</link><description>Abstract reasoning poses significant challenges to artificial intelligencealgorithms, demanding a cognitive ability beyond that required for perceptualtasks. In this study, we introduce the Cross-Feature Network (CFN), a novelframework designed to separately extract concepts and features from images.This framework utilizes the responses of features to concepts asrepresentations for reasoning, particularly in addressing the Bongard-Logoproblem. By integrating an Expectation-Maximization process between theextracted concepts and features within the CFN, we have achieved notableresults, albeit with certain limitations. To overcome these limitations, wepropose the Triple-CFN, an efficient model that maximizes feature extractionfrom images and demonstrates effectiveness in both the Bongard-Logo and Raven'sProgressive Matrices (RPM) problems. Furthermore, we introduce Meta Triple-CFN,an advanced version of Triple-CFN, which explicitly constructs a concept spacetailored for RPM problems. This ensures high accuracy of reasoning andinterpretability of the concepts involved. Overall, this work exploresinnovative network designs for abstract reasoning, thereby advancing thefrontiers of machine intelligence.</description><author>Ruizhuo Song, Beiming Yuan</author><pubDate>Mon, 20 May 2024 15:40:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03190v8</guid></item><item><title>AutoSoccerPose: Automated 3D posture Analysis of Soccer Shot Movements</title><link>http://arxiv.org/abs/2405.12070v1</link><description>Image understanding is a foundational task in computer vision, with recentapplications emerging in soccer posture analysis. However, existing publiclyavailable datasets lack comprehensive information, notably in the form ofposture sequences and 2D pose annotations. Moreover, current analysis modelsoften rely on interpretable linear models (e.g., PCA and regression), limitingtheir capacity to capture non-linear spatiotemporal relationships in complexand diverse scenarios. To address these gaps, we introduce the 3D Shot Posture(3DSP) dataset in soccer broadcast videos, which represents the most extensivesports image dataset with 2D pose annotations to our knowledge. Additionally,we present the 3DSP-GRAE (Graph Recurrent AutoEncoder) model, a non-linearapproach for embedding pose sequences. Furthermore, we propose AutoSoccerPose,a pipeline aimed at semi-automating 2D and 3D pose estimation and postureanalysis. While achieving full automation proved challenging, we provide afoundational baseline, extending its utility beyond the scope of annotateddata. We validate AutoSoccerPose on SoccerNet and 3DSP datasets, and presentposture analysis results based on 3DSP. The dataset, code, and models areavailable at: https://github.com/calvinyeungck/3D-Shot-Posture-Dataset.</description><author>Calvin Yeung, Kenjiro Ide, Keisuke Fujii</author><pubDate>Mon, 20 May 2024 15:40:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12070v1</guid></item><item><title>Special Characters Attack: Toward Scalable Training Data Extraction From Large Language Models</title><link>http://arxiv.org/abs/2405.05990v2</link><description>Large language models (LLMs) have achieved remarkable performance on a widerange of tasks. However, recent studies have shown that LLMs can memorizetraining data and simple repeated tokens can trick the model to leak the data.In this paper, we take a step further and show that certain special charactersor their combinations with English letters are stronger memory triggers,leading to more severe data leakage. The intuition is that, since LLMs aretrained with massive data that contains a substantial amount of specialcharacters (e.g. structural symbols {, } of JSON files, and @, # in emails andonline posts), the model may memorize the co-occurrence between these specialcharacters and the raw texts. This motivates us to propose a simple buteffective Special Characters Attack (SCA) to induce training data leakage. Ourexperiments verify the high effectiveness of SCA against state-of-the-art LLMs:they can leak diverse training data, such as code corpus, web pages, andpersonally identifiable information, and sometimes generate non-stop outputs asa byproduct. We further show that the composition of the training data corpuscan be revealed by inspecting the leaked data -- one crucial piece ofinformation for pre-training high-performance LLMs. Our work can helpunderstand the sensitivity of LLMs to special characters and identify potentialareas for improvement.</description><author>Yang Bai, Ge Pei, Jindong Gu, Yong Yang, Xingjun Ma</author><pubDate>Mon, 20 May 2024 15:40:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05990v2</guid></item><item><title>Gaussian Head &amp; Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping</title><link>http://arxiv.org/abs/2405.12069v1</link><description>By equipping the most recent 3D Gaussian Splatting representation with head3D morphable models (3DMM), existing methods manage to create head avatars withhigh fidelity. However, most existing methods only reconstruct a head withoutthe body, substantially limiting their application scenarios. We found thatnaively applying Gaussians to model the clothed chest and shoulders tends toresult in blurry reconstruction and noisy floaters under novel poses. This isbecause of the fundamental limitation of Gaussians and point clouds -- eachGaussian or point can only have a single directional radiance without spatialvariance, therefore an unnecessarily large number of them is required torepresent complicated spatially varying texture, even for simple geometry. Incontrast, we propose to model the body part with a neural texture that consistsof coarse and pose-dependent fine colors. To properly render the body texturefor each view and pose without accurate geometry nor UV mapping, we optimizeanother sparse set of Gaussians as anchors that constrain the neural warpingfield that maps image plane coordinates to the texture space. We demonstratethat Gaussian Head &amp; Shoulders can fit the high-frequency details on theclothed upper body with high fidelity and potentially improve the accuracy andfidelity of the head region. We evaluate our method with casual phone-capturedand internet videos and show our method archives superior reconstructionquality and robustness in both self and cross reenactment tasks. To fullyutilize the efficient rendering speed of Gaussian splatting, we additionallypropose an accelerated inference method of our trained model withoutMulti-Layer Perceptron (MLP) queries and reach a stable rendering speed ofaround 130 FPS for any subjects.</description><author>Tianhao Wu, Jing Yang, Zhilin Guo, Jingyi Wan, Fangcheng Zhong, Cengiz Oztireli</author><pubDate>Mon, 20 May 2024 15:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12069v1</guid></item><item><title>DemOpts: Fairness corrections in COVID-19 case prediction models</title><link>http://arxiv.org/abs/2405.09483v2</link><description>COVID-19 forecasting models have been used to inform decision making aroundresource allocation and intervention decisions e.g., hospital beds orstay-at-home orders. State of the art deep learning models often use multimodaldata such as mobility or socio-demographic data to enhance COVID-19 caseprediction models. Nevertheless, related work has revealed under-reporting biasin COVID-19 cases as well as sampling bias in mobility data for certainminority racial and ethnic groups, which could in turn affect the fairness ofthe COVID-19 predictions along race labels. In this paper, we show that stateof the art deep learning models output mean prediction errors that aresignificantly different across racial and ethnic groups; and which could, inturn, support unfair policy decisions. We also propose a novel de-biasingmethod, DemOpts, to increase the fairness of deep learning based forecastingmodels trained on potentially biased datasets. Our results show that DemOptscan achieve better error parity that other state of the art de-biasingapproaches, thus effectively reducing the differences in the mean errordistributions across more racial and ethnic groups.</description><author>Naman Awasthi, Saad Abrar, Daniel Smolyak, Vanessa Frias-Martinez</author><pubDate>Mon, 20 May 2024 15:34:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09483v2</guid></item><item><title>CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models</title><link>http://arxiv.org/abs/2405.12063v1</link><description>Large language models (LLMs) are increasingly used to meet user informationneeds, but their effectiveness in dealing with user queries that containvarious types of ambiguity remains unknown, ultimately risking user trust andsatisfaction. To this end, we introduce CLAMBER, a benchmark for evaluatingLLMs using a well-organized taxonomy. Building upon the taxonomy, we construct~12K high-quality data to assess the strengths, weaknesses, and potential risksof various off-the-shelf LLMs. Our findings indicate the limited practicalutility of current LLMs in identifying and clarifying ambiguous user queries,even enhanced by chain-of-thought (CoT) and few-shot prompting. Thesetechniques may result in overconfidence in LLMs and yield only marginalenhancements in identifying ambiguity. Furthermore, current LLMs fall short ingenerating high-quality clarifying questions due to a lack of conflictresolution and inaccurate utilization of inherent knowledge. In this paper,CLAMBER presents a guidance and promotes further research on proactive andtrustworthy LLMs. Our dataset is available athttps://github.com/zt991211/CLAMBER</description><author>Tong Zhang, Peixin Qin, Yang Deng, Chen Huang, Wenqiang Lei, Junhong Liu, Dingnan Jin, Hongru Liang, Tat-Seng Chua</author><pubDate>Mon, 20 May 2024 15:34:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12063v1</guid></item><item><title>On the Communication Complexity of Decentralized Bilevel Optimization</title><link>http://arxiv.org/abs/2311.11342v3</link><description>Decentralized bilevel optimization has been actively studied in the past fewyears since it has widespread applications in machine learning. However,existing algorithms suffer from large communication complexity caused by theestimation of stochastic hypergradient, limiting their application toreal-world tasks. To address this issue, we develop a novel decentralizedstochastic bilevel gradient descent algorithm under the heterogeneous setting,which enjoys a small communication cost in each round and a small number ofcommunication rounds. As such, it can achieve a much better communicationcomplexity than existing algorithms without any strong assumptions regardingheterogeneity. To the best of our knowledge, this is the first stochasticalgorithm achieving these theoretical results under the heterogeneous setting.At last, the experimental results confirm the efficacy of our algorithm.</description><author>Yihan Zhang, My T. Thai, Jie Wu, Hongchang Gao</author><pubDate>Mon, 20 May 2024 15:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11342v3</guid></item><item><title>STYLE: Improving Domain Transferability of Asking Clarification Questions in Large Language Model Powered Conversational Agents</title><link>http://arxiv.org/abs/2405.12059v1</link><description>Equipping a conversational search engine with strategies regarding when toask clarification questions is becoming increasingly important across variousdomains. Attributing to the context understanding capability of LLMs and theiraccess to domain-specific sources of knowledge, LLM-based clarificationstrategies feature rapid transfer to various domains in a post-hoc manner.However, they still struggle to deliver promising performance on unseendomains, struggling to achieve effective domain transferability. We take thefirst step to investigate this issue and existing methods tend to produceone-size-fits-all strategies across diverse domains, limiting their searcheffectiveness. In response, we introduce a novel method, called Style, toachieve effective domain transferability. Our experimental results indicatethat Style bears strong domain transferability, resulting in an average searchperformance improvement of ~10% on four unseen domains.</description><author>Yue Chen, Chen Huang, Yang Deng, Wenqiang Lei, Dingnan Jin, Jia Liu, Tat-Seng Chua</author><pubDate>Mon, 20 May 2024 15:28:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12059v1</guid></item><item><title>Realistic Evaluation of Toxicity in Large Language Models</title><link>http://arxiv.org/abs/2405.10659v2</link><description>Large language models (LLMs) have become integral to our professionalworkflows and daily lives. Nevertheless, these machine companions of ours havea critical flaw: the huge amount of data which endows them with vast anddiverse knowledge, also exposes them to the inevitable toxicity and bias. Whilemost LLMs incorporate defense mechanisms to prevent the generation of harmfulcontent, these safeguards can be easily bypassed with minimal promptengineering. In this paper, we introduce the new Thoroughly Engineered Toxicity(TET) dataset, comprising manually crafted prompts designed to nullify theprotective layers of such models. Through extensive evaluations, we demonstratethe pivotal role of TET in providing a rigorous benchmark for evaluation oftoxicity awareness in several popular LLMs: it highlights the toxicity in theLLMs that might remain hidden when using normal prompts, thus revealing subtlerissues in their behavior.</description><author>Tinh Son Luong, Thanh-Thien Le, Linh Ngo Van, Thien Huu Nguyen</author><pubDate>Mon, 20 May 2024 15:27:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10659v2</guid></item><item><title>NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo</title><link>http://arxiv.org/abs/2405.12057v1</link><description>In this work we present a novel multi-view photometric stereo (PS) method.Like many works in 3D reconstruction we are leveraging neural shaperepresentations and learnt renderers. However, our work differs from thestate-of-the-art multi-view PS methods such as PS-NeRF or SuperNormal weexplicity leverage per-pixel intensity renderings rather than relying mainly onestimated normals. We model point light attenuation and explicitly raytrace cast shadows inorder to best approximate each points incoming radiance. This is used as inputto a fully neural material renderer that uses minimal prior assumptions and itis jointly optimised with the surface. Finally, estimated normal andsegmentation maps can also incorporated in order to maximise the surfaceaccuracy. Our method is among the first to outperform the classical approach ofDiLiGenT-MV and achieves average 0.2mm Chamfer distance for objects imaged atapprox 1.5m distance away with approximate 400x400 resolution. Moreover, weshow robustness to poor normals in low light count scenario, achieving 0.27mmChamfer distance when pixel rendering is used instead of estimated normals.</description><author>Fotios Logothetis, Ignas Budvytis, Roberto Cipolla</author><pubDate>Mon, 20 May 2024 15:26:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12057v1</guid></item><item><title>Unveiling factors influencing judgment variation in Sentiment Analysis with Natural Language Processing and Statistics</title><link>http://arxiv.org/abs/2405.12055v1</link><description>TripAdvisor reviews and comparable data sources play an important role inmany tasks in Natural Language Processing (NLP), providing a data basis for theidentification and classification of subjective judgments, such as hotel orrestaurant reviews, into positive or negative polarities. This study exploresthree important factors influencing variation in crowdsourced polarityjudgments, focusing on TripAdvisor reviews in Spanish. Three hypotheses aretested: the role of Part Of Speech (POS), the impact of sentiment words such as"tasty", and the influence of neutral words like "ok" on judgment variation.The study's methodology employs one-word titles, demonstrating their efficacyin studying polarity variation of words. Statistical tests on mean equality areperformed on word groups of our interest. The results of this study reveal thatadjectives in one-word titles tend to result in lower judgment variationcompared to other word types or POS. Sentiment words contribute to lowerjudgment variation as well, emphasizing the significance of sentiment words inresearch on polarity judgments, and neutral words are associated with higherjudgment variation as expected. However, these effects cannot be alwaysreproduced in longer titles, which suggests that longer titles do not representthe best data source for testing the ambiguity of single words due to theinfluence on word polarity by other words like negation in longer titles. Thisempirical investigation contributes valuable insights into the factorsinfluencing polarity variation of words, providing a foundation for NLPpractitioners that aim to capture and predict polarity judgments in Spanish andfor researchers that aim to understand factors influencing judgment variation.</description><author>Olga Kellert, Carlos Gómez-Rodríguez, Mahmud Uz Zaman</author><pubDate>Mon, 20 May 2024 15:24:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12055v1</guid></item><item><title>Parallelization of the K-Means Algorithm with Applications to Big Data Clustering</title><link>http://arxiv.org/abs/2405.12052v1</link><description>The K-Means clustering using LLoyd's algorithm is an iterative approach topartition the given dataset into K different clusters. The algorithm assignseach point to the cluster based on the following objective function \[\ \min \Sigma_{i=1}^{n}||x_i-\mu_{x_i}||^2\] The serial algorithm involvesiterative steps where we compute the distance of each datapoint from thecentroids and assign the datapoint to the nearest centroid. This approach isessentially known as the expectation-maximization step. Clustering involvesextensive computations to calculate distances at each iteration, whichincreases as the number of data points increases. This provides scope forparallelism. However, we must ensure that in a parallel process, each threadhas access to the updated centroid value and no racing condition exists on anycentroid values. We will compare two different approaches in this project. Thefirst approach is an OpenMP flat synchronous method where all processes are runin parallel, and we use synchronization to ensure safe updates of clusters. Thesecond approach we adopt is a GPU based parallelization approach using OpenACCwherein we will try to make use of GPU architecture to parallelize chunks ofthe algorithm to observe decreased computation time. We will analyze metricssuch as speed up, efficiency,time taken with varying data points, and number ofprocesses to compare the two approaches and understand the relative performanceimprovement we can get.</description><author>Ashish Srivastava, Mohammed Nawfal</author><pubDate>Mon, 20 May 2024 15:18:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12052v1</guid></item><item><title>Energy-Efficient Federated Edge Learning with Streaming Data: A Lyapunov Optimization Approach</title><link>http://arxiv.org/abs/2405.12046v1</link><description>Federated learning (FL) has received significant attention in recent yearsfor its advantages in efficient training of machine learning models acrossdistributed clients without disclosing user-sensitive data. Specifically, infederated edge learning (FEEL) systems, the time-varying nature of wirelesschannels introduces inevitable system dynamics in the communication process,thereby affecting training latency and energy consumption. In this work, wefurther consider a streaming data scenario where new training data samples arerandomly generated over time at edge devices. Our goal is to develop a dynamicscheduling and resource allocation algorithm to address the inherent randomnessin data arrivals and resource availability under long-term energy constraints.To achieve this, we formulate a stochastic network optimization problem and usethe Lyapunov drift-plus-penalty framework to obtain a dynamic resourcemanagement design. Our proposed algorithm makes adaptive decisions on devicescheduling, computational capacity adjustment, and allocation of bandwidth andtransmit power in every round. We provide convergence analysis for theconsidered setting with heterogeneous data and time-varying objectivefunctions, which supports the rationale behind our proposed scheduling design.The effectiveness of our scheme is verified through simulation results,demonstrating improved learning performance and energy efficiency as comparedto baseline schemes.</description><author>Chung-Hsuan Hu, Zheng Chen, Erik G. Larsson</author><pubDate>Mon, 20 May 2024 15:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12046v1</guid></item><item><title>Adaptive Extraction Network for Multivariate Long Sequence Time-Series Forecasting</title><link>http://arxiv.org/abs/2405.12038v1</link><description>Models employing CNN architecture have made significant progress inmultivariate long sequence time-series forecasting (MLSTF), particularly inmodeling local time series characteristics. However, during the MLSTF process,extracting the global time series patterns and understanding the correlationsamong different variables are highly significant. To address this challenge, weintroduce multi-resolution convolution and deformable convolution operations.By enlarging the receptive field using convolution kernels with differentdilation factors to capture temporal correlation information across differentresolutions, and adaptively adjusting the sampling positions through additionaloffset vectors, we enhance the network's ability to capture correlated featuresbetween variables. Building upon this, we propose ATVCNet, an adaptivetemporal-variable convolutional network designed to effectively model thelocal/global temporal dependencies and inter-variable dependencies ofmultivariate time series. Specifically, extracting and fusing time seriesfeatures at different resolutions, captures both local contextual informationand global patterns in the time series. The designed inter-variable featureadaptive extraction module captures the correlation among different variablesin the time series. We evaluated the performance of ATVCNet across eightreal-world datasets. The results indicate that ATVCNet achieved a performanceimprovement of approximately 63.4% over state-of-the-art MLSTF models.</description><author>Dandan Zhang, Yun Wang</author><pubDate>Mon, 20 May 2024 15:05:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12038v1</guid></item><item><title>Pre-Training on Large-Scale Generated Docking Conformations with HelixDock to Unlock the Potential of Protein-ligand Structure Prediction Models</title><link>http://arxiv.org/abs/2310.13913v3</link><description>Protein-ligand structure prediction is an essential task in drug discovery,predicting the binding interactions between small molecules (ligands) andtarget proteins (receptors). Recent advances have incorporated deep learningtechniques to improve the accuracy of protein-ligand structure prediction.Nevertheless, the experimental validation of docking conformations remainscostly, it raises concerns regarding the generalizability of these deeplearning-based methods due to the limited training data. In this work, we showthat by pre-training on a large-scale docking conformation generated bytraditional physics-based docking tools and then fine-tuning with a limited setof experimentally validated receptor-ligand complexes, we can obtain aprotein-ligand structure prediction model with outstanding performance.Specifically, this process involved the generation of 100 million dockingconformations for protein-ligand pairings, an endeavor consuming roughly 1million CPU core days. The proposed model, HelixDock, aims to acquire thephysical knowledge encapsulated by the physics-based docking tools during thepre-training phase. HelixDock has been rigorously benchmarked against bothphysics-based and deep learning-based baselines, demonstrating its exceptionalprecision and robust transferability in predicting binding confirmation. Inaddition, our investigation reveals the scaling laws governing pre-trainedprotein-ligand structure prediction models, indicating a consistent enhancementin performance with increases in model parameters and the volume ofpre-training data. Moreover, we applied HelixDock to several drugdiscovery-related tasks to validate its practical utility. HelixDockdemonstrates outstanding capabilities on both cross-docking and structure-basedvirtual screening benchmarks.</description><author>Lihang Liu, Shanzhuo Zhang, Donglong He, Xianbin Ye, Jingbo Zhou, Xiaonan Zhang, Yaoyao Jiang, Weiming Diao, Hang Yin, Hua Chai, Fan Wang, Jingzhou He, Liang Zheng, Yonghui Li, Xiaomin Fang</author><pubDate>Mon, 20 May 2024 15:05:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13913v3</guid></item><item><title>KG-RAG: Bridging the Gap Between Knowledge and Creativity</title><link>http://arxiv.org/abs/2405.12035v1</link><description>Ensuring factual accuracy while maintaining the creative capabilities ofLarge Language Model Agents (LMAs) poses significant challenges in thedevelopment of intelligent agent systems. LMAs face prevalent issues such asinformation hallucinations, catastrophic forgetting, and limitations inprocessing long contexts when dealing with knowledge-intensive tasks. Thispaper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)pipeline, a novel framework designed to enhance the knowledge capabilities ofLMAs by integrating structured Knowledge Graphs (KGs) with the functionalitiesof LLMs, thereby significantly reducing the reliance on the latent knowledge ofLLMs. The KG-RAG pipeline constructs a KG from unstructured text and thenperforms information retrieval over the newly created graph to perform KGQA(Knowledge Graph Question Answering). The retrieval methodology leverages anovel algorithm called Chain of Explorations (CoE) which benefits from LLMsreasoning to explore nodes and relationships within the KG sequentially.Preliminary experiments on the ComplexWebQuestions dataset demonstrate notableimprovements in the reduction of hallucinated content and suggest a promisingpath toward developing intelligent systems adept at handlingknowledge-intensive tasks.</description><author>Diego Sanmartin</author><pubDate>Mon, 20 May 2024 15:03:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12035v1</guid></item><item><title>Few Shot Semantic Segmentation: a review of methodologies, benchmarks, and open challenges</title><link>http://arxiv.org/abs/2304.05832v2</link><description>Semantic segmentation, vital for applications ranging from autonomous drivingto robotics, faces significant challenges in domains where collecting largeannotated datasets is difficult or prohibitively expensive. In such contexts,such as medicine and agriculture, the scarcity of training images hampersprogress. Introducing Few-Shot Semantic Segmentation, a novel task in computer vision,which aims at designing models capable of segmenting new semantic classes withonly a few examples. This paper consists of a comprehensive survey of Few-ShotSemantic Segmentation, tracing its evolution and exploring various modeldesigns, from the more popular conditional and prototypical networks to themore niche latent space optimization methods, presenting also the newopportunities offered by recent foundational models. Through a chronologicalnarrative, we dissect influential trends and methodologies, providing insightsinto their strengths and limitations. A temporal timeline offers a visualroadmap, marking key milestones in the field's progression. Complemented by quantitative analyses on benchmark datasets and qualitativeshowcases of seminal works, this survey equips readers with a deepunderstanding of the topic. By elucidating current challenges, state-of-the-artmodels, and prospects, we aid researchers and practitioners in navigating theintricacies of Few-Shot Semantic Segmentation and provide ground for futuredevelopment.</description><author>Nico Catalano, Matteo Matteucci</author><pubDate>Mon, 20 May 2024 14:55:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05832v2</guid></item><item><title>PatchAD: A Lightweight Patch-based MLP-Mixer for Time Series Anomaly Detection</title><link>http://arxiv.org/abs/2401.09793v4</link><description>Anomaly detection in time series analysis is a pivotal task, yet it poses thechallenge of discerning normal and abnormal patterns in label-deficientscenarios. While prior studies have largely employed reconstruction-basedapproaches, which limits the models' representational capacities. Moreover,existing deep learning-based methods are not sufficiently lightweight.Addressing these issues, we present PatchAD, our novel, highly efficientmultiscale patch-based MLP-Mixer architecture that utilizes contrastivelearning for representation extraction and anomaly detection. With its fourdistinct MLP Mixers and innovative dual project constraint module, PatchADmitigates potential model degradation and offers a lightweight solution,requiring only $3.2$MB. Its efficacy is demonstrated by state-of-the-artresults across $9$ datasets sourced from different application scenarios,outperforming over $30$ comparative algorithms. PatchAD significantly improvesthe classical F1 score by $50.5\%$, the Aff-F1 score by $7.8\%$, and the AUC by$10.0\%$. The code is publicly available.\url{https://github.com/EmorZz1G/PatchAD}</description><author>Zhijie Zhong, Zhiwen Yu, Yiyuan Yang, Weizheng Wang, Kaixiang Yang</author><pubDate>Mon, 20 May 2024 14:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09793v4</guid></item><item><title>Deepfake Text Detection in the Wild</title><link>http://arxiv.org/abs/2305.13242v2</link><description>Large language models (LLMs) have achieved human-level text generation,emphasizing the need for effective AI-generated text detection to mitigaterisks like the spread of fake news and plagiarism. Existing research has beenconstrained by evaluating detection methods on specific domains or particularlanguage models. In practical scenarios, however, the detector faces texts fromvarious domains or LLMs without knowing their sources. To this end, we build acomprehensive testbed by gathering texts from diverse human writings and textsgenerated by different LLMs. Empirical results show challenges indistinguishing machine-generated texts from human-authored ones across variousscenarios, especially out-of-distribution. These challenges are due to thedecreasing linguistic distinctions between the two sources. Despite challenges,the top-performing detector can identify 86.54% out-of-domain texts generatedby a new LLM, indicating the feasibility for application scenarios. We releaseour resources at https://github.com/yafuly/MAGE.</description><author>Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Zhilin Wang, Longyue Wang, Linyi Yang, Shuming Shi, Yue Zhang</author><pubDate>Mon, 20 May 2024 14:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13242v2</guid></item><item><title>Deep Learning-Based Object Pose Estimation: A Comprehensive Survey</title><link>http://arxiv.org/abs/2405.07801v2</link><description>Object pose estimation is a fundamental computer vision problem with broadapplications in augmented reality and robotics. Over the past decade, deeplearning models, due to their superior accuracy and robustness, haveincreasingly supplanted conventional algorithms reliant on engineered pointpair features. Nevertheless, several challenges persist in contemporarymethods, including their dependency on labeled training data, modelcompactness, robustness under challenging conditions, and their ability togeneralize to novel unseen objects. A recent survey discussing the progressmade on different aspects of this area, outstanding challenges, and promisingfuture directions, is missing. To fill this gap, we discuss the recent advancesin deep learning-based object pose estimation, covering all three formulationsof the problem, i.e., instance-level, category-level, and unseen object poseestimation. Our survey also covers multiple input data modalities,degrees-of-freedom of output poses, object properties, and downstream tasks,providing readers with a holistic understanding of this field. Additionally, itdiscusses training paradigms of different domains, inference modes, applicationareas, evaluation metrics, and benchmark datasets, as well as reports theperformance of current state-of-the-art methods on these benchmarks, therebyfacilitating readers in selecting the most suitable method for theirapplication. Finally, the survey identifies key challenges, reviews prevailingtrends along with their pros and cons, and identifies promising directions forfuture research. We also keep tracing the latest works athttps://github.com/CNJianLiu/Awesome-Object-Pose-Estimation.</description><author>Jian Liu, Wei Sun, Hui Yang, Zhiwen Zeng, Chongpei Liu, Jin Zheng, Xingyu Liu, Hossein Rahmani, Nicu Sebe, Ajmal Mian</author><pubDate>Mon, 20 May 2024 14:46:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07801v2</guid></item><item><title>A New Baseline Assumption of Integated Gradients Based on Shaply value</title><link>http://arxiv.org/abs/2310.04821v3</link><description>Efforts to decode deep neural networks (DNNs) often involve mapping theirpredictions back to the input features. Among these methods, IntegratedGradients (IG) has emerged as a significant technique. The selection ofappropriate baselines in IG is crucial for crafting meaningful and unbiasedexplanations of model predictions in diverse settings. The standard approach ofutilizing a single baseline, however, is frequently inadequate, prompting theneed for multiple baselines. Leveraging the natural link between IG and theAumann-Shapley Value, we provide a novel outlook on baseline design.Theoretically, we demonstrate that under certain assumptions, a collection ofbaselines aligns with the coalitions described by the Shapley Value. Buildingon this insight, we develop a new baseline method called Shapley IntegratedGradients (SIG), which uses proportional sampling to mirror the Shapley Valuecomputation process. Simulations conducted in GridWorld validate that SIGeffectively emulates the distribution of Shapley Values. Moreover, empiricaltests on various image processing tasks show that SIG surpasses traditional IGbaseline methods by offering more precise estimates of feature contributions,providing consistent explanations across different applications, and ensuringadaptability to diverse data types with negligible additional computationaldemand.</description><author>Shuyang Liu, Zixuan Chen, Ge Shi, Ji Wang, Changjie Fan, Yu Xiong, Runze Wu Yujing Hu, Ze Ji, Yang Gao</author><pubDate>Mon, 20 May 2024 14:44:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04821v3</guid></item><item><title>Can AI Relate: Testing Large Language Model Response for Mental Health Support</title><link>http://arxiv.org/abs/2405.12021v1</link><description>Large language models (LLMs) are already being piloted for clinical use inhospital systems like NYU Langone, Dana-Farber and the NHS. A proposeddeployment use case is psychotherapy, where a LLM-powered chatbot can treat apatient undergoing a mental health crisis. Deployment of LLMs for mental healthresponse could hypothetically broaden access to psychotherapy and provide newpossibilities for personalizing care. However, recent high-profile failures,like damaging dieting advice offered by the Tessa chatbot to patients witheating disorders, have led to doubt about their reliability in high-stakes andsafety-critical settings. In this work, we develop an evaluation framework for determining whether LLMresponse is a viable and ethical path forward for the automation of mentalhealth treatment. Using human evaluation with trained clinicians and automaticquality-of-care metrics grounded in psychology research, we compare theresponses provided by peer-to-peer responders to those provided by astate-of-the-art LLM. We show that LLMs like GPT-4 use implicit and explicit cues to infer patientdemographics like race. We then show that there are statistically significantdiscrepancies between patient subgroups: Responses to Black postersconsistently have lower empathy than for any other demographic group (2%-13%lower than the control group). Promisingly, we do find that the manner in whichresponses are generated significantly impacts the quality of the response. Weconclude by proposing safety guidelines for the potential deployment of LLMsfor mental health response.</description><author>Saadia Gabriel, Isha Puri, Xuhai Xu, Matteo Malgaroli, Marzyeh Ghassemi</author><pubDate>Mon, 20 May 2024 14:42:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12021v1</guid></item><item><title>Continuous Sign Language Recognition with Adapted Conformer via Unsupervised Pretraining</title><link>http://arxiv.org/abs/2405.12018v1</link><description>Conventional Deep Learning frameworks for continuous sign languagerecognition (CSLR) are comprised of a single or multi-modal feature extractor,a sequence-learning module, and a decoder for outputting the glosses. Thesequence learning module is a crucial part wherein transformers havedemonstrated their efficacy in the sequence-to-sequence tasks. Analyzing theresearch progress in the field of Natural Language Processing and SpeechRecognition, a rapid introduction of various transformer variants is observed.However, in the realm of sign language, experimentation in the sequencelearning component is limited. In this work, the state-of-the-art Conformermodel for Speech Recognition is adapted for CSLR and the proposed model istermed ConSignformer. This marks the first instance of employing Conformer fora vision-based task. ConSignformer has bimodal pipeline of CNN as featureextractor and Conformer for sequence learning. For improved context learning wealso introduce Cross-Modal Relative Attention (CMRA). By incorporating CMRAinto the model, it becomes more adept at learning and utilizing complexrelationships within the data. To further enhance the Conformer model,unsupervised pretraining called Regressional Feature Extraction is conducted ona curated sign language dataset. The pretrained Conformer is then fine-tunedfor the downstream recognition task. The experimental results confirm theeffectiveness of the adopted pretraining strategy and demonstrate how CMRAcontributes to the recognition process. Remarkably, leveraging aConformer-based backbone, our model achieves state-of-the-art performance onthe benchmark datasets: PHOENIX-2014 and PHOENIX-2014T.</description><author>Neena Aloysius, Geetha M, Prema Nedungadi</author><pubDate>Mon, 20 May 2024 14:40:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12018v1</guid></item><item><title>Strategy-Proof Auctions through Conformal Prediction</title><link>http://arxiv.org/abs/2405.12016v1</link><description>Auctions are key for maximizing sellers' revenue and ensuring truthfulbidding among buyers. Recently, an approach known as differentiable economicsbased on deep learning shows promise in learning optimal auction mechanisms formultiple items and participants. However, this approach has no guarantee ofstrategy-proofness at test time. Strategy-proofness is crucial as it ensuresthat buyers are incentivized to bid their true valuations, leading to optimaland fair auction outcomes without the risk of manipulation. Building onconformal prediction, we introduce a novel approach to achievestrategy-proofness with rigorous statistical guarantees. The key novelties ofour method are: (i) the formulation of a regret prediction model, used toquantify at test time violations of strategy-proofness; and (ii) an auctionacceptance rule that leverages the predicted regret to ensure that for a newauction, the data-driven mechanism meets the strategy-proofness requirementwith high probability (e.g., 99\%). Numerical experiments demonstrate thenecessity for rigorous guarantees, the validity of our theoretical results, andthe applicability of our proposed method.</description><author>Roy Maor Lotan, Inbal Talgam-Chohen, Yaniv Romano</author><pubDate>Mon, 20 May 2024 14:39:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12016v1</guid></item><item><title>GIST: Generated Inputs Sets Transferability in Deep Learning</title><link>http://arxiv.org/abs/2311.00801v3</link><description>To foster the verifiability and testability of Deep Neural Networks (DNN), anincreasing number of methods for test case generation techniques are beingdeveloped. When confronted with testing DNN models, the user can apply any existing testgeneration technique. However, it needs to do so for each technique and eachDNN model under test, which can be expensive. Therefore, a paradigm shift couldbenefit this testing process: rather than regenerating the test setindependently for each DNN model under test, we could transfer from existingDNN models. This paper introduces GIST (Generated Inputs Sets Transferability), a novelapproach for the efficient transfer of test sets. Given a property selected bya user (e.g., neurons covered, faults), GIST enables the selection of good testsets from the point of view of this property among available test sets. Thisallows the user to recover similar properties on the transferred test sets ashe would have obtained by generating the test set from scratch with a testcases generation technique. Experimental results show that GIST can selecteffective test sets for the given property to transfer. Moreover, GIST scalesbetter than reapplying test case generation techniques from scratch on DNNmodels under test.</description><author>Florian Tambon, Foutse Khomh, Giuliano Antoniol</author><pubDate>Mon, 20 May 2024 14:33:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00801v3</guid></item><item><title>Depth Reconstruction with Neural Signed Distance Fields in Structured Light Systems</title><link>http://arxiv.org/abs/2405.12006v1</link><description>We introduce a novel depth estimation technique for multi-frame structuredlight setups using neural implicit representations of 3D space. Our approachemploys a neural signed distance field (SDF), trained through self-superviseddifferentiable rendering. Unlike passive vision, where joint estimation ofradiance and geometry fields is necessary, we capitalize on known radiancefields from projected patterns in structured light systems. This enablesisolated optimization of the geometry field, ensuring convergence and networkefficacy with fixed device positioning. To enhance geometric fidelity, weincorporate an additional color loss based on object surfaces during training.Real-world experiments demonstrate our method's superiority in geometricperformance for few-shot scenarios, while achieving comparable results withincreased pattern availability.</description><author>Rukun Qiao, Hiroshi Kawasaki, Hongbin Zha</author><pubDate>Mon, 20 May 2024 14:24:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12006v1</guid></item><item><title>IT5: Text-to-text Pretraining for Italian Language Understanding and Generation</title><link>http://arxiv.org/abs/2203.03759v2</link><description>We introduce IT5, the first family of encoder-decoder transformer modelspretrained specifically on Italian. We document and perform a thorough cleaningprocedure for a large Italian corpus and use it to pretrain four IT5 modelsizes. We then introduce the ItaGen benchmark, which includes a broad range ofnatural language understanding and generation tasks for Italian, and use it toevaluate the performance of IT5 models and multilingual baselines. We findmonolingual IT5 models to provide the best scale-to-performance ratio acrosstested models, consistently outperforming their multilingual counterparts andsetting a new state-of-the-art for Italian language generation.</description><author>Gabriele Sarti, Malvina Nissim</author><pubDate>Mon, 20 May 2024 14:19:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.03759v2</guid></item><item><title>Mamba-in-Mamba: Centralized Mamba-Cross-Scan in Tokenized Mamba Model for Hyperspectral Image Classification</title><link>http://arxiv.org/abs/2405.12003v1</link><description>Hyperspectral image (HSI) classification is pivotal in the remote sensing(RS) field, particularly with the advancement of deep learning techniques.Sequential models, adapted from the natural language processing (NLP) fieldsuch as Recurrent Neural Networks (RNNs) and Transformers, have been tailoredto this task, offering a unique viewpoint. However, several challenges persist1) RNNs struggle with centric feature aggregation and are sensitive tointerfering pixels, 2) Transformers require significant computational resourcesand often underperform with limited HSI training samples, and 3) Currentscanning methods for converting images into sequence-data are simplistic andinefficient. In response, this study introduces the innovative Mamba-in-Mamba(MiM) architecture for HSI classification, the first attempt of deploying StateSpace Model (SSM) in this task. The MiM model includes 1) A novel centralizedMamba-Cross-Scan (MCS) mechanism for transforming images into sequence-data, 2)A Tokenized Mamba (T-Mamba) encoder that incorporates a Gaussian Decay Mask(GDM), a Semantic Token Learner (STL), and a Semantic Token Fuser (STF) forenhanced feature generation and concentration, and 3) A Weighted MCS Fusion(WMF) module coupled with a Multi-Scale Loss Design to improve decodingefficiency. Experimental results from three public HSI datasets with fixed anddisjoint training-testing samples demonstrate that our method outperformsexisting baselines and state-of-the-art approaches, highlighting its efficacyand potential in HSI applications.</description><author>Weilian Zhou, Sei-Ichiro Kamata, Haipeng Wang, Man-Sing Wong, Huiying, Hou</author><pubDate>Mon, 20 May 2024 14:19:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12003v1</guid></item><item><title>UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction</title><link>http://arxiv.org/abs/2402.11838v2</link><description>Urban spatio-temporal prediction is crucial for informed decision-making,such as transportation management, resource optimization, and urban planning.Although pretrained foundation models for natural languages have experiencedremarkable breakthroughs, wherein one general-purpose model can tackle multipletasks across various domains, urban spatio-temporal modeling lags behind.Existing approaches for urban prediction are usually tailored for specificspatio-temporal scenarios, requiring task-specific model designs and extensivein-domain training data. In this work, we propose a universal model, UniST, forurban spatio-temporal prediction. Drawing inspiration from large languagemodels, UniST achieves success through: (i) flexibility towards diversespatio-temporal data characteristics, (ii) effective generative pre-trainingwith elaborated masking strategies to capture complex spatio-temporalrelationships, (iii) spatio-temporal knowledge-guided prompts that align andleverage intrinsic and shared knowledge across scenarios. These designstogether unlock the potential of a one-for-all model for spatio-temporalprediction with powerful generalization capability. Extensive experiments on 15cities and 6 domains demonstrate the universality of UniST in advancingstate-of-the-art prediction performance, especially in few-shot and zero-shotscenarios.</description><author>Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, Yong Li</author><pubDate>Mon, 20 May 2024 14:18:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11838v2</guid></item><item><title>Scrutinize What We Ignore: Reining Task Representation Shift In Context-Based Offline Meta Reinforcement Learning</title><link>http://arxiv.org/abs/2405.12001v1</link><description>Offline meta reinforcement learning (OMRL) has emerged as a promisingapproach for interaction avoidance and strong generalization performance byleveraging pre-collected data and meta-learning techniques. Previouscontext-based approaches predominantly rely on the intuition that maximizingthe mutual information between the task and the task representation ($I(Z;M)$)can lead to performance improvements. Despite achieving attractive results, thetheoretical justification of performance improvement for such intuition hasbeen lacking. Motivated by the return discrepancy scheme in the model-based RLfield, we find that maximizing $I(Z;M)$ can be interpreted as consistentlyraising the lower bound of the expected return for a given policy conditioningon the optimal task representation. However, this optimization process ignoresthe task representation shift between two consecutive updates, which may leadto performance improvement collapse. To address this problem, we turn to usethe framework of performance difference bound to consider the impacts of taskrepresentation shift explicitly. We demonstrate that by reining the taskrepresentation shift, it is possible to achieve monotonic performanceimprovements, thereby showcasing the advantage against previous approaches. Tomake it practical, we design an easy yet highly effective algorithm RETRO(\underline{RE}ining \underline{T}ask \underline{R}epresentation shift incontext-based \underline{O}ffline meta reinforcement learning) with only addingone line of code compared to the backbone. Empirical results validate itsstate-of-the-art (SOTA) asymptotic performance, training stability andtraining-time consumption on MuJoCo and MetaWorld benchmarks.</description><author>Hai Zhang, Boyuan Zheng, Anqi Guo, Tianying Ji, Pheng-Ann Heng, Junqiao Zhao, Lanqing Li</author><pubDate>Mon, 20 May 2024 14:14:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12001v1</guid></item><item><title>KI-PMF: Knowledge Integrated Plausible Motion Forecasting</title><link>http://arxiv.org/abs/2310.12007v2</link><description>Accurately forecasting the motion of traffic actors is crucial for thedeployment of autonomous vehicles at a large scale. Current trajectoryforecasting approaches primarily concentrate on optimizing a loss function witha specific metric, which can result in predictions that do not adhere tophysical laws or violate external constraints. Our objective is to incorporateexplicit knowledge priors that allow a network to forecast future trajectoriesin compliance with both the kinematic constraints of a vehicle and the geometryof the driving environment. To achieve this, we introduce a non-parametricpruning layer and attention layers to integrate the defined knowledge priors.Our proposed method is designed to ensure reachability guarantees for trafficactors in both complex and dynamic situations. By conditioning the network tofollow physical laws, we can obtain accurate and safe predictions, essentialfor maintaining autonomous vehicles' safety and efficiency in real-worldsettings.In summary, this paper presents concepts that prevent off-roadpredictions for safe and reliable motion forecasting by incorporating knowledgepriors into the training process.</description><author>Abhishek Vivekanandan, Ahmed Abouelazm, Philip Schörner, J. Marius Zöllner</author><pubDate>Mon, 20 May 2024 14:04:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12007v2</guid></item><item><title>Boosting Fair Classifier Generalization through Adaptive Priority Reweighing</title><link>http://arxiv.org/abs/2309.08375v3</link><description>With the increasing penetration of machine learning applications in criticaldecision-making areas, calls for algorithmic fairness are more prominent.Although there have been various modalities to improve algorithmic fairnessthrough learning with fairness constraints, their performance does notgeneralize well in the test set. A performance-promising fair algorithm withbetter generalizability is needed. This paper proposes a novel adaptivereweighing method to eliminate the impact of the distribution shifts betweentraining and test data on model generalizability. Most previous reweighingmethods propose to assign a unified weight for each (sub)group. Rather, ourmethod granularly models the distance from the sample predictions to thedecision boundary. Our adaptive reweighing method prioritizes samples closer tothe decision boundary and assigns a higher weight to improve thegeneralizability of fair classifiers. Extensive experiments are performed tovalidate the generalizability of our adaptive priority reweighing method foraccuracy and fairness measures (i.e., equal opportunity, equalized odds, anddemographic parity) in tabular benchmarks. We also highlight the performance ofour method in improving the fairness of language and vision models. The code isavailable at https://github.com/che2198/APW.</description><author>Zhihao Hu, Yiran Xu, Mengnan Du, Jindong Gu, Xinmei Tian, Fengxiang He</author><pubDate>Mon, 20 May 2024 14:03:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08375v3</guid></item><item><title>Exploiting Style Latent Flows for Generalizing Deepfake Video Detection</title><link>http://arxiv.org/abs/2403.06592v3</link><description>This paper presents a new approach for the detection of fake videos, based onthe analysis of style latent vectors and their abnormal behavior in temporalchanges in the generated videos. We discovered that the generated facial videossuffer from the temporal distinctiveness in the temporal changes of stylelatent vectors, which are inevitable during the generation of temporally stablevideos with various facial expressions and geometric transformations. Ourframework utilizes the StyleGRU module, trained by contrastive learning, torepresent the dynamic properties of style latent vectors. Additionally, weintroduce a style attention module that integrates StyleGRU-generated featureswith content-based features, enabling the detection of visual and temporalartifacts. We demonstrate our approach across various benchmark scenarios indeepfake detection, showing its superiority in cross-dataset andcross-manipulation scenarios. Through further analysis, we also validate theimportance of using temporal changes of style latent vectors to improve thegenerality of deepfake video detection.</description><author>Jongwook Choi, Taehoon Kim, Yonghyun Jeong, Seungryul Baek, Jongwon Choi</author><pubDate>Mon, 20 May 2024 14:01:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06592v3</guid></item><item><title>Boolean matrix logic programming for active learning of gene functions in genome-scale metabolic network models</title><link>http://arxiv.org/abs/2405.06724v2</link><description>Techniques to autonomously drive research have been prominent inComputational Scientific Discovery, while Synthetic Biology is a field ofscience that focuses on designing and constructing new biological systems foruseful purposes. Here we seek to apply logic-based machine learning techniquesto facilitate cellular engineering and drive biological discovery.Comprehensive databases of metabolic processes called genome-scale metabolicnetwork models (GEMs) are often used to evaluate cellular engineeringstrategies to optimise target compound production. However, predicted hostbehaviours are not always correctly described by GEMs, often due to errors inthe models. The task of learning the intricate genetic interactions within GEMspresents computational and empirical challenges. To address these, we describea novel approach called Boolean Matrix Logic Programming (BMLP) by leveragingboolean matrices to evaluate large logic programs. We introduce a new system,$BMLP_{active}$, which efficiently explores the genomic hypothesis space byguiding informative experimentation through active learning. In contrast tosub-symbolic methods, $BMLP_{active}$ encodes a state-of-the-art GEM of awidely accepted bacterial host in an interpretable and logical representationusing datalog logic programs. Notably, $BMLP_{active}$ can successfully learnthe interaction between a gene pair with fewer training examples than randomexperimentation, overcoming the increase in experimental design space.$BMLP_{active}$ enables rapid optimisation of metabolic models to reliablyengineer biological systems for producing useful compounds. It offers arealistic approach to creating a self-driving lab for microbial engineering.</description><author>Lun Ai, Stephen H. Muggleton, Shi-Shun Liang, Geoff S. Baldwin</author><pubDate>Mon, 20 May 2024 14:01:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06724v2</guid></item><item><title>Non-autoregressive Generative Models for Reranking Recommendation</title><link>http://arxiv.org/abs/2402.06871v2</link><description>Contemporary recommendation systems are designed to meet users' needs bydelivering tailored lists of items that align with their specific demands orinterests. In a multi-stage recommendation system, reranking plays a crucialrole by modeling the intra-list correlations among items. The key challenge ofreranking lies in the exploration of optimal sequences within the combinatorialspace of permutations. Recent research proposes a generator-evaluator learningparadigm, where the generator generates multiple feasible sequences and theevaluator picks out the best sequence based on the estimated listwise score.The generator is of vital importance, and generative models are well-suited forthe generator function. Current generative models employ an autoregressivestrategy for sequence generation. However, deploying autoregressive models inreal-time industrial systems is challenging. To address these issues, wepropose a Non-AutoRegressive generative model for reranking Recommendation(NAR4Rec) designed to enhance efficiency and effectiveness. To tacklechallenges such as sparse training samples and dynamic candidates, we introducea matching model. Considering the diverse nature of user feedback, we employ asequence-level unlikelihood training objective to differentiate feasiblesequences from unfeasible ones. Additionally, to overcome the lack ofdependency modeling in non-autoregressive models regarding target items, weintroduce contrastive decoding to capture correlations among these items.Extensive offline experiments validate the superior performance of NAR4Rec overstate-of-the-art reranking methods. Online A/B tests reveal that NAR4Recsignificantly enhances the user experience. Furthermore, NAR4Rec has been fullydeployed in a popular video app Kuaishou with over 300 million daily activeusers.</description><author>Yuxin Ren, Qiya Yang, Yichun Wu, Wei Xu, Yalong Wang, Zhiqiang Zhang</author><pubDate>Mon, 20 May 2024 13:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06871v2</guid></item><item><title>GGAvatar: Geometric Adjustment of Gaussian Head Avatar</title><link>http://arxiv.org/abs/2405.11993v1</link><description>We propose GGAvatar, a novel 3D avatar representation designed to robustlymodel dynamic head avatars with complex identities and deformations. GGAvataremploys a coarse-to-fine structure, featuring two core modules: NeutralGaussian Initialization Module and Geometry Morph Adjuster. Neutral GaussianInitialization Module pairs Gaussian primitives with deformable triangularmeshes, employing an adaptive density control strategy to model the geometricstructure of the target subject with neutral expressions. Geometry MorphAdjuster introduces deformation bases for each Gaussian in global space,creating fine-grained low-dimensional representations of deformation behaviorsto address the Linear Blend Skinning formula's limitations effectively.Extensive experiments show that GGAvatar can produce high-fidelity renderings,outperforming state-of-the-art methods in visual quality and quantitativemetrics.</description><author>Xinyang Li, Jiaxin Wang, Yixin Xuan, Gongxin Yao, Yu Pan</author><pubDate>Mon, 20 May 2024 13:54:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11993v1</guid></item><item><title>Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models</title><link>http://arxiv.org/abs/2312.07887v2</link><description>Incremental Learning (IL) has been a long-standing problem in both vision andNatural Language Processing (NLP) communities. In recent years, as Pre-trainedLanguage Models (PLMs) have achieved remarkable progress in various NLPdownstream tasks, utilizing PLMs as backbones has become a common practice inrecent research of IL in NLP. Most assume that catastrophic forgetting is thebiggest obstacle to achieving superior IL performance and propose varioustechniques to overcome this issue. However, we find that this assumption isproblematic. Specifically, we revisit more than 20 methods on fourclassification tasks (Text Classification, Intent Classification, RelationExtraction, and Named Entity Recognition) under the two most popular ILsettings (Class-Incremental and Task-Incremental) and reveal that most of themseverely underestimate the inherent anti-forgetting ability of PLMs. Based onthe observation, we propose a frustratingly easy method called SEQ* for IL withPLMs. The results show that SEQ* has competitive or superior performancecompared to state-of-the-art (SOTA) IL methods and requires considerably lesstrainable parameters and training time. These findings urge us to revisit theIL with PLMs and encourage future studies to have a fundamental understandingof the catastrophic forgetting in PLMs. The data, code and scripts are publiclyavailable athttps://github.com/zzz47zzz/pretrained-lm-for-incremental-learning.</description><author>Junhao Zheng, Shengjie Qiu, Qianli Ma</author><pubDate>Mon, 20 May 2024 13:53:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07887v2</guid></item></channel></rss>