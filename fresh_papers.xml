<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 25 Nov 2024 13:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving</title><link>http://arxiv.org/abs/2411.15139v1</link><description>Recently, the diffusion model has emerged as a powerful generative techniquefor robotic policy learning, capable of modeling multi-mode actiondistributions. Leveraging its capability for end-to-end autonomous driving is apromising direction. However, the numerous denoising steps in the roboticdiffusion policy and the more dynamic, open-world nature of traffic scenes posesubstantial challenges for generating diverse driving actions at a real-timespeed. To address these challenges, we propose a novel truncated diffusionpolicy that incorporates prior multi-mode anchors and truncates the diffusionschedule, enabling the model to learn denoising from anchored Gaussiandistribution to the multi-mode driving action distribution. Additionally, wedesign an efficient cascade diffusion decoder for enhanced interaction withconditional scene context. The proposed model, DiffusionDrive, demonstrates10$\times$ reduction in denoising steps compared to vanilla diffusion policy,delivering superior diversity and quality in just 2 steps. On theplanning-oriented NAVSIM dataset, with the aligned ResNet-34 backbone,DiffusionDrive achieves 88.1 PDMS without bells and whistles, setting a newrecord, while running at a real-time speed of 45 FPS on an NVIDIA 4090.Qualitative results on challenging scenarios further confirm thatDiffusionDrive can robustly generate diverse plausible driving actions. Codeand model will be available at https://github.com/hustvl/DiffusionDrive.</description><author>Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, Qian Zhang, Xinggang Wang</author><pubDate>Fri, 22 Nov 2024 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15139v1</guid></item><item><title>Material Anything: Generating Materials for Any 3D Object via Diffusion</title><link>http://arxiv.org/abs/2411.15138v1</link><description>We present Material Anything, a fully-automated, unified diffusion frameworkdesigned to generate physically-based materials for 3D objects. Unlike existingmethods that rely on complex pipelines or case-specific optimizations, MaterialAnything offers a robust, end-to-end solution adaptable to objects underdiverse lighting conditions. Our approach leverages a pre-trained imagediffusion model, enhanced with a triple-head architecture and rendering loss toimprove stability and material quality. Additionally, we introduce confidencemasks as a dynamic switcher within the diffusion model, enabling it toeffectively handle both textured and texture-less objects across varyinglighting conditions. By employing a progressive material generation strategyguided by these confidence masks, along with a UV-space material refiner, ourmethod ensures consistent, UV-ready material outputs. Extensive experimentsdemonstrate our approach outperforms existing methods across a wide range ofobject categories and lighting conditions.</description><author>Xin Huang, Tengfei Wang, Ziwei Liu, Qing Wang</author><pubDate>Fri, 22 Nov 2024 18:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15138v1</guid></item><item><title>WildLMa: Long Horizon Loco-Manipulation in the Wild</title><link>http://arxiv.org/abs/2411.15131v1</link><description>`In-the-wild' mobile manipulation aims to deploy robots in diverse real-worldenvironments, which requires the robot to (1) have skills that generalizeacross object configurations; (2) be capable of long-horizon task execution indiverse environments; and (3) perform complex manipulation beyondpick-and-place. Quadruped robots with manipulators hold promise for extendingthe workspace and enabling robust locomotion, but existing results do notinvestigate such a capability. This paper proposes WildLMa with threecomponents to address these issues: (1) adaptation of learned low-levelcontroller for VR-enabled whole-body teleoperation and traversability; (2)WildLMa-Skill -- a library of generalizable visuomotor skills acquired viaimitation learning or heuristics and (3) WildLMa-Planner -- an interface oflearned skills that allow LLM planners to coordinate skills for long-horizontasks. We demonstrate the importance of high-quality training data by achievinghigher grasping success rate over existing RL baselines using only tens ofdemonstrations. WildLMa exploits CLIP for language-conditioned imitationlearning that empirically generalizes to objects unseen in trainingdemonstrations. Besides extensive quantitative evaluation, we qualitativelydemonstrate practical robot applications, such as cleaning up trash inuniversity hallways or outdoor terrains, operating articulated objects, andrearranging items on a bookshelf.</description><author>Ri-Zhao Qiu, Yuchen Song, Xuanbin Peng, Sai Aneesh Suryadevara, Ge Yang, Minghuan Liu, Mazeyu Ji, Chengzhe Jia, Ruihan Yang, Xueyan Zou, Xiaolong Wang</author><pubDate>Fri, 22 Nov 2024 18:56:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15131v1</guid></item><item><title>Measuring Bullshit in the Language Games played by ChatGPT</title><link>http://arxiv.org/abs/2411.15129v1</link><description>Generative large language models (LLMs), which create text without directcorrespondence to truth value, are widely understood to resemble the uses oflanguage described in Frankfurt's popular monograph On Bullshit. In this paper,we offer a rigorous investigation of this topic, identifying how the phenomenonhas arisen, and how it might be analysed. In this paper, we elaborate on thisargument to propose that LLM-based chatbots play the 'language game ofbullshit'. We use statistical text analysis to investigate the features of thisWittgensteinian language game, based on a dataset constructed to contrast thelanguage of 1,000 scientific publications with typical pseudo-scientific textgenerated by ChatGPT. We then explore whether the same language features can bedetected in two well-known contexts of social dysfunction: George Orwell'scritique of politics and language, and David Graeber's characterisation ofbullshit jobs. Using simple hypothesis-testing methods, we demonstrate that astatistical model of the language of bullshit can reliably relate theFrankfurtian artificial bullshit of ChatGPT to the political and workplacefunctions of bullshit as observed in natural human language.</description><author>Alessandro Trevisan, Harry Giddens, Sarah Dillon, Alan F. Blackwell</author><pubDate>Fri, 22 Nov 2024 18:55:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15129v1</guid></item><item><title>Health AI Developer Foundations</title><link>http://arxiv.org/abs/2411.15128v1</link><description>Robust medical Machine Learning (ML) models have the potential torevolutionize healthcare by accelerating clinical research, improving workflowsand outcomes, and producing novel insights or capabilities. Developing such MLmodels from scratch is cost prohibitive and requires substantial compute, data,and time (e.g., expert labeling). To address these challenges, we introduceHealth AI Developer Foundations (HAI-DEF), a suite of pre-trained,domain-specific foundation models, tools, and recipes to accelerate building MLfor health applications. The models cover various modalities and domains,including radiology (X-rays and computed tomography), histopathology,dermatological imaging, and audio. These models provide domain specificembeddings that facilitate AI development with less labeled data, shortertraining times, and reduced computational costs compared to traditionalapproaches. In addition, we utilize a common interface and style across thesemodels, and prioritize usability to enable developers to integrate HAI-DEFefficiently. We present model evaluations across various tasks and concludewith a discussion of their application and evaluation, covering the importanceof ensuring efficacy, fairness, and equity. Finally, while HAI-DEF andspecifically the foundation models lower the barrier to entry for ML inhealthcare, we emphasize the importance of validation with problem- andpopulation-specific data for each desired usage setting. This technical reportwill be updated over time as more modalities and features are added.</description><author>Atilla P. Kiraly, Sebastien Baur, Kenneth Philbrick, Fereshteh Mahvar, Liron Yatziv, Tiffany Chen, Bram Sterling, Nick George, Fayaz Jamil, Jing Tang, Kai Bailey, Faruk Ahmed, Akshay Goel, Abbi Ward, Lin Yang, Andrew Sellergren, Yossi Matias, Avinatan Hassidim, Shravya Shetty, Daniel Golden, Shekoofeh Azizi, David F. Steiner, Yun Liu, Tim Thelin, Rory Pilgrim, Can Kirmizibayrak</author><pubDate>Fri, 22 Nov 2024 18:51:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15128v1</guid></item><item><title>PRIMUS: Pretraining IMU Encoders with Multimodal Self-Supervision</title><link>http://arxiv.org/abs/2411.15127v1</link><description>Sensing human motions through Inertial Measurement Units (IMUs) embedded inpersonal devices has enabled significant applications in health and wellness.While labeled IMU data is scarce, we can collect unlabeled or weakly labeledIMU data to model human motions. For video or text modalities, the "pretrainand adapt" approach utilizes large volumes of unlabeled or weakly labeled datafor pretraining, building a strong feature extractor, followed by adaptation tospecific tasks using limited labeled data. This approach has not been widelyadopted in the IMU domain for two reasons: (1) pretraining methods are poorlyunderstood in the context of IMU, and (2) open-source pretrained models thatgeneralize across datasets are rarely publicly available. In this paper, we aimto address the first issue by proposing PRIMUS, a method for PRetraining IMUencoderS. We conduct a systematic and unified evaluation of variousself-supervised and multimodal learning pretraining objectives. Our findingsindicate that using PRIMUS, which combines self-supervision, multimodalsupervision, and nearest-neighbor supervision, can significantly enhancedownstream performance. With fewer than 500 labeled samples per class, PRIMUSeffectively enhances downstream performance by up to 15% in held-out test data,compared to the state-of-the-art multimodal training method. To benefit thebroader community, our code and pre-trained IMU encoders will be made publiclyavailable at github.com/nokia-bell-labs upon publication.</description><author>Arnav M. Das, Chi Ian Tang, Fahim Kawsar, Mohammad Malekzadeh</author><pubDate>Fri, 22 Nov 2024 18:46:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15127v1</guid></item><item><title>TÜLU 3: Pushing Frontiers in Open Language Model Post-Training</title><link>http://arxiv.org/abs/2411.15124v1</link><description>Language model post-training is applied to refine behaviors and unlock newskills across a wide range of recent language models, but open recipes forapplying these techniques lag behind proprietary ones. The underlying trainingdata and recipes for post-training are simultaneously the most important piecesof the puzzle and the portion with the least transparency. To bridge this gap,we introduce T\"ULU 3, a family of fully-open state-of-the-art post-trainedmodels, alongside its data, code, and training recipes, serving as acomprehensive guide for modern post-training techniques. T\"ULU 3, which buildson Llama 3.1 base models, achieves results surpassing the instruct versions ofLlama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini andClaude 3.5-Haiku. The training algorithms for our models include supervisedfinetuning (SFT), Direct Preference Optimization (DPO), and a novel method wecall Reinforcement Learning with Verifiable Rewards (RLVR). With T\"ULU 3, weintroduce a multi-task evaluation scheme for post-training recipes withdevelopment and unseen evaluations, standard benchmark implementations, andsubstantial decontamination of existing open datasets on said benchmarks. Weconclude with analysis and discussion of training methods that did not reliablyimprove performance. In addition to the T\"ULU 3 model weights and demo, we release the completerecipe -- including datasets for diverse core skills, a robust toolkit for datacuration and evaluation, the training code and infrastructure, and, mostimportantly, a detailed report for reproducing and further adapting the T\"ULU3 approach to more domains.</description><author>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, Hannaneh Hajishirzi</author><pubDate>Fri, 22 Nov 2024 18:44:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15124v1</guid></item><item><title>ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation</title><link>http://arxiv.org/abs/2411.15122v1</link><description>AI-driven models have demonstrated significant potential in automatingradiology report generation for chest X-rays. However, there is no standardizedbenchmark for objectively evaluating their performance. To address this, wepresent ReXrank, https://rexrank.ai, a public leaderboard and challenge forassessing AI-powered radiology report generation. Our framework incorporatesReXGradient, the largest test dataset consisting of 10,000 studies, and threepublic datasets (MIMIC-CXR, IU-Xray, CheXpert Plus) for report generationassessment. ReXrank employs 8 evaluation metrics and separately assesses modelscapable of generating only findings sections and those providing both findingsand impressions sections. By providing this standardized evaluation framework,ReXrank enables meaningful comparisons of model performance and offers crucialinsights into their robustness across diverse clinical settings. Beyond itscurrent focus on chest X-rays, ReXrank's framework sets the stage forcomprehensive evaluation of automated reporting across the full spectrum ofmedical imaging.</description><author>Xiaoman Zhang, Hong-Yu Zhou, Xiaoli Yang, Oishi Banerjee, Julián N. Acosta, Josh Miller, Ouwen Huang, Pranav Rajpurkar</author><pubDate>Fri, 22 Nov 2024 18:40:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15122v1</guid></item><item><title>Financial Fraud Detection using Jump-Attentive Graph Neural Networks</title><link>http://arxiv.org/abs/2411.05857v2</link><description>As the availability of financial services online continues to grow, theincidence of fraud has surged correspondingly. Fraudsters continually seek newand innovative ways to circumvent the detection algorithms in place.Traditionally, fraud detection relied on rule-based methods, where rules weremanually created based on transaction data features. However, these techniquessoon became ineffective due to their reliance on manual rule creation and theirinability to detect complex data patterns. Today, a significant portion of thefinancial services sector employs various machine learning algorithms, such asXGBoost, Random Forest, and neural networks, to model transaction data. Whilethese techniques have proven more efficient than rule-based methods, they stillfail to capture interactions between different transactions and theirinterrelationships. Recently, graph-based techniques have been adopted forfinancial fraud detection, leveraging graph topology to aggregate neighborhoodinformation of transaction data using Graph Neural Networks (GNNs). Despiteshowing improvements over previous methods, these techniques still struggle tokeep pace with the evolving camouflaging tactics of fraudsters and suffer frominformation loss due to over-smoothing. In this paper, we propose a novelalgorithm that employs an efficient neighborhood sampling method, effective forcamouflage detection and preserving crucial feature information fromnon-similar nodes. Additionally, we introduce a novel GNN architecture thatutilizes attention mechanisms and preserves holistic neighborhood informationto prevent information loss. We test our algorithm on financial data to showthat our method outperforms other state-of-the-art graph algorithms.</description><author>Prashank Kadam</author><pubDate>Fri, 22 Nov 2024 18:34:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05857v2</guid></item><item><title>VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement</title><link>http://arxiv.org/abs/2411.15115v1</link><description>Recent text-to-video (T2V) diffusion models have demonstrated impressivegeneration capabilities across various domains. However, these models oftengenerate videos that have misalignments with text prompts, especially when theprompts describe complex scenes with multiple objects and attributes. Toaddress this, we introduce VideoRepair, a novel model-agnostic, training-freevideo refinement framework that automatically identifies fine-grainedtext-video misalignments and generates explicit spatial and textual feedback,enabling a T2V diffusion model to perform targeted, localized refinements.VideoRepair consists of four stages: In (1) video evaluation, we detectmisalignments by generating fine-grained evaluation questions and answeringthose questions with MLLM. In (2) refinement planning, we identify accuratelygenerated objects and then create localized prompts to refine other areas inthe video. Next, in (3) region decomposition, we segment the correctlygenerated area using a combined grounding module. We regenerate the video byadjusting the misaligned regions while preserving the correct regions in (4)localized refinement. On two popular video generation benchmarks (EvalCrafterand T2V-CompBench), VideoRepair substantially outperforms recent baselinesacross various text-video alignment metrics. We provide a comprehensiveanalysis of VideoRepair components and qualitative examples.</description><author>Daeun Lee, Jaehong Yoon, Jaemin Cho, Mohit Bansal</author><pubDate>Fri, 22 Nov 2024 18:31:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15115v1</guid></item><item><title>RE-Bench: Evaluating frontier AI R&amp;D capabilities of language model agents against human experts</title><link>http://arxiv.org/abs/2411.15114v1</link><description>Frontier AI safety policies highlight automation of AI research anddevelopment (R&amp;D) by AI agents as an important capability to anticipate.However, there exist few evaluations for AI R&amp;D capabilities, and none that arehighly realistic and have a direct comparison to human performance. Weintroduce RE-Bench (Research Engineering Benchmark, v1), which consists of 7challenging, open-ended ML research engineering environments and data from 718-hour attempts by 61 distinct human experts. We confirm that our experts makeprogress in the environments given 8 hours, with 82% of expert attemptsachieving a non-zero score and 24% matching or exceeding our strong referencesolutions. We compare humans to several public frontier models throughbest-of-k with varying time budgets and agent designs, and find that the bestAI agents achieve a score 4x higher than human experts when both are given atotal time budget of 2 hours per environment. However, humans currently displaybetter returns to increasing time budgets, narrowly exceeding the top AI agentscores given an 8-hour budget, and achieving 2x the score of the top AI agentwhen both are given 32 total hours (across different attempts). Qualitatively,we find that modern AI agents possess significant expertise in many ML topics-- e.g. an agent wrote a faster custom Triton kernel than any of our humanexperts' -- and can generate and test solutions over ten times faster thanhumans, at much lower cost. We open-source the evaluation environments, humanexpert data, analysis code and agent trajectories to facilitate futureresearch.</description><author>Hjalmar Wijk, Tao Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh Clymer, Jai Dhyani, Elena Ericheva, Katharyn Garcia, Brian Goodrich, Nikola Jurkovic, Megan Kinniment, Aron Lajko, Seraphina Nix, Lucas Sato, William Saunders, Maksym Taran, Ben West, Elizabeth Barnes</author><pubDate>Fri, 22 Nov 2024 18:30:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15114v1</guid></item><item><title>Sketched Equivariant Imaging Regularization and Deep Internal Learning for Inverse Problems</title><link>http://arxiv.org/abs/2411.05771v2</link><description>Equivariant Imaging (EI) regularization has become the de-facto technique forunsupervised training of deep imaging networks, without any need ofground-truth data. Observing that the EI-based unsupervised training paradigmcurrently has significant computational redundancy leading to inefficiency inhigh-dimensional applications, we propose a sketched EI regularization whichleverages the randomized sketching techniques for acceleration. We then extendour sketched EI regularization to develop an accelerated deep internal learningframework -- Sketched Equivariant Deep Image Prior (Sk-EI-DIP), which can beefficiently applied for single-image and task-adapted reconstruction.Additionally, for network adaptation tasks, we propose a parameter-efficientapproach for accelerating both EI-DIP and Sk-EI-DIP via optimizing only thenormalization layers. Our numerical study on X-ray CT image reconstructiontasks demonstrate that our approach can achieve order-of-magnitudecomputational acceleration over standard EI-based counterpart in single-inputsetting, and network adaptation at test time.</description><author>Guixian Xu, Jinglai Li, Junqi Tang</author><pubDate>Fri, 22 Nov 2024 18:29:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05771v2</guid></item><item><title>Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion</title><link>http://arxiv.org/abs/2411.15113v1</link><description>As text-to-image models grow increasingly powerful and complex, theirburgeoning size presents a significant obstacle to widespread adoption,especially on resource-constrained devices. This paper presents a pioneeringstudy on post-training pruning of Stable Diffusion 2, addressing the criticalneed for model compression in text-to-image domain. Our study tackles thepruning techniques for the previously unexplored multi-modal generation models,and particularly examines the pruning impact on the textual component and theimage generation component separately. We conduct a comprehensive comparison onpruning the model or the single component of the model in various sparsities.Our results yield previously undocumented findings. For example, contrary toestablished trends in language model pruning, we discover that simple magnitudepruning outperforms more advanced techniques in text-to-image context.Furthermore, our results show that Stable Diffusion 2 can be pruned to 38.5%sparsity with minimal quality loss, achieving a significant reduction in modelsize. We propose an optimal pruning configuration that prunes the text encoderto 47.5% and the diffusion generator to 35%. This configuration maintains imagegeneration quality while substantially reducing computational requirements. Inaddition, our work uncovers intriguing questions about information encoding intext-to-image models: we observe that pruning beyond certain thresholds leadsto sudden performance drops (unreadable images), suggesting that specificweights encode critical semantics information. This finding opens new avenuesfor future research in model compression, interoperability, and biasidentification in text-to-image models. By providing crucial insights into thepruning behavior of text-to-image models, our study lays the groundwork fordeveloping more efficient and accessible AI-driven image generation systems</description><author>Samarth N Ramesh, Zhixue Zhao</author><pubDate>Fri, 22 Nov 2024 18:29:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15113v1</guid></item><item><title>Efficient Brain Imaging Analysis for Alzheimer's and Dementia Detection Using Convolution-Derivative Operations</title><link>http://arxiv.org/abs/2411.13490v2</link><description>Alzheimer's disease (AD) is characterized by progressive neurodegenerationand results in detrimental structural changes in human brains. Detecting thesechanges is crucial for early diagnosis and timely intervention of diseaseprogression. Jacobian maps, derived from spatial normalization in voxel-basedmorphometry (VBM), have been instrumental in interpreting volume alterationsassociated with AD. However, the computational cost of generating Jacobian mapslimits its clinical adoption. In this study, we explore alternative methods andpropose Sobel kernel angle difference (SKAD) as a computationally efficientalternative. SKAD is a derivative operation that offers an optimized approachto quantifying volumetric alterations through localized analysis of thegradients. By efficiently extracting gradient amplitude changes at criticalspatial regions, this derivative operation captures regional volume variationsEvaluation of SKAD over various medical datasets demonstrates that it is 6.3xfaster than Jacobian maps while still maintaining comparable accuracy. Thismakes it an efficient and competitive approach in neuroimaging research andclinical practice.</description><author>Yasmine Mustafa, Mohamed Elmahallawy, Tie Luo</author><pubDate>Fri, 22 Nov 2024 18:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.13490v2</guid></item><item><title>Learnable Activation Functions in Physics-Informed Neural Networks for Solving Partial Differential Equations</title><link>http://arxiv.org/abs/2411.15111v1</link><description>We investigate the use of learnable activation functions in Physics-InformedNeural Networks (PINNs) for solving Partial Differential Equations (PDEs).Specifically, we compare the efficacy of traditional Multilayer Perceptrons(MLPs) with fixed and learnable activations against Kolmogorov-Arnold Networks(KANs), which employ learnable basis functions. Physics-informed neuralnetworks (PINNs) have emerged as an effective method for directly incorporatingphysical laws into the learning process, offering a data-efficient solution forboth the forward and inverse problems associated with PDEs. However, challengessuch as effective training and spectral bias, where low-frequency componentsare learned more effectively, often limit their applicability to problemscharacterized by rapid oscillations or sharp transitions. By employingdifferent activation or basis functions on MLP and KAN, we assess their impacton convergence behavior and spectral bias mitigation, and the accurateapproximation of PDEs. The findings offer insights into the design of neuralnetwork architectures that balance training efficiency, convergence speed, andtest accuracy for PDE solvers. By evaluating the influence of activation orbasis function choices, this work provides guidelines for developing morerobust and accurate PINN models. The source code and pre-trained models used inthis study are made publicly available to facilitate reproducibility and futureexploration.</description><author>Afrah Fareaa, Mustafa Serdar Celebi</author><pubDate>Fri, 22 Nov 2024 18:25:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15111v1</guid></item><item><title>A Real-Time DETR Approach to Bangladesh Road Object Detection for Autonomous Vehicles</title><link>http://arxiv.org/abs/2411.15110v1</link><description>In the recent years, we have witnessed a paradigm shift in the field ofComputer Vision, with the forthcoming of the transformer architecture.Detection Transformers has become a state of the art solution to objectdetection and is a potential candidate for Road Object Detection in AutonomousVehicles. Despite the abundance of object detection schemes, real-time DETRmodels are shown to perform significantly better on inference times, withminimal loss of accuracy and performance. In our work, we used Real-Time DETR(RTDETR) object detection on the BadODD Road Object Detection dataset based inBangladesh, and performed necessary experimentation and testing. Our resultsgave a mAP50 score of 0.41518 in the public 60% test set, and 0.28194 in theprivate 40% test set.</description><author>Irfan Nafiz Shahan, Arban Hossain, Saadman Sakib, Al-Mubin Nabil</author><pubDate>Fri, 22 Nov 2024 18:21:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15110v1</guid></item><item><title>MaGS: Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed Gaussian Splatting</title><link>http://arxiv.org/abs/2406.01593v2</link><description>3D reconstruction and simulation, although interrelated, have distinctobjectives: reconstruction requires a flexible 3D representation that can adaptto diverse scenes, while simulation needs a structured representation to modelmotion principles effectively. This paper introduces the Mesh-adsorbed GaussianSplatting (MaGS) method to address this challenge. MaGS constrains 3D Gaussiansto roam near the mesh, creating a mutually adsorbed mesh-Gaussian 3Drepresentation. Such representation harnesses both the rendering flexibility of3D Gaussians and the structured property of meshes. To achieve this, weintroduce RMD-Net, a network that learns motion priors from video data torefine mesh deformations, alongside RGD-Net, which models the relativedisplacement between the mesh and Gaussians to enhance rendering fidelity undermesh constraints. To generalize to novel, user-defined deformations beyondinput video without reliance on temporal data, we propose MPE-Net, whichleverages inherent mesh information to bootstrap RMD-Net and RGD-Net. Due tothe universality of meshes, MaGS is compatible with various deformation priorssuch as ARAP, SMPL, and soft physics simulation. Extensive experiments on theD-NeRF, DG-Mesh, and PeopleSnapshot datasets demonstrate that MaGS achievesstate-of-the-art performance in both reconstruction and simulation.</description><author>Shaojie Ma, Yawei Luo, Wei Yang, Yi Yang</author><pubDate>Fri, 22 Nov 2024 18:20:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01593v2</guid></item><item><title>AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies</title><link>http://arxiv.org/abs/2402.04292v2</link><description>Diffusion-based imitation learning improves Behavioral Cloning (BC) onmulti-modal decision-making, but comes at the cost of significantly slowerinference due to the recursion in the diffusion process. It urges us to designefficient policy generators while keeping the ability to generate diverseactions. To address this challenge, we propose AdaFlow, an imitation learningframework based on flow-based generative modeling. AdaFlow represents thepolicy with state-conditioned ordinary differential equations (ODEs), which areknown as probability flows. We reveal an intriguing connection between theconditional variance of their training loss and the discretization error of theODEs. With this insight, we propose a variance-adaptive ODE solver that canadjust its step size in the inference stage, making AdaFlow an adaptivedecision-maker, offering rapid inference without sacrificing diversity.Interestingly, it automatically reduces to a one-step generator when the actiondistribution is uni-modal. Our comprehensive empirical evaluation shows thatAdaFlow achieves high performance with fast inference speed.</description><author>Xixi Hu, Bo Liu, Xingchao Liu, Qiang Liu</author><pubDate>Fri, 22 Nov 2024 18:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04292v2</guid></item><item><title>Effective Littlestone Dimension</title><link>http://arxiv.org/abs/2411.15109v1</link><description>Delle Rose et al.~(COLT'23) introduced an effective version of theVapnik-Chervonenkis dimension, and showed that it characterizes improper PAClearning with total computable learners. In this paper, we introduce and studya similar effectivization of the notion of Littlestone dimension. Finiteeffective Littlestone dimension is a necessary condition for computable onlinelearning but is not a sufficient one -- which we already establish for classesof the effective Littlestone dimension 2. However, the effective Littlestonedimension equals the optimal mistake bound for computable learners in twospecial cases: a) for classes of Littlestone dimension 1 and b) when thelearner receives as additional information an upper bound on the numbers to beguessed. Interestingly, finite effective Littlestone dimension also guaranteesthat the class consists only of computable functions.</description><author>Valentino Delle Rose, Alexander Kozachinskiy, Tomasz Steifer</author><pubDate>Fri, 22 Nov 2024 18:11:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15109v1</guid></item><item><title>About Time: Advances, Challenges, and Outlooks of Action Understanding</title><link>http://arxiv.org/abs/2411.15106v1</link><description>We have witnessed impressive advances in video action understanding.Increased dataset sizes, variability, and computation availability have enabledleaps in performance and task diversification. Current systems can providecoarse- and fine-grained descriptions of video scenes, extract segmentscorresponding to queries, synthesize unobserved parts of videos, and predictcontext. This survey comprehensively reviews advances in uni- and multi-modalaction understanding across a range of tasks. We focus on prevalent challenges,overview widely adopted datasets, and survey seminal works with an emphasis onrecent advances. We broadly distinguish between three temporal scopes: (1)recognition tasks of actions observed in full, (2) prediction tasks for ongoingpartially observed actions, and (3) forecasting tasks for subsequent unobservedaction. This division allows us to identify specific action modeling and videorepresentation challenges. Finally, we outline future directions to addresscurrent shortcomings.</description><author>Alexandros Stergiou, Ronald Poppe</author><pubDate>Fri, 22 Nov 2024 18:09:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15106v1</guid></item><item><title>AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out Context Attribution</title><link>http://arxiv.org/abs/2411.15102v1</link><description>The influence of contextual input on the behavior of large language models(LLMs) has prompted the development of context attribution methods that aim toquantify each context span's effect on an LLM's generations. The leave-one-out(LOO) error, which measures the change in the likelihood of the LLM's responsewhen a given span of the context is removed, provides a principled way toperform context attribution, but can be prohibitively expensive to compute forlarge models. In this work, we introduce AttriBoT, a series of novel techniquesfor efficiently computing an approximation of the LOO error for contextattribution. Specifically, AttriBoT uses cached activations to avoid redundantoperations, performs hierarchical attribution to reduce computation, andemulates the behavior of large target models with smaller proxy models. Takentogether, AttriBoT can provide a &gt;300x speedup while remaining more faithful toa target model's LOO error than prior context attribution methods. This starkincrease in performance makes computing context attributions for a givenresponse 30x faster than generating the response itself, empowering real-worldapplications that require computing attributions at scale. We release auser-friendly and efficient implementation of AttriBoT to enable efficient LLMinterpretability as well as encourage future development of efficient contextattribution methods.</description><author>Fengyuan Liu, Nikhil Kandpal, Colin Raffel</author><pubDate>Fri, 22 Nov 2024 18:06:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15102v1</guid></item><item><title>What You See is Not What You Get: Neural Partial Differential Equations and The Illusion of Learning</title><link>http://arxiv.org/abs/2411.15101v1</link><description>Differentiable Programming for scientific machine learning (SciML) hasrecently seen considerable interest and success, as it directly embeds neuralnetworks inside PDEs, often called as NeuralPDEs, derived from first principlephysics. Therefore, there is a widespread assumption in the community thatNeuralPDEs are more trustworthy and generalizable than black box models.However, like any SciML model, differentiable programming relies predominantlyon high-quality PDE simulations as "ground truth" for training. However,mathematics dictates that these are only discrete numerical approximations ofthe true physics. Therefore, we ask: Are NeuralPDEs and differentiableprogramming models trained on PDE simulations as physically interpretable as wethink? In this work, we rigorously attempt to answer these questions, usingestablished ideas from numerical analysis, experiments, and analysis of modelJacobians. Our study shows that NeuralPDEs learn the artifacts in thesimulation training data arising from the discretized Taylor Series truncationerror of the spatial derivatives. Additionally, NeuralPDE models aresystematically biased, and their generalization capability is likely enabled bya fortuitous interplay of numerical dissipation and truncation error in thetraining dataset and NeuralPDE, which seldom happens in practical applications.This bias manifests aggressively even in relatively accessible 1-D equations,raising concerns about the veracity of differentiable programming on complex,high-dimensional, real-world PDEs, and in dataset integrity of foundationmodels. Further, we observe that the initial condition constrains thetruncation error in initial-value problems in PDEs, thereby exertinglimitations to extrapolation. Finally, we demonstrate that an eigenanalysis ofmodel weights can indicate a priori if the model will be inaccurate forout-of-distribution testing.</description><author>Arvind Mohan, Ashesh Chattopadhyay, Jonah Miller</author><pubDate>Fri, 22 Nov 2024 18:04:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15101v1</guid></item><item><title>Persistent Homology for Structural Characterization in Disordered Systems</title><link>http://arxiv.org/abs/2411.14390v2</link><description>We propose a unified framework based on persistent homology (PH) tocharacterize both local and global structures in disordered systems. It cansimultaneously generate local and global descriptors using the same algorithmand data structure, and has shown to be highly effective and interpretable inpredicting particle rearrangements and classifying global phases. Based on thisframework, we define a non-parametric metric, the Separation Index (SI), whichnot only outperforms traditional bond-orientational order parameters in phaseclassification tasks but also establishes a connection between particleenvironments and the global phase structure. Our methods provide an effectiveframework for understanding and analyzing the properties of disorderedmaterials, with broad potential applications in materials science and evenwider studies of complex systems.</description><author>An Wang, Li Zou</author><pubDate>Fri, 22 Nov 2024 18:04:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14390v2</guid></item><item><title>XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models</title><link>http://arxiv.org/abs/2411.15100v1</link><description>The applications of LLM Agents are becoming increasingly complex and diverse,leading to a high demand for structured outputs that can be parsed into code,structured function calls, and embodied agent commands. These developmentsbring significant demands for structured generation in LLM inference.Context-free grammar is a flexible approach to enable structured generation viaconstrained decoding. However, executing context-free grammar requires goingthrough several stack states over all tokens in vocabulary during runtime,bringing non-negligible overhead for structured generation. In this paper, wepropose XGrammar, a flexible and efficient structure generation engine forlarge language models. XGrammar accelerates context-free grammar execution bydividing the vocabulary into context-independent tokens that can be precheckedand context-dependent tokens that need to be interpreted during runtime. Wefurther build transformations to expand the grammar context and reduce thenumber of context-independent tokens. Additionally, we build an efficientpersistent stack to accelerate the context-dependent token checks. Finally, weco-design the grammar engine with LLM inference engine to overlap grammarcomputation with GPU executions. Evaluation results show that XGrammar canachieve up to 100x speedup over existing solutions. Combined with an LLMinference engine, it can generate near-zero overhead structure generation inend-to-end low-LLM serving.</description><author>Yixin Dong, Charlie F. Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, Tianqi Chen</author><pubDate>Fri, 22 Nov 2024 18:01:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15100v1</guid></item><item><title>FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI</title><link>http://arxiv.org/abs/2411.04872v4</link><description>We introduce FrontierMath, a benchmark of hundreds of original, exceptionallychallenging mathematics problems crafted and vetted by expert mathematicians.The questions cover most major branches of modern mathematics -- fromcomputationally intensive problems in number theory and real analysis toabstract questions in algebraic geometry and category theory. Solving a typicalproblem requires multiple hours of effort from a researcher in the relevantbranch of mathematics, and for the upper end questions, multiple days.FrontierMath uses new, unpublished problems and automated verification toreliably evaluate models while minimizing risk of data contamination. Currentstate-of-the-art AI models solve under 2% of problems, revealing a vast gapbetween AI capabilities and the prowess of the mathematical community. As AIsystems advance toward expert-level mathematical abilities, FrontierMath offersa rigorous testbed that quantifies their progress.</description><author>Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli Järviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, Mark Wildon</author><pubDate>Fri, 22 Nov 2024 18:00:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04872v4</guid></item><item><title>UnMarker: A Universal Attack on Defensive Image Watermarking</title><link>http://arxiv.org/abs/2405.08363v2</link><description>Reports regarding the misuse of Generative AI (GenAI) to create deepfakes arefrequent. Defensive watermarking enables GenAI providers to hide fingerprintsin their images and use them later for deepfake detection. Yet, its potentialhas not been fully explored. We present UnMarker -- the first practicaluniversal attack on defensive watermarking. Unlike existing attacks, UnMarkerrequires no detector feedback, no unrealistic knowledge of the watermarkingscheme or similar models, and no advanced denoising pipelines that may not beavailable. Instead, being the product of an in-depth analysis of thewatermarking paradigm revealing that robust schemes must construct theirwatermarks in the spectral amplitudes, UnMarker employs two novel adversarialoptimizations to disrupt the spectra of watermarked images, erasing thewatermarks. Evaluations against SOTA schemes prove UnMarker's effectiveness. Itnot only defeats traditional schemes while retaining superior quality comparedto existing attacks but also breaks semantic watermarks that alter an image'sstructure, reducing the best detection rate to $43\%$ and rendering themuseless. To our knowledge, UnMarker is the first practical attack on semanticwatermarks, which have been deemed the future of defensive watermarking. Ourfindings show that defensive watermarking is not a viable defense againstdeepfakes, and we urge the community to explore alternatives.</description><author>Andre Kassis, Urs Hengartner</author><pubDate>Fri, 22 Nov 2024 17:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08363v2</guid></item><item><title>Context-Aware Multimodal Pretraining</title><link>http://arxiv.org/abs/2411.15099v1</link><description>Large-scale multimodal representation learning successfully optimizes forzero-shot transfer at test time. Yet the standard pretraining paradigm(contrastive learning on large amounts of image-text data) does not explicitlyencourage representations to support few-shot adaptation. In this work, wepropose a simple, but carefully designed extension to multimodal pretrainingwhich enables representations to accommodate additional context. Using thisobjective, we show that vision-language models can be trained to exhibitsignificantly increased few-shot adaptation: across 21 downstream tasks, wefind up to four-fold improvements in test-time sample efficiency, and averagefew-shot adaptation gains of over 5%, while retaining zero-shot generalizationperformance across model scales and training durations. In particular, equippedwith simple, training-free, metric-based adaptation mechanisms, ourrepresentations easily surpass more complex and expensive optimization-basedschemes, vastly simplifying generalization to new domains.</description><author>Karsten Roth, Zeynep Akata, Dima Damen, Ivana Balažević, Olivier J. Hénaff</author><pubDate>Fri, 22 Nov 2024 17:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15099v1</guid></item><item><title>OminiControl: Minimal and Universal Control for Diffusion Transformer</title><link>http://arxiv.org/abs/2411.15098v1</link><description>In this paper, we introduce OminiControl, a highly versatile andparameter-efficient framework that integrates image conditions into pre-trainedDiffusion Transformer (DiT) models. At its core, OminiControl leverages aparameter reuse mechanism, enabling the DiT to encode image conditions usingitself as a powerful backbone and process them with its flexible multi-modalattention processors. Unlike existing methods, which rely heavily on additionalencoder modules with complex architectures, OminiControl (1) effectively andefficiently incorporates injected image conditions with only ~0.1% additionalparameters, and (2) addresses a wide range of image conditioning tasks in aunified manner, including subject-driven generation and spatially-alignedconditions such as edges, depth, and more. Remarkably, these capabilities areachieved by training on images generated by the DiT itself, which isparticularly beneficial for subject-driven generation. Extensive evaluationsdemonstrate that OminiControl outperforms existing UNet-based and DiT-adaptedmodels in both subject-driven and spatially-aligned conditional generation.Additionally, we release our training dataset, Subjects200K, a diversecollection of over 200,000 identity-consistent images, along with an efficientdata synthesis pipeline to advance research in subject-consistent generation.</description><author>Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, Xinchao Wang</author><pubDate>Fri, 22 Nov 2024 17:55:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15098v1</guid></item><item><title>RED: Effective Trajectory Representation Learning with Comprehensive Information</title><link>http://arxiv.org/abs/2411.15096v1</link><description>Trajectory representation learning (TRL) maps trajectories to vectors thatcan then be used for various downstream tasks, including trajectory similaritycomputation, trajectory classification, and travel-time estimation. However,existing TRL methods often produce vectors that, when used in downstream tasks,yield insufficiently accurate results. A key reason is that they fail toutilize the comprehensive information encompassed by trajectories. We propose aself-supervised TRL framework, called RED, which effectively exploits multipletypes of trajectory information. Overall, RED adopts the Transformer as thebackbone model and masks the constituting paths in trajectories to train amasked autoencoder (MAE). In particular, RED considers the moving patterns oftrajectories by employing a Road-aware masking strategy} that retains key pathsof trajectories during masking, thereby preserving crucial information of thetrajectories. RED also adopts a spatial-temporal-user joint Embedding scheme toencode comprehensive information when preparing the trajectories as modelinputs. To conduct training, RED adopts Dual-objective task learning}: theTransformer encoder predicts the next segment in a trajectory, and theTransformer decoder reconstructs the entire trajectory. RED also considers thespatial-temporal correlations of trajectories by modifying the attentionmechanism of the Transformer. We compare RED with 9 state-of-the-art TRLmethods for 4 downstream tasks on 3 real-world datasets, finding that RED canusually improve the accuracy of the best-performing baseline by over 5%.</description><author>Silin Zhou, Shuo Shang, Lisi Chen, Christian S. Jensen, Panos Kalnis</author><pubDate>Fri, 22 Nov 2024 17:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15096v1</guid></item><item><title>Dimension-independent rates for structured neural density estimation</title><link>http://arxiv.org/abs/2411.15095v1</link><description>We show that deep neural networks achieve dimension-independent rates ofconvergence for learning structured densities such as those arising in image,audio, video, and text applications. More precisely, we demonstrate that neuralnetworks with a simple $L^2$-minimizing loss achieve a rate of $n^{-1/(4+r)}$in nonparametric density estimation when the underlying density is Markov to agraph whose maximum clique size is at most $r$, and we provide evidence that inthe aforementioned applications, this size is typically constant, i.e.,$r=O(1)$. We then establish that the optimal rate in $L^1$ is $n^{-1/(2+r)}$which, compared to the standard nonparametric rate of $n^{-1/(2+d)}$, revealsthat the effective dimension of such problems is the size of the largest cliquein the Markov random field. These rates are independent of the data's ambientdimension, making them applicable to realistic models of image, sound, video,and text data. Our results provide a novel justification for deep learning'sability to circumvent the curse of dimensionality, demonstratingdimension-independent convergence rates in these contexts.</description><author>Robert A. Vandermeulen, Wai Ming Tai, Bryon Aragam</author><pubDate>Fri, 22 Nov 2024 17:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15095v1</guid></item><item><title>The Art of Saying No: Contextual Noncompliance in Language Models</title><link>http://arxiv.org/abs/2407.12043v2</link><description>Chat-based language models are designed to be helpful, yet they should notcomply with every user request. While most existing work primarily focuses onrefusal of "unsafe" queries, we posit that the scope of noncompliance should bebroadened. We introduce a comprehensive taxonomy of contextual noncompliancedescribing when and how models should not comply with user requests. Ourtaxonomy spans a wide range of categories including incomplete, unsupported,indeterminate, and humanizing requests (in addition to unsafe requests). Totest noncompliance capabilities of language models, we use this taxonomy todevelop a new evaluation suite of 1000 noncompliance prompts. We find that mostexisting models show significantly high compliance rates in certain previouslyunderstudied categories with models like GPT-4 incorrectly complying with asmany as 30% of requests. To address these gaps, we explore different trainingstrategies using a synthetically-generated training set of requests andexpected noncompliant responses. Our experiments demonstrate that while directfinetuning of instruction-tuned models can lead to both over-refusal and adecline in general capabilities, using parameter efficient methods like lowrank adapters helps to strike a good balance between appropriate noncomplianceand other capabilities.</description><author>Faeze Brahman, Sachin Kumar, Vidhisha Balachandran, Pradeep Dasigi, Valentina Pyatkin, Abhilasha Ravichander, Sarah Wiegreffe, Nouha Dziri, Khyathi Chandu, Jack Hessel, Yulia Tsvetkov, Noah A. Smith, Yejin Choi, Hannaneh Hajishirzi</author><pubDate>Fri, 22 Nov 2024 17:48:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12043v2</guid></item><item><title>Just In Time Transformers</title><link>http://arxiv.org/abs/2410.16881v2</link><description>Precise energy load forecasting in residential households is crucial formitigating carbon emissions and enhancing energy efficiency; indeed, accurateforecasting enables utility companies and policymakers, who advocatesustainable energy practices, to optimize resource utilization. Moreover, smartmeters provide valuable information by allowing for granular insights intoconsumption patterns. Building upon available smart meter data, our study aimsto cluster consumers into distinct groups according to their energy usagebehaviours, effectively capturing a diverse spectrum of consumption patterns.Next, we design JITtrans (Just In Time transformer), a novel transformer deeplearning model that significantly improves energy consumption forecastingaccuracy, with respect to traditional forecasting methods. Extensiveexperimental results validate our claims using proprietary smart meter data.Our findings highlight the potential of advanced predictive technologies torevolutionize energy management and advance sustainable power systems: thedevelopment of efficient and eco-friendly energy solutions critically dependson such technologies.</description><author>Ahmed Ala Eddine Benali, Massimo Cafaro, Italo Epicoco, Marco Pulimeno, Enrico Junior Schioppa</author><pubDate>Fri, 22 Nov 2024 17:47:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16881v2</guid></item><item><title>Multi-Branch Generative Models for Multichannel Imaging with an Application to PET/CT Synergistic Reconstruction</title><link>http://arxiv.org/abs/2404.08748v3</link><description>This paper presents a novel approach for learned synergistic reconstructionof medical images using multi-branch generative models. Leveraging variationalautoencoders (VAEs), our model learns from pairs of images simultaneously,enabling effective denoising and reconstruction. Synergistic imagereconstruction is achieved by incorporating the trained models in a regularizerthat evaluates the distance between the images and the model. We demonstratethe efficacy of our approach on both Modified National Institute of Standardsand Technology (MNIST) and positron emission tomography (PET)/computedtomography (CT) datasets, showcasing improved image quality for low-doseimaging. Despite challenges such as patch decomposition and model limitations,our results underscore the potential of generative models for enhancing medicalimaging reconstruction.</description><author>Noel Jeffrey Pinton, Alexandre Bousse, Catherine Cheze-Le-Rest, Dimitris Visvikis</author><pubDate>Fri, 22 Nov 2024 17:42:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08748v3</guid></item><item><title>NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics</title><link>http://arxiv.org/abs/2306.06202v4</link><description>Machine learning provides a valuable tool for analyzing high-dimensionalfunctional neuroimaging data, and is proving effective in predicting variousneurological conditions, psychiatric disorders, and cognitive patterns. Infunctional magnetic resonance imaging (MRI) research, interactions betweenbrain regions are commonly modeled using graph-based representations. Thepotency of graph machine learning methods has been established across myriaddomains, marking a transformative step in data interpretation and predictivemodeling. Yet, despite their promise, the transposition of these techniques tothe neuroimaging domain has been challenging due to the expansive number ofpotential preprocessing pipelines and the large parameter search space forgraph-based dataset construction. In this paper, we introduce NeuroGraph, acollection of graph-based neuroimaging datasets, and demonstrated its utilityfor predicting multiple categories of behavioral and cognitive traits. We delvedeeply into the dataset generation search space by crafting 35 datasets thatencompass static and dynamic brain connectivity, running in excess of 15baseline methods for benchmarking. Additionally, we provide generic frameworksfor learning on both static and dynamic graphs. Our extensive experiments leadto several key observations. Notably, using correlation vectors as nodefeatures, incorporating larger number of regions of interest, and employingsparser graphs lead to improved performance. To foster further advancements ingraph-based data driven neuroimaging analysis, we offer a comprehensiveopen-source Python package that includes the benchmark datasets, baselineimplementations, model training, and standard evaluation.</description><author>Anwar Said, Roza G. Bayrak, Tyler Derr, Mudassir Shabbir, Daniel Moyer, Catie Chang, Xenofon Koutsoukos</author><pubDate>Fri, 22 Nov 2024 17:39:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06202v4</guid></item><item><title>Instance-Aware Generalized Referring Expression Segmentation</title><link>http://arxiv.org/abs/2411.15087v1</link><description>Recent works on Generalized Referring Expression Segmentation (GRES) strugglewith handling complex expressions referring to multiple distinct objects. Thisis because these methods typically employ an end-to-end foreground-backgroundsegmentation and lack a mechanism to explicitly differentiate and associatedifferent object instances to the text query. To this end, we proposeInstAlign, a method that incorporates object-level reasoning into thesegmentation process. Our model leverages both text and image inputs to extracta set of object-level tokens that capture both the semantic information in theinput prompt and the objects within the image. By modeling the text-objectalignment via instance-level supervision, each token uniquely represents anobject segment in the image, while also aligning with relevant semanticinformation from the text. Extensive experiments on the gRefCOCO and Ref-ZOMbenchmarks demonstrate that our method significantly advances state-of-the-artperformance, setting a new standard for precise and flexible GRES.</description><author>E-Ro Nguyen, Hieu Le, Dimitris Samaras, Michael Ryoo</author><pubDate>Fri, 22 Nov 2024 17:28:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15087v1</guid></item><item><title>Quantum-enhanced unsupervised image segmentation for medical images analysis</title><link>http://arxiv.org/abs/2411.15086v1</link><description>Breast cancer remains the leading cause of cancer-related mortality amongwomen worldwide, necessitating the meticulous examination of mammograms byradiologists to characterize abnormal lesions. This manual process demands highaccuracy and is often time-consuming, costly, and error-prone. Automated imagesegmentation using artificial intelligence offers a promising alternative tostreamline this workflow. However, most existing methods are supervised,requiring large, expertly annotated datasets that are not always available, andthey experience significant generalization issues. Thus, unsupervised learningmodels can be leveraged for image segmentation, but they come at a cost ofreduced accuracy, or require extensive computational resourcess. In this paper,we propose the first end-to-end quantum-enhanced framework for unsupervisedmammography medical images segmentation that balances between performanceaccuracy and computational requirements. We first introduce a quantum-inspiredimage representation that serves as an initial approximation of thesegmentation mask. The segmentation task is then formulated as a QUBO problem,aiming to maximize the contrast between the background and the tumor regionwhile ensuring a cohesive segmentation mask with minimal connected components.We conduct an extensive evaluation of quantum and quantum-inspired methods forimage segmentation, demonstrating that quantum annealing and variationalquantum circuits achieve performance comparable to classical optimizationtechniques. Notably, quantum annealing is shown to be an order of magnitudefaster than the classical optimization method in our experiments. Our findingsdemonstrate that this framework achieves performance comparable tostate-of-the-art supervised methods, including UNet-based architectures,offering a viable unsupervised alternative for breast cancer imagesegmentation.</description><author>Laia Domingo, Mahdi Chehimi</author><pubDate>Fri, 22 Nov 2024 17:28:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15086v1</guid></item><item><title>Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation</title><link>http://arxiv.org/abs/2408.15205v3</link><description>Promptable segmentation typically requires instance-specific manual promptsto guide the segmentation of each desired object. To minimize such a need,task-generic promptable segmentation has been introduced, which employs asingle task-generic prompt to segment various images of different objects inthe same task. Current methods use Multimodal Large Language Models (MLLMs) toreason detailed instance-specific prompts from a task-generic prompt forimproving segmentation accuracy. The effectiveness of this segmentation heavilydepends on the precision of these derived prompts. However, MLLMs often sufferhallucinations during reasoning, resulting in inaccurate prompting. Whileexisting methods focus on eliminating hallucinations to improve a model, weargue that MLLM hallucinations can reveal valuable contextual insights whenleveraged correctly, as they represent pre-trained large-scale knowledge beyondindividual images. In this paper, we utilize hallucinations to minetask-related information from images and verify its accuracy for enhancingprecision of the generated prompts. Specifically, we introduce an iterativePrompt-Mask Cycle generation framework (ProMaC) with a prompt generator and amask generator.The prompt generator uses a multi-scale chain of thoughtprompting, initially exploring hallucinations for extracting extendedcontextual knowledge on a test image.These hallucinations are then reduced toformulate precise instance-specific prompts, directing the mask generator toproduce masks that are consistent with task semantics by mask semanticalignment. The generated masks iteratively induce the prompt generator to focusmore on task-relevant image areas and reduce irrelevant hallucinations,resulting jointly in better prompts and masks. Experiments on 5 benchmarksdemonstrate the effectiveness of ProMaC. Code given inhttps://lwpyh.github.io/ProMaC/.</description><author>Jian Hu, Jiayi Lin, Junchi Yan, Shaogang Gong</author><pubDate>Fri, 22 Nov 2024 17:27:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15205v3</guid></item><item><title>Adaptive Communications in Collaborative Perception with Domain Alignment for Autonomous Driving</title><link>http://arxiv.org/abs/2310.00013v4</link><description>Collaborative perception among multiple connected and autonomous vehicles cangreatly enhance perceptive capabilities by allowing vehicles to exchangesupplementary information via communications. Despite advances in previousapproaches, challenges still remain due to channel variations and dataheterogeneity among collaborative vehicles. To address these issues, we proposeACC-DA, a channel-aware collaborative perception framework to dynamicallyadjust the communication graph and minimize the average transmission delaywhile mitigating the side effects from the data heterogeneity. Our noveltieslie in three aspects. We first design a transmission delay minimization method,which can construct the communication graph and minimize the transmission delayaccording to different channel information state. We then propose an adaptivedata reconstruction mechanism, which can dynamically adjust the rate-distortiontrade-off to enhance perception efficiency. Moreover, it minimizes the temporalredundancy during data transmissions. Finally, we conceive a domain alignmentscheme to align the data distribution from different vehicles, which canmitigate the domain gap between different vehicles and improve the performanceof the target task. Comprehensive experiments demonstrate the effectiveness ofour method in comparison to the existing state-of-the-art works.</description><author>Senkang Hu, Zhengru Fang, Haonan An, Guowen Xu, Yuan Zhou, Xianhao Chen, Yuguang Fang</author><pubDate>Fri, 22 Nov 2024 17:25:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00013v4</guid></item><item><title>Leapfrog Latent Consistency Model (LLCM) for Medical Images Generation</title><link>http://arxiv.org/abs/2411.15084v1</link><description>The scarcity of accessible medical image data poses a significant obstacle ineffectively training deep learning models for medical diagnosis, as hospitalsrefrain from sharing their data due to privacy concerns. In response, wegathered a diverse dataset named MedImgs, which comprises over 250,127 imagesspanning 61 disease types and 159 classes of both humans and animals fromopen-source repositories. We propose a Leapfrog Latent Consistency Model (LLCM)that is distilled from a retrained diffusion model based on the collectedMedImgs dataset, which enables our model to generate real-time high-resolutionimages. We formulate the reverse diffusion process as a probability flowordinary differential equation (PF-ODE) and solve it in latent space using theLeapfrog algorithm. This formulation enables rapid sampling withoutnecessitating additional iterations. Our model demonstrates state-of-the-artperformance in generating medical images. Furthermore, our model can befine-tuned with any custom medical image datasets, facilitating the generationof a vast array of images. Our experimental results outperform those ofexisting models on unseen dog cardiac X-ray images. Source code is available athttps://github.com/lskdsjy/LeapfrogLCM.</description><author>Lakshmikar R. Polamreddy, Kalyan Roy, Sheng-Han Yueh, Deepshikha Mahato, Shilpa Kuppili, Jialu Li, Youshan Zhang</author><pubDate>Fri, 22 Nov 2024 17:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15084v1</guid></item><item><title>Towards Speaker Identification with Minimal Dataset and Constrained Resources using 1D-Convolution Neural Network</title><link>http://arxiv.org/abs/2411.15082v1</link><description>Voice recognition and speaker identification are vital for applications insecurity and personal assistants. This paper presents a lightweight1D-Convolutional Neural Network (1D-CNN) designed to perform speakeridentification on minimal datasets. Our approach achieves a validation accuracyof 97.87%, leveraging data augmentation techniques to handle background noiseand limited training samples. Future improvements include testing on largerdatasets and integrating transfer learning methods to enhance generalizability.We provide all code, the custom dataset, and the trained models to facilitatereproducibility. These resources are available on our GitHub repository:https://github.com/IrfanNafiz/RecMe.</description><author>Irfan Nafiz Shahan, Pulok Ahmed Auvi</author><pubDate>Fri, 22 Nov 2024 17:18:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15082v1</guid></item><item><title>RankByGene: Gene-Guided Histopathology Representation Learning Through Cross-Modal Ranking Consistency</title><link>http://arxiv.org/abs/2411.15076v1</link><description>Spatial transcriptomics (ST) provides essential spatial context by mappinggene expression within tissue, enabling detailed study of cellularheterogeneity and tissue organization. However, aligning ST data with histologyimages poses challenges due to inherent spatial distortions andmodality-specific variations. Existing methods largely rely on directalignment, which often fails to capture complex cross-modal relationships. Toaddress these limitations, we propose a novel framework that aligns gene andimage features using a ranking-based alignment loss, preserving relativesimilarity across modalities and enabling robust multi-scale alignment. Tofurther enhance the alignment's stability, we employ self-supervised knowledgedistillation with a teacher-student network architecture, effectivelymitigating disruptions from high dimensionality, sparsity, and noise in geneexpression data. Extensive experiments on gene expression prediction andsurvival analysis demonstrate our framework's effectiveness, showing improvedalignment and predictive performance over existing methods and establishing arobust tool for gene-guided image representation learning in digital pathology.</description><author>Wentao Huang, Meilong Xu, Xiaoling Hu, Shahira Abousamra, Aniruddha Ganguly, Saarthak Kapse, Alisa Yurovsky, Prateek Prasanna, Tahsin Kurc, Joel Saltz, Michael L. Miller, Chao Chen</author><pubDate>Fri, 22 Nov 2024 17:08:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15076v1</guid></item><item><title>Learning to Stabilize Faces</title><link>http://arxiv.org/abs/2411.15074v1</link><description>Nowadays, it is possible to scan faces and automatically register them withhigh quality. However, the resulting face meshes often need further processing:we need to stabilize them to remove unwanted head movement. Stabilization isimportant for tasks like game development or movie making which require facialexpressions to be cleanly separated from rigid head motion. Since manualstabilization is labor-intensive, there have been attempts to automate it.However, previous methods remain impractical: they either still require somemanual input, produce imprecise alignments, rely on dubious heuristics and slowoptimization, or assume a temporally ordered input. Instead, we present a newlearning-based approach that is simple and fully automatic. We treatstabilization as a regression problem: given two face meshes, our networkdirectly predicts the rigid transform between them that brings their skullsinto alignment. We generate synthetic training data using a 3D Morphable Model(3DMM), exploiting the fact that 3DMM parameters separate skull motion fromfacial skin motion. Through extensive experiments we show that our approachoutperforms the state-of-the-art both quantitatively and qualitatively on thetasks of stabilizing discrete sets of facial expressions as well as dynamicfacial performances. Furthermore, we provide an ablation study detailing thedesign choices and best practices to help others adopt our approach for theirown uses. Supplementary videos can be found on the project webpagesyntec-research.github.io/FaceStab.</description><author>Jan Bednarik, Erroll Wood, Vasileios Choutas, Timo Bolkart, Daoye Wang, Chenglei Wu, Thabo Beeler</author><pubDate>Fri, 22 Nov 2024 17:03:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15074v1</guid></item><item><title>Single color digital H&amp;E staining with In-and-Out Net</title><link>http://arxiv.org/abs/2405.13278v2</link><description>Virtual staining streamlines traditional staining procedures by digitallygenerating stained images from unstained or differently stained images. Whileconventional staining methods involve time-consuming chemical processes,virtual staining offers an efficient and low infrastructure alternative.Leveraging microscopy-based techniques, such as confocal microscopy,researchers can expedite tissue analysis without the need for physicalsectioning. However, interpreting grayscale or pseudo-color microscopic imagesremains a challenge for pathologists and surgeons accustomed to traditionalhistologically stained images. To fill this gap, various studies exploredigitally simulating staining to mimic targeted histological stains. This paperintroduces a novel network, In-and-Out Net, specifically designed for virtualstaining tasks. Based on Generative Adversarial Networks (GAN), our modelefficiently transforms Reflectance Confocal Microscopy (RCM) images intoHematoxylin and Eosin (H&amp;E) stained images. We enhance nuclei contrast in RCMimages using aluminum chloride preprocessing for skin tissues. Training themodel with virtual H\&amp;E labels featuring two fluorescence channels eliminatesthe need for image registration and provides pixel-level ground truth. Ourcontributions include proposing an optimal training strategy, conducting acomparative analysis demonstrating state-of-the-art performance, validating themodel through an ablation study, and collecting perfectly matched input andground truth images without registration. In-and-Out Net showcases promisingresults, offering a valuable tool for virtual staining tasks and advancing thefield of histological image analysis.</description><author>Mengkun Chen, Yen-Tung Liu, Fadeel Sher Khan, Matthew C. Fox, Jason S. Reichenberg, Fabiana C. P. S. Lopes, Katherine R. Sebastian, Mia K. Markey, James W. Tunnell</author><pubDate>Fri, 22 Nov 2024 16:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13278v2</guid></item><item><title>Locating the Leading Edge of Cultural Change</title><link>http://arxiv.org/abs/2411.15068v1</link><description>Measures of textual similarity and divergence are increasingly used to studycultural change. But which measures align, in practice, with social evidenceabout change? We apply three different representations of text (topic models,document embeddings, and word-level perplexity) to three different corpora(literary studies, economics, and fiction). In every case, works byhighly-cited authors and younger authors are textually ahead of the curve. Wedon't find clear evidence that one representation of text is to be preferredover the others. But alignment with social evidence is strongest when texts arerepresented through the top quartile of passages, suggesting that a text'simpact may depend more on its most forward-looking moments than on sustaining ahigh level of innovation throughout.</description><author>Sarah Griebel, Becca Cohen, Lucian Li, Jaihyun Park, Jiayu Liu, Jana Perkins, Ted Underwood</author><pubDate>Fri, 22 Nov 2024 16:56:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15068v1</guid></item><item><title>Linear convergence of proximal descent schemes on the Wasserstein space</title><link>http://arxiv.org/abs/2411.15067v1</link><description>We investigate proximal descent methods, inspired by the minimizing movementscheme introduced by Jordan, Kinderlehrer and Otto, for optimizingentropy-regularized functionals on the Wasserstein space. We establish linearconvergence under flat convexity assumptions, thereby relaxing the commonreliance on geodesic convexity. Our analysis circumvents the need fordiscrete-time adaptations of the Evolution Variational Inequality (EVI).Instead, we leverage a uniform logarithmic Sobolev inequality (LSI) and theentropy "sandwich" lemma, extending the analysis from arXiv:2201.10469 andarXiv:2202.01009. The major challenge in the proof via LSI is to show that therelative Fisher information $I(\cdot|\pi)$ is well-defined at every step of thescheme. Since the relative entropy is not Wasserstein differentiable, we provethat along the scheme the iterates belong to a certain class of Sobolevregularity, and hence the relative entropy $\operatorname{KL}(\cdot|\pi)$ has aunique Wasserstein sub-gradient, and that the relative Fisher information isindeed finite.</description><author>Razvan-Andrei Lascu, Mateusz B. Majka, David Šiška, Łukasz Szpruch</author><pubDate>Fri, 22 Nov 2024 16:56:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15067v1</guid></item><item><title>SPAC-Net: Rethinking Point Cloud Completion with Structural Prior</title><link>http://arxiv.org/abs/2411.15066v1</link><description>Point cloud completion aims to infer a complete shape from its partialobservation. Many approaches utilize a pure encoderdecoder paradigm in whichcomplete shape can be directly predicted by shape priors learned from partialscans, however, these methods suffer from the loss of details inevitably due tothe feature abstraction issues. In this paper, we propose a novelframework,termed SPAC-Net, that aims to rethink the completion task under theguidance of a new structural prior, we call it interface. Specifically, ourmethod first investigates Marginal Detector (MAD) module to localize theinterface, defined as the intersection between the known observation and themissing parts. Based on the interface, our method predicts the coarse shape bylearning the displacement from the points in interface move to theircorresponding position in missing parts. Furthermore, we devise an additionalStructure Supplement(SSP) module before the upsampling stage to enhance thestructural details of the coarse shape, enabling the upsampling module to focusmore on the upsampling task. Extensive experiments have been conducted onseveral challenging benchmarks, and the results demonstrate that our methodoutperforms existing state-of-the-art approaches.</description><author>Zizhao Wu, Jian Shi, Xuan Deng, Cheng Zhang, Genfu Yang, Ming Zeng, Yunhai Wang</author><pubDate>Fri, 22 Nov 2024 16:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15066v1</guid></item><item><title>Empowering Clients: Transformation of Design Processes Due to Generative AI</title><link>http://arxiv.org/abs/2411.15061v1</link><description>The domain of computational design, driven by advancements in Generative AI,is transforming creative fields. We explore the transformative effects ofGenerative AI on the architectural design process and discuss the role of thearchitect. The case of architecture is interesting as designing houses iscomplex, involving extensive customer interaction. We employ a within-subjectexperiment using a popular general-purpose text-to-image tool for generatingdesigns and providing feedback on existing designs, followed by expertinterviews. The study reveals that AI can disrupt the ideation phase byenabling clients to engage in the design process through rapid visualization oftheir own ideas. In turn, the architect's role shifts more towards assessingthe feasibility of designs generated conjointly by clients and AI. Our studyalso shows that while AI can provide valuable feedback on designs, it mightfail to generate such designs, allowing for interesting connections tofoundations in computer science, i.e., NP-completeness. AI's feedback alsotends to hamper creativity and innovation by suggesting altering novel,innovative approaches toward more standardized designs. Our study also revealsthat there is uncertainty among architects about the interpretative sovereigntyof architecture and loss of meaning and identity when AI increasingly takesover authorship in the design process.</description><author>Johannes Schneider, Kilic Sinem, Daniel Stockhammer</author><pubDate>Fri, 22 Nov 2024 16:48:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15061v1</guid></item><item><title>Detecting Hallucinations in Virtual Histology with Neural Precursors</title><link>http://arxiv.org/abs/2411.15060v1</link><description>Significant biomedical research and clinical care rely on the histopathologicexamination of tissue structure using microscopy of stained tissue. Virtualstaining (VS) offers a promising alternative with the potential to reduce costand eliminate the use of toxic reagents. However, the critical challenge ofhallucinations limits confidence in its use, necessitating a VS co-pilot todetect these hallucinations. Here, we first formally establish the problem ofhallucination detection in VS. Next, we introduce a scalable, post-hochallucination detection method that identifies a Neural Hallucination Precursor(NHP) from VS model embeddings for test-time detection. We report extensivevalidation across diverse and challenging VS settings to demonstrate NHP'seffectiveness and robustness. Furthermore, we show that VS models with fewerhallucinations do not necessarily disclose them better, risking a false senseof security when reporting just the former metric. This highlights the need fora reassessment of current VS evaluation practices.</description><author>Ji-Hun Oh, Kianoush Falahkheirkhah, Rohit Bhargava</author><pubDate>Fri, 22 Nov 2024 16:46:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15060v1</guid></item><item><title>Financial Risk Assessment via Long-term Payment Behavior Sequence Folding</title><link>http://arxiv.org/abs/2411.15056v1</link><description>Online inclusive financial services encounter significant financial risks dueto their expansive user base and low default costs. By real-world practice, wereveal that utilizing longer-term user payment behaviors can enhance models'ability to forecast financial risks. However, learning long behavior sequencesis non-trivial for deep sequential models. Additionally, the diverse fields ofpayment behaviors carry rich information, requiring thorough exploitation.These factors collectively complicate the task of long-term user behaviormodeling. To tackle these challenges, we propose a Long-term Payment BehaviorSequence Folding method, referred to as LBSF. In LBSF, payment behaviorsequences are folded based on merchants, using the merchant field as anintrinsic grouping criterion, which enables informative parallelism withoutreliance on external knowledge. Meanwhile, we maximize the utility of paymentdetails through a multi-field behavior encoding mechanism. Subsequently,behavior aggregation at the merchant level followed by relational learningacross merchants facilitates comprehensive user financial representation. Weevaluate LBSF on the financial risk assessment task using a large-scalereal-world dataset. The results demonstrate that folding long behaviorsequences based on internal behavioral cues effectively models long-termpatterns and changes, thereby generating more accurate user financial profilesfor practical applications.</description><author>Yiran Qiao, Yateng Tang, Xiang Ao, Qi Yuan, Ziming Liu, Chen Shen, Xuehao Zheng</author><pubDate>Fri, 22 Nov 2024 16:43:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15056v1</guid></item><item><title>OCD-FL: A Novel Communication-Efficient Peer Selection-based Decentralized Federated Learning</title><link>http://arxiv.org/abs/2403.04037v2</link><description>The conjunction of edge intelligence and the ever-growing Internet-of-Things(IoT) network heralds a new era of collaborative machine learning, withfederated learning (FL) emerging as the most prominent paradigm. With thegrowing interest in these learning schemes, researchers started addressing someof their most fundamental limitations. Indeed, conventional FL with a centralaggregator presents a single point of failure and a network bottleneck. Tobypass this issue, decentralized FL where nodes collaborate in a peer-to-peernetwork has been proposed. Despite the latter's efficiency, communication costsand data heterogeneity remain key challenges in decentralized FL. In thiscontext, we propose a novel scheme, called opportunisticcommunication-efficient decentralized federated learning, a.k.a., OCD-FL,consisting of a systematic FL peer selection for collaboration, aiming toachieve maximum FL knowledge gain while reducing energy consumption.Experimental results demonstrate the capability of OCD-FL to achieve similar orbetter performances than the fully collaborative FL, while significantlyreducing consumed energy by at least 30% and up to 80%.</description><author>Nizar Masmoudi, Wael Jaafar</author><pubDate>Fri, 22 Nov 2024 16:42:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04037v2</guid></item><item><title>Characterizing User Archetypes and Discussions on Scored.co</title><link>http://arxiv.org/abs/2407.21753v2</link><description>In recent years, the proliferation of social platforms has drasticallytransformed the way individuals interact, organize, and share information. Inthis scenario, we experience an unprecedented increase in the scale andcomplexity of interactions and, at the same time, little to no research aboutsome fringe social platforms. In this paper, we present a multi-dimensionalframework for characterizing nodes and hyperedges in social hypernetworks, witha focus on the understudied alt-right platform Scored.co. Our approachintegrates the possibility of studying higher-order interactions, thanks to thehypernetwork representation, and various node features such as user activity,sentiment, and toxicity, with the aim to define distinct user archetypes andunderstand their roles within the network. Utilizing a comprehensive datasetfrom Scored.co, we analyze the dynamics of these archetypes over time andexplore their interactions and influence within the community. The framework'sversatility allows for detailed analysis of both individual user behaviors andbroader social structures. Our findings highlight the importance ofhigher-order interactions in understanding social dynamics, offering newinsights into the roles and behaviors that emerge in complex onlineenvironments.</description><author>Andrea Failla, Salvatore Citraro, Giulio Rossetti, Francesco Cauteruccio</author><pubDate>Fri, 22 Nov 2024 16:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21753v2</guid></item><item><title>Fantastic Biases (What are They) and Where to Find Them</title><link>http://arxiv.org/abs/2411.15051v1</link><description>Deep Learning models tend to learn correlations of patterns on huge datasets.The bigger these systems are, the more complex are the phenomena they candetect, and the more data they need for this. The use of ArtificialIntelligence (AI) is becoming increasingly ubiquitous in our society, and itsimpact is growing everyday. The promises it holds strongly depend on their fairand universal use, such as access to information or education for all. In aworld of inequalities, they can help to reach the most disadvantaged areas.However, such a universal systems must be able to represent society, withoutbenefiting some at the expense of others. We must not reproduce theinequalities observed throughout the world, but educate these IAs to go beyondthem. We have seen cases where these systems use gender, race, or even classinformation in ways that are not appropriate for resolving their tasks. Insteadof real causal reasoning, they rely on spurious correlations, which is what weusually call a bias. In this paper, we first attempt to define what is a biasin general terms. It helps us to demystify the concept of bias, to understandwhy we can find them everywhere and why they are sometimes useful. Second, wefocus over the notion of what is generally seen as negative bias, the one wewant to avoid in machine learning, before presenting a general zoologycontaining the most common of these biases. We finally conclude by looking atclassical methods to detect them, by means of specially crafted datasets oftemplates and specific algorithms, and also classical methods to mitigate them.</description><author>Valentin Barriere</author><pubDate>Fri, 22 Nov 2024 16:38:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15051v1</guid></item><item><title>Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits</title><link>http://arxiv.org/abs/2312.03720v2</link><description>Large language models LLMs like ChatGPT have reached the 100 Mio user barrierin record time and might increasingly enter all areas of our life leading to adiverse set of interactions between those Artificial Intelligence models andhumans. While many studies have discussed governance and regulationsdeductively from first-order principles, few studies provide an inductive,data-driven lens based on observing dialogues between humans and LLMsespecially when it comes to non-collaborative, competitive situations that havethe potential to pose a serious threat to people. In this work, we conduct auser study engaging over 40 individuals across all age groups in pricenegotiations with an LLM. We explore how people interact with an LLM,investigating differences in negotiation outcomes and strategies. Furthermore,we highlight shortcomings of LLMs with respect to their reasoning capabilitiesand, in turn, susceptiveness to prompt hacking, which intends to manipulate theLLM to make agreements that are against its instructions or beyond anyrationality. We also show that the negotiated prices humans manage to achievespan a broad range, which points to a literacy gap in effectively interactingwith LLMs.</description><author>Johannes Schneider, Steffi Haag, Leona Chandra Kruse</author><pubDate>Fri, 22 Nov 2024 16:34:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03720v2</guid></item><item><title>On Multi-Agent Inverse Reinforcement Learning</title><link>http://arxiv.org/abs/2411.15046v1</link><description>In multi-agent systems, the agent behavior is highly influenced by itsutility function, as these utilities shape both individual goals as well asinteractions with the other agents. Inverse Reinforcement Learning (IRL) is awell-established approach to inferring the utility function by observing anexpert behavior within a given environment. In this paper, we extend the IRLframework to the multi-agent setting, assuming to observe agents who arefollowing Nash Equilibrium (NE) policies. We theoretically investigate the setof utilities that explain the behavior of NE experts. Specifically, we providean explicit characterization of the feasible reward set and analyze how errorsin estimating the transition dynamics and expert behavior impact the recoveredrewards. Building on these findings, we provide the first sample complexityanalysis for the multi-agent IRL problem. Finally, we provide a numericalevaluation of our theoretical results.</description><author>Till Freihaut, Giorgia Ramponi</author><pubDate>Fri, 22 Nov 2024 16:31:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15046v1</guid></item><item><title>Causal Representation Learning with Generative Artificial Intelligence: Application to Texts as Treatments</title><link>http://arxiv.org/abs/2410.00903v2</link><description>In this paper, we demonstrate how to enhance the validity of causal inferencewith unstructured high-dimensional treatments like texts, by leveraging thepower of generative Artificial Intelligence. Specifically, we propose to use adeep generative model such as large language models (LLMs) to efficientlygenerate treatments and use their internal representation for subsequent causaleffect estimation. We show that the knowledge of this true internalrepresentation helps disentangle the treatment features of interest, such asspecific sentiments and certain topics, from other possibly unknown confoundingfeatures. Unlike the existing methods, our proposed approach eliminates theneed to learn causal representation from the data and hence produces moreaccurate and efficient estimates. We formally establish the conditions requiredfor the nonparametric identification of the average treatment effect, proposean estimation strategy that avoids the violation of the overlap assumption, andderive the asymptotic properties of the proposed estimator through theapplication of double machine learning. Finally, using an instrumentalvariables approach, we extend the proposed methodology to the settings, inwhich the treatment feature is based on human perception rather than is assumedto be fixed given the treatment object. The proposed methodology is alsoapplicable to text reuse where an LLM is used to regenerate the existing texts.We conduct simulation and empirical studies, using the generated text data froman open-source LLM, Llama 3, to illustrate the advantages of our estimator overthe state-of-the-art causal representation learning algorithms.</description><author>Kosuke Imai, Kentaro Nakamura</author><pubDate>Fri, 22 Nov 2024 16:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.00903v2</guid></item><item><title>OVO-SLAM: Open-Vocabulary Online Simultaneous Localization and Mapping</title><link>http://arxiv.org/abs/2411.15043v1</link><description>This paper presents the first Open-Vocabulary Online 3D semantic SLAMpipeline, that we denote as OVO-SLAM. Our primary contribution is in thepipeline itself, particularly in the mapping thread. Given a set of posed RGB-Dframes, we detect and track 3D segments, which we describe using CLIP vectors,calculated through a novel aggregation from the viewpoints where these 3Dsegments are observed. Notably, our OVO-SLAM pipeline is not only faster butalso achieves better segmentation metrics compared to offline approaches in theliterature. Along with superior segmentation performance, we show experimentalresults of our contributions integrated with Gaussian-SLAM, being the firstones demonstrating end-to-end open-vocabulary online 3D reconstructions withoutrelying on ground-truth camera poses or scene geometry.</description><author>Tomas Berriel Martins, Martin R. Oswald, Javier Civera</author><pubDate>Fri, 22 Nov 2024 16:25:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15043v1</guid></item><item><title>Robustness and Confounders in the Demographic Alignment of LLMs with Human Perceptions of Offensiveness</title><link>http://arxiv.org/abs/2411.08977v2</link><description>Large language models (LLMs) are known to exhibit demographic biases, yet fewstudies systematically evaluate these biases across multiple datasets oraccount for confounding factors. In this work, we examine LLM alignment withhuman annotations in five offensive language datasets, comprising approximately220K annotations. Our findings reveal that while demographic traits,particularly race, influence alignment, these effects are inconsistent acrossdatasets and often entangled with other factors. Confounders -- such asdocument difficulty, annotator sensitivity, and within-group agreement --account for more variation in alignment patterns than demographic traits alone.Specifically, alignment increases with higher annotator sensitivity and groupagreement, while greater document difficulty corresponds to reduced alignment.Our results underscore the importance of multi-dataset analyses andconfounder-aware methodologies in developing robust measures of demographicbias in LLMs.</description><author>Shayan Alipour, Indira Sen, Mattia Samory, Tanushree Mitra</author><pubDate>Fri, 22 Nov 2024 16:22:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08977v2</guid></item><item><title>HiBO: Hierarchical Bayesian Optimization via Adaptive Search Space Partitioning</title><link>http://arxiv.org/abs/2410.23148v3</link><description>Optimizing black-box functions in high-dimensional search spaces has beenknown to be challenging for traditional Bayesian Optimization (BO). In thispaper, we introduce HiBO, a novel hierarchical algorithm integratingglobal-level search space partitioning information into the acquisitionstrategy of a local BO-based optimizer. HiBO employs a search-tree-basedglobal-level navigator to adaptively split the search space into partitionswith different sampling potential. The local optimizer then utilizes thisglobal-level information to guide its acquisition strategy towards mostpromising regions within the search space. A comprehensive set of evaluationsdemonstrates that HiBO outperforms state-of-the-art methods in high-dimensionalsynthetic benchmarks and presents significant practical effectiveness in thereal-world task of tuning configurations of database management systems(DBMSs).</description><author>Wenxuan Li, Taiyi Wang, Eiko Yoneki</author><pubDate>Fri, 22 Nov 2024 16:18:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23148v3</guid></item><item><title>Enhancing Autonomous Driving Safety through World Model-Based Predictive Navigation and Adaptive Learning Algorithms for 5G Wireless Applications</title><link>http://arxiv.org/abs/2411.15042v1</link><description>Addressing the challenge of ensuring safety in ever-changing andunpredictable environments, particularly in the swiftly advancing realm ofautonomous driving in today's 5G wireless communication world, we presentNavigation Secure (NavSecure). This vision-based navigation framework mergesthe strengths of world models with crucial safety-focused decision-makingcapabilities, enabling autonomous vehicles to navigate real-world complexitiessecurely. Our approach anticipates potential threats and formulates saferroutes by harnessing the predictive capabilities of world models, thussignificantly reducing the need for extensive real-world trial-and-errorlearning. Additionally, our method empowers vehicles to autonomously learn anddevelop through continuous practice, ensuring the system evolves and adapts tonew challenges. Incorporating radio frequency technology, NavSecure leverages5G networks to enhance real-time data exchange, improving communication andresponsiveness. Validated through rigorous experiments under simulation-to-realdriving conditions, NavSecure has shown exceptional performance insafety-critical scenarios, such as sudden obstacle avoidance. Results indicatethat NavSecure excels in key safety metrics, including collision prevention andrisk reduction, surpassing other end-to-end methodologies. This framework notonly advances autonomous driving safety but also demonstrates how world modelscan enhance decision-making in critical applications. NavSecure sets a newstandard for developing more robust and trustworthy autonomous driving systems,capable of handling the inherent dynamics and uncertainties of real-worldenvironments.</description><author>Hong Ding, Ziming Wang, Yi Ding, Hongjie Lin, SuYang Xi, Chia Chao Kang</author><pubDate>Fri, 22 Nov 2024 16:16:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15042v1</guid></item><item><title>mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA</title><link>http://arxiv.org/abs/2411.15041v1</link><description>Advanced Multimodal Large Language Models (MLLMs) struggle with recentKnowledge-based VQA tasks, such as INFOSEEK and Encyclopedic-VQA, due to theirlimited and frozen knowledge scope, often leading to ambiguous and inaccurateresponses. Thus, multimodal Retrieval-Augmented Generation (mRAG) is naturallyintroduced to provide MLLMs with comprehensive and up-to-date knowledge,effectively expanding the knowledge scope. However, current mRAG methods haveinherent drawbacks, including: 1) Performing retrieval even when externalknowledge is not needed. 2) Lacking of identification of evidence that supportsthe query. 3) Increasing model complexity due to additional informationfiltering modules or rules. To address these shortcomings, we propose a novelgeneralized framework called \textbf{m}ultimodal\textbf{R}etrieval-\textbf{R}eflection-\textbf{A}ugmented \textbf{G}eneration(mR$^2$AG), which achieves adaptive retrieval and useful informationlocalization to enable answers through two easy-to-implement reflectionoperations, preventing high model complexity. In mR$^2$AG, Retrieval-Reflectionis designed to distinguish different user queries and avoids redundantretrieval calls, and Relevance-Reflection is introduced to guide the MLLM inlocating beneficial evidence of the retrieved content and generating answersaccordingly. In addition, mR$^2$AG can be integrated into any well-trained MLLMwith efficient fine-tuning on the proposed mR$^2$AG Instruction-Tuning dataset(mR$^2$AG-IT). mR$^2$AG significantly outperforms state-of-the-art MLLMs (e.g.,GPT-4v/o) and RAG-based MLLMs on INFOSEEK and Encyclopedic-VQA, whilemaintaining the exceptional capabilities of base MLLMs across a wide range ofVisual-dependent tasks.</description><author>Tao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen, Zhongang Qi, Chunfeng Yuan, Bing Li, Junfu Pu, Yuxuan Zhao, Zehua Xie, Jin Ma, Ying Shan, Weiming Hu</author><pubDate>Fri, 22 Nov 2024 16:15:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15041v1</guid></item><item><title>Can Features for Phishing URL Detection Be Trusted Across Diverse Datasets? A Case Study with Explainable AI</title><link>http://arxiv.org/abs/2411.09813v2</link><description>Phishing has been a prevalent cyber threat that manipulates users intorevealing sensitive private information through deceptive tactics, designed tomasquerade as trustworthy entities. Over the years, proactively detection ofphishing URLs (or websites) has been established as an widely-accepted defenseapproach. In literature, we often find supervised Machine Learning (ML) modelswith highly competitive performance for detecting phishing websites based onthe extracted features from both phishing and benign (i.e., legitimate)websites. However, it is still unclear if these features or indicators aredependent on a particular dataset or they are generalized for overall phishingdetection. In this paper, we delve deeper into this issue by analyzing twopublicly available phishing URL datasets, where each dataset has its own set ofunique and overlapping features related to URL string and website contents. Wewant to investigate if overlapping features are similar in nature acrossdatasets and how does the model perform when trained on one dataset and testedon the other. We conduct practical experiments and leverage explainable AI(XAI) methods such as SHAP plots to provide insights into different features'contributions in case of phishing detection to answer our primary question,"Can features for phishing URL detection be trusted across diverse dataset?".Our case study experiment results show that features for phishing URL detectioncan often be dataset-dependent and thus may not be trusted across differentdatasets even though they share same set of feature behaviors.</description><author>Maraz Mia, Darius Derakhshan, Mir Mehedi A. Pritom</author><pubDate>Fri, 22 Nov 2024 16:14:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09813v2</guid></item><item><title>Continuous Generative Neural Networks: A Wavelet-Based Architecture in Function Spaces</title><link>http://arxiv.org/abs/2205.14627v3</link><description>In this work, we present and study Continuous Generative Neural Networks(CGNNs), namely, generative models in the continuous setting: the output of aCGNN belongs to an infinite-dimensional function space. The architecture isinspired by DCGAN, with one fully connected layer, several convolutional layersand nonlinear activation functions. In the continuous $L^2$ setting, thedimensions of the spaces of each layer are replaced by the scales of amultiresolution analysis of a compactly supported wavelet. We presentconditions on the convolutional filters and on the nonlinearity that guaranteethat a CGNN is injective. This theory finds applications to inverse problems,and allows for deriving Lipschitz stability estimates for (possibly nonlinear)infinite-dimensional inverse problems with unknowns belonging to the manifoldgenerated by a CGNN. Several numerical simulations, including signaldeblurring, illustrate and validate this approach.</description><author>Giovanni S. Alberti, Matteo Santacesaria, Silvia Sciutto</author><pubDate>Fri, 22 Nov 2024 16:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.14627v3</guid></item><item><title>Safe Multi-Agent Reinforcement Learning with Convergence to Generalized Nash Equilibrium</title><link>http://arxiv.org/abs/2411.15036v1</link><description>Multi-agent reinforcement learning (MARL) has achieved notable success incooperative tasks, demonstrating impressive performance and scalability.However, deploying MARL agents in real-world applications presents criticalsafety challenges. Current safe MARL algorithms are largely based on theconstrained Markov decision process (CMDP) framework, which enforcesconstraints only on discounted cumulative costs and lacks an all-time safetyassurance. Moreover, these methods often overlook the feasibility issue (thesystem will inevitably violate state constraints within certain regions of theconstraint set), resulting in either suboptimal performance or increasedconstraint violations. To address these challenges, we propose a noveltheoretical framework for safe MARL with $\textit{state-wise}$ constraints,where safety requirements are enforced at every state the agents visit. Toresolve the feasibility issue, we leverage a control-theoretic notion of thefeasible region, the controlled invariant set (CIS), characterized by thesafety value function. We develop a multi-agent method for identifying CISs,ensuring convergence to a Nash equilibrium on the safety value function. Byincorporating CIS identification into the learning process, we introduce amulti-agent dual policy iteration algorithm that guarantees convergence to ageneralized Nash equilibrium in state-wise constrained cooperative Markovgames, achieving an optimal balance between feasibility and performance.Furthermore, for practical deployment in complex high-dimensional systems, wepropose $\textit{Multi-Agent Dual Actor-Critic}$ (MADAC), a safe MARL algorithmthat approximates the proposed iteration scheme within the deep RL paradigm.Empirical evaluations on safe MARL benchmarks demonstrate that MADACconsistently outperforms existing methods, delivering much higher rewards whilereducing constraint violations.</description><author>Zeyang Li, Navid Azizan</author><pubDate>Fri, 22 Nov 2024 16:08:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15036v1</guid></item><item><title>HeadRouter: A Training-free Image Editing Framework for MM-DiTs by Adaptively Routing Attention Heads</title><link>http://arxiv.org/abs/2411.15034v1</link><description>Diffusion Transformers (DiTs) have exhibited robust capabilities in imagegeneration tasks. However, accurate text-guided image editing for multimodalDiTs (MM-DiTs) still poses a significant challenge. Unlike UNet-basedstructures that could utilize self/cross-attention maps for semantic editing,MM-DiTs inherently lack support for explicit and consistent incorporated textguidance, resulting in semantic misalignment between the edited results andtexts. In this study, we disclose the sensitivity of different attention headsto different image semantics within MM-DiTs and introduce HeadRouter, atraining-free image editing framework that edits the source image by adaptivelyrouting the text guidance to different attention heads in MM-DiTs. Furthermore,we present a dual-token refinement module to refine text/image tokenrepresentations for precise semantic guidance and accurate region expression.Experimental results on multiple benchmarks demonstrate HeadRouter'sperformance in terms of editing fidelity and image quality.</description><author>Yu Xu, Fan Tang, Juan Cao, Yuxin Zhang, Xiaoyu Kong, Jintao Li, Oliver Deussen, Tong-Yee Lee</author><pubDate>Fri, 22 Nov 2024 16:08:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15034v1</guid></item><item><title>Scrapping The Web For Early Wildfire Detection: A New Annotated Dataset of Images and Videos of Smoke Plumes In-the-wild</title><link>http://arxiv.org/abs/2402.05349v2</link><description>Early wildfire detection is of the utmost importance to enable rapid responseefforts, and thus minimize the negative impacts of wildfire spreads. To thisend, we present PyroNear-2024, a new dataset composed of both images andvideos, allowing for the training and evaluation of smoke plume detectionmodels, including sequential models. The data is sourced from: \textit{(i)}web-scraped videos of wildfires from public networks of cameras for wildfiredetection in-the-wild, \text{(ii)} videos from our in-house network of cameras,and \textit{(iii)} a small portion of synthetic and real images. This datasetincludes around 150,000 manual annotations on 50,000 images, covering 400wildfires, \Pyro surpasses existing datasets in size and diversity. It includesdata from France, Spain, and the United States. Finally, it is composed of bothimages and videos, allowing for the training and evaluation of smoke plumedetection models, including sequential models. We ran cross-dataset experimentsusing a lightweight state-of-the-art object detection model and found out theproposed dataset is particularly challenging, with F1 score of around 60%, butmore stable than existing datasets. The video part of the dataset can be usedto train a lightweight sequential model, improving global recall whilemaintaining precision. Finally, its use in concordance with other publicdataset helps to reach higher results overall. We will make both our code anddata available.</description><author>Mateo Lostanlen, Nicolas Isla, Jose Guillen, Felix Veith, Cristian Buc, Valentin Barriere</author><pubDate>Fri, 22 Nov 2024 16:07:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05349v2</guid></item><item><title>One to rule them all: natural language to bind communication, perception and action</title><link>http://arxiv.org/abs/2411.15033v1</link><description>In recent years, research in the area of human-robot interaction has focusedon developing robots capable of understanding complex human instructions andperforming tasks in dynamic and diverse environments. These systems have a widerange of applications, from personal assistance to industrial robotics,emphasizing the importance of robots interacting flexibly, naturally and safelywith humans. This paper presents an advanced architecture for robotic actionplanning that integrates communication, perception, and planning with LargeLanguage Models (LLMs). Our system is designed to translate commands expressedin natural language into executable robot actions, incorporating environmentalinformation and dynamically updating plans based on real-time feedback. ThePlanner Module is the core of the system where LLMs embedded in a modifiedReAct framework are employed to interpret and carry out user commands. Byleveraging their extensive pre-trained knowledge, LLMs can effectively processuser requests without the need to introduce new knowledge on the changingenvironment. The modified ReAct framework further enhances the execution spaceby providing real-time environmental perception and the outcomes of physicalactions. By combining robust and dynamic semantic map representations as graphswith control components and failure explanations, this architecture enhances arobot adaptability, task execution, and seamless collaboration with human usersin shared and dynamic environments. Through the integration of continuousfeedback loops with the environment the system can dynamically adjusts the planto accommodate unexpected changes, optimizing the robot ability to performtasks. Using a dataset of previous experience is possible to provide detailedfeedback about the failure. Updating the LLMs context of the next iterationwith suggestion on how to overcame the issue.</description><author>Simone Colombani, Dimitri Ognibene, Giuseppe Boccignone</author><pubDate>Fri, 22 Nov 2024 16:05:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15033v1</guid></item><item><title>ArcSin: Adaptive ranged cosine Similarity injected noise for Language-Driven Visual Tasks</title><link>http://arxiv.org/abs/2402.17298v2</link><description>"A data scientist is tasked with developing a low-cost surgical VQA systemfor a 2-month workshop. Due to data sensitivity, she collects 50 hours ofsurgical video from a hospital, requiring two months for privacy approvals.Privacy restrictions prevent uploading data to platforms like ChatGPT, so sheassembles one annotator and a medical expert to manually create QA pairs. Thisprocess takes three weeks and costs over $10,000. The trained model providesaccurate responses within the limited data scope but lacks broadergeneralizability, completing the project in 3 months." To simplify the challenges presented in the scenario above. In this paper, wereplace the image input with text for Vision-language training. Inspired byprior noise injection methods to reduce modality gaps, we introduce Adaptiveranged cosine Similarity injected noise (ArcSin). First, we introduce aninnovative adaptive noise scale that effectively generates the textual elementswith more variability while preserving the original text feature's integrity.Second, a similarity pool strategy is employed, expanding the domaingeneralization potential by broadening the overall noise scale. This dualstrategy effectively broadens the scope of the original domain whilesafeguarding content integrity. Our empirical results demonstrate that thesemodels closely rival those trained on images in terms of performance.Specifically, our method exhibits substantial improvements over the previousstate-of-the-art, achieving gains of 1.9 and 1.1 CIDEr points in S-Cap andM-Cap, respectively. Additionally, we observe increases of 0.5 percentagepoints (pp), 1.4 pp, and 1.4 pp in accuracy for VQA, VQA-E, and VE,respectively, pushing the boundaries of what is achievable within theconstraints of image-trained model benchmarks.</description><author>Yang Liu, Xiaomin Yu, Gongyu Zhang, Zhen Zhu, Christos Bergeles, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin</author><pubDate>Fri, 22 Nov 2024 16:05:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17298v2</guid></item><item><title>Controlling Language and Diffusion Models by Transporting Activations</title><link>http://arxiv.org/abs/2410.23054v2</link><description>The increasing capabilities of large generative models and their ever morewidespread deployment have raised concerns about their reliability, safety, andpotential misuse. To address these issues, recent works have proposed tocontrol model generation by steering model activations in order to effectivelyinduce or prevent the emergence of concepts or behaviors in the generatedoutput. In this paper we introduce Activation Transport (AcT), a generalframework to steer activations guided by optimal transport theory thatgeneralizes many previous activation-steering works. AcT is modality-agnosticand provides fine-grained control over the model behavior with negligiblecomputational overhead, while minimally impacting model abilities. Weexperimentally show the effectiveness and versatility of our approach byaddressing key challenges in large language models (LLMs) and text-to-imagediffusion models (T2Is). For LLMs, we show that AcT can effectively mitigatetoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,we show how AcT enables fine-grained style control and concept negation.</description><author>Pau Rodriguez, Arno Blaas, Michal Klein, Luca Zappella, Nicholas Apostoloff, Marco Cuturi, Xavier Suau</author><pubDate>Fri, 22 Nov 2024 16:04:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23054v2</guid></item><item><title>FloAt: Flow Warping of Self-Attention for Clothing Animation Generation</title><link>http://arxiv.org/abs/2411.15028v1</link><description>We propose a diffusion model-based approach, FloAtControlNet to generatecinemagraphs composed of animations of human clothing. We focus on humanclothing like dresses, skirts and pants. The input to our model is a textprompt depicting the type of clothing and the texture of clothing like leopard,striped, or plain, and a sequence of normal maps that capture the underlyinganimation that we desire in the output. The backbone of our method is anormal-map conditioned ControlNet which is operated in a training-free regime.The key observation is that the underlying animation is embedded in the flow ofthe normal maps. We utilize the flow thus obtained to manipulate theself-attention maps of appropriate layers. Specifically, the self-attentionmaps of a particular layer and frame are recomputed as a linear combination ofitself and the self-attention maps of the same layer and the previous frame,warped by the flow on the normal maps of the two frames. We show thatmanipulating the self-attention maps greatly enhances the quality of theclothing animation, making it look more natural as well as suppressing thebackground artifacts. Through extensive experiments, we show that the methodproposed beats all baselines both qualitatively in terms of visual results anduser study. Specifically, our method is able to alleviate the backgroundflickering that exists in other diffusion model-based baselines that weconsider. In addition, we show that our method beats all baselines in terms ofRMSE and PSNR computed using the input normal map sequences and the normal mapsequences obtained from the output RGB frames. Further, we show thatwell-established evaluation metrics like LPIPS, SSIM, and CLIP scores that aregenerally for visual quality are not necessarily suitable for capturing thesubtle motions in human clothing animations.</description><author>Swasti Shreya Mishra, Kuldeep Kulkarni, Duygu Ceylan, Balaji Vasan Srinivasan</author><pubDate>Fri, 22 Nov 2024 15:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15028v1</guid></item><item><title>Semantically-Prompted Language Models Improve Visual Descriptions</title><link>http://arxiv.org/abs/2306.06077v4</link><description>Language-vision models like CLIP have made significant strides in visiontasks, such as zero-shot image classification (ZSIC). However, generatingspecific and expressive visual descriptions remains challenging; descriptionsproduced by current methods are often ambiguous and lacking in granularity. Totackle these issues, we propose V-GLOSS: Visual Glosses, a novel method builtupon two key ideas. The first is Semantic Prompting, which conditions alanguage model on structured semantic knowledge. The second is a newcontrastive algorithm that elicits fine-grained distinctions between similarconcepts. With both ideas, we demonstrate that V-GLOSS improves visualdescriptions and achieves strong results in the zero-shot setting on generaland fine-grained image-classification datasets, including ImageNet, STL-10,FGVC Aircraft, and Flowers 102. Moreover, these descriptive capabilitiescontribute to enhancing image-generation performance. Finally, we introduce aquality-tested silver dataset with descriptions generated with V-GLOSS for allImageNet classes.</description><author>Michael Ogezi, Bradley Hauer, Grzegorz Kondrak</author><pubDate>Fri, 22 Nov 2024 15:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06077v4</guid></item><item><title>Time is on my sight: scene graph filtering for dynamic environment perception in an LLM-driven robot</title><link>http://arxiv.org/abs/2411.15027v1</link><description>Robots are increasingly being used in dynamic environments like workplaces,hospitals, and homes. As a result, interactions with robots must be simple andintuitive, with robots perception adapting efficiently to human-inducedchanges. This paper presents a robot control architecture that addresses keychallenges in human-robot interaction, with a particular focus on the dynamiccreation and continuous update of the robot state representation. Thearchitecture uses Large Language Models to integrate diverse informationsources, including natural language commands, robotic skills representation,real-time dynamic semantic mapping of the perceived scene. This enablesflexible and adaptive robotic behavior in complex, dynamic environments.Traditional robotic systems often rely on static, pre-programmed instructionsand settings, limiting their adaptability to dynamic environments and real-timecollaboration. In contrast, this architecture uses LLMs to interpret complex,high-level instructions and generate actionable plans that enhance human-robotcollaboration. At its core, the system Perception Module generates andcontinuously updates a semantic scene graph using RGB-D sensor data, providinga detailed and structured representation of the environment. A particle filteris employed to ensure accurate object localization in dynamic, real-worldsettings. The Planner Module leverages this up-to-date semantic map to breakdown high-level tasks into sub-tasks and link them to robotic skills such asnavigation, object manipulation (e.g., PICK and PLACE), and movement (e.g.,GOTO). By combining real-time perception, state tracking, and LLM-drivencommunication and task planning, the architecture enhances adaptability, taskefficiency, and human-robot collaboration in dynamic environments.</description><author>Simone Colombani, Luca Brini, Dimitri Ognibene, Giuseppe Boccignone</author><pubDate>Fri, 22 Nov 2024 15:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15027v1</guid></item><item><title>DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models</title><link>http://arxiv.org/abs/2411.15024v1</link><description>Video large language models (VLLMs) have significantly advanced recently inprocessing complex video content, yet their inference efficiency remainsconstrained because of the high computational cost stemming from the thousandsof visual tokens generated from the video inputs. We empirically observe that,unlike single image inputs, VLLMs typically attend visual tokens from differentframes at different decoding iterations, making a one-shot pruning strategyprone to removing important tokens by mistake. Motivated by this, we presentDyCoke, a training-free token compression method to optimize tokenrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-playtemporal compression module to minimize temporal redundancy by mergingredundant tokens across frames, and applies dynamic KV cache reduction to prunespatially redundant tokens selectively. It ensures high-quality inference bydynamically retaining the critical tokens at each decoding step. Extensiveexperimental results demonstrate that DyCoke can outperform the prior SoTAcounterparts, achieving 1.5X inference speedup, 1.4X memory reduction againstthe baseline VLLM, while still improving the performance, with no training.</description><author>Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang</author><pubDate>Fri, 22 Nov 2024 15:55:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15024v1</guid></item><item><title>Hierarchical localization with panoramic views and triplet loss functions</title><link>http://arxiv.org/abs/2404.14117v2</link><description>The main objective of this paper is to tackle visual localization, which isessential for the safe navigation of mobile robots. The solution we proposeemploys panoramic images and triplet convolutional neural networks. We seek toexploit the properties of such architectures to address both hierarchical andglobal localization in indoor environments, which are prone to visual aliasingand other phenomena. Considering their importance in these architectures, acomplete comparative evaluation of different triplet loss functions isperformed. The experimental section proves that triplet networks can be trainedwith a relatively low number of images captured under a specific lightingcondition and even so, the resulting networks are a robust tool to performvisual localization under dynamic conditions. Our approach has been evaluatedagainst some of these effects, such as changes in the lighting conditions,occlusions, noise and motion blurring. Furthermore, to explore the limits ofour approach, triplet networks have been tested in different indoorenvironments simultaneously. In all the cases, these architectures havedemonstrated a great capability to generalize to diverse and challengingscenarios. The code used in the experiments is available athttps://github.com/MarcosAlfaro/TripletNetworksIndoorLocalization.git.</description><author>Marcos Alfaro, Juan José Cabrera, María Flores, Óscar Reinoso, Luis Payá</author><pubDate>Fri, 22 Nov 2024 15:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14117v2</guid></item><item><title>TDSM: Triplet Diffusion for Skeleton-Text Matching in Zero-Shot Action Recognition</title><link>http://arxiv.org/abs/2411.10745v2</link><description>We firstly present a diffusion-based action recognition with zero-shotlearning for skeleton inputs. In zero-shot skeleton-based action recognition,aligning skeleton features with the text features of action labels is essentialfor accurately predicting unseen actions. Previous methods focus on directalignment between skeleton and text latent spaces, but the modality gapsbetween these spaces hinder robust generalization learning. Motivated from theremarkable performance of text-to-image diffusion models, we leverage theiralignment capabilities between different modalities mostly by focusing on thetraining process during reverse diffusion rather than using their generativepower. Based on this, our framework is designed as a Triplet Diffusion forSkeleton-Text Matching (TDSM) method which aligns skeleton features with textprompts through reverse diffusion, embedding the prompts into the unifiedskeleton-text latent space to achieve robust matching. To enhancediscriminative power, we introduce a novel triplet diffusion (TD) loss thatencourages our TDSM to correct skeleton-text matches while pushing apartincorrect ones. Our TDSM significantly outperforms the very recentstate-of-the-art methods with large margins of 2.36%-point to 13.05%-point,demonstrating superior accuracy and scalability in zero-shot settings througheffective skeleton-text matching.</description><author>Jeonghyeok Do, Munchurl Kim</author><pubDate>Fri, 22 Nov 2024 15:49:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.10745v2</guid></item><item><title>Neural 4D Evolution under Large Topological Changes from 2D Images</title><link>http://arxiv.org/abs/2411.15018v1</link><description>In the literature, it has been shown that the evolution of the known explicit3D surface to the target one can be learned from 2D images using theinstantaneous flow field, where the known and target 3D surfaces may largelydiffer in topology. We are interested in capturing 4D shapes whose topologychanges largely over time. We encounter that the straightforward extension ofthe existing 3D-based method to the desired 4D case performs poorly. In this work, we address the challenges in extending 3D neural evolution to4D under large topological changes by proposing two novel modifications. Moreprecisely, we introduce (i) a new architecture to discretize and encode thedeformation and learn the SDF and (ii) a technique to impose the temporalconsistency. (iii) Also, we propose a rendering scheme for color predictionbased on Gaussian splatting. Furthermore, to facilitate learning directly from2D images, we propose a learning framework that can disentangle the geometryand appearance from RGB images. This method of disentanglement, while alsouseful for the 4D evolution problem that we are concentrating on, is also noveland valid for static scenes. Our extensive experiments on various data provideawesome results and, most importantly, open a new approach towardreconstructing challenging scenes with significant topological changes anddeformations. Our source code and the dataset are publicly available athttps://github.com/insait-institute/N4DE.</description><author>AmirHossein Naghi Razlighi, Tiago Novello, Asen Nachkov, Thomas Probst, Danda Paudel</author><pubDate>Fri, 22 Nov 2024 15:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15018v1</guid></item><item><title>Structural Group Unfairness: Measurement and Mitigation by means of the Effective Resistance</title><link>http://arxiv.org/abs/2305.03223v3</link><description>Social networks contribute to the distribution of social capital, defined asthe relationships, norms of trust and reciprocity within a community or societythat facilitate cooperation and collective action. Therefore, better positionedmembers in a social network benefit from faster access to diverse informationand higher influence on information dissemination. A variety of methods havebeen proposed in the literature to measure social capital at an individuallevel. However, there is a lack of methods to quantify social capital at agroup level, which is particularly important when the groups are defined on thegrounds of protected attributes. To fill this gap, we propose to measure thesocial capital of a group of nodes by means of the effective resistance andemphasize the importance of considering the entire network topology. Groundedin spectral graph theory, we introduce three effective resistance-basedmeasures of group social capital, namely group isolation, group diameter andgroup control, where the groups are defined according to the value of aprotected attribute. We denote the social capital disparity among differentgroups in a network as structural group unfairness, and propose to mitigate itby means of a budgeted edge augmentation heuristic that systematicallyincreases the social capital of the most disadvantaged group. In experiments onreal-world networks, we uncover significant levels of structural groupunfairness when using gender as the protected attribute, with females being themost disadvantaged group in comparison to males. We also illustrate how ourproposed edge augmentation approach is able to not only effectively mitigatethe structural group unfairness but also increase the social capital of allgroups in the network.</description><author>Adrian Arnaiz-Rodriguez, Georgina Curto, Nuria Oliver</author><pubDate>Fri, 22 Nov 2024 15:46:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03223v3</guid></item><item><title>MSSF: A 4D Radar and Camera Fusion Framework With Multi-Stage Sampling for 3D Object Detection in Autonomous Driving</title><link>http://arxiv.org/abs/2411.15016v1</link><description>As one of the automotive sensors that have emerged in recent years, 4Dmillimeter-wave radar has a higher resolution than conventional 3D radar andprovides precise elevation measurements. But its point clouds are still sparseand noisy, making it challenging to meet the requirements of autonomousdriving. Camera, as another commonly used sensor, can capture rich semanticinformation. As a result, the fusion of 4D radar and camera can provide anaffordable and robust perception solution for autonomous driving systems.However, previous radar-camera fusion methods have not yet been thoroughlyinvestigated, resulting in a large performance gap compared to LiDAR-basedmethods. Specifically, they ignore the feature-blurring problem and do notdeeply interact with image semantic information. To this end, we present asimple but effective multi-stage sampling fusion (MSSF) network based on 4Dradar and camera. On the one hand, we design a fusion block that can deeplyinteract point cloud features with image features, and can be applied tocommonly used single-modal backbones in a plug-and-play manner. The fusionblock encompasses two types, namely, simple feature fusion (SFF) and multiscaledeformable feature fusion (MSDFF). The SFF is easy to implement, while theMSDFF has stronger fusion abilities. On the other hand, we propose asemantic-guided head to perform foreground-background segmentation on voxelswith voxel feature re-weighting, further alleviating the problem of featureblurring. Extensive experiments on the View-of-Delft (VoD) and TJ4DRadsetdatasets demonstrate the effectiveness of our MSSF. Notably, compared tostate-of-the-art methods, MSSF achieves a 7.0% and 4.0% improvement in 3D meanaverage precision on the VoD and TJ4DRadSet datasets, respectively. It evensurpasses classical LiDAR-based methods on the VoD dataset.</description><author>Hongsi Liu, Jun Liu, Guangfeng Jiang, Xin Jin</author><pubDate>Fri, 22 Nov 2024 15:45:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15016v1</guid></item><item><title>On the Linear Speedup of Personalized Federated Reinforcement Learning with Shared Representations</title><link>http://arxiv.org/abs/2411.15014v1</link><description>Federated reinforcement learning (FedRL) enables multiple agents tocollaboratively learn a policy without sharing their local trajectoriescollected during agent-environment interactions. However, in practice, theenvironments faced by different agents are often heterogeneous, leading to poorperformance by the single policy learned by existing FedRL algorithms onindividual agents. In this paper, we take a further step and introduce a\emph{personalized} FedRL framework (PFedRL) by taking advantage of possiblyshared common structure among agents in heterogeneous environments.Specifically, we develop a class of PFedRL algorithms named PFedRL-Rep thatlearns (1) a shared feature representation collaboratively among all agents,and (2) an agent-specific weight vector personalized to its local environment.We analyze the convergence of PFedTD-Rep, a particular instance of theframework with temporal difference (TD) learning and linear representations. Tothe best of our knowledge, we are the first to prove a linear convergencespeedup with respect to the number of agents in the PFedRL setting. To achievethis, we show that PFedTD-Rep is an example of the federated two-timescalestochastic approximation with Markovian noise. Experimental results demonstratethat PFedTD-Rep, along with an extension to the control setting based on deepQ-networks (DQN), not only improve learning in heterogeneous settings, but alsoprovide better generalization to new environments.</description><author>Guojun Xiong, Shufan Wang, Daniel Jiang, Jian Li</author><pubDate>Fri, 22 Nov 2024 15:42:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15014v1</guid></item><item><title>What Do GNNs Actually Learn? Towards Understanding their Representations</title><link>http://arxiv.org/abs/2304.10851v2</link><description>In recent years, graph neural networks (GNNs) have achieved great success inthe field of graph representation learning. Although prior work has shed lighton the expressiveness of those models (\ie whether they can distinguish pairsof non-isomorphic graphs), it is still not clear what structural information isencoded into the node representations that are learned by those models. In thispaper, we address this gap by studying the node representations learned by fourstandard GNN models. We find that some models produce identical representationsfor all nodes, while the representations learned by other models are linked tosome notion of walks of specific length that start from the nodes. We establishLipschitz bounds for these models with respect to the number of (normalized)walks. Additionally, we investigate the influence of node features on thelearned representations. We find that if the initial representations of allnodes point in the same direction, the representations learned at the $k$-thlayer of the models are also related to the initial features of nodes that canbe reached in exactly $k$ steps. We also apply our findings to understand thephenomenon of oversquashing that occurs in GNNs. Our theoretical analysis isvalidated through experiments on synthetic and real-world datasets.</description><author>Giannis Nikolentzos, Michail Chatzianastasis, Michalis Vazirgiannis</author><pubDate>Fri, 22 Nov 2024 15:41:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10851v2</guid></item><item><title>Natural Language Processing RELIES on Linguistics</title><link>http://arxiv.org/abs/2405.05966v3</link><description>Large Language Models (LLMs) have become capable of generating highly fluenttext in certain languages, without modules specially designed to capturegrammar or semantic coherence. What does this mean for the future of linguisticexpertise in NLP? We highlight several aspects in which NLP (still) relies onlinguistics, or where linguistic thinking can illuminate new directions. Weargue our case around the acronym RELIES that encapsulates six major facetswhere linguistics contributes to NLP: Resources, Evaluation, Low-resourcesettings, Interpretability, Explanation, and the Study of language. This listis not exhaustive, nor is linguistics the main point of reference for everyeffort under these themes; but at a macro level, these facets highlight theenduring importance of studying machine systems vis-\`a-vis systems of humanlanguage.</description><author>Juri Opitz, Shira Wein, Nathan Schneider</author><pubDate>Fri, 22 Nov 2024 15:36:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05966v3</guid></item><item><title>BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel View Synthesis</title><link>http://arxiv.org/abs/2411.08508v2</link><description>We present billboard Splatting (BBSplat) - a novel approach for 3D scenerepresentation based on textured geometric primitives. BBSplat represents thescene as a set of optimizable textured planar primitives with learnable RGBtextures and alpha-maps to control their shape. BBSplat primitives can be usedin any Gaussian Splatting pipeline as drop-in replacements for Gaussians. Ourmethod's qualitative and quantitative improvements over 3D and 2D Gaussians aremost noticeable when fewer primitives are used, when BBSplat achieves over 1200FPS. Our novel regularization term encourages textures to have a sparserstructure, unlocking an efficient compression that leads to a reduction instorage space of the model. Our experiments show the efficiency of BBSplat onstandard datasets of real indoor and outdoor scenes such as Tanks&amp;Temples, DTU,and Mip-NeRF-360. We demonstrate improvements on PSNR, SSIM, and LPIPS metricscompared to the state-of-the-art, especially for the case when fewer primitivesare used, which, on the other hand, leads to up to 2 times inference speedimprovement for the same rendering quality.</description><author>David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue</author><pubDate>Fri, 22 Nov 2024 15:35:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08508v2</guid></item><item><title>Evolutionary Automata and Deep Evolutionary Computation</title><link>http://arxiv.org/abs/2411.15008v1</link><description>Evolution by natural selection, which is one of the most compelling themes ofmodern science, brought forth evolutionary algorithms and evolutionarycomputation, applying mechanisms of evolution in nature to various problemssolved by computers. In this paper we concentrate on evolutionary automata thatconstitute an analogous model of evolutionary computation compared towell-known evolutionary algorithms. Evolutionary automata provide a morecomplete dual model of evolutionary computation, similar like abstract automata(e.g., Turing machines) form a more formal and precise model compared torecursive algorithms and their subset - evolutionary algorithms. Anevolutionary automaton is an automaton that evolves performing evolutionarycomputation perhaps using an infinite number of generations. This model allowsfor a direct modeling evolution of evolution, and leads to tremendousexpressiveness of evolutionary automata and evolutionary computation. This alsogives the hint to the power of natural evolution that is self-evolving byinteractive feedback with the environment.</description><author>Eugene Eberbach</author><pubDate>Fri, 22 Nov 2024 15:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15008v1</guid></item><item><title>FTA generation using GenAI with an Autonomy sensor Usecase</title><link>http://arxiv.org/abs/2411.15007v1</link><description>Functional safety forms an important aspect in the design of systems. Itsemphasis on the automotive industry has evolved significantly over the years.Till date many methods have been developed to get appropriate FTA(Fault Treeanalysis) for various scenarios and features pertaining to Autonomous Driving.This paper is an attempt to explore the scope of using Generative ArtificialIntelligence(GenAI) in order to develop Fault Tree Analysis(FTA) with the usecase of malfunction for the Lidar sensor in mind. We explore various availableopen source Large Language Models(LLM) models and then dive deep into one ofthem to study its responses and provide our analysis. This paper successfullyshows the possibility to train existing Large Language models through PromptEngineering for fault tree analysis for any Autonomy usecase aided withPlantUML tool.</description><author>Sneha Sudhir Shetiya, Divya Garikapati, Veeraja Sohoni</author><pubDate>Fri, 22 Nov 2024 15:31:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15007v1</guid></item><item><title>ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data</title><link>http://arxiv.org/abs/2411.15004v1</link><description>Large Language Model (LLM) agents are rapidly improving to handleincreasingly complex web-based tasks. Most of these agents rely ongeneral-purpose, proprietary models like GPT-4 and focus on designing betterprompts to improve their planning abilities. However, general-purpose LLMs arenot specifically trained to understand specialized web contexts such as HTML,and they often struggle with long-horizon planning. We explore an alternativeapproach that fine-tunes open-source LLMs using production-scale workflow datacollected from over 250 domains corresponding to 6 billion tokens. This simpleyet effective approach shows substantial gains over prompting-based agents onexisting benchmarks -- ScribeAgent achieves state-of-the-art direct generationperformance on Mind2Web and improves the task success rate by 14.1% over theprevious best text-only web agents on WebArena. We further perform detailedablation studies on various fine-tuning design choices and provide insightsinto LLM selection, training recipes, context window optimization, and effectof dataset sizes.</description><author>Junhong Shen, Atishay Jain, Zedian Xiao, Ishan Amlekar, Mouad Hadji, Aaron Podolny, Ameet Talwalkar</author><pubDate>Fri, 22 Nov 2024 15:26:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15004v1</guid></item><item><title>Sketch-guided Cage-based 3D Gaussian Splatting Deformation</title><link>http://arxiv.org/abs/2411.12168v2</link><description>3D Gaussian Splatting (GS) is one of the most promising novel 3Drepresentations that has received great interest in computer graphics andcomputer vision. While various systems have introduced editing capabilities for3D GS, such as those guided by text prompts, fine-grained control overdeformation remains an open challenge. In this work, we present a novelsketch-guided 3D GS deformation system that allows users to intuitively modifythe geometry of a 3D GS model by drawing a silhouette sketch from a singleviewpoint. Our approach introduces a new deformation method that combinescage-based deformations with a variant of Neural Jacobian Fields, enablingprecise, fine-grained control. Additionally, it leverages large-scale 2Ddiffusion priors and ControlNet to ensure the generated deformations aresemantically plausible. Through a series of experiments, we demonstrate theeffectiveness of our method and showcase its ability to animate static 3D GSmodels as one of its key applications.</description><author>Tianhao Xie, Noam Aigerman, Eugene Belilovsky, Tiberiu Popa</author><pubDate>Fri, 22 Nov 2024 15:25:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.12168v2</guid></item><item><title>Utilizing Large Language Models to Synthesize Product Desirability Datasets</title><link>http://arxiv.org/abs/2411.13485v2</link><description>This research explores the application of large language models (LLMs) togenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, akey component in evaluating user sentiment and product experience. Utilizinggpt-4o-mini, a cost-effective alternative to larger commercial LLMs, threemethods, Word+Review, Review+Word, and Supply-Word, were each used tosynthesize 1000 product reviews. The generated datasets were assessed forsentiment alignment, textual diversity, and data generation cost. Resultsdemonstrated high sentiment alignment across all methods, with Pearsoncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highestdiversity and coverage of PDT terms, although with increased generation costs.Despite minor biases toward positive sentiments, in situations with limitedtest data, LLM-generated synthetic data offers significant advantages,including scalability, cost savings, and flexibility in dataset production.</description><author>John D. Hastings, Sherri Weitl-Harms, Joseph Doty, Zachary J. Myers, Warren Thompson</author><pubDate>Fri, 22 Nov 2024 15:24:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.13485v2</guid></item><item><title>A New Way: Kronecker-Factored Approximate Curvature Deep Hedging and its Benefits</title><link>http://arxiv.org/abs/2411.15002v1</link><description>This paper advances the computational efficiency of Deep Hedging frameworksthrough the novel integration of Kronecker-Factored Approximate Curvature(K-FAC) optimization. While recent literature has established Deep Hedging as adata-driven alternative to traditional risk management strategies, thecomputational burden of training neural networks with first-order methodsremains a significant impediment to practical implementation. The proposedarchitecture couples Long Short-Term Memory (LSTM) networks with K-FACsecond-order optimization, specifically addressing the challenges of sequentialfinancial data and curvature estimation in recurrent networks. Empiricalvalidation using simulated paths from a calibrated Heston stochastic volatilitymodel demonstrates that the K-FAC implementation achieves marked improvementsin convergence dynamics and hedging efficacy. The methodology yields a 78.3%reduction in transaction costs ($t = 56.88$, $p &lt; 0.001$) and a 34.4% decreasein profit and loss (P&amp;L) variance compared to Adam optimization. Moreover, theK-FAC-enhanced model exhibits superior risk-adjusted performance with a Sharperatio of 0.0401, contrasting with $-0.0025$ for the baseline model. Theseresults provide compelling evidence that second-order optimization methods canmaterially enhance the tractability of Deep Hedging implementations. Thefindings contribute to the growing literature on computational methods inquantitative finance while highlighting the potential for advanced optimizationtechniques to bridge the gap between theoretical frameworks and practicalapplications in financial markets.</description><author>Tsogt-Ochir Enkhbayar</author><pubDate>Fri, 22 Nov 2024 15:19:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15002v1</guid></item><item><title>Differentiable Physics-based System Identification for Robotic Manipulation of Elastoplastic Materials</title><link>http://arxiv.org/abs/2411.00554v2</link><description>Robotic manipulation of volumetric elastoplastic deformable materials, fromfoods such as dough to construction materials like clay, is in its infancy,largely due to the difficulty of modelling and perception in a high-dimensionalspace. Simulating the dynamics of such materials is computationally expensive.It tends to suffer from inaccurately estimated physics parameters of thematerials and the environment, impeding high-precision manipulation. Estimatingsuch parameters from raw point clouds captured by optical cameras suffersfurther from heavy occlusions. To address this challenge, this work introducesa novel Differentiable Physics-based System Identification (DPSI) frameworkthat enables a robot arm to infer the physics parameters of elastoplasticmaterials and the environment using simple manipulation motions and incomplete3D point clouds, aligning the simulation with the real world. Extensiveexperiments show that with only a single real-world interaction, the estimatedparameters, Young's modulus, Poisson's ratio, yield stress and frictioncoefficients, can accurately simulate visually and physically realisticdeformation behaviours induced by unseen and long-horizon manipulation motions.Additionally, the DPSI framework inherently provides physically intuitiveinterpretations for the parameters in contrast to black-box approaches such asdeep neural networks.</description><author>Xintong Yang, Ze Ji, Yu-Kun Lai</author><pubDate>Fri, 22 Nov 2024 15:15:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.00554v2</guid></item><item><title>Learning Lifted STRIPS Models from Action Traces Alone: A Simple, General, and Scalable Solution</title><link>http://arxiv.org/abs/2411.14995v1</link><description>Learning STRIPS action models from action traces alone is a challengingproblem as it involves learning the domain predicates as well. In this work, anovel approach is introduced which, like the well-known LOCM systems, isscalable, but like SAT approaches, is sound and complete. Furthermore, theapproach is general and imposes no restrictions on the hidden domain or thenumber or arity of the predicates. The new learning method is based on an\emph{efficient, novel test} that checks whether the assumption that apredicate is affected by a set of action patterns, namely, actions withspecific argument positions, is consistent with the traces. The predicates andaction patterns that pass the test provide the basis for the learned domainthat is then easily completed with preconditions and static predicates. The newmethod is studied theoretically and experimentally. For the latter, the methodis evaluated on traces and graphs obtained from standard classical domains likethe 8-puzzle, which involve hundreds of thousands of states and transitions.The learned representations are then verified on larger instances.</description><author>Jonas Gösgens, Niklas Jansen, Hector Geffner</author><pubDate>Fri, 22 Nov 2024 15:09:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14995v1</guid></item><item><title>Differentiable Biomechanics for Markerless Motion Capture in Upper Limb Stroke Rehabilitation: A Comparison with Optical Motion Capture</title><link>http://arxiv.org/abs/2411.14992v1</link><description>Marker-based Optical Motion Capture (OMC) paired with biomechanical modelingis currently considered the most precise and accurate method for measuringhuman movement kinematics. However, combining differentiable biomechanicalmodeling with Markerless Motion Capture (MMC) offers a promising approach tomotion capture in clinical settings, requiring only minimal equipment, such assynchronized webcams, and minimal effort for data collection. This studycompares key kinematic outcomes from biomechanically modeled MMC and OMC datain 15 stroke patients performing the drinking task, a functional taskrecommended for assessing upper limb movement quality. We observed a high levelof agreement in kinematic trajectories between MMC and OMC, as indicated byhigh correlations (median r above 0.95 for the majority of kinematictrajectories) and median RMSE values ranging from 2-5 degrees for joint angles,0.04 m/s for end-effector velocity, and 6 mm for trunk displacement.Trial-to-trial biases between OMC and MMC were consistent within participantsessions, with interquartile ranges of bias around 1-3 degrees for jointangles, 0.01 m/s in end-effector velocity, and approximately 3mm for trunkdisplacement. Our findings indicate that our MMC for arm tracking isapproaching the accuracy of marker-based methods, supporting its potential foruse in clinical settings. MMC could provide valuable insights into movementrehabilitation in stroke patients, potentially enhancing the effectiveness ofrehabilitation strategies.</description><author>Tim Unger, Arash Sal Moslehian, J. D. Peiffer, Johann Ullrich, Roger Gassert, Olivier Lambercy, R. James Cotton, Chris Awai Easthope</author><pubDate>Fri, 22 Nov 2024 15:02:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14992v1</guid></item><item><title>Free Energy Projective Simulation (FEPS): Active inference with interpretability</title><link>http://arxiv.org/abs/2411.14991v1</link><description>In the last decade, the free energy principle (FEP) and active inference(AIF) have achieved many successes connecting conceptual models of learning andcognition to mathematical models of perception and action. This effort isdriven by a multidisciplinary interest in understanding aspects ofself-organizing complex adaptive systems, including elements of agency. Variousreinforcement learning (RL) models performing active inference have beenproposed and trained on standard RL tasks using deep neural networks. Recentwork has focused on improving such agents' performance in complex environmentsby incorporating the latest machine learning techniques. In this paper, we takean alternative approach. Within the constraints imposed by the FEP and AIF, weattempt to model agents in an interpretable way without deep neural networks byintroducing Free Energy Projective Simulation (FEPS). Using internal rewardsonly, FEPS agents build a representation of their partially observableenvironments with which they interact. Following AIF, the policy to achieve agiven task is derived from this world model by minimizing the expected freeenergy. Leveraging the interpretability of the model, techniques are introducedto deal with long-term goals and reduce prediction errors caused by erroneoushidden state estimation. We test the FEPS model on two RL environments inspiredfrom behavioral biology: a timed response task and a navigation task in apartially observable grid. Our results show that FEPS agents fully resolve theambiguity of both environments by appropriately contextualizing theirobservations based on prediction accuracy only. In addition, they infer optimalpolicies flexibly for any target observation in the environment.</description><author>Joséphine Pazem, Marius Krumm, Alexander Q. Vining, Lukas J. Fiderer, Hans J. Briegel</author><pubDate>Fri, 22 Nov 2024 15:01:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14991v1</guid></item><item><title>Interval Abstractions for Robust Counterfactual Explanations</title><link>http://arxiv.org/abs/2404.13736v2</link><description>Counterfactual Explanations (CEs) have emerged as a major paradigm inexplainable AI research, providing recourse recommendations for users affectedby the decisions of machine learning models. However, CEs found by existingmethods often become invalid when slight changes occur in the parameters of themodel they were generated for. The literature lacks a way to provide exhaustiverobustness guarantees for CEs under model changes, in that existing methods toimprove CEs' robustness are mostly heuristic, and the robustness performancesare evaluated empirically using only a limited number of retrained models. Tobridge this gap, we propose a novel interval abstraction technique forparametric machine learning models, which allows us to obtain provablerobustness guarantees for CEs under a possibly infinite set of plausible modelchanges $\Delta$. Based on this idea, we formalise a robustness notion for CEs,which we call $\Delta$-robustness, in both binary and multi-classclassification settings. We present procedures to verify $\Delta$-robustnessbased on Mixed Integer Linear Programming, using which we further proposealgorithms to generate CEs that are $\Delta$-robust. In an extensive empiricalstudy involving neural networks and logistic regression models, we demonstratethe practical applicability of our approach. We discuss two strategies fordetermining the appropriate hyperparameters in our method, and wequantitatively benchmark CEs generated by eleven methods, highlighting theeffectiveness of our algorithms in finding robust CEs.</description><author>Junqi Jiang, Francesco Leofante, Antonio Rago, Francesca Toni</author><pubDate>Fri, 22 Nov 2024 15:01:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13736v2</guid></item><item><title>TIPO: Text to Image with Text Presampling for Prompt Optimization</title><link>http://arxiv.org/abs/2411.08127v2</link><description>TIPO (Text to Image with text pre-sampling for Prompt Optimization) is aninnovative framework designed to enhance text-to-image (T2I) generation bylanguage model (LM) for automatic prompt engineering. By refining and extendinguser-provided prompts, TIPO bridges the gap between simple inputs and thedetailed prompts required for high-quality image generation. Unlike previousapproaches that rely on Large Language Models (LLMs) or reinforcement learning(RL), TIPO adjusts user input prompts with the distribution of a trained promptdataset, eliminating the need for complex runtime cost via lightweight model.This pre-sampling approach enables efficient and scalable prompt optimization,grounded in the model's training distribution. Experimental results demonstrateTIPO's effectiveness in improving aesthetic scores, reducing image corruption,and better aligning generated images with dataset distributions. These findingshighlight the critical role of prompt engineering in T2I systems and openavenues for broader applications of automatic prompt refinement.</description><author>Shih-Ying Yeh, Sang-Hyun Park, Giyeong Oh, Min Song, Youngjae Yu</author><pubDate>Fri, 22 Nov 2024 14:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08127v2</guid></item><item><title>Adaptive Group Robust Ensemble Knowledge Distillation</title><link>http://arxiv.org/abs/2411.14984v1</link><description>Neural networks can learn spurious correlations in the data, often leading toperformance disparity for underrepresented subgroups. Studies have demonstratedthat the disparity is amplified when knowledge is distilled from a complexteacher model to a relatively "simple" student model. Prior work has shown thatensemble deep learning methods can improve the performance of the worst-casesubgroups; however, it is unclear if this advantage carries over whendistilling knowledge from an ensemble of teachers, especially when the teachermodels are debiased. This study demonstrates that traditional ensembleknowledge distillation can significantly drop the performance of the worst-casesubgroups in the distilled student model even when the teacher models aredebiased. To overcome this, we propose Adaptive Group Robust Ensemble KnowledgeDistillation (AGRE-KD), a simple ensembling strategy to ensure that the studentmodel receives knowledge beneficial for unknown underrepresented subgroups.Leveraging an additional biased model, our method selectively chooses teacherswhose knowledge would better improve the worst-performing subgroups byupweighting the teachers with gradient directions deviating from the biasedmodel. Our experiments on several datasets demonstrate the superiority of theproposed ensemble distillation technique and show that it can even outperformclassic model ensembles based on majority voting.</description><author>Patrik Kenfack, Ulrich Aïvodji, Samira Ebrahimi Kahou</author><pubDate>Fri, 22 Nov 2024 14:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14984v1</guid></item><item><title>Large Multi-modal Models Can Interpret Features in Large Multi-modal Models</title><link>http://arxiv.org/abs/2411.14982v1</link><description>Recent advances in Large Multimodal Models (LMMs) lead to significantbreakthroughs in both academia and industry. One question that arises is howwe, as humans, can understand their internal neural representations. This papertakes an initial step towards addressing this question by presenting aversatile framework to identify and interpret the semantics within LMMs.Specifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle therepresentations into human understandable features. 2) We then present anautomatic interpretation framework to interpreted the open-semantic featureslearned in SAE by the LMMs themselves. We employ this framework to analyze theLLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that thesefeatures can effectively steer the model's behavior. Our results contribute toa deeper understanding of why LMMs excel in specific tasks, including EQ tests,and illuminate the nature of their mistakes along with potential strategies fortheir rectification. These findings offer new insights into the internalmechanisms of LMMs and suggest parallels with the cognitive processes of thehuman brain.</description><author>Kaichen Zhang, Yifei Shen, Bo Li, Ziwei Liu</author><pubDate>Fri, 22 Nov 2024 14:41:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14982v1</guid></item><item><title>Neural Network-Based Bandit: A Medium Access Control for the IIoT Alarm Scenario</title><link>http://arxiv.org/abs/2407.16877v2</link><description>Efficient Random Access (RA) is critical for enabling reliable communicationin Industrial Internet of Things (IIoT) networks. Herein, we propose a deepreinforcement learning based distributed RA scheme, entitled NeuralNetwork-Based Bandit (NNBB), for the IIoT alarm scenario. In such a scenario,the devices may detect a common critical event, and the goal is to ensure thealarm information is delivered successfully from at least one device. Theproposed NNBB scheme is implemented at each device, where it trains itselfonline and establishes implicit inter-device coordination to achieve the commongoal. Devices can transmit simultaneously on multiple orthogonal channels andeach possible transmission pattern constitutes a possible action for the NNBB,which uses a deep neural network to determine the action. Our simulationresults show that as the number of devices in the network increases, so doesthe performance gain of the NNBB compared to the Multi-Armed Bandit (MAB) RAbenchmark. For instance, NNBB experiences a 7% success rate drop when there arefour channels and the number of devices increases from 10 to 60, while MABfaces a 25% drop.</description><author>Prasoon Raghuwanshi, Onel Luis Alcaraz López, Neelesh B. Mehta, Hirley Alves, Matti Latva-aho</author><pubDate>Fri, 22 Nov 2024 14:35:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16877v2</guid></item><item><title>Exploring Foundation Models Fine-Tuning for Cytology Classification</title><link>http://arxiv.org/abs/2411.14975v1</link><description>Cytology slides are essential tools in diagnosing and staging cancer, buttheir analysis is time-consuming and costly. Foundation models have shown greatpotential to assist in these tasks. In this paper, we explore how existingfoundation models can be applied to cytological classification. Moreparticularly, we focus on low-rank adaptation, a parameter-efficientfine-tuning method suited to few-shot learning. We evaluated five foundationmodels across four cytological classification datasets. Our results demonstratethat fine-tuning the pre-trained backbones with LoRA significantly improvesmodel performance compared to fine-tuning only the classifier head, achievingstate-of-the-art results on both simple and complex classification tasks whilerequiring fewer data samples.</description><author>Manon Dausort, Tiffanie Godelaine, Maxime Zanella, Karim El Khoury, Isabelle Salmon, Benoît Macq</author><pubDate>Fri, 22 Nov 2024 14:34:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14975v1</guid></item><item><title>3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes</title><link>http://arxiv.org/abs/2411.14974v1</link><description>Recent advances in radiance field reconstruction, such as 3D GaussianSplatting (3DGS), have achieved high-quality novel view synthesis and fastrendering by representing scenes with compositions of Gaussian primitives.However, 3D Gaussians present several limitations for scene reconstruction.Accurately capturing hard edges is challenging without significantly increasingthe number of Gaussians, creating a large memory footprint. Moreover, theystruggle to represent flat surfaces, as they are diffused in space. Withouthand-crafted regularizers, they tend to disperse irregularly around the actualsurface. To circumvent these issues, we introduce a novel method, named 3DConvex Splatting (3DCS), which leverages 3D smooth convexes as primitives formodeling geometrically-meaningful radiance fields from multi-view images.Smooth convex shapes offer greater flexibility than Gaussians, allowing for abetter representation of 3D scenes with hard edges and dense volumes usingfewer primitives. Powered by our efficient CUDA-based rasterizer, 3DCS achievessuperior performance over 3DGS on benchmarks such as Mip-NeRF360, Tanks andTemples, and Deep Blending. Specifically, our method attains an improvement ofup to 0.81 in PSNR and 0.026 in LPIPS compared to 3DGS while maintaining highrendering speeds and reducing the number of required primitives. Our resultshighlight the potential of 3D Convex Splatting to become the new standard forhigh-quality scene reconstruction and novel view synthesis. Project page:www.convexsplatting.com.</description><author>Jan Held, Renaud Vandeghen, Abdullah Hamdi, Adrien Deliege, Anthony Cioppa, Silvio Giancola, Andrea Vedaldi, Bernard Ghanem, Marc Van Droogenbroeck</author><pubDate>Fri, 22 Nov 2024 14:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14974v1</guid></item><item><title>Open-Amp: Synthetic Data Framework for Audio Effect Foundation Models</title><link>http://arxiv.org/abs/2411.14972v1</link><description>This paper introduces Open-Amp, a synthetic data framework for generatinglarge-scale and diverse audio effects data. Audio effects are relevant to manymusical audio processing and Music Information Retrieval (MIR) tasks, such asmodelling of analog audio effects, automatic mixing, tone matching andtranscription. Existing audio effects datasets are limited in scope, usuallyincluding relatively few audio effects processors and a limited amount of inputaudio signals. Our proposed framework overcomes these issues, by crowdsourcingneural network emulations of guitar amplifiers and effects, created by users ofopen-source audio effects emulation software. This allows users of Open-Ampcomplete control over the input signals to be processed by the effects models,as well as providing high-quality emulations of hundreds of devices. Open-Ampcan render audio online during training, allowing great flexibility in dataaugmentation. Our experiments show that using Open-Amp to train a guitareffects encoder achieves new state-of-the-art results on multiple guitareffects classification tasks. Furthermore, we train a one-to-many guitareffects model using Open-Amp, and use it to emulate unseen analog effects viamanipulation of its learned latent space, indicating transferability to analogguitar effects data.</description><author>Alec Wright, Alistair Carson, Lauri Juvela</author><pubDate>Fri, 22 Nov 2024 14:27:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14972v1</guid></item><item><title>Leveraging LLMs for Legacy Code Modernization: Challenges and Opportunities for LLM-Generated Documentation</title><link>http://arxiv.org/abs/2411.14971v1</link><description>Legacy software systems, written in outdated languages like MUMPS andmainframe assembly, pose challenges in efficiency, maintenance, staffing, andsecurity. While LLMs offer promise for modernizing these systems, their abilityto understand legacy languages is largely unknown. This paper investigates theutilization of LLMs to generate documentation for legacy code using twodatasets: an electronic health records (EHR) system in MUMPS and open-sourceapplications in IBM mainframe Assembly Language Code (ALC). We propose aprompting strategy for generating line-wise code comments and a rubric toevaluate their completeness, readability, usefulness, and hallucination. Ourstudy assesses the correlation between human evaluations and automated metrics,such as code complexity and reference-based metrics. We find that LLM-generatedcomments for MUMPS and ALC are generally hallucination-free, complete,readable, and useful compared to ground-truth comments, though ALC poseschallenges. However, no automated metrics strongly correlate with commentquality to predict or measure LLM performance. Our findings highlight thelimitations of current automated measures and the need for better evaluationmetrics for LLM-generated documentation in legacy systems.</description><author>Colin Diggs, Michael Doyle, Amit Madan, Siggy Scott, Emily Escamilla, Jacob Zimmer, Naveed Nekoo, Paul Ursino, Michael Bartholf, Zachary Robin, Anand Patel, Chris Glasz, William Macke, Paul Kirk, Jasper Phillips, Arun Sridharan, Doug Wendt, Scott Rosen, Nitin Naik, Justin F. Brunelle, Samruddhi Thaker</author><pubDate>Fri, 22 Nov 2024 14:27:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14971v1</guid></item></channel></rss>