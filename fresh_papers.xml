<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 13 May 2023 06:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion</title><link>http://arxiv.org/abs/2305.06356v2</link><description>Representing human performance at high-fidelity is an essential buildingblock in diverse applications, such as film production, computer games orvideoconferencing. To close the gap to production-level quality, we introduceHumanRF, a 4D dynamic neural scene representation that captures full-bodyappearance in motion from multi-view video input, and enables playback fromnovel, unseen viewpoints. Our novel representation acts as a dynamic videoencoding that captures fine details at high compression rates by factorizingspace-time into a temporal matrix-vector decomposition. This allows us toobtain temporally coherent reconstructions of human actors for long sequences,while representing high-resolution details even in the context of challengingmotion. While most research focuses on synthesizing at resolutions of 4MP orlower, we address the challenge of operating at 12MP. To this end, we introduceActorsHQ, a novel multi-view dataset that provides 12MP footage from 160cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions. Wedemonstrate challenges that emerge from using such high-resolution data andshow that our newly introduced HumanRF effectively leverages this data, makinga significant step towards production-level quality novel view synthesis.</description><author>Mustafa Işık, Martin Rünz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, Matthias Nießner</author><pubDate>Thu, 11 May 2023 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06356v2</guid></item><item><title>EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention</title><link>http://arxiv.org/abs/2305.07027v1</link><description>Vision transformers have shown great success due to their high modelcapabilities. However, their remarkable performance is accompanied by heavycomputation costs, which makes them unsuitable for real-time applications. Inthis paper, we propose a family of high-speed vision transformers namedEfficientViT. We find that the speed of existing transformer models is commonlybounded by memory inefficient operations, especially the tensor reshaping andelement-wise functions in MHSA. Therefore, we design a new building block witha sandwich layout, i.e., using a single memory-bound MHSA between efficient FFNlayers, which improves memory efficiency while enhancing channel communication.Moreover, we discover that the attention maps share high similarities acrossheads, leading to computational redundancy. To address this, we present acascaded group attention module feeding attention heads with different splitsof the full feature, which not only saves computation cost but also improvesattention diversity. Comprehensive experiments demonstrate EfficientViToutperforms existing efficient models, striking a good trade-off between speedand accuracy. For instance, our EfficientViT-M5 surpasses MobileNetV3-Large by1.9% in accuracy, while getting 40.4% and 45.2% higher throughput on NvidiaV100 GPU and Intel Xeon CPU, respectively. Compared to the recent efficientmodel MobileViT-XXS, EfficientViT-M2 achieves 1.8% superior accuracy, whilerunning 5.8x/3.7x faster on the GPU/CPU, and 7.4x faster when converted to ONNXformat. Code and models are available athttps://github.com/microsoft/Cream/tree/main/EfficientViT.</description><author>Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, Yixuan Yuan</author><pubDate>Thu, 11 May 2023 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07027v1</guid></item><item><title>Decentralization and Acceleration Enables Large-Scale Bundle Adjustment</title><link>http://arxiv.org/abs/2305.07026v1</link><description>Scaling to arbitrarily large bundle adjustment problems requires data andcompute to be distributed across multiple devices. Centralized methods in priorworks are only able to solve small or medium size problems due to overhead incomputation and communication. In this paper, we present a fully decentralizedmethod that alleviates computation and communication bottlenecks to solvearbitrarily large bundle adjustment problems. We achieve this by reformulatingthe reprojection error and deriving a novel surrogate function that decouplesoptimization variables from different devices. This function makes it possibleto use majorization minimization techniques and reduces bundle adjustment toindependent optimization subproblems that can be solved in parallel. We furtherapply Nesterov's acceleration and adaptive restart to improve convergence whilemaintaining its theoretical guarantees. Despite limited peer-to-peercommunication, our method has provable convergence to first-order criticalpoints under mild conditions. On extensive benchmarks with public datasets, ourmethod converges much faster than decentralized baselines with similar memoryusage and communication load. Compared to centralized baselines using a singledevice, our method, while being decentralized, yields more accurate solutionswith significant speedups of up to 940.7x over Ceres and 175.2x over DeepLM.Code: https://github.com/facebookresearch/DBA.</description><author>Taosha Fan, Joseph Ortiz, Ming Hsiao, Maurizio Monge, Jing Dong, Todd Murphey, Mustafa Mukadam</author><pubDate>Thu, 11 May 2023 18:58:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07026v1</guid></item><item><title>SparseGNV: Generating Novel Views of Indoor Scenes with Sparse Input Views</title><link>http://arxiv.org/abs/2305.07024v1</link><description>We study to generate novel views of indoor scenes given sparse input views.The challenge is to achieve both photorealism and view consistency. We presentSparseGNV: a learning framework that incorporates 3D structures and imagegenerative models to generate novel views with three modules. The first modulebuilds a neural point cloud as underlying geometry, providing contextualinformation and guidance for the target novel view. The second module utilizesa transformer-based network to map the scene context and the guidance into ashared latent space and autoregressively decodes the target view in the form ofdiscrete image tokens. The third module reconstructs the tokens into the imageof the target view. SparseGNV is trained across a large indoor scene dataset tolearn generalizable priors. Once trained, it can efficiently generate novelviews of an unseen indoor scene in a feed-forward manner. We evaluate SparseGNVon both real-world and synthetic indoor scenes and demonstrate that itoutperforms state-of-the-art methods based on either neural radiance fields orconditional image generation.</description><author>Weihao Cheng, Yan-Pei Cao, Ying Shan</author><pubDate>Thu, 11 May 2023 18:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07024v1</guid></item><item><title>Simple Token-Level Confidence Improves Caption Correctness</title><link>http://arxiv.org/abs/2305.07021v1</link><description>The ability to judge whether a caption correctly describes an image is acritical part of vision-language understanding. However, state-of-the-artmodels often misinterpret the correctness of fine-grained details, leading toerrors in outputs such as hallucinating objects in generated captions or poorcompositional reasoning. In this work, we explore Token-Level Confidence, orTLC, as a simple yet surprisingly effective method to assess captioncorrectness. Specifically, we fine-tune a vision-language model on imagecaptioning, input an image and proposed caption to the model, and aggregateeither algebraic or learned token confidences over words or sequences toestimate image-caption consistency. Compared to sequence-level scores frompretrained models, TLC with algebraic confidence measures achieves a relativeimprovement in accuracy by 10% on verb understanding in SVO-Probes andoutperforms prior state-of-the-art in image and group scores for compositionalreasoning in Winoground by a relative 37% and 9%, respectively. When trainingdata are available, a learned confidence estimator provides further improvedperformance, reducing object hallucination rates in MS COCO Captions by arelative 30% over the original model and setting a new state-of-the-art.</description><author>Suzanne Petryk, Spencer Whitehead, Joseph E. Gonzalez, Trevor Darrell, Anna Rohrbach, Marcus Rohrbach</author><pubDate>Thu, 11 May 2023 18:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07021v1</guid></item><item><title>Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts</title><link>http://arxiv.org/abs/2305.07019v1</link><description>We present a sequence-to-sequence vision-language model whose parameters arejointly trained on all tasks (all for one) and fully shared among multipletasks (one for all), resulting in a single model which we named Musketeer. Theintegration of knowledge across heterogeneous tasks is enabled by a novelfeature called Task Explanation Prompt (TEP). TEP reduces interference amongtasks, allowing the model to focus on their shared structure. With a singlemodel, Musketeer achieves results comparable to or better than strong baselinestrained on single tasks, almost uniformly across multiple tasks.</description><author>Zhaoyang Zhang, Yantao Shen, Kunyu Shi, Zhaowei Cai, Jun Fang, Siqi Deng, Hao Yang, Davide Modolo, Zhuowen Tu, Stefano Soatto</author><pubDate>Thu, 11 May 2023 18:57:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07019v1</guid></item><item><title>An Inverse Scaling Law for CLIP Training</title><link>http://arxiv.org/abs/2305.07017v1</link><description>CLIP, the first foundation model that connects images and text, has enabledmany recent breakthroughs in computer vision. However, its associated trainingcost is prohibitively high, imposing a significant barrier to its widespreadexploration. In this paper, we present a surprising finding that there existsan inverse scaling law for CLIP training, whereby the larger the image/textencoders used, the shorter the sequence length of image/text tokens that can beapplied in training. Moreover, we showcase that the strategy for reducingimage/text token length plays a crucial role in determining the quality of thisscaling law. As a result of this finding, we are able to successfully train CLIP even byusing academic resources. For example, on an A100 eight-GPU server, our CLIPmodels achieve zero-shot top-1 ImageNet accuracies of 63.2% in ~2 days, 67.8%in ~3 days, and 69.3% in ~4 days. By reducing the computation barrierassociated with CLIP, we hope to inspire more research in this field,particularly from academics. Our code is available athttps://github.com/UCSC-VLAA/CLIPA.</description><author>Xianhang Li, Zeyu Wang, Cihang Xie</author><pubDate>Thu, 11 May 2023 18:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07017v1</guid></item><item><title>A General-Purpose Multilingual Document Encoder</title><link>http://arxiv.org/abs/2305.07016v1</link><description>Massively multilingual pretrained transformers (MMTs) have tremendouslypushed the state of the art on multilingual NLP and cross-lingual transfer ofNLP models in particular. While a large body of work leveraged MMTs to mineparallel data and induce bilingual document embeddings, much less effort hasbeen devoted to training general-purpose (massively) multilingual documentencoder that can be used for both supervised and unsupervised document-leveltasks. In this work, we pretrain a massively multilingual document encoder as ahierarchical transformer model (HMDE) in which a shallow document transformercontextualizes sentence representations produced by a state-of-the-artpretrained multilingual sentence encoder. We leverage Wikipedia as a readilyavailable source of comparable documents for creating training data, and trainHMDE by means of a cross-lingual contrastive objective, further exploiting thecategory hierarchy of Wikipedia for creation of difficult negatives. Weevaluate the effectiveness of HMDE in two arguably most common and prominentcross-lingual document-level tasks: (1) cross-lingual transfer for topicaldocument classification and (2) cross-lingual document retrieval. HMDE issignificantly more effective than (i) aggregations of segment-basedrepresentations and (ii) multilingual Longformer. Crucially, owing to itsmassively multilingual lower transformer, HMDE successfully generalizes tolanguages unseen in document-level pretraining. We publicly release our codeand models athttps://github.com/ogaloglu/pre-training-multilingual-document-encoders .</description><author>Onur Galoğlu, Robert Litschko, Goran Glavaš</author><pubDate>Thu, 11 May 2023 18:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07016v1</guid></item><item><title>Exploiting Diffusion Prior for Real-World Image Super-Resolution</title><link>http://arxiv.org/abs/2305.07015v1</link><description>We present a novel approach to leverage prior knowledge encapsulated inpre-trained text-to-image diffusion models for blind super-resolution (SR).Specifically, by employing our time-aware encoder, we can achieve promisingrestoration results without altering the pre-trained synthesis model, therebypreserving the generative prior and minimizing training cost. To remedy theloss of fidelity caused by the inherent stochasticity of diffusion models, weintroduce a controllable feature wrapping module that allows users to balancequality and fidelity by simply adjusting a scalar value during the inferenceprocess. Moreover, we develop a progressive aggregation sampling strategy toovercome the fixed-size constraints of pre-trained diffusion models, enablingadaptation to resolutions of any size. A comprehensive evaluation of our methodusing both synthetic and real-world benchmarks demonstrates its superiorityover current state-of-the-art approaches.</description><author>Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C. K. Chan, Chen Change Loy</author><pubDate>Thu, 11 May 2023 18:55:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07015v1</guid></item><item><title>Virtual Occlusions Through Implicit Depth</title><link>http://arxiv.org/abs/2305.07014v1</link><description>For augmented reality (AR), it is important that virtual assets appear to`sit among' real world objects. The virtual element should variously occludeand be occluded by real matter, based on a plausible depth ordering. Thisocclusion should be consistent over time as the viewer's camera moves.Unfortunately, small mistakes in the estimated scene depth can ruin thedownstream occlusion mask, and thereby the AR illusion. Especially in real-timesettings, depths inferred near boundaries or across time can be inconsistent.In this paper, we challenge the need for depth-regression as an intermediatestep. We instead propose an implicit model for depth and use that to predict theocclusion mask directly. The inputs to our network are one or more colorimages, plus the known depths of any virtual geometry. We show how ourocclusion predictions are more accurate and more temporally stable thanpredictions derived from traditional depth-estimation models. We obtainstate-of-the-art occlusion results on the challenging ScanNetv2 dataset andsuperior qualitative results on real scenes.</description><author>Jamie Watson, Mohamed Sayed, Zawar Qureshi, Gabriel J. Brostow, Sara Vicente, Oisin Mac Aodha, Michael Firman</author><pubDate>Thu, 11 May 2023 18:55:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07014v1</guid></item><item><title>Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers</title><link>http://arxiv.org/abs/2305.07011v1</link><description>We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - acontrastive image-text pretraining recipe to bridge the gap between image-levelpretraining and open-vocabulary object detection. At the pretraining phase, wepropose to randomly crop and resize regions of positional embeddings instead ofusing the whole image positional embeddings. This better matches the use ofpositional embeddings at region-level in the detection finetuning phase. Inaddition, we replace the common softmax cross entropy loss in contrastivelearning with focal loss to better learn the informative yet difficultexamples. Finally, we leverage recent advances in novel object proposals toimprove open-vocabulary detection finetuning. We evaluate our full model on theLVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer.RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the bestexisting approach by +5.8 points in addition to competitive zero-shot transferdetection. Surprisingly, RO-ViT improves the image-level representation as welland achieves the state of the art on 9 out of 12 metrics on COCO and Flickrimage-text retrieval benchmarks, outperforming competitive approaches withlarger models.</description><author>Dahun Kim, Anelia Angelova, Weicheng Kuo</author><pubDate>Thu, 11 May 2023 18:53:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07011v1</guid></item><item><title>Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation</title><link>http://arxiv.org/abs/2305.07005v1</link><description>Subword segmenters like BPE operate as a preprocessing step in neural machinetranslation and other (conditional) language models. They are applied todatasets before training, so translation or text generation quality relies onthe quality of segmentations. We propose a departure from this paradigm, calledsubword segmental machine translation (SSMT). SSMT unifies subword segmentationand MT in a single trainable model. It learns to segment target sentence wordswhile jointly learning to generate target sentences. To use SSMT duringinference we propose dynamic decoding, a text generation algorithm that adaptssegmentations as it generates translations. Experiments across 6 translationdirections show that SSMT improves chrF scores for morphologically richagglutinative languages. Gains are strongest in the very low-resource scenario.SSMT also learns subwords that are closer to morphemes compared to baselinesand proves more robust on a test set constructed for evaluating morphologicalcompositional generalisation.</description><author>Francois Meyer, Jan Buys</author><pubDate>Thu, 11 May 2023 18:44:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07005v1</guid></item><item><title>Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting</title><link>http://arxiv.org/abs/2305.07004v1</link><description>Large language models (LLMs) demonstrate impressive multilingual capability,but their performance varies substantially across different languages. In thiswork, we introduce a simple yet effective method, called cross-lingual-thoughtprompting (XLT), to systematically improve the multilingual capability of LLMs.Specifically, XLT is a generic template prompt that stimulates cross-lingualand logical reasoning skills to enhance task performance across languages. Weconduct comprehensive evaluations on 7 typical benchmarks related to reasoning,understanding, and generation tasks, covering both high-resource andlow-resource languages. Experimental results show that XLT not only remarkablyenhances the performance of various multilingual tasks but also significantlyreduces the gap between the average performance and the best performance ofeach task in different languages. Notably, XLT brings over 10 points of averageimprovement in arithmetic reasoning and open-domain question-answering tasks.</description><author>Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting Song, Yan Xia, Furu Wei</author><pubDate>Thu, 11 May 2023 18:44:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07004v1</guid></item><item><title>Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach</title><link>http://arxiv.org/abs/2305.07001v1</link><description>In the past decades, recommender systems have attracted much attention inboth research and industry communities, and a large number of studies have beendevoted to developing effective recommendation models. Basically speaking,these models mainly learn the underlying user preference from historicalbehavior data, and then estimate the user-item matching relationships forrecommendations. Inspired by the recent progress on large language models(LLMs), we take a different approach to developing the recommendation models,considering recommendation as instruction following by LLMs. The key idea isthat the preferences or needs of a user can be expressed in natural languagedescriptions (called instructions), so that LLMs can understand and furtherexecute the instruction for fulfilling the recommendation task. Instead ofusing public APIs of LLMs, we instruction tune an open-source LLM (3BFlan-T5-XL), in order to better adapt LLMs to recommender systems. For thispurpose, we first design a general instruction format for describing thepreference, intention, task form and context of a user in natural language.Then we manually design 39 instruction templates and automatically generate alarge amount of user-personalized instruction data (252K instructions) withvarying types of preferences and intentions. To demonstrate the effectivenessof our approach, we instantiate the instruction templates into severalwidely-studied recommendation (or search) tasks, and conduct extensiveexperiments on these tasks with real-world datasets. Experiment results showthat the proposed approach can outperform several competitive baselines,including the powerful GPT-3.5, on these evaluation tasks. Our approach shedslight on developing more user-friendly recommender systems, in which users canfreely communicate with the system and obtain more accurate recommendations vianatural language instructions.</description><author>Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, Ji-Rong Wen</author><pubDate>Thu, 11 May 2023 18:39:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07001v1</guid></item><item><title>A statistical approach to detect sensitive features in a group fairness setting</title><link>http://arxiv.org/abs/2305.06994v1</link><description>The use of machine learning models in decision support systems with highsocietal impact raised concerns about unfair (disparate) results for differentgroups of people. When evaluating such unfair decisions, one generally relieson predefined groups that are determined by a set of features that areconsidered sensitive. However, such an approach is subjective and does notguarantee that these features are the only ones to be considered as sensitivenor that they entail unfair (disparate) outcomes. In this paper, we propose a preprocessing step to address the task ofautomatically recognizing sensitive features that does not require a trainedmodel to verify unfair results. Our proposal is based on the Hilber-Schmidtindependence criterion, which measures the statistical dependence of variabledistributions. We hypothesize that if the dependence between the label vectorand a candidate is high for a sensitive feature, then the information providedby this feature will entail disparate performance measures between groups. Ourempirical results attest our hypothesis and show that several featuresconsidered as sensitive in the literature do not necessarily entail disparate(unfair) results.</description><author>Guilherme Dean Pelegrina, Miguel Couceiro, Leonardo Tomazeli Duarte</author><pubDate>Thu, 11 May 2023 18:30:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06994v1</guid></item><item><title>SMATCH++: Standardized and Extended Evaluation of Semantic Graphs</title><link>http://arxiv.org/abs/2305.06993v1</link><description>The Smatch metric is a popular method for evaluating graph distances, as isnecessary, for instance, to assess the performance of semantic graph parsingsystems. However, we observe some issues in the metric that jeopardizemeaningful evaluation. E.g., opaque pre-processing choices can affect results,and current graph-alignment solvers do not provide us with upper-bounds.Without upper-bounds, however, fair evaluation is not guaranteed. Furthermore,adaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity)are spread out, and lack a unifying framework. For better inspection, we divide the metric into three modules:pre-processing, alignment, and scoring. Examining each module, we specify itsgoals and diagnose potential issues, for which we discuss and test mitigationstrategies. For pre-processing, we show how to fully conform to annotationguidelines that allow structurally deviating but valid graphs. For safer andenhanced alignment, we show the feasibility of optimal alignment in a standardevaluation setup, and develop a lossless graph compression method that shrinksthe search space and significantly increases efficiency. For improved scoring,we propose standardized and extended metric calculation of fine-grainedsub-graph meaning aspects. Our code is available athttps://github.com/flipz357/smatchpp</description><author>Juri Opitz</author><pubDate>Thu, 11 May 2023 18:29:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06993v1</guid></item><item><title>Neural Wave Functions for Superfluids</title><link>http://arxiv.org/abs/2305.06989v1</link><description>Understanding superfluidity remains a major goal of condensed matter physics.Here we tackle this challenge utilizing the recently developed Fermionic neuralnetwork (FermiNet) wave function Ansatz for variational Monte Carlocalculations. We study the unitary Fermi gas, a system with strong,short-range, two-body interactions known to possess a superfluid ground statebut difficult to describe quantitively. We demonstrate key limitations of theFermiNet Ansatz in studying the unitary Fermi gas and propose a simplemodification that outperforms the original FermiNet significantly, givinghighly accurate results. We prove mathematically that the new Ansatz is astrict generalization of the original FermiNet architecture, despite the use offewer parameters. Our approach shares several advantanges with the FermiNet:the use of a neural network removes the need for an underlying basis set; andthe flexiblity of the network yields extremely accurate results within avariational quantum Monte Carlo framework that provides access to unbiasedestimates of arbitrary ground-state expectation values. We discuss how themethod can be extended to study other superfluids.</description><author>Wan Tong Lou, Halvard Sutterud, Gino Cassella, W. M. C. Foulkes, Johannes Knolle, David Pfau, James S. Spencer</author><pubDate>Thu, 11 May 2023 18:23:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06989v1</guid></item><item><title>Self-Chained Image-Language Model for Video Localization and Question Answering</title><link>http://arxiv.org/abs/2305.06988v1</link><description>Recent studies have shown promising results on utilizing pre-trainedimage-language models for video question answering. While these image-languagemodels can efficiently bootstrap the representation learning of video-languagemodels, they typically concatenate uniformly sampled video frames as visualinputs without explicit language-aware, temporal modeling. When only a portionof a video input is relevant to the language query, such uniform frame samplingcan often lead to missing important visual cues. Although humans often find avideo moment to focus on and rewind the moment to answer questions, training aquery-aware video moment localizer often requires expensive annotations andhigh computational costs. To address this issue, we propose Self-Chained VideoLocalization-Answering (SeViLA), a novel framework that leverages a singleimage-language model (BLIP-2) to tackle both temporal keyframe localization andQA on videos. SeViLA framework consists of two modules: Localizer and Answerer,where both are parameter-efficiently fine-tuned from BLIP-2. We chain thesemodules for cascaded inference and self-refinement. First, in the forwardchain, the Localizer finds multiple language-aware keyframes in a video, whichthe Answerer uses to predict the answer. Second, in the reverse chain, theAnswerer generates keyframe pseudo-labels to refine the Localizer, alleviatingthe need for expensive video moment localization annotations. SeViLAoutperforms several strong baselines/previous works on five video QA and eventprediction tasks, and achieves the state-of-the-art in both fine-tuning(NExT-QA, STAR) and zero-shot (NExT-QA, STAR, How2QA, VLEP) settings. We show acomprehensive analysis, e.g., the impact of Localizer, comparisons of Localizerwith other temporal localization models, pre-training/self-refinement ofLocalizer, and varying the number of keyframes.</description><author>Shoubin Yu, Jaemin Cho, Prateek Yadav, Mohit Bansal</author><pubDate>Thu, 11 May 2023 18:23:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06988v1</guid></item><item><title>Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks</title><link>http://arxiv.org/abs/2305.06986v1</link><description>One of the central questions in the theory of deep learning is to understandhow neural networks learn hierarchical features. The ability of deep networksto extract salient features is crucial to both their outstanding generalizationability and the modern deep learning paradigm of pretraining and finetuneing.However, this feature learning process remains poorly understood from atheoretical perspective, with existing analyses largely restricted to two-layernetworks. In this work we show that three-layer neural networks have provablyricher feature learning capabilities than two-layer networks. We analyze thefeatures learned by a three-layer network trained with layer-wise gradientdescent, and present a general purpose theorem which upper bounds the samplecomplexity and width needed to achieve low test error when the target hasspecific hierarchical structure. We instantiate our framework in specificstatistical learning settings -- single-index models and functions of quadraticfeatures -- and show that in the latter setting three-layer networks obtain asample complexity improvement over all existing guarantees for two-layernetworks. Crucially, this sample complexity improvement relies on the abilityof three-layer networks to efficiently learn nonlinear features. We thenestablish a concrete optimization-based depth separation by constructing afunction which is efficiently learnable via gradient descent on a three-layernetwork, yet cannot be learned efficiently by a two-layer network. Our workmakes progress towards understanding the provable benefit of three-layer neuralnetworks over two-layer networks in the feature learning regime.</description><author>Eshaan Nichani, Alex Damian, Jason D. Lee</author><pubDate>Thu, 11 May 2023 18:19:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06986v1</guid></item><item><title>Evaluating Open-Domain Question Answering in the Era of Large Language Models</title><link>http://arxiv.org/abs/2305.06984v1</link><description>Lexical matching remains the de facto evaluation method for open-domainquestion answering (QA). Unfortunately, lexical matching fails completely whena plausible candidate answer does not appear in the list of gold answers, whichis increasingly the case as we shift from extractive to generative models. Therecent success of large language models (LLMs) for QA aggravates lexicalmatching failures since candidate answers become longer, thereby makingmatching with the gold answers even more challenging. Without accurateevaluation, the true progress in open-domain QA remains unknown. In this paper,we conduct a thorough analysis of various open-domain QA models, includingLLMs, by manually evaluating their answers on a subset of NQ-open, a popularbenchmark. Our assessments reveal that while the true performance of all modelsis significantly underestimated, the performance of the InstructGPT (zero-shot)LLM increases by nearly +60%, making it on par with existing top models, andthe InstructGPT (few-shot) model actually achieves a new state-of-the-art onNQ-open. We also find that more than 50% of lexical matching failures areattributed to semantically equivalent answers. We further demonstrate thatregex matching ranks QA models consistent with human judgments, although stillsuffering from unnecessary strictness. Finally, we demonstrate that automatedevaluation models are a reasonable surrogate for lexical matching in somecircumstances, but not for long-form answers generated by LLMs. The automatedmodels struggle in detecting hallucinations in LLM answers and are thus unableto evaluate LLMs. At this time, there appears to be no substitute for humanevaluation.</description><author>Ehsan Kamalloo, Nouha Dziri, Charles L. A. Clarke, Davood Rafiei</author><pubDate>Thu, 11 May 2023 18:14:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06984v1</guid></item><item><title>More Communication Does Not Result in Smaller Generalization Error in Federated Learning</title><link>http://arxiv.org/abs/2304.12216v2</link><description>We study the generalization error of statistical learning models in aFederated Learning (FL) setting. Specifically, there are $K$ devices orclients, each holding an independent own dataset of size $n$. Individualmodels, learned locally via Stochastic Gradient Descent, are aggregated(averaged) by a central server into a global model and then sent back to thedevices. We consider multiple (say $R \in \mathbb N^*$) rounds of modelaggregation and study the effect of $R$ on the generalization error of thefinal aggregated model. We establish an upper bound on the generalization errorthat accounts explicitly for the effect of $R$ (in addition to the number ofparticipating devices $K$ and dataset size $n$). It is observed that, for fixed$(n, K)$, the bound increases with $R$, suggesting that the generalization ofsuch learning algorithms is negatively affected by more frequent communicationwith the parameter server. Combined with the fact that the empirical risk,however, generally decreases for larger values of $R$, this indicates that $R$might be a parameter to optimize to reduce the population risk of FLalgorithms. The results of this paper, which extend straightforwardly to theheterogeneous data setting, are also illustrated through numerical examples.</description><author>Romain Chor, Milad Sefidgaran, Abdellatif Zaidi</author><pubDate>Thu, 11 May 2023 18:13:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12216v2</guid></item><item><title>Active Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2305.06983v1</link><description>Despite the remarkable ability of large language models (LMs) to comprehendand generate language, they have a tendency to hallucinate and create factuallyinaccurate output. Augmenting LMs by retrieving information from externalknowledge resources is one promising solution. Most existingretrieval-augmented LMs employ a retrieve-and-generate setup that onlyretrieves information once based on the input. This is limiting, however, inmore general scenarios involving generation of long texts, where continuallygathering information throughout the generation process is essential. Therehave been some past efforts to retrieve information multiple times whilegenerating outputs, which mostly retrieve documents at fixed intervals usingthe previous context as queries. In this work, we provide a generalized view ofactive retrieval augmented generation, methods that actively decide when andwhat to retrieve across the course of the generation. We proposeForward-Looking Active REtrieval augmented generation (FLARE), a genericretrieval-augmented generation method which iteratively uses a prediction ofthe upcoming sentence to anticipate future content, which is then utilized as aquery to retrieve relevant documents to regenerate the sentence if it containslow-confidence tokens. We test FLARE along with baselines comprehensively over4 long-form knowledge-intensive generation tasks/datasets. FLARE achievessuperior or competitive performance on all tasks, demonstrating theeffectiveness of our method. Code and datasets are available athttps://github.com/jzbjyb/FLARE.</description><author>Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig</author><pubDate>Thu, 11 May 2023 18:13:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06983v1</guid></item><item><title>Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning</title><link>http://arxiv.org/abs/2208.08831v2</link><description>Automatically discovering failures in vision models under real-world settingsremains an open challenge. This work demonstrates how off-the-shelf,large-scale, image-to-text and text-to-image models, trained on vast amounts ofdata, can be leveraged to automatically find such failures. In essence, aconditional text-to-image generative model is used to generate large amounts ofsynthetic, yet realistic, inputs given a ground-truth label. Misclassifiedinputs are clustered and a captioning model is used to describe each cluster.Each cluster's description is used in turn to generate more inputs and assesswhether specific clusters induce more failures than expected. We use thispipeline to demonstrate that we can effectively interrogate classifiers trainedon ImageNet to find specific failure cases and discover spurious correlations.We also show that we can scale the approach to generate adversarial datasetstargeting specific classifier architectures. This work serves as aproof-of-concept demonstrating the utility of large-scale generative models toautomatically discover bugs in vision models in an open-ended manner. We alsodescribe a number of limitations and pitfalls related to this approach.</description><author>Olivia Wiles, Isabela Albuquerque, Sven Gowal</author><pubDate>Thu, 11 May 2023 18:13:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.08831v2</guid></item><item><title>Making Intelligence: Ethical Values in IQ and ML Benchmarks</title><link>http://arxiv.org/abs/2209.00692v4</link><description>In recent years, ML researchers have wrestled with defining and improvingmachine learning (ML) benchmarks and datasets. In parallel, some have trained acritical lens on the ethics of dataset creation and ML research. In thisposition paper, we highlight the entanglement of ethics with seemingly``technical'' or ``scientific'' decisions about the design of ML benchmarks.Our starting point is the existence of multiple overlooked structuralsimilarities between human intelligence benchmarks and ML benchmarks. Bothtypes of benchmarks set standards for describing, evaluating, and comparingperformance on tasks relevant to intelligence -- standards that many scholarsof human intelligence have long recognized as value-laden. We use perspectivesfrom feminist philosophy of science on IQ benchmarks and thick concepts insocial science to argue that values need to be considered and documented whencreating ML benchmarks. It is neither possible nor desirable to avoid thischoice by creating value-neutral benchmarks. Finally, we outline practicalrecommendations for ML benchmark research ethics and ethics review.</description><author>Borhane Blili-Hamelin, Leif Hancox-Li</author><pubDate>Thu, 11 May 2023 18:09:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.00692v4</guid></item><item><title>Meta-hallucinator: Towards Few-Shot Cross-Modality Cardiac Image Segmentation</title><link>http://arxiv.org/abs/2305.06978v1</link><description>Domain shift and label scarcity heavily limit deep learning applications tovarious medical image analysis tasks. Unsupervised domain adaptation (UDA)techniques have recently achieved promising cross-modality medical imagesegmentation by transferring knowledge from a label-rich source domain to anunlabeled target domain. However, it is also difficult to collect annotationsfrom the source domain in many clinical applications, rendering most priorworks suboptimal with the label-scarce source domain, particularly for few-shotscenarios, where only a few source labels are accessible. To achieve efficientfew-shot cross-modality segmentation, we propose a noveltransformation-consistent meta-hallucination framework, meta-hallucinator, withthe goal of learning to diversify data distributions and generate usefulexamples for enhancing cross-modality performance. In our framework,hallucination and segmentation models are jointly trained with thegradient-based meta-learning strategy to synthesize examples that lead to goodsegmentation performance on the target domain. To further facilitate datahallucination and cross-domain knowledge transfer, we develop a self-ensemblingmodel with a hallucination-consistent property. Our meta-hallucinator canseamlessly collaborate with the meta-segmenter for learning to hallucinate withmutual benefits from a combined view of meta-learning and self-ensemblinglearning. Extensive studies on MM-WHS 2017 dataset for cross-modality cardiacsegmentation demonstrate that our method performs favorably against variousapproaches by a lot in the few-shot UDA scenario.</description><author>Ziyuan Zhao, Fangcheng Zhou, Zeng Zeng, Cuntai Guan, S. Kevin Zhou</author><pubDate>Thu, 11 May 2023 18:06:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06978v1</guid></item><item><title>FreePoint: Unsupervised Point Cloud Instance Segmentation</title><link>http://arxiv.org/abs/2305.06973v1</link><description>Instance segmentation of point clouds is a crucial task in 3D field withnumerous applications that involve localizing and segmenting objects in ascene. However, achieving satisfactory results requires a large number ofmanual annotations, which is a time-consuming and expensive process. Toalleviate dependency on annotations, we propose a method, called FreePoint, forunderexplored unsupervised class-agnostic instance segmentation on pointclouds. In detail, we represent the point features by combining coordinates,colors, normals, and self-supervised deep features. Based on the pointfeatures, we perform a multicut algorithm to segment point clouds into coarseinstance masks as pseudo labels, which are used to train a point cloud instancesegmentation model. To alleviate the inaccuracy of coarse masks duringtraining, we propose a weakly-supervised training strategy and correspondingloss. Our work can also serve as an unsupervised pre-training pretext forsupervised semantic instance segmentation with limited annotations. Forclass-agnostic instance segmentation on point clouds, FreePoint largely fillsthe gap with its fully-supervised counterpart based on the state-of-the-artinstance segmentation model Mask3D and even surpasses some previousfully-supervised methods. When serving as a pretext task and fine-tuning onS3DIS, FreePoint outperforms training from scratch by 5.8% AP with only 10%mask annotations.</description><author>Zhikai Zhang, Jian Ding, Li Jiang, Dengxin Dai, Gui-Song Xia</author><pubDate>Thu, 11 May 2023 17:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06973v1</guid></item><item><title>Large Language Models Can Be Used To Effectively Scale Spear Phishing Campaigns</title><link>http://arxiv.org/abs/2305.06972v1</link><description>Recent progress in artificial intelligence (AI), particularly in the domainof large language models (LLMs), has resulted in powerful and versatiledual-use systems. Indeed, cognition can be put towards a wide variety of tasks,some of which can result in harm. This study investigates how LLMs can be usedfor spear phishing, a prevalent form of cybercrime that involves manipulatingtargets into divulging sensitive information. I first explore LLMs' ability toassist with the reconnaissance and message generation stages of a successfulspear phishing attack, where I find that advanced LLMs are capable ofmeaningfully improving cybercriminals' efficiency during these stages. Next, Iconduct an empirical test by creating unique spear phishing messages for over600 British Members of Parliament using OpenAI's GPT-3.5 and GPT-4 models. Myfindings reveal that these messages are not only realistic but also remarkablycost-effective, as each email cost only a fraction of a cent to generate. Next,I demonstrate how basic prompt engineering can circumvent safeguards installedin LLMs by the reinforcement learning from human feedback fine-tuning process,highlighting the need for more robust governance interventions aimed atmitigating misuse. To address these evolving risks, I propose two potentialsolutions: structured access schemes, such as application programminginterfaces, and LLM-based defensive systems.</description><author>Julian Hazell</author><pubDate>Thu, 11 May 2023 17:55:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06972v1</guid></item><item><title>A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges</title><link>http://arxiv.org/abs/2305.06969v1</link><description>The widespread adoption of Machine Learning systems, especially in moredecision-critical applications such as criminal sentencing and bank loans, hasled to increased concerns about fairness implications. Algorithms and metricshave been developed to mitigate and measure these discriminations. Morerecently, works have identified a more challenging form of bias calledintersectional bias, which encompasses multiple sensitive attributes, such asrace and gender, together. In this survey, we review the state-of-the-art inintersectional fairness. We present a taxonomy for intersectional notions offairness and mitigation. Finally, we identify the key challenges and provideresearchers with guidelines for future directions.</description><author>Usman Gohar, Lu Cheng</author><pubDate>Thu, 11 May 2023 17:49:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06969v1</guid></item><item><title>HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation</title><link>http://arxiv.org/abs/2305.06968v1</link><description>Monocular 3D human pose and shape estimation is an ill-posed problem sincemultiple 3D solutions can explain a 2D image of a subject. Recent approachespredict a probability distribution over plausible 3D pose and shape parametersconditioned on the image. We show that these approaches exhibit a trade-offbetween three key properties: (i) accuracy - the likelihood of the ground-truth3D solution under the predicted distribution, (ii) sample-input consistency -the extent to which 3D samples from the predicted distribution match thevisible 2D image evidence, and (iii) sample diversity - the range of plausible3D solutions modelled by the predicted distribution. Our method, HuManiFlow,predicts simultaneously accurate, consistent and diverse distributions. We usethe human kinematic tree to factorise full body pose into ancestor-conditionedper-body-part pose distributions in an autoregressive manner. Per-body-partdistributions are implemented using normalising flows that respect the manifoldstructure of SO(3), the Lie group of per-body-part poses. We show thatill-posed, but ubiquitous, 3D point estimate losses reduce sample diversity,and employ only probabilistic training losses. Code is available at:https://github.com/akashsengupta1997/HuManiFlow.</description><author>Akash Sengupta, Ignas Budvytis, Roberto Cipolla</author><pubDate>Thu, 11 May 2023 17:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06968v1</guid></item><item><title>Data quality dimensions for fair AI</title><link>http://arxiv.org/abs/2305.06967v1</link><description>AI systems are not intrinsically neutral and biases trickle in any type oftechnological tool. In particular when dealing with people, AI algorithmsreflect technical errors originating with mislabeled data. As they feed wrongand discriminatory classifications, perpetuating structural racism andmarginalization, these systems are not systematically guarded against bias. Inthis article we consider the problem of bias in AI systems from the point ofview of Information Quality dimensions. We illustrate potential improvements ofa bias mitigation tool in gender classification errors, referring to twotypically difficult contexts: the classification of non-binary individuals andthe classification of transgender individuals. The identification of dataquality dimensions to implement in bias mitigation tool may help achieve morefairness. Hence, we propose to consider this issue in terms of completeness,consistency, timeliness and reliability, and offer some theoretical results.</description><author>Camilla Quaresmini, Giuseppe Primiero</author><pubDate>Thu, 11 May 2023 17:48:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06967v1</guid></item><item><title>Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most</title><link>http://arxiv.org/abs/2302.09195v3</link><description>Self-supervised learning (SSL) learns high-quality representations from largepools of unlabeled training data. As datasets grow larger, it becomes crucialto identify the examples that contribute the most to learning suchrepresentations. This enables efficient SSL by reducing the volume of datarequired for learning high-quality representations. Nevertheless, quantifyingthe value of examples for SSL has remained an open question. In this work, weaddress this for the first time, by proving that examples that contribute themost to contrastive SSL are those that have the most similar augmentations toother examples, in expectation. We provide rigorous guarantees for thegeneralization performance of SSL on such subsets. Empirically, we discover,perhaps surprisingly, the subsets that contribute the most to SSL are thosethat contribute the least to supervised learning. Through extensiveexperiments, we show that our subsets outperform random subsets by more than 3%on CIFAR100, CIFAR10, and STL10. Interestingly, we also find that we can safelyexclude 20% of examples from CIFAR100 and 40% from STL10, without affectingdownstream task performance.</description><author>Siddharth Joshi, Baharan Mirzasoleiman</author><pubDate>Thu, 11 May 2023 17:47:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09195v3</guid></item><item><title>Transformers for CT Reconstruction From Monoplanar and Biplanar Radiographs</title><link>http://arxiv.org/abs/2305.06965v1</link><description>Computed Tomography (CT) scans provide detailed and accurate information ofinternal structures in the body. They are constructed by sending x-rays throughthe body from different directions and combining this information into athree-dimensional volume. Such volumes can then be used to diagnose a widerange of conditions and allow for volumetric measurements of organs. In thiswork, we tackle the problem of reconstructing CT images from biplanar x-raysonly. X-rays are widely available and even if the CT reconstructed from theseradiographs is not a replacement of a complete CT in the diagnostic setting, itmight serve to spare the patients from radiation where a CT is only acquiredfor rough measurements such as determining organ size. We propose a novelmethod based on the transformer architecture, by framing the underlying task asa language translation problem. Radiographs and CT images are first embeddedinto latent quantized codebook vectors using two different autoencodernetworks. We then train a GPT model, to reconstruct the codebook vectors of theCT image, conditioned on the codebook vectors of the x-rays and show that thisapproach leads to realistic looking images. To encourage further research inthis direction, we make our code publicly available on GitHub: XXX.</description><author>Firas Khader, Gustav Müller-Franzes, Tianyu Han, Sven Nebelung, Christiane Kuhl, Johannes Stegmaier, Daniel Truhn</author><pubDate>Thu, 11 May 2023 17:43:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06965v1</guid></item><item><title>Cascaded Cross-Attention Networks for Data-Efficient Whole-Slide Image Classification Using Transformers</title><link>http://arxiv.org/abs/2305.06963v1</link><description>Whole-Slide Imaging allows for the capturing and digitization ofhigh-resolution images of histological specimen. An automated analysis of suchimages using deep learning models is therefore of high demand. The transformerarchitecture has been proposed as a possible candidate for effectivelyleveraging the high-resolution information. Here, the whole-slide image ispartitioned into smaller image patches and feature tokens are extracted fromthese image patches. However, while the conventional transformer allows for asimultaneous processing of a large set of input tokens, the computationaldemand scales quadratically with the number of input tokens and thusquadratically with the number of image patches. To address this problem wepropose a novel cascaded cross-attention network (CCAN) based on thecross-attention mechanism that scales linearly with the number of extractedpatches. Our experiments demonstrate that this architecture is at least on-parwith and even outperforms other attention-based state-of-the-art methods on twopublic datasets: On the use-case of lung cancer (TCGA NSCLC) our model reachesa mean area under the receiver operating characteristic (AUC) of 0.970 $\pm$0.008 and on renal cancer (TCGA RCC) reaches a mean AUC of 0.985 $\pm$ 0.004.Furthermore, we show that our proposed model is efficient in low-data regimes,making it a promising approach for analyzing whole-slide images inresource-limited settings. To foster research in this direction, we make ourcode publicly available on GitHub: XXX.</description><author>Firas Khader, Jakob Nikolas Kather, Tianyu Han, Sven Nebelung, Christiane Kuhl, Johannes Stegmaier, Daniel Truhn</author><pubDate>Thu, 11 May 2023 17:42:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06963v1</guid></item><item><title>Embedded Feature Similarity Optimization with Specific Parameter Initialization for 2D/3D Registration</title><link>http://arxiv.org/abs/2305.06252v2</link><description>We present a novel deep learning-based framework: Embedded Feature SimilarityOptimization with Specific Parameter Initialization (SOPI) for 2D/3Dregistration which is a most challenging problem due to the difficulty such asdimensional mismatch, heavy computation load and lack of golden evaluatingstandard. The framework we designed includes a parameter specification moduleto efficiently choose initialization pose parameter and a fine-registrationnetwork to align images. The proposed framework takes extracting multi-scalefeatures into consideration using a novel composite connection encoder withspecial training techniques. The method is compared with both learning-basedmethods and optimization-based methods to further evaluate the performance. Ourexperiments demonstrate that the method in this paper has improved theregistration performance, and thereby outperforms the existing methods in termsof accuracy and running time. We also show the potential of the proposed methodas an initial pose estimator.</description><author>Minheng Chen, Zhirun Zhang, Shuheng Gu, Youyong Kong</author><pubDate>Thu, 11 May 2023 17:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06252v2</guid></item><item><title>FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery</title><link>http://arxiv.org/abs/2211.08316v2</link><description>Understanding users' intentions in e-commerce platforms requires commonsenseknowledge. In this paper, we present FolkScope, an intention knowledge graphconstruction framework to reveal the structure of humans' minds aboutpurchasing items. As commonsense knowledge is usually ineffable and notexpressed explicitly, it is challenging to perform information extraction.Thus, we propose a new approach that leverages the generation power of largelanguage models~(LLMs) and human-in-the-loop annotation to semi-automaticallyconstruct the knowledge graph. LLMs first generate intention assertions viae-commerce-specific prompts to explain shopping behaviors, where the intentioncan be an open reason or a predicate falling into one of 18 categories aligningwith ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibilityand typicality labels of sampled intentions as training data in order topopulate human judgments to all automatic generations. Last, to structurize theassertions, we propose pattern mining and conceptualization to form morecondensed and abstract knowledge. Extensive evaluations and studies demonstratethat our constructed knowledge graph can well model e-commerce knowledge andhave many potential applications.</description><author>Changlong Yu, Weiqi Wang, Xin Liu, Jiaxin Bai, Yangqiu Song, Zheng Li, Yifan Gao, Tianyu Cao, Bing Yin</author><pubDate>Thu, 11 May 2023 17:33:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.08316v2</guid></item><item><title>FastDiagP: An Algorithm for Parallelized Direct Diagnosis</title><link>http://arxiv.org/abs/2305.06951v1</link><description>Constraint-based applications attempt to identify a solution that meets alldefined user requirements. If the requirements are inconsistent with theunderlying constraint set, algorithms that compute diagnoses for inconsistentconstraints should be implemented to help users resolve the "no solution couldbe found" dilemma. FastDiag is a typical direct diagnosis algorithm thatsupports diagnosis calculation without predetermining conflicts. However, thisapproach faces runtime performance issues, especially when analyzing complexand large-scale knowledge bases. In this paper, we propose a novel algorithm,so-called FastDiagP, which is based on the idea of speculative programming.This algorithm extends FastDiag by integrating a parallelization mechanism thatanticipates and pre-calculates consistency checks requested by FastDiag. Thismechanism helps to provide consistency checks with fast answers and boosts thealgorithm's runtime performance. The performance improvements of our proposedalgorithm have been shown through empirical results using the Linux-2.6.3.33configuration knowledge base.</description><author>Viet-Man Le, Cristian Vidal Silva, Alexander Felfernig, David Benavides, José Galindo, Thi Ngoc Trang Tran</author><pubDate>Thu, 11 May 2023 17:26:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06951v1</guid></item><item><title>SalienDet: A Saliency-based Feature Enhancement Algorithm for Object Detection for Autonomous Driving</title><link>http://arxiv.org/abs/2305.06940v1</link><description>Object detection (OD) is crucial to autonomous driving. Unknown objects areone of the reasons that hinder autonomous vehicles from driving beyond theoperational domain. We propose a saliency-based OD algorithm (SalienDet) todetect objects that do not appear in the training sample set. SalienDetutilizes a saliency-based algorithm to enhance image features for objectproposal generation. Then, we design a dataset relabeling approach todifferentiate the unknown objects from all objects to achieve open-worlddetection. We evaluate SalienDet on KITTI, NuScenes, and BDD datasets, and theresult indicates that it outperforms existing algorithms for unknown objectdetection. Additionally, SalienDet can be easily adapted for incrementallearning in open-world detection tasks.</description><author>Ning Ding, Ce Zhang, Azim Eskandarian</author><pubDate>Thu, 11 May 2023 17:19:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06940v1</guid></item><item><title>Deep Multi-View Subspace Clustering with Anchor Graph</title><link>http://arxiv.org/abs/2305.06939v1</link><description>Deep multi-view subspace clustering (DMVSC) has recently attracted increasingattention due to its promising performance. However, existing DMVSC methodsstill have two issues: (1) they mainly focus on using autoencoders tononlinearly embed the data, while the embedding may be suboptimal forclustering because the clustering objective is rarely considered inautoencoders, and (2) existing methods typically have a quadratic or even cubiccomplexity, which makes it challenging to deal with large-scale data. Toaddress these issues, in this paper we propose a novel deep multi-view subspaceclustering method with anchor graph (DMCAG). To be specific, DMCAG firstlylearns the embedded features for each view independently, which are used toobtain the subspace representations. To significantly reduce the complexity, weconstruct an anchor graph with small size for each view. Then, spectralclustering is performed on an integrated anchor graph to obtain pseudo-labels.To overcome the negative impact caused by suboptimal embedded features, we usepseudo-labels to refine the embedding process to make it more suitable for theclustering task. Pseudo-labels and embedded features are updated alternately.Furthermore, we design a strategy to keep the consistency of the labels basedon contrastive learning to enhance the clustering performance. Empiricalstudies on real-world datasets show that our method achieves superiorclustering performance over other state-of-the-art methods.</description><author>Chenhang Cui, Yazhou Ren, Jingyu Pu, Xiaorong Pu, Lifang He</author><pubDate>Thu, 11 May 2023 17:17:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06939v1</guid></item><item><title>Convergence of Alternating Gradient Descent for Matrix Factorization</title><link>http://arxiv.org/abs/2305.06927v1</link><description>We consider alternating gradient descent (AGD) with fixed step size $\eta &gt;0$, applied to the asymmetric matrix factorization objective. We show that, fora rank-$r$ matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$, $T = \left(\left(\frac{\sigma_1(\mathbf{A})}{\sigma_r(\mathbf{A})}\right)^2\log(1/\epsilon)\right)$ iterations of alternating gradient descent suffice toreach an $\epsilon$-optimal factorization $\| \mathbf{A} -\mathbf{X}_T^{\vphantom{\intercal}} \mathbf{Y}_T^{\intercal} \|_{\rm F}^2 \leq\epsilon \| \mathbf{A} \|_{\rm F}^2$ with high probability starting from anatypical random initialization. The factors have rank $d&gt;r$ so that$\mathbf{X}_T\in\mathbb{R}^{m \times d}$ and $\mathbf{Y}_T \in\mathbb{R}^{n\times d}$. Experiments suggest that our proposed initialization is not merelyof theoretical benefit, but rather significantly improves convergence ofgradient descent in practice. Our proof is conceptually simple: a uniformPL-inequality and uniform Lipschitz smoothness constant are guaranteed for asufficient number of iterations, starting from our random initialization. Ourproof method should be useful for extending and simplifying convergenceanalyses for a broader class of nonconvex low-rank factorization problems.</description><author>Rachel Ward, Tamara G. Kolda</author><pubDate>Thu, 11 May 2023 17:07:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06927v1</guid></item><item><title>EAML: Ensemble Self-Attention-based Mutual Learning Network for Document Image Classification</title><link>http://arxiv.org/abs/2305.06923v1</link><description>In the recent past, complex deep neural networks have received huge interestin various document understanding tasks such as document image classificationand document retrieval. As many document types have a distinct visual style,learning only visual features with deep CNNs to classify document images haveencountered the problem of low inter-class discrimination, and high intra-classstructural variations between its categories. In parallel, text-levelunderstanding jointly learned with the corresponding visual properties within agiven document image has considerably improved the classification performancein terms of accuracy. In this paper, we design a self-attention-based fusionmodule that serves as a block in our ensemble trainable network. It allows tosimultaneously learn the discriminant features of image and text modalitiesthroughout the training stage. Besides, we encourage mutual learning bytransferring the positive knowledge between image and text modalities duringthe training stage. This constraint is realized by adding atruncated-Kullback-Leibler divergence loss Tr-KLD-Reg as a new regularizationterm, to the conventional supervised setting. To the best of our knowledge,this is the first time to leverage a mutual learning approach along with aself-attention-based fusion module to perform document image classification.The experimental results illustrate the effectiveness of our approach in termsof accuracy for the single-modal and multi-modal modalities. Thus, the proposedensemble self-attention-based mutual learning model outperforms thestate-of-the-art classification results based on the benchmark RVL-CDIP andTobacco-3482 datasets.</description><author>Souhail Bakkali, Ziheng Ming, Mickael Coustaty, Marçal Rusiñol</author><pubDate>Thu, 11 May 2023 17:05:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06923v1</guid></item><item><title>A Machine Learning Approach to Improving Timing Consistency between Global Route and Detailed Route</title><link>http://arxiv.org/abs/2305.06917v1</link><description>Due to the unavailability of routing information in design stages prior todetailed routing (DR), the tasks of timing prediction and optimization posemajor challenges. Inaccurate timing prediction wastes design effort, hurtscircuit performance, and may lead to design failure. This work focuses ontiming prediction after clock tree synthesis and placement legalization, whichis the earliest opportunity to time and optimize a "complete" netlist. Thepaper first documents that having "oracle knowledge" of the final post-DRparasitics enables post-global routing (GR) optimization to produce improvedfinal timing outcomes. To bridge the gap between GR-based parasitic and timingestimation and post-DR results during post-GR optimization, machine learning(ML)-based models are proposed, including the use of features for macroblockages for accurate predictions for designs with macros. Based on a set ofexperimental evaluations, it is demonstrated that these models show higheraccuracy than GR-based timing estimation. When used during post-GRoptimization, the ML-based models show demonstrable improvements in post-DRcircuit performance. The methodology is applied to two different tool flows -OpenROAD and a commercial tool flow - and results on 45nm bulk and 12nm FinFETenablements show improvements in post-DR slack metrics without increasingcongestion. The models are demonstrated to be generalizable to designsgenerated under different clock period constraints and are robust to trainingdata with small levels of noise.</description><author>Vidya A. Chhabria Wenjing Jiang Andrew B. Kahng Sachin S. Sapatnekar</author><pubDate>Thu, 11 May 2023 17:01:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06917v1</guid></item><item><title>Self-contained Beta-with-Spikes Approximation for Inference Under a Wright-Fisher Model</title><link>http://arxiv.org/abs/2303.04691v2</link><description>We construct a reliable estimation of evolutionary parameters within theWright-Fisher model, which describes changes in allele frequencies due toselection and genetic drift, from time-series data. Such data exists forbiological populations, for example via artificial evolution experiments, andfor the cultural evolution of behavior, such as linguistic corpora thatdocument historical usage of different words with similar meanings. Our methodof analysis builds on a Beta-with-Spikes approximation to the distribution ofallele frequencies predicted by the Wright-Fisher model. We introduce aself-contained scheme for estimating the parameters in the approximation, anddemonstrate its robustness with synthetic data, especially in thestrong-selection and near-extinction regimes where previous approaches fail. Wefurther apply to allele frequency data for baker's yeast (Saccharomycescerevisiae), finding a significant signal of selection in cases whereindependent evidence supports such a conclusion. We further demonstrate thepossibility of detecting time-points at which evolutionary parameters change inthe context of a historical spelling reform in the Spanish language.</description><author>Juan Guerrero Montero, Richard A. Blythe</author><pubDate>Thu, 11 May 2023 16:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04691v2</guid></item><item><title>Meta-Learners for Few-Shot Weakly-Supervised Medical Image Segmentation</title><link>http://arxiv.org/abs/2305.06912v1</link><description>Most uses of Meta-Learning in visual recognition are very often applied toimage classification, with a relative lack of works in other tasks {such} assegmentation and detection. We propose a generic Meta-Learning framework forfew-shot weakly-supervised segmentation in medical imaging domains. We conducta comparative analysis of meta-learners from distinct paradigms adapted tofew-shot image segmentation in different sparsely annotated radiological tasks.The imaging modalities include 2D chest, mammographic and dental X-rays, aswell as 2D slices of volumetric tomography and resonance images. Ourexperiments consider a total of 9 meta-learners, 4 backbones and multipletarget organ segmentation tasks. We explore small-data scenarios in radiologywith varying weak annotation styles and densities. Our analysis shows thatmetric-based meta-learning approaches achieve better segmentation results intasks with smaller domain shifts in comparison to the meta-training datasets,while some gradient- and fusion-based meta-learners are more generalizable tolarger domain shifts.</description><author>Hugo Oliveira, Pedro H. T. Gama, Isabelle Bloch, Roberto Marcondes Cesar Jr</author><pubDate>Thu, 11 May 2023 16:57:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06912v1</guid></item><item><title>Pushing the Boundaries of Tractable Multiperspective Reasoning: A Deduction Calculus for Standpoint EL+</title><link>http://arxiv.org/abs/2304.14323v2</link><description>Standpoint EL is a multi-modal extension of the popular description logic ELthat allows for the integrated representation of domain knowledge relative todiverse standpoints or perspectives. Advantageously, its satisfiability problemhas recently been shown to be in PTime, making it a promising framework forlarge-scale knowledge integration. In this paper, we show that we can further push the expressivity of thisformalism, arriving at an extended logic, called Standpoint EL+, which allowsfor axiom negation, role chain axioms, self-loops, and other features, whilemaintaining tractability. This is achieved by designing asatisfiability-checking deduction calculus, which at the same time addressesthe need for practical algorithms. We demonstrate the feasibility of ourcalculus by presenting a prototypical Datalog implementation of its deductionrules.</description><author>Lucía Gómez Álvarez, Sebastian Rudolph, Hannes Strass</author><pubDate>Thu, 11 May 2023 16:55:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14323v2</guid></item><item><title>Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)</title><link>http://arxiv.org/abs/2305.06299v2</link><description>Large language models, particularly GPT-3, are able to produce high qualitysummaries of general domain news articles in few- and zero-shot settings.However, it is unclear if such models are similarly capable in morespecialized, high-stakes domains such as biomedicine. In this paper, we enlistdomain experts (individuals with medical training) to evaluate summaries ofbiomedical articles generated by GPT-3, given zero supervision. We considerboth single- and multi-document settings. In the former, GPT-3 is tasked withgenerating regular and plain-language summaries of articles describingrandomized controlled trials; in the latter, we assess the degree to whichGPT-3 is able to \emph{synthesize} evidence reported across a collection ofarticles. We design an annotation scheme for evaluating model outputs, with anemphasis on assessing the factual accuracy of generated summaries. We find thatwhile GPT-3 is able to summarize and simplify single biomedical articlesfaithfully, it struggles to provide accurate aggregations of findings overmultiple documents. We release all data and annotations used in this work.</description><author>Chantal Shaib, Millicent L. Li, Sebastian Joseph, Iain J. Marshall, Junyi Jessy Li, Byron C. Wallace</author><pubDate>Thu, 11 May 2023 16:51:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06299v2</guid></item><item><title>CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model</title><link>http://arxiv.org/abs/2305.06908v1</link><description>Denoising diffusion probabilistic models (DDPMs) have shown promisingperformance for speech synthesis. However, a large number of iterative stepsare required to achieve high sample quality, which restricts the inferencespeed. Maintaining sample quality while increasing sampling speed has become achallenging task. In this paper, we propose a "Co"nsistency "Mo"del-based"Speech" synthesis method, CoMoSpeech, which achieve speech synthesis through asingle diffusion sampling step while achieving high audio quality. Theconsistency constraint is applied to distill a consistency model from awell-designed diffusion-based teacher model, which ultimately yields superiorperformances in the distilled CoMoSpeech. Our experiments show that bygenerating audio recordings by a single sampling step, the CoMoSpeech achievesan inference speed more than 150 times faster than real-time on a single NVIDIAA100 GPU, which is comparable to FastSpeech2, making diffusion-sampling basedspeech synthesis truly practical. Meanwhile, objective and subjectiveevaluations on text-to-speech and singing voice synthesis show that theproposed teacher models yield the best audio quality, and the one-step samplingbased CoMoSpeech achieves the best inference speed with better or comparableaudio quality to other conventional multi-step diffusion model baselines. Audiosamples are available at https://comospeech.github.io/.</description><author>Zhen Ye, Wei Xue, Xu Tan, Jie Chen, Qifeng Liu, Yike Guo</author><pubDate>Thu, 11 May 2023 16:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06908v1</guid></item><item><title>Enhancing Robustness of Gradient-Boosted Decision Trees through One-Hot Encoding and Regularization</title><link>http://arxiv.org/abs/2304.13761v3</link><description>Gradient-boosted decision trees (GBDT) are widely used and highly effectivemachine learning approach for tabular data modeling. However, their complexstructure may lead to low robustness against small covariate perturbation inunseen data. In this study, we apply one-hot encoding to convert a GBDT modelinto a linear framework, through encoding of each tree leaf to one dummyvariable. This allows for the use of linear regression techniques, plus a novelrisk decomposition for assessing the robustness of a GBDT model againstcovariate perturbations. We propose to enhance the robustness of GBDT models byrefitting their linear regression forms with $L_1$ or $L_2$ regularization.Theoretical results are obtained about the effect of regularization on themodel performance and robustness. It is demonstrated through numericalexperiments that the proposed regularization approach can enhance therobustness of the one-hot-encoded GBDT models.</description><author>Shijie Cui, Agus Sudjianto, Aijun Zhang, Runze Li</author><pubDate>Thu, 11 May 2023 16:47:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13761v3</guid></item><item><title>Combating noisy labels in object detection datasets</title><link>http://arxiv.org/abs/2211.13993v2</link><description>The quality of training datasets for deep neural networks is a key factorcontributing to the accuracy of resulting models. This effect is amplified indifficult tasks such as object detection. Dealing with errors in datasets isoften limited to accepting that some fraction of examples is incorrect,estimating their confidence and assigning appropriate weights or ignoringuncertain ones during training. In this work, we propose a different approach.We introduce the Confident Learning for Object Detection (CLOD) algorithm forassessing the quality of each label in object detection datasets, identifyingmissing, spurious, mislabeled and mislocated bounding boxes and suggestingcorrections. By focusing on finding incorrect examples in the trainingdatasets, we can eliminate them at the root. Suspicious bounding boxes can bereviewed in order to improve the quality of the dataset, leading to bettermodels without further complicating their already complex architectures. Theproposed method is able to point out 99% of artificially disturbed boundingboxes with a false positive rate below 0.3. We see this method as a promisingpath to correcting popular object detection datasets.</description><author>Krystian Chachuła, Jakub Łyskawa, Bartłomiej Olber, Piotr Frątczak, Adam Popowicz, Krystian Radlak</author><pubDate>Thu, 11 May 2023 16:46:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13993v2</guid></item><item><title>Improving Hyperspectral Adversarial Robustness Under Multiple Attacks</title><link>http://arxiv.org/abs/2210.16346v4</link><description>Semantic segmentation models classifying hyperspectral images (HSI) arevulnerable to adversarial examples. Traditional approaches to adversarialrobustness focus on training or retraining a single network on attacked data,however, in the presence of multiple attacks these approaches decrease inperformance compared to networks trained individually on each attack. To combatthis issue we propose an Adversarial Discriminator Ensemble Network (ADE-Net)which focuses on attack type detection and adversarial robustness under aunified model to preserve per data-type weight optimally while robustifiyingthe overall network. In the proposed method, a discriminator network is used toseparate data by attack type into their specific attack-expert ensemblenetwork.</description><author>Nicholas Soucy, Salimeh Yasaei Sekeh</author><pubDate>Thu, 11 May 2023 16:44:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16346v4</guid></item><item><title>AfriQA: Cross-lingual Open-Retrieval Question Answering for African Languages</title><link>http://arxiv.org/abs/2305.06897v1</link><description>African languages have far less in-language content available digitally,making it challenging for question answering systems to satisfy the informationneeds of users. Cross-lingual open-retrieval question answering (XOR QA)systems -- those that retrieve answer content from other languages whileserving people in their native language -- offer a means of filling this gap.To this end, we create AfriQA, the first cross-lingual QA dataset with a focuson African languages. AfriQA includes 12,000+ XOR QA examples across 10 Africanlanguages. While previous datasets have focused primarily on languages wherecross-lingual QA augments coverage from the target language, AfriQA focuses onlanguages where cross-lingual answer content is the only high-coverage sourceof answer content. Because of this, we argue that African languages are one ofthe most important and realistic use cases for XOR QA. Our experimentsdemonstrate the poor performance of automatic translation and multilingualretrieval methods. Overall, AfriQA proves challenging for state-of-the-art QAmodels. We hope that the dataset enables the development of more equitable QAtechnology.</description><author>Odunayo Ogundepo, Tajuddeen R. Gwadabe, Clara E. Rivera, Jonathan H. Clark, Sebastian Ruder, David Ifeoluwa Adelani, Bonaventure F. P. Dossou, Abdou Aziz DIOP, Claytone Sikasote, Gilles Hacheme, Happy Buzaaba, Ignatius Ezeani, Rooweither Mabuya, Salomey Osei, Chris Emezue, Albert Njoroge Kahira, Shamsuddeen H. Muhammad, Akintunde Oladipo, Abraham Toluwase Owodunni, Atnafu Lambebo Tonja, Iyanuoluwa Shode, Akari Asai, Tunde Oluwaseyi Ajayi, Clemencia Siro, Steven Arthur, Mofetoluwa Adeyemi, Orevaoghene Ahia, Aremu Anuoluwapo, Oyinkansola Awosan, Chiamaka Chukwuneke, Bernard Opoku, Awokoya Ayodele, Verrah Otiende, Christine Mwase, Boyd Sinkala, Andre Niyongabo Rubungo, Daniel A. Ajisafe, Emeka Felix Onwuegbuzia, Habib Mbow, Emile Niyomutabazi, Eunice Mukonde, Falalu Ibrahim Lawan, Ibrahim Sai</author><pubDate>Thu, 11 May 2023 16:34:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06897v1</guid></item><item><title>Covariance regression with random forests</title><link>http://arxiv.org/abs/2209.08173v3</link><description>Capturing the conditional covariances or correlations among the elements of amultivariate response vector based on covariates is important to various fieldsincluding neuroscience, epidemiology and biomedicine. We propose a new methodcalled Covariance Regression with Random Forests (CovRegRF) to estimate thecovariance matrix of a multivariate response given a set of covariates, using arandom forest framework. Random forest trees are built with a splitting rulespecially designed to maximize the difference between the sample covariancematrix estimates of the child nodes. We also propose a significance test forthe partial effect of a subset of covariates. We evaluate the performance ofthe proposed method and significance test through a simulation study whichshows that the proposed method provides accurate covariance matrix estimatesand that the Type-1 error is well controlled. An application of the proposedmethod to thyroid disease data is also presented. CovRegRF is implemented in afreely available R package on CRAN.</description><author>Cansu Alakus, Denis Larocque, Aurelie Labbe</author><pubDate>Thu, 11 May 2023 16:32:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.08173v3</guid></item><item><title>Reinterpreting causal discovery as the task of predicting unobserved joint statistics</title><link>http://arxiv.org/abs/2305.06894v1</link><description>If $X,Y,Z$ denote sets of random variables, two different data sources maycontain samples from $P_{X,Y}$ and $P_{Y,Z}$, respectively. We argue thatcausal discovery can help inferring properties of the `unobserved jointdistributions' $P_{X,Y,Z}$ or $P_{X,Z}$. The properties may be conditionalindependences (as in `integrative causal inference') or also quantitativestatements about dependences. More generally, we define a learning scenario where the input is a subset ofvariables and the label is some statistical property of that subset. Sets ofjointly observed variables define the training points, while unobserved setsare possible test points. To solve this learning task, we infer, as anintermediate step, a causal model from the observations that then entailsproperties of unobserved sets. Accordingly, we can define the VC dimension of aclass of causal models and derive generalization bounds for the predictions. Here, causal discovery becomes more modest and better accessible to empiricaltests than usual: rather than trying to find a causal hypothesis that is `true'a causal hypothesis is {\it useful} whenever it correctly predicts statisticalproperties of unobserved joint distributions. This way, a sparse causal graphthat omits weak influences may be more useful than a dense one (despite beingless accurate) because it is able to reconstruct the full joint distributionfrom marginal distributions of smaller subsets. Within such a `pragmatic' application of causal discovery, some popularheuristic approaches become justified in retrospect. It is, for instance,allowed to infer DAGs from partial correlations instead of conditionalindependences if the DAGs are only used to predict partial correlations.</description><author>Dominik Janzing, Philipp M. Faller, Leena Chennuru Vankadara</author><pubDate>Thu, 11 May 2023 16:30:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06894v1</guid></item><item><title>IUST_NLP at SemEval-2023 Task 10: Explainable Detecting Sexism with Transformers and Task-adaptive Pretraining</title><link>http://arxiv.org/abs/2305.06892v1</link><description>This paper describes our system on SemEval-2023 Task 10: ExplainableDetection of Online Sexism (EDOS). This work aims to design an automatic systemfor detecting and classifying sexist content in online spaces. We propose a setof transformer-based pre-trained models with task-adaptive pretraining andensemble learning. The main contributions of our system include analyzing theperformance of different transformer-based pre-trained models and combiningthese models, as well as providing an efficient method using large amounts ofunlabeled data for model adaptive pretraining. We have also explored severalother strategies. On the test dataset, our system achieves F1-scores of 83%,64%, and 47% on subtasks A, B, and C, respectively.</description><author>Hadiseh Mahmoudi</author><pubDate>Thu, 11 May 2023 16:29:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06892v1</guid></item><item><title>Run-Off Election: Improved Provable Defense against Data Poisoning Attacks</title><link>http://arxiv.org/abs/2302.02300v2</link><description>In data poisoning attacks, an adversary tries to change a model's predictionby adding, modifying, or removing samples in the training data. Recently,ensemble-based approaches for obtaining provable defenses against datapoisoning have been proposed where predictions are done by taking a majorityvote across multiple base models. In this work, we show that merely consideringthe majority vote in ensemble defenses is wasteful as it does not effectivelyutilize available information in the logits layers of the base models. Instead,we propose Run-Off Election (ROE), a novel aggregation method based on atwo-round election across the base models: In the first round, models vote fortheir preferred class and then a second, Run-Off election is held between thetop two classes in the first round. Based on this approach, we propose DPA+ROEand FA+ROE defense methods based on Deep Partition Aggregation (DPA) and FiniteAggregation (FA) approaches from prior work. We evaluate our methods on MNIST,CIFAR-10, and GTSRB and obtain improvements in certified accuracy by up to3%-4%. Also, by applying ROE on a boosted version of DPA, we gain improvementsaround 12%-27% comparing to the current state-of-the-art, establishing a newstate-of-the-art in (pointwise) certified robustness against data poisoning. Inmany cases, our approach outperforms the state-of-the-art, even when using 32times less computational power.</description><author>Keivan Rezaei, Kiarash Banihashem, Atoosa Chegini, Soheil Feizi</author><pubDate>Thu, 11 May 2023 16:26:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02300v2</guid></item><item><title>A Category-theoretical Meta-analysis of Definitions of Disentanglement</title><link>http://arxiv.org/abs/2305.06886v1</link><description>Disentangling the factors of variation in data is a fundamental concept inmachine learning and has been studied in various ways by different researchers,leading to a multitude of definitions. Despite the numerous empirical studies,more theoretical research is needed to fully understand the defining propertiesof disentanglement and how different definitions relate to each other. Thispaper presents a meta-analysis of existing definitions of disentanglement,using category theory as a unifying and rigorous framework. We propose that theconcepts of the cartesian and monoidal products should serve as the core ofdisentanglement. With these core concepts, we show the similarities and crucialdifferences in dealing with (i) functions, (ii) equivariant maps, (iii)relations, and (iv) stochastic maps. Overall, our meta-analysis deepens ourunderstanding of disentanglement and its various formulations and can helpresearchers navigate different definitions and choose the most appropriate onefor their specific context.</description><author>Yivan Zhang, Masashi Sugiyama</author><pubDate>Thu, 11 May 2023 16:24:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06886v1</guid></item><item><title>A preferential interpretation of MultiLayer Perceptrons in a conditional logic with typicality</title><link>http://arxiv.org/abs/2305.00304v2</link><description>In this paper we investigate the relationships between a multipreferentialsemantics for defeasible reasoning in knowledge representation and a multilayerneural network model. Weighted knowledge bases for a simple description logicwith typicality are considered under a (many-valued) ``concept-wise"multipreference semantics. The semantics is used to provide a preferentialinterpretation of MultiLayer Perceptrons (MLPs). A model checking and anentailment based approach are exploited in the verification of conditionalproperties of MLPs.</description><author>Mario Alviano, Francesco Bartoli, Marco Botta, Roberto Esposito, Laura Giordano, Daniele Theseider Dupré</author><pubDate>Thu, 11 May 2023 16:13:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00304v2</guid></item><item><title>Language modeling via stochastic processes</title><link>http://arxiv.org/abs/2203.11370v2</link><description>Modern language models can generate high-quality short texts. However, theyoften meander or are incoherent when generating longer texts. These issuesarise from the next-token-only language modeling objective. Recent work inself-supervised learning suggests that models can learn good latentrepresentations via contrastive learning, which can be effective fordiscriminative tasks. Our work analyzes the application of contrastiverepresentations for generative tasks, like long text generation. We propose oneapproach for leveraging constrastive representations, which we call TimeControl (TC). TC first learns a contrastive representation of the target textdomain, then generates text by decoding from these representations. Compared todomain-specific methods and fine-tuning GPT2 across a variety of text domains,TC performs competitively to methods specific for learning sentencerepresentations on discourse coherence. On long text generation settings, TCpreserves the text structure both in terms of ordering (up to $+15\%$ better)and text length consistency (up to $+90\%$ better).</description><author>Rose E Wang, Esin Durmus, Noah Goodman, Tatsunori Hashimoto</author><pubDate>Thu, 11 May 2023 16:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.11370v2</guid></item><item><title>Multi-Tier Client Selection for Mobile Federated Learning Networks</title><link>http://arxiv.org/abs/2305.06865v1</link><description>Federated learning (FL), which addresses data privacy issues by trainingmodels on resource-constrained mobile devices in a distributed manner, hasattracted significant research attention. However, the problem of optimizing FLclient selection in mobile federated learning networks (MFLNs), where devicesmove in and out of each others' coverage and no FL server knows all the dataowners, remains open. To bridge this gap, we propose a first-of-its-kind\underline{Soc}ially-aware \underline{Fed}erated \underline{C}lient\underline{S}election (SocFedCS) approach to minimize costs and trainhigh-quality FL models. SocFedCS enriches the candidate FL client pool byenabling data owners to propagate FL task information through their localnetworks of trust, even as devices are moving into and out of each others'coverage. Based on Lyapunov optimization, we first transform this time-coupledproblem into a step-by-step optimization problem. Then, we design a methodbased on alternating minimization and self-adaptive global best harmony searchto solve this mixed-integer optimization problem. Extensive experimentscomparing SocFedCS against five state-of-the-art approaches based on fourreal-world multimedia datasets demonstrate that it achieves 2.06\% higher testaccuracy and 12.24\% lower cost on average than the best-performing baseline.</description><author>Yulan Gao, Yansong Zhao, Han Yu</author><pubDate>Thu, 11 May 2023 16:06:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06865v1</guid></item><item><title>A General Framework for Visualizing Embedding Spaces of Neural Survival Analysis Models Based on Angular Information</title><link>http://arxiv.org/abs/2305.06862v1</link><description>We propose a general framework for visualizing any intermediate embeddingrepresentation used by any neural survival analysis model. Our framework isbased on so-called anchor directions in an embedding space. We show how toestimate these anchor directions using clustering or, alternatively, usinguser-supplied "concepts" defined by collections of raw inputs (e.g., featurevectors all from female patients could encode the concept "female"). Fortabular data, we present visualization strategies that reveal how anchordirections relate to raw clinical features and to survival time distributions.We then show how these visualization ideas extend to handling raw inputs thatare images. Our framework is built on looking at angles between vectors in anembedding space, where there could be "information loss" by ignoring magnitudeinformation. We show how this loss results in a "clumping" artifact thatappears in our visualizations, and how to reduce this information loss inpractice.</description><author>George H. Chen</author><pubDate>Thu, 11 May 2023 16:01:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06862v1</guid></item><item><title>Categorical Vector Space Semantics for Lambek Calculus with a Relevant Modality</title><link>http://arxiv.org/abs/2005.03074v4</link><description>We develop a categorical compositional distributional semantics for LambekCalculus with a Relevant Modality !L*, which has a limited edition of thecontraction and permutation rules. The categorical part of the semantics is amonoidal biclosed category with a coalgebra modality, very similar to thestructure of a Differential Category. We instantiate this category to finitedimensional vector spaces and linear maps via "quantisation" functors and workwith three concrete interpretations of the coalgebra modality. We apply themodel to construct categorical and concrete semantic interpretations for themotivating example of !L*: the derivation of a phrase with a parasitic gap. Theeffectiveness of the concrete interpretations are evaluated via adisambiguation task, on an extension of a sentence disambiguation dataset toparasitic gap phrases, using BERT, Word2Vec, and FastText vectors andRelational tensors.</description><author>Lachlan McPheat, Mehrnoosh Sadrzadeh, Hadi Wazni, Gijs Wijnholds</author><pubDate>Thu, 11 May 2023 15:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2005.03074v4</guid></item><item><title>Enhancing Datalog Reasoning with Hypertree Decompositions</title><link>http://arxiv.org/abs/2305.06854v1</link><description>Datalog reasoning based on the semina\"ive evaluation strategy evaluatesrules using traditional join plans, which often leads to redundancy andinefficiency in practice, especially when the rules are complex. Hypertreedecompositions help identify efficient query plans and reduce similarredundancy in query answering. However, it is unclear how this can be appliedto materialisation and incremental reasoning with recursive Datalog programs.Moreover, hypertree decompositions require additional data structures and thusintroduce nonnegligible overhead in both runtime and memory consumption. Inthis paper, we provide algorithms that exploit hypertree decompositions for thematerialisation and incremental evaluation of Datalog programs. Furthermore, wecombine this approach with standard Datalog reasoning algorithms in a modularfashion so that the overhead caused by the decompositions is reduced. Ourempirical evaluation shows that, when the program contains complex rules, thecombined approach is usually significantly faster than the baseline approach,sometimes by orders of magnitude.</description><author>Xinyue Zhang, Pan Hu, Yavor Nenov, Ian Horrocks</author><pubDate>Thu, 11 May 2023 15:51:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06854v1</guid></item><item><title>Untargeted Near-collision Attacks in Biometric Recognition</title><link>http://arxiv.org/abs/2304.01580v2</link><description>A biometric recognition system can operate in two distinct modes,identification or verification. In the first mode, the system recognizes anindividual by searching the enrolled templates of all the users for a match. Inthe second mode, the system validates a user's identity claim by comparing thefresh provided template with the enrolled template. The biometrictransformation schemes usually produce binary templates that are better handledby cryptographic schemes, and the comparison is based on a distance that leaksinformation about the similarities between two biometric templates. Both theexperimentally determined false match rate and false non-match rate throughrecognition threshold adjustment define the recognition accuracy, and hence thesecurity of the system. To the best of our knowledge, few works provide aformal treatment of the security under minimum leakage of information, i.e.,the binary outcome of a comparison with a threshold. In this paper, we rely onprobabilistic modelling to quantify the security strength of binary templates.We investigate the influence of template size, database size and threshold onthe probability of having a near-collision. We highlight several untargetedattacks on biometric systems considering naive and adaptive adversaries.Interestingly, these attacks can be launched both online and offline and, bothin the identification mode and in the verification mode. We discuss the choiceof parameters through the generic presented attacks.</description><author>Axel Durbet, Paul-Marie Grollemund, Kevin Thiry-Atighehchi</author><pubDate>Thu, 11 May 2023 15:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01580v2</guid></item><item><title>Policy Gradient Algorithms Implicitly Optimize by Continuation</title><link>http://arxiv.org/abs/2305.06851v1</link><description>Direct policy optimization in reinforcement learning is usually solved withpolicy-gradient algorithms, which optimize policy parameters via stochasticgradient ascent. This paper provides a new theoretical interpretation andjustification of these algorithms. First, we formulate direct policyoptimization in the optimization by continuation framework. The latter is aframework for optimizing nonconvex functions where a sequence of surrogateobjective functions, called continuations, are locally optimized. Second, weshow that optimizing affine Gaussian policies and performing entropyregularization can be interpreted as implicitly optimizing deterministicpolicies by continuation. Based on these theoretical results, we argue thatexploration in policy-gradient algorithms consists in computing a continuationof the return of the policy at hand, and that the variance of policies shouldbe history-dependent functions adapted to avoid local extrema rather than tomaximize the return of the policy.</description><author>Adrien Bolland, Gilles Louppe, Damien Ernst</author><pubDate>Thu, 11 May 2023 15:50:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06851v1</guid></item><item><title>WebCPM: Interactive Web Search for Chinese Long-form Question Answering</title><link>http://arxiv.org/abs/2305.06849v1</link><description>Long-form question answering (LFQA) aims at answering complex, open-endedquestions with detailed, paragraph-length responses. The de facto paradigm ofLFQA necessitates two procedures: information retrieval, which searches forrelevant supporting facts, and information synthesis, which integrates thesefacts into a coherent answer. In this paper, we introduce WebCPM, the firstChinese LFQA dataset. One unique feature of WebCPM is that its informationretrieval is based on interactive web search, which engages with a searchengine in real time. Following WebGPT, we develop a web search interface. Werecruit annotators to search for relevant information using our interface andthen answer questions. Meanwhile, the web search behaviors of our annotatorswould be recorded. In total, we collect 5,500 high-quality question-answerpairs, together with 14,315 supporting facts and 121,330 web search actions. Wefine-tune pre-trained language models to imitate human behaviors for web searchand to generate answers based on the collected facts. Our LFQA pipeline, builton these fine-tuned models, generates answers that are no worse thanhuman-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader,respectively.</description><author>Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, Jie Zhou</author><pubDate>Thu, 11 May 2023 15:47:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06849v1</guid></item><item><title>High-Dimensional Smoothed Entropy Estimation via Dimensionality Reduction</title><link>http://arxiv.org/abs/2305.04712v2</link><description>We study the problem of overcoming exponential sample complexity indifferential entropy estimation under Gaussian convolutions. Specifically, weconsider the estimation of the differential entropy $h(X+Z)$ via $n$independently and identically distributed samples of $X$, where $X$ and $Z$ areindependent $D$-dimensional random variables with $X$ sub-Gaussian with boundedsecond moment and $Z\sim\mathcal{N}(0,\sigma^2I_D)$. Under the absolute-errorloss, the above problem has a parametric estimation rate of$\frac{c^D}{\sqrt{n}}$, which is exponential in data dimension $D$ and oftenproblematic for applications. We overcome this exponential sample complexity byprojecting $X$ to a low-dimensional space via principal component analysis(PCA) before the entropy estimation, and show that the asymptotic erroroverhead vanishes as the unexplained variance of the PCA vanishes. This impliesnear-optimal performance for inherently low-dimensional structures embedded inhigh-dimensional spaces, including hidden-layer outputs of deep neural networks(DNN), which can be used to estimate mutual information (MI) in DNNs. Weprovide numerical results verifying the performance of our PCA approach onGaussian and spiral data. We also apply our method to analysis of informationflow through neural network layers (c.f. information bottleneck), with resultsmeasuring mutual information in a noisy fully connected network and a noisyconvolutional neural network (CNN) for MNIST classification.</description><author>Kristjan Greenewald, Brian Kingsbury, Yuancheng Yu</author><pubDate>Thu, 11 May 2023 15:47:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04712v2</guid></item><item><title>Detection and Classification of Pole-like Landmarks for Domain-invariant 3D Point Cloud Map Matching</title><link>http://arxiv.org/abs/2305.06845v1</link><description>In 3D point cloud-based visual self-localization, pole landmarks have a greatpotential as landmarks for accurate and reliable localization due to theirlong-term stability under seasonal and weather changes. In this study, we aimto explore the use of recently developed deep learning models for poleclassification in the context of pole landmark-based self-localization.Specifically, the proposed scheme consists of two main modules: pole mapmatching and pole class matching. In the former module, local pole map isconstructed and its configuration is compared against a precomputed global polemap. An efficient RANSAC map matching is employed to achieve a good tradeoffbetween computational efficiency and accuracy. In the latter pole classmatching module, the local and global poles paired by the RANSAC map-matchingare further compared by means of pole attribute class. To this end, apredefined set of pseudo pole classes is learned via k-means clustering in aself-supervised manner. Experiments using publicly available NCLT datasetshowed that the pole-like landmark classification method has an improved effecton the visual self-localization system compared with the baseline method.</description><author>Sun Yifei, Li Dingrui, Ye Minying, Tanaka Kanji</author><pubDate>Thu, 11 May 2023 15:40:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06845v1</guid></item><item><title>Emotion Recognition for Challenged People Facial Appearance in Social using Neural Network</title><link>http://arxiv.org/abs/2305.06842v1</link><description>Human communication is the vocal and non verbal signal to communicate withothers. Human expression is a significant biometric object in picture andrecord databases of surveillance systems. Face appreciation has a serious rolein biometric methods and is good-looking for plentiful applications, includingvisual scrutiny and security. Facial expressions are a form of nonverbalcommunication; recognizing them helps improve the human machine interaction.This paper proposes an idea for face and enlightenment invariant credit offacial expressions by the images. In order on, the person's face can becomputed. Face expression is used in CNN classifier to categorize the acquiredpicture into different emotion categories. It is a deep, feed-forwardartificial neural network. Outcome surpasses human presentation and shows posesalternate performance. Varying lighting conditions can influence the fittingprocess and reduce recognition precision. Results illustrate that dependablefacial appearance credited with changing lighting conditions for separatingreasonable facial terminology display emotions is an efficient representationof clean and assorted moving expressions. This process can also manage theproportions of dissimilar basic affecting expressions of those mixed jointly toproduce sensible emotional facial expressions. Our system contains apre-defined data set, which was residential by a statistics scientist andincludes all pure and varied expressions. On average, a data set has achieved92.4% exact validation of the expressions synthesized by our technique. Thesefacial expressions are compared through the pre-defined data-position insideour system. If it recognizes the person in an abnormal condition, an alert willbe passed to the nearby hospital/doctor seeing that a message.</description><author>P. Deivendran, P. Suresh Babu, G. Malathi, K. Anbazhagan, R. Senthil Kumar</author><pubDate>Thu, 11 May 2023 15:38:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06842v1</guid></item><item><title>Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models</title><link>http://arxiv.org/abs/2305.06841v1</link><description>While the Large Language Models (LLMs) dominate a majority of languageunderstanding tasks, previous work shows that some of these results aresupported by modelling spurious correlations of training datasets. Authorscommonly assess model robustness by evaluating their models onout-of-distribution (OOD) datasets of the same task, but these datasets mightshare the bias of the training dataset. We propose a simple method for measuring a scale of models' reliance on anyidentified spurious feature and assess the robustness towards a large set ofknown and newly found prediction biases for various pre-trained models anddebiasing methods in Question Answering (QA). We find that the reported OODgains of debiasing methods can not be explained by mitigated reliance on biasedfeatures, suggesting that biases are shared among QA datasets. We furtherevidence this by measuring that performance of OOD models depends on biasfeatures comparably to the ID model, motivating future work to refine thereports of LLMs' robustness to a level of known spurious features.</description><author>Lukáš Mikula, Michal Štefánik, Marek Petrovič, Petr Sojka</author><pubDate>Thu, 11 May 2023 15:35:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06841v1</guid></item><item><title>Gemino: Practical and Robust Neural Compression for Video Conferencing</title><link>http://arxiv.org/abs/2209.10507v3</link><description>Video conferencing systems suffer from poor user experience when networkconditions deteriorate because current video codecs simply cannot operate atextremely low bitrates. Recently, several neural alternatives have beenproposed that reconstruct talking head videos at very low bitrates using sparserepresentations of each frame such as facial landmark information. However,these approaches produce poor reconstructions in scenarios with major movementor occlusions over the course of a call, and do not scale to higherresolutions. We design Gemino, a new neural compression system for videoconferencing based on a novel high-frequency-conditional super-resolutionpipeline. Gemino upsamples a very low-resolution version of each target framewhile enhancing high-frequency details (e.g., skin texture, hair, etc.) basedon information extracted from a single high-resolution reference image. We usea multi-scale architecture that runs different components of the model atdifferent resolutions, allowing it to scale to resolutions comparable to 720p,and we personalize the model to learn specific details of each person,achieving much better fidelity at low bitrates. We implement Gemino atopaiortc, an open-source Python implementation of WebRTC, and show that itoperates on 1024x1024 videos in real-time on a Titan X GPU, and achieves 2.2-5xlower bitrate than traditional video codecs for the same perceptual quality.</description><author>Vibhaalakshmi Sivaraman, Pantea Karimi, Vedantha Venkatapathy, Mehrdad Khani, Sadjad Fouladi, Mohammad Alizadeh, Frédo Durand, Vivienne Sze</author><pubDate>Thu, 11 May 2023 15:24:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.10507v3</guid></item><item><title>FlowMap: Path Generation for Automated Vehicles in Open Space Using Traffic Flow</title><link>http://arxiv.org/abs/2305.01622v2</link><description>There is extensive literature on perceiving road structures by fusing varioussensor inputs such as lidar point clouds and camera images using deep neuralnets. Leveraging the latest advance of neural architects (such as transformers)and bird-eye-view (BEV) representation, the road cognition accuracy keepsimproving. However, how to cognize the ``road'' for automated vehicles wherethere is no well-defined ``roads'' remains an open problem. For example, how tofind paths inside intersections without HD maps is hard since there is neitheran explicit definition for ``roads'' nor explicit features such as lanemarkings. The idea of this paper comes from a proverb: it becomes a way whenpeople walk on it. Although there are no ``roads'' from sensor readings, thereare ``roads'' from tracks of other vehicles. In this paper, we propose FlowMap,a path generation framework for automated vehicles based on traffic flows.FlowMap is built by extending our previous work RoadMap, a light-weightsemantic map, with an additional traffic flow layer. A path generationalgorithm on traffic flow fields (TFFs) is proposed to generate human-likepaths. The proposed framework is validated using real-world driving data and isamenable to generating paths for super complicated intersections without usingHD maps.</description><author>Wenchao Ding, Jieru Zhao, Yubin Chu, Haihui Huang, Tong Qin, Chunjing Xu, Yuxiang Guan, Zhongxue Gan</author><pubDate>Thu, 11 May 2023 15:21:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01622v2</guid></item><item><title>A Generic Approach to Integrating Time into Spatial-Temporal Forecasting via Conditional Neural Fields</title><link>http://arxiv.org/abs/2305.06827v1</link><description>Self-awareness is the key capability of autonomous systems, e.g., autonomousdriving network, which relies on highly efficient time series forecastingalgorithm to enable the system to reason about the future state of theenvironment, as well as its effect on the system behavior as time progresses.Recently, a large number of forecasting algorithms using either convolutionalneural networks or graph neural networks have been developed to exploit thecomplex temporal and spatial dependencies present in the time series. Whilethese solutions have shown significant advantages over statistical approaches,one open question is to effectively incorporate the global information whichrepresents the seasonality patterns via the time component of time series intothe forecasting models to improve their accuracy. This paper presents a generalapproach to integrating the time component into forecasting models. The mainidea is to employ conditional neural fields to represent the auxiliary featuresextracted from the time component to obtain the global information, which willbe effectively combined with the local information extracted fromautoregressive neural networks through a layer-wise gated fusion module.Extensive experiments on road traffic and cellular network traffic datasetsprove the effectiveness of the proposed approach.</description><author>Minh-Thanh Bui, Duc-Thinh Ngo, Zonghua Zhang</author><pubDate>Thu, 11 May 2023 15:20:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06827v1</guid></item><item><title>Implicit Neural Networks with Fourier-Feature Inputs for Free-breathing Cardiac MRI Reconstruction</title><link>http://arxiv.org/abs/2305.06822v1</link><description>In this paper, we propose an approach for cardiac magnetic resonance imaging(MRI), which aims to reconstruct a real-time video of a beating heart fromcontinuous highly under-sampled measurements. This task is challenging sincethe object to be reconstructed (the heart) is continuously changing duringsignal acquisition. To address this challenge, we represent the beating heartwith an implicit neural network and fit the network so that the representationof the heart is consistent with the measurements. The network in the form of amulti-layer perceptron with Fourier-feature inputs acts as an effective signalprior and enables adjusting the regularization strength in both the spatial andtemporal dimensions of the signal. We examine the proposed approach for 2Dfree-breathing cardiac real-time MRI in different operating regimes, i.e., fordifferent image resolutions, slice thicknesses, and acquisition lengths. Ourmethod achieves reconstruction quality on par with or slightly better thanstate-of-the-art untrained convolutional neural networks and superior imagequality compared to a recent method that fits an implicit representationdirectly to Fourier-domain measurements. However, this comes at a highercomputational cost. Our approach does not require any additional patient dataor biosensors including electrocardiography, making it potentially applicablein a wide range of clinical scenarios.</description><author>Johannes F. Kunz, Stefan Ruschke, Reinhard Heckel</author><pubDate>Thu, 11 May 2023 15:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06822v1</guid></item><item><title>DeepSTEP -- Deep Learning-Based Spatio-Temporal End-To-End Perception for Autonomous Vehicles</title><link>http://arxiv.org/abs/2305.06820v1</link><description>Autonomous vehicles demand high accuracy and robustness of perceptionalgorithms. To develop efficient and scalable perception algorithms, themaximum information should be extracted from the available sensor data. In thiswork, we present our concept for an end-to-end perception architecture, namedDeepSTEP. The deep learning-based architecture processes raw sensor data fromthe camera, LiDAR, and RaDAR, and combines the extracted data in a deep fusionnetwork. The output of this deep fusion network is a shared feature space,which is used by perception head networks to fulfill several perception tasks,such as object detection or local mapping. DeepSTEP incorporates multiple ideasto advance state of the art: First, combining detection and localization into asingle pipeline allows for efficient processing to reduce computationaloverhead and further improves overall performance. Second, the architectureleverages the temporal domain by using a self-attention mechanism that focuseson the most important features. We believe that our concept of DeepSTEP willadvance the development of end-to-end perception systems. The network will bedeployed on our research vehicle, which will be used as a platform for datacollection, real-world testing, and validation. In conclusion, DeepSTEPrepresents a significant advancement in the field of perception for autonomousvehicles. The architecture's end-to-end design, time-aware attention mechanism,and integration of multiple perception tasks make it a promising solution forreal-world deployment. This research is a work in progress and presents thefirst concept of establishing a novel perception pipeline.</description><author>Sebastian Huch, Florian Sauerbeck, Johannes Betz</author><pubDate>Thu, 11 May 2023 15:13:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06820v1</guid></item><item><title>Schelling Games with Continuous Types</title><link>http://arxiv.org/abs/2305.06819v1</link><description>In most major cities and urban areas, residents form homogeneousneighborhoods along ethnic or socioeconomic lines. This phenomenon is widelyknown as residential segregation and has been studied extensively. Fifty yearsago, Schelling proposed a landmark model that explains residential segregationin an elegant agent-based way. A recent stream of papers analyzed Schelling'smodel using game-theoretic approaches. However, all these works consideredmodels with a given number of discrete types modeling different ethnic groups. We focus on segregation caused by non-categorical attributes, such ashousehold income or position in a political left-right spectrum. For this, weconsider agent types that can be represented as real numbers. This opens up agreat variety of reasonable models and, as a proof of concept, we focus onseveral natural candidates. In particular, we consider agents that evaluatetheir location by the average type-difference or the maximum type-difference totheir neighbors, or by having a certain tolerance range for type-values ofneighboring agents. We study the existence and computation of equilibria andprovide bounds on the Price of Anarchy and Stability. Also, we presentsimulation results that compare our models and shed light on the obtainedequilibria for our variants.</description><author>Davide Bilò, Vittorio Bilò, Michelle Döring, Pascal Lenzner, Louise Molitor, Jonas Schmidt</author><pubDate>Thu, 11 May 2023 15:13:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06819v1</guid></item><item><title>Towards a Computational Analysis of Suspense: Detecting Dangerous Situations</title><link>http://arxiv.org/abs/2305.06818v1</link><description>Suspense is an important tool in storytelling to keep readers engaged andwanting to read more. However, it has so far not been studied extensively inComputational Literary Studies. In this paper, we focus on one of the elementsauthors can use to build up suspense: dangerous situations. We introduce acorpus of texts annotated with dangerous situations, distinguishing between 7types of danger. Additionally, we annotate parts of the text that describe fearexperienced by a character, regardless of the actual presence of danger. Wepresent experiments towards the automatic detection of these situations,finding that unsupervised baseline methods can provide valuable signals for thedetection, but more complex methods are necessary for further analysis. Notunexpectedly, the description of danger and fear often relies heavily on thecontext, both local (e.g., situations where danger is only mentioned, but notactually present) and global (e.g., "storm" being used in a literal sense in anadventure novel, but metaphorically in a romance novel).</description><author>Albin Zehe, Julian Schröter, Andreas Hotho</author><pubDate>Thu, 11 May 2023 15:12:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06818v1</guid></item><item><title>THUIR@COLIEE 2023: More Parameters and Legal Knowledge for Legal Case Entailment</title><link>http://arxiv.org/abs/2305.06817v1</link><description>This paper describes the approach of the THUIR team at the COLIEE 2023 LegalCase Entailment task. This task requires the participant to identify a specificparagraph from a given supporting case that entails the decision for the querycase. We try traditional lexical matching methods and pre-trained languagemodels with different sizes. Furthermore, learning-to-rank methods are employedto further improve performance. However, learning-to-rank is not very robust onthis task. which suggests that answer passages cannot simply be determined withinformation retrieval techniques. Experimental results show that moreparameters and legal knowledge contribute to the legal case entailment task.Finally, we get the third place in COLIEE 2023. The implementation of ourmethod can be found at https://github.com/CSHaitao/THUIR-COLIEE2023.</description><author>Haitao Li, Changyue Wang, Weihang Su, Yueyue Wu, Qingyao Ai, Yiqun Liu</author><pubDate>Thu, 11 May 2023 15:11:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06817v1</guid></item><item><title>Generation of Structurally Realistic Retinal Fundus Images with Diffusion Models</title><link>http://arxiv.org/abs/2305.06813v1</link><description>We introduce a new technique for generating retinal fundus images that haveanatomically accurate vascular structures, using diffusion models. We generateartery/vein masks to create the vascular structure, which we then condition toproduce retinal fundus images. The proposed method can generate high-qualityimages with more realistic vascular structures and can create a diverse rangeof images based on the strengths of the diffusion model. We presentquantitative evaluations that demonstrate the performance improvement using ourmethod for data augmentation on vessel segmentation and artery/veinclassification. We also present Turing test results by clinical experts,showing that our generated images are difficult to distinguish with realimages. We believe that our method can be applied to construct stand-alonedatasets that are irrelevant of patient privacy.</description><author>Sojung Go, Younghoon Ji, Sang Jun Park, Soochahn Lee</author><pubDate>Thu, 11 May 2023 15:09:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06813v1</guid></item><item><title>THUIR@COLIEE 2023: Incorporating Structural Knowledge into Pre-trained Language Models for Legal Case Retrieval</title><link>http://arxiv.org/abs/2305.06812v1</link><description>Legal case retrieval techniques play an essential role in modern intelligentlegal systems. As an annually well-known international competition, COLIEE isaiming to achieve the state-of-the-art retrieval model for legal texts. Thispaper summarizes the approach of the championship team THUIR in COLIEE 2023. Tobe specific, we design structure-aware pre-trained language models to enhancethe understanding of legal cases. Furthermore, we propose heuristicpre-processing and post-processing approaches to reduce the influence ofirrelevant messages. In the end, learning-to-rank methods are employed to mergefeatures with different dimensions. Experimental results demonstrate thesuperiority of our proposal. Official results show that our run has the bestperformance among all submissions. The implementation of our method can befound at https://github.com/CSHaitao/THUIR-COLIEE2023.</description><author>Haitao Li, Weihang Su, Changyue Wang, Yueyue Wu, Qingyao Ai, Yiqun Liu</author><pubDate>Thu, 11 May 2023 15:08:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06812v1</guid></item><item><title>Collection Space Navigator: An Interactive Visualization Interface for Multidimensional Datasets</title><link>http://arxiv.org/abs/2305.06809v1</link><description>We introduce the Collection Space Navigator (CSN), a browser-basedvisualization tool to explore, research, and curate large collections of visualdigital artifacts that are associated with multidimensional data, such asvector embeddings or tables of metadata. Media objects such as images are oftenencoded as numerical vectors, for e.g. based on metadata or using machinelearning to embed image information. Yet, while such procedures are widespreadfor a range of applications, it remains a challenge to explore, analyze, andunderstand the resulting multidimensional spaces in a more comprehensivemanner. Dimensionality reduction techniques such as t-SNE or UMAP often serveto project high-dimensional data into low dimensional visualizations, yetrequire interpretation themselves as the remaining dimensions are typicallyabstract. Here, the Collection Space Navigator provides a customizableinterface that combines two-dimensional projections with a set of configurablemultidimensional filters. As a result, the user is able to view and investigatecollections, by zooming and scaling, by transforming between projections, byfiltering dimensions via range sliders, and advanced text filters. Insightsthat are gained during the interaction can be fed back into the original datavia ad hoc exports of filtered metadata and projections. This paper comes witha functional showcase demo using a large digitized collection of classicalWestern art. The Collection Space Navigator is open source. Users canreconfigure the interface to fit their own data and research needs, includingprojections and filter controls. The CSN is ready to serve a broad community.</description><author>Tillmann Ohm, Mar Canet Solà, Andres Karjus, Maximilian Schich</author><pubDate>Thu, 11 May 2023 15:03:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06809v1</guid></item><item><title>MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos</title><link>http://arxiv.org/abs/2304.05292v2</link><description>Deep machine learning models including Convolutional Neural Networks (CNN)have been successful in the detection of Mild Cognitive Impairment (MCI) usingmedical images, questionnaires, and videos. This paper proposes a novelMulti-branch Classifier-Video Vision Transformer (MC-ViViT) model todistinguish MCI from those with normal cognition by analyzing facial features.The data comes from the I-CONECT, a behavioral intervention trial aimed atimproving cognitive function by providing frequent video chats. MC-ViViTextracts spatiotemporal features of videos in one branch and augmentsrepresentations by the MC module. The I-CONECT dataset is challenging as thedataset is imbalanced containing Hard-Easy and Positive-Negative samples, whichimpedes the performance of MC-ViViT. We propose a loss function for Hard-Easyand Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORREloss to address the imbalanced problem. Our experimental results on theI-CONECT dataset show the great potential of MC-ViViT in predicting MCI with ahigh accuracy of 90.63\% accuracy on some of the interview videos.</description><author>Jian Sun, Hiroko H. Dodge, Mohammad H. Mahoor</author><pubDate>Thu, 11 May 2023 15:03:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05292v2</guid></item><item><title>Are Character-level Translations Worth the Wait? Comparing Character- and Subword-level Models for Machine Translation</title><link>http://arxiv.org/abs/2302.14220v2</link><description>Pretrained character-level language models were recently shown to becompetitive with popular subword models across a range of NLP tasks. However,there has been little research on their effectiveness for neural machinetranslation (NMT). This work performs an extensive comparison across multiplelanguages and experimental conditions of state-of-the-art character- andsubword-level pre-trained models (ByT5 and mT5, respectively) on NMT, showingthe effectiveness of character-level modeling in translation, particularly incases where training data is limited. In our analysis, we show how charactermodels' performance gains are reflected in better translations oforthographically similar words and rare words. While evaluating the importanceof source texts in driving model predictions, we highlight ByT5 word-levelpatterns suggesting an ability to modulate word and character-level informationduring the translation, providing insights into a potential weakness ofcharacter-level modeling. We conclude by assessing the efficiency tradeoff ofcharacter models, suggesting their usage in non-time-critical scenarios toboost translation quality.</description><author>Lukas Edman, Gabriele Sarti, Antonio Toral, Gertjan van Noord, Arianna Bisazza</author><pubDate>Thu, 11 May 2023 15:00:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14220v2</guid></item><item><title>Initial Steps Towards Tackling High-dimensional Surrogate Modeling for Neuroevolution Using Kriging Partial Least Squares</title><link>http://arxiv.org/abs/2305.03612v3</link><description>Surrogate-assisted evolutionary algorithms (SAEAs) aim to use efficientcomputational models with the goal of approximating the fitness function inevolutionary computation systems. This area of research has been active forover two decades and has received significant attention from the specialisedresearch community in different areas, for example, single and many objectiveoptimisation or dynamic and stationary optimisation problems. An emergent andexciting area that has received little attention from the SAEAs community is inneuroevolution. This refers to the use of evolutionary algorithms in theautomatic configuration of artificial neural network (ANN) architectures,hyper-parameters and/or the training of ANNs. However, ANNs suffer from twomajor issues: (a) the use of highly-intense computational power for theircorrect training, and (b) the highly specialised human expertise required tocorrectly configure ANNs necessary to get a well-performing network. This workaims to fill this important research gap in SAEAs in neuroevolution byaddressing these two issues. We demonstrate how one can use a Kriging PartialLeast Squares method that allows efficient computation of good approximatesurrogate models compared to the well-known Kriging method, which normallycannot be used in neuroevolution due to the high dimensionality of the data.</description><author>Fergal Stapleton, Edgar Galván</author><pubDate>Thu, 11 May 2023 14:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03612v3</guid></item><item><title>Knowledge Transfer For On-Device Speech Emotion Recognition with Neural Structured Learning</title><link>http://arxiv.org/abs/2210.14977v3</link><description>Speech emotion recognition (SER) has been a popular research topic inhuman-computer interaction (HCI). As edge devices are rapidly springing up,applying SER to edge devices is promising for a huge number of HCIapplications. Although deep learning has been investigated to improve theperformance of SER by training complex models, the memory space andcomputational capability of edge devices represents a constraint for embeddingdeep learning models. We propose a neural structured learning (NSL) frameworkthrough building synthesized graphs. An SER model is trained on a sourcedataset and used to build graphs on a target dataset. A relatively lightweightmodel is then trained with the speech samples and graphs together as the input.Our experiments demonstrate that training a lightweight SER model on the targetdataset with speech samples and graphs can not only produce small SER models,but also enhance the model performance compared to models with speech samplesonly and those using classic transfer learning strategies.</description><author>Yi Chang, Zhao Ren, Thanh Tam Nguyen, Kun Qian, Björn W. Schuller</author><pubDate>Thu, 11 May 2023 14:54:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.14977v3</guid></item><item><title>Physics-Informed Neural Networks for Discovering Localised Eigenstates in Disordered Media</title><link>http://arxiv.org/abs/2305.06802v1</link><description>The Schr\"{o}dinger equation with random potentials is a fundamental modelfor understanding the behaviour of particles in disordered systems. Disorderedmedia are characterised by complex potentials that lead to the localisation ofwavefunctions, also called Anderson localisation. These wavefunctions may havesimilar scales of eigenenergies which poses difficulty in their discovery. Ithas been a longstanding challenge due to the high computational cost andcomplexity of solving the Schr\"{o}dinger equation. Recently, machine-learningtools have been adopted to tackle these challenges. In this paper, based uponrecent advances in machine learning, we present a novel approach fordiscovering localised eigenstates in disordered media using physics-informedneural networks (PINNs). We focus on the spectral approximation of Hamiltoniansin one dimension with potentials that are randomly generated according to theBernoulli, normal, and uniform distributions. We introduce a novel feature tothe loss function that exploits known physical phenomena occurring in theseregions to scan across the domain and successfully discover these eigenstates,regardless of the similarity of their eigenenergies. We present variousexamples to demonstrate the performance of the proposed approach and compare itwith isogeometric analysis.</description><author>Liam Harcombe, Quanling Deng</author><pubDate>Thu, 11 May 2023 14:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06802v1</guid></item><item><title>Detecting Idiomatic Multiword Expressions in Clinical Terminology using Definition-Based Representation Learning</title><link>http://arxiv.org/abs/2305.06801v1</link><description>This paper shines a light on the potential of definition-based semanticmodels for detecting idiomatic and semi-idiomatic multiword expressions (MWEs)in clinical terminology. Our study focuses on biomedical entities defined inthe UMLS ontology and aims to help prioritize the translation efforts of theseentities. In particular, we develop an effective tool for scoring theidiomaticity of biomedical MWEs based on the degree of similarity between thesemantic representations of those MWEs and a weighted average of therepresentation of their constituents. We achieve this using a biomedicallanguage model trained to produce similar representations for entity names andtheir definitions, called BioLORD. The importance of this definition-basedapproach is highlighted by comparing the BioLORD model to two otherstate-of-the-art biomedical language models based on Transformer: SapBERT andCODER. Our results show that the BioLORD model has a strong ability to identifyidiomatic MWEs, not replicated in other models. Our corpus-free idiomaticityestimation helps ontology translators to focus on more challenging MWEs.</description><author>François Remy, Alfiya Khabibullina, Thomas Demeester</author><pubDate>Thu, 11 May 2023 14:42:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06801v1</guid></item><item><title>Multi-Task Learning based Video Anomaly Detection with Attention</title><link>http://arxiv.org/abs/2210.07697v2</link><description>Multi-task learning based video anomaly detection methods combine multipleproxy tasks in different branches to detect video anomalies in differentsituations. Most existing methods either do not combine complementary tasks toeffectively cover all motion patterns, or the class of the objects is notexplicitly considered. To address the aforementioned shortcomings, we propose anovel multi-task learning based method that combines complementary proxy tasksto better consider the motion and appearance features. We combine the semanticsegmentation and future frame prediction tasks in a single branch to learn theobject class and consistent motion patterns, and to detect respective anomaliessimultaneously. In the second branch, we added several attention mechanisms todetect motion anomalies with attention to object parts, the direction ofmotion, and the distance of the objects from the camera. Our qualitativeresults show that the proposed method considers the object class effectivelyand learns motion with attention to the aforementioned important factors whichresults in a precise motion modeling and a better motion anomaly detection.Additionally, quantitative results show the superiority of our method comparedwith state-of-the-art methods.</description><author>Mohammad Baradaran, Robert Bergevin</author><pubDate>Thu, 11 May 2023 14:41:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.07697v2</guid></item><item><title>GCFAgg: Global and Cross-view Feature Aggregation for Multi-view Clustering</title><link>http://arxiv.org/abs/2305.06799v1</link><description>Multi-view clustering can partition data samples into their categories bylearning a consensus representation in unsupervised way and has received moreand more attention in recent years. However, most existing deep clusteringmethods learn consensus representation or view-specific representations frommultiple views via view-wise aggregation way, where they ignore structurerelationship of all samples. In this paper, we propose a novel multi-viewclustering network to address these problems, called Global and Cross-viewFeature Aggregation for Multi-View Clustering (GCFAggMVC). Specifically, theconsensus data presentation from multiple views is obtained via cross-sampleand cross-view feature aggregation, which fully explores the complementaryofsimilar samples. Moreover, we align the consensus representation and theview-specific representation by the structure-guided contrastive learningmodule, which makes the view-specific representations from different sampleswith high structure relationship similar. The proposed module is a flexiblemulti-view data representation module, which can be also embedded to theincomplete multi-view data clustering task via plugging our module into otherframeworks. Extensive experiments show that the proposed method achievesexcellent performance in both complete multi-view data clustering tasks andincomplete multi-view data clustering tasks.</description><author>Weiqing Yan, Yuanyang Zhang, Chenlei Lv, Chang Tang, Guanghui Yue, Liang Liao, Weisi Lin</author><pubDate>Thu, 11 May 2023 14:41:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06799v1</guid></item><item><title>Towards Theoretical Understanding of Data-Driven Policy Refinement</title><link>http://arxiv.org/abs/2305.06796v1</link><description>This paper presents an approach for data-driven policy refinement inreinforcement learning, specifically designed for safety-critical applications.Our methodology leverages the strengths of data-driven optimization andreinforcement learning to enhance policy safety and optimality throughiterative refinement. Our principal contribution lies in the mathematicalformulation of this data-driven policy refinement concept. This frameworksystematically improves reinforcement learning policies by learning fromcounterexamples surfaced during data-driven verification. Furthermore, wepresent a series of theorems elucidating key theoretical properties of ourapproach, including convergence, robustness bounds, generalization error, andresilience to model mismatch. These results not only validate the effectivenessof our methodology but also contribute to a deeper understanding of itsbehavior in different environments and scenarios.</description><author>Ali Baheri</author><pubDate>Thu, 11 May 2023 14:36:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06796v1</guid></item><item><title>Multi-modal Multi-level Fusion for 3D Single Object Tracking</title><link>http://arxiv.org/abs/2305.06794v1</link><description>3D single object tracking plays a crucial role in computer vision. Mainstreammethods mainly rely on point clouds to achieve geometry matching between targettemplate and search area. However, textureless and incomplete point clouds makeit difficult for single-modal trackers to distinguish objects with similarstructures. To overcome the limitations of geometry matching, we propose aMulti-modal Multi-level Fusion Tracker (MMF-Track), which exploits the imagetexture and geometry characteristic of point clouds to track 3D target.Specifically, we first propose a Space Alignment Module (SAM) to align RGBimages with point clouds in 3D space, which is the prerequisite forconstructing inter-modal associations. Then, in feature interaction level, wedesign a Feature Interaction Module (FIM) based on dual-stream structure, whichenhances intra-modal features in parallel and constructs inter-modal semanticassociations. Meanwhile, in order to refine each modal feature, we introduce aCoarse-to-Fine Interaction Module (CFIM) to realize the hierarchical featureinteraction at different scales. Finally, in similarity fusion level, wepropose a Similarity Fusion Module (SFM) to aggregate geometry and textureclues from the target. Experiments show that our method achievesstate-of-the-art performance on KITTI (39% Success and 42% Precision gainsagainst previous multi-modal method) and is also competitive on NuScenes.</description><author>Zhiheng Li, Yubo Cui, Zuoxu Gu, Zheng Fang</author><pubDate>Thu, 11 May 2023 14:34:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06794v1</guid></item><item><title>Towards Understanding Omission in Dialogue Summarization</title><link>http://arxiv.org/abs/2211.07145v2</link><description>Dialogue summarization aims to condense the lengthy dialogue into a concisesummary, and has recently achieved significant progress. However, the result ofexisting methods is still far from satisfactory. Previous works indicated thatomission is a major factor in affecting the quality of summarization, but fewof them have further explored the omission problem, such as how omissionaffects summarization results and how to detect omission, which is critical forreducing omission and improving summarization quality. Moreover, analyzing anddetecting omission relies on summarization datasets with omission labels (i.e.,which dialogue utterances are omitted in the summarization), which are notavailable in the current literature. In this paper, we propose the OLDSdataset, which provides high-quality Omission Labels for DialogueSummarization. By analyzing this dataset, we find that a large improvement insummarization quality can be achieved by providing ground-truth omission labelsfor the summarization model to recover omission information, which demonstratesthe importance of omission detection for omission mitigation in dialoguesummarization. Therefore, we formulate an omission detection task anddemonstrate our proposed dataset can support the training and evaluation ofthis task well. We also call for research action on omission detection based onour proposed datasets. Our dataset and codes are publicly available.</description><author>Yicheng Zou, Kaitao Song, Xu Tan, Zhongkai Fu, Qi Zhang, Dongsheng Li, Tao Gui</author><pubDate>Thu, 11 May 2023 14:26:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07145v2</guid></item><item><title>Integrating nearest neighbors on neural network models for treatment effect estimation</title><link>http://arxiv.org/abs/2305.06789v1</link><description>Treatment effect estimation is of high-importance for both researchers andpractitioners across many scientific and industrial domains. The abundance ofobservational data makes them increasingly used by researchers for theestimation of causal effects. However, these data suffer from biases, fromseveral weaknesses, leading to inaccurate causal effect estimations, if nothandled properly. Therefore, several machine learning techniques have beenproposed, most of them focusing on leveraging the predictive power of neuralnetwork models to attain more precise estimation of causal effects. In thiswork, we propose a new methodology, named Nearest Neighboring Information forCausal Inference (NNCI), for integrating valuable nearest neighboringinformation on neural network-based models for estimating treatment effects.The proposed NNCI methodology is applied to some of the most well establishedneural network-based models for treatment effect estimation with the use ofobservational data. Numerical experiments and analysis provide empirical andstatistical evidence that the integration of NNCI with state-of-the-art neuralnetwork models leads to considerably improved treatment effect estimations on avariety of well-known challenging benchmarks.</description><author>Niki Kiriakidou, Christos Diou</author><pubDate>Thu, 11 May 2023 14:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06789v1</guid></item><item><title>ReMark: Receptive Field based Spatial WaterMark Embedding Optimization using Deep Network</title><link>http://arxiv.org/abs/2305.06786v1</link><description>Watermarking is one of the most important copyright protection tools fordigital media. The most challenging type of watermarking is the imperceptibleone, which embeds identifying information in the data while retaining thelatter's original quality. To fulfill its purpose, watermarks need to withstandvarious distortions whose goal is to damage their integrity. In this study, weinvestigate a novel deep learning-based architecture for embeddingimperceptible watermarks. The key insight guiding our architecture design isthe need to correlate the dimensions of our watermarks with the sizes ofreceptive fields (RF) of modules of our architecture. This adaptation makes ourwatermarks more robust, while also enabling us to generate them in a way thatbetter maintains image quality. Extensive evaluations on a wide variety ofdistortions show that the proposed method is robust against most commondistortions on watermarks including collusive distortion.</description><author>Natan Semyonov, Rami Puzis, Asaf Shabtai, Gilad Katz</author><pubDate>Thu, 11 May 2023 14:21:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06786v1</guid></item><item><title>Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning</title><link>http://arxiv.org/abs/2305.06784v1</link><description>Auction-based Federated Learning (AFL) has attracted extensive researchinterest due to its ability to motivate data owners to join FL through economicmeans. Existing works assume that only one data consumer and multiple dataowners exist in an AFL marketplace (i.e., a monopoly market). Therefore, dataowners bid to join the data consumer for FL. However, this assumption is notrealistic in practical AFL marketplaces in which multiple data consumers cancompete to attract data owners to join their respective FL tasks. In thispaper, we bridge this gap by proposing a first-of-its-kind utility-maximizingbidding strategy for data consumers in federated learning (Fed-Bidder). Itenables multiple FL data consumers to compete for data owners via AFLeffectively and efficiently by providing with utility estimation capabilitieswhich can accommodate diverse forms of winning functions, each reflectingdifferent market dynamics. Extensive experiments based on six commonly adoptedbenchmark datasets show that Fed-Bidder is significantly more advantageouscompared to four state-of-the-art approaches.</description><author>Xiaoli Tang, Han Yu</author><pubDate>Thu, 11 May 2023 14:16:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06784v1</guid></item><item><title>Estimating and Maximizing Mutual Information for Knowledge Distillation</title><link>http://arxiv.org/abs/2110.15946v3</link><description>In this work, we propose Mutual Information Maximization KnowledgeDistillation (MIMKD). Our method uses a contrastive objective to simultaneouslyestimate and maximize a lower bound on the mutual information of local andglobal feature representations between a teacher and a student network. Wedemonstrate through extensive experiments that this can be used to improve theperformance of low capacity models by transferring knowledge from moreperformant but computationally expensive models. This can be used to producebetter models that can be run on devices with low computational resources. Ourmethod is flexible, we can distill knowledge from teachers with arbitrarynetwork architectures to arbitrary student networks. Our empirical results showthat MIMKD outperforms competing approaches across a wide range ofstudent-teacher pairs with different capacities, with different architectures,and when student networks are with extremely low capacity. We are able toobtain 74.55% accuracy on CIFAR100 with a ShufflenetV2 from a baseline accuracyof 69.8% by distilling knowledge from ResNet-50. On Imagenet we improve aResNet-18 network from 68.88% to 70.32% accuracy (1.44%+) using a ResNet-34teacher network.</description><author>Aman Shrivastava, Yanjun Qi, Vicente Ordonez</author><pubDate>Thu, 11 May 2023 14:08:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.15946v3</guid></item><item><title>From Denoising Diffusions to Denoising Markov Models</title><link>http://arxiv.org/abs/2211.03595v2</link><description>Denoising diffusions are state-of-the-art generative models exhibitingremarkable empirical performance. They work by diffusing the data distributioninto a Gaussian distribution and then learning to reverse this noising processto obtain synthetic datapoints. The denoising diffusion relies onapproximations of the logarithmic derivatives of the noised data densitiesusing score matching. Such models can also be used to perform approximateposterior simulation when one can only sample from the prior and likelihood. Wepropose a unifying framework generalising this approach to a wide class ofspaces and leading to an original extension of score matching. We illustratethe resulting models on various applications.</description><author>Joe Benton, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, Arnaud Doucet</author><pubDate>Thu, 11 May 2023 14:03:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03595v2</guid></item><item><title>Generating high-quality 3DMPCs by adaptive data acquisition and NeREF-based reflectance correction to facilitate efficient plant phenotyping</title><link>http://arxiv.org/abs/2305.06777v1</link><description>Non-destructive assessments of plant phenotypic traits using high-qualitythree-dimensional (3D) and multispectral data can deepen breeders'understanding of plant growth and allow them to make informed managerialdecisions. However, subjective viewpoint selection and complex illuminationeffects under natural light conditions decrease the data quality and increasethe difficulty of resolving phenotypic parameters. We proposed methods foradaptive data acquisition and reflectance correction respectively, to generatehigh-quality 3D multispectral point clouds (3DMPCs) of plants. In the firststage, we proposed an efficient next-best-view (NBV) planning method based on anovel UGV platform with a multi-sensor-equipped robotic arm. In the secondstage, we eliminated the illumination effects by using the neural referencefield (NeREF) to predict the digital number (DN) of the reference. We testedthem on 6 perilla and 6 tomato plants, and selected 2 visible leaves and 4regions of interest (ROIs) for each plant to assess the biomass and thechlorophyll content. For NBV planning, the average execution time for singleperilla and tomato plant at a joint speed of 1.55 rad/s was 58.70 s and 53.60 srespectively. The whole-plant data integrity was improved by an average of 27%compared to using fixed viewpoints alone, and the coefficients of determination(R2) for leaf biomass estimation reached 0.99 and 0.92. For reflectancecorrection, the average root mean squared error of the reflectance spectra withhemisphere reference-based correction at different ROIs was 0.08 and 0.07 forperilla and tomato. The R2 of chlorophyll content estimation was 0.91 and 0.93respectively when principal component analysis and Gaussian process regressionwere applied. Our approach is promising for generating high-quality 3DMPCs ofplants under natural light conditions and facilitates accurate plantphenotyping.</description><author>Pengyao Xie, Zhihong Ma, Ruiming Du, Mengqi Lv, Yutao Shen, Xuqi Lu, Jiangpeng Zhu, Haiyan Cen</author><pubDate>Thu, 11 May 2023 13:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06777v1</guid></item><item><title>Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities</title><link>http://arxiv.org/abs/2303.12706v2</link><description>One of the challenges of studying common neurological disorders is diseaseheterogeneity including differences in causes, neuroimaging characteristics,comorbidities, or genetic variation. Normative modelling has become a popularmethod for studying such cohorts where the 'normal' behaviour of aphysiological system is modelled and can be used at subject level to detectdeviations relating to disease pathology. For many heterogeneous diseases, weexpect to observe abnormalities across a range of neuroimaging and biologicalvariables. However, thus far, normative models have largely been developed forstudying a single imaging modality. We aim to develop a multi-modal normativemodelling framework where abnormality is aggregated across variables ofmultiple modalities and is better able to detect deviations than uni-modalbaselines. We propose two multi-modal VAE normative models to detect subjectlevel deviations across T1 and DTI data. Our proposed models were better ableto detect diseased individuals, capture disease severity, and correlate withpatient cognition than baseline approaches. We also propose a multivariatelatent deviation metric, measuring deviations from the joint latent space,which outperformed feature-based metrics.</description><author>Ana Lawry Aguila, James Chapman, Andre Altmann</author><pubDate>Thu, 11 May 2023 13:56:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12706v2</guid></item></channel></rss>