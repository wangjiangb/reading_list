<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 21 Mar 2024 06:00:44 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>On Pretraining Data Diversity for Self-Supervised Learning</title><link>http://arxiv.org/abs/2403.13808v1</link><description>We explore the impact of training with more diverse datasets, characterizedby the number of unique samples, on the performance of self-supervised learning(SSL) under a fixed computational budget. Our findings consistently demonstratethat increasing pretraining data diversity enhances SSL performance, albeitonly when the distribution distance to the downstream data is minimal. Notably,even with an exceptionally large pretraining data diversity achieved throughmethods like web crawling or diffusion-generated data, among other ways, thedistribution shift remains a challenge. Our experiments are comprehensive withseven SSL methods using large-scale datasets such as ImageNet and YFCC100Mamounting to over 200 GPU days. Code and trained models will be available athttps://github.com/hammoudhasan/DiversitySSL .</description><author>Hasan Abed Al Kader Hammoud, Tuhin Das, Fabio Pizzati, Philip Torr, Adel Bibi, Bernard Ghanem</author><pubDate>Wed, 20 Mar 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13808v1</guid></item><item><title>Editing Massive Concepts in Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2403.13807v1</link><description>Text-to-image diffusion models suffer from the risk of generating outdated,copyrighted, incorrect, and biased content. While previous methods havemitigated the issues on a small scale, it is essential to handle themsimultaneously in larger-scale real-world scenarios. We propose a two-stagemethod, Editing Massive Concepts In Diffusion Models (EMCID). The first stageperforms memory optimization for each individual concept with dualself-distillation from text alignment loss and diffusion noise prediction loss.The second stage conducts massive concept editing with multi-layer, closed formmodel editing. We further propose a comprehensive benchmark, named ImageNetConcept Editing Benchmark (ICEB), for evaluating massive concept editing forT2I models with two subtasks, free-form prompts, massive concept categories,and extensive evaluation metrics. Extensive experiments conducted on ourproposed benchmark and previous benchmarks demonstrate the superior scalabilityof EMCID for editing up to 1,000 concepts, providing a practical approach forfast adjustment and re-deployment of T2I diffusion models in real-worldapplications.</description><author>Tianwei Xiong, Yue Wu, Enze Xie, Yue Wu, Zhenguo Li, Xihui Liu</author><pubDate>Wed, 20 Mar 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13807v1</guid></item><item><title>RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS</title><link>http://arxiv.org/abs/2403.13806v1</link><description>Recent advances in view synthesis and real-time rendering have achievedphotorealistic quality at impressive rendering speeds. While RadianceField-based methods achieve state-of-the-art quality in challenging scenariossuch as in-the-wild captures and large-scale scenes, they often suffer fromexcessively high compute requirements linked to volumetric rendering. GaussianSplatting-based methods, on the other hand, rely on rasterization and naturallyachieve real-time rendering but suffer from brittle optimization heuristicsthat underperform on more challenging scenes. In this work, we presentRadSplat, a lightweight method for robust real-time rendering of complexscenes. Our main contributions are threefold. First, we use radiance fields asa prior and supervision signal for optimizing point-based scenerepresentations, leading to improved quality and more robust optimization.Next, we develop a novel pruning technique reducing the overall point countwhile maintaining high quality, leading to smaller and more compact scenerepresentations with faster inference speeds. Finally, we propose a noveltest-time filtering approach that further accelerates rendering and allows toscale to larger, house-sized scenes. We find that our method enablesstate-of-the-art synthesis of complex captures at 900+ FPS.</description><author>Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, Federico Tombari</author><pubDate>Wed, 20 Mar 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13806v1</guid></item><item><title>RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition</title><link>http://arxiv.org/abs/2403.13805v1</link><description>CLIP (Contrastive Language-Image Pre-training) uses contrastive learning fromnoise image-text pairs to excel at recognizing a wide array of candidates, yetits focus on broad associations hinders the precision in distinguishing subtledifferences among fine-grained items. Conversely, Multimodal Large LanguageModels (MLLMs) excel at classifying fine-grained categories, thanks to theirsubstantial knowledge from pre-training on web-level corpora. However, theperformance of MLLMs declines with an increase in category numbers, primarilydue to growing complexity and constraints of limited context window size. Tosynergize the strengths of both approaches and enhance the few-shot/zero-shotrecognition abilities for datasets characterized by extensive and fine-grainedvocabularies, this paper introduces RAR, a Retrieving And Ranking augmentedmethod for MLLMs. We initially establish a multi-modal retriever based on CLIPto create and store explicit memory for different categories beyond theimmediate context window. During inference, RAR retrieves the top-k similarresults from the memory and uses MLLMs to rank and make the final predictions.Our proposed approach not only addresses the inherent limitations infine-grained recognition but also preserves the model's comprehensive knowledgebase, significantly boosting accuracy across a range of vision-languagerecognition tasks. Notably, our approach demonstrates a significant improvementin performance on 5 fine-grained visual recognition benchmarks, 11 few-shotimage recognition datasets, and the 2 object detection datasets under thezero-shot recognition setting.</description><author>Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang</author><pubDate>Wed, 20 Mar 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13805v1</guid></item><item><title>Learning from Models and Data for Visual Grounding</title><link>http://arxiv.org/abs/2403.13804v1</link><description>We introduce SynGround, a novel framework that combines data-driven learningand knowledge transfer from various large-scale pretrained models to enhancethe visual grounding capabilities of a pretrained vision-and-language model.The knowledge transfer from the models initiates the generation of imagedescriptions through an image description generator. These descriptions servedual purposes: they act as prompts for synthesizing images through atext-to-image generator, and as queries for synthesizing text, from whichphrases are extracted using a large language model. Finally, we leverage anopen-vocabulary object detector to generate synthetic bounding boxes for thesynthetic images and texts. We finetune a pretrained vision-and-language modelon this dataset by optimizing a mask-attention consistency objective thataligns region annotations with gradient-based model explanations. The resultingmodel improves the grounding capabilities of an off-the-shelfvision-and-language model. Particularly, SynGround improves the pointing gameaccuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and onRefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to63.67%.</description><author>Ruozhen He, Paola Cascante-Bonilla, Ziyan Yang, Alexander C. Berg, Vicente Ordonez</author><pubDate>Wed, 20 Mar 2024 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13804v1</guid></item><item><title>Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments</title><link>http://arxiv.org/abs/2403.13803v1</link><description>Bounding boxes uniquely characterize object detection, where a good detectorgives accurate bounding boxes of categories of interest. However, in thereal-world where test ground truths are not provided, it is non-trivial to findout whether bounding boxes are accurate, thus preventing us from assessing thedetector generalization ability. In this work, we find under feature mapdropout, good detectors tend to output bounding boxes whose locations do notchange much, while bounding boxes of poor detectors will undergo noticeableposition changes. We compute the box stability score (BoS score) to reflectthis stability. Specifically, given an image, we compute a normal set ofbounding boxes and a second set after feature map dropout. To obtain BoS score,we use bipartite matching to find the corresponding boxes between the two setsand compute the average Intersection over Union (IoU) across the entire testset. We contribute to finding that BoS score has a strong, positive correlationwith detection accuracy measured by mean average precision (mAP) under varioustest environments. This relationship allows us to predict the accuracy ofdetectors on various real-world test sets without accessing test ground truths,verified on canonical detection tasks such as vehicle detection and pedestriandetection. Code and data are available at https://github.com/YangYangGirl/BoS.</description><author>Yang Yang, Wenhai Wang, Zhe Chen, Jifeng Dai, Liang Zheng</author><pubDate>Wed, 20 Mar 2024 18:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13803v1</guid></item><item><title>ZigMa: Zigzag Mamba Diffusion Model</title><link>http://arxiv.org/abs/2403.13802v1</link><description>The diffusion model has long been plagued by scalability and quadraticcomplexity issues, especially within transformer-based structures. In thisstudy, we aim to leverage the long sequence modeling capability of aState-Space Model called Mamba to extend its applicability to visual datageneration. Firstly, we identify a critical oversight in most currentMamba-based vision methods, namely the lack of consideration for spatialcontinuity in the scan scheme of Mamba. Secondly, building upon this insight,we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,which outperforms Mamba-based baselines and demonstrates improved speed andmemory utilization compared to transformer-based baselines. Lastly, weintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigatethe scalability of the model on large-resolution visual datasets, such asFacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO$256\times 256$. Code will be released at https://taohu.me/zigma/</description><author>Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, Bjorn Ommer</author><pubDate>Wed, 20 Mar 2024 18:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13802v1</guid></item><item><title>Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs</title><link>http://arxiv.org/abs/2403.13801v1</link><description>We demonstrate experimental results with LLMs that address robotics actionplanning problems. Recently, LLMs have been applied in robotics actionplanning, particularly using a code generation approach that converts complexhigh-level instructions into mid-level policy codes. In contrast, our approachacquires text descriptions of the task and scene objects, then formulatesaction planning through natural language reasoning, and outputs coordinatelevel control commands, thus reducing the necessity for intermediaterepresentation code as policies. Our approach is evaluated on a multi-modalprompt simulation benchmark, demonstrating that our prompt engineeringexperiments with natural language reasoning significantly enhance success ratescompared to its absence. Furthermore, our approach illustrates the potentialfor natural language descriptions to transfer robotics skills from known tasksto previously unseen tasks.</description><author>Yusuke Mikami, Andrew Melnik, Jun Miura, Ville Hautamäki</author><pubDate>Wed, 20 Mar 2024 18:58:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13801v1</guid></item><item><title>AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes</title><link>http://arxiv.org/abs/2312.06644v2</link><description>Inspired by cognitive theories, we introduce AnyHome, a framework thattranslates any text into well-structured and textured indoor scenes at ahouse-scale. By prompting Large Language Models (LLMs) with designed templates,our approach converts provided textual narratives into amodal structuredrepresentations. These representations guarantee consistent and realisticspatial layouts by directing the synthesis of a geometry mesh within definedconstraints. A Score Distillation Sampling process is then employed to refinethe geometry, followed by an egocentric inpainting process that adds lifeliketextures to it. AnyHome stands out with its editability, customizability,diversity, and realism. The structured representations for scenes allow forextensive editing at varying levels of granularity. Capable of interpretingtexts ranging from simple labels to detailed narratives, AnyHome generatesdetailed geometries and textures that outperform existing methods in bothquantitative and qualitative measures.</description><author>Rao Fu, Zehao Wen, Zichen Liu, Srinath Sridhar</author><pubDate>Wed, 20 Mar 2024 18:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06644v2</guid></item><item><title>TimeRewind: Rewinding Time with Image-and-Events Video Diffusion</title><link>http://arxiv.org/abs/2403.13800v1</link><description>This paper addresses the novel challenge of ``rewinding'' time from a singlecaptured image to recover the fleeting moments missed just before the shutterbutton is pressed. This problem poses a significant challenge in computervision and computational photography, as it requires predicting plausiblepre-capture motion from a single static frame, an inherently ill-posed task dueto the high degree of freedom in potential pixel movements. We overcome thischallenge by leveraging the emerging technology of neuromorphic event cameras,which capture motion information with high temporal resolution, and integratingthis data with advanced image-to-video diffusion models. Our proposed frameworkintroduces an event motion adaptor conditioned on event camera data, guidingthe diffusion model to generate videos that are visually coherent andphysically grounded in the captured events. Through extensive experimentation,we demonstrate the capability of our approach to synthesize high-quality videosthat effectively ``rewind'' time, showcasing the potential of combining eventcamera technology with generative models. Our work opens new avenues forresearch at the intersection of computer vision, computational photography, andgenerative modeling, offering a forward-thinking solution to capturing missedmoments and enhancing future consumer cameras and smartphones. Please see theproject page at https://timerewind.github.io/ for video results and coderelease.</description><author>Jingxi Chen, Brandon Y. Feng, Haoming Cai, Mingyang Xie, Christopher Metzler, Cornelia Fermuller, Yiannis Aloimonos</author><pubDate>Wed, 20 Mar 2024 18:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13800v1</guid></item><item><title>The Expressive Power of Transformers with Chain of Thought</title><link>http://arxiv.org/abs/2310.07923v4</link><description>Recent theoretical work has identified surprisingly simple reasoningproblems, such as checking if two nodes in a graph are connected or simulatingfinite-state machines, that are provably unsolvable by standard transformersthat answer immediately after reading their input. However, in practice,transformers' reasoning can be improved by allowing them to use a "chain ofthought" or "scratchpad", i.e., generate and condition on a sequence ofintermediate tokens before answering. Motivated by this, we ask: Does suchintermediate generation fundamentally extend the computational power of adecoder-only transformer? We show that the answer is yes, but the amount ofincrease depends crucially on the amount of intermediate generation. Forinstance, we find that transformer decoders with a logarithmic number ofdecoding steps (w.r.t. the input length) push the limits of standardtransformers only slightly, while a linear number of decoding steps, assuming aslight generalization to standard pre-norm, adds a clear new ability (understandard complexity conjectures): recognizing all regular languages. Ourresults also imply that linear steps keep transformer decoders withincontext-sensitive languages, and polynomial steps with generalized pre-normmake them recognize exactly the class of polynomial-time solvable problems --the first exact characterization of a type of transformers in terms of standardcomplexity classes. Together, our results provide a nuanced framework forunderstanding how the length of a transformer's chain of thought or scratchpadimpacts its reasoning power.</description><author>William Merrill, Ashish Sabharwal</author><pubDate>Wed, 20 Mar 2024 18:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07923v4</guid></item><item><title>Reverse Training to Nurse the Reversal Curse</title><link>http://arxiv.org/abs/2403.13799v1</link><description>Large language models (LLMs) have a surprising failure: when trained on "Ahas a feature B", they do not generalize to "B is a feature of A", which istermed the Reversal Curse. Even when training with trillions of tokens thisissue still appears due to Zipf's law - hence even if we train on the entireinternet. This work proposes an alternative training scheme, called reversetraining, whereby all words are used twice, doubling the amount of availabletokens. The LLM is trained in both forward and reverse directions by reversingthe training strings while preserving (i.e., not reversing) chosen substrings,such as entities. We show that data-matched reverse-trained models providesuperior performance to standard models on standard tasks, and compute-matchedreverse-trained models provide far superior performance on reversal tasks,helping resolve the reversal curse issue.</description><author>Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, Sainbayar Sukhbaatar</author><pubDate>Wed, 20 Mar 2024 18:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13799v1</guid></item><item><title>Hierarchical NeuroSymbolic Approach for Action Quality Assessment</title><link>http://arxiv.org/abs/2403.13798v1</link><description>Action quality assessment (AQA) applies computer vision to quantitativelyassess the performance or execution of a human action. Current AQA approachesare end-to-end neural models, which lack transparency and tend to be biasedbecause they are trained on subjective human judgements as ground-truth. Toaddress these issues, we introduce a neuro-symbolic paradigm for AQA, whichuses neural networks to abstract interpretable symbols from video data andmakes quality assessments by applying rules to those symbols. We take diving asthe case study. We found that domain experts prefer our system and find it moreinformative than purely neural approaches to AQA in diving. Our system alsoachieves state-of-the-art action recognition and temporal segmentation, andautomatically generates a detailed report that breaks the dive down into itselements and provides objective scoring with visual evidence. As verified by agroup of domain experts, this report may be used to assist judges in scoring,help train judges, and provide feedback to divers. We will open-source all ofour annotated training data and code for ease of reproducibility.</description><author>Lauren Okamoto, Paritosh Parmar</author><pubDate>Wed, 20 Mar 2024 18:55:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13798v1</guid></item><item><title>Bridge the Modality and Capacity Gaps in Vision-Language Model Selection</title><link>http://arxiv.org/abs/2403.13797v1</link><description>Vision Language Models (VLMs) excel in zero-shot image classification bypairing images with textual category names. The expanding variety ofPre-Trained VLMs enhances the likelihood of identifying a suitable VLM forspecific tasks. Thus, a promising zero-shot image classification strategy isselecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solelyon the text data of the target dataset without access to the dataset's images.In this paper, we analyze two inherent challenges in assessing the ability of aVLM in this Language-Only VLM selection: the "Modality Gap" -- the disparity inVLM's embeddings across two different modalities, making text a less reliablesubstitute for images; and the "Capability Gap" -- the discrepancy between theVLM's overall ranking and its ranking for target dataset, hindering directprediction of a model's dataset-specific performance from its generalperformance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate thenegative impact of these two gaps. SWAB first adopts optimal transport tocapture the relevance between open-source datasets and target dataset with atransportation matrix. It then uses this matrix to transfer useful statisticsof VLMs from open-source datasets to the target dataset for bridging those twogaps and enhancing the VLM's capacity estimation for VLM selection. Experimentsacross various VLMs and image classification datasets validate SWAB'seffectiveness.</description><author>Chao Yi, De-Chuan Zhan, Han-Jia Ye</author><pubDate>Wed, 20 Mar 2024 18:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13797v1</guid></item><item><title>Evaluating Frontier Models for Dangerous Capabilities</title><link>http://arxiv.org/abs/2403.13793v1</link><description>To understand the risks posed by a new AI system, we must understand what itcan and cannot do. Building on prior work, we introduce a programme of new"dangerous capability" evaluations and pilot them on Gemini 1.0 models. Ourevaluations cover four areas: (1) persuasion and deception; (2) cyber-security;(3) self-proliferation; and (4) self-reasoning. We do not find evidence ofstrong dangerous capabilities in the models we evaluated, but we flag earlywarning signs. Our goal is to help advance a rigorous science of dangerouscapability evaluation, in preparation for future models.</description><author>Mary Phuong, Matthew Aitchison, Elliot Catt, Sarah Cogan, Alexandre Kaskasoli, Victoria Krakovna, David Lindner, Matthew Rahtz, Yannis Assael, Sarah Hodkinson, Heidi Howard, Tom Lieberum, Ramana Kumar, Maria Abi Raad, Albert Webson, Lewis Ho, Sharon Lin, Sebastian Farquhar, Marcus Hutter, Gregoire Deletang, Anian Ruoss, Seliem El-Sayed, Sasha Brown, Anca Dragan, Rohin Shah, Allan Dafoe, Toby Shevlane</author><pubDate>Wed, 20 Mar 2024 18:54:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13793v1</guid></item><item><title>DepthFM: Fast Monocular Depth Estimation with Flow Matching</title><link>http://arxiv.org/abs/2403.13788v1</link><description>Monocular depth estimation is crucial for numerous downstream vision tasksand applications. Current discriminative approaches to this problem are limiteddue to blurry artifacts, while state-of-the-art generative methods suffer fromslow sampling due to their SDE nature. Rather than starting from noise, we seeka direct mapping from input image to depth map. We observe that this can beeffectively framed using flow matching, since its straight trajectories throughsolution space offer efficiency and high quality. Our study demonstrates that apre-trained image diffusion model can serve as an adequate prior for a flowmatching depth model, allowing efficient training on only synthetic data togeneralize to real images. We find that an auxiliary surface normals lossfurther improves the depth estimates. Due to the generative nature of ourapproach, our model reliably predicts the confidence of its depth estimates. Onstandard benchmarks of complex natural scenes, our lightweight approachexhibits state-of-the-art performance at favorable low computational costdespite only being trained on little synthetic data.</description><author>Ming Gui, Johannes S. Fischer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, Björn Ommer</author><pubDate>Wed, 20 Mar 2024 18:51:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13788v1</guid></item><item><title>RewardBench: Evaluating Reward Models for Language Modeling</title><link>http://arxiv.org/abs/2403.13787v1</link><description>Reward models (RMs) are at the crux of successful RLHF to align pretrainedmodels to human preferences, yet there has been relatively little study thatfocuses on evaluation of those reward models. Evaluating reward models presentsan opportunity to understand the opaque technologies used for alignment oflanguage models and which values are embedded in them. To date, very fewdescriptors of capabilities, training methods, or open-source reward modelsexist. In this paper, we present RewardBench, a benchmark dataset and code-basefor evaluation, to enhance scientific understanding of reward models. TheRewardBench dataset is a collection of prompt-win-lose trios spanning chat,reasoning, and safety, to benchmark how reward models perform on challenging,structured and out-of-distribution queries. We created specific comparisondatasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrectfacts) why one answer should be preferred to another. On the RewardBenchleaderboard, we evaluate reward models trained with a variety of methods, suchas the direct MLE training of classifiers and the implicit reward modeling ofDirect Preference Optimization (DPO), and on a spectrum of datasets. We presentmany findings on propensity for refusals, reasoning limitations, andinstruction following shortcomings of various reward models towards a betterunderstanding of the RLHF process.</description><author>Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi</author><pubDate>Wed, 20 Mar 2024 18:49:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13787v1</guid></item><item><title>Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts</title><link>http://arxiv.org/abs/2403.13786v1</link><description>Automatic coding patient behaviors is essential to support decision makingfor psychotherapists during the motivational interviewing (MI), a collaborativecommunication intervention approach to address psychiatric issues, such asalcohol and drug addiction. While the behavior coding task has rapidly adaptedmachine learning to predict patient states during the MI sessions, lacking ofdomain-specific knowledge and overlooking patient-therapist interactions aremajor challenges in developing and deploying those models in real practice. Toencounter those challenges, we introduce the Chain-of-Interaction (CoI)prompting method aiming to contextualize large language models (LLMs) forpsychiatric decision support by the dyadic interactions. The CoI promptingapproach systematically breaks down the coding task into three key reasoningsteps, extract patient engagement, learn therapist question strategies, andintegrates dyadic interactions between patients and therapists. This approachenables large language models to leverage the coding scheme, patient state, anddomain knowledge for patient behavioral coding. Experiments on real-worlddatasets can prove the effectiveness and flexibility of our prompting methodwith multiple state-of-the-art LLMs over existing prompting baselines. We haveconducted extensive ablation analysis and demonstrate the critical role ofdyadic interactions in applying LLMs for psychotherapy behavior understanding.</description><author>Guangzeng Han, Weisi Liu, Xiaolei Huang, Brian Borsari</author><pubDate>Wed, 20 Mar 2024 18:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13786v1</guid></item><item><title>Towards an extension of Fault Trees in the Predictive Maintenance Scenario</title><link>http://arxiv.org/abs/2403.13785v1</link><description>One of the most appreciated features of Fault Trees (FTs) is theirsimplicity, making them fit into industrial processes. As such processes evolvein time, considering new aspects of large modern systems, modelling techniquesbased on FTs have adapted to these needs. This paper proposes an extension ofFTs to take into account the problem of Predictive Maintenance, one of thechallenges of the modern dependability field of study. The paper sketches thePredictive Fault Tree language and proposes some use cases to support theirmodelling and analysis in concrete industrial settings.</description><author>Roberta De Fazio, Stefano Marrone, Laura Verde, Vincenzo Reccia, Paolo Valletta</author><pubDate>Wed, 20 Mar 2024 18:47:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13785v1</guid></item><item><title>The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI</title><link>http://arxiv.org/abs/2403.13784v1</link><description>Generative AI (GAI) offers unprecedented possibilities but itscommercialization has raised concerns about transparency, reproducibility,bias, and safety. Many "open-source" GAI models lack the necessary componentsfor full understanding and reproduction, and some use restrictive licenses, apractice known as "openwashing." We propose the Model Openness Framework (MOF),a ranked classification system that rates machine learning models based ontheir completeness and openness, following principles of open science, opensource, open data, and open access. The MOF requires specific components of themodel development lifecycle to be included and released under appropriate openlicenses. This framework aims to prevent misrepresentation of models claimingto be open, guide researchers and developers in providing all model componentsunder permissive licenses, and help companies, academia, and hobbyists identifymodels that can be safely adopted without restrictions. Wide adoption of theMOF will foster a more open AI ecosystem, accelerating research, innovation,and adoption.</description><author>Matt White, Ibrahim Haddad, Cailean Osborne, Xiao-Yang, Liu, Ahmed Abdelmonsef, Sachin Varghese</author><pubDate>Wed, 20 Mar 2024 18:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13784v1</guid></item><item><title>Sparse Implementation of Versatile Graph-Informed Layers</title><link>http://arxiv.org/abs/2403.13781v1</link><description>Graph Neural Networks (GNNs) have emerged as effective tools for learningtasks on graph-structured data. Recently, Graph-Informed (GI) layers wereintroduced to address regression tasks on graph nodes, extending theirapplicability beyond classic GNNs. However, existing implementations of GIlayers lack efficiency due to dense memory allocation. This paper presents asparse implementation of GI layers, leveraging the sparsity of adjacencymatrices to reduce memory usage significantly. Additionally, a versatilegeneral form of GI layers is introduced, enabling their application to subsetsof graph nodes. The proposed sparse implementation improves the concretecomputational efficiency and scalability of the GI layers, permitting to builddeeper Graph-Informed Neural Networks (GINNs) and facilitating theirscalability to larger graphs.</description><author>Francesco Della Santa</author><pubDate>Wed, 20 Mar 2024 18:43:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13781v1</guid></item><item><title>Information-Theoretic Distillation for Reference-less Summarization</title><link>http://arxiv.org/abs/2403.13780v1</link><description>The current winning recipe for automatic summarization is using proprietarylarge-scale language models (LLMs) such as ChatGPT as is, or imitation learningfrom them as teacher models. While increasingly ubiquitous dependence on suchlarge-scale language models is convenient, there remains an important questionof whether small-scale models could have achieved competitive results, if wewere to seek an alternative learning method -- that allows for a morecost-efficient, controllable, yet powerful summarizer. We present InfoSumm, anovel framework to distill a powerful summarizer based on theinformation-theoretic objective for summarization, without relying on eitherthe LLM's capability or human-written references. To achieve this, we firstpropose a novel formulation of the desiderata of summarization (saliency,faithfulness and brevity) through the lens of mutual information between theoriginal document and the summary. Based on this formulation, we start off fromPythia-2.8B as the teacher model, which is not yet capable of summarization,then self-train the model to optimize for the information-centric measures ofideal summaries. Distilling from the improved teacher, we arrive at a compactbut powerful summarizer with only 568M parameters that performs competitivelyagainst ChatGPT, without ever relying on ChatGPT's capabilities. Extensiveanalysis demonstrates that our approach outperforms in-domain supervised modelsin human evaluation, let alone state-of-the-art unsupervised methods, and winsover ChatGPT in controllable summarization.</description><author>Jaehun Jung, Ximing Lu, Liwei Jiang, Faeze Brahman, Peter West, Pang Wei Koh, Yejin Choi</author><pubDate>Wed, 20 Mar 2024 18:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13780v1</guid></item><item><title>Certified Human Trajectory Prediction</title><link>http://arxiv.org/abs/2403.13778v1</link><description>Trajectory prediction plays an essential role in autonomous vehicles. Whilenumerous strategies have been developed to enhance the robustness of trajectoryprediction models, these methods are predominantly heuristic and do not offerguaranteed robustness against adversarial attacks and noisy observations. Inthis work, we propose a certification approach tailored for the task oftrajectory prediction. To this end, we address the inherent challengesassociated with trajectory prediction, including unbounded outputs, andmutli-modality, resulting in a model that provides guaranteed robustness.Furthermore, we integrate a denoiser into our method to further improve theperformance. Through comprehensive evaluations, we demonstrate theeffectiveness of the proposed technique across various baselines and usingstandard trajectory prediction datasets. The code will be made availableonline: https://s-attack.github.io/</description><author>Mohammadhossein Bahari, Saeed Saadatnejad, Amirhossein Asgari Farsangi, Seyed-Mohsen Moosavi-Dezfooli, Alexandre Alahi</author><pubDate>Wed, 20 Mar 2024 18:41:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13778v1</guid></item><item><title>Magic-Me: Identity-Specific Video Customized Diffusion</title><link>http://arxiv.org/abs/2402.09368v2</link><description>Creating content with specified identities (ID) has attracted significantinterest in the field of generative models. In the field of text-to-imagegeneration (T2I), subject-driven creation has achieved great progress with theidentity controlled via reference images. However, its extension to videogeneration is not well explored. In this work, we propose a simple yeteffective subject identity controllable video generation framework, termedVideo Custom Diffusion (VCD). With a specified identity defined by a fewimages, VCD reinforces the identity characteristics and injects frame-wisecorrelation at the initialization stage for stable video outputs. To achievethis, we propose three novel components that are essential for high-qualityidentity preservation and stable video generation: 1) a noise initializationmethod with 3D Gaussian Noise Prior for better inter-frame stability; 2) an IDmodule based on extended Textual Inversion trained with the cropped identity todisentangle the ID information from the background 3) Face VCD and Tiled VCDmodules to reinforce faces and upscale the video to higher resolution whilepreserving the identity's features. We conducted extensive experiments toverify that VCD is able to generate stable videos with better ID over thebaselines. Besides, with the transferability of the encoded identity in the IDmodule, VCD is also working well with personalized text-to-image modelsavailable publicly. The codes are available athttps://github.com/Zhen-Dong/Magic-Me.</description><author>Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng</author><pubDate>Wed, 20 Mar 2024 18:36:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09368v2</guid></item><item><title>Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy Learning for Robotic Navigation</title><link>http://arxiv.org/abs/2306.06192v5</link><description>Trajectory length stands as a crucial hyperparameter within reinforcementlearning (RL) algorithms, significantly contributing to the sample inefficiencyin robotics applications. Motivated by the pivotal role trajectory length playsin the training process, we introduce Ada-NAV, a novel adaptive trajectorylength scheme designed to enhance the training sample efficiency of RLalgorithms in robotic navigation tasks. Unlike traditional approaches thattreat trajectory length as a fixed hyperparameter, we propose to dynamicallyadjust it based on the entropy of the underlying navigation policy.Interestingly, Ada-NAV can be applied to both existing on-policy and off-policyRL methods, which we demonstrate by empirically validating its efficacy onthree popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), andSoft Actor-Critic (SAC). We demonstrate through simulated and real-worldrobotic experiments that Ada-NAV outperforms conventional methods that employconstant or randomly sampled trajectory lengths. Specifically, for a fixedsample budget, Ada-NAV achieves an 18\% increase in navigation success rate, a20-38\% reduction in navigation path length, and a 9.32\% decrease in elevationcosts. Furthermore, we showcase the versatility of Ada-NAV by integrating itwith the Clearpath Husky robot, illustrating its applicability in complexoutdoor environments.</description><author>Bhrij Patel, Kasun Weerakoon, Wesley A. Suttle, Alec Koppel, Brian M. Sadler, Tianyi Zhou, Amrit Singh Bedi, Dinesh Manocha</author><pubDate>Wed, 20 Mar 2024 18:36:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06192v5</guid></item><item><title>m&amp;m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks</title><link>http://arxiv.org/abs/2403.11085v2</link><description>Real-world multi-modal problems are rarely solved by a single machinelearning model, and often require multi-step computational plans that involvestitching several models. Tool-augmented LLMs hold tremendous promise forautomating the generation of such computational plans. However, the lack ofstandardized benchmarks for evaluating LLMs as planners for multi-stepmulti-modal tasks has prevented a systematic study of planner design decisions.Should LLMs generate a full plan in a single shot or step-by-step? Should theyinvoke tools directly with Python code or through structured data formats likeJSON? Does feedback improve planning? To answer these questions and more, weintroduce m&amp;m's: a benchmark containing 4K+ multi-step multi-modal tasksinvolving 33 tools that include multi-modal models, (free) public APIs, andimage processing modules. For each of these task queries, we provideautomatically generated plans using this realistic toolset. We further providea high-quality subset of 1,565 task plans that are human-verified and correctlyexecutable. With m&amp;m's, we evaluate 6 popular LLMs with 2 planning strategies(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3types of feedback (parsing/verification/execution). Finally, we summarizetakeaways from our extensive experiments. Our dataset and code are available onHuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github(https://github.com/RAIVNLab/mnms).</description><author>Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna</author><pubDate>Wed, 20 Mar 2024 18:35:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11085v2</guid></item><item><title>Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models</title><link>http://arxiv.org/abs/2403.13771v1</link><description>In this paper, we propose Describe-and-Dissect (DnD), a novel method todescribe the roles of hidden neurons in vision networks. DnD utilizes recentadvancements in multimodal deep learning to produce complex natural languagedescriptions, without the need for labeled training data or a predefined set ofconcepts to choose from. Additionally, DnD is training-free, meaning we don'ttrain any new models and can easily leverage more capable general purposemodels in the future. We have conducted extensive qualitative and quantitativeanalysis to show that DnD outperforms prior work by providing higher qualityneuron descriptions. Specifically, our method on average provides the highestquality labels and is more than 2 times as likely to be selected as the bestexplanation for a neuron than the best baseline.</description><author>Nicholas Bai, Rahul A. Iyer, Tuomas Oikarinen, Tsui-Wei Weng</author><pubDate>Wed, 20 Mar 2024 18:33:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13771v1</guid></item><item><title>Towards Principled Representation Learning from Videos for Reinforcement Learning</title><link>http://arxiv.org/abs/2403.13765v1</link><description>We study pre-training representations for decision-making using video data,which is abundantly available for tasks such as game agents and softwaretesting. Even though significant empirical advances have been made on thisproblem, a theoretical understanding remains absent. We initiate thetheoretical investigation into principled approaches for representationlearning and focus on learning the latent state representations of theunderlying MDP using video data. We study two types of settings: one wherethere is iid noise in the observation, and a more challenging setting wherethere is also the presence of exogenous noise, which is non-iid noise that istemporally correlated, such as the motion of people or cars in the background.We study three commonly used approaches: autoencoding, temporal contrastivelearning, and forward modeling. We prove upper bounds for temporal contrastivelearning and forward modeling in the presence of only iid noise. We show thatthese approaches can learn the latent state and use it to do efficientdownstream RL with polynomial sample complexity. When exogenous noise is alsopresent, we establish a lower bound result showing that the sample complexityof learning from video data can be exponentially worse than learning fromaction-labeled trajectory data. This partially explains why reinforcementlearning with video pre-training is hard. We evaluate these representationallearning methods in two visual domains, yielding results that are consistentwith our theoretical findings.</description><author>Dipendra Misra, Akanksha Saran, Tengyang Xie, Alex Lamb, John Langford</author><pubDate>Wed, 20 Mar 2024 18:28:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13765v1</guid></item><item><title>TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion Models</title><link>http://arxiv.org/abs/2312.00651v2</link><description>Despite remarkable achievements in video synthesis, achieving granularcontrol over complex dynamics, such as nuanced movement among multipleinteracting objects, still presents a significant hurdle for dynamic worldmodeling, compounded by the necessity to manage appearance and disappearance,drastic scale changes, and ensure consistency for instances across frames.These challenges hinder the development of video generation that can faithfullymimic real-world complexity, limiting utility for applications requiringhigh-level realism and controllability, including advanced scene simulation andtraining of perception systems. To address that, we propose TrackDiffusion, anovel video generation framework affording fine-grained trajectory-conditionedmotion control via diffusion models, which facilitates the precise manipulationof the object trajectories and interactions, overcoming the prevalentlimitation of scale and continuity disruptions. A pivotal component ofTrackDiffusion is the instance enhancer, which explicitly ensures inter-frameconsistency of multiple objects, a critical factor overlooked in the currentliterature. Moreover, we demonstrate that generated video sequences by ourTrackDiffusion can be used as training data for visual perception models. Tothe best of our knowledge, this is the first work to apply video diffusionmodels with tracklet conditions and demonstrate that generated frames can bebeneficial for improving the performance of object trackers.</description><author>Pengxiang Li, Kai Chen, Zhili Liu, Ruiyuan Gao, Lanqing Hong, Guo Zhou, Hua Yao, Dit-Yan Yeung, Huchuan Lu, Xu Jia</author><pubDate>Wed, 20 Mar 2024 18:28:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00651v2</guid></item><item><title>Practical End-to-End Optical Music Recognition for Pianoform Music</title><link>http://arxiv.org/abs/2403.13763v1</link><description>The majority of recent progress in Optical Music Recognition (OMR) has beenachieved with Deep Learning methods, especially models following the end-to-endparadigm, reading input images and producing a linear sequence of tokens.Unfortunately, many music scores, especially piano music, cannot be easilyconverted to a linear sequence. This has led OMR researchers to use customlinearized encodings, instead of broadly accepted structured formats for musicnotation. Their diversity makes it difficult to compare the performance of OMRsystems directly. To bring recent OMR model progress closer to useful results:(a) We define a sequential format called Linearized MusicXML, allowing to trainan end-to-end model directly and maintaining close cohesion and compatibilitywith the industry-standard MusicXML format. (b) We create a dev and test setfor benchmarking typeset OMR with MusicXML ground truth based on the OpenScoreLieder corpus. They contain 1,438 and 1,493 pianoform systems, each with animage from IMSLP. (c) We train and fine-tune an end-to-end model to serve as abaseline on the dataset and employ the TEDn metric to evaluate the model. Wealso test our model against the recently published synthetic pianoform datasetGrandStaff and surpass the state-of-the-art results.</description><author>Jiří Mayer, Milan Straka, Jan Hajič jr., Pavel Pecina</author><pubDate>Wed, 20 Mar 2024 18:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13763v1</guid></item><item><title>Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. II</title><link>http://arxiv.org/abs/2305.17282v5</link><description>We continue to investigate the $k$ nearest neighbour ($k$-NN) learning rulein complete separable metric spaces. Thanks to the results of C\'erou andGuyader (2006) and Preiss (1983), this rule is known to be universallyconsistent in every such metric space that is sigma-finite dimensional in thesense of Nagata. Here we show that the rule is strongly universally consistentin such spaces in the absence of ties. Under the tie-breaking strategy appliedby Devroye, Gy\"{o}rfi, Krzy\.{z}ak, and Lugosi (1994) in the Euclideansetting, we manage to show the strong universal consistency in non-Archimedianmetric spaces (that is, those of Nagata dimension zero). Combining the theoremof C\'erou and Guyader with results of Assouad and Quentin de Gromard (2006),one deduces that the $k$-NN rule is universally consistent in metric spaceshaving finite dimension in the sense of de Groot. In particular, the $k$-NNrule is universally consistent in the Heisenberg group which is notsigma-finite dimensional in the sense of Nagata as follows from an exampleindependently constructed by Kor\'anyi and Reimann (1995) and Sawyer andWheeden (1992).</description><author>Sushma Kumari, Vladimir G. Pestov</author><pubDate>Wed, 20 Mar 2024 18:25:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17282v5</guid></item><item><title>HierCode: A Lightweight Hierarchical Codebook for Zero-shot Chinese Text Recognition</title><link>http://arxiv.org/abs/2403.13761v1</link><description>Text recognition, especially for complex scripts like Chinese, faces uniquechallenges due to its intricate character structures and vast vocabulary.Traditional one-hot encoding methods struggle with the representation ofhierarchical radicals, recognition of Out-Of-Vocabulary (OOV) characters, andon-device deployment due to their computational intensity. To address thesechallenges, we propose HierCode, a novel and lightweight codebook that exploitsthe innate hierarchical nature of Chinese characters. HierCode employs amulti-hot encoding strategy, leveraging hierarchical binary tree encoding andprototype learning to create distinctive, informative representations for eachcharacter. This approach not only facilitates zero-shot recognition of OOVcharacters by utilizing shared radicals and structures but also excels inline-level recognition tasks by computing similarity with visual features, anotable advantage over existing methods. Extensive experiments across diversebenchmarks, including handwritten, scene, document, web, and ancient text, haveshowcased HierCode's superiority for both conventional and zero-shot Chinesecharacter or text recognition, exhibiting state-of-the-art performance withsignificantly fewer parameters and fast inference speed.</description><author>Yuyi Zhang, Yuanzhi Zhu, Dezhi Peng, Peirong Zhang, Zhenhua Yang, Zhibo Yang, Cong Yao, Lianwen Jin</author><pubDate>Wed, 20 Mar 2024 18:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13761v1</guid></item><item><title>When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather</title><link>http://arxiv.org/abs/2403.13762v1</link><description>In Federated Learning (FL), multiple clients collaboratively train a globalmodel without sharing private data. In semantic segmentation, the Federatedsource Free Domain Adaptation (FFreeDA) setting is of particular interest,where clients undergo unsupervised training after supervised pretraining at theserver side. While few recent works address FL for autonomous vehicles,intrinsic real-world challenges such as the presence of adverse weatherconditions and the existence of different autonomous agents are stillunexplored. To bridge this gap, we address both problems and introduce a newfederated semantic segmentation setting where both car and drone clientsco-exist and collaborate. Specifically, we propose a novel approach for thissetting which exploits a batch-norm weather-aware strategy to dynamically adaptthe model to the different weather conditions, while hyperbolic spaceprototypes are used to align the heterogeneous client representations. Finally,we introduce FLYAWARE, the first semantic segmentation dataset with adverseweather data for aerial vehicles.</description><author>Giulia Rizzoli, Matteo Caligiuri, Donald Shenaj, Francesco Barbato, Pietro Zanuttigh</author><pubDate>Wed, 20 Mar 2024 18:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13762v1</guid></item><item><title>Having Beer after Prayer? Measuring Cultural Bias in Large Language Models</title><link>http://arxiv.org/abs/2305.14456v4</link><description>As the reach of large language models (LMs) expands globally, their abilityto cater to diverse cultural contexts becomes crucial. Despite advancements inmultilingual capabilities, models are not designed with appropriate culturalnuances. In this paper, we show that multilingual and Arabic monolingual LMsexhibit bias towards entities associated with Western culture. We introduceCAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entitiesspanning eight types that contrast Arab and Western cultures. CAMeL provides afoundation for measuring cultural biases in LMs through both extrinsic andintrinsic evaluations. Using CAMeL, we examine the cross-cultural performancein Arabic of 16 different LMs on tasks such as story generation, NER, andsentiment analysis, where we find concerning cases of stereotyping and culturalunfairness. We further test their text-infilling performance, revealing theincapability of appropriate adaptation to Arab cultural contexts. Finally, weanalyze 6 Arabic pre-training corpora and find that commonly used sources suchas Wikipedia may not be best suited to build culturally aware LMs, if used asthey are without adjustment. We will make CAMeL publicly available at:https://github.com/tareknaous/camel</description><author>Tarek Naous, Michael J. Ryan, Alan Ritter, Wei Xu</author><pubDate>Wed, 20 Mar 2024 18:16:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14456v4</guid></item><item><title>The Unreasonable Effectiveness of Greedy Algorithms in Multi-Armed Bandit with Many Arms</title><link>http://arxiv.org/abs/2002.10121v4</link><description>We investigate a Bayesian $k$-armed bandit problem in the \emph{many-armed}regime, where $k \geq \sqrt{T}$ and $T$ represents the time horizon. Initially,and aligned with recent literature on many-armed bandit problems, we observethat subsampling plays a key role in designing optimal algorithms; theconventional UCB algorithm is sub-optimal, whereas a subsampled UCB (SS-UCB),which selects $\Theta(\sqrt{T})$ arms for execution under the UCB framework,achieves rate-optimality. However, despite SS-UCB's theoretical promise ofoptimal regret, it empirically underperforms compared to a greedy algorithmthat consistently chooses the empirically best arm. This observation extends tocontextual settings through simulations with real-world data. Our findingssuggest a new form of \emph{free exploration} beneficial to greedy algorithmsin the many-armed context, fundamentally linked to a tail event concerning theprior distribution of arm rewards. This finding diverges from the notion offree exploration, which relates to covariate variation, as recently discussedin contextual bandit literature. Expanding upon these insights, we establishthat the subsampled greedy approach not only achieves rate-optimality forBernoulli bandits within the many-armed regime but also attains sublinearregret across broader distributions. Collectively, our research indicates thatin the many-armed regime, practitioners might find greater value in adoptinggreedy algorithms.</description><author>Mohsen Bayati, Nima Hamidi, Ramesh Johari, Khashayar Khosravi</author><pubDate>Wed, 20 Mar 2024 18:15:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2002.10121v4</guid></item><item><title>PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology</title><link>http://arxiv.org/abs/2401.16355v3</link><description>The emergence of large multimodal models has unlocked remarkable potential inAI, particularly in pathology. However, the lack of specialized, high-qualitybenchmark impeded their development and precise evaluation. To address this, weintroduce PathMMU, the largest and highest-quality expert-validated pathologybenchmark for Large Multimodal Models (LMMs). It comprises 33,428 multimodalmulti-choice questions and 24,067 images from various sources, each accompaniedby an explanation for the correct answer. The construction of PathMMU harnessesGPT-4V's advanced capabilities, utilizing over 30,000 image-caption pairs toenrich captions and generate corresponding Q&amp;As in a cascading process.Significantly, to maximize PathMMU's authority, we invite seven pathologists toscrutinize each question under strict standards in PathMMU's validation andtest sets, while simultaneously setting an expert-level performance benchmarkfor PathMMU. We conduct extensive evaluations, including zero-shot assessmentsof 14 open-sourced and 4 closed-sourced LMMs and their robustness to imagecorruption. We also fine-tune representative LMMs to assess their adaptabilityto PathMMU. The empirical findings indicate that advanced LMMs struggle withthe challenging PathMMU benchmark, with the top-performing LMM, GPT-4V,achieving only a 49.8% zero-shot performance, significantly lower than the71.8% demonstrated by human pathologists. After fine-tuning, significantlysmaller open-sourced LMMs can outperform GPT-4V but still fall short of theexpertise shown by pathologists. We hope that the PathMMU will offer valuableinsights and foster the development of more specialized, next-generation LMMsfor pathology.</description><author>Yuxuan Sun, Hao Wu, Chenglu Zhu, Sunyi Zheng, Qizi Chen, Kai Zhang, Yunlong Zhang, Dan Wan, Xiaoxiao Lan, Mengyue Zheng, Jingxiong Li, Xinheng Lyu, Tao Lin, Lin Yang</author><pubDate>Wed, 20 Mar 2024 18:13:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16355v3</guid></item><item><title>Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge Augmentation in Vision Language Model</title><link>http://arxiv.org/abs/2403.13756v1</link><description>We present a knowledge augmentation strategy for assessing the diagnosticgroups and gait impairment from monocular gait videos. Based on a large-scalepre-trained Vision Language Model (VLM), our model learns and improves visual,textual, and numerical representations of patient gait videos, through acollective learning across three distinct modalities: gait videos,class-specific descriptions, and numerical gait parameters. Our specificcontributions are two-fold: First, we adopt a knowledge-aware prompt tuningstrategy to utilize the class-specific medical description in guiding the textprompt learning. Second, we integrate the paired gait parameters in the form ofnumerical texts to enhance the numeracy of the textual representation. Resultsdemonstrate that our model not only significantly outperforms state-of-the-art(SOTA) in video-based classification tasks but also adeptly decodes the learnedclass-specific text features into natural language descriptions using thevocabulary of quantitative gait parameters. The code and the model will be madeavailable at our project page.</description><author>Diwei Wang, Kun Yuan, Candice Muller, Frédéric Blanc, Nicolas Padoy, Hyewon Seo</author><pubDate>Wed, 20 Mar 2024 18:03:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13756v1</guid></item><item><title>Different Tokenization Schemes Lead to Comparable Performance in Spanish Number Agreement</title><link>http://arxiv.org/abs/2403.13754v1</link><description>The relationship between language model tokenization and performance is anopen area of research. Here, we investigate how different tokenization schemesimpact number agreement in Spanish plurals. We find thatmorphologically-aligned tokenization performs similarly to other tokenizationschemes, even when induced artificially for words that would not be tokenizedthat way during training. We then present exploratory analyses demonstratingthat language model embeddings for different plural tokenizations have similardistributions along the embedding space axis that maximally distinguishessingular and plural nouns. Our results suggest that morphologically-alignedtokenization is a viable tokenization approach, and existing models alreadygeneralize some morphological patterns to new items. However, our resultsindicate that morphological tokenization is not strictly required forperformance.</description><author>Catherine Arnett, Pamela D. Rivière, Tyler A. Chang, Sean Trott</author><pubDate>Wed, 20 Mar 2024 18:01:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13754v1</guid></item><item><title>Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning</title><link>http://arxiv.org/abs/2403.13749v1</link><description>We introduce $r$-loopy Weisfeiler-Leman ($r$-$\ell{}$WL), a novel hierarchyof graph isomorphism tests and a corresponding GNN framework, $r$-$\ell{}$MPNN,that can count cycles up to length $r + 2$. Most notably, we show that$r$-$\ell{}$WL can count homomorphisms of cactus graphs. This strictly extendsclassical 1-WL, which can only count homomorphisms of trees and, in fact, isincomparable to $k$-WL for any fixed $k$. We empirically validate theexpressive and counting power of the proposed $r$-$\ell{}$MPNN on severalsynthetic datasets and present state-of-the-art predictive performance onvarious real-world datasets. The code is available athttps://github.com/RPaolino/loopy</description><author>Raffaele Paolino, Sohir Maskey, Pascal Welke, Gitta Kutyniok</author><pubDate>Wed, 20 Mar 2024 17:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13749v1</guid></item><item><title>HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models</title><link>http://arxiv.org/abs/2309.02706v5</link><description>Large language models (LLMs) trained on massive corpora demonstrateimpressive capabilities in a wide range of tasks. While there are ongoingefforts to adapt these models to languages beyond English, the attention givento their evaluation methodologies remains limited. Current multilingualbenchmarks often rely on back translations or re-implementations of Englishtests, limiting their capacity to capture unique cultural and linguisticnuances. To bridge this gap for the Korean language, we introduce the HAE-RAEBench, a dataset curated to challenge models lacking Korean cultural andcontextual depth. The dataset encompasses six downstream tasks across fourdomains: vocabulary, history, general knowledge, and reading comprehension.Unlike traditional evaluation suites focused on token and sequenceclassification or mathematical and logical reasoning, the HAE-RAE Benchemphasizes a model's aptitude for recalling Korean-specific knowledge andcultural contexts. Comparative analysis with prior Korean benchmarks indicatesthat the HAE-RAE Bench presents a greater challenge to non-Korean models bydisturbing abilities and knowledge learned from English being transferred.</description><author>Guijin Son, Hanwool Lee, Suwan Kim, Huiseo Kim, Jaecheol Lee, Je Won Yeom, Jihyu Jung, Jung Woo Kim, Songseong Kim</author><pubDate>Wed, 20 Mar 2024 17:56:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02706v5</guid></item><item><title>An Ordering of Divergences for Variational Inference with Factorized Gaussian Approximations</title><link>http://arxiv.org/abs/2403.13748v1</link><description>Given an intractable distribution $p$, the problem of variational inference(VI) is to compute the best approximation $q$ from some more tractable family$\mathcal{Q}$. Most commonly the approximation is found by minimizing aKullback-Leibler (KL) divergence. However, there exist other valid choices ofdivergences, and when $\mathcal{Q}$ does not contain~$p$, each divergencechampions a different solution. We analyze how the choice of divergence affectsthe outcome of VI when a Gaussian with a dense covariance matrix isapproximated by a Gaussian with a diagonal covariance matrix. In this settingwe show that different divergences can be \textit{ordered} by the amount thattheir variational approximations misestimate various measures of uncertainty,such as the variance, precision, and entropy. We also derive an impossibilitytheorem showing that no two of these measures can be simultaneously matched bya factorized approximation; hence, the choice of divergence informs whichmeasure, if any, is correctly estimated. Our analysis covers the KL divergence,the R\'enyi divergences, and a score-based divergence that compares $\nabla\logp$ and $\nabla\log q$. We empirically evaluate whether these orderings holdwhen VI is used to approximate non-Gaussian distributions.</description><author>Charles C. Margossian, Loucas Pillaud-Vivien, Lawrence K. Saul</author><pubDate>Wed, 20 Mar 2024 17:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13748v1</guid></item><item><title>Leveraging High-Resolution Features for Improved Deep Hashing-based Image Retrieval</title><link>http://arxiv.org/abs/2403.13747v1</link><description>Deep hashing techniques have emerged as the predominant approach forefficient image retrieval. Traditionally, these methods utilize pre-trainedconvolutional neural networks (CNNs) such as AlexNet and VGG-16 as featureextractors. However, the increasing complexity of datasets poses challenges forthese backbone architectures in capturing meaningful features essential foreffective image retrieval. In this study, we explore the efficacy of employinghigh-resolution features learned through state-of-the-art techniques for imageretrieval tasks. Specifically, we propose a novel methodology that utilizesHigh-Resolution Networks (HRNets) as the backbone for the deep hashing task,termed High-Resolution Hashing Network (HHNet). Our approach demonstratessuperior performance compared to existing methods across all tested benchmarkdatasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performanceimprovement is more pronounced for complex datasets, which highlights the needto learn high-resolution features for intricate image retrieval tasks.Furthermore, we conduct a comprehensive analysis of different HRNetconfigurations and provide insights into the optimal architecture for the deephashing task</description><author>Aymene Berriche, Mehdi Adjal Zakaria, Riyadh Baghdadi</author><pubDate>Wed, 20 Mar 2024 17:54:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13747v1</guid></item><item><title>Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation</title><link>http://arxiv.org/abs/2403.13745v1</link><description>Video outpainting is a challenging task, aiming at generating video contentoutside the viewport of the input video while maintaining inter-frame andintra-frame consistency. Existing methods fall short in either generationquality or flexibility. We introduce MOTIA Mastering Video Outpainting ThroughInput-Specific Adaptation, a diffusion-based pipeline that leverages both theintrinsic data-specific patterns of the source video and the image/videogenerative prior for effective outpainting. MOTIA comprises two main phases:input-specific adaptation and pattern-aware outpainting. The input-specificadaptation phase involves conducting efficient and effective pseudo outpaintinglearning on the single-shot source video. This process encourages the model toidentify and learn patterns within the source video, as well as bridging thegap between standard generative processes and outpainting. The subsequentphase, pattern-aware outpainting, is dedicated to the generalization of theselearned patterns to generate outpainting outcomes. Additional strategiesincluding spatial-aware insertion and noise travel are proposed to betterleverage the diffusion model's generative prior and the acquired video patternsfrom source videos. Extensive evaluations underscore MOTIA's superiority,outperforming existing state-of-the-art methods in widely recognizedbenchmarks. Notably, these advancements are achieved without necessitatingextensive, task-specific tuning.</description><author>Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, Guanglu Song, Yu Liu, Hongsheng Li</author><pubDate>Wed, 20 Mar 2024 17:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13745v1</guid></item><item><title>Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels</title><link>http://arxiv.org/abs/2302.05666v5</link><description>Intersection over Union (IoU) losses are surrogates that directly optimizethe Jaccard index. Leveraging IoU losses as part of the loss function havedemonstrated superior performance in semantic segmentation tasks compared tooptimizing pixel-wise losses such as the cross-entropy loss alone. However, weidentify a lack of flexibility in these losses to support vital trainingtechniques like label smoothing, knowledge distillation, and semi-supervisedlearning, mainly due to their inability to process soft labels. To addressthis, we introduce Jaccard Metric Losses (JMLs), which are identical to thesoft Jaccard loss in standard settings with hard labels but are fullycompatible with soft labels. We apply JMLs to three prominent use cases of softlabels: label smoothing, knowledge distillation and semi-supervised learning,and demonstrate their potential to enhance model accuracy and calibration. Ourexperiments show consistent improvements over the cross-entropy loss across 4semantic segmentation datasets (Cityscapes, PASCAL VOC, ADE20K, DeepGlobe Land)and 13 architectures, including classic CNNs and recent vision transformers.Remarkably, our straightforward approach significantly outperformsstate-of-the-art knowledge distillation and semi-supervised learning methods.The code is available at\href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.</description><author>Zifu Wang, Xuefei Ning, Matthew B. Blaschko</author><pubDate>Wed, 20 Mar 2024 17:50:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05666v5</guid></item><item><title>Hyper Strategy Logic</title><link>http://arxiv.org/abs/2403.13741v1</link><description>Strategy logic (SL) is a powerful temporal logic that enables strategicreasoning in multi-agent systems. SL supports explicit (first-order)quantification over strategies and provides a logical framework to express manyimportant properties such as Nash equilibria, dominant strategies, etc. Whilein SL the same strategy can be used in multiple strategy profiles, each suchprofile is evaluated w.r.t. a path-property, i.e., a property that considersthe single path resulting from a particular strategic interaction. In thispaper, we present Hyper Strategy Logic (HyperSL), a strategy logic where theoutcome of multiple strategy profiles can be compared w.r.t. a hyperproperty,i.e., a property that relates multiple paths. We show that HyperSL can captureimportant properties that cannot be expressed in SL, includingnon-interference, quantitative Nash equilibria, optimal adversarial planning,and reasoning under imperfect information. On the algorithmic side, we identifyan expressive fragment of HyperSL with decidable model checking and present amodel-checking algorithm. We contribute a prototype implementation of ouralgorithm and report on encouraging experimental results.</description><author>Raven Beutner, Bernd Finkbeiner</author><pubDate>Wed, 20 Mar 2024 17:47:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13741v1</guid></item><item><title>Uncertainty-Aware Explanations Through Probabilistic Self-Explainable Neural Networks</title><link>http://arxiv.org/abs/2403.13740v1</link><description>The lack of transparency of Deep Neural Networks continues to be a limitationthat severely undermines their reliability and usage in high-stakesapplications. Promising approaches to overcome such limitations arePrototype-Based Self-Explainable Neural Networks (PSENNs), whose predictionsrely on the similarity between the input at hand and a set of prototypicalrepresentations of the output classes, offering therefore a deep, yettransparent-by-design, architecture. So far, such models have been designed byconsidering pointwise estimates for the prototypes, which remain fixed afterthe learning phase of the model. In this paper, we introduce a probabilisticreformulation of PSENNs, called Prob-PSENN, which replaces point estimates forthe prototypes with probability distributions over their values. This providesnot only a more flexible framework for an end-to-end learning of prototypes,but can also capture the explanatory uncertainty of the model, which is amissing feature in previous approaches. In addition, since the prototypesdetermine both the explanation and the prediction, Prob-PSENNs allow us todetect when the model is making uninformed or uncertain predictions, and toobtain valid explanations for them. Our experiments demonstrate thatProb-PSENNs provide more meaningful and robust explanations than theirnon-probabilistic counterparts, thus enhancing the explainability andreliability of the models.</description><author>Jon Vadillo, Roberto Santana, Jose A. Lozano, Marta Kwiatkowska</author><pubDate>Wed, 20 Mar 2024 17:47:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13740v1</guid></item><item><title>Roto-translated Local Coordinate Frames For Interacting Dynamical Systems</title><link>http://arxiv.org/abs/2110.14961v3</link><description>Modelling interactions is critical in learning complex dynamical systems,namely systems of interacting objects with highly non-linear and time-dependentbehaviour. A large class of such systems can be formalized as$\textit{geometric graphs}$, $\textit{i.e.}$, graphs with nodes positioned inthe Euclidean space given an $\textit{arbitrarily}$ chosen global coordinatesystem, for instance vehicles in a traffic scene. Notwithstanding the arbitraryglobal coordinate system, the governing dynamics of the respective dynamicalsystems are invariant to rotations and translations, also known as$\textit{Galilean invariance}$. As ignoring these invariances leads to worsegeneralization, in this work we propose local coordinate frames per node-objectto induce roto-translation invariance to the geometric graph of the interactingdynamical system. Further, the local coordinate frames allow for a naturaldefinition of anisotropic filtering in graph neural networks. Experiments intraffic scenes, 3D motion capture, and colliding particles demonstrate that theproposed approach comfortably outperforms the recent state-of-the-art.</description><author>Miltiadis Kofinas, Naveen Shankar Nagaraja, Efstratios Gavves</author><pubDate>Wed, 20 Mar 2024 17:45:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.14961v3</guid></item><item><title>EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation</title><link>http://arxiv.org/abs/2403.13737v1</link><description>Large language models (LLMs) have gained popularity recently due to theiroutstanding performance in various downstream Natural Language Processing (NLP)tasks. However, low-resource languages are still lagging behind currentstate-of-the-art (SOTA) developments in the field of NLP due to insufficientresources to train LLMs. Ethiopian languages exhibit remarkable linguisticdiversity, encompassing a wide array of scripts, and are imbued with profoundreligious and cultural significance. This paper introduces EthioLLM --multilingual large language models for five Ethiopian languages (Amharic,Ge'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- anew benchmark dataset for various downstream NLP tasks. We evaluate theperformance of these models across five downstream NLP tasks. We open-sourceour multilingual language models, new benchmark datasets for various downstreamtasks, and task-specific fine-tuned language models and discuss the performanceof the models. Our dataset and models are available at thehttps://huggingface.co/EthioNLP repository.</description><author>Atnafu Lambebo Tonja, Israel Abebe Azime, Tadesse Destaw Belay, Mesay Gemeda Yigezu, Moges Ahmed Mehamed, Abinew Ali Ayele, Ebrahim Chekol Jibril, Michael Melese Woldeyohannis, Olga Kolesnikova, Philipp Slusallek, Dietrich Klakow, Shengwu Xiong, Seid Muhie Yimam</author><pubDate>Wed, 20 Mar 2024 17:43:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13737v1</guid></item><item><title>Enhancing Phrase Representation by Information Bottleneck Guided Text Diffusion Process for Keyphrase Extraction</title><link>http://arxiv.org/abs/2308.08739v2</link><description>Keyphrase extraction (KPE) is an important task in Natural LanguageProcessing for many scenarios, which aims to extract keyphrases that arepresent in a given document. Many existing supervised methods treat KPE assequential labeling, span-level classification, or generative tasks. However,these methods lack the ability to utilize keyphrase information, which mayresult in biased results. In this study, we propose Diff-KPE, which leveragesthe supervised Variational Information Bottleneck (VIB) to guide the textdiffusion process for generating enhanced keyphrase representations. Diff-KPEfirst generates the desired keyphrase embeddings conditioned on the entiredocument and then injects the generated keyphrase embeddings into each phraserepresentation. A ranking network and VIB are then optimized together with rankloss and classification loss, respectively. This design of Diff-KPE allows usto rank each candidate phrase by utilizing both the information of keyphrasesand the document. Experiments show that Diff-KPE outperforms existing KPEmethods on a large open domain keyphrase extraction benchmark, OpenKP, and ascientific domain dataset, KP20K.</description><author>Yuanzhen Luo, Qingyu Zhou, Feng Zhou</author><pubDate>Wed, 20 Mar 2024 17:41:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08739v2</guid></item><item><title>Reinforcement Learning for Online Testing of Autonomous Driving Systems: a Replication and Extension Study</title><link>http://arxiv.org/abs/2403.13729v1</link><description>In a recent study, Reinforcement Learning (RL) used in combination withmany-objective search, has been shown to outperform alternative techniques(random search and many-objective search) for online testing of Deep NeuralNetwork-enabled systems. The empirical evaluation of these techniques wasconducted on a state-of-the-art Autonomous Driving System (ADS). This work is areplication and extension of that empirical study. Our replication shows thatRL does not outperform pure random test generation in a comparison conductedunder the same settings of the original study, but with no confounding factorcoming from the way collisions are measured. Our extension aims at eliminatingsome of the possible reasons for the poor performance of RL observed in ourreplication: (1) the presence of reward components providing contrasting oruseless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning)which requires discretization of an intrinsically continuous state space.Results show that our new RL agent is able to converge to an effective policythat outperforms random testing. Results also highlight other possibleimprovements, which open to further investigations on how to best leverage RLfor online ADS testing.</description><author>Luca Giamattei, Matteo Biagiola, Roberto Pietrantuono, Stefano Russo, Paolo Tonella</author><pubDate>Wed, 20 Mar 2024 17:39:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13729v1</guid></item><item><title>M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via Multiplier Induced Loss Landscape Scheduling</title><link>http://arxiv.org/abs/2403.13728v1</link><description>When a neural network parameterized loss function consists of many terms, thecombinatorial choice of weight multipliers during the optimization processforms a challenging problem. To address this, we proposed a probabilisticgraphical model (PGM) for the joint model parameter and multiplier evolutionprocess, with a hypervolume based likelihood that promotes multi-objectivedescent of each loss term. The corresponding parameter and multiplierestimation as a sequential decision process is then cast into an optimalcontrol problem, where the multi-objective descent goal is dispatchedhierarchically into a series of constraint optimization sub-problems. Thesub-problem constraint automatically adapts itself according to Paretodominance and serves as the setpoint for the low level multiplier controller toschedule loss landscapes via output feedback of each loss term. Our method ismultiplier-free and operates at the timescale of epochs, thus saves tremendouscomputational resources compared to full training cycle multiplier tuning. Weapplied it to domain invariant variational auto-encoding with 6 loss terms onthe PACS domain generalization task, and observed robust performance across arange of controller hyperparameters, as well as different multiplier initialconditions, outperforming other multiplier scheduling methods. We offeredmodular implementation of our method, admitting custom definition of many lossterms for applying our multi-objective hierarchical output feedback trainingscheme to other deep learning fields.</description><author>Xudong Sun, Nutan Chen, Alexej Gossmann, Yu Xing, Carla Feistner, Emilio Dorigatt, Felix Drost, Daniele Scarcella, Lisa Beer, Carsten Marr</author><pubDate>Wed, 20 Mar 2024 17:38:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13728v1</guid></item><item><title>AutoMix: Automatically Mixing Language Models</title><link>http://arxiv.org/abs/2310.12963v3</link><description>Large language models (LLMs) are now available from cloud API providers invarious sizes and configurations. While this diversity offers a broad spectrumof choices, effectively leveraging the options to optimize computational costand performance remains challenging. In this work, we present AutoMix, anapproach that strategically routes queries to larger LMs, based on theapproximate correctness of outputs from a smaller LM. Central to AutoMix is afew-shot self-verification mechanism, which estimates the reliability of itsown outputs without requiring training. Given that verifications can be noisy,we employ a meta-verifier in AutoMix to refine the accuracy of theseassessments. Our experiments using LLAMA2-13B and GPT-4, on fivecontext-grounded reasoning datasets demonstrate that AutoMix surpassesestablished baselines, improving the incremental benefit per cost by up to 86%.Our code and data are available at https://github.com/automix-llm/automix.</description><author>Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Mausam, Manaal Faruqui</author><pubDate>Wed, 20 Mar 2024 17:36:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12963v3</guid></item><item><title>Learning Algorithms for Verification of Markov Decision Processes</title><link>http://arxiv.org/abs/2403.09184v2</link><description>We present a general framework for applying learning algorithms andheuristical guidance to the verification of Markov decision processes (MDPs).The primary goal of our techniques is to improve performance by avoiding anexhaustive exploration of the state space, instead focussing on particularlyrelevant areas of the system, guided by heuristics. Our work builds on theprevious results of Br{\'{a}}zdil et al., significantly extending it as well asrefining several details and fixing errors. The presented framework focuses on probabilistic reachability, which is acore problem in verification, and is instantiated in two distinct scenarios.The first assumes that full knowledge of the MDP is available, in particularprecise transition probabilities. It performs a heuristic-driven partialexploration of the model, yielding precise lower and upper bounds on therequired probability. The second tackles the case where we may only sample theMDP without knowing the exact transition dynamics. Here, we obtainprobabilistic guarantees, again in terms of both the lower and upper bounds,which provides efficient stopping criteria for the approximation. Inparticular, the latter is an extension of statistical model-checking (SMC) forunbounded properties in MDPs. In contrast to other related approaches, we donot restrict our attention to time-bounded (finite-horizon) or discountedproperties, nor assume any particular structural properties of the MDP.</description><author>Tomáš Brázdil, Krishnendu Chatterjee, Martin Chmelik, Vojtěch Forejt, Jan Křetínský, Marta Kwiatkowska, Tobias Meggendorfer, David Parker, Mateusz Ujma</author><pubDate>Wed, 20 Mar 2024 17:34:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09184v2</guid></item><item><title>Deep Reinforcement Learning: A Convex Optimization Approach</title><link>http://arxiv.org/abs/2402.19212v3</link><description>In this paper, we consider reinforcement learning of nonlinear systems withcontinuous state and action spaces. We present an episodic learning algorithm,where we for each episode use convex optimization to find a two-layer neuralnetwork approximation of the optimal $Q$-function. The convex optimizationapproach guarantees that the weights calculated at each episode are optimal,with respect to the given sampled states and actions of the current episode.For stable nonlinear systems, we show that the algorithm converges and that theconverging parameters of the trained neural network can be made arbitrarilyclose to the optimal neural network parameters. In particular, if theregularization parameter is $\rho$ and the time horizon is $T$, then theparameters of the trained neural network converge to $w$, where the distancebetween $w$ from the optimal parameters $w^\star$ is bounded by$\mathcal{O}(\rho T^{-1})$. That is, when the number of episodes goes toinfinity, there exists a constant $C$ such that \[\|w-w^\star\| \leC\cdot\frac{\rho}{T}.\] In particular, our algorithm converges arbitrarilyclose to the optimal neural network parameters as the time horizon increases oras the regularization parameter decreases.</description><author>Ather Gattami</author><pubDate>Wed, 20 Mar 2024 17:34:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19212v3</guid></item><item><title>Probabilistic Forecasting with Stochastic Interpolants and Föllmer Processes</title><link>http://arxiv.org/abs/2403.13724v1</link><description>We propose a framework for probabilistic forecasting of dynamical systemsbased on generative modeling. Given observations of the system state over time,we formulate the forecasting problem as sampling from the conditionaldistribution of the future system state given its current state. To this end,we leverage the framework of stochastic interpolants, which facilitates theconstruction of a generative model between an arbitrary base distribution andthe target. We design a fictitious, non-physical stochastic dynamics that takesas initial condition the current system state and produces as output a samplefrom the target conditional distribution in finite time and without bias. Thisprocess therefore maps a point mass centered at the current state onto aprobabilistic ensemble of forecasts. We prove that the drift coefficiententering the stochastic differential equation (SDE) achieving this task isnon-singular, and that it can be learned efficiently by square loss regressionover the time-series data. We show that the drift and the diffusioncoefficients of this SDE can be adjusted after training, and that a specificchoice that minimizes the impact of the estimation error gives a F\"ollmerprocess. We highlight the utility of our approach on several complex,high-dimensional forecasting problems, including stochastically forcedNavier-Stokes and video prediction on the KTH and CLEVRER datasets.</description><author>Yifan Chen, Mark Goldstein, Mengjian Hua, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden</author><pubDate>Wed, 20 Mar 2024 17:33:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13724v1</guid></item><item><title>MCRAGE: Synthetic Healthcare Data for Fairness</title><link>http://arxiv.org/abs/2310.18430v3</link><description>In the field of healthcare, electronic health records (EHR) serve as crucialtraining data for developing machine learning models for diagnosis, treatment,and the management of healthcare resources. However, medical datasets are oftenimbalanced in terms of sensitive attributes such as race/ethnicity, gender, andage. Machine learning models trained on class-imbalanced EHR datasets performsignificantly worse in deployment for individuals of the minority classescompared to those from majority classes, which may lead to inequitablehealthcare outcomes for minority groups. To address this challenge, we proposeMinority Class Rebalancing through Augmentation by Generative modeling(MCRAGE), a novel approach to augment imbalanced datasets using samplesgenerated by a deep generative model. The MCRAGE process involves training aConditional Denoising Diffusion Probabilistic Model (CDDPM) capable ofgenerating high-quality synthetic EHR samples from underrepresented classes. Weuse this synthetic data to augment the existing imbalanced dataset, resultingin a more balanced distribution across all classes, which can be used to trainless biased downstream models. We measure the performance of MCRAGE versusalternative approaches using Accuracy, F1 score and AUROC of these downstreammodels. We provide theoretical justification for our method in terms of recentconvergence results for DDPMs.</description><author>Keira Behal, Jiayi Chen, Caleb Fikes, Sophia Xiao</author><pubDate>Wed, 20 Mar 2024 17:32:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18430v3</guid></item><item><title>Large Language Models meet Network Slicing Management and Orchestration</title><link>http://arxiv.org/abs/2403.13721v1</link><description>Network slicing, a cornerstone technology for future networks, enables thecreation of customized virtual networks on a shared physical infrastructure.This fosters innovation and agility by providing dedicated resources tailoredto specific applications. However, current orchestration and managementapproaches face limitations in handling the complexity of new service demandswithin multi-administrative domain environments. This paper proposes a futurevision for network slicing powered by Large Language Models (LLMs) andmulti-agent systems, offering a framework that can be integrated with existingManagement and Orchestration (MANO) frameworks. This framework leverages LLMsto translate user intent into technical requirements, map network functions toinfrastructure, and manage the entire slice lifecycle, while multi-agentsystems facilitate collaboration across different administrative domains. Wealso discuss the challenges associated with implementing this framework andpotential solutions to mitigate them.</description><author>Abdulhalim Dandoush, Viswanath Kumarskandpriya, Mueen Uddin, Usman Khalil</author><pubDate>Wed, 20 Mar 2024 17:29:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13721v1</guid></item><item><title>Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time Rendering</title><link>http://arxiv.org/abs/2311.18561v2</link><description>Modeling dynamic, large-scale urban scenes is challenging due to their highlyintricate geometric structures and unconstrained dynamics in both space andtime. Prior methods often employ high-level architectural priors, separatingstatic and dynamic elements, resulting in suboptimal capture of theirsynergistic interactions. To address this challenge, we present a unifiedrepresentation model, called Periodic Vibration Gaussian (PVG). PVG builds uponthe efficient 3D Gaussian splatting technique, originally designed for staticscene representation, by introducing periodic vibration-based temporaldynamics. This innovation enables PVG to elegantly and uniformly represent thecharacteristics of various objects and elements in dynamic urban scenes. Toenhance temporally coherent and large scene representation learning with sparsetraining data, we introduce a novel temporal smoothing mechanism and aposition-aware adaptive control strategy respectively. Extensive experiments onWaymo Open Dataset and KITTI benchmarks demonstrate that PVG surpassesstate-of-the-art alternatives in both reconstruction and novel view synthesisfor both dynamic and static scenes. Notably, PVG achieves this without relyingon manually labeled object bounding boxes or expensive optical flow estimation.Moreover, PVG exhibits 900-fold acceleration in rendering over the bestalternative.</description><author>Yurui Chen, Chun Gu, Junzhe Jiang, Xiatian Zhu, Li Zhang</author><pubDate>Wed, 20 Mar 2024 17:27:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18561v2</guid></item><item><title>Normalizing flow-based deep variational Bayesian network for seismic multi-hazards and impacts estimation from InSAR imagery</title><link>http://arxiv.org/abs/2310.13805v2</link><description>Onsite disasters like earthquakes can trigger cascading hazards and impacts,such as landslides and infrastructure damage, leading to catastrophic losses;thus, rapid and accurate estimates are crucial for timely and effectivepost-disaster responses. Interferometric Synthetic aperture radar (InSAR) datais important in providing high-resolution onsite information for rapid hazardestimation. Most recent methods using InSAR imagery signals predict a singletype of hazard and thus often suffer low accuracy due to noisy and complexsignals induced by co-located hazards, impacts, and irrelevant environmentalchanges (e.g., vegetation changes, human activities). We introduce a novelstochastic variational inference with normalizing flows derived to jointlyapproximate posteriors of multiple unobserved hazards and impacts from noisyInSAR imagery.</description><author>Xuechun Li, Paula M. Burgi, Wei Ma, Hae Young Noh, David J. Wald, Susu Xu</author><pubDate>Wed, 20 Mar 2024 17:23:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13805v2</guid></item><item><title>Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with Wavelet Augmentation Transformer</title><link>http://arxiv.org/abs/2303.17783v5</link><description>Unsupervised Domain Adaptation (UDA) can effectively address domain gapissues in real-world image Super-Resolution (SR) by accessing both the sourceand target data. Considering privacy policies or transmission restrictions ofsource data in practical scenarios, we propose a SOurce-free Domain Adaptationframework for image SR (SODA-SR) to address this issue, i.e., adapt asource-trained model to a target domain with only unlabeled target data.SODA-SR leverages the source-trained model to generate refined pseudo-labelsfor teacher-student learning. To better utilize pseudo-labels, we propose anovel wavelet-based augmentation method, named Wavelet Augmentation Transformer(WAT), which can be flexibly incorporated with existing networks, to implicitlyproduce useful augmented data. WAT learns low-frequency information of varyinglevels across diverse samples, which is aggregated efficiently via deformableattention. Furthermore, an uncertainty-aware self-training mechanism isproposed to improve the accuracy of pseudo-labels, with inaccurate predictionsbeing rectified by uncertainty estimation. To acquire better SR results andavoid overfitting pseudo-labels, several regularization losses are proposed toconstrain target LR and SR images in the frequency domain. Experiments showthat without accessing source data, SODA-SR outperforms state-of-the-art UDAmethods in both synthetic$\rightarrow$real and real$\rightarrow$real adaptationsettings, and is not constrained by specific network architectures.</description><author>Yuang Ai, Xiaoqiang Zhou, Huaibo Huang, Lei Zhang, Ran He</author><pubDate>Wed, 20 Mar 2024 17:21:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17783v5</guid></item><item><title>DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with Multiple Sensors for Large-Scale Localization and Mapping</title><link>http://arxiv.org/abs/2403.13714v1</link><description>Visual simultaneous localization and mapping (VSLAM) has broad applications,with state-of-the-art methods leveraging deep neural networks for betterrobustness and applicability. However, there is a lack of research in fusingthese learning-based methods with multi-sensor information, which could beindispensable to push related applications to large-scale and complexscenarios. In this paper, we tightly integrate the trainable deep dense bundleadjustment (DBA) with multi-sensor information through a factor graph. In theframework, recurrent optical flow and DBA are performed among sequentialimages. The Hessian information derived from DBA is fed into a generic factorgraph for multi-sensor fusion, which employs a sliding window and supportsprobabilistic marginalization. A pipeline for visual-inertial integration isfirstly developed, which provides the minimum ability of metric-scalelocalization and mapping. Furthermore, other sensors (e.g., global navigationsatellite system) are integrated for driftless and geo-referencingfunctionality. Extensive tests are conducted on both public datasets andself-collected datasets. The results validate the superior localizationperformance of our approach, which enables real-time dense mapping inlarge-scale environments. The code has been made open-source(https://github.com/GREAT-WHU/DBA-Fusion).</description><author>Yuxuan Zhou, Xingxing Li, Shengyu Li, Xuanbin Wang, Shaoquan Feng, Yuxuan Tan</author><pubDate>Wed, 20 Mar 2024 17:20:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13714v1</guid></item><item><title>Towards Architecture-Agnostic Untrained Network Priors for Image Reconstruction with Frequency Regularization</title><link>http://arxiv.org/abs/2312.09988v2</link><description>Untrained networks inspired by deep image prior have shown promisingcapabilities in recovering a high-quality image from noisy or partialmeasurements, without requiring training data. Their success has been widelyattributed to the spectral bias acting as an implicit regularization induced bysuitable network architectures. However, applications of such network-basedpriors often entail superfluous architectural decisions, overfitting risks, andslow optimization, all of which hinder their practicality. In this work, wepropose efficient, architecture-agnostic methods for a more direct frequencycontrol over the network priors: 1) constraining the bandwidth of thewhite-noise input, 2) controlling the bandwidth of the interpolation-basedupsamplers, and 3) regularizing the Lipschitz constants of the layers. We showthat even with just one extra line of code, the overfitting issues inunderperforming architectures can be alleviated such that their performancegaps with the high-performing counterparts can be largely closed despite theirdistinct configurations, mitigating the need for architecture tuning. This thenmakes it possible to employ a more compact model to achieve similar or superiorperformance to larger models with greater efficiency. Our regularized networkpriors compare favorably with current supervised and self-supervised methods onMRI reconstruction and image inpainting tasks, serving as a stronger zero-shotbaseline reconstructor. Our code will be made publicly available.</description><author>Yilin Liu, Yunkui Pang, Jiang Li, Yong Chen, Pew-Thian Yap</author><pubDate>Wed, 20 Mar 2024 17:19:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09988v2</guid></item><item><title>Simple Semantic-Aided Few-Shot Learning</title><link>http://arxiv.org/abs/2311.18649v2</link><description>Learning from a limited amount of data, namely Few-Shot Learning, stands outas a challenging computer vision task. Several works exploit semantics anddesign complicated semantic fusion mechanisms to compensate for rarerepresentative features within restricted data. However, relying on naivesemantics such as class names introduces biases due to their brevity, whileacquiring extensive semantics from external knowledge takes a huge time andeffort. This limitation severely constrains the potential of semantics infew-shot learning. In this paper, we design an automatic way called SemanticEvolution to generate high-quality semantics. The incorporation of high-qualitysemantics alleviates the need for complex network structures and learningalgorithms used in previous works. Hence, we employ a simple two-layer networktermed Semantic Alignment Network to transform semantics and visual featuresinto robust class prototypes with rich discriminative features for few-shotclassification. The experimental results show our framework outperforms allprevious methods on six benchmarks, demonstrating a simple network withhigh-quality semantics can beat intricate multi-modal modules on few-shotclassification tasks. Code is available athttps://github.com/zhangdoudou123/SemFew.</description><author>Hai Zhang, Junzhe Xu, Shanlin Jiang, Zhenan He</author><pubDate>Wed, 20 Mar 2024 17:18:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18649v2</guid></item><item><title>MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning</title><link>http://arxiv.org/abs/2309.07915v3</link><description>Since the resurgence of deep learning, vision-language models (VLMs) enhancedby large language models (LLMs) have grown exponentially in popularity.However, while LLMs can utilize extensive background knowledge and taskinformation with in-context learning, most VLMs still struggle withunderstanding complex multi-modal prompts with multiple images, making VLMsless effective in downstream vision-language tasks. In this paper, we addressthe limitation above by 1) introducing vision-language Model with Multi-ModalIn-Context Learning(MMICL), a new approach to allow the VLM to deal withmulti-modal inputs efficiently; 2) proposing a novel context scheme to augmentthe in-context learning ability of the VLM; 3) constructing the Multi-modalIn-Context Learning (MIC) dataset, designed to enhance the VLM's ability tounderstand complex multi-modal prompts. Our experiments confirm that MMICLachieves new state-of-the-art zero-shot performance on a wide range of generalvision-language tasks, especially for complex benchmarks, including MME andMMBench. Our analysis demonstrates that MMICL effectively tackles the challengeof complex multi-modal prompt understanding and emerges the impressive ICLability. Furthermore, we observe that MMICL successfully alleviates languagebias in VLMs, a common issue for VLMs that often leads to hallucination whenfaced with extensive textual context. Our code, dataset, dataset tool, andmodel are available at https://github.com/PKUnlp-icler/MIC</description><author>Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang</author><pubDate>Wed, 20 Mar 2024 17:17:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07915v3</guid></item><item><title>Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration</title><link>http://arxiv.org/abs/2312.02918v2</link><description>Despite substantial progress, all-in-one image restoration (IR) grapples withpersistent challenges in handling intricate real-world degradations. This paperintroduces MPerceiver: a novel multimodal prompt learning approach thatharnesses Stable Diffusion (SD) priors to enhance adaptiveness,generalizability and fidelity for all-in-one image restoration. Specifically,we develop a dual-branch module to master two types of SD prompts: textual forholistic representation and visual for multiscale detail representation. Bothprompts are dynamically adjusted by degradation predictions from the CLIP imageencoder, enabling adaptive responses to diverse unknown degradations. Moreover,a plug-in detail refinement module improves restoration fidelity via directencoder-to-decoder information transformation. To assess our method, MPerceiveris trained on 9 tasks for all-in-one IR and outperforms state-of-the-arttask-specific methods across most tasks. Post multitask pre-training,MPerceiver attains a generalized representation in low-level vision, exhibitingremarkable zero-shot and few-shot capabilities in unseen tasks. Extensiveexperiments on 16 IR tasks underscore the superiority of MPerceiver in terms ofadaptiveness, generalizability and fidelity.</description><author>Yuang Ai, Huaibo Huang, Xiaoqiang Zhou, Jiexiang Wang, Ran He</author><pubDate>Wed, 20 Mar 2024 17:12:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02918v2</guid></item><item><title>Graph Neural Networks for Learning Equivariant Representations of Neural Networks</title><link>http://arxiv.org/abs/2403.12143v2</link><description>Neural networks that process the parameters of other neural networks findapplications in domains as diverse as classifying implicit neuralrepresentations, generating neural network weights, and predictinggeneralization errors. However, existing approaches either overlook theinherent permutation symmetry in the neural network or rely on intricateweight-sharing patterns to achieve equivariance, while ignoring the impact ofthe network architecture itself. In this work, we propose to represent neuralnetworks as computational graphs of parameters, which allows us to harnesspowerful graph neural networks and transformers that preserve permutationsymmetry. Consequently, our approach enables a single model to encode neuralcomputational graphs with diverse architectures. We showcase the effectivenessof our method on a wide range of tasks, including classification and editing ofimplicit neural representations, predicting generalization performance, andlearning to optimize, while consistently outperforming state-of-the-artmethods. The source code is open-sourced athttps://github.com/mkofinas/neural-graphs.</description><author>Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J. Burghouts, Efstratios Gavves, Cees G. M. Snoek, David W. Zhang</author><pubDate>Wed, 20 Mar 2024 17:12:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12143v2</guid></item><item><title>Auto-Vocabulary Semantic Segmentation</title><link>http://arxiv.org/abs/2312.04539v2</link><description>Open-ended image understanding tasks gained significant attention from theresearch community, particularly with the emergence of Vision-Language Models.Open-Vocabulary Segmentation (OVS) methods are capable of performing semanticsegmentation without relying on a fixed vocabulary, and in some cases, theyoperate without the need for training or fine-tuning. However, OVS methodstypically require users to specify the vocabulary based on the task or datasetat hand. In this paper, we introduce \textit{Auto-Vocabulary SemanticSegmentation (AVS)}, advancing open-ended image understanding by eliminatingthe necessity to predefine object categories for segmentation. Our approach,\ours, presents a framework that autonomously identifies relevant class namesusing enhanced BLIP embeddings, which are utilized for segmentation afterwards.Given that open-ended object category predictions cannot be directly comparedwith a fixed ground truth, we develop a Large Language Model-basedAuto-Vocabulary Evaluator (LAVE) to efficiently evaluate the automaticallygenerated class names and their corresponding segments. Our method sets newbenchmarks on datasets such as PASCAL VOC and Context, ADE20K, and Cityscapesfor AVS and showcases competitive performance to OVS methods that requirespecified class names.</description><author>Osman Ülger, Maksymilian Kulicki, Yuki Asano, Martin R. Oswald</author><pubDate>Wed, 20 Mar 2024 17:11:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04539v2</guid></item><item><title>CoNeS: Conditional neural fields with shift modulation for multi-sequence MRI translation</title><link>http://arxiv.org/abs/2309.03320v3</link><description>Multi-sequence magnetic resonance imaging (MRI) has found wide applicationsin both modern clinical studies and deep learning research. However, inclinical practice, it frequently occurs that one or more of the MRI sequencesare missing due to different image acquisition protocols or contrast agentcontraindications of patients, limiting the utilization of deep learning modelstrained on multi-sequence data. One promising approach is to leveragegenerative models to synthesize the missing sequences, which can serve as asurrogate acquisition. State-of-the-art methods tackling this problem are basedon convolutional neural networks (CNN) which usually suffer from spectralbiases, resulting in poor reconstruction of high-frequency fine details. Inthis paper, we propose Conditional Neural fields with Shift modulation (CoNeS),a model that takes voxel coordinates as input and learns a representation ofthe target images for multi-sequence MRI translation. The proposed model uses amulti-layer perceptron (MLP) instead of a CNN as the decoder for pixel-to-pixelmapping. Hence, each target image is represented as a neural field that isconditioned on the source image via shift modulation with a learned latentcode. Experiments on BraTS 2018 and an in-house clinical dataset of vestibularschwannoma patients showed that the proposed method outperformedstate-of-the-art methods for multi-sequence MRI translation both visually andquantitatively. Moreover, we conducted spectral analysis, showing that CoNeSwas able to overcome the spectral bias issue common in conventional CNN models.To further evaluate the usage of synthesized images in clinical downstreamtasks, we tested a segmentation network using the synthesized images atinference.</description><author>Yunjie Chen, Marius Staring, Olaf M. Neve, Stephan R. Romeijn, Erik F. Hensen, Berit M. Verbist, Jelmer M. Wolterink, Qian Tao</author><pubDate>Wed, 20 Mar 2024 17:10:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03320v3</guid></item><item><title>Research Re: search &amp; Re-search</title><link>http://arxiv.org/abs/2403.13705v1</link><description>Search algorithms are often categorized by their node expansion strategy. Oneoption is the depth-first strategy, a simple backtracking strategy thattraverses the search space in the order in which successor nodes are generated.An alternative is the best-first strategy, which was designed to make itpossible to use domain-specific heuristic information. By exploring promisingparts of the search space first, best-first algorithms are usually moreefficient than depth-first algorithms. In programs that play minimax games such as chess and checkers, theefficiency of the search is of crucial importance. Given the success ofbest-first algorithms in other domains, one would expect them to be used forminimax games too. However, all high-performance game-playing programs arebased on a depth-first algorithm. This study takes a closer look at a depth-first algorithm, AB, and abest-first algorithm, SSS. The prevailing opinion on these algorithms is thatSSS offers the potential for a more efficient search, but that its complicatedformulation and exponential memory requirements render it impractical. Thetheoretical part of this work shows that there is a surprisinglystraightforward link between the two algorithms -- for all practical purposes,SSS is a special case of AB. Subsequent empirical evidence proves theprevailing opinion on SSS to be wrong: it is not a complicated algorithm, itdoes not need too much memory, and it is also not more efficient thandepth-first search.</description><author>Aske Plaat</author><pubDate>Wed, 20 Mar 2024 17:08:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13705v1</guid></item><item><title>Improving the Adaptive Moment Estimation (ADAM) stochastic optimizer through an Implicit-Explicit (IMEX) time-stepping approach</title><link>http://arxiv.org/abs/2403.13704v1</link><description>The Adam optimizer, often used in Machine Learning for neural networktraining, corresponds to an underlying ordinary differential equation (ODE) inthe limit of very small learning rates. This work shows that the classical Adamalgorithm is a first order implicit-explicit (IMEX) Euler discretization of theunderlying ODE. Employing the time discretization point of view, we propose newextensions of the Adam scheme obtained by using higher order IMEX methods tosolve the ODE. Based on this approach, we derive a new optimization algorithmfor neural network training that performs better than classical Adam on severalregression and classification problems.</description><author>Abhinab Bhattacharjee, Andrey A. Popov, Arash Sarshar, Adrian Sandu</author><pubDate>Wed, 20 Mar 2024 17:08:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13704v1</guid></item><item><title>Fostc3net:A Lightweight YOLOv5 Based On the Network Structure Optimization</title><link>http://arxiv.org/abs/2403.13703v1</link><description>Transmission line detection technology is crucial for automatic monitoringand ensuring the safety of electrical facilities. The YOLOv5 series iscurrently one of the most advanced and widely used methods for objectdetection. However, it faces inherent challenges, such as high computationalload on devices and insufficient detection accuracy. To address these concerns,this paper presents an enhanced lightweight YOLOv5 technique customized formobile devices, specifically intended for identifying objects associated withtransmission lines. The C3Ghost module is integrated into the convolutionalnetwork of YOLOv5 to reduce floating point operations per second (FLOPs) in thefeature channel fusion process and improve feature expression performance. Inaddition, a FasterNet module is introduced to replace the c3 module in theYOLOv5 Backbone. The FasterNet module uses Partial Convolutions to process onlya portion of the input channels, improving feature extraction efficiency andreducing computational overhead. To address the imbalance between simple andchallenging samples in the dataset and the diversity of aspect ratios ofbounding boxes, the wIoU v3 LOSS is adopted as the loss function. To validatethe performance of the proposed approach, Experiments are conducted on a customdataset of transmission line poles. The results show that the proposed modelachieves a 1% increase in detection accuracy, a 13% reduction in FLOPs, and a26% decrease in model parameters compared to the existing YOLOv5.In theablation experiment, it was also discovered that while the Fastnet module andthe CSghost module improved the precision of the original YOLOv5 baselinemodel, they caused a decrease in the mAP@.5-.95 metric. However, theimprovement of the wIoUv3 loss function significantly mitigated the decline ofthe mAP@.5-.95 metric.</description><author>Danqing Ma, Shaojie Li, Bo Dang, Hengyi Zang, Xinqi Dong</author><pubDate>Wed, 20 Mar 2024 17:07:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13703v1</guid></item><item><title>What Matters for Active Texture Recognition With Vision-Based Tactile Sensors</title><link>http://arxiv.org/abs/2403.13701v1</link><description>This paper explores active sensing strategies that employ vision-basedtactile sensors for robotic perception and classification of fabric textures.We formalize the active sampling problem in the context of tactile fabricrecognition and provide an implementation of information-theoretic explorationstrategies based on minimizing predictive entropy and variance of probabilisticmodels. Through ablation studies and human experiments, we investigate whichcomponents are crucial for quick and reliable texture recognition. Along withthe active sampling strategies, we evaluate neural network architectures,representations of uncertainty, influence of data augmentation, and datasetvariability. By evaluating our method on a previously published Active ClothingPerception Dataset and on a real robotic system, we establish that the choiceof the active exploration strategy has only a minor influence on therecognition accuracy, whereas data augmentation and dropout rate play asignificantly larger role. In a comparison study, while humans achieve 66.9%recognition accuracy, our best approach reaches 90.0% in under 5 touches,highlighting that vision-based tactile sensors are highly effective for fabrictexture recognition.</description><author>Alina Böhm, Tim Schneider, Boris Belousov, Alap Kshirsagar, Lisa Lin, Katja Doerschner, Knut Drewing, Constantin A. Rothkopf, Jan Peters</author><pubDate>Wed, 20 Mar 2024 17:06:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13701v1</guid></item><item><title>Latent Field Discovery In Interacting Dynamical Systems With Neural Fields</title><link>http://arxiv.org/abs/2310.20679v2</link><description>Systems of interacting objects often evolve under the influence of fieldeffects that govern their dynamics, yet previous works have abstracted awayfrom such effects, and assume that systems evolve in a vacuum. In this work, wefocus on discovering these fields, and infer them from the observed dynamicsalone, without directly observing them. We theorize the presence of latentforce fields, and propose neural fields to learn them. Since the observeddynamics constitute the net effect of local object interactions and globalfield effects, recently popularized equivariant networks are inapplicable, asthey fail to capture global information. To address this, we propose todisentangle local object interactions -- which are $\mathrm{SE}(n)$ equivariantand depend on relative states -- from external global field effects -- whichdepend on absolute states. We model interactions with equivariant graphnetworks, and combine them with neural fields in a novel graph network thatintegrates field forces. Our experiments show that we can accurately discoverthe underlying fields in charged particles settings, traffic scenes, andgravitational n-body problems, and effectively use them to learn the system andforecast future trajectories.</description><author>Miltiadis Kofinas, Erik J. Bekkers, Naveen Shankar Nagaraja, Efstratios Gavves</author><pubDate>Wed, 20 Mar 2024 17:05:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.20679v2</guid></item><item><title>Insight Into the Collocation of Multi-Source Satellite Imagery for Multi-Scale Vessel Detection</title><link>http://arxiv.org/abs/2403.13698v1</link><description>Ship detection from satellite imagery using Deep Learning (DL) is anindispensable solution for maritime surveillance. However, applying DL modelstrained on one dataset to others having differences in spatial resolution andradiometric features requires many adjustments. To overcome this issue, thispaper focused on the DL models trained on datasets that consist of differentoptical images and a combination of radar and optical data. When dealing with alimited number of training images, the performance of DL models via thisapproach was satisfactory. They could improve 5-20% of average precision,depending on the optical images tested. Likewise, DL models trained on thecombined optical and radar dataset could be applied to both optical and radarimages. Our experiments showed that the models trained on an optical datasetcould be used for radar images, while those trained on a radar dataset offeredvery poor scores when applied to optical images.</description><author>Tran-Vu La, Minh-Tan Pham, Marco Chini</author><pubDate>Wed, 20 Mar 2024 17:03:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13698v1</guid></item><item><title>Loss Regularizing Robotic Terrain Classification</title><link>http://arxiv.org/abs/2403.13695v1</link><description>Locomotion mechanics of legged robots are suitable when pacing throughdifficult terrains. Recognising terrains for such robots are important to fullyyoke the versatility of their movements. Consequently, robotic terrainclassification becomes significant to classify terrains in real time with highaccuracy. The conventional classifiers suffer from overfitting problem, lowaccuracy problem, high variance problem, and not suitable for live dataset. Onthe other hand, classifying a growing dataset is difficult for convolutionbased terrain classification. Supervised recurrent models are also notpractical for this classification. Further, the existing recurrentarchitectures are still evolving to improve accuracy of terrain classificationbased on live variable-length sensory data collected from legged robots. Thispaper proposes a new semi-supervised method for terrain classification oflegged robots, avoiding preprocessing of long variable-length dataset. Theproposed method has a stacked Long Short-Term Memory architecture, including anew loss regularization. The proposed method solves the existing problems andimproves accuracy. Comparison with the existing architectures show theimprovements.</description><author>Shakti Deo Kumar, Sudhanshu Tripathi, Krishna Ujjwal, Sarvada Sakshi Jha, Suddhasil De</author><pubDate>Wed, 20 Mar 2024 16:57:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13695v1</guid></item><item><title>Correct Like Humans: Progressive Learning Framework for Chinese Text Error Correction</title><link>http://arxiv.org/abs/2306.17447v3</link><description>Chinese Text Error Correction (CTEC) aims to detect and correct errors in theinput text, which benefits human daily life and various downstream tasks.Recent approaches mainly employ Pre-trained Language Models (PLMs) to resolveCTEC. Although PLMs have achieved remarkable success in CTEC, we argue thatprevious studies still overlook the importance of human thinking patterns. Toenhance the development of PLMs for CTEC, inspired by humans' dailyerror-correcting behavior, we propose a novel model-agnostic progressivelearning framework, named ProTEC, which guides PLMs-based CTEC models to learnto correct like humans. During the training process, ProTEC guides the model tolearn text error correction by incorporating these sub-tasks into a progressiveparadigm. During the inference process, the model completes these sub-tasks inturn to generate the correction results. Extensive experiments and detailedanalyses demonstrate the effectiveness and efficiency of our proposedmodel-agnostic ProTEC framework.</description><author>Yinghui Li, Shirong Ma, Shaoshen Chen, Haojing Huang, Shulin Huang, Yangning Li, Hai-Tao Zheng, Ying Shen</author><pubDate>Wed, 20 Mar 2024 16:53:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17447v3</guid></item><item><title>MotorEase: Automated Detection of Motor Impairment Accessibility Issues in Mobile App UIs</title><link>http://arxiv.org/abs/2403.13690v1</link><description>Recent research has begun to examine the potential of automatically findingand fixing accessibility issues that manifest in software. However, whilerecent work makes important progress, it has generally been skewed towardidentifying issues that affect users with certain disabilities, such as thosewith visual or hearing impairments. However, there are other groups of userswith different types of disabilities that also need software tooling support toimprove their experience. As such, this paper aims to automatically identifyaccessibility issues that affect users with motor-impairments. To move toward this goal, this paper introduces a novel approach, calledMotorEase, capable of identifying accessibility issues in mobile app UIs thatimpact motor-impaired users. Motor-impaired users often have limited ability tointeract with touch-based devices, and instead may make use of a switch orother assistive mechanism -- hence UIs must be designed to support both limitedtouch gestures and the use of assistive devices. MotorEase adapts computervision and text processing techniques to enable a semantic understanding of appUI screens, enabling the detection of violations related to four popular,previously unexplored UI design guidelines that support motor-impaired users,including: (i) visual touch target size, (ii) expanding sections, (iii)persisting elements, and (iv) adjacent icon visual distance. We evaluateMotorEase on a newly derived benchmark, called MotorCheck, that contains 555manually annotated examples of violations to the above accessibilityguidelines, across 1599 screens collected from 70 applications via a mobile apptesting tool. Our experiments illustrate that MotorEase is able to identifyviolations with an average accuracy of ~90%, and a false positive rate of lessthan 9%, outperforming baseline techniques.</description><author>Arun Krishnavajjala, SM Hasan Mansur, Justin Jose, Kevin Moran</author><pubDate>Wed, 20 Mar 2024 16:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13690v1</guid></item><item><title>Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels</title><link>http://arxiv.org/abs/2303.16296v4</link><description>The soft Dice loss (SDL) has taken a pivotal role in numerous automatedsegmentation pipelines in the medical imaging community. Over the last years,some reasons behind its superior functioning have been uncovered and furtheroptimizations have been explored. However, there is currently no implementationthat supports its direct utilization in scenarios involving soft labels. Hence,a synergy between the use of SDL and research leveraging the use of softlabels, also in the context of model calibration, is still missing. In thiswork, we introduce Dice semimetric losses (DMLs), which (i) are by designidentical to SDL in a standard setting with hard labels, but (ii) can beemployed in settings with soft labels. Our experiments on the public QUBIQ,LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels(e.g. averaging, label smoothing, and knowledge distillation) over hard labels(e.g. majority voting and random selection). As a result, we obtain superiorDice scores and model calibration, which supports the wider adoption of DMLs inpractice. The code is available at https://github.com/zifuwanggg/JDTLosses</description><author>Zifu Wang, Teodora Popordanoska, Jeroen Bertels, Robin Lemmens, Matthew B. Blaschko</author><pubDate>Wed, 20 Mar 2024 16:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16296v4</guid></item><item><title>SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning</title><link>http://arxiv.org/abs/2403.13684v1</link><description>Generalized Category Discovery (GCD) aims to classify unlabelled images fromboth `seen' and `unseen' classes by transferring knowledge from a set oflabelled `seen' class images. A key theme in existing GCD approaches isadapting large-scale pre-trained models for the GCD task. An alternateperspective, however, is to adapt the data representation itself for betteralignment with the pre-trained model. As such, in this paper, we introduce atwo-stage adaptation approach termed SPTNet, which iteratively optimizes modelparameters (i.e., model-finetuning) and data parameters (i.e., promptlearning). Furthermore, we propose a novel spatial prompt tuning method (SPT)which considers the spatial property of image data, enabling the method tobetter focus on object parts, which can transfer between seen and unseenclasses. We thoroughly evaluate our SPTNet on standard benchmarks anddemonstrate that our method outperforms existing GCD methods. Notably, we findour method achieves an average accuracy of 61.4% on the SSB, surpassing priorstate-of-the-art methods by approximately 10%. The improvement is particularlyremarkable as our method yields extra parameters amounting to only 0.117% ofthose in the backbone architecture. Project page:https://visual-ai.github.io/sptnet.</description><author>Hongjun Wang, Sagar Vaze, Kai Han</author><pubDate>Wed, 20 Mar 2024 16:41:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13684v1</guid></item><item><title>DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses</title><link>http://arxiv.org/abs/2403.13683v1</link><description>Determining the relative pose of an object between two images is pivotal tothe success of generalizable object pose estimation. Existing approachestypically approximate the continuous pose representation with a large number ofdiscrete pose hypotheses, which incurs a computationally expensive process ofscoring each hypothesis at test time. By contrast, we present a Deep VoxelMatching Network (DVMNet) that eliminates the need for pose hypotheses andcomputes the relative object pose in a single pass. To this end, we map the twoinput RGB images, reference and query, to their respective voxelized 3Drepresentations. We then pass the resulting voxels through a pose estimationmodule, where the voxels are aligned and the pose is computed in an end-to-endfashion by solving a least-squares problem. To enhance robustness, we introducea weighted closest voxel algorithm capable of mitigating the impact of noisyvoxels. We conduct extensive experiments on the CO3D, LINEMOD, and Objaversedatasets, demonstrating that our method delivers more accurate relative poseestimates for novel objects at a lower computational cost compared tostate-of-the-art methods. Our code is released at:https://github.com/sailor-z/DVMNet/.</description><author>Chen Zhao, Tong Zhang, Zheng Dang, Mathieu Salzmann</author><pubDate>Wed, 20 Mar 2024 16:41:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13683v1</guid></item><item><title>Weight-Inherited Distillation for Task-Agnostic BERT Compression</title><link>http://arxiv.org/abs/2305.09098v2</link><description>Knowledge Distillation (KD) is a predominant approach for BERT compression.Previous KD-based methods focus on designing extra alignment losses for thestudent model to mimic the behavior of the teacher model. These methodstransfer the knowledge in an indirect way. In this paper, we propose a novelWeight-Inherited Distillation (WID), which directly transfers knowledge fromthe teacher. WID does not require any additional alignment loss and trains acompact student by inheriting the weights, showing a new perspective ofknowledge distillation. Specifically, we design the row compactors and columncompactors as mappings and then compress the weights via structuralre-parameterization. Experimental results on the GLUE and SQuAD benchmarks showthat WID outperforms previous state-of-the-art KD-based baselines. Furtheranalysis indicates that WID can also learn the attention patterns from theteacher model without any alignment loss on attention distributions. The codeis available at https://github.com/wutaiqiang/WID-NAACL2024.</description><author>Taiqiang Wu, Cheng Hou, Shanshan Lao, Jiayi Li, Ngai Wong, Zhe Zhao, Yujiu Yang</author><pubDate>Wed, 20 Mar 2024 16:41:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09098v2</guid></item><item><title>Threats, Attacks, and Defenses in Machine Unlearning: A Survey</title><link>http://arxiv.org/abs/2403.13682v1</link><description>Recently, Machine Unlearning (MU) has gained considerable attention for itspotential to improve AI safety by removing the influence of specific data fromtrained Machine Learning (ML) models. This process, known as knowledge removal,addresses concerns about data such as sensitivity, copyright restrictions,obsolescence, or low quality. This capability is also crucial for ensuringcompliance with privacy regulations such as the Right To Be Forgotten (RTBF).Therefore, strategic knowledge removal mitigates the risk of harmful outcomes,safeguarding against biases, misinformation, and unauthorized dataexploitation, thereby enhancing the ethical use and reliability of AI systems.Efforts have been made to design efficient unlearning approaches, with MUservices being examined for integration with existing machine learning as aservice (MLaaS), allowing users to submit requests to erase data. However,recent research highlights vulnerabilities in machine unlearning systems, suchas information leakage and malicious unlearning requests, that can lead tosignificant security and privacy concerns. Moreover, extensive researchindicates that unlearning methods and prevalent attacks fulfill diverse roleswithin MU systems. For instance, unlearning can act as a mechanism to recovermodels from backdoor attacks, while backdoor attacks themselves can serve as anevaluation metric for unlearning effectiveness. This underscores the intricaterelationship and complex interplay between these elements in maintaining systemfunctionality and safety. Therefore, this survey seeks to bridge the gapbetween the extensive number of studies on threats, attacks, and defenses inmachine unlearning and the absence of a comprehensive review that categorizestheir taxonomy, methods, and solutions, thus offering valuable insights forfuture research directions and practical implementations.</description><author>Ziyao Liu, Huanyi Ye, Chen Chen, Kwok-Yan Lam</author><pubDate>Wed, 20 Mar 2024 16:40:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13682v1</guid></item><item><title>PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents</title><link>http://arxiv.org/abs/2403.13681v1</link><description>In this paper, we present PARAMANU-AYN, a language model based exclusively oncase documents of the Supreme Court of India, the Constitution of India, andthe Indian Penal Code. The novel Auto Regressive (AR) decoder based model ispretrained from scratch at a context size of 8192. We evaluated our pretrainedlegal model on perplexity metrics. We also instruction-tuned our pretrainedmodel on a set of 10,763 instructions covering various legal tasks such aslegal reasoning, judgement explanation, legal clause generation, legaldrafting, legal contract drafting, case summarization, constitutionalquestion-answering, etc. We also evaluated the responses of prompts forinstruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness,and legal reasoning metrics in a scale of 10. Our model can be run on CPU andachieved 42.46 tokens/sec CPU inference speed. We found that our models,despite not being pretrained on legal books, various legal contracts, and legaldocuments, were able to learn the domain knowledge required for draftingvarious legal contracts and legal clauses, and generalize to draft legalcontracts and legal clauses with limited instruction tuning. Hence, we concludethat for a strong domain-specialized generative language model (such as legal),very large amounts of data are not required to develop models from scratch. Webelieve that this work is the first attempt to make a dedicated generativelegal language model from scratch for Indian Supreme Court jurisdiction or inlegal NLP overall. We plan to release our Paramanu-Ayn model athttps://www.bharatgpts.com.</description><author>Mitodru Niyogi, Arnab Bhattacharya</author><pubDate>Wed, 20 Mar 2024 16:39:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13681v1</guid></item><item><title>Step-Calibrated Diffusion for Biomedical Optical Image Restoration</title><link>http://arxiv.org/abs/2403.13680v1</link><description>High-quality, high-resolution medical imaging is essential for clinical care.Raman-based biomedical optical imaging uses non-ionizing infrared radiation toevaluate human tissues in real time and is used for early cancer detection,brain tumor diagnosis, and intraoperative tissue analysis. Unfortunately,optical imaging is vulnerable to image degradation due to laser scattering andabsorption, which can result in diagnostic errors and misguided treatment.Restoration of optical images is a challenging computer vision task because thesources of image degradation are multi-factorial, stochastic, andtissue-dependent, preventing a straightforward method to obtain pairedlow-quality/high-quality data. Here, we present Restorative Step-CalibratedDiffusion (RSCD), an unpaired image restoration method that views the imagerestoration problem as completing the finishing steps of a diffusion-basedimage generation task. RSCD uses a step calibrator model to dynamicallydetermine the severity of image degradation and the number of steps required tocomplete the reverse diffusion process for image restoration. RSCD outperformsother widely used unpaired image restoration methods on both image quality andperceptual evaluation metrics for restoring optical images. Medical imagingexperts consistently prefer images restored using RSCD in blinded comparisonexperiments and report minimal to no hallucinations. Finally, we show that RSCDimproves performance on downstream clinical imaging tasks, including automatedbrain tumor diagnosis and deep tissue imaging. Our code is available athttps://github.com/MLNeurosurg/restorative_step-calibrated_diffusion.</description><author>Yiwei Lyu, Sung Jik Cha, Cheng Jiang, Asadur Chowdury, Xinhai Hou, Edward Harake, Akhil Kondepudi, Christian Freudiger, Honglak Lee, Todd C. Hollon</author><pubDate>Wed, 20 Mar 2024 16:38:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13680v1</guid></item><item><title>RoleInteract: Evaluating the Social Interaction of Role-Playing Agents</title><link>http://arxiv.org/abs/2403.13679v1</link><description>Large language models (LLMs) have advanced the development of various AIconversational agents, including role-playing conversational agents that mimicdiverse characters and human behaviors. While prior research has predominantlyfocused on enhancing the conversational capability, role-specific knowledge,and stylistic attributes of these agents, there has been a noticeable gap inassessing their social intelligence. In this paper, we introduce RoleInteract,the first benchmark designed to systematically evaluate the sociality ofrole-playing conversational agents at both individual and group levels ofsocial interactions. The benchmark is constructed from a variety of sources andcovers a wide range of 500 characters and over 6,000 question prompts and30,800 multi-turn role-playing utterances. We conduct comprehensive evaluationson this benchmark using mainstream open-source and closed-source LLMs. We findthat agents excelling in individual level does not imply their proficiency ingroup level. Moreover, the behavior of individuals may drift as a result of theinfluence exerted by other agents within the group. Experimental results onRoleInteract confirm its significance as a testbed for assessing the socialinteraction of role-playing conversational agents. The benchmark is publiclyaccessible at https://github.com/X-PLUG/RoleInteract.</description><author>Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Xing Gao, Weizhou Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, Fei Huang, Jingren Zhou</author><pubDate>Wed, 20 Mar 2024 16:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13679v1</guid></item><item><title>AUD-TGN: Advancing Action Unit Detection with Temporal Convolution and GPT-2 in Wild Audiovisual Contexts</title><link>http://arxiv.org/abs/2403.13678v1</link><description>Leveraging the synergy of both audio data and visual data is essential forunderstanding human emotions and behaviors, especially in in-the-wild setting.Traditional methods for integrating such multimodal information often stumble,leading to less-than-ideal outcomes in the task of facial action unitdetection. To overcome these shortcomings, we propose a novel approachutilizing audio-visual multimodal data. This method enhances audio featureextraction by leveraging Mel Frequency Cepstral Coefficients (MFCC) and Log-Melspectrogram features alongside a pre-trained VGGish network. Moreover, thispaper adaptively captures fusion features across modalities by modeling thetemporal relationships, and ultilizes a pre-trained GPT-2 model forsophisticated context-aware fusion of multimodal information. Our methodnotably improves the accuracy of AU detection by understanding the temporal andcontextual nuances of the data, showcasing significant advancements in thecomprehension of intricate scenarios. These findings underscore the potentialof integrating temporal dynamics and contextual interpretation, paving the wayfor future research endeavors.</description><author>Jun Yu, Zerui Zhang, Zhihong Wei, Gongpeng Zhao, Zhongpeng Cai, Yongqi Wang, Guochen Xie, Jichao Zhu, Wangyuan Zhu</author><pubDate>Wed, 20 Mar 2024 16:37:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13678v1</guid></item><item><title>Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into Vision Transformers</title><link>http://arxiv.org/abs/2403.13677v1</link><description>Humans see low and high spatial frequency components at the same time, andcombine the information from both to form a visual scene. Drawing on thisneuroscientific inspiration, we propose an altered Vision Transformerarchitecture where patches from scaled down versions of the input image areadded to the input of the first Transformer Encoder layer. We name this modelRetina Vision Transformer (RetinaViT) due to its inspiration from the humanvisual system. Our experiments show that when trained on the ImageNet-1Kdataset with a moderate configuration, RetinaViT achieves a 3.3% performanceimprovement over the original ViT. We hypothesize that this improvement can beattributed to the inclusion of low spatial frequency components in the input,which improves the ability to capture structural features, and to select andforward important features to deeper layers. RetinaViT thereby opens doors tofurther investigations into vertical pathways and attention patterns.</description><author>Yuyang Shu, Michael E. Bain</author><pubDate>Wed, 20 Mar 2024 16:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13677v1</guid></item><item><title>S$Ω$I: Score-based O-INFORMATION Estimation</title><link>http://arxiv.org/abs/2402.05667v2</link><description>The analysis of scientific data and complex multivariate systems requiresinformation quantities that capture relationships among multiple randomvariables. Recently, new information-theoretic measures have been developed toovercome the shortcomings of classical ones, such as mutual information, thatare restricted to considering pairwise interactions. Among them, the concept ofinformation synergy and redundancy is crucial for understanding the high-orderdependencies between variables. One of the most prominent and versatilemeasures based on this concept is O-information, which provides a clear andscalable way to quantify the synergy-redundancy balance in multivariatesystems. However, its practical application is limited to simplified cases. Inthis work, we introduce S$\Omega$I, which allows for the first time to computeO-information without restrictive assumptions about the system. Our experimentsvalidate our approach on synthetic data, and demonstrate the effectiveness ofS$\Omega$I in the context of a real-world use case.</description><author>Mustapha Bounoua, Giulio Franzese, Pietro Michiardi</author><pubDate>Wed, 20 Mar 2024 16:33:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05667v2</guid></item><item><title>A Causal Framework to Evaluate Racial Bias in Law Enforcement Systems</title><link>http://arxiv.org/abs/2402.14959v2</link><description>We are interested in developing a data-driven method to evaluate race-inducedbiases in law enforcement systems. While the recent works have addressed thisquestion in the context of police-civilian interactions using police stop data,they have two key limitations. First, bias can only be properly quantified iftrue criminality is accounted for in addition to race, but it is absent inprior works. Second, law enforcement systems are multi-stage and hence it isimportant to isolate the true source of bias within the "causal chain ofinteractions" rather than simply focusing on the end outcome; this can helpguide reforms. In this work, we address these challenges by presenting amulti-stage causal framework incorporating criminality. We provide atheoretical characterization and an associated data-driven method to evaluate(a) the presence of any form of racial bias, and (b) if so, the primary sourceof such a bias in terms of race and criminality. Our framework identifies threecanonical scenarios with distinct characteristics: in settings like (1) airportsecurity, the primary source of observed bias against a race is likely to bebias in law enforcement against innocents of that race; (2) AI-empoweredpolicing, the primary source of observed bias against a race is likely to bebias in law enforcement against criminals of that race; and (3) police-civilianinteraction, the primary source of observed bias against a race could be biasin law enforcement against that race or bias from the general public inreporting against the other race. Through an extensive empirical study usingpolice-civilian interaction data and 911 call data, we find an instance of sucha counter-intuitive phenomenon: in New Orleans, the observed bias is againstthe majority race and the likely reason for it is the over-reporting (via 911calls) of incidents involving the minority race by the general public.</description><author>Jessy Xinyi Han, Andrew Miller, S. Craig Watkins, Christopher Winship, Fotini Christia, Devavrat Shah</author><pubDate>Wed, 20 Mar 2024 16:32:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14959v2</guid></item><item><title>Poly Kernel Inception Network for Remote Sensing Detection</title><link>http://arxiv.org/abs/2403.06258v2</link><description>Object detection in remote sensing images (RSIs) often suffers from severalincreasing challenges, including the large variation in object scales and thediverse-ranging context. Prior methods tried to address these challenges byexpanding the spatial receptive field of the backbone, either throughlarge-kernel convolution or dilated convolution. However, the former typicallyintroduces considerable background noise, while the latter risks generatingoverly sparse feature representations. In this paper, we introduce the PolyKernel Inception Network (PKINet) to handle the above challenges. PKINetemploys multi-scale convolution kernels without dilation to extract objectfeatures of varying scales and capture local context. In addition, a ContextAnchor Attention (CAA) module is introduced in parallel to capture long-rangecontextual information. These two components work jointly to advance theperformance of PKINet on four challenging remote sensing detection benchmarks,namely DOTA-v1.0, DOTA-v1.5, HRSC2016, and DIOR-R.</description><author>Xinhao Cai, Qiuxia Lai, Yuwei Wang, Wenguan Wang, Zeren Sun, Yazhou Yao</author><pubDate>Wed, 20 Mar 2024 16:31:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06258v2</guid></item><item><title>Guaranteeing Control Requirements via Reward Shaping in Reinforcement Learning</title><link>http://arxiv.org/abs/2311.10026v2</link><description>In addressing control problems such as regulation and tracking throughreinforcement learning, it is often required to guarantee that the acquiredpolicy meets essential performance and stability criteria such as a desiredsettling time and steady-state error prior to deployment. Motivated by thisnecessity, we present a set of results and a systematic reward shapingprocedure that (i) ensures the optimal policy generates trajectories that alignwith specified control requirements and (ii) allows to assess whether any givenpolicy satisfies them. We validate our approach through comprehensive numericalexperiments conducted in two representative environments from OpenAI Gym: theInverted Pendulum swing-up problem and the Lunar Lander. Utilizing both tabularand deep reinforcement learning methods, our experiments consistently affirmthe efficacy of our proposed framework, highlighting its effectiveness inensuring policy adherence to the prescribed control requirements.</description><author>Francesco De Lellis, Marco Coraggio, Giovanni Russo, Mirco Musolesi, Mario di Bernardo</author><pubDate>Wed, 20 Mar 2024 16:30:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10026v2</guid></item><item><title>Machine Learning Optimized Approach for Parameter Selection in MESHFREE Simulations</title><link>http://arxiv.org/abs/2403.13672v1</link><description>Meshfree simulation methods are emerging as compelling alternatives toconventional mesh-based approaches, particularly in the fields of ComputationalFluid Dynamics (CFD) and continuum mechanics. In this publication, we provide acomprehensive overview of our research combining Machine Learning (ML) andFraunhofer's MESHFREE software (www.meshfree.eu), a powerful tool utilizing anumerical point cloud in a Generalized Finite Difference Method (GFDM). Thistool enables the effective handling of complex flow domains, moving geometries,and free surfaces, while allowing users to finely tune local refinement andquality parameters for an optimal balance between computation time and resultsaccuracy. However, manually determining the optimal parameter combination poseschallenges, especially for less experienced users. We introduce a novelML-optimized approach, using active learning, regression trees, andvisualization on MESHFREE simulation data, demonstrating the impact of inputcombinations on results quality and computation time. This research contributesvaluable insights into parameter optimization in meshfree simulations,enhancing accessibility and usability for a broader user base in scientific andengineering applications.</description><author>Paulami Banerjee, Mohan Padmanabha, Chaitanya Sanghavi, Isabel Michel, Simone Gramsch</author><pubDate>Wed, 20 Mar 2024 16:29:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13672v1</guid></item><item><title>Weakly supervised segmentation of intracranial aneurysms using a novel 3D focal modulation UNet</title><link>http://arxiv.org/abs/2308.03001v2</link><description>Accurate identification and quantification of unruptured intracranialaneurysms (UIAs) is crucial for the risk assessment and treatment of thiscerebrovascular disorder. Current 2D manual assessment on 3D magnetic resonanceangiography (MRA) is suboptimal and time-consuming. In addition, one majorissue in medical image segmentation is the need for large well-annotated data,which can be expensive to obtain. Techniques that mitigate this requirement,such as weakly supervised learning with coarse labels are highly desirable. Inthe paper, we propose FocalSegNet, a novel 3D focal modulation UNet, to detectan aneurysm and offer an initial, coarse segmentation of it from time-of-flightMRA image patches, which is further refined with a dense conditional randomfield (CRF) post-processing layer to produce a final segmentation map. Wetrained and evaluated our model on a public dataset, and in terms of UIAdetection, our model showed a low false-positive rate of 0.21 and a highsensitivity of 0.80. For voxel-wise aneurysm segmentation, we achieved a Dicescore of 0.68 and a 95% Hausdorff distance of ~0.95 mm, demonstrating itsstrong performance. We evaluated our algorithms against the state-of-the-art 3DResidual-UNet and Swin-UNETR, and illustrated the superior performance of ourproposed FocalSegNet, highlighting the advantages of employing focal modulationfor this task.</description><author>Amirhossein Rasoulian, Arash Harirpoush, Soorena Salari, Yiming Xiao</author><pubDate>Wed, 20 Mar 2024 16:29:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03001v2</guid></item><item><title>The Power of Few: Accelerating and Enhancing Data Reweighting with Coreset Selection</title><link>http://arxiv.org/abs/2403.12166v2</link><description>As machine learning tasks continue to evolve, the trend has been to gatherlarger datasets and train increasingly larger models. While this has led toadvancements in accuracy, it has also escalated computational costs tounsustainable levels. Addressing this, our work aims to strike a delicatebalance between computational efficiency and model accuracy, a persistingchallenge in the field. We introduce a novel method that employs core subsetselection for reweighting, effectively optimizing both computational time andmodel performance. By focusing on a strategically selected coreset, ourapproach offers a robust representation, as it efficiently minimizes theinfluence of outliers. The re-calibrated weights are then mapped back to andpropagated across the entire dataset. Our experimental results substantiate theeffectiveness of this approach, underscoring its potential as a scalable andprecise solution for model training.</description><author>Mohammad Jafari, Yimeng Zhang, Yihua Zhang, Sijia Liu</author><pubDate>Wed, 20 Mar 2024 16:27:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12166v2</guid></item><item><title>Observational and Experimental Insights into Machine Learning-Based Defect Classification in Wafers</title><link>http://arxiv.org/abs/2310.10705v4</link><description>This survey paper offers a comprehensive review of methodologies utilizingmachine learning (ML) classification techniques for identifying wafer defectsin semiconductor manufacturing. Despite the growing body of researchdemonstrating the effectiveness of ML in wafer defect identification, there isa noticeable absence of comprehensive reviews on this subject. This surveyattempts to fill this void by amalgamating available literature and providingan in-depth analysis of the advantages, limitations, and potential applicationsof various ML classification algorithms in the realm of wafer defect detection.An innovative taxonomy of methodologies that we present provides a detailedclassification of algorithms into more refined categories and techniques. Thistaxonomy follows a three-tier structure, starting from broad methodologycategories and ending with specific techniques. It aids researchers incomprehending the complex relationships between different algorithms and theirtechniques. We employ a rigorous Observational and experimental evaluation torank these varying techniques. For the Observational evaluation, we assesstechniques based on a set of four criteria. The experimental evaluation ranksthe algorithms employing the same techniques, sub-categories, and categories.Also the paper illuminates the future prospects of ML classification techniquesfor wafer defect identification, underscoring potential advancements andopportunities for further research in this field</description><author>Kamal Taha</author><pubDate>Wed, 20 Mar 2024 16:26:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10705v4</guid></item><item><title>Interpretable Meta-Learning of Physical Systems</title><link>http://arxiv.org/abs/2312.00477v2</link><description>Machine learning methods can be a valuable aid in the scientific process, butthey need to face challenging settings where data come from inhomogeneousexperimental conditions. Recent meta-learning methods have made significantprogress in multi-task learning, but they rely on black-box neural networks,resulting in high computational costs and limited interpretability. Leveragingthe structure of the learning problem, we argue that multi-environmentgeneralization can be achieved using a simpler learning model, with an affinestructure with respect to the learning task. Crucially, we prove that thisarchitecture can identify the physical parameters of the system, enablinginterpreable learning. We demonstrate the competitive generalizationperformance and the low computational cost of our method by comparing it tostate-of-the-art algorithms on physical systems, ranging from toy models tocomplex, non-analytical systems. The interpretability of our method isillustrated with original applications to physical-parameter-induced adaptationand to adaptive control.</description><author>Matthieu Blanke, Marc Lelarge</author><pubDate>Wed, 20 Mar 2024 16:25:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00477v2</guid></item><item><title>DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance</title><link>http://arxiv.org/abs/2403.13667v1</link><description>Choreographers determine what the dances look like, while cameramen determinethe final presentation of dances. Recently, various methods and datasets haveshowcased the feasibility of dance synthesis. However, camera movementsynthesis with music and dance remains an unsolved challenging problem due tothe scarcity of paired data. Thus, we present DCM, a new multi-modal 3Ddataset, which for the first time combines camera movement with dance motionand music audio. This dataset encompasses 108 dance sequences (3.2 hours) ofpaired dance-camera-music data from the anime community, covering 4 musicgenres. With this dataset, we uncover that dance camera movement ismultifaceted and human-centric, and possesses multiple influencing factors,making dance camera synthesis a more challenging task compared to camera ordance synthesis alone. To overcome these difficulties, we proposeDanceCamera3D, a transformer-based diffusion model that incorporates a novelbody attention loss and a condition separation strategy. For evaluation, wedevise new metrics measuring camera movement quality, diversity, and dancerfidelity. Utilizing these metrics, we conduct extensive experiments on our DCMdataset, providing both quantitative and qualitative evidence showcasing theeffectiveness of our DanceCamera3D model. Code and video demos are available athttps://github.com/Carmenw1203/DanceCamera3D-Official.</description><author>Zixuan Wang, Jia Jia, Shikun Sun, Haozhe Wu, Rong Han, Zhenyu Li, Di Tang, Jiaqing Zhou, Jiebo Luo</author><pubDate>Wed, 20 Mar 2024 16:24:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13667v1</guid></item><item><title>View-Consistent 3D Editing with Gaussian Splatting</title><link>http://arxiv.org/abs/2403.11868v2</link><description>The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,offering efficient, high-fidelity rendering and enabling precise localmanipulations. Currently, diffusion-based 2D editing models are harnessed tomodify multi-view rendered images, which then guide the editing of 3DGS models.However, this approach faces a critical issue of multi-view inconsistency,where the guidance images exhibit significant discrepancies across views,leading to mode collapse and visual artifacts of 3DGS. To this end, weintroduce View-consistent Editing (VcEdit), a novel framework that seamlesslyincorporates 3DGS into image editing processes, ensuring multi-view consistencyin edited guidance images and effectively mitigating mode collapse issues.VcEdit employs two innovative consistency modules: the Cross-attentionConsistency Module and the Editing Consistency Module, both designed to reduceinconsistencies in edited images. By incorporating these consistency modulesinto an iterative pattern, VcEdit proficiently resolves the issue of multi-viewinconsistency, facilitating high-quality 3DGS editing across a diverse range ofscenes.</description><author>Yuxuan Wang, Xuanyu Yi, Zike Wu, Na Zhao, Long Chen, Hanwang Zhang</author><pubDate>Wed, 20 Mar 2024 16:22:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11868v2</guid></item><item><title>Grounding Spatial Relations in Text-Only Language Models</title><link>http://arxiv.org/abs/2403.13666v1</link><description>This paper shows that text-only Language Models (LM) can learn to groundspatial relations like "left of" or "below" if they are provided with explicitlocation information of objects and they are properly trained to leverage thoselocations. We perform experiments on a verbalized version of the Visual SpatialReasoning (VSR) dataset, where images are coupled with textual statements whichcontain real or fake spatial relations between two objects of the image. Weverbalize the images using an off-the-shelf object detector, adding locationtokens to every object label to represent their bounding boxes in textual form.Given the small size of VSR, we do not observe any improvement when usinglocations, but pretraining the LM over a synthetic dataset automaticallyderived by us improves results significantly when using location tokens. Wethus show that locations allow LMs to ground spatial relations, with ourtext-only LMs outperforming Vision-and-Language Models and setting the newstate-of-the-art for the VSR dataset. Our analysis show that our text-only LMscan generalize beyond the relations seen in the synthetic dataset to someextent, learning also more useful information than that encoded in the spatialrules we used to create the synthetic dataset itself.</description><author>Gorka Azkune, Ander Salaberria, Eneko Agirre</author><pubDate>Wed, 20 Mar 2024 16:20:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13666v1</guid></item><item><title>Bounce: Reliable High-Dimensional Bayesian Optimization for Combinatorial and Mixed Spaces</title><link>http://arxiv.org/abs/2307.00618v2</link><description>Impactful applications such as materials discovery, hardware design, neuralarchitecture search, or portfolio optimization require optimizinghigh-dimensional black-box functions with mixed and combinatorial input spaces.While Bayesian optimization has recently made significant progress in solvingsuch problems, an in-depth analysis reveals that the current state-of-the-artmethods are not reliable. Their performances degrade substantially when theunknown optima of the function do not have a certain structure. To fill theneed for a reliable algorithm for combinatorial and mixed spaces, this paperproposes Bounce that relies on a novel map of various variable types intonested embeddings of increasing dimensionality. Comprehensive experiments showthat Bounce reliably achieves and often even improves upon state-of-the-artperformance on a variety of high-dimensional problems.</description><author>Leonard Papenmeier, Luigi Nardi, Matthias Poloczek</author><pubDate>Wed, 20 Mar 2024 16:17:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00618v2</guid></item></channel></rss>