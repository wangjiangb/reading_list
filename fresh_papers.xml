<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 29 Apr 2024 06:00:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Learning to Visually Connect Actions and their Effects</title><link>http://arxiv.org/abs/2401.10805v2</link><description>In this work, we introduce the novel concept of visually Connecting Actionsand Their Effects (CATE) in video understanding. CATE can have applications inareas like task planning and learning from demonstration. We identify andexplore two different aspects of the concept of CATE: Action Selection andEffect-Affinity Assessment, where video understanding models connect actionsand effects at semantic and fine-grained levels, respectively. We observe thatdifferent formulations produce representations capturing intuitive actionproperties. We also design various baseline models for Action Selection andEffect-Affinity Assessment. Despite the intuitive nature of the task, weobserve that models struggle, and humans outperform them by a large margin. Thestudy aims to establish a foundation for future efforts, showcasing theflexibility and versatility of connecting actions and effects in videounderstanding, with the hope of inspiring advanced formulations and models.</description><author>Eric Peh, Paritosh Parmar, Basura Fernando</author><pubDate>Fri, 26 Apr 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10805v2</guid></item><item><title>Tunnel Try-on: Excavating Spatial-temporal Tunnels for High-quality Virtual Try-on in Videos</title><link>http://arxiv.org/abs/2404.17571v1</link><description>Video try-on is a challenging task and has not been well tackled in previousworks. The main obstacle lies in preserving the details of the clothing andmodeling the coherent motions simultaneously. Faced with those difficulties, weaddress video try-on by proposing a diffusion-based framework named "TunnelTry-on." The core idea is excavating a "focus tunnel" in the input video thatgives close-up shots around the clothing regions. We zoom in on the region inthe tunnel to better preserve the fine details of the clothing. To generatecoherent motions, we first leverage the Kalman filter to construct smooth cropsin the focus tunnel and inject the position embedding of the tunnel intoattention layers to improve the continuity of the generated videos. Inaddition, we develop an environment encoder to extract the context informationoutside the tunnels as supplementary cues. Equipped with these techniques,Tunnel Try-on keeps the fine details of the clothing and synthesizes stable andsmooth videos. Demonstrating significant advancements, Tunnel Try-on could beregarded as the first attempt toward the commercial-level application ofvirtual try-on in videos.</description><author>Zhengze Xu, Mengting Chen, Zhao Wang, Linyu Xing, Zhonghua Zhai, Nong Sang, Jinsong Lan, Shuai Xiao, Changxin Gao</author><pubDate>Fri, 26 Apr 2024 18:55:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17571v1</guid></item><item><title>MaPa: Text-driven Photorealistic Material Painting for 3D Shapes</title><link>http://arxiv.org/abs/2404.17569v1</link><description>This paper aims to generate materials for 3D meshes from text descriptions.Unlike existing methods that synthesize texture maps, we propose to generatesegment-wise procedural material graphs as the appearance representation, whichsupports high-quality rendering and provides substantial flexibility inediting. Instead of relying on extensive paired data, i.e., 3D meshes withmaterial graphs and corresponding text descriptions, to train a material graphgenerative model, we propose to leverage the pre-trained 2D diffusion model asa bridge to connect the text and material graphs. Specifically, our approachdecomposes a shape into a set of segments and designs a segment-controlleddiffusion model to synthesize 2D images that are aligned with mesh parts. Basedon generated images, we initialize parameters of material graphs and fine-tunethem through the differentiable rendering module to produce materials inaccordance with the textual description. Extensive experiments demonstrate thesuperior performance of our framework in photorealism, resolution, andeditability over existing methods. Project page:https://zhanghe3z.github.io/MaPa/</description><author>Shangzhan Zhang, Sida Peng, Tao Xu, Yuanbo Yang, Tianrun Chen, Nan Xue, Yujun Shen, Hujun Bao, Ruizhen Hu, Xiaowei Zhou</author><pubDate>Fri, 26 Apr 2024 18:54:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17569v1</guid></item><item><title>ChangeBind: A Hybrid Change Encoder for Remote Sensing Change Detection</title><link>http://arxiv.org/abs/2404.17565v1</link><description>Change detection (CD) is a fundamental task in remote sensing (RS) which aimsto detect the semantic changes between the same geographical regions atdifferent time stamps. Existing convolutional neural networks (CNNs) basedapproaches often struggle to capture long-range dependencies. Whereas recenttransformer-based methods are prone to the dominant global representation andmay limit their capabilities to capture the subtle change regions due to thecomplexity of the objects in the scene. To address these limitations, wepropose an effective Siamese-based framework to encode the semantic changesoccurring in the bi-temporal RS images. The main focus of our design is tointroduce a change encoder that leverages local and global featurerepresentations to capture both subtle and large change feature informationfrom multi-scale features to precisely estimate the change regions. Ourexperimental study on two challenging CD datasets reveals the merits of ourapproach and obtains state-of-the-art performance.</description><author>Mubashir Noman, Mustansar Fiaz, Hisham Cholakkal</author><pubDate>Fri, 26 Apr 2024 18:47:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17565v1</guid></item><item><title>An exactly solvable model for emergence and scaling laws</title><link>http://arxiv.org/abs/2404.17563v1</link><description>Deep learning models can exhibit what appears to be a sudden ability to solvea new problem as training time ($T$), training data ($D$), or model size ($N$)increases, a phenomenon known as emergence. In this paper, we present aframework where each new ability (a skill) is represented as a basis function.We solve a simple multi-linear model in this skill-basis, finding analyticexpressions for the emergence of new skills, as well as for scaling laws of theloss with training time, data size, model size, and optimal compute ($C$). Wecompare our detailed calculations to direct simulations of a two-layer neuralnetwork trained on multitask sparse parity, where the tasks in the dataset aredistributed according to a power-law. Our simple model captures, using a singlefit parameter, the sigmoidal emergence of multiple new skills as training time,data size or model size increases in the neural network.</description><author>Yoonsoo Nam, Nayara Fonseca, Seok Hyeong Lee, Ard Louis</author><pubDate>Fri, 26 Apr 2024 18:45:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17563v1</guid></item><item><title>Structured Conformal Inference for Matrix Completion with Applications to Group Recommender Systems</title><link>http://arxiv.org/abs/2404.17561v1</link><description>We develop a conformal inference method to construct joint confidence regionsfor structured groups of missing entries within a sparsely observed matrix.This method is useful to provide reliable uncertainty estimation forgroup-level collaborative filtering; for example, it can be applied to helpsuggest a movie for a group of friends to watch together. Unlike standardconformal techniques, which make inferences for one individual at a time, ourmethod achieves stronger group-level guarantees by carefully assembling astructured calibration data set mimicking the patterns expected among the testgroup of interest. We propose a generalized weighted conformalization frameworkto deal with the lack of exchangeability arising from such structuredcalibration, and in this process we introduce several innovations to overcomecomputational challenges. The practicality and effectiveness of our method aredemonstrated through extensive numerical experiments and an analysis of theMovieLens 100K data set.</description><author>Ziyi Liang, Tianmin Xie, Xin Tong, Matteo Sesia</author><pubDate>Fri, 26 Apr 2024 18:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17561v1</guid></item><item><title>Laplace-HDC: Understanding the geometry of binary hyperdimensional computing</title><link>http://arxiv.org/abs/2404.10759v2</link><description>This paper studies the geometry of binary hyperdimensional computing (HDC), acomputational scheme in which data are encoded using high-dimensional binaryvectors. We establish a result about the similarity structure induced by theHDC binding operator and show that the Laplace kernel naturally arises in thissetting, motivating our new encoding method Laplace-HDC, which improves uponprevious methods. We describe how our results indicate limitations of binaryHDC in encoding spatial information from images and discuss potentialsolutions, including using Haar convolutional features and the definition of atranslation-equivariant HDC encoding. Several numerical experimentshighlighting the improved accuracy of Laplace-HDC in contrast to alternativemethods are presented. We also numerically study other aspects of the proposedframework such as robustness and the underlying translation-equivariantencoding.</description><author>Saeid Pourmand, Wyatt D. Whiting, Alireza Aghasi, Nicholas F. Marshall</author><pubDate>Fri, 26 Apr 2024 18:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10759v2</guid></item><item><title>Federated Transfer Component Analysis Towards Effective VNF Profiling</title><link>http://arxiv.org/abs/2404.17553v1</link><description>The increasing concerns of knowledge transfer and data privacy challenge thetraditional gather-and-analyse paradigm in networks. Specifically, theintelligent orchestration of Virtual Network Functions (VNFs) requiresunderstanding and profiling the resource consumption. However, profiling allkinds of VNFs is time-consuming. It is important to consider transferring thewell-profiled VNF knowledge to other lack-profiled VNF types while keeping dataprivate. To this end, this paper proposes a Federated Transfer ComponentAnalysis (FTCA) method between the source and target VNFs. FTCA first trainsGenerative Adversarial Networks (GANs) based on the source VNF profiling data,and the trained GANs model is sent to the target VNF domain. Then, FTCArealizes federated domain adaptation by using the generated source VNF data andless target VNF profiling data, while keeping the raw data locally. Experimentsshow that the proposed FTCA can effectively predict the required resources forthe target VNF. Specifically, the RMSE index of the regression model decreasesby 38.5% and the R-squared metric advances up to 68.6%.</description><author>Xunzheng ZhangB, Shadi Moazzeni, Juan Marcelo Parra-Ullauri, Reza Nejabati, Dimitra Simeonidou</author><pubDate>Fri, 26 Apr 2024 18:31:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17553v1</guid></item><item><title>A Semi-Automatic Approach to Create Large Gender- and Age-Balanced Speaker Corpora: Usefulness of Speaker Diarization &amp; Identification</title><link>http://arxiv.org/abs/2404.17552v1</link><description>This paper presents a semi-automatic approach to create a diachronic corpusof voices balanced for speaker's age, gender, and recording period, accordingto 32 categories (2 genders, 4 age ranges and 4 recording periods). Corporawere selected at French National Institute of Audiovisual (INA) to obtain atleast 30 speakers per category (a total of 960 speakers; only 874 have be foundyet). For each speaker, speech excerpts were extracted from audiovisualdocuments using an automatic pipeline consisting of speech detection,background music and overlapped speech removal and speaker diarization, used topresent clean speaker segments to human annotators identifying target speakers.This pipeline proved highly effective, cutting down manual processing by afactor of ten. Evaluation of the quality of the automatic processing and of thefinal output is provided. It shows the automatic processing compare toup-to-date process, and that the output provides high quality speech for mostof the selected excerpts. This method shows promise for creating large corporaof known target speakers.</description><author>Rémi Uro, David Doukhan, Albert Rilliard, Laëtitia Larcher, Anissa-Claire Adgharouamane, Marie Tahon, Antoine Laurent</author><pubDate>Fri, 26 Apr 2024 18:30:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17552v1</guid></item><item><title>All You Need is Resistance: On the Equivalence of Effective Resistance and Certain Optimal Transport Problems on Graphs</title><link>http://arxiv.org/abs/2404.15261v2</link><description>The fields of effective resistance and optimal transport on graphs are filledwith rich connections to combinatorics, geometry, machine learning, and beyond.In this article we put forth a bold claim: that the two fields should beunderstood as one and the same, up to a choice of $p$. We make this claimprecise by introducing the parameterized family of $p$-Beckmann distances forprobability measures on graphs and relate them sharply to certain Wassersteindistances. Then, we break open a suite of results including explicitconnections to optimal stopping times and random walks on graphs, graph Sobolevspaces, and a Benamou-Brenier type formula for $2$-Beckmann distance. Wefurther explore empirical implications in the world of unsupervised learningfor graph data and propose further study of the usage of these metrics whereWasserstein distance may produce computational bottlenecks.</description><author>Sawyer Robertson, Zhengchao Wan, Alexander Cloninger</author><pubDate>Fri, 26 Apr 2024 18:25:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15261v2</guid></item><item><title>Overload: Latency Attacks on Object Detection for Edge Devices</title><link>http://arxiv.org/abs/2304.05370v4</link><description>Nowadays, the deployment of deep learning-based applications is an essentialtask owing to the increasing demands on intelligent services. In this paper, weinvestigate latency attacks on deep learning applications. Unlike commonadversarial attacks for misclassification, the goal of latency attacks is toincrease the inference time, which may stop applications from responding to therequests within a reasonable time. This kind of attack is ubiquitous forvarious applications, and we use object detection to demonstrate how such kindof attacks work. We also design a framework named Overload to generate latencyattacks at scale. Our method is based on a newly formulated optimizationproblem and a novel technique, called spatial attention. This attack serves toescalate the required computing costs during the inference time, consequentlyleading to an extended inference time for object detection. It presents asignificant threat, especially to systems with limited computing resources. Weconducted experiments using YOLOv5 models on Nvidia NX. Compared to existingmethods, our method is simpler and more effective. The experimental resultsshow that with latency attacks, the inference time of a single image can beincreased ten times longer in reference to the normal setting. Moreover, ourfindings pose a potential new threat to all object detection tasks requiringnon-maximum suppression (NMS), as our attack is NMS-agnostic.</description><author>Erh-Chung Chen, Pin-Yu Chen, I-Hsin Chung, Che-rung Lee</author><pubDate>Fri, 26 Apr 2024 18:23:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05370v4</guid></item><item><title>Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo</title><link>http://arxiv.org/abs/2404.17546v1</link><description>Numerous capability and safety techniques of Large Language Models (LLMs),including RLHF, automated red-teaming, prompt engineering, and infilling, canbe cast as sampling from an unnormalized target distribution defined by a givenreward or potential function over the full sequence. In this work, we leveragethe rich toolkit of Sequential Monte Carlo (SMC) for these probabilisticinference problems. In particular, we use learned twist functions to estimatethe expected future value of the potential at each timestep, which enables usto focus inference-time computation on promising partial sequences. We proposea novel contrastive method for learning the twist functions, and establishconnections with the rich literature of soft reinforcement learning. As acomplementary application of our twisted SMC framework, we present methods forevaluating the accuracy of language model inference techniques using novelbidirectional SMC bounds on the log partition function. These bounds can beused to estimate the KL divergence between the inference and targetdistributions in both directions. We apply our inference evaluation techniquesto show that twisted SMC is effective for sampling undesirable outputs from apretrained model (a useful component of harmlessness training and automatedred-teaming), generating reviews with varied sentiment, and performinginfilling tasks.</description><author>Stephen Zhao, Rob Brekelmans, Alireza Makhzani, Roger Grosse</author><pubDate>Fri, 26 Apr 2024 18:18:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17546v1</guid></item><item><title>Subtractive Mixture Models via Squaring: Representation and Learning</title><link>http://arxiv.org/abs/2310.00724v3</link><description>Mixture models are traditionally represented and learned by adding severaldistributions as components. Allowing mixtures to subtract probability mass ordensity can drastically reduce the number of components needed to model complexdistributions. However, learning such subtractive mixtures while ensuring theystill encode a non-negative function is challenging. We investigate how tolearn and perform inference on deep subtractive mixtures by squaring them. Wedo this in the framework of probabilistic circuits, which enable us torepresent tensorized mixtures and generalize several other subtractive models.We theoretically prove that the class of squared circuits allowing subtractionscan be exponentially more expressive than traditional additive mixtures; and,we empirically show this increased expressiveness on a series of real-worlddistribution estimation tasks.</description><author>Lorenzo Loconte, Aleksanteri M. Sladek, Stefan Mengel, Martin Trapp, Arno Solin, Nicolas Gillis, Antonio Vergari</author><pubDate>Fri, 26 Apr 2024 18:08:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00724v3</guid></item><item><title>Generating Hidden Markov Models from Process Models Through Nonnegative Tensor Factorization</title><link>http://arxiv.org/abs/2210.01060v2</link><description>Monitoring of industrial processes is a critical capability in industry andin government to ensure reliability of production cycles, quick emergencyresponse, and national security. Process monitoring allows users to gauge theprogress of an organization in an industrial process or predict the degradationor aging of machine parts in processes taking place at a remote location.Similar to many data science applications, we usually only have access tolimited raw data, such as satellite imagery, short video clips, event logs, andsignatures captured by a small set of sensors. To combat data scarcity, weleverage the knowledge of Subject Matter Experts (SMEs) who are familiar withthe actions of interest. SMEs provide expert knowledge of the essentialactivities required for task completion and the resources necessary to carryout each of these activities. Various process mining techniques have beendeveloped for this type of analysis; typically such approaches combinetheoretical process models built based on domain expert insights with ad-hocintegration of available pieces of raw data. Here, we introduce a novelmathematically sound method that integrates theoretical process models (asproposed by SMEs) with interrelated minimal Hidden Markov Models (HMM), builtvia nonnegative tensor factorization. Our method consolidates: (a) theoreticalprocess models, (b) HMMs, (c) coupled nonnegative matrix-tensor factorizations,and (d) custom model selection. To demonstrate our methodology and itsabilities, we apply it on simple synthetic and real world process models.</description><author>Erik Skau, Andrew Hollis, Stephan Eidenbenz, Kim Rasmussen, Boian Alexandrov</author><pubDate>Fri, 26 Apr 2024 18:05:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.01060v2</guid></item><item><title>Predicting Properties of Nodes via Community-Aware Features</title><link>http://arxiv.org/abs/2311.04730v2</link><description>This paper shows how information about the network's community structure canbe used to define node features with high predictive power for classificationtasks. To do so, we define a family of community-aware node features andinvestigate their properties. Those features are designed to ensure that theycan be efficiently computed even for large graphs. We show that community-awarenode features contain information that cannot be completely recovered byclassical node features or node embeddings (both classical and structural) andbring value in node classification tasks. This is verified for variousclassification tasks on synthetic and real-life networks.</description><author>Bogumił Kamiński, Paweł Prałat, François Théberge, Sebastian Zając</author><pubDate>Fri, 26 Apr 2024 18:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04730v2</guid></item><item><title>Using Neural Implicit Flow To Represent Latent Dynamics Of Canonical Systems</title><link>http://arxiv.org/abs/2404.17535v1</link><description>The recently introduced class of architectures known as Neural Operators hasemerged as highly versatile tools applicable to a wide range of tasks in thefield of Scientific Machine Learning (SciML), including data representation andforecasting. In this study, we investigate the capabilities of Neural ImplicitFlow (NIF), a recently developed mesh-agnostic neural operator, forrepresenting the latent dynamics of canonical systems such as theKuramoto-Sivashinsky (KS), forced Korteweg-de Vries (fKdV), and Sine-Gordon(SG) equations, as well as for extracting dynamically relevant information fromthem. Finally we assess the applicability of NIF as a dimensionality reductionalgorithm and conduct a comparative analysis with another widely recognizedfamily of neural operators, known as Deep Operator Networks (DeepONets).</description><author>Imran Nasim, Joaõ Lucas de Sousa Almeida</author><pubDate>Fri, 26 Apr 2024 18:01:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17535v1</guid></item><item><title>Exploring the Distinctiveness and Fidelity of the Descriptions Generated by Large Vision-Language Models</title><link>http://arxiv.org/abs/2404.17534v1</link><description>Large Vision-Language Models (LVLMs) are gaining traction for theirremarkable ability to process and integrate visual and textual data. Despitetheir popularity, the capacity of LVLMs to generate precise, fine-grainedtextual descriptions has not been fully explored. This study addresses this gapby focusing on \textit{distinctiveness} and \textit{fidelity}, assessing howmodels like Open-Flamingo, IDEFICS, and MiniGPT-4 can distinguish betweensimilar objects and accurately describe visual features. We proposed theTextual Retrieval-Augmented Classification (TRAC) framework, which, byleveraging its generative capabilities, allows us to delve deeper intoanalyzing fine-grained visual description generation. This research providesvaluable insights into the generation quality of LVLMs, enhancing theunderstanding of multimodal language models. Notably, MiniGPT-4 stands out forits better ability to generate fine-grained descriptions, outperforming theother two models in this aspect. The code is provided at\url{https://anonymous.4open.science/r/Explore_FGVDs-E277}.</description><author>Yuhang Huang, Zihan Wu, Chongyang Gao, Jiawei Peng, Xu Yang</author><pubDate>Fri, 26 Apr 2024 17:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17534v1</guid></item><item><title>CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and Radiology Reports for Full-Body Scenarios</title><link>http://arxiv.org/abs/2404.15272v2</link><description>Medical Vision-Language Pretraining (Med-VLP) establishes a connectionbetween visual content from medical images and the relevant textualdescriptions. Existing Med-VLP methods primarily focus on 2D images depicting asingle body part, notably chest X-rays. In this paper, we extend the scope ofMed-VLP to encompass 3D images, specifically targeting full-body scenarios, byusing a multimodal dataset of CT images and reports. Compared with the 2Dcounterpart, 3D VLP is required to effectively capture essential semantics fromsignificantly sparser representation in 3D imaging. In this paper, we introduceCT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel methodthat constructs organ-level image-text pairs to enhance multimodal contrastivelearning, aligning grounded visual features with precise diagnostic text.Additionally, we developed an abnormality dictionary to augment contrastivelearning with diverse contrastive pairs. Our method, trained on a multimodal CTdataset comprising 44,011 organ-level vision-text pairs from 17,702 patientsacross 104 organs, demonstrates it can identify organs and abnormalities in azero-shot manner using natural languages. The performance of CT-GLIP isvalidated on a separate test set of 1,130 patients, focusing on the 16 mostfrequent abnormalities across 7 organs. The experimental results show ourmodel's superior performance over the standard CLIP framework across zero-shotand fine-tuning scenarios, using both CNN and ViT architectures.</description><author>Jingyang Lin, Yingda Xia, Jianpeng Zhang, Ke Yan, Le Lu, Jiebo Luo, Ling Zhang</author><pubDate>Fri, 26 Apr 2024 17:50:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15272v2</guid></item><item><title>Geometry-aware Reconstruction and Fusion-refined Rendering for Generalizable Neural Radiance Fields</title><link>http://arxiv.org/abs/2404.17528v1</link><description>Generalizable NeRF aims to synthesize novel views for unseen scenes. Commonpractices involve constructing variance-based cost volumes for geometryreconstruction and encoding 3D descriptors for decoding novel views. However,existing methods show limited generalization ability in challenging conditionsdue to inaccurate geometry, sub-optimal descriptors, and decoding strategies.We address these issues point by point. First, we find the variance-based costvolume exhibits failure patterns as the features of pixels corresponding to thesame point can be inconsistent across different views due to occlusions orreflections. We introduce an Adaptive Cost Aggregation (ACA) approach toamplify the contribution of consistent pixel pairs and suppress inconsistentones. Unlike previous methods that solely fuse 2D features into descriptors,our approach introduces a Spatial-View Aggregator (SVA) to incorporate 3Dcontext into descriptors through spatial and inter-view interaction. Whendecoding the descriptors, we observe the two existing decoding strategies excelin different areas, which are complementary. A Consistency-Aware Fusion (CAF)strategy is proposed to leverage the advantages of both. We incorporate theabove ACA, SVA, and CAF into a coarse-to-fine framework, termed Geometry-awareReconstruction and Fusion-refined Rendering (GeFu). GeFu attainsstate-of-the-art performance across multiple datasets. Code is available athttps://github.com/TQTQliu/GeFu .</description><author>Tianqi Liu, Xinyi Ye, Min Shi, Zihao Huang, Zhiyu Pan, Zhan Peng, Zhiguo Cao</author><pubDate>Fri, 26 Apr 2024 17:46:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17528v1</guid></item><item><title>ChemScraper: Leveraging PDF Graphics Instructions for Molecular Diagram Parsing</title><link>http://arxiv.org/abs/2311.12161v3</link><description>Most molecular diagram parsers recover chemical structure from raster images(e.g., PNGs). However, many PDFs include commands giving explicit locations andshapes for characters, lines, and polygons. We present a new parser that usesthese born-digital PDF primitives as input. The parsing model is fast andaccurate, and does not require GPUs, Optical Character Recognition (OCR), orvectorization. We use the parser to annotate raster images and then train a newmulti-task neural network for recognizing molecules in raster images. Weevaluate our parsers using SMILES and standard benchmarks, along with a novelevaluation protocol comparing molecular graphs directly that supports automaticerror compilation and reveals errors missed by SMILES-based evaluation.</description><author>Ayush Kumar Shah, Bryan Manrique Amador, Abhisek Dey, Ming Creekmore, Blake Ocampo, Scott Denmark, Richard Zanibbi</author><pubDate>Fri, 26 Apr 2024 17:43:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12161v3</guid></item><item><title>Learning Performance-Improving Code Edits</title><link>http://arxiv.org/abs/2302.07867v5</link><description>With the decline of Moore's law, optimizing program performance has become amajor focus of software research. However, high-level optimizations such as APIand algorithm changes remain elusive due to the difficulty of understanding thesemantics of code. Simultaneously, pretrained large language models (LLMs) havedemonstrated strong capabilities at solving a wide range of programming tasks.To that end, we introduce a framework for adapting LLMs to high-level programoptimization. First, we curate a dataset of performance-improving edits made byhuman programmers of over 77,000 competitive C++ programming submission pairs,accompanied by extensive unit tests. A major challenge is the significantvariability of measuring performance on commodity hardware, which can lead tospurious "improvements." To isolate and reliably evaluate the impact of programoptimizations, we design an environment based on the gem5 full systemsimulator, the de facto simulator used in academia and industry. Next, wepropose a broad range of adaptation strategies for code optimization; forprompting, these include retrieval-based few-shot prompting andchain-of-thought, and for finetuning, these include performance-conditionedgeneration and synthetic data augmentation based on self-play. A combination ofthese techniques achieves a mean speedup of 6.86 with eight generations, higherthan average optimizations from individual programmers (3.66). Using ourmodel's fastest generations, we set a new upper limit on the fastest speeduppossible for our dataset at 9.64 compared to using the fastest humansubmissions available (9.56).</description><author>Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, Amir Yazdanbakhsh</author><pubDate>Fri, 26 Apr 2024 17:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07867v5</guid></item><item><title>Large Language Model Agent as a Mechanical Designer</title><link>http://arxiv.org/abs/2404.17525v1</link><description>Conventional mechanical design paradigms rely on experts systematicallyrefining concepts through experience-guided modification and FEA to meetspecific requirements. However, this approach can be time-consuming and heavilydependent on prior knowledge and experience. While numerous machine learningmodels have been developed to streamline this intensive and expert-driveniterative process, these methods typically demand extensive training data andconsiderable computational resources. Furthermore, methods based on deeplearning are usually restricted to the specific domains and tasks for whichthey were trained, limiting their applicability across different tasks. Thiscreates a trade-off between the efficiency of automation and the demand forresources. In this study, we present a novel approach that integratespre-trained LLMs with a FEM module. The FEM module evaluates each design andprovides essential feedback, guiding the LLMs to continuously learn, plan,generate, and optimize designs without the need for domain-specific training.We demonstrate the effectiveness of our proposed framework in managing theiterative optimization of truss structures, showcasing its capability to reasonabout and refine designs according to structured feedback and criteria. Ourresults reveal that these LLM-based agents can successfully generate trussdesigns that comply with natural language specifications with a success rate ofup to 90%, which varies according to the applied constraints. By employingprompt-based optimization techniques we show that LLM based agents exhibitoptimization behavior when provided with solution-score pairs to iterativelyrefine designs to meet specifications. This ability of LLM agents to produceviable designs and optimize them based on their inherent reasoning capabilitieshighlights their potential to develop and implement effective design strategiesautonomously.</description><author>Yayati Jadhav, Amir Barati Farimani</author><pubDate>Fri, 26 Apr 2024 17:41:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17525v1</guid></item><item><title>On the Use of Large Language Models to Generate Capability Ontologies</title><link>http://arxiv.org/abs/2404.17524v1</link><description>Capability ontologies are increasingly used to model functionalities ofsystems or machines. The creation of such ontological models with allproperties and constraints of capabilities is very complex and can only be doneby ontology experts. However, Large Language Models (LLMs) have shown that theycan generate machine-interpretable models from natural language text input andthus support engineers / ontology experts. Therefore, this paper investigateshow LLMs can be used to create capability ontologies. We present a study with aseries of experiments in which capabilities with varying complexities aregenerated using different prompting techniques and with different LLMs. Errorsin the generated ontologies are recorded and compared. To analyze the qualityof the generated ontologies, a semi-automated approach based on RDF syntaxchecking, OWL reasoning, and SHACL constraints is used. The results of thisstudy are very promising because even for complex capabilities, the generatedontologies are almost free of errors.</description><author>Luis Miguel Vieira da Silva, Aljosha Köcher, Felix Gehlhoff, Alexander Fay</author><pubDate>Fri, 26 Apr 2024 17:41:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17524v1</guid></item><item><title>Enhancing Legal Compliance and Regulation Analysis with Large Language Models</title><link>http://arxiv.org/abs/2404.17522v1</link><description>This research explores the application of Large Language Models (LLMs) forautomating the extraction of requirement-related legal content in the foodsafety domain and checking legal compliance of regulatory artifacts. WithIndustry 4.0 revolutionizing the food industry and with the General DataProtection Regulation (GDPR) reshaping privacy policies and data processingagreements, there is a growing gap between regulatory analysis and recenttechnological advancements. This study aims to bridge this gap by leveragingLLMs, namely BERT and GPT models, to accurately classify legal provisions andautomate compliance checks. Our findings demonstrate promising results,indicating LLMs' significant potential to enhance legal compliance andregulatory analysis efficiency, notably by reducing manual workload andimproving accuracy within reasonable time and financial constraints.</description><author>Shabnam Hassani</author><pubDate>Fri, 26 Apr 2024 17:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17522v1</guid></item><item><title>Ag2Manip: Learning Novel Manipulation Skills with Agent-Agnostic Visual and Action Representations</title><link>http://arxiv.org/abs/2404.17521v1</link><description>Autonomous robotic systems capable of learning novel manipulation tasks arepoised to transform industries from manufacturing to service automation.However, modern methods (e.g., VIP and R3M) still face significant hurdles,notably the domain gap among robotic embodiments and the sparsity of successfultask executions within specific action spaces, resulting in misaligned andambiguous task representations. We introduce Ag2Manip (Agent-Agnosticrepresentations for Manipulation), a framework aimed at surmounting thesechallenges through two key innovations: a novel agent-agnostic visualrepresentation derived from human manipulation videos, with the specifics ofembodiments obscured to enhance generalizability; and an agent-agnostic actionrepresentation abstracting a robot's kinematics to a universal agent proxy,emphasizing crucial interactions between end-effector and object. Ag2Manip'sempirical validation across simulated benchmarks like FrankaKitchen, ManiSkill,and PartManip shows a 325% increase in performance, achieved withoutdomain-specific demonstrations. Ablation studies underline the essentialcontributions of the visual and action representations to this success.Extending our evaluations to the real world, Ag2Manip significantly improvesimitation learning success rates from 50% to 77.5%, demonstrating itseffectiveness and generalizability across both simulated and physicalenvironments.</description><author>Puhao Li, Tengyu Liu, Yuyang Li, Muzhi Han, Haoran Geng, Shu Wang, Yixin Zhu, Song-Chun Zhu, Siyuan Huang</author><pubDate>Fri, 26 Apr 2024 17:40:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17521v1</guid></item><item><title>Transformers in the Service of Description Logic-based Contexts</title><link>http://arxiv.org/abs/2311.08941v3</link><description>Recent advancements in transformer-based models have initiated researchinterests in investigating their ability to learn to perform reasoning tasks.However, most of the contexts used for this purpose are in practice verysimple: generated from short (fragments of) first-order logic sentences withonly a few logical operators and quantifiers. In this work, we construct thenatural language dataset, DELTA$_D$, using the description logic language$\mathcal{ALCQ}$. DELTA$_D$ contains 384K examples, and increases in twodimensions: i) reasoning depth, and ii) linguistic complexity. In this way, wesystematically investigate the reasoning ability of a supervised fine-tunedDeBERTa-based model and of two large language models (GPT-3.5, GPT-4) withfew-shot prompting. Our results demonstrate that the DeBERTa-based model canmaster the reasoning task and that the performance of GPTs can improvesignificantly even when a small number of samples is provided (9 shots). Weopen-source our code and datasets.</description><author>Angelos Poulis, Eleni Tsalapati, Manolis Koubarakis</author><pubDate>Fri, 26 Apr 2024 17:32:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08941v3</guid></item><item><title>MAIRA-1: A specialised large multimodal model for radiology report generation</title><link>http://arxiv.org/abs/2311.13668v3</link><description>We present a radiology-specific multimodal model for the task for generatingradiological reports from chest X-rays (CXRs). Our work builds on the idea thatlarge language model(s) can be equipped with multimodal capabilities throughalignment with pre-trained vision encoders. On natural images, this has beenshown to allow multimodal models to gain image understanding and descriptioncapabilities. Our proposed model (MAIRA-1) leverages a CXR-specific imageencoder in conjunction with a fine-tuned large language model based onVicuna-7B, and text-based data augmentation, to produce reports withstate-of-the-art quality. In particular, MAIRA-1 significantly improves on theradiologist-aligned RadCliQ metric and across all lexical metrics considered.Manual review of model outputs demonstrates promising fluency and accuracy ofgenerated reports while uncovering failure modes not captured by existingevaluation practices. More information and resources can be found on theproject website: https://aka.ms/maira.</description><author>Stephanie L. Hyland, Shruthi Bannur, Kenza Bouzid, Daniel C. Castro, Mercy Ranjit, Anton Schwaighofer, Fernando Pérez-García, Valentina Salvatelli, Shaury Srivastav, Anja Thieme, Noel Codella, Matthew P. Lungren, Maria Teodora Wetscherek, Ozan Oktay, Javier Alvarez-Valle</author><pubDate>Fri, 26 Apr 2024 17:29:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13668v3</guid></item><item><title>DeepClean: Machine Unlearning on the Cheap by Resetting Privacy Sensitive Weights using the Fisher Diagonal</title><link>http://arxiv.org/abs/2311.10448v2</link><description>Machine learning models trained on sensitive or private data caninadvertently memorize and leak that information. Machine unlearning seeks toretroactively remove such details from model weights to protect privacy. Wecontribute a lightweight unlearning algorithm that leverages the FisherInformation Matrix (FIM) for selective forgetting. Prior work in this arearequires full retraining or large matrix inversions, which are computationallyexpensive. Our key insight is that the diagonal elements of the FIM, whichmeasure the sensitivity of log-likelihood to changes in weights, containsufficient information for effective forgetting. Specifically, we compute theFIM diagonal over two subsets -- the data to retain and forget -- for alltrainable weights. This diagonal representation approximates the complete FIMwhile dramatically reducing computation. We then use it to selectively updateweights to maximize forgetting of the sensitive subset while minimizing impacton the retained subset. Experiments show that our algorithm can successfullyforget any randomly selected subsets of training data across neural networkarchitectures. By leveraging the FIM diagonal, our approach provides aninterpretable, lightweight, and efficient solution for machine unlearning withpractical privacy benefits.</description><author>Jiaeli Shi, Najah Ghalyan, Kostis Gourgoulias, John Buford, Sean Moran</author><pubDate>Fri, 26 Apr 2024 17:29:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10448v2</guid></item><item><title>A Comprehensive Evaluation on Event Reasoning of Large Language Models</title><link>http://arxiv.org/abs/2404.17513v1</link><description>Event reasoning is a fundamental ability that underlies many applications. Itrequires event schema knowledge to perform global reasoning and needs to dealwith the diversity of the inter-event relations and the reasoning paradigms.How well LLMs accomplish event reasoning on various relations and reasoningparadigms remains unknown. To mitigate this disparity, we comprehensivelyevaluate the abilities of event reasoning of LLMs. We introduce a novelbenchmark EV2 for EValuation of EVent reasoning. EV2 consists of two levels ofevaluation of schema and instance and is comprehensive in relations andreasoning paradigms. We conduct extensive experiments on EV2. We find that LLMshave abilities to accomplish event reasoning but their performances are farfrom satisfactory. We also notice the imbalance of event reasoning abilities inLLMs. Besides, LLMs have event schema knowledge, however, they're not alignedwith humans on how to utilize the knowledge. Based on these findings, weintroduce two methods to guide the LLMs to utilize the event schema knowledge.Both methods achieve improvements.</description><author>Zhengwei Tao, Zhi Jin, Yifan Zhang, Xiancai Chen, Xiaoying Bai, Yue Fang, Haiyan Zhao, Jia Li, Chongyang Tao</author><pubDate>Fri, 26 Apr 2024 17:28:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17513v1</guid></item><item><title>Bridging the Fairness Divide: Achieving Group and Individual Fairness in Graph Neural Networks</title><link>http://arxiv.org/abs/2404.17511v1</link><description>Graph neural networks (GNNs) have emerged as a powerful tool for analyzingand learning from complex data structured as graphs, demonstrating remarkableeffectiveness in various applications, such as social network analysis,recommendation systems, and drug discovery. However, despite their impressiveperformance, the fairness problem has increasingly gained attention as acrucial aspect to consider. Existing research in graph learning focuses oneither group fairness or individual fairness. However, since each conceptprovides unique insights into fairness from distinct perspectives, integratingthem into a fair graph neural network system is crucial. To the best of ourknowledge, no study has yet to comprehensively tackle both individual and groupfairness simultaneously. In this paper, we propose a new concept of individualfairness within groups and a novel framework named Fairness for Group andIndividual (FairGI), which considers both group fairness and individualfairness within groups in the context of graph learning. FairGI employs thesimilarity matrix of individuals to achieve individual fairness within groups,while leveraging adversarial learning to address group fairness in terms ofboth Equal Opportunity and Statistical Parity. The experimental resultsdemonstrate that our approach not only outperforms other state-of-the-artmodels in terms of group fairness and individual fairness within groups, butalso exhibits excellent performance in population-level individual fairness,while maintaining comparable prediction accuracy.</description><author>Duna Zhan, Dongliang Guo, Pengsheng Ji, Sheng Li</author><pubDate>Fri, 26 Apr 2024 17:26:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17511v1</guid></item><item><title>Probing Conceptual Understanding of Large Visual-Language Models</title><link>http://arxiv.org/abs/2304.03659v3</link><description>In recent years large visual-language (V+L) models have achieved greatsuccess in various downstream tasks. However, it is not well studied whetherthese models have a conceptual grasp of the visual content. In this work wefocus on conceptual understanding of these large V+L models. To facilitate thisstudy, we propose novel benchmarking datasets for probing three differentaspects of content understanding, 1) \textit{relations}, 2)\textit{composition}, and 3) \textit{context}. Our probes are grounded incognitive science and help determine if a V+L model can, for example, determineif snow garnished with a man is implausible, or if it can identify beachfurniture by knowing it is located on a beach. We experimented with many recentstate-of-the-art V+L models and observe that these models mostly \textit{failto demonstrate} a conceptual understanding. This study reveals severalinteresting insights such as that \textit{cross-attention} helps learningconceptual understanding, and that CNNs are better with \textit{texture andpatterns}, while Transformers are better at \textit{color and shape}. Wefurther utilize some of these insights and investigate a \textit{simplefinetuning technique} that rewards the three conceptual understanding measureswith promising initial results. The proposed benchmarks will drive thecommunity to delve deeper into conceptual understanding and foster advancementsin the capabilities of large V+L models. The code and dataset is available at:\url{https://tinyurl.com/vlm-robustness}</description><author>Madeline Schiappa, Raiyaan Abdullah, Shehreen Azad, Jared Claypoole, Michael Cogswell, Ajay Divakaran, Yogesh Rawat</author><pubDate>Fri, 26 Apr 2024 17:23:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03659v3</guid></item><item><title>Constrained Neural Networks for Interpretable Heuristic Creation to Optimise Computer Algebra Systems</title><link>http://arxiv.org/abs/2404.17508v1</link><description>We present a new methodology for utilising machine learning technology insymbolic computation research. We explain how a well known human-designedheuristic to make the choice of variable ordering in cylindrical algebraicdecomposition may be represented as a constrained neural network. This allowsus to then use machine learning methods to further optimise the heuristic,leading to new networks of similar size, representing new heuristics of similarcomplexity as the original human-designed one. We present this as a form ofante-hoc explainability for use in computer algebra development.</description><author>Dorian Florescu, Matthew England</author><pubDate>Fri, 26 Apr 2024 17:20:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17508v1</guid></item><item><title>HYPE: Hyperbolic Entailment Filtering for Underspecified Images and Texts</title><link>http://arxiv.org/abs/2404.17507v1</link><description>In an era where the volume of data drives the effectiveness ofself-supervised learning, the specificity and clarity of data semantics play acrucial role in model training. Addressing this, we introduce HYPerbolicEntailment filtering (HYPE), a novel methodology designed to meticulouslyextract modality-wise meaningful and well-aligned data from extensive, noisyimage-text pair datasets. Our approach leverages hyperbolic embeddings and theconcept of entailment cones to evaluate and filter out samples with meaninglessor underspecified semantics, focusing on enhancing the specificity of each datasample. HYPE not only demonstrates a significant improvement in filteringefficiency but also sets a new state-of-the-art in the DataComp benchmark whencombined with existing filtering techniques. This breakthrough showcases thepotential of HYPE to refine the data selection process, thereby contributing tothe development of more accurate and efficient self-supervised learning models.Additionally, the image specificity $\epsilon_{i}$ can be independently appliedto induce an image-only dataset from an image-text or image-only data pool fortraining image-only self-supervised models and showed superior performance whencompared to the dataset induced by CLIP score.</description><author>Wonjae Kim, Sanghyuk Chun, Taekyung Kim, Dongyoon Han, Sangdoo Yun</author><pubDate>Fri, 26 Apr 2024 17:19:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17507v1</guid></item><item><title>Inhomogeneous illuminated image enhancement under extremely low visibility condition</title><link>http://arxiv.org/abs/2404.17503v1</link><description>Imaging through fog significantly impacts fields such as object detection andrecognition. In conditions of extremely low visibility, essential imageinformation can be obscured, rendering standard extraction methods ineffective.Traditional digital processing techniques, such as histogram stretching, aim tomitigate fog effects by enhancing object light contrast diminished byatmospheric scattering. However, these methods often experience reduceeffectiveness under inhomogeneous illumination. This paper introduces a novelapproach that adaptively filters background illumination under extremely lowvisibility and preserve only the essential signal information. Additionally, weemploy a visual optimization strategy based on image gradients to eliminategrayscale banding. Finally, the image is transformed to achieve high contrastand maintain fidelity to the original information through maximum histogramequalization. Our proposed method significantly enhances signal clarity inconditions of extremely low visibility and outperforms existing algorithms.</description><author>Libang Chen, Yikun Liu, Jianying Zhou</author><pubDate>Fri, 26 Apr 2024 17:09:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17503v1</guid></item><item><title>Boosting Defect Detection in Manufacturing using Tensor Convolutional Neural Networks</title><link>http://arxiv.org/abs/2401.01373v2</link><description>Defect detection is one of the most important yet challenging tasks in thequality control stage in the manufacturing sector. In this work, we introduce aTensor Convolutional Neural Network (T-CNN) and examine its performance on areal defect detection application in one of the components of the ultrasonicsensors produced at Robert Bosch's manufacturing plants. Our quantum-inspiredT-CNN operates on a reduced model parameter space to substantially improve thetraining speed and performance of an equivalent CNN model without sacrificingaccuracy. More specifically, we demonstrate how T-CNNs are able to reach thesame performance as classical CNNs as measured by quality metrics, with up tofifteen times fewer parameters and 4% to 19% faster training times. Our resultsdemonstrate that the T-CNN greatly outperforms the results of traditional humanvisual inspection, providing value in a current real application inmanufacturing.</description><author>Pablo Martin-Ramiro, Unai Sainz de la Maza, Sukhbinder Singh, Roman Orus, Samuel Mugel</author><pubDate>Fri, 26 Apr 2024 17:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01373v2</guid></item><item><title>Learning text-to-video retrieval from image captioning</title><link>http://arxiv.org/abs/2404.17498v1</link><description>We describe a protocol to study text-to-video retrieval training withunlabeled videos, where we assume (i) no access to labels for any videos, i.e.,no access to the set of ground-truth captions, but (ii) access to labeledimages in the form of text. Using image expert models is a realistic scenariogiven that annotating images is cheaper therefore scalable, in contrast toexpensive video labeling schemes. Recently, zero-shot image experts such asCLIP have established a new strong baseline for video understanding tasks. Inthis paper, we make use of this progress and instantiate the image experts fromtwo types of models: a text-to-image retrieval model to provide an initialbackbone, and image captioning models to provide supervision signal intounlabeled videos. We show that automatically labeling video frames with imagecaptioning allows text-to-video retrieval training. This process adapts thefeatures to the target domain at no manual annotation cost, consequentlyoutperforming the strong zero-shot CLIP baseline. During training, we samplecaptions from multiple video frames that best match the visual content, andperform a temporal pooling over frame representations by scoring framesaccording to their relevance to each caption. We conduct extensive ablations toprovide insights and demonstrate the effectiveness of this simple framework byoutperforming the CLIP zero-shot baselines on text-to-video retrieval on threestandard datasets, namely ActivityNet, MSR-VTT, and MSVD.</description><author>Lucas Ventura, Cordelia Schmid, Gül Varol</author><pubDate>Fri, 26 Apr 2024 16:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17498v1</guid></item><item><title>Q-Learning to navigate turbulence without a map</title><link>http://arxiv.org/abs/2404.17495v1</link><description>We consider the problem of olfactory searches in a turbulent environment. Wefocus on agents that respond solely to odor stimuli, with no access to spatialperception nor prior information about the odor location. We ask whethernavigation strategies to a target can be learned robustly within a sequentialdecision making framework. We develop a reinforcement learning algorithm usinga small set of interpretable olfactory states and train it with realisticturbulent odor cues. By introducing a temporal memory, we demonstrate that twosalient features of odor traces, discretized in few olfactory states, aresufficient to learn navigation in a realistic odor plume. Performance isdictated by the sparse nature of turbulent plumes. An optimal memory existswhich ignores blanks within the plume and activates a recovery strategy outsidethe plume. We obtain the best performance by letting agents learn theirrecovery strategy and show that it is mostly casting cross wind, similar tobehavior observed in flying insects. The optimal strategy is robust tosubstantial changes in the odor plumes, suggesting minor parameter tuning maybe sufficient to adapt to different environments.</description><author>Marco Rando, Martin James, Alessandro Verri, Lorenzo Rosasco, Agnese Seminara</author><pubDate>Fri, 26 Apr 2024 16:51:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17495v1</guid></item><item><title>Causally Abstracted Multi-armed Bandits</title><link>http://arxiv.org/abs/2404.17493v1</link><description>Multi-armed bandits (MAB) and causal MABs (CMAB) are established frameworksfor decision-making problems. The majority of prior work typically studies andsolves individual MAB and CMAB in isolation for a given problem and associateddata. However, decision-makers are often faced with multiple related problemsand multi-scale observations where joint formulations are needed in order toefficiently exploit the problem structures and data dependencies. Transferlearning for CMABs addresses the situation where models are defined onidentical variables, although causal connections may differ. In this work, weextend transfer learning to setups involving CMABs defined on potentiallydifferent variables, with varying degrees of granularity, and related via anabstraction map. Formally, we introduce the problem of causally abstracted MABs(CAMABs) by relying on the theory of causal abstraction in order to express arigorous abstraction map. We propose algorithms to learn in a CAMAB, and studytheir regret. We illustrate the limitations and the strengths of our algorithmson a real-world scenario related to online advertising.</description><author>Fabio Massimo Zennaro, Nicholas Bishop, Joel Dyer, Yorgos Felekis, Anisoara Calinescu, Michael Wooldridge, Theodoros Damoulas</author><pubDate>Fri, 26 Apr 2024 16:48:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17493v1</guid></item><item><title>Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific Boundaries for Domain Adaptation</title><link>http://arxiv.org/abs/2403.18360v3</link><description>Most domain adaptation (DA) methods are based on either a convolutionalneural networks (CNNs) or a vision transformers (ViTs). They align thedistribution differences between domains as encoders without considering theirunique characteristics. For instance, ViT excels in accuracy due to itssuperior ability to capture global representations, while CNN has an advantagein capturing local representations. This fact has led us to design a hybridmethod to fully take advantage of both ViT and CNN, called ExplicitlyClass-specific Boundaries (ECB). ECB learns CNN on ViT to combine theirdistinct strengths. In particular, we leverage ViT's properties to explicitlyfind class-specific decision boundaries by maximizing the discrepancy betweenthe outputs of the two classifiers to detect target samples far from the sourcesupport. In contrast, the CNN encoder clusters target features based on thepreviously defined class-specific boundaries by minimizing the discrepancybetween the probabilities of the two classifiers. Finally, ViT and CNN mutuallyexchange knowledge to improve the quality of pseudo labels and reduce theknowledge discrepancies of these models. Compared to conventional DA methods,our ECB achieves superior performance, which verifies its effectiveness in thishybrid model. The project website can be foundhttps://dotrannhattuong.github.io/ECB/website.</description><author>Ba Hung Ngo, Nhat-Tuong Do-Tran, Tuan-Ngoc Nguyen, Hae-Gon Jeon, Tae Jong Choi</author><pubDate>Fri, 26 Apr 2024 16:46:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18360v3</guid></item><item><title>Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation</title><link>http://arxiv.org/abs/2404.17489v1</link><description>Contrastive learning is a model pre-training technique by first creatingsimilar views of the original data, and then encouraging the data and itscorresponding views to be close in the embedding space. Contrastive learninghas witnessed success in image and natural language data, thanks to thedomain-specific augmentation techniques that are both intuitive and effective.Nonetheless, in tabular domain, the predominant augmentation technique forcreating views is through corrupting tabular entries via swapping values, whichis not as sound or effective. We propose a simple yet powerful improvement tothis augmentation technique: corrupting tabular data conditioned on classidentity. Specifically, when corrupting a specific tabular entry from an anchorrow, instead of randomly sampling a value in the same feature column from theentire table uniformly, we only sample from rows that are identified to bewithin the same class as the anchor row. We assume the semi-supervised learningsetting, and adopt the pseudo labeling technique for obtaining class identitiesover all table rows. We also explore the novel idea of selecting features to becorrupted based on feature correlation structures. Extensive experiments showthat the proposed approach consistently outperforms the conventional corruptionmethod for tabular data classification tasks. Our code is available athttps://github.com/willtop/Tabular-Class-Conditioned-SSL.</description><author>Wei Cui, Rasa Hosseinzadeh, Junwei Ma, Tongzi Wu, Yi Sui, Keyvan Golestan</author><pubDate>Fri, 26 Apr 2024 16:43:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17489v1</guid></item><item><title>Low Cost Machine Vision for Insect Classification</title><link>http://arxiv.org/abs/2404.17488v1</link><description>Preserving the number and diversity of insects is one of our society's mostimportant goals in the area of environmental sustainability. A prerequisite forthis is a systematic and up-scaled monitoring in order to detect correlationsand identify countermeasures. Therefore, automatized monitoring using livetraps is important, but so far there is no system that provides image data ofsufficient detailed information for entomological classification. In this work, we present an imaging method as part of a multisensor systemdeveloped as a low-cost, scalable, open-source system that is adaptable toclassical trap types. The image quality meets the requirements needed forclassification in the taxonomic tree. Therefore, illumination and resolutionhave been optimized and motion artefacts have been suppressed. The system isevaluated exemplarily on a dataset consisting of 16 insect species of the sameas well as different genus, family and order. We demonstrate that standardCNN-architectures like ResNet50 (pretrained on iNaturalist data) or MobileNetperform very well for the prediction task after re-training. Smaller custommade CNNs also lead to promising results. Classification accuracy of $&gt;96\%$has been achieved. Moreover, it was proved that image cropping of insects isnecessary for classification of species with high inter-class similarity.</description><author>Danja Brandt, Martin Tschaikner, Teodor Chiaburu, Henning Schmidt, Ilona Schrimpf, Alexandra Stadel, Ingeborg E. Beckers, Frank Haußer</author><pubDate>Fri, 26 Apr 2024 16:43:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17488v1</guid></item><item><title>Conformal Prediction with Learned Features</title><link>http://arxiv.org/abs/2404.17487v1</link><description>In this paper, we focus on the problem of conformal prediction withconditional guarantees. Prior work has shown that it is impossible to constructnontrivial prediction sets with full conditional coverage guarantees. A wealthof research has considered relaxations of full conditional guarantees, relyingon some predefined uncertainty structures. Departing from this line ofthinking, we propose Partition Learning Conformal Prediction (PLCP), aframework to improve conditional validity of prediction sets through learninguncertainty-guided features from the calibration data. We implement PLCPefficiently with alternating gradient descent, utilizing off-the-shelf machinelearning models. We further analyze PLCP theoretically and provide conditionalguarantees for infinite and finite sample sizes. Finally, our experimentalresults over four real-world and synthetic datasets show the superiorperformance of PLCP compared to state-of-the-art methods in terms of coverageand length in both classification and regression scenarios.</description><author>Shayan Kiyani, George Pappas, Hamed Hassani</author><pubDate>Fri, 26 Apr 2024 16:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17487v1</guid></item><item><title>Adversarial Estimation of Riesz Representers</title><link>http://arxiv.org/abs/2101.00009v3</link><description>Many causal parameters are linear functionals of an underlying regression.The Riesz representer is a key component in the asymptotic variance of asemiparametrically estimated linear functional. We propose an adversarialframework to estimate the Riesz representer using general function spaces. Weprove a nonasymptotic mean square rate in terms of an abstract quantity calledthe critical radius, then specialize it for neural networks, random forests,and reproducing kernel Hilbert spaces as leading cases. Our estimators arehighly compatible with targeted and debiased machine learning with samplesplitting; our guarantees directly verify general conditions for inference thatallow mis-specification. We also use our guarantees to prove inference withoutsample splitting, based on stability or complexity. Our estimators achievenominal coverage in highly nonlinear simulations where some previous methodsbreak down. They shed new light on the heterogeneous effects of matchinggrants.</description><author>Victor Chernozhukov, Whitney Newey, Rahul Singh, Vasilis Syrgkanis</author><pubDate>Fri, 26 Apr 2024 16:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2101.00009v3</guid></item><item><title>TextGaze: Gaze-Controllable Face Generation with Natural Language</title><link>http://arxiv.org/abs/2404.17486v1</link><description>Generating face image with specific gaze information has attractedconsiderable attention. Existing approaches typically input gaze valuesdirectly for face generation, which is unnatural and requires annotated gazedatasets for training, thereby limiting its application. In this paper, wepresent a novel gaze-controllable face generation task. Our approach inputstextual descriptions that describe human gaze and head behavior and generatescorresponding face images. Our work first introduces a text-of-gaze datasetcontaining over 90k text descriptions spanning a dense distribution of gaze andhead poses. We further propose a gaze-controllable text-to-face method. Ourmethod contains a sketch-conditioned face diffusion module and a model-basedsketch diffusion module. We define a face sketch based on facial landmarks andeye segmentation map. The face diffusion module generates face images from theface sketch, and the sketch diffusion module employs a 3D face model togenerate face sketch from text description. Experiments on the FFHQ datasetshow the effectiveness of our method. We will release our dataset and code forfuture research.</description><author>Hengfei Wang, Zhongqun Zhang, Yihua Cheng, Hyung Jin Chang</author><pubDate>Fri, 26 Apr 2024 16:42:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17486v1</guid></item><item><title>Sparse Reconstruction of Optical Doppler Tomography Based on State Space Model</title><link>http://arxiv.org/abs/2404.17484v1</link><description>Optical Doppler Tomography (ODT) is a blood flow imaging technique popularlyused in bioengineering applications. The fundamental unit of ODT is the 1Dfrequency response along the A-line (depth), named raw A-scan. A 2D ODT image(B-scan) is obtained by first sensing raw A-scans along the B-line (width), andthen constructing the B-scan from these raw A-scans via magnitude-phaseanalysis and post-processing. To obtain a high-resolution B-scan with a preciseflow map, densely sampled A-scans are required in current methods, causing bothcomputational and storage burdens. To address this issue, in this paper wepropose a novel sparse reconstruction framework with four main sequentialsteps: 1) early magnitude-phase fusion that encourages rich interaction of thecomplementary information in magnitude and phase, 2) State Space Model(SSM)-based representation learning, inspired by recent successes in Mamba andVMamba, to naturally capture both the intra-A-scan sequential information andbetween-A-scan interactions, 3) an Inception-based Feedforward Network module(IncFFN) to further boost the SSM-module, and 4) a B-line Pixel Shuffle (BPS)layer to effectively reconstruct the final results. In the experiments onreal-world animal data, our method shows clear effectiveness in reconstructionaccuracy. As the first application of SSM for image reconstruction tasks, weexpect our work to inspire related explorations in not only efficient ODTimaging techniques but also generic image enhancement.</description><author>Zhenghong Li, Jiaxiang Ren, Wensheng Cheng, Congwu Du, Yingtian Pan, Haibin Ling</author><pubDate>Fri, 26 Apr 2024 16:37:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17484v1</guid></item><item><title>Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation</title><link>http://arxiv.org/abs/2404.17483v1</link><description>There is a growing interest in estimating heterogeneous treatment effectsacross individuals using their high-dimensional feature attributes. Achievinghigh performance in such high-dimensional heterogeneous treatment effectestimation is challenging because in this setup, it is usual that some featuresinduce sample selection bias while others do not but are predictive ofpotential outcomes. To avoid losing such predictive feature information,existing methods learn separate feature representations using the inverse ofprobability weighting (IPW). However, due to the numerically unstable IPWweights, they suffer from estimation bias under a finite sample setup. Todevelop a numerically robust estimator via weighted representation learning, wepropose a differentiable Pareto-smoothed weighting framework that replacesextreme weight values in an end-to-end fashion. Experimental results show thatby effectively correcting the weight values, our method outperforms theexisting ones, including traditional weighting schemes.</description><author>Yoichi Chikahara, Kansei Ushiyama</author><pubDate>Fri, 26 Apr 2024 16:34:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17483v1</guid></item><item><title>ReproHum #0087-01: Human Evaluation Reproduction Report for Generating Fact Checking Explanations</title><link>http://arxiv.org/abs/2404.17481v1</link><description>This paper presents a partial reproduction of Generating Fact CheckingExplanations by Anatanasova et al (2020) as part of the ReproHum element of theReproNLP shared task to reproduce the findings of NLP research regarding humanevaluation. This shared task aims to investigate the extent to which NLP as afield is becoming more or less reproducible over time. Following theinstructions provided by the task organisers and the original authors, wecollect relative rankings of 3 fact-checking explanations (comprising a goldstandard and the outputs of 2 models) for 40 inputs on the criteria ofCoverage. The results of our reproduction and reanalysis of the original work'sraw results lend support to the original findings, with similar patterns seenbetween the original work and our reproduction. Whilst we observe slightvariation from the original results, our findings support the main conclusionsdrawn by the original authors pertaining to the efficacy of their proposedmodels.</description><author>Tyler Loakman, Chenghua Lin</author><pubDate>Fri, 26 Apr 2024 16:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17481v1</guid></item><item><title>CEval: A Benchmark for Evaluating Counterfactual Text Generation</title><link>http://arxiv.org/abs/2404.17475v1</link><description>Counterfactual text generation aims to minimally change a text, such that itis classified differently. Judging advancements in method development forcounterfactual text generation is hindered by a non-uniform usage of data setsand metrics in related work. We propose CEval, a benchmark for comparingcounterfactual text generation methods. CEval unifies counterfactual and textquality metrics, includes common counterfactual datasets with humanannotations, standard baselines (MICE, GDBA, CREST) and the open-sourcelanguage model LLAMA-2. Our experiments found no perfect method for generatingcounterfactual text. Methods that excel at counterfactual metrics often producelower-quality text while LLMs with simple prompts generate high-quality textbut struggle with counterfactual criteria. By making CEval available as anopen-source Python library, we encourage the community to contribute moremethods and maintain consistent evaluation in future work.</description><author>Van Bach Nguyen, Jörg Schlötterer, Christin Seifert</author><pubDate>Fri, 26 Apr 2024 16:23:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17475v1</guid></item><item><title>Open World Learning Graph Convolution for Latency Estimation in Routing Networks</title><link>http://arxiv.org/abs/2207.14643v2</link><description>Accurate routing network status estimation is a key component in SoftwareDefined Networking. However, existing deep-learning-based methods for modelingnetwork routing are not able to extrapolate towards unseen featuredistributions. Nor are they able to handle scaled and drifted networkattributes in test sets that include open-world inputs. To deal with thesechallenges, we propose a novel approach for modeling network routing, usingGraph Neural Networks. Our method can also be used for network-latencyestimation. Supported by a domain-knowledge-assisted graph formulation, ourmodel shares a stable performance across different network sizes andconfigurations of routing networks, while at the same time being able toextrapolate towards unseen sizes, configurations, and user behavior. We showthat our model outperforms most conventional deep-learning-based models, interms of prediction accuracy, computational resources, inference speed, as wellas ability to generalize towards open-world input.</description><author>Yifei Jin, Marios Daoutis, Sarunas Girdzijauskas, Aristides Gionis</author><pubDate>Fri, 26 Apr 2024 16:22:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.14643v2</guid></item><item><title>FTL: Transfer Learning Nonlinear Plasma Dynamic Transitions in Low Dimensional Embeddings via Deep Neural Networks</title><link>http://arxiv.org/abs/2404.17466v1</link><description>Deep learning algorithms provide a new paradigm to study high-dimensionaldynamical behaviors, such as those in fusion plasma systems. Development ofnovel model reduction methods, coupled with detection of abnormal modes withplasma physics, opens a unique opportunity for building efficient models toidentify plasma instabilities for real-time control. Our Fusion TransferLearning (FTL) model demonstrates success in reconstructing nonlinear kink modestructures by learning from a limited amount of nonlinear simulation data. Theknowledge transfer process leverages a pre-trained neural encoder-decodernetwork, initially trained on linear simulations, to effectively capturenonlinear dynamics. The low-dimensional embeddings extract the coherentstructures of interest, while preserving the inherent dynamics of the complexsystem. Experimental results highlight FTL's capacity to capture transitionalbehaviors and dynamical features in plasma dynamics -- a task often challengingfor conventional methods. The model developed in this study is generalizableand can be extended broadly through transfer learning to address variousmagnetohydrodynamics (MHD) modes.</description><author>Zhe Bai, Xishuo Wei, William Tang, Leonid Oliker, Zhihong Lin, Samuel Williams</author><pubDate>Fri, 26 Apr 2024 16:08:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17466v1</guid></item><item><title>Fast Abstracts and Student Forum Proceedings -- EDCC 2024 -- 19th European Dependable Computing Conference</title><link>http://arxiv.org/abs/2404.17465v1</link><description>The goal of the Fast Abstracts track is to bring together researchers andpractitioners working on dependable computing to discuss work in progress oropinion pieces. Contributions are welcome from academia and industry. FastAbstracts aim to serve as a rapid and flexible mechanism to: (i) Report oncurrent work that may or may not be complete; (ii) Introduce new ideas to thecommunity; (iii) State positions on controversial issues or open problems; (iv)Share lessons learnt from real-word dependability engineering; and (v) Debunkor question results from other papers based on contra-indications. The StudentForum aims at creating a vibrant and friendly environment where students canpresent and discuss their work, and exchange ideas and experiences with otherstudents, researchers and industry. One of the key goals of the Forum is toprovide students with feedback on their preliminary results that might helpwith their future research directions.</description><author>Simona Bernardi, Tommaso Zoppi</author><pubDate>Fri, 26 Apr 2024 16:08:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17465v1</guid></item><item><title>FENet: Focusing Enhanced Network for Lane Detection</title><link>http://arxiv.org/abs/2312.17163v5</link><description>Inspired by human driving focus, this research pioneers networks augmentedwith Focusing Sampling, Partial Field of View Evaluation, Enhanced FPNarchitecture and Directional IoU Loss - targeted innovations addressingobstacles to precise lane detection for autonomous driving. Experimentsdemonstrate our Focusing Sampling strategy, emphasizing vital distant detailsunlike uniform approaches, significantly boosts both benchmark and practicalcurved/distant lane recognition accuracy essential for safety. While FENetV1achieves state-of-the-art conventional metric performance via enhancementsisolating perspective-aware contexts mimicking driver vision, FENetV2 provesmost reliable on the proposed Partial Field analysis. Hence we specificallyrecommend V2 for practical lane navigation despite fractional degradation onstandard entire-image measures. Future directions include collecting on-roaddata and integrating complementary dual frameworks to further breakthroughsguided by human perception principles. The Code is available athttps://github.com/HanyangZhong/FENet.</description><author>Liman Wang, Hanyang Zhong</author><pubDate>Fri, 26 Apr 2024 16:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17163v5</guid></item><item><title>Bayesian Federated Inference for Survival Models</title><link>http://arxiv.org/abs/2404.17464v1</link><description>In cancer research, overall survival and progression free survival are oftenanalyzed with the Cox model. To estimate accurately the parameters in themodel, sufficient data and, more importantly, sufficient events need to beobserved. In practice, this is often a problem. Merging data sets fromdifferent medical centers may help, but this is not always possible due tostrict privacy legislation and logistic difficulties. Recently, the BayesianFederated Inference (BFI) strategy for generalized linear models was proposed.With this strategy the statistical analyses are performed in the local centerswhere the data were collected (or stored) and only the inference results arecombined to a single estimated model; merging data is not necessary. The BFImethodology aims to compute from the separate inference results in the localcenters what would have been obtained if the analysis had been based on themerged data sets. In this paper we generalize the BFI methodology as initiallydeveloped for generalized linear models to survival models. Simulation studiesand real data analyses show excellent performance; i.e., the results obtainedwith the BFI methodology are very similar to the results obtained by analyzingthe merged data. An R package for doing the analyses is available.</description><author>Hassan Pazira, Emanuele Massa, Jetty AM Weijers, Anthony CC Coolen, Marianne A Jonker</author><pubDate>Fri, 26 Apr 2024 16:05:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17464v1</guid></item><item><title>Multi-layer random features and the approximation power of neural networks</title><link>http://arxiv.org/abs/2404.17461v1</link><description>A neural architecture with randomly initialized weights, in the infinitewidth limit, is equivalent to a Gaussian Random Field whose covariance functionis the so-called Neural Network Gaussian Process kernel (NNGP). We prove that areproducing kernel Hilbert space (RKHS) defined by the NNGP contains onlyfunctions that can be approximated by the architecture. To achieve a certainapproximation error the required number of neurons in each layer is defined bythe RKHS norm of the target function. Moreover, the approximation can beconstructed from a supervised dataset by a random multi-layer representation ofan input vector, together with training of the last layer's weights. For a 2-layer NN and a domain equal to an $n-1$-dimensional sphere in${\mathbb R}^n$, we compare the number of neurons required by Barron's theoremand by the multi-layer features construction. We show that if eigenvalues ofthe integral operator of the NNGP decay slower than $k^{-n-\frac{2}{3}}$ where$k$ is an order of an eigenvalue, then our theorem guarantees a more succinctneural network approximation than Barron's theorem. We also make somecomputational experiments to verify our theoretical findings. Our experimentsshow that realistic neural networks easily learn target functions even whenboth theorems do not give any guarantees.</description><author>Rustem Takhanov</author><pubDate>Fri, 26 Apr 2024 15:57:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17461v1</guid></item><item><title>Ruffle&amp;Riley: Insights from Designing and Evaluating a Large Language Model-Based Conversational Tutoring System</title><link>http://arxiv.org/abs/2404.17460v1</link><description>Conversational tutoring systems (CTSs) offer learning experiences throughinteractions based on natural language. They are recognized for promotingcognitive engagement and improving learning outcomes, especially in reasoningtasks. Nonetheless, the cost associated with authoring CTS content is a majorobstacle to widespread adoption and to research on effective instructionaldesign. In this paper, we discuss and evaluate a novel type of CTS thatleverages recent advances in large language models (LLMs) in two ways: First,the system enables AI-assisted content authoring by inducing an easily editabletutoring script automatically from a lesson text. Second, the system automatesthe script orchestration in a learning-by-teaching format via two LLM-basedagents (Ruffle&amp;Riley) acting as a student and a professor. The system allowsfor free-form conversations that follow the ITS-typical inner and outer loopstructure. We evaluate Ruffle&amp;Riley's ability to support biology lessons in twobetween-subject online user studies (N = 200) comparing the system to simplerQA chatbots and reading activity. Analyzing system usage patterns,pre/post-test scores and user experience surveys, we find that Ruffle&amp;Rileyusers report high levels of engagement, understanding and perceive the offeredsupport as helpful. Even though Ruffle&amp;Riley users require more time tocomplete the activity, we did not find significant differences in short-termlearning gains over the reading activity. Our system architecture and userstudy provide various insights for designers of future CTSs. We furtheropen-source our system to support ongoing research on effective instructionaldesign of LLM-based learning technologies.</description><author>Robin Schmucker, Meng Xia, Amos Azaria, Tom Mitchell</author><pubDate>Fri, 26 Apr 2024 15:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17460v1</guid></item><item><title>Converting High-Performance and Low-Latency SNNs through Explicit Modelling of Residual Error in ANNs</title><link>http://arxiv.org/abs/2404.17456v1</link><description>Spiking neural networks (SNNs) have garnered interest due to their energyefficiency and superior effectiveness on neuromorphic chips compared withtraditional artificial neural networks (ANNs). One of the mainstream approachesto implementing deep SNNs is the ANN-SNN conversion, which integrates theefficient training strategy of ANNs with the energy-saving potential and fastinference capability of SNNs. However, under extreme low-latency conditions,the existing conversion theory suggests that the problem of misrepresentationof residual membrane potentials in SNNs, i.e., the inability of IF neurons witha reset-by-subtraction mechanism to respond to residual membrane potentialsbeyond the range from resting potential to threshold, leads to a performancegap in the converted SNNs compared to the original ANNs. This severely limitsthe possibility of practical application of SNNs on delay-sensitive edgedevices. Existing conversion methods addressing this problem usually involvemodifying the state of the conversion spiking neurons. However, these methodsdo not consider their adaptability and compatibility with neuromorphic chips.We propose a new approach based on explicit modeling of residual errors asadditive noise. The noise is incorporated into the activation function of thesource ANN, which effectively reduces the residual error. Our experiments onthe CIFAR10/100 dataset verify that our approach exceeds the prevailing ANN-SNNconversion methods and directly trained SNNs concerning accuracy and therequired time steps. Overall, our method provides new ideas for improving SNNperformance under ultra-low-latency conditions and is expected to promotepractical neuromorphic hardware applications for further development.</description><author>Zhipeng Huang, Jianhao Ding, Zhiyu Pan, Haoran Li, Ying Fang, Zhaofei Yu, Jian K. Liu</author><pubDate>Fri, 26 Apr 2024 15:50:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17456v1</guid></item><item><title>Audio-Visual Person Verification based on Recursive Fusion of Joint Cross-Attention</title><link>http://arxiv.org/abs/2403.04654v3</link><description>Person or identity verification has been recently gaining a lot of attentionusing audio-visual fusion as faces and voices share close associations witheach other. Conventional approaches based on audio-visual fusion rely onscore-level or early feature-level fusion techniques. Though existingapproaches showed improvement over unimodal systems, the potential ofaudio-visual fusion for person verification is not fully exploited. In thispaper, we have investigated the prospect of effectively capturing both theintra- and inter-modal relationships across audio and visual modalities, whichcan play a crucial role in significantly improving the fusion performance overunimodal systems. In particular, we introduce a recursive fusion of a jointcross-attentional model, where a joint audio-visual feature representation isemployed in the cross-attention framework in a recursive fashion toprogressively refine the feature representations that can efficiently capturethe intra-and inter-modal relationships. To further enhance the audio-visualfeature representations, we have also explored BLSTMs to improve the temporalmodeling of audio-visual feature representations. Extensive experiments areconducted on the Voxceleb1 dataset to evaluate the proposed model. Resultsindicate that the proposed model shows promising improvement in fusionperformance by adeptly capturing the intra-and inter-modal relationships acrossaudio and visual modalities.</description><author>R. Gnana Praveen, Jahangir Alam</author><pubDate>Fri, 26 Apr 2024 15:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04654v3</guid></item><item><title>Domain Adaptive and Fine-grained Anomaly Detection for Single-cell Sequencing Data and Beyond</title><link>http://arxiv.org/abs/2404.17454v1</link><description>Fined-grained anomalous cell detection from affected tissues is critical forclinical diagnosis and pathological research. Single-cell sequencing dataprovide unprecedented opportunities for this task. However, current anomalydetection methods struggle to handle domain shifts prevalent in multi-sampleand multi-domain single-cell sequencing data, leading to suboptimalperformance. Moreover, these methods fall short of distinguishing anomalouscells into pathologically distinct subtypes. In response, we propose ACSleuth,a novel, reconstruction deviation-guided generative framework that integratesthe detection, domain adaptation, and fine-grained annotating of anomalouscells into a methodologically cohesive workflow. Notably, we present the firsttheoretical analysis of using reconstruction deviations output by generativemodels for anomaly detection in lieu of domain shifts. This analysis informs usto develop a novel and superior maximum mean discrepancy-based anomaly scorerin ACSleuth. Extensive benchmarks over various single-cell data and other typesof tabular data demonstrate ACSleuth's superiority over the state-of-the-artmethods in identifying and subtyping anomalies in multi-sample and multi-domaincontexts. Our code is available at https://github.com/Catchxu/ACsleuth.</description><author>Kaichen Xu, Yueyang Ding, Suyang Hou, Weiqiang Zhan, Nisang Chen, Jun Wang, Xiaobo Sun</author><pubDate>Fri, 26 Apr 2024 15:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17454v1</guid></item><item><title>A Continuous Relaxation for Discrete Bayesian Optimization</title><link>http://arxiv.org/abs/2404.17452v1</link><description>To optimize efficiently over discrete data and with only few available targetobservations is a challenge in Bayesian optimization. We propose a continuousrelaxation of the objective function and show that inference and optimizationcan be computationally tractable. We consider in particular the optimizationdomain where very few observations and strict budgets exist; motivated byoptimizing protein sequences for expensive to evaluate bio-chemical properties.The advantages of our approach are two-fold: the problem is treated in thecontinuous setting, and available prior knowledge over sequences can beincorporated directly. More specifically, we utilize available and learneddistributions over the problem domain for a weighting of the Hellinger distancewhich yields a covariance function. We show that the resulting acquisitionfunction can be optimized with both continuous or discrete optimizationalgorithms and empirically assess our method on two bio-chemical sequenceoptimization tasks.</description><author>Richard Michael, Simon Bartels, Miguel González-Duque, Yevgen Zainchkovskyy, Jes Frellsen, Søren Hauberg, Wouter Boomsma</author><pubDate>Fri, 26 Apr 2024 15:47:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17452v1</guid></item><item><title>Any-Quantile Probabilistic Forecasting of Short-Term Electricity Demand</title><link>http://arxiv.org/abs/2404.17451v1</link><description>Power systems operate under uncertainty originating from multiple factorsthat are impossible to account for deterministically. Distributionalforecasting is used to control and mitigate risks associated with thisuncertainty. Recent progress in deep learning has helped to significantlyimprove the accuracy of point forecasts, while accurate distributionalforecasting still presents a significant challenge. In this paper, we propose anovel general approach for distributional forecasting capable of predictingarbitrary quantiles. We show that our general approach can be seamlesslyapplied to two distinct neural architectures leading to the state-of-the-artdistributional forecasting results in the context of short-term electricitydemand forecasting task. We empirically validate our method on 35 hourlyelectricity demand time-series for European countries. Our code is availablehere: https://github.com/boreshkinai/any-quantile.</description><author>Slawek Smyl, Boris N. Oreshkin, Paweł Pełka, Grzegorz Dudek</author><pubDate>Fri, 26 Apr 2024 15:43:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17451v1</guid></item><item><title>Conditional Variational Diffusion Models</title><link>http://arxiv.org/abs/2312.02246v4</link><description>Inverse problems aim to determine parameters from observations, a crucialtask in engineering and science. Lately, generative models, especiallydiffusion models, have gained popularity in this area for their ability toproduce realistic solutions and their good mathematical properties. Despitetheir success, an important drawback of diffusion models is their sensitivityto the choice of variance schedule, which controls the dynamics of thediffusion process. Fine-tuning this schedule for specific applications iscrucial but time-costly and does not guarantee an optimal result. We propose anovel approach for learning the schedule as part of the training process. Ourmethod supports probabilistic conditioning on data, provides high-qualitysolutions, and is flexible, proving able to adapt to different applicationswith minimum overhead. This approach is tested in two unrelated inverseproblems: super-resolution microscopy and quantitative phase imaging, yieldingcomparable or superior results to previous methods and fine-tuned diffusionmodels. We conclude that fine-tuning the schedule by experimentation should beavoided because it can be learned during training in a stable way that yieldsbetter results.</description><author>Gabriel della Maggiora, Luis Alberto Croquevielle, Nikita Deshpande, Harry Horsley, Thomas Heinis, Artur Yakimovich</author><pubDate>Fri, 26 Apr 2024 15:40:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02246v4</guid></item><item><title>Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</title><link>http://arxiv.org/abs/2404.14471v2</link><description>In this paper, we investigate a new problem called narrative actionevaluation (NAE). NAE aims to generate professional commentary that evaluatesthe execution of an action. Unlike traditional tasks such as score-based actionquality assessment and video captioning involving superficial sentences, NAEfocuses on creating detailed narratives in natural language. These narrativesprovide intricate descriptions of actions along with objective evaluations. NAEis a more challenging task because it requires both narrative flexibility andevaluation rigor. One existing possible solution is to use multi-task learning,where narrative language and evaluative information are predicted separately.However, this approach results in reduced performance for individual tasksbecause of variations between tasks and differences in modality betweenlanguage information and evaluation information. To address this, we propose aprompt-guided multimodal interaction framework. This framework utilizes a pairof transformers to facilitate the interaction between different modalities ofinformation. It also uses prompts to transform the score regression task into avideo-text matching task, thus enabling task interactivity. To support furtherresearch in this field, we re-annotate the MTL-AQA and FineGym datasets withhigh-quality and comprehensive action narration. Additionally, we establishbenchmarks for NAE. Extensive experiment results prove that our methodoutperforms separate learning methods and naive multi-task learning methods.Data and code are released at https://github.com/shiyi-zh0408/NAE_CVPR2024.</description><author>Shiyi Zhang, Sule Bai, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, Yansong Tang</author><pubDate>Fri, 26 Apr 2024 15:35:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14471v2</guid></item><item><title>Inverse analysis of granular flows using differentiable graph neural network simulator</title><link>http://arxiv.org/abs/2401.13695v3</link><description>Inverse problems in granular flows, such as landslides and debris flows,involve estimating material parameters or boundary conditions based on targetrunout profile. Traditional high-fidelity simulators for these inverse problemsare computationally demanding, restricting the number of simulations possible.Additionally, their non-differentiable nature makes gradient-based optimizationmethods, known for their efficiency in high-dimensional problems, inapplicable.While machine learning-based surrogate models offer computational efficiencyand differentiability, they often struggle to generalize beyond their trainingdata due to their reliance on low-dimensional input-output mappings that failto capture the complete physics of granular flows. We propose a noveldifferentiable graph neural network simulator (GNS) by combining reverse modeautomatic differentiation of graph neural networks with gradient-basedoptimization for solving inverse problems. GNS learns the dynamics of granularflow by representing the system as a graph and predicts the evolution of thegraph at the next time step, given the current state. The differentiable GNSshows optimization capabilities beyond the training data. We demonstrate theeffectiveness of our method for inverse estimation across single andmulti-parameter optimization problems, including evaluating material propertiesand boundary conditions for a target runout distance and designing bafflelocations to limit a landslide runout. Our proposed differentiable GNSframework offers an orders of magnitude faster solution to these inverseproblems than the conventional finite difference approach to gradient-basedoptimization.</description><author>Yongjin Choi, Krishna Kumar</author><pubDate>Fri, 26 Apr 2024 15:35:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13695v3</guid></item><item><title>Learning to Defer in Content Moderation: The Human-AI Interplay</title><link>http://arxiv.org/abs/2402.12237v2</link><description>Successful content moderation in online platforms relies on a human-AIcollaboration approach. A typical heuristic estimates the expected harmfulnessof a post and uses fixed thresholds to decide whether to remove it and whetherto send it for human review. This disregards the prediction uncertainty, thetime-varying element of human review capacity and post arrivals, and theselective sampling in the dataset (humans only review posts filtered by theadmission algorithm). In this paper, we introduce a model to capture the human-AI interplay incontent moderation. The algorithm observes contextual information for incomingposts, makes classification and admission decisions, and schedules posts forhuman review. Only admitted posts receive human reviews on their harmfulness.These reviews help educate the machine-learning algorithms but are delayed dueto congestion in the human review system. The classical learning-theoretic wayto capture this human-AI interplay is via the framework of learning to defer,where the algorithm has the option to defer a classification task to humans fora fixed cost and immediately receive feedback. Our model contributes to thisliterature by introducing congestion in the human review system. Moreover,unlike work on online learning with delayed feedback where the delay in thefeedback is exogenous to the algorithm's decisions, the delay in our model isendogenous to both the admission and the scheduling decisions. We propose a near-optimal learning algorithm that carefully balances theclassification loss from a selectively sampled dataset, the idiosyncratic lossof non-reviewed posts, and the delay loss of having congestion in the humanreview system. To the best of our knowledge, this is the first result foronline learning in contextual queueing systems and hence our analyticalframework may be of independent interest.</description><author>Thodoris Lykouris, Wentao Weng</author><pubDate>Fri, 26 Apr 2024 15:33:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12237v2</guid></item><item><title>The Power of Resets in Online Reinforcement Learning</title><link>http://arxiv.org/abs/2404.15417v2</link><description>Simulators are a pervasive tool in reinforcement learning, but most existingalgorithms cannot efficiently exploit simulator access -- particularly inhigh-dimensional domains that require general function approximation. Weexplore the power of simulators through online reinforcement learning with{local simulator access} (or, local planning), an RL protocol where the agentis allowed to reset to previously observed states and follow their dynamicsduring training. We use local simulator access to unlock new statisticalguarantees that were previously out of reach: - We show that MDPs with low coverability (Xie et al. 2023) -- a generalstructural condition that subsumes Block MDPs and Low-Rank MDPs -- can belearned in a sample-efficient fashion with only $Q^{\star}$-realizability(realizability of the optimal state-value function); existing online RLalgorithms require significantly stronger representation conditions. - As a consequence, we show that the notorious Exogenous Block MDP problem(Efroni et al. 2022) is tractable under local simulator access. The results above are achieved through a computationally inefficientalgorithm. We complement them with a more computationally efficient algorithm,RVFS (Recursive Value Function Search), which achieves provable samplecomplexity guarantees under a strengthened statistical assumption known aspushforward coverability. RVFS can be viewed as a principled, provablecounterpart to a successful empirical paradigm that combines recursive search(e.g., MCTS) with value function approximation.</description><author>Zakaria Mhammedi, Dylan J. Foster, Alexander Rakhlin</author><pubDate>Fri, 26 Apr 2024 15:31:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15417v2</guid></item><item><title>"ChatGPT Is Here to Help, Not to Replace Anybody" -- An Evaluation of Students' Opinions On Integrating ChatGPT In CS Courses</title><link>http://arxiv.org/abs/2404.17443v1</link><description>Large Language Models (LLMs) like GPT and Bard are capable of producing codebased on textual descriptions, with remarkable efficacy. Such technology willhave profound implications for computing education, raising concerns aboutcheating, excessive dependence, and a decline in computational thinking skills,among others. There has been extensive research on how teachers should handlethis challenge but it is also important to understand how students feel aboutthis paradigm shift. In this research, 52 first-year CS students were surveyedin order to assess their views on technologies with code-generationcapabilities, both from academic and professional perspectives. Our findingsindicate that while students generally favor the academic use of GPT, theydon't over rely on it, only mildly asking for its help. Although most studentsbenefit from GPT, some struggle to use it effectively, urging the need forspecific GPT training. Opinions on GPT's impact on their professional livesvary, but there is a consensus on its importance in academic practice.</description><author>Bruno Pereira Cipriano, Pedro Alves</author><pubDate>Fri, 26 Apr 2024 15:29:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17443v1</guid></item><item><title>Uniform Generalization Bounds on Data-Dependent Hypothesis Sets via PAC-Bayesian Theory on Random Sets</title><link>http://arxiv.org/abs/2404.17442v1</link><description>We propose data-dependent uniform generalization bounds by approaching theproblem from a PAC-Bayesian perspective. We first apply the PAC-Bayesianframework on `random sets' in a rigorous way, where the training algorithm isassumed to output a data-dependent hypothesis set after observing the trainingdata. This approach allows us to prove data-dependent bounds, which can beapplicable in numerous contexts. To highlight the power of our approach, weconsider two main applications. First, we propose a PAC-Bayesian formulation ofthe recently developed fractal-dimension-based generalization bounds. Thederived results are shown to be tighter and they unify the existing resultsaround one simple proof technique. Second, we prove uniform bounds over thetrajectories of continuous Langevin dynamics and stochastic gradient Langevindynamics. These results provide novel information about the generalizationproperties of noisy algorithms.</description><author>Benjamin Dupuis, Paul Viallard, George Deligiannidis, Umut Simsekli</author><pubDate>Fri, 26 Apr 2024 15:28:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17442v1</guid></item><item><title>Real-World Deployment of a Hierarchical Uncertainty-Aware Collaborative Multiagent Planning System</title><link>http://arxiv.org/abs/2404.17438v1</link><description>We would like to enable a collaborative multiagent team to navigate at longlength scales and under uncertainty in real-world environments. In practice,planning complexity scales with the number of agents in the team, with thelength scale of the environment, and with environmental uncertainty. Enablingtractable planning requires developing abstract models that can representcomplex, high-quality plans. However, such models often abstract awayinformation needed to generate directly-executable plans for real-world agentsin real-world environments, as planning in such detail, especially in thepresence of real-world uncertainty, would be computationally intractable. Inthis paper, we describe the deployment of a planning system that used ahierarchy of planners to execute collaborative multiagent navigation tasks inreal-world, unknown environments. By developing a planning system that wasrobust to failures at every level of the planning hierarchy, we enabled theteam to complete collaborative navigation tasks, even in the presence ofimperfect planning abstractions and real-world uncertainty. We deployed ourapproach on a Clearpath Husky-Jackal team navigating in a structured outdoorenvironment, and demonstrated that the system enabled the agents tosuccessfully execute collaborative plans.</description><author>Martina Stadler Kurtz, Samuel Prentice, Yasmin Veys, Long Quang, Carlos Nieto-Granda, Michael Novitzky, Ethan Stump, Nicholas Roy</author><pubDate>Fri, 26 Apr 2024 15:22:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17438v1</guid></item><item><title>Guardians of the Quantum GAN</title><link>http://arxiv.org/abs/2404.16156v2</link><description>Quantum Generative Adversarial Networks (qGANs) are at the forefront ofimage-generating quantum machine learning models. To accommodate the growingdemand for Noisy Intermediate-Scale Quantum (NISQ) devices to train and inferquantum machine learning models, the number of third-party vendors offeringquantum hardware as a service is expected to rise. This expansion introducesthe risk of untrusted vendors potentially stealing proprietary information fromthe quantum machine learning models. To address this concern we propose a novelwatermarking technique that exploits the noise signature embedded during thetraining phase of qGANs as a non-invasive watermark. The watermark isidentifiable in the images generated by the qGAN allowing us to trace thespecific quantum hardware used during training hence providing strong proof ofownership. To further enhance the security robustness, we propose the trainingof qGANs on a sequence of multiple quantum hardware, embedding a complexwatermark comprising the noise signatures of all the training hardware that isdifficult for adversaries to replicate. We also develop a machine learningclassifier to extract this watermark robustly, thereby identifying the traininghardware (or the suite of hardware) from the images generated by the qGANvalidating the authenticity of the model. We note that the watermark signatureis robust against inferencing on hardware different than the hardware that wasused for training. We obtain watermark extraction accuracy of 100% and ~90% fortraining the qGAN on individual and multiple quantum hardware setups (andinferencing on different hardware), respectively. Since parameter evolutionduring training is strongly modulated by quantum noise, the proposed watermarkcan be extended to other quantum machine learning models as well.</description><author>Archisman Ghosh, Debarshi Kundu, Avimita Chatterjee, Swaroop Ghosh</author><pubDate>Fri, 26 Apr 2024 15:22:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16156v2</guid></item><item><title>PromptCIR: Blind Compressed Image Restoration with Prompt Learning</title><link>http://arxiv.org/abs/2404.17433v1</link><description>Blind Compressed Image Restoration (CIR) has garnered significant attentiondue to its practical applications. It aims to mitigate compression artifactscaused by unknown quality factors, particularly with JPEG codecs. Existingworks on blind CIR often seek assistance from a quality factor predictionnetwork to facilitate their network to restore compressed images. However, thepredicted numerical quality factor lacks spatial information, preventingnetwork adaptability toward image contents. Recent studies inprompt-learning-based image restoration have showcased the potential of promptsto generalize across varied degradation types and degrees. This motivated us todesign a prompt-learning-based compressed image restoration network, dubbedPromptCIR, which can effectively restore images from various compress levels.Specifically, PromptCIR exploits prompts to encode compression informationimplicitly, where prompts directly interact with soft weights generated fromimage features, thus providing dynamic content-aware and distortion-awareguidance for the restoration process. The light-weight prompts enable ourmethod to adapt to different compression levels, while introducing minimalparameter overhead. Overall, PromptCIR leverages the powerful transformer-basedbackbone with the dynamic prompt module to proficiently handle blind CIR tasks,winning first place in the NTIRE 2024 challenge of blind compressed imageenhancement track. Extensive experiments have validated the effectiveness ofour proposed PromptCIR. The code is available athttps://github.com/lbc12345/PromptCIR-NTIRE24.</description><author>Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen</author><pubDate>Fri, 26 Apr 2024 15:20:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17433v1</guid></item><item><title>Separation capacity of linear reservoirs with random connectivity matrix</title><link>http://arxiv.org/abs/2404.17429v1</link><description>We argue that the success of reservoir computing lies within the separationcapacity of the reservoirs and show that the expected separation capacity ofrandom linear reservoirs is fully characterised by the spectral decompositionof an associated generalised matrix of moments. Of particular interest arereservoirs with Gaussian matrices that are either symmetric or whose entriesare all independent. In the symmetric case, we prove that the separationcapacity always deteriorates with time; while for short inputs, separation withlarge reservoirs is best achieved when the entries of the matrix are scaledwith a factor $\rho_T/\sqrt{N}$, where $N$ is the dimension of the reservoirand $\rho_T$ depends on the maximum length of the input time series. In thei.i.d. case, we establish that optimal separation with large reservoirs isconsistently achieved when the entries of the reservoir matrix are scaled withthe exact factor $1/\sqrt{N}$. We further give upper bounds on the quality ofseparation in function of the length of the time series. We complement thisanalysis with an investigation of the likelihood of this separation and theimpact of the chosen architecture on separation consistency.</description><author>Youness Boutaib</author><pubDate>Fri, 26 Apr 2024 15:10:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17429v1</guid></item><item><title>Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models</title><link>http://arxiv.org/abs/2306.05272v5</link><description>The advent of large pre-trained models has brought about a paradigm shift inboth visual representation learning and natural language processing. However,clustering unlabeled images, as a fundamental and classic machine learningproblem, still lacks an effective solution, particularly for large-scaledatasets. In this paper, we propose a novel image clustering pipeline thatleverages the powerful feature representation of large pre-trained models suchas CLIP and cluster images effectively and efficiently at scale. We firstdeveloped a novel algorithm to estimate the number of clusters in a givendataset. We then show that the pre-trained features are significantly morestructured by further optimizing the rate reduction objective. The resultingfeatures may significantly improve the clustering accuracy, e.g., from 57\% to66\% on ImageNet-1k. Furthermore, by leveraging CLIP's multimodality bridgebetween image and text, we develop a simple yet effective self-labelingalgorithm that produces meaningful captions for the clusters. Through extensiveexperiments, we show that our pipeline works well on standard datasets such asCIFAR-10, CIFAR-100, and ImageNet-1k. It also extends to datasets that are notcurated for clustering, such as LAION-Aesthetics and WikiArts. We released thecode in https://github.com/LeslieTrue/CPP.</description><author>Tianzhe Chu, Shengbang Tong, Tianjiao Ding, Xili Dai, Benjamin David Haeffele, René Vidal, Yi Ma</author><pubDate>Fri, 26 Apr 2024 15:10:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05272v5</guid></item><item><title>Cost-Sensitive Uncertainty-Based Failure Recognition for Object Detection</title><link>http://arxiv.org/abs/2404.17427v1</link><description>Object detectors in real-world applications often fail to detect objects dueto varying factors such as weather conditions and noisy input. Therefore, aprocess that mitigates false detections is crucial for both safety andaccuracy. While uncertainty-based thresholding shows promise, previous worksdemonstrate an imperfect correlation between uncertainty and detection errors.This hinders ideal thresholding, prompting us to further investigate thecorrelation and associated cost with different types of uncertainty. Wetherefore propose a cost-sensitive framework for object detection tailored touser-defined budgets on the two types of errors, missing and false detections.We derive minimum thresholding requirements to prevent performance degradationand define metrics to assess the applicability of uncertainty for failurerecognition. Furthermore, we automate and optimize the thresholding process tomaximize the failure recognition rate w.r.t. the specified budget. Evaluationon three autonomous driving datasets demonstrates that our approachsignificantly enhances safety, particularly in challenging scenarios.Leveraging localization aleatoric uncertainty and softmax-based entropy only,our method boosts the failure recognition rate by 36-60\% compared toconventional approaches. Code is available athttps://mos-ks.github.io/publications.</description><author>Moussa Kassem Sbeyti, Michelle Karg, Christian Wirth, Nadja Klein, Sahin Albayrak</author><pubDate>Fri, 26 Apr 2024 15:03:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17427v1</guid></item><item><title>One-Shot Image Restoration</title><link>http://arxiv.org/abs/2404.17426v1</link><description>Image restoration, or inverse problems in image processing, has long been anextensively studied topic. In recent years supervised learning approaches havebecome a popular strategy attempting to tackle this task. Unfortunately, mostsupervised learning-based methods are highly demanding in terms ofcomputational resources and training data (sample complexity). In addition,trained models are sensitive to domain changes, such as varying acquisitionsystems, signal sampling rates, resolution and contrast. In this work, we tryto answer a fundamental question: Can supervised learning models generalizewell solely by learning from one image or even part of an image? If so, thenwhat is the minimal amount of patches required to achieve acceptablegeneralization? To this end, we focus on an efficient patch-based learningframework that requires a single image input-output pair for training.Experimental results demonstrate the applicability, robustness andcomputational efficiency of the proposed approach for supervised imagedeblurring and super-resolution. Our results showcase significant improvementof learning models' sample efficiency, generalization and time complexity, thatcan hopefully be leveraged for future real-time applications, and applied toother signals and modalities.</description><author>Deborah Pereg</author><pubDate>Fri, 26 Apr 2024 15:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17426v1</guid></item><item><title>Optimizing ZX-Diagrams with Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2311.18588v2</link><description>ZX-diagrams are a powerful graphical language for the description of quantumprocesses with applications in fundamental quantum mechanics, quantum circuitoptimization, tensor network simulation, and many more. The utility ofZX-diagrams relies on a set of local transformation rules that can be appliedto them without changing the underlying quantum process they describe. Theserules can be exploited to optimize the structure of ZX-diagrams for a range ofapplications. However, finding an optimal sequence of transformation rules isgenerally an open problem. In this work, we bring together ZX-diagrams withreinforcement learning, a machine learning technique designed to discover anoptimal sequence of actions in a decision-making problem and show that atrained reinforcement learning agent can significantly outperform otheroptimization techniques like a greedy strategy or simulated annealing. The useof graph neural networks to encode the policy of the agent enablesgeneralization to diagrams much bigger than seen during the training phase.</description><author>Maximilian Nägele, Florian Marquardt</author><pubDate>Fri, 26 Apr 2024 15:02:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18588v2</guid></item><item><title>If It's Not Enough, Make It So: Reducing Authentic Data Demand in Face Recognition through Synthetic Faces</title><link>http://arxiv.org/abs/2404.03537v4</link><description>Recent advances in deep face recognition have spurred a growing demand forlarge, diverse, and manually annotated face datasets. Acquiring authentic,high-quality data for face recognition has proven to be a challenge, primarilydue to privacy concerns. Large face datasets are primarily sourced fromweb-based images, lacking explicit user consent. In this paper, we examinewhether and how synthetic face data can be used to train effective facerecognition models with reduced reliance on authentic images, therebymitigating data collection concerns. First, we explored the performance gapamong recent state-of-the-art face recognition models, trained with syntheticdata only and authentic (scarce) data only. Then, we deepened our analysis bytraining a state-of-the-art backbone with various combinations of synthetic andauthentic data, gaining insights into optimizing the limited use of the latterfor verification accuracy. Finally, we assessed the effectiveness of dataaugmentation approaches on synthetic and authentic data, with the same goal inmind. Our results highlighted the effectiveness of FR trained on combineddatasets, particularly when combined with appropriate augmentation techniques.</description><author>Andrea Atzori, Fadi Boutros, Naser Damer, Gianni Fenu, Mirko Marras</author><pubDate>Fri, 26 Apr 2024 15:01:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03537v4</guid></item><item><title>Joint Service Caching, Communication and Computing Resource Allocation in Collaborative MEC Systems: A DRL-based Two-timescale Approach</title><link>http://arxiv.org/abs/2307.09691v2</link><description>Meeting the strict Quality of Service (QoS) requirements of terminals hasimposed a signiffcant challenge on Multiaccess Edge Computing (MEC) systems,due to the limited multidimensional resources. To address this challenge, wepropose a collaborative MEC framework that facilitates resource sharing betweenthe edge servers, and with the aim to maximize the long-term QoS and reduce thecache switching cost through joint optimization of service caching,collaborative offfoading, and computation and communication resourceallocation. The dual timescale feature and temporal recurrence relationshipbetween service caching and other resource allocation make solving the problemeven more challenging. To solve it, we propose a deep reinforcement learning(DRL)-based dual timescale scheme, called DGL-DDPG, which is composed of ashort-term genetic algorithm (GA) and a long short-term memory network-baseddeep deterministic policy gradient (LSTM-DDPG). In doing so, we reformulate theoptimization problem as a Markov decision process (MDP) where thesmall-timescale resource allocation decisions generated by an improved GA aretaken as the states and input into a centralized LSTM-DDPG agent to generatethe service caching decision for the large-timescale. Simulation resultsdemonstrate that our proposed algorithm outperforms the baseline algorithms interms of the average QoS and cache switching cost.</description><author>Qianqian Liu, Haixia Zhang, Xin Zhang, Dongfeng Yuan</author><pubDate>Fri, 26 Apr 2024 14:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09691v2</guid></item><item><title>Multi-view Image Prompted Multi-view Diffusion for Improved 3D Generation</title><link>http://arxiv.org/abs/2404.17419v1</link><description>Using image as prompts for 3D generation demonstrate particularly strongperformances compared to using text prompts alone, for images provide a moreintuitive guidance for the 3D generation process. In this work, we delve intothe potential of using multiple image prompts, instead of a single imageprompt, for 3D generation. Specifically, we build on ImageDream, a novelimage-prompt multi-view diffusion model, to support multi-view images as theinput prompt. Our method, dubbed MultiImageDream, reveals that transitioningfrom a single-image prompt to multiple-image prompts enhances the performanceof multi-view and 3D object generation according to various quantitativeevaluation metrics and qualitative assessments. This advancement is achievedwithout the necessity of fine-tuning the pre-trained ImageDream multi-viewdiffusion model.</description><author>Seungwook Kim, Yichun Shi, Kejie Li, Minsu Cho, Peng Wang</author><pubDate>Fri, 26 Apr 2024 14:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17419v1</guid></item><item><title>Dual-Mandate Patrols: Multi-Armed Bandits for Green Security</title><link>http://arxiv.org/abs/2009.06560v3</link><description>Conservation efforts in green security domains to protect wildlife andforests are constrained by the limited availability of defenders (i.e.,patrollers), who must patrol vast areas to protect from attackers (e.g.,poachers or illegal loggers). Defenders must choose how much time to spend ineach region of the protected area, balancing exploration of infrequentlyvisited regions and exploitation of known hotspots. We formulate the problem asa stochastic multi-armed bandit, where each action represents a patrolstrategy, enabling us to guarantee the rate of convergence of the patrollingpolicy. However, a naive bandit approach would compromise short-termperformance for long-term optimality, resulting in animals poached and forestsdestroyed. To speed up performance, we leverage smoothness in the rewardfunction and decomposability of actions. We show a synergy betweenLipschitz-continuity and decomposition as each aids the convergence of theother. In doing so, we bridge the gap between combinatorial and Lipschitzbandits, presenting a no-regret approach that tightens existing guaranteeswhile optimizing for short-term performance. We demonstrate that our algorithm,LIZARD, improves performance on real-world poaching data from Cambodia.</description><author>Lily Xu, Elizabeth Bondi, Fei Fang, Andrew Perrault, Kai Wang, Milind Tambe</author><pubDate>Fri, 26 Apr 2024 14:51:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2009.06560v3</guid></item><item><title>Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</title><link>http://arxiv.org/abs/2403.14608v4</link><description>Large models represent a groundbreaking advancement in multiple applicationfields, enabling remarkable achievements across various tasks. However, theirunprecedented scale comes with significant computational costs. These models,often consisting of billions of parameters, require vast amounts ofcomputational resources for execution. Especially, the expansive scale andcomputational demands pose considerable challenges when customizing them forparticular downstream tasks, particularly over the hardware platformsconstrained by computational capabilities. Parameter Efficient Fine-Tuning(PEFT) provides a practical solution by efficiently adapt the large models overthe various downstream tasks. In particular, PEFT refers to the process ofadjusting the parameters of a pre-trained large models to adapt it to aspecific task while minimizing the number of additional parameters introducedor computational resources required. This approach is particularly importantwhen dealing with large language models with high parameter counts, asfine-tuning these models from scratch can be computationally expensive andresource-intensive, posing considerable challenges in the supporting systemplatform design. In this survey, we present comprehensive studies of variousPEFT algorithms, examining their performance and computational overhead.Moreover, we provide an overview of applications developed using different PEFTalgorithms and discuss common techniques employed to mitigate computation costsfor PEFT. In addition to the algorithmic perspective, we overview variousreal-world system designs to investigate the implementation costs associatedwith different PEFT algorithms. This survey serves as an indispensable resourcefor researchers aiming to understand both the PEFT algorithm and its systemimplementation, offering detailed insights into recent advancements andpractical applications.</description><author>Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang</author><pubDate>Fri, 26 Apr 2024 14:46:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14608v4</guid></item><item><title>Fairness Auditing with Multi-Agent Collaboration</title><link>http://arxiv.org/abs/2402.08522v2</link><description>Existing work in fairness auditing assumes that each audit is performedindependently. In this paper, we consider multiple agents working together,each auditing the same platform for different tasks. Agents have two levers:their collaboration strategy, with or without coordination beforehand, andtheir strategy for sampling appropriate data points. We theoretically comparethe interplay of these levers. Our main findings are that (i) collaboration isgenerally beneficial for accurate audits, (ii) basic sampling methods oftenprove to be effective, and (iii) counter-intuitively, extensive coordination onqueries often deteriorates audits accuracy as the number of agents increases.Experiments on three large datasets confirm our theoretical results. Ourfindings motivate collaboration during fairness audits of platforms that use MLmodels for decision-making.</description><author>Martijn de Vos, Akash Dhasade, Jade Garcia Bourrée, Anne-Marie Kermarrec, Erwan Le Merrer, Benoit Rottembourg, Gilles Tredan</author><pubDate>Fri, 26 Apr 2024 14:44:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08522v2</guid></item><item><title>Joint covariance properties under geometric image transformations for spatio-temporal receptive fields according to the generalized Gaussian derivative model for visual receptive fields</title><link>http://arxiv.org/abs/2311.10543v4</link><description>The influence of natural image transformations on receptive field responsesis crucial for modelling visual operations in computer vision and biologicalvision. In this regard, covariance properties with respect to geometric imagetransformations in the earliest layers of the visual hierarchy are essentialfor expressing robust image operations, and for formulating invariant visualoperations at higher levels. This paper defines and proves a set of joint covariance properties undercompositions of spatial scaling transformations, spatial affinetransformations, Galilean transformations and temporal scaling transformations,which make it possible to characterize how different types of imagetransformations interact with each other and the associated spatio-temporalreceptive field responses. In this regard, we also extend the notion ofscale-normalized derivatives to affine-normalized derivatives, to be able toobtain true affine-covariant properties of spatial derivatives, that arecomputed based on spatial smoothing with affine Gaussian kernels. The derived relations show how the parameters of the receptive fields need tobe transformed, in order to match the output from spatio-temporal receptivefields under composed spatio-temporal image transformations. As a side effect,the presented proof for the joint covariance property over the integratedcombination of the different geometric image transformations also providesspecific proofs for the individual transformation properties, which have notpreviously been fully reported in the literature. The paper also presents an in-depth theoretical analysis of geometricinterpretations of the derived covariance properties, as well as outlines anumber of biological interpretations of these results.</description><author>Tony Lindeberg</author><pubDate>Fri, 26 Apr 2024 14:43:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10543v4</guid></item><item><title>LEAF: Unveiling Two Sides of the Same Coin in Semi-supervised Facial Expression Recognition</title><link>http://arxiv.org/abs/2404.15041v2</link><description>Semi-supervised learning has emerged as a promising approach to tackle thechallenge of label scarcity in facial expression recognition (FER) task.However, current state-of-the-art methods primarily focus on one side of thecoin, i.e., generating high-quality pseudo-labels, while overlooking the otherside: enhancing expression-relevant representations. In this paper, we unveilboth sides of the coin by proposing a unified framework termed hierarchicaLdEcoupling And Fusing (LEAF) to coordinate expression-relevant representationsand pseudo-labels for semi-supervised FER. LEAF introduces a hierarchicalexpression-aware aggregation strategy that operates at three levels: semantic,instance, and category. (1) At the semantic and instance levels, LEAF decouplesrepresentations into expression-agnostic and expression-relevant components,and adaptively fuses them using learnable gating weights. (2) At the categorylevel, LEAF assigns ambiguous pseudo-labels by decoupling predictions intopositive and negative parts, and employs a consistency loss to ensure agreementbetween two augmented views of the same image. Extensive experiments onbenchmark datasets demonstrate that by unveiling and harmonizing both sides ofthe coin, LEAF outperforms state-of-the-art semi-supervised FER methods,effectively leveraging both labeled and unlabeled data. Moreover, the proposedexpression-aware aggregation strategy can be seamlessly integrated intoexisting semi-supervised frameworks, leading to significant performance gains.Our code is available at https://anonymous.4open.science/r/LEAF-BC57/.</description><author>Fan Zhang, Zhi-Qi Cheng, Jian Zhao, Xiaojiang Peng, Xuelong Li</author><pubDate>Fri, 26 Apr 2024 14:38:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15041v2</guid></item><item><title>A machine-learning approach to thunderstorm forecasting through post-processing of simulation data</title><link>http://arxiv.org/abs/2303.08736v3</link><description>Thunderstorms pose a major hazard to society and economy, which calls forreliable thunderstorm forecasts. In this work, we introduce a Signature-basedApproach of identifying Lightning Activity using MAchine learning (SALAMA), afeedforward neural network model for identifying thunderstorm occurrence innumerical weather prediction (NWP) data. The model is trained onconvection-resolving ensemble forecasts over Central Europe and lightningobservations. Given only a set of pixel-wise input parameters that areextracted from NWP data and related to thunderstorm development, SALAMA infersthe probability of thunderstorm occurrence in a reliably calibrated manner. Forlead times up to eleven hours, we find a forecast skill superior toclassification based only on NWP reflectivity. Varying the spatiotemporalcriteria by which we associate lightning observations with NWP data, we showthat the time scale for skillful thunderstorm predictions increases linearlywith the spatial scale of the forecast.</description><author>Kianusch Vahid Yousefnia, Tobias Bölle, Isabella Zöbisch, Thomas Gerz</author><pubDate>Fri, 26 Apr 2024 14:34:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08736v3</guid></item><item><title>Evaluation of Geographical Distortions in Language Models: A Crucial Step Towards Equitable Representations</title><link>http://arxiv.org/abs/2404.17401v1</link><description>Language models now constitute essential tools for improving efficiency formany professional tasks such as writing, coding, or learning. For this reason,it is imperative to identify inherent biases. In the field of Natural LanguageProcessing, five sources of bias are well-identified: data, annotation,representation, models, and research design. This study focuses on biasesrelated to geographical knowledge. We explore the connection between geographyand language models by highlighting their tendency to misrepresent spatialinformation, thus leading to distortions in the representation of geographicaldistances. This study introduces four indicators to assess these distortions,by comparing geographical and semantic distances. Experiments are conductedfrom these four indicators with ten widely used language models. Resultsunderscore the critical necessity of inspecting and rectifying spatial biasesin language models to ensure accurate and equitable representations.</description><author>Rémy Decoupes, Roberto Interdonato, Mathieu Roche, Maguelonne Teisseire, Sarah Valentin</author><pubDate>Fri, 26 Apr 2024 14:22:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17401v1</guid></item><item><title>Spatial-frequency Dual-Domain Feature Fusion Network for Low-Light Remote Sensing Image Enhancement</title><link>http://arxiv.org/abs/2404.17400v1</link><description>Low-light remote sensing images generally feature high resolution and highspatial complexity, with continuously distributed surface features in space.This continuity in scenes leads to extensive long-range correlations in spatialdomains within remote sensing images. Convolutional Neural Networks, which relyon local correlations for long-distance modeling, struggle to establishlong-range correlations in such images. On the other hand, transformer-basedmethods that focus on global information face high computational complexitieswhen processing high-resolution remote sensing images. From anotherperspective, Fourier transform can compute global information withoutintroducing a large number of parameters, enabling the network to moreefficiently capture the overall image structure and establish long-rangecorrelations. Therefore, we propose a Dual-Domain Feature Fusion Network (DFFN)for low-light remote sensing image enhancement. Specifically, this challengingtask of low-light enhancement is divided into two more manageable sub-tasks:the first phase learns amplitude information to restore image brightness, andthe second phase learns phase information to refine details. To facilitateinformation exchange between the two phases, we designed an information fusionaffine block that combines data from different phases and scales. Additionally,we have constructed two dark light remote sensing datasets to address thecurrent lack of datasets in dark light remote sensing image enhancement.Extensive evaluations show that our method outperforms existingstate-of-the-art methods. The code is available athttps://github.com/iijjlk/DFFN.</description><author>Zishu Yao, Guodong Fan, Jinfu Fan, Min Gan, C. L. Philip Chen</author><pubDate>Fri, 26 Apr 2024 14:21:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17400v1</guid></item><item><title>Evaluations of Machine Learning Privacy Defenses are Misleading</title><link>http://arxiv.org/abs/2404.17399v1</link><description>Empirical defenses for machine learning privacy forgo the provable guaranteesof differential privacy in the hope of achieving higher utility while resistingrealistic adversaries. We identify severe pitfalls in existing empiricalprivacy evaluations (based on membership inference attacks) that result inmisleading conclusions. In particular, we show that prior evaluations fail tocharacterize the privacy leakage of the most vulnerable samples, use weakattacks, and avoid comparisons with practical differential privacy baselines.In 5 case studies of empirical privacy defenses, we find that prior evaluationsunderestimate privacy leakage by an order of magnitude. Under our strongerevaluation, none of the empirical defenses we study are competitive with aproperly tuned, high-utility DP-SGD baseline (with vacuous provableguarantees).</description><author>Michael Aerni, Jie Zhang, Florian Tramèr</author><pubDate>Fri, 26 Apr 2024 14:21:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17399v1</guid></item><item><title>On Computing Plans with Uniform Action Costs</title><link>http://arxiv.org/abs/2402.09877v2</link><description>In many real-world planning applications, agents might be interested infinding plans whose actions have costs that are as uniform as possible. Suchplans provide agents with a sense of stability and predictability, which arekey features when humans are the agents executing plans suggested by planningtools. This paper adapts three uniformity metrics to automated planning, andintroduce planning-based compilations that allow to lexicographically optimizesum of action costs and action costs uniformity. Experimental results both inwell-known and novel planning benchmarks show that the reformulated tasks canbe effectively solved in practice to generate uniform plans.</description><author>Alberto Pozanco, Daniel Borrajo, Manuela Veloso</author><pubDate>Fri, 26 Apr 2024 14:20:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09877v2</guid></item><item><title>Online Policy Learning and Inference by Matrix Completion</title><link>http://arxiv.org/abs/2404.17398v1</link><description>Making online decisions can be challenging when features are sparse andorthogonal to historical ones, especially when the optimal policy is learnedthrough collaborative filtering. We formulate the problem as a matrixcompletion bandit (MCB), where the expected reward under each arm ischaracterized by an unknown low-rank matrix. The $\epsilon$-greedy bandit andthe online gradient descent algorithm are explored. Policy learning and regretperformance are studied under a specific schedule for exploration probabilitiesand step sizes. A faster decaying exploration probability yields smaller regretbut learns the optimal policy less accurately. We investigate an onlinedebiasing method based on inverse propensity weighting (IPW) and a generalframework for online policy inference. The IPW-based estimators areasymptotically normal under mild arm-optimality conditions. Numericalsimulations corroborate our theoretical findings. Our methods are applied tothe San Francisco parking pricing project data, revealing intriguingdiscoveries and outperforming the benchmark policy.</description><author>Congyuan Duan, Jingyang Li, Dong Xia</author><pubDate>Fri, 26 Apr 2024 14:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17398v1</guid></item><item><title>The Promise and Challenges of Using LLMs to Accelerate the Screening Process of Systematic Reviews</title><link>http://arxiv.org/abs/2404.15667v3</link><description>Systematic review (SR) is a popular research method in software engineering(SE). However, conducting an SR takes an average of 67 weeks. Thus, automatingany step of the SR process could reduce the effort associated with SRs. Ourobjective is to investigate if Large Language Models (LLMs) can acceleratetitle-abstract screening by simplifying abstracts for human screeners, andautomating title-abstract screening. We performed an experiment where humansscreened titles and abstracts for 20 papers with both original and simplifiedabstracts from a prior SR. The experiment with human screeners was reproducedwith GPT-3.5 and GPT-4 LLMs to perform the same screening tasks. We alsostudied if different prompting techniques (Zero-shot (ZS), One-shot (OS),Few-shot (FS), and Few-shot with Chain-of-Thought (FS-CoT)) improve thescreening performance of LLMs. Lastly, we studied if redesigning the promptused in the LLM reproduction of screening leads to improved performance. Textsimplification did not increase the screeners' screening performance, butreduced the time used in screening. Screeners' scientific literacy skills andresearcher status predict screening performance. Some LLM and promptcombinations perform as well as human screeners in the screening tasks. Ourresults indicate that the GPT-4 LLM is better than its predecessor, GPT-3.5.Additionally, Few-shot and One-shot prompting outperforms Zero-shot prompting.Using LLMs for text simplification in the screening process does notsignificantly improve human performance. Using LLMs to automate title-abstractscreening seems promising, but current LLMs are not significantly more accuratethan human screeners. To recommend the use of LLMs in the screening process ofSRs, more research is needed. We recommend future SR studies publishreplication packages with screening data to enable more conclusiveexperimenting with LLM screening.</description><author>Aleksi Huotala, Miikka Kuutila, Paul Ralph, Mika Mäntylä</author><pubDate>Fri, 26 Apr 2024 14:15:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15667v3</guid></item><item><title>Child Speech Recognition in Human-Robot Interaction: Problem Solved?</title><link>http://arxiv.org/abs/2404.17394v1</link><description>Automated Speech Recognition shows superhuman performance for adult Englishspeech on a range of benchmarks, but disappoints when fed children's speech.This has long sat in the way of child-robot interaction. Recent evolutions indata-driven speech recognition, including the availability of Transformerarchitectures and unprecedented volumes of training data, might mean abreakthrough for child speech recognition and social robot applications aimedat children. We revisit a study on child speech recognition from 2017 and showthat indeed performance has increased, with newcomer OpenAI Whisper doingmarkedly better than leading commercial cloud services. While transcription isnot perfect yet, the best model recognises 60.3% of sentences correctly barringsmall grammatical differences, with sub-second transcription time running on alocal GPU, showing potential for usable autonomous child-robot speechinteractions.</description><author>Ruben Janssens, Eva Verhelst, Giulio Antonio Abbo, Qiaoqiao Ren, Maria Jose Pinto Bernal, Tony Belpaeme</author><pubDate>Fri, 26 Apr 2024 14:14:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17394v1</guid></item><item><title>MEIA: Towards Realistic Multimodal Interaction and Manipulation for Embodied Robots</title><link>http://arxiv.org/abs/2402.00290v2</link><description>With the surge in the development of large language models, embodiedintelligence has attracted increasing attention. Nevertheless, prior works onembodied intelligence typically encode scene or historical memory in anunimodal manner, either visual or linguistic, which complicates the alignmentof the model's action planning with embodied control. To overcome thislimitation, we introduce the Multimodal Embodied Interactive Agent (MEIA),capable of translating high-level tasks expressed in natural language into asequence of executable actions. Specifically, we propose a novel MultimodalEnvironment Memory (MEM) module, facilitating the integration of embodiedcontrol with large models through the visual-language memory of scenes. Thiscapability enables MEIA to generate executable action plans based on diverserequirements and the robot's capabilities. Furthermore, we construct anembodied question answering dataset based on a dynamic virtual cafe environmentwith the help of the large language model. In this virtual environment, weconduct several experiments, utilizing multiple large models through zero-shotlearning, and carefully design scenarios for various situations. Theexperimental results showcase the promising performance of our MEIA in variousembodied interactive tasks.</description><author>Yang Liu, Xinshuai Song, Kaixuan Jiang, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin</author><pubDate>Fri, 26 Apr 2024 14:13:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00290v2</guid></item><item><title>M3BAT: Unsupervised Domain Adaptation for Multimodal Mobile Sensing with Multi-Branch Adversarial Training</title><link>http://arxiv.org/abs/2404.17391v1</link><description>Over the years, multimodal mobile sensing has been used extensively forinferences regarding health and well being, behavior, and context. However, asignificant challenge hindering the widespread deployment of such models inreal world scenarios is the issue of distribution shift. This is the phenomenonwhere the distribution of data in the training set differs from thedistribution of data in the real world, the deployment environment. Whileextensively explored in computer vision and natural language processing, andwhile prior research in mobile sensing briefly addresses this concern, currentwork primarily focuses on models dealing with a single modality of data, suchas audio or accelerometer readings, and consequently, there is little researchon unsupervised domain adaptation when dealing with multimodal sensor data. Toaddress this gap, we did extensive experiments with domain adversarial neuralnetworks (DANN) showing that they can effectively handle distribution shifts inmultimodal sensor data. Moreover, we proposed a novel improvement over DANN,called M3BAT, unsupervised domain adaptation for multimodal mobile sensing withmulti-branch adversarial training, to account for the multimodality of sensordata during domain adaptation with multiple branches. Through extensiveexperiments conducted on two multimodal mobile sensing datasets, threeinference tasks, and 14 source-target domain pairs, including both regressionand classification, we demonstrate that our approach performs effectively onunseen domains. Compared to directly deploying a model trained in the sourcedomain to the target domain, the model shows performance increases up to 12%AUC (area under the receiver operating characteristics curves) onclassification tasks, and up to 0.13 MAE (mean absolute error) on regressiontasks.</description><author>Lakmal Meegahapola, Hamza Hassoune, Daniel Gatica-Perez</author><pubDate>Fri, 26 Apr 2024 14:09:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17391v1</guid></item><item><title>How Could AI Support Design Education? A Study Across Fields Fuels Situating Analytics</title><link>http://arxiv.org/abs/2404.17390v1</link><description>We use the process and findings from a case study of design educators'practices of assessment and feedback to fuel theorizing about how to make AIuseful in service of human experience. We build on Suchman's theory of situatedactions. We perform a qualitative study of 11 educators in 5 fields, who teachdesign processes situated in project-based learning contexts. Throughqualitative data gathering and analysis, we derive codes: design process;assessment and feedback challenges; and computational support. We twice invoke creative cognition's family resemblance principle. First, toexplain how design instructors already use assessment rubrics and second, toexplain the analogous role for design creativity analytics: no particular traitis necessary or sufficient; each only tends to indicate good design work. Humanteachers remain essential. We develop a set of situated design creativityanalytics--Fluency, Flexibility, Visual Consistency, Multiscale Organization,and Legible Contrast--to support instructors' efforts, by providing on-demand,learning objectives-based assessment and feedback to students. We theorize a methodology, which we call situating analytics, firstly becausemaking AI support living human activity depends on aligning what analyticsmeasure with situated practices. Further, we realize that analytics can becomemost significant to users by situating them through interfaces that integratethem into the material contexts of their use. Here, this means situating designcreativity analytics into actual design environments. Through the case study,we identify situating analytics as a methodology for explaining analytics tousers, because the iterative process of alignment with practice has thepotential to enable data scientists to derive analytics that make sense as partof and support situated human experiences.</description><author>Ajit Jain, Andruid Kerne, Hannah Fowler, Jinsil Seo, Galen Newman, Nic Lupfer, Aaron Perrine</author><pubDate>Fri, 26 Apr 2024 14:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17390v1</guid></item><item><title>Frequency-Guided Multi-Level Human Action Anomaly Detection with Normalizing Flows</title><link>http://arxiv.org/abs/2404.17381v1</link><description>We introduce the task of human action anomaly detection (HAAD), which aims toidentify anomalous motions in an unsupervised manner given only thepre-determined normal category of training action samples. Compared to priorhuman-related anomaly detection tasks which primarily focus on unusual eventsfrom videos, HAAD involves the learning of specific action labels to recognizesemantically anomalous human behaviors. To address this task, we propose anormalizing flow (NF)-based detection framework where the sample likelihood iseffectively leveraged to indicate anomalies. As action anomalies often occur insome specific body parts, in addition to the full-body action feature learning,we incorporate extra encoding streams into our framework for a finer modelingof body subsets. Our framework is thus multi-level to jointly discover globaland local motion anomalies. Furthermore, to show awareness of the potentiallyjittery data during recording, we resort to discrete cosine transformation byconverting the action samples from the temporal to the frequency domain tomitigate the issue of data instability. Extensive experimental results on twohuman action datasets demonstrate that our method outperforms the baselinesformed by adapting state-of-the-art human activity AD approaches to our task ofHAAD.</description><author>Shun Maeda, Chunzhi Gu, Jun Yu, Shogo Tokai, Shangce Gao, Chao Zhang</author><pubDate>Fri, 26 Apr 2024 13:56:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17381v1</guid></item><item><title>Quantum Adjoint Convolutional Layers for Effective Data Representation</title><link>http://arxiv.org/abs/2404.17378v1</link><description>Quantum Convolutional Layer (QCL) is considered as one of the core of QuantumConvolutional Neural Networks (QCNNs) due to its efficient data featureextraction capability. However, the current principle of QCL is not asmathematically understandable as Classical Convolutional Layer (CCL) due to itsblack-box structure. Moreover, classical data mapping in many QCLs isinefficient. To this end, firstly, the Quantum Adjoint Convolution Operation(QACO) consisting of a quantum amplitude encoding and its inverse istheoretically shown to be equivalent to the quantum normalization of theconvolution operation based on the Frobenius inner product while achieving anefficient characterization of the data. Subsequently, QACO is extended into aQuantum Adjoint Convolutional Layer (QACL) by Quantum Phase Estimation (QPE) tocompute all Frobenius inner products in parallel. At last, comparativesimulation experiments are carried out on PennyLane and TensorFlow platforms,mainly for the two cases of kernel fixed and unfixed in QACL. The resultsdemonstrate that QACL with the insight of special quantum properties for thesame images, provides higher training accuracy in MNIST and Fashion MNISTclassification experiments, but sacrifices the learning performance to someextent. Predictably, our research lays the foundation for the development ofefficient and interpretable quantum convolutional networks and also advancesthe field of quantum machine vision.</description><author>Ren-Xin Zhao, Shi Wang, Yaonan Wang</author><pubDate>Fri, 26 Apr 2024 13:52:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17378v1</guid></item><item><title>Estimating the Robustness Radius for Randomized Smoothing with 100$\times$ Sample Efficiency</title><link>http://arxiv.org/abs/2404.17371v1</link><description>Randomized smoothing (RS) has successfully been used to improve therobustness of predictions for deep neural networks (DNNs) by adding randomnoise to create multiple variations of an input, followed by deciding theconsensus. To understand if an RS-enabled DNN is effective in the sampled inputdomains, it is mandatory to sample data points within the operational designdomain, acquire the point-wise certificate regarding robustness radius, andcompare it with pre-defined acceptance criteria. Consequently, ensuring that apoint-wise robustness certificate for any given data point is obtainedrelatively cost-effectively is crucial. This work demonstrates that reducingthe number of samples by one or two orders of magnitude can still enable thecomputation of a slightly smaller robustness radius (commonly ~20% radiusreduction) with the same confidence. We provide the mathematical foundation forexplaining the phenomenon while experimentally showing promising results on thestandard CIFAR-10 and ImageNet datasets.</description><author>Emmanouil Seferis, Stefanos Kollias, Chih-Hong Cheng</author><pubDate>Fri, 26 Apr 2024 13:43:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17371v1</guid></item><item><title>Assessing the Potential of AI for Spatially Sensitive Nature-Related Financial Risks</title><link>http://arxiv.org/abs/2404.17369v1</link><description>There is growing recognition among financial institutions, financialregulators and policy makers of the importance of addressing nature-relatedrisks and opportunities. Evaluating and assessing nature-related risks forfinancial institutions is challenging due to the large volume of heterogeneousdata available on nature and the complexity of investment value chains and thevarious components' relationship to nature. The dual problem of scaling dataanalytics and analysing complex systems can be addressed using ArtificialIntelligence (AI). We address issues such as plugging existing data gaps withdiscovered data, data estimation under uncertainty, time series analysis and(near) real-time updates. This report presents potential AI solutions formodels of two distinct use cases, the Brazil Beef Supply Use Case and the WaterUtility Use Case. Our two use cases cover a broad perspective withinsustainable finance. The Brazilian cattle farming use case is an example ofgreening finance - integrating nature-related considerations into mainstreamfinancial decision-making to transition investments away from sectors with poorhistorical track records and unsustainable operations. The deployment ofnature-based solutions in the UK water utility use case is an example offinancing green - driving investment to nature-positive outcomes. The two usecases also cover different sectors, geographies, financial assets and AImodelling techniques, providing an overview on how AI could be applied todifferent challenges relating to nature's integration into finance. This reportis primarily aimed at financial institutions but is also of interest to ESGdata providers, TNFD, systems modellers, and, of course, AI practitioners.</description><author>Steven Reece, Emma O donnell, Felicia Liu, Joanna Wolstenholme, Frida Arriaga, Giacomo Ascenzi, Richard Pywell</author><pubDate>Fri, 26 Apr 2024 13:42:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17369v1</guid></item><item><title>Similarity Equivariant Graph Neural Networks for Homogenization of Metamaterials</title><link>http://arxiv.org/abs/2404.17365v1</link><description>Soft, porous mechanical metamaterials exhibit pattern transformations thatmay have important applications in soft robotics, sound reduction andbiomedicine. To design these innovative materials, it is important to be ableto simulate them accurately and quickly, in order to tune their mechanicalproperties. Since conventional simulations using the finite element methodentail a high computational cost, in this article we aim to develop a machinelearning-based approach that scales favorably to serve as a surrogate model. Toensure that the model is also able to handle various microstructures, includingthose not encountered during training, we include the microstructure as part ofthe network input. Therefore, we introduce a graph neural network that predictsglobal quantities (energy, stress stiffness) as well as the patterntransformations that occur (the kinematics). To make our model as accurate anddata-efficient as possible, various symmetries are incorporated into the model.The starting point is an E(n)-equivariant graph neural network (which respectstranslation, rotation and reflection) that has periodic boundary conditions(i.e., it is in-/equivariant with respect to the choice of RVE), is scalein-/equivariant, can simulate large deformations, and can predict scalars,vectors as well as second and fourth order tensors (specifically energy, stressand stiffness). The incorporation of scale equivariance makes the modelequivariant with respect to the similarities group, of which the Euclideangroup E(n) is a subgroup. We show that this network is more accurate anddata-efficient than graph neural networks with fewer symmetries. To create anefficient graph representation of the finite element discretization, we useonly the internal geometrical hole boundaries from the finite element mesh toachieve a better speed-up and scaling with the mesh size.</description><author>Fleur Hendriks, Vlado Menkovski, Martin Doškář, Marc G. D. Geers, Ondřej Rokoš</author><pubDate>Fri, 26 Apr 2024 13:30:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17365v1</guid></item><item><title>MV-VTON: Multi-View Virtual Try-On with Diffusion Models</title><link>http://arxiv.org/abs/2404.17364v1</link><description>The goal of image-based virtual try-on is to generate an image of the targetperson naturally wearing the given clothing. However, most existing methodssolely focus on the frontal try-on using the frontal clothing. When the viewsof the clothing and person are significantly inconsistent, particularly whenthe person's view is non-frontal, the results are unsatisfactory. To addressthis challenge, we introduce Multi-View Virtual Try-ON (MV-VTON), which aims toreconstruct the dressing results of a person from multiple views using thegiven clothes. On the one hand, given that single-view clothes provideinsufficient information for MV-VTON, we instead employ two images, i.e., thefrontal and back views of the clothing, to encompass the complete view as muchas possible. On the other hand, the diffusion models that have demonstratedsuperior abilities are adopted to perform our MV-VTON. In particular, wepropose a view-adaptive selection method where hard-selection andsoft-selection are applied to the global and local clothing feature extraction,respectively. This ensures that the clothing features are roughly fit to theperson's view. Subsequently, we suggest a joint attention block to align andfuse clothing features with person features. Additionally, we collect a MV-VTONdataset, i.e., Multi-View Garment (MVG), in which each person has multiplephotos with diverse views and poses. Experiments show that the proposed methodnot only achieves state-of-the-art results on MV-VTON task using our MVGdataset, but also has superiority on frontal-view virtual try-on task usingVITON-HD and DressCode datasets. Codes and datasets will be publicly releasedat https://github.com/hywang2002/MV-VTON .</description><author>Haoyu Wang, Zhilu Zhang, Donglin Di, Shiliang Zhang, Wangmeng Zuo</author><pubDate>Fri, 26 Apr 2024 13:27:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17364v1</guid></item></channel></rss>