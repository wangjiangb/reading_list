<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 08 Dec 2023 06:00:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Scaling Laws of Synthetic Images for Model Training ... for Now</title><link>http://arxiv.org/abs/2312.04567v1</link><description>Recent significant advances in text-to-image models unlock the possibility oftraining vision systems using synthetic images, potentially overcoming thedifficulty of collecting curated data at scale. It is unclear, however, howthese models behave at scale, as more synthetic data is added to the trainingset. In this paper we study the scaling laws of synthetic images generated bystate of the art text-to-image models, for the training of supervised models:image classifiers with label supervision, and CLIP with language supervision.We identify several factors, including text prompts, classifier-free guidancescale, and types of text-to-image models, that significantly affect scalingbehavior. After tuning these factors, we observe that synthetic imagesdemonstrate a scaling trend similar to, but slightly less effective than, realimages in CLIP training, while they significantly underperform in scaling whentraining supervised image classifiers. Our analysis indicates that the mainreason for this underperformance is the inability of off-the-shelftext-to-image models to generate certain concepts, a limitation thatsignificantly impairs the training of image classifiers. Our findings alsosuggest that scaling synthetic data can be particularly effective in scenariossuch as: (1) when there is a limited supply of real images for a supervisedproblem (e.g., fewer than 0.5 million images in ImageNet), (2) when theevaluation dataset diverges significantly from the training data, indicatingthe out-of-distribution scenario, or (3) when synthetic data is used inconjunction with real images, as demonstrated in the training of CLIP models.</description><author>Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi, Phillip Isola, Yonglong Tian</author><pubDate>Thu, 07 Dec 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04567v1</guid></item><item><title>CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2310.19784v2</link><description>Incorporating a customized object into image generation presents anattractive feature in text-to-image generation. However, existingoptimization-based and encoder-based methods are hindered by drawbacks such astime-consuming optimization, insufficient identity preservation, and aprevalent copy-pasting effect. To overcome these limitations, we introduceCustomNet, a novel object customization approach that explicitly incorporates3D novel view synthesis capabilities into the object customization process.This integration facilitates the adjustment of spatial position relationshipsand viewpoints, yielding diverse outputs while effectively preserving objectidentity. Moreover, we introduce delicate designs to enable location controland flexible background control through textual descriptions or specificuser-defined images, overcoming the limitations of existing 3D novel viewsynthesis methods. We further leverage a dataset construction pipeline that canbetter handle real-world objects and complex backgrounds. Equipped with thesedesigns, our method facilitates zero-shot object customization withouttest-time optimization, offering simultaneous control over the viewpoints,location, and background. As a result, our CustomNet ensures enhanced identitypreservation and generates diverse, harmonious outputs.</description><author>Ziyang Yuan, Mingdeng Cao, Xintao Wang, Zhongang Qi, Chun Yuan, Ying Shan</author><pubDate>Thu, 07 Dec 2023 15:22:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19784v2</guid></item><item><title>Retrieval-Based Reconstruction For Time-series Contrastive Learning</title><link>http://arxiv.org/abs/2311.00519v2</link><description>The success of self-supervised contrastive learning hinges on identifyingpositive data pairs that, when pushed together in embedding space, encodeuseful information for subsequent downstream tasks. However, in time-series,this is challenging because creating positive pairs via augmentations may breakthe original semantic meaning. We hypothesize that if we can retrieveinformation from one subsequence to successfully reconstruct anothersubsequence, then they should form a positive pair. Harnessing this intuition,we introduce our novel approach: REtrieval-BAsed Reconstruction (REBAR)contrastive learning. First, we utilize a convolutional cross-attentionarchitecture to calculate the REBAR error between two different time-series.Then, through validation experiments, we show that the REBAR error is apredictor of mutual class membership, justifying its usage as apositive/negative labeler. Finally, once integrated into a contrastive learningframework, our REBAR method can learn an embedding that achievesstate-of-the-art performance on downstream tasks across various modalities.</description><author>Maxwell A. Xu, Alexander Moreno, Hui Wei, Benjamin M. Marlin, James M. Rehg</author><pubDate>Thu, 07 Dec 2023 15:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00519v2</guid></item><item><title>CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models</title><link>http://arxiv.org/abs/2312.04350v1</link><description>The ability to perform causal reasoning is widely considered a core featureof intelligence. In this work, we investigate whether large language models(LLMs) can coherently reason about causality. Much of the existing work innatural language processing (NLP) focuses on evaluating commonsense causalreasoning in LLMs, thus failing to assess whether a model can perform causalinference in accordance with a set of well-defined formal rules. To addressthis, we propose a new NLP task, causal inference in natural language, inspiredby the "causal inference engine" postulated by Judea Pearl et al. We compose alarge dataset, CLadder, with 10K samples: based on a collection of causalgraphs and queries (associational, interventional, and counterfactual), weobtain symbolic questions and ground-truth answers, through an oracle causalinference engine. These are then translated into natural language. We evaluatemultiple LLMs on our dataset, and we introduce and evaluate a bespokechain-of-thought prompting strategy, CausalCoT. We show that our task is highlychallenging for LLMs, and we conduct an in-depth analysis to gain deeperinsight into the causal reasoning abilities of LLMs. Our data is open-sourcedat https://huggingface.co/datasets/causalNLP/cladder, and our code can be foundat https://github.com/causalNLP/cladder.</description><author>Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, Bernhard Sch√∂lkopf</author><pubDate>Thu, 07 Dec 2023 15:12:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04350v1</guid></item><item><title>Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs</title><link>http://arxiv.org/abs/2309.07311v4</link><description>Most interpretability research in NLP focuses on understanding the behaviorand features of a fully trained model. However, certain insights into modelbehavior may only be accessible by observing the trajectory of the trainingprocess. We present a case study of syntax acquisition in masked languagemodels (MLMs) that demonstrates how analyzing the evolution of interpretableartifacts throughout training deepens our understanding of emergent behavior.In particular, we study Syntactic Attention Structure (SAS), a naturallyemerging property of MLMs wherein specific Transformer heads tend to focus onspecific syntactic relations. We identify a brief window in pretraining whenmodels abruptly acquire SAS, concurrent with a steep drop in loss. Thisbreakthrough precipitates the subsequent acquisition of linguisticcapabilities. We then examine the causal role of SAS by manipulating SAS duringtraining, and demonstrate that SAS is necessary for the development ofgrammatical capabilities. We further find that SAS competes with otherbeneficial traits during training, and that briefly suppressing SAS improvesmodel quality. These findings offer an interpretation of a real-world exampleof both simplicity bias and breakthrough training dynamics.</description><author>Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt, Naomi Saphra</author><pubDate>Thu, 07 Dec 2023 15:09:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07311v4</guid></item><item><title>When Input Integers are Given in the Unary Numeral Representation</title><link>http://arxiv.org/abs/2312.04348v1</link><description>Many NP-complete problems take integers as part of their input instances.These input integers are generally binarized, that is, provided in the form ofthe "binary" numeral representation, and the lengths of such binary forms areused as a basis unit to measure the computational complexity of the problems.In sharp contrast, the "unarization" (or the "unary" numeral representation) ofnumbers has been known to bring a remarkably different effect onto thecomputational complexity of the problems. When no computational-complexitydifference is observed between binarization and unarization of instances, onthe contrary, the problems are said to be strong NP-complete. This workattempts to spotlight an issue of how the unarization of instances affects thecomputational complexity of various combinatorial problems. We present numerousNP-complete (or even NP-hard) problems, which turn out to be easily solvablewhen input integers are represented in unary. We then discuss the computationalcomplexities of such problems when taking unary-form integer inputs. We hopethat a list of such problems signifies the structural differences betweenstrong NP-completeness and non-strong NP-completeness.</description><author>Tomoyuki Yamakami</author><pubDate>Thu, 07 Dec 2023 15:09:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04348v1</guid></item><item><title>Improved Efficient Two-Stage Denoising Diffusion Power System Measurement Recovery Against False Data Injection Attacks and Data Losses</title><link>http://arxiv.org/abs/2312.04346v1</link><description>Measurement uncertainties, represented by cyber-attacks and data losses,seriously degrade the quality of power system measurements. Fortunately, thepowerful generation ability of the denoising diffusion models can enable moreprecise measurement generation for power system data recovery. However, thecontrollable data generation and efficient computing methods of denoisingdiffusion models for deterministic trajectory still need further investigation.To this end, this paper proposes an improved two-stage denoising diffusionmodel (TSDM) to identify and reconstruct the measurements with variousmeasurement uncertainties. The first stage of the model comprises aclassifier-guided conditional anomaly detection component, while the secondstage involves diffusion-based measurement imputation component. Moreover, theproposed TSDM adopts precise means and optimal variances to accelerate thediffusion generation process with subsequence sampling. Extensive numericalcase studies demonstrate that the proposed TSDM can accurately recover powersystem measurements despite strong randomness under renewable energyintegration and highly nonlinear dynamics under complex cyber-physicalcontingencies. Additionally, the proposed TSDM has stronger robustness comparedto existing reconstruction networks and exhibits lower computational complexitythan general denoising diffusion models.</description><author>Jianhua Pei, Jingyu Wang, Dongyuan Shi, Ping Wang</author><pubDate>Thu, 07 Dec 2023 15:06:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04346v1</guid></item><item><title>Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies</title><link>http://arxiv.org/abs/2312.04344v1</link><description>OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piquedconsiderable interest for its potential in medical applications. Despite itspromise, recent studies and internal reviews highlight its underperformance inspecialized medical tasks. This paper explores the boundary of GPT-4V'scapabilities in medicine, particularly in processing complex imaging data fromendoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, weassessed its foundational competencies, identifying substantial areas forenhancement. Our research emphasizes prompt engineering, an often-underutilizedstrategy for improving AI responsiveness. Through iterative testing, we refinedthe model's prompts, significantly improving its interpretative accuracy andrelevance in medical imaging. From our comprehensive evaluations, we distilled10 effective prompt engineering techniques, each fortifying GPT-4V's medicalacumen. These methodical enhancements facilitate more reliable, precise, andclinically valuable insights from GPT-4V, advancing its operability in criticalhealthcare environments. Our findings are pivotal for those employing AI inmedicine, providing clear, actionable guidance on harnessing GPT-4V's fulldiagnostic potential.</description><author>Pengcheng Chen, Ziyan Huang, Zhongying Deng, Tianbin Li, Yanzhou Su, Haoyu Wang, Jin Ye, Yu Qiao, Junjun He</author><pubDate>Thu, 07 Dec 2023 15:05:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04344v1</guid></item><item><title>Causality and Explainability for Trustworthy Integrated Pest Management</title><link>http://arxiv.org/abs/2312.04343v1</link><description>Pesticides serve as a common tool in agricultural pest control butsignificantly contribute to the climate crisis. To combat this, Integrated PestManagement (IPM) stands as a climate-smart alternative. Despite its potential,IPM faces low adoption rates due to farmers' skepticism about itseffectiveness. To address this challenge, we introduce an advanced dataanalysis framework tailored to enhance IPM adoption. Our framework provides i)robust pest population predictions across diverse environments with invariantand causal learning, ii) interpretable pest presence predictions usingtransparent models, iii) actionable advice through counterfactual explanationsfor in-season IPM interventions, iv) field-specific treatment effectestimations, and v) assessments of the effectiveness of our advice using causalinference. By incorporating these features, our framework aims to alleviateskepticism and encourage wider adoption of IPM practices among farmers.</description><author>Ilias Tsoumas, Vasileios Sitokonstantinou, Georgios Giannarakis, Evagelia Lampiri, Christos Athanassiou, Gustau Camps-Valls, Charalampos Kontoes, Ioannis Athanasiadis</author><pubDate>Thu, 07 Dec 2023 15:05:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04343v1</guid></item><item><title>Merging by Matching Models in Task Subspaces</title><link>http://arxiv.org/abs/2312.04339v1</link><description>Model merging aims to cheaply combine individual task-specific models into asingle multitask model. In this work, we view past merging methods asleveraging different notions of a ''task subspace'' in which models are matchedbefore being merged. We connect the task subspace of a given model to its losslandscape and formalize how this approach to model merging can be seen assolving a linear system of equations. While past work has generally beenlimited to linear systems that have a closed-form solution, we consider usingthe conjugate gradient method to find a solution. We show that using theconjugate gradient method can outperform closed-form solutions, enables mergingvia linear systems that are otherwise intractable to solve, and flexibly allowschoosing from a wide variety of initializations and estimates for the ''tasksubspace''. We ultimately demonstrate that our merging framework called''Matching Models in their Task Subspace'' (MaTS) achieves state-of-the-artresults in multitask and intermediate-task model merging. We release all of thecode and checkpoints used in our work at https://github.com/r-three/mats.</description><author>Derek Tam, Mohit Bansal, Colin Raffel</author><pubDate>Thu, 07 Dec 2023 14:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04339v1</guid></item><item><title>Recurrent neural networks and transfer learning for elasto-plasticity in woven composites</title><link>http://arxiv.org/abs/2311.13434v2</link><description>As a surrogate for computationally intensive meso-scale simulation of wovencomposites, this article presents Recurrent Neural Network (RNN) models.Leveraging the power of transfer learning, the initialization challenges andsparse data issues inherent in cyclic shear strain loads are addressed in theRNN models. A mean-field model generates a comprehensive data set representingelasto-plastic behavior. In simulations, arbitrary six-dimensional strainhistories are used to predict stresses under random walking as the source taskand cyclic loading conditions as the target task. Incorporating sub-scaleproperties enhances RNN versatility. In order to achieve accurate predictions,the model uses a grid search method to tune network architecture andhyper-parameter configurations. The results of this study demonstrate thattransfer learning can be used to effectively adapt the RNN to varying strainconditions, which establishes its potential as a useful tool for modelingpath-dependent responses in woven composites.</description><author>Ehsan Ghane, Martin Fagerstr√∂m, Mohsen Mirkhalaf</author><pubDate>Thu, 07 Dec 2023 14:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13434v2</guid></item><item><title>Multi-View Unsupervised Image Generation with Cross Attention Guidance</title><link>http://arxiv.org/abs/2312.04337v1</link><description>The growing interest in novel view synthesis, driven by Neural Radiance Field(NeRF) models, is hindered by scalability issues due to their reliance onprecisely annotated multi-view images. Recent models address this byfine-tuning large text2image diffusion models on synthetic multi-view data.Despite robust zero-shot generalization, they may need post-processing and canface quality issues due to the synthetic-real domain gap. This paper introducesa novel pipeline for unsupervised training of a pose-conditioned diffusionmodel on single-category datasets. With the help of pretrained self-supervisedVision Transformers (DINOv2), we identify object poses by clustering thedataset through comparing visibility and locations of specific object parts.The pose-conditioned diffusion model, trained on pose labels, and equipped withcross-frame attention at inference time ensures cross-view consistency, that isfurther aided by our novel hard-attention guidance. Our model, MIRAGE,surpasses prior work in novel view synthesis on real images. Furthermore,MIRAGE is robust to diverse textures and geometries, as demonstrated with ourexperiments on synthetic images generated with pretrained Stable Diffusion.</description><author>Llukman Cerkezi, Aram Davtyan, Sepehr Sameni, Paolo Favaro</author><pubDate>Thu, 07 Dec 2023 14:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04337v1</guid></item><item><title>Navigating News Narratives: A Media Bias Analysis Dataset</title><link>http://arxiv.org/abs/2312.00168v2</link><description>The proliferation of biased news narratives across various media platformshas become a prominent challenge, influencing public opinion on critical topicslike politics, health, and climate change. This paper introduces the"Navigating News Narratives: A Media Bias Analysis Dataset", a comprehensivedataset to address the urgent need for tools to detect and analyze media bias.This dataset encompasses a broad spectrum of biases, making it a unique andvaluable asset in the field of media studies and artificial intelligence. Thedataset is available athttps://huggingface.co/datasets/newsmediabias/news-bias-full-data.</description><author>Shaina Raza</author><pubDate>Thu, 07 Dec 2023 14:53:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00168v2</guid></item><item><title>Towards a Perceptual Evaluation Framework for Lighting Estimation</title><link>http://arxiv.org/abs/2312.04334v1</link><description>Progress in lighting estimation is tracked by computing existing imagequality assessment (IQA) metrics on images from standard datasets. While thismay appear to be a reasonable approach, we demonstrate that doing so does notcorrelate to human preference when the estimated lighting is used to relight avirtual scene into a real photograph. To study this, we design a controlledpsychophysical experiment where human observers must choose their preferenceamongst rendered scenes lit using a set of lighting estimation algorithmsselected from the recent literature, and use it to analyse how these algorithmsperform according to human perception. Then, we demonstrate that none of themost popular IQA metrics from the literature, taken individually, correctlyrepresent human perception. Finally, we show that by learning a combination ofexisting IQA metrics, we can more accurately represent human preference. Thisprovides a new perceptual framework to help evaluate future lighting estimationalgorithms.</description><author>Justine Giroux, Mohammad Reza Karimi Dastjerdi, Yannick Hold-Geoffroy, Javier Vazquez-Corral, Jean-Fran√ßois Lalonde</author><pubDate>Thu, 07 Dec 2023 14:51:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04334v1</guid></item><item><title>Beyond Surface: Probing LLaMA Across Scales and Layers</title><link>http://arxiv.org/abs/2312.04333v1</link><description>This paper presents an in-depth analysis of Large Language Models (LLMs),focusing on LLaMA, a prominent open-source foundational model in naturallanguage processing. Instead of assessing LLaMA through its generative output,we design multiple-choice tasks to probe its intrinsic understanding inhigh-order tasks such as reasoning and computation. We examine the modelhorizontally, comparing different sizes, and vertically, assessing differentlayers. We unveil several key and uncommon findings based on the designedprobing tasks: (1) Horizontally, enlarging model sizes almost could notautomatically impart additional knowledge or computational prowess. Instead, itcan enhance reasoning abilities, especially in math problem solving, and helpsreduce hallucinations, but only beyond certain size thresholds; (2) In verticalanalysis, the lower layers of LLaMA lack substantial arithmetic and factualknowledge, showcasing logical thinking, multilingual and recognitive abilities,with top layers housing most computational power and real-world knowledge.</description><author>Nuo Chen, Ning Wu, Shining Liang, Ming Gong, Linjun Shou, Dongmei Zhang, Jia Li</author><pubDate>Thu, 07 Dec 2023 14:50:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04333v1</guid></item><item><title>Surrogate Modelling for Sea Ice Concentration using Lightweight Neural Ensemble</title><link>http://arxiv.org/abs/2312.04330v1</link><description>The modeling and forecasting of sea ice conditions in the Arctic region areimportant tasks for ship routing, offshore oil production, and environmentalmonitoring. We propose the adaptive surrogate modeling approach named LANE-SI(Lightweight Automated Neural Ensembling for Sea Ice) that uses ensemble ofrelatively simple deep learning models with different loss functions forforecasting of spatial distribution for sea ice concentration in the specifiedwater area. Experimental studies confirm the quality of a long-term forecastbased on a deep learning model fitted to the specific water area is comparableto resource-intensive physical modeling, and for some periods of the year, itis superior. We achieved a 20% improvement against the state-of-the-artphysics-based forecast system SEAS5 for the Kara Sea.</description><author>Julia Borisova, Nikolay O. Nikitin</author><pubDate>Thu, 07 Dec 2023 14:48:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04330v1</guid></item><item><title>A Multi-scale Information Integration Framework for Infrared and Visible Image Fusion</title><link>http://arxiv.org/abs/2312.04328v1</link><description>Infrared and visible image fusion aims at generating a fused image containingthe intensity and detail information of source images, and the key issue iseffectively measuring and integrating the complementary information ofmulti-modality images from the same scene. Existing methods mostly adopt asimple weight in the loss function to decide the information retention of eachmodality rather than adaptively measuring complementary information fordifferent image pairs. In this study, we propose a multi-scale dual attention(MDA) framework for infrared and visible image fusion, which is designed tomeasure and integrate complementary information in both structure and lossfunction at the image and patch level. In our method, the residual downsampleblock decomposes source images into three scales first. Then, dual attentionfusion block integrates complementary information and generates a spatial andchannel attention map at each scale for feature fusion. Finally, the outputimage is reconstructed by the residual reconstruction block. Loss functionconsists of image-level, feature-level and patch-level three parts, of whichthe calculation of the image-level and patch-level two parts are based on theweights generated by the complementary information measurement. Indeed, toconstrain the pixel intensity distribution between the output and infraredimage, a style loss is added. Our fusion results perform robust and informativeacross different scenarios. Qualitative and quantitative results on twodatasets illustrate that our method is able to preserve both thermal radiationand detailed information from two modalities and achieve comparable resultscompared with the other state-of-the-art methods. Ablation experiments show theeffectiveness of our information integration architecture and adaptivelymeasure complementary information retention in the loss function.</description><author>Guang Yang, Jie Li, Hanxiao Lei, Xinbo Gao</author><pubDate>Thu, 07 Dec 2023 14:40:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04328v1</guid></item><item><title>PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization</title><link>http://arxiv.org/abs/2310.16427v2</link><description>Highly effective, task-specific prompts are often heavily engineered byexperts to integrate detailed instructions and domain insights based on a deepunderstanding of both instincts of large language models (LLMs) and theintricacies of the target task. However, automating the generation of suchexpert-level prompts remains elusive. Existing prompt optimization methods tendto overlook the depth of domain knowledge and struggle to efficiently explorethe vast space of expert-level prompts. Addressing this, we presentPromptAgent, an optimization method that autonomously crafts prompts equivalentin quality to those handcrafted by experts. At its core, PromptAgent viewsprompt optimization as a strategic planning problem and employs a principledplanning algorithm, rooted in Monte Carlo tree search, to strategicallynavigate the expert-level prompt space. Inspired by human-like trial-and-errorexploration, PromptAgent induces precise expert-level insights and in-depthinstructions by reflecting on model errors and generating constructive errorfeedback. Such a novel framework allows the agent to iteratively examineintermediate prompts (states), refine them based on error feedbacks (actions),simulate future rewards, and search for high-reward paths leading to expertprompts. We apply PromptAgent to 12 tasks spanning three practical domains:BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showingit significantly outperforms strong Chain-of-Thought and recent promptoptimization baselines. Extensive analyses emphasize its capability to craftexpert-level, detailed, and domain-insightful prompts with great efficiency andgeneralizability.</description><author>Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, Zhiting Hu</author><pubDate>Thu, 07 Dec 2023 14:39:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16427v2</guid></item><item><title>Learning to sample in Cartesian MRI</title><link>http://arxiv.org/abs/2312.04327v1</link><description>Despite its exceptional soft tissue contrast, Magnetic Resonance Imaging(MRI) faces the challenge of long scanning times compared to other modalitieslike X-ray radiography. Shortening scanning times is crucial in clinicalsettings, as it increases patient comfort, decreases examination costs andimproves throughput. Recent advances in compressed sensing (CS) and deeplearning allow accelerated MRI acquisition by reconstructing high-qualityimages from undersampled data. While reconstruction algorithms have receivedmost of the focus, designing acquisition trajectories to optimizereconstruction quality remains an open question. This thesis explores twoapproaches to address this gap in the context of Cartesian MRI. First, wepropose two algorithms, lazy LBCS and stochastic LBCS, that significantlyimprove upon G\"ozc\"u et al.'s greedy learning-based CS (LBCS) approach. Thesealgorithms scale to large, clinically relevant scenarios like multi-coil 3D MRand dynamic MRI, previously inaccessible to LBCS. Additionally, we demonstratethat generative adversarial networks (GANs) can serve as a natural criterionfor adaptive sampling by leveraging variance in the measurement domain to guideacquisition. Second, we delve into the underlying structures or assumptionsthat enable mask design algorithms to perform well in practice. Our experimentsreveal that state-of-the-art deep reinforcement learning (RL) approaches, whilecapable of adaptation and long-horizon planning, offer only marginalimprovements over stochastic LBCS, which is neither adaptive nor does long-termplanning. Altogether, our findings suggest that stochastic LBCS and similarmethods represent promising alternatives to deep RL. They shine in particularby their scalability and computational efficiency and could be key in thedeployment of optimized acquisition trajectories in Cartesian MRI.</description><author>Thomas Sanchez</author><pubDate>Thu, 07 Dec 2023 14:38:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04327v1</guid></item><item><title>iDesigner: A High-Resolution and Complex-Prompt Following Text-to-Image Diffusion Model for Interior Design</title><link>http://arxiv.org/abs/2312.04326v1</link><description>With the open-sourcing of text-to-image models (T2I) such as stable diffusion(SD) and stable diffusion XL (SD-XL), there is an influx of models fine-tunedin specific domains based on the open-source SD model, such as in anime,character portraits, etc. However, there are few specialized models in certaindomains, such as interior design, which is attributed to the complex textualdescriptions and detailed visual elements inherent in design, alongside thenecessity for adaptable resolution. Therefore, text-to-image models forinterior design are required to have outstanding prompt-following capabilities,as well as iterative collaboration with design professionals to achieve thedesired outcome. In this paper, we collect and optimize text-image data in thedesign field and continue training in both English and Chinese on the basis ofthe open-source CLIP model. We also proposed a fine-tuning strategy withcurriculum learning and reinforcement learning from CLIP feedback to enhancethe prompt-following capabilities of our approach so as to improve the qualityof image generation. The experimental results on the collected datasetdemonstrate the effectiveness of the proposed approach, which achievesimpressive results and outperforms strong baselines.</description><author>Ruyi Gan, Xiaojun Wu, Junyu Lu, Yuanhe Tian, Dixiang Zhang, Ziwei Wu, Renliang Sun, Chang Liu, Jiaxing Zhang, Pingjian Zhang, Yan Song</author><pubDate>Thu, 07 Dec 2023 14:37:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04326v1</guid></item><item><title>Equivariant Scalar Fields for Molecular Docking with Fast Fourier Transforms</title><link>http://arxiv.org/abs/2312.04323v1</link><description>Molecular docking is critical to structure-based virtual screening, yet thethroughput of such workflows is limited by the expensive optimization ofscoring functions involved in most docking algorithms. We explore how machinelearning can accelerate this process by learning a scoring function with afunctional form that allows for more rapid optimization. Specifically, wedefine the scoring function to be the cross-correlation of multi-channel ligandand protein scalar fields parameterized by equivariant graph neural networks,enabling rapid optimization over rigid-body degrees of freedom with fastFourier transforms. The runtime of our approach can be amortized at severallevels of abstraction, and is particularly favorable for virtual screeningsettings with a common binding pocket. We benchmark our scoring functions ontwo simplified docking-related tasks: decoy pose scoring and rigid conformerdocking. Our method attains similar but faster performance on crystalstructures compared to the widely-used Vina and Gnina scoring functions, and ismore robust on computationally predicted structures. Code is available athttps://github.com/bjing2016/scalar-fields.</description><author>Bowen Jing, Tommi Jaakkola, Bonnie Berger</author><pubDate>Thu, 07 Dec 2023 14:32:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04323v1</guid></item><item><title>Post Hoc Explanations of Language Models Can Improve Language Models</title><link>http://arxiv.org/abs/2305.11426v3</link><description>Large Language Models (LLMs) have demonstrated remarkable capabilities inperforming complex tasks. Moreover, recent research has shown thatincorporating human-annotated rationales (e.g., Chain-of-Thought prompting)during in-context learning can significantly enhance the performance of thesemodels, particularly on tasks that require reasoning capabilities. However,incorporating such rationales poses challenges in terms of scalability as thisrequires a high degree of human involvement. In this work, we present a novelframework, Amplifying Model Performance by Leveraging In-Context Learning withPost Hoc Explanations (AMPLIFY), which addresses the aforementioned challengesby automating the process of rationale generation. To this end, we leveragepost hoc explanation methods which output attribution scores (explanations)capturing the influence of each of the input features on model predictions.More specifically, we construct automated natural language rationales thatembed insights from post hoc explanations to provide corrective signals toLLMs. Extensive experimentation with real-world datasets demonstrates that ourframework, AMPLIFY, leads to prediction accuracy improvements of about 10-25%over a wide range of tasks, including those where prior approaches which relyon human-annotated rationales such as Chain-of-Thought prompting fall short.Our work makes one of the first attempts at highlighting the potential of posthoc explanations as valuable tools for enhancing the effectiveness of LLMs.Furthermore, we conduct additional empirical analyses and ablation studies todemonstrate the impact of each of the components of AMPLIFY, which, in turn,leads to critical insights for refining in-context learning.</description><author>Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, Himabindu Lakkaraju</author><pubDate>Thu, 07 Dec 2023 14:27:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11426v3</guid></item><item><title>How (not) to ensemble LVLMs for VQA</title><link>http://arxiv.org/abs/2310.06641v2</link><description>This paper studies ensembling in the era of Large Vision-Language Models(LVLMs). Ensembling is a classical method to combine different models to getincreased performance. In the recent work on Encyclopedic-VQA the authorsexamine a wide variety of models to solve their task: from vanilla LVLMs, tomodels including the caption as extra context, to models augmented withLens-based retrieval of Wikipedia pages. Intuitively these models are highlycomplementary, which should make them ideal for ensembling. Indeed, an oracleexperiment shows potential gains from 48.8% accuracy (the best single model)all the way up to 67% (best possible ensemble). So it is a trivial exercise tocreate an ensemble with substantial real gains. Or is it?</description><author>Lisa Alazraki, Lluis Castrejon, Mostafa Dehghani, Fantine Huot, Jasper Uijlings, Thomas Mensink</author><pubDate>Thu, 07 Dec 2023 14:24:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06641v2</guid></item><item><title>MIMo: A Multi-Modal Infant Model for Studying Cognitive Development</title><link>http://arxiv.org/abs/2312.04318v1</link><description>Human intelligence and human consciousness emerge gradually during theprocess of cognitive development. Understanding this development is anessential aspect of understanding the human mind and may facilitate theconstruction of artificial minds with similar properties. Importantly, humancognitive development relies on embodied interactions with the physical andsocial environment, which is perceived via complementary sensory modalities.These interactions allow the developing mind to probe the causal structure ofthe world. This is in stark contrast to common machine learning approaches,e.g., for large language models, which are merely passively ``digesting'' largeamounts of training data, but are not in control of their sensory inputs.However, computational modeling of the kind of self-determined embodiedinteractions that lead to human intelligence and consciousness is a formidablechallenge. Here we present MIMo, an open-source multi-modal infant model forstudying early cognitive development through computer simulations. MIMo's bodyis modeled after an 18-month-old child with detailed five-fingered hands. MIMoperceives its surroundings via binocular vision, a vestibular system,proprioception, and touch perception through a full-body virtual skin, whiletwo different actuation models allow control of his body. We describe thedesign and interfaces of MIMo and provide examples illustrating its use. Allcode is available at https://github.com/trieschlab/MIMo .</description><author>Dominik Mattern, Pierre Schumacher, Francisco M. L√≥pez, Marcel C. Raabe, Markus R. Ernst, Arthur Aubret, Jochen Triesch</author><pubDate>Thu, 07 Dec 2023 14:21:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04318v1</guid></item><item><title>Towards Knowledge-driven Autonomous Driving</title><link>http://arxiv.org/abs/2312.04316v1</link><description>This paper explores the emerging knowledge-driven autonomous drivingtechnologies. Our investigation highlights the limitations of currentautonomous driving systems, in particular their sensitivity to data bias,difficulty in handling long-tail scenarios, and lack of interpretability.Conversely, knowledge-driven methods with the abilities of cognition,generalization and life-long learning emerge as a promising way to overcomethese challenges. This paper delves into the essence of knowledge-drivenautonomous driving and examines its core components: dataset \&amp; benchmark,environment, and driver agent. By leveraging large language models, worldmodels, neural rendering, and other advanced artificial intelligencetechniques, these components collectively contribute to a more holistic,adaptive, and intelligent autonomous driving system. The paper systematicallyorganizes and reviews previous research efforts in this area, and providesinsights and guidance for future research and practical applications ofautonomous driving. We will continually share the latest updates oncutting-edge developments in knowledge-driven autonomous driving along with therelevant valuable open-source resources at:\url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.</description><author>Xin Li, Yeqi Bai, Pinlong Cai, Licheng Wen, Daocheng Fu, Bo Zhang, Xuemeng Yang, Xinyu Cai, Tao Ma, Jianfei Guo, Xing Gao, Min Dou, Botian Shi, Yong Liu, Liang He, Yu Qiao</author><pubDate>Thu, 07 Dec 2023 14:17:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04316v1</guid></item><item><title>GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific Narratives</title><link>http://arxiv.org/abs/2312.04314v1</link><description>Learning scene graphs from natural language descriptions has proven to be acheap and promising scheme for Scene Graph Generation (SGG). However, suchunstructured caption data and its processing are troubling the learning anacurrate and complete scene graph. This dilema can be summarized as threepoints. First, traditional language parsers often fail to extract meaningfulrelationship triplets from caption data. Second, grounding unlocalized objectsin parsed triplets will meet ambiguity in visual-language alignment. Last,caption data typically are sparse and exhibit bias to partial observations ofimage content. These three issues make it hard for the model to generatecomprehensive and accurate scene graphs. To fill this gap, we propose a simpleyet effective framework, GPT4SGG, to synthesize scene graphs from holistic andregion-specific narratives. The framework discards traditional language parser,and localize objects before obtaining relationship triplets. To obtainrelationship triplets, holistic and dense region-specific narratives aregenerated from the image. With such textual representation of image data and atask-specific prompt, an LLM, particularly GPT-4, directly synthesizes a scenegraph as "pseudo labels". Experimental results showcase GPT4SGG significantlyimproves the performance of SGG models trained on image-caption data. Webelieve this pioneering work can motivate further research into mining thevisual reasoning capabilities of LLMs.</description><author>Zuyao Chen, Jinlin Wu, Zhen Lei, Zhaoxiang Zhang, Changwen Chen</author><pubDate>Thu, 07 Dec 2023 14:11:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04314v1</guid></item><item><title>Stochastic-Constrained Stochastic Optimization with Markovian Data</title><link>http://arxiv.org/abs/2312.04312v1</link><description>This paper considers stochastic-constrained stochastic optimization where thestochastic constraint is to satisfy that the expectation of a random functionis below a certain threshold. In particular, we study the setting where datasamples are drawn from a Markov chain and thus are not independent andidentically distributed. We generalize the drift-plus-penalty framework, aprimal-dual stochastic gradient method developed for the i.i.d. case, to theMarkov chain sampling setting. We propose two variants of drift-plus-penalty;one is for the case when the mixing time of the underlying Markov chain isknown while the other is for the case of unknown mixing time. In fact, ouralgorithms apply to a more general setting of constrained online convexoptimization where the sequence of constraint functions follows a Markov chain.Both algorithms are adaptive in that the first works without knowledge of thetime horizon while the second uses AdaGrad-style algorithm parameters, which isof independent interest. We demonstrate the effectiveness of our proposedmethods through numerical experiments on classification with fairnessconstraints.</description><author>Yeongjong Kim, Dabeen Lee</author><pubDate>Thu, 07 Dec 2023 14:09:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04312v1</guid></item><item><title>Finding Interpretable Class-Specific Patterns through Efficient Neural Search</title><link>http://arxiv.org/abs/2312.04311v1</link><description>Discovering patterns in data that best describe the differences betweenclasses allows to hypothesize and reason about class-specific mechanisms. Inmolecular biology, for example, this bears promise of advancing theunderstanding of cellular processes differing between tissues or diseases,which could lead to novel treatments. To be useful in practice, methods thattackle the problem of finding such differential patterns have to be readilyinterpretable by domain experts, and scalable to the extremely high-dimensionaldata. In this work, we propose a novel, inherently interpretable binary neuralnetwork architecture DIFFNAPS that extracts differential patterns from data.DiffNaps is scalable to hundreds of thousands of features and robust to noise,thus overcoming the limitations of current state-of-the-art methods inlarge-scale applications such as in biology. We show on synthetic and realworld data, including three biological applications, that, unlike itscompetitors, DiffNaps consistently yields accurate, succinct, and interpretableclass descriptions</description><author>Nils Philipp Walter, Jonas Fischer, Jilles Vreeken</author><pubDate>Thu, 07 Dec 2023 14:09:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04311v1</guid></item><item><title>A Structural-Clustering Based Active Learning for Graph Neural Networks</title><link>http://arxiv.org/abs/2312.04307v1</link><description>In active learning for graph-structured data, Graph Neural Networks (GNNs)have shown effectiveness. However, a common challenge in these applications isthe underutilization of crucial structural information. To address thisproblem, we propose the Structural-Clustering PageRank method for improvedActive learning (SPA) specifically designed for graph-structured data. SPAintegrates community detection using the SCAN algorithm with the PageRankscoring method for efficient and informative sample selection. SPA prioritizesnodes that are not only informative but also central in structure. Throughextensive experiments, SPA demonstrates higher accuracy and macro-F1 score overexisting methods across different annotation budgets and achieves significantreductions in query time. In addition, the proposed method only adds twohyperparameters, $\epsilon$ and $\mu$ in the algorithm to finely tune thebalance between structural learning and node selection. This simplicity is akey advantage in active learning scenarios, where extensive hyperparametertuning is often impractical.</description><author>Ricky Maulana Fajri, Yulong Pei, Lu Yin, Mykola Pechenizkiy</author><pubDate>Thu, 07 Dec 2023 14:04:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04307v1</guid></item><item><title>nerblackbox: A High-level Library for Named Entity Recognition in Python</title><link>http://arxiv.org/abs/2312.04306v1</link><description>We present nerblackbox, a python library to facilitate the use ofstate-of-the-art transformer-based models for named entity recognition. Itprovides simple-to-use yet powerful methods to access data and models from awide range of sources, for fully automated model training and evaluation aswell as versatile model inference. While many technical challenges are solvedand hidden from the user by default, nerblackbox also offers fine-grainedcontrol and a rich set of customizable features. It is thus targeted both atapplication-oriented developers as well as machine learning experts andresearchers.</description><author>Felix Stollenwerk</author><pubDate>Thu, 07 Dec 2023 14:04:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04306v1</guid></item><item><title>Reinforcement Learning for Combining Search Methods in the Calibration of Economic ABMs</title><link>http://arxiv.org/abs/2302.11835v3</link><description>Calibrating agent-based models (ABMs) in economics and finance typicallyinvolves a derivative-free search in a very large parameter space. In thiswork, we benchmark a number of search methods in the calibration of awell-known macroeconomic ABM on real data, and further assess the performanceof "mixed strategies" made by combining different methods. We find that methodsbased on random-forest surrogates are particularly efficient, and thatcombining search methods generally increases performance since the biases ofany single method are mitigated. Moving from these observations, we propose areinforcement learning (RL) scheme to automatically select and combine searchmethods on-the-fly during a calibration run. The RL agent keeps exploiting aspecific method only as long as this keeps performing well, but explores newstrategies when the specific method reaches a performance plateau. Theresulting RL search scheme outperforms any other method or method combinationtested, and does not rely on any prior information or trial and errorprocedure.</description><author>Aldo Glielmo, Marco Favorito, Debmallya Chanda, Domenico Delli Gatti</author><pubDate>Thu, 07 Dec 2023 13:54:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11835v3</guid></item><item><title>Prompt Highlighter: Interactive Control for Multi-Modal LLMs</title><link>http://arxiv.org/abs/2312.04302v1</link><description>This study targets a critical aspect of multi-modal LLMs' (LLMs&amp;VLMs)inference: explicit controllable text generation. Multi-modal LLMs empowermulti-modality understanding with the capability of semantic generation yetbring less explainability and heavier reliance on prompt contents due to theirautoregressive generative nature. While manipulating prompt formats couldimprove outputs, designing specific and precise prompts per task can bechallenging and ineffective. To tackle this issue, we introduce a novelinference method, Prompt Highlighter, which enables users to highlight specificprompt spans to interactively control the focus during generation. Motivated bythe classifier-free diffusion guidance, we form regular and unconditionalcontext pairs based on highlighted tokens, demonstrating that theautoregressive generation in models can be guided in a classifier-free way.Notably, we find that, during inference, guiding the models with highlightedtokens through the attention weights leads to more desired outputs. Ourapproach is compatible with current LLMs and VLMs, achieving impressivecustomized generation results without training. Experiments confirm itseffectiveness in focusing on input contexts and generating reliable content.Without tuning on LLaVA-v1.5, our method secured 69.5 in the MMBench test and1552.5 in MME-perception. The code is available at:https://github.com/dvlab-research/Prompt-Highlighter/</description><author>Yuechen Zhang, Shengju Qian, Bohao Peng, Shu Liu, Jiaya Jia</author><pubDate>Thu, 07 Dec 2023 13:53:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04302v1</guid></item><item><title>TSGBench: Time Series Generation Benchmark</title><link>http://arxiv.org/abs/2309.03755v2</link><description>Synthetic Time Series Generation (TSG) is crucial in a range of applications,including data augmentation, anomaly detection, and privacy preservation.Although significant strides have been made in this field, existing methodsexhibit three key limitations: (1) They often benchmark against similar modeltypes, constraining a holistic view of performance capabilities. (2) The use ofspecialized synthetic and private datasets introduces biases and hampersgeneralizability. (3) Ambiguous evaluation measures, often tied to customnetworks or downstream tasks, hinder consistent and fair comparison. To overcome these limitations, we introduce \textsf{TSGBench}, the inauguralTime Series Generation Benchmark, designed for a unified and comprehensiveassessment of TSG methods. It comprises three modules: (1) a curated collectionof publicly available, real-world datasets tailored for TSG, together with astandardized preprocessing pipeline; (2) a comprehensive evaluation measuressuite including vanilla measures, new distance-based assessments, andvisualization tools; (3) a pioneering generalization test rooted in DomainAdaptation (DA), compatible with all methods. We have conducted comprehensiveexperiments using \textsf{TSGBench} across a spectrum of ten real-worlddatasets from diverse domains, utilizing ten advanced TSG methods and twelveevaluation measures. The results highlight the reliability and efficacy of\textsf{TSGBench} in evaluating TSG methods. Crucially, \textsf{TSGBench}delivers a statistical analysis of the performance rankings of these methods,illuminating their varying performance across different datasets and measuresand offering nuanced insights into the effectiveness of each method.</description><author>Yihao Ang, Qiang Huang, Yifan Bao, Anthony K. H. Tung, Zhiyong Huang</author><pubDate>Thu, 07 Dec 2023 13:42:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03755v2</guid></item><item><title>Cross-codex Learning for Reliable Scribe Identification in Medieval Manuscripts</title><link>http://arxiv.org/abs/2312.04296v1</link><description>Historic scribe identification is a substantial task for obtaininginformation about the past. Uniform script styles, such as the Carolingianminuscule, make it a difficult task for classification to focus on meaningfulfeatures. Therefore, we demonstrate in this paper the importance of cross-codextraining data for CNN based text-independent off-line scribe identification, toovercome codex dependent overfitting. We report three main findings: First, wefound that preprocessing with masked grayscale images instead of RGB imagesclearly increased the F1-score of the classification results. Second, wetrained different neural networks on our complex data, validating time andaccuracy differences in order to define the most reliable network architecture.With AlexNet, the network with the best trade-off between F1-score and time, weachieved for individual classes F1-scores of up to 0,96 on line level and up to1.0 on page level in classification. Third, we could replicate the finding thatthe CNN output can be further improved by implementing a reject option, givingmore stable results. We present the results on our large scale open sourcedataset -- the Codex Claustroneoburgensis database (CCl-DB) -- containing asignificant number of writings from different scribes in several codices. Wedemonstrate for the first time on a dataset with such a variety of codices thatpaleographic decisions can be reproduced automatically and precisely with CNNs.This gives manifold new and fast possibilities for paleographers to gaininsights into unlabeled material, but also to develop further hypotheses.</description><author>Julius Wei√ümann, Markus Seidl, Anya Dietrich, Martin Haltrich</author><pubDate>Thu, 07 Dec 2023 13:40:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04296v1</guid></item><item><title>Calculating the maximum number of maximum cliques for simple graphs</title><link>http://arxiv.org/abs/2307.14120v3</link><description>A simple graph on $n$ vertices may contain a lot of maximum cliques. But howmany can it potentially contain? We will define prime and composite graphs, andwe will show that if $n \ge 15$, then the grpahs with the maximum number ofmaximum cliques have to be composite. Moreover, we will show an edge bound fromwhich we will prove that if any factor of a composite graph has $\omega(G_i)\ge 5$, then it cannot have the maximum number of maximum cliques. Using thiswe will show that the graph that contains $3^{\lfloor n/3 \rfloor}c$ maximumcliques has the most number of maximum cliques on $n$ vertices, where$c\in\{1,\frac{4}{3},2\}$, depending on $n \text{ mod } 3$.</description><author>D√°niel Pfeifer</author><pubDate>Thu, 07 Dec 2023 13:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14120v3</guid></item><item><title>GPT-4V with Emotion: A Zero-shot Benchmark for Multimodal Emotion Understanding</title><link>http://arxiv.org/abs/2312.04293v1</link><description>Recently, GPT-4 with Vision (GPT-4V) has shown remarkable performance acrossvarious multimodal tasks. However, its efficacy in emotion recognition remainsa question. This paper quantitatively evaluates GPT-4V's capabilities inmultimodal emotion understanding, encompassing tasks such as facial emotionrecognition, visual sentiment analysis, micro-expression recognition, dynamicfacial emotion recognition, and multimodal emotion recognition. Our experimentsshow that GPT-4V exhibits impressive multimodal and temporal understandingcapabilities, even surpassing supervised systems in some tasks. Despite theseachievements, GPT-4V is currently tailored for general domains. It performspoorly in micro-expression recognition that requires specialized expertise. Themain purpose of this paper is to present quantitative results of GPT-4V onemotion understanding and establish a zero-shot benchmark for future research.Code and evaluation results are available at:https://github.com/zeroQiaoba/gpt4v-emotion.</description><author>Zheng Lian, Licai Sun, Haiyang Sun, Kang Chen, Zhuofan Wen, Hao Gu, Shun Chen, Bin Liu, Jianhua Tao</author><pubDate>Thu, 07 Dec 2023 13:27:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04293v1</guid></item><item><title>Simulating the Air Quality Impact of Prescribed Fires Using a Graph Neural Network-Based PM$_{2.5}$ Emissions Forecasting System</title><link>http://arxiv.org/abs/2312.04291v1</link><description>The increasing size and severity of wildfires across western North Americahave generated dangerous levels of PM$_{2.5}$ pollution in recent years. In awarming climate, expanding the use of prescribed fires is widely considered tobe the most robust fire mitigation strategy. However, reliably forecasting thepotential air quality impact from these prescribed fires, a critical ingredientin determining the fires' location and time, at hourly to daily time scalesremains a challenging problem. This paper proposes a novel integration ofprescribed fire simulation with a spatio-temporal graph neural network-basedPM$_{2.5}$ forecasting model. The experiments in this work focus on determiningthe optimal time for implementing prescribed fires in California as well asquantifying the potential air quality trade-offs involved in conducting moreprescribed fires outside the fire season.</description><author>Kyleen Liao, Jatan Buch, Kara Lamb, Pierre Gentine</author><pubDate>Thu, 07 Dec 2023 13:18:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04291v1</guid></item><item><title>Factor-Assisted Federated Learning for Personalized Optimization with Heterogeneous Data</title><link>http://arxiv.org/abs/2312.04281v1</link><description>Federated learning is an emerging distributed machine learning frameworkaiming at protecting data privacy. Data heterogeneity is one of the corechallenges in federated learning, which could severely degrade the convergencerate and prediction performance of deep neural networks. To address this issue,we develop a novel personalized federated learning framework for heterogeneousdata, which we refer to as FedSplit. This modeling framework is motivated bythe finding that, data in different clients contain both common knowledge andpersonalized knowledge. Then the hidden elements in each neural layer can besplit into the shared and personalized groups. With this decomposition, a novelobjective function is established and optimized. We demonstrate FedSplitenjoyers a faster convergence speed than the standard federated learning methodboth theoretically and empirically. The generalization bound of the FedSplitmethod is also studied. To practically implement the proposed method on realdatasets, factor analysis is introduced to facilitate the decoupling of hiddenelements. This leads to a practically implemented model for FedSplit and wefurther refer to as FedFac. We demonstrated by simulation studies that, usingfactor analysis can well recover the underlying shared/personalizeddecomposition. The superior prediction performance of FedFac is furtherverified empirically by comparison with various state-of-the-art federatedlearning methods on several real datasets.</description><author>Feifei Wang, Huiyun Tang, Yang Li</author><pubDate>Thu, 07 Dec 2023 13:05:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04281v1</guid></item><item><title>D2S: Representing local descriptors and global scene coordinates for camera relocalization</title><link>http://arxiv.org/abs/2307.15250v2</link><description>State-of-the-art visual localization methods mostly rely on complexprocedures to match local descriptors and 3D point clouds. However, theseprocedures can incur significant cost in terms of inference, storage, andupdates over time. In this study, we propose a direct learning-based approachthat utilizes a simple network named D2S to represent local descriptors andtheir scene coordinates. Our method is characterized by its simplicity andcost-effectiveness. It solely leverages a single RGB image for localizationduring the testing phase and only requires a lightweight model to encode acomplex sparse scene. The proposed D2S employs a combination of a simple lossfunction and graph attention to selectively focus on robust descriptors whiledisregarding areas such as clouds, trees, and several dynamic objects. Thisselective attention enables D2S to effectively perform a binary-semanticclassification for sparse descriptors. Additionally, we propose a new outdoordataset to evaluate the capabilities of visual localization methods in terms ofscene generalization and self-updating from unlabeled observations. Ourapproach outperforms the state-of-the-art CNN-based methods in scene coordinateregression in indoor and outdoor environments. It demonstrates the ability togeneralize beyond training data, including scenarios involving transitions fromday to night and adapting to domain shifts, even in the absence of the labeleddata sources. The source code, trained models, dataset, and demo videos areavailable at the following link: https://thpjp.github.io/d2s</description><author>Bach-Thuan Bui, Dinh-Tuan Tran, Joo-Ho Lee</author><pubDate>Thu, 07 Dec 2023 13:00:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15250v2</guid></item><item><title>Estimating Countries with Similar Maternal Mortality Rate using Cluster Analysis and Pairing Countries with Identical MMR</title><link>http://arxiv.org/abs/2312.04275v1</link><description>In the evolving world, we require more additionally the young era to flourishand evolve into developed land. Most of the population all around the world areunaware of the complications involved in the routine they follow while they arepregnant and how hospital facilities affect maternal health. Maternal Mortalityis the death of a pregnant woman due to intricacies correlated to pregnancy,underlying circumstances exacerbated by the pregnancy or management of thesesituations. It is crucial to consider the Maternal Mortality Rate (MMR) indiverse locations and determine which human routines and hospital facilitiesdiminish the Maternal Mortality Rate (MMR). This research aims to examine anddiscover the countries which are keeping more lavish threats of MMR andcountries alike in MMR encountered. Data is examined and collected for variouscountries, data consists of the earlier years' observation. From theperspective of Machine Learning, Unsupervised Machine Learning is implementedto perform Cluster Analysis. Therefore the pairs of countries with similar MMRas well as the extreme opposite pair concerning the MMR are found.</description><author>S. Nandini, Sanjjushri Varshini R</author><pubDate>Thu, 07 Dec 2023 12:54:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04275v1</guid></item><item><title>Invariant Random Forest: Tree-Based Model Solution for OOD Generalization</title><link>http://arxiv.org/abs/2312.04273v1</link><description>Out-Of-Distribution (OOD) generalization is an essential topic in machinelearning. However, recent research is only focusing on the correspondingmethods for neural networks. This paper introduces a novel and effectivesolution for OOD generalization of decision tree models, named InvariantDecision Tree (IDT). IDT enforces a penalty term with regard to theunstable/varying behavior of a split across different environments during thegrowth of the tree. Its ensemble version, the Invariant Random Forest (IRF), isconstructed. Our proposed method is motivated by a theoretical result undermild conditions, and validated by numerical tests with both synthetic and realdatasets. The superior performance compared to non-OOD tree models implies thatconsidering OOD generalization for tree models is absolutely necessary andshould be given more attention.</description><author>Yufan Liao, Qi Wu, Xing Yan</author><pubDate>Thu, 07 Dec 2023 12:53:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04273v1</guid></item><item><title>HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting</title><link>http://arxiv.org/abs/2312.03461v2</link><description>We have recently seen tremendous progress in photo-real human modeling andrendering. Yet, efficiently rendering realistic human performance andintegrating it into the rasterization pipeline remains challenging. In thispaper, we present HiFi4G, an explicit and compact Gaussian-based approach forhigh-fidelity human performance rendering from dense footage. Our coreintuition is to marry the 3D Gaussian representation with non-rigid tracking,achieving a compact and compression-friendly representation. We first propose adual-graph mechanism to obtain motion priors, with a coarse deformation graphfor effective initialization and a fine-grained Gaussian graph to enforcesubsequent constraints. Then, we utilize a 4D Gaussian optimization scheme withadaptive spatial-temporal regularizers to effectively balance the non-rigidprior and Gaussian updating. We also present a companion compression schemewith residual compensation for immersive experiences on various platforms. Itachieves a substantial compression rate of approximately 25 times, with lessthan 2MB of storage per frame. Extensive experiments demonstrate theeffectiveness of our approach, which significantly outperforms existingapproaches in terms of optimization speed, rendering quality, and storageoverhead.</description><author>Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, Lan Xu</author><pubDate>Thu, 07 Dec 2023 12:46:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03461v2</guid></item><item><title>Activity Grammars for Temporal Action Segmentation</title><link>http://arxiv.org/abs/2312.04266v1</link><description>Sequence prediction on temporal data requires the ability to understandcompositional structures of multi-level semantics beyond individual andcontextual properties. The task of temporal action segmentation, which aims attranslating an untrimmed activity video into a sequence of action segments,remains challenging for this reason. This paper addresses the problem byintroducing an effective activity grammar to guide neural predictions fortemporal action segmentation. We propose a novel grammar induction algorithmthat extracts a powerful context-free grammar from action sequence data. Wealso develop an efficient generalized parser that transforms frame-levelprobability distributions into a reliable sequence of actions according to theinduced grammar with recursive rules. Our approach can be combined with anyneural network for temporal action segmentation to enhance the sequenceprediction and discover its compositional structure. Experimental resultsdemonstrate that our method significantly improves temporal action segmentationin terms of both performance and interpretability on two standard benchmarks,Breakfast and 50 Salads.</description><author>Dayoung Gong, Joonseok Lee, Deunsol Jung, Suha Kwak, Minsu Cho</author><pubDate>Thu, 07 Dec 2023 12:45:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04266v1</guid></item><item><title>Stronger, Fewer, &amp; Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation</title><link>http://arxiv.org/abs/2312.04265v1</link><description>In this paper, we first assess and harness various Vision Foundation Models(VFMs) in the context of Domain Generalized Semantic Segmentation (DGSS).Driven by the motivation that Leveraging Stronger pre-trained models and Fewertrainable parameters for Superior generalizability, we introduce a robustfine-tuning approach, namely Rein, to parameter-efficiently harness VFMs forDGSS. Built upon a set of trainable tokens, each linked to distinct instances,Rein precisely refines and forwards the feature maps from each layer to thenext layer within the backbone. This process produces diverse refinements fordifferent categories within a single image. With fewer trainable parameters,Rein efficiently fine-tunes VFMs for DGSS tasks, surprisingly surpassing fullparameter fine-tuning. Extensive experiments across various settingsdemonstrate that Rein significantly outperforms state-of-the-art methods.Remarkably, with just an extra 1% of trainable parameters within the frozenbackbone, Rein achieves a mIoU of 68.1% on the Cityscapes, without accessingany real urban-scene datasets.</description><author>Zhixiang Wei, Lin Chen, Yi Jin, Xiaoxiao Ma, Tianle Liu, Pengyang Lin, Ben Wang, Huaian Chen, Jinjin Zheng</author><pubDate>Thu, 07 Dec 2023 12:43:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04265v1</guid></item><item><title>Multi-agricultural Machinery Collaborative Task Assignment Based on Improved Genetic Hybrid Optimization Algorithm</title><link>http://arxiv.org/abs/2312.04264v1</link><description>To address the challenges of delayed scheduling information, heavy relianceon manual labour, and low operational efficiency in traditional large-scaleagricultural machinery operations, this study proposes a method formulti-agricultural machinery collaborative task assignment based on an improvedgenetic hybrid optimisation algorithm. The proposed method establishes amulti-agricultural machinery task allocation model by combining the pathpre-planning of a simulated annealing algorithm and the static task allocationof a genetic algorithm. By sequentially fusing these two algorithms, theirrespective shortcomings can be overcome, and their advantages in global andlocal search can be utilised. Consequently, the search capability of thepopulation is enhanced, leading to the discovery of more optimal solutions.Then, an adaptive crossover operator is constructed according to the taskassignment model, considering the capacity, path cost, and time of agriculturalmachinery; two-segment coding and multi-population adaptive mutation are usedto assign tasks to improve the diversity of the population and enhance theexploration ability of the population; and to improve the global optimisationability of the hybrid algorithm, a 2-Opt local optimisation operator and anCircle modification algorithm are introduced. Finally, simulation experimentswere conducted in MATLAB to evaluate the performance of the multi-agriculturalmachinery collaborative task assignment based on the improved genetic hybridalgorithm. The algorithm's capabilities were assessed through comparativeanalysis in the simulation trials. The results demonstrate that the developedhybrid algorithm can effectively reduce path costs, and the efficiency of theassignment outcomes surpasses that of the classical genetic algorithm. Thisapproach proves particularly suitable for addressing large-scale taskallocation problems.</description><author>Haohao Du</author><pubDate>Thu, 07 Dec 2023 12:42:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04264v1</guid></item><item><title>Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications</title><link>http://arxiv.org/abs/2311.05876v2</link><description>Large language models (LLMs) exhibit superior performance on various naturallanguage tasks, but they are susceptible to issues stemming from outdated dataand domain-specific limitations. In order to address these challenges,researchers have pursued two primary strategies, knowledge editing andretrieval augmentation, to enhance LLMs by incorporating external informationfrom different aspects. Nevertheless, there is still a notable absence of acomprehensive survey. In this paper, we propose a review to discuss the trendsin integration of knowledge and large language models, including taxonomy ofmethods, benchmarks, and applications. In addition, we conduct an in-depthanalysis of different methods and point out potential research directions inthe future. We hope this survey offers the community quick access and acomprehensive overview of this research area, with the intention of inspiringfuture research endeavors.</description><author>Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting liu</author><pubDate>Thu, 07 Dec 2023 12:42:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05876v2</guid></item><item><title>Low-complexity subspace-descent over symmetric positive definite manifold</title><link>http://arxiv.org/abs/2305.02041v3</link><description>This work puts forth low-complexity Riemannian subspace descent algorithmsfor the minimization of functions over the symmetric positive definite (SPD)manifold. Different from the existing Riemannian gradient descent variants, theproposed approach utilizes carefully chosen subspaces that allow the update tobe written as a product of the Cholesky factor of the iterate and a sparsematrix. The resulting updates avoid the costly matrix operations like matrixexponentiation and dense matrix multiplication, which are generally required inalmost all other Riemannian optimization algorithms on SPD manifold. We furtheridentify a broad class of functions, arising in diverse applications, such askernel matrix learning, covariance estimation of Gaussian distributions,maximum likelihood parameter estimation of elliptically contoureddistributions, and parameter estimation in Gaussian mixture model problems,over which the Riemannian gradients can be calculated efficiently. The proposeduni-directional and multi-directional Riemannian subspace descent variantsincur per-iteration complexities of $\O(n)$ and $\O(n^2)$ respectively, ascompared to the $\O(n^3)$ or higher complexity incurred by all existingRiemannian gradient descent variants. The superior runtime and lowper-iteration complexity of the proposed algorithms is also demonstrated vianumerical tests on large-scale covariance estimation and matrix square rootproblems.</description><author>Yogesh Darmwal, Ketan Rajawat</author><pubDate>Thu, 07 Dec 2023 12:41:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02041v3</guid></item><item><title>PsyChat: A Client-Centric Dialogue System for Mental Health Support</title><link>http://arxiv.org/abs/2312.04262v1</link><description>Dialogue systems are increasingly integrated into mental health support tohelp clients facilitate exploration, gain insight, take action, and ultimatelyheal themselves. For a dialogue system to be practical and user-friendly, itshould be client-centric, focusing on the client's behaviors. However, existingdialogue systems publicly available for mental health support often concentratesolely on the counselor's strategies rather than the behaviors expressed byclients. This can lead to the implementation of unreasonable or inappropriatecounseling strategies and corresponding responses from the dialogue system. Toaddress this issue, we propose PsyChat, a client-centric dialogue system thatprovides psychological support through online chat. The client-centric dialoguesystem comprises five modules: client behavior recognition, counselor strategyselection, input packer, response generator intentionally fine-tuned to produceresponses, and response selection. Both automatic and human evaluationsdemonstrate the effectiveness and practicality of our proposed dialogue systemfor real-life mental health support. Furthermore, we employ our proposeddialogue system to simulate a real-world client-virtual-counselor interactionscenario. The system is capable of predicting the client's behaviors, selectingappropriate counselor strategies, and generating accurate and suitableresponses, as demonstrated in the scenario.</description><author>Huachuan Qiu, Anqi Li, Lizhi Ma, Zhenzhong Lan</author><pubDate>Thu, 07 Dec 2023 12:40:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04262v1</guid></item><item><title>Dense Optical Tracking: Connecting the Dots</title><link>http://arxiv.org/abs/2312.00786v2</link><description>Recent approaches to point tracking are able to recover the trajectory of anyscene point through a large portion of a video despite the presence ofocclusions. They are, however, too slow in practice to track every pointobserved in a single frame in a reasonable amount of time. This paperintroduces DOT, a novel, simple and efficient method for solving this problem.It first extracts a small set of tracks from key regions at motion boundariesusing an off-the-shelf point tracking algorithm. Given source and targetframes, DOT then computes rough initial estimates of a dense flow field andvisibility mask through nearest-neighbor interpolation, before refining themusing a learnable optical flow estimator that explicitly handles occlusions andcan be trained on synthetic data with ground-truth correspondences. We showthat DOT is significantly more accurate than current optical flow techniques,outperforms sophisticated "universal" trackers like OmniMotion, and is on parwith, or better than, the best point tracking algorithms like CoTracker whilebeing at least two orders of magnitude faster. Quantitative and qualitativeexperiments with synthetic and real videos validate the promise of the proposedapproach. Code, data, and videos showcasing the capabilities of our approachare available in the project webpage: https://16lemoing.github.io/dot .</description><author>Guillaume Le Moing, Jean Ponce, Cordelia Schmid</author><pubDate>Thu, 07 Dec 2023 12:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00786v2</guid></item><item><title>Trajectory-User Linking via Hierarchical Spatio-Temporal Attention Networks</title><link>http://arxiv.org/abs/2302.10903v2</link><description>Trajectory-User Linking (TUL) is crucial for human mobility modeling bylinking diferent trajectories to users with the exploration of complex mobilitypatterns. Existing works mainly rely on the recurrent neural framework toencode the temporal dependencies in trajectories, have fall short in capturingspatial-temporal global context for TUL prediction. To ill this gap, this workpresents a new hierarchical spatio-temporal attention neural network, calledAttnTUL, to jointly encode the local trajectory transitional patterns andglobal spatial dependencies for TUL. Speciically, our irst model component isbuilt over the graph neural architecture to preserve the local and globalcontext and enhance the representation paradigm of geographical regions anduser trajectories. Additionally, a hierarchically structured attention networkis designed to simultaneously encode the intra-trajectory and inter-trajectorydependencies, with the integration of the temporal attention mechanism andglobal elastic attentional encoder. Extensive experiments demonstrate thesuperiority of our AttnTUL method as compared to state-of-the-art baselines onvarious trajectory datasets. The source code of our model is available athttps://github.com/Onedean/AttnTUL.</description><author>Wei Chen, Chao Huang, Yanwei Yu, Yongguo Jiang, Junyu Dong</author><pubDate>Thu, 07 Dec 2023 12:27:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10903v2</guid></item><item><title>A Parameterized Generative Adversarial Network Using Cyclic Projection for Explainable Medical Image Classification</title><link>http://arxiv.org/abs/2311.14388v2</link><description>Although current data augmentation methods are successful to alleviate thedata insufficiency, conventional augmentation are primarily intra-domain whileadvanced generative adversarial networks (GANs) generate images remaininguncertain, particularly in small-scale datasets. In this paper, we propose aparameterized GAN (ParaGAN) that effectively controls the changes of syntheticsamples among domains and highlights the attention regions for downstreamclassification. Specifically, ParaGAN incorporates projection distanceparameters in cyclic projection and projects the source images to the decisionboundary to obtain the class-difference maps. Our experiments show that ParaGANcan consistently outperform the existing augmentation methods with explainableclassification on two small-scale medical datasets.</description><author>Xiangyu Xiong, Yue Sun, Xiaohong Liu, Chan-Tong Lam, Tong Tong, Hao Chen, Qinquan Gao, Wei Ke, Tao Tan</author><pubDate>Thu, 07 Dec 2023 12:20:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14388v2</guid></item><item><title>Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications</title><link>http://arxiv.org/abs/2311.17663v2</link><description>Understanding how the surrounding environment changes is crucial forperforming downstream tasks safely and reliably in autonomous drivingapplications. Recent occupancy estimation techniques using only camera imagesas input can provide dense occupancy representations of large-scale scenesbased on the current observation. However, they are mostly limited torepresenting the current 3D space and do not consider the future state ofsurrounding objects along the time axis. To extend camera-only occupancyestimation into spatiotemporal prediction, we propose Cam4DOcc, a new benchmarkfor camera-only 4D occupancy forecasting, evaluating the surrounding scenechanges in a near future. We build our benchmark based on multiple publiclyavailable datasets, including nuScenes, nuScenes-Occupancy, and Lyft-Level5,which provides sequential occupancy states of general movable and staticobjects, as well as their 3D backward centripetal flow. To establish thisbenchmark for future research with comprehensive comparisons, we introduce fourbaseline types from diverse camera-based perception and predictionimplementations, including a static-world occupancy model, voxelization ofpoint cloud prediction, 2D-3D instance-based prediction, and our proposed novelend-to-end 4D occupancy forecasting network. Furthermore, the standardizedevaluation protocol for preset multiple tasks is also provided to compare theperformance of all the proposed baselines on present and future occupancyestimation with respect to objects of interest in autonomous driving scenarios.The dataset and our implementation of all four baselines in the proposedCam4DOcc benchmark will be released here: https://github.com/haomo-ai/Cam4DOcc.</description><author>Junyi Ma, Xieyuanli Chen, Jiawei Huang, Jingyi Xu, Zhen Luo, Jintao Xu, Weihao Gu, Rui Ai, Hesheng Wang</author><pubDate>Thu, 07 Dec 2023 12:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17663v2</guid></item><item><title>visClust: A visual clustering algorithm based on orthogonal projections</title><link>http://arxiv.org/abs/2211.03894v3</link><description>We present a novel clustering algorithm, visClust, that is based on lowerdimensional data representations and visual interpretation. Thereto, we designa transformation that allows the data to be represented by a binary integerarray enabling the use of image processing methods to select a partition.Qualitative and quantitative analyses measured in accuracy and an adjustedRand-Index show that the algorithm performs well while requiring low runtimeand RAM. We compare the results to 6 state-of-the-art algorithms with availablecode, confirming the quality of visClust by superior performance in mostexperiments. Moreover, the algorithm asks for just one obligatory inputparameter while allowing optimization via optional parameters. The code is madeavailable on GitHub and straightforward to use.</description><author>Anna Breger, Clemens Karner, Martin Ehler</author><pubDate>Thu, 07 Dec 2023 12:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03894v3</guid></item><item><title>Efficient LLM Inference on CPUs</title><link>http://arxiv.org/abs/2311.00502v2</link><description>Large language models (LLMs) have demonstrated remarkable performance andtremendous potential across a wide range of tasks. However, deploying thesemodels has been challenging due to the astronomical amount of model parameters,which requires a demand for large memory capacity and high memory bandwidth. Inthis paper, we propose an effective approach that can make the deployment ofLLMs more efficiently. We support an automatic INT4 weight-only quantizationflow and design a special LLM runtime with highly-optimized kernels toaccelerate the LLM inference on CPUs. We demonstrate the general applicabilityof our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcasethe extreme inference efficiency on CPUs. The code is publicly available at:https://github.com/intel/intel-extension-for-transformers.</description><author>Haihao Shen, Hanwen Chang, Bo Dong, Yu Luo, Hengyu Meng</author><pubDate>Thu, 07 Dec 2023 12:16:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00502v2</guid></item><item><title>Extending Answer Set Programming with Rational Numbers</title><link>http://arxiv.org/abs/2312.04249v1</link><description>Answer Set Programming (ASP) is a widely used declarative programmingparadigm that has shown great potential in solving complex computationalproblems. However, the inability to natively support non-integer arithmetic hasbeen highlighted as a major drawback in real-world applications. This featureis crucial to accurately model and manage real-world data and information asemerged in various contexts, such as the smooth movement of video gamecharacters, the 3D movement of mechanical arms, and data streamed by sensors.Nevertheless, extending ASP in this direction, without affecting itsdeclarative nature and its well-defined semantics, poses non-trivialchallenges; thus, no ASP system is able to reason natively with non-integerdomains. Indeed, the widespread floating-point arithmetic is not applicable tothe ASP case, as the reproducibility of results cannot be guaranteed and thesemantics of an ASP program would not be uniquely and declaratively determined,regardless of the employed machine or solver. To overcome such limitations andin the realm of pure ASP, this paper proposes an extension of ASP in whichnon-integers are approximated to rational numbers, fully grantingreproducibility and declarativity. We provide a well-defined semantics for theASP-Core-2 standard extended with rational numbers and an implementationthereof. We hope this work could serve as a stepping stone towards a moreexpressive and versatile ASP language that can handle a broader range ofreal-world problems.</description><author>Francesco Pacenza, Jessica Zangari</author><pubDate>Thu, 07 Dec 2023 12:11:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04249v1</guid></item><item><title>TeMO: Towards Text-Driven 3D Stylization for Multi-Object Meshes</title><link>http://arxiv.org/abs/2312.04248v1</link><description>Recent progress in the text-driven 3D stylization of a single object has beenconsiderably promoted by CLIP-based methods. However, the stylization ofmulti-object 3D scenes is still impeded in that the image-text pairs used forpre-training CLIP mostly consist of an object. Meanwhile, the local details ofmultiple objects may be susceptible to omission due to the existing supervisionmanner primarily relying on coarse-grained contrast of image-text pairs. Toovercome these challenges, we present a novel framework, dubbed TeMO, to parsemulti-object 3D scenes and edit their styles under the contrast supervision atmultiple levels. We first propose a Decoupled Graph Attention (DGA) module todistinguishably reinforce the features of 3D surface points. Particularly, across-modal graph is constructed to align the object points accurately and nounphrases decoupled from the 3D mesh and textual description. Then, we develop aCross-Grained Contrast (CGC) supervision system, where a fine-grained lossbetween the words in the textual description and the randomly rendered imagesare constructed to complement the coarse-grained loss. Extensive experimentsshow that our method can synthesize high-quality stylized content andoutperform the existing methods over a wide range of multi-object 3D meshes.Our code and results will be made publicly available</description><author>Xuying Zhang, Bo-Wen Yin, Yuming Chen, Zheng Lin, Yunheng Li, Qibin Hou, Ming-Ming Cheng</author><pubDate>Thu, 07 Dec 2023 12:10:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04248v1</guid></item><item><title>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</title><link>http://arxiv.org/abs/2310.08528v2</link><description>Representing and rendering dynamic scenes has been an important butchallenging task. Especially, to accurately model complex motions, highefficiency is usually hard to guarantee. To achieve real-time dynamic scenerendering while also enjoying high training and storage efficiency, we propose4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenesrather than applying 3D-GS for each individual frame. In 4D-GS, a novelexplicit representation containing both 3D Gaussians and 4D neural voxels isproposed. A decomposed neural voxel encoding algorithm inspired by HexPlane isproposed to efficiently build Gaussian features from 4D neural voxels and thena lightweight MLP is applied to predict Gaussian deformations at noveltimestamps. Our 4D-GS method achieves real-time rendering under highresolutions, 82 FPS at an 800$\times$800 resolution on an RTX 3090 GPU whilemaintaining comparable or better quality than previous state-of-the-artmethods. More demos and code are available athttps://guanjunwu.github.io/4dgs/.</description><author>Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang</author><pubDate>Thu, 07 Dec 2023 12:03:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08528v2</guid></item><item><title>Mastering Complex Coordination through Attention-based Dynamic Graph</title><link>http://arxiv.org/abs/2312.04245v1</link><description>The coordination between agents in multi-agent systems has become a populartopic in many fields. To catch the inner relationship between agents, the graphstructure is combined with existing methods and improves the results. But inlarge-scale tasks with numerous agents, an overly complex graph would lead to aboost in computational cost and a decline in performance. Here we presentDAGMIX, a novel graph-based value factorization method. Instead of a completegraph, DAGMIX generates a dynamic graph at each time step during training, onwhich it realizes a more interpretable and effective combining process throughthe attention mechanism. Experiments show that DAGMIX significantly outperformsprevious SOTA methods in large-scale scenarios, as well as achieving promisingresults on other tasks.</description><author>Guangchong Zhou, Zhiwei Xu, Zeren Zhang, Guoliang Fan</author><pubDate>Thu, 07 Dec 2023 12:02:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04245v1</guid></item><item><title>Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?</title><link>http://arxiv.org/abs/2307.14023v2</link><description>Existing analyses of the expressive capacity of Transformer models haverequired excessively deep layers for data memorization, leading to adiscrepancy with the Transformers actually used in practice. This is primarilydue to the interpretation of the softmax function as an approximation of thehardmax function. By clarifying the connection between the softmax function andthe Boltzmann operator, we prove that a single layer of self-attention withlow-rank weight matrices possesses the capability to perfectly capture thecontext of an entire input sequence. As a consequence, we show that one-layerand single-head Transformers have a memorization capacity for finite samples,and that Transformers consisting of one self-attention layer with twofeed-forward neural networks are universal approximators for continuouspermutation equivariant functions on a compact domain.</description><author>Tokio Kajitsuka, Issei Sato</author><pubDate>Thu, 07 Dec 2023 11:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14023v2</guid></item><item><title>Detecting and Restoring Non-Standard Hands in Stable Diffusion Generated Images</title><link>http://arxiv.org/abs/2312.04236v1</link><description>We introduce a pipeline to address anatomical inaccuracies in StableDiffusion generated hand images. The initial step involves constructing aspecialized dataset, focusing on hand anomalies, to train our modelseffectively. A finetuned detection model is pivotal for precise identificationof these anomalies, ensuring targeted correction. Body pose estimation aids inunderstanding hand orientation and positioning, crucial for accurate anomalycorrection. The integration of ControlNet and InstructPix2Pix facilitatessophisticated inpainting and pixel-level transformation, respectively. Thisdual approach allows for high-fidelity image adjustments. This comprehensiveapproach ensures the generation of images with anatomically accurate hands,closely resembling real-world appearances. Our experimental results demonstratethe pipeline's efficacy in enhancing hand image realism in Stable Diffusionoutputs. We provide an online demo at https://fixhand.yiqun.io</description><author>Yiqun Zhang, Zhenyue Qin, Yang Liu, Dylan Campbell</author><pubDate>Thu, 07 Dec 2023 11:41:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04236v1</guid></item><item><title>Graph Convolutions Enrich the Self-Attention in Transformers!</title><link>http://arxiv.org/abs/2312.04234v1</link><description>Transformers, renowned for their self-attention mechanism, have achievedstate-of-the-art performance across various tasks in natural languageprocessing, computer vision, time-series modeling, etc. However, one of thechallenges with deep Transformer models is the oversmoothing problem, whererepresentations across layers converge to indistinguishable values, leading tosignificant performance degradation. We interpret the original self-attentionas a simple graph filter and redesign it from a graph signal processing (GSP)perspective. We propose graph-filter-based self-attention (GFSA) to learn ageneral yet effective one, whose complexity, however, is slightly larger thanthat of the original self-attention mechanism. We demonstrate that GFSAimproves the performance of Transformers in various fields, including computervision, natural language processing, graph pattern classification, speechrecognition, and code classification.</description><author>Jeongwhan Choi, Hyowon Wi, Jayoung Kim, Yehjin Shin, Kookjin Lee, Nathaniel Trask, Noseong Park</author><pubDate>Thu, 07 Dec 2023 11:40:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04234v1</guid></item><item><title>Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations</title><link>http://arxiv.org/abs/2307.13423v3</link><description>Self-supervised speech representations (SSSRs) have been successfully appliedto a number of speech-processing tasks, e.g. as feature extractor for speechquality (SQ) prediction, which is, in turn, relevant for assessment andtraining speech enhancement systems for users with normal or impaired hearing.However, exact knowledge of why and how quality-related information is encodedwell in such representations remains poorly understood. In this work,techniques for non-intrusive prediction of SQ ratings are extended to theprediction of intelligibility for hearing-impaired users. It is found thatself-supervised representations are useful as input features to non-intrusiveprediction models, achieving competitive performance to more complex systems. Adetailed analysis of the performance depending on Clarity Prediction Challenge1 listeners and enhancement systems indicates that more data might be needed toallow generalisation to unknown systems and (hearing-impaired) individuals</description><author>George Close, Thomas Hain, Stefan Goetze</author><pubDate>Thu, 07 Dec 2023 11:39:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13423v3</guid></item><item><title>Fine-tune vision foundation model for crack segmentation in civil infrastructures</title><link>http://arxiv.org/abs/2312.04233v1</link><description>Large-scale foundation models have become the mainstream method in the fieldof deep learning, while in civil engineering, the scale of AI models isstrictly limited. In this work, vision foundation model is introduced for cracksegmentation. Two Parameter-efficient fine-tuning methods, adapter and low-rankadaptation, are adopted to fine-tune the foundation model in the field ofsemantic segmentation: Segment Anything Model (SAM). The fine-tuned modelCrackSAM is much larger than all the existing crack segmentation models, butshows excellent performance. To test the zero-shot performance of the proposedmethod, two unique datasets related to road and exterior wall cracks arecollected, annotated and open-sourced, in total 810 images. Comparativeexperiments are conducted with twelve mature semantic segmentation models. Ondatasets with artificial noise and previously unseen datasets, the performanceof CrackSAM far exceeds that of all state-of-the-art models. CrackSAM exhibitsremarkable superiority, particularly in challenging conditions such as dimlighting, shadows, road markings, construction joints, and other interferencefactors. Such cross-scenario results demonstrate the outstanding zero-shotcapability of foundation models, and provide new ideas for the development ofvision models in civil engineering.</description><author>Kang Ge, Chen Wang, Yutao Guo</author><pubDate>Thu, 07 Dec 2023 11:39:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04233v1</guid></item><item><title>Adventures of Trustworthy Vision-Language Models: A Survey</title><link>http://arxiv.org/abs/2312.04231v1</link><description>Recently, transformers have become incredibly popular in computer vision andvision-language tasks. This notable rise in their usage can be primarilyattributed to the capabilities offered by attention mechanisms and theoutstanding ability of transformers to adapt and apply themselves to a varietyof tasks and domains. Their versatility and state-of-the-art performance haveestablished them as indispensable tools for a wide array of applications.However, in the constantly changing landscape of machine learning, theassurance of the trustworthiness of transformers holds utmost importance. Thispaper conducts a thorough examination of vision-language transformers,employing three fundamental principles of responsible AI: Bias, Robustness, andInterpretability. The primary objective of this paper is to delve into theintricacies and complexities associated with the practical use of transformers,with the overarching goal of advancing our comprehension of how to enhancetheir reliability and accountability.</description><author>Mayank Vatsa, Anubhooti Jain, Richa Singh</author><pubDate>Thu, 07 Dec 2023 11:31:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04231v1</guid></item><item><title>Bayesian Methods for Media Mix Modelling with shape and funnel effects</title><link>http://arxiv.org/abs/2311.05587v3</link><description>In recent years, significant progress in generative AI has highlighted theimportant role of physics-inspired models that utilize advanced mathematicalconcepts based on fundamental physics principles to enhance artificialintelligence capabilities. Among these models, those based on diffusionequations have greatly improved image quality. This study aims to explore thepotential uses of Maxwell-Boltzmann equation, which forms the basis of thekinetic theory of gases, and the Michaelis-Menten model in Marketing MixModelling (MMM) applications. We propose incorporating these equations intoHierarchical Bayesian models to analyse consumer behaviour in the context ofadvertising. These equation sets excel in accurately describing the randomdynamics in complex systems like social interactions and consumer-advertisinginteractions.</description><author>Javier Marin</author><pubDate>Thu, 07 Dec 2023 11:25:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05587v3</guid></item><item><title>Similarity of Neural Architectures using Adversarial Attack Transferability</title><link>http://arxiv.org/abs/2210.11407v3</link><description>In recent years, many deep neural architectures have been developed for imageclassification. Whether they are similar or dissimilar and what factorscontribute to their (dis)similarities remains curious. To address thisquestion, we aim to design a quantitative and scalable similarity measurebetween neural architectures. We propose Similarity by Attack Transferability(SAT) from the observation that adversarial attack transferability containsinformation related to input gradients and decision boundaries widely used tounderstand model behaviors. We conduct a large-scale analysis on 69state-of-the-art ImageNet classifiers using our proposed similarity function toanswer the question. Moreover, we observe neural architecture-related phenomenausing model similarity that model diversity can lead to better performance onmodel ensembles and knowledge distillation under specific conditions. Ourresults provide insights into why developing diverse neural architectures withdistinct components is necessary.</description><author>Jaehui Hwang, Dongyoon Han, Byeongho Heo, Song Park, Sanghyuk Chun, Jong-Seok Lee</author><pubDate>Thu, 07 Dec 2023 11:24:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.11407v3</guid></item><item><title>Internal-Coordinate Density Modelling of Protein Structure: Covariance Matters</title><link>http://arxiv.org/abs/2302.13711v2</link><description>After the recent ground-breaking advances in protein structure prediction,one of the remaining challenges in protein machine learning is to reliablypredict distributions of structural states. Parametric models of fluctuationsare difficult to fit due to complex covariance structures between degrees offreedom in the protein chain, often causing models to either violate local orglobal structural constraints. In this paper, we present a new strategy formodelling protein densities in internal coordinates, which uses constraints in3D space to induce covariance structure between the internal degrees offreedom. We illustrate the potential of the procedure by constructing avariational autoencoder with full covariance output induced by the constraintsimplied by the conditional mean in 3D, and demonstrate that our approach makesit possible to scale density models of internal coordinates to full proteinbackbones in two settings: 1) a unimodal setting for proteins exhibiting smallfluctuations and limited amounts of available data, and 2) a multimodal settingfor larger conformational changes in a high data regime.</description><author>Marloes Arts, Jes Frellsen, Wouter Boomsma</author><pubDate>Thu, 07 Dec 2023 11:23:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13711v2</guid></item><item><title>Classical Verification of Quantum Learning</title><link>http://arxiv.org/abs/2306.04843v2</link><description>Quantum data access and quantum processing can make certain classicallyintractable learning tasks feasible. However, quantum capabilities will only beavailable to a select few in the near future. Thus, reliable schemes that allowclassical clients to delegate learning to untrusted quantum servers arerequired to facilitate widespread access to quantum learning advantages.Building on a recently introduced framework of interactive proof systems forclassical machine learning, we develop a framework for classical verificationof quantum learning. We exhibit learning problems that a classical learnercannot efficiently solve on their own, but that they can efficiently andreliably solve when interacting with an untrusted quantum prover. Concretely,we consider the problems of agnostic learning parities and Fourier-sparsefunctions with respect to distributions with uniform input marginal. We proposea new quantum data access model that we call "mixture-of-superpositions"quantum examples, based on which we give efficient quantum learning algorithmsfor these tasks. Moreover, we prove that agnostic quantum parity andFourier-sparse learning can be efficiently verified by a classical verifierwith only random example or statistical query access. Finally, we showcase twogeneral scenarios in learning and verification in which quantummixture-of-superpositions examples do not lead to sample complexityimprovements over classical data. Our results demonstrate that the potentialpower of quantum data for learning tasks, while not unlimited, can be utilizedby classical agents through interaction with untrusted quantum entities.</description><author>Matthias C. Caro, Marcel Hinsche, Marios Ioannou, Alexander Nietner, Ryan Sweke</author><pubDate>Thu, 07 Dec 2023 11:19:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04843v2</guid></item><item><title>Dynamic Data-Driven Digital Twins for Blockchain Systems</title><link>http://arxiv.org/abs/2312.04226v1</link><description>In recent years, we have seen an increase in the adoption of blockchain-basedsystems in non-financial applications, looking to benefit from what thetechnology has to offer. Although many fields have managed to includeblockchain in their core functionalities, the adoption of blockchain, ingeneral, is constrained by the so-called trilemma trade-off betweendecentralization, scalability, and security. In our previous work, we haveshown that using a digital twin for dynamically managing blockchain systemsduring runtime can be effective in managing the trilemma trade-off. Our DigitalTwin leverages DDDAS feedback loop, which is responsible for getting the datafrom the system to the digital twin, conducting optimisation, and updating thephysical system. This paper examines how leveraging DDDAS feedback loop cansupport the optimisation component of the trilemma benefiting fromReinforcement Learning agents and a simulation component to augment the qualityof the learned model while reducing the computational overhead required fordecision-making.</description><author>Georgios Diamantopoulos, Nikos Tziritas, Rami Bahsoon, Georgios Theodoropoulos</author><pubDate>Thu, 07 Dec 2023 11:18:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04226v1</guid></item><item><title>TLCE: Transfer-Learning Based Classifier Ensembles for Few-Shot Class-Incremental Learning</title><link>http://arxiv.org/abs/2312.04225v1</link><description>Few-shot class-incremental learning (FSCIL) struggles to incrementallyrecognize novel classes from few examples without catastrophic forgetting ofold classes or overfitting to new classes. We propose TLCE, which ensemblesmultiple pre-trained models to improve separation of novel and old classes.TLCE minimizes interference between old and new classes by mapping old classimages to quasi-orthogonal prototypes using episodic training. It thenensembles diverse pre-trained models to better adapt to novel classes despitedata imbalance. Extensive experiments on various datasets demonstrate that ourtransfer learning ensemble approach outperforms state-of-the-art FSCIL methods.</description><author>Shuangmei Wang, Yang Cao, Tieru Wu</author><pubDate>Thu, 07 Dec 2023 11:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04225v1</guid></item><item><title>Optimizing K-means for Big Data: A Comparative Study</title><link>http://arxiv.org/abs/2310.09819v2</link><description>This paper presents a comparative analysis of different optimizationtechniques for the K-means algorithm in the context of big data. K-means is awidely used clustering algorithm, but it can suffer from scalability issueswhen dealing with large datasets. The paper explores different approaches toovercome these issues, including parallelization, approximation, and samplingmethods. The authors evaluate the performance of these techniques on variousbenchmark datasets and compare them in terms of speed, quality of clustering,and scalability according to the LIMA dominance criterion. The results showthat different techniques are more suitable for different types of datasets andprovide insights into the trade-offs between speed and accuracy in K-meansclustering for big data. Overall, the paper offers a comprehensive guide forpractitioners and researchers on how to optimize K-means for big dataapplications.</description><author>Ravil Mussabayev, Rustam Mussabayev</author><pubDate>Thu, 07 Dec 2023 11:11:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09819v2</guid></item><item><title>Swap distance minimization in SOV languages. Cognitive and mathematical foundations</title><link>http://arxiv.org/abs/2312.04219v1</link><description>Distance minimization is a general principle of language. A special case ofthis principle in the domain of word order is swap distance minimization. Thisprinciple predicts that variations from a canonical order that are reached byfewer swaps of adjacent constituents are lest costly and thus more likely. Herewe investigate the principle in the context of the triple formed by subject(S), object (O) and verb (V). We introduce the concept of word order rotationas a cognitive underpinning of that prediction. When the canonical order of alanguage is SOV, the principle predicts SOV &lt; SVO, OSV &lt; VSO, OVS &lt; VOS, inorder of increasing cognitive cost. We test the prediction in three flexibleorder SOV languages: Korean (Koreanic), Malayalam (Dravidian), and Sinhalese(Indo-European). Evidence of swap distance minimization is found in all threelanguages, but it is weaker in Sinhalese. Swap distance minimization isstronger than a preference for the canonical order in Korean and especiallyMalayalam.</description><author>Ramon Ferrer-i-Cancho, Savithry Namboodiripad</author><pubDate>Thu, 07 Dec 2023 11:10:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04219v1</guid></item><item><title>CODEX: A Cluster-Based Method for Explainable Reinforcement Learning</title><link>http://arxiv.org/abs/2312.04216v1</link><description>Despite the impressive feats demonstrated by Reinforcement Learning (RL),these algorithms have seen little adoption in high-risk, real-worldapplications due to current difficulties in explaining RL agent actions andbuilding user trust. We present Counterfactual Demonstrations for Explanation(CODEX), a method that incorporates semantic clustering, which can effectivelysummarize RL agent behavior in the state-action space. Experimentation on theMiniGrid and StarCraft II gaming environments reveals the semantic clustersretain temporal as well as entity information, which is reflected in theconstructed summary of agent behavior. Furthermore, clustering thediscrete+continuous game-state latent representations identifies the mostcrucial episodic events, demonstrating a relationship between the latent andsemantic spaces. This work contributes to the growing body of work that strivesto unlock the power of RL for widespread use by leveraging and extendingtechniques from Natural Language Processing.</description><author>Timothy K. Mathes, Jessica Inman, Andr√©s Col√≥n, Simon Khan</author><pubDate>Thu, 07 Dec 2023 11:04:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04216v1</guid></item><item><title>Guided Reconstruction with Conditioned Diffusion Models for Unsupervised Anomaly Detection in Brain MRIs</title><link>http://arxiv.org/abs/2312.04215v1</link><description>Unsupervised anomaly detection in Brain MRIs aims to identify abnormalitiesas outliers from a healthy training distribution. Reconstruction-basedapproaches that use generative models to learn to reconstruct healthy brainanatomy are commonly used for this task. Diffusion models are an emerging classof deep generative models that show great potential regarding reconstructionfidelity. However, they face challenges in preserving intensity characteristicsin the reconstructed images, limiting their performance in anomaly detection.To address this challenge, we propose to condition the denoising mechanism ofdiffusion models with additional information about the image to reconstructcoming from a latent representation of the noise-free input image. Thisconditioning enables high-fidelity reconstruction of healthy brain structureswhile aligning local intensity characteristics of input-reconstruction pairs.We evaluate our method's reconstruction quality, domain adaptation features andfinally segmentation performance on publicly available data sets with variouspathologies. Using our proposed conditioning mechanism we can reduce thefalse-positive predictions and enable a more precise delineation of anomalieswhich significantly enhances the anomaly detection performance compared toestablished state-of-the-art approaches to unsupervised anomaly detection inbrain MRI. Furthermore, our approach shows promise in domain adaptation acrossdifferent MRI acquisitions and simulated contrasts, a crucial property ofgeneral anomaly detection methods.</description><author>Finn Behrendt, Debayan Bhattacharya, Robin Mieling, Lennart Maack, Julia Kr√ºger, Roland Opfer, Alexander Schlaefer</author><pubDate>Thu, 07 Dec 2023 11:03:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04215v1</guid></item><item><title>Universal Segmentation at Arbitrary Granularity with Language Instruction</title><link>http://arxiv.org/abs/2312.01623v3</link><description>This paper aims to achieve universal segmentation of arbitrary semanticlevel. Despite significant progress in recent years, specialist segmentationapproaches are limited to specific tasks and data distribution. Retraining anew model for adaptation to new scenarios or settings takes expensivecomputation and time cost, which raises the demand for versatile and universalsegmentation model that can cater to various granularity. Although someattempts have been made for unifying different segmentation tasks orgeneralization to various scenarios, limitations in the definition of paradigmsand input-output spaces make it difficult for them to achieve accurateunderstanding of content at arbitrary granularity. To this end, we presentUniLSeg, a universal segmentation model that can perform segmentation at anysemantic level with the guidance of language instructions. For trainingUniLSeg, we reorganize a group of tasks from original diverse distributionsinto a unified data format, where images with texts describing segmentationtargets as input and corresponding masks are output. Combined with a automaticannotation engine for utilizing numerous unlabeled data, UniLSeg achievesexcellent performance on various tasks and settings, surpassing both specialistand unified segmentation models.</description><author>Yong Liu, Cairong Zhang, Yitong Wang, Jiahao Wang, Yujiu Yang, Yansong Tang</author><pubDate>Thu, 07 Dec 2023 10:55:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01623v3</guid></item><item><title>Constraint Model for the Satellite Image Mosaic Selection Problem</title><link>http://arxiv.org/abs/2312.04210v1</link><description>Satellite imagery solutions are widely used to study and monitor differentregions of the Earth. However, a single satellite image can cover only alimited area. In cases where a larger area of interest is studied, severalimages must be stitched together to create a single larger image, called amosaic, that can cover the area. Today, with the increasing number of satelliteimages available for commercial use, selecting the images to build the mosaicis challenging, especially when the user wants to optimize one or moreparameters, such as the total cost and the cloud coverage percentage in themosaic. More precisely, for this problem the input is an area of interest,several satellite images intersecting the area, a list of requirements relativeto the image and the mosaic, such as cloud coverage percentage, imageresolution, and a list of objectives to optimize. We contribute to theconstraint and mixed integer lineal programming formulation of this newproblem, which we call the \textit{satellite image mosaic selection problem},which is a multi-objective extension of the polygon cover problem. We propose adataset of realistic and challenging instances, where the images were capturedby the satellite constellations SPOT, Pl\'eiades and Pl\'eiades Neo. Weevaluate and compare the two proposed models and show their efficiency forlarge instances, up to 200 images.</description><author>Manuel Combarro Sim√≥n, Pierre Talbot, Gr√©goire Danoy, Jedrzej Musial, Mohammed Alswaitti, Pascal Bouvry</author><pubDate>Thu, 07 Dec 2023 10:52:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04210v1</guid></item><item><title>Constrained Hierarchical Clustering via Graph Coarsening and Optimal Cuts</title><link>http://arxiv.org/abs/2312.04209v1</link><description>Motivated by extracting and summarizing relevant information in shortsentence settings, such as satisfaction questionnaires, hotel reviews, andX/Twitter, we study the problem of clustering words in a hierarchical fashion.In particular, we focus on the problem of clustering with horizontal andvertical structural constraints. Horizontal constraints are typicallycannot-link and must-link among words, while vertical constraints areprecedence constraints among cluster levels. We overcome state-of-the-artbottlenecks by formulating the problem in two steps: first, as asoft-constrained regularized least-squares which guides the result of asequential graph coarsening algorithm towards the horizontal feasible set.Then, flat clusters are extracted from the resulting hierarchical tree bycomputing optimal cut heights based on the available constraints. We show thatthe resulting approach compares very well with respect to existing algorithmsand is computationally light.</description><author>Eliabelle Mauduit, Andrea Simonetto</author><pubDate>Thu, 07 Dec 2023 10:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04209v1</guid></item><item><title>Wavelength-multiplexed Delayed Inputs for Memory Enhancement of Microring-based Reservoir Computing</title><link>http://arxiv.org/abs/2312.04204v1</link><description>We numerically demonstrate a silicon add-drop microring-based reservoircomputing scheme that combines parallel delayed inputs and wavelength divisionmultiplexing. The scheme solves memory-demanding tasks like time-seriesprediction with good performance without requiring external optical feedback.</description><author>Bernard J. Giron Castro, Christophe Peucheret, Francesco Da Ros</author><pubDate>Thu, 07 Dec 2023 10:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04204v1</guid></item><item><title>SILC: Improving Vision Language Pretraining with Self-Distillation</title><link>http://arxiv.org/abs/2310.13355v2</link><description>Image-Text pretraining on web-scale image caption datasets has become thedefault recipe for open vocabulary classification and retrieval models thanksto the success of CLIP and its variants. Several works have also used CLIPfeatures for dense prediction tasks and have shown the emergence of open-setabilities. However, the contrastive objective used by these models only focuseson image-text alignment and does not incentivise image feature learning fordense prediction tasks. In this work, we introduce SILC, a novel framework forvision language pretraining. SILC improves image-text contrastive learning withthe simple addition of local-to-global correspondence learning byself-distillation. We show that distilling local image features from anexponential moving average (EMA) teacher model significantly improves modelperformance on dense predictions tasks like detection and segmentation, whilealso providing improvements on image-level tasks such as classification andretrieval. SILC models sets a new state of the art for zero-shotclassification, few shot classification, image and text retrieval, zero-shotsegmentation, and open vocabulary segmentation. We further show that SILCfeatures greatly benefit open vocabulary detection, captioning and visualquestion answering.</description><author>Muhammad Ferjad Naeem, Yongqin Xian, Xiaohua Zhai, Lukas Hoyer, Luc Van Gool, Federico Tombari</author><pubDate>Thu, 07 Dec 2023 10:39:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13355v2</guid></item><item><title>Towards Sobolev Pruning</title><link>http://arxiv.org/abs/2312.03510v2</link><description>The increasing use of stochastic models for describing complex phenomenawarrants surrogate models that capture the reference model characteristics at afraction of the computational cost, foregoing potentially expensive Monte Carlosimulation. The predominant approach of fitting a large neural network and thenpruning it to a reduced size has commonly neglected shortcomings. The producedsurrogate models often will not capture the sensitivities and uncertaintiesinherent in the original model. In particular, (higher-order) derivativeinformation of such surrogates could differ drastically. Given a large enoughnetwork, we expect this derivative information to match. However, the prunedmodel will almost certainly not share this behavior. In this paper, we propose to find surrogate models by using sensitivityinformation throughout the learning and pruning process. We build on work usingInterval Adjoint Significance Analysis for pruning and combine it with therecent advancements in Sobolev Training to accurately model the originalsensitivity information in the pruned neural network based surrogate model. Weexperimentally underpin the method on an example of pricing a multidimensionalBasket option modelled through a stochastic differential equation with Brownianmotion. The proposed method is, however, not limited to the domain ofquantitative finance, which was chosen as a case study for intuitiveinterpretations of the sensitivities. It serves as a foundation for buildingfurther surrogate modelling techniques considering sensitivity information.</description><author>Neil Kichler, Sher Afghan, Uwe Naumann</author><pubDate>Thu, 07 Dec 2023 10:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03510v2</guid></item><item><title>XAI-TRIS: Non-linear image benchmarks to quantify false positive post-hoc attribution of feature importance</title><link>http://arxiv.org/abs/2306.12816v2</link><description>The field of 'explainable' artificial intelligence (XAI) has produced highlycited methods that seek to make the decisions of complex machine learning (ML)methods 'understandable' to humans, for example by attributing 'importance'scores to input features. Yet, a lack of formal underpinning leaves it unclearas to what conclusions can safely be drawn from the results of a given XAImethod and has also so far hindered the theoretical verification and empiricalvalidation of XAI methods. This means that challenging non-linear problems,typically solved by deep neural networks, presently lack appropriate remedies.Here, we craft benchmark datasets for three different non-linear classificationscenarios, in which the important class-conditional features are known bydesign, serving as ground truth explanations. Using novel quantitative metrics,we benchmark the explanation performance of a wide set of XAI methods acrossthree deep learning model architectures. We show that popular XAI methods areoften unable to significantly outperform random performance baselines and edgedetection methods. Moreover, we demonstrate that explanations derived fromdifferent model architectures can be vastly different; thus, prone tomisinterpretation even under controlled conditions.</description><author>Benedict Clark, Rick Wilming, Stefan Haufe</author><pubDate>Thu, 07 Dec 2023 10:37:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12816v2</guid></item><item><title>SAMBA: A Trainable Segmentation Web-App with Smart Labelling</title><link>http://arxiv.org/abs/2312.04197v1</link><description>Segmentation is the assigning of a semantic class to every pixel in an imageand is a prerequisite for various statistical analysis tasks in materialsscience, like phase quantification, physics simulations or morphologicalcharacterization. The wide range of length scales, imaging techniques andmaterials studied in materials science means any segmentation algorithm mustgeneralise to unseen data and support abstract, user-defined semantic classes.Trainable segmentation is a popular interactive segmentation paradigm where aclassifier is trained to map from image features to user drawn labels. SAMBA isa trainable segmentation tool that uses Meta's Segment Anything Model (SAM) forfast, high-quality label suggestions and a random forest classifier for robust,generalizable segmentations. It is accessible in the browser(https://www.sambasegment.com/) without the need to download any externaldependencies. The segmentation backend is run in the cloud, so does not requirethe user to have powerful hardware.</description><author>Ronan Docherty, Isaac Squires, Antonis Vamvakeros, Samuel J. Cooper</author><pubDate>Thu, 07 Dec 2023 10:31:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04197v1</guid></item><item><title>Language Model Knowledge Distillation for Efficient Question Answering in Spanish</title><link>http://arxiv.org/abs/2312.04193v1</link><description>Recent advances in the development of pre-trained Spanish language models hasled to significant progress in many Natural Language Processing (NLP) tasks,such as question answering. However, the lack of efficient models imposes abarrier for the adoption of such models in resource-constrained environments.Therefore, smaller distilled models for the Spanish language could be proven tobe highly scalable and facilitate their further adoption on a variety of tasksand scenarios. In this work, we take one step in this direction by developingSpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficientquestion answering in Spanish. To achieve this, we employ knowledgedistillation from a large model onto a lighter model that allows for a widerimplementation, even in areas with limited computational resources, whilstattaining negligible performance sacrifice. Our experiments show that the densedistilled model can still preserve the performance of its larger counterpart,while significantly increasing inference speedup. This work serves as astarting point for further research and investigation of model compressionefforts for Spanish language models across various NLP tasks.</description><author>Adri√°n Bazaga, Pietro Li√≤, Gos Micklem</author><pubDate>Thu, 07 Dec 2023 10:21:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04193v1</guid></item><item><title>PROMISE: A Framework for Model-Driven Stateful Prompt Orchestration</title><link>http://arxiv.org/abs/2312.03699v2</link><description>The advent of increasingly powerful language models has raised expectationsfor language-based interactions. However, controlling these models is achallenge, emphasizing the need to be able to investigate the feasibility andvalue of their application. We present PROMISE, a framework that facilitatesthe development of complex language-based interactions with informationsystems. Its use of state machine modeling concepts enables model-driven,dynamic prompt orchestration across hierarchically nested states andtransitions. This improves the control of the behavior of language models andthus enables their effective and efficient use. We show the benefits of PROMISEin the context of application scenarios within health information systems anddemonstrate its ability to handle complex interactions.</description><author>Wenyuan Wu, Jasmin Heierli, Max Meisterhans, Adrian Moser, Andri F√§rber, Mateusz Dolata, Elena Gavagnin, Alexandre de Spindler, Gerhard Schwabe</author><pubDate>Thu, 07 Dec 2023 10:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03699v2</guid></item><item><title>Joint-Individual Fusion Structure with Fusion Attention Module for Multi-Modal Skin Cancer Classification</title><link>http://arxiv.org/abs/2312.04189v1</link><description>Most convolutional neural network (CNN) based methods for skin cancerclassification obtain their results using only dermatological images. Althoughgood classification results have been shown, more accurate results can beachieved by considering the patient's metadata, which is valuable clinicalinformation for dermatologists. Current methods only use the simple jointfusion structure (FS) and fusion modules (FMs) for the multi-modalclassification methods, there still is room to increase the accuracy byexploring more advanced FS and FM. Therefore, in this paper, we design a newfusion method that combines dermatological images (dermoscopy images orclinical images) and patient metadata for skin cancer classification from theperspectives of FS and FM. First, we propose a joint-individual fusion (JIF)structure that learns the shared features of multi-modality data and preservesspecific features simultaneously. Second, we introduce a fusion attention (FA)module that enhances the most relevant image and metadata features based onboth the self and mutual attention mechanism to support the decision-makingpipeline. We compare the proposed JIF-MMFA method with other state-of-the-artfusion methods on three different public datasets. The results show that ourJIF-MMFA method improves the classification results for all tested CNNbackbones and performs better than the other fusion methods on the three publicdatasets, demonstrating our method's effectiveness and robustness</description><author>Peng Tang, Xintong Yan, Yang Nan, Xiaobin Hu, Xiaobin Hu, Bjoern H Menzee. Sebastian Krammer, Tobias Lasser</author><pubDate>Thu, 07 Dec 2023 10:16:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04189v1</guid></item><item><title>Exploring the Interactive Guidance for Unified and Effective Image Matting</title><link>http://arxiv.org/abs/2205.08324v3</link><description>Recent image matting studies are developing towards proposing trimap-free orinteractive methods for complete complex image matting tasks. Although avoidingthe extensive labors of trimap annotation, existing methods still suffer fromtwo limitations: (1) For the single image with multiple objects, it isessential to provide extra interaction information to help determining thematting target; (2) For transparent objects, the accurate regression of alphamatte from RGB image is much more difficult compared with the opaque ones. Inthis work, we propose a Unified Interactive image Matting method, named UIM,which solves the limitations and achieves satisfying matting results for anyscenario. Specifically, UIM leverages multiple types of user interaction toavoid the ambiguity of multiple matting targets, and we compare the pros andcons of different annotation types in detail. To unify the matting performancefor transparent and opaque objects, we decouple image matting into two stages,i.e., foreground segmentation and transparency prediction. Moreover, we designa multi-scale attentive fusion module to alleviate the vagueness in theboundary region. Experimental results demonstrate that UIM achievesstate-of-the-art performance on the Composition-1K test set and a syntheticunified dataset.</description><author>Dinghao Yang, Bin Wang, Weijia Li, Yiqi Lin, Conghui He</author><pubDate>Thu, 07 Dec 2023 10:08:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.08324v3</guid></item><item><title>AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online Labor Platform</title><link>http://arxiv.org/abs/2312.04180v1</link><description>Artificial intelligence (AI) refers to the ability of machines or software tomimic or even surpass human intelligence in a given cognitive task. Whilehumans learn by both induction and deduction, the success of current AI isrooted in induction, relying on its ability to detect statistical regularitiesin task input -- an ability learnt from a vast amount of training data usingenormous computation resources. We examine the performance of such astatistical AI in a human task through the lens of four factors, including tasklearnability, statistical resource, computation resource, and learningtechniques, and then propose a three-phase visual framework to understand theevolving relation between AI and jobs. Based on this conceptual framework, wedevelop a simple economic model of competition to show the existence of aninflection point for each occupation. Before AI performance crosses theinflection point, human workers always benefit from an improvement in AIperformance, but after the inflection point, human workers become worse offwhenever such an improvement occurs. To offer empirical evidence, we firstargue that AI performance has passed the inflection point for the occupation oftranslation but not for the occupation of web development. We then study howthe launch of ChatGPT, which led to significant improvement of AI performanceon many tasks, has affected workers in these two occupations on a large onlinelabor platform. Consistent with the inflection point conjecture, we find thattranslators are negatively affected by the shock both in terms of the number ofaccepted jobs and the earnings from those jobs, while web developers arepositively affected by the very same shock. Given the potentially largedisruption of AI on employment, more studies on more occupations using datafrom different platforms are urgently needed.</description><author>Dandan Qiao, Huaxia Rui, Qian Xiong</author><pubDate>Thu, 07 Dec 2023 10:06:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04180v1</guid></item><item><title>LIPEx-Locally Interpretable Probabilistic Explanations-To Look Beyond The True Class</title><link>http://arxiv.org/abs/2310.04856v2</link><description>In this work, we instantiate a novel perturbation-based multi-classexplanation framework, LIPEx (Locally Interpretable Probabilistic Explanation).We demonstrate that LIPEx not only locally replicates the probabilitydistributions output by the widely used complex classification models but alsoprovides insight into how every feature deemed to be important affects theprediction probability for each of the possible classes. We achieve this bydefining the explanation as a matrix obtained via regression with respect tothe Hellinger distance in the space of probability distributions. Ablationtests on text and image data, show that LIPEx-guided removal of importantfeatures from the data causes more change in predictions for the underlyingmodel than similar tests based on other saliency-based or featureimportance-based Explainable AI (XAI) methods. It is also shown that comparedto LIME, LIPEx is more data efficient in terms of using a lesser number ofperturbations of the data to obtain a reliable explanation. Thisdata-efficiency is seen to manifest as LIPEx being able to compute itsexplanation matrix around 53% faster than all-class LIME, for classificationexperiments with text data.</description><author>Hongbo Zhu, Angelo Cangelosi, Procheta Sen, Anirbit Mukherjee</author><pubDate>Thu, 07 Dec 2023 10:02:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04856v2</guid></item><item><title>Temporal Shuffling for Defending Deep Action Recognition Models against Adversarial Attacks</title><link>http://arxiv.org/abs/2112.07921v2</link><description>Recently, video-based action recognition methods using convolutional neuralnetworks (CNNs) achieve remarkable recognition performance. However, there isstill lack of understanding about the generalization mechanism of actionrecognition models. In this paper, we suggest that action recognition modelsrely on the motion information less than expected, and thus they are robust torandomization of frame orders. Furthermore, we find that motion monotonicityremaining after randomization also contributes to such robustness. Based onthis observation, we develop a novel defense method using temporal shuffling ofinput videos against adversarial attacks for action recognition models. Anotherobservation enabling our defense method is that adversarial perturbations onvideos are sensitive to temporal destruction. To the best of our knowledge,this is the first attempt to design a defense method without additionaltraining for 3D CNN-based video action recognition models.</description><author>Jaehui Hwang, Huan Zhang, Jun-Ho Choi, Cho-Jui Hsieh, Jong-Seok Lee</author><pubDate>Thu, 07 Dec 2023 09:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.07921v2</guid></item><item><title>Coherent energy and force uncertainty in deep learning force fields</title><link>http://arxiv.org/abs/2312.04174v1</link><description>In machine learning energy potentials for atomic systems, forces are commonlyobtained as the negative derivative of the energy function with respect toatomic positions. To quantify aleatoric uncertainty in the predicted energies,a widely used modeling approach involves predicting both a mean and variancefor each energy value. However, this model is not differentiable under theusual white noise assumption, so energy uncertainty does not naturallytranslate to force uncertainty. In this work we propose a machine learningpotential energy model in which energy and force aleatoric uncertainty arelinked through a spatially correlated noise process. We demonstrate ourapproach on an equivariant messages passing neural network potential trained onenergies and forces on two out-of-equilibrium molecular datasets. Furthermore,we also show how to obtain epistemic uncertainties in this setting based on aBayesian interpretation of deep ensemble models.</description><author>Peter Bj√∏rn J√∏rgensen, Jonas Busk, Ole Winther, Mikkel N. Schmidt</author><pubDate>Thu, 07 Dec 2023 09:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04174v1</guid></item><item><title>A novel feature selection framework for incomplete data</title><link>http://arxiv.org/abs/2312.04171v1</link><description>Feature selection on incomplete datasets is an exceptionally challengingtask. Existing methods address this challenge by first employing imputationmethods to complete the incomplete data and then conducting feature selectionbased on the imputed data. Since imputation and feature selection are entirelyindependent steps, the importance of features cannot be considered duringimputation. However, in real-world scenarios or datasets, different featureshave varying degrees of importance. To address this, we propose a novelincomplete data feature selection framework that considers feature importance.The framework mainly consists of two alternating iterative stages: the M-stageand the W-stage. In the M-stage, missing values are imputed based on a givenfeature importance vector and multiple initial imputation results. In theW-stage, an improved reliefF algorithm is employed to learn the featureimportance vector based on the imputed data. Specifically, the featureimportance vector obtained in the current iteration of the W-stage serves asinput for the next iteration of the M-stage. Experimental results on bothartificially generated and real incomplete datasets demonstrate that theproposed method outperforms other approaches significantly.</description><author>Cong Guo</author><pubDate>Thu, 07 Dec 2023 09:45:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04171v1</guid></item><item><title>Open-vocabulary object 6D pose estimation</title><link>http://arxiv.org/abs/2312.00690v2</link><description>We introduce the new setting of open-vocabulary object 6D pose estimation, inwhich a textual prompt is used to specify the object of interest. In contrastto existing approaches, in our setting (i) the object of interest is specifiedsolely through the textual prompt, (ii) no object model (e.g. CAD or videosequence) is required at inference, (iii) the object is imaged from twodifferent viewpoints of two different scenes, and (iv) the object was notobserved during the training phase. To operate in this setting, we introduce anovel approach that leverages a Vision-Language Model to segment the object ofinterest from two distinct scenes and to estimate its relative 6D pose. The keyof our approach is a carefully devised strategy to fuse object-levelinformation provided by the prompt with local image features, resulting in afeature space that can generalize to novel concepts. We validate our approachon a new benchmark based on two popular datasets, REAL275 and Toyota-Light,which collectively encompass 39 object instances appearing in four thousandimage pairs. The results demonstrate that our approach outperforms both awell-established hand-crafted method and a recent deep learning-based baselinein estimating the relative 6D pose of objects in different scenes. Projectpage: https://jcorsetti.github.io/oryon/.</description><author>Jaime Corsetti, Davide Boscaini, Changjae Oh, Andrea Cavallaro, Fabio Poiesi</author><pubDate>Thu, 07 Dec 2023 09:41:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00690v2</guid></item><item><title>Augmentation-Free Dense Contrastive Knowledge Distillation for Efficient Semantic Segmentation</title><link>http://arxiv.org/abs/2312.04168v1</link><description>In recent years, knowledge distillation methods based on contrastive learninghave achieved promising results on image classification and object detectiontasks. However, in this line of research, we note that less attention is paidto semantic segmentation. Existing methods heavily rely on data augmentationand memory buffer, which entail high computational resource demands whenapplying them to handle semantic segmentation that requires to preservehigh-resolution feature maps for making dense pixel-wise predictions. In orderto address this problem, we present Augmentation-free Dense ContrastiveKnowledge Distillation (Af-DCD), a new contrastive distillation learningparadigm to train compact and accurate deep neural networks for semanticsegmentation applications. Af-DCD leverages a masked feature mimickingstrategy, and formulates a novel contrastive learning loss via taking advantageof tactful feature partitions across both channel and spatial dimensions,allowing to effectively transfer dense and structured local knowledge learnt bythe teacher model to a target student model while maintaining trainingefficiency. Extensive experiments on five mainstream benchmarks with variousteacher-student network pairs demonstrate the effectiveness of our approach.For instance, the DeepLabV3-Res18|DeepLabV3-MBV2 model trained by Af-DCDreaches 77.03%|76.38% mIOU on Cityscapes dataset when choosing DeepLabV3-Res101as the teacher, setting new performance records. Besides that, Af-DCD achievesan absolute mIOU improvement of 3.26%|3.04%|2.75%|2.30%|1.42% compared withindividually trained counterpart on Cityscapes|PascalVOC|Camvid|ADE20K|COCO-Stuff-164K. Code is available athttps://github.com/OSVAI/Af-DCD</description><author>Jiawei Fan, Chao Li, Xiaolong Liu, Meina Song, Anbang Yao</author><pubDate>Thu, 07 Dec 2023 09:37:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04168v1</guid></item><item><title>Mixture of Dynamical Variational Autoencoders for Multi-Source Trajectory Modeling and Separation</title><link>http://arxiv.org/abs/2312.04167v1</link><description>In this paper, we propose a latent-variable generative model called mixtureof dynamical variational autoencoders (MixDVAE) to model the dynamics of asystem composed of multiple moving sources. A DVAE model is pre-trained on asingle-source dataset to capture the source dynamics. Then, multiple instancesof the pre-trained DVAE model are integrated into a multi-source mixture modelwith a discrete observation-to-source assignment latent variable. The posteriordistributions of both the discrete observation-to-source assignment variableand the continuous DVAE variables representing the sources content/position areestimated using a variational expectation-maximization algorithm, leading tomulti-source trajectories estimation. We illustrate the versatility of theproposed MixDVAE model on two tasks: a computer vision task, namelymulti-object tracking, and an audio processing task, namely single-channelaudio source separation. Experimental results show that the proposed methodworks well on these two tasks, and outperforms several baseline methods.</description><author>Xiaoyu Lin, Laurent Girin, Xavier Alameda-Pineda</author><pubDate>Thu, 07 Dec 2023 09:36:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04167v1</guid></item><item><title>Improving Communication Efficiency of Federated Distillation via Accumulating Local Updates</title><link>http://arxiv.org/abs/2312.04166v1</link><description>As an emerging federated learning paradigm, federated distillation enablescommunication-efficient model training by transmitting only small-scaleknowledge during the learning process. To further improve the communicationefficiency of federated distillation, we propose a novel technique, ALU, whichaccumulates multiple rounds of local updates before transferring the knowledgeto the central server. ALU drastically decreases the frequency of communicationin federated distillation, thereby significantly reducing the communicationoverhead during the training process. Empirical experiments demonstrate thesubstantial effect of ALU in improving the communication efficiency offederated distillation.</description><author>Zhiyuan Wu, Sheng Sun, Yuwei Wang, Min Liu, Tian Wen, Wen Wang</author><pubDate>Thu, 07 Dec 2023 09:36:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04166v1</guid></item><item><title>DualGenerator: Information Interaction-based Generative Network for Point Cloud Completion</title><link>http://arxiv.org/abs/2305.09132v2</link><description>Point cloud completion estimates complete shapes from incomplete point cloudsto obtain higher-quality point cloud data. Most existing methods only considerglobal object features, ignoring spatial and semantic information of adjacentpoints. They cannot distinguish structural information well between differentobject parts, and the robustness of models is poor. To tackle these challenges,we propose an information interaction-based generative network for point cloudcompletion ($\mathbf{DualGenerator}$). It contains an adversarial generationpath and a variational generation path, which interact with each other andshare weights. DualGenerator introduces a local refinement module in generationpaths, which captures general structures from partial inputs, and then refinesshape details of the point cloud. It promotes completion in the unknown regionand makes a distinction between different parts more obvious. Moreover, wedesign DGStyleGAN to improve the generation quality further. It promotes therobustness of this network combined with fusion analysis of dual-pathcompletion results. Qualitative and quantitative evaluations demonstrate thatour method is superior on MVP and Completion3D datasets. The performance willnot degrade significantly after adding noise interference or sparse sampling.</description><author>Pengcheng Shi, Haozhe Cheng, Xu Han, Yiyang Zhou, Jihua Zhu</author><pubDate>Thu, 07 Dec 2023 09:33:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09132v2</guid></item><item><title>RDF Stream Taxonomy: Systematizing RDF Stream Types in Research and Practice</title><link>http://arxiv.org/abs/2311.14540v2</link><description>Over the years, RDF streaming was explored in research and practice from manyangles, resulting in a wide range of RDF stream definitions. This varietypresents a major challenge in discussing and integrating streaming solutions,due to the lack of a common language. This work attempts to address thiscritical research gap, by systematizing RDF stream types present in theliterature in a novel taxonomy. The proposed RDF Stream Taxonomy (RDF-STaX) isembodied in an OWL 2 DL ontology that follows the FAIR principles, making itreadily applicable in practice. Extensive documentation and additionalresources are provided, to foster the adoption of the ontology. Two realizeduse cases are presented, demonstrating the usefulness of the resource indiscussing research works and annotating streaming datasets. Another result ofthis contribution is the novel nanopublications dataset, which serves as acollaborative, living state-of-the-art review of RDF streaming. The aim ofRDF-STaX is to address a real need of the community for a better way tosystematize and describe RDF streams. The resource is designed to help driveinnovation in RDF streaming, by fostering scientific discussion, cooperation,and tool interoperability.</description><author>Piotr Sowinski, Pawel Szmeja, Maria Ganzha, Marcin Paprzycki</author><pubDate>Thu, 07 Dec 2023 09:29:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14540v2</guid></item><item><title>Multi-scale Residual Transformer for VLF Lightning Transients Classification</title><link>http://arxiv.org/abs/2312.04163v1</link><description>The utilization of Very Low Frequency (VLF) electromagnetic signals innavigation systems is widespread. However, the non-stationary behavior oflightning signals can affect VLF electromagnetic signal transmission.Accurately classifying lightning signals is important for reducing interferenceand noise in VLF, thereby improving the reliability and overall performance ofnavigation systems. In recent years, the evolution of deep learning,specifically Convolutional Neural Network (CNNs), has sparked a transformationin lightning classification, surpassing traditional statistical methodologies.Existing CNN models have limitations as they overlook the diverse attributes oflightning signals across different scales and neglect the significance oftemporal sequencing in sequential signals. This study introduces an innovativemulti-scale residual transform (MRTransformer) that not only has the ability todiscern intricate fine-grained patterns while also weighing the significance ofdifferent aspects within the input lightning signal sequence. This modelperforms the attributes of the lightning signal across different scales and thelevel of accuracy reached 90% in the classification. In future work, this modelhas the potential applied to a comprehensive understanding of the localizationand waveform characteristics of lightning signals.</description><author>Jinghao Sun, Tingting Ji, Guoyu Wang, Rui Wang</author><pubDate>Thu, 07 Dec 2023 09:26:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04163v1</guid></item><item><title>Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment</title><link>http://arxiv.org/abs/2312.03549v2</link><description>Large language models (LLMs) such as GPT-3, OPT, and LLaMA have demonstratedremarkable accuracy in a wide range of tasks. However, training these modelscan incur significant expenses, often requiring tens of thousands of GPUs formonths of continuous operation. Typically, this training is carried out inspecialized GPU clusters equipped with homogeneous high-speed Remote DirectMemory Access (RDMA) network interface cards (NICs). The acquisition andmaintenance of such dedicated clusters is challenging. Current LLM trainingframeworks, like Megatron-LM and Megatron-DeepSpeed, focus primarily onoptimizing training within homogeneous cluster settings. In this paper, weintroduce Holmes, a training framework for LLMs that employs thoughtfullycrafted data and model parallelism strategies over the heterogeneous NICenvironment. Our primary technical contribution lies in a novel schedulingmethod that intelligently allocates distinct computational tasklets in LLMtraining to specific groups of GPU devices based on the characteristics oftheir connected NICs. Furthermore, our proposed framework, utilizing pipelineparallel techniques, demonstrates scalability to multiple GPU clusters, even inscenarios without high-speed interconnects between nodes in distinct clusters.We conducted comprehensive experiments that involved various scenarios in theheterogeneous NIC environment. In most cases, our framework achievesperformance levels close to those achievable with homogeneous RDMA-capablenetworks (InfiniBand or RoCE), significantly exceeding training efficiencywithin the pure Ethernet environment. Additionally, we verified that ourframework outperforms other mainstream LLM frameworks under heterogeneous NICenvironment in terms of training efficiency and can be seamlessly integratedwith them.</description><author>Fei Yang, Shuang Peng, Ning Sun, Fangyu Wang, Ke Tan, Fu Wu, Jiezhong Qiu, Aimin Pan</author><pubDate>Thu, 07 Dec 2023 09:26:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03549v2</guid></item><item><title>Unveiling Objects with SOLA: An Annotation-Free Image Search on the Object Level for Automotive Data Sets</title><link>http://arxiv.org/abs/2312.01860v2</link><description>Huge image data sets are the fundament for the development of the perceptionof automated driving systems. A large number of images is necessary to trainrobust neural networks that can cope with diverse situations. A sufficientlylarge data set contains challenging situations and objects. For testing theresulting functions, it is necessary that these situations and objects can befound and extracted from the data set. While it is relatively easy to record alarge amount of unlabeled data, it is far more difficult to find demandingsituations and objects. However, during the development of perception systems,it must be possible to access challenging data without having to performlengthy and time-consuming annotations. A developer must therefore be able tosearch dynamically for specific situations and objects in a data set. Thus, wedesigned a method which is based on state-of-the-art neural networks to searchfor objects with certain properties within an image. For the ease of use, thequery of this search is described using natural language. To determine the timesavings and performance gains, we evaluated our method qualitatively andquantitatively on automotive data sets.</description><author>Philipp Rigoll, Jacob Langner, Eric Sax</author><pubDate>Thu, 07 Dec 2023 09:24:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01860v2</guid></item></channel></rss>