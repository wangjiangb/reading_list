<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 05 Oct 2023 06:00:43 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving</title><link>http://arxiv.org/abs/2310.03026v1</link><description>Existing learning-based autonomous driving (AD) systems face challenges incomprehending high-level information, generalizing to rare events, andproviding interpretability. To address these problems, this work employs LargeLanguage Models (LLMs) as a decision-making component for complex AD scenariosthat require human commonsense understanding. We devise cognitive pathways toenable comprehensive reasoning with LLMs, and develop algorithms fortranslating LLM decisions into actionable driving commands. Through thisapproach, LLM decisions are seamlessly integrated with low-level controllers byguided parameter matrix adaptation. Extensive experiments demonstrate that ourproposed method not only consistently surpasses baseline approaches insingle-vehicle tasks, but also helps handle complex driving behaviors evenmulti-vehicle coordination, thanks to the commonsense reasoning capabilities ofLLMs. This paper presents an initial step toward leveraging LLMs as effectivedecision-makers for intricate AD scenarios in terms of safety, efficiency,generalizability, and interoperability. We aspire for it to serve asinspiration for future research in this field. Project page:https://sites.google.com/view/llm-mpc</description><author>Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei Zhan, Mingyu Ding</author><pubDate>Wed, 04 Oct 2023 18:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03026v1</guid></item><item><title>Retrieval meets Long Context Large Language Models</title><link>http://arxiv.org/abs/2310.03025v1</link><description>Extending the context window of large language models (LLMs) is gettingpopular recently, while the solution of augmenting LLMs with retrieval hasexisted for years. The natural questions are: i) Retrieval-augmentation versuslong context window, which one is better for downstream tasks? ii) Can bothmethods be combined to get the best of both worlds? In this work, we answerthese questions by studying both solutions using two state-of-the-artpretrained LLMs, i.e., a proprietary 43B GPT and LLaMA2-70B. Perhapssurprisingly, we find that LLM with 4K context window using simpleretrieval-augmentation at generation can achieve comparable performance tofinetuned LLM with 16K context window via positional interpolation on longcontext tasks, while taking much less computation. More importantly, wedemonstrate that retrieval can significantly improve the performance of LLMsregardless of their extended context window sizes. Our best model,retrieval-augmented LLaMA2-70B with 32K context window, outperformsGPT-3.5-turbo-16k and Davinci003 in terms of average score on seven longcontext tasks including question answering and query-based summarization. Italso outperforms its non-retrieval LLaMA2-70B-32k baseline by a margin, whilebeing much faster at generation. Our study provides general insights on thechoice of retrieval-augmentation versus long context extension of LLM forpractitioners.</description><author>Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, Bryan Catanzaro</author><pubDate>Wed, 04 Oct 2023 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03025v1</guid></item><item><title>Human-oriented Representation Learning for Robotic Manipulation</title><link>http://arxiv.org/abs/2310.03023v1</link><description>Humans inherently possess generalizable visual representations that empowerthem to efficiently explore and interact with the environments in manipulationtasks. We advocate that such a representation automatically arises fromsimultaneously learning about multiple simple perceptual skills that arecritical for everyday scenarios (e.g., hand detection, state estimate, etc.)and is better suited for learning robot manipulation policies compared tocurrent state-of-the-art visual representations purely based on self-supervisedobjectives. We formalize this idea through the lens of human-orientedmulti-task fine-tuning on top of pre-trained visual encoders, where each taskis a perceptual skill tied to human-environment interactions. We introduce TaskFusion Decoder as a plug-and-play embedding translator that utilizes theunderlying relationships among these perceptual skills to guide therepresentation learning towards encoding meaningful structure for what'simportant for all perceptual skills, ultimately empowering learning ofdownstream robotic manipulation tasks. Extensive experiments across a range ofrobotic tasks and embodiments, in both simulations and real-world environments,show that our Task Fusion Decoder consistently improves the representation ofthree state-of-the-art visual encoders including R3M, MVP, and EgoVLP, fordownstream manipulation policy-learning. Project page:https://sites.google.com/view/human-oriented-robot-learning</description><author>Mingxiao Huo, Mingyu Ding, Chenfeng Xu, Thomas Tian, Xinghao Zhu, Yao Mu, Lingfeng Sun, Masayoshi Tomizuka, Wei Zhan</author><pubDate>Wed, 04 Oct 2023 18:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03023v1</guid></item><item><title>AstroCLIP: Cross-Modal Pre-Training for Astronomical Foundation Models</title><link>http://arxiv.org/abs/2310.03024v1</link><description>We present AstroCLIP, a strategy to facilitate the construction ofastronomical foundation models that bridge the gap between diverseobservational modalities. We demonstrate that a cross-modal contrastivelearning approach between images and optical spectra of galaxies yields highlyinformative embeddings of both modalities. In particular, we apply our methodon multi-band images and optical spectra from the Dark Energy SpectroscopicInstrument (DESI), and show that: (1) these embeddings are well-aligned betweenmodalities and can be used for accurate cross-modal searches, and (2) theseembeddings encode valuable physical information about the galaxies -- inparticular redshift and stellar mass -- that can be used to achieve competitivezero- and few- shot predictions without further finetuning. Additionally, inthe process of developing our approach, we also construct a novel,transformer-based model and pretraining approach for processing galaxy spectra.</description><author>Francois Lanusse, Liam Parker, Siavash Golkar, Miles Cranmer, Alberto Bietti, Michael Eickenberg, Geraud Krawezik, Michael McCabe, Ruben Ohana, Mariel Pettee, Bruno Regaldo-Saint Blancard, Tiberiu Tesileanu, Kyunghyun Cho, Shirley Ho</author><pubDate>Wed, 04 Oct 2023 18:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03024v1</guid></item><item><title>Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making</title><link>http://arxiv.org/abs/2310.03022v1</link><description>The recent success of Transformer in natural language processing has sparkedits use in various domains. In offline reinforcement learning (RL), DecisionTransformer (DT) is emerging as a promising model based on Transformer.However, we discovered that the attention module of DT is not appropriate tocapture the inherent local dependence pattern in trajectories of RL modeled asa Markov decision process. To overcome the limitations of DT, we propose anovel action sequence predictor, named Decision ConvFormer (DC), based on thearchitecture of MetaFormer, which is a general structure to process multipleentities in parallel and understand the interrelationship among the multipleentities. DC employs local convolution filtering as the token mixer and caneffectively capture the inherent local associations of the RL dataset. Inextensive experiments, DC achieved state-of-the-art performance across variousstandard RL benchmarks while requiring fewer resources. Furthermore, we showthat DC better understands the underlying meaning in data and exhibits enhancedgeneralization capability.</description><author>Jeonghye Kim, Suyoung Lee, Woojun Kim, Youngchul Sung</author><pubDate>Wed, 04 Oct 2023 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03022v1</guid></item><item><title>Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models</title><link>http://arxiv.org/abs/2310.03020v1</link><description>Zero-shot novel view synthesis (NVS) from a single image is an essentialproblem in 3D object understanding. While recent approaches that leveragepre-trained generative models can synthesize high-quality novel views fromin-the-wild inputs, they still struggle to maintain 3D consistency acrossdifferent views. In this paper, we present Consistent-1-to-3, which is agenerative framework that significantly mitigate this issue. Specifically, wedecompose the NVS task into two stages: (i) transforming observed regions to anovel view, and (ii) hallucinating unseen regions. We design a scenerepresentation transformer and view-conditioned diffusion model for performingthese two stages respectively. Inside the models, to enforce 3D consistency, wepropose to employ epipolor-guided attention to incorporate geometryconstraints, and multi-view attention to better aggregate multi-viewinformation. Finally, we design a hierarchy generation paradigm to generatelong sequences of consistent views, allowing a full 360 observation of theprovided object image. Qualitative and quantitative evaluation over multipledatasets demonstrate the effectiveness of the proposed mechanisms againststate-of-the-art approaches. Our project page is athttps://jianglongye.com/consistent123/</description><author>Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, Heng Wang</author><pubDate>Wed, 04 Oct 2023 18:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03020v1</guid></item><item><title>Zero Resource Code-switched Speech Benchmark Using Speech Utterance Pairs For Multiple Spoken Languages</title><link>http://arxiv.org/abs/2310.03018v1</link><description>We introduce a new zero resource code-switched speech benchmark designed todirectly assess the code-switching capabilities of self-supervised speechencoders. We showcase a baseline system of language modeling on discrete unitsto demonstrate how the code-switching abilities of speech encoders can beassessed in a zero-resource manner. Our experiments encompass a variety ofwell-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. Weexamine the impact of pre-training languages and model size on benchmarkperformance. Notably, though our results demonstrate that speech encoders withmultilingual pre-training, exemplified by XLSR, outperform monolingual variants(Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantialroom for improvement in their code-switching linguistic abilities.</description><author>Kuan-Po Huang, Chih-Kai Yang, Yu-Kuan Fu, Ewan Dunbar, Hung-yi Lee</author><pubDate>Wed, 04 Oct 2023 18:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03018v1</guid></item><item><title>Multimodal Question Answering for Unified Information Extraction</title><link>http://arxiv.org/abs/2310.03017v1</link><description>Multimodal information extraction (MIE) aims to extract structuredinformation from unstructured multimedia content. Due to the diversity of tasksand settings, most current MIE models are task-specific and data-intensive,which limits their generalization to real-world scenarios with diverse taskrequirements and limited labeled data. To address these issues, we propose anovel multimodal question answering (MQA) framework to unify three MIE tasks byreformulating them into a unified span extraction and multi-choice QA pipeline.Extensive experiments on six datasets show that: 1) Our MQA frameworkconsistently and significantly improves the performances of variousoff-the-shelf large multimodal models (LMM) on MIE tasks, compared to vanillaprompting. 2) In the zero-shot setting, MQA outperforms previousstate-of-the-art baselines by a large margin. In addition, the effectiveness ofour framework can successfully transfer to the few-shot setting, enhancing LMMson a scale of 10B parameters to be competitive or outperform much largerlanguage models such as ChatGPT and GPT-4. Our MQA framework can serve as ageneral principle of utilizing LMMs to better solve MIE and potentially otherdownstream multimodal tasks.</description><author>Yuxuan Sun, Kai Zhang, Yu Su</author><pubDate>Wed, 04 Oct 2023 18:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03017v1</guid></item><item><title>ConR: Contrastive Regularizer for Deep Imbalanced Regression</title><link>http://arxiv.org/abs/2309.06651v2</link><description>Imbalanced distributions are ubiquitous in real-world data. They createconstraints on Deep Neural Networks to represent the minority labels and avoidbias towards majority labels. The extensive body of imbalanced approachesaddress categorical label spaces but fail to effectively extend to regressionproblems where the label space is continuous. Local and global correlationsamong continuous labels provide valuable insights towards effectively modellingrelationships in feature space. In this work, we propose ConR, a contrastiveregularizer that models global and local label similarities in feature spaceand prevents the features of minority samples from being collapsed into theirmajority neighbours. ConR discerns the disagreements between the label spaceand feature space and imposes a penalty on these disagreements. ConR addressesthe continuous nature of label space with two main strategies in a contrastivemanner: incorrect proximities are penalized proportionate to the labelsimilarities and the correct ones are encouraged to model local similarities.ConR consolidates essential considerations into a generic, easy-to-integrate,and efficient method that effectively addresses deep imbalanced regression.Moreover, ConR is orthogonal to existing approaches and smoothly extends touni- and multi-dimensional label spaces. Our comprehensive experiments showthat ConR significantly boosts the performance of all the state-of-the-artmethods on four large-scale deep imbalanced regression benchmarks. Our code ispublicly available in https://github.com/BorealisAI/ConR.</description><author>Mahsa Keramati, Lili Meng, R. David Evans</author><pubDate>Wed, 04 Oct 2023 18:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06651v2</guid></item><item><title>Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions</title><link>http://arxiv.org/abs/2310.03016v1</link><description>In order to understand the in-context learning phenomenon, recent works haveadopted a stylized experimental framework and demonstrated that Transformerscan learn gradient-based learning algorithms for various classes of real-valuedfunctions. However, the limitations of Transformers in implementing learningalgorithms, and their ability to learn other forms of algorithms are not wellunderstood. Additionally, the degree to which these capabilities are confinedto attention-based models is unclear. Furthermore, it remains to be seenwhether the insights derived from these stylized settings can be extrapolatedto pretrained Large Language Models (LLMs). In this work, we take a steptowards answering these questions by demonstrating the following: (a) On atest-bed with a variety of Boolean function classes, we find that Transformerscan nearly match the optimal learning algorithm for 'simpler' tasks, whiletheir performance deteriorates on more 'complex' tasks. Additionally, we findthat certain attention-free models perform (almost) identically to Transformerson a range of tasks. (b) When provided a teaching sequence, i.e. a set ofexamples that uniquely identifies a function in a class, we show thatTransformers learn more sample-efficiently. Interestingly, our results showthat Transformers can learn to implement two distinct algorithms to solve asingle task, and can adaptively select the more sample-efficient algorithmdepending on the sequence of in-context examples. (c) Lastly, we show thatextant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselineson prediction tasks that are guaranteed to not be in their training set.</description><author>Satwik Bhattamishra, Arkil Patel, Phil Blunsom, Varun Kanade</author><pubDate>Wed, 04 Oct 2023 18:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03016v1</guid></item><item><title>Efficient-3DiM: Learning a Generalizable Single-image Novel-view Synthesizer in One Day</title><link>http://arxiv.org/abs/2310.03015v1</link><description>The task of novel view synthesis aims to generate unseen perspectives of anobject or scene from a limited set of input images. Nevertheless, synthesizingnovel views from a single image still remains a significant challenge in therealm of computer vision. Previous approaches tackle this problem by adoptingmesh prediction, multi-plain image construction, or more advanced techniquessuch as neural radiance fields. Recently, a pre-trained diffusion model that isspecifically designed for 2D image synthesis has demonstrated its capability inproducing photorealistic novel views, if sufficiently optimized on a 3Dfinetuning task. Although the fidelity and generalizability are greatlyimproved, training such a powerful diffusion model requires a vast volume oftraining data and model parameters, resulting in a notoriously long time andhigh computational costs. To tackle this issue, we propose Efficient-3DiM, asimple but effective framework to learn a single-image novel-view synthesizer.Motivated by our in-depth analysis of the inference process of diffusionmodels, we propose several pragmatic strategies to reduce the training overheadto a manageable scale, including a crafted timestep sampling strategy, asuperior 3D feature extractor, and an enhanced training scheme. When combined,our framework is able to reduce the total training time from 10 days to lessthan 1 day, significantly accelerating the training process under the samecomputational platform (one instance with 8 Nvidia A100 GPUs). Comprehensiveexperiments are conducted to demonstrate the efficiency and generalizability ofour proposed method.</description><author>Yifan Jiang, Hao Tang, Jen-Hao Rick Chang, Liangchen Song, Zhangyang Wang, Liangliang Cao</author><pubDate>Wed, 04 Oct 2023 18:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03015v1</guid></item><item><title>SemiReward: A General Reward Model for Semi-supervised Learning</title><link>http://arxiv.org/abs/2310.03013v1</link><description>Semi-supervised learning (SSL) has witnessed great progress with variousimprovements in the self-training framework with pseudo labeling. The mainchallenge is how to distinguish high-quality pseudo labels against theconfirmation bias. However, existing pseudo-label selection strategies arelimited to pre-defined schemes or complex hand-crafted policies speciallydesigned for classification, failing to achieve high-quality labels, fastconvergence, and task versatility simultaneously. To these ends, we propose aSemi-supervised Reward framework (SemiReward) that predicts reward scores toevaluate and filter out high-quality pseudo labels, which is pluggable tomainstream SSL methods in wide task types and scenarios. To mitigateconfirmation bias, SemiReward is trained online in two stages with a generatormodel and subsampling strategy. With classification and regression tasks on 13standard SSL benchmarks of three modalities, extensive experiments verify thatSemiReward achieves significant performance gains and faster convergence speedsupon Pseudo Label, FlexMatch, and Free/SoftMatch.</description><author>Siyuan Li, Weiyang Jin, Zedong Wang, Fang Wu, Zicheng Liu, Cheng Tan, Stan Z. Li</author><pubDate>Wed, 04 Oct 2023 18:56:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03013v1</guid></item><item><title>Learning Adaptive Safety for Multi-Agent Systems</title><link>http://arxiv.org/abs/2309.10657v2</link><description>Ensuring safety in dynamic multi-agent systems is challenging due to limitedinformation about the other agents. Control Barrier Functions (CBFs) areshowing promise for safety assurance but current methods make strongassumptions about other agents and often rely on manual tuning to balancesafety, feasibility, and performance. In this work, we delve into the problemof adaptive safe learning for multi-agent systems with CBF. We show howemergent behavior can be profoundly influenced by the CBF configuration,highlighting the necessity for a responsive and dynamic approach to CBF design.We present ASRL, a novel adaptive safe RL framework, to fully automate theoptimization of policy and CBF coefficients, to enhance safety and long-termperformance through reinforcement learning. By directly interacting with theother agents, ASRL learns to cope with diverse agent behaviours and maintainsthe cost violations below a desired limit. We evaluate ASRL in a multi-robotsystem and a competitive multi-agent racing scenario, against learning-basedand control-theoretic approaches. We empirically demonstrate the efficacy andflexibility of ASRL, and assess generalization and scalability toout-of-distribution scenarios. Code and supplementary material are publiconline.</description><author>Luigi Berducci, Shuo Yang, Rahul Mangharam, Radu Grosu</author><pubDate>Wed, 04 Oct 2023 18:55:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10657v2</guid></item><item><title>One Sense per Translation</title><link>http://arxiv.org/abs/2106.06082v2</link><description>Word sense disambiguation (WSD) is the task of determining the sense of aword in context. Translations have been used in WSD as a source of knowledge,and even as a means of delimiting word senses. In this paper, we define threetheoretical properties of the relationship between senses and translations, andargue that they constitute necessary conditions for using translations as senseinventories. The key property of One Sense per Translation (OSPT) provides afoundation for a translation-based WSD method. The results of an intrinsicevaluation experiment indicate that our method achieves a precision ofapproximately 93% compared to manual corpus annotations. Our extrinsicevaluation experiments demonstrate WSD improvements of up to 4.6% F1-score ondifficult WSD datasets.</description><author>Bradley Hauer, Grzegorz Kondrak</author><pubDate>Wed, 04 Oct 2023 18:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.06082v2</guid></item><item><title>High-dimensional SGD aligns with emerging outlier eigenspaces</title><link>http://arxiv.org/abs/2310.03010v1</link><description>We rigorously study the joint evolution of training dynamics via stochasticgradient descent (SGD) and the spectra of empirical Hessian and gradientmatrices. We prove that in two canonical classification tasks for multi-classhigh-dimensional mixtures and either 1 or 2-layer neural networks, the SGDtrajectory rapidly aligns with emerging low-rank outlier eigenspaces of theHessian and gradient matrices. Moreover, in multi-layer settings this alignmentoccurs per layer, with the final layer's outlier eigenspace evolving over thecourse of training, and exhibiting rank deficiency when the SGD converges tosub-optimal classifiers. This establishes some of the rich predictions thathave arisen from extensive numerical studies in the last decade about thespectra of Hessian and information matrices over the course of training inoverparametrized networks.</description><author>Gerard Ben Arous, Reza Gheissari, Jiaoyang Huang, Aukosh Jagannath</author><pubDate>Wed, 04 Oct 2023 18:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03010v1</guid></item><item><title>LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples</title><link>http://arxiv.org/abs/2310.01469v2</link><description>Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to beknowledgeable and able to adapt to many tasks. However, we still can notcompletely trust their answer, since LLMs suffer fromhallucination--fabricating non-existent facts to cheat users withoutperception. And the reasons for their existence and pervasiveness remainunclear. In this paper, we demonstrate that non-sense prompts composed ofrandom tokens can also elicit the LLMs to respond with hallucinations. Thisphenomenon forces us to revisit that hallucination may be another view ofadversarial examples, and it shares similar features with conventionaladversarial examples as the basic feature of LLMs. Therefore, we formalize anautomatic hallucination triggering method as the hallucination attack in anadversarial way. Finally, we explore basic feature of attacked adversarialprompts and propose a simple yet effective defense strategy. Our code isreleased on GitHub.</description><author>Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, Li Yuan</author><pubDate>Wed, 04 Oct 2023 18:53:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01469v2</guid></item><item><title>Towards Domain-Specific Features Disentanglement for Domain Generalization</title><link>http://arxiv.org/abs/2310.03007v1</link><description>Distributional shift between domains poses great challenges to modern machinelearning algorithms. The domain generalization (DG) signifies a popular linetargeting this issue, where these methods intend to uncover universal patternsacross disparate distributions. Noted, the crucial challenge behind DG is theexistence of irrelevant domain features, and most prior works overlook thisinformation. Motivated by this, we propose a novel contrastive-baseddisentanglement method CDDG, to effectively utilize the disentangled featuresto exploit the over-looked domain-specific features, and thus facilitating theextraction of the desired cross-domain category features for DG tasks.Specifically, CDDG learns to decouple inherent mutually exclusive features byleveraging them in the latent space, thus making the learning discriminative.Extensive experiments conducted on various benchmark datasets demonstrate thesuperiority of our method compared to other state-of-the-art approaches.Furthermore, visualization evaluations confirm the potential of our method inachieving effective feature disentanglement.</description><author>Hao Chen, Qi Zhang, Zenan Huang, Haobo Wang, Junbo Zhao</author><pubDate>Wed, 04 Oct 2023 18:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03007v1</guid></item><item><title>COOLer: Class-Incremental Learning for Appearance-Based Multiple Object Tracking</title><link>http://arxiv.org/abs/2310.03006v1</link><description>Continual learning allows a model to learn multiple tasks sequentially whileretaining the old knowledge without the training data of the preceding tasks.This paper extends the scope of continual learning research toclass-incremental learning for \ac{mot}, which is desirable to accommodate thecontinuously evolving needs of autonomous systems. Previous solutions forcontinual learning of object detectors do not address the data associationstage of appearance-based trackers, leading to catastrophic forgetting ofprevious classes' re-identification features. We introduce COOLer, aCOntrastive- and cOntinual-Learning-based tracker, which incrementally learnsto track new categories while preserving past knowledge by training on acombination of currently available ground truth labels and pseudo-labelsgenerated by the past tracker. To further exacerbate the disentanglement ofinstance representations, we introduce a novel contrastive class-incrementalinstance representation learning technique. Finally, we propose a practicalevaluation protocol for continual learning for MOT and conduct experiments onthe \bdd and \shift datasets. Experimental results demonstrate that COOLercontinually learns while effectively addressing catastrophic forgetting of bothtracking and detection. The code is available at\url{https://github.com/BoSmallEar/COOLer}.</description><author>Zhizheng Liu, Mattia Segu, Fisher Yu</author><pubDate>Wed, 04 Oct 2023 18:49:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03006v1</guid></item><item><title>Reversing Deep Face Embeddings with Probable Privacy Protection</title><link>http://arxiv.org/abs/2310.03005v1</link><description>Generally, privacy-enhancing face recognition systems are designed to offerpermanent protection of face embeddings. Recently, so-called soft-biometricprivacy-enhancement approaches have been introduced with the aim of cancelingsoft-biometric attributes. These methods limit the amount of soft-biometricinformation (gender or skin-colour) that can be inferred from face embeddings.Previous work has underlined the need for research into rigorous evaluationsand standardised evaluation protocols when assessing privacy protectioncapabilities. Motivated by this fact, this paper explores to what extent thenon-invertibility requirement can be met by methods that claim to providesoft-biometric privacy protection. Additionally, a detailed vulnerabilityassessment of state-of-the-art face embedding extractors is analysed in termsof the transformation complexity used for privacy protection. In this context,a well-known state-of-the-art face image reconstruction approach has beenevaluated on protected face embeddings to break soft biometric privacyprotection. Experimental results show that biometric privacy-enhanced faceembeddings can be reconstructed with an accuracy of up to approximately 98%,depending on the complexity of the protection algorithm.</description><author>Daile Osorio-Roig, Paul A. Gerlitz, Christian Rathgeb, Christoph Busch</author><pubDate>Wed, 04 Oct 2023 18:48:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03005v1</guid></item><item><title>Soft Convex Quantization: Revisiting Vector Quantization with Convex Optimization</title><link>http://arxiv.org/abs/2310.03004v1</link><description>Vector Quantization (VQ) is a well-known technique in deep learning forextracting informative discrete latent representations. VQ-embedded models haveshown impressive results in a range of applications including image and speechgeneration. VQ operates as a parametric K-means algorithm that quantizes inputsusing a single codebook vector in the forward pass. While powerful, thistechnique faces practical challenges including codebook collapse,non-differentiability and lossy compression. To mitigate the aforementionedissues, we propose Soft Convex Quantization (SCQ) as a direct substitute forVQ. SCQ works like a differentiable convex optimization (DCO) layer: in theforward pass, we solve for the optimal convex combination of codebook vectorsthat quantize the inputs. In the backward pass, we leverage differentiabilitythrough the optimality conditions of the forward solution. We then introduce ascalable relaxation of the SCQ optimization and demonstrate its efficacy on theCIFAR-10, GTSRB and LSUN datasets. We train powerful SCQ autoencoder modelsthat significantly outperform matched VQ-based architectures, observing anorder of magnitude better image reconstruction and codebook usage withcomparable quantization runtime.</description><author>Tanmay Gautam, Reid Pryzant, Ziyi Yang, Chenguang Zhu, Somayeh Sojoudi</author><pubDate>Wed, 04 Oct 2023 18:45:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03004v1</guid></item><item><title>Faithful and Efficient Explanations for Neural Networks via Neural Tangent Kernel Surrogate Models</title><link>http://arxiv.org/abs/2305.14585v2</link><description>A recent trend in explainable AI research has focused on surrogate modeling,where neural networks are approximated as simpler ML algorithms such as kernelmachines. A second trend has been to utilize kernel functions in variousexplain-by-example or data attribution tasks to investigate a diverse set ofneural network behavior. In this work, we combine these two trends to analyzeapproximate empirical neural tangent kernels (eNTK) for data attribution.Approximation is critical for eNTK analysis due to the high computational costto compute the eNTK. We define new approximate eNTK and perform novel analysison how well the resulting kernel machine surrogate models correlate with theunderlying neural network. We introduce two new random projection variants ofapproximate eNTK which allow users to tune the time and memory complexity oftheir calculation. We conclude that kernel machines using approximate neuraltangent kernel as the kernel function are effective surrogate models, with theintroduced trace NTK the most consistent performer.</description><author>Andrew Engel, Zhichao Wang, Natalie S. Frank, Ioana Dumitriu, Sutanay Choudhury, Anand Sarwate, Tony Chiang</author><pubDate>Wed, 04 Oct 2023 18:44:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14585v2</guid></item><item><title>From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference</title><link>http://arxiv.org/abs/2310.03003v1</link><description>Large language models (LLMs) have exploded in popularity due to their newgenerative capabilities that go far beyond prior state-of-the-art. Thesetechnologies are increasingly being leveraged in various domains such as law,finance, and medicine. However, these models carry significant computationalchallenges, especially the compute and energy costs required for inference.Inference energy costs already receive less attention than the energy costs oftraining LLMs -- despite how often these large models are called on to conductinference in reality (e.g., ChatGPT). As these state-of-the-art LLMs seeincreasing usage and deployment in various domains, a better understanding oftheir resource utilization is crucial for cost-savings, scaling performance,efficient hardware usage, and optimal inference strategies. In this paper, we describe experiments conducted to study the computationaland energy utilization of inference with LLMs. We benchmark and conduct apreliminary analysis of the inference performance and inference energy costs ofdifferent sizes of LLaMA -- a recent state-of-the-art LLM -- developed by MetaAI on two generations of popular GPUs (NVIDIA V100 \&amp; A100) and two datasets(Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs inresearch and practice. We present the results of multi-node, multi-GPUinference using model sharding across up to 32 GPUs. To our knowledge, our workis the one of the first to study LLM inference performance from the perspectiveof computational and energy resources at this scale.</description><author>Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, Vijay Gadepally</author><pubDate>Wed, 04 Oct 2023 18:41:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03003v1</guid></item><item><title>Learning characteristic parameters and dynamics of centrifugal pumps under multi-phase flow using physics-informed neural networks</title><link>http://arxiv.org/abs/2310.03001v1</link><description>Electrical submersible pumps (ESP) are the second most used artificiallifting equipment in the oil and gas industry due to their high flow rates andboost pressures. They often have to handle multiphase flows, which usuallycontain a mixture of hydrocarbons, water, and/or sediments. Given thesecircumstances, emulsions are commonly formed. It is a liquid-liquid flowcomposed of two immiscible fluids whose effective viscosity and density differfrom the single phase separately. In this context, accurate modeling of ESPsystems is crucial for optimizing oil production and implementing controlstrategies. However, real-time and direct measurement of fluid and systemcharacteristics is often impractical due to time constraints and economy.Hence, indirect methods are generally considered to estimate the systemparameters. In this paper, we formulate a machine learning model based onPhysics-Informed Neural Networks (PINNs) to estimate crucial system parameters.In order to study the efficacy of the proposed PINN model, we conductcomputational studies using not only simulated but also experimental data fordifferent water-oil ratios. We evaluate the state variable's dynamics andunknown parameters for various combinations when only intake and dischargepressure measurements are available. We also study structural and practicalidentifiability analyses based on commonly available pressure measurements. ThePINN model could reduce the requirement of expensive field laboratory testsused to estimate fluid properties.</description><author>Felipe de Castro Teixeira Carvalho, Kamaljyoti Nath, Alberto Luiz Serpa, George Em Karniadakis</author><pubDate>Wed, 04 Oct 2023 18:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03001v1</guid></item><item><title>ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models</title><link>http://arxiv.org/abs/2310.02998v1</link><description>Large Vision-Language Models (LVLMs) can understand the world comprehensivelyby integrating rich information from different modalities, achieving remarkableperformance improvements on various multimodal downstream tasks. However,deploying LVLMs is often problematic due to their massive computational/energycosts and carbon consumption. Such issues make it infeasible to adoptconventional iterative global pruning, which is costly due to computing theHessian matrix of the entire large model for sparsification. Alternatively,several studies have recently proposed layer-wise pruning approaches to avoidthe expensive computation of global pruning and efficiently compress modelweights according to their importance within a layer. However, these methodsoften suffer from suboptimal model compression due to their lack of a globalperspective. To address this limitation in recent efficient pruning methods forlarge models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP),a two-stage coarse-to-fine weight pruning approach for LVLMs. We firstdetermine the sparsity ratios of different layers or blocks by leveraging theglobal importance score, which is efficiently computed based on thezeroth-order approximation of the global model gradients. Then, the multimodalmodel performs local layer-wise unstructured weight pruning based onglobally-informed sparsity ratios. We validate our proposed method acrossvarious multimodal and unimodal models and datasets, demonstrating significantperformance improvements over prevalent pruning techniques in the high-sparsityregime.</description><author>Yi-Lin Sung, Jaehong Yoon, Mohit Bansal</author><pubDate>Wed, 04 Oct 2023 18:34:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02998v1</guid></item><item><title>Optimizing Key-Selection for Face-based One-Time Biometrics via Morphing</title><link>http://arxiv.org/abs/2310.02997v1</link><description>Nowadays, facial recognition systems are still vulnerable to adversarialattacks. These attacks vary from simple perturbations of the input image tomodifying the parameters of the recognition model to impersonate an authorisedsubject. So-called privacy-enhancing facial recognition systems have beenmostly developed to provide protection of stored biometric reference data, i.e.templates. In the literature, privacy-enhancing facial recognition approacheshave focused solely on conventional security threats at the template level,ignoring the growing concern related to adversarial attacks. Up to now, fewworks have provided mechanisms to protect face recognition against adversarialattacks while maintaining high security at the template level. In this paper,we propose different key selection strategies to improve the security of acompetitive cancelable scheme operating at the signal level. Experimentalresults show that certain strategies based on signal-level key selection canlead to complete blocking of the adversarial attack based on an iterativeoptimization for the most secure threshold, while for the most practicalthreshold, the attack success chance can be decreased to approximately 5.0%.</description><author>Daile Osorio-Roig, Mahdi Ghafourian, Christian Rathgeb, Ruben Vera-Rodriguez, Christoph Busch, Julian Fierrez</author><pubDate>Wed, 04 Oct 2023 18:32:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02997v1</guid></item><item><title>IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning</title><link>http://arxiv.org/abs/2310.02995v1</link><description>Like generic multi-task learning, continual learning has the nature ofmulti-objective optimization, and therefore faces a trade-off between theperformance of different tasks. That is, to optimize for the current taskdistribution, it may need to compromise performance on some previous tasks.This means that there exist multiple models that are Pareto-optimal atdifferent times, each addressing a distinct task performance trade-off.Researchers have discussed how to train particular models to address specifictrade-off preferences. However, existing algorithms require training overheadsproportional to the number of preferences -- a large burden when there aremultiple, possibly infinitely many, preferences. As a response, we proposeImprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updatesa knowledge base in the form of a convex hull of model parameter distributionsand (2) obtains particular models to address task trade-off preferences withzero-shot. That is, IBCL does not require any additional training overhead togenerate preference-addressing models from its knowledge base. We show thatmodels obtained by IBCL have guarantees in identifying the Pareto optimalparameters. Moreover, experiments on standard image classification and NLPtasks support this guarantee. Statistically, IBCL improves average per-taskaccuracy by at most 23\% and peak per-task accuracy by at most 15\% withrespect to the baseline methods, with steadily near-zero or positive backwardtransfer. Most importantly, IBCL significantly reduces the training overheadfrom training 1 model per preference to at most 3 models for all preferences.</description><author>Pengyuan Lu, Michele Caprio, Eric Eaton, Insup Lee</author><pubDate>Wed, 04 Oct 2023 18:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02995v1</guid></item><item><title>Multiple Physics Pretraining for Physical Surrogate Models</title><link>http://arxiv.org/abs/2310.02994v1</link><description>We introduce multiple physics pretraining (MPP), an autoregressivetask-agnostic pretraining approach for physical surrogate modeling. MPPinvolves training large surrogate models to predict the dynamics of multipleheterogeneous physical systems simultaneously by learning features that arebroadly useful across diverse physical tasks. In order to learn effectively inthis setting, we introduce a shared embedding and normalization strategy thatprojects the fields of multiple systems into a single shared embedding space.We validate the efficacy of our approach on both pretraining and downstreamtasks over a broad fluid mechanics-oriented benchmark. We show that a singleMPP-pretrained transformer is able to match or outperform task-specificbaselines on all pretraining sub-tasks without the need for finetuning. Fordownstream tasks, we demonstrate that finetuning MPP-trained models results inmore accurate predictions across multiple time-steps on new physics compared totraining from scratch or finetuning pretrained video foundation models. Weopen-source our code and model weights trained at multiple scales forreproducibility and community experimentation.</description><author>Michael McCabe, Bruno Régaldo-Saint Blancard, Liam Holden Parker, Ruben Ohana, Miles Cranmer, Alberto Bietti, Michael Eickenberg, Siavash Golkar, Geraud Krawezik, Francois Lanusse, Mariel Pettee, Tiberiu Tesileanu, Kyunghyun Cho, Shirley Ho</author><pubDate>Wed, 04 Oct 2023 18:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02994v1</guid></item><item><title>Kosmos-G: Generating Images in Context with Multimodal Large Language Models</title><link>http://arxiv.org/abs/2310.02992v1</link><description>Recent advancements in text-to-image (T2I) and vision-language-to-image(VL2I) generation have made significant strides. However, the generation fromgeneralized vision-language inputs, especially involving multiple images,remains under-explored. This paper presents Kosmos-G, a model that leveragesthe advanced perception capabilities of Multimodal Large Language Models(MLLMs) to tackle the aforementioned challenge. Our approach aligns the outputspace of MLLM with CLIP using the textual modality as an anchor and performscompositional instruction tuning on curated data. Kosmos-G demonstrates aunique capability of zero-shot multi-entity subject-driven generation. Notably,the score distillation instruction tuning requires no modifications to theimage decoder. This allows for a seamless substitution of CLIP and effortlessintegration with a myriad of U-Net techniques ranging from fine-grainedcontrols to personalized image decoder variants. We posit Kosmos-G as aninitial attempt towards the goal of "image as a foreign language in imagegeneration."</description><author>Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, Furu Wei</author><pubDate>Wed, 04 Oct 2023 18:28:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02992v1</guid></item><item><title>xVal: A Continuous Number Encoding for Large Language Models</title><link>http://arxiv.org/abs/2310.02989v1</link><description>Large Language Models have not yet been broadly adapted for the analysis ofscientific datasets due in part to the unique difficulties of tokenizingnumbers. We propose xVal, a numerical encoding scheme that represents any realnumber using just a single token. xVal represents a given real number byscaling a dedicated embedding vector by the number value. Combined with amodified number-inference approach, this strategy renders the model end-to-endcontinuous when considered as a map from the numbers of the input string tothose of the output string. This leads to an inductive bias that is generallymore suitable for applications in scientific domains. We empirically evaluateour proposal on a number of synthetic and real-world datasets. Compared withexisting number encoding schemes, we find that xVal is more token-efficient anddemonstrates improved generalization.</description><author>Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, Bruno Régaldo-Saint Blancard, Tiberiu Tesileanu, Kyunghyun Cho, Shirley Ho</author><pubDate>Wed, 04 Oct 2023 18:26:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02989v1</guid></item><item><title>Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples</title><link>http://arxiv.org/abs/2310.02988v1</link><description>While vision-language models (VLMs) have achieved remarkable performanceimprovements recently, there is growing evidence that these models also possesharmful biases with respect to social attributes such as gender and race. Priorstudies have primarily focused on probing such bias attributes individuallywhile ignoring biases associated with intersections between social attributes.This could be due to the difficulty of collecting an exhaustive set ofimage-text pairs for various combinations of social attributes from existingdatasets. To address this challenge, we employ text-to-image diffusion modelsto produce counterfactual examples for probing intserctional social biases atscale. Our approach utilizes Stable Diffusion with cross attention control toproduce sets of counterfactual image-text pairs that are highly similar intheir depiction of a subject (e.g., a given occupation) while differing only intheir depiction of intersectional social attributes (e.g., race &amp; gender). Weconduct extensive experiments using our generated dataset which reveal theintersectional social biases present in state-of-the-art VLMs.</description><author>Phillip Howard, Avinash Madasu, Tiep Le, Gustavo Lujan Moreno, Vasudev Lal</author><pubDate>Wed, 04 Oct 2023 18:25:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02988v1</guid></item><item><title>Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions</title><link>http://arxiv.org/abs/2310.02987v1</link><description>Machine learning approaches relying on such criteria as adversarialrobustness or multi-agent settings have raised the need for solvinggame-theoretic equilibrium problems. Of particular relevance to theseapplications are methods targeting finite-sum structure, which genericallyarises in empirical variants of learning problems in these contexts. Further,methods with computable approximation errors are highly desirable, as theyprovide verifiable exit criteria. Motivated by these applications, we studyfinite-sum monotone inclusion problems, which model broad classes ofequilibrium problems. Our main contributions are variants of the classicalHalpern iteration that employ variance reduction to obtain improved complexityguarantees in which $n$ component operators in the finite sum are ``onaverage'' either cocoercive or Lipschitz continuous and monotone, withparameter $L$. The resulting oracle complexity of our methods, which provideguarantees for the last iterate and for a (computable) operator norm residual,is $\widetilde{\mathcal{O}}( n + \sqrt{n}L\varepsilon^{-1})$, which improvesupon existing methods by a factor up to $\sqrt{n}$. This constitutes the firstvariance reduction-type result for general finite-sum monotone inclusions andfor more specific problems such as convex-concave optimization when operatornorm residual is the optimality measure. We further argue that, up topoly-logarithmic factors, this complexity is unimprovable in the monotoneLipschitz setting; i.e., the provided result is near-optimal.</description><author>Xufeng Cai, Ahmet Alacaoglu, Jelena Diakonikolas</author><pubDate>Wed, 04 Oct 2023 18:24:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02987v1</guid></item><item><title>Exploring the Impact of Disrupted Peer-to-Peer Communications on Fully Decentralized Learning in Disaster Scenarios</title><link>http://arxiv.org/abs/2310.02986v1</link><description>Fully decentralized learning enables the distribution of learning resourcesand decision-making capabilities across multiple user devices or nodes, and israpidly gaining popularity due to its privacy-preserving and decentralizednature. Importantly, this crowdsourcing of the learning process allows thesystem to continue functioning even if some nodes are affected or disconnected.In a disaster scenario, communication infrastructure and centralized systemsmay be disrupted or completely unavailable, hindering the possibility ofcarrying out standard centralized learning tasks in these settings. Thus, fullydecentralized learning can help in this case. However, transitioning fromcentralized to peer-to-peer communications introduces a dependency between thelearning process and the topology of the communication graph among nodes. In adisaster scenario, even peer-to-peer communications are susceptible to abruptchanges, such as devices running out of battery or getting disconnected fromothers due to their position. In this study, we investigate the effects ofvarious disruptions to peer-to-peer communications on decentralized learning ina disaster setting. We examine the resilience of a decentralized learningprocess when a subset of devices drop from the process abruptly. To this end,we analyze the difference between losing devices holding data, i.e., potentialknowledge, vs. devices contributing only to the graph connectivity, i.e., withno data. Our findings on a Barabasi-Albert graph topology, where training datais distributed across nodes in an IID fashion, indicate that the accuracy ofthe learning process is more affected by a loss of connectivity than by a lossof data. Nevertheless, the network remains relatively robust, and the learningprocess can achieve a good level of accuracy.</description><author>Luigi Palmieri, Chiara Boldrini, Lorenzo Valerio, Andrea Passarella, Marco Conti</author><pubDate>Wed, 04 Oct 2023 18:24:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02986v1</guid></item><item><title>Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks</title><link>http://arxiv.org/abs/2310.02244v2</link><description>By classifying infinite-width neural networks and identifying the *optimal*limit, Tensor Programs IV and V demonstrated a universal way, called $\mu$P,for *widthwise hyperparameter transfer*, i.e., predicting optimalhyperparameters of wide neural networks from narrow ones. Here we investigatethe analogous classification for *depthwise parametrizations* of deep residualnetworks (resnets). We classify depthwise parametrizations of block multiplierand learning rate by their infinite-width-then-depth limits. In resnets whereeach block has only one layer, we identify a unique optimal parametrization,called Depth-$\mu$P that extends $\mu$P and show empirically it admitsdepthwise hyperparameter transfer. We identify *feature diversity* as a crucialfactor in deep networks, and Depth-$\mu$P can be characterized as maximizingboth feature learning and feature diversity. Exploiting this, we find thatabsolute value, among all homogeneous nonlinearities, maximizes featurediversity and indeed empirically leads to significantly better performance.However, if each block is deeper (such as modern transformers), then we findfundamental limitations in all possible infinite-depth limits of suchparametrizations, which we illustrate both theoretically and empirically onsimple networks as well as Megatron transformer trained on Common Crawl.</description><author>Greg Yang, Dingli Yu, Chen Zhu, Soufiane Hayou</author><pubDate>Wed, 04 Oct 2023 18:23:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02244v2</guid></item><item><title>Scaling Laws for Associative Memories</title><link>http://arxiv.org/abs/2310.02984v1</link><description>Learning arguably involves the discovery and memorization of abstract rules.The aim of this paper is to study associative memory mechanisms. Our model isbased on high-dimensional matrices consisting of outer products of embeddings,which relates to the inner layers of transformer language models. We deriveprecise scaling laws with respect to sample size and parameter size, anddiscuss the statistical efficiency of different estimators, includingoptimization-based algorithms. We provide extensive numerical experiments tovalidate and interpret theoretical results, including fine-grainedvisualizations of the stored memory associations.</description><author>Vivien Cabannes, Elvis Dohmatob, Alberto Bietti</author><pubDate>Wed, 04 Oct 2023 18:20:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02984v1</guid></item><item><title>Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors</title><link>http://arxiv.org/abs/2310.02980v1</link><description>Modeling long-range dependencies across sequences is a longstanding goal inmachine learning and has led to architectures, such as state space models, thatdramatically outperform Transformers on long sequences. However, theseimpressive empirical gains have been by and large demonstrated on benchmarks(e.g. Long Range Arena), where models are randomly initialized and trained topredict a target label from an input sequence. In this work, we show thatrandom initialization leads to gross overestimation of the differences betweenarchitectures and that pretraining with standard denoising objectives, using$\textit{only the downstream task data}$, leads to dramatic gains acrossmultiple architectures and to very small gaps between Transformers and statespace models (SSMs). In stark contrast to prior works, we find vanillaTransformers to match the performance of S4 on Long Range Arena when properlypretrained, and we improve the best reported results of SSMs on the PathX-256task by 20 absolute points. Subsequently, we analyze the utility ofpreviously-proposed structured parameterizations for SSMs and show they becomemostly redundant in the presence of data-driven initialization obtained throughpretraining. Our work shows that, when evaluating different architectures onsupervised tasks, incorporation of data-driven priors via pretraining isessential for reliable performance estimation, and can be done efficiently.</description><author>Ido Amos, Jonathan Berant, Ankit Gupta</author><pubDate>Wed, 04 Oct 2023 18:17:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02980v1</guid></item><item><title>T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation</title><link>http://arxiv.org/abs/2310.02977v1</link><description>Recent methods in text-to-3D leverage powerful pretrained diffusion models tooptimize NeRF. Notably, these methods are able to produce high-quality 3Dscenes without training on 3D data. Due to the open-ended nature of the task,most studies evaluate their results with subjective case studies and userexperiments, thereby presenting a challenge in quantitatively addressing thequestion: How has current progress in Text-to-3D gone so far? In this paper, weintroduce T$^3$Bench, the first comprehensive text-to-3D benchmark containingdiverse text prompts of three increasing complexity levels that are speciallydesigned for 3D generation. To assess both the subjective quality and the textalignment, we propose two automatic metrics based on multi-view images producedby the 3D contents. The quality metric combines multi-view text-image scoresand regional convolution to detect quality and view inconsistency. Thealignment metric uses multi-view captioning and Large Language Model (LLM)evaluation to measure text-3D consistency. Both metrics closely correlate withdifferent dimensions of human judgments, providing a paradigm for efficientlyevaluating text-to-3D models. The benchmarking results, shown in Fig. 1, revealperformance differences among six prevalent text-to-3D methods. Our analysisfurther highlights the common struggles for current methods on generatingsurroundings and multi-object scenes, as well as the bottleneck of leveraging2D guidance for 3D generation. Our project page is available at:https://t3bench.com.</description><author>Yuze He, Yushi Bai, Matthieu Lin, Wang Zhao, Yubin Hu, Jenny Sheng, Ran Yi, Juanzi Li, Yong-Jin Liu</author><pubDate>Wed, 04 Oct 2023 18:12:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02977v1</guid></item><item><title>Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits</title><link>http://arxiv.org/abs/2310.02975v1</link><description>Heavy-tailed distributions naturally arise in many settings, from finance totelecommunications. While regret minimization under sub-Gaussian or boundedsupport rewards has been widely studied, learning on heavy-tailed distributionsonly gained popularity over the last decade. In the stochastic heavy-tailedbandit problem, an agent learns under the assumption that the distributionshave finite moments of maximum order $1+\epsilon$ which are uniformly boundedby a constant $u$, for some $\epsilon \in (0,1]$. To the best of our knowledge,literature only provides algorithms requiring these two quantities as an input.In this paper, we study the stochastic adaptive heavy-tailed bandit, avariation of the standard setting where both $\epsilon$ and $u$ are unknown tothe agent. We show that adaptivity comes at a cost, introducing two lowerbounds on the regret of any adaptive algorithm, implying a higher regret w.r.t.the standard setting. Finally, we introduce a specific distributionalassumption and provide Adaptive Robust UCB, a regret minimization strategymatching the known lower bound for the heavy-tailed MAB problem.</description><author>Gianmarco Genalti, Lupo Marsigli, Nicola Gatti, Alberto Maria Metelli</author><pubDate>Wed, 04 Oct 2023 18:11:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02975v1</guid></item><item><title>Private Ad Modeling with DP-SGD</title><link>http://arxiv.org/abs/2211.11896v3</link><description>A well-known algorithm in privacy-preserving ML is differentially privatestochastic gradient descent (DP-SGD). While this algorithm has been evaluatedon text and image data, it has not been previously applied to ads data, whichare notorious for their high class imbalance and sparse gradient updates. Inthis work we apply DP-SGD to several ad modeling tasks including predictingclick-through rates, conversion rates, and number of conversion events, andevaluate their privacy-utility trade-off on real-world datasets. Our work isthe first to empirically demonstrate that DP-SGD can provide both privacy andutility for ad modeling tasks.</description><author>Carson Denison, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Krishna Giri Narra, Amer Sinha, Avinash V Varadarajan, Chiyuan Zhang</author><pubDate>Wed, 04 Oct 2023 18:10:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11896v3</guid></item><item><title>UniverSLU: Universal Spoken Language Understanding for Diverse Classification and Sequence Generation Tasks with a Single Network</title><link>http://arxiv.org/abs/2310.02973v1</link><description>Recent studies have demonstrated promising outcomes by employing largelanguage models with multi-tasking capabilities. They utilize prompts to guidethe model's behavior and surpass performance of task-specific models. Motivatedby this, we ask: can we build a single model that jointly perform variousspoken language understanding (SLU) tasks? To address this, we utilizepre-trained automatic speech recognition (ASR) models and employ various taskand dataset specifiers as discrete prompts. We demonstrate efficacy of oursingle multi-task learning (MTL) model "UniverSLU" for 12 different speechclassification and sequence generation tasks across 17 datasets and 9languages. Results show that UniverSLU achieves competitive performance andeven surpasses task-specific models. We also conduct preliminary investigationsinto enabling human-interpretable natural phrases instead of task specifiers asdiscrete prompts and test the model's generalization capabilities to newparaphrases.</description><author>Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan Sharma, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe</author><pubDate>Wed, 04 Oct 2023 18:10:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02973v1</guid></item><item><title>Fully Automatic Segmentation of Gross Target Volume and Organs-at-Risk for Radiotherapy Planning of Nasopharyngeal Carcinoma</title><link>http://arxiv.org/abs/2310.02972v1</link><description>Target segmentation in CT images of Head&amp;Neck (H&amp;N) region is challenging dueto low contrast between adjacent soft tissue. The SegRap 2023 challenge hasbeen focused on benchmarking the segmentation algorithms of NasopharyngealCarcinoma (NPC) which would be employed as auto-contouring tools for radiationtreatment planning purposes. We propose a fully-automatic framework and developtwo models for a) segmentation of 45 Organs at Risk (OARs) and b) two GrossTumor Volumes (GTVs). To this end, we preprocess the image volumes byharmonizing the intensity distributions and then automatically cropping thevolumes around the target regions. The preprocessed volumes were employed totrain a standard 3D U-Net model for each task, separately. Our method tooksecond place for each of the tasks in the validation phase of the challenge.The proposed framework is available at https://github.com/Astarakee/segrap2023</description><author>Mehdi Astaraki, Simone Bendazzoli, Iuliana Toma-Dasu</author><pubDate>Wed, 04 Oct 2023 18:10:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02972v1</guid></item><item><title>Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model</title><link>http://arxiv.org/abs/2310.02971v1</link><description>Prompting and adapter tuning have emerged as efficient alternatives tofine-tuning (FT) methods. However, existing studies on speech prompting focusedon classification tasks and failed on more complex sequence generation tasks.Besides, adapter tuning is primarily applied with a focus on encoder-onlyself-supervised models. Our experiments show that prompting on Wav2Seq, aself-supervised encoder-decoder model, surpasses previous works in sequencegeneration tasks. It achieves a remarkable 53% relative improvement in worderror rate for ASR and a 27% in F1 score for slot filling. Additionally,prompting competes with the FT method in the low-resource scenario. Moreover,we show the transferability of prompting and adapter tuning on Wav2Seq incross-lingual ASR. When limited trainable parameters are involved, promptingand adapter tuning consistently outperform conventional FT across 7 languages.Notably, in the low-resource scenario, prompting consistently outperformsadapter tuning.</description><author>Kai-Wei Chang, Ming-Hsin Chen, Yun-Ping Lin, Jing Neng Hsu, Paul Kuo-Ming Huang, Chien-yu Huang, Shang-Wen Li, Hung-yi Lee</author><pubDate>Wed, 04 Oct 2023 18:07:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02971v1</guid></item><item><title>Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space</title><link>http://arxiv.org/abs/2310.02970v1</link><description>Based on the theory of homogeneous spaces we derive \textit{geometricallyoptimal edge attributes} to be used within the flexible message passingframework. We formalize the notion of weight sharing in convolutional networksas the sharing of message functions over point-pairs that should be treatedequally. We define equivalence classes of point-pairs that are identical up toa transformation in the group and derive attributes that uniquely identifythese classes. Weight sharing is then obtained by conditioning messagefunctions on these attributes. As an application of the theory, we develop anefficient equivariant group convolutional network for processing 3D pointclouds. The theory of homogeneous spaces tells us how to do group convolutionswith feature maps over the homogeneous space of positions $\mathbb{R}^3$,position and orientations $\mathbb{R}^3 {\times} S^2$, and the group SE$(3)$itself. Among these, $\mathbb{R}^3 {\times} S^2$ is an optimal choice due tothe ability to represent directional information, which $\mathbb{R}^3$ methodscannot, and it significantly enhances computational efficiency compared toindexing features on the full SE$(3)$ group. We empirically support this claimby reaching state-of-the-art results -- in accuracy and speed -- on threedifferent benchmarks: interatomic potential energy prediction, trajectoryforecasting in N-body systems, and generating molecules via equivariantdiffusion models.</description><author>Erik J Bekkers, Sharvaree Vadgama, Rob D Hesselink, Putri A van der Linden, David W Romero</author><pubDate>Wed, 04 Oct 2023 18:06:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02970v1</guid></item><item><title>Dual Conic Proxies for AC Optimal Power Flow</title><link>http://arxiv.org/abs/2310.02969v1</link><description>In recent years, there has been significant interest in the development ofmachine learning-based optimization proxies for AC Optimal Power Flow (AC-OPF).Although significant progress has been achieved in predicting high-qualityprimal solutions, no existing learning-based approach can provide valid dualbounds for AC-OPF. This paper addresses this gap by training optimizationproxies for a convex relaxation of AC-OPF. Namely, the paper considers asecond-order cone (SOC) relaxation of ACOPF, and proposes a novel dualarchitecture that embeds a fast, differentiable (dual) feasibility recovery,thus providing valid dual bounds. The paper combines this new architecture witha self-supervised learning scheme, which alleviates the need for costlytraining data generation. Extensive numerical experiments on medium- andlarge-scale power grids demonstrate the efficiency and scalability of theproposed methodology.</description><author>Guancheng Qiu, Mathieu Tanneau, Pascal Van Hentenryck</author><pubDate>Wed, 04 Oct 2023 18:06:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02969v1</guid></item><item><title>Co-modeling the Sequential and Graphical Route for Peptide</title><link>http://arxiv.org/abs/2310.02964v1</link><description>Peptides are formed by the dehydration condensation of multiple amino acids.The primary structure of a peptide can be represented either as an amino acidsequence or as a molecular graph consisting of atoms and chemical bonds.Previous studies have indicated that deep learning routes specific tosequential and graphical peptide forms exhibit comparable performance ondownstream tasks. Despite the fact that these models learn representations ofthe same modality of peptides, we find that they explain their predictionsdifferently. Considering sequential and graphical models as two experts makinginferences from different perspectives, we work on fusing expert knowledge toenrich the learned representations for improving the discriminativeperformance. To achieve this, we propose a peptide co-modeling method, RepCon,which employs a contrastive learning-based framework to enhance the mutualinformation of representations from decoupled sequential and graphicalend-to-end models. It considers representations from the sequential encoder andthe graphical encoder for the same peptide sample as a positive pair and learnsto enhance the consistency of representations between positive sample pairs andto repel representations between negative pairs. Empirical studies of RepConand other co-modeling methods are conducted on open-source discriminativedatasets, including aggregation propensity, retention time, antimicrobialpeptide prediction, and family classification from Peptide Database. Ourresults demonstrate the superiority of the co-modeling approach overindependent modeling, as well as the superiority of RepCon over other methodsunder the co-modeling framework. In addition, the attribution on RepCon furthercorroborates the validity of the approach at the level of model explanation.</description><author>Zihan Liu, Ge Wang, Jiaqi Wang, Jiangbin Zheng, Stan Z. Li</author><pubDate>Wed, 04 Oct 2023 17:58:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02964v1</guid></item><item><title>DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models</title><link>http://arxiv.org/abs/2309.14509v2</link><description>Computation in a typical Transformer-based large language model (LLM) can becharacterized by batch size, hidden dimension, number of layers, and sequencelength. Until now, system works for accelerating LLM training have focused onthe first three dimensions: data parallelism for batch size, tensor parallelismfor hidden size and pipeline parallelism for model depth or layers. Thesewidely studied forms of parallelism are not targeted or optimized for longsequence Transformer models. Given practical application needs for longsequence LLM, renewed attentions are being drawn to sequence parallelism.However, existing works in sequence parallelism are constrained bymemory-communication inefficiency, limiting their scalability to long sequencelarge models. In this work, we introduce DeepSpeed-Ulysses, a novel, portableand effective methodology for enabling highly efficient and scalable LLMtraining with extremely long sequence length. DeepSpeed-Ulysses at its corepartitions input data along the sequence dimension and employs an efficientall-to-all collective communication for attention computation. Theoreticalcommunication analysis shows that whereas other methods incur communicationoverhead as sequence length increases, DeepSpeed-Ulysses maintains constantcommunication volume when sequence length and compute devices are increasedproportionally. Furthermore, experimental evaluations show thatDeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than theexisting method SOTA baseline.</description><author>Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, Yuxiong He</author><pubDate>Wed, 04 Oct 2023 17:51:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14509v2</guid></item><item><title>CoDA: Collaborative Novel Box Discovery and Cross-modal Alignment for Open-vocabulary 3D Object Detection</title><link>http://arxiv.org/abs/2310.02960v1</link><description>Open-vocabulary 3D Object Detection (OV-3DDet) aims to detect objects from anarbitrary list of categories within a 3D scene, which remains seldom exploredin the literature. There are primarily two fundamental problems in OV-3DDet,i.e., localizing and classifying novel objects. This paper aims at addressingthe two problems simultaneously via a unified framework, under the condition oflimited base categories. To localize novel 3D objects, we propose an effective3D Novel Object Discovery strategy, which utilizes both the 3D box geometrypriors and 2D semantic open-vocabulary priors to generate pseudo box labels ofthe novel objects. To classify novel object boxes, we further develop across-modal alignment module based on discovered novel boxes, to align featurespaces between 3D point cloud and image/text modalities. Specifically, thealignment process contains a class-agnostic and a class-discriminativealignment, incorporating not only the base objects with annotations but alsothe increasingly discovered novel objects, resulting in an iteratively enhancedalignment. The novel box discovery and crossmodal alignment are jointly learnedto collaboratively benefit each other. The novel object discovery can directlyimpact the cross-modal alignment, while a better feature alignment can, inturn, boost the localization capability, leading to a unified OV-3DDetframework, named CoDA, for simultaneous novel object localization andclassification. Extensive experiments on two challenging datasets (i.e.,SUN-RGBD and ScanNet) demonstrate the effectiveness of our method and also showa significant mAP improvement upon the best-performing alternative method by80%. Codes and pre-trained models are released on the project page.</description><author>Yang Cao, Yihan Zeng, Hang Xu, Dan Xu</author><pubDate>Wed, 04 Oct 2023 17:50:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02960v1</guid></item><item><title>Credit card score prediction using machine learning models: A new dataset</title><link>http://arxiv.org/abs/2310.02956v1</link><description>The use of credit cards has recently increased, creating an essential needfor credit card assessment methods to minimize potential risks. This studyinvestigates the utilization of machine learning (ML) models for credit carddefault prediction system. The main goal here is to investigate thebest-performing ML model for new proposed credit card scoring dataset. This newdataset includes credit card transaction histories and customer profiles, isproposed and tested using a variety of machine learning algorithms, includinglogistic regression, decision trees, random forests, multi layer perceptron(MLP) neural network, XGBoost, and LightGBM. To prepare the data for machinelearning models, we perform data pre-proccessing, feature extraction, featureselection, and data balancing techniques. Experimental results demonstrate thatMLP outperforms logistic regression, decision trees, random forests, LightGBM,and XGBoost in terms of predictive performance in true positive rate, achievingan impressive area under the curve (AUC) of 86.7% and an accuracy rate of91.6%, with a recall rate exceeding 80%. These results indicate the superiorityof MLP in predicting the default customers and assessing the potential risks.Furthermore, they help banks and other financial institutions in predictingloan defaults at an earlier stage.</description><author>Anas Arram, Masri Ayob, Musatafa Abbas Abbood Albadr, Alaa Sulaiman, Dheeb Albashish</author><pubDate>Wed, 04 Oct 2023 17:46:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02956v1</guid></item><item><title>DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning</title><link>http://arxiv.org/abs/2310.02954v1</link><description>Recent advances in natural language processing, primarily propelled by LargeLanguage Models (LLMs), have showcased their remarkable capabilities groundedin in-context learning. A promising avenue for guiding LLMs in intricatereasoning tasks involves the utilization of intermediate reasoning steps withinthe Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge liesin the effective selection of exemplars for facilitating in-context learning.In this study, we introduce a framework that leverages Dual Queries andLow-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplarsfor in-context learning. Dual Queries first query LLM to obtain LLM-generatedknowledge such as CoT, then query the retriever to obtain the final exemplarsvia both question and the knowledge. Moreover, for the second query, LoReemploys dimensionality reduction techniques to refine exemplar selection,ensuring close alignment with the input question's knowledge. Through extensiveexperiments, we demonstrate that DQ-LoRe significantly outperforms priorstate-of-the-art methods in the automatic selection of exemplars for GPT-4,enhancing performance from 92.5\% to 94.2\%. Our comprehensive analysis furtherreveals that DQ-LoRe consistently outperforms retrieval-based approaches interms of both performance and adaptability, especially in scenarioscharacterized by distribution shifts. DQ-LoRe pushes the boundaries ofin-context learning and opens up new avenues for addressing complex reasoningchallenges. We will release the code soon.</description><author>Jiong Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang</author><pubDate>Wed, 04 Oct 2023 17:44:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02954v1</guid></item><item><title>JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning</title><link>http://arxiv.org/abs/2310.02953v1</link><description>Instruction tuning has emerged as a crucial process for harnessing thecapabilities of large language models (LLMs) by providing explicit taskinstructions, leading to improved performance in various tasks. However,prevalent text-to-text instruction tuning (TextTuning) methods suffer fromlimitations in generalization, robustness, and controllability due to theambiguity and lack of explicit structure in tasks. In this paper, we proposeJsonTuning, a novel structure-to-structure approach for instruction tuning. Byleveraging the versatility and structured nature of JSON to represent tasks,JsonTuning enhances generalization by helping the model understand essentialtask elements and their relations, improves robustness by minimizing ambiguity,and increases controllability by providing explicit control over the output. Weconduct a comprehensive comparative study with diverse language models andevaluation benchmarks. Experimental results show that JsonTuning outperformsTextTuning in various applications, showcasing improved performance,adaptability, robustness, and controllability. By overcoming the limitations ofTextTuning, JsonTuning demonstrates significant potential for more effectiveand reliable LLMs capable of handling diverse scenarios.</description><author>Chang Gao, Wenxuan Zhang, Guizhen Chen, Wai Lam</author><pubDate>Wed, 04 Oct 2023 17:44:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02953v1</guid></item><item><title>A Fisher-Rao gradient flow for entropy-regularised Markov decision processes in Polish spaces</title><link>http://arxiv.org/abs/2310.02951v1</link><description>We study the global convergence of a Fisher-Rao policy gradient flow forinfinite-horizon entropy-regularised Markov decision processes with Polishstate and action space. The flow is a continuous-time analogue of a policymirror descent method. We establish the global well-posedness of the gradientflow and demonstrate its exponential convergence to the optimal policy.Moreover, we prove the flow is stable with respect to gradient evaluation,offering insights into the performance of a natural policy gradient flow withlog-linear policy parameterisation. To overcome challenges stemming from thelack of the convexity of the objective function and the discontinuity arisingfrom the entropy regulariser, we leverage the performance difference lemma andthe duality relationship between the gradient and mirror descent flows.</description><author>Bekzhan Kerimkulov, James-Michael Leahy, David Siska, Lukasz Szpruch, Yufei Zhang</author><pubDate>Wed, 04 Oct 2023 17:41:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02951v1</guid></item><item><title>Estimation of Models with Limited Data by Leveraging Shared Structure</title><link>http://arxiv.org/abs/2310.02864v1</link><description>Modern data sets, such as those in healthcare and e-commerce, are oftenderived from many individuals or systems but have insufficient data from eachsource alone to separately estimate individual, often high-dimensional, modelparameters. If there is shared structure among systems however, it may bepossible to leverage data from other systems to help estimate individualparameters, which could otherwise be non-identifiable. In this paper, we assumesystems share a latent low-dimensional parameter space and propose a method forrecovering $d$-dimensional parameters for $N$ different linear systems, evenwhen there are only $T&lt;d$ observations per system. To do so, we develop athree-step algorithm which estimates the low-dimensional subspace spanned bythe systems' parameters and produces refined parameter estimates within thesubspace. We provide finite sample subspace estimation error guarantees for ourproposed method. Finally, we experimentally validate our method on simulationswith i.i.d. regression data and as well as correlated time series data.</description><author>Maryann Rui, Thibaut Horel, Munther Dahleh</author><pubDate>Wed, 04 Oct 2023 15:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02864v1</guid></item><item><title>Conformal Predictions for Longitudinal Data</title><link>http://arxiv.org/abs/2310.02863v1</link><description>We introduce Longitudinal Predictive Conformal Inference (LPCI), a noveldistribution-free conformal prediction algorithm for longitudinal data. Currentconformal prediction approaches for time series data predominantly focus on theunivariate setting, and thus lack cross-sectional coverage when appliedindividually to each time series in a longitudinal dataset. The currentstate-of-the-art for longitudinal data relies on creating infinitely-wideprediction intervals to guarantee both cross-sectional and asymptoticlongitudinal coverage. The proposed LPCI method addresses this by ensuring thatboth longitudinal and cross-sectional coverages are guaranteed withoutresorting to infinitely wide intervals. In our approach, we model the residualdata as a quantile fixed-effects regression problem, constructing predictionintervals with a trained quantile regressor. Our extensive experimentsdemonstrate that LPCI achieves valid cross-sectional coverage and outperformsexisting benchmarks in terms of longitudinal coverage rates. Theoretically, weestablish LPCI's asymptotic coverage guarantees for both dimensions, withfinite-width intervals. The robust performance of LPCI in generating reliableprediction intervals for longitudinal data underscores its potential for broadapplications, including in medicine, finance, and supply chain management.</description><author>Devesh Batra, Salvatore Mercuri, Raad Khraishi</author><pubDate>Wed, 04 Oct 2023 15:51:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02863v1</guid></item><item><title>FG-NeRF: Flow-GAN based Probabilistic Neural Radiance Field for Independence-Assumption-Free Uncertainty Estimation</title><link>http://arxiv.org/abs/2309.16364v2</link><description>Neural radiance fields with stochasticity have garnered significant interestby enabling the sampling of plausible radiance fields and quantifyinguncertainty for downstream tasks. Existing works rely on the independenceassumption of points in the radiance field or the pixels in input views toobtain tractable forms of the probability density function. However, thisassumption inadvertently impacts performance when dealing with intricategeometry and texture. In this work, we propose an independence-assumption-freeprobabilistic neural radiance field based on Flow-GAN. By combining thegenerative capability of adversarial learning and the powerful expressivity ofnormalizing flow, our method explicitly models the density-radiancedistribution of the whole scene. We represent our probabilistic NeRF as amean-shifted probabilistic residual neural model. Our model is trained withoutan explicit likelihood function, thereby avoiding the independence assumption.Specifically, We downsample the training images with different strides andcenters to form fixed-size patches which are used to train the generator withpatch-based adversarial learning. Through extensive experiments, our methoddemonstrates state-of-the-art performance by predicting lower rendering errorsand more reliable uncertainty on both synthetic and real-world datasets.</description><author>Songlin Wei, Jiazhao Zhang, Yang Wang, Fanbo Xiang, Hao Su, He Wang</author><pubDate>Wed, 04 Oct 2023 15:51:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16364v2</guid></item><item><title>A novel asymmetrical autoencoder with a sparsifying discrete cosine Stockwell transform layer for gearbox sensor data compression</title><link>http://arxiv.org/abs/2310.02862v1</link><description>The lack of an efficient compression model remains a challenge for thewireless transmission of gearbox data in non-contact gear fault diagnosisproblems. In this paper, we present a signal-adaptive asymmetrical autoencoderwith a transform domain layer to compress sensor signals. First, a new discretecosine Stockwell transform (DCST) layer is introduced to replace linear layersin a multi-layer autoencoder. A trainable filter is implemented in the DCSTdomain by utilizing the multiplication property of the convolution. A trainablehard-thresholding layer is applied to reduce redundant data in the DCST layerto make the feature map sparse. In comparison to the linear layer, the DCSTlayer reduces the number of trainable parameters and improves the accuracy ofdata reconstruction. Second, training the autoencoder with a sparsifying DCSTlayer only requires a small number of datasets. The proposed method is superiorto other autoencoder-based methods on the University of Connecticut (UoC) andSoutheast University (SEU) gearbox datasets, as the average quality score isimproved by 2.00% at the lowest and 32.35% at the highest with a limited numberof training samples</description><author>Xin Zhu, Daoguang Yang, Hongyi Pan, Hamid Reza Karimi, Didem Ozevin, Ahmet Enis Cetin</author><pubDate>Wed, 04 Oct 2023 15:50:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02862v1</guid></item><item><title>Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection</title><link>http://arxiv.org/abs/2310.02861v1</link><description>Graph-level anomaly detection has gained significant attention as it findsmany applications in various domains, such as cancer diagnosis and enzymeprediction. However, existing methods fail to capture the underlying propertiesof graph anomalies, resulting in unexplainable framework design andunsatisfying performance. In this paper, we take a step back and re-investigatethe spectral differences between anomalous and normal graphs. Our mainobservation shows a significant disparity in the accumulated spectral energybetween these two classes. Moreover, we prove that the accumulated spectralenergy of the graph signal can be represented by its Rayleigh Quotient,indicating that the Rayleigh Quotient is a driving factor behind the anomalousproperties of graphs. Motivated by this, we propose Rayleigh Quotient GraphNeural Network (RQGNN), the first spectral GNN for graph-level anomalydetection, providing a new perspective on exploring the inherent spectralfeatures of anomalous graphs. Specifically, we introduce a novel framework thatconsists of two components: the Rayleigh Quotient learning component (RQL) andChebyshev Wavelet GNN with RQ-pooling (CWGNN-RQ). RQL explicitly captures theRayleigh Quotient of graphs and CWGNN-RQ implicitly explores the spectral spaceof graphs. Extensive experiments on 10 real-world datasets show that RQGNNoutperforms the best rival by 6.74% in Macro-F1 score and 1.44% in AUC,demonstrating the effectiveness of our framework.</description><author>Xiangyu Dong, Xingyi Zhang, Sibo Wang</author><pubDate>Wed, 04 Oct 2023 15:47:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02861v1</guid></item><item><title>On the efficiency of Stochastic Quasi-Newton Methods for Deep Learning</title><link>http://arxiv.org/abs/2205.09121v2</link><description>While first-order methods are popular for solving optimization problems thatarise in large-scale deep learning problems, they come with some acutedeficiencies. To diminish such shortcomings, there has been recent interest inapplying second-order methods such as quasi-Newton based methods whichconstruct Hessians approximations using only gradient information. The mainfocus of our work is to study the behaviour of stochastic quasi-Newtonalgorithms for training deep neural networks. We have analyzed the performanceof two well-known quasi-Newton updates, the limited memoryBroyden-Fletcher-Goldfarb-Shanno (BFGS) and the Symmetric Rank One (SR1). Thisstudy fills a gap concerning the real performance of both updates and analyzeswhether more efficient training is obtained when using the more robust BFGSupdate or the cheaper SR1 formula which allows for indefinite Hessianapproximations and thus can potentially help to better navigate thepathological saddle points present in the non-convex loss functions found indeep learning. We present and discuss the results of an extensive experimentalstudy which includes the effect of batch normalization and network'sarchitecture, the limited memory parameter, the batch size and the type ofsampling strategy. we show that stochastic quasi-Newton optimizers areefficient and able to outperform in some instances the well-known first-orderAdam optimizer run with the optimal combination of its numeroushyperparameters.</description><author>Mahsa Yousefi, Angeles Martinez</author><pubDate>Wed, 04 Oct 2023 15:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.09121v2</guid></item><item><title>Multi-Resolution Fusion for Fully Automatic Cephalometric Landmark Detection</title><link>http://arxiv.org/abs/2310.02855v1</link><description>Cephalometric landmark detection on lateral skull X-ray images plays acrucial role in the diagnosis of certain dental diseases. Accurate andeffective identification of these landmarks presents a significant challenge.Based on extensive data observations and quantitative analyses, we discoveredthat visual features from different receptive fields affect the detectionaccuracy of various landmarks differently. As a result, we employed an imagepyramid structure, integrating multiple resolutions as input to train a seriesof models with different receptive fields, aiming to achieve the optimalfeature combination for each landmark. Moreover, we applied several dataaugmentation techniques during training to enhance the model's robustnessacross various devices and measurement alternatives. We implemented this methodin the Cephalometric Landmark Detection in Lateral X-ray Images 2023 Challengeand achieved a Mean Radial Error (MRE) of 1.62 mm and a Success Detection Rate(SDR) 2.0mm of 74.18% in the final testing phase.</description><author>Dongqian Guo, Wencheng Han</author><pubDate>Wed, 04 Oct 2023 15:42:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02855v1</guid></item><item><title>How to fix a broken confidence estimator: Evaluating post-hoc methods for selective classification with deep neural networks</title><link>http://arxiv.org/abs/2305.15508v2</link><description>This paper addresses the problem of selective classification for deep neuralnetworks, where a model is allowed to abstain from low-confidence predictionsto avoid potential errors. We focus on so-called post-hoc methods, whichreplace the confidence estimator of a given classifier without retraining ormodifying it, thus being practically appealing. Considering neural networkswith softmax outputs, our goal is to identify the best confidence estimatorthat can be computed directly from the unnormalized logits. This problem ismotivated by the intriguing observation in recent work that many classifiersappear to have a "broken" confidence estimator, in the sense that theirselective classification performance is much worse than what could be expectedby their corresponding accuracies. We perform an extensive experimental studyof many existing and proposed confidence estimators applied to 84 pretrainedImageNet classifiers available from popular repositories. Our results show thata simple $p$-norm normalization of the logits, followed by taking the maximumlogit as the confidence estimator, can lead to considerable gains in selectiveclassification performance, completely fixing the pathological behaviorobserved in many classifiers. As a consequence, the selective classificationperformance of any classifier becomes almost entirely determined by itscorresponding accuracy. Moreover, these results are shown to be consistentunder distribution shift. We also investigate why certain classifiers innatelyhave a good confidence estimator that apparently cannot be improved by post-hocmethods.</description><author>Luís Felipe P. Cattelan, Danilo Silva</author><pubDate>Wed, 04 Oct 2023 15:42:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15508v2</guid></item><item><title>Multi-Domain Causal Representation Learning via Weak Distributional Invariances</title><link>http://arxiv.org/abs/2310.02854v1</link><description>Causal representation learning has emerged as the center of action in causalmachine learning research. In particular, multi-domain datasets present anatural opportunity for showcasing the advantages of causal representationlearning over standard unsupervised representation learning. While recent workshave taken crucial steps towards learning causal representations, they oftenlack applicability to multi-domain datasets due to over-simplifying assumptionsabout the data; e.g. each domain comes from a different single-node perfectintervention. In this work, we relax these assumptions and capitalize on thefollowing observation: there often exists a subset of latents whose certaindistributional properties (e.g., support, variance) remain stable acrossdomains; this property holds when, for example, each domain comes from amulti-node imperfect intervention. Leveraging this observation, we show thatautoencoders that incorporate such invariances can provably identify the stableset of latents from the rest across different settings.</description><author>Kartik Ahuja, Amin Mansouri, Yixin Wang</author><pubDate>Wed, 04 Oct 2023 15:41:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02854v1</guid></item><item><title>PersA-FL: Personalized Asynchronous Federated Learning</title><link>http://arxiv.org/abs/2210.01176v2</link><description>We study the personalized federated learning problem under asynchronousupdates. In this problem, each client seeks to obtain a personalized model thatsimultaneously outperforms local and global models. We consider twooptimization-based frameworks for personalization: (i) Model-AgnosticMeta-Learning (MAML) and (ii) Moreau Envelope (ME). MAML involves learning ajoint model adapted for each client through fine-tuning, whereas ME requires abi-level optimization problem with implicit gradients to enforcepersonalization via regularized losses. We focus on improving the scalabilityof personalized federated learning by removing the synchronous communicationassumption. Moreover, we extend the studied function class by removingboundedness assumptions on the gradient norm. Our main technical contributionis a unified proof for asynchronous federated learning with bounded stalenessthat we apply to MAML and ME personalization frameworks. For the smooth andnon-convex functions class, we show the convergence of our method to afirst-order stationary point. We illustrate the performance of our method andits tolerance to staleness through experiments for classification tasks overheterogeneous datasets.</description><author>Mohammad Taha Toghani, Soomin Lee, César A. Uribe</author><pubDate>Wed, 04 Oct 2023 15:41:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.01176v2</guid></item><item><title>Magicremover: Tuning-free Text-guided Image inpainting with Diffusion Models</title><link>http://arxiv.org/abs/2310.02848v1</link><description>Image inpainting aims to fill in the missing pixels with visually coherentand semantically plausible content. Despite the great progress brought fromdeep generative models, this task still suffers from i. the difficulties inlarge-scale realistic data collection and costly model training; and ii. theintrinsic limitations in the traditionally user-defined binary masks on objectswith unclear boundaries or transparent texture. In this paper, we proposeMagicRemover, a tuning-free method that leverages the powerful diffusion modelsfor text-guided image inpainting. We introduce an attention guidance strategyto constrain the sampling process of diffusion models, enabling the erasing ofinstructed areas and the restoration of occluded content. We further propose aclassifier optimization algorithm to facilitate the denoising stability withinless sampling steps. Extensive comparisons are conducted among our MagicRemoverand state-of-the-art methods including quantitative evaluation and user study,demonstrating the significant improvement of MagicRemover on high-quality imageinpainting. We will release our code at https://github.com/exisas/Magicremover.</description><author>Siyuan Yang, Lu Zhang, Liqian Ma, Yu Liu, JingJing Fu, You He</author><pubDate>Wed, 04 Oct 2023 15:34:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02848v1</guid></item><item><title>Explaining $\mathcal{ELH}$ Concept Descriptions through Counterfactual Reasoning</title><link>http://arxiv.org/abs/2301.05109v2</link><description>Knowledge bases are widely used for information management, enablinghigh-impact applications such as web search, question answering, and naturallanguage processing. They also serve as the backbone for automatic decisionsystems, e.g., for medical diagnostics and credit scoring. As stakeholdersaffected by these decisions would like to understand their situation and verifyhow fair the decisions are, a number of explanation approaches have beenproposed. An intrinsically transparent way to do classification is by usingconcepts in description logics. However, these concepts can become long anddifficult to fathom for non-experts, even when verbalized. One solution is toemploy counterfactuals to answer the question, ``How must feature values bechanged to obtain a different classification?'' By focusing on the minimalfeature changes, the explanations are short, human-friendly, and provide aclear path of action regarding the change in prediction. While previous workinvestigated counterfactuals for tabular data, in this paper, we transfer thenotion of counterfactuals to knowledge bases and the description logic$\mathcal{ELH}$. Our approach starts by generating counterfactual candidatesfrom concepts, followed by selecting the candidates requiring the fewestfeature changes as counterfactuals. When multiple counterfactuals exist, werank them based on the likeliness of their feature combinations. We evaluateour method by conducting a user survey to determine which counterfactualcandidates participants prefer for explanation.</description><author>Leonie Nora Sieger, Stefan Heindorf, Yasir Mahmood, Lukas Blübaum, Axel-Cyrille Ngonga Ngomo</author><pubDate>Wed, 04 Oct 2023 15:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.05109v2</guid></item><item><title>Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks</title><link>http://arxiv.org/abs/2305.17212v2</link><description>Weight decay can significantly impact the optimization dynamics of deepneural networks. In certain situations the effects of weight decay and gradientupdates on the magnitude of a parameter vector cancel out on average, forming astate known as equilibrium. This causes the expected rotation of the vector ineach update to remain constant along with its magnitude. Importantly,equilibrium can arise independently for the weight vectors of different layersand neurons. These equilibria are highly homogeneous for some optimizer andnormalization configurations, effectively balancing the average rotation--aproxy for the effective learning rate--across network components. In this workwe explore the equilibrium states of multiple optimizers including AdamW andSGD with momentum, providing insights into interactions between the learningrate, weight decay, initialization, normalization and learning rate schedule.We show how rotational equilibrium can be enforced throughout training,eliminating the chaotic transient phase corresponding to the transition towardsequilibrium, thus simplifying the training dynamics. Finally, we show thatrotational behavior may play a key role in the effectiveness of AdamW comparedto Adam with L2-regularization, the performance of different normalizationlayers, and the need for learning rate warmup.</description><author>Atli Kosson, Bettina Messmer, Martin Jaggi</author><pubDate>Wed, 04 Oct 2023 15:28:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17212v2</guid></item><item><title>MUSTANG: Multi-Stain Self-Attention Graph Multiple Instance Learning Pipeline for Histopathology Whole Slide Images</title><link>http://arxiv.org/abs/2309.10650v2</link><description>Whole Slide Images (WSIs) present a challenging computer vision task due totheir gigapixel size and presence of numerous artefacts. Yet they are avaluable resource for patient diagnosis and stratification, often representingthe gold standard for diagnostic tasks. Real-world clinical datasets tend tocome as sets of heterogeneous WSIs with labels present at the patient-level,with poor to no annotations. Weakly supervised attention-based multipleinstance learning approaches have been developed in recent years to addressthese challenges, but can fail to resolve both long and short-rangedependencies. Here we propose an end-to-end multi-stain self-attention graph(MUSTANG) multiple instance learning pipeline, which is designed to solve aweakly-supervised gigapixel multi-image classification task, where the label isassigned at the patient-level, but no slide-level labels or region annotationsare available. The pipeline uses a self-attention based approach by restrictingthe operations to a highly sparse k-Nearest Neighbour Graph of embedded WSIpatches based on the Euclidean distance. We show this approach achieves astate-of-the-art F1-score/AUC of 0.89/0.92, outperforming the widely used CLAMmodel. Our approach is highly modular and can easily be modified to suitdifferent clinical datasets, as it only requires a patient-level label withoutannotations and accepts WSI sets of different sizes, as the graphs can be ofvarying sizes and structures. The source code can be found athttps://github.com/AmayaGS/MUSTANG.</description><author>Amaya Gallagher-Syed, Luca Rossi, Felice Rivellese, Costantino Pitzalis, Myles Lewis, Michael Barnes, Gregory Slabaugh</author><pubDate>Wed, 04 Oct 2023 15:24:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10650v2</guid></item><item><title>Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation</title><link>http://arxiv.org/abs/2310.02842v1</link><description>Large Language Models (LLMs) have the ability to solve a variety of tasks,such as text summarization and mathematical questions, just out of the box, butthey are often trained with a single task in mind. Due to high computationalcosts, the current trend is to use prompt instruction tuning to better adjustmonolithic, pretrained LLMs for new -- but often individual -- downstreamtasks. Thus, how one would expand prompt tuning to handle -- concomitantly --heterogeneous tasks and data distributions is a widely open question. Toaddress this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs,associated with smart gating functionality: the latter -- whose design is oneof the contributions of this paper -- can identify relevant skills embedded indifferent groups of prompts and dynamically assign combined experts (i.e.,collection of prompts), based on the target task. Additionally, MoPs areempirically agnostic to any model compression technique applied -- forefficiency reasons -- as well as instruction data source and task composition.In practice, MoPs can simultaneously mitigate prompt training "interference" inmulti-task, multi-source scenarios (e.g., task and data heterogeneity acrosssources), as well as possible implications from model approximations. As ahighlight, MoPs manage to decrease final perplexity from $\sim20\%$ up to$\sim70\%$, as compared to baselines, in the federated scenario, and from $\sim3\%$ up to $\sim30\%$ in the centralized scenario.</description><author>Chen Dun, Mirian Del Carmen Hipolito Garcia, Guoqing Zheng, Ahmed Hassan Awadallah, Anastasios Kyrillidis, Robert Sim</author><pubDate>Wed, 04 Oct 2023 15:11:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02842v1</guid></item><item><title>From Zero to Turbulence: Generative Modeling for 3D Flow Simulation</title><link>http://arxiv.org/abs/2306.01776v2</link><description>Simulations of turbulent flows in 3D are one of the most expensivesimulations in computational fluid dynamics (CFD). Many works have been writtenon surrogate models to replace numerical solvers for fluid flows with faster,learned, autoregressive models. However, the intricacies of turbulence in threedimensions necessitate training these models with very small time steps, whilegenerating realistic flow states requires either long roll-outs with many stepsand significant error accumulation or starting from a known, realistic flowstate - something we aimed to avoid in the first place. Instead, we propose toapproach turbulent flow simulation as a generative task directly learning themanifold of all possible turbulent flow states without relying on any initialflow state. For our experiments, we introduce a challenging 3D turbulencedataset of high-resolution flows and detailed vortex structures caused byvarious objects and derive two novel sample evaluation metrics for turbulentflows. On this dataset, we show that our generative model captures thedistribution of turbulent flows caused by unseen objects and generateshigh-quality, realistic samples amenable for downstream applications withoutaccess to any initial state.</description><author>Marten Lienen, David Lüdke, Jan Hansen-Palmus, Stephan Günnemann</author><pubDate>Wed, 04 Oct 2023 15:02:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01776v2</guid></item><item><title>Delving into CLIP latent space for Video Anomaly Recognition</title><link>http://arxiv.org/abs/2310.02835v1</link><description>We tackle the complex problem of detecting and recognising anomalies insurveillance videos at the frame level, utilising only video-level supervision.We introduce the novel method AnomalyCLIP, the first to combine Large Languageand Vision (LLV) models, such as CLIP, with multiple instance learning forjoint video anomaly detection and classification. Our approach specificallyinvolves manipulating the latent CLIP feature space to identify the normalevent subspace, which in turn allows us to effectively learn text-drivendirections for abnormal events. When anomalous frames are projected onto thesedirections, they exhibit a large feature magnitude if they belong to aparticular class. We also introduce a computationally efficient Transformerarchitecture to model short- and long-term temporal dependencies betweenframes, ultimately producing the final anomaly score and class predictionprobabilities. We compare AnomalyCLIP against state-of-the-art methodsconsidering three major anomaly detection benchmarks, i.e. ShanghaiTech,UCF-Crime, and XD-Violence, and empirically show that it outperforms baselinesin recognising video anomalies.</description><author>Luca Zanella, Benedetta Liberatori, Willi Menapace, Fabio Poiesi, Yiming Wang, Elisa Ricci</author><pubDate>Wed, 04 Oct 2023 15:01:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02835v1</guid></item><item><title>Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness</title><link>http://arxiv.org/abs/2310.02832v1</link><description>Effective OOD detection is crucial for reliable machine learning models, yetmost current methods are limited in practical use due to requirements likeaccess to training data or intervention in training. We present a novel methodfor detecting OOD data in deep neural networks based on transformationsmoothness between intermediate layers of a network (BLOOD), which isapplicable to pre-trained models without access to training data. BLOODutilizes the tendency of between-layer representation transformations ofin-distribution (ID) data to be smoother than the corresponding transformationsof OOD data, a property that we also demonstrate empirically for Transformernetworks. We evaluate BLOOD on several text classification tasks withTransformer networks and demonstrate that it outperforms methods withcomparable resource requirements. Our analysis also suggests that when learningsimpler tasks, OOD data transformations maintain their original sharpness,whereas sharpness increases with more complex tasks.</description><author>Fran Jelenić, Josip Jukić, Martin Tutek, Mate Puljiz, Jan Šnajder</author><pubDate>Wed, 04 Oct 2023 14:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02832v1</guid></item><item><title>All Sizes Matter: Improving Volumetric Brain Segmentation on Small Lesions</title><link>http://arxiv.org/abs/2310.02829v1</link><description>Brain metastases (BMs) are the most frequently occurring brain tumors. Thetreatment of patients having multiple BMs with stereo tactic radiosurgerynecessitates accurate localization of the metastases. Neural networks canassist in this time-consuming and costly task that is typically performed byhuman experts. Particularly challenging is the detection of small lesions sincethey are often underrepresented in exist ing approaches. Yet, lesion detectionis equally important for all sizes. In this work, we develop an ensemble ofneural networks explicitly fo cused on detecting and segmenting small BMs. Toaccomplish this task, we trained several neural networks focusing on individualaspects of the BM segmentation problem: We use blob loss that specificallyaddresses the imbalance of lesion instances in terms of size and texture andis, therefore, not biased towards larger lesions. In addition, a model using asubtraction sequence between the T1 and T1 contrast-enhanced sequence focuseson low-contrast lesions. Furthermore, we train additional models only on smalllesions. Our experiments demonstrate the utility of the ad ditional blob lossand the subtraction sequence. However, including the specialized small lesionmodels in the ensemble deteriorates segmentation results. We also finddomain-knowledge-inspired postprocessing steps to drastically increase ourperformance in most experiments. Our approach enables us to submit acompetitive challenge entry to the ASNR-MICCAI BraTS Brain Metastasis Challenge2023.</description><author>Ayhan Can Erdur, Daniel Scholz, Josef A. Buchner, Stephanie E. Combs, Daniel Rueckert, Jan C. Peeken</author><pubDate>Wed, 04 Oct 2023 14:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02829v1</guid></item><item><title>Learning to Scale Logits for Temperature-Conditional GFlowNets</title><link>http://arxiv.org/abs/2310.02823v1</link><description>GFlowNets are probabilistic models that learn a stochastic policy thatsequentially generates compositional structures, such as molecular graphs. Theyare trained with the objective of sampling such objects with probabilityproportional to the object's reward. Among GFlowNets, thetemperature-conditional GFlowNets represent a family of policies indexed bytemperature, and each is associated with the correspondingly tempered rewardfunction. The major benefit of temperature-conditional GFlowNets is thecontrollability of GFlowNets' exploration and exploitation through adjustingtemperature. We propose Learning to Scale Logits for temperature-conditionalGFlowNets (LSL-GFN), a novel architectural design that greatly accelerates thetraining of temperature-conditional GFlowNets. It is based on the idea thatpreviously proposed temperature-conditioning approaches introduced numericalchallenges in the training of the deep network because different temperaturesmay give rise to very different gradient profiles and ideal scales of thepolicy's logits. We find that the challenge is greatly reduced if a learnedfunction of the temperature is used to scale the policy's logits directly. Weempirically show that our strategy dramatically improves the performances ofGFlowNets, outperforming other baselines, including reinforcement learning andsampling methods, in terms of discovering diverse modes in multiple biochemicaltasks.</description><author>Minsu Kim, Joohwan Ko, Dinghuai Zhang, Ling Pan, Taeyoung Yun, Woochang Kim, Jinkyoo Park, Yoshua Bengio</author><pubDate>Wed, 04 Oct 2023 14:45:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02823v1</guid></item><item><title>Improving Vision Anomaly Detection with the Guidance of Language Modality</title><link>http://arxiv.org/abs/2310.02821v1</link><description>Recent years have seen a surge of interest in anomaly detection for tacklingindustrial defect detection, event detection, etc. However, existingunsupervised anomaly detectors, particularly those for the vision modality,face significant challenges due to redundant information and sparse latentspace. Conversely, the language modality performs well due to its relativelysingle data. This paper tackles the aforementioned challenges for visionmodality from a multimodal point of view. Specifically, we propose Cross-modalGuidance (CMG), which consists of Cross-modal Entropy Reduction (CMER) andCross-modal Linear Embedding (CMLE), to tackle the redundant information issueand sparse space issue, respectively. CMER masks parts of the raw image andcomputes the matching score with the text. Then, CMER discards irrelevantpixels to make the detector focus on critical contents. To learn a more compactlatent space for the vision anomaly detector, CMLE learns a correlationstructure matrix from the language modality, and then the latent space ofvision modality will be learned with the guidance of the matrix. Thereafter,the vision latent space will get semantically similar images closer. Extensiveexperiments demonstrate the effectiveness of the proposed methods.Particularly, CMG outperforms the baseline that only uses images by 16.81%.Ablation experiments further confirm the synergy among the proposed methods, aseach component depends on the other to achieve optimal performance.</description><author>Dong Chen, Kaihang Pan, Guoming Wang, Yueting Zhuang, Siliang Tang</author><pubDate>Wed, 04 Oct 2023 14:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02821v1</guid></item><item><title>CoBEV: Elevating Roadside 3D Object Detection with Depth and Height Complementarity</title><link>http://arxiv.org/abs/2310.02815v1</link><description>Roadside camera-driven 3D object detection is a crucial task in intelligenttransportation systems, which extends the perception range beyond thelimitations of vision-centric vehicles and enhances road safety. While previousstudies have limitations in using only depth or height information, we findboth depth and height matter and they are in fact complementary. The depthfeature encompasses precise geometric cues, whereas the height feature isprimarily focused on distinguishing between various categories of heightintervals, essentially providing semantic context. This insight motivates thedevelopment of Complementary-BEV (CoBEV), a novel end-to-end monocular 3Dobject detection framework that integrates depth and height to construct robustBEV representations. In essence, CoBEV estimates each pixel's depth and heightdistribution and lifts the camera features into 3D space for lateral fusionusing the newly proposed two-stage complementary feature selection (CFS)module. A BEV feature distillation framework is also seamlessly integrated tofurther enhance the detection accuracy from the prior knowledge of thefusion-modal CoBEV teacher. We conduct extensive experiments on the public 3Ddetection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well asthe private Supremind-Road dataset, demonstrating that CoBEV not only achievesthe accuracy of the new state-of-the-art, but also significantly advances therobustness of previous methods in challenging long-distance scenarios and noisycamera disturbance, and enhances generalization by a large margin inheterologous settings with drastic changes in scene and camera parameters. Forthe first time, the vehicle AP score of a camera model reaches 80% onDAIR-V2X-I in terms of easy mode. The source code will be made publiclyavailable at https://github.com/MasterHow/CoBEV.</description><author>Hao Shi, Chengshan Pang, Jiaming Zhang, Kailun Yang, Yuhao Wu, Huajian Ni, Yining Lin, Rainer Stiefelhagen, Kaiwei Wang</author><pubDate>Wed, 04 Oct 2023 14:38:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02815v1</guid></item><item><title>Expanding Small-Scale Datasets with Guided Imagination</title><link>http://arxiv.org/abs/2211.13976v5</link><description>The power of DNNs relies heavily on the quantity and quality of trainingdata. However, collecting and annotating data on a large scale is oftenexpensive and time-consuming. To address this issue, we explore a new task,termed dataset expansion, aimed at expanding a ready-to-use small dataset byautomatically creating new labeled samples. To this end, we present a GuidedImagination Framework (GIF) that leverages cutting-edge generative models likeDALL-E2 and Stable Diffusion (SD) to "imagine" and create informative new datafrom the input seed data. Specifically, GIF conducts data imagination byoptimizing the latent features of the seed data in the semantically meaningfulspace of the prior model, resulting in the creation of photo-realistic imageswith new content. To guide the imagination towards creating informative samplesfor model training, we introduce two key criteria, i.e., class-maintainedinformation boosting and sample diversity promotion. These criteria areverified to be essential for effective dataset expansion: GIF-SD obtains 13.5%higher model accuracy on natural image datasets than unguided expansion withSD. With these essential criteria, GIF successfully expands small datasets invarious scenarios, boosting model accuracy by 36.9% on average over six naturalimage datasets and by 13.5% on average over three medical datasets. The sourcecode is available at https://github.com/Vanint/DatasetExpansion.</description><author>Yifan Zhang, Daquan Zhou, Bryan Hooi, Kai Wang, Jiashi Feng</author><pubDate>Wed, 04 Oct 2023 14:37:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13976v5</guid></item><item><title>Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms</title><link>http://arxiv.org/abs/2310.02812v1</link><description>Manufacturing is gathering extensive amounts of diverse data, thanks to thegrowing number of sensors and rapid advances in sensing technologies. Among thevarious data types available in SMS settings, time-series data plays a pivotalrole. Hence, TSC emerges is crucial in this domain. The objective of this studyis to fill this gap by providing a rigorous experimental evaluation of the SoTAML and DL algorithms for TSC tasks in manufacturing and industrial settings. Wefirst explored and compiled a comprehensive list of more than 92 SoTAalgorithms from both TSC and manufacturing literature. Following, we selectedthe 36 most representative algorithms from this list. To evaluate theirperformance across various manufacturing classification tasks, we curated a setof 22 manufacturing datasets, representative of different characteristics thatcover diverse manufacturing problems. Subsequently, we implemented andevaluated the algorithms on the manufacturing benchmark datasets, and analyzedthe results for each dataset. Based on the results, ResNet, DrCIF,InceptionTime, and ARSENAL are the top-performing algorithms, boasting anaverage accuracy of over 96.6% across all 22 manufacturing TSC datasets. Thesefindings underscore the robustness, efficiency, scalability, and effectivenessof convolutional kernels in capturing temporal features in time-series data, asthree out of the top four performing algorithms leverage these kernels forfeature extraction. Additionally, LSTM, BiLSTM, and TS-LSTM algorithms deserverecognition for their effectiveness in capturing features within time-seriesdata using RNN-based structures.</description><author>Mojtaba A. Farahani, M. R. McCormick, Ramy Harik, Thorsten Wuest</author><pubDate>Wed, 04 Oct 2023 14:37:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02812v1</guid></item><item><title>Improved Anisotropic Gaussian Filters</title><link>http://arxiv.org/abs/2303.13278v2</link><description>Elongated anisotropic Gaussian filters are used for the orientationestimation of fibers. In cases where computed tomography images are noisy,roughly resolved, and of low contrast, they are the method of choice even ifbeing efficient only in virtual 2D slices. However, minor inaccuracies in theanisotropic Gaussian filters can carry over to the orientation estimation.Therefore, this paper proposes a modified algorithm for 2D anisotropic Gaussianfilters and shows that this improves their precision. Applied to syntheticimages of fiber bundles, it is more accurate and robust to noise. Finally, theeffectiveness of the approach is shown by applying it to real-world images ofsheet molding compounds.</description><author>Alex Keilmann, Michael Godehardt, Ali Moghiseh, Claudia Redenbach, Katja Schladitz</author><pubDate>Wed, 04 Oct 2023 14:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13278v2</guid></item><item><title>A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability</title><link>http://arxiv.org/abs/2310.02807v1</link><description>In the past few years, there has been an explosive surge in the use ofmachine learning (ML) techniques to address combinatorial optimization (CO)problems, especially mixed-integer linear programs (MILPs). Despite theachievements, the limited availability of real-world instances often leads tosub-optimal decisions and biased solver assessments, which motivates a suite ofsynthetic MILP instance generation techniques. However, existing methods eitherrely heavily on expert-designed formulations or struggle to capture the richfeatures of real-world instances. To tackle this problem, we propose G2MILP,which to the best of our knowledge is the first deep generative framework forMILP instances. Specifically, G2MILP represents MILP instances as bipartitegraphs, and applies a masked variational autoencoder to iteratively corrupt andreplace parts of the original graphs to generate new ones. The appealingfeature of G2MILP is that it can learn to generate novel and realistic MILPinstances without prior expert-designed formulations, while preserving thestructures and computational hardness of real-world datasets, simultaneously.Thus the generated instances can facilitate downstream tasks for enhancing MILPsolvers under limited data availability. We design a suite of benchmarks toevaluate the quality of the generated MILP instances. Experiments demonstratethat our method can produce instances that closely resemble real-world datasetsin terms of both structures and computational hardness.</description><author>Zijie Geng, Xijun Li, Jie Wang, Xiao Li, Yongdong Zhang, Feng Wu</author><pubDate>Wed, 04 Oct 2023 14:34:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02807v1</guid></item><item><title>A Data-facilitated Numerical Method for Richards Equation to Model Water Flow Dynamics in Soil</title><link>http://arxiv.org/abs/2310.02806v1</link><description>Root-zone soil moisture monitoring is essential for precision agriculture,smart irrigation, and drought prevention. Modeling the spatiotemporal waterflow dynamics in soil is typically achieved by solving a hydrological model,such as the Richards equation which is a highly nonlinear partial differentialequation (PDE). In this paper, we present a novel data-facilitated numericalmethod for solving the mixed-form Richards equation. This numerical method,which we call the D-GRW (Data-facilitated global Random Walk) method,synergistically integrates adaptive linearization scheme, neural networks, andglobal random walk in a finite volume discretization framework to produceaccurate numerical solutions of the Richards equation with guaranteedconvergence under reasonable assumptions. Through three illustrative examples,we demonstrate and discuss the superior accuracy and mass conservationperformance of our D-GRW method and compare it with benchmark numerical methodsand commercial solver.</description><author>Zeyuan Song, Zheyu Jiang</author><pubDate>Wed, 04 Oct 2023 14:33:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02806v1</guid></item><item><title>Regularised neural networks mimic human insight</title><link>http://arxiv.org/abs/2302.11351v3</link><description>Humans sometimes show sudden improvements in task performance that have beenlinked to moments of insight. Such insight-related performance improvementsappear special because they are preceded by an extended period of impasse, areunusually abrupt, and occur only in some, but not all, learners. Here, we askwhether insight-like behaviour also occurs in artificial neural networkstrained with gradient descent algorithms. We compared learning dynamics inhumans and regularised neural networks in a perceptual decision task thatprovided a hidden opportunity which allowed to solve the task more efficiently.We show that humans tend to discover this regularity through insight, ratherthan gradually. Notably, neural networks with regularised gate modulationclosely mimicked behavioural characteristics of human insights, exhibitingdelay of insight, suddenness and selective occurrence. Analyses of networklearning dynamics revealed that insight-like behaviour crucially depended onnoise added to gradient updates, and was preceded by ``silent knowledge'' thatis initially suppressed by regularised (attentional) gating. This suggests thatinsights can arise naturally from gradual learning, where they reflect thecombined influences of noise, attentional gating and regularisation.</description><author>Anika T. Löwe, Léo Touzo, Paul S. Muhle-Karbe, Andrew M. Saxe, Christopher Summerfield, Nicolas W. Schuck</author><pubDate>Wed, 04 Oct 2023 14:30:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11351v3</guid></item><item><title>DOMINO: A Dual-System for Multi-step Visual Language Reasoning</title><link>http://arxiv.org/abs/2310.02804v1</link><description>Visual language reasoning requires a system to extract text or numbers frominformation-dense images like charts or plots and perform logical or arithmeticreasoning to arrive at an answer. To tackle this task, existing work relies oneither (1) an end-to-end vision-language model trained on a large amount ofdata, or (2) a two-stage pipeline where a captioning model converts the imageinto text that is further read by another large language model to deduce theanswer. However, the former approach forces the model to answer a complexquestion with one single step, and the latter approach is prone to inaccurateor distracting information in the converted text that can confuse the languagemodel. In this work, we propose a dual-system for multi-step multimodalreasoning, which consists of a "System-1" step for visual informationextraction and a "System-2" step for deliberate reasoning. Given an input,System-2 breaks down the question into atomic sub-steps, each guiding System-1to extract the information required for reasoning from the image. Experimentson chart and plot datasets show that our method with a pre-trained System-2module performs competitively compared to prior work on in- andout-of-distribution data. By fine-tuning the System-2 module (LLaMA-2 70B) ononly a small amount of data on multi-step reasoning, the accuracy of our methodis further improved and surpasses the best fully-supervised end-to-end approachby 5.7% and a pipeline approach with FlanPaLM (540B) by 7.5% on a challengingdataset with human-authored questions.</description><author>Peifang Wang, Olga Golovneva, Armen Aghajanyan, Xiang Ren, Muhao Chen, Asli Celikyilmaz, Maryam Fazel-Zarandi</author><pubDate>Wed, 04 Oct 2023 14:29:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02804v1</guid></item><item><title>AQUILA: Communication Efficient Federated Learning with Adaptive Quantization in Device Selection Strategy</title><link>http://arxiv.org/abs/2308.00258v2</link><description>The widespread adoption of Federated Learning (FL), a privacy-preservingdistributed learning methodology, has been impeded by the challenge of highcommunication overheads, typically arising from the transmission of large-scalemodels. Existing adaptive quantization methods, designed to mitigate theseoverheads, operate under the impractical assumption of uniform deviceparticipation in every training round. Additionally, these methods are limitedin their adaptability due to the necessity of manual quantization levelselection and often overlook biases inherent in local devices' data, therebyaffecting the robustness of the global model. In response, this paperintroduces AQUILA (adaptive quantization in device selection strategy), a noveladaptive framework devised to effectively handle these issues, enhancing theefficiency and robustness of FL. AQUILA integrates a sophisticated deviceselection method that prioritizes the quality and usefulness of device updates.Utilizing the exact global model stored by devices, it enables a more precisedevice selection criterion, reduces model deviation, and limits the need forhyperparameter adjustments. Furthermore, AQUILA presents an innovativequantization criterion, optimized to improve communication efficiency whileassuring model convergence. Our experiments demonstrate that AQUILAsignificantly decreases communication costs compared to existing methods, whilemaintaining comparable model performance across diverse non-homogeneous FLsettings, such as Non-IID data and heterogeneous model architectures.</description><author>Zihao Zhao, Yuzhu Mao, Zhenpeng Shi, Yang Liu, Tian Lan, Wenbo Ding, Xiao-Ping Zhang</author><pubDate>Wed, 04 Oct 2023 14:27:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00258v2</guid></item><item><title>Trimap-guided Feature Mining and Fusion Network for Natural Image Matting</title><link>http://arxiv.org/abs/2112.00510v4</link><description>Utilizing trimap guidance and fusing multi-level features are two importantissues for trimap-based matting with pixel-level prediction. To utilize trimapguidance, most existing approaches simply concatenate trimaps and imagestogether to feed a deep network or apply an extra network to extract moretrimap guidance, which meets the conflict between efficiency and effectiveness.For emerging content-based feature fusion, most existing matting methods onlyfocus on local features which lack the guidance of a global feature with strongsemantic information related to the interesting object. In this paper, wepropose a trimap-guided feature mining and fusion network consisting of ourtrimap-guided non-background multi-scale pooling (TMP) module and global-localcontext-aware fusion (GLF) modules. Considering that trimap provides strongsemantic guidance, our TMP module focuses effective feature mining oninteresting objects under the guidance of trimap without extra parameters.Furthermore, our GLF modules use global semantic information of interestingobjects mined by our TMP module to guide an effective global-localcontext-aware multi-level feature fusion. In addition, we build a commoninteresting object matting (CIOM) dataset to advance high-quality imagematting. Particularly, results on the Composition-1k and our CIOM show that ourTMFNet achieves 13% and 25% relative improvement on SAD, respectively, againsta strong baseline with fewer parameters and 14% fewer FLOPs. Experimentalresults on the Composition-1k test set, Alphamatting benchmark, and our CIOMtest set demonstrate that our method outperforms state-of-the-art approaches.Our code and models are available athttps://github.com/Serge-weihao/TMF-Matting.</description><author>Weihao Jiang, Dongdong Yu, Zhaozhi Xie, Yaoyi Li, Zehuan Yuan, Hongtao Lu</author><pubDate>Wed, 04 Oct 2023 14:26:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.00510v4</guid></item><item><title>Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models</title><link>http://arxiv.org/abs/2309.06256v2</link><description>Foundation models, including Vision Language Models (VLMs) and Large LanguageModels (LLMs), possess the $generality$ to handle diverse distributions andtasks, which stems from their extensive pre-training datasets. The fine-tuningof foundation models is a common practice to enhance task performance or alignthe model's behavior with human expectations, allowing them to gain$speciality$. However, the small datasets used for fine-tuning may notadequately cover the diverse distributions and tasks encountered duringpre-training. Consequently, the pursuit of speciality during fine-tuning canlead to a loss of {generality} in the model, which is related to catastrophicforgetting (CF) in deep learning. In this study, we demonstrate this phenomenonin both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNetresults in a loss of generality in handling diverse distributions, andfine-tuning LLMs like Galactica in the medical domain leads to a loss infollowing instructions and common sense. To address the trade-off between the speciality and generality, weinvestigate multiple regularization methods from continual learning, the weightaveraging method (Wise-FT) from out-of-distributional (OOD) generalization,which interpolates parameters between pre-trained and fine-tuned models, andparameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA). Ourfindings show that both continual learning and Wise-ft methods effectivelymitigate the loss of generality, with Wise-FT exhibiting the strongestperformance in balancing speciality and generality.</description><author>Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, Tong Zhang</author><pubDate>Wed, 04 Oct 2023 14:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06256v2</guid></item><item><title>Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning</title><link>http://arxiv.org/abs/2310.00648v2</link><description>Parameter-efficient fine-tuning (PEFT) enables efficient adaptation ofpre-trained language models (PLMs) to specific tasks. By tuning only a minimalset of (extra) parameters, PEFT achieves performance comparable to fullfine-tuning. However, despite its prevalent use, the security implications ofPEFT remain largely unexplored. In this paper, we conduct a pilot studyrevealing that PEFT exhibits unique vulnerability to trojan attacks.Specifically, we present PETA, a novel attack that accounts for downstreamadaptation through bilevel optimization: the upper-level objective embeds thebackdoor into a PLM while the lower-level objective simulates PEFT to retainthe PLM's task-specific performance. With extensive evaluation across a varietyof downstream tasks and trigger designs, we demonstrate PETA's effectiveness interms of both attack success rate and unaffected clean accuracy, even after thevictim user performs PEFT over the backdoored PLM using untainted data.Moreover, we empirically provide possible explanations for PETA's efficacy: thebilevel optimization inherently 'orthogonalizes' the backdoor and PEFT modules,thereby retaining the backdoor throughout PEFT. Based on this insight, weexplore a simple defense that omits PEFT in selected layers of the backdooredPLM and unfreezes a subset of these layers' parameters, which is shown toeffectively neutralize PETA.</description><author>Lauren Hong, Ting Wang</author><pubDate>Wed, 04 Oct 2023 14:21:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00648v2</guid></item><item><title>Variantional autoencoder with decremental information bottleneck for disentanglement</title><link>http://arxiv.org/abs/2303.12959v2</link><description>One major challenge of disentanglement learning with variational autoencodersis the trade-off between disentanglement and reconstruction fidelity. Previousstudies, which increase the information bottleneck during training, tend tolose the constraint of disentanglement, leading to the information diffusionproblem. In this paper, we present a novel framework for disentangledrepresentation learning, DeVAE, which utilizes hierarchical latent spaces withdecreasing information bottlenecks across these spaces. The key innovation ofour approach lies in connecting the hierarchical latent spaces throughdisentanglement-invariant transformations, allowing the sharing ofdisentanglement properties among spaces while maintaining an acceptable levelof reconstruction performance. We demonstrate the effectiveness of DeVAE inachieving a balance between disentanglement and reconstruction through a seriesof experiments and ablation studies on dSprites and Shapes3D datasets. Code isavailable at https://github.com/erow/disentanglement_lib/tree/pytorch#devae.</description><author>Jiantao Wu, Shentong Mo, Xiang Yang, Muhammad Awais, Sara Atito, Xingshen Zhang, Lin Wang, Xiang Yang</author><pubDate>Wed, 04 Oct 2023 14:20:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12959v2</guid></item><item><title>Sparse Autoencoders Find Highly Interpretable Features in Language Models</title><link>http://arxiv.org/abs/2309.08600v3</link><description>One of the roadblocks to a better understanding of neural networks' internalsis \textit{polysemanticity}, where neurons appear to activate in multiple,semantically distinct contexts. Polysemanticity prevents us from identifyingconcise, human-understandable explanations for what neural networks are doinginternally. One hypothesised cause of polysemanticity is\textit{superposition}, where neural networks represent more features than theyhave neurons by assigning features to an overcomplete set of directions inactivation space, rather than to individual neurons. Here, we attempt toidentify those directions, using sparse autoencoders to reconstruct theinternal activations of a language model. These autoencoders learn sets ofsparsely activating features that are more interpretable and monosemantic thandirections identified by alternative approaches, where interpretability ismeasured by automated methods. Moreover, we show that with our learned set offeatures, we can pinpoint the features that are causally responsible forcounterfactual behaviour on the indirect object identification task\citep{wang2022interpretability} to a finer degree than previousdecompositions. This work indicates that it is possible to resolvesuperposition in language models using a scalable, unsupervised method. Ourmethod may serve as a foundation for future mechanistic interpretability work,which we hope will enable greater model transparency and steerability.</description><author>Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, Lee Sharkey</author><pubDate>Wed, 04 Oct 2023 14:17:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08600v3</guid></item><item><title>GAMMA: Generalizable Articulation Modeling and Manipulation for Articulated Objects</title><link>http://arxiv.org/abs/2309.16264v2</link><description>Articulated objects like cabinets and doors are widespread in daily life.However, directly manipulating 3D articulated objects is challenging becausethey have diverse geometrical shapes, semantic categories, and kineticconstraints. Prior works mostly focused on recognizing and manipulatingarticulated objects with specific joint types. They can either estimate thejoint parameters or distinguish suitable grasp poses to facilitate trajectoryplanning. Although these approaches have succeeded in certain types ofarticulated objects, they lack generalizability to unseen objects, whichsignificantly impedes their application in broader scenarios. In this paper, wepropose a novel framework of Generalizable Articulation Modeling andManipulating for Articulated Objects (GAMMA), which learns both articulationmodeling and grasp pose affordance from diverse articulated objects withdifferent categories. In addition, GAMMA adopts adaptive manipulation toiteratively reduce the modeling errors and enhance manipulation performance. Wetrain GAMMA with the PartNet-Mobility dataset and evaluate with comprehensiveexperiments in SAPIEN simulation and real-world Franka robot. Results show thatGAMMA significantly outperforms SOTA articulation modeling and manipulationalgorithms in unseen and cross-category articulated objects. We willopen-source all codes and datasets in both simulation and real robots forreproduction in the final version. Images and videos are published on theproject website at: http://sites.google.com/view/gamma-articulation</description><author>Qiaojun Yu, Junbo Wang, Wenhai Liu, Ce Hao, Liu Liu, Lin Shao, Weiming Wang, Cewu Lu</author><pubDate>Wed, 04 Oct 2023 14:16:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16264v2</guid></item><item><title>Tracking Anything in Heart All at Once</title><link>http://arxiv.org/abs/2310.02792v1</link><description>Myocardial motion tracking stands as an essential clinical tool in theprevention and detection of Cardiovascular Diseases (CVDs), the foremost causeof death globally. However, current techniques suffer incomplete and inaccuratemotion estimation of the myocardium both in spatial and temporal dimensions,hindering the early identification of myocardial dysfunction. In addressingthese challenges, this paper introduces the Neural Cardiac Motion Field(NeuralCMF). NeuralCMF leverages the implicit neural representation (INR) tomodel the 3D structure and the comprehensive 6D forward/backward motion of theheart. This approach offers memory-efficient storage and continuous capabilityto query the precise shape and motion of the myocardium throughout the cardiaccycle at any specific point. Notably, NeuralCMF operates without the need forpaired datasets, and its optimization is self-supervised through the physicsknowledge priors both in space and time dimensions, ensuring compatibility withboth 2D and 3D echocardiogram video inputs. Experimental validations acrossthree representative datasets support the robustness and innovative nature ofthe NeuralCMF, marking significant advantages over existing state-of-the-artsin cardiac imaging and motion tracking.</description><author>Chengkang Shen, Hao Zhu, You Zhou, Yu Liu, Si Yi, Lili Dong, Weipeng Zhao, David J. Brady, Xun Cao, Zhan Ma, Yi Lin</author><pubDate>Wed, 04 Oct 2023 14:11:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02792v1</guid></item><item><title>Low Resource Summarization using Pre-trained Language Models</title><link>http://arxiv.org/abs/2310.02790v1</link><description>With the advent of Deep Learning based Artificial Neural Networks models,Natural Language Processing (NLP) has witnessed significant improvements intextual data processing in terms of its efficiency and accuracy. However, theresearch is mostly restricted to high-resource languages such as English andlow-resource languages still suffer from a lack of available resources in termsof training datasets as well as models with even baseline evaluation results.Considering the limited availability of resources for low-resource languages,we propose a methodology for adapting self-attentive transformer-basedarchitecture models (mBERT, mT5) for low-resource summarization, supplementedby the construction of a new baseline dataset (76.5k article, summary pairs) ina low-resource language Urdu. Choosing news (a publicly available source) asthe application domain has the potential to make the proposed methodologyuseful for reproducing in other languages with limited resources. Our adaptedsummarization model \textit{urT5} with up to 44.78\% reduction in size ascompared to \textit{mT5} can capture contextual information of low resourcelanguage effectively with evaluation score (up to 46.35 ROUGE-1, 77 BERTScore)at par with state-of-the-art models in high resource language English\textit{(PEGASUS: 47.21, BART: 45.14 on XSUM Dataset)}. The proposed methodprovided a baseline approach towards extractive as well as abstractivesummarization with competitive evaluation results in a limited resource setup.</description><author>Mubashir Munaf, Hammad Afzal, Naima Iltaf, Khawir Mahmood</author><pubDate>Wed, 04 Oct 2023 14:09:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02790v1</guid></item><item><title>MAD Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems</title><link>http://arxiv.org/abs/2310.02784v1</link><description>Training and deploying large machine learning (ML) models is time-consumingand requires significant distributed computing infrastructures. Based onreal-world large model training on datacenter-scale infrastructures, we show14~32% of all GPU hours are spent on communication with no overlappingcomputation. To minimize the outstanding communication latency, in this work,we develop an agile performance modeling framework to guide parallelization andhardware-software co-design strategies. Using the suite of real-world large MLmodels on state-of-the-art GPU training hardware, we demonstrate 2.24x and5.27x throughput improvement potential for pre-training and inferencescenarios, respectively.</description><author>Samuel Hsia, Alicia Golden, Bilge Acun-Uyan, Newsha Ardalani, Zachary DeVito, Gu-Yeon Wei, David Brooks, Carole-Jean Wu</author><pubDate>Wed, 04 Oct 2023 14:00:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02784v1</guid></item><item><title>Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design</title><link>http://arxiv.org/abs/2310.02782v1</link><description>The past decade has seen vast progress in deep reinforcement learning (RL) onthe back of algorithms manually designed by human researchers. Recently, it hasbeen shown that it is possible to meta-learn update rules, with the hope ofdiscovering algorithms that can perform well on a wide range of RL tasks.Despite impressive initial results from algorithms such as Learned PolicyGradient (LPG), there remains a generalization gap when these algorithms areapplied to unseen environments. In this work, we examine how characteristics ofthe meta-training distribution impact the generalization performance of thesealgorithms. Motivated by this analysis and building on ideas from UnsupervisedEnvironment Design (UED), we propose a novel approach for automaticallygenerating curricula to maximize the regret of a meta-learned optimizer, inaddition to a novel approximation of regret, which we name algorithmic regret(AR). The result is our method, General RL Optimizers Obtained Via EnvironmentDesign (GROOVE). In a series of experiments, we show that GROOVE achievessuperior generalization to LPG, and evaluate AR against baseline metrics fromUED, identifying it as a critical component of environment design in thissetting. We believe this approach is a step towards the discovery of trulygeneral RL algorithms, capable of solving a wide range of real-worldenvironments.</description><author>Matthew Thomas Jackson, Minqi Jiang, Jack Parker-Holder, Risto Vuorio, Chris Lu, Gregory Farquhar, Shimon Whiteson, Jakob Nicolaus Foerster</author><pubDate>Wed, 04 Oct 2023 13:52:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02782v1</guid></item><item><title>Vision-based DRL Autonomous Driving Agent with Sim2Real Transfer</title><link>http://arxiv.org/abs/2305.11589v2</link><description>To achieve fully autonomous driving, vehicles must be capable of continuouslyperforming various driving tasks, including lane keeping and car following,both of which are fundamental and well-studied driving ones. However, previousstudies have mainly focused on individual tasks, and car following tasks havetypically relied on complete leader-follower information to attain optimalperformance. To address this limitation, we propose a vision-based deepreinforcement learning (DRL) agent that can simultaneously perform lane keepingand car following maneuvers. To evaluate the performance of our DRL agent, wecompare it with a baseline controller and use various performance metrics forquantitative analysis. Furthermore, we conduct a real-world evaluation todemonstrate the Sim2Real transfer capability of the trained DRL agent. To thebest of our knowledge, our vision-based car following and lane keeping agentwith Sim2Real transfer capability is the first of its kind.</description><author>Dianzhao Li, Ostap Okhrin</author><pubDate>Wed, 04 Oct 2023 13:52:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11589v2</guid></item><item><title>LROC-PANGU-GAN: Closing the Simulation Gap in Learning Crater Segmentation with Planetary Simulators</title><link>http://arxiv.org/abs/2310.02781v1</link><description>It is critical for probes landing on foreign planetary bodies to be able torobustly identify and avoid hazards - as, for example, steep cliffs or deepcraters can pose significant risks to a probe's landing and operationalsuccess. Recent applications of deep learning to this problem show promisingresults. These models are, however, often learned with explicit supervisionover annotated datasets. These human-labelled crater databases, such as fromthe Lunar Reconnaissance Orbiter Camera (LROC), may lack in consistency andquality, undermining model performance - as incomplete and/or inaccurate labelsintroduce noise into the supervisory signal, which encourages the model tolearn incorrect associations and results in the model making unreliablepredictions. Physics-based simulators, such as the Planet and Asteroid NaturalScene Generation Utility, have, in contrast, perfect ground truth, as theinternal state that they use to render scenes is known with exactness. However,they introduce a serious simulation-to-real domain gap - because of fundamentaldifferences between the simulated environment and the real-world arising frommodelling assumptions, unaccounted for physical interactions, environmentalvariability, etc. Therefore, models trained on their outputs suffer whendeployed in the face of realism they have not encountered in their trainingdata distributions. In this paper, we therefore introduce a system to closethis "realism" gap while retaining label fidelity. We train a CycleGAN model tosynthesise LROC from Planet and Asteroid Natural Scene Generation Utility(PANGU) images. We show that these improve the training of a downstream cratersegmentation network, with segmentation performance on a test set of real LROCimages improved as compared to using only simulated PANGU images.</description><author>Jaewon La, Jaime Phadke, Matt Hutton, Marius Schwinning, Gabriele De Canio, Florian Renk, Lars Kunze, Matthew Gadd</author><pubDate>Wed, 04 Oct 2023 13:52:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02781v1</guid></item><item><title>Expected flow networks in stochastic environments and two-player zero-sum games</title><link>http://arxiv.org/abs/2310.02779v1</link><description>Generative flow networks (GFlowNets) are sequential sampling models trainedto match a given distribution. GFlowNets have been successfully applied tovarious structured object generation tasks, sampling a diverse set ofhigh-reward objects quickly. We propose expected flow networks (EFlowNets),which extend GFlowNets to stochastic environments. We show that EFlowNetsoutperform other GFlowNet formulations in stochastic tasks such as proteindesign. We then extend the concept of EFlowNets to adversarial environments,proposing adversarial flow networks (AFlowNets) for two-player zero-sum games.We show that AFlowNets learn to find above 80% of optimal moves in Connect-4via self-play and outperform AlphaZero in tournaments.</description><author>Marco Jiralerspong, Bilun Sun, Danilo Vucetic, Tianyu Zhang, Yoshua Bengio, Gauthier Gidel, Nikolay Malkin</author><pubDate>Wed, 04 Oct 2023 13:50:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02779v1</guid></item><item><title>A UMLS-Augmented Framework for Improving Factuality in Large Language Models within Healthcare</title><link>http://arxiv.org/abs/2310.02778v1</link><description>Large language models (LLMs) have demonstrated powerful text generationcapabilities, bringing unprecedented innovation to the healthcare field. WhileLLMs hold immense promise for applications in healthcare, applying them to realclinical scenarios presents significant challenges, as these models maygenerate content that deviates from established medical facts and even exhibitpotential biases. In our research, we develop an augmented LLM framework basedon the Unified Medical Language System (UMLS), aiming to better serve thehealthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as ourbenchmark models, and conduct automatic evaluations using the ROUGE Score andBERTScore on 104 questions from the LiveQA test set. Additionally, we establishcriteria for physician-evaluation based on four dimensions: Factuality,Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physicianevaluation with 20 questions on the LiveQA test set. Multiple residentphysicians conducted blind reviews to evaluate the generated content, and theresults indicate that this framework effectively enhances the factuality,completeness, and relevance of generated content. Our research demonstrates theeffectiveness of using UMLS-augmented LLMs and highlights the potentialapplication value of LLMs in in medical question-answering.</description><author>Rui Yang, Edison Marrese-Taylor, Yuhe Ke, Lechao Cheng, Qingyu Chen, Irene Li</author><pubDate>Wed, 04 Oct 2023 13:50:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02778v1</guid></item><item><title>The Role of Linguistic Priors in Measuring Compositional Generalization of Vision-Language Models</title><link>http://arxiv.org/abs/2310.02777v1</link><description>Compositionality is a common property in many modalities including naturallanguages and images, but the compositional generalization of multi-modalmodels is not well-understood. In this paper, we identify two sources ofvisual-linguistic compositionality: linguistic priors and the interplay betweenimages and texts. We show that current attempts to improve compositionalgeneralization rely on linguistic priors rather than on information in theimage. We also propose a new metric for compositionality without suchlinguistic priors.</description><author>Chenwei Wu, Li Erran Li, Stefano Ermon, Patrick Haffner, Rong Ge, Zaiwei Zhang</author><pubDate>Wed, 04 Oct 2023 13:48:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02777v1</guid></item><item><title>Dynamic Shuffle: An Efficient Channel Mixture Method</title><link>http://arxiv.org/abs/2310.02776v1</link><description>The redundancy of Convolutional neural networks not only depends on weightsbut also depends on inputs. Shuffling is an efficient operation for mixingchannel information but the shuffle order is usually pre-defined. To reduce thedata-dependent redundancy, we devise a dynamic shuffle module to generatedata-dependent permutation matrices for shuffling. Since the dimension ofpermutation matrix is proportional to the square of the number of inputchannels, to make the generation process efficiently, we divide the channelsinto groups and generate two shared small permutation matrices for each group,and utilize Kronecker product and cross group shuffle to obtain the finalpermutation matrices. To make the generation process learnable, based ontheoretical analysis, softmax, orthogonal regularization, and binarization areemployed to asymptotically approximate the permutation matrix. Dynamic shuffleadaptively mixes channel information with negligible extra computation andmemory occupancy. Experiment results on image classification benchmark datasetsCIFAR-10, CIFAR-100, Tiny ImageNet and ImageNet have shown that our methodsignificantly increases ShuffleNets' performance. Adding dynamic generatedmatrix with learnable static matrix, we further propose static-dynamic-shuffleand show that it can serve as a lightweight replacement of ordinary pointwiseconvolution.</description><author>Kaijun Gong, Zhuowen Yin, Yushu Li, Kailing Guo, Xiangmin Xu</author><pubDate>Wed, 04 Oct 2023 13:47:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02776v1</guid></item><item><title>Graph Neural Networks and Time Series as Directed Graphs for Quality Recognition</title><link>http://arxiv.org/abs/2310.02774v1</link><description>Graph Neural Networks (GNNs) are becoming central in the study of timeseries, coupled with existing algorithms as Temporal Convolutional Networks andRecurrent Neural Networks. In this paper, we see time series themselves asdirected graphs, so that their topology encodes time dependencies and we startto explore the effectiveness of GNNs architectures on them. We develop twodistinct Geometric Deep Learning models, a supervised classifier and anautoencoder-like model for signal reconstruction. We apply these models on aquality recognition problem.</description><author>Angelica Simonetti, Ferdinando Zanchetta</author><pubDate>Wed, 04 Oct 2023 13:43:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02774v1</guid></item><item><title>Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks</title><link>http://arxiv.org/abs/2310.02772v1</link><description>In this article, we propose a new paradigm for training spiking neuralnetworks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs areenergy-efficient but difficult to train. Consequently, many researchers haveproposed various methods to solve this problem, among which online trainingthrough time (OTTT) is a method that allows inferring at each time step whilesuppressing the memory cost. However, to compute efficiently on GPUs, OTTTrequires operations with spike trains and weighted summation of spike trainsduring forwarding. In addition, OTTT has shown a relationship with the SpikeRepresentation, an alternative training method, though theoretical agreementwith Spike Representation has yet to be proven. Our proposed method can solvethese problems; namely, SAF can halve the number of operations during theforward process, and it can be theoretically proven that SAF is consistent withthe Spike Representation and OTTT, respectively. Furthermore, we confirmed theabove contents through experiments and showed that it is possible to reducememory and training time while maintaining accuracy.</description><author>Ryuji Saiin, Tomoya Shirakawa, Sota Yoshihara, Yoshihide Sawada, Hiroyuki Kusumoto</author><pubDate>Wed, 04 Oct 2023 13:42:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02772v1</guid></item><item><title>Text-to-Motion Retrieval: Towards Joint Understanding of Human Motion Data and Natural Language</title><link>http://arxiv.org/abs/2305.15842v2</link><description>Due to recent advances in pose-estimation methods, human motion can beextracted from a common video in the form of 3D skeleton sequences. Despitewonderful application opportunities, effective and efficient content-basedaccess to large volumes of such spatio-temporal skeleton data still remains achallenging problem. In this paper, we propose a novel content-basedtext-to-motion retrieval task, which aims at retrieving relevant motions basedon a specified natural-language textual description. To define baselines forthis uncharted task, we employ the BERT and CLIP language representations toencode the text modality and successful spatio-temporal models to encode themotion modality. We additionally introduce our transformer-based approach,called Motion Transformer (MoT), which employs divided space-time attention toeffectively aggregate the different skeleton joints in space and time. Inspiredby the recent progress in text-to-image/video matching, we experiment with twowidely-adopted metric-learning loss functions. Finally, we set up a commonevaluation protocol by defining qualitative metrics for assessing the qualityof the retrieved motions, targeting the two recently-introduced KITMotion-Language and HumanML3D datasets. The code for reproducing our results isavailable at https://github.com/mesnico/text-to-motion-retrieval.</description><author>Nicola Messina, Jan Sedmidubsky, Fabrizio Falchi, Tomáš Rebok</author><pubDate>Wed, 04 Oct 2023 13:37:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15842v2</guid></item><item><title>Fused Gromov-Wasserstein Graph Mixup for Graph-level Classifications</title><link>http://arxiv.org/abs/2306.15963v2</link><description>Graph data augmentation has shown superiority in enhancing generalizabilityand robustness of GNNs in graph-level classifications. However, existingmethods primarily focus on the augmentation in the graph signal space and thegraph structure space independently, neglecting the joint interaction betweenthem. In this paper, we address this limitation by formulating the problem asan optimal transport problem that aims to find an optimal inter-graph nodematching strategy considering the interactions between graph structures andsignals. To solve this problem, we propose a novel graph mixup algorithm calledFGWMixup, which seeks a midpoint of source graphs in the FusedGromov-Wasserstein (FGW) metric space. To enhance the scalability of ourmethod, we introduce a relaxed FGW solver that accelerates FGWMixup byimproving the convergence rate from $\mathcal{O}(t^{-1})$ to$\mathcal{O}(t^{-2})$. Extensive experiments conducted on five datasets usingboth classic (MPNNs) and advanced (Graphormers) GNN backbones demonstrate thatFGWMixup effectively improves the generalizability and robustness of GNNs.Codes are available at https://github.com/ArthurLeoM/FGWMixup.</description><author>Xinyu Ma, Xu Chu, Yasha Wang, Yang Lin, Junfeng Zhao, Liantao Ma, Wenwu Zhu</author><pubDate>Wed, 04 Oct 2023 13:33:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15963v2</guid></item></channel></rss>