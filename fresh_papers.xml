<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 31 Jul 2024 13:00:25 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ThinK: Thinner Key Cache by Query-Driven Pruning</title><link>http://arxiv.org/abs/2407.21018v1</link><description>Large Language Models (LLMs) have revolutionized the field of naturallanguage processing, achieving unprecedented performance across a variety ofapplications by leveraging increased model sizes and sequence lengths. However,the associated rise in computational and memory costs poses significantchallenges, particularly in managing long sequences due to the quadraticcomplexity of the transformer attention mechanism. This paper focuses on thelong-context scenario, addressing the inefficiencies in KV cache memoryconsumption during inference. Unlike existing approaches that optimize thememory based on the sequence lengths, we uncover that the channel dimension ofthe KV cache exhibits significant redundancy, characterized by unbalancedmagnitude distribution and low-rank structure in attention weights. Based onthese observations, we propose ThinK, a novel query-dependent KV cache pruningmethod designed to minimize attention weight loss while selectively pruning theleast significant channels. Our approach not only maintains or enhances modelaccuracy but also achieves a reduction in memory costs by over 20% comparedwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 andMistral models across various long-sequence datasets confirm the efficacy ofThinK, setting a new precedent for efficient LLM deployment withoutcompromising performance. We also outline the potential of extending our methodto value cache pruning, demonstrating ThinK's versatility and broadapplicability in reducing both memory and computational overheads.</description><author>Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou, Amrita Saha, Caiming Xiong, Doyen Sahoo</author><pubDate>Tue, 30 Jul 2024 17:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21018v1</guid></item><item><title>Matting by Generation</title><link>http://arxiv.org/abs/2407.21017v1</link><description>This paper introduces an innovative approach for image matting that redefinesthe traditional regression-based task as a generative modeling challenge. Ourmethod harnesses the capabilities of latent diffusion models, enriched withextensive pre-trained knowledge, to regularize the matting process. We presentnovel architectural innovations that empower our model to produce mattes withsuperior resolution and detail. The proposed method is versatile and canperform both guidance-free and guidance-based image matting, accommodating avariety of additional cues. Our comprehensive evaluation across three benchmarkdatasets demonstrates the superior performance of our approach, bothquantitatively and qualitatively. The results not only reflect our method'srobust effectiveness but also highlight its ability to generate visuallycompelling mattes that approach photorealistic quality. The project page forthis paper is available athttps://lightchaserx.github.io/matting-by-generation/</description><author>Zhixiang Wang, Baiang Li, Jian Wang, Yu-Lun Liu, Jinwei Gu, Yung-Yu Chuang, Shin'ichi Satoh</author><pubDate>Tue, 30 Jul 2024 17:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21017v1</guid></item><item><title>Language-based Valence and Arousal Expressions between the United States and China: a Cross-Cultural Examination</title><link>http://arxiv.org/abs/2401.05254v3</link><description>Although affective expressions of individuals have been extensively studiedusing social media, research has primarily focused on the Western context.There are substantial differences among cultures that contribute to theiraffective expressions. This paper examines the differences between Twitter (X)in the United States and Sina Weibo posts in China on two primary dimensions ofaffect - valence and arousal. We study the difference in the functionalrelationship between arousal and valence (so-called V-shaped) among individualsin the US and China and explore the associated content differences.Furthermore, we correlate word usage and topics in both platforms to interprettheir differences. We observe that for Twitter users, the variation inemotional intensity is less distinct between negative and positive emotionscompared to Weibo users, and there is a sharper escalation in arousalcorresponding with heightened emotions. From language features, we discoverthat affective expressions are associated with personal life and feelings onTwitter, while on Weibo such discussions are about socio-political topics inthe society. These results suggest a West-East difference in the V-shapedrelationship between valence and arousal of affective expressions on socialmedia influenced by content differences. Our findings have implications forapplications and theories related to cultural differences in affectiveexpressions.</description><author>Young-Min Cho, Dandan Pang, Stuti Thapa, Garrick Sherman, Lyle Ungar, Louis Tay, Sharath Chandra Guntuku</author><pubDate>Tue, 30 Jul 2024 17:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05254v3</guid></item><item><title>Add-SD: Rational Generation without Manual Reference</title><link>http://arxiv.org/abs/2407.21016v1</link><description>Diffusion models have exhibited remarkable prowess in visual generalization.Building on this success, we introduce an instruction-based object additionpipeline, named Add-SD, which automatically inserts objects into realisticscenes with rational sizes and positions. Different from layout-conditionedmethods, Add-SD is solely conditioned on simple text prompts rather than anyother human-costly references like bounding boxes. Our work contributes inthree aspects: proposing a dataset containing numerous instructed image pairs;fine-tuning a diffusion model for rational generation; and generating syntheticdata to boost downstream tasks. The first aspect involves creating aRemovalDataset consisting of original-edited image pairs with textualinstructions, where an object has been removed from the original image whilemaintaining strong pixel consistency in the background. These data pairs arethen used for fine-tuning the Stable Diffusion (SD) model. Subsequently, thepretrained Add-SD model allows for the insertion of expected objects into animage with good rationale. Additionally, we generate synthetic instances fordownstream task datasets at scale, particularly for tail classes, to alleviatethe long-tailed problem. Downstream tasks benefit from the enriched datasetwith enhanced diversity and rationale. Experiments on LVIS val demonstrate thatAdd-SD yields an improvement of 4.3 mAP on rare classes over the baseline. Codeand models are available at https://github.com/ylingfeng/Add-SD.</description><author>Lingfeng Yang, Xinyu Zhang, Xiang Li, Jinwen Chen, Kun Yao, Gang Zhang, Errui Ding, Lingqiao Liu, Jingdong Wang, Jian Yang</author><pubDate>Tue, 30 Jul 2024 17:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21016v1</guid></item><item><title>CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning</title><link>http://arxiv.org/abs/2407.21011v1</link><description>Recent advancements in Contrastive Language-Image Pre-training (CLIP) havedemonstrated notable success in self-supervised representation learning acrossvarious tasks. However, the existing CLIP-like approaches often demandextensive GPU resources and prolonged training times due to the considerablesize of the model and dataset, making them poor for medical applications, inwhich large datasets are not always common. Meanwhile, the language modelprompts are mainly manually derived from labels tied to images, potentiallyoverlooking the richness of information within training samples. We introduce anovel language-image Contrastive Learning method with an Efficient largelanguage model and prompt Fine-Tuning (CLEFT) that harnesses the strengths ofthe extensive pre-trained language and visual models. Furthermore, we presentan efficient strategy for learning context-based prompts that mitigates the gapbetween informative clinical diagnostic data and simple class labels. Ourmethod demonstrates state-of-the-art performance on multiple chest X-ray andmammography datasets compared with various baselines. The proposed parameterefficient framework can reduce the total trainable model size by 39% and reducethe trainable language model to only 4% compared with the current BERT encoder.</description><author>Yuexi Du, Brian Chang, Nicha C. Dvornek</author><pubDate>Tue, 30 Jul 2024 17:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21011v1</guid></item><item><title>AI-Assisted Generation of Difficult Math Questions</title><link>http://arxiv.org/abs/2407.21009v1</link><description>Current LLM training positions mathematical reasoning as a core capability.With publicly available sources fully tapped, there is unmet demand for diverseand challenging math questions. Relying solely on human experts is bothtime-consuming and costly, while LLM-generated questions often lack therequisite diversity and difficulty. We present a design framework that combinesthe strengths of LLMs with a human-in-the-loop approach to generate a diversearray of challenging math questions. We leverage LLM metacognition skills[Didolkar et al., 2024] of a strong LLM to extract core "skills" from existingmath datasets. These skills serve as the basis for generating novel anddifficult questions by prompting the LLM with random pairs of core skills. Theuse of two different skills within each question makes finding such questionsan "out of distribution" task for both LLMs and humans. Our pipeline employsLLMs to iteratively generate and refine questions and solutions throughmultiturn prompting. Human annotators then verify and further refine thequestions, with their efficiency enhanced via further LLM interactions.Applying this pipeline on skills extracted from the MATH dataset [Hendrycks etal., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions,as evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH(b) Higher performance on MATH when using MATH$^2$ questions as in-contextexamples. Although focused on mathematics, our methodology seems applicable toother domains requiring structured reasoning, and potentially as a component ofscalable oversight. Also of interest is a striking relationship observedbetween models' performance on the new dataset: the success rate on MATH$^2$ isthe square on MATH, suggesting that successfully solving the question inMATH$^2$ requires a nontrivial combination of two distinct math skills.</description><author>Vedant Shah, Dingli Yu, Kaifeng Lyu, Simon Park, Nan Rosemary Ke, Michael Mozer, Yoshua Bengio, Sanjeev Arora, Anirudh Goyal</author><pubDate>Tue, 30 Jul 2024 17:55:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21009v1</guid></item><item><title>Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection</title><link>http://arxiv.org/abs/2407.21004v1</link><description>Recent advances show that two-stream approaches have achieved outstandingperformance in hateful meme detection. However, hateful memes constantly evolveas new memes emerge by fusing progressive cultural ideas, making existingmethods obsolete or ineffective. In this work, we explore the potential ofLarge Multimodal Models (LMMs) for hateful meme detection. To this end, wepropose Evolver, which incorporates LMMs via Chain-of-Evolution (CoE)Prompting, by integrating the evolution attribute and in-context information ofmemes. Specifically, Evolver simulates the evolving and expressing process ofmemes and reasons through LMMs in a step-by-step manner. First, an evolutionarypair mining module retrieves the top-k most similar memes in the externalcurated meme set with the input meme. Second, an evolutionary informationextractor is designed to summarize the semantic regularities between the pairedmemes for prompting. Finally, a contextual relevance amplifier enhances thein-context hatefulness information to boost the search for evolutionaryprocesses. Extensive experiments on public FHM, MAMI, and HarM datasets showthat CoE prompting can be incorporated into existing LMMs to improve theirperformance. More encouragingly, it can serve as an interpretive tool topromote the understanding of the evolution of social memes.</description><author>Jinfa Huang, Jinsheng Pan, Zhongwei Wan, Hanjia Lyu, Jiebo Luo</author><pubDate>Tue, 30 Jul 2024 17:51:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21004v1</guid></item><item><title>XHand: Real-time Expressive Hand Avatar</title><link>http://arxiv.org/abs/2407.21002v1</link><description>Hand avatars play a pivotal role in a wide array of digital interfaces,enhancing user immersion and facilitating natural interaction within virtualenvironments. While previous studies have focused on photo-realistic handrendering, little attention has been paid to reconstruct the hand geometry withfine details, which is essential to rendering quality. In the realms ofextended reality and gaming, on-the-fly rendering becomes imperative. To thisend, we introduce an expressive hand avatar, named XHand, that is designed tocomprehensively generate hand shape, appearance, and deformations in real-time.To obtain fine-grained hand meshes, we make use of three feature embeddingmodules to predict hand deformation displacements, albedo, and linear blendingskinning weights, respectively. To achieve photo-realistic hand rendering onfine-grained meshes, our method employs a mesh-based neural renderer byleveraging mesh topological consistency and latent codes from embeddingmodules. During training, a part-aware Laplace smoothing strategy is proposedby incorporating the distinct levels of regularization to effectively maintainthe necessary details and eliminate the undesired artifacts. The experimentalevaluations on InterHand2.6M and DeepHandMesh datasets demonstrate the efficacyof XHand, which is able to recover high-fidelity geometry and texture for handanimations across diverse poses in real-time. To reproduce our results, we willmake the full implementation publicly available athttps://github.com/agnJason/XHand.</description><author>Qijun Gan, Zijie Zhou, Jianke Zhu</author><pubDate>Tue, 30 Jul 2024 17:49:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21002v1</guid></item><item><title>GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language Models</title><link>http://arxiv.org/abs/2407.21001v1</link><description>Vision-language models (VLMs) are intensively used in many downstream tasks,including those requiring assessments of individuals appearing in the images.While VLMs perform well in simple single-person scenarios, in real-worldapplications, we often face complex situations in which there are persons ofdifferent genders doing different activities. We show that in such cases, VLMsare biased towards identifying the individual with the expected gender(according to ingrained gender stereotypes in the model or other forms ofsample selection bias) as the performer of the activity. We refer to this biasin associating an activity with the gender of its actual performer in an imageor text as the Gender-Activity Binding (GAB) bias and analyze how this bias isinternalized in VLMs. To assess this bias, we have introduced the GAB datasetwith approximately 5500 AI-generated images that represent a variety ofactivities, addressing the scarcity of real-world images for some scenarios. Tohave extensive quality control, the generated images are evaluated for theirdiversity, quality, and realism. We have tested 12 renowned pre-trained VLMs onthis dataset in the context of text-to-image and image-to-text retrieval tomeasure the effect of this bias on their predictions. Additionally, we havecarried out supplementary experiments to quantify the bias in VLMs' textencoders and to evaluate VLMs' capability to recognize activities. Ourexperiments indicate that VLMs experience an average performance decline ofabout 13.2% when confronted with gender-activity binding bias.</description><author>Ali Abdollahi, Mahdi Ghaznavi, Mohammad Reza Karimi Nejad, Arash Mari Oriyad, Reza Abbasi, Ali Salesi, Melika Behjati, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah</author><pubDate>Tue, 30 Jul 2024 17:46:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21001v1</guid></item><item><title>MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning</title><link>http://arxiv.org/abs/2407.20999v1</link><description>Recently, large language models (LLMs) have demonstrated remarkablecapabilities in a wide range of tasks. Typically, an LLM is pre-trained onlarge corpora and subsequently fine-tuned on task-specific datasets. However,during finetuning, LLMs may forget the knowledge acquired in the pretrainingstage, leading to a decline in general capabilities. To address this issue, wepropose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO).The key idea of MoFO is to iteratively select and update the model parameterswith the largest momentum magnitudes. Compared to full-parameter training, MoFOachieves similar fine-tuning performance while keeping parameters closer to thepre-trained model, thereby mitigating knowledge forgetting. Unlike mostexisting methods for forgetting mitigation, MoFO combines the following twoadvantages. First, MoFO does not require access to pre-training data. Thismakes MoFO particularly suitable for fine-tuning scenarios where pre-trainingdata is unavailable, such as fine-tuning checkpoint-only open-source LLMs.Second, MoFO does not alter the original loss function. This could avoidimpairing the model performance on the fine-tuning tasks. We validate MoFOthrough rigorous convergence analysis and extensive experiments, demonstratingits superiority over existing methods in mitigating forgetting and enhancingfine-tuning performance.</description><author>Yupeng Chen, Senmiao Wang, Zhihang Lin, Zeyu Qin, Yushun Zhang, Tian Ding, Ruoyu Sun</author><pubDate>Tue, 30 Jul 2024 17:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20999v1</guid></item><item><title>Pattern recognition using spiking antiferromagnetic neurons</title><link>http://arxiv.org/abs/2308.09071v3</link><description>Spintronic devices offer a promising avenue for the development of nanoscale,energy-efficient artificial neurons for neuromorphic computing. It haspreviously been shown that with antiferromagnetic (AFM) oscillators, ultra-fastspiking artificial neurons can be made that mimic many unique features ofbiological neurons. In this work, we train an artificial neural network of AFMneurons to perform pattern recognition. A simple machine learning algorithmcalled spike pattern association neuron (SPAN), which relies on the temporalposition of neuron spikes, is used during training. In under a microsecond ofphysical time, the AFM neural network is trained to recognize symbols composedfrom a grid by producing a spike within a specified time window. We furtherachieve multi-symbol recognition with the addition of an output layer tosuppress undesirable spikes. Through the utilization of AFM neurons and theSPAN algorithm, we create a neural network capable of high-accuracy recognitionwith overall power consumption on the order of picojoules.</description><author>Hannah Bradley, Steven Louis, Andrei Slavin, Vasyl Tyberkevych</author><pubDate>Tue, 30 Jul 2024 17:28:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09071v3</guid></item><item><title>From Feature Importance to Natural Language Explanations Using LLMs with RAG</title><link>http://arxiv.org/abs/2407.20990v1</link><description>As machine learning becomes increasingly integral to autonomousdecision-making processes involving human interaction, the necessity ofcomprehending the model's outputs through conversational means increases. Mostrecently, foundation models are being explored for their potential as post hocexplainers, providing a pathway to elucidate the decision-making mechanisms ofpredictive models. In this work, we introduce traceable question-answering,leveraging an external knowledge repository to inform the responses of LargeLanguage Models (LLMs) to user queries within a scene understanding task. Thisknowledge repository comprises contextual details regarding the model's output,containing high-level features, feature importance, and alternativeprobabilities. We employ subtractive counterfactual reasoning to computefeature importance, a method that entails analysing output variations resultingfrom decomposing semantic features. Furthermore, to maintain a seamlessconversational flow, we integrate four key characteristics - social, causal,selective, and contrastive - drawn from social science research on humanexplanations into a single-shot prompt, guiding the response generationprocess. Our evaluation demonstrates that explanations generated by the LLMsencompassed these elements, indicating its potential to bridge the gap betweencomplex model outputs and natural language expressions.</description><author>Sule Tekkesinoglu, Lars Kunze</author><pubDate>Tue, 30 Jul 2024 17:27:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20990v1</guid></item><item><title>Mixture of Nested Experts: Adaptive Processing of Visual Tokens</title><link>http://arxiv.org/abs/2407.19985v2</link><description>The visual medium (images and videos) naturally contains a large amount ofinformation redundancy, thereby providing a great opportunity for leveragingefficiency in processing. While Vision Transformer (ViT) based models scaleeffectively to large data regimes, they fail to capitalize on this inherentredundancy, leading to higher computational costs. Mixture of Experts (MoE)networks demonstrate scalability while maintaining same inference-time costs,but they come with a larger parameter footprint. We present Mixture of NestedExperts (MoNE), which utilizes a nested structure for experts, whereinindividual experts fall on an increasing compute-accuracy curve. Given acompute budget, MoNE learns to dynamically choose tokens in a priority order,and thus redundant tokens are processed through cheaper nested experts. Usingthis framework, we achieve equivalent performance as the baseline models, whilereducing inference time compute by over two-fold. We validate our approach onstandard image and video datasets - ImageNet-21K, Kinetics400, andSomething-Something-v2. We further highlight MoNE$'$s adaptability byshowcasing its ability to maintain strong performance across differentinference-time compute budgets on videos, using only a single trained model.</description><author>Gagan Jain, Nidhi Hegde, Aditya Kusupati, Arsha Nagrani, Shyamal Buch, Prateek Jain, Anurag Arnab, Sujoy Paul</author><pubDate>Tue, 30 Jul 2024 17:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19985v2</guid></item><item><title>Contrasting Deep Learning Models for Direct Respiratory Insufficiency Detection Versus Blood Oxygen Saturation Estimation</title><link>http://arxiv.org/abs/2407.20989v1</link><description>We contrast high effectiveness of state of the art deep learningarchitectures designed for general audio classification tasks, refined forrespiratory insufficiency (RI) detection and blood oxygen saturation (SpO2)estimation and classification through automated audio analysis. Recently,multiple deep learning architectures have been proposed to detect RI in COVIDpatients through audio analysis, achieving accuracy above 95% and F1-scoreabove 0.93. RI is a condition associated with low SpO2 levels, commonly definedas the threshold SpO2 &lt;92%. While SpO2 serves as a crucial determinant of RI, amedical doctor's diagnosis typically relies on multiple factors. These includerespiratory frequency, heart rate, SpO2 levels, among others. Here we studypretrained audio neural networks (CNN6, CNN10 and CNN14) and the MaskedAutoencoder (Audio-MAE) for RI detection, where these models achieve nearperfect accuracy, surpassing previous results. Yet, for the regression task ofestimating SpO2 levels, the models achieve root mean square error valuesexceeding the accepted clinical range of 3.5% for finger oximeters.Additionally, Pearson correlation coefficients fail to surpass 0.3. As deeplearning models perform better in classification than regression, we transformSpO2-regression into a SpO2-threshold binary classification problem, with athreshold of 92%. However, this task still yields an F1-score below 0.65. Thus,audio analysis offers valuable insights into a patient's RI status, but doesnot provide accurate information about actual SpO2 levels, indicating aseparation of domains in which voice and speech biomarkers may and may not beuseful in medical diagnostics under current technologies.</description><author>Marcelo Matheus Gauy, Natalia Hitomi Koza, Ricardo Mikio Morita, Gabriel Rocha Stanzione, Arnaldo Candido Junior, Larissa Cristina Berti, Anna Sara Shafferman Levin, Ester Cerdeira Sabino, Flaviane Romani Fernandes Svartman, Marcelo Finger</author><pubDate>Tue, 30 Jul 2024 17:26:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20989v1</guid></item><item><title>PIXELMOD: Improving Soft Moderation of Visual Misleading Information on Twitter</title><link>http://arxiv.org/abs/2407.20987v1</link><description>Images are a powerful and immediate vehicle to carry misleading or outrightfalse messages, yet identifying image-based misinformation at scale posesunique challenges. In this paper, we present PIXELMOD, a system that leveragesperceptual hashes, vector databases, and optical character recognition (OCR) toefficiently identify images that are candidates to receive soft moderationlabels on Twitter. We show that PIXELMOD outperforms existing image similarityapproaches when applied to soft moderation, with negligible performanceoverhead. We then test PIXELMOD on a dataset of tweets surrounding the 2020 USPresidential Election, and find that it is able to identify visually misleadingimages that are candidates for soft moderation with 0.99% false detection and2.06% false negatives.</description><author>Pujan Paudel, Chen Ling, Jeremy Blackburn, Gianluca Stringhini</author><pubDate>Tue, 30 Jul 2024 17:21:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20987v1</guid></item><item><title>Large Language Models (LLMs) for Semantic Communication in Edge-based IoT Networks</title><link>http://arxiv.org/abs/2407.20970v1</link><description>With the advent of Fifth Generation (5G) and Sixth Generation (6G)communication technologies, as well as the Internet of Things (IoT), semanticcommunication is gaining attention among researchers as current communicationtechnologies are approaching Shannon's limit. On the other hand, Large LanguageModels (LLMs) can understand and generate human-like text, based on extensivetraining on diverse datasets with billions of parameters. Considering therecent near-source computational technologies like Edge, in this article, wegive an overview of a framework along with its modules, where LLMs can be usedunder the umbrella of semantic communication at the network edge for efficientcommunication in IoT networks. Finally, we discuss a few applications andanalyze the challenges and opportunities to develop such systems.</description><author>Alakesh Kalita</author><pubDate>Tue, 30 Jul 2024 16:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20970v1</guid></item><item><title>Differentiable Voxelization and Mesh Morphing</title><link>http://arxiv.org/abs/2407.11272v2</link><description>In this paper, we propose the differentiable voxelization of 3D meshes viathe winding number and solid angles. The proposed approach achieves fast,flexible, and accurate voxelization of 3D meshes, admitting the computation ofgradients with respect to the input mesh and GPU acceleration. We furtherdemonstrate the application of the proposed voxelization in mesh morphing,where the voxelized mesh is deformed by a neural network. The proposed methodis evaluated on the ShapeNet dataset and achieves state-of-the-art performancein terms of both accuracy and efficiency.</description><author>Yihao Luo, Yikai Wang, Zhengrui Xiang, Yuliang Xiu, Guang Yang, ChoonHwai Yap</author><pubDate>Tue, 30 Jul 2024 16:50:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11272v2</guid></item><item><title>MMTrail: A Multimodal Trailer Video Dataset with Language and Music Descriptions</title><link>http://arxiv.org/abs/2407.20962v1</link><description>Massive multi-modality datasets play a significant role in facilitating thesuccess of large video-language models. However, current video-languagedatasets primarily provide text descriptions for visual frames, consideringaudio to be weakly related information. They usually overlook exploring thepotential of inherent audio-visual correlation, leading to monotonousannotation within each modality instead of comprehensive and precisedescriptions. Such ignorance results in the difficulty of multiplecross-modality studies. To fulfill this gap, we present MMTrail, a large-scalemulti-modality video-language dataset incorporating more than 20M trailer clipswith visual captions, and 2M high-quality clips with multimodal captions.Trailers preview full-length video works and integrate context, visual frames,and background music. In particular, the trailer has two main advantages: (1)the topics are diverse, and the content characters are of various types, e.g.,film, news, and gaming. (2) the corresponding background music iscustom-designed, making it more coherent with the visual context. Upon theseinsights, we propose a systemic captioning framework, achieving variousmodality annotations with more than 27.1k hours of trailer videos. Here, toensure the caption retains music perspective while preserving the authority ofvisual context, we leverage the advanced LLM to merge all annotationsadaptively. In this fashion, our MMtrail dataset potentially paves the path forfine-grained large multimodal-language model training. In experiments, weprovide evaluation metrics and benchmark results on our dataset, demonstratingthe high quality of our annotation and its effectiveness for model training.</description><author>Xiaowei Chi, Yatian Wang, Aosong Cheng, Pengjun Fang, Zeyue Tian, Yingqing He, Zhaoyang Liu, Xingqun Qi, Jiahao Pan, Rongyu Zhang, Mengfei Li, Ruibin Yuan, Yanbing Jiang, Wei Xue, Wenhan Luo, Qifeng Chen, Shanghang Zhang, Qifeng Liu, Yike Guo</author><pubDate>Tue, 30 Jul 2024 16:43:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20962v1</guid></item><item><title>Surgical Text-to-Image Generation</title><link>http://arxiv.org/abs/2407.09230v2</link><description>Acquiring surgical data for research and development is significantlyhindered by high annotation costs and practical and ethical constraints.Utilizing synthetically generated images could offer a valuable alternative. Inthis work, we explore adapting text-to-image generative models for the surgicaldomain using the CholecT50 dataset, which provides surgical images annotatedwith action triplets (instrument, verb, target). We investigate severallanguage models and find T5 to offer more distinct features for differentiatingsurgical actions on triplet-based textual inputs, and showcasing strongeralignment between long and triplet-based captions. To address challenges intraining text-to-image models solely on triplet-based captions withoutadditional inputs and supervisory signals, we discover that triplet textembeddings are instrument-centric in the latent space. Leveraging this insight,we design an instrument-based class balancing technique to counteract dataimbalance and skewness, improving training convergence. Extending Imagen, adiffusion-based generative model, we develop Surgical Imagen to generatephotorealistic and activity-aligned surgical images from triplet-based textualprompts. We assess the model on quality, alignment, reasoning, and knowledge,achieving FID and CLIP scores of 3.7 and 26.8% respectively. Human expertsurvey shows that participants were highly challenged by the realisticcharacteristics of the generated samples, demonstrating Surgical Imagen'seffectiveness as a practical alternative to real data collection.</description><author>Chinedu Innocent Nwoye, Rupak Bose, Kareem Elgohary, Lorenzo Arboit, Giorgio Carlino, Joël L. Lavanchy, Pietro Mascagni, Nicolas Padoy</author><pubDate>Tue, 30 Jul 2024 16:40:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09230v2</guid></item><item><title>Learning Ordinality in Semantic Segmentation</title><link>http://arxiv.org/abs/2407.20959v1</link><description>Semantic segmentation consists of predicting a semantic label for each imagepixel. Conventional deep learning models do not take advantage of ordinalrelations that might exist in the domain at hand. For example, it is known thatthe pupil is inside the iris, and the lane markings are inside the road. Suchdomain knowledge can be employed as constraints to make the model more robust.The current literature on this topic has explored pixel-wise ordinalsegmentation methods, which treat each pixel as an independent observation andpromote ordinality in its representation. This paper proposes novel spatialordinal segmentation methods, which take advantage of the structured imagespace by considering each pixel as an observation dependent on its neighborhoodcontext to also promote ordinal spatial consistency. When evaluated with fivebiomedical datasets and multiple configurations of autonomous driving datasets,ordinal methods resulted in more ordinally-consistent models, with substantialimprovements in ordinal metrics and some increase in the Dice coefficient. Itwas also shown that the incorporation of ordinal consistency results in modelswith better generalization abilities.</description><author>Rafael Cristino, Ricardo P. M. Cruz, Jaime S. Cardoso</author><pubDate>Tue, 30 Jul 2024 16:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20959v1</guid></item><item><title>Transfer learning for conflict and duplicate detection in software requirement pairs</title><link>http://arxiv.org/abs/2301.03709v2</link><description>Consistent and holistic expression of software requirements is important forthe success of software projects. In this study, we aim to enhance theefficiency of the software development processes by automatically identifyingconflicting and duplicate software requirement specifications. We formulate theconflict and duplicate detection problem as a requirement pair classificationtask. We design a novel transformers-based architecture, SR-BERT, whichincorporates Sentence-BERT and Bi-encoders for the conflict and duplicateidentification task. Furthermore, we apply supervised multi-stage fine-tuningto the pre-trained transformer models. We test the performance of differenttransfer models using four different datasets. We find that sequentiallytrained and fine-tuned transformer models perform well across the datasets withSR-BERT achieving the best performance for larger datasets. We also explore thecross-domain performance of conflict detection models and adopt a rule-basedfiltering approach to validate the model classifications. Our analysisindicates that the sentence pair classification approach and the proposedtransformer-based natural language processing strategies can contributesignificantly to achieving automation in conflict and duplicate detection</description><author>Garima Malik, Savas Yildirim, Mucahit Cevik, Ayse Bener, Devang Parikh</author><pubDate>Tue, 30 Jul 2024 16:31:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.03709v2</guid></item><item><title>An Effective Dynamic Gradient Calibration Method for Continual Learning</title><link>http://arxiv.org/abs/2407.20956v1</link><description>Continual learning (CL) is a fundamental topic in machine learning, where thegoal is to train a model with continuously incoming data and tasks. Due to thememory limit, we cannot store all the historical data, and therefore confrontthe ``catastrophic forgetting'' problem, i.e., the performance on the previoustasks can substantially decrease because of the missing information in thelatter period. Though a number of elegant methods have been proposed, thecatastrophic forgetting phenomenon still cannot be well avoided in practice. Inthis paper, we study the problem from the gradient perspective, where our aimis to develop an effective algorithm to calibrate the gradient in each updatingstep of the model; namely, our goal is to guide the model to be updated in theright direction under the situation that a large amount of historical data areunavailable. Our idea is partly inspired by the seminal stochastic variancereduction methods (e.g., SVRG and SAGA) for reducing the variance of gradientestimation in stochastic gradient descent algorithms. Another benefit is thatour approach can be used as a general tool, which is able to be incorporatedwith several existing popular CL methods to achieve better performance. We alsoconduct a set of experiments on several benchmark datasets to evaluate theperformance in practice.</description><author>Weichen Lin, Jiaxiang Chen, Ruomin Huang, Hu Ding</author><pubDate>Tue, 30 Jul 2024 16:30:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20956v1</guid></item><item><title>Emotion-driven Piano Music Generation via Two-stage Disentanglement and Functional Representation</title><link>http://arxiv.org/abs/2407.20955v1</link><description>Managing the emotional aspect remains a challenge in automatic musicgeneration. Prior works aim to learn various emotions at once, leading toinadequate modeling. This paper explores the disentanglement of emotions inpiano performance generation through a two-stage framework. The first stagefocuses on valence modeling of lead sheet, and the second stage addressesarousal modeling by introducing performance-level attributes. To furthercapture features that shape valence, an aspect less explored by previousapproaches, we introduce a novel functional representation of symbolic music.This representation aims to capture the emotional impact of major-minortonality, as well as the interactions among notes, chords, and key signatures.Objective and subjective experiments validate the effectiveness of ourframework in both emotional valence and arousal modeling. We further leverageour framework in a novel application of emotional controls, showing a broadpotential in emotion-driven music generation.</description><author>Jingyue Huang, Ke Chen, Yi-Hsuan Yang</author><pubDate>Tue, 30 Jul 2024 16:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20955v1</guid></item><item><title>An evidence-based methodology for human rights impact assessment (HRIA) in the development of AI data-intensive systems</title><link>http://arxiv.org/abs/2407.20951v1</link><description>Different approaches have been adopted in addressing the challenges ofArtificial Intelligence (AI), some centred on personal data and others onethics, respectively narrowing and broadening the scope of AI regulation. Thiscontribution aims to demonstrate that a third way is possible, starting fromthe acknowledgement of the role that human rights can play in regulating theimpact of data-intensive systems. The focus on human rights is neither aparadigm shift nor a mere theoretical exercise. Through the analysis of morethan 700 decisions and documents of the data protection authorities of sixcountries, we show that human rights already underpin the decisions in thefield of data use. Based on empirical analysis of this evidence, this workpresents a methodology and a model for a Human Rights Impact Assessment (HRIA).The methodology and related assessment model are focused on AI applications,whose nature and scale require a proper contextualisation of HRIA methodology.Moreover, the proposed models provide a more measurable approach to riskassessment which is consistent with the regulatory proposals centred on riskthresholds. The proposed methodology is tested in concrete case-studies toprove its feasibility and effectiveness. The overall goal is to respond to thegrowing interest in HRIA, moving from a mere theoretical debate to a concreteand context-specific implementation in the field of data-intensive applicationsbased on AI.</description><author>Alessandro Mantelero, Maria Samantha Esposito</author><pubDate>Tue, 30 Jul 2024 16:27:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20951v1</guid></item><item><title>dopanim: A Dataset of Doppelganger Animals with Noisy Annotations from Multiple Humans</title><link>http://arxiv.org/abs/2407.20950v1</link><description>Human annotators typically provide annotated data for training machinelearning models, such as neural networks. Yet, human annotations are subject tonoise, impairing generalization performances. Methodological research onapproaches counteracting noisy annotations requires corresponding datasets fora meaningful empirical evaluation. Consequently, we introduce a novel benchmarkdataset, dopanim, consisting of about 15,750 animal images of 15 classes withground truth labels. For approximately 10,500 of these images, 20 humansprovided over 52,000 annotations with an accuracy of circa 67%. Its keyattributes include (1) the challenging task of classifying doppelgangeranimals, (2) human-estimated likelihoods as annotations, and (3) annotatormetadata. We benchmark well-known multi-annotator learning approaches usingseven variants of this dataset and outline further evaluation use cases such aslearning beyond hard class labels and active learning. Our dataset and acomprehensive codebase are publicly available to emulate the data collectionprocess and to reproduce all empirical results.</description><author>Marek Herde, Denis Huseljic, Lukas Rauch, Bernhard Sick</author><pubDate>Tue, 30 Jul 2024 16:27:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20950v1</guid></item><item><title>An Asynchronous Multi-core Accelerator for SNN inference</title><link>http://arxiv.org/abs/2407.20947v1</link><description>Spiking Neural Networks (SNNs) are extensively utilized in brain-inspiredcomputing and neuroscience research. To enhance the speed and energy efficiencyof SNNs, several many-core accelerators have been developed. However,maintaining the accuracy of SNNs often necessitates frequent explicitsynchronization among all cores, which presents a challenge to overallefficiency. In this paper, we propose an asynchronous architecture for SpikingNeural Networks (SNNs) that eliminates the need for inter-core synchronization,thus enhancing speed and energy efficiency. This approach leverages thepre-determined dependencies of neuromorphic cores established duringcompilation. Each core is equipped with a scheduler that monitors the status ofits dependencies, allowing it to safely advance to the next timestep withoutwaiting for other cores. This eliminates the necessity for globalsynchronization and minimizes core waiting time despite inherent workloadimbalances. Comprehensive evaluations using five different SNN workloads showthat our architecture achieves a 1.86x speedup and a 1.55x increase in energyefficiency compared to state-of-the-art synchronization architectures.</description><author>Zhuo Chen, De Ma, Xiaofei Jin, Qinghui Xing, Ouwen Jin, Xin Du, Shuibing He, Gang Pan</author><pubDate>Tue, 30 Jul 2024 16:25:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20947v1</guid></item><item><title>EAR: Edge-Aware Reconstruction of 3-D vertebrae structures from bi-planar X-ray images</title><link>http://arxiv.org/abs/2407.20937v1</link><description>X-ray images ease the diagnosis and treatment process due to their rapidimaging speed and high resolution. However, due to the projection process ofX-ray imaging, much spatial information has been lost. To accurately provideefficient spinal morphological and structural information, reconstructing the3-D structures of the spine from the 2-D X-ray images is essential. It ischallenging for current reconstruction methods to preserve the edge informationand local shapes of the asymmetrical vertebrae structures. In this study, wepropose a new Edge-Aware Reconstruction network (EAR) to focus on theperformance improvement of the edge information and vertebrae shapes. In ournetwork, by using the auto-encoder architecture as the backbone, the edgeattention module and frequency enhancement module are proposed to strengthenthe perception of the edge reconstruction. Meanwhile, we also combine four lossterms, including reconstruction loss, edge loss, frequency loss and projectionloss. The proposed method is evaluated using three publicly accessible datasetsand compared with four state-of-the-art models. The proposed method is superiorto other methods and achieves 25.32%, 15.32%, 86.44%, 80.13%, 23.7612 and0.3014 with regard to MSE, MAE, Dice, SSIM, PSNR and frequency distance. Due tothe end-to-end and accurate reconstruction process, EAR can provide sufficient3-D spatial information and precise preoperative surgical planning guidance.</description><author>Lixing Tan, Shuang Song, Yaofeng He, Kangneng Zhou, Tong Lu, Ruoxiu Xiao</author><pubDate>Tue, 30 Jul 2024 16:19:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20937v1</guid></item><item><title>On the Exploitation of DCT-Traces in the Generative-AI Domain</title><link>http://arxiv.org/abs/2402.02209v3</link><description>Deepfakes represent one of the toughest challenges in the world ofCybersecurity and Digital Forensics, especially considering the high-qualityresults obtained with recent generative AI-based solutions. Almost allgenerative models leave unique traces in synthetic data that, if analyzed andidentified in detail, can be exploited to improve the generalizationlimitations of existing deepfake detectors. In this paper we analyzed deepfakeimages in the frequency domain generated by both GAN and Diffusion Modelengines, examining in detail the underlying statistical distribution ofDiscrete Cosine Transform (DCT) coefficients. Recognizing that not allcoefficients contribute equally to image detection, we hypothesize theexistence of a unique ``discriminative fingerprint", embedded in specificcombinations of coefficients. To identify them, Machine Learning classifierswere trained on various combinations of coefficients. In addition, theExplainable AI (XAI) LIME algorithm was used to search for intrinsicdiscriminative combinations of coefficients. Finally, we performed a robustnesstest to analyze the persistence of traces by applying JPEG compression. Theexperimental results reveal the existence of traces left by the generativemodels that are more discriminative and persistent at JPEG attacks. Code anddataset are available at https://github.com/opontorno/dcts_analysis_deepfakes.</description><author>Orazio Pontorno, Luca Guarnera, Sebastiano Battiato</author><pubDate>Tue, 30 Jul 2024 16:16:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02209v3</guid></item><item><title>Complete Approximations of Incomplete Queries</title><link>http://arxiv.org/abs/2407.20932v1</link><description>This paper studies the completeness of conjunctive queries over a partiallycomplete database and the approximation of incomplete queries. Given a queryand a set of completeness rules (a special kind of tuple generatingdependencies) that specify which parts of the database are complete, weinvestigate whether the query can be fully answered, as if all data wereavailable. If not, we explore reformulating the query into either MaximalComplete Specializations (MCSs) or the (unique up to equivalence) MinimalComplete Generalization (MCG) that can be fully answered, that is, the bestcomplete approximations of the query from below or above in the sense of querycontainment. We show that the MSG can be characterized as the least fixed-pointof a monotonic operator in a preorder. Then, we show that an MCS can becomputed by recursive backward application of completeness rules. We study thecomplexity of both problems and discuss implementation techniques that rely onan ASP and Prolog engines, respectively.</description><author>Julien Corman, Werner Nutt, Ognjen Savković</author><pubDate>Tue, 30 Jul 2024 16:13:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20932v1</guid></item><item><title>Rethinking Radiology Report Generation via Causal Inspired Counterfactual Augmentation</title><link>http://arxiv.org/abs/2311.13307v3</link><description>Radiology Report Generation (RRG) draws attention as a vision-and-languageinteraction of biomedical fields. Previous works inherited the ideology oftraditional language generation tasks, aiming to generate paragraphs with highreadability as reports. Despite significant progress, the independence betweendiseases-a specific property of RRG-was neglected, yielding the models beingconfused by the co-occurrence of diseases brought on by the biased datadistribution, thus generating inaccurate reports. In this paper, to rethinkthis issue, we first model the causal effects between the variables from acausal perspective, through which we prove that the co-occurrence relationshipsbetween diseases on the biased distribution function as confounders, confusingthe accuracy through two backdoor paths, i.e. the Joint Vision Coupling and theConditional Sequential Coupling. Then, we proposed a novel model-agnosticcounterfactual augmentation method that contains two strategies, i.e. thePrototype-based Counterfactual Sample Synthesis (P-CSS) and the Magic-Cube-likeCounterfactual Report Reconstruction (Cube), to intervene the backdoor paths,thus enhancing the accuracy and generalization of RRG models. Experimentalresults on the widely used MIMIC-CXR dataset demonstrate the effectiveness ofour proposed method. Additionally, a generalization performance is evaluated onIU X-Ray dataset, which verifies our work can effectively reduce the impact ofco-occurrences caused by different distributions on the results.</description><author>Xiao Song, Jiafan Liu, Yun Li, Yan Liu, Wenbin Lei, Ruxin Wang</author><pubDate>Tue, 30 Jul 2024 16:11:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13307v3</guid></item><item><title>UniProcessor: A Text-induced Unified Low-level Image Processor</title><link>http://arxiv.org/abs/2407.20928v1</link><description>Image processing, including image restoration, image enhancement, etc.,involves generating a high-quality clean image from a degraded input. Deeplearning-based methods have shown superior performance for various imageprocessing tasks in terms of single-task conditions. However, they require totrain separate models for different degradations and levels, which limits thegeneralization abilities of these models and restricts their applications inreal-world. In this paper, we propose a text-induced unified image processorfor low-level vision tasks, termed UniProcessor, which can effectively processvarious degradation types and levels, and support multimodal control.Specifically, our UniProcessor encodes degradation-specific information withthe subject prompt and process degradations with the manipulation prompt. Thesecontext control features are injected into the UniProcessor backbone viacross-attention to control the processing procedure. For automaticsubject-prompt generation, we further build a vision-language model forgeneral-purpose low-level degradation perception via instruction tuningtechniques. Our UniProcessor covers 30 degradation types, and extensiveexperiments demonstrate that our UniProcessor can well process thesedegradations without additional training or tuning and outperforms othercompeting methods. Moreover, with the help of degradation-aware contextcontrol, our UniProcessor first shows the ability to individually handle asingle distortion in an image with multiple degradations.</description><author>Huiyu Duan, Xiongkuo Min, Sijing Wu, Wei Shen, Guangtao Zhai</author><pubDate>Tue, 30 Jul 2024 16:06:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20928v1</guid></item><item><title>When AI Meets Finance (StockAgent): Large Language Model-based Stock Trading in Simulated Real-world Environments</title><link>http://arxiv.org/abs/2407.18957v2</link><description>Can AI Agents simulate real-world trading environments to investigate theimpact of external factors on stock trading activities (e.g., macroeconomics,policy changes, company fundamentals, and global events)? These factors, whichfrequently influence trading behaviors, are critical elements in the quest formaximizing investors' profits. Our work attempts to solve this problem throughlarge language model based agents. We have developed a multi-agent AI systemcalled StockAgent, driven by LLMs, designed to simulate investors' tradingbehaviors in response to the real stock market. The StockAgent allows users toevaluate the impact of different external factors on investor trading and toanalyze trading behavior and profitability effects. Additionally, StockAgentavoids the test set leakage issue present in existing trading simulationsystems based on AI Agents. Specifically, it prevents the model from leveragingprior knowledge it may have acquired related to the test data. We evaluatedifferent LLMs under the framework of StockAgent in a stock trading environmentthat closely resembles real-world conditions. The experimental resultsdemonstrate the impact of key external factors on stock market trading,including trading behavior and stock price fluctuation rules. This researchexplores the study of agents' free trading gaps in the context of no priorknowledge related to market data. The patterns identified through StockAgentsimulations provide valuable insights for LLM-based investment advice and stockrecommendation. The code is available athttps://github.com/MingyuJ666/Stockagent.</description><author>Chong Zhang, Xinyi Liu, Mingyu Jin, Zhongmou Zhang, Lingyao Li, Zhenting Wang, Wenyue Hua, Dong Shu, Suiyuan Zhu, Xiaobo Jin, Sujian Li, Mengnan Du, Yongfeng Zhang</author><pubDate>Tue, 30 Jul 2024 16:01:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18957v2</guid></item><item><title>SSPA: Split-and-Synthesize Prompting with Gated Alignments for Multi-Label Image Recognition</title><link>http://arxiv.org/abs/2407.20920v1</link><description>Multi-label image recognition is a fundamental task in computer vision.Recently, Vision-Language Models (VLMs) have made notable advancements in thisarea. However, previous methods fail to effectively leverage the rich knowledgein language models and often incorporate label semantics into visual featuresunidirectionally. To overcome these problems, we propose a Split-and-SynthesizePrompting with Gated Alignments (SSPA) framework to amplify the potential ofVLMs. Specifically, we develop an in-context learning approach to associate theinherent knowledge from LLMs. Then we propose a novel Split-and-SynthesizePrompting (SSP) strategy to first model the generic knowledge and downstreamlabel semantics individually and then aggregate them carefully through thequaternion network. Moreover, we present Gated Dual-Modal Alignments (GDMA) tobidirectionally interact visual and linguistic modalities while eliminatingredundant cross-modal information, enabling more efficient region-levelalignments. Rather than making the final prediction by a sharp manner inprevious works, we propose a soft aggregator to jointly consider results fromall image regions. With the help of flexible prompting and gated alignments,SSPA is generalizable to specific domains. Extensive experiments on ninedatasets from three domains (i.e., natural, pedestrian attributes and remotesensing) demonstrate the state-of-the-art performance of SSPA. Further analysesverify the effectiveness of SSP and the interpretability of GDMA. The code willbe made public.</description><author>Hao Tan, Zichang Tan, Jun Li, Jun Wan, Zhen Lei, Stan Z. Li</author><pubDate>Tue, 30 Jul 2024 15:58:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20920v1</guid></item><item><title>The Realizability of Revision and Contraction Operators in Epistemic Spaces</title><link>http://arxiv.org/abs/2407.20918v1</link><description>This paper studies the realizability of belief revision and beliefcontraction operators in epistemic spaces. We observe that AGM revision and AGMcontraction operators for epistemic spaces are only realizable in preciselydetermined epistemic spaces. We define the class of linear change operators, aspecial kind of maxichoice operator. When AGM revision, respectively, AGMcontraction, is realizable, linear change operators are a canonicalrealization.</description><author>Kai Sauerwald, Matthias Thimm</author><pubDate>Tue, 30 Jul 2024 15:55:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20918v1</guid></item><item><title>vSHARP: variable Splitting Half-quadratic Admm algorithm for Reconstruction of inverse-Problems</title><link>http://arxiv.org/abs/2309.09954v2</link><description>Medical Imaging (MI) tasks, such as accelerated parallel Magnetic ResonanceImaging (MRI), often involve reconstructing an image from noisy or incompletemeasurements. This amounts to solving ill-posed inverse problems, where asatisfactory closed-form analytical solution is not available. Traditionalmethods such as Compressed Sensing (CS) in MRI reconstruction can betime-consuming or prone to obtaining low-fidelity images. Recently, a plethoraof Deep Learning (DL) approaches have demonstrated superior performance ininverse-problem solving, surpassing conventional methods. In this study, wepropose vSHARP (variable Splitting Half-quadratic ADMM algorithm forReconstruction of inverse Problems), a novel DL-based method for solvingill-posed inverse problems arising in MI. vSHARP utilizes the Half-QuadraticVariable Splitting method and employs the Alternating Direction Method ofMultipliers (ADMM) to unroll the optimization process. For data consistency,vSHARP unrolls a differentiable gradient descent process in the image domain,while a DL-based denoiser, such as a U-Net architecture, is applied to enhanceimage quality. vSHARP also employs a dilated-convolution DL-based model topredict the Lagrange multipliers for the ADMM initialization. We evaluatevSHARP on tasks of accelerated parallel MRI Reconstruction using two distinctdatasets and on accelerated parallel dynamic MRI Reconstruction using anotherdataset. Our comparative analysis with state-of-the-art methods demonstratesthe superior performance of vSHARP in these applications.</description><author>George Yiasemis, Nikita Moriakov, Jan-Jakob Sonke, Jonas Teuwen</author><pubDate>Tue, 30 Jul 2024 15:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09954v2</guid></item><item><title>How to Choose a Reinforcement-Learning Algorithm</title><link>http://arxiv.org/abs/2407.20917v1</link><description>The field of reinforcement learning offers a large variety of concepts andmethods to tackle sequential decision-making problems. This variety has becomeso large that choosing an algorithm for a task at hand can be challenging. Inthis work, we streamline the process of choosing reinforcement-learningalgorithms and action-distribution families. We provide a structured overviewof existing methods and their properties, as well as guidelines for when tochoose which methods. An interactive version of these guidelines is availableonline at https://rl-picker.github.io/.</description><author>Fabian Bongratz, Vladimir Golkov, Lukas Mautner, Luca Della Libera, Frederik Heetmeyer, Felix Czaja, Julian Rodemann, Daniel Cremers</author><pubDate>Tue, 30 Jul 2024 15:54:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20917v1</guid></item><item><title>Ponymation: Learning 3D Animal Motions from Unlabeled Online Videos</title><link>http://arxiv.org/abs/2312.13604v2</link><description>We introduce Ponymation, a new method for learning a generative model ofarticulated 3D animal motions from raw, unlabeled online videos. Unlikeexisting approaches for motion synthesis, our model does not require any poseannotations or parametric shape models for training, and is learned purely froma collection of raw video clips obtained from the Internet. We build upon arecent work, MagicPony, which learns articulated 3D animal shapes purely fromsingle image collections, and extend it on two fronts. First, instead oftraining on static images, we augment the framework with a video trainingpipeline that incorporates temporal regularizations, achieving more accurateand temporally consistent reconstructions. Second, we learn a generative modelof the underlying articulated 3D motion sequences via a spatio-temporaltransformer VAE, simply using 2D reconstruction losses without relying on anyexplicit pose annotations. At inference time, given a single 2D image of a newanimal instance, our model reconstructs an articulated, textured 3D mesh, andgenerates plausible 3D animations by sampling from the learned motion latentspace.</description><author>Keqiang Sun, Dor Litvak, Yunzhi Zhang, Hongsheng Li, Jiajun Wu, Shangzhe Wu</author><pubDate>Tue, 30 Jul 2024 15:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13604v2</guid></item><item><title>Sequential Knockoffs for Variable Selection in Reinforcement Learning</title><link>http://arxiv.org/abs/2303.14281v2</link><description>In real-world applications of reinforcement learning, it is often challengingto obtain a state representation that is parsimonious and satisfies the Markovproperty without prior knowledge. Consequently, it is common practice toconstruct a state larger than necessary, e.g., by concatenating measurementsover contiguous time points. However, needlessly increasing the dimension ofthe state may slow learning and obfuscate the learned policy. We introduce thenotion of a minimal sufficient state in a Markov decision process (MDP) as thesubvector of the original state under which the process remains an MDP andshares the same reward function as the original process. We propose a novelSEquEntial Knockoffs (SEEK) algorithm that estimates the minimal sufficientstate in a system with high-dimensional complex nonlinear dynamics. In largesamples, the proposed method achieves selection consistency. As the method isagnostic to the reinforcement learning algorithm being applied, it benefitsdownstream tasks such as policy learning. Empirical experiments verifytheoretical results and show the proposed approach outperforms severalcompeting methods regarding variable selection accuracy and regret.</description><author>Tao Ma, Jin Zhu, Hengrui Cai, Zhengling Qi, Yunxiao Chen, Chengchun Shi, Eric B. Laber</author><pubDate>Tue, 30 Jul 2024 15:42:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14281v2</guid></item><item><title>SceneX:Procedural Controllable Large-scale Scene Generation via Large-language Models</title><link>http://arxiv.org/abs/2403.15698v2</link><description>Due to its great application potential, large-scale scene generation hasdrawn extensive attention in academia and industry. Recent research employspowerful generative models to create desired scenes and achieves promisingresults. However, most of these methods represent the scene using 3D primitives(e.g. point cloud or radiance field) incompatible with the industrial pipeline,which leads to a substantial gap between academic research and industrialdeployment. Procedural Controllable Generation (PCG) is an efficient techniquefor creating scalable and high-quality assets, but it is unfriendly forordinary users as it demands profound domain expertise. To address theseissues, we resort to using the large language model (LLM) to drive theprocedural modeling. In this paper, we introduce a large-scale scene generationframework, SceneX, which can automatically produce high-quality proceduralmodels according to designers' textual descriptions.Specifically, the proposedmethod comprises two components, PCGBench and PCGPlanner. The formerencompasses an extensive collection of accessible procedural assets andthousands of hand-craft API documents. The latter aims to generate executableactions for Blender to produce controllable and precise 3D assets guided by theuser's instructions. Our SceneX can generate a city spanning 2.5 km times 2.5km with delicate layout and geometric structures, drastically reducing the timecost from several weeks for professional PCG engineers to just a few hours foran ordinary user. Extensive experiments demonstrated the capability of ourmethod in controllable large-scale scene generation and editing, includingasset placement and season translation.</description><author>Mengqi Zhou, Yuxi Wang, Jun Hou, Chuanchen Luo, Zhaoxiang Zhang, Junran Peng</author><pubDate>Tue, 30 Jul 2024 15:41:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15698v2</guid></item><item><title>What Are Good Positional Encodings for Directed Graphs?</title><link>http://arxiv.org/abs/2407.20912v1</link><description>Positional encodings (PE) for graphs are essential in constructing powerfuland expressive graph neural networks and graph transformers as they effectivelycapture relative spatial relations between nodes. While PEs for undirectedgraphs have been extensively studied, those for directed graphs remain largelyunexplored, despite the fundamental role of directed graphs in representingentities with strong logical dependencies, such as those in program analysisand circuit designs. This work studies the design of PEs for directed graphsthat are expressive to represent desired directed spatial relations. We firstpropose walk profile, a generalization of walk counting sequence to directedgraphs. We identify limitations in existing PE methods, including symmetrizedLaplacian PE, Singular Value Decomposition PE, and Magnetic Laplacian PE, intheir ability to express walk profiles. To address these limitations, wepropose the Multi-q Magnetic Laplacian PE, which extends Magnetic Laplacian PEwith multiple potential factors. This simple variant turns out to be capable ofprovably expressing walk profiles. Furthermore, we generalize previousbasis-invariant and stable networks to handle complex-domain PEs decomposedfrom Magnetic Laplacians. Our numerical experiments demonstrate theeffectiveness of Multi-q Magnetic Laplacian PE with a stable neuralarchitecture, outperforming previous PE methods (with stable networks) onpredicting directed distances/walk profiles, sorting network satisfiability,and on general circuit benchmarks. Our code is available athttps://github.com/Graph-COM/Multi-q-Maglap.</description><author>Yinan Huang, Haoyu Wang, Pan Li</author><pubDate>Tue, 30 Jul 2024 15:38:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20912v1</guid></item><item><title>Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation</title><link>http://arxiv.org/abs/2407.20910v1</link><description>Automated soft moderation systems are unable to ascertain if a post supportsor refutes a false claim, resulting in a large number of contextual falsepositives. This limits their effectiveness, for example undermining trust inhealth experts by adding warnings to their posts or resorting to vague warningsinstead of granular fact-checks, which result in desensitizing users. In thispaper, we propose to incorporate stance detection into existing automatedsoft-moderation pipelines, with the goal of ruling out contextual falsepositives and providing more precise recommendations for social media contentthat should receive warnings. We develop a textual deviation task calledContrastive Textual Deviation (CTD) and show that it outperforms existingstance detection approaches when applied to soft moderation.We then integrateCTD into the stateof-the-art system for automated soft moderation Lambretta,showing that our approach can reduce contextual false positives from 20% to2.1%, providing another important building block towards deploying reliableautomated soft moderation tools on social media.</description><author>Pujan Paudel, Mohammad Hammas Saeed, Rebecca Auger, Chris Wells, Gianluca Stringhini</author><pubDate>Tue, 30 Jul 2024 15:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20910v1</guid></item><item><title>Dynamic Scene Understanding through Object-Centric Voxelization and Neural Rendering</title><link>http://arxiv.org/abs/2407.20908v1</link><description>Learning object-centric representations from unsupervised videos ischallenging. Unlike most previous approaches that focus on decomposing 2Dimages, we present a 3D generative model named DynaVol-S for dynamic scenesthat enables object-centric learning within a differentiable volume renderingframework. The key idea is to perform object-centric voxelization to capturethe 3D nature of the scene, which infers per-object occupancy probabilities atindividual spatial locations. These voxel features evolve through acanonical-space deformation function and are optimized in an inverse renderingpipeline with a compositional NeRF. Additionally, our approach integrates 2Dsemantic features to create 3D semantic grids, representing the scene throughmultiple disentangled voxel grids. DynaVol-S significantly outperforms existingmodels in both novel view synthesis and unsupervised decomposition tasks fordynamic scenes. By jointly considering geometric structures and semanticfeatures, it effectively addresses challenging real-world scenarios involvingcomplex object interactions. Furthermore, once trained, the explicitlymeaningful voxel features enable additional capabilities that 2D scenedecomposition methods cannot achieve, such as novel scene generation throughediting geometric shapes or manipulating the motion trajectories of objects.</description><author>Yanpeng Zhao, Yiwei Hao, Siyu Gao, Yunbo Wang, Xiaokang Yang</author><pubDate>Tue, 30 Jul 2024 15:33:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20908v1</guid></item><item><title>SpaER: Learning Spatio-temporal Equivariant Representations for Fetal Brain Motion Tracking</title><link>http://arxiv.org/abs/2407.20198v2</link><description>In this paper, we introduce SpaER, a pioneering method for fetal motiontracking that leverages equivariant filters and self-attention mechanisms toeffectively learn spatio-temporal representations. Different from conventionalapproaches that statically estimate fetal brain motions from pairs of images,our method dynamically tracks the rigid movement patterns of the fetal headacross temporal and spatial dimensions. Specifically, we first develop anequivariant neural network that efficiently learns rigid motion sequencesthrough low-dimensional spatial representations of images. Subsequently, welearn spatio-temporal representations by incorporating time encoding andself-attention neural network layers. This approach allows for the capture oflong-term dependencies of fetal brain motion and addresses alignment errors dueto contrast changes and severe motion artifacts. Our model also provides ageometric deformation estimation that properly addresses image distortionsamong all time frames. To the best of our knowledge, our approach is the firstto learn spatial-temporal representations via deep neural networks for fetalmotion tracking without data augmentation. We validated our model using realfetal echo-planar images with simulated and real motions. Our method carriessignificant potential value in accurately measuring, tracking, and correctingfetal motion in fetal MRI sequences.</description><author>Jian Wang, Razieh Faghihpirayesh, Polina Golland, Ali Ghoulipour</author><pubDate>Tue, 30 Jul 2024 15:33:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20198v2</guid></item><item><title>Automated Review Generation Method Based on Large Language Models</title><link>http://arxiv.org/abs/2407.20906v1</link><description>Literature research, vital for scientific advancement, is overwhelmed by thevast ocean of available information. Addressing this, we propose an automatedreview generation method based on Large Language Models (LLMs) to streamlineliterature processing and reduce cognitive load. In case study on propanedehydrogenation (PDH) catalysts, our method swiftly generated comprehensivereviews from 343 articles, averaging seconds per article per LLM account.Extended analysis of 1041 articles provided deep insights into catalysts'composition, structure, and performance. Recognizing LLMs' hallucinations, weemployed a multi-layered quality control strategy, ensuring our method'sreliability and effective hallucination mitigation. Expert verificationconfirms the accuracy and citation integrity of generated reviews,demonstrating LLM hallucination risks reduced to below 0.5% with over 95%confidence. Released Windows application enables one-click review generation,aiding researchers in tracking advancements and recommending literature. Thisapproach showcases LLMs' role in enhancing scientific research productivity andsets the stage for further exploration.</description><author>Shican Wu, Xiao Ma, Dehui Luo, Lulu Li, Xiangcheng Shi, Xin Chang, Xiaoyun Lin, Ran Luo, Chunlei Pei, Zhi-Jian Zhao, Jinlong Gong</author><pubDate>Tue, 30 Jul 2024 15:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20906v1</guid></item><item><title>Machine learning surrogates for efficient hydrologic modeling: Insights from stochastic simulations of managed aquifer recharge</title><link>http://arxiv.org/abs/2407.20902v1</link><description>Process-based hydrologic models are invaluable tools for understanding theterrestrial water cycle and addressing modern water resources problems.However, many hydrologic models are computationally expensive and, depending onthe resolution and scale, simulations can take on the order of hours to days tocomplete. While techniques such as uncertainty quantification and optimizationhave become valuable tools for supporting management decisions, these analysestypically require hundreds of model simulations, which are too computationallyexpensive to perform with a process-based hydrologic model. To address thisgap, we propose a hybrid modeling workflow in which a process-based model isused to generate an initial set of simulations and a machine learning (ML)surrogate model is then trained to perform the remaining simulations requiredfor downstream analysis. As a case study, we apply this workflow to simulationsof variably saturated groundwater flow at a prospective managed aquiferrecharge (MAR) site. We compare the accuracy and computational efficiency ofseveral ML architectures, including deep convolutional networks, recurrentneural networks, vision transformers, and networks with Fourier transforms. Ourresults demonstrate that ML surrogate models can achieve under 10% meanabsolute percentage error and yield order-of-magnitude runtime savings overprocessed-based models. We also offer practical recommendations for traininghydrologic surrogate models, including implementing data normalization toimprove accuracy, using a normalized loss function to improve trainingstability and downsampling input features to decrease memory requirements.</description><author>Timothy Dai, Kate Maher, Zach Perzan</author><pubDate>Tue, 30 Jul 2024 15:24:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20902v1</guid></item><item><title>Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach</title><link>http://arxiv.org/abs/2407.20899v1</link><description>Existing explanation methods for image classification struggle to providefaithful and plausible explanations. This paper addresses this issue byproposing a post-hoc natural language explanation method that can be applied toany CNN-based classifier without altering its training process or affectingpredictive performance. By analysing influential neurons and the correspondingactivation maps, the method generates a faithful description of theclassifier's decision process in the form of a structured meaningrepresentation, which is then converted into text by a language model. Throughthis pipeline approach, the generated explanations are grounded in the neuralnetwork architecture, providing accurate insight into the classificationprocess while remaining accessible to non-experts. Experimental results showthat the NLEs constructed by our method are significantly more plausible andfaithful. In particular, user interventions in the neural network structure(masking of neurons) are three times more effective than the baselines.</description><author>Adam Wojciechowski, Mateusz Lango, Ondrej Dusek</author><pubDate>Tue, 30 Jul 2024 15:17:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20899v1</guid></item><item><title>Graph Reinforcement Learning in Power Grids: A Survey</title><link>http://arxiv.org/abs/2407.04522v2</link><description>The challenges posed by renewable energy and distributed electricitygeneration motivate the development of deep learning approaches to overcome thelack of flexibility of traditional methods in power grids use cases. Theapplication of GNNs is particularly promising due to their ability to learnfrom graph-structured data present in power grids. Combined with RL, they canserve as control approaches to determine remedial grid actions. This reviewanalyses the ability of GRL to capture the inherent graph structure of powergrids to improve representation learning and decision making in different powergrid use cases. It distinguishes between common problems in transmission anddistribution grids and explores the synergy between RL and GNNs. Intransmission grids, GRL typically addresses automated grid management andtopology control, whereas on the distribution side, GRL concentrates more onvoltage regulation. We analyzed the selected papers based on their graphstructure and GNN model, the applied RL algorithm, and their overallcontributions. Although GRL demonstrate adaptability in the face ofunpredictable events and noisy or incomplete data, it primarily serves as aproof of concept at this stage. There are multiple open challenges andlimitations that need to be addressed when considering the application of RL toreal power grid operation.</description><author>Mohamed Hassouna, Clara Holzhüter, Pawel Lytaev, Josephine Thomas, Bernhard Sick, Christoph Scholz</author><pubDate>Tue, 30 Jul 2024 15:14:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04522v2</guid></item><item><title>MambaCapsule: Towards Transparent Cardiac Disease Diagnosis with Electrocardiography Using Mamba Capsule Network</title><link>http://arxiv.org/abs/2407.20893v1</link><description>Cardiac arrhythmia, a condition characterized by irregular heartbeats, oftenserves as an early indication of various heart ailments. With the advent ofdeep learning, numerous innovative models have been introduced for diagnosingarrhythmias using Electrocardiogram (ECG) signals. However, recent studiessolely focus on the performance of models, neglecting the interpretation oftheir results. This leads to a considerable lack of transparency, posing asignificant risk in the actual diagnostic process. To solve this problem, thispaper introduces MambaCapsule, a deep neural networks for ECG arrhythmiasclassification, which increases the explainability of the model while enhancingthe accuracy.Our model utilizes Mamba for feature extraction and Capsulenetworks for prediction, providing not only a confidence score but also signalfeatures. Akin to the processing mechanism of human brain, the model learnssignal features and their relationship between them by reconstructing ECGsignals in the predicted selection. The model evaluation was conducted onMIT-BIH and PTB dataset, following the AAMI standard. MambaCapsule has achieveda total accuracy of 99.54% and 99.59% on the test sets respectively. Theseresults demonstrate the promising performance of under the standard testprotocol.</description><author>Yinlong Xu, Xiaoqiang Liu, Zitai Kong, Yixuan Wu, Yue Wang, Yingzhou Lu, Honghao Gao, Jian Wu, Hongxia Xu</author><pubDate>Tue, 30 Jul 2024 15:12:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20893v1</guid></item><item><title>What is YOLOv5: A deep look into the internal features of the popular object detector</title><link>http://arxiv.org/abs/2407.20892v1</link><description>This study presents a comprehensive analysis of the YOLOv5 object detectionmodel, examining its architecture, training methodologies, and performance. Keycomponents, including the Cross Stage Partial backbone and PathAggregation-Network, are explored in detail. The paper reviews the model'sperformance across various metrics and hardware platforms. Additionally, thestudy discusses the transition from Darknet to PyTorch and its impact on modeldevelopment. Overall, this research provides insights into YOLOv5'scapabilities and its position within the broader landscape of object detectionand why it is a popular choice for constrained edge deployment scenarios.</description><author>Rahima Khanam, Muhammad Hussain</author><pubDate>Tue, 30 Jul 2024 15:09:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20892v1</guid></item><item><title>Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian Neural Networks</title><link>http://arxiv.org/abs/2407.20891v1</link><description>Computational complexity of Bayesian learning is impeding its adoption inpractical, large-scale tasks. Despite demonstrations of significant merits suchas improved robustness and resilience to unseen or out-of-distribution inputsover their non- Bayesian counterparts, their practical use has faded to nearinsignificance. In this study, we introduce an innovative framework to mitigatethe computational burden of Bayesian neural networks (BNNs). Our approachfollows the principle of Bayesian techniques based on deep ensembles, butsignificantly reduces their cost via multiple low-rank perturbations ofparameters arising from a pre-trained neural network. Both vanilla version ofensembles as well as more sophisticated schemes such as Bayesian learning withStein Variational Gradient Descent (SVGD), previously deemed impractical forlarge models, can be seamlessly implemented within the proposed framework,called Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves adramatic reduction in the number of trainable parameters required toapproximate a Bayesian posterior; and ii) it not only maintains, but in someinstances, surpasses the performance of conventional Bayesian learning methodsand non-Bayesian baselines. Our results with large-scale tasks such asImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate theeffectiveness and versatility of Bella in building highly scalable andpractical Bayesian deep models for real-world applications.</description><author>Bao Gia Doan, Afshar Shamsi, Xiao-Yu Guo, Arash Mohammadi, Hamid Alinejad-Rokny, Dino Sejdinovic, Damith C. Ranasinghe, Ehsan Abbasnejad</author><pubDate>Tue, 30 Jul 2024 15:07:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20891v1</guid></item><item><title>Learning reduced-order Quadratic-Linear models in Process Engineering using Operator Inference</title><link>http://arxiv.org/abs/2402.17698v2</link><description>In this work, we address the challenge of efficiently modeling dynamicalsystems in process engineering. We use reduced-order model learning,specifically operator inference. This is a non-intrusive, data-driven methodfor learning dynamical systems from time-domain data. The application in ourstudy is carbon dioxide methanation, an important reaction within thePower-to-X framework, to demonstrate its potential. The numerical results showthe ability of the reduced-order models constructed with operator inference toprovide a reduced yet accurate surrogate solution. This represents an importantmilestone towards the implementation of fast and reliable digital twinarchitectures.</description><author>Ion Victor Gosea, Luisa Peterson, Pawan Goyal, Jens Bremer, Kai Sundmacher, Peter Benner</author><pubDate>Tue, 30 Jul 2024 15:06:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17698v2</guid></item><item><title>Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences</title><link>http://arxiv.org/abs/2310.11960v3</link><description>Transformer-based models have achieved state-of-the-art performance in manyareas. However, the quadratic complexity of self-attention with respect to theinput length hinders the applicability of Transformer-based models to longsequences. To address this, we present Fast Multipole Attention, a newattention mechanism that uses a divide-and-conquer strategy to reduce the timeand memory complexity of attention for sequences of length $n$ from$\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining aglobal receptive field. The hierarchical approach groups queries, keys, andvalues into $\mathcal{O}( \log n)$ levels of resolution, where groups atgreater distances are increasingly larger in size and the weights to computegroup quantities are learned. As such, the interaction between tokens far fromeach other is considered in lower resolution in an efficient hierarchicalmanner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$or $\mathcal{O}(n \log n)$, depending on whether the queries are down-sampledor not. This multi-level divide-and-conquer strategy is inspired by fastsummation methods from $n$-body physics and the Fast Multipole Method. Weperform evaluation on autoregressive and bidirectional language modeling tasksand compare our Fast Multipole Attention model with other efficient attentionvariants on medium-size datasets. We find empirically that the Fast MultipoleTransformer performs much better than other efficient transformers in terms ofmemory size and accuracy. The Fast Multipole Attention mechanism has thepotential to empower large language models with much greater sequence lengths,taking the full context into account in an efficient, naturally hierarchicalmanner during training and when generating long sequences.</description><author>Yanming Kang, Giang Tran, Hans De Sterck</author><pubDate>Tue, 30 Jul 2024 15:02:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11960v3</guid></item><item><title>Effective Black Box Testing of Sentiment Analysis Classification Networks</title><link>http://arxiv.org/abs/2407.20884v1</link><description>Transformer-based neural networks have demonstrated remarkable performance innatural language processing tasks such as sentiment analysis. Nevertheless, theissue of ensuring the dependability of these complicated architectures throughcomprehensive testing is still open. This paper presents a collection ofcoverage criteria specifically designed to assess test suites created fortransformer-based sentiment analysis networks. Our approach utilizes inputspace partitioning, a black-box method, by considering emotionally relevantlinguistic features such as verbs, adjectives, adverbs, and nouns. In order toeffectively produce test cases that encompass a wide range of emotionalelements, we utilize the k-projection coverage metric. This metric minimizesthe complexity of the problem by examining subsets of k features at the sametime, hence reducing dimensionality. Large language models are employed togenerate sentences that display specific combinations of emotional features.The findings from experiments obtained from a sentiment analysis datasetillustrate that our criteria and generated tests have led to an averageincrease of 16\% in test coverage. In addition, there is a correspondingaverage decrease of 6.5\% in model accuracy, showing the ability to identifyvulnerabilities. Our work provides a foundation for improving the dependabilityof transformer-based sentiment analysis systems through comprehensive testevaluation.</description><author>Parsa Karbasizadeh, Fathiyeh Faghih, Pouria Golshanrad</author><pubDate>Tue, 30 Jul 2024 14:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20884v1</guid></item><item><title>A Scalable Tool For Analyzing Genomic Variants Of Humans Using Knowledge Graphs and Machine Learning</title><link>http://arxiv.org/abs/2407.20879v1</link><description>The integration of knowledge graphs and graph machine learning (GML) ingenomic data analysis offers several opportunities for understanding complexgenetic relationships, especially at the RNA level. We present a comprehensiveapproach for leveraging these technologies to analyze genomic variants,specifically in the context of RNA sequencing (RNA-seq) data from COVID-19patient samples. The proposed method involves extracting variant-level geneticinformation, annotating the data with additional metadata using SnpEff, andconverting the enriched Variant Call Format (VCF) files into ResourceDescription Framework (RDF) triples. The resulting knowledge graph is furtherenhanced with patient metadata and stored in a graph database, facilitatingefficient querying and indexing. We utilize the Deep Graph Library (DGL) toperform graph machine learning tasks, including node classification withGraphSAGE and Graph Convolutional Networks (GCNs). Our approach demonstratessignificant utility using our proposed tool, VariantKG, in three key scenarios:enriching graphs with new VCF data, creating subgraphs based on user-definedfeatures, and conducting graph machine learning for node classification.</description><author>Shivika Prasanna, Ajay Kumar, Deepthi Rao, Eduardo Simoes, Praveen Rao</author><pubDate>Tue, 30 Jul 2024 14:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20879v1</guid></item><item><title>S3PET: Semi-supervised Standard-dose PET Image Reconstruction via Dose-aware Token Swap</title><link>http://arxiv.org/abs/2407.20878v1</link><description>To acquire high-quality positron emission tomography (PET) images whilereducing the radiation tracer dose, numerous efforts have been devoted toreconstructing standard-dose PET (SPET) images from low-dose PET (LPET).However, the success of current fully-supervised approaches relies on abundantpaired LPET and SPET images, which are often unavailable in clinic. Moreover,these methods often mix the dose-invariant content with dose level-relateddose-specific details during reconstruction, resulting in distorted images. Toalleviate these problems, in this paper, we propose a two-stage Semi-SupervisedSPET reconstruction framework, namely S3PET, to accommodate the training ofabundant unpaired and limited paired SPET and LPET images. Our S3PET involvesan un-supervised pre-training stage (Stage I) to extract representations fromunpaired images, and a supervised dose-aware reconstruction stage (Stage II) toachieve LPET-to-SPET reconstruction by transferring the dose-specific knowledgebetween paired images. Specifically, in stage I, two independent dose-specificmasked autoencoders (DsMAEs) are adopted to comprehensively understand theunpaired SPET and LPET images. Then, in Stage II, the pre-trained DsMAEs arefurther finetuned using paired images. To prevent distortions in both contentand details, we introduce two elaborate modules, i.e., a dose knowledgedecouple module to disentangle the respective dose-specific and dose-invariantknowledge of LPET and SPET, and a dose-specific knowledge learning module totransfer the dose-specific information from SPET to LPET, thereby achievinghigh-quality SPET reconstruction from LPET images. Experiments on two datasetsdemonstrate that our S3PET achieves state-of-the-art performance quantitativelyand qualitatively.</description><author>Jiaqi Cui, Pinxian Zeng, Yuanyuan Xu, Xi Wu, Jiliu Zhou, Yan Wang</author><pubDate>Tue, 30 Jul 2024 14:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20878v1</guid></item><item><title>Automatic Die Studies for Ancient Numismatics</title><link>http://arxiv.org/abs/2407.20876v1</link><description>Die studies are fundamental to quantifying ancient monetary production,providing insights into the relationship between coinage, politics, andhistory. The process requires tedious manual work, which limits the size of thecorpora that can be studied. Few works have attempted to automate this task,and none have been properly released and evaluated from a computer visionperspective. We propose a fully automatic approach that introduces severalinnovations compared to previous methods. We rely on fast and robust localdescriptors matching that is set automatically. Second, the core of ourproposal is a clustering-based approach that uses an intrinsic metric (thatdoes not need the ground truth labels) to determine its criticalhyper-parameters. We validate the approach on two corpora of Greek coins,propose an automatic implementation and evaluation of previous baselines, andshow that our approach significantly outperforms them.</description><author>Clément Cornet, Héloïse Aumaître, Romaric Besançon, Julien Olivier, Thomas Faucher, Hervé Le Borgne</author><pubDate>Tue, 30 Jul 2024 14:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20876v1</guid></item><item><title>Semi-supervised learning via DQN for log anomaly detection</title><link>http://arxiv.org/abs/2401.03151v2</link><description>Log anomaly detection is a critical component in modern software systemsecurity and maintenance, serving as a crucial support and basis for systemmonitoring, operation, and troubleshooting. It aids operations personnel intimely identification and resolution of issues. However, current methods in loganomaly detection still face challenges such as underutilization of unlabeleddata, imbalance between normal and anomaly class data, and high rates of falsepositives and false negatives, leading to insufficient effectiveness in anomalyrecognition. In this study, we propose a semi-supervised log anomaly detectionmethod named DQNLog, which integrates deep reinforcement learning to enhanceanomaly detection performance by leveraging a small amount of labeled data andlarge-scale unlabeled data. To address issues of imbalanced data andinsufficient labeling, we design a state transition function biased towardsanomalies based on cosine similarity, aiming to capture semantic-similaranomalies rather than favoring the majority class. To enhance the model'scapability in learning anomalies, we devise a joint reward function thatencourages the model to utilize labeled anomalies and explore unlabeledanomalies, thereby reducing false positives and false negatives. Additionally,to prevent the model from deviating from normal trajectories due tomisestimation, we introduce a regularization term in the loss function toensure the model retains prior knowledge during updates. We evaluate DQNLog onthree widely used datasets, demonstrating its ability to effectively utilizelarge-scale unlabeled data and achieve promising results across allexperimental datasets.</description><author>Yingying He, Xiaobing Pei</author><pubDate>Tue, 30 Jul 2024 14:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.03151v2</guid></item><item><title>A Comparative Analysis of YOLOv5, YOLOv8, and YOLOv10 in Kitchen Safety</title><link>http://arxiv.org/abs/2407.20872v1</link><description>Knife safety in the kitchen is essential for preventing accidents or injurieswith an emphasis on proper handling, maintenance, and storage methods. Thisresearch presents a comparative analysis of three YOLO models, YOLOv5, YOLOv8,and YOLOv10, to detect the hazards involved in handling knife, concentratingmainly on ensuring fingers are curled while holding items to be cut and thathands should only be in contact with knife handle avoiding the blade.Precision, recall, F-score, and normalized confusion matrix are used toevaluate the performance of the models. The results indicate that YOLOv5performed better than the other two models in identifying the hazard ofensuring hands only touch the blade, while YOLOv8 excelled in detecting thehazard of curled fingers while holding items. YOLOv5 and YOLOv8 performedalmost identically in recognizing classes such as hand, knife, and vegetable,whereas YOLOv5, YOLOv8, and YOLOv10 accurately identified the cutting board.This paper provides insights into the advantages and shortcomings of thesemodels in real-world settings. Moreover, by detailing the optimization of YOLOarchitectures for safe knife handling, this study promotes the development ofincreased accuracy and efficiency in safety surveillance systems.</description><author>Athulya Sundaresan Geetha, Muhammad Hussain</author><pubDate>Tue, 30 Jul 2024 14:50:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20872v1</guid></item><item><title>JSSL: Joint Supervised and Self-supervised Learning for MRI Reconstruction</title><link>http://arxiv.org/abs/2311.15856v2</link><description>Purpose: MRI represents an important diagnostic modality; however, itsinherently slow acquisition process poses challenges in obtaining fully-sampledk-space data under motion. In the absence of fully-sampled acquisitions,serving as ground truths, training deep learning algorithms in a supervisedmanner to predict the underlying ground truth image becomes challenging. Toaddress this limitation, self-supervised methods have emerged as a viablealternative, leveraging available subsampled k-space data to train deep neuralnetworks for MRI reconstruction. Nevertheless, these approaches often fallshort when compared to supervised methods. Methods: We propose Joint Supervised and Self-supervised Learning (JSSL), anovel training approach for deep learning-based MRI reconstruction algorithmsaimed at enhancing reconstruction quality in cases where target datasetscontaining fully-sampled k-space measurements are unavailable. JSSL operates bysimultaneously training a model in a self-supervised learning setting, usingsubsampled data from the target dataset(s), and in a supervised learningmanner, utilizing datasets with fully-sampled k-space data, referred to asproxy datasets. We demonstrate JSSL's efficacy using subsampled prostate orcardiac MRI data as the target datasets, with fully-sampled brain and knee, orbrain, knee and prostate k-space acquisitions, respectively, as proxy datasets. Results: Our results showcase substantial improvements over conventionalself-supervised methods, validated using common image quality metrics.Furthermore, we provide theoretical motivations for JSSL and establishrule-of-thumb guidelines for training MRI reconstruction models. Conclusion: JSSL effectively enhances MRI reconstruction quality in scenarioswhere fully-sampled k-space data is not available, leveraging the strengths ofsupervised learning by incorporating proxy datasets.</description><author>George Yiasemis, Nikita Moriakov, Clara I. Sánchez, Jan-Jakob Sonke, Jonas Teuwen</author><pubDate>Tue, 30 Jul 2024 14:49:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15856v2</guid></item><item><title>Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity Prices</title><link>http://arxiv.org/abs/2403.05441v2</link><description>We present a first study of Bayesian forecasting of electricity prices tradedon the German continuous intraday market which fully incorporates parameteruncertainty. A particularly large set of endogenous and exogenous covariablesis used, handled through feature selection with Orthogonal Matching Pursuit(OMP) and regularising priors. Our target variable is the IDFull price index,forecasts are given in terms of posterior predictive distributions. Forvalidation we use the exceedingly volatile electricity prices of 2022, whichhave hardly been the subject of forecasting studies before. As a benchmarkmodel, we use all available intraday transactions at the time of forecastcreation to compute a current value for the IDFull. According to the weak-formefficiency hypothesis, it would not be possible to significantly improve thisbenchmark built from last price information. We do, however, observestatistically significant improvement in terms of both point measures andprobability scores. Finally, we challenge the declared gold standard of usingLASSO for feature selection in electricity price forecasting by presentingstrong statistical evidence that OMP leads to better forecasting performance.</description><author>Daniel Nickelsen, Gernot Müller</author><pubDate>Tue, 30 Jul 2024 14:46:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05441v2</guid></item><item><title>Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction</title><link>http://arxiv.org/abs/2407.20871v1</link><description>Structure encoding has proven to be the key feature to distinguishing linksin a graph. However, Structure encoding in the temporal graph keeps changing asthe graph evolves, repeatedly computing such features can be time-consuming dueto the high-order subgraph construction. We develop the Co-Neighbor EncodingSchema (CNES) to address this issue. Instead of recomputing the feature by thelink, CNES stores information in the memory to avoid redundant calculations.Besides, unlike the existing memory-based dynamic graph learning method thatstores node hidden states, we introduce a hashtable-based memory to compressthe adjacency matrix for efficient structure feature construction and updatingwith vector computation in parallel. Furthermore, CNES introduces aTemporal-Diverse Memory to generate long-term and short-term structure encodingfor neighbors with different structural information. A dynamic graph learningframework, Co-Neighbor Encoding Network (CNE-N), is proposed using theaforementioned techniques. Extensive experiments on thirteen public datasetsverify the effectiveness and efficiency of the proposed method.</description><author>Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, Bowen Du</author><pubDate>Tue, 30 Jul 2024 14:45:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20871v1</guid></item><item><title>Mean of Means: A 10-dollar Solution for Human Localization with Calibration-free and Unconstrained Camera Settings</title><link>http://arxiv.org/abs/2407.20870v1</link><description>Accurate human localization is crucial for various applications, especiallyin the Metaverse era. Existing high precision solutions rely on expensive,tag-dependent hardware, while vision-based methods offer a cheaper, tag-freealternative. However, current vision solutions based on stereo vision facelimitations due to rigid perspective transformation principles and errorpropagation in multi-stage SVD solvers. These solutions also require multiplehigh-resolution cameras with strict setup constraints. To address theselimitations, we propose a probabilistic approach that considers all points onthe human body as observations generated by a distribution centered around thebody's geometric center. This enables us to improve sampling significantly,increasing the number of samples for each point of interest from hundreds tobillions. By modeling the relation between the means of the distributions ofworld coordinates and pixel coordinates, leveraging the Central Limit Theorem,we ensure normality and facilitate the learning process. Experimental resultsdemonstrate human localization accuracy of 95% within a 0.3m range and nearly100% accuracy within a 0.5m range, achieved at a low cost of only 10 USD usingtwo web cameras with a resolution of 640x480 pixels.</description><author>Tianyi Zhang, Wengyu Zhang, Xulu Zhang, Jiaxin Wu, Xiao-Yong Wei, Jiannong Cao, Qing Li</author><pubDate>Tue, 30 Jul 2024 14:45:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20870v1</guid></item><item><title>A Comparative Study of Neural Surface Reconstruction for Scientific Visualization</title><link>http://arxiv.org/abs/2407.20868v1</link><description>This comparative study evaluates various neural surface reconstructionmethods, particularly focusing on their implications for scientificvisualization through reconstructing 3D surfaces via multi-view renderingimages. We categorize ten methods into neural radiance fields and neuralimplicit surfaces, uncovering the benefits of leveraging distance functions(i.e., SDFs and UDFs) to enhance the accuracy and smoothness of thereconstructed surfaces. Our findings highlight the efficiency and quality ofNeuS2 for reconstructing closed surfaces and identify NeUDF as a promisingcandidate for reconstructing open surfaces despite some limitations. By sharingour benchmark dataset, we invite researchers to test the performance of theirmethods, contributing to the advancement of surface reconstruction solutionsfor scientific visualization.</description><author>Siyuan Yao, Weixi Song, Chaoli Wang</author><pubDate>Tue, 30 Jul 2024 14:43:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20868v1</guid></item><item><title>Accounting for shared covariates in semi-parametric Bayesian additive regression trees</title><link>http://arxiv.org/abs/2108.07636v7</link><description>We propose some extensions to semi-parametric models based on Bayesianadditive regression trees (BART). In the semi-parametric BART paradigm, theresponse variable is approximated by a linear predictor and a BART model, wherethe linear component is responsible for estimating the main effects and BARTaccounts for non-specified interactions and non-linearities. Previoussemi-parametric models based on BART have assumed that the set of covariates inthe linear predictor and the BART model are mutually exclusive in an attempt toavoid poor coverage properties and reduce bias in the estimates of theparameters in the linear predictor. The main novelty in our approach lies inthe way we change the tree-generation moves in BART to deal with this bias andresolve non-identifiability issues between the parametric and non-parametriccomponents, even when they have covariates in common. This allows us to modelcomplex interactions involving the covariates of primary interest, both amongthemselves and with those in the BART component. Our novel method is developedwith a view to analysing data from an international education assessment, wherecertain predictors of students' achievements in mathematics are of particularinterpretational interest. Through additional simulation studies and anotherapplication to a well-known benchmark dataset, we also show competitiveperformance when compared to regression models, alternative formulations ofsemi-parametric BART, and other tree-based methods. The implementation of theproposed method is available at \url{https://github.com/ebprado/CSP-BART}.</description><author>Estevão B. Prado, Andrew C. Parnell, Keefe Murphy, Nathan McJames, Ann O'Shea, Rafael A. Moral</author><pubDate>Tue, 30 Jul 2024 14:40:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.07636v7</guid></item><item><title>Versatile audio-visual learning for emotion recognition</title><link>http://arxiv.org/abs/2305.07216v2</link><description>Most current audio-visual emotion recognition models lack the flexibilityneeded for deployment in practical applications. We envision a multimodalsystem that works even when only one modality is available and can beimplemented interchangeably for either predicting emotional attributes orrecognizing categorical emotions. Achieving such flexibility in a multimodalemotion recognition system is difficult due to the inherent challenges inaccurately interpreting and integrating varied data sources. It is also achallenge to robustly handle missing or partial information while allowingdirect switch between regression or classification tasks. This study proposes aversatile audio-visual learning (VAVL) framework for handling unimodal andmultimodal systems for emotion regression or emotion classification tasks. Weimplement an audio-visual framework that can be trained even when audio andvisual paired data is not available for part of the training set (i.e., audioonly or only video is present). We achieve this effective representationlearning with audio-visual shared layers, residual connections over sharedlayers, and a unimodal reconstruction task. Our experimental results revealthat our architecture significantly outperforms strong baselines on theCREMA-D, MSP-IMPROV, and CMU-MOSEI corpora. Notably, VAVL attains a newstate-of-the-art performance in the emotional attribute prediction task on theMSP-IMPROV corpus.</description><author>Lucas Goncalves, Seong-Gyun Leem, Wei-Cheng Lin, Berrak Sisman, Carlos Busso</author><pubDate>Tue, 30 Jul 2024 14:36:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07216v2</guid></item><item><title>Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification</title><link>http://arxiv.org/abs/2407.20859v1</link><description>Recently, autonomous agents built on large language models (LLMs) haveexperienced significant development and are being deployed in real-worldapplications. These agents can extend the base LLM's capabilities in multipleways. For example, a well-built agent using GPT-3.5-Turbo as its core canoutperform the more advanced GPT-4 model by leveraging external components.More importantly, the usage of tools enables these systems to perform actionsin the real world, moving from merely generating text to actively interactingwith their environment. Given the agents' practical applications and theirability to execute consequential actions, it is crucial to assess potentialvulnerabilities. Such autonomous systems can cause more severe damage than astandalone language model if compromised. While some existing research hasexplored harmful actions by LLM agents, our study approaches the vulnerabilityfrom a different perspective. We introduce a new type of attack that causesmalfunctions by misleading the agent into executing repetitive or irrelevantactions. We conduct comprehensive evaluations using various attack methods,surfaces, and properties to pinpoint areas of susceptibility. Our experimentsreveal that these attacks can induce failure rates exceeding 80\% in multiplescenarios. Through attacks on implemented and deployable agents in multi-agentscenarios, we accentuate the realistic risks associated with thesevulnerabilities. To mitigate such attacks, we propose self-examinationdetection methods. However, our findings indicate these attacks are difficultto detect effectively using LLMs alone, highlighting the substantial risksassociated with this vulnerability.</description><author>Boyang Zhang, Yicong Tan, Yun Shen, Ahmed Salem, Michael Backes, Savvas Zannettou, Yang Zhang</author><pubDate>Tue, 30 Jul 2024 14:35:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20859v1</guid></item><item><title>Learn by Selling: Equipping Large Language Models with Product Knowledge for Context-Driven Recommendations</title><link>http://arxiv.org/abs/2407.20856v1</link><description>The rapid evolution of large language models (LLMs) has opened up newpossibilities for applications such as context-driven product recommendations.However, the effectiveness of these models in this context is heavily relianton their comprehensive understanding of the product inventory. This paperpresents a novel approach to equipping LLMs with product knowledge by trainingthem to respond contextually to synthetic search queries that include productIDs. We delve into an extensive analysis of this method, evaluating itseffectiveness, outlining its benefits, and highlighting its constraints. Thepaper also discusses the potential improvements and future directions for thisapproach, providing a comprehensive understanding of the role of LLMs inproduct recommendations.</description><author>Sarthak Anand, Yutong Jiang, Giorgi Kokaia</author><pubDate>Tue, 30 Jul 2024 14:31:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20856v1</guid></item><item><title>DeTurb: Atmospheric Turbulence Mitigation with Deformable 3D Convolutions and 3D Swin Transformers</title><link>http://arxiv.org/abs/2407.20855v1</link><description>Atmospheric turbulence in long-range imaging significantly degrades thequality and fidelity of captured scenes due to random variations in bothspatial and temporal dimensions. These distortions present a formidablechallenge across various applications, from surveillance to astronomy,necessitating robust mitigation strategies. While model-based approachesachieve good results, they are very slow. Deep learning approaches show promisein image and video restoration but have struggled to address thesespatiotemporal variant distortions effectively. This paper proposes a newframework that combines geometric restoration with an enhancement module.Random perturbations and geometric distortion are removed using a pyramidarchitecture with deformable 3D convolutions, resulting in aligned frames.These frames are then used to reconstruct a sharp, clear image via amulti-scale architecture of 3D Swin Transformers. The proposed frameworkdemonstrates superior performance over the state of the art for both syntheticand real atmospheric turbulence effects, with reasonable speed and model size.</description><author>Zhicheng Zou, Nantheera Anantrasirichai</author><pubDate>Tue, 30 Jul 2024 14:31:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20855v1</guid></item><item><title>NIS-SLAM: Neural Implicit Semantic RGB-D SLAM for 3D Consistent Scene Understanding</title><link>http://arxiv.org/abs/2407.20853v1</link><description>In recent years, the paradigm of neural implicit representations has gainedsubstantial attention in the field of Simultaneous Localization and Mapping(SLAM). However, a notable gap exists in the existing approaches when it comesto scene understanding. In this paper, we introduce NIS-SLAM, an efficientneural implicit semantic RGB-D SLAM system, that leverages a pre-trained 2Dsegmentation network to learn consistent semantic representations.Specifically, for high-fidelity surface reconstruction and spatial consistentscene understanding, we combine high-frequency multi-resolutiontetrahedron-based features and low-frequency positional encoding as theimplicit scene representations. Besides, to address the inconsistency of 2Dsegmentation results from multiple views, we propose a fusion strategy thatintegrates the semantic probabilities from previous non-keyframes intokeyframes to achieve consistent semantic learning. Furthermore, we implement aconfidence-based pixel sampling and progressive optimization weight functionfor robust camera tracking. Extensive experimental results on various datasetsshow the better or more competitive performance of our system when compared toother existing neural dense implicit RGB-D SLAM approaches. Finally, we alsoshow that our approach can be used in augmented reality applications. Projectpage:\href{https://zju3dv.github.io/nis_slam}{https://zju3dv.github.io/nis\_slam}.</description><author>Hongjia Zhai, Gan Huang, Qirui Hu, Guanglin Li, Hujun Bao, Guofeng Zhang</author><pubDate>Tue, 30 Jul 2024 14:27:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20853v1</guid></item><item><title>Large Language Models Assume People are More Rational than We Really are</title><link>http://arxiv.org/abs/2406.17055v3</link><description>In order for AI systems to communicate effectively with people, they mustunderstand how we make decisions. However, people's decisions are not alwaysrational, so the implicit internal models of human decision-making in LargeLanguage Models (LLMs) must account for this. Previous empirical evidence seemsto suggest that these implicit models are accurate -- LLMs offer believableproxies of human behavior, acting how we expect humans would in everydayinteractions. However, by comparing LLM behavior and predictions to a largedataset of human decisions, we find that this is actually not the case: whenboth simulating and predicting people's choices, a suite of cutting-edge LLMs(GPT-4o &amp; 4-Turbo, Llama-3-8B &amp; 70B, Claude 3 Opus) assume that people are morerational than we really are. Specifically, these models deviate from humanbehavior and align more closely with a classic model of rational choice --expected value theory. Interestingly, people also tend to assume that otherpeople are rational when interpreting their behavior. As a consequence, when wecompare the inferences that LLMs and people draw from the decisions of othersusing another psychological dataset, we find that these inferences are highlycorrelated. Thus, the implicit decision-making models of LLMs appear to bealigned with the human expectation that other people will act rationally,rather than with how people actually act.</description><author>Ryan Liu, Jiayi Geng, Joshua C. Peterson, Ilia Sucholutsky, Thomas L. Griffiths</author><pubDate>Tue, 30 Jul 2024 14:22:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17055v3</guid></item><item><title>Assessing Graphical Perception of Image Embedding Models using Channel Effectiveness</title><link>http://arxiv.org/abs/2407.20845v1</link><description>Recent advancements in vision models have greatly improved their ability tohandle complex chart understanding tasks, like chart captioning and questionanswering. However, it remains challenging to assess how these models processcharts. Existing benchmarks only roughly evaluate model performance withoutevaluating the underlying mechanisms, such as how models extract imageembeddings. This limits our understanding of the model's ability to perceivefundamental graphical components. To address this, we introduce a novelevaluation framework to assess the graphical perception of image embeddingmodels. For chart comprehension, we examine two main aspects of channeleffectiveness: accuracy and discriminability of various visual channels.Channel accuracy is assessed through the linearity of embeddings, measuring howwell the perceived magnitude aligns with the size of the stimulus.Discriminability is evaluated based on the distances between embeddings,indicating their distinctness. Our experiments with the CLIP model show that itperceives channel accuracy differently from humans and shows uniquediscriminability in channels like length, tilt, and curvature. We aim todevelop this work into a broader benchmark for reliable visual encoders,enhancing models for precise chart comprehension and human-like perception infuture applications.</description><author>Soohyun Lee, Minsuk Chang, Seokhyeon Park, Jinwook Seo</author><pubDate>Tue, 30 Jul 2024 14:22:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20845v1</guid></item><item><title>DFE-IANet: A Method for Polyp Image Classification Based on Dual-domain Feature Extraction and Interaction Attention</title><link>http://arxiv.org/abs/2407.20843v1</link><description>It is helpful in preventing colorectal cancer to detect and treat polyps inthe gastrointestinal tract early. However, there have been few studies to dateon designing polyp image classification networks that balance efficiency andaccuracy. This challenge is mainly attributed to the fact that polyps aresimilar to other pathologies and have complex features influenced by texture,color, and morphology. In this paper, we propose a novel network DFE-IANetbased on both spectral transformation and feature interaction. Firstly, toextract detailed features and multi-scale features, the features aretransformed by the multi-scale frequency domain feature extraction (MSFD) blockto extract texture details at the fine-grained level in the frequency domain.Secondly, the multi-scale interaction attention (MSIA) block is designed toenhance the network's capability of extracting critical features. This blockintroduces multi-scale features into self-attention, aiming to adaptively guidethe network to concentrate on vital regions. Finally, with a compact parameterof only 4M, DFE-IANet outperforms the latest and classical networks in terms ofefficiency. Furthermore, DFE-IANet achieves state-of-the-art (SOTA) results onthe challenging Kvasir dataset, demonstrating a remarkable Top-1 accuracy of93.94%. This outstanding accuracy surpasses ViT by 8.94%, ResNet50 by 1.69%,and VMamba by 1.88%. Our code is publicly available athttps://github.com/PURSUETHESUN/DFE-IANet.</description><author>Wei Wang, Jixing He, Xin Wang</author><pubDate>Tue, 30 Jul 2024 14:16:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20843v1</guid></item><item><title>Neural networks for bifurcation and linear stability analysis of steady states in partial differential equations</title><link>http://arxiv.org/abs/2407.19707v2</link><description>This research introduces an extended application of neural networks forsolving nonlinear partial differential equations (PDEs). A neural network,combined with a pseudo-arclength continuation, is proposed to constructbifurcation diagrams from parameterized nonlinear PDEs. Additionally, a neuralnetwork approach is also presented for solving eigenvalue problems to analyzesolution linear stability, focusing on identifying the largest eigenvalue. Theeffectiveness of the proposed neural network is examined through experiments onthe Bratu equation and the Burgers equation. Results from a finite differencemethod are also presented as comparison. Varying numbers of grid points areemployed in each case to assess the behavior and accuracy of both the neuralnetwork and the finite difference method. The experimental results demonstratethat the proposed neural network produces better solutions, generates moreaccurate bifurcation diagrams, has reasonable computational times, and proveseffective for linear stability analysis.</description><author>Muhammad Luthfi Shahab, Hadi Susanto</author><pubDate>Tue, 30 Jul 2024 14:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19707v2</guid></item><item><title>Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks</title><link>http://arxiv.org/abs/2407.20836v1</link><description>Recent advancements in image synthesis, particularly with the advent of GANand Diffusion models, have amplified public concerns regarding thedissemination of disinformation. To address such concerns, numerousAI-generated Image (AIGI) Detectors have been proposed and achieved promisingperformance in identifying fake images. However, there still lacks a systematicunderstanding of the adversarial robustness of these AIGI detectors. In thispaper, we examine the vulnerability of state-of-the-art AIGI detectors againstadversarial attack under white-box and black-box settings, which has beenrarely investigated so far. For the task of AIGI detection, we propose a newattack containing two main parts. First, inspired by the obvious differencebetween real images and fake images in the frequency domain, we addperturbations under the frequency domain to push the image away from itsoriginal frequency distribution. Second, we explore the full posteriordistribution of the surrogate model to further narrow this gap betweenheterogeneous models, e.g. transferring adversarial examples across CNNs andViTs. This is achieved by introducing a novel post-train Bayesian strategy thatturns a single surrogate into a Bayesian one, capable of simulating diversevictim models using one pre-trained surrogate, without the need forre-training. We name our method as frequency-based post-train Bayesian attack,or FPBA. Through FPBA, we show that adversarial attack is truly a real threatto AIGI detectors, because FPBA can deliver successful black-box attacks acrossmodels, generators, defense methods, and even evade cross-generator detection,which is a crucial real-world detection scenario.</description><author>Yunfeng Diao, Naixin Zhai, Changtao Miao, Xun Yang, Meng Wang</author><pubDate>Tue, 30 Jul 2024 14:07:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20836v1</guid></item><item><title>Federated Knowledge Recycling: Privacy-Preserving Synthetic Data Sharing</title><link>http://arxiv.org/abs/2407.20830v1</link><description>Federated learning has emerged as a paradigm for collaborative learning,enabling the development of robust models without the need to centralisesensitive data. However, conventional federated learning techniques haveprivacy and security vulnerabilities due to the exposure of models, parametersor updates, which can be exploited as an attack surface. This paper presentsFederated Knowledge Recycling (FedKR), a cross-silo federated learning approachthat uses locally generated synthetic data to facilitate collaboration betweeninstitutions. FedKR combines advanced data generation techniques with a dynamicaggregation process to provide greater security against privacy attacks thanexisting methods, significantly reducing the attack surface. Experimentalresults on generic and medical datasets show that FedKR achieves competitiveperformance, with an average improvement in accuracy of 4.24% compared totraining models from local data, demonstrating particular effectiveness in datascarcity scenarios.</description><author>Eugenio Lomurno, Matteo Matteucci</author><pubDate>Tue, 30 Jul 2024 13:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20830v1</guid></item><item><title>How to Measure the Intelligence of Large Language Models?</title><link>http://arxiv.org/abs/2407.20828v1</link><description>With the release of ChatGPT and other large language models (LLMs) thediscussion about the intelligence, possibilities, and risks, of current andfuture models have seen large attention. This discussion included much debatedscenarios about the imminent rise of so-called "super-human" AI, i.e., AIsystems that are orders of magnitude smarter than humans. In the spirit of AlanTuring, there is no doubt that current state-of-the-art language models alreadypass his famous test. Moreover, current models outperform humans in severalbenchmark tests, so that publicly available LLMs have already become versatilecompanions that connect everyday life, industry and science. Despite theirimpressive capabilities, LLMs sometimes fail completely at tasks that arethought to be trivial for humans. In other cases, the trustworthiness of LLMsbecomes much more elusive and difficult to evaluate. Taking the example ofacademia, language models are capable of writing convincing research articleson a given topic with only little input. Yet, the lack of trustworthiness interms of factual consistency or the existence of persistent hallucinations inAI-generated text bodies has led to a range of restrictions for AI-basedcontent in many scientific journals. In view of these observations, thequestion arises as to whether the same metrics that apply to human intelligencecan also be applied to computational methods and has been discussedextensively. In fact, the choice of metrics has already been shown todramatically influence assessments on potential intelligence emergence. Here,we argue that the intelligence of LLMs should not only be assessed bytask-specific statistical metrics, but separately in terms of qualitative andquantitative measures.</description><author>Nils Körber, Silvan Wehrli, Christopher Irrgang</author><pubDate>Tue, 30 Jul 2024 13:53:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20828v1</guid></item><item><title>From Majority to Minority: A Diffusion-based Augmentation for Underrepresented Groups in Skin Lesion Analysis</title><link>http://arxiv.org/abs/2406.18375v2</link><description>AI-based diagnoses have demonstrated dermatologist-level performance inclassifying skin cancer. However, such systems are prone to under-performingwhen tested on data from minority groups that lack sufficient representation inthe training sets. Although data collection and annotation offer the best meansfor promoting minority groups, these processes are costly and time-consuming.Prior works have suggested that data from majority groups may serve as avaluable information source to supplement the training of diagnosis tools forminority groups. In this work, we propose an effective diffusion-basedaugmentation framework that maximizes the use of rich information from majoritygroups to benefit minority groups. Using groups with different skin types as acase study, our results show that the proposed framework can generate syntheticimages that improve diagnostic results for the minority groups, even when thereis little or no reference data from these target groups. The practical value ofour work is evident in medical imaging analysis, where under-diagnosis persistsas a problem for certain groups due to insufficient representation.</description><author>Janet Wang, Yunsung Chung, Zhengming Ding, Jihun Hamm</author><pubDate>Tue, 30 Jul 2024 13:43:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18375v2</guid></item><item><title>Light and Optimal Schrödinger Bridge Matching</title><link>http://arxiv.org/abs/2402.03207v2</link><description>Schr\"odinger Bridges (SB) have recently gained the attention of the MLcommunity as a promising extension of classic diffusion models which is alsointerconnected to the Entropic Optimal Transport (EOT). Recent solvers for SBexploit the pervasive bridge matching procedures. Such procedures aim torecover a stochastic process transporting the mass between distributions givenonly a transport plan between them. In particular, given the EOT plan, theseprocedures can be adapted to solve SB. This fact is heavily exploited by recentworks giving rise to matching-based SB solvers. The cornerstone here isrecovering the EOT plan: recent works either use heuristical approximations(e.g., the minibatch OT) or establish iterative matching procedures which bythe design accumulate the error during the training. We address theselimitations and propose a novel procedure to learn SB which we call the\textbf{optimal Schr\"odinger bridge matching}. It exploits the optimalparameterization of the diffusion process and provably recovers the SB process\textbf{(a)} with a single bridge matching step and \textbf{(b)} with arbitrarytransport plan as the input. Furthermore, we show that the optimal bridgematching objective coincides with the recently discovered energy-based modeling(EBM) objectives to learn EOT/SB. Inspired by this observation, we develop alight solver (which we call LightSB-M) to implement optimal matching inpractice using the Gaussian mixture parameterization of the adjustedSchr\"odinger potential. We experimentally showcase the performance of oursolver in a range of practical tasks. The code for our solver can be found athttps://github.com/SKholkin/LightSB-Matching.</description><author>Nikita Gushchin, Sergei Kholkin, Evgeny Burnaev, Alexander Korotin</author><pubDate>Tue, 30 Jul 2024 13:43:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03207v2</guid></item><item><title>DyGKT: Dynamic Graph Learning for Knowledge Tracing</title><link>http://arxiv.org/abs/2407.20824v1</link><description>Knowledge Tracing aims to assess student learning states by predicting theirperformance in answering questions. Different from the existing research whichutilizes fixed-length learning sequence to obtain the student states andregards KT as a static problem, this work is motivated by three dynamicalcharacteristics: 1) The scales of students answering records are constantlygrowing; 2) The semantics of time intervals between the records vary; 3) Therelationships between students, questions and concepts are evolving. The threedynamical characteristics above contain the great potential to revolutionizethe existing knowledge tracing methods. Along this line, we propose a DynamicGraph-based Knowledge Tracing model, namely DyGKT. In particular, acontinuous-time dynamic question-answering graph for knowledge tracing isconstructed to deal with the infinitely growing answering behaviors, and it isworth mentioning that it is the first time dynamic graph learning technology isused in this field. Then, a dual time encoder is proposed to capture long-termand short-term semantics among the different time intervals. Finally, amultiset indicator is utilized to model the evolving relationships betweenstudents, questions, and concepts via the graph structural feature. Numerousexperiments are conducted on five real-world datasets, and the resultsdemonstrate the superiority of our model. All the used resources are publiclyavailable at https://github.com/PengLinzhi/DyGKT.</description><author>Ke Cheng, Linzhi Peng, Pengyang Wang, Junchen Ye, Leilei Sun, Bowen Du</author><pubDate>Tue, 30 Jul 2024 13:43:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20824v1</guid></item><item><title>Adding Circumscription to Decidable Fragments of First-Order Logic: A Complexity Rollercoaster</title><link>http://arxiv.org/abs/2407.20822v1</link><description>We study extensions of expressive decidable fragments of first-order logicwith circumscription, in particular the two-variable fragment FO$^2$, itsextension C$^2$ with counting quantifiers, and the guarded fragment GF. Weprove that if only unary predicates are minimized (or fixed) duringcircumscription, then decidability of logical consequence is preserved. ForFO$^2$ the complexity increases from $\textrm{coNexp}$ to$\textrm{coNExp}^\textrm{NP}$-complete, for GF it (remarkably!) increases from$\textrm{2Exp}$ to $\textrm{Tower}$-complete, and for C$^2$ the complexityremains open. We also consider querying circumscribed knowledge bases whoseontology is a GF sentence, showing that the problem is decidable for unions ofconjunctive queries, $\textrm{Tower}$-complete in combined complexity, andelementary in data complexity. Already for atomic queries and ontologies thatare sets of guarded existential rules, however, for every $k \geq 0$ there isan ontology and query that are $k$-$\textrm{Exp}$-hard in data complexity.</description><author>Carsten Lutz, Quentin Manière</author><pubDate>Tue, 30 Jul 2024 13:39:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20822v1</guid></item><item><title>KI-PMF: Knowledge Integrated Plausible Motion Forecasting</title><link>http://arxiv.org/abs/2310.12007v3</link><description>Accurately forecasting the motion of traffic actors is crucial for thedeployment of autonomous vehicles at a large scale. Current trajectoryforecasting approaches primarily concentrate on optimizing a loss function witha specific metric, which can result in predictions that do not adhere tophysical laws or violate external constraints. Our objective is to incorporateexplicit knowledge priors that allow a network to forecast future trajectoriesin compliance with both the kinematic constraints of a vehicle and the geometryof the driving environment. To achieve this, we introduce a non-parametricpruning layer and attention layers to integrate the defined knowledge priors.Our proposed method is designed to ensure reachability guarantees for trafficactors in both complex and dynamic situations. By conditioning the network tofollow physical laws, we can obtain accurate and safe predictions, essentialfor maintaining autonomous vehicles' safety and efficiency in real-worldsettings.In summary, this paper presents concepts that prevent off-roadpredictions for safe and reliable motion forecasting by incorporating knowledgepriors into the training process.</description><author>Abhishek Vivekanandan, Ahmed Abouelazm, Philip Schörner, J. Marius Zöllner</author><pubDate>Tue, 30 Jul 2024 13:35:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12007v3</guid></item><item><title>WARM-3D: A Weakly-Supervised Sim2Real Domain Adaptation Framework for Roadside Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2407.20818v1</link><description>Existing roadside perception systems are limited by the absence of publiclyavailable, large-scale, high-quality 3D datasets. Exploring the use ofcost-effective, extensive synthetic datasets offers a viable solution to tacklethis challenge and enhance the performance of roadside monocular 3D detection.In this study, we introduce the TUMTraf Synthetic Dataset, offering a diverseand substantial collection of high-quality 3D data to augment scarce real-worlddatasets. Besides, we present WARM-3D, a concise yet effective framework to aidthe Sim2Real domain transfer for roadside monocular 3D detection. Our methodleverages cheap synthetic datasets and 2D labels from an off-the-shelf 2Ddetector for weak supervision. We show that WARM-3D significantly enhancesperformance, achieving a +12.40% increase in mAP 3D over the baseline with onlypseudo-2D supervision. With 2D GT as weak labels, WARM-3D even reachesperformance close to the Oracle baseline. Moreover, WARM-3D improves theability of 3D detectors to unseen sample recognition across various real-worldenvironments, highlighting its potential for practical applications.</description><author>Xingcheng Zhou, Deyu Fu, Walter Zimmer, Mingyu Liu, Venkatnarayanan Lakshminarasimhan, Leah Strand, Alois C. Knoll</author><pubDate>Tue, 30 Jul 2024 13:32:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20818v1</guid></item><item><title>Robust Load Prediction of Power Network Clusters Based on Cloud-Model-Improved Transformer</title><link>http://arxiv.org/abs/2407.20817v1</link><description>Load data from power network clusters indicates economic development in eacharea, crucial for predicting regional trends and guiding power enterprisedecisions. The Transformer model, a leading method for load prediction, faceschallenges modeling historical data due to variables like weather, events,festivals, and data volatility. To tackle this, the cloud model's fuzzy featureis utilized to manage uncertainties effectively. Presenting an innovativeapproach, the Cloud Model Improved Transformer (CMIT) method integrates theTransformer model with the cloud model utilizing the particle swarmoptimization algorithm, with the aim of achieving robust and precise power loadpredictions. Through comparative experiments conducted on 31 real datasetswithin a power network cluster, it is demonstrated that CMIT significantlysurpasses the Transformer model in terms of prediction accuracy, therebyhighlighting its effectiveness in enhancing forecasting capabilities within thepower network cluster sector.</description><author>Cheng Jiang, Gang Lu, Xue Ma, Di Wu</author><pubDate>Tue, 30 Jul 2024 13:32:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20817v1</guid></item><item><title>Savile Row Manual</title><link>http://arxiv.org/abs/2201.03472v2</link><description>We describe the constraint modelling tool Savile Row, its input language andits main features. Savile Row translates a solver-independent constraintmodelling language to the input languages for various solvers includingconstraint, SAT, and SMT solvers. After a brief introduction, the manualdescribes the Essence Prime language, which is the input language of SavileRow. Then we describe the functions of the tool, its main features and optionsand how to install and use it.</description><author>Peter Nightingale</author><pubDate>Tue, 30 Jul 2024 13:31:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.03472v2</guid></item><item><title>Structure Unbiased Adversarial Model for Medical Image Segmentation</title><link>http://arxiv.org/abs/2205.12857v4</link><description>Generative models have been widely proposed in image recognition to generatemore images where the distribution is similar to that of the real ones. Itoften introduces a discriminator network to differentiate the real data fromthe generated ones. Such models utilise a discriminator network tasked withdifferentiating style transferred data from data contained in the targetdataset. However in doing so the network focuses on discrepancies in theintensity distribution and may overlook structural differences between thedatasets. In this paper we formulate a new image-to-image translation problemto ensure that the structure of the generated images is similar to that in thetarget dataset. We propose a simple, yet powerful Structure-UnbiasedAdversarial (SUA) network which accounts for both intensity and structuraldifferences between the training and test sets when performing imagesegmentation. It consists of a spatial transformation block followed by anintensity distribution rendering module. The spatial transformation block isproposed to reduce the structure gap between the two images, and also producean inverse deformation field to warp the final segmented image back. Theintensity distribution rendering module then renders the deformed structure toan image with the target intensity distribution. Experimental results show thatthe proposed SUA method has the capability to transfer both intensitydistribution and structural content between multiple datasets.</description><author>Tianyang Zhang, Shaoming Zheng, Jun Cheng, Xi Jia, Joseph Bartlett, Xinxing Cheng, Huazhu Fu, Zhaowen Qiu, Jiang Liu, Jinming Duan</author><pubDate>Tue, 30 Jul 2024 13:29:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.12857v4</guid></item><item><title>WindsorML: High-Fidelity Computational Fluid Dynamics Dataset For Automotive Aerodynamics</title><link>http://arxiv.org/abs/2407.19320v2</link><description>This paper presents a new open-source high-fidelity dataset for MachineLearning (ML) containing 355 geometric variants of the Windsor body, to helpthe development and testing of ML surrogate models for external automotiveaerodynamics. Each Computational Fluid Dynamics (CFD) simulation was run with aGPU-native high-fidelity Wall-Modeled Large-Eddy Simulations (WMLES) using aCartesian immersed-boundary method using more than 280M cells to ensure thegreatest possible accuracy. The dataset contains geometry variants thatexhibits a wide range of flow characteristics that are representative of thoseobserved on road-cars. The dataset itself contains the 3D time-averaged volume&amp; boundary data as well as the geometry and force &amp; moment coefficients. Thispaper discusses the validation of the underlying CFD methods as well ascontents and structure of the dataset. To the authors knowledge, thisrepresents the first, large-scale high-fidelity CFD dataset for the Windsorbody with a permissive open-source license (CC-BY-SA).</description><author>Neil Ashton, Jordan B. Angel, Aditya S. Ghate, Gaetan K. W. Kenway, Man Long Wong, Cetin Kiris, Astrid Walle, Danielle C. Maddix, Gary Page</author><pubDate>Tue, 30 Jul 2024 13:22:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19320v2</guid></item><item><title>Look Around and Learn: Self-Training Object Detection by Exploration</title><link>http://arxiv.org/abs/2302.03566v4</link><description>When an object detector is deployed in a novel setting it often experiences adrop in performance. This paper studies how an embodied agent can automaticallyfine-tune a pre-existing object detector while exploring and acquiring imagesin a new environment without relying on human intervention, i.e., a fullyself-supervised approach. In our setting, an agent initially learns to explorethe environment using a pre-trained off-the-shelf detector to locate objectsand associate pseudo-labels. By assuming that pseudo-labels for the same objectmust be consistent across different views, we learn the exploration policy LookAround to mine hard samples, and we devise a novel mechanism calledDisagreement Reconciliation for producing refined pseudo-labels from theconsensus among observations. We implement a unified benchmark of the currentstate-of-the-art and compare our approach with pre-existing explorationpolicies and perception mechanisms. Our method is shown to outperform existingapproaches, improving the object detector by 6.2% in a simulated scenario, a3.59% advancement over other state-of-the-art methods, and by 9.97% in the realrobotic test without relying on ground-truth. Code for the proposed approachand baselines are available athttps://iit-pavis.github.io/Look_Around_And_Learn/.</description><author>Gianluca Scarpellini, Stefano Rosa, Pietro Morerio, Lorenzo Natale, Alessio Del Bue</author><pubDate>Tue, 30 Jul 2024 13:18:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03566v4</guid></item><item><title>A Survey on Model Compression for Large Language Models</title><link>http://arxiv.org/abs/2308.07633v4</link><description>Large Language Models (LLMs) have transformed natural language processingtasks successfully. Yet, their large size and high computational needs posechallenges for practical use, especially in resource-limited settings. Modelcompression has emerged as a key research area to address these challenges.This paper presents a survey of model compression techniques for LLMs. We covermethods like quantization, pruning, and knowledge distillation, highlightingrecent advancements. We also discuss benchmarking strategies and evaluationmetrics crucial for assessing compressed LLMs. This survey offers valuableinsights for researchers and practitioners, aiming to enhance efficiency andreal-world applicability of LLMs while laying a foundation for futureadvancements.</description><author>Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang</author><pubDate>Tue, 30 Jul 2024 13:14:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07633v4</guid></item><item><title>ARCLE: The Abstraction and Reasoning Corpus Learning Environment for Reinforcement Learning</title><link>http://arxiv.org/abs/2407.20806v1</link><description>This paper introduces ARCLE, an environment designed to facilitatereinforcement learning research on the Abstraction and Reasoning Corpus (ARC).Addressing this inductive reasoning benchmark with reinforcement learningpresents these challenges: a vast action space, a hard-to-reach goal, and avariety of tasks. We demonstrate that an agent with proximal policyoptimization can learn individual tasks through ARCLE. The adoption ofnon-factorial policies and auxiliary losses led to performance enhancements,effectively mitigating issues associated with action spaces and goalattainment. Based on these insights, we propose several research directions andmotivations for using ARCLE, including MAML, GFlowNets, and World Models.</description><author>Hosung Lee, Sejin Kim, Seungpil Lee, Sanha Hwang, Jihwan Lee, Byung-Jun Lee, Sundong Kim</author><pubDate>Tue, 30 Jul 2024 13:11:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20806v1</guid></item><item><title>Semantic Image Synthesis via Class-Adaptive Cross-Attention</title><link>http://arxiv.org/abs/2308.16071v3</link><description>In semantic image synthesis the state of the art is dominated by methods thatuse customized variants of the SPatially-Adaptive DE-normalization (SPADE)layers, which allow for good visual generation quality and editing versatility.By design, such layers learn pixel-wise modulation parameters to de-normalizethe generator activations based on the semantic class each pixel belongs to.Thus, they tend to overlook global image statistics, ultimately leading tounconvincing local style editing and causing global inconsistencies such ascolor or illumination distribution shifts. Also, SPADE layers require thesemantic segmentation mask for mapping styles in the generator, preventingshape manipulations without manual intervention. In response, we designed anovel architecture where cross-attention layers are used in place of SPADE forlearning shape-style correlations and so conditioning the image generationprocess. Our model inherits the versatility of SPADE, at the same timeobtaining state-of-the-art generation quality, as well as improved global andlocal style transfer. Code and models available athttps://github.com/TFonta/CA2SIS.</description><author>Tomaso Fontanini, Claudio Ferrari, Giuseppe Lisanti, Massimo Bertozzi, Andrea Prati</author><pubDate>Tue, 30 Jul 2024 13:09:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16071v3</guid></item><item><title>AhmedML: High-Fidelity Computational Fluid Dynamics Dataset for Incompressible, Low-Speed Bluff Body Aerodynamics</title><link>http://arxiv.org/abs/2407.20801v1</link><description>The development of Machine Learning (ML) methods for Computational FluidDynamics (CFD) is currently limited by the lack of openly available trainingdata. This paper presents a new open-source dataset comprising of highfidelity, scale-resolving CFD simulations of 500 geometric variations of theAhmed Car Body - a simplified car-like shape that exhibits many of the flowtopologies that are present on bluff bodies such as road vehicles. The datasetcontains simulation results that exhibit a broad set of fundamental flowphysics such as geometry and pressure-induced flow separation as well as 3Dvortical structures. Each variation of the Ahmed car body were run using ahigh-fidelity, time-accurate, hybrid Reynolds-Averaged Navier-Stokes (RANS) -Large-Eddy Simulation (LES) turbulence modelling approach using the open-sourceCFD code OpenFOAM. The dataset contains boundary, volume, geometry, andtime-averaged forces/moments in widely used open-source formats. In addition,the OpenFOAM case setup is provided so that others can reproduce or extend thedataset. This represents to the authors knowledge, the first open-sourcelarge-scale dataset using high-fidelity CFD methods for the widely used Ahmedcar body that is available to freely download with a permissive license(CC-BY-SA).</description><author>Neil Ashton, Danielle C. Maddix, Samuel Gundry, Parisa M. Shabestari</author><pubDate>Tue, 30 Jul 2024 13:07:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20801v1</guid></item><item><title>Synthetic Image Learning: Preserving Performance and Preventing Membership Inference Attacks</title><link>http://arxiv.org/abs/2407.15526v2</link><description>Generative artificial intelligence has transformed the generation ofsynthetic data, providing innovative solutions to challenges like data scarcityand privacy, which are particularly critical in fields such as medicine.However, the effective use of this synthetic data to train high-performancemodels remains a significant challenge. This paper addresses this issue byintroducing Knowledge Recycling (KR), a pipeline designed to optimise thegeneration and use of synthetic data for training downstream classifiers. Atthe heart of this pipeline is Generative Knowledge Distillation (GKD), theproposed technique that significantly improves the quality and usefulness ofthe information provided to classifiers through a synthetic datasetregeneration and soft labelling mechanism. The KR pipeline has been tested on avariety of datasets, with a focus on six highly heterogeneous medical imagedatasets, ranging from retinal images to organ scans. The results show asignificant reduction in the performance gap between models trained on real andsynthetic data, with models based on synthetic data outperforming those trainedon real data in some cases. Furthermore, the resulting models show almostcomplete immunity to Membership Inference Attacks, manifesting privacyproperties missing in models trained with conventional techniques.</description><author>Eugenio Lomurno, Matteo Matteucci</author><pubDate>Tue, 30 Jul 2024 13:03:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15526v2</guid></item><item><title>SpotFormer: Multi-Scale Spatio-Temporal Transformer for Facial Expression Spotting</title><link>http://arxiv.org/abs/2407.20799v1</link><description>Facial expression spotting, identifying periods where facial expressionsoccur in a video, is a significant yet challenging task in facial expressionanalysis. The issues of irrelevant facial movements and the challenge ofdetecting subtle motions in micro-expressions remain unresolved, hinderingaccurate expression spotting. In this paper, we propose an efficient frameworkfor facial expression spotting. First, we propose a Sliding Window-basedMulti-Resolution Optical flow (SW-MRO) feature, which calculatesmulti-resolution optical flow of the input image sequence within compactsliding windows. The window length is tailored to perceive completemicro-expressions and distinguish between general macro- and micro-expressions.SW-MRO can effectively reveal subtle motions while avoiding severe headmovement problems. Second, we propose SpotFormer, a multi-scale spatio-temporalTransformer that simultaneously encodes spatio-temporal relationships of theSW-MRO features for accurate frame-level probability estimation. In SpotFormer,our proposed Facial Local Graph Pooling (FLGP) and convolutional layers areapplied for multi-scale spatio-temporal feature extraction. We show thevalidity of the architecture of SpotFormer by comparing it with several modelvariants. Third, we introduce supervised contrastive learning into SpotFormerto enhance the discriminability between different types of expressions.Extensive experiments on SAMM-LV and CAS(ME)^2 show that our method outperformsstate-of-the-art models, particularly in micro-expression spotting.</description><author>Yicheng Deng, Hideaki Hayashi, Hajime Nagahara</author><pubDate>Tue, 30 Jul 2024 13:02:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20799v1</guid></item><item><title>Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning</title><link>http://arxiv.org/abs/2407.20798v1</link><description>We introduce Diffusion Augmented Agents (DAAG), a novel framework thatleverages large language models, vision language models, and diffusion modelsto improve sample efficiency and transfer learning in reinforcement learningfor embodied agents. DAAG hindsight relabels the agent's past experience byusing diffusion models to transform videos in a temporally and geometricallyconsistent way to align with target instructions with a technique we callHindsight Experience Augmentation. A large language model orchestrates thisautonomous process without requiring human supervision, making it well-suitedfor lifelong learning scenarios. The framework reduces the amount ofreward-labeled data needed to 1) finetune a vision language model that acts asa reward detector, and 2) train RL agents on new tasks. We demonstrate thesample efficiency gains of DAAG in simulated robotics environments involvingmanipulation and navigation. Our results show that DAAG improves learning ofreward detectors, transferring past experience, and acquiring new tasks - keyabilities for developing efficient lifelong learning agents. Supplementarymaterial and visualizations are available on our websitehttps://sites.google.com/view/diffusion-augmented-agents/</description><author>Norman Di Palo, Leonard Hasenclever, Jan Humplik, Arunkumar Byravan</author><pubDate>Tue, 30 Jul 2024 13:01:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20798v1</guid></item><item><title>Local-peak scale-invariant feature transform for fast and random image stitching</title><link>http://arxiv.org/abs/2405.08578v2</link><description>Image stitching aims to construct a wide field of view with high spatialresolution, which cannot be achieved in a single exposure. Typically,conventional image stitching techniques, other than deep learning, requirecomplex computation and thus computational pricy, especially for stitchinglarge raw images. In this study, inspired by the multiscale feature of fluidturbulence, we developed a fast feature point detection algorithm namedlocal-peak scale-invariant feature transform (LP-SIFT), based on the multiscalelocal peaks and scale-invariant feature transform method. By combining LP-SIFTand RANSAC in image stitching, the stitching speed can be improved by orders,compared with the original SIFT method. Nine large images (over 2600*1600pixels), arranged randomly without prior knowledge, can be stitched within158.94 s. The algorithm is highly practical for applications requiring a widefield of view in diverse application scenes, e.g., terrain mapping, biologicalanalysis, and even criminal investigation.</description><author>Hao Li, Lipo Wang, Tianyun Zhao, Wei Zhao</author><pubDate>Tue, 30 Jul 2024 12:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08578v2</guid></item><item><title>How Novice Programmers Use and Experience ChatGPT when Solving Programming Exercises in an Introductory Course</title><link>http://arxiv.org/abs/2407.20792v1</link><description>This research paper contributes to the computing education researchcommunity's understanding of Generative AI (GenAI) in the context ofintroductory programming, and specifically, how students utilize related tools,such as ChatGPT. An increased understanding of students' use is mandatory foreducators and higher education institutions, as GenAI is here to stay, and itsperformance is likely to improve rapidly in the near future. Learning aboutstudents' use patterns is not only crucial to support their learning, but todevelop adequate forms of instruction and assessment. With the rapidadvancement of AI, its broad availability, and ubiquitous presence ineducational environments, elaborating how AI can enhance learning experiences,especially in courses such as introductory programming is important. To date,most studies have focused on the educator's perspective on GenAI, itsperformance, characteristics, and limitations. However, the studentperspective, and how they actually use GenAI tools in course contexts, has notbeen subject to a great number of studies. Therefore, this study is guided bythe following research questions: (1) What do students report on their usepattern of ChatGPT in the context of introductory programming exercises? and(2) How do students perceive ChatGPT in the context of introductory programmingexercises? To address these questions, computing students at a large Germanuniversity were asked to solve programming tasks with the assistance of ChatGPTas part of their introductory programming course. Students (n=298) providedinformation regarding the use of ChatGPT, and their evaluation of the tool viaan online survey. This research provides a comprehensive evaluation ofChatGPT-3.5's application by novice programmers in a higher educationcontext...</description><author>Andreas Scholl, Natalie Kiesler</author><pubDate>Tue, 30 Jul 2024 12:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20792v1</guid></item><item><title>SALSA: Swift Adaptive Lightweight Self-Attention for Enhanced LiDAR Place Recognition</title><link>http://arxiv.org/abs/2407.08260v2</link><description>Large-scale LiDAR mappings and localization leverage place recognitiontechniques to mitigate odometry drifts, ensuring accurate mapping. Thesetechniques utilize scene representations from LiDAR point clouds to identifypreviously visited sites within a database. Local descriptors, assigned to eachpoint within a point cloud, are aggregated to form a scene representation forthe point cloud. These descriptors are also used to re-rank the retrieved pointclouds based on geometric fitness scores. We propose SALSA, a novel,lightweight, and efficient framework for LiDAR place recognition. It consistsof a Sphereformer backbone that uses radial window attention to enableinformation aggregation for sparse distant points, an adaptive self-attentionlayer to pool local descriptors into tokens, and a multi-layer-perceptron Mixerlayer for aggregating the tokens to generate a scene descriptor. The proposedframework outperforms existing methods on various LiDAR place recognitiondatasets in terms of both retrieval and metric localization while operating inreal-time.</description><author>Raktim Gautam Goswami, Naman Patel, Prashanth Krishnamurthy, Farshad Khorrami</author><pubDate>Tue, 30 Jul 2024 12:54:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08260v2</guid></item><item><title>The Sky's the Limit: Re-lightable Outdoor Scenes via a Sky-pixel Constrained Illumination Prior and Outside-In Visibility</title><link>http://arxiv.org/abs/2311.16937v2</link><description>Inverse rendering of outdoor scenes from unconstrained image collections is achallenging task, particularly illumination/albedo ambiguities and occlusion ofthe illumination environment (shadowing) caused by geometry. However, there aremany cues in an image that can aid in the disentanglement of geometry, albedoand shadows. Whilst sky is frequently masked out in state-of-the-art methods,we exploit the fact that any sky pixel provides a direct observation of distantlighting in the corresponding direction and, via a neural illumination prior, astatistical cue to derive the remaining illumination environment. Theincorporation of our illumination prior is enabled by a novel `outside-in'method for computing differentiable sky visibility based on a neuraldirectional distance function. This is highly efficient and can be trained inparallel with the neural scene representation, allowing gradients fromappearance loss to flow from shadows to influence the estimation ofillumination and geometry. Our method estimates high-quality albedo, geometry,illumination and sky visibility, achieving state-of-the-art results on theNeRF-OSR relighting benchmark. Our code and models can be found athttps://github.com/JADGardner/neusky</description><author>James A. D. Gardner, Evgenii Kashin, Bernhard Egger, William A. P. Smith</author><pubDate>Tue, 30 Jul 2024 12:46:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16937v2</guid></item><item><title>Be aware of overfitting by hyperparameter optimization!</title><link>http://arxiv.org/abs/2407.20786v1</link><description>Hyperparameter optimization is very frequently employed in machine learning.However, an optimization of a large space of parameters could result inoverfitting of models. In recent studies on solubility prediction the authorscollected seven thermodynamic and kinetic solubility datasets from differentdata sources. They used state-of-the-art graph-based methods and comparedmodels developed for each dataset using different data cleaning protocols andhyperparameter optimization. In our study we showed that hyperparameteroptimization did not always result in better models, possibly due tooverfitting when using the same statistical measures. Similar results could becalculated using pre-set hyperparameters, reducing the computational effort byaround 10,000 times. We also extended the previous analysis by adding arepresentation learning method based on Natural Language Processing of smilescalled Transformer CNN. We show that across all analyzed sets using exactly thesame protocol, Transformer CNN provided better results than graph-based methodsfor 26 out of 28 pairwise comparisons by using only a tiny fraction of time ascompared to other methods. Last but not least we stressed the importance ofcomparing calculation results using exactly the same statistical measures.</description><author>Igor V. Tetko, Ruud van Deursen, Guillaume Godin</author><pubDate>Tue, 30 Jul 2024 12:45:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20786v1</guid></item><item><title>Metaheuristic Enhanced with Feature-Based Guidance and Diversity Management for Solving the Capacitated Vehicle Routing Problem</title><link>http://arxiv.org/abs/2407.20777v1</link><description>We propose a metaheuristic algorithm enhanced with feature-based guidancethat is designed to solve the Capacitated Vehicle Routing Problem (CVRP). Toformulate the proposed guidance, we developed and explained a supervisedMachine Learning (ML) model, that is used to formulate the guidance and controlthe diversity of the solution during the optimization process. We propose ametaheuristic algorithm combining neighborhood search and a novel mechanism ofhybrid split and path relinking to implement the proposed guidance. Theproposed guidance has proven to give a statistically significant improvement tothe proposed metaheuristic algorithm when solving CVRP. Moreover, the proposedguided metaheuristic is also capable of producing competitive solutions amongstate-of-the-art metaheuristic algorithms.</description><author>Bachtiar Herdianto, Romain Billot, Flavien Lucas, Marc Sevaux</author><pubDate>Tue, 30 Jul 2024 12:26:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20777v1</guid></item></channel></rss>