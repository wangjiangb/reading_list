<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 30 Apr 2024 06:01:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Hallucination of Multimodal Large Language Models: A Survey</title><link>http://arxiv.org/abs/2404.18930v1</link><description>This survey presents a comprehensive analysis of the phenomenon ofhallucination in multimodal large language models (MLLMs), also known as LargeVision-Language Models (LVLMs), which have demonstrated significantadvancements and remarkable abilities in multimodal tasks. Despite thesepromising developments, MLLMs often generate outputs that are inconsistent withthe visual content, a challenge known as hallucination, which poses substantialobstacles to their practical deployment and raises concerns regarding theirreliability in real-world applications. This problem has attracted increasingattention, prompting efforts to detect and mitigate such inaccuracies. Wereview recent advances in identifying, evaluating, and mitigating thesehallucinations, offering a detailed overview of the underlying causes,evaluation benchmarks, metrics, and strategies developed to address this issue.Additionally, we analyze the current challenges and limitations, formulatingopen questions that delineate potential pathways for future research. Bydrawing the granular classification and landscapes of hallucination causes,evaluation benchmarks, and mitigation methods, this survey aims to deepen theunderstanding of hallucinations in MLLMs and inspire further advancements inthe field. Through our thorough and in-depth review, we contribute to theongoing dialogue on enhancing the robustness and reliability of MLLMs,providing valuable insights and resources for researchers and practitionersalike. Resources are available at:https://github.com/showlab/Awesome-MLLM-Hallucination.</description><author>Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, Mike Zheng Shou</author><pubDate>Mon, 29 Apr 2024 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18930v1</guid></item><item><title>DGE: Direct Gaussian 3D Editing by Consistent Multi-view Editing</title><link>http://arxiv.org/abs/2404.18929v1</link><description>We consider the problem of editing 3D objects and scenes based on open-endedlanguage instructions. The established paradigm to solve this problem is to usea 2D image generator or editor to guide the 3D editing process. However, thisis often slow as it requires do update a computationally expensive 3Drepresentations such as a neural radiance field, and to do so by usingcontradictory guidance from a 2D model which is inherently not multi-viewconsistent. We thus introduce the Direct Gaussian Editor (DGE), a method thataddresses these issues in two ways. First, we modify a given high-quality imageeditor like InstructPix2Pix to be multi-view consistent. We do so by utilizinga training-free approach which integrates cues from the underlying 3D geometryof the scene. Second, given a multi-view consistent edited sequence of imagesof the object, we directly and efficiently optimize the 3D objectrepresentation, which is based on 3D Gaussian Splatting. Because it does notrequire to apply edits incrementally and iteratively, DGE is significantly moreefficient than existing approaches, and comes with other perks such as allowingselective editing of parts of the scene.</description><author>Minghao Chen, Iro Laina, Andrea Vedaldi</author><pubDate>Mon, 29 Apr 2024 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18929v1</guid></item><item><title>Stylus: Automatic Adapter Selection for Diffusion Models</title><link>http://arxiv.org/abs/2404.18928v1</link><description>Beyond scaling base models with more data or parameters, fine-tuned adaptersprovide an alternative way to generate high fidelity, custom images at reducedcosts. As such, adapters have been widely adopted by open-source communities,accumulating a database of over 100K adapters-most of which are highlycustomized with insufficient descriptions. This paper explores the problem ofmatching the prompt to a set of relevant adapters, built on recent work thathighlight the performance gains of composing adapters. We introduce Stylus,which efficiently selects and automatically composes task-specific adaptersbased on a prompt's keywords. Stylus outlines a three-stage approach that firstsummarizes adapters with improved descriptions and embeddings, retrievesrelevant adapters, and then further assembles adapters based on prompts'keywords by checking how well they fit the prompt. To evaluate Stylus, wedeveloped StylusDocs, a curated dataset featuring 75K adapters withpre-computed adapter embeddings. In our evaluation on popular Stable Diffusioncheckpoints, Stylus achieves greater CLIP-FID Pareto efficiency and is twice aspreferred, with humans and multimodal models as evaluators, over the basemodel. See stylus-diffusion.github.io for more.</description><author>Michael Luo, Justin Wong, Brandon Trabucco, Yanping Huang, Joseph E. Gonzalez, Zhifeng Chen, Ruslan Salakhutdinov, Ion Stoica</author><pubDate>Mon, 29 Apr 2024 18:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18928v1</guid></item><item><title>Point Cloud Models Improve Visual Robustness in Robotic Learners</title><link>http://arxiv.org/abs/2404.18926v1</link><description>Visual control policies can encounter significant performance degradationwhen visual conditions like lighting or camera position differ from those seenduring training -- often exhibiting sharp declines in capability even for minordifferences. In this work, we examine robustness to a suite of these types ofvisual changes for RGB-D and point cloud based visual control policies. Toperform these experiments on both model-free and model-based reinforcementlearners, we introduce a novel Point Cloud World Model (PCWM) and point cloudbased control policies. Our experiments show that policies that explicitlyencode point clouds are significantly more robust than their RGB-Dcounterparts. Further, we find our proposed PCWM significantly outperformsprior works in terms of sample efficiency during training. Taken together,these results suggest reasoning about the 3D scene through point clouds canimprove performance, reduce learning time, and increase robustness for roboticlearners. Project Webpage: https://pvskand.github.io/projects/PCWM</description><author>Skand Peri, Iain Lee, Chanho Kim, Li Fuxin, Tucker Hermans, Stefan Lee</author><pubDate>Mon, 29 Apr 2024 18:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18926v1</guid></item><item><title>Swin2-MoSE: A New Single Image Super-Resolution Model for Remote Sensing</title><link>http://arxiv.org/abs/2404.18924v1</link><description>Due to the limitations of current optical and sensor technologies and thehigh cost of updating them, the spectral and spatial resolution of satellitesmay not always meet desired requirements. For these reasons, Remote-SensingSingle-Image Super-Resolution (RS-SISR) techniques have gained significantinterest. In this paper, we propose Swin2-MoSE model, an enhanced version ofSwin2SR. Our model introduces MoE-SM, an enhanced Mixture-of-Experts (MoE) toreplace the Feed-Forward inside all Transformer block. MoE-SM is designed withSmart-Merger, and new layer for merging the output of individual experts, andwith a new way to split the work between experts, defining a new per-examplestrategy instead of the commonly used per-token one. Furthermore, we analyzehow positional encodings interact with each other, demonstrating thatper-channel bias and per-head bias can positively cooperate. Finally, wepropose to use a combination of Normalized-Cross-Correlation (NCC) andStructural Similarity Index Measure (SSIM) losses, to avoid typical MSE losslimitations. Experimental results demonstrate that Swin2-MoSE outperforms SOTAby up to 0.377 ~ 0.958 dB (PSNR) on task of 2x, 3x and 4x resolution-upscaling(Sen2Venus and OLI2MSI datasets). We show the efficacy of Swin2-MoSE, applyingit to a semantic segmentation task (SeasoNet dataset). Code and pretrained areavailable on https://github.com/IMPLabUniPr/swin2-mose/tree/official_code</description><author>Leonardo Rossi, Vittorio Bernuzzi, Tomaso Fontanini, Massimo Bertozzi, Andrea Prati</author><pubDate>Mon, 29 Apr 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18924v1</guid></item><item><title>Holmes: Benchmark the Linguistic Competence of Language Models</title><link>http://arxiv.org/abs/2404.18923v1</link><description>We introduce Holmes, a benchmark to assess the linguistic competence oflanguage models (LMs) - their ability to grasp linguistic phenomena. Unlikeprior prompting-based evaluations, Holmes assesses the linguistic competence ofLMs via their internal representations using classifier-based probing. In doingso, we disentangle specific phenomena (e.g., part-of-speech of words) fromother cognitive abilities, like following textual instructions, and meet recentcalls to assess LMs' linguistic competence in isolation. Composing Holmes, wereview over 250 probing studies and feature more than 200 datasets to assesssyntax, morphology, semantics, reasoning, and discourse phenomena. Analyzingover 50 LMs reveals that, aligned with known trends, their linguisticcompetence correlates with model size. However, surprisingly, modelarchitecture and instruction tuning also significantly influence performance,particularly in morphology and syntax. Finally, we propose FlashHolmes, astreamlined version of Holmes designed to lower the high computation load whilemaintaining high-ranking precision.</description><author>Andreas Waldis, Yotam Perlitz, Leshem Choshen, Yufang Hou, Iryna Gurevych</author><pubDate>Mon, 29 Apr 2024 18:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18923v1</guid></item><item><title>DPO Meets PPO: Reinforced Token Optimization for RLHF</title><link>http://arxiv.org/abs/2404.18922v1</link><description>In the classical Reinforcement Learning from Human Feedback (RLHF) framework,Proximal Policy Optimization (PPO) is employed to learn from sparse,sentence-level rewards -- a challenging scenario in traditional deepreinforcement learning. Despite the great successes of PPO in the alignment ofstate-of-the-art closed-source large language models (LLMs), its open-sourceimplementation is still largely sub-optimal, as widely reported by numerousresearch studies. To address these issues, we introduce a framework that modelsRLHF problems as a Markov decision process (MDP), enabling the capture offine-grained token-wise information. Furthermore, we provide theoreticalinsights that demonstrate the superiority of our MDP framework over theprevious sentence-level bandit formulation. Under this framework, we introducean algorithm, dubbed as Reinforced Token Optimization (\texttt{RTO}), whichlearns the token-wise reward function from preference data and performs policyoptimization based on this learned token-wise reward signal. Theoretically,\texttt{RTO} is proven to have the capability of finding the near-optimalpolicy sample-efficiently. For its practical implementation, \texttt{RTO}innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,originally derived from sparse sentence rewards, surprisingly provides us witha token-wise characterization of response quality, which is seamlesslyincorporated into our subsequent PPO training stage. Extensive real-worldalignment experiments verify the effectiveness of the proposed approach.</description><author>Han Zhong, Guhao Feng, Wei Xiong, Li Zhao, Di He, Jiang Bian, Liwei Wang</author><pubDate>Mon, 29 Apr 2024 18:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18922v1</guid></item><item><title>TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation</title><link>http://arxiv.org/abs/2404.18919v1</link><description>Recent advances in diffusion models can generate high-quality and stunningimages from text. However, multi-turn image generation, which is of high demandin real-world scenarios, still faces challenges in maintaining semanticconsistency between images and texts, as well as contextual consistency of thesame subject across multiple interactive turns. To address this issue, weintroduce TheaterGen, a training-free framework that integrates large languagemodels (LLMs) and text-to-image (T2I) models to provide the capability ofmulti-turn image generation. Within this framework, LLMs, acting as a"Screenwriter", engage in multi-turn interaction, generating and managing astandardized prompt book that encompasses prompts and layout designs for eachcharacter in the target image. Based on these, Theatergen generate a list ofcharacter images and extract guidance information, akin to the "Rehearsal".Subsequently, through incorporating the prompt book and guidance informationinto the reverse denoising process of T2I diffusion models, Theatergen generatethe final image, as conducting the "Final Performance". With the effectivemanagement of prompt books and character images, TheaterGen significantlyimproves semantic and contextual consistency in synthesized images.Furthermore, we introduce a dedicated benchmark, CMIGBench (ConsistentMulti-turn Image Generation Benchmark) with 8000 multi-turn instructions.Different from previous multi-turn benchmarks, CMIGBench does not definecharacters in advance. Both the tasks of story generation and multi-turnediting are included on CMIGBench for comprehensive evaluation. Extensiveexperimental results show that TheaterGen outperforms state-of-the-art methodssignificantly. It raises the performance bar of the cutting-edge Mini DALLE 3model by 21% in average character-character similarity and 19% in averagetext-image similarity.</description><author>Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang</author><pubDate>Mon, 29 Apr 2024 18:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18919v1</guid></item><item><title>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</title><link>http://arxiv.org/abs/2311.18259v3</link><description>We present Ego-Exo4D, a diverse, large-scale multimodal multiview videodataset and benchmark challenge. Ego-Exo4D centers aroundsimultaneously-captured egocentric and exocentric video of skilled humanactivities (e.g., sports, music, dance, bike repair). 740 participants from 13cities worldwide performed these activities in 123 different natural scenecontexts, yielding long-form captures from 1 to 42 minutes each and 1,286 hoursof video combined. The multimodal nature of the dataset is unprecedented: thevideo is accompanied by multichannel audio, eye gaze, 3D point clouds, cameraposes, IMU, and multiple paired language descriptions -- including a novel"expert commentary" done by coaches and teachers and tailored to theskilled-activity domain. To push the frontier of first-person videounderstanding of skilled human activity, we also present a suite of benchmarktasks and their annotations, including fine-grained activity understanding,proficiency estimation, cross-view translation, and 3D hand/body pose. Allresources are open sourced to fuel new research in the community. Project page:http://ego-exo4d-data.org/</description><author>Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu-Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria Escobar, Cristhian Forigua, Abrham Gebreselasie, Sanjay Haresh, Jing Huang, Md Mohaiminul Islam, Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin J Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh Kumar Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Wesli</author><pubDate>Mon, 29 Apr 2024 18:55:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18259v3</guid></item><item><title>Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting</title><link>http://arxiv.org/abs/2404.18911v1</link><description>Speculative decoding has demonstrated its effectiveness in accelerating theinference of large language models while maintaining a consistent samplingdistribution. However, the conventional approach of training a separate draftmodel to achieve a satisfactory token acceptance rate can be costly. Drawinginspiration from early exiting, we propose a novel self-speculative decodingframework \emph{Kangaroo}, which uses a fixed shallow sub-network as aself-draft model, with the remaining layers serving as the larger target model.We train a lightweight and efficient adapter module on top of the sub-networkto bridge the gap between the sub-network and the full model's representationability. It is noteworthy that the inference latency of the self-draft modelmay no longer be negligible compared to the large model, necessitatingstrategies to increase the token acceptance rate while minimizing the draftingsteps of the small model. To address this challenge, we introduce an additionalearly exiting mechanism for generating draft tokens. Specifically, we halt thesmall model's subsequent prediction during the drafting phase once theconfidence level for the current token falls below a certain threshold.Extensive experiments on the Spec-Bench demonstrate the effectiveness ofKangaroo. Under single-sequence verification, Kangaroo achieves speedups up to$1.68\times$ on Spec-Bench, outperforming Medusa-1 with 88.7\% fewer additionalparameters (67M compared to 591M). The code for Kangaroo is available athttps://github.com/Equationliu/Kangaroo.</description><author>Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Kai Han, Yunhe Wang</author><pubDate>Mon, 29 Apr 2024 18:53:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18911v1</guid></item><item><title>Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face of Environmental Uncertainty</title><link>http://arxiv.org/abs/2404.18909v1</link><description>To overcome the sim-to-real gap in reinforcement learning (RL), learnedpolicies must maintain robustness against environmental uncertainties. Whilerobust RL has been widely studied in single-agent regimes, in multi-agentenvironments, the problem remains understudied -- despite the fact that theproblems posed by environmental uncertainties are often exacerbated bystrategic interactions. This work focuses on learning in distributionallyrobust Markov games (RMGs), a robust variant of standard Markov games, whereineach agent aims to learn a policy that maximizes its own worst-case performancewhen the deployed environment deviates within its own prescribed uncertaintyset. This results in a set of robust equilibrium strategies for all agents thatalign with classic notions of game-theoretic equilibria. Assuming anon-adaptive sampling mechanism from a generative model, we propose asample-efficient model-based algorithm (DRNVI) with finite-sample complexityguarantees for learning robust variants of various notions of game-theoreticequilibria. We also establish an information-theoretic lower bound for solvingRMGs, which confirms the near-optimal sample complexity of DRNVI with respectto problem-dependent factors such as the size of the state space, the targetaccuracy, and the horizon length.</description><author>Laixi Shi, Eric Mazumdar, Yuejie Chi, Adam Wierman</author><pubDate>Mon, 29 Apr 2024 18:51:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18909v1</guid></item><item><title>Make-it-Real: Unleashing Large Multimodal Model's Ability for Painting 3D Objects with Realistic Materials</title><link>http://arxiv.org/abs/2404.16829v2</link><description>Physically realistic materials are pivotal in augmenting the realism of 3Dassets across various applications and lighting conditions. However, existing3D assets and generative models often lack authentic material properties.Manual assignment of materials using graphic software is a tedious andtime-consuming task. In this paper, we exploit advancements in Multimodal LargeLanguage Models (MLLMs), particularly GPT-4V, to present a novel approach,Make-it-Real: 1) We demonstrate that GPT-4V can effectively recognize anddescribe materials, allowing the construction of a detailed material library.2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4Vprecisely identifies and aligns materials with the corresponding components of3D objects. 3) The correctly matched materials are then meticulously applied asreference for the new SVBRDF material generation according to the originaldiffuse map, significantly enhancing their visual authenticity. Make-it-Realoffers a streamlined integration into the 3D content creation workflow,showcasing its utility as an essential tool for developers of 3D assets.</description><author>Ye Fang, Zeyi Sun, Tong Wu, Jiaqi Wang, Ziwei Liu, Gordon Wetzstein, Dahua Lin</author><pubDate>Mon, 29 Apr 2024 18:48:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16829v2</guid></item><item><title>Benchmarking the CoW with the TopCoW Challenge: Topology-Aware Anatomical Segmentation of the Circle of Willis for CTA and MRA</title><link>http://arxiv.org/abs/2312.17670v3</link><description>The Circle of Willis (CoW) is an important network of arteries connectingmajor circulations of the brain. Its vascular architecture is believed toaffect the risk, severity, and clinical outcome of serious neuro-vasculardiseases. However, characterizing the highly variable CoW anatomy is still amanual and time-consuming expert task. The CoW is usually imaged by twoangiographic imaging modalities, magnetic resonance angiography (MRA) andcomputed tomography angiography (CTA), but there exist limited public datasetswith annotations on CoW anatomy, especially for CTA. Therefore we organized theTopCoW Challenge in 2023 with the release of an annotated CoW dataset. TheTopCoW dataset was the first public dataset with voxel-level annotations forthirteen possible CoW vessel components, enabled by virtual-reality (VR)technology. It was also the first large dataset with paired MRA and CTA fromthe same patients. TopCoW challenge formalized the CoW characterization problemas a multiclass anatomical segmentation task with an emphasis on topologicalmetrics. We invited submissions worldwide for the CoW segmentation task, whichattracted over 140 registered participants from four continents. The topperforming teams managed to segment many CoW components to Dice scores around90%, but with lower scores for communicating arteries and rare variants. Therewere also topological mistakes for predictions with high Dice scores.Additional topological analysis revealed further areas for improvement indetecting certain CoW components and matching CoW variant topology accurately.TopCoW represented a first attempt at benchmarking the CoW anatomicalsegmentation task for MRA and CTA, both morphologically and topologically.</description><author>Kaiyuan Yang, Fabio Musio, Yihui Ma, Norman Juchler, Johannes C. Paetzold, Rami Al-Maskari, Luciano Höher, Hongwei Bran Li, Ibrahim Ethem Hamamci, Anjany Sekuboyina, Suprosanna Shit, Houjing Huang, Chinmay Prabhakar, Ezequiel de la Rosa, Diana Waldmannstetter, Florian Kofler, Fernando Navarro, Martin Menten, Ivan Ezhov, Daniel Rueckert, Iris Vos, Ynte Ruigrok, Birgitta Velthuis, Hugo Kuijf, Julien Hämmerli, Catherine Wurster, Philippe Bijlenga, Laura Westphal, Jeroen Bisschop, Elisa Colombo, Hakim Baazaoui, Andrew Makmur, James Hallinan, Bene Wiestler, Jan S. Kirschke, Roland Wiest, Emmanuel Montagnon, Laurent Letourneau-Guillon, Adrian Galdran, Francesco Galati, Daniele Falcetta, Maria A. Zuluaga, Chaolong Lin, Haoran Zhao, Zehan Zhang, Sinyoung Ra, Jongyun Hwang, Hyunjin Park, Junqiang C</author><pubDate>Mon, 29 Apr 2024 18:45:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17670v3</guid></item><item><title>Detecting critical treatment effect bias in small subgroups</title><link>http://arxiv.org/abs/2404.18905v1</link><description>Randomized trials are considered the gold standard for making informeddecisions in medicine, yet they often lack generalizability to the patientpopulations in clinical practice. Observational studies, on the other hand,cover a broader patient population but are prone to various biases. Thus,before using an observational study for decision-making, it is crucial tobenchmark its treatment effect estimates against those derived from arandomized trial. We propose a novel strategy to benchmark observationalstudies beyond the average treatment effect. First, we design a statisticaltest for the null hypothesis that the treatment effects estimated from the twostudies, conditioned on a set of relevant features, differ up to sometolerance. We then estimate an asymptotically valid lower bound on the maximumbias strength for any subgroup in the observational study. Finally, we validateour benchmarking strategy in a real-world setting and show that it leads toconclusions that align with established medical knowledge.</description><author>Piersilvio De Bartolomeis, Javier Abad, Konstantin Donhauser, Fanny Yang</author><pubDate>Mon, 29 Apr 2024 18:44:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18905v1</guid></item><item><title>Talking Nonsense: Probing Large Language Models' Understanding of Adversarial Gibberish Inputs</title><link>http://arxiv.org/abs/2404.17120v2</link><description>Large language models (LLMs) exhibit excellent ability to understand humanlanguages, but do they also understand their own language that appearsgibberish to us? In this work we delve into this question, aiming to uncoverthe mechanisms underlying such behavior in LLMs. We employ the GreedyCoordinate Gradient optimizer to craft prompts that compel LLMs to generatecoherent responses from seemingly nonsensical inputs. We call these inputs LMBabel and this work systematically studies the behavior of LLMs manipulated bythese prompts. We find that the manipulation efficiency depends on the targettext's length and perplexity, with the Babel prompts often located in lowerloss minima compared to natural prompts. We further examine the structure ofthe Babel prompts and evaluate their robustness. Notably, we find that guidingthe model to generate harmful texts is not more difficult than into generatingbenign texts, suggesting lack of alignment for out-of-distribution prompts.</description><author>Valeriia Cherepanova, James Zou</author><pubDate>Mon, 29 Apr 2024 18:41:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17120v2</guid></item><item><title>Stick to your Role! Context-dependence and Stability of Personal Values Expression in Large Language Models</title><link>http://arxiv.org/abs/2402.14846v2</link><description>The standard way to study Large Language Models (LLMs) with benchmarks orpsychology questionnaires is to provide many different queries from similarminimal contexts (e.g. multiple choice questions). However, due to LLMs' highlycontext-dependent nature, conclusions from such minimal-context evaluations maybe little informative about the model's behavior in deployment (where it willbe exposed to many new contexts). We argue that context-dependence(specifically, value stability) should be studied a specific property of LLMsand used as another dimension of LLM comparison (alongside others such ascognitive abilities, knowledge, or model size). We present a case-study on thestability of value expression over different contexts (simulated conversationson different topics) as measured using a standard psychology questionnaire(PVQ) and on behavioral downstream tasks. Reusing methods from psychology, westudy Rank-order stability on the population (interpersonal) level, andIpsative stability on the individual (intrapersonal) level. We consider twosettings (with and without instructing LLMs to simulate particular personas),two simulated populations, and three downstream tasks. We observe consistenttrends in the stability of models and model families - Mixtral, Mistral,GPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistencyof these trends implies that some models exhibit higher value-stability thanothers, and that value stability can be estimated with the set of introducedmethodological tools. When instructed to simulate particular personas, LLMsexhibit low Rank-Order stability, which further diminishes with conversationlength. This highlights the need for future research on LLMs that coherentlysimulate different personas. This paper provides a foundational step in thatdirection, and, to our knowledge, it is the first study of value stability inLLMs.</description><author>Grgur Kovač, Rémy Portelas, Masataka Sawayama, Peter Ford Dominey, Pierre-Yves Oudeyer</author><pubDate>Mon, 29 Apr 2024 18:36:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14846v2</guid></item><item><title>Amodal Ground Truth and Completion in the Wild</title><link>http://arxiv.org/abs/2312.17247v2</link><description>This paper studies amodal image segmentation: predicting entire objectsegmentation masks including both visible and invisible (occluded) parts. Inprevious work, the amodal segmentation ground truth on real images is usuallypredicted by manual annotaton and thus is subjective. In contrast, we use 3Ddata to establish an automatic pipeline to determine authentic ground truthamodal masks for partially occluded objects in real images. This pipeline isused to construct an amodal completion evaluation benchmark, MP3D-Amodal,consisting of a variety of object categories and labels. To better handle theamodal completion task in the wild, we explore two architecture variants: atwo-stage model that first infers the occluder, followed by amodal maskcompletion; and a one-stage model that exploits the representation power ofStable Diffusion for amodal segmentation across many categories. Without bellsand whistles, our method achieves a new state-of-the-art performance on Amodalsegmentation datasets that cover a large variety of objects, including COCOAand our new MP3D-Amodal dataset. The dataset, model, and code are available athttps://www.robots.ox.ac.uk/~vgg/research/amodal/.</description><author>Guanqi Zhan, Chuanxia Zheng, Weidi Xie, Andrew Zisserman</author><pubDate>Mon, 29 Apr 2024 18:35:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17247v2</guid></item><item><title>Overcoming Knowledge Barriers: Online Imitation Learning from Observation with Pretrained World Models</title><link>http://arxiv.org/abs/2404.18896v1</link><description>Incorporating the successful paradigm of pretraining and finetuning fromComputer Vision and Natural Language Processing into decision-making has becomeincreasingly popular in recent years. In this paper, we study ImitationLearning from Observation with pretrained models and find existing approachessuch as BCO and AIME face knowledge barriers, specifically the EmbodimentKnowledge Barrier (EKB) and the Demonstration Knowledge Barrier (DKB), greatlylimiting their performance. The EKB arises when pretrained models lackknowledge about unseen observations, leading to errors in action inference. TheDKB results from policies trained on limited demonstrations, hinderingadaptability to diverse scenarios. We thoroughly analyse the underlyingmechanism of these barriers and propose AIME-v2 upon AIME as a solution.AIME-v2 uses online interactions with data-driven regulariser to alleviate theEKB and mitigates the DKB by introducing a surrogate reward function to enhancepolicy training. Experimental results on tasks from the DeepMind Control Suiteand Meta-World benchmarks demonstrate the effectiveness of these modificationsin improving both sample-efficiency and converged performance. The studycontributes valuable insights into resolving knowledge barriers for enhanceddecision-making in pretraining-based approaches. Code will be available athttps://github.com/argmax-ai/aime-v2.</description><author>Xingyuan Zhang, Philip Becker-Ehmck, Patrick van der Smagt, Maximilian Karl</author><pubDate>Mon, 29 Apr 2024 18:33:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18896v1</guid></item><item><title>RSCaMa: Remote Sensing Image Change Captioning with State Space Model</title><link>http://arxiv.org/abs/2404.18895v1</link><description>Remote Sensing Image Change Captioning (RSICC) aims to identify surfacechanges in multi-temporal remote sensing images and describe them in naturallanguage. Current methods typically rely on an encoder-decoder architecture andfocus on designing a sophisticated neck to process bi-temporal featuresextracted by the backbone. Recently, State Space Models (SSMs), especiallyMamba, have demonstrated outstanding performance in many fields, owing to theirefficient feature-selective modelling capability. However, their potential inthe RSICC task remains unexplored. In this paper, we introduce Mamba into RSICCand propose a novel approach called RSCaMa (Remote Sensing Change CaptioningMamba). Specifically, we utilize Siamese backbones to extract bi-temporalfeatures, which are then processed through multiple CaMa layers consisting ofSpatial Difference-guided SSM (SD-SSM) and Temporal Traveling SSM (TT-SSM).SD-SSM uses differential features to enhance change perception, while TT-SSMpromotes bitemporal interactions in a token-wise cross-scanning manner.Experimental results validate the effectiveness of CaMa layers and demonstratethe superior performance of RSCaMa, as well as the potential of Mamba in theRSICC task. Additionally, we systematically compare the effects of threelanguage decoders, including Mamba, GPT-style decoder with causal attentionmechanism, and Transformer decoder with cross-attention mechanism. Thisprovides valuable insights for future RSICC research. The code will beavailable at https://github.com/Chen-Yang-Liu/RSCaMa</description><author>Chenyang Liu, Keyan Chen, Bowen Chen, Haotian Zhang, Zhengxia Zou, Zhenwei Shi</author><pubDate>Mon, 29 Apr 2024 18:31:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18895v1</guid></item><item><title>Learning general Gaussian mixtures with efficient score matching</title><link>http://arxiv.org/abs/2404.18893v1</link><description>We study the problem of learning mixtures of $k$ Gaussians in $d$ dimensions.We make no separation assumptions on the underlying mixture components: we onlyrequire that the covariance matrices have bounded condition number and that themeans and covariances lie in a ball of bounded radius. We give an algorithmthat draws $d^{\mathrm{poly}(k/\varepsilon)}$ samples from the target mixture,runs in sample-polynomial time, and constructs a sampler whose outputdistribution is $\varepsilon$-far from the unknown mixture in total variation.Prior works for this problem either (i) required exponential runtime in thedimension $d$, (ii) placed strong assumptions on the instance (e.g., sphericalcovariances or clusterability), or (iii) had doubly exponential dependence onthe number of components $k$. Our approach departs from commonly used techniques for this problem like themethod of moments. Instead, we leverage a recently developed reduction, basedon diffusion models, from distribution learning to a supervised learning taskcalled score matching. We give an algorithm for the latter by proving astructural result showing that the score function of a Gaussian mixture can beapproximated by a piecewise-polynomial function, and there is an efficientalgorithm for finding it. To our knowledge, this is the first example ofdiffusion models achieving a state-of-the-art theoretical guarantee for anunsupervised learning task.</description><author>Sitan Chen, Vasilis Kontonis, Kulin Shah</author><pubDate>Mon, 29 Apr 2024 18:30:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18893v1</guid></item><item><title>IPixMatch: Boost Semi-supervised Semantic Segmentation with Inter-Pixel Relation</title><link>http://arxiv.org/abs/2404.18891v1</link><description>The scarcity of labeled data in real-world scenarios is a critical bottleneckof deep learning's effectiveness. Semi-supervised semantic segmentation hasbeen a typical solution to achieve a desirable tradeoff between annotation costand segmentation performance. However, previous approaches, whether based onconsistency regularization or self-training, tend to neglect the contextualknowledge embedded within inter-pixel relations. This negligence leads tosuboptimal performance and limited generalization. In this paper, we propose anovel approach IPixMatch designed to mine the neglected but valuableInter-Pixel information for semi-supervised learning. Specifically, IPixMatchis constructed as an extension of the standard teacher-student network,incorporating additional loss terms to capture inter-pixel relations. It shinesin low-data regimes by efficiently leveraging the limited labeled data andextracting maximum utility from the available unlabeled data. Furthermore,IPixMatch can be integrated seamlessly into most teacher-student frameworkswithout the need of model modification or adding additional components. Ourstraightforward IPixMatch method demonstrates consistent performanceimprovements across various benchmark datasets under different partitioningprotocols.</description><author>Kebin Wu, Wenbin Li, Xiaofei Xiao</author><pubDate>Mon, 29 Apr 2024 18:27:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18891v1</guid></item><item><title>Hide and Seek: How Does Watermarking Impact Face Recognition?</title><link>http://arxiv.org/abs/2404.18890v1</link><description>The recent progress in generative models has revolutionized the synthesis ofhighly realistic images, including face images. This technological developmenthas undoubtedly helped face recognition, such as training data augmentation forhigher recognition accuracy and data privacy. However, it has also introducednovel challenges concerning the responsible use and proper attribution ofcomputer generated images. We investigate the impact of digital watermarking, atechnique for embedding ownership signatures into images, on the effectivenessof face recognition models. We propose a comprehensive pipeline that integratesface image generation, watermarking, and face recognition to systematicallyexamine this question. The proposed watermarking scheme, based on anencoder-decoder architecture, successfully embeds and recovers signatures fromboth real and synthetic face images while preserving their visual fidelity.Through extensive experiments, we unveil that while watermarking enables robustimage attribution, it results in a slight decline in face recognition accuracy,particularly evident for face images with challenging poses and expressions.Additionally, we find that directly training face recognition models onwatermarked images offers only a limited alleviation of this performancedecline. Our findings underscore the intricate trade off between watermarkingand face recognition accuracy. This work represents a pivotal step towards theresponsible utilization of generative models in face recognition and serves toinitiate discussions regarding the broader implications of watermarking inbiometrics.</description><author>Yuguang Yao, Steven Grosz, Sijia Liu, Anil Jain</author><pubDate>Mon, 29 Apr 2024 18:27:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18890v1</guid></item><item><title>Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models</title><link>http://arxiv.org/abs/2310.13828v3</link><description>Data poisoning attacks manipulate training data to introduce unexpectedbehaviors into machine learning models at training time. For text-to-imagegenerative models with massive training datasets, current understanding ofpoisoning attacks suggests that a successful attack would require injectingmillions of poison samples into their training pipeline. In this paper, we showthat poisoning attacks can be successful on generative models. We observe thattraining data per concept can be quite limited in these models, making themvulnerable to prompt-specific poisoning attacks, which target a model's abilityto respond to individual prompts. We introduce Nightshade, an optimized prompt-specific poisoning attack wherepoison samples look visually identical to benign images with matching textprompts. Nightshade poison samples are also optimized for potency and cancorrupt an Stable Diffusion SDXL prompt in &lt;100 poison samples. Nightshadepoison effects "bleed through" to related concepts, and multiple attacks cancomposed together in a single prompt. Surprisingly, we show that a moderatenumber of Nightshade attacks can destabilize general features in atext-to-image generative model, effectively disabling its ability to generatemeaningful images. Finally, we propose the use of Nightshade and similar toolsas a last defense for content creators against web scrapers that ignoreopt-out/do-not-crawl directives, and discuss possible implications for modeltrainers and content creators.</description><author>Shawn Shan, Wenxin Ding, Josephine Passananti, Stanley Wu, Haitao Zheng, Ben Y. Zhao</author><pubDate>Mon, 29 Apr 2024 18:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13828v3</guid></item><item><title>Interpolation and differentiation of alchemical degrees of freedom in machine learning interatomic potentials</title><link>http://arxiv.org/abs/2404.10746v2</link><description>Machine learning interatomic potentials (MLIPs) have become a workhorse ofmodern atomistic simulations, and recently published universal MLIPs,pre-trained on large datasets, have demonstrated remarkable accuracy andgeneralizability. However, the computational cost of MLIPs limits theirapplicability to chemically disordered systems requiring large simulation cellsor to sample-intensive statistical methods. Here, we report the use ofcontinuous and differentiable alchemical degrees of freedom in atomisticmaterials simulations, exploiting the fact that graph neural network MLIPsrepresent discrete elements as real-valued tensors. The proposed methodintroduces alchemical atoms with corresponding weights into the input graph,alongside modifications to the message-passing and readout mechanisms of MLIPs,and allows smooth interpolation between the compositional states of materials.The end-to-end differentiability of MLIPs enables efficient calculation of thegradient of energy with respect to the compositional weights. Leveraging thesegradients, we propose methodologies for optimizing the composition of solidsolutions towards target macroscopic properties and conducting alchemical freeenergy simulations to quantify the free energy of vacancy formation andcomposition changes. The approach offers an avenue for extending thecapabilities of universal MLIPs in the modeling of compositional disorder andcharacterizing the phase stabilities of complex materials systems.</description><author>Juno Nam, Rafael Gómez-Bombarelli</author><pubDate>Mon, 29 Apr 2024 18:21:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10746v2</guid></item><item><title>Adaptive Input-image Normalization for Solving the Mode Collapse Problem in GAN-based X-ray Images</title><link>http://arxiv.org/abs/2309.12245v3</link><description>Biomedical image datasets can be imbalanced due to the rarity of targeteddiseases. Generative Adversarial Networks play a key role in addressing thisimbalance by enabling the generation of synthetic images to augment datasets.It is important to generate synthetic images that incorporate a diverse rangeof features to accurately represent the distribution of features present in thetraining imagery. Furthermore, the absence of diverse features in syntheticimages can degrade the performance of machine learning classifiers. The modecollapse problem impacts Generative Adversarial Networks' capacity to generatediversified images. Mode collapse comes in two varieties: intra-class andinter-class. In this paper, both varieties of the mode collapse problem areinvestigated, and their subsequent impact on the diversity of synthetic X-rayimages is evaluated. This work contributes an empirical demonstration of thebenefits of integrating the adaptive input-image normalization with the DeepConvolutional GAN and Auxiliary Classifier GAN to alleviate the mode collapseproblems. Synthetically generated images are utilized for data augmentation andtraining a Vision Transformer model. The classification performance of themodel is evaluated using accuracy, recall, and precision scores. Resultsdemonstrate that the DCGAN and the ACGAN with adaptive input-imagenormalization outperform the DCGAN and ACGAN with un-normalized X-ray images asevidenced by the superior diversity scores and classification scores.</description><author>Muhammad Muneeb Saad, Mubashir Husain Rehmani, Ruairi O'Reilly</author><pubDate>Mon, 29 Apr 2024 18:19:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12245v3</guid></item><item><title>A Survey on Diffusion Models for Time Series and Spatio-Temporal Data</title><link>http://arxiv.org/abs/2404.18886v1</link><description>The study of time series data is crucial for understanding trends andanomalies over time, enabling predictive insights across various sectors.Spatio-temporal data, on the other hand, is vital for analyzing phenomena inboth space and time, providing a dynamic perspective on complex systeminteractions. Recently, diffusion models have seen widespread application intime series and spatio-temporal data mining. Not only do they enhance thegenerative and inferential capabilities for sequential and temporal data, butthey also extend to other downstream tasks. In this survey, we comprehensivelyand thoroughly review the use of diffusion models in time series andspatio-temporal data, categorizing them by model category, task type, datamodality, and practical application domain. In detail, we categorize diffusionmodels into unconditioned and conditioned types and discuss time series dataand spatio-temporal data separately. Unconditioned models, which operateunsupervised, are subdivided into probability-based and score-based models,serving predictive and generative tasks such as forecasting, anomaly detection,classification, and imputation. Conditioned models, on the other hand, utilizeextra information to enhance performance and are similarly divided for bothpredictive and generative tasks. Our survey extensively covers theirapplication in various fields, including healthcare, recommendation, climate,energy, audio, and transportation, providing a foundational understanding ofhow these models analyze and generate data. Through this structured overview,we aim to provide researchers and practitioners with a comprehensiveunderstanding of diffusion models for time series and spatio-temporal dataanalysis, aiming to direct future innovations and applications by addressingtraditional challenges and exploring innovative solutions within the diffusionmodel framework.</description><author>Yiyuan Yang, Ming Jin, Haomin Wen, Chaoli Zhang, Yuxuan Liang, Lintao Ma, Yi Wang, Chenghao Liu, Bin Yang, Zenglin Xu, Jiang Bian, Shirui Pan, Qingsong Wen</author><pubDate>Mon, 29 Apr 2024 18:19:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18886v1</guid></item><item><title>Human-in-the-Loop Synthetic Text Data Inspection with Provenance Tracking</title><link>http://arxiv.org/abs/2404.18881v1</link><description>Data augmentation techniques apply transformations to existing texts togenerate additional data. The transformations may produce low-quality texts,where the meaning of the text is changed and the text may even be mangledbeyond human comprehension. Analyzing the synthetically generated texts andtheir corresponding labels is slow and demanding. To winnow out texts withincorrect labels, we develop INSPECTOR, a human-in-the-loop data inspectiontechnique. INSPECTOR combines the strengths of provenance tracking techniqueswith assistive labeling. INSPECTOR allows users to group related texts by theirtransformation provenance, i.e., the transformations applied to the originaltext, or feature provenance, the linguistic features of the original text. Forassistive labeling, INSPECTOR computes metrics that approximate data quality,and allows users to compare the corresponding label of each text against thepredictions of a large language model. In a user study, INSPECTOR increases thenumber of texts with correct labels identified by 3X on a sentiment analysistask and by 4X on a hate speech detection task. The participants found groupingthe synthetically generated texts by their common transformation to be the mostuseful technique. Surprisingly, grouping texts by common linguistic featureswas perceived to be unhelpful. Contrary to prior work, our study finds that nosingle technique obviates the need for human inspection effort. This validatesthe design of INSPECTOR which combines both analysis of data provenance andassistive labeling to reduce human inspection effort.</description><author>Hong Jin Kang, Fabrice Harel-Canada, Muhammad Ali Gulzar, Violet Peng, Miryung Kim</author><pubDate>Mon, 29 Apr 2024 18:16:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18881v1</guid></item><item><title>Emergent specialization from participation dynamics and multi-learner retraining</title><link>http://arxiv.org/abs/2206.02667v3</link><description>Numerous online services are data-driven: the behavior of users affects thesystem's parameters, and the system's parameters affect the users' experienceof the service, which in turn affects the way users may interact with thesystem. For example, people may choose to use a service only for tasks thatalready works well, or they may choose to switch to a different service. Theseadaptations influence the ability of a system to learn about a population ofusers and tasks in order to improve its performance broadly. In this work, weanalyze a class of such dynamics -- where users allocate their participationamongst services to reduce the individual risk they experience, and servicesupdate their model parameters to reduce the service's risk on their currentuser population. We refer to these dynamics as \emph{risk-reducing}, whichcover a broad class of common model updates including gradient descent andmultiplicative weights. For this general class of dynamics, we show thatasymptotically stable equilibria are always segmented, with sub-populationsallocated to a single learner. Under mild assumptions, the utilitarian socialoptimum is a stable equilibrium. In contrast to previous work, which shows thatrepeated risk minimization can result in (Hashimoto et al., 2018; Miller etal., 2021), we find that repeated myopic updates with multiple learners lead tobetter outcomes. We illustrate the phenomena via a simulated exampleinitialized from real data.</description><author>Sarah Dean, Mihaela Curmei, Lillian J. Ratliff, Jamie Morgenstern, Maryam Fazel</author><pubDate>Mon, 29 Apr 2024 18:16:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.02667v3</guid></item><item><title>Spivavtor: An Instruction Tuned Ukrainian Text Editing Model</title><link>http://arxiv.org/abs/2404.18880v1</link><description>We introduce Spivavtor, a dataset, and instruction-tuned models for textediting focused on the Ukrainian language. Spivavtor is the Ukrainian-focusedadaptation of the English-only CoEdIT model. Similar to CoEdIT, Spivavtorperforms text editing tasks by following instructions in Ukrainian. This paperdescribes the details of the Spivavtor-Instruct dataset and Spivavtor models.We evaluate Spivavtor on a variety of text editing tasks in Ukrainian, such asGrammatical Error Correction (GEC), Text Simplification, Coherence, andParaphrasing, and demonstrate its superior performance on all of them. Wepublicly release our best-performing models and data as resources to thecommunity to advance further research in this space.</description><author>Aman Saini, Artem Chernodub, Vipul Raheja, Vivek Kulkarni</author><pubDate>Mon, 29 Apr 2024 18:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18880v1</guid></item><item><title>A Multilevel Strategy to Improve People Tracking in a Real-World Scenario</title><link>http://arxiv.org/abs/2404.18876v1</link><description>The Pal\'acio do Planalto, office of the President of Brazil, was invaded byprotesters on January 8, 2023. Surveillance videos taken from inside thebuilding were subsequently released by the Brazilian Supreme Court for publicscrutiny. We used segments of such footage to create the UFPR-Planalto801dataset for people tracking and re-identification in a real-world scenario.This dataset consists of more than 500,000 images. This paper presents atracking approach targeting this dataset. The method proposed in this paperrelies on the use of known state-of-the-art trackers combined in a multilevelhierarchy to correct the ID association over the trajectories. We evaluated ourmethod using IDF1, MOTA, MOTP and HOTA metrics. The results show improvementsfor every tracker used in the experiments, with IDF1 score increasing by amargin up to 9.5%.</description><author>Cristiano B. de Oliveira, Joao C. Neves, Rafael O. Ribeiro, David Menotti</author><pubDate>Mon, 29 Apr 2024 18:10:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18876v1</guid></item><item><title>OpenStreetView-5M: The Many Roads to Global Visual Geolocation</title><link>http://arxiv.org/abs/2404.18873v1</link><description>Determining the location of an image anywhere on Earth is a complex visualtask, which makes it particularly relevant for evaluating computer visionalgorithms. Yet, the absence of standard, large-scale, open-access datasetswith reliably localizable images has limited its potential. To address thisissue, we introduce OpenStreetView-5M, a large-scale, open-access datasetcomprising over 5.1 million geo-referenced street view images, covering 225countries and territories. In contrast to existing benchmarks, we enforce astrict train/test separation, allowing us to evaluate the relevance of learnedgeographical features beyond mere memorization. To demonstrate the utility ofour dataset, we conduct an extensive benchmark of various state-of-the-artimage encoders, spatial representations, and training strategies. Allassociated codes and models can be found at https://github.com/gastruc/osv5m.</description><author>Guillaume Astruc, Nicolas Dufour, Ioannis Siglidis, Constantin Aronssohn, Nacim Bouia, Stephanie Fu, Romain Loiseau, Van Nguyen Nguyen, Charles Raude, Elliot Vincent, Lintao XU, Hongyu Zhou, Loic Landrieu</author><pubDate>Mon, 29 Apr 2024 18:06:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18873v1</guid></item><item><title>More RLHF, More Trust? On The Impact of Human Preference Alignment On Language Model Trustworthiness</title><link>http://arxiv.org/abs/2404.18870v1</link><description>The surge in Large Language Models (LLMs) development has led to improvedperformance on cognitive tasks as well as an urgent need to align these modelswith human values in order to safely exploit their power. Despite theeffectiveness of preference learning algorithms like Reinforcement LearningFrom Human Feedback (RLHF) in aligning human preferences, their assumedimprovements on model trustworthiness haven't been thoroughly testified. Towardthis end, this study investigates how models that have been aligned withgeneral-purpose preference data on helpfulness and harmlessness perform acrossfive trustworthiness verticals: toxicity, stereotypical bias, machine ethics,truthfulness, and privacy. For model alignment, we focus on three widely usedRLHF variants: Supervised Finetuning (SFT), Proximal Policy Optimization (PPO),and Direct Preference Optimization (DPO). Through extensive empiricalinvestigations, we discover that the improvement in trustworthiness by RLHF isfar from guaranteed, and there exists a complex interplay between preferencedata, alignment algorithms, and specific trustworthiness aspects. Together, ourresults underscore the need for more nuanced approaches for model alignment. Byshedding light on the intricate dynamics of these components within modelalignment, we hope this research will guide the community towards developinglanguage models that are both capable and trustworthy.</description><author>Aaron J. Li, Satyapriya Krishna, Himabindu Lakkaraju</author><pubDate>Mon, 29 Apr 2024 18:00:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18870v1</guid></item><item><title>Learning Mixtures of Gaussians Using Diffusion Models</title><link>http://arxiv.org/abs/2404.18869v1</link><description>We give a new algorithm for learning mixtures of $k$ Gaussians (with identitycovariance in $\mathbb{R}^n$) to TV error $\varepsilon$, with quasi-polynomial($O(n^{\text{poly log}\left(\frac{n+k}{\varepsilon}\right)})$) time and samplecomplexity, under a minimum weight assumption. Unlike previous approaches, mostof which are algebraic in nature, our approach is analytic and relies on theframework of diffusion models. Diffusion models are a modern paradigm forgenerative modeling, which typically rely on learning the score function(gradient log-pdf) along a process transforming a pure noise distribution, inour case a Gaussian, to the data distribution. Despite their dazzlingperformance in tasks such as image generation, there are few end-to-endtheoretical guarantees that they can efficiently learn nontrivial families ofdistributions; we give some of the first such guarantees. We proceed byderiving higher-order Gaussian noise sensitivity bounds for the score functionsfor a Gaussian mixture to show that that they can be inductively learned usingpiecewise polynomial regression (up to poly-logarithmic degree), and combinethis with known convergence results for diffusion models. Our results extend tocontinuous mixtures of Gaussians where the mixing distribution is supported ona union of $k$ balls of constant radius. In particular, this applies to thecase of Gaussian convolutions of distributions on low-dimensional manifolds, ormore generally sets with small covering number.</description><author>Khashayar Gatmiry, Jonathan Kelner, Holden Lee</author><pubDate>Mon, 29 Apr 2024 18:00:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18869v1</guid></item><item><title>Truth-value judgment in language models: belief directions are context sensitive</title><link>http://arxiv.org/abs/2404.18865v1</link><description>Recent work has demonstrated that the latent spaces of large language models(LLMs) contain directions predictive of the truth of sentences. Multiplemethods recover such directions and build probes that are described as gettingat a model's "knowledge" or "beliefs". We investigate this phenomenon, lookingclosely at the impact of context on the probes. Our experiments establish wherein the LLM the probe's predictions can be described as being conditional on thepreceding (related) sentences. Specifically, we quantify the responsiveness ofthe probes to the presence of (negated) supporting and contradicting sentences,and score the probes on their consistency. We also perform a causalintervention experiment, investigating whether moving the representation of apremise along these belief directions influences the position of the hypothesisalong that same direction. We find that the probes we test are generallycontext sensitive, but that contexts which should not affect the truth oftenstill impact the probe outputs. Our experiments show that the type of errorsdepend on the layer, the (type of) model, and the kind of data. Finally, ourresults suggest that belief directions are (one of the) causal mediators in theinference process that incorporates in-context information.</description><author>Stefan F. Schouten, Peter Bloem, Ilia Markov, Piek Vossen</author><pubDate>Mon, 29 Apr 2024 17:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18865v1</guid></item><item><title>Performance-Aligned LLMs for Generating Fast Code</title><link>http://arxiv.org/abs/2404.18864v1</link><description>Optimizing scientific software is a difficult task because codebases areoften large and complex, and performance can depend upon several factorsincluding the algorithm, its implementation, and hardware among others. Causesof poor performance can originate from disparate sources and be difficult todiagnose. Recent years have seen a multitude of work that use large languagemodels (LLMs) to assist in software development tasks. However, these tools aretrained to model the distribution of code as text, and are not specificallydesigned to understand performance aspects of code. In this work, we introducea reinforcement learning based methodology to align the outputs of code LLMswith performance. This allows us to build upon the current code modelingcapabilities of LLMs and extend them to generate better performing code. Wedemonstrate that our fine-tuned model improves the expected speedup ofgenerated code over base models for a set of benchmark tasks from 0.9 to 1.6for serial code and 1.9 to 4.5 for OpenMP code.</description><author>Daniel Nichols, Pranav Polasam, Harshitha Menon, Aniruddha Marathe, Todd Gamblin, Abhinav Bhatele</author><pubDate>Mon, 29 Apr 2024 17:52:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18864v1</guid></item><item><title>A Survey on Vision Mamba: Models, Applications and Challenges</title><link>http://arxiv.org/abs/2404.18861v1</link><description>Mamba, a recent selective structured state space model, performs excellentlyon long sequence modeling tasks. Mamba mitigates the modeling constraints ofconvolutional neural networks and offers advanced modeling capabilities similarto those of Transformers, through global receptive fields and dynamicweighting. Crucially, it achieves this without incurring the quadraticcomputational complexity typically associated with Transformers. Due to itsadvantages over the former two mainstream foundation models, Mamba exhibitsgreat potential to be a visual foundation model. Researchers are activelyapplying Mamba to various computer vision tasks, leading to numerous emergingworks. To help keep pace with the rapid advancements in computer vision, thispaper aims to provide a comprehensive review of visual Mamba approaches. Thispaper begins by delineating the formulation of the original Mamba model.Subsequently, our review of visual Mamba delves into several representativebackbone networks to elucidate the core insights of the visual Mamba. We thencategorize related works using different modalities, including image, video,point cloud, multi-modal, and others. Specifically, for image applications, wefurther organize them into distinct tasks to facilitate a more structureddiscussion. Finally, we discuss the challenges and future research directionsfor visual Mamba, providing insights for future research in this quicklyevolving area. A comprehensive list of visual Mamba models reviewed in thiswork is available at https://github.com/Ruixxxx/Awesome-Vision-Mamba-Models.</description><author>Rui Xu, Shu Yang, Yihui Wang, Bo Du, Hao Chen</author><pubDate>Mon, 29 Apr 2024 17:51:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18861v1</guid></item><item><title>A Comprehensive Rubric for Annotating Pathological Speech</title><link>http://arxiv.org/abs/2404.18851v1</link><description>Rubrics are a commonly used tool for labeling voice corpora in speech qualityassessment, although their application in the context of pathological speechremains relatively limited. In this study, we introduce a comprehensive rubricbased on various dimensions of speech quality, including phonetics, fluency,and prosody. The objective is to establish standardized criteria foridentifying errors within the speech of individuals with Down syndrome, therebyenabling the development of automated assessment systems. To achieve thisobjective, we utilized the Prautocal corpus. To assess the quality ofannotations using our rubric, two experiments were conducted, focusing onphonetics and fluency. For phonetic evaluation, we employed the Goodness ofPronunciation (GoP) metric, utilizing automatic segmentation systems andcorrelating the results with evaluations conducted by a specialized speechtherapist. While the obtained correlation values were not notably high, apositive trend was observed. In terms of fluency assessment, deep learningmodels like wav2vec were used to extract audio features, and we employed an SVMclassifier trained on a corpus focused on identifying fluency issues tocategorize Prautocal corpus samples. The outcomes highlight the complexities ofevaluating such phenomena, with variability depending on the specific type ofdisfluency detected.</description><author>Mario Corrales-Astorgano, David Escudero-Mancebo, Lourdes Aguilar, Valle Flores-Lucas, Valentín Cardeñoso-Payo, Carlos Vivaracho-Pascual, César González-Ferreras</author><pubDate>Mon, 29 Apr 2024 17:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18851v1</guid></item><item><title>MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection</title><link>http://arxiv.org/abs/2404.18849v1</link><description>In this paper, we present a different way to use two modalities, in whicheither one modality or the other is seen by a single model. This can be usefulwhen adapting an unimodal model to leverage more information while respecting alimited computational budget. This would mean having a single model that isable to deal with any modalities. To describe this, we coined the term anymodallearning. An example of this, is a use case where, surveillance in a room whenthe lights are off would be much more valuable using an infrared modality whilea visible one would provide more discriminative information when lights are on.This work investigates how to efficiently leverage visible and infrared/thermalmodalities for transformer-based object detection backbone to create ananymodal architecture. Our work does not create any inference overhead duringthe testing while exploring an effective way to exploit the two modalitiesduring the training. To accomplish such a task, we introduce the novel anymodaltraining technique: Mixed Patches (MiPa), in conjunction with a patch-wisedomain agnostic module, which is responsible of learning the best way to find acommon representation of both modalities. This approach proves to be able tobalance modalities by reaching competitive results on individual modalitybenchmarks with the alternative of using an unimodal architecture on threedifferent visible-infrared object detection datasets. Finally, our proposedmethod, when used as a regularization for the strongest modality, can beat theperformance of multimodal fusion methods while only requiring a single modalityduring inference. Notably, MiPa became the state-of-the-art on the LLVIPvisible/infrared benchmark. Code: https://github.com/heitorrapela/MiPa</description><author>Heitor R. Medeiros, David Latortue, Fidel Guerrero Pena, Eric Granger, Marco Pedersoli</author><pubDate>Mon, 29 Apr 2024 17:42:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18849v1</guid></item><item><title>FeDeRA:Efficient Fine-tuning of Language Models in Federated Learning Leveraging Weight Decomposition</title><link>http://arxiv.org/abs/2404.18848v1</link><description>Pre-trained Language Models (PLMs) have shown excellent performance onvarious downstream tasks after fine-tuning. Nevertheless, the escalatingconcerns surrounding user privacy have posed significant challenges tocentralized training reliant on extensive data collection. Federatedlearning(FL), which only requires training on the clients and aggregatesweights on the server without sharing data, has emerged as a solution. However,the substantial parameter size of PLMs places a significant burden on thecomputational resources of client devices, while also leading to costlycommunication expenses. Introducing Parameter-Efficient Fine-Tuning(PEFT) intoFL can effectively address this problem. However, we observe that the non-IIDdata in federated learning leads to a gap in performance between the PEFTmethod and full parameter fine-tuning(FT). To overcome this, we propose FeDeRA,an improvement over the LoRA method in FL. FeDeRA uses the same adapter moduleas LoRA. However, the difference lies in FeDeRA's initialization of the adaptermodule by performing Singular Value Decomposition (SVD) on the pre-trainedmatrix and selecting its principal components. We conducted extensiveexperiments, using RoBERTa and DeBERTaV3, on three tasks and six datasets,comparing the methods including FT and the other three different PEFT methods.FeDeRA outperforms all other PEFT methods and is comparable to or evensurpasses the performance of FT methods. We also deployed federated learning onJetson AGX Orin and compared the time required by different methods to achievethe target accuracy on specific tasks. Compared to FT, FeDeRA reduces thetraining time by 95.9%, 97.9%, 96.9%, and 97.3%, 96.5%, and 96.5% respectivelyon three tasks using RoBERTa and DeBERTaV3. The overall experiments indicatethat FeDeRA achieves good performance while also maintaining efficiency.</description><author>Yuxuan Yan, Shunpu Tang, Zhiguo Shi, Qianqian Yang</author><pubDate>Mon, 29 Apr 2024 17:42:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18848v1</guid></item><item><title>Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</title><link>http://arxiv.org/abs/2403.14608v5</link><description>Large models represent a groundbreaking advancement in multiple applicationfields, enabling remarkable achievements across various tasks. However, theirunprecedented scale comes with significant computational costs. These models,often consisting of billions of parameters, require vast amounts ofcomputational resources for execution. Especially, the expansive scale andcomputational demands pose considerable challenges when customizing them forparticular downstream tasks, particularly over the hardware platformsconstrained by computational capabilities. Parameter Efficient Fine-Tuning(PEFT) provides a practical solution by efficiently adapt the large models overthe various downstream tasks. In particular, PEFT refers to the process ofadjusting the parameters of a pre-trained large models to adapt it to aspecific task while minimizing the number of additional parameters introducedor computational resources required. This approach is particularly importantwhen dealing with large language models with high parameter counts, asfine-tuning these models from scratch can be computationally expensive andresource-intensive, posing considerable challenges in the supporting systemplatform design. In this survey, we present comprehensive studies of variousPEFT algorithms, examining their performance and computational overhead.Moreover, we provide an overview of applications developed using different PEFTalgorithms and discuss common techniques employed to mitigate computation costsfor PEFT. In addition to the algorithmic perspective, we overview variousreal-world system designs to investigate the implementation costs associatedwith different PEFT algorithms. This survey serves as an indispensable resourcefor researchers aiming to understand both the PEFT algorithm and its systemimplementation, offering detailed insights into recent advancements andpractical applications.</description><author>Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang</author><pubDate>Mon, 29 Apr 2024 17:42:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14608v5</guid></item><item><title>Intention Analysis Makes LLMs A Good Jailbreak Defender</title><link>http://arxiv.org/abs/2401.06561v3</link><description>Aligning large language models (LLMs) with human values, particularly in theface of complex and stealthy jailbreak attacks, presents a formidablechallenge. In this study, we present a simple yet highly effective defensestrategy, i.e., Intention Analysis ($\mathbb{IA}$). The principle behind thisis to trigger LLMs' inherent self-correct and improve ability through atwo-stage process: 1) essential intention analysis, and 2) policy-alignedresponse. Notably, $\mathbb{IA}$ is an inference-only method, thus couldenhance the safety of LLMs without compromising their helpfulness. Extensiveexperiments on varying jailbreak benchmarks across ChatGLM, LLaMA2, Vicuna,MPT, DeepSeek, and GPT-3.5 show that $\mathbb{IA}$ could consistently andsignificantly reduce the harmfulness in responses (averagely -53.1% attacksuccess rate) and maintain the general helpfulness. Encouragingly, with thehelp of our $\mathbb{IA}$, Vicuna-7B even outperforms GPT-3.5 in terms ofattack success rate. Further analyses present some insights into how our methodworks. To facilitate reproducibility, we release our code and scripts at:https://github.com/alphadl/SafeLLM_with_IntentionAnalysis.</description><author>Yuqi Zhang, Liang Ding, Lefei Zhang, Dacheng Tao</author><pubDate>Mon, 29 Apr 2024 17:40:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06561v3</guid></item><item><title>VISION: Toward a Standardized Process for Radiology Image Management at the National Level</title><link>http://arxiv.org/abs/2404.18842v1</link><description>The compilation and analysis of radiological images poses numerous challengesfor researchers. The sheer volume of data as well as the computational needs ofalgorithms capable of operating on images are extensive. Additionally, theassembly of these images alone is difficult, as these exams may differ widelyin terms of clinical context, structured annotation available for modeltraining, modality, and patient identifiers. In this paper, we describe ourexperiences and challenges in establishing a trusted collection of radiologyimages linked to the United States Department of Veterans Affairs (VA)electronic health record database. We also discuss implications in making thisrepository research-ready for medical investigators. Key insights includeuncovering the specific procedures required for transferring images from aclinical to a research-ready environment, as well as roadblocks and bottlenecksin this process that may hinder future efforts at automation.</description><author>Kathryn Knight, Ioana Danciu, Olga Ovchinnikova, Jacob Hinkle, Mayanka Chandra Shekar, Debangshu Mukherjee, Eileen McAllister, Caitlin Rizy, Kelly Cho, Amy C. Justice, Joseph Erdos, Peter Kuzmak, Lauren Costa, Yuk-Lam Ho, Reddy Madipadga, Suzanne Tamang, Ian Goethert</author><pubDate>Mon, 29 Apr 2024 17:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18842v1</guid></item><item><title>Fast Quantum Process Tomography via Riemannian Gradient Descent</title><link>http://arxiv.org/abs/2404.18840v1</link><description>Constrained optimization plays a crucial role in the fields of quantumphysics and quantum information science and becomes especially challenging forhigh-dimensional complex structure problems. One specific issue is that ofquantum process tomography, in which the goal is to retrieve the underlyingquantum process based on a given set of measurement data. In this paper, weintroduce a modified version of stochastic gradient descent on a Riemannianmanifold that integrates recent advancements in numerical methods forRiemannian optimization. This approach inherently supports the physicallydriven constraints of a quantum process, takes advantage of state-of-the-artlarge-scale stochastic objective optimization, and has superior performance totraditional approaches such as maximum likelihood estimation and projectedleast squares. The data-driven approach enables accurate, order-of-magnitudefaster results, and works with incomplete data. We demonstrate our approach onsimulations of quantum processes and in hardware by characterizing anengineered process on quantum computers.</description><author>Daniel Volya, Andrey Nikitin, Prabhat Mishra</author><pubDate>Mon, 29 Apr 2024 17:28:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18840v1</guid></item><item><title>SPGNN: Recognizing Salient Subgraph Patterns via Enhanced Graph Convolution and Pooling</title><link>http://arxiv.org/abs/2404.13655v2</link><description>Graph neural networks (GNNs) have revolutionized the field of machinelearning on non-Euclidean data such as graphs and networks. GNNs effectivelyimplement node representation learning through neighborhood aggregation andachieve impressive results in many graph-related tasks. However, mostneighborhood aggregation approaches are summation-based, which can beproblematic as they may not be sufficiently expressive to encode informativegraph structures. Furthermore, though the graph pooling module is also of vitalimportance for graph learning, especially for the task of graph classification,research on graph down-sampling mechanisms is rather limited. To address the above challenges, we propose a concatenation-based graphconvolution mechanism that injectively updates node representations to maximizethe discriminative power in distinguishing non-isomorphic subgraphs. Inaddition, we design a novel graph pooling module, called WL-SortPool, to learnimportant subgraph patterns in a deep-learning manner. WL-SortPool layer-wisesorts node representations (i.e. continuous WL colors) to separately learn therelative importance of subtrees with different depths for the purpose ofclassification, thus better characterizing the complex graph topology and richinformation encoded in the graph. We propose a novel Subgraph Pattern GNN(SPGNN) architecture that incorporates these enhancements. We test the proposedSPGNN architecture on many graph classification benchmarks. Experimentalresults show that our method can achieve highly competitive results withstate-of-the-art graph kernels and other GNN approaches.</description><author>Zehao Dong, Muhan Zhang, Yixin Chen</author><pubDate>Mon, 29 Apr 2024 17:21:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13655v2</guid></item><item><title>SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels</title><link>http://arxiv.org/abs/2309.08513v5</link><description>Pre-trained vision transformers have strong representation benefits tovarious downstream tasks. Recently, many parameter-efficient fine-tuning (PEFT)methods have been proposed, and their experiments demonstrate that tuning only1\% extra parameters could surpass full fine-tuning in low-data resourcescenarios. However, these methods overlook the task-specific information whenfine-tuning diverse downstream tasks. In this paper, we propose a simple yeteffective method called "Salient Channel Tuning" (SCT) to leverage thetask-specific information by forwarding the model with the task images toselect partial channels in a feature map that enables us to tune only 1/8channels leading to significantly lower parameter costs. Experiments on 19visual transfer learning downstream tasks demonstrate that our SCT outperformsfull fine-tuning on 18 out of 19 tasks by adding only 0.11M parameters of theViT-B, which is 780$\times$ fewer than its full fine-tuning counterpart.Furthermore, experiments on domain generalization and few-shot classificationfurther demonstrate the effectiveness and generic of our approach. The code isavailable at https://github.com/showlab/SCT.</description><author>Henry Hengyuan Zhao, Pichao Wang, Yuyang Zhao, Hao Luo, Fan Wang, Mike Zheng Shou</author><pubDate>Mon, 29 Apr 2024 17:20:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08513v5</guid></item><item><title>It's Difficult to be Neutral -- Human and LLM-based Sentiment Annotation of Patient Comments</title><link>http://arxiv.org/abs/2404.18832v1</link><description>Sentiment analysis is an important tool for aggregating patient voices, inorder to provide targeted improvements in healthcare services. A prerequisitefor this is the availability of in-domain data annotated for sentiment. Thisarticle documents an effort to add sentiment annotations to free-text commentsin patient surveys collected by the Norwegian Institute of Public Health(NIPH). However, annotation can be a time-consuming and resource-intensiveprocess, particularly when it requires domain expertise. We therefore alsoevaluate a possible alternative to human annotation, using large languagemodels (LLMs) as annotators. We perform an extensive evaluation of the approachfor two openly available pretrained LLMs for Norwegian, experimenting withdifferent configurations of prompts and in-context learning, comparing theirperformance to human annotators. We find that even for zero-shot runs, modelsperform well above the baseline for binary sentiment, but still cannot competewith human annotators on the full dataset.</description><author>Petter Mæhlum, David Samuel, Rebecka Maria Norman, Elma Jelin, Øyvind Andresen Bjertnæs, Lilja Øvrelid, Erik Velldal</author><pubDate>Mon, 29 Apr 2024 17:19:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18832v1</guid></item><item><title>ConPro: Learning Severity Representation for Medical Images using Contrastive Learning and Preference Optimization</title><link>http://arxiv.org/abs/2404.18831v1</link><description>Understanding the severity of conditions shown in images in medical diagnosisis crucial, serving as a key guide for clinical assessment, treatment, as wellas evaluating longitudinal progression. This paper proposes Con- PrO: a novelrepresentation learning method for severity assessment in medical images usingContrastive learningintegrated Preference Optimization. Different fromconventional contrastive learning methods that maximize the distance betweenclasses, ConPrO injects into the latent vector the distance preferenceknowledge between various severity classes and the normal class. Wesystematically examine the key components of our framework to illuminate howcontrastive prediction tasks acquire valuable representations. We show that ourrepresentation learning framework offers valuable severity ordering in thefeature space while outperforming previous state-of-the-art methods onclassification tasks. We achieve a 6% and 20% relative improvement compared toa supervised and a self-supervised baseline, respectively. In addition, wederived discussions on severity indicators and related applications ofpreference comparison in the medical domain.</description><author>Hong Nguyen, Hoang Nguyen, Melinda Chang, Hieu Pham, Shrikanth Narayanan, Michael Pazzani</author><pubDate>Mon, 29 Apr 2024 17:16:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18831v1</guid></item><item><title>Dealing with Structure Constraints in Evolutionary Pareto Set Learning</title><link>http://arxiv.org/abs/2310.20426v4</link><description>In the past few decades, many multiobjective evolutionary optimizationalgorithms (MOEAs) have been proposed to find a finite set of approximatePareto solutions for a given problem in a single run, each with its ownstructure. However, in many real-world applications, it could be desirable tohave structure constraints on the entire optimal solution set, which define thepatterns shared among all solutions. The current population-based MOEAs cannotproperly handle such requirements. In this work, we make the first attempt toincorporate the structure constraints into the whole solution set by a singlePareto set model, which can be efficiently learned by a simple evolutionarystochastic optimization method. With our proposed method, the decision-makerscan flexibly trade off the Pareto optimality with preferred structures amongall solutions, which is not supported by previous MOEAs. A set of experimentson benchmark test suites and real-world application problems fully demonstratesthe efficiency of our proposed method.</description><author>Xi Lin, Xiaoyuan Zhang, Zhiyuan Yang, Qingfu Zhang</author><pubDate>Mon, 29 Apr 2024 17:09:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.20426v4</guid></item><item><title>Harmonic Machine Learning Models are Robust</title><link>http://arxiv.org/abs/2404.18825v1</link><description>We introduce Harmonic Robustness, a powerful and intuitive method to test therobustness of any machine-learning model either during training or in black-boxreal-time inference monitoring without ground-truth labels. It is based onfunctional deviation from the harmonic mean value property, indicatinginstability and lack of explainability. We show implementation examples inlow-dimensional trees and feedforward NNs, where the method reliably identifiesoverfitting, as well as in more complex high-dimensional models such asResNet-50 and Vision Transformer where it efficiently measures adversarialvulnerability across image classes.</description><author>Nicholas S. Kersting, Yi Li, Aman Mohanty, Oyindamola Obisesan, Raphael Okochu</author><pubDate>Mon, 29 Apr 2024 17:07:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18825v1</guid></item><item><title>Benchmarking Benchmark Leakage in Large Language Models</title><link>http://arxiv.org/abs/2404.18824v1</link><description>Amid the expanding use of pre-training data, the phenomenon of benchmarkdataset leakage has become increasingly prominent, exacerbated by opaquetraining processes and the often undisclosed inclusion of supervised data incontemporary Large Language Models (LLMs). This issue skews benchmarkeffectiveness and fosters potentially unfair comparisons, impeding the field'shealthy development. To address this, we introduce a detection pipelineutilizing Perplexity and N-gram accuracy, two simple and scalable metrics thatgauge a model's prediction precision on benchmark, to identify potential dataleakages. By analyzing 31 LLMs under the context of mathematical reasoning, wereveal substantial instances of training even test set misuse, resulting inpotentially unfair comparisons. These findings prompt us to offer severalrecommendations regarding model documentation, benchmark setup, and futureevaluations. Notably, we propose the "Benchmark Transparency Card" to encourageclear documentation of benchmark utilization, promoting transparency andhealthy developments of LLMs. we have made our leaderboard, pipelineimplementation, and model predictions publicly available, fostering futureresearch.</description><author>Ruijie Xu, Zengzhi Wang, Run-Ze Fan, Pengfei Liu</author><pubDate>Mon, 29 Apr 2024 17:05:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18824v1</guid></item><item><title>Explaining Text Classifiers with Counterfactual Representations</title><link>http://arxiv.org/abs/2402.00711v2</link><description>One well motivated explanation method for classifiers leveragescounterfactuals which are hypothetical events identical to real observations inall aspects except for one categorical feature. Constructing suchcounterfactual poses specific challenges for texts, however, as some attributevalues may not necessarily align with plausible real-world events. In thispaper we propose a simple method for generating counterfactuals by interveningin the space of text representations which bypasses this limitation. We arguethat our interventions are minimally disruptive and that they are theoreticallysound as they align with counterfactuals as defined in Pearl's causal inferenceframework. To validate our method, we conducted experiments first on asynthetic dataset and then on a realistic dataset of counterfactuals. Thisallows for a direct comparison between classifier predictions based on groundtruth counterfactuals - obtained through explicit text interventions - and ourcounterfactuals, derived through interventions in the representation space.Eventually, we study a real world scenario where our counterfactuals can beleveraged both for explaining a classifier and for bias mitigation.</description><author>Pirmin Lemberger, Antoine Saillenfest</author><pubDate>Mon, 29 Apr 2024 17:04:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00711v2</guid></item><item><title>Control Policy Correction Framework for Reinforcement Learning-based Energy Arbitrage Strategies</title><link>http://arxiv.org/abs/2404.18821v1</link><description>A continuous rise in the penetration of renewable energy sources, along withthe use of the single imbalance pricing, provides a new opportunity for balanceresponsible parties to reduce their cost through energy arbitrage in theimbalance settlement mechanism. Model-free reinforcement learning (RL) methodsare an appropriate choice for solving the energy arbitrage problem due to theiroutstanding performance in solving complex stochastic sequential problems.However, RL is rarely deployed in real-world applications since its learnedpolicy does not necessarily guarantee safety during the execution phase. Inthis paper, we propose a new RL-based control framework for batteries to obtaina safe energy arbitrage strategy in the imbalance settlement mechanism. In ourproposed control framework, the agent initially aims to optimize the arbitragerevenue. Subsequently, in the post-processing step, we correct (constrain) thelearned policy following a knowledge distillation process based on propertiesthat follow human intuition. Our post-processing step is a generic method andis not restricted to the energy arbitrage domain. We use the Belgian imbalanceprice of 2023 to evaluate the performance of our proposed framework.Furthermore, we deploy our proposed control framework on a real battery to showits capability in the real world.</description><author>Seyed Soroush Karimi Madahi, Gargya Gokhale, Marie-Sophie Verwee, Bert Claessens, Chris Develder</author><pubDate>Mon, 29 Apr 2024 17:03:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18821v1</guid></item><item><title>Towards Extreme Image Compression with Latent Feature Guidance and Diffusion Prior</title><link>http://arxiv.org/abs/2404.18820v1</link><description>Compressing images at extremely low bitrates (below 0.1 bits per pixel (bpp))is a significant challenge due to substantial information loss. Existingextreme image compression methods generally suffer from heavy compressionartifacts or low-fidelity reconstructions. To address this problem, we proposea novel extreme image compression framework that combines compressive VAEs andpre-trained text-to-image diffusion models in an end-to-end manner.Specifically, we introduce a latent feature-guided compression module based oncompressive VAEs. This module compresses images and initially decodes thecompressed information into content variables. To enhance the alignment betweencontent variables and the diffusion space, we introduce external guidance tomodulate intermediate feature maps. Subsequently, we develop a conditionaldiffusion decoding module that leverages pre-trained diffusion models tofurther decode these content variables. To preserve the generative capabilityof pre-trained diffusion models, we keep their parameters fixed and use acontrol module to inject content information. We also design a space alignmentloss to provide sufficient constraints for the latent feature-guidedcompression module. Extensive experiments demonstrate that our methodoutperforms state-of-the-art approaches in terms of both visual performance andimage fidelity at extremely low bitrates.</description><author>Zhiyuan Li, Yanhui Zhou, Hao Wei, Chenyang Ge, Jingwen Jiang</author><pubDate>Mon, 29 Apr 2024 17:02:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18820v1</guid></item><item><title>A supplemental investigation of non-linearity in quantum generative models with respect to simulatability and optimization</title><link>http://arxiv.org/abs/2302.00788v2</link><description>Recent work has demonstrated the utility of introducing non-linearity throughrepeat-until-success (RUS) sub-routines into quantum circuits for generativemodeling. As a follow-up to this work, we investigate two questions ofrelevance to the quantum algorithms and machine learning communities: Doesintroducing this form of non-linearity make the learning model classicallysimulatable due to the deferred measurement principle? And does introducingthis form of non-linearity make the overall model's training more unstable?With respect to the first question, we demonstrate that the RUS sub-routines donot allow us to trivially map this quantum model to a classical one, whereas amodel without RUS sub-circuits containing mid-circuit measurements could bemapped to a classical Bayesian network due to the deferred measurementprinciple of quantum mechanics. This strongly suggests that the proposed formof non-linearity makes the model classically in-efficient to simulate. In thepursuit of the second question, we train larger models than previously shown onthree different probability distributions, one continuous and two discrete, andcompare the training performance across multiple random trials. We see thatwhile the model is able to perform exceptionally well in some trials, thevariance across trials with certain datasets quantifies its relatively poortraining stability.</description><author>Kaitlin Gili, Rohan S. Kumar, Mykolas Sveistrys, C. J. Ballance</author><pubDate>Mon, 29 Apr 2024 16:55:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00788v2</guid></item><item><title>Algorithms for automatic intents extraction and utterances classification for goal-oriented dialogue systems</title><link>http://arxiv.org/abs/2312.09658v2</link><description>Modern machine learning techniques in the natural language processing domaincan be used to automatically generate scripts for goal-oriented dialoguesystems. The current article presents a general framework for studying theautomatic generation of scripts for goal-oriented dialogue systems. A methodfor preprocessing dialog data sets in JSON format is described. A comparison ismade of two methods for extracting user intent based on BERTopic and latentDirichlet allocation. A comparison has been made of two implemented algorithmsfor classifying statements of users of a goal-oriented dialogue system based onlogistic regression and BERT transformer models. The BERT transformer approachusing the bert-base-uncased model showed better results for the three metricsPrecision (0.80), F1-score (0.78) and Matthews correlation coefficient (0.74)in comparison with other methods.</description><author>Leonid Legashev, Alexander Shukhman, Vadim Badikov</author><pubDate>Mon, 29 Apr 2024 16:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09658v2</guid></item><item><title>Learning Quantum Processes with Quantum Statistical Queries</title><link>http://arxiv.org/abs/2310.02075v3</link><description>Learning complex quantum processes is a central challenge in many areas ofquantum computing and quantum machine learning, with applications in quantumbenchmarking, cryptanalysis, and variational quantum algorithms. This paperintroduces the first learning framework for studying quantum process learningwithin the Quantum Statistical Query (QSQ) model, providing the first formaldefinition of statistical queries to quantum processes (QPSQs). The frameworkallows us to propose an efficient QPSQ learner for arbitrary quantum processesaccompanied by a provable performance guarantee. We also provide numericalsimulations to demonstrate the efficacy of this algorithm. In our newframework, we prove exponential query complexity lower bounds for learningunitary 2-designs, and a doubly exponential lower bound for learninghaar-random unitaries. The practical relevance of this framework is exemplifiedthrough application in cryptography, highlighting vulnerabilities of a largeclass of Classical-Readout Quantum Physical Unclonable Functions (CR-QPUFs),addressing an important open question in the field of quantum hardwaresecurity. This work marks a significant step towards understanding thelearnability of quantum processes and shedding light on their securityimplications.</description><author>Chirag Wadhwa, Mina Doosti</author><pubDate>Mon, 29 Apr 2024 16:51:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02075v3</guid></item><item><title>Safe Reach Set Computation via Neural Barrier Certificates</title><link>http://arxiv.org/abs/2404.18813v1</link><description>We present a novel technique for online safety verification of autonomoussystems, which performs reachability analysis efficiently for both bounded andunbounded horizons by employing neural barrier certificates. Our approach usesbarrier certificates given by parameterized neural networks that depend on agiven initial set, unsafe sets, and time horizon. Such networks are trainedefficiently offline using system simulations sampled from regions of the statespace. We then employ a meta-neural network to generalize the barriercertificates to state space regions that are outside the training set. Thesecertificates are generated and validated online as sound over-approximations ofthe reachable states, thus either ensuring system safety or activatingappropriate alternative actions in unsafe scenarios. We demonstrate ourtechnique on case studies from linear models to nonlinear control-dependentmodels for online autonomous driving scenarios.</description><author>Alessandro Abate, Sergiy Bogomolov, Alec Edwards, Kostiantyn Potomkin, Sadegh Soudjani, Paolo Zuliani</author><pubDate>Mon, 29 Apr 2024 16:49:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18813v1</guid></item><item><title>Unknown Script: Impact of Script on Cross-Lingual Transfer</title><link>http://arxiv.org/abs/2404.18810v1</link><description>Cross-lingual transfer has become an effective way of transferring knowledgebetween languages. In this paper, we explore an often-overlooked aspect in thisdomain: the influence of the source language of the base language model ontransfer performance. We conduct a series of experiments to determine theeffect of the script and tokenizer used in the pre-trained model on theperformance of the downstream task. Our findings reveal the importance of thetokenizer as a stronger factor than the sharing of the script, the languagetypology match, and the model size.</description><author>Wondimagegnhue Tsegaye Tufa, Ilia Markov, Piek Vossen</author><pubDate>Mon, 29 Apr 2024 16:48:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18810v1</guid></item><item><title>The Landscape of Unfolding with Machine Learning</title><link>http://arxiv.org/abs/2404.18807v1</link><description>Recent innovations from machine learning allow for data unfolding, withoutbinning and including correlations across many dimensions. We describe a set ofknown, upgraded, and new methods for ML-based unfolding. The performance ofthese approaches are evaluated on the same two datasets. We find that alltechniques are capable of accurately reproducing the particle-level spectraacross complex observables. Given that these approaches are conceptuallydiverse, they offer an exciting toolkit for a new class of measurements thatcan probe the Standard Model with an unprecedented level of detail and mayenable sensitivity to new phenomena.</description><author>Nathan Huetsch, Javier Mariño Villadamigo, Alexander Shmakov, Sascha Diefenbacher, Vinicius Mikuni, Theo Heimel, Michael Fenton, Kevin Greif, Benjamin Nachman, Daniel Whiteson, Anja Butter, Tilman Plehn</author><pubDate>Mon, 29 Apr 2024 16:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18807v1</guid></item><item><title>A Partial Replication of MaskFormer in TensorFlow on TPUs for the TensorFlow Model Garden</title><link>http://arxiv.org/abs/2404.18801v1</link><description>This paper undertakes the task of replicating the MaskFormer model auniversal image segmentation model originally developed using the PyTorchframework, within the TensorFlow ecosystem, specifically optimized forexecution on Tensor Processing Units (TPUs). Our implementation exploits themodular constructs available within the TensorFlow Model Garden (TFMG),encompassing elements such as the data loader, training orchestrator, andvarious architectural components, tailored and adapted to meet thespecifications of the MaskFormer model. We address key challenges encounteredduring the replication, non-convergence issues, slow training, adaptation ofloss functions, and the integration of TPU-specific functionalities. We verifyour reproduced implementation and present qualitative results on the COCOdataset. Although our implementation meets some of the objectives forend-to-end reproducibility, we encountered challenges in replicating thePyTorch version of MaskFormer in TensorFlow. This replication process is notstraightforward and requires substantial engineering efforts. Specifically, itnecessitates the customization of various components within the TFMG, alongsidethorough verification and hyper-parameter tuning. The replication is availableat:https://github.com/PurdueDualityLab/tf-maskformer/tree/main/official/projects/maskformer</description><author>Vishal Purohit, Wenxin Jiang, Akshath R. Ravikiran, James C. Davis</author><pubDate>Mon, 29 Apr 2024 16:40:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18801v1</guid></item><item><title>Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation</title><link>http://arxiv.org/abs/2211.01939v3</link><description>We study the problem of model selection in causal inference, specifically forconditional average treatment effect (CATE) estimation. Unlike machinelearning, there is no perfect analogue of cross-validation for model selectionas we do not observe the counterfactual potential outcomes. Towards this, avariety of surrogate metrics have been proposed for CATE model selection thatuse only observed data. However, we do not have a good understanding regardingtheir effectiveness due to limited comparisons in prior studies. We conduct anextensive empirical analysis to benchmark the surrogate model selection metricsintroduced in the literature, as well as the novel ones introduced in thiswork. We ensure a fair comparison by tuning the hyperparameters associated withthese metrics via AutoML, and provide more detailed trends by incorporatingrealistic datasets via generative modeling. Our analysis suggests novel modelselection strategies based on careful hyperparameter selection of CATEestimators and causal ensembling.</description><author>Divyat Mahajan, Ioannis Mitliagkas, Brady Neal, Vasilis Syrgkanis</author><pubDate>Mon, 29 Apr 2024 16:34:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01939v3</guid></item><item><title>Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models</title><link>http://arxiv.org/abs/2404.18796v1</link><description>As Large Language Models (LLMs) have become more advanced, they have outpacedour abilities to accurately evaluate their quality. Not only is finding data toadequately probe particular model properties difficult, but evaluating thecorrectness of a model's freeform generation alone is a challenge. To addressthis, many evaluations now rely on using LLMs themselves as judges to score thequality of outputs from other LLMs. Evaluations most commonly use a singlelarge model like GPT4. While this method has grown in popularity, it is costly,has been shown to introduce intramodel bias, and in this work, we find thatvery large models are often unnecessary. We propose instead to evaluate modelsusing a Panel of LLm evaluators (PoLL). Across three distinct judge settingsand spanning six different datasets, we find that using a PoLL composed of alarger number of smaller models outperforms a single large judge, exhibits lessintra-model bias due to its composition of disjoint model families, and does sowhile being over seven times less expensive.</description><author>Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, Patrick Lewis</author><pubDate>Mon, 29 Apr 2024 16:33:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18796v1</guid></item><item><title>Robust Bayesian Inference for Berkson and Classical Measurement Error Models</title><link>http://arxiv.org/abs/2306.01468v2</link><description>Measurement error occurs when a covariate influencing a response variable iscorrupted by noise. This can lead to misleading inference outcomes,particularly in problems where accurately estimating the relationship betweencovariates and response variables is crucial, such as causal effect estimation.Existing methods for dealing with measurement error often rely on strongassumptions such as knowledge of the error distribution or its variance andavailability of replicated measurements of the covariates. We propose aBayesian Nonparametric Learning framework that is robust to mismeasuredcovariates, does not require the preceding assumptions, and can incorporateprior beliefs about the error distribution. This approach gives rise to ageneral framework that is suitable for both Classical and Berkson error modelsvia the appropriate specification of the prior centering measure of a DirichletProcess (DP). Moreover, it offers flexibility in the choice of loss functiondepending on the type of regression model. We provide bounds on thegeneralization error based on the Maximum Mean Discrepancy (MMD) loss whichallows for generalization to non-Gaussian distributed errors and nonlinearcovariate-response relationships. We showcase the effectiveness of the proposedframework versus prior art in real-world problems containing either Berkson orClassical measurement errors.</description><author>Charita Dellaporta, Theodoros Damoulas</author><pubDate>Mon, 29 Apr 2024 16:31:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01468v2</guid></item><item><title>Size and depth of monotone neural networks: interpolation and approximation</title><link>http://arxiv.org/abs/2207.05275v2</link><description>We study monotone neural networks with threshold gates where all the weights(other than the biases) are non-negative. We focus on the expressive power andefficiency of representation of such networks. Our first result establishesthat every monotone function over $[0,1]^d$ can be approximated withinarbitrarily small additive error by a depth-4 monotone network. When $d &gt; 3$,we improve upon the previous best-known construction which has depth $d+1$. Ourproof goes by solving the monotone interpolation problem for monotone datasetsusing a depth-4 monotone threshold network. In our second main result wecompare size bounds between monotone and arbitrary neural networks withthreshold gates. We find that there are monotone real functions that can becomputed efficiently by networks with no restriction on the gates whereasmonotone networks approximating these functions need exponential size in thedimension.</description><author>Dan Mikulincer, Daniel Reichman</author><pubDate>Mon, 29 Apr 2024 16:24:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.05275v2</guid></item><item><title>Certification of Speaker Recognition Models to Additive Perturbations</title><link>http://arxiv.org/abs/2404.18791v1</link><description>Speaker recognition technology is applied in various tasks ranging frompersonal virtual assistants to secure access systems. However, the robustnessof these systems against adversarial attacks, particularly to additiveperturbations, remains a significant challenge. In this paper, we pioneerapplying robustness certification techniques to speaker recognition, originallydeveloped for the image domain. In our work, we cover this gap by transferringand improving randomized smoothing certification techniques againstnorm-bounded additive perturbations for classification and few-shot learningtasks to speaker recognition. We demonstrate the effectiveness of these methodson VoxCeleb 1 and 2 datasets for several models. We expect this work to improvevoice-biometry robustness, establish a new certification benchmark, andaccelerate research of certification methods in the audio domain.</description><author>Dmitrii Korzh, Elvir Karimov, Mikhail Pautov, Oleg Y. Rogov, Ivan Oseledets</author><pubDate>Mon, 29 Apr 2024 16:23:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18791v1</guid></item><item><title>Gradient-based Local Next-best-view Planning for Improved Perception of Targeted Plant Nodes</title><link>http://arxiv.org/abs/2311.16759v2</link><description>Robots are increasingly used in tomato greenhouses to automatelabour-intensive tasks such as selective harvesting and de-leafing. To performthese tasks, robots must be able to accurately and efficiently perceive theplant nodes that need to be cut, despite the high levels of occlusion fromother plant parts. We formulate this problem as a local next-best-view (NBV)planning task where the robot has to plan an efficient set of camera viewpointsto overcome occlusion and improve the quality of perception. Our formulationfocuses on quickly improving the perception accuracy of a single target node tomaximise its chances of being cut. Previous methods of NBV planning mostlyfocused on global view planning and used random sampling of candidateviewpoints for exploration, which could suffer from high computational costs,ineffective view selection due to poor candidates, or non-smooth trajectoriesdue to inefficient sampling. We propose a gradient-based NBV planner usingdifferential ray sampling, which directly estimates the local gradientdirection for viewpoint planning to overcome occlusion and improve perception.Through simulation experiments, we showed that our planner can handleocclusions and improve the 3D reconstruction and position estimation of nodesequally well as a sampling-based NBV planner, while taking ten times lesscomputation and generating 28% more efficient trajectories.</description><author>Akshay K. Burusa, Eldert J. van Henten, Gert Kootstra</author><pubDate>Mon, 29 Apr 2024 16:21:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16759v2</guid></item><item><title>MMBench: Is Your Multi-modal Model an All-around Player?</title><link>http://arxiv.org/abs/2307.06281v4</link><description>Large vision-language models have recently achieved remarkable progress,exhibiting great perception and reasoning abilities concerning visualinformation. However, how to effectively evaluate these large vision-languagemodels remains a major obstacle, hindering future model development.Traditional benchmarks like VQAv2 or COCO Caption provide quantitativeperformance measurements but suffer from a lack of fine-grained abilityassessment and non-robust evaluation metrics. Recent subjective benchmarks,such as OwlEval, offer comprehensive evaluations of a model's abilities byincorporating human labor, but they are not scalable and display significantbias. In response to these challenges, we propose MMBench, a novelmulti-modality benchmark. MMBench methodically develops a comprehensiveevaluation pipeline, primarily comprised of two elements. The first element isa meticulously curated dataset that surpasses existing similar benchmarks interms of the number and variety of evaluation questions and abilities. Thesecond element introduces a novel CircularEval strategy and incorporates theuse of ChatGPT. This implementation is designed to convert free-formpredictions into pre-defined choices, thereby facilitating a more robustevaluation of the model's predictions. MMBench is a systematically-designedobjective benchmark for robustly evaluating the various abilities ofvision-language models. We hope MMBench will assist the research community inbetter evaluating their models and encourage future advancements in thisdomain. Project page: https://opencompass.org.cn/mmbench.</description><author>Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, Dahua Lin</author><pubDate>Mon, 29 Apr 2024 16:21:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06281v4</guid></item><item><title>The Brain Tumor Segmentation in Pediatrics (BraTS-PEDs) Challenge: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs)</title><link>http://arxiv.org/abs/2404.15009v3</link><description>Pediatric tumors of the central nervous system are the most common cause ofcancer-related death in children. The five-year survival rate for high-gradegliomas in children is less than 20%. Due to their rarity, the diagnosis ofthese entities is often delayed, their treatment is mainly based on historictreatment concepts, and clinical trials require multi-institutionalcollaborations. Here we present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDschallenge, focused on pediatric brain tumors with data acquired across multipleinternational consortia dedicated to pediatric neuro-oncology and clinicaltrials. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs challenge brings togetherclinicians and AI/imaging scientists to lead to faster development of automatedsegmentation techniques that could benefit clinical trials, and ultimately thecare of children with brain tumors.</description><author>Anahita Fathi Kazerooni, Nastaran Khalili, Deep Gandhi, Xinyang Liu, Zhifan Jiang, Syed Muhammed Anwar, Jake Albrecht, Maruf Adewole, Udunna Anazodo, Hannah Anderson, Sina Bagheri, Ujjwal Baid, Timothy Bergquist, Austin J. Borja, Evan Calabrese, Verena Chung, Gian-Marco Conte, Farouk Dako, James Eddy, Ivan Ezhov, Ariana Familiar, Keyvan Farahani, Anurag Gottipati, Debanjan Haldar, Shuvanjan Haldar, Juan Eugenio Iglesias, Anastasia Janas, Elaine Johansen, Blaise V Jones, Neda Khalili, Florian Kofler, Dominic LaBella, Hollie Anne Lai, Koen Van Leemput, Hongwei Bran Li, Nazanin Maleki, Aaron S McAllister, Zeke Meier, Bjoern Menze, Ahmed W Moawad, Khanak K Nandolia, Julija Pavaine, Marie Piraud, Tina Poussaint, Sanjay P Prabhu, Zachary Reitman, Andres Rodriguez, Jeffrey D Rudie, Mariana Sanche</author><pubDate>Mon, 29 Apr 2024 16:19:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15009v3</guid></item><item><title>Where on Earth Do Users Say They Are?: Geo-Entity Linking for Noisy Multilingual User Input</title><link>http://arxiv.org/abs/2404.18784v1</link><description>Geo-entity linking is the task of linking a location mention to thereal-world geographic location. In this paper we explore the challenging taskof geo-entity linking for noisy, multilingual social media data. There are fewopen-source multilingual geo-entity linking tools available and existing onesare often rule-based, which break easily in social media settings, orLLM-based, which are too expensive for large-scale datasets. We present amethod which represents real-world locations as averaged embeddings fromlabeled user-input location names and allows for selective prediction via aninterpretable confidence score. We show that our approach improves geo-entitylinking on a global and multilingual social media dataset, and discuss progressand problems with evaluating at different geographic granularities.</description><author>Tessa Masis, Brendan O'Connor</author><pubDate>Mon, 29 Apr 2024 16:18:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18784v1</guid></item><item><title>Optimal time sampling in physics-informed neural networks</title><link>http://arxiv.org/abs/2404.18780v1</link><description>Physics-informed neural networks (PINN) is a extremely powerful paradigm usedto solve equations encountered in scientific computing applications. Animportant part of the procedure is the minimization of the equation residualwhich includes, when the equation is time-dependent, a time sampling. It wasargued in the literature that the sampling need not be uniform but shouldoverweight initial time instants, but no rigorous explanation was provided forthese choice. In this paper we take some prototypical examples and, understandard hypothesis concerning the neural network convergence, we show that theoptimal time sampling follows a truncated exponential distribution. Inparticular we explain when the time sampling is best to be uniform and when itshould not be. The findings are illustrated with numerical examples on linearequation, Burgers' equation and the Lorenz system.</description><author>Gabriel Turinici</author><pubDate>Mon, 29 Apr 2024 16:16:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18780v1</guid></item><item><title>Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences</title><link>http://arxiv.org/abs/2403.19871v3</link><description>Retraining machine learning models (ML) when new batches of data becomeavailable is an important task in real-world pipelines. Existing methods focuslargely on greedy approaches to find the best-performing model for each batch,without considering the stability of the model's structure across retrainingiterations. In this study, we propose a methodology for finding sequences of MLmodels that are stable across retraining iterations. We develop a mixed-integeroptimization algorithm that is guaranteed to recover Pareto optimal models (interms of the predictive power-stability trade-off) and an efficientpolynomial-time algorithm that performs well in practice. Our method focuses onretaining consistent analytical insights -- which is important to modelinterpretability, ease of implementation, and fostering trust with users -- byusing custom-defined distance metrics that can be directly incorporated intothe optimization problem. Importantly, our method shows stronger stability thangreedily trained models with a small, controllable sacrifice in modelperformance in a real-world case study. Using SHAP feature importance, we showthat analytical insights are consistent across retraining iterations.</description><author>Dimitris Bertsimas, Vassilis Digalakis Jr, Yu Ma, Phevos Paschalidis</author><pubDate>Mon, 29 Apr 2024 16:12:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19871v3</guid></item><item><title>Self-training superconducting neuromorphic circuits using reinforcement learning rules</title><link>http://arxiv.org/abs/2404.18774v1</link><description>Reinforcement learning algorithms are used in a wide range of applications,from gaming and robotics to autonomous vehicles. In this paper we describe aset of reinforcement learning-based local weight update rules and theirimplementation in superconducting hardware. Using SPICE circuit simulations, weimplement a small-scale neural network with a learning time of order onenanosecond. This network can be trained to learn new functions simply bychanging the target output for a given set of inputs, without the need for anyexternal adjustments to the network. In this implementation the weights areadjusted based on the current state of the overall network response and locallystored information about the previous action. This removes the need to programexplicit weight values in these networks, which is one of the primarychallenges that analog hardware implementations of neural networks face. Theadjustment of weights is based on a global reinforcement signal that obviatesthe need for circuitry to back-propagate errors.</description><author>M. L. Schneider, E. M. Jué, M. R. Pufall, K. Segall, C. W. Anderson</author><pubDate>Mon, 29 Apr 2024 16:09:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18774v1</guid></item><item><title>A Universal Metric of Dataset Similarity for Cross-silo Federated Learning</title><link>http://arxiv.org/abs/2404.18773v1</link><description>Federated Learning is increasingly used in domains such as healthcare tofacilitate collaborative model training without data-sharing. However, datasetslocated in different sites are often non-identically distributed, leading todegradation of model performance in FL. Most existing methods for assessingthese distribution shifts are limited by being dataset or task-specific.Moreover, these metrics can only be calculated by exchanging data, a practicerestricted in many FL scenarios. To address these challenges, we propose anovel metric for assessing dataset similarity. Our metric exhibits severaldesirable properties for FL: it is dataset-agnostic, is calculated in aprivacy-preserving manner, and is computationally efficient, requiring no modeltraining. In this paper, we first establish a theoretical connection betweenour metric and training dynamics in FL. Next, we extensively evaluate ourmetric on a range of datasets including synthetic, benchmark, and medicalimaging datasets. We demonstrate that our metric shows a robust andinterpretable relationship with model performance and can be calculated inprivacy-preserving manner. As the first federated dataset similarity metric, webelieve this metric can better facilitate successful collaborations betweensites.</description><author>Ahmed Elhussein, Gamze Gursoy</author><pubDate>Mon, 29 Apr 2024 16:08:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18773v1</guid></item><item><title>ZeroSwap: Data-driven Optimal Market Making in DeFi</title><link>http://arxiv.org/abs/2310.09413v3</link><description>Automated Market Makers (AMMs) are major centers of matching liquidity supplyand demand in Decentralized Finance. Their functioning relies primarily on thepresence of liquidity providers (LPs) incentivized to invest their assets intoa liquidity pool. However, the prices at which a pooled asset is traded isoften more stale than the prices on centralized and more liquid exchanges. Thisleads to the LPs suffering losses to arbitrage. This problem is addressed byadapting market prices to trader behavior, captured via the classical marketmicrostructure model of Glosten and Milgrom. In this paper, we propose thefirst optimal Bayesian and the first model-free data-driven algorithm tooptimally track the external price of the asset. The notion of optimality thatwe use enforces a zero-profit condition on the prices of the market maker,hence the name ZeroSwap. This ensures that the market maker balances losses toinformed traders with profits from noise traders. The key property of ourapproach is the ability to estimate the external market price without the needfor price oracles or loss oracles. Our theoretical guarantees on theperformance of both these algorithms, ensuring the stability and convergence oftheir price recommendations, are of independent interest in the theory ofreinforcement learning. We empirically demonstrate the robustness of ouralgorithms to changing market conditions.</description><author>Viraj Nadkarni, Jiachen Hu, Ranvir Rana, Chi Jin, Sanjeev Kulkarni, Pramod Viswanath</author><pubDate>Mon, 29 Apr 2024 16:08:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09413v3</guid></item><item><title>Saliency Suppressed, Semantics Surfaced: Visual Transformations in Neural Networks and the Brain</title><link>http://arxiv.org/abs/2404.18772v1</link><description>Deep learning algorithms lack human-interpretable accounts of how theytransform raw visual input into a robust semantic understanding, which impedescomparisons between different architectures, training objectives, and the humanbrain. In this work, we take inspiration from neuroscience and employrepresentational approaches to shed light on how neural networks encodeinformation at low (visual saliency) and high (semantic similarity) levels ofabstraction. Moreover, we introduce a custom image dataset where wesystematically manipulate salient and semantic information. We find thatResNets are more sensitive to saliency information than ViTs, when trained withobject classification objectives. We uncover that networks suppress saliency inearly layers, a process enhanced by natural language supervision (CLIP) inResNets. CLIP also enhances semantic encoding in both architectures. Finally,we show that semantic encoding is a key factor in aligning AI with human visualperception, while saliency suppression is a non-brain-like strategy.</description><author>Gustaw Opiełka, Jessica Loke, Steven Scholte</author><pubDate>Mon, 29 Apr 2024 16:05:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18772v1</guid></item><item><title>Learning with Norm Constrained, Over-parameterized, Two-layer Neural Networks</title><link>http://arxiv.org/abs/2404.18769v1</link><description>Recent studies show that a reproducing kernel Hilbert space (RKHS) is not asuitable space to model functions by neural networks as the curse ofdimensionality (CoD) cannot be evaded when trying to approximate even a singleReLU neuron (Bach, 2017). In this paper, we study a suitable function space forover-parameterized two-layer neural networks with bounded norms (e.g., the pathnorm, the Barron norm) in the perspective of sample complexity andgeneralization properties. First, we show that the path norm (as well as theBarron norm) is able to obtain width-independence sample complexity bounds,which allows for uniform convergence guarantees. Based on this result, wederive the improved result of metric entropy for $\epsilon$-covering up to$\mathcal{O}(\epsilon^{-\frac{2d}{d+2}})$ ($d$ is the input dimension and thedepending constant is at most polynomial order of $d$) via the convex hulltechnique, which demonstrates the separation with kernel methods with$\Omega(\epsilon^{-d})$ to learn the target function in a Barron space. Second,this metric entropy result allows for building a sharper generalization boundunder a general moment hypothesis setting, achieving the rate at$\mathcal{O}(n^{-\frac{d+2}{2d+2}})$. Our analysis is novel in that it offers asharper and refined estimation for metric entropy (with a clear dependencerelationship on the dimension $d$) and unbounded sampling in the estimation ofthe sample error and the output error.</description><author>Fanghui Liu, Leello Dadi, Volkan Cevher</author><pubDate>Mon, 29 Apr 2024 16:04:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18769v1</guid></item><item><title>LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding</title><link>http://arxiv.org/abs/2404.16710v2</link><description>We present LayerSkip, an end-to-end solution to speed-up inference of largelanguage models (LLMs). First, during training we apply layer dropout, with lowdropout rates for earlier layers and higher dropout rates for later layers, andan early exit loss where all transformer layers share the same exit. Second,during inference, we show that this training recipe increases the accuracy ofearly exit at earlier layers, without adding any auxiliary layers or modules tothe model. Third, we present a novel self-speculative decoding solution wherewe exit at early layers and verify and correct with remaining layers of themodel. Our proposed self-speculative decoding approach has less memoryfootprint than other speculative decoding approaches and benefits from sharedcompute and activations of the draft and verification stages. We runexperiments on different Llama model sizes on different types of training:pretraining from scratch, continual pretraining, finetuning on specific datadomain, and finetuning on specific task. We implement our inference solutionand show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82xon coding, and 2.0x on TOPv2 semantic parsing task.</description><author>Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, Carole-Jean Wu</author><pubDate>Mon, 29 Apr 2024 16:02:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16710v2</guid></item><item><title>PECC: Problem Extraction and Coding Challenges</title><link>http://arxiv.org/abs/2404.18766v1</link><description>Recent advancements in large language models (LLMs) have showcased theirexceptional abilities across various tasks, such as code generation,problem-solving and reasoning. Existing benchmarks evaluate tasks in isolation,yet the extent to which LLMs can understand prose-style tasks, identify theunderlying problems, and then generate appropriate code solutions is stillunexplored. Addressing this gap, we introduce PECC, a novel benchmark derivedfrom Advent Of Code (AoC) challenges and Project Euler, including 2396problems. Unlike conventional benchmarks, PECC requires LLMs to interpretnarrative-embedded problems, extract requirements, and generate executablecode. A key feature of our dataset is the complexity added by natural languageprompting in chat-based evaluations, mirroring real-world instructionambiguities. Results show varying model performance between narrative andneutral problems, with specific challenges in the Euler math-based subset withGPT-3.5-Turbo passing 50% of the AoC challenges and only 8% on the Eulerproblems. By probing the limits of LLMs' capabilities, our benchmark provides aframework to monitor and assess the subsequent progress of LLMs as a universalproblem solver.</description><author>Patrick Haller, Jonas Golde, Alan Akbik</author><pubDate>Mon, 29 Apr 2024 16:02:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18766v1</guid></item><item><title>From Density to Geometry: YOLOv8 Instance Segmentation for Reverse Engineering of Optimized Structures</title><link>http://arxiv.org/abs/2404.18763v1</link><description>This paper introduces YOLOv8-TO, a novel approach for reverse engineering oftopology-optimized structures into interpretable geometric parameters using theYOLOv8 instance segmentation model. Density-based topology optimization methodsrequire post-processing to convert the optimal density distribution into aparametric representation for design exploration and integration with CADtools. Traditional methods such as skeletonization struggle with complexgeometries and require manual intervention. YOLOv8-TO addresses thesechallenges by training a custom YOLOv8 model to automatically detect andreconstruct structural components from binary density distributions. The modelis trained on a diverse dataset of both optimized and random structuresgenerated using the Moving Morphable Components method. A custom reconstructionloss function based on the dice coefficient of the predicted geometry is usedto train the new regression head of the model via self-supervised learning. Themethod is evaluated on test sets generated from different topology optimizationmethods, including out-of-distribution samples, and compared against askeletonization approach. Results show that YOLOv8-TO significantly outperformsskeletonization in reconstructing visually and structurally similar designs.The method showcases an average improvement of 13.84% in the Dice coefficient,with peak enhancements reaching 20.78%. The method demonstrates goodgeneralization to complex geometries and fast inference times, making itsuitable for integration into design workflows using regular workstations.Limitations include the sensitivity to non-max suppression thresholds.YOLOv8-TO represents a significant advancement in topology optimizationpost-processing, enabling efficient and accurate reverse engineering ofoptimized structures for design exploration and manufacturing.</description><author>Thomas Rochefort-Beaudoin, Aurelian Vadean, Sofiane Achiche, Niels Aage</author><pubDate>Mon, 29 Apr 2024 16:01:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18763v1</guid></item><item><title>Understanding the (Extra-)Ordinary: Validating Deep Model Decisions with Prototypical Concept-based Explanations</title><link>http://arxiv.org/abs/2311.16681v2</link><description>Ensuring both transparency and safety is critical when deploying Deep NeuralNetworks (DNNs) in high-risk applications, such as medicine. The field ofexplainable AI (XAI) has proposed various methods to comprehend thedecision-making processes of opaque DNNs. However, only few XAI methods aresuitable of ensuring safety in practice as they heavily rely on repeatedlabor-intensive and possibly biased human assessment. In this work, we presenta novel post-hoc concept-based XAI framework that conveys besides instance-wise(local) also class-wise (global) decision-making strategies via prototypes.What sets our approach apart is the combination of local and global strategies,enabling a clearer understanding of the (dis-)similarities in model decisionscompared to the expected (prototypical) concept use, ultimately reducing thedependence on human long-term assessment. Quantifying the deviation fromprototypical behavior not only allows to associate predictions with specificmodel sub-strategies but also to detect outlier behavior. As such, our approachconstitutes an intuitive and explainable tool for model validation. Wedemonstrate the effectiveness of our approach in identifyingout-of-distribution samples, spurious model behavior and data quality issuesacross three datasets (ImageNet, CUB-200, and CIFAR-10) utilizing VGG, ResNet,and EfficientNet architectures. Code is available onhttps://github.com/maxdreyer/pcx.</description><author>Maximilian Dreyer, Reduan Achtibat, Wojciech Samek, Sebastian Lapuschkin</author><pubDate>Mon, 29 Apr 2024 15:58:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16681v2</guid></item><item><title>Flow AM: Generating Point Cloud Global Explanations by Latent Alignment</title><link>http://arxiv.org/abs/2404.18760v1</link><description>Although point cloud models have gained significant improvements inprediction accuracy over recent years, their trustworthiness is still notsufficiently investigated. In terms of global explainability, ActivationMaximization (AM) techniques in the image domain are not directlytransplantable due to the special structure of the point cloud models. Existingstudies exploit generative models to yield global explanations that can beperceived by humans. However, the opacity of the generative models themselvesand the introduction of additional priors call into question the plausibilityand fidelity of the explanations. In this work, we demonstrate that when theclassifier predicts different types of instances, the intermediate layeractivations are differently activated, known as activation flows. Based on thisproperty, we propose an activation flow-based AM method that generates globalexplanations that can be perceived without incorporating any generative model.Furthermore, we reveal that AM based on generative models fails the sanitychecks and thus lack of fidelity. Extensive experiments show that our approachdramatically enhances the perceptibility of explanations compared to other AMmethods that are not based on generative models. Our code is available at:https://github.com/Explain3D/FlowAM</description><author>Hanxiao Tan</author><pubDate>Mon, 29 Apr 2024 15:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18760v1</guid></item><item><title>Towards A Structured Overview of Use Cases for Natural Language Processing in the Legal Domain: A German Perspective</title><link>http://arxiv.org/abs/2404.18759v1</link><description>In recent years, the field of Legal Tech has risen in prevalence, as theNatural Language Processing (NLP) and legal disciplines have combined forces todigitalize legal processes. Amidst the steady flow of research solutionsstemming from the NLP domain, the study of use cases has fallen behind, leadingto a number of innovative technical methods without a place in practice. Inthis work, we aim to build a structured overview of Legal Tech use cases,grounded in NLP literature, but also supplemented by voices from legal practicein Germany. Based upon a Systematic Literature Review, we identify sevencategories of NLP technologies for the legal domain, which are then studied injuxtaposition to 22 legal use cases. In the investigation of these use cases,we identify 15 ethical, legal, and social aspects (ELSA), shedding light on thepotential concerns of digitally transforming the legal domain.</description><author>Juraj Vladika, Stephen Meisenbacher, Martina Preis, Alexandra Klymenko, Florian Matthes</author><pubDate>Mon, 29 Apr 2024 15:56:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18759v1</guid></item><item><title>Transitive Vision-Language Prompt Learning for Domain Generalization</title><link>http://arxiv.org/abs/2404.18758v1</link><description>The vision-language pre-training has enabled deep models to make a huge stepforward in generalizing across unseen domains. The recent learning method basedon the vision-language pre-training model is a great tool for domaingeneralization and can solve this problem to a large extent. However, there arestill some issues that an advancement still suffers from trading-off betweendomain invariance and class separability, which are crucial in current DGproblems. However, there are still some issues that an advancement stillsuffers from trading-off between domain invariance and class separability,which are crucial in current DG problems. In this paper, we introduce a novelprompt learning strategy that leverages deep vision prompts to address domaininvariance while utilizing language prompts to ensure class separability,coupled with adaptive weighting mechanisms to balance domain invariance andclass separability. Extensive experiments demonstrate that deep vision promptseffectively extract domain-invariant features, significantly improving thegeneralization ability of deep models and achieving state-of-the-artperformance on three datasets.</description><author>Liyuan Wang, Yan Jin, Zhen Chen, Jinlin Wu, Mengke Li, Yang Lu, Hanzi Wang</author><pubDate>Mon, 29 Apr 2024 15:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18758v1</guid></item><item><title>PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning</title><link>http://arxiv.org/abs/2404.16994v2</link><description>Vision-language pre-training has significantly elevated performance across awide range of image-language applications. Yet, the pre-training process forvideo-related tasks demands exceptionally large computational and dataresources, which hinders the progress of video-language models. This paperinvestigates a straight-forward, highly efficient, and resource-light approachto adapting an existing image-language pre-trained model for dense videounderstanding. Our preliminary experiments reveal that directly fine-tuningpre-trained image-language models with multiple frames as inputs on videodatasets leads to performance saturation or even a drop. Our furtherinvestigation reveals that it is largely attributed to the bias of learnedhigh-norm visual features. Motivated by this finding, we propose a simple buteffective pooling strategy to smooth the feature distribution along thetemporal dimension and thus reduce the dominant impacts from the extremefeatures. The new model is termed Pooling LLaVA, or PLLaVA in short. PLLaVAachieves new state-of-the-art performance on modern benchmark datasets for bothvideo question-answer and captioning tasks. Notably, on the recent popularVideoChatGPT benchmark, PLLaVA achieves a score of 3.48 out of 5 on average offive evaluated dimensions, exceeding the previous SOTA results from GPT4V(IG-VLM) by 9%. On the latest multi-choice benchmark MVBench, PLLaVA achieves58.1% accuracy on average across 20 sub-tasks, 14.5% higher than GPT4V(IG-VLM). Code is available at https://pllava.github.io/</description><author>Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, Jiashi Feng</author><pubDate>Mon, 29 Apr 2024 15:52:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16994v2</guid></item><item><title>Survey on Datasets for Perception in Unstructured Outdoor Environments</title><link>http://arxiv.org/abs/2404.18750v1</link><description>Perception is an essential component of pipelines in field robotics. In thissurvey, we quantitatively compare publicly available datasets available inunstructured outdoor environments. We focus on datasets for common perceptiontasks in field robotics. Our survey categorizes and compares available researchdatasets. This survey also reports on relevant dataset characteristics to helppractitioners determine which dataset fits best for their own application. Webelieve more consideration should be taken in choosing compatible annotationpolicies across the datasets in unstructured outdoor environments.</description><author>Peter Mortimer, Mirko Maehlisch</author><pubDate>Mon, 29 Apr 2024 15:49:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18750v1</guid></item><item><title>Warmth and competence in human-agent cooperation</title><link>http://arxiv.org/abs/2201.13448v3</link><description>Interaction and cooperation with humans are overarching aspirations ofartificial intelligence (AI) research. Recent studies demonstrate that AIagents trained with deep reinforcement learning are capable of collaboratingwith humans. These studies primarily evaluate human compatibility through"objective" metrics such as task performance, obscuring potential variation inthe levels of trust and subjective preference that different agents garner. Tobetter understand the factors shaping subjective preferences in human-agentcooperation, we train deep reinforcement learning agents in Coins, a two-playersocial dilemma. We recruit $N = 501$ participants for a human-agent cooperationstudy and measure their impressions of the agents they encounter. Participants'perceptions of warmth and competence predict their stated preferences fordifferent agents, above and beyond objective performance metrics. Drawinginspiration from social science and biology research, we subsequently implementa new ``partner choice'' framework to elicit revealed preferences: afterplaying an episode with an agent, participants are asked whether they wouldlike to play the next episode with the same agent or to play alone. As withstated preferences, social perception better predicts participants' revealedpreferences than does objective performance. Given these results, we recommendhuman-agent interaction researchers routinely incorporate the measurement ofsocial perception and subjective preferences into their studies.</description><author>Kevin R. McKee, Xuechunzi Bai, Susan T. Fiske</author><pubDate>Mon, 29 Apr 2024 15:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.13448v3</guid></item><item><title>Physics-Informed Machine Learning for Seismic Response Prediction OF Nonlinear Steel Moment Resisting Frame Structures</title><link>http://arxiv.org/abs/2402.17992v3</link><description>There is growing interest in using machine learning (ML) methods forstructural metamodeling due to the substantial computational cost oftraditional simulations. Purely data-driven strategies often face limitationsin model robustness, interpretability, and dependency on extensive data. Toaddress these challenges, this paper introduces a novel physics-informedmachine learning (PiML) method that integrates scientific principles andphysical laws into deep neural networks to model seismic responses of nonlinearstructures. The approach constrains the ML model's solution space within knownphysical bounds through three main features: dimensionality reduction viacombined model order reduction and wavelet analysis, long short-term memory(LSTM) networks, and Newton's second law. Dimensionality reduction addressesstructural systems' redundancy and boosts efficiency while extracting essentialfeatures through wavelet analysis. LSTM networks capture temporal dependenciesfor accurate time-series predictions. Manipulating the equation of motion helpslearn system nonlinearities and confines solutions within physicallyinterpretable results. These attributes allow for model training with sparsedata, enhancing accuracy, interpretability, and robustness. Furthermore, adataset of archetype steel moment resistant frames under seismic loading,available in the DesignSafe-CI Database [1], is considered for evaluation. Theresulting metamodel handles complex data better than existing physics-guidedLSTM models and outperforms other non-physics data-driven networks.</description><author>R. Bailey Bond, Pu Ren, Jerome F. Hajjar, Hao Sun</author><pubDate>Mon, 29 Apr 2024 15:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17992v3</guid></item><item><title>Evaluating the Effectiveness of Video Anomaly Detection in the Wild: Online Learning and Inference for Real-world Deployment</title><link>http://arxiv.org/abs/2404.18747v1</link><description>Video Anomaly Detection (VAD) identifies unusual activities in video streams,a key technology with broad applications ranging from surveillance tohealthcare. Tackling VAD in real-life settings poses significant challenges dueto the dynamic nature of human actions, environmental variations, and domainshifts. Many research initiatives neglect these complexities, oftenconcentrating on traditional testing methods that fail to account forperformance on unseen datasets, creating a gap between theoretical models andtheir real-world utility. Online learning is a potential strategy to mitigatethis issue by allowing models to adapt to new information continuously. Thispaper assesses how well current VAD algorithms can adjust to real-lifeconditions through an online learning framework, particularly those based onpose analysis, for their efficiency and privacy advantages. Our proposedframework enables continuous model updates with streaming data from novelenvironments, thus mirroring actual world challenges and evaluating the models'ability to adapt in real-time while maintaining accuracy. We investigate threestate-of-the-art models in this setting, focusing on their adaptability acrossdifferent domains. Our findings indicate that, even under the most challengingconditions, our online learning approach allows a model to preserve 89.39% ofits original effectiveness compared to its offline-trained counterpart in aspecific target domain.</description><author>Shanle Yao, Ghazal Alinezhad Noghre, Armin Danesh Pazho, Hamed Tabkhi</author><pubDate>Mon, 29 Apr 2024 15:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18747v1</guid></item><item><title>Enhancing Interactive Image Retrieval With Query Rewriting Using Large Language Models and Vision Language Models</title><link>http://arxiv.org/abs/2404.18746v1</link><description>Image search stands as a pivotal task in multimedia and computer vision,finding applications across diverse domains, ranging from internet search tomedical diagnostics. Conventional image search systems operate by acceptingtextual or visual queries, retrieving the top-relevant candidate results fromthe database. However, prevalent methods often rely on single-turn procedures,introducing potential inaccuracies and limited recall. These methods also facethe challenges, such as vocabulary mismatch and the semantic gap, constrainingtheir overall effectiveness. To address these issues, we propose an interactiveimage retrieval system capable of refining queries based on user relevancefeedback in a multi-turn setting. This system incorporates a vision languagemodel (VLM) based image captioner to enhance the quality of text-based queries,resulting in more informative queries with each iteration. Moreover, weintroduce a large language model (LLM) based denoiser to refine text-basedquery expansions, mitigating inaccuracies in image descriptions generated bycaptioning models. To evaluate our system, we curate a new dataset by adaptingthe MSR-VTT video retrieval dataset to the image retrieval task, offeringmultiple relevant ground truth images for each query. Through comprehensiveexperiments, we validate the effectiveness of our proposed system againstbaseline methods, achieving state-of-the-art performance with a notable 10\%improvement in terms of recall. Our contributions encompass the development ofan innovative interactive image retrieval system, the integration of anLLM-based denoiser, the curation of a meticulously designed evaluation dataset,and thorough experimental validation.</description><author>Hongyi Zhu, Jia-Hong Huang, Stevan Rudinac, Evangelos Kanoulas</author><pubDate>Mon, 29 Apr 2024 15:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18746v1</guid></item><item><title>Exposure Bracketing is All You Need for Unifying Image Restoration and Enhancement Tasks</title><link>http://arxiv.org/abs/2401.00766v3</link><description>It is highly desired but challenging to acquire high-quality photos withclear content in low-light environments. Although multi-image processingmethods (using burst, dual-exposure, or multi-exposure images) have madesignificant progress in addressing this issue, they typically focus on specificrestoration or enhancement problems, being insufficient in exploitingmulti-image. Motivated by that multi-exposure images are complementary indenoising, deblurring, high dynamic range imaging, and super-resolution, wepropose to utilize exposure bracketing photography to unify restoration andenhancement tasks in this work. Due to the difficulty in collecting real-worldpairs, we suggest a solution that first pre-trains the model with syntheticpaired data and then adapts it to real-world unlabeled images. In particular, atemporally modulated recurrent network (TMRNet) and self-supervised adaptationmethod are proposed. Moreover, we construct a data simulation pipeline tosynthesize pairs and collect real-world images from 200 nighttime scenarios.Experiments on both datasets show that our method performs favorably againstthe state-of-the-art multi-image processing ones. The dataset, code, andpre-trained models are available at https://github.com/cszhilu1998/BracketIRE.</description><author>Zhilu Zhang, Shuohao Zhang, Renlong Wu, Zifei Yan, Wangmeng Zuo</author><pubDate>Mon, 29 Apr 2024 15:43:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00766v3</guid></item><item><title>Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification</title><link>http://arxiv.org/abs/2404.18739v1</link><description>Similar to humans, animals make extensive use of verbal and non-verbal formsof communication, including a large range of audio signals. In this paper, weaddress dog vocalizations and explore the use of self-supervised speechrepresentation models pre-trained on human speech to address dog barkclassification tasks that find parallels in human-centered tasks in speechrecognition. We specifically address four tasks: dog recognition, breedidentification, gender classification, and context grounding. We show thatusing speech embedding representations significantly improves over simplerclassification baselines. Further, we also find that models pre-trained onlarge human speech acoustics can provide additional performance boosts onseveral tasks.</description><author>Artem Abzaliev, Humberto Pérez Espinosa, Rada Mihalcea</author><pubDate>Mon, 29 Apr 2024 15:41:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18739v1</guid></item><item><title>Annotating Ambiguous Images: General Annotation Strategy for High-Quality Data with Real-World Biomedical Validation</title><link>http://arxiv.org/abs/2306.12189v2</link><description>In the field of image classification, existing methods often struggle withbiased or ambiguous data, a prevalent issue in real-world scenarios. Currentstrategies, including semi-supervised learning and class blending, offerpartial solutions but lack a definitive resolution. Addressing this gap, ourpaper introduces a novel strategy for generating high-quality labels inchallenging datasets. Central to our approach is a clearly designed flowchart,based on a broad literature review, which enables the creation of reliablelabels. We validate our methodology through a rigorous real-world test case inthe biomedical field, specifically in deducing height reduction from vertebralimaging. Our empirical study, leveraging over 250,000 annotations, demonstratesthe effectiveness of our strategies decisions compared to their alternatives.</description><author>Lars Schmarje, Vasco Grossmann, Claudius Zelenka, Johannes Brünger, Reinhard Koch</author><pubDate>Mon, 29 Apr 2024 15:40:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12189v2</guid></item><item><title>Mapping the Potential of Explainable Artificial Intelligence (XAI) for Fairness Along the AI Lifecycle</title><link>http://arxiv.org/abs/2404.18736v1</link><description>The widespread use of artificial intelligence (AI) systems across variousdomains is increasingly highlighting issues related to algorithmic fairness,especially in high-stakes scenarios. Thus, critical considerations of howfairness in AI systems might be improved, and what measures are available toaid this process, are overdue. Many researchers and policymakers seeexplainable AI (XAI) as a promising way to increase fairness in AI systems.However, there is a wide variety of XAI methods and fairness conceptionsexpressing different desiderata, and the precise connections between XAI andfairness remain largely nebulous. Besides, different measures to increasealgorithmic fairness might be applicable at different points throughout an AIsystem's lifecycle. Yet, there currently is no coherent mapping of fairnessdesiderata along the AI lifecycle. In this paper, we set out to bridge boththese gaps: We distill eight fairness desiderata, map them along the AIlifecycle, and discuss how XAI could help address each of them. We hope toprovide orientation for practical applications and to inspire XAI researchspecifically focused on these fairness desiderata.</description><author>Luca Deck, Astrid Schoemäcker, Timo Speith, Jakob Schöffer, Lena Kästner, Niklas Kühl</author><pubDate>Mon, 29 Apr 2024 15:34:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18736v1</guid></item><item><title>Tensor cumulants for statistical inference on invariant distributions</title><link>http://arxiv.org/abs/2404.18735v1</link><description>Many problems in high-dimensional statistics appear to have astatistical-computational gap: a range of values of the signal-to-noise ratiowhere inference is information-theoretically possible, but (conjecturally)computationally intractable. A canonical such problem is Tensor PCA, where weobserve a tensor $Y$ consisting of a rank-one signal plus Gaussian noise.Multiple lines of work suggest that Tensor PCA becomes computationally hard ata critical value of the signal's magnitude. In particular, below thistransition, no low-degree polynomial algorithm can detect the signal with highprobability; conversely, various spectral algorithms are known to succeed abovethis transition. We unify and extend this work by considering tensor networks,orthogonally invariant polynomials where multiple copies of $Y$ are"contracted" to produce scalars, vectors, matrices, or other tensors. We definea new set of objects, tensor cumulants, which provide an explicit,near-orthogonal basis for invariant polynomials of a given degree. This basislets us unify and strengthen previous results on low-degree hardness, giving acombinatorial explanation of the hardness transition and of a continuum ofsubexponential-time algorithms that work below it, and proving tight lowerbounds against low-degree polynomials for recovering rather than just detectingthe signal. It also lets us analyze a new problem of distinguishing betweendifferent tensor ensembles, such as Wigner and Wishart tensors, establishing asharp computational threshold and giving evidence of a newstatistical-computational gap in the Central Limit Theorem for random tensors.Finally, we believe these cumulants are valuable mathematical objects in theirown right: they generalize the free cumulants of free probability theory frommatrices to tensors, and share many of their properties, including additivityunder additive free convolution.</description><author>Dmitriy Kunisky, Cristopher Moore, Alexander S. Wein</author><pubDate>Mon, 29 Apr 2024 15:33:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18735v1</guid></item><item><title>Raising the Bar of AI-generated Image Detection with CLIP</title><link>http://arxiv.org/abs/2312.00195v2</link><description>The aim of this work is to explore the potential of pre-trainedvision-language models (VLMs) for universal detection of AI-generated images.We develop a lightweight detection strategy based on CLIP features and studyits performance in a wide variety of challenging scenarios. We find that,contrary to previous beliefs, it is neither necessary nor convenient to use alarge domain-specific dataset for training. On the contrary, by using only ahandful of example images from a single generative model, a CLIP-based detectorexhibits surprising generalization ability and high robustness across differentarchitectures, including recent commercial tools such as Dalle-3, Midjourneyv5, and Firefly. We match the state-of-the-art (SoTA) on in-distribution dataand significantly improve upon it in terms of generalization toout-of-distribution data (+6% AUC) and robustness to impaired/laundered data(+13%). Our project is available athttps://grip-unina.github.io/ClipBased-SyntheticImageDetection/</description><author>Davide Cozzolino, Giovanni Poggi, Riccardo Corvi, Matthias Nießner, Luisa Verdoliva</author><pubDate>Mon, 29 Apr 2024 15:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00195v2</guid></item><item><title>Real Time Multi Organ Classification on Computed Tomography Images</title><link>http://arxiv.org/abs/2404.18731v1</link><description>Organ segmentation is a fundamental task in medical imaging, and it is usefulfor many clinical automation pipelines. Typically, the process involvessegmenting the entire volume, which can be unnecessary when the points ofinterest are limited. In those cases, a classifier could be used instead ofsegmentation. However, there is an inherent trade-off between the context sizeand the speed of classifiers. To address this issue, we propose a new methodthat employs a data selection strategy with sparse sampling across a wide fieldof view without image resampling. This sparse sampling strategy makes itpossible to classify voxels into multiple organs in real time without usingaccelerators. Although our method is an independent classifier, it can generatefull segmentation by querying grid locations at any resolution. We havecompared our method with existing segmentation techniques, demonstrating itspotential for superior runtime in practical applications in medical imaging.</description><author>Halid Ziya Yerebakan, Yoshihisa Shinagawa, Gerardo Hermosillo Valadez</author><pubDate>Mon, 29 Apr 2024 15:17:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18731v1</guid></item><item><title>CVTN: Cross Variable and Temporal Integration for Time Series Forecasting</title><link>http://arxiv.org/abs/2404.18730v1</link><description>In multivariate time series forecasting, the Transformer architectureencounters two significant challenges: effectively mining features fromhistorical sequences and avoiding overfitting during the learning of temporaldependencies. To tackle these challenges, this paper deconstructs time seriesforecasting into the learning of historical sequences and prediction sequences,introducing the Cross-Variable and Time Network (CVTN). This unique methoddivides multivariate time series forecasting into two phases: cross-variablelearning for effectively mining fea tures from historical sequences, andcross-time learning to capture the temporal dependencies of predictionsequences. Separating these two phases helps avoid the impact of overfitting incross-time learning on cross-variable learning. Exten sive experiments onvarious real-world datasets have confirmed its state-of-the-art (SOTA)performance. CVTN emphasizes three key dimensions in time series fore casting:the short-term and long-term nature of time series (locality and longevity),feature mining from both historical and prediction sequences, and theintegration of cross-variable and cross-time learning. This approach not onlyadvances the current state of time series forecasting but also provides a morecomprehensive framework for future research in this field.</description><author>Han Zhou, Yuntian Chen</author><pubDate>Mon, 29 Apr 2024 15:16:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18730v1</guid></item><item><title>Over-Squashing in Graph Neural Networks: A Comprehensive survey</title><link>http://arxiv.org/abs/2308.15568v6</link><description>Graph Neural Networks (GNNs) revolutionize machine learning forgraph-structured data, effectively capturing complex relationships. Theydisseminate information through interconnected nodes, but long-rangeinteractions face challenges known as "over-squashing". This survey delves intothe challenge of over-squashing in Graph Neural Networks (GNNs), wherelong-range information dissemination is hindered, impacting tasks reliant onintricate long-distance interactions. It comprehensively explores the causes,consequences, and mitigation strategies for over-squashing. Variousmethodologies are reviewed, including graph rewiring, novel normalization,spectral analysis, and curvature-based strategies, with a focus on theirtrade-offs and effectiveness. The survey also discusses the interplay betweenover-squashing and other GNN limitations, such as over-smoothing, and providesa taxonomy of models designed to address these issues in node and graph-leveltasks. Benchmark datasets for performance evaluation are also detailed, makingthis survey a valuable resource for researchers and practitioners in the GNNfield.</description><author>Singh Akansha</author><pubDate>Mon, 29 Apr 2024 15:15:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15568v6</guid></item><item><title>The Constant in HATE: Analyzing Toxicity in Reddit across Topics and Languages</title><link>http://arxiv.org/abs/2404.18726v1</link><description>Toxic language remains an ongoing challenge on social media platforms,presenting significant issues for users and communities. This paper provides across-topic and cross-lingual analysis of toxicity in Reddit conversations. Wecollect 1.5 million comment threads from 481 communities in six languages:English, German, Spanish, Turkish,Arabic, and Dutch, covering 80 topics such asCulture, Politics, and News. We thoroughly analyze how toxicity spikes withindifferent communities in relation to specific topics. We observe consistentpatterns of increased toxicity across languages for certain topics, while alsonoting significant variations within specific language communities.</description><author>Wondimagegnhue Tsegaye Tufa, Ilia Markov, Piek Vossen</author><pubDate>Mon, 29 Apr 2024 15:14:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18726v1</guid></item><item><title>Dual Expert Distillation Network for Generalized Zero-Shot Learning</title><link>http://arxiv.org/abs/2404.16348v2</link><description>Zero-shot learning has consistently yielded remarkable progress via modelingnuanced one-to-one visual-attribute correlation. Existing studies resort torefining a uniform mapping function to align and correlate the sample regionsand subattributes, ignoring two crucial issues: 1) the inherent asymmetry ofattributes; and 2) the unutilized channel information. This paper addressesthese issues by introducing a simple yet effective approach, dubbed Dual ExpertDistillation Network (DEDN), where two experts are dedicated to coarse- andfine-grained visual-attribute modeling, respectively. Concretely, one coarseexpert, namely cExp, has a complete perceptual scope to coordinatevisual-attribute similarity metrics across dimensions, and moreover, anotherfine expert, namely fExp, consists of multiple specialized subnetworks, eachcorresponds to an exclusive set of attributes. Two experts cooperativelydistill from each other to reach a mutual agreement during training. Meanwhile,we further equip DEDN with a newly designed backbone network, i.e., DualAttention Network (DAN), which incorporates both region and channel attentioninformation to fully exploit and leverage visual semantic knowledge.Experiments on various benchmark datasets indicate a new state-of-the-art.</description><author>Zhijie Rao, Jingcai Guo, Xiaocheng Lu, Jingming Liang, Jie Zhang, Haozhao Wang, Kang Wei, Xiaofeng Cao</author><pubDate>Mon, 29 Apr 2024 15:12:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16348v2</guid></item></channel></rss>