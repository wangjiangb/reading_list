<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 15 Jan 2024 06:01:22 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Machine Translation Models are Zero-Shot Detectors of Translation Direction</title><link>http://arxiv.org/abs/2401.06769v1</link><description>Detecting the translation direction of parallel text has applications formachine translation training and evaluation, but also has forensic applicationssuch as resolving plagiarism or forgery allegations. In this work, we explorean unsupervised approach to translation direction detection based on the simplehypothesis that$p(\text{translation}|\text{original})&gt;p(\text{original}|\text{translation})$,motivated by the well-known simplification effect in translationese ormachine-translationese. In experiments with massively multilingual machinetranslation models across 20 translation directions, we confirm theeffectiveness of the approach for high-resource language pairs, achievingdocument-level accuracies of 82-96% for NMT-produced translations, and 60-81%for human translations, depending on the model used. Code and demo areavailable at https://github.com/ZurichNLP/translation-direction-detection</description><author>Michelle Wastl, Jannis Vamvas, Rico Sennrich</author><pubDate>Fri, 12 Jan 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06769v1</guid></item><item><title>Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements</title><link>http://arxiv.org/abs/2401.06766v1</link><description>Large language models demonstrate a remarkable capability for learning tosolve new tasks from a few examples. The prompt template, or the way the inputexamples are formatted to obtain the prompt, is an important yet oftenoverlooked aspect of in-context learning. In this work, we conduct acomprehensive study of the template format's influence on the in-contextlearning performance. We evaluate the impact of the prompt template acrossmodels (from 770M to 70B parameters) and 4 standard classification datasets. Weshow that a poor choice of the template can reduce the performance of thestrongest models and inference methods to a random guess level. Moreimportantly, the best templates do not transfer between different setups andeven between models of the same family. Our findings show that the currentlyprevalent approach to evaluation, which ignores template selection, may givemisleading results due to different templates in different works. As a firststep towards mitigating this issue, we propose Template Ensembles thataggregate model predictions across several templates. This simple test-timeaugmentation boosts average performance while being robust to the choice ofrandom set of templates.</description><author>Anton Voronov, Lena Wolf, Max Ryabinin</author><pubDate>Fri, 12 Jan 2024 18:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06766v1</guid></item><item><title>Robust Peak Detection for Holter ECGs by Self-Organized Operational Neural Networks</title><link>http://arxiv.org/abs/2110.02381v2</link><description>Although numerous R-peak detectors have been proposed in the literature,their robustness and performance levels may significantly deteriorate inlow-quality and noisy signals acquired from mobile electrocardiogram (ECG)sensors, such as Holter monitors. Recently, this issue has been addressed bydeep 1-D convolutional neural networks (CNNs) that have achievedstate-of-the-art performance levels in Holter monitors; however, they pose ahigh complexity level that requires special parallelized hardware setup forreal-time processing. On the other hand, their performance deteriorates when acompact network configuration is used instead. This is an expected outcome asrecent studies have demonstrated that the learning performance of CNNs islimited due to their strictly homogenous configuration with the sole linearneuron model. In this study, to further boost the peak detection performancealong with an elegant computational efficiency, we propose 1-D Self-OrganizedONNs (Self-ONNs) with generative neurons. The most crucial advantage of 1-DSelf-ONNs over the ONNs is their self-organization capability that voids theneed to search for the best operator set per neuron since each generativeneuron has the ability to create the optimal operator during training. Theexperimental results over the China Physiological Signal Challenge-2020 (CPSC)dataset with more than one million ECG beats show that the proposed 1-DSelf-ONNs can significantly surpass the state-of-the-art deep CNN with lesscomputational complexity. Results demonstrate that the proposed solutionachieves a 99.10% F1-score, 99.79% sensitivity, and 98.42% positivepredictivity in the CPSC dataset, which is the best R-peak detectionperformance ever achieved.</description><author>Moncef Gabbouj, Serkan Kiranyaz, Junaid Malik, Muhammad Uzair Zahid, Turker Ince, Muhammad Chowdhury, Amith Khandakar, Anas Tahir</author><pubDate>Fri, 12 Jan 2024 18:53:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.02381v2</guid></item><item><title>Seeing the roads through the trees: A benchmark for modeling spatial dependencies with aerial imagery</title><link>http://arxiv.org/abs/2401.06762v1</link><description>Fully understanding a complex high-resolution satellite or aerial imageryscene often requires spatial reasoning over a broad relevant context. The humanobject recognition system is able to understand object in a scene over along-range relevant context. For example, if a human observes an aerial scenethat shows sections of road broken up by tree canopy, then they will beunlikely to conclude that the road has actually been broken up into disjointpieces by trees and instead think that the canopy of nearby trees is occludingthe road. However, there is limited research being conducted to understandlong-range context understanding of modern machine learning models. In thiswork we propose a road segmentation benchmark dataset, Chesapeake Roads SpatialContext (RSC), for evaluating the spatial long-range context understanding ofgeospatial machine learning models and show how commonly used semanticsegmentation models can fail at this task. For example, we show that a U-Nettrained to segment roads from background in aerial imagery achieves an 84%recall on unoccluded roads, but just 63.5% recall on roads covered by treecanopy despite being trained to model both the same way. We further analyze howthe performance of models changes as the relevant context for a decision(unoccluded roads in our case) varies in distance. We release the code toreproduce our experiments and dataset of imagery and masks to encourage futureresearch in this direction -- https://github.com/isaaccorley/ChesapeakeRSC.</description><author>Caleb Robinson, Isaac Corley, Anthony Ortiz, Rahul Dodhia, Juan M. Lavista Ferres, Peyman Najafirad</author><pubDate>Fri, 12 Jan 2024 18:50:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06762v1</guid></item><item><title>APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding</title><link>http://arxiv.org/abs/2401.06761v1</link><description>The massive adoption of large language models (LLMs) demands efficientdeployment strategies. However, the auto-regressive decoding process, which isfundamental to how most LLMs generate text, poses challenges to achieveefficient serving. In this work, we introduce a parallel auto-regressivegeneration method. By instruct-tuning on general domain data that containshierarchical structures, we enable LLMs to independently plan their generationprocess and perform auto-parallel auto-regressive (APAR) generation,significantly reducing the number of generation steps. APAR alone can achieveup to 2x speed-up, and when combined with speculative decoding, the speed-upcan reach up to 4x. In addition, APAR reduces the key-value cache consumptionand attention computation during generation. This leads to a throughputincrease of 20-70% and a latency reduce of 20-35% in high-throughput scenarios,compared to state-of-the-art serving frameworks.</description><author>Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, Yuxiao Dong</author><pubDate>Fri, 12 Jan 2024 18:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06761v1</guid></item><item><title>Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies</title><link>http://arxiv.org/abs/2401.06760v1</link><description>Ten years ago a single metric, BLEU, governed progress in machine translationresearch. For better or worse, there is no such consensus today, andconsequently it is difficult for researchers to develop and retain the kinds ofheuristic intuitions about metric deltas that drove earlier research anddeployment decisions. This paper investigates the "dynamic range" of a numberof modern metrics in an effort to provide a collective understanding of themeaning of differences in scores both within and among metrics; in other words,we ask what point difference X in metric Y is required between two systems forhumans to notice? We conduct our evaluation on a new large dataset, ToShip23,using it to discover deltas at which metrics achieve system-level differencesthat are meaningful to humans, which we measure by pairwise system accuracy. Weadditionally show that this method of establishing delta-accuracy is morestable than the standard use of statistical p-values in regards to testsetsize. Where data size permits, we also explore the effect of metric deltas andaccuracy across finer-grained features such as translation direction, domain,and system closeness.</description><author>Tom Kocmi, Vil√©m Zouhar, Christian Federmann, Matt Post</author><pubDate>Fri, 12 Jan 2024 18:47:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06760v1</guid></item><item><title>Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction</title><link>http://arxiv.org/abs/2401.06757v1</link><description>Pedestrian intention prediction is crucial for autonomous driving. Inparticular, knowing if pedestrians are going to cross in front of theego-vehicle is core to performing safe and comfortable maneuvers. Creatingaccurate and fast models that predict such intentions from sequential images ischallenging. A factor contributing to this is the lack of datasets with diversecrossing and non-crossing (C/NC) scenarios. We address this scarceness byintroducing a framework, named ARCANE, which allows programmatically generatingsynthetic datasets consisting of C/NC video clip samples. As an example, we useARCANE to generate a large and diverse dataset named PedSynth. We will show howPedSynth complements widely used real-world datasets such as JAAD and PIE, soenabling more accurate models for C/NC prediction. Considering the onboarddeployment of C/NC prediction models, we also propose a deep model namedPedGNN, which is fast and has a very low memory footprint. PedGNN is based on aGNN-GRU architecture that takes a sequence of pedestrian skeletons as input topredict crossing intentions.</description><author>Muhammad Naveed Riaz, Maciej Wielgosz, Abel Garcia Romera, Antonio M. Lopez</author><pubDate>Fri, 12 Jan 2024 18:44:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06757v1</guid></item><item><title>Solving the Discretised Multiphase Flow Equations with Interface Capturing on Structured Grids Using Machine Learning Libraries</title><link>http://arxiv.org/abs/2401.06755v1</link><description>This paper solves the multiphase flow equations with interface capturingusing the AI4PDEs approach (Artificial Intelligence for Partial DifferentialEquations). The solver within AI4PDEs uses tools from machine learning (ML)libraries to solve (exactly) partial differential equations (PDEs) that havebeen discretised using numerical methods. Convolutional layers can be used toexpress the discretisations as a neural network, whose weights are determinedby the numerical method, rather than by training. To solve the system, amultigrid solver is implemented through a neural network with a U-Netarchitecture. Immiscible two-phase flow is modelled by the 3D incompressibleNavier-Stokes equations with surface tension and advection of a volume fractionfield, which describes the interface between the fluids. A new compressivealgebraic volume-of-fluids method is introduced, based on a residualformulation using Petrov-Galerkin for accuracy and designed with AI4PDEs inmind. High-order finite-element based schemes are chosen to model a collapsingwater column and a rising bubble. Results compare well with experimental dataand other numerical results from the literature, demonstrating that, for thefirst time, finite element discretisations of multiphase flows can be solvedusing the neural network solver from the AI4PDEs approach. A benefit ofexpressing numerical discretisations as neural networks is that the code canrun, without modification, on CPUs, GPUs or the latest accelerators designedespecially to run AI codes.</description><author>Boyang Chen, Claire E. Heaney, Jefferson L. M. A. Gomes, Omar K. Matar, Christopher C. Pain</author><pubDate>Fri, 12 Jan 2024 18:42:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06755v1</guid></item><item><title>Stylometry Analysis of Multi-authored Documents for Authorship and Author Style Change Detection</title><link>http://arxiv.org/abs/2401.06752v1</link><description>In recent years, the increasing use of Artificial Intelligence based textgeneration tools has posed new challenges in document provenance,authentication, and authorship detection. However, advancements in stylometryhave provided opportunities for automatic authorship and author changedetection in multi-authored documents using style analysis techniques. Styleanalysis can serve as a primary step toward document provenance andauthentication through authorship detection. This paper investigates three keytasks of style analysis: (i) classification of single and multi-authoreddocuments, (ii) single change detection, which involves identifying the pointwhere the author switches, and (iii) multiple author-switching detection inmulti-authored documents. We formulate all three tasks as classificationproblems and propose a merit-based fusion framework that integrates severalstate-of-the-art natural language processing (NLP) algorithms and weightoptimization techniques. We also explore the potential of special characters,which are typically removed during pre-processing in NLP applications, on theperformance of the proposed methods for these tasks by conducting extensiveexperiments on both cleaned and raw datasets. Experimental results demonstratesignificant improvements over existing solutions for all three tasks on abenchmark dataset.</description><author>Muhammad Tayyab Zamir, Muhammad Asif Ayub, Asma Gul, Nasir Ahmad, Kashif Ahmad</author><pubDate>Fri, 12 Jan 2024 18:36:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06752v1</guid></item><item><title>The Unreasonable Effectiveness of Easy Training Data for Hard Tasks</title><link>http://arxiv.org/abs/2401.06751v1</link><description>How can we train models to perform well on hard test data when hard trainingdata is by definition difficult to label correctly? This question has beentermed the scalable oversight problem and has drawn increasing attention aslanguage models have continually improved. In this paper, we present thesurprising conclusion that current language models often generalize relativelywell from easy to hard data, even performing as well as "oracle" models trainedon hard data. We demonstrate this kind of easy-to-hard generalization usingsimple training methods like in-context learning, linear classifier heads, andQLoRA for seven different measures of datapoint hardness, including sixempirically diverse human hardness measures (like grade level) and onemodel-based measure (loss-based). Furthermore, we show that even if one caresmost about model performance on hard data, it can be better to collect andtrain on easy data rather than hard data, since hard data is generally noisierand costlier to collect. Our experiments use open models up to 70b in size andfour publicly available question-answering datasets with questions ranging indifficulty from 3rd grade science questions to college level STEM questions andgeneral-knowledge trivia. We conclude that easy-to-hard generalization in LMsis surprisingly strong for the tasks studied, suggesting the scalable oversightproblem may be easier than previously thought. Our code is available athttps://github.com/allenai/easy-to-hard-generalization</description><author>Peter Hase, Mohit Bansal, Peter Clark, Sarah Wiegreffe</author><pubDate>Fri, 12 Jan 2024 18:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06751v1</guid></item><item><title>Learning Temporal Resolution in Spectrogram for Audio Classification</title><link>http://arxiv.org/abs/2210.01719v3</link><description>The audio spectrogram is a time-frequency representation that has been widelyused for audio classification. One of the key attributes of the audiospectrogram is the temporal resolution, which depends on the hop size used inthe Short-Time Fourier Transform (STFT). Previous works generally assume thehop size should be a constant value (e.g., 10 ms). However, a fixed temporalresolution is not always optimal for different types of sound. The temporalresolution affects not only classification accuracy but also computationalcost. This paper proposes a novel method, DiffRes, that enables differentiabletemporal resolution modeling for audio classification. Given a spectrogramcalculated with a fixed hop size, DiffRes merges non-essential time frameswhile preserving important frames. DiffRes acts as a "drop-in" module betweenan audio spectrogram and a classifier and can be jointly optimized with theclassification task. We evaluate DiffRes on five audio classification tasks,using mel-spectrograms as the acoustic features, followed by off-the-shelfclassifier backbones. Compared with previous methods using the fixed temporalresolution, the DiffRes-based method can achieve the equivalent or betterclassification accuracy with at least 25% computational cost reduction. Wefurther show that DiffRes can improve classification accuracy by increasing thetemporal resolution of input acoustic features, without adding to thecomputational cost.</description><author>Haohe Liu, Xubo Liu, Qiuqiang Kong, Wenwu Wang, Mark D. Plumbley</author><pubDate>Fri, 12 Jan 2024 18:35:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.01719v3</guid></item><item><title>Using Natural Language Inference to Improve Persona Extraction from Dialogue in a New Domain</title><link>http://arxiv.org/abs/2401.06742v1</link><description>While valuable datasets such as PersonaChat provide a foundation for trainingpersona-grounded dialogue agents, they lack diversity in conversational andnarrative settings, primarily existing in the "real" world. To develop dialogueagents with unique personas, models are trained to converse given a specificpersona, but hand-crafting these persona can be time-consuming, thus methodsexist to automatically extract persona information from existingcharacter-specific dialogue. However, these persona-extraction models are alsotrained on datasets derived from PersonaChat and struggle to providehigh-quality persona information from conversational settings that do not takeplace in the real world, such as the fantasy-focused dataset, LIGHT. Creatingnew data to train models on a specific setting is human-intensive, thusprohibitively expensive. To address both these issues, we introduce a naturallanguage inference method for post-hoc adapting a trained persona extractionmodel to a new setting. We draw inspiration from the literature of dialognatural language inference (NLI), and devise NLI-reranking methods to extractstructured persona information from dialogue. Compared to existing personaextraction models, our method returns higher-quality extracted persona andrequires less human annotation.</description><author>Alexandra DeLucia, Mengjie Zhao, Yoshinori Maeda, Makoto Yoda, Keiichi Yamada, Hiromi Wakaki</author><pubDate>Fri, 12 Jan 2024 18:25:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06742v1</guid></item><item><title>A deep implicit-explicit minimizing movement method for option pricing in jump-diffusion models</title><link>http://arxiv.org/abs/2401.06740v1</link><description>We develop a novel deep learning approach for pricing European basket optionswritten on assets that follow jump-diffusion dynamics. The option pricingproblem is formulated as a partial integro-differential equation, which isapproximated via a new implicit-explicit minimizing movement time-steppingapproach, involving approximation by deep, residual-type Artificial NeuralNetworks (ANNs) for each time step. The integral operator is discretized viatwo different approaches: a) a sparse-grid Gauss--Hermite approximationfollowing localised coordinate axes arising from singular value decompositions,and b) an ANN-based high-dimensional special-purpose quadrature rule.Crucially, the proposed ANN is constructed to ensure the asymptotic behavior ofthe solution for large values of the underlyings and also leads to consistentoutputs with respect to a priori known qualitative properties of the solution.The performance and robustness with respect to the dimension of the methods areassessed in a series of numerical experiments involving the Mertonjump-diffusion model.</description><author>Emmanuil H. Georgoulis, Antonis Papapantoleon, Costas Smaragdakis</author><pubDate>Fri, 12 Jan 2024 18:21:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06740v1</guid></item><item><title>Diffusion Models for Multi-target Adversarial Tracking</title><link>http://arxiv.org/abs/2307.06244v2</link><description>Target tracking plays a crucial role in real-world scenarios, particularly indrug-trafficking interdiction, where the knowledge of an adversarial target'slocation is often limited. Improving autonomous tracking systems will enableunmanned aerial, surface, and underwater vehicles to better assist ininterdicting smugglers that use manned surface, semi-submersible, and aerialvessels. As unmanned drones proliferate, accurate autonomous target estimationis even more crucial for security and safety. This paper presents ConstrainedAgent-based Diffusion for Enhanced Multi-Agent Tracking (CADENCE), an approachaimed at generating comprehensive predictions of adversary locations byleveraging past sparse state information. To assess the effectiveness of thisapproach, we evaluate predictions on single-target and multi-target pursuitenvironments, employing Monte-Carlo sampling of the diffusion model to estimatethe probability associated with each generated trajectory. We propose a novelcross-attention based diffusion model that utilizes constraint-based samplingto generate multimodal track hypotheses. Our single-target model surpasses theperformance of all baseline methods on Average Displacement Error (ADE) forpredictions across all time horizons.</description><author>Sean Ye, Manisha Natarajan, Zixuan Wu, Matthew Gombolay</author><pubDate>Fri, 12 Jan 2024 18:20:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06244v2</guid></item><item><title>Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum</title><link>http://arxiv.org/abs/2401.06738v1</link><description>We analyze the convergence of stochastic heavy ball (SHB) momentum in thesmooth, strongly-convex setting. Kidambi et al. (2018) show that SHB (withsmall mini-batches) cannot attain an accelerated rate of convergence even forquadratics, and conjecture that the practical gain of SHB is a by-product ofmini-batching. We substantiate this claim by showing that SHB can obtain anaccelerated rate when the mini-batch size is larger than some threshold. Inparticular, for strongly-convex quadratics with condition number $\kappa$, weprove that SHB with the standard step-size and momentum parameters results inan $O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$ convergence rate,where $T$ is the number of iterations and $\sigma^2$ is the variance in thestochastic gradients. To ensure convergence to the minimizer, we propose amulti-stage approach that results in a noise-adaptive$O\left(\exp\left(-\frac{T}{\sqrt{\kappa}} \right) + \frac{\sigma}{T}\right)$rate. For general strongly-convex functions, we use the averaginginterpretation of SHB along with exponential step-sizes to prove an$O\left(\exp\left(-\frac{T}{\kappa} \right) + \frac{\sigma^2}{T} \right)$convergence to the minimizer in a noise-adaptive manner. Finally, weempirically demonstrate the effectiveness of the proposed algorithms.</description><author>Anh Dang, Reza Babanezhad, Sharan Vaswani</author><pubDate>Fri, 12 Jan 2024 18:17:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06738v1</guid></item><item><title>A finite sample analysis of the benign overfitting phenomenon for ridge function estimation</title><link>http://arxiv.org/abs/2007.12882v5</link><description>Recent extensive numerical experiments in high scale machine learning haveallowed to uncover a quite counterintuitive phase transition, as a function ofthe ratio between the sample size and the number of parameters in the model. Asthe number of parameters $p$ approaches the sample size $n$, the generalisationerror increases, but surprisingly, it starts decreasing again past thethreshold $p=n$. This phenomenon, brought to the theoretical communityattention in \cite{belkin2019reconciling}, has been thoroughly investigatedlately, more specifically for simpler models than deep neural networks, such asthe linear model when the parameter is taken to be the minimum norm solution tothe least-squares problem, firstly in the asymptotic regime when $p$ and $n$tend to infinity, see e.g. \cite{hastie2019surprises}, and recently in thefinite dimensional regime and more specifically for linear models\cite{bartlett2020benign}, \cite{tsigler2020benign},\cite{lecue2022geometrical}. In the present paper, we propose a finite sampleanalysis of non-linear models of \textit{ridge} type, where we investigate the\textit{overparametrised regime} of the double descent phenomenon for both the\textit{estimation problem} and the \textit{prediction} problem. Our resultsprovide a precise analysis of the distance of the best estimator from the trueparameter as well as a generalisation bound which complements recent works of\cite{bartlett2020benign} and \cite{chinot2020benign}. Our analysis is based ontools closely related to the continuous Newton method\cite{neuberger2007continuous} and a refined quantitative analysis of theperformance in prediction of the minimum $\ell_2$-norm solution.</description><author>Emmanuel Caron, Stephane Chretien</author><pubDate>Fri, 12 Jan 2024 18:04:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2007.12882v5</guid></item><item><title>Can AI Be as Creative as Humans?</title><link>http://arxiv.org/abs/2401.01623v3</link><description>Creativity serves as a cornerstone for societal progress and innovation. Withthe rise of advanced generative AI models capable of tasks once reserved forhuman creativity, the study of AI's creative potential becomes imperative forits responsible development and application. In this paper, we provide atheoretical answer to the question of whether AI can be creative. We prove intheory that AI can be as creative as humans under the condition that AI can fitthe existing data generated by human creators. Therefore, the debate on AI'screativity is reduced into the question of its ability of fitting a massiveamount of data. To arrive at this conclusion, this paper first addresses thecomplexities in defining creativity by introducing a new concept calledRelative Creativity. Instead of trying to define creativity universally, weshift the focus to whether AI can match the creative abilities of ahypothetical human. This perspective draws inspiration from the Turing Test,expanding upon it to address the challenges and subjectivities inherent inassessing creativity. This methodological shift leads to a statisticallyquantifiable assessment of AI's creativity, which we term StatisticalCreativity. This concept allows for comparisons of AI's creative abilities withthose of specific human groups, and facilitates the theoretical findings ofAI's creative potential. Building on this foundation, we discuss theapplication of statistical creativity in prompt-conditioned autoregressivemodels, providing a practical means for evaluating creative abilities ofcontemporary AI models, such as Large Language Models (LLMs). In addition todefining and analyzing creativity, we introduce an actionable trainingguideline, effectively bridging the gap between theoretical quantification ofcreativity and practical model training.</description><author>Haonan Wang, James Zou, Michael Mozer, Anirudh Goyal, Alex Lamb, Linjun Zhang, Weijie J Su, Zhun Deng, Michael Qizhe Xie, Hannah Brown, Kenji Kawaguchi</author><pubDate>Fri, 12 Jan 2024 18:03:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01623v3</guid></item><item><title>Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty</title><link>http://arxiv.org/abs/2401.06730v1</link><description>As natural language becomes the default interface for human-AI interaction,there is a critical need for LMs to appropriately communicate uncertainties indownstream applications. In this work, we investigate how LMs incorporateconfidence about their responses via natural language and how downstream usersbehave in response to LM-articulated uncertainties. We examine publiclydeployed models and find that LMs are unable to express uncertainties whenanswering questions even when they produce incorrect responses. LMs can beexplicitly prompted to express confidences, but tend to be overconfident,resulting in high error rates (on average 47%) among confident responses. Wetest the risks of LM overconfidence by running human experiments and show thatusers rely heavily on LM generations, whether or not they are marked bycertainty. Lastly, we investigate the preference-annotated datasets used inRLHF alignment and find that humans have a bias against texts with uncertainty.Our work highlights a new set of safety harms facing human-LM interactions andproposes design recommendations and mitigating strategies moving forward.</description><author>Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, Maarten Sap</author><pubDate>Fri, 12 Jan 2024 18:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06730v1</guid></item><item><title>Deep Manifold Graph Auto-Encoder for Attributed Graph Embedding</title><link>http://arxiv.org/abs/2401.06727v1</link><description>Representing graph data in a low-dimensional space for subsequent tasks isthe purpose of attributed graph embedding. Most existing neural networkapproaches learn latent representations by minimizing reconstruction errors.Rare work considers the data distribution and the topological structure oflatent codes simultaneously, which often results in inferior embeddings inreal-world graph data. This paper proposes a novel Deep Manifold (Variational)Graph Auto-Encoder (DMVGAE/DMGAE) method for attributed graph data to improvethe stability and quality of learned representations to tackle the crowdingproblem. The node-to-node geodesic similarity is preserved between the originaland latent space under a pre-defined distribution. The proposed methodsurpasses state-of-the-art baseline algorithms by a significant margin ondifferent downstream tasks across popular datasets, which validates oursolutions. We promise to release the code after acceptance.</description><author>Bozhen Hu, Zelin Zang, Jun Xia, Lirong Wu, Cheng Tan, Stan Z. Li</author><pubDate>Fri, 12 Jan 2024 17:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06727v1</guid></item><item><title>Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models</title><link>http://arxiv.org/abs/2401.06102v2</link><description>Inspecting the information encoded in hidden representations of largelanguage models (LLMs) can explain models' behavior and verify their alignmentwith human values. Given the capabilities of LLMs in generatinghuman-understandable text, we propose leveraging the model itself to explainits internal representations in natural language. We introduce a frameworkcalled Patchscopes and show how it can be used to answer a wide range ofquestions about an LLM's computation. We show that prior interpretabilitymethods based on projecting representations into the vocabulary space andintervening on the LLM computation can be viewed as instances of thisframework. Moreover, several of their shortcomings such as failure ininspecting early layers or lack of expressivity can be mitigated byPatchscopes. Beyond unifying prior inspection techniques, Patchscopes alsoopens up new possibilities such as using a more capable model to explain therepresentations of a smaller model, and unlocks new applications such asself-correction in multi-hop reasoning.</description><author>Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, Mor Geva</author><pubDate>Fri, 12 Jan 2024 17:54:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06102v2</guid></item><item><title>Reframing Tax Law Entailment as Analogical Reasoning</title><link>http://arxiv.org/abs/2401.06715v1</link><description>Statutory reasoning refers to the application of legislative provisions to aseries of case facts described in natural language. We re-frame statutoryreasoning as an analogy task, where each instance of the analogy task involvesa combination of two instances of statutory reasoning. This increases thedataset size by two orders of magnitude, and introduces an element ofinterpretability. We show that this task is roughly as difficult to NaturalLanguage Processing models as the original task. Finally, we come back tostatutory reasoning, solving it with a combination of a retrieval mechanism andanalogy models, and showing some progress on prior comparable work.</description><author>Xinrui Zou, Ming Zhang, Nathaniel Weir, Benjamin Van Durme, Nils Holzenberger</author><pubDate>Fri, 12 Jan 2024 17:37:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06715v1</guid></item><item><title>Few-Shot Detection of Machine-Generated Text using Style Representations</title><link>http://arxiv.org/abs/2401.06712v1</link><description>The advent of instruction-tuned language models that convincingly mimic humanwriting poses a significant risk of abuse. For example, such models could beused for plagiarism, disinformation, spam, or phishing. However, such abuse maybe counteracted with the ability to detect whether a piece of text was composedby a language model rather than a human. Some previous approaches to thisproblem have relied on supervised methods trained on corpora of confirmed humanand machine-written documents. Unfortunately, model under-specification posesan unavoidable challenge for neural network-based detectors, making thembrittle in the face of data shifts, such as the release of further languagemodels producing still more fluent text than the models used to train thedetectors. Other previous approaches require access to the models that may havegenerated a document in question at inference or detection time, which is oftenimpractical. In light of these challenges, we pursue a fundamentally differentapproach not relying on samples from language models of concern at trainingtime. Instead, we propose to leverage representations of writing styleestimated from human-authored text. Indeed, we find that features effective atdistinguishing among human authors are also effective at distinguishing humanfrom machine authors, including state of the art large language models likeLlama 2, ChatGPT, and GPT-4. Furthermore, given a handful of examples composedby each of several specific language models of interest, our approach affordsthe ability to predict which model generated a given document.</description><author>Rafael Rivera Soto, Kailin Koch, Aleem Khan, Barry Chen, Marcus Bishop, Nicholas Andrews</author><pubDate>Fri, 12 Jan 2024 17:26:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06712v1</guid></item><item><title>Model-Free Approximate Bayesian Learning for Large-Scale Conversion Funnel Optimization</title><link>http://arxiv.org/abs/2401.06710v1</link><description>The flexibility of choosing the ad action as a function of the consumer stateis critical for modern-day marketing campaigns. We study the problem ofidentifying the optimal sequential personalized interventions that maximize theadoption probability for a new product. We model consumer behavior by aconversion funnel that captures the state of each consumer (e.g., interactionhistory with the firm) and allows the consumer behavior to vary as a functionof both her state and firm's sequential interventions. We show our modelcaptures consumer behavior with very high accuracy (out-of-sample AUC of over0.95) in a real-world email marketing dataset. However, it results in a verylarge-scale learning problem, where the firm must learn the state-specificeffects of various interventions from consumer interactions. We propose a novelattribution-based decision-making algorithm for this problem that we callmodel-free approximate Bayesian learning. Our algorithm inherits theinterpretability and scalability of Thompson sampling for bandits and maintainsan approximate belief over the value of each state-specific intervention. Thebelief is updated as the algorithm interacts with the consumers. Despite beingan approximation to the Bayes update, we prove the asymptotic optimality of ouralgorithm and analyze its convergence rate. We show that our algorithmsignificantly outperforms traditional approaches on extensive simulationscalibrated to a real-world email marketing dataset.</description><author>Garud Iyengar, Raghav Singal</author><pubDate>Fri, 12 Jan 2024 17:19:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06710v1</guid></item><item><title>Reliability Analysis of Psychological Concept Extraction and Classification in User-penned Text</title><link>http://arxiv.org/abs/2401.06709v1</link><description>The social NLP research community witness a recent surge in the computationaladvancements of mental health analysis to build responsible AI models for acomplex interplay between language use and self-perception. Such responsible AImodels aid in quantifying the psychological concepts from user-penned texts onsocial media. On thinking beyond the low-level (classification) task, weadvance the existing binary classification dataset, towards a higher-level taskof reliability analysis through the lens of explanations, posing it as one ofthe safety measures. We annotate the LoST dataset to capture nuanced textualcues that suggest the presence of low self-esteem in the posts of Reddit users.We further state that the NLP models developed for determining the presence oflow self-esteem, focus more on three types of textual cues: (i) Trigger: wordsthat triggers mental disturbance, (ii) LoST indicators: text indicatorsemphasizing low self-esteem, and (iii) Consequences: words describing theconsequences of mental disturbance. We implement existing classifiers toexamine the attention mechanism in pre-trained language models (PLMs) for adomain-specific psychology-grounded task. Our findings suggest the need ofshifting the focus of PLMs from Trigger and Consequences to a morecomprehensive explanation, emphasizing LoST indicators while determining lowself-esteem in Reddit posts.</description><author>Muskan Garg, MSVPJ Sathvik, Amrit Chadha, Shaina Raza, Sunghwan Sohn</author><pubDate>Fri, 12 Jan 2024 17:19:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06709v1</guid></item><item><title>Nonlinear Meta-Learning Can Guarantee Faster Rates</title><link>http://arxiv.org/abs/2307.10870v3</link><description>Many recent theoretical works on \emph{meta-learning} aim to achieveguarantees in leveraging similar representational structures from related taskstowards simplifying a target task. Importantly, the main aim in theory works onthe subject is to understand the extent to which convergence rates -- inlearning a common representation -- \emph{may scale with the number $N$ oftasks} (as well as the number of samples per task). First steps in this settingdemonstrate this property when both the shared representation amongst tasks,and task-specific regression functions, are linear. This linear setting readilyreveals the benefits of aggregating tasks, e.g., via averaging arguments. Inpractice, however, the representation is often highly nonlinear, introducingnontrivial biases in each task that cannot easily be averaged out as in thelinear case. In the present work, we derive theoretical guarantees formeta-learning with nonlinear representations. In particular, assuming theshared nonlinearity maps to an infinite-dimensional RKHS, we show thatadditional biases can be mitigated with careful regularization that leveragesthe smoothness of task-specific regression functions,</description><author>Dimitri Meunier, Zhu Li, Arthur Gretton, Samory Kpotufe</author><pubDate>Fri, 12 Jan 2024 17:15:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10870v3</guid></item><item><title>Multi-Candidate Speculative Decoding</title><link>http://arxiv.org/abs/2401.06706v1</link><description>Large language models have shown impressive capabilities across a variety ofNLP tasks, yet their generating text autoregressively is time-consuming. Oneway to speed them up is speculative decoding, which generates candidatesegments (a sequence of tokens) from a fast draft model that is then verifiedin parallel by the target model. However, the acceptance rate of candidatetokens receives limitations from several factors, such as the model, thedataset, and the decoding setup. This paper proposes sampling multiplecandidates from a draft model and then organising them in batches forverification. We design algorithms for efficient multi-candidate verificationwhile maintaining the distribution of the target model. Our approach showssignificant improvements in acceptance rates on multiple datasets and models,consistently outperforming standard speculative decoding.</description><author>Sen Yang, Shujian Huang, Xinyu Dai, Jiajun Chen</author><pubDate>Fri, 12 Jan 2024 17:15:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06706v1</guid></item><item><title>milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing</title><link>http://arxiv.org/abs/2306.17010v4</link><description>Approaching the era of ubiquitous computing, human motion sensing plays acrucial role in smart systems for decision making, user interaction, andpersonalized services. Extensive research has been conducted on human tracking,pose estimation, gesture recognition, and activity recognition, which arepredominantly based on cameras in traditional methods. However, the intrusivenature of cameras limits their use in smart home applications. To address this,mmWave radars have gained popularity due to their privacy-friendly features. Inthis work, we propose milliFlow, a novel deep learning method for scene flowestimation as a complementary motion information for mmWave point cloud,serving as an intermediate level of features and directly benefiting downstreamhuman motion sensing tasks. Experimental results demonstrate the superiorperformance of our method with an average 3D endpoint error of 4.6cm,significantly surpassing the competing approaches. Furthermore, byincorporating scene flow information, we achieve remarkable improvements inhuman activity recognition, human parsing, and human body part tracking. Tofoster further research in this area, we will provide our codebase and datasetfor open access upon acceptance.</description><author>Fangqiang Ding, Zhen Luo, Peijun Zhao, Chris Xiaoxuan Lu</author><pubDate>Fri, 12 Jan 2024 17:12:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17010v4</guid></item><item><title>LabelBench: A Comprehensive Framework for Benchmarking Adaptive Label-Efficient Learning</title><link>http://arxiv.org/abs/2306.09910v3</link><description>Labeled data are critical to modern machine learning applications, butobtaining labels can be expensive. To mitigate this cost, machine learningmethods, such as transfer learning, semi-supervised learning and activelearning, aim to be label-efficient: achieving high predictive performance fromrelatively few labeled examples. While obtaining the best label-efficiency inpractice often requires combinations of these techniques, existing benchmarkand evaluation frameworks do not capture a concerted combination of all suchtechniques. This paper addresses this deficiency by introducing LabelBench, anew computationally-efficient framework for joint evaluation of multiplelabel-efficient learning techniques. As an application of LabelBench, weintroduce a novel benchmark of state-of-the-art active learning methods incombination with semi-supervised learning for fine-tuning pretrained visiontransformers. Our benchmark demonstrates better label-efficiencies thanpreviously reported in active learning. LabelBench's modular codebase isopen-sourced for the broader community to contribute label-efficient learningmethods and benchmarks. The repository can be found at:https://github.com/EfficientTraining/LabelBench.</description><author>Jifan Zhang, Yifang Chen, Gregory Canal, Stephen Mussmann, Arnav M. Das, Gantavya Bhatt, Yinglun Zhu, Jeffrey Bilmes, Simon Shaolei Du, Kevin Jamieson, Robert D Nowak</author><pubDate>Fri, 12 Jan 2024 17:12:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09910v3</guid></item><item><title>Scalable 3D Panoptic Segmentation With Superpoint Graph Clustering</title><link>http://arxiv.org/abs/2401.06704v1</link><description>We introduce a highly efficient method for panoptic segmentation of large 3Dpoint clouds by redefining this task as a scalable graph clustering problem.This approach can be trained using only local auxiliary tasks, therebyeliminating the resource-intensive instance-matching step during training.Moreover, our formulation can easily be adapted to the superpoint paradigm,further increasing its efficiency. This allows our model to process scenes withmillions of points and thousands of objects in a single inference. Our method,called SuperCluster, achieves a new state-of-the-art panoptic segmentationperformance for two indoor scanning datasets: $50.1$ PQ ($+7.8$) for S3DISArea~5, and $58.7$ PQ ($+25.2$) for ScanNetV2. We also set the firststate-of-the-art for two large-scale mobile mapping benchmarks: KITTI-360 andDALES. With only $209$k parameters, our model is over $30$ times smaller thanthe best-competing method and trains up to $15$ times faster. Our code andpretrained models are available athttps://github.com/drprojects/superpoint_transformer.</description><author>Damien Robert, Hugo Raguet, Loic Landrieu</author><pubDate>Fri, 12 Jan 2024 17:10:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06704v1</guid></item><item><title>A Closed-form Solution for Weight Optimization in Fully-connected Feed-forward Neural Networks</title><link>http://arxiv.org/abs/2401.06699v1</link><description>This work addresses weight optimization problem for fully-connectedfeed-forward neural networks. Unlike existing approaches that are based onback-propagation (BP) and chain rule gradient-based optimization (which impliesiterative execution, potentially burdensome and time-consuming in some cases),the proposed approach offers the solution for weight optimization inclosed-form by means of least squares (LS) methodology. In the case where theinput-to-output mapping is injective, the new approach optimizes the weights ina back-propagating fashion in a single iteration by jointly optimizing a set ofweights in each layer for each neuron. In the case where the input-to-outputmapping is not injective (e.g., in classification problems), the proposedsolution is easily adapted to obtain its final solution in a few iterations. Animportant advantage over the existing solutions is that these computations (forall neurons in a layer) are independent from each other; thus, they can becarried out in parallel to optimize all weights in a given layersimultaneously. Furthermore, its running time is deterministic in the sensethat one can obtain the exact number of computations necessary to optimize theweights in all network layers (per iteration, in the case of non-injectivemapping). Our simulation and empirical results show that the proposed scheme,BPLS, works well and is competitive with existing ones in terms of accuracy,but significantly surpasses them in terms of running time. To summarize, thenew method is straightforward to implement, is competitive and computationallymore efficient than the existing ones, and is well-tailored for parallelimplementation.</description><author>Slavisa Tomic, Jo√£o Pedro Matos-Carvalho, Marko Beko</author><pubDate>Fri, 12 Jan 2024 17:03:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06699v1</guid></item><item><title>An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models</title><link>http://arxiv.org/abs/2401.06692v1</link><description>Supervised finetuning (SFT) on instruction datasets has played a crucial rolein achieving the remarkable zero-shot generalization capabilities observed inmodern large language models (LLMs). However, the annotation efforts requiredto produce high quality responses for instructions are becoming prohibitivelyexpensive, especially as the number of tasks spanned by instruction datasetscontinues to increase. Active learning is effective in identifying usefulsubsets of samples to annotate from an unlabeled pool, but its highcomputational cost remains a barrier to its widespread applicability in thecontext of LLMs. To mitigate the annotation cost of SFT and circumvent thecomputational bottlenecks of active learning, we propose using experimentaldesign. Experimental design techniques select the most informative samples tolabel, and typically maximize some notion of uncertainty and/or diversity. Inour work, we implement a framework that evaluates several existing and novelexperimental design techniques and find that these methods consistently yieldsignificant gains in label efficiency with little computational overhead. Ongenerative tasks, our methods achieve the same generalization performance withonly $50\%$ of annotation cost required by random sampling.</description><author>Gantavya Bhatt, Yifang Chen, Arnav M. Das, Jifan Zhang, Sang T. Truong, Stephen Mussmann, Yinglun Zhu, Jeffrey Bilmes, Simon S. Du, Kevin Jamieson, Jordan T. Ash, Robert D. Nowak</author><pubDate>Fri, 12 Jan 2024 16:56:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06692v1</guid></item><item><title>Embedded Planogram Compliance Control System</title><link>http://arxiv.org/abs/2401.06690v1</link><description>The retail sector presents several open and challenging problems that couldbenefit from advanced pattern recognition and computer vision techniques. Onesuch critical challenge is planogram compliance control. In this study, wepropose a complete embedded system to tackle this issue. Our system consists offour key components as image acquisition and transfer via stand-alone embeddedcamera module, object detection via computer vision and deep learning methodsworking on single board computers, planogram compliance control method againworking on single board computers, and energy harvesting and power managementblock to accompany the embedded camera modules. The image acquisition andtransfer block is implemented on the ESP-EYE camera module. The objectdetection block is based on YOLOv5 as the deep learning method and localfeature extraction. We implement these methods on Raspberry Pi 4, NVIDIA JetsonOrin Nano, and NVIDIA Jetson AGX Orin as single board computers. The planogramcompliance control block utilizes sequence alignment through a modifiedNeedleman-Wunsch algorithm. This block is also working along with the objectdetection block on the same single board computers. The energy harvesting andpower management block consists of solar and RF energy harvesting modules withsuitable battery pack for operation. We tested the proposed embedded planogramcompliance control system on two different datasets to provide valuableinsights on its strengths and weaknesses. The results show that our methodachieves F1 scores of 0.997 and 1.0 in object detection and planogramcompliance control blocks, respectively. Furthermore, we calculated that thecomplete embedded system can work in stand-alone form up to two years based onbattery. This duration can be further extended with the integration of theproposed solar and RF energy harvesting options.</description><author>M. Erkin Y√ºcel, Serkan Topaloƒülu, Cem √únsalan</author><pubDate>Fri, 12 Jan 2024 16:54:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06690v1</guid></item><item><title>Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation</title><link>http://arxiv.org/abs/2401.06688v1</link><description>Neural machine translation systems estimate probabilities of target sentencesgiven source sentences, yet these estimates may not align with humanpreferences. This work introduces QE-fusion, a method utilizing a qualityestimation metric (QE) that better correlates with human judgments tosynthesize improved translations. QE-fusion leverages a candidate pool sampledfrom a model, combining spans from different candidates using QE metrics suchas CometKiwi. We compare QE-fusion against beam search and recent rerankingtechniques, such as Minimum Bayes Risk decoding or QE-reranking. Our methodconsistently improves translation quality in terms of COMET and BLEURT scoreswhen applied to large language models (LLMs) used for translation (PolyLM,XGLM, Llama2, and Mistral) and to multilingual translation models (NLLB), overfive language pairs. Notably, QE-fusion exhibits larger improvements for LLMsdue to their ability to generate diverse outputs. We demonstrate that ourapproach generates novel translations in over half of the cases andconsistently outperforms other methods across varying numbers of candidates(5-200). Furthermore, we empirically establish that QE-fusion scales linearlywith the number of candidates in the pool. QE-fusion proves effective inenhancing LLM-based translation without the need for costly retraining of LLMs.</description><author>Giorgos Vernikos, Andrei Popescu-Belis</author><pubDate>Fri, 12 Jan 2024 16:52:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06688v1</guid></item><item><title>Proximal Causal Inference With Text Data</title><link>http://arxiv.org/abs/2401.06687v1</link><description>Recent text-based causal methods attempt to mitigate confounding bias byincluding unstructured text data as proxies of confounding variables that arepartially or imperfectly measured. These approaches assume analysts havesupervised labels of the confounders given text for a subset of instances, aconstraint that is not always feasible due to data privacy or cost. Here, weaddress settings in which an important confounding variable is completelyunobserved. We propose a new causal inference method that splits pre-treatmenttext data, infers two proxies from two zero-shot models on the separate splits,and applies these proxies in the proximal g-formula. We prove that ourtext-based proxy method satisfies identification conditions required by theproximal g-formula while other seemingly reasonable proposals do not. Weevaluate our method in synthetic and semi-synthetic settings and find that itproduces estimates with low bias. This combination of proximal causal inferenceand zero-shot classifiers is novel (to our knowledge) and expands the set oftext-specific causal methods available to practitioners.</description><author>Jacob M. Chen, Rohit Bhattacharya, Katherine A. Keith</author><pubDate>Fri, 12 Jan 2024 16:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06687v1</guid></item><item><title>Grounding Foundation Models through Federated Transfer Learning: A General Framework</title><link>http://arxiv.org/abs/2311.17431v8</link><description>Foundation Models (FMs) such as GPT-4 encoded with vast knowledge andpowerful emergent abilities have achieved remarkable success in various naturallanguage processing and computer vision tasks. Grounding FMs by adapting themto domain-specific tasks or augmenting them with domain-specific knowledgeenables us to exploit the full potential of FMs. However, grounding FMs facesseveral challenges, stemming primarily from constrained computing resources,data privacy, model heterogeneity, and model ownership. Federated TransferLearning (FTL), the combination of federated learning and transfer learning,provides promising solutions to address these challenges. In recent years, theneed for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly inboth academia and industry. Motivated by the strong growth in FTL-FM researchand the potential impact of FTL-FM on industrial applications, we propose anFTL-FM framework that formulates problems of grounding FMs in the federatedlearning setting, construct a detailed taxonomy based on the FTL-FM frameworkto categorize state-of-the-art FTL-FM works, and comprehensively overviewFTL-FM works based on the proposed taxonomy. We also establish correspondencesbetween FTL-FM and conventional phases of adapting FM so that FM practitionerscan align their research works with FTL-FM. In addition, we overview advancedefficiency-improving and privacy-preserving techniques because efficiency andprivacy are critical concerns in FTL-FM. Last, we discuss opportunities andfuture research directions of FTL-FM.</description><author>Yan Kang, Tao Fan, Hanlin Gu, Xiaojin Zhang, Lixin Fan, Qiang Yang</author><pubDate>Fri, 12 Jan 2024 16:46:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17431v8</guid></item><item><title>DQNC2S: DQN-based Cross-stream Crisis event Summarizer</title><link>http://arxiv.org/abs/2401.06683v1</link><description>Summarizing multiple disaster-relevant data streams simultaneously isparticularly challenging as existing Retrieve&amp;Re-ranking strategies suffer fromthe inherent redundancy of multi-stream data and limited scalability in amulti-query setting. This work proposes an online approach to crisis timelinegeneration based on weak annotation with Deep Q-Networks. It selects on-the-flythe relevant pieces of text without requiring neither human annotations norcontent re-ranking. This makes the inference time independent of the number ofinput queries. The proposed approach also incorporates a redundancy filter intothe reward function to effectively handle cross-stream content overlaps. Theachieved ROUGE and BERTScore results are superior to those of best-performingmodels on the CrisisFACTS 2022 benchmark.</description><author>Daniele Rege Cambrin, Luca Cagliero, Paolo Garza</author><pubDate>Fri, 12 Jan 2024 16:43:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06683v1</guid></item><item><title>O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models</title><link>http://arxiv.org/abs/2310.14403v3</link><description>Recent advancements in large language models (LLMs) have exhibited promisingperformance in solving sequential decision-making problems. By imitatingfew-shot examples provided in the prompts (i.e., in-context learning), an LLMagent can interact with an external environment and complete given taskswithout additional training. However, such few-shot examples are ofteninsufficient to generate high-quality solutions for complex and long-horizontasks, while the limited context length cannot consume larger-scaledemonstrations. To this end, we propose an offline learning framework thatutilizes offline data at scale (e.g, logs of human interactions) to facilitatethe in-context learning performance of LLM agents. We formally defineLLM-powered policies with both text-based approaches and code-based approaches.We then introduce an Offline Data-driven Discovery and Distillation (O3D)framework to improve LLM-powered policies without finetuning. O3D automaticallydiscovers reusable skills and distills generalizable knowledge across multipletasks based on offline interaction data, advancing the capability of solvingdownstream tasks. Empirical results under two interactive decision-makingbenchmarks (ALFWorld and WebShop) demonstrate that O3D can notably enhance thedecision-making capabilities of LLMs through the offline discovery anddistillation process, and consistently outperform baselines across various LLMswith both text-based-policy and code-based-policy.</description><author>Yuchen Xiao, Yanchao Sun, Mengda Xu, Udari Madhushani, Jared Vann, Deepeka Garg, Sumitra Ganesh</author><pubDate>Fri, 12 Jan 2024 16:42:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14403v3</guid></item><item><title>Advancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2401.00280v2</link><description>Tactics, Techniques, and Procedures (TTPs) outline the methods attackers useto exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&amp;CKframework can be challenging for cybersecurity practitioners due to presumedexpertise, complex dependencies, and inherent ambiguity. Meanwhile,advancements with Large Language Models (LLMs) have led to recent surge instudies exploring its uses in cybersecurity operations. This leads us toquestion how well encoder-only (e.g., RoBERTa) and decoder-only (e.g., GPT-3.5)LLMs can comprehend and summarize TTPs to inform analysts of the intendedpurposes (i.e., tactics) of a cyberattack procedure. The state-of-the-art LLMshave shown to be prone to hallucination by providing inaccurate information,which is problematic in critical domains like cybersecurity. Therefore, wepropose the use of Retrieval Augmented Generation (RAG) techniques to extractrelevant contexts for each cyberattack procedure for decoder-only LLMs (withoutfine-tuning). We further contrast such approach against supervised fine-tuning(SFT) of encoder-only LLMs. Our results reveal that both the direct-use ofdecoder-only LLMs (i.e., its pre-trained knowledge) and the SFT of encoder-onlyLLMs offer inaccurate interpretation of cyberattack procedures. Significantimprovements are shown when RAG is used for decoder-only LLMs, particularlywhen directly relevant context is found. This study further sheds insights onthe limitations and capabilities of using RAG for LLMs in interpreting TTPs.</description><author>Reza Fayyazi, Rozhina Taghdimi, Shanchieh Jay Yang</author><pubDate>Fri, 12 Jan 2024 16:37:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00280v2</guid></item><item><title>LLMRS: Unlocking Potentials of LLM-Based Recommender Systems for Software Purchase</title><link>http://arxiv.org/abs/2401.06676v1</link><description>Recommendation systems are ubiquitous, from Spotify playlist suggestions toAmazon product suggestions. Nevertheless, depending on the methodology or thedataset, these systems typically fail to capture user preferences and generategeneral recommendations. Recent advancements in Large Language Models (LLM)offer promising results for analyzing user queries. However, employing thesemodels to capture user preferences and efficiency remains an open question. Inthis paper, we propose LLMRS, an LLM-based zero-shot recommender system wherewe employ pre-trained LLM to encode user reviews into a review score andgenerate user-tailored recommendations. We experimented with LLMRS on areal-world dataset, the Amazon product reviews, for software purchase usecases. The results show that LLMRS outperforms the ranking-based baseline modelwhile successfully capturing meaningful information from product reviews,thereby providing more reliable recommendations.</description><author>Angela John, Theophilus Aidoo, Hamayoon Behmanush, Irem B. Gunduz, Hewan Shrestha, Maxx Richard Rahman, Wolfgang Maa√ü</author><pubDate>Fri, 12 Jan 2024 16:33:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06676v1</guid></item><item><title>TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models</title><link>http://arxiv.org/abs/2309.04027v2</link><description>Machine learning models can perpetuate unintended biases from unfair andimbalanced datasets. Evaluating and debiasing these datasets and models isespecially hard in text datasets where sensitive attributes such as race,gender, and sexual orientation may not be available. When these models aredeployed into society, they can lead to unfair outcomes for historicallyunderrepresented groups. In this paper, we present a dataset coupled with anapproach to improve text fairness in classifiers and language models. We createa new, more comprehensive identity lexicon, TIDAL, which includes 15,123identity terms and associated sense context across three demographiccategories. We leverage TIDAL to develop an identity annotation andaugmentation tool that can be used to improve the availability of identitycontext and the effectiveness of ML fairness techniques. We evaluate ourapproaches using human contributors, and additionally run experiments focusedon dataset and model debiasing. Results show our assistive annotation techniqueimproves the reliability and velocity of human-in-the-loop processes. Ourdataset and methods uncover more disparities during evaluation, and alsoproduce more fair models during remediation. These approaches provide apractical path forward for scaling classifier and generative model fairness inreal-world settings.</description><author>Emmanuel Klu, Sameer Sethi</author><pubDate>Fri, 12 Jan 2024 16:20:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04027v2</guid></item><item><title>Generalizable Sleep Staging via Multi-Level Domain Alignment</title><link>http://arxiv.org/abs/2401.05363v2</link><description>Automatic sleep staging is essential for sleep assessment and disorderdiagnosis. Most existing methods depend on one specific dataset and are limitedto be generalized to other unseen datasets, for which the training data andtesting data are from the same dataset. In this paper, we introduce domaingeneralization into automatic sleep staging and propose the task ofgeneralizable sleep staging which aims to improve the model generalizationability to unseen datasets. Inspired by existing domain generalization methods,we adopt the feature alignment idea and propose a framework called SleepDG tosolve it. Considering both of local salient features and sequential featuresare important for sleep staging, we propose a Multi-level Feature Alignmentcombining epoch-level and sequence-level feature alignment to learndomain-invariant feature representations. Specifically, we design anEpoch-level Feature Alignment to align the feature distribution of each singlesleep epoch among different domains, and a Sequence-level Feature Alignment tominimize the discrepancy of sequential features among different domains.SleepDG is validated on five public datasets, achieving the state-of-the-artperformance.</description><author>Jiquan Wang, Sha Zhao, Haiteng Jiang, Shijian Li, Tao Li, Gang Pan</author><pubDate>Fri, 12 Jan 2024 16:17:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05363v2</guid></item><item><title>Asynchronous Algorithmic Alignment with Cocycles</title><link>http://arxiv.org/abs/2306.15632v3</link><description>State-of-the-art neural algorithmic reasoners make use of message passing ingraph neural networks (GNNs). But typical GNNs blur the distinction between thedefinition and invocation of the message function, forcing a node to sendmessages to its neighbours at every layer, synchronously. When applying GNNs tolearn to execute dynamic programming algorithms, however, on most steps only ahandful of the nodes would have meaningful updates to send. One, hence, runsthe risk of inefficiencies by sending too much irrelevant data across thegraph. But more importantly, many intermediate GNN steps have to learn theidentity functions, which is a non-trivial learning problem. In this work, weexplicitly separate the concepts of node state update and message functioninvocation. With this separation, we obtain a mathematical formulation thatallows us to reason about asynchronous computation in both algorithms andneural networks. Our analysis yields several practical implementations ofsynchronous scalable GNN layers that are provably invariant under various formsof asynchrony.</description><author>Andrew Dudzik, Tamara von Glehn, Razvan Pascanu, Petar Veliƒçkoviƒá</author><pubDate>Fri, 12 Jan 2024 16:13:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15632v3</guid></item><item><title>How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs</title><link>http://arxiv.org/abs/2401.06373v1</link><description>Most traditional AI safety research has approached AI models as machines andcentered on algorithm-focused attacks developed by security experts. As largelanguage models (LLMs) become increasingly common and competent, non-expertusers can also impose risks during daily interactions. This paper introduces anew perspective to jailbreak LLMs as human-like communicators, to explore thisoverlooked intersection between everyday language interaction and AI safety.Specifically, we study how to persuade LLMs to jailbreak them. First, wepropose a persuasion taxonomy derived from decades of social science research.Then, we apply the taxonomy to automatically generate interpretable persuasiveadversarial prompts (PAP) to jailbreak LLMs. Results show that persuasionsignificantly increases the jailbreak performance across all risk categories:PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7bChat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focusedattacks. On the defense side, we explore various mechanisms against PAP and,found a significant gap in existing defenses, and advocate for more fundamentalmitigation for highly interactive LLMs</description><author>Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, Weiyan Shi</author><pubDate>Fri, 12 Jan 2024 16:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06373v1</guid></item><item><title>PolyTOPS: Reconfigurable and Flexible Polyhedral Scheduler</title><link>http://arxiv.org/abs/2401.06665v1</link><description>Polyhedral techniques have been widely used for automatic code optimizationin low-level compilers and higher-level processes. Loop optimization is centralto this technique, and several polyhedral schedulers like Feautrier, Pluto, island Tensor Scheduler have been proposed, each of them targeting a differentarchitecture, parallelism model, or application scenario. The need forscenario-specific optimization is growing due to the heterogeneity ofarchitectures. One of the most critical cases is represented by NPUs (NeuralProcessing Units) used for AI, which may require loop optimization withdifferent objectives. Another factor to be considered is the framework orcompiler in which polyhedral optimization takes place. Different scenarios,depending on the target architecture, compilation environment, and applicationdomain, may require different kinds of optimization to best exploit thearchitecture feature set. We introduce a new configurable polyhedral scheduler, PolyTOPS, that can beadjusted to various scenarios with straightforward, high-level configurations.This scheduler allows the creation of diverse scheduling strategies that can beboth scenario-specific (like state-of-the-art schedulers) and kernel-specific,breaking the concept of a one-size-fits-all scheduler approach. PolyTOPS hasbeen used with isl and CLooG as code generators and has been integrated inMindSpore AKG deep learning compiler. Experimental results in differentscenarios show good performance: a geomean speedup of 7.66x on MindSpore (forthe NPU Ascend architecture) hybrid custom operators over isl scheduling, ageomean speedup up to 1.80x on PolyBench on different multicore architecturesover Pluto scheduling. Finally, some comparisons with differentstate-of-the-art tools are presented in the PolyMage scenario.</description><author>Gianpietro Consolaro, Zhen Zhang, Harenome Razanajato, Nelson Lossing, Nassim Tchoulak, Adilla Susungi, Artur Cesar Araujo Alves, Renwei Zhang, Denis Barthou, Corinne Ancourt, Cedric Bastoul</author><pubDate>Fri, 12 Jan 2024 16:11:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06665v1</guid></item><item><title>US \&amp; MRI Image Fusion Based on Markerless Skin Registration</title><link>http://arxiv.org/abs/2307.14288v2</link><description>This paper presents an innovative automatic fusion imaging system thatcombines 3D CT/MR images with real-time ultrasound (US) acquisition. The systemeliminates the need for external physical markers and complex training, makingimage fusion feasible for physicians with different experience levels. Theintegrated system involves a portable 3D camera for patient-specific surfaceacquisition, an electromagnetic tracking system, and US components. The fusionalgorithm comprises two main parts: skin segmentation and rigidco-registration, both integrated into the US machine. The co-registrationsoftware aligns the surface extracted from CT/MR images with patient-specificcoordinates, facilitating rapid and effective fusion. Experimental testing indifferent settings, including the clinical environment, validates the system'saccuracy, computational efficiency, noise robustness, and operatorindependence. The co-registration error remains under the acceptable rangeof~$1$ cm.</description><author>Martina Paccini, Giacomo Paschina, Stefano De Beni, Giuseppe Patan√®</author><pubDate>Fri, 12 Jan 2024 16:11:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14288v2</guid></item><item><title>WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge</title><link>http://arxiv.org/abs/2401.06659v1</link><description>Sentiment analysis is rapidly advancing by utilizing various data modalities(e.g., text, image). However, most previous works relied on superficialinformation, neglecting the incorporation of contextual world knowledge (e.g.,background information derived from but beyond the given image and text pairs)and thereby restricting their ability to achieve better multimodal sentimentanalysis. In this paper, we proposed a plug-in framework named WisdoM, designedto leverage contextual world knowledge induced from the large vision-languagemodels (LVLMs) for enhanced multimodal sentiment analysis. WisdoM utilizes aLVLM to comprehensively analyze both images and corresponding sentences,simultaneously generating pertinent context. To reduce the noise in thecontext, we also introduce a training-free Contextual Fusion mechanism.Experimental results across diverse granularities of multimodal sentimentanalysis tasks consistently demonstrate that our approach has substantialimprovements (brings an average +1.89 F1 score among five advanced methods)over several state-of-the-art methods. Code will be released.</description><author>Wenbin Wang, Liang Ding, Li Shen, Yong Luo, Han Hu, Dacheng Tao</author><pubDate>Fri, 12 Jan 2024 16:08:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06659v1</guid></item><item><title>Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partially observability</title><link>http://arxiv.org/abs/2212.07946v2</link><description>Reinforcement learning (RL) has garnered significant attention for developingdecision-making agents that aim to maximize rewards, specified by an externalsupervisor, within fully observable environments. However, many real-worldproblems involve partial observations, formulated as partially observableMarkov decision processes (POMDPs). Previous studies have tackled RL in POMDPsby either incorporating the memory of past actions and observations or byinferring the true state of the environment from observed data. However,aggregating observed data over time becomes impractical in continuous spaces.Moreover, inference-based RL approaches often require many samples to performwell, as they focus solely on reward maximization and neglect uncertainty inthe inferred state. Active inference (AIF) is a framework formulated in POMDPsand directs agents to select actions by minimizing a function called expectedfree energy (EFE). This supplies reward-maximizing (exploitative) behaviour, asin RL, with information-seeking (exploratory) behaviour. Despite thisexploratory behaviour of AIF, its usage is limited to discrete spaces due tothe computational challenges associated with EFE. In this paper, we propose aunified principle that establishes a theoretical connection between AIF and RL,enabling seamless integration of these two approaches and overcoming theiraforementioned limitations in continuous space POMDP settings. We substantiateour findings with theoretical analysis, providing novel perspectives forutilizing AIF in the design of artificial agents. Experimental resultsdemonstrate the superior learning capabilities of our method in solvingcontinuous space partially observable tasks. Notably, our approach harnessesinformation-seeking exploration, enabling it to effectively solve reward-freeproblems and rendering explicit task reward design by an external supervisoroptional.</description><author>Parvin Malekzadeh, Konstantinos N. Plataniotis</author><pubDate>Fri, 12 Jan 2024 16:02:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07946v2</guid></item><item><title>Neural Networks for Singular Perturbations</title><link>http://arxiv.org/abs/2401.06656v1</link><description>We prove deep neural network (DNN for short) expressivity rate bounds forsolution sets of a model class of singularly perturbed, elliptic two-pointboundary value problems, in Sobolev norms, on the bounded interval $(-1,1)$. Weassume that the given source term and reaction coefficient are analytic in$[-1,1]$. We establish expression rate bounds in Sobolev norms in terms of the NN sizewhich are uniform with respect to the singular perturbation parameter forseveral classes of DNN architectures. In particular, ReLU NNs, spiking NNs, and$\tanh$- and sigmoid-activated NNs. The latter activations can represent``exponential boundary layer solution features'' explicitly, in the last hiddenlayer of the DNN, i.e. in a shallow subnetwork, and afford improved robustexpression rate bounds in terms of the NN size. We prove that all DNN architectures allow robust exponential solutionexpression in so-called `energy' as well as in `balanced' Sobolev norms, foranalytic input data.</description><author>Joost A. A. Opschoor, Christoph Schwab, Christos Xenophontos</author><pubDate>Fri, 12 Jan 2024 16:02:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06656v1</guid></item><item><title>Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI Benchmarks</title><link>http://arxiv.org/abs/2401.06654v1</link><description>Feature removal is a central building block for eXplainable AI (XAI), bothfor occlusion-based explanations (Shapley values) as well as their evaluation(pixel flipping, PF). However, occlusion strategies can vary significantly fromsimple mean replacement up to inpainting with state-of-the-art diffusionmodels. This ambiguity limits the usefulness of occlusion-based approaches. Forexample, PF benchmarks lead to contradicting rankings. This is amplified bycompeting PF measures: Features are either removed starting with mostinfluential first (MIF) or least influential first (LIF). This study proposestwo complementary perspectives to resolve this disagreement problem. Firstly,we address the common criticism of occlusion-based XAI, that artificial sampleslead to unreliable model evaluations. We propose to measure the reliability bythe R(eference)-Out-of-Model-Scope (OMS) score. The R-OMS score enables asystematic comparison of occlusion strategies and resolves the disagreementproblem by grouping consistent PF rankings. Secondly, we show that theinsightfulness of MIF and LIF is conversely dependent on the R-OMS score. Toleverage this, we combine the MIF and LIF measures into the symmetric relevancegain (SRG) measure. This breaks the inherent connection to the underlyingocclusion strategy and leads to consistent rankings. This resolves thedisagreement problem, which we verify for a set of 40 different occlusionstrategies.</description><author>Stefan Bl√ºcher, Johanna Vielhaben, Nils Strodthoff</author><pubDate>Fri, 12 Jan 2024 16:01:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06654v1</guid></item><item><title>Data-Efficient Interactive Multi-Objective Optimization Using ParEGO</title><link>http://arxiv.org/abs/2401.06649v1</link><description>Multi-objective optimization is a widely studied problem in diverse fields,such as engineering and finance, that seeks to identify a set of non-dominatedsolutions that provide optimal trade-offs among competing objectives. However,the computation of the entire Pareto front can become prohibitively expensive,both in terms of computational resources and time, particularly when dealingwith a large number of objectives. In practical applications, decision-makers(DMs) will select a single solution of the Pareto front that aligns with theirpreferences to be implemented; thus, traditional multi-objective algorithmsinvest a lot of budget sampling solutions that are not interesting for the DM.In this paper, we propose two novel algorithms that employ Gaussian Processesand advanced discretization methods to efficiently locate the most preferredregion of the Pareto front in expensive-to-evaluate problems. Our approachinvolves interacting with the decision-maker to guide the optimization processtowards their preferred trade-offs. Our experimental results demonstrate thatour proposed algorithms are effective in finding non-dominated solutions thatalign with the decision-maker's preferences while maintaining computationalefficiency.</description><author>Arash Heidari, Sebastian Rojas Gonzalez, Tom Dhaene, Ivo Couckuyt</author><pubDate>Fri, 12 Jan 2024 15:55:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06649v1</guid></item><item><title>Simultaneous Task Allocation and Planning for Multi-Robots under Hierarchical Temporal Logic Specifications</title><link>http://arxiv.org/abs/2401.04003v2</link><description>Past research into robotic planning with temporal logic specifications,notably Linear Temporal Logic (LTL), was largely based on singular formulas forindividual or groups of robots. But with increasing task complexity, LTLformulas unavoidably grow lengthy, complicating interpretation andspecification generation, and straining the computational capacities of theplanners. By leveraging the intrinsic structure of tasks, we introduced ahierarchical structure to LTL specifications with requirements on syntax andsemantics, and proved that they are more expressive than their flatcounterparts. Second, we employ a search-based approach to synthesize plans fora multi-robot system, accomplishing simultaneous task allocation and planning.The search space is approximated by loosely interconnected sub-spaces, witheach sub-space corresponding to one LTL specification. The search ispredominantly confined to a single sub-space, transitioning to anothersub-space under certain conditions, determined by the decomposition ofautomatons. Moreover, multiple heuristics are formulated to expedite the searchsignificantly. A theoretical analysis concerning completeness and optimality isconducted under mild assumptions. When compared with existing methods onservice tasks, our method outperforms in terms of execution times withcomparable solution quality. Finally, scalability is evaluated by testing agroup of 30 robots and achieving reasonable runtimes.</description><author>Xusheng Luo, Changliu Liu</author><pubDate>Fri, 12 Jan 2024 15:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.04003v2</guid></item><item><title>Block Majorization Minimization with Extrapolation and Application to $Œ≤$-NMF</title><link>http://arxiv.org/abs/2401.06646v1</link><description>We propose a Block Majorization Minimization method with Extrapolation (BMMe)for solving a class of multi-convex optimization problems. The extrapolationparameters of BMMe are updated using a novel adaptive update rule. By showingthat block majorization minimization can be reformulated as a block mirrordescent method, with the Bregman divergence adaptively updated at eachiteration, we establish subsequential convergence for BMMe. We use this methodto design efficient algorithms to tackle nonnegative matrix factorizationproblems with the $\beta$-divergences ($\beta$-NMF) for $\beta\in [1,2]$. Thesealgorithms, which are multiplicative updates with extrapolation, benefit fromour novel results that offer convergence guarantees. We also empiricallyillustrate the significant acceleration of BMMe for $\beta$-NMF throughextensive experiments.</description><author>Le Thi Khanh Hien, Valentin Leplat, Nicolas Gillis</author><pubDate>Fri, 12 Jan 2024 15:52:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06646v1</guid></item><item><title>SeizNet: An AI-enabled Implantable Sensor Network System for Seizure Prediction</title><link>http://arxiv.org/abs/2401.06644v1</link><description>In this paper, we introduce SeizNet, a closed-loop system for predictingepileptic seizures through the use of Deep Learning (DL) method and implantablesensor networks. While pharmacological treatment is effective for some epilepsypatients (with ~65M people affected worldwide), one out of three suffer fromdrug-resistant epilepsy. To alleviate the impact of seizure, predictive systemshave been developed that can notify such patients of an impending seizure,allowing them to take precautionary measures. SeizNet leverages DL techniquesand combines data from multiple recordings, specifically intracranialelectroencephalogram (iEEG) and electrocardiogram (ECG) sensors, that cansignificantly improve the specificity of seizure prediction while preservingvery high levels of sensitivity. SeizNet DL algorithms are designed forefficient real-time execution at the edge, minimizing data privacy concerns,data transmission overhead, and power inefficiencies associated withcloud-based solutions. Our results indicate that SeizNet outperformstraditional single-modality and non-personalized prediction systems in allmetrics, achieving up to 99% accuracy in predicting seizure, offering apromising new avenue in refractory epilepsy treatment.</description><author>Ali Saeizadeh, Douglas Schonholtz, Daniel Uvaydov, Raffaele Guida, Emrecan Demirors, Pedram Johari, Jorge M. Jimenez, Joseph S. Neimat, Tommaso Melodia</author><pubDate>Fri, 12 Jan 2024 15:51:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06644v1</guid></item><item><title>Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation</title><link>http://arxiv.org/abs/2401.06643v1</link><description>The latest generative large language models (LLMs) have found theirapplication in data augmentation tasks, where small numbers of text samples areLLM-paraphrased and then used to fine-tune the model. However, more research isneeded to assess how different prompts, seed data selection strategies,filtering methods, or model settings affect the quality of paraphrased data(and downstream models). In this study, we investigate three text diversityincentive methods well established in crowdsourcing: taboo words, hints byprevious outlier solutions, and chaining on previous outlier solutions. Usingthese incentive methods as part of instructions to LLMs augmenting textdatasets, we measure their effects on generated texts' lexical diversity anddownstream model performance. We compare the effects over 5 different LLMs and6 datasets. We show that diversity is most increased by taboo words, whiledownstream model performance is highest when previously created paraphrases areused as hints.</description><author>Jan Cegin, Branislav Pecher, Jakub Simko, Ivan Srba, Maria Bielikova, Peter Brusilovsky</author><pubDate>Fri, 12 Jan 2024 15:46:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06643v1</guid></item><item><title>Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently</title><link>http://arxiv.org/abs/2401.06640v1</link><description>Recent zero-shot evaluations have highlighted important limitations in theabilities of language models (LMs) to perform meaning extraction. However, itis now well known that LMs can demonstrate radical improvements in the presenceof experimental contexts such as in-context examples and instructions. How welldoes this translate to previously studied meaning-sensitive tasks? We present acase-study on the extent to which experimental contexts can improve LMs'robustness in performing property inheritance -- predicting semantic propertiesof novel concepts, a task that they have been previously shown to fail on. Uponcarefully controlling the nature of the in-context examples and theinstructions, our work reveals that they can indeed lead to non-trivialproperty inheritance behavior in LMs. However, this ability is inconsistent:with a minimal reformulation of the task, some LMs were found to pick up onshallow, non-semantic heuristics from their inputs, suggesting that thecomputational principles of semantic property inference are yet to be masteredby LMs.</description><author>Kanishka Misra, Allyson Ettinger, Kyle Mahowald</author><pubDate>Fri, 12 Jan 2024 15:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06640v1</guid></item><item><title>FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning</title><link>http://arxiv.org/abs/2309.14062v3</link><description>Exemplar-free class-incremental learning (CIL) poses several challenges sinceit prohibits the rehearsal of data from previous tasks and thus suffers fromcatastrophic forgetting. Recent approaches to incrementally learning theclassifier by freezing the feature extractor after the first task have gainedmuch attention. In this paper, we explore prototypical networks for CIL, whichgenerate new class prototypes using the frozen feature extractor and classifythe features based on the Euclidean distance to the prototypes. In an analysisof the feature distributions of classes, we show that classification based onEuclidean metrics is successful for jointly trained features. However, whenlearning from non-stationary data, we observe that the Euclidean metric issuboptimal and that feature distributions are heterogeneous. To address thischallenge, we revisit the anisotropic Mahalanobis distance for CIL. Inaddition, we empirically show that modeling the feature covariance relations isbetter than previous attempts at sampling features from normal distributionsand training a linear classifier. Unlike existing methods, our approachgeneralizes to both many- and few-shot CIL settings, as well as todomain-incremental settings. Interestingly, without updating the backbonenetwork, our method obtains state-of-the-art results on several standardcontinual learning benchmarks. Code is available athttps://github.com/dipamgoswami/FeCAM.</description><author>Dipam Goswami, Yuyang Liu, Bart≈Çomiej Twardowski, Joost van de Weijer</author><pubDate>Fri, 12 Jan 2024 15:35:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14062v3</guid></item><item><title>Vocabulary-free Image Classification</title><link>http://arxiv.org/abs/2306.00917v3</link><description>Recent advances in large vision-language models have revolutionized the imageclassification paradigm. Despite showing impressive zero-shot capabilities, apre-defined set of categories, a.k.a. the vocabulary, is assumed at test timefor composing the textual prompts. However, such assumption can be impracticalwhen the semantic context is unknown and evolving. We thus formalize a noveltask, termed as Vocabulary-free Image Classification (VIC), where we aim toassign to an input image a class that resides in an unconstrainedlanguage-induced semantic space, without the prerequisite of a knownvocabulary. VIC is a challenging task as the semantic space is extremely large,containing millions of concepts, with hard-to-discriminate fine-grainedcategories. In this work, we first empirically verify that representing thissemantic space by means of an external vision-language database is the mosteffective way to obtain semantically relevant content for classifying theimage. We then propose Category Search from External Databases (CaSED), amethod that exploits a pre-trained vision-language model and an externalvision-language database to address VIC in a training-free manner. CaSED firstextracts a set of candidate categories from captions retrieved from thedatabase based on their semantic similarity to the image, and then assigns tothe image the best matching candidate category according to the samevision-language model. Experiments on benchmark datasets validate that CaSEDoutperforms other complex vision-language frameworks, while being efficientwith much fewer parameters, paving the way for future research in thisdirection.</description><author>Alessandro Conti, Enrico Fini, Massimiliano Mancini, Paolo Rota, Yiming Wang, Elisa Ricci</author><pubDate>Fri, 12 Jan 2024 15:34:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00917v3</guid></item><item><title>Adversarial Examples are Misaligned in Diffusion Model Manifolds</title><link>http://arxiv.org/abs/2401.06637v1</link><description>In recent years, diffusion models (DMs) have drawn significant attention fortheir success in approximating data distributions, yielding state-of-the-artgenerative results. Nevertheless, the versatility of these models extendsbeyond their generative capabilities to encompass various vision applications,such as image inpainting, segmentation, adversarial robustness, among others.This study is dedicated to the investigation of adversarial attacks through thelens of diffusion models. However, our objective does not involve enhancing theadversarial robustness of image classifiers. Instead, our focus lies inutilizing the diffusion model to detect and analyze the anomalies introduced bythese attacks on images. To that end, we systematically examine the alignmentof the distributions of adversarial examples when subjected to the process oftransformation using diffusion models. The efficacy of this approach isassessed across CIFAR-10 and ImageNet datasets, including varying image sizesin the latter. The results demonstrate a notable capacity to discriminateeffectively between benign and attacked images, providing compelling evidencethat adversarial instances do not align with the learned manifold of the DMs.</description><author>Peter Lorenz, Ricard Durall, Jansi Keuper</author><pubDate>Fri, 12 Jan 2024 15:29:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06637v1</guid></item><item><title>Shape-IoU: More Accurate Metric considering Bounding Box Shape and Scale</title><link>http://arxiv.org/abs/2312.17663v2</link><description>As an important component of the detector localization branch, bounding boxregression loss plays a significant role in object detection tasks. Theexisting bounding box regression methods usually consider the geometricrelationship between the GT box and the predicted box, and calculate the lossby using the relative position and shape of the bounding boxes, while ignoringthe influence of inherent properties such as the shape and scale of thebounding boxes on bounding box regression. In order to make up for theshortcomings of existing research, this article proposes a bounding boxregression method that focuses on the shape and scale of the bounding boxitself. Firstly, we analyzed the regression characteristics of the boundingboxes and found that the shape and scale factors of the bounding boxesthemselves will have an impact on the regression results. Based on the aboveconclusions, we propose the Shape IoU method, which can calculate the loss byfocusing on the shape and scale of the bounding box itself, thereby making thebounding box regression more accurate. Finally, we validated our method througha large number of comparative experiments, which showed that our method caneffectively improve detection performance and outperform existing methods,achieving state-of-the-art performance in different detection tasks.Code isavailable at https://github.com/malagoutou/Shape-IoU</description><author>Hao Zhang, Shuaijie Zhang</author><pubDate>Fri, 12 Jan 2024 15:28:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17663v2</guid></item><item><title>CCFC: Bridging Federated Clustering and Contrastive Learning</title><link>http://arxiv.org/abs/2401.06634v1</link><description>Federated clustering, an essential extension of centralized clustering forfederated scenarios, enables multiple data-holding clients to collaborativelygroup data while keeping their data locally. In centralized scenarios,clustering driven by representation learning has made significant advancementsin handling high-dimensional complex data. However, the combination offederated clustering and representation learning remains underexplored. Tobridge this, we first tailor a cluster-contrastive model for learningclustering-friendly representations. Then, we harness this model as thefoundation for proposing a new federated clustering method, namedcluster-contrastive federated clustering (CCFC). Benefiting from representationlearning, the clustering performance of CCFC even double those of the bestbaseline methods in some cases. Compared to the most related baseline, thebenefit results in substantial NMI score improvements of up to 0.4155 on themost conspicuous case. Moreover, CCFC also shows superior performance inhandling device failures from a practical viewpoint.</description><author>Jie Yan, Jing Liu, Zhong-Yuan Zhang</author><pubDate>Fri, 12 Jan 2024 15:26:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06634v1</guid></item><item><title>Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential Recommendations</title><link>http://arxiv.org/abs/2401.06633v1</link><description>Retrieval models aim at selecting a small set of item candidates which matchthe preference of a given user. They play a vital role in large-scalerecommender systems since subsequent models such as rankers highly depend onthe quality of item candidates. However, most existing retrieval models employa single-round inference paradigm, which may not adequately capture the dynamicnature of user preferences and stuck in one area in the item space. In thispaper, we propose Ada-Retrieval, an adaptive multi-round retrieval paradigm forrecommender systems that iteratively refines user representations to bettercapture potential candidates in the full item space. Ada-Retrieval comprisestwo key modules: the item representation adapter and the user representationadapter, designed to inject context information into items' and users'representations. The framework maintains a model-agnostic design, allowingseamless integration with various backbone models such as RNNs or Transformers.We perform experiments on three widely used public datasets, incorporating fivepowerful sequential recommenders as backbone models. Our results demonstratethat Ada-Retrieval significantly enhances the performance of various basemodels, with consistent improvements observed across different datasets. Ourcode and data are publicly available at:https://github.com/ll0ruc/Ada-Retrieval.</description><author>Lei Li, Jianxun Lian, Xiao Zhou, Xing Xie</author><pubDate>Fri, 12 Jan 2024 15:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06633v1</guid></item><item><title>OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models</title><link>http://arxiv.org/abs/2401.06628v1</link><description>Advancing automated programming necessitates robust and comprehensive codegeneration benchmarks, yet current evaluation frameworks largely neglectobject-oriented programming (OOP) in favor of functional programming (FP),e.g., HumanEval and MBPP. To address this, our study introduces a pioneeringOOP-focused benchmark, featuring 431 Python programs that encompass essentialOOP concepts and features like classes and encapsulation methods. We propose anovel evaluation metric, pass@o, tailored for OOP, enhancing traditional pass@kmeasures. Our evaluation of 23 leading large language models (LLMs), includingboth general and code-specialized models, reveals three key insights: 1) pass@ooffers a more relevant and comprehensive assessment for OOP code generation; 2)Despite excelling in FP, code-specialized LLMs like WizardCoder lag in OOPcompared to models like ChatGPT; 3) The poor performance of all advanced LLMson our OOP benchmark highlights a critical need for improvements in this field.Our benchmark and scripts are publicly released at:https://github.com/alphadl/OOP-eval.</description><author>Shuai Wang, Liang Ding, Li Shen, Yong Luo, Bo Du, Dacheng Tao</author><pubDate>Fri, 12 Jan 2024 15:21:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06628v1</guid></item><item><title>TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models</title><link>http://arxiv.org/abs/2401.06620v1</link><description>There are 293 scripts representing over 7,000 languages in the written form.Due to various reasons, many closely related languages use different scripts,which poses difficulty for multilingual pretrained language models (mPLMs) inlearning crosslingual knowledge through lexical overlap. As a result, mPLMspresent a script barrier: representations from different scripts are located indifferent subspaces, which is a strong indicator of why crosslingual transferinvolving languages of different scripts shows sub-optimal performance. Toaddress this problem, we propose a simple framework TransliCo that containsTransliteration Contrastive Modeling (TCM) to fine-tune an mPLM by contrastingsentences in its training data and their transliterations in a unified script(Latn, in our case), which ensures uniformity in the representation space fordifferent scripts. Using Glot500-m, an mPLM pretrained on over 500 languages,as our source model, we find-tune it on a small portion (5\%) of its trainingdata, and refer to the resulting model as Furina. We show that Furina not onlybetter aligns representations from distinct scripts but also outperforms theoriginal Glot500-m on various crosslingual transfer tasks. Additionally, weachieve consistent improvement in a case study on the Indic group where thelanguages are highly related but use different scripts. We make our code andmodels publicly available.</description><author>Yihong Liu, Chunlan Ma, Haotian Ye, Hinrich Sch√ºtze</author><pubDate>Fri, 12 Jan 2024 15:12:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06620v1</guid></item><item><title>Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape Reconstruction and Tracking</title><link>http://arxiv.org/abs/2401.06614v1</link><description>We introduce Motion2VecSets, a 4D diffusion model for dynamic surfacereconstruction from point cloud sequences. While existing state-of-the-artmethods have demonstrated success in reconstructing non-rigid objects usingneural field representations, conventional feed-forward networks encounterchallenges with ambiguous observations from noisy, partial, or sparse pointclouds. To address these challenges, we introduce a diffusion model thatexplicitly learns the shape and motion distribution of non-rigid objectsthrough an iterative denoising process of compressed latent representations.The diffusion-based prior enables more plausible and probabilisticreconstructions when handling ambiguous inputs. We parameterize 4D dynamicswith latent vector sets instead of using a global latent. This novel 4Drepresentation allows us to learn local surface shape and deformation patterns,leading to more accurate non-linear motion capture and significantly improvinggeneralizability to unseen motions and identities. For more temporal-coherentobject tracking, we synchronously denoise deformation latent sets and exchangeinformation across multiple frames. To avoid the computational overhead, wedesign an interleaved space and time attention block to alternately aggregatedeformation latents along spatial and temporal domains. Extensive comparisonsagainst the state-of-the-art methods demonstrate the superiority of ourMotion2VecSets in 4D reconstruction from various imperfect observations,notably achieving a 19% improvement in Intersection over Union (IoU) comparedto CaDex for reconstructing unseen individuals from sparse point clouds on theDeformingThings4D-Animals dataset. More detailed information can be found athttps://vveicao.github.io/projects/Motion2VecSets/.</description><author>Wei Cao, Chang Luo, Biao Zhang, Matthias Nie√üner, Jiapeng Tang</author><pubDate>Fri, 12 Jan 2024 15:05:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06614v1</guid></item><item><title>EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search</title><link>http://arxiv.org/abs/2210.06015v3</link><description>Energy consumption from the selection, training, and deployment of deeplearning models has seen a significant uptick recently. This work aims tofacilitate the design of energy-efficient deep learning models that requireless computational resources and prioritize environmental sustainability byfocusing on the energy consumption. Neural architecture search (NAS) benefitsfrom tabular benchmarks, which evaluate NAS strategies cost-effectively throughprecomputed performance statistics. We advocate for including energy efficiencyas an additional performance criterion in NAS. To this end, we introduce anenhanced tabular benchmark encompassing data on energy consumption for variedarchitectures. The benchmark, designated as EC-NAS, has been made available inan open-source format to advance research in energy-conscious NAS. EC-NASincorporates a surrogate model to predict energy consumption, aiding indiminishing the energy expenditure of the dataset creation. Our findingsemphasize the potential of EC-NAS by leveraging multi-objective optimizationalgorithms, revealing a balance between energy usage and accuracy. Thissuggests the feasibility of identifying energy-lean architectures with littleor no compromise in performance.</description><author>Pedram Bakhtiarifard, Christian Igel, Raghavendra Selvan</author><pubDate>Fri, 12 Jan 2024 15:04:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06015v3</guid></item><item><title>MERA: A Comprehensive LLM Evaluation in Russian</title><link>http://arxiv.org/abs/2401.04531v2</link><description>Over the past few years, one of the most notable advancements in AI researchhas been in foundation models (FMs), headlined by the rise of language models(LMs). As the models' size increases, LMs demonstrate enhancements inmeasurable aspects and the development of new qualitative features. However,despite researchers' attention and the rapid growth in LM application, thecapabilities, limitations, and associated risks still need to be betterunderstood. To address these issues, we introduce an open Multimodal Evaluationof Russian-language Architectures (MERA), a new instruction benchmark forevaluating foundation models oriented towards the Russian language. Thebenchmark encompasses 21 evaluation tasks for generative models in 11 skilldomains and is designed as a black-box test to ensure the exclusion of dataleakage. The paper introduces a methodology to evaluate FMs and LMs in zero-and few-shot fixed instruction settings that can be extended to othermodalities. We propose an evaluation methodology, an open-source code base forthe MERA assessment, and a leaderboard with a submission system. We evaluateopen LMs as baselines and find that they are still far behind the human level.We publicly release MERA to guide forthcoming research, anticipategroundbreaking model features, standardize the evaluation procedure, andaddress potential societal drawbacks.</description><author>Alena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, Maria Tikhonova, Albina Akhmetgareeva, Anton Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid Sinev, Ulyana Isaeva, Katerina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin, Polina Mikhailova, Denis Dimitrov, Alexander Panchenko, Sergei Markov</author><pubDate>Fri, 12 Jan 2024 15:04:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.04531v2</guid></item><item><title>NAAQA: A Neural Architecture for Acoustic Question Answering</title><link>http://arxiv.org/abs/2106.06147v3</link><description>The goal of the Acoustic Question Answering (AQA) task is to answer afree-form text question about the content of an acoustic scene. It was inspiredby the Visual Question Answering (VQA) task. In this paper, based on thepreviously introduced CLEAR dataset, we propose a new benchmark for AQA, namelyCLEAR2, that emphasizes the specific challenges of acoustic inputs. Theseinclude handling of variable duration scenes, and scenes built with elementarysounds that differ between training and test set. We also introduce NAAQA, aneural architecture that leverages specific properties of acoustic inputs. Theuse of 1D convolutions in time and frequency to process 2D spectro-temporalrepresentations of acoustic content shows promising results and enablesreductions in model complexity. We show that time coordinate maps augmenttemporal localization capabilities which enhance performance of the network by~17 percentage points. On the other hand, frequency coordinate maps have littleinfluence on this task. NAAQA achieves 79.5% of accuracy on the AQA task with~4 times fewer parameters than the previously explored VQA model. We evaluatethe perfomance of NAAQA on an independent data set reconstructed from DAQA. Wealso test the addition of a MALiMo module in our model on both CLEAR2 and DAQA.We provide a detailed analysis of the results for the different question types.We release the code to produce CLEAR2 as well as NAAQA to foster research inthis newly emerging machine learning task.</description><author>Jerome Abdelnour, Jean Rouat, Giampiero Salvi</author><pubDate>Fri, 12 Jan 2024 14:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.06147v3</guid></item><item><title>SE(3) Equivariant Augmented Coupling Flows</title><link>http://arxiv.org/abs/2308.10364v5</link><description>Coupling normalizing flows allow for fast sampling and density evaluation,making them the tool of choice for probabilistic modeling of physical systems.However, the standard coupling architecture precludes endowing flows thatoperate on the Cartesian coordinates of atoms with the SE(3) and permutationinvariances of physical systems. This work proposes a coupling flow thatpreserves SE(3) and permutation equivariance by performing coordinate splitsalong additional augmented dimensions. At each layer, the flow maps atoms'positions into learned SE(3) invariant bases, where we apply standard flowtransformations, such as monotonic rational-quadratic splines, before returningto the original basis. Crucially, our flow preserves fast sampling and densityevaluation, and may be used to produce unbiased estimates of expectations withrespect to the target distribution via importance sampling. When trained on theDW4, LJ13, and QM9-positional datasets, our flow is competitive withequivariant continuous normalizing flows and diffusion models, while allowingsampling more than an order of magnitude faster. Moreover, to the best of ourknowledge, we are the first to learn the full Boltzmann distribution of alaninedipeptide by only modeling the Cartesian positions of its atoms. Lastly, wedemonstrate that our flow can be trained to approximately sample from theBoltzmann distribution of the DW4 and LJ13 particle systems using only theirenergy functions.</description><author>Laurence I. Midgley, Vincent Stimper, Javier Antor√°n, Emile Mathieu, Bernhard Sch√∂lkopf, Jos√© Miguel Hern√°ndez-Lobato</author><pubDate>Fri, 12 Jan 2024 14:49:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10364v5</guid></item><item><title>Identifying Policy Gradient Subspaces</title><link>http://arxiv.org/abs/2401.06604v1</link><description>Policy gradient methods hold great potential for solving complex continuouscontrol tasks. Still, their training efficiency can be improved by exploitingstructure within the optimization problem. Recent work indicates thatsupervised learning can be accelerated by leveraging the fact that gradientslie in a low-dimensional and slowly-changing subspace. In this paper, weconduct a thorough evaluation of this phenomenon for two popular deep policygradient methods on various simulated benchmark tasks. Our results demonstratethe existence of such gradient subspaces despite the continuously changing datadistribution inherent to reinforcement learning. These findings revealpromising directions for future work on more efficient reinforcement learning,e.g., through improving parameter-space exploration or enabling second-orderoptimization.</description><author>Jan Schneider, Pierre Schumacher, Simon Guist, Le Chen, Daniel H√§ufle, Bernhard Sch√∂lkopf, Dieter B√ºchler</author><pubDate>Fri, 12 Jan 2024 14:40:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06604v1</guid></item><item><title>DDPM-CD: Denoising Diffusion Probabilistic Models as Feature Extractors for Change Detection</title><link>http://arxiv.org/abs/2206.11892v3</link><description>Remote sensing change detection is crucial for understanding the dynamics ofour planet's surface, facilitating the monitoring of environmental changes,evaluating human impact, predicting future trends, and supportingdecision-making. In this work, we introduce a novel approach for changedetection that can leverage off-the-shelf, unlabeled remote sensing images inthe training process by pre-training a Denoising Diffusion Probabilistic Model(DDPM) - a class of generative models used in image synthesis. DDPMs learn thetraining data distribution by gradually converting training images into aGaussian distribution using a Markov chain. During inference (i.e., sampling),they can generate a diverse set of samples closer to the training distribution,starting from Gaussian noise, achieving state-of-the-art image synthesisresults. However, in this work, our focus is not on image synthesis but onutilizing it as a pre-trained feature extractor for the downstream applicationof change detection. Specifically, we fine-tune a lightweight change classifierutilizing the feature representations produced by the pre-trained DDPMalongside change labels. Experiments conducted on the LEVIR-CD, WHU-CD,DSIFN-CD, and CDD datasets demonstrate that the proposed DDPM-CD methodsignificantly outperforms the existing state-of-the-art change detectionmethods in terms of F1 score, IoU, and overall accuracy, highlighting thepivotal role of pre-trained DDPM as a feature extractor for downstreamapplications. We have made both the code and pre-trained models available athttps://github.com/wgcban/ddpm-cd</description><author>Wele Gedara Chaminda Bandara, Nithin Gopalakrishnan Nair, Vishal M. Patel</author><pubDate>Fri, 12 Jan 2024 14:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11892v3</guid></item><item><title>Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study</title><link>http://arxiv.org/abs/2401.06603v1</link><description>Large Language Models (LLMs) have demonstrated remarkable capabilities forreinforcement learning (RL) models, such as planning and reasoningcapabilities. However, the problems of LLMs and RL model collaboration stillneed to be solved. In this study, we employ a teacher-student learningframework to tackle these problems, specifically by offering feedback for LLMsusing RL models and providing high-level information for RL models with LLMs ina cooperative multi-agent setting. Within this framework, the LLM acts as ateacher, while the RL model acts as a student. The two agents cooperativelyassist each other through a process of recursive help, such as "I help you helpI help." The LLM agent supplies abstract information to the RL agent, enablingefficient exploration and policy improvement. In turn, the RL agent offersfeedback to the LLM agent, providing valuable, real-time information that helpsgenerate more useful tokens. This bi-directional feedback loop promotesoptimization, exploration, and mutual improvement for both agents, enablingthem to accomplish increasingly challenging tasks. Remarkably, we propose apractical algorithm to address the problem and conduct empirical experiments toevaluate the effectiveness of our method.</description><author>Shangding Gu</author><pubDate>Fri, 12 Jan 2024 14:35:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06603v1</guid></item><item><title>Multiplayer Bandit Learning, from Competition to Cooperation</title><link>http://arxiv.org/abs/1908.01135v4</link><description>The stochastic multi-armed bandit model captures the tradeoff betweenexploration and exploitation. We study the effects of competition andcooperation on this tradeoff. Suppose there are $k$ arms and two players, Aliceand Bob. In every round, each player pulls an arm, receives the resultingreward, and observes the choice of the other player but not their reward.Alice's utility is $\Gamma_A + \lambda \Gamma_B$ (and similarly for Bob), where$\Gamma_A$ is Alice's total reward and $\lambda \in [-1, 1]$ is a cooperationparameter. At $\lambda = -1$ the players are competing in a zero-sum game, at$\lambda = 1$, they are fully cooperating, and at $\lambda = 0$, they areneutral: each player's utility is their own reward. The model is related to theeconomics literature on strategic experimentation, where usually playersobserve each other's rewards. With discount factor $\beta$, the Gittins index reduces the one-playerproblem to the comparison between a risky arm, with a prior $\mu$, and apredictable arm, with success probability $p$. The value of $p$ where theplayer is indifferent between the arms is the Gittins index $g = g(\mu,\beta) &gt;m$, where $m$ is the mean of the risky arm. We show that competing players explore less than a single player: there is$p^* \in (m, g)$ so that for all $p &gt; p^*$, the players stay at the predictablearm. However, the players are not myopic: they still explore for some $p &gt; m$.On the other hand, cooperating players explore more than a single player. Wealso show that neutral players learn from each other, receiving strictly highertotal rewards than they would playing alone, for all $ p\in (p^*, g)$, where$p^*$ is the threshold from the competing case. Finally, we show that competing and neutral players eventually settle on thesame arm in every Nash equilibrium, while this can fail for cooperatingplayers.</description><author>Simina Br√¢nzei, Yuval Peres</author><pubDate>Fri, 12 Jan 2024 14:32:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1908.01135v4</guid></item><item><title>Stable Nonconvex-Nonconcave Training via Linear Interpolation</title><link>http://arxiv.org/abs/2310.13459v2</link><description>This paper presents a theoretical analysis of linear interpolation as aprincipled method for stabilizing (large-scale) neural network training. Weargue that instabilities in the optimization process are often caused by thenonmonotonicity of the loss landscape and show how linear interpolation canhelp by leveraging the theory of nonexpansive operators. We construct a newoptimization scheme called relaxed approximate proximal point (RAPP), which isthe first 1-SCLI method to achieve last iterate convergence rates for$\rho$-comonotone problems while only requiring $\rho &gt; -\tfrac{1}{2L}$. Theconstruction extends to constrained and regularized settings. By replacing theinner optimizer in RAPP we rediscover the family of Lookahead algorithms forwhich we establish convergence in cohypomonotone problems even when the baseoptimizer is taken to be gradient descent ascent. The range of cohypomonotoneproblems in which Lookahead converges is further expanded by exploiting thatLookahead inherits the properties of the base optimizer. We corroborate theresults with experiments on generative adversarial networks which demonstratesthe benefits of the linear interpolation present in both RAPP and Lookahead.</description><author>Thomas Pethick, Wanyun Xie, Volkan Cevher</author><pubDate>Fri, 12 Jan 2024 14:30:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13459v2</guid></item><item><title>Every Node is Different: Dynamically Fusing Self-Supervised Tasks for Attributed Graph Clustering</title><link>http://arxiv.org/abs/2401.06595v1</link><description>Attributed graph clustering is an unsupervised task that partitions nodesinto different groups. Self-supervised learning (SSL) shows great potential inhandling this task, and some recent studies simultaneously learn multiple SSLtasks to further boost performance. Currently, different SSL tasks are assignedthe same set of weights for all graph nodes. However, we observe that somegraph nodes whose neighbors are in different groups require significantlydifferent emphases on SSL tasks. In this paper, we propose to dynamically learnthe weights of SSL tasks for different nodes and fuse the embeddings learnedfrom different SSL tasks to boost performance. We design an innovative graphclustering approach, namely Dynamically Fusing Self-Supervised Learning(DyFSS). Specifically, DyFSS fuses features extracted from diverse SSL tasksusing distinct weights derived from a gating network. To effectively learn thegating network, we design a dual-level self-supervised strategy thatincorporates pseudo labels and the graph structure. Extensive experiments onfive datasets show that DyFSS outperforms the state-of-the-art multi-task SSLmethods by up to 8.66% on the accuracy metric. The code of DyFSS is availableat: https://github.com/q086/DyFSS.</description><author>Pengfei Zhu, Qian Wang, Yu Wang, Jialu Li, Qinghua Hu</author><pubDate>Fri, 12 Jan 2024 14:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06595v1</guid></item><item><title>Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation</title><link>http://arxiv.org/abs/2401.06591v1</link><description>Assessing long-form responses generated by Vision-Language Models (VLMs) ischallenging. It not only requires checking whether the VLM follows the giveninstruction but also verifying whether the text output is properly grounded onthe given image. Inspired by the recent approach of evaluating LMs with LMs, inthis work, we propose to evaluate VLMs with VLMs. For this purpose, we presenta new feedback dataset called the Perception Collection, encompassing 15Kcustomized score rubrics that users might care about during assessment. Usingthe Perception Collection, we train Prometheus-Vision, the first open-sourceVLM evaluator model that can understand the user-defined score criteria duringevaluation. Prometheus-Vision shows the highest Pearson correlation with humanevaluators and GPT-4V among open-source models, showing its effectiveness fortransparent and accessible evaluation of VLMs. We open-source our code,dataset, and model at https://github.com/kaistAI/prometheus-vision</description><author>Seongyun Lee, Seungone Kim, Sue Hyun Park, Geewook Kim, Minjoon Seo</author><pubDate>Fri, 12 Jan 2024 14:19:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06591v1</guid></item><item><title>Accelerating the Global Aggregation of Local Explanations</title><link>http://arxiv.org/abs/2312.07991v3</link><description>Local explanation methods highlight the input tokens that have a considerableimpact on the outcome of classifying the document at hand. For example, theAnchor algorithm applies a statistical analysis of the sensitivity of theclassifier to changes in the token. Aggregating local explanations over adataset provides a global explanation of the model. Such aggregation aims todetect words with the most impact, giving valuable insights about the model,like what it has learned in training and which adversarial examples expose itsweaknesses. However, standard aggregation methods bear a high computationalcost: a na\"ive implementation applies a costly algorithm to each token of eachdocument, and hence, it is infeasible for a simple user running in the scope ofa short analysis session. % We devise techniques for accelerating the globalaggregation of the Anchor algorithm. Specifically, our goal is to compute a setof top-$k$ words with the highest global impact according to differentaggregation functions. Some of our techniques are lossless and some are lossy.We show that for a very mild loss of quality, we are able to accelerate thecomputation by up to 30$\times$, reducing the computation from hours tominutes. We also devise and study a probabilistic model that accounts for noisein the Anchor algorithm and diminishes the bias toward words that are frequentyet low in impact.</description><author>Alon Mor, Yonatan Belinkov, Benny Kimelfeld</author><pubDate>Fri, 12 Jan 2024 14:18:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07991v3</guid></item><item><title>Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data</title><link>http://arxiv.org/abs/2311.17492v2</link><description>The Manchu language, with its roots in the historical Manchurian region ofNortheast China, is now facing a critical threat of extinction, as there arevery few speakers left. In our efforts to safeguard the Manchu language, weintroduce Mergen, the first-ever attempt at a Manchu-Korean Machine Translation(MT) model. To develop this model, we utilize valuable resources such as theManwen Laodang(a historical book) and a Manchu-Korean dictionary. Due to thescarcity of a Manchu-Korean parallel dataset, we expand our data by employingword replacement guided by GloVe embeddings, trained on both monolingual andparallel texts. Our approach is built around an encoder-decoder neural machinetranslation model, incorporating a bi-directional Gated Recurrent Unit (GRU)layer. The experiments have yielded promising results, showcasing a significantenhancement in Manchu-Korean translation, with a remarkable 20-30 pointincrease in the BLEU score.</description><author>Jean Seo, Sungjoo Byun, Minha Kang, Sangah Lee</author><pubDate>Fri, 12 Jan 2024 14:18:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17492v2</guid></item><item><title>Dynamic Behaviour of Connectionist Speech Recognition with Strong Latency Constraints</title><link>http://arxiv.org/abs/2401.06588v1</link><description>This paper describes the use of connectionist techniques in phonetic speechrecognition with strong latency constraints. The constraints are imposed by thetask of deriving the lip movements of a synthetic face in real time from thespeech signal, by feeding the phonetic string into an articulatory synthesiser.Particular attention has been paid to analysing the interaction between thetime evolution model learnt by the multi-layer perceptrons and the transitionmodel imposed by the Viterbi decoder, in different latency conditions. Twoexperiments were conducted in which the time dependencies in the language model(LM) were controlled by a parameter. The results show a strong interactionbetween the three factors involved, namely the neural network topology, thelength of time dependencies in the LM and the decoder latency.</description><author>Giampiero Salvi</author><pubDate>Fri, 12 Jan 2024 14:10:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06588v1</guid></item><item><title>TraffNet: Learning Causality of Traffic Generation for What-if Prediction</title><link>http://arxiv.org/abs/2303.15954v5</link><description>Real-time what-if traffic prediction is crucial for decision making inintelligent traffic management and control. Although current deep learningmethods demonstrate significant advantages in traffic prediction, they arepowerless in what-if traffic prediction due to their nature ofcorrelation-based. Here, we present a simple deep learning framework calledTraffNet that learns the mechanisms of traffic generation for what-ifprediction from vehicle trajectory data. First, we use a heterogeneous graph torepresent the road network, allowing the model to incorporate causal featuresof traffic flows, such as Origin-Destination (OD) demands and routes. Next, wepropose a method for learning segment representations, which involves modelingthe process of assigning OD demands onto the road network. The learned segmentrepresentations effectively encapsulate the intricate causes of trafficgeneration, facilitating downstream what-if traffic prediction. Finally, weconduct experiments on synthetic datasets to evaluate the effectiveness ofTraffNet. The code and datasets of TraffNet is available athttps://github.com/mayunyi-1999/TraffNet_code.git.</description><author>Ming Xu, Qiang Ai, Ruimin Li, Yunyi Ma, Geqi Qi, Xiangfu Meng, Haibo Jin</author><pubDate>Fri, 12 Jan 2024 14:08:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15954v5</guid></item><item><title>On the Three Demons in Causality in Finance: Time Resolution, Nonstationarity, and Latent Factors</title><link>http://arxiv.org/abs/2401.05414v2</link><description>Financial data is generally time series in essence and thus suffers fromthree fundamental issues: the mismatch in time resolution, the time-varyingproperty of the distribution - nonstationarity, and causal factors that areimportant but unknown/unobserved. In this paper, we follow a causal perspectiveto systematically look into these three demons in finance. Specifically, wereexamine these issues in the context of causality, which gives rise to a noveland inspiring understanding of how the issues can be addressed. Following thisperspective, we provide systematic solutions to these problems, which hopefullywould serve as a foundation for future research in the area.</description><author>Xinshuai Dong, Haoyue Dai, Yewen Fan, Songyao Jin, Sathyamoorthy Rajendran, Kun Zhang</author><pubDate>Fri, 12 Jan 2024 14:03:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05414v2</guid></item><item><title>Mapping Transformer Leveraged Embeddings for Cross-Lingual Document Representation</title><link>http://arxiv.org/abs/2401.06583v1</link><description>Recommendation systems, for documents, have become tools to find relevantcontent on the Web. However, these systems have limitations when it comes torecommending documents in languages different from the query language, whichmeans they might overlook resources in non-native languages. This researchfocuses on representing documents across languages by using TransformerLeveraged Document Representations (TLDRs) that are mapped to a cross-lingualdomain. Four multilingual pre-trained transformer models (mBERT, mT5 XLMRoBERTa, ErnieM) were evaluated using three mapping methods across 20 languagepairs representing combinations of five selected languages of the EuropeanUnion. Metrics like Mate Retrieval Rate and Reciprocal Rank were used tomeasure the effectiveness of mapped TLDRs compared to non-mapped ones. Theresults highlight the power of cross-lingual representations achieved throughpre-trained transformers and mapping approaches suggesting a promisingdirection for expanding beyond language connections, between two specificlanguages.</description><author>Tsegaye Misikir Tashu, Eduard-Raul Kontos, Matthia Sabatelli, Matias Valdenegro-Toro</author><pubDate>Fri, 12 Jan 2024 14:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06583v1</guid></item><item><title>Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks</title><link>http://arxiv.org/abs/2310.02772v4</link><description>In this article, we propose a new paradigm for training spiking neuralnetworks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs areenergy-efficient but difficult to train. Consequently, many researchers haveproposed various methods to solve this problem, among which online trainingthrough time (OTTT) is a method that allows inferring at each time step whilesuppressing the memory cost. However, to compute efficiently on GPUs, OTTTrequires operations with spike trains and weighted summation of spike trainsduring forwarding. In addition, OTTT has shown a relationship with the SpikeRepresentation, an alternative training method, though theoretical agreementwith Spike Representation has yet to be proven. Our proposed method can solvethese problems; namely, SAF can halve the number of operations during theforward process, and it can be theoretically proven that SAF is consistent withthe Spike Representation and OTTT, respectively. Furthermore, we confirmed theabove contents through experiments and showed that it is possible to reducememory and training time while maintaining accuracy.</description><author>Ryuji Saiin, Tomoya Shirakawa, Sota Yoshihara, Yoshihide Sawada, Hiroyuki Kusumoto</author><pubDate>Fri, 12 Jan 2024 13:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02772v4</guid></item><item><title>Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation</title><link>http://arxiv.org/abs/2311.08640v2</link><description>We study semi-supervised sequence generation tasks where labeled data are tooscarce to effectively finetune a model and at the same time few-shot promptingof a large language model (LLM) has suboptimal performance. This happens when atask, such as parsing, is expensive to annotate and also unfamiliar to apretrained LLM. In this paper, we present a discovery that student modelsdistilled from an in-context learned LLM can often generalize better than theirteacher on such tasks. Leveraging this finding, we present a new method --multistage collaborative knowledge distillation from an LLM (MCKD) -- for suchtasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeleddata. At each intermediate knowledge distillation (KD) stage, a new pair ofstudents is trained on disjoint partitions of the pseudolabeled data. Eachstudent then produces new and improved pseudolabels for its unseen partition tobe used in the next stage of distillation. We demonstrate the advantage ofmultistage cross-partition labeling on several syntactic and semantic parsingtasks. On CRAFT biomedical parsing, for example, 3-stage MCKD with 50 labeledexamples outperforms the prompted LLM and vanilla KD by 7.5% and 3.7% parsingF1, respectively, and matches the performance of supervised finetuning with 500examples.</description><author>Jiachen Zhao, Wenlong Zhao, Andrew Drozdov, Benjamin Rozonoyer, Md Arafat Sultan, Jay-Yoon Lee, Mohit Iyyer, Andrew McCallum</author><pubDate>Fri, 12 Jan 2024 13:57:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08640v2</guid></item><item><title>360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model</title><link>http://arxiv.org/abs/2401.06578v1</link><description>360-degree panoramic videos recently attract more interest in both studiesand applications, courtesy of the heightened immersive experiences theyengender. Due to the expensive cost of capturing 360-degree panoramic videos,generating desirable panoramic videos by given prompts is urgently required.Recently, the emerging text-to-video (T2V) diffusion methods demonstratenotable effectiveness in standard video generation. However, due to thesignificant gap in content and motion patterns between panoramic and standardvideos, these methods encounter challenges in yielding satisfactory 360-degreepanoramic videos. In this paper, we propose a controllable panorama videogeneration pipeline named 360-Degree Video Diffusion model (360DVD) forgenerating panoramic videos based on the given prompts and motion conditions.Concretely, we introduce a lightweight module dubbed 360-Adapter and assisted360 Enhancement Techniques to transform pre-trained T2V models for 360-degreevideo generation. We further propose a new panorama dataset named WEB360consisting of 360-degree video-text pairs for training 360DVD, addressing theabsence of captioned panoramic video datasets. Extensive experimentsdemonstrate the superiority and effectiveness of 360DVD for panorama videogeneration. The code and dataset will be released soon.</description><author>Qian Wang, Weiqi Li, Chong Mou, Xinhua Cheng, Jian Zhang</author><pubDate>Fri, 12 Jan 2024 13:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06578v1</guid></item><item><title>Tripletformer for Probabilistic Interpolation of Irregularly sampled Time Series</title><link>http://arxiv.org/abs/2210.02091v2</link><description>Irregularly sampled time series data with missing values is observed in manyfields like healthcare, astronomy, and climate science. Interpolation of thesetypes of time series is crucial for tasks such as root cause analysis andmedical diagnosis, as well as for smoothing out irregular or noisy data. Toaddress this challenge, we present a novel encoder-decoder architecture called"Tripletformer" for probabilistic interpolation of irregularly sampled timeseries with missing values. This attention-based model operates on sets ofobservations, where each element is composed of a triple of time, channel, andvalue. The encoder and decoder of the Tripletformer are designed with attentionlayers and fully connected layers, enabling the model to effectively processthe presented set elements. We evaluate the Tripletformer against a range ofbaselines on multiple real-world and synthetic datasets and show that itproduces more accurate and certain interpolations. Results indicate animprovement in negative loglikelihood error by up to 32% on real-world datasetsand 85% on synthetic datasets when using the Tripletformer compared to the nextbest model.</description><author>Vijaya Krishna Yalavarthi, Johannes Burchert, Lars Schmidt-thieme</author><pubDate>Fri, 12 Jan 2024 13:43:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.02091v2</guid></item><item><title>Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation</title><link>http://arxiv.org/abs/2401.06568v1</link><description>Large Language Models (LLMs) have achieved remarkable results in the machinetranslation evaluation task, yet there remains a gap in knowledge regarding howthey utilize the provided data to conduct evaluations. This study aims toexplore how LLMs leverage source and reference information in evaluatingtranslations, with the ultimate goal of better understanding the workingmechanism of LLMs. To this end, we design the controlled experiments acrossvarious input modes and model types, and employ both coarse-grained andfine-grained prompts to discern the utility of source versus referenceinformation. Surprisingly, we find that reference information significantlyenhances the evaluation accuracy, while source information sometimes iscounterproductive, indicating a lack of cross-lingual capability when usingLLMs to evaluate translations. We further conduct a meta-evaluation fortranslation error detection of LLMs, observing a similar phenomenon. Thesefindings also suggest a potential research direction for LLMs that fullyexploits the cross-lingual capability of LLMs to achieve better performance inmachine translation evaluation tasks.</description><author>Xu Huang, Zhirui Zhang, Xiang Geng, Yichao Du, Jiajun Chen, Shujian Huang</author><pubDate>Fri, 12 Jan 2024 13:23:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06568v1</guid></item><item><title>Maximum Causal Entropy Inverse Reinforcement Learning for Mean-Field Games</title><link>http://arxiv.org/abs/2401.06566v1</link><description>In this paper, we introduce the maximum casual entropy Inverse ReinforcementLearning (IRL) problem for discrete-time mean-field games (MFGs) under aninfinite-horizon discounted-reward optimality criterion. The state space of atypical agent is finite. Our approach begins with a comprehensive review of themaximum entropy IRL problem concerning deterministic and stochastic Markovdecision processes (MDPs) in both finite and infinite-horizon scenarios.Subsequently, we formulate the maximum casual entropy IRL problem for MFGs - anon-convex optimization problem with respect to policies. Leveraging the linearprogramming formulation of MDPs, we restructure this IRL problem into a convexoptimization problem and establish a gradient descent algorithm to compute theoptimal solution with a rate of convergence. Finally, we present a newalgorithm by formulating the MFG problem as a generalized Nash equilibriumproblem (GNEP), which is capable of computing the mean-field equilibrium (MFE)for the forward RL problem. This method is employed to produce data for anumerical example. We note that this novel algorithm is also applicable togeneral MFE computations.</description><author>Berkay Anahtarci, Can Deha Kariksiz, Naci Saldi</author><pubDate>Fri, 12 Jan 2024 13:22:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06566v1</guid></item><item><title>Valid causal inference with unobserved confounding in high-dimensional settings</title><link>http://arxiv.org/abs/2401.06564v1</link><description>Various methods have recently been proposed to estimate causal effects withconfidence intervals that are uniformly valid over a set of data generatingprocesses when high-dimensional nuisance models are estimated bypost-model-selection or machine learning estimators. These methods typicallyrequire that all the confounders are observed to ensure identification of theeffects. We contribute by showing how valid semiparametric inference can beobtained in the presence of unobserved confounders and high-dimensionalnuisance models. We propose uncertainty intervals which allow for unobservedconfounding, and show that the resulting inference is valid when the amount ofunobserved confounding is small relative to the sample size; the latter isformalized in terms of convergence rates. Simulation experiments illustrate thefinite sample properties of the proposed intervals and investigate analternative procedure that improves the empirical coverage of the intervalswhen the amount of unobserved confounding is large. Finally, a case study onthe effect of smoking during pregnancy on birth weight is used to illustratethe use of the methods introduced to perform a sensitivity analysis tounobserved confounding.</description><author>Niloofar Moosavi, Tetiana Gorbach, Xavier de Luna</author><pubDate>Fri, 12 Jan 2024 13:21:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06564v1</guid></item><item><title>Resource-Efficient Gesture Recognition using Low-Resolution Thermal Camera via Spiking Neural Networks and Sparse Segmentation</title><link>http://arxiv.org/abs/2401.06563v1</link><description>This work proposes a novel approach for hand gesture recognition using aninexpensive, low-resolution (24 x 32) thermal sensor processed by a SpikingNeural Network (SNN) followed by Sparse Segmentation and feature-based gestureclassification via Robust Principal Component Analysis (R-PCA). Compared to theuse of standard RGB cameras, the proposed system is insensitive to lightingvariations while being significantly less expensive compared to high-frequencyradars, time-of-flight cameras and high-resolution thermal sensors previouslyused in literature. Crucially, this paper shows that the innovative use of therecently proposed Monostable Multivibrator (MMV) neural networks as a new classof SNN achieves more than one order of magnitude smaller memory and computecomplexity compared to deep learning approaches, while reaching a top gesturerecognition accuracy of 93.9% using a 5-class thermal camera dataset acquiredin a car cabin, within an automotive context. Our dataset is released forhelping future research.</description><author>Ali Safa, Wout Mommen, Lars Keuninckx</author><pubDate>Fri, 12 Jan 2024 13:20:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06563v1</guid></item><item><title>EdgeFace: Efficient Face Recognition Model for Edge Devices</title><link>http://arxiv.org/abs/2307.01838v2</link><description>In this paper, we present EdgeFace, a lightweight and efficient facerecognition network inspired by the hybrid architecture of EdgeNeXt. Byeffectively combining the strengths of both CNN and Transformer models, and alow rank linear layer, EdgeFace achieves excellent face recognition performanceoptimized for edge devices. The proposed EdgeFace network not only maintainslow computational costs and compact storage, but also achieves high facerecognition accuracy, making it suitable for deployment on edge devices.Extensive experiments on challenging benchmark face datasets demonstrate theeffectiveness and efficiency of EdgeFace in comparison to state-of-the-artlightweight models and deep face recognition models. Our EdgeFace model with1.77M parameters achieves state of the art results on LFW (99.73%), IJB-B(92.67%), and IJB-C (94.85%), outperforming other efficient models with largercomputational complexities. The code to replicate the experiments will be madeavailable publicly.</description><author>Anjith George, Christophe Ecabert, Hatef Otroshi Shahreza, Ketan Kotwal, Sebastien Marcel</author><pubDate>Fri, 12 Jan 2024 13:19:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01838v2</guid></item><item><title>Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender</title><link>http://arxiv.org/abs/2401.06561v1</link><description>Aligning large language models (LLMs) with human values, particularly in theface of stealthy and complex jailbreaks, presents a formidable challenge. Inthis study, we present a simple yet highly effective defense strategy, i.e.,Intention Analysis Prompting (IAPrompt). The principle behind is to triggerLLMs' inherent self-correct and improve ability through a two-stage process: 1)essential intention analysis, and 2) policy-aligned response. Notably, IAPromptis an inference-only method, thus could enhance the safety of LLMs withoutcompromising their helpfulness. Extensive experiments on SAP200 and DANbenchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show thatIAPrompt could consistently and significantly reduce the harmfulness inresponse (averagely -46.5% attack success rate) and maintain the generalhelpfulness. Further analyses present some insights into how our method works.To facilitate reproducibility, We release our code and scripts at:https://github.com/alphadl/SafeLLM_with_IntentionAnalysis</description><author>Yuqi Zhang, Liang Ding, Lefei Zhang, Dacheng Tao</author><pubDate>Fri, 12 Jan 2024 13:15:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06561v1</guid></item><item><title>A General Benchmark Framework is Dynamic Graph Neural Network Need</title><link>http://arxiv.org/abs/2401.06559v1</link><description>Dynamic graph learning is crucial for modeling real-world systems withevolving relationships and temporal dynamics. However, the lack of a unifiedbenchmark framework in current research has led to inaccurate evaluations ofdynamic graph models. This paper highlights the significance of dynamic graphlearning and its applications in various domains. It emphasizes the need for astandardized benchmark framework that captures temporal dynamics, evolvinggraph structures, and downstream task requirements. Establishing a unifiedbenchmark will help researchers understand the strengths and limitations ofexisting models, foster innovation, and advance dynamic graph learning. Inconclusion, this paper identifies the lack of a standardized benchmarkframework as a current limitation in dynamic graph learning research . Such aframework will facilitate accurate model evaluation, drive advancements indynamic graph learning techniques, and enable the development of more effectivemodels for real-world applications.</description><author>Yusen Zhang</author><pubDate>Fri, 12 Jan 2024 13:12:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06559v1</guid></item><item><title>State-of-the-art generalisation research in NLP: A taxonomy and review</title><link>http://arxiv.org/abs/2210.03050v4</link><description>The ability to generalise well is one of the primary desiderata of naturallanguage processing (NLP). Yet, what 'good generalisation' entails and how itshould be evaluated is not well understood, nor are there any evaluationstandards for generalisation. In this paper, we lay the groundwork to addressboth of these issues. We present a taxonomy for characterising andunderstanding generalisation research in NLP. Our taxonomy is based on anextensive literature review of generalisation research, and contains five axesalong which studies can differ: their main motivation, the type ofgeneralisation they investigate, the type of data shift they consider, thesource of this data shift, and the locus of the shift within the modellingpipeline. We use our taxonomy to classify over 400 papers that testgeneralisation, for a total of more than 600 individual experiments.Considering the results of this review, we present an in-depth analysis thatmaps out the current state of generalisation research in NLP, and we makerecommendations for which areas might deserve attention in the future. Alongwith this paper, we release a webpage where the results of our review can bedynamically explored, and which we intend to update as new NLP generalisationstudies are published. With this work, we aim to take steps towards makingstate-of-the-art generalisation testing the new status quo in NLP.</description><author>Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Arabella Sinclair, Dennis Ulmer, Florian Schottmann, Khuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske, Ryan Cotterell, Zhijing Jin</author><pubDate>Fri, 12 Jan 2024 13:11:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03050v4</guid></item><item><title>Treatment-Aware Hyperbolic Representation Learning for Causal Effect Estimation with Social Networks</title><link>http://arxiv.org/abs/2401.06557v1</link><description>Estimating the individual treatment effect (ITE) from observational data is acrucial research topic that holds significant value across multiple domains.How to identify hidden confounders poses a key challenge in ITE estimation.Recent studies have incorporated the structural information of social networksto tackle this challenge, achieving notable advancements. However, thesemethods utilize graph neural networks to learn the representation of hiddenconfounders in Euclidean space, disregarding two critical issues: (1) thesocial networks often exhibit a scalefree structure, while Euclidean embeddingssuffer from high distortion when used to embed such graphs, and (2) eachego-centric network within a social network manifests a treatment-relatedcharacteristic, implying significant patterns of hidden confounders. To addressthese issues, we propose a novel method called Treatment-Aware HyperbolicRepresentation Learning (TAHyper). Firstly, TAHyper employs the hyperbolicspace to encode the social networks, thereby effectively reducing thedistortion of confounder representation caused by Euclidean embeddings.Secondly, we design a treatment-aware relationship identification module thatenhances the representation of hidden confounders by identifying whether anindividual and her neighbors receive the same treatment. Extensive experimentson two benchmark datasets are conducted to demonstrate the superiority of ourmethod.</description><author>Ziqiang Cui, Xing Tang, Yang Qiao, Bowei He, Liang Chen, Xiuqiang He, Chen Ma</author><pubDate>Fri, 12 Jan 2024 13:02:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06557v1</guid></item><item><title>Multimodal Learning for detecting urban functional zones using remote sensing image and multi-semantic information</title><link>http://arxiv.org/abs/2401.06550v1</link><description>Urban area-of-interest (AOI) refers to an integrated urban functional zonewith defined boundaries. The rapid development of urban commerce has resultedin an increased demand for more precise requirements in defining AOIs. However,existing research primarily concentrates on broad AOI mining for urban planningor regional economic analysis, failing to cater to the precise requirements ofmobile Internet online-to-offline businesses. These businesses necessitateaccuracy down to a specific community, school, or hospital. In this paper, wepropose an end-to-end multimodal deep learning algorithm for detecting AOIfence polygon using remote sensing images and multi-semantics referenceinformation. We then evaluate its timeliness through a cascaded module thatincorporates dynamic human mobility and logistics address information.Specifically, we begin by selecting a point-of-interest (POI) of specificcategory, and use it to recall corresponding remote sensing images, nearbyPOIs, road nodes, human mobility, and logistics addresses to build a multimodaldetection model based on transformer encoder-decoder architecture, titledAOITR. In the model, in addition to the remote sensing images, multi-semanticinformation including core POI and road nodes is embedded and reorganized asthe query content part for the transformer decoder to generate the AOI polygon.Meanwhile, relatively dynamic distribution features of human mobility, nearbyPOIs, and logistics addresses are used for AOI reliability evaluation through acascaded feedforward network. The experimental results demonstrate that ouralgorithm significantly outperforms two existing methods.</description><author>Chuanji Shi, Yingying Zhang, Jiaotuan Wang, Qiqi Zhu</author><pubDate>Fri, 12 Jan 2024 12:54:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06550v1</guid></item><item><title>Enhancing Consistency and Mitigating Bias: A Data Replay Approach for Incremental Learning</title><link>http://arxiv.org/abs/2401.06548v1</link><description>Deep learning systems are prone to catastrophic forgetting when learning froma sequence of tasks, where old data from experienced tasks is unavailable whenlearning from a new task. To mitigate the problem, a line of methods propose toreplay the data of experienced tasks when learning new tasks. These methodsusually adopt an extra memory to store the data for replay. However, it is notexpected in practice considering the memory constraint or data privacy issue.As a replacement, data-free data replay methods are proposed by invertingsamples from the classification model. Though achieving good results, thesemethods still suffer from the inconsistency of the inverted and real trainingdata, which is neglected in the inversion stage in recent works. To thateffect, we propose to measure the data consistency quantitatively by somesimplification and assumptions. Using the measurement, we analyze existingtechniques for inverting samples and get some insightful information thatinspires a novel loss function to reduce the inconsistency. Specifically, theloss minimizes the KL divergence of the distributions of inverted and real dataunder the tied multivariate Gaussian assumption, which is easy to implement incontinual learning. In addition, we observe that the norms of old class weightsturn to decrease continually as learning progresses. We thus analyze theunderlying reasons and propose a simple regularization term to balance theclass weights so that the samples of old classes are more distinguishable. Toconclude, we propose the Consistency enhanced data replay with debiasedclassifier for Class Incremental Learning (CCIL). Extensive experiments onCIFAR-100, Tiny-ImageNet, and ImageNet100 show consistently improvedperformance of CCIL compared to previous approaches.</description><author>Chenyang Wang, Junjun Jiang, Xingyu Hu, Xianming Liu, Xiangyang Ji</author><pubDate>Fri, 12 Jan 2024 12:51:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06548v1</guid></item><item><title>Optimizing Feature Selection for Binary Classification with Noisy Labels: A Genetic Algorithm Approach</title><link>http://arxiv.org/abs/2401.06546v1</link><description>Feature selection in noisy label scenarios remains an understudied topic. Wepropose a novel genetic algorithm-based approach, the Noise-AwareMulti-Objective Feature Selection Genetic Algorithm (NMFS-GA), for selectingoptimal feature subsets in binary classification with noisy labels. NMFS-GAoffers a unified framework for selecting feature subsets that are both accurateand interpretable. We evaluate NMFS-GA on synthetic datasets with label noise,a Breast Cancer dataset enriched with noisy features, and a real-world ADNIdataset for dementia conversion prediction. Our results indicate that NMFS-GAcan effectively select feature subsets that improve the accuracy andinterpretability of binary classifiers in scenarios with noisy labels.</description><author>Vandad Imani, Elaheh Moradi, Carlos Sevilla-Salcedo, Vittorio Fortino, Jussi Tohka</author><pubDate>Fri, 12 Jan 2024 12:42:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06546v1</guid></item><item><title>Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and Outlook</title><link>http://arxiv.org/abs/2401.06542v1</link><description>In the realm of modern autonomous driving, the perception system isindispensable for accurately assessing the state of the surroundingenvironment, thereby enabling informed prediction and planning. Key to thissystem is 3D object detection methods, that utilize vehicle-mounted sensorssuch as LiDAR and cameras to identify the size, category, and location ofnearby objects. Despite the surge in 3D object detection methods aimed atenhancing detection precision and efficiency, there is a gap in the literaturethat systematically examines their resilience against environmental variations,noise, and weather changes. This study emphasizes the importance of robustness,alongside accuracy and latency, in evaluating perception systems underpractical scenarios. Our work presents an extensive survey of camera-based,LiDAR-based, and multimodal 3D object detection algorithms, thoroughlyevaluating their trade-off between accuracy, latency, and robustness,particularly on datasets like KITTI-C and nuScenes-C to ensure faircomparisons. Among these,multimodal 3D detection approaches exhibit superiorrobustness and a novel taxonomy is introduced to reorganize its literature forenhanced clarity. This survey aims to offer a more practical perspective on thecurrent capabilities and constraints of 3D object detection algorithms inreal-world applications, thus steering future research towardsrobustness-centric advancements</description><author>Ziying Song, Lin Liu, Feiyang Jia, Yadan Luo, Guoxin Zhang, Lei Yang, Li Wang, Caiyan Jia</author><pubDate>Fri, 12 Jan 2024 12:35:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06542v1</guid></item><item><title>Medical Dialogue Generation via Intuitive-then-Analytical Differential Diagnosis</title><link>http://arxiv.org/abs/2401.06541v1</link><description>Medical dialogue systems have attracted growing research attention as theyhave the potential to provide rapid diagnoses, treatment plans, and healthconsultations. In medical dialogues, a proper diagnosis is crucial as itestablishes the foundation for future consultations. Clinicians typicallyemploy both intuitive and analytic reasoning to formulate a differentialdiagnosis. This reasoning process hypothesizes and verifies a variety ofpossible diseases and strives to generate a comprehensive and rigorousdiagnosis. However, recent studies on medical dialogue generation haveoverlooked the significance of modeling a differential diagnosis, which hindersthe practical application of these systems. To address the above issue, wepropose a medical dialogue generation framework with theIntuitive-then-Analytic Differential Diagnosis (IADDx). Our method starts witha differential diagnosis via retrieval-based intuitive association andsubsequently refines it through a graph-enhanced analytic procedure. Theresulting differential diagnosis is then used to retrieve medical knowledge andguide response generation. Experimental results on two datasets validate theefficacy of our method. Besides, we demonstrate how our framework assists bothclinicians and patients in understanding the diagnostic process, for instance,by producing intermediate results and graph-based diagnosis paths.</description><author>Kaishuai Xu, Wenjun Hou, Yi Cheng, Jian Wang, Wenjie Li</author><pubDate>Fri, 12 Jan 2024 12:35:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06541v1</guid></item><item><title>Lightweight Full-Convolutional Siamese Tracker</title><link>http://arxiv.org/abs/2310.05392v3</link><description>Although single object trackers have achieved advanced performance, theirlarge-scale models hinder their application on limited resources platforms.Moreover, existing lightweight trackers only achieve a balance between 2-3points in terms of parameters, performance, Flops and FPS. To achieve theoptimal balance among these points, this paper proposes a lightweightfull-convolutional Siamese tracker called LightFC. LightFC employs a novelefficient cross-correlation module (ECM) and a novel efficient rep-center head(ERH) to improve the feature representation of the convolutional trackingpipeline. The ECM uses an attention-like module design, which conducts spatialand channel linear fusion of fused features and enhances the nonlinearity ofthe fused features. Additionally, it refers to successful factors of currentlightweight trackers and introduces skip-connections and reuse of search areafeatures. The ERH reparameterizes the feature dimensional stage in the standardcenter-head and introduces channel attention to optimize the bottleneck of keyfeature flows. Comprehensive experiments show that LightFC achieves the optimalbalance between performance, parameters, Flops and FPS. The precision score ofLightFC outperforms MixFormerV2-S on LaSOT and TNL2K by 3.7 % and 6.5 %,respectively, while using 5x fewer parameters and 4.6x fewer Flops. Besides,LightFC runs 2x faster than MixFormerV2-S on CPUs. In addition, ahigher-performance version named LightFC-vit is proposed by replacing a morepowerful backbone network. The code and raw results can be found athttps://github.com/LiYunfengLYF/LightFC.</description><author>Yunfeng Li, Bo Wang, Xueyi Wu, Zhuoyan Liu, Ye Li</author><pubDate>Fri, 12 Jan 2024 12:34:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05392v3</guid></item></channel></rss>