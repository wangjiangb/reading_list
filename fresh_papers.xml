<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 21 Dec 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency</title><link>http://arxiv.org/abs/2412.15216v1</link><description>We propose an unsupervised model for instruction-based image editing thateliminates the need for ground-truth edited images during training. Existingsupervised methods depend on datasets containing triplets of input image,edited image, and edit instruction. These are generated by either existingediting methods or human-annotations, which introduce biases and limit theirgeneralization ability. Our method addresses these challenges by introducing anovel editing mechanism called Cycle Edit Consistency (CEC), which appliesforward and backward edits in one training step and enforces consistency inimage and attention spaces. This allows us to bypass the need for ground-truthedited images and unlock training for the first time on datasets comprisingeither real image-caption pairs or image-caption-edit triplets. We empiricallyshow that our unsupervised technique performs better across a broader range ofedits with high fidelity and precision. By eliminating the need forpre-existing datasets of triplets, reducing biases associated with supervisedmethods, and proposing CEC, our work represents a significant advancement inunblocking scaling of instruction-based image editing.</description><author>Enis Simsar, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, Federico Tombari</author><pubDate>Thu, 19 Dec 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15216v1</guid></item><item><title>EnvGS: Modeling View-Dependent Appearance with Environment Gaussian</title><link>http://arxiv.org/abs/2412.15215v1</link><description>Reconstructing complex reflections in real-world scenes from 2D images isessential for achieving photorealistic novel view synthesis. Existing methodsthat utilize environment maps to model reflections from distant lighting oftenstruggle with high-frequency reflection details and fail to account fornear-field reflections. In this work, we introduce EnvGS, a novel approach thatemploys a set of Gaussian primitives as an explicit 3D representation forcapturing reflections of environments. These environment Gaussian primitivesare incorporated with base Gaussian primitives to model the appearance of thewhole scene. To efficiently render these environment Gaussian primitives, wedeveloped a ray-tracing-based renderer that leverages the GPU's RT core forfast rendering. This allows us to jointly optimize our model for high-qualityreconstruction while maintaining real-time rendering speeds. Results frommultiple real-world and synthetic datasets demonstrate that our method producessignificantly more detailed reflections, achieving the best rendering qualityin real-time novel view synthesis.</description><author>Tao Xie, Xi Chen, Zhen Xu, Yiman Xie, Yudong Jin, Yujun Shen, Sida Peng, Hujun Bao, Xiaowei Zhou</author><pubDate>Thu, 19 Dec 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15215v1</guid></item><item><title>LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis</title><link>http://arxiv.org/abs/2412.15214v1</link><description>The intuitive nature of drag-based interaction has led to its growingadoption for controlling object trajectories in image-to-video synthesis.Still, existing methods that perform dragging in the 2D space usually faceambiguity when handling out-of-plane movements. In this work, we augment theinteraction with a new dimension, i.e., the depth dimension, such that usersare allowed to assign a relative depth for each point on the trajectory. Thatway, our new interaction paradigm not only inherits the convenience from 2Ddragging, but facilitates trajectory control in the 3D space, broadening thescope of creativity. We propose a pioneering method for 3D trajectory controlin image-to-video synthesis by abstracting object masks into a few clusterpoints. These points, accompanied by the depth information and the instanceinformation, are finally fed into a video diffusion model as the controlsignal. Extensive experiments validate the effectiveness of our approach,dubbed LeviTor, in precisely manipulating the object movements when producingphoto-realistic videos from static images. Project page:https://ppetrichor.github.io/levitor.github.io/</description><author>Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, Limin Wang</author><pubDate>Thu, 19 Dec 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15214v1</guid></item><item><title>Flowing from Words to Pixels: A Framework for Cross-Modality Evolution</title><link>http://arxiv.org/abs/2412.15213v1</link><description>Diffusion models, and their generalization, flow matching, have had aremarkable impact on the field of media generation. Here, the conventionalapproach is to learn the complex mapping from a simple source distribution ofGaussian noise to the target media distribution. For cross-modal tasks such astext-to-image generation, this same mapping from noise to image is learntwhilst including a conditioning mechanism in the model. One key and thus farrelatively unexplored feature of flow matching is that, unlike Diffusionmodels, they are not constrained for the source distribution to be noise.Hence, in this paper, we propose a paradigm shift, and ask the question ofwhether we can instead train flow matching models to learn a direct mappingfrom the distribution of one modality to the distribution of another, thusobviating the need for both the noise distribution and conditioning mechanism.We present a general and simple framework, CrossFlow, for cross-modal flowmatching. We show the importance of applying Variational Encoders to the inputdata, and introduce a method to enable Classifier-free guidance. Surprisingly,for text-to-image, CrossFlow with a vanilla transformer without cross attentionslightly outperforms standard flow matching, and we show that it scales betterwith training steps and model size, while also allowing for interesting latentarithmetic which results in semantically meaningful edits in the output space.To demonstrate the generalizability of our approach, we also show thatCrossFlow is on par with or outperforms the state-of-the-art for variouscross-modal / intra-modal mapping tasks, viz. image captioning, depthestimation, and image super-resolution. We hope this paper contributes toaccelerating progress in cross-modal media generation.</description><author>Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, Mannat Singh</author><pubDate>Thu, 19 Dec 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15213v1</guid></item><item><title>Scaling 4D Representations</title><link>http://arxiv.org/abs/2412.15212v1</link><description>Scaling has not yet been convincingly demonstrated for pure self-supervisedlearning from video. However, prior work has focused evaluations onsemantic-related tasks $\unicode{x2013}$ action classification, ImageNetclassification, etc. In this paper we focus on evaluating self-supervisedlearning on non-semantic vision tasks that are more spatial (3D) and temporal(+1D = 4D), such as camera pose estimation, point and object tracking, anddepth estimation. We show that by learning from very large video datasets,masked auto-encoding (MAE) with transformer video models actually scales,consistently improving performance on these 4D tasks, as model size increasesfrom 20M all the way to the largest by far reported self-supervised video model$\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison withmany recent image and video models demonstrates the benefits of scaling 4Drepresentations.</description><author>João Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Greff, Guillaume Le Moing, Sjoerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro Vélez, Luisa Polanía, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Allen, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica Pătrăucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, Andrew Zisserman</author><pubDate>Thu, 19 Dec 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15212v1</guid></item><item><title>Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation</title><link>http://arxiv.org/abs/2412.15211v1</link><description>Reconstructing the geometry and appearance of objects from photographs takenin different environments is difficult as the illumination and therefore theobject appearance vary across captured images. This is particularly challengingfor more specular objects whose appearance strongly depends on the viewingdirection. Some prior approaches model appearance variation across images usinga per-image embedding vector, while others use physically-based rendering torecover the materials and per-image illumination. Such approaches fail atfaithfully recovering view-dependent appearance given significant variation ininput illumination and tend to produce mostly diffuse results. We present anapproach that reconstructs objects from images taken under differentilluminations by first relighting the images under a single referenceillumination with a multiview relighting diffusion model and thenreconstructing the object's geometry and appearance with a radiance fieldarchitecture that is robust to the small remaining inconsistencies among therelit images. We validate our proposed approach on both synthetic and realdatasets and demonstrate that it greatly outperforms existing techniques atreconstructing high-fidelity appearance from images taken under extremeillumination variation. Moreover, our approach is particularly effective atrecovering view-dependent "shiny" appearance which cannot be reconstructed byprior methods.</description><author>Hadi Alzayer, Philipp Henzler, Jonathan T. Barron, Jia-Bin Huang, Pratul P. Srinivasan, Dor Verbin</author><pubDate>Thu, 19 Dec 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15211v1</guid></item><item><title>Tokenisation is NP-Complete</title><link>http://arxiv.org/abs/2412.15210v1</link><description>In this work, we prove the NP-completeness of two variants of tokenisation,defined as the problem of compressing a dataset to at most $\delta$ symbols byeither finding a vocabulary directly (direct tokenisation), or selecting asequence of merge operations (bottom-up tokenisation).</description><author>Philip Whittington, Gregor Bachmann, Tiago Pimentel</author><pubDate>Thu, 19 Dec 2024 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15210v1</guid></item><item><title>PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation</title><link>http://arxiv.org/abs/2412.15209v1</link><description>Despite significant advancements in Large Vision-Language Models (LVLMs),existing pixel-grounding models operate on single-image settings, limitingtheir ability to perform detailed, fine-grained comparisons across multipleimages. Conversely, current multi-image understanding models lack pixel-levelgrounding. Our work addresses this gap by introducing the task of multi-imagepixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integratespixel-level grounding with robust multi-image reasoning capabilities to producecontextually rich, pixel-grounded explanations. Central to PRIMA is anefficient vision module that queries fine-grained visual representations acrossmultiple images, reducing TFLOPs by $25.3\%$. To support training andevaluation, we curate $M^4Seg$, a new reasoning segmentation benchmarkconsisting of $\sim$224K question-answer pairs that require fine-grained visualunderstanding across multiple images. Experimental results demonstrate PRIMAoutperforms state-of-the-art baselines.</description><author>Muntasir Wahed, Kiet A. Nguyen, Adheesh Sunil Juvekar, Xinzhuo Li, Xiaona Zhou, Vedant Shah, Tianjiao Yu, Pinar Yanardag, Ismini Lourentzou</author><pubDate>Thu, 19 Dec 2024 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15209v1</guid></item><item><title>OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving</title><link>http://arxiv.org/abs/2412.15208v1</link><description>Since the advent of Multimodal Large Language Models (MLLMs), they have madea significant impact across a wide range of real-world applications,particularly in Autonomous Driving (AD). Their ability to process complexvisual data and reason about intricate driving scenarios has paved the way fora new paradigm in end-to-end AD systems. However, the progress of developingend-to-end models for AD has been slow, as existing fine-tuning methods demandsubstantial resources, including extensive computational power, large-scaledatasets, and significant funding. Drawing inspiration from recent advancementsin inference computing, we propose OpenEMMA, an open-source end-to-endframework based on MLLMs. By incorporating the Chain-of-Thought reasoningprocess, OpenEMMA achieves significant improvements compared to the baselinewhen leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrateseffectiveness, generalizability, and robustness across a variety of challengingdriving scenarios, offering a more efficient and effective approach toautonomous driving. We release all the codes inhttps://github.com/taco-group/OpenEMMA.</description><author>Shuo Xing, Chengyuan Qian, Yuping Wang, Hongyuan Hua, Kexin Tian, Yang Zhou, Zhengzhong Tu</author><pubDate>Thu, 19 Dec 2024 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15208v1</guid></item><item><title>AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for Autonomous Driving</title><link>http://arxiv.org/abs/2412.15206v1</link><description>Recent advancements in large vision language models (VLMs) tailored forautonomous driving (AD) have shown strong scene understanding and reasoningcapabilities, making them undeniable candidates for end-to-end driving systems.However, limited work exists on studying the trustworthiness of DriveVLMs -- acritical factor that directly impacts public transportation safety. In thispaper, we introduce AutoTrust, a comprehensive trustworthiness benchmark forlarge vision-language models in autonomous driving (DriveVLMs), consideringdiverse perspectives -- including trustfulness, safety, robustness, privacy,and fairness. We constructed the largest visual question-answering dataset forinvestigating trustworthiness issues in driving scenarios, comprising over 10kunique scenes and 18k queries. We evaluated six publicly available VLMs,spanning from generalist to specialist, from open-source to commercial models.Our exhaustive evaluations have unveiled previously undiscoveredvulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we foundthat the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperformspecialized models fine-tuned for driving in terms of overall trustworthiness.DriveVLMs like DriveLM-Agent are particularly vulnerable to disclosingsensitive information. Additionally, both generalist and specialist VLMs remainsusceptible to adversarial attacks and struggle to ensure unbiaseddecision-making across diverse environments and populations. Our findings callfor immediate and decisive action to address the trustworthiness of DriveVLMs-- an issue of critical importance to public safety and the welfare of allcitizens relying on autonomous transportation systems. Our benchmark ispublicly available at \url{https://github.com/taco-group/AutoTrust}, and theleaderboard is released at \url{https://taco-group.github.io/AutoTrust/}.</description><author>Shuo Xing, Hongyuan Hua, Xiangbo Gao, Shenzhe Zhu, Renjie Li, Kexin Tian, Xiaopeng Li, Heng Huang, Tianbao Yang, Zhangyang Wang, Yang Zhou, Huaxiu Yao, Zhengzhong Tu</author><pubDate>Thu, 19 Dec 2024 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15206v1</guid></item><item><title>FlowAR: Scale-wise Autoregressive Image Generation Meets Flow Matching</title><link>http://arxiv.org/abs/2412.15205v1</link><description>Autoregressive (AR) modeling has achieved remarkable success in naturallanguage processing by enabling models to generate text with coherence andcontextual understanding through next token prediction. Recently, in imagegeneration, VAR proposes scale-wise autoregressive modeling, which extends thenext token prediction to the next scale prediction, preserving the 2D structureof images. However, VAR encounters two primary challenges: (1) its complex andrigid scale design limits generalization in next scale prediction, and (2) thegenerator's dependence on a discrete tokenizer with the same complex scalestructure restricts modularity and flexibility in updating the tokenizer. Toaddress these limitations, we introduce FlowAR, a general next scale predictionmethod featuring a streamlined scale design, where each subsequent scale issimply double the previous one. This eliminates the need for VAR's intricatemulti-scale residual tokenizer and enables the use of any off-the-shelfVariational AutoEncoder (VAE). Our simplified design enhances generalization innext scale prediction and facilitates the integration of Flow Matching forhigh-quality image synthesis. We validate the effectiveness of FlowAR on thechallenging ImageNet-256 benchmark, demonstrating superior generationperformance compared to previous methods. Codes will be available at\url{https://github.com/OliverRensu/FlowAR}.</description><author>Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen</author><pubDate>Thu, 19 Dec 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15205v1</guid></item><item><title>LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks</title><link>http://arxiv.org/abs/2412.15204v1</link><description>This paper introduces LongBench v2, a benchmark designed to assess theability of LLMs to handle long-context problems requiring deep understandingand reasoning across real-world multitasks. LongBench v2 consists of 503challenging multiple-choice questions, with contexts ranging from 8k to 2Mwords, across six major task categories: single-document QA, multi-document QA,long in-context learning, long-dialogue history understanding, code repositoryunderstanding, and long structured data understanding. To ensure the breadthand the practicality, we collect data from nearly 100 highly educatedindividuals with diverse professional backgrounds. We employ both automated andmanual review processes to maintain high quality and difficulty, resulting inhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.Our evaluation reveals that the best-performing model, when directly answersthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,which includes longer reasoning, achieves 57.7%, surpassing the human baselineby 4%. These results highlight the importance of enhanced reasoning ability andscaling inference-time compute to tackle the long-context challenges inLongBench v2. The project is available at https://longbench2.github.io.</description><author>Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li</author><pubDate>Thu, 19 Dec 2024 18:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15204v1</guid></item><item><title>DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation</title><link>http://arxiv.org/abs/2412.15200v1</link><description>Procedural Content Generation (PCG) is powerful in creating high-quality 3Dcontents, yet controlling it to produce desired shapes is difficult and oftenrequires extensive parameter tuning. Inverse Procedural Content Generation aimsto automatically find the best parameters under the input condition. However,existing sampling-based and neural network-based methods still suffer fromnumerous sample iterations or limited controllability. In this work, we presentDI-PCG, a novel and efficient method for Inverse PCG from general imageconditions. At its core is a lightweight diffusion transformer model, where PCGparameters are directly treated as the denoising target and the observed imagesas conditions to control parameter generation. DI-PCG is efficient andeffective. With only 7.6M network parameters and 30 GPU hours to train, itdemonstrates superior performance in recovering parameters accurately, andgeneralizing well to in-the-wild images. Quantitative and qualitativeexperiment results validate the effectiveness of DI-PCG in inverse PCG andimage-to-3D generation tasks. DI-PCG offers a promising approach for efficientinverse PCG and represents a valuable exploration step towards a 3D generationpath that models how to construct a 3D asset using parametric models.</description><author>Wang Zhao, Yan-Pei Cao, Jiale Xu, Yuejiang Dong, Ying Shan</author><pubDate>Thu, 19 Dec 2024 18:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15200v1</guid></item><item><title>LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation</title><link>http://arxiv.org/abs/2412.15199v1</link><description>This paper targets the challenge of real-time LiDAR re-simulation in dynamicdriving scenarios. Recent approaches utilize neural radiance fields combinedwith the physical modeling of LiDAR sensors to achieve high-fidelityre-simulation results. Unfortunately, these methods face limitations due tohigh computational demands in large-scale scenes and cannot perform real-timeLiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novelframework that supports real-time, physically accurate LiDAR re-simulation fordriving scenes. Our primary contribution is the development of an efficient andeffective rendering pipeline, which integrates Gaussian primitives andhardware-accelerated ray tracing technology. Specifically, we model thephysical properties of LiDAR sensors using Gaussian primitives with learnableparameters and incorporate scene graphs to handle scene dynamics. Building uponthis scene representation, our framework first constructs a bounding volumehierarchy (BVH), then casts rays for each pixel and generates novel LiDAR viewsthrough a differentiable rendering algorithm. Importantly, our frameworksupports realistic rendering with flexible scene editing operations and varioussensor configurations. Extensive experiments across multiple public benchmarksdemonstrate that our method outperforms state-of-the-art methods in terms ofrendering quality and efficiency. Our project page is athttps://zju3dv.github.io/lidar-rt.</description><author>Chenxu Zhou, Lvchang Fu, Sida Peng, Yunzhi Yan, Zhanhua Zhang, Yong Chen, Jiazhi Xia, Xiaowei Zhou</author><pubDate>Thu, 19 Dec 2024 18:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15199v1</guid></item><item><title>Preventing Local Pitfalls in Vector Quantization via Optimal Transport</title><link>http://arxiv.org/abs/2412.15195v1</link><description>Vector-quantized networks (VQNs) have exhibited remarkable performance acrossvarious tasks, yet they are prone to training instability, which complicatesthe training process due to the necessity for techniques such as subtleinitialization and model distillation. In this study, we identify the localminima issue as the primary cause of this instability. To address this, weintegrate an optimal transport method in place of the nearest neighbor searchto achieve a more globally informed assignment. We introduce OptVQ, a novelvector quantization method that employs the Sinkhorn algorithm to optimize theoptimal transport problem, thereby enhancing the stability and efficiency ofthe training process. To mitigate the influence of diverse data distributionson the Sinkhorn algorithm, we implement a straightforward yet effectivenormalization strategy. Our comprehensive experiments on image reconstructiontasks demonstrate that OptVQ achieves 100% codebook utilization and surpassescurrent state-of-the-art VQNs in reconstruction quality.</description><author>Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu</author><pubDate>Thu, 19 Dec 2024 18:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15195v1</guid></item><item><title>MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark</title><link>http://arxiv.org/abs/2412.15194v1</link><description>Multiple-choice question (MCQ) datasets like Massive Multitask LanguageUnderstanding (MMLU) are widely used to evaluate the commonsense,understanding, and problem-solving abilities of large language models (LLMs).However, the open-source nature of these benchmarks and the broad sources oftraining data for LLMs have inevitably led to benchmark contamination,resulting in unreliable evaluation results. To alleviate this issue, we proposea contamination-free and more challenging MCQ benchmark called MMLU-CF. Thisbenchmark reassesses LLMs' understanding of world knowledge by averting bothunintentional and malicious data leakage. To avoid unintentional data leakage,we source data from a broader domain and design three decontamination rules. Toprevent malicious data leakage, we divide the benchmark into validation andtest sets with similar difficulty and subject distributions. The test setremains closed-source to ensure reliable results, while the validation set ispublicly available to promote transparency and facilitate independentverification. Our evaluation of mainstream LLMs reveals that the powerfulGPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% onthe test set, which indicates the effectiveness of our approach in creating amore rigorous and contamination-free evaluation standard. The GitHub repositoryis available at https://github.com/microsoft/MMLU-CF and the dataset refers tohttps://huggingface.co/datasets/microsoft/MMLU-CF.</description><author>Qihao Zhao, Yangyu Huang, Tengchao Lv, Lei Cui, Qinzheng Sun, Shaoguang Mao, Xin Zhang, Ying Xin, Qiufeng Yin, Scarlett Li, Furu Wei</author><pubDate>Thu, 19 Dec 2024 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15194v1</guid></item><item><title>AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation</title><link>http://arxiv.org/abs/2412.15191v1</link><description>We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Videogeneration that leverages the activations of frozen video and audio diffusionmodels for temporally-aligned cross-modal conditioning. The key to ourframework is a Fusion Block that enables bidirectional information exchangebetween our backbone video and audio diffusion models through atemporally-aligned self attention operation. Unlike prior work that usesfeature extractors pretrained for other tasks for the conditioning signal,AV-Link can directly leverage features obtained by the complementary modalityin a single framework i.e. video features to generate audio, or audio featuresto generate video. We extensively evaluate our design choices and demonstratethe ability of our method to achieve synchronized and high-quality audiovisualcontent, showcasing its potential for applications in immersive mediageneration. Project Page: snap-research.github.io/AVLink/</description><author>Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Alper Canberk, Kwot Sin Lee, Vicente Ordonez, Sergey Tulyakov</author><pubDate>Thu, 19 Dec 2024 18:57:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15191v1</guid></item><item><title>EarthDial: Turning Multi-sensory Earth Observations to Interactive Dialogues</title><link>http://arxiv.org/abs/2412.15190v1</link><description>Automated analysis of vast Earth observation data via interactiveVision-Language Models (VLMs) can unlock new opportunities for environmentalmonitoring, disaster response, and resource management. Existing generic VLMsdo not perform well on Remote Sensing data, while the recent Geo-spatial VLMsremain restricted to a fixed resolution and few sensor modalities. In thispaper, we introduce EarthDial, a conversational assistant specifically designedfor Earth Observation (EO) data, transforming complex, multi-sensory Earthobservations into interactive, natural language dialogues. EarthDial supportsmulti-spectral, multi-temporal, and multi-resolution imagery, enabling a widerange of remote sensing tasks, including classification, detection, captioning,question answering, visual reasoning, and visual grounding. To achieve this, weintroduce an extensive instruction tuning dataset comprising over 11.11Minstruction pairs covering RGB, Synthetic Aperture Radar (SAR), andmultispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore,EarthDial handles bi-temporal and multi-temporal sequence analysis forapplications like change detection. Our extensive experimental results on 37downstream applications demonstrate that EarthDial outperforms existing genericand domain-specific models, achieving better generalization across various EOtasks.</description><author>Sagar Soni, Akshay Dudhane, Hiyam Debary, Mustansar Fiaz, Muhammad Akhtar Munir, Muhammad Sohail Danish, Paolo Fraccaro, Campbell D Watson, Levente J Klein, Fahad Shahbaz Khan, Salman Khan</author><pubDate>Thu, 19 Dec 2024 18:57:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15190v1</guid></item><item><title>Face the Facts! Evaluating RAG-based Fact-checking Pipelines in Realistic Settings</title><link>http://arxiv.org/abs/2412.15189v1</link><description>Natural Language Processing and Generation systems have recently shown thepotential to complement and streamline the costly and time-consuming job ofprofessional fact-checkers. In this work, we lift several constraints ofcurrent state-of-the-art pipelines for automated fact-checking based on theRetrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, undermore realistic scenarios, RAG-based methods for the generation of verdicts -i.e., short texts discussing the veracity of a claim - evaluating them onstylistically complex claims and heterogeneous, yet reliable, knowledge bases.Our findings show a complex landscape, where, for example, LLM-based retrieversoutperform other retrieval techniques, though they still struggle withheterogeneous knowledge bases; larger models excel in verdict faithfulness,while smaller models provide better context adherence, with human evaluationsfavouring zero-shot and one-shot approaches for informativeness, and fine-tunedmodels for emotional alignment.</description><author>Daniel Russo, Stefano Menini, Jacopo Staiano, Marco Guerini</author><pubDate>Thu, 19 Dec 2024 18:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15189v1</guid></item><item><title>LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation</title><link>http://arxiv.org/abs/2412.15188v1</link><description>We present LlamaFusion, a framework for empowering pretrained text-only largelanguage models (LLMs) with multimodal generative capabilities, enabling themto understand and generate both text and images in arbitrary sequences.LlamaFusion leverages existing Llama-3's weights for processing textsautoregressively while introducing additional and parallel transformer modulesfor processing images with diffusion. During training, the data from eachmodality is routed to its dedicated modules: modality-specific feedforwardlayers, query-key-value projections, and normalization layers process eachmodality independently, while the shared self-attention layers allowinteractions across text and image features. By freezing the text-specificmodules and only training the image-specific modules, LlamaFusion preserves thelanguage capabilities of text-only LLMs while developing strong visualunderstanding and generation abilities. Compared to methods that pretrainmultimodal generative models from scratch, our experiments demonstrate that,LlamaFusion improves image understanding by 20% and image generation by 3.6%using only 50% of the FLOPs while maintaining Llama-3's language capabilities.We also demonstrate that this framework can adapt existing vision-languagemodels with multimodal generation ability. Overall, this framework not onlyleverages existing computational investments in text-only LLMs but also enablesthe parallel development of language and vision capabilities, presenting apromising direction for efficient multimodal model development.</description><author>Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu</author><pubDate>Thu, 19 Dec 2024 18:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15188v1</guid></item><item><title>Tiled Diffusion</title><link>http://arxiv.org/abs/2412.15185v1</link><description>Image tiling -- the seamless connection of disparate images to create acoherent visual field -- is crucial for applications such as texture creation,video game asset development, and digital art. Traditionally, tiles have beenconstructed manually, a method that poses significant limitations inscalability and flexibility. Recent research has attempted to automate thisprocess using generative models. However, current approaches primarily focus ontiling textures and manipulating models for single-image generation, withoutinherently supporting the creation of multiple interconnected tiles acrossdiverse domains. This paper presents Tiled Diffusion, a novel approach thatextends the capabilities of diffusion models to accommodate the generation ofcohesive tiling patterns across various domains of image synthesis that requiretiling. Our method supports a wide range of tiling scenarios, from self-tilingto complex many-to-many connections, enabling seamless integration of multipleimages. Tiled Diffusion automates the tiling process, eliminating the need formanual intervention and enhancing creative possibilities in variousapplications, such as seamlessly tiling of existing images, tiled texturecreation, and 360{\deg} synthesis.</description><author>Or Madar, Ohad Fried</author><pubDate>Thu, 19 Dec 2024 18:55:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15185v1</guid></item><item><title>Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning</title><link>http://arxiv.org/abs/2412.15184v1</link><description>The suite of datasets commonly used to train and evaluate the mathematicalcapabilities of AI-based mathematical copilots (primarily large languagemodels) exhibit several shortcomings. These limitations include a restrictedscope of mathematical complexity, typically not exceeding lowerundergraduate-level mathematics, binary rating protocols and other issues,which makes comprehensive proof-based evaluation suites difficult. Wesystematically explore these limitations and contend that enhancing thecapabilities of large language models, or any forthcoming advancements inAI-based mathematical assistants (copilots or "thought partners"), necessitatesa paradigm shift in the design of mathematical datasets and the evaluationcriteria of mathematical ability: It is necessary to move away fromresult-based datasets (theorem statement to theorem proof) and convert the richfacets of mathematical research practice to data LLMs can train on. Examples ofthese are mathematical workflows (sequences of atomic, potentiallysubfield-dependent tasks that are often performed when creating newmathematics), which are an important part of the proof-discovery process.Additionally, we advocate for mathematical dataset developers to consider theconcept of "motivated proof", introduced by G. P\'olya in 1949, which can serveas a blueprint for datasets that offer a better proof learning signal,alleviating some of the mentioned limitations. Lastly, we introduce mathdatasheets for datasets, extending the general, dataset-agnostic variants ofdatasheets: We provide a questionnaire designed specifically for math datasetsthat we urge dataset creators to include with their datasets. This will makecreators aware of potential limitations of their datasets while at the sametime making it easy for readers to assess it from the point of view of trainingand evaluating mathematical copilots.</description><author>Simon Frieder, Jonas Bayer, Katherine M. Collins, Julius Berner, Jacob Loader, András Juhász, Fabian Ruehle, Sean Welleck, Gabriel Poesia, Ryan-Rhys Griffiths, Adrian Weller, Anirudh Goyal, Thomas Lukasiewicz, Timothy Gowers</author><pubDate>Thu, 19 Dec 2024 18:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15184v1</guid></item><item><title>STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning</title><link>http://arxiv.org/abs/2412.15182v1</link><description>Robot learning is witnessing a significant increase in the size, diversity,and complexity of pre-collected datasets, mirroring trends in domains such asnatural language processing and computer vision. Many robot learning methodstreat such datasets as multi-task expert data and learn a multi-task,generalist policy by training broadly across them. Notably, while thesegeneralist policies can improve the average performance across many tasks, theperformance of generalist policies on any one task is often suboptimal due tonegative transfer between partitions of the data, compared to task-specificspecialist policies. In this work, we argue for the paradigm of trainingpolicies during deployment given the scenarios they encounter: rather thandeploying pre-trained policies to unseen problems in a zero-shot manner, wenon-parametrically retrieve and train models directly on relevant data at testtime. Furthermore, we show that many robotics tasks share considerable amountsof low-level behaviors and that retrieval at the "sub"-trajectory granularityenables significantly improved data utilization, generalization, and robustnessin adapting policies to novel problems. In contrast, existing full-trajectoryretrieval methods tend to underutilize the data and miss out on sharedcross-task content. This work proposes STRAP, a technique for leveragingpre-trained vision foundation models and dynamic time warping to retrievesub-sequences of trajectories from large training corpora in a robust fashion.STRAP outperforms both prior retrieval algorithms and multi-task learningmethods in simulated and real experiments, showing the ability to scale to muchlarger offline datasets in the real world as well as the ability to learnrobust control policies with just a handful of real-world demonstrations.</description><author>Marius Memmel, Jacob Berg, Bingqing Chen, Abhishek Gupta, Jonathan Francis</author><pubDate>Thu, 19 Dec 2024 18:54:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15182v1</guid></item><item><title>HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages</title><link>http://arxiv.org/abs/2412.15178v1</link><description>Large Language Model (LLM) based coding tools have been tremendouslysuccessful as software development assistants, yet they are often designed forgeneral purpose programming tasks and perform poorly for more specializeddomains such as high performance computing. Creating specialized models andtools for these domains is crucial towards gaining the benefits of LLMs inareas such as HPC. While previous work has explored HPC-specific models, LLMsstill struggle to generate parallel code and it is not at all clear whathurdles are still holding back these LLMs and what must be done to overcomethem. In this work, we conduct an in-depth study along the many axes offine-tuning a specialized HPC LLM in order to better understand the challenges.Based on our findings we fine-tune and evaluate a specialized HPC LLM that isshown to be the best performing open-source code LLM for parallel codegeneration to date.</description><author>Aman Chaturvedi, Daniel Nichols, Siddharth Singh, Abhinav Bhatele</author><pubDate>Thu, 19 Dec 2024 18:52:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15178v1</guid></item><item><title>Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying</title><link>http://arxiv.org/abs/2412.15177v1</link><description>Studies have underscored how, regardless of the recent breakthrough and swiftadvances in AI research, even state-of-the-art Large Language models (LLMs)continue to struggle when performing logical and mathematical reasoning. Theresults seem to suggest that LLMs still work as (highly advanced) data patternidentifiers, scoring poorly when attempting to generalise and solve reasoningproblems the models have never previously seen or that are not close to samplespresented in their training data. To address this compelling concern, thispaper makes use of the notion of critical questions from the literature onargumentation theory, focusing in particular on Toulmin's model ofargumentation. We show that employing these critical questions can improve thereasoning capabilities of LLMs. By probing the rationale behind the models'reasoning process, the LLM can assess whether some logical mistake is occurringand correct it before providing the final reply to the user prompt. Theunderlying idea is drawn from the gold standard of any valid argumentativeprocedure: the conclusion is valid if it is entailed by accepted premises. Or,to paraphrase such Aristotelian principle in a real-world approximation,characterised by incomplete information and presumptive logic, the conclusionis valid if not proved otherwise. This approach successfully steers the models'output through a reasoning pipeline, resulting in better performance againstthe baseline and its Chain-of-Thought (CoT) implementation. To this end, anextensive evaluation of the proposed approach on the MT-Bench Reasoning andMath tasks across a range of LLMs is provided.</description><author>Federico Castagna, Isabel Sassoon, Simon Parsons</author><pubDate>Thu, 19 Dec 2024 18:51:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15177v1</guid></item><item><title>Tracing the Roots: Leveraging Temporal Dynamics in Diffusion Trajectories for Origin Attribution</title><link>http://arxiv.org/abs/2411.07449v2</link><description>Diffusion models have revolutionized image synthesis, garnering significantresearch interest in recent years. Diffusion is an iterative algorithm in whichsamples are generated step-by-step, starting from pure noise. This processintroduces the notion of diffusion trajectories, i.e., paths from the standardGaussian distribution to the target image distribution. In this context, westudy discriminative algorithms operating on these trajectories. Specifically,given a pre-trained diffusion model, we consider the problem of classifyingimages as part of the training dataset, generated by the model or originatingfrom an external source. Our approach demonstrates the presence of patternsacross steps that can be leveraged for classification. We also conduct ablationstudies, which reveal that using higher-order gradient features to characterizethe trajectories leads to significant performance gains and more robustalgorithms.</description><author>Andreas Floros, Seyed-Mohsen Moosavi-Dezfooli, Pier Luigi Dragotti</author><pubDate>Thu, 19 Dec 2024 18:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07449v2</guid></item><item><title>Rethinking Uncertainty Estimation in Natural Language Generation</title><link>http://arxiv.org/abs/2412.15176v1</link><description>Large Language Models (LLMs) are increasingly employed in real-worldapplications, driving the need to evaluate the trustworthiness of theirgenerated text. To this end, reliable uncertainty estimation is essential.Since current LLMs generate text autoregressively through a stochastic process,the same prompt can lead to varying outputs. Consequently, leading uncertaintyestimation methods generate and analyze multiple output sequences to determinethe LLM's uncertainty. However, generating output sequences is computationallyexpensive, making these methods impractical at scale. In this work, we inspectthe theoretical foundations of the leading methods and explore new directionsto enhance their computational efficiency. Building on the framework of properscoring rules, we find that the negative log-likelihood of the most likelyoutput sequence constitutes a theoretically grounded uncertainty measure. Toapproximate this alternative measure, we propose G-NLL, which has the advantageof being obtained using only a single output sequence generated by greedydecoding. This makes uncertainty estimation more efficient and straightforward,while preserving theoretical rigor. Empirical results demonstrate that G-NLLachieves state-of-the-art performance across various LLMs and tasks. Our worklays the foundation for efficient and reliable uncertainty estimation innatural language generation, challenging the necessity of more computationallyinvolved methods currently leading the field.</description><author>Lukas Aichberger, Kajetan Schweighofer, Sepp Hochreiter</author><pubDate>Thu, 19 Dec 2024 18:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15176v1</guid></item><item><title>SoK: Watermarking for AI-Generated Content</title><link>http://arxiv.org/abs/2411.18479v2</link><description>As the outputs of generative AI (GenAI) techniques improve in quality, itbecomes increasingly challenging to distinguish them from human-createdcontent. Watermarking schemes are a promising approach to address the problemof distinguishing between AI and human-generated content. These schemes embedhidden signals within AI-generated content to enable reliable detection. Whilewatermarking is not a silver bullet for addressing all risks associated withGenAI, it can play a crucial role in enhancing AI safety and trustworthiness bycombating misinformation and deception. This paper presents a comprehensiveoverview of watermarking techniques for GenAI, beginning with the need forwatermarking from historical and regulatory perspectives. We formalize thedefinitions and desired properties of watermarking schemes and examine the keyobjectives and threat models for existing approaches. Practical evaluationstrategies are also explored, providing insights into the development of robustwatermarking techniques capable of resisting various attacks. Additionally, wereview recent representative works, highlight open challenges, and discusspotential directions for this emerging field. By offering a thoroughunderstanding of watermarking in GenAI, this work aims to guide researchers inadvancing watermarking methods and applications, and support policymakers inaddressing the broader implications of GenAI.</description><author>Xuandong Zhao, Sam Gunn, Miranda Christ, Jaiden Fairoze, Andres Fabrega, Nicholas Carlini, Sanjam Garg, Sanghyun Hong, Milad Nasr, Florian Tramer, Somesh Jha, Lei Li, Yu-Xiang Wang, Dawn Song</author><pubDate>Thu, 19 Dec 2024 18:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18479v2</guid></item><item><title>S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity</title><link>http://arxiv.org/abs/2412.06289v3</link><description>Current PEFT methods for LLMs can achieve either high quality, efficienttraining, or scalable serving, but not all three simultaneously. To addressthis limitation, we investigate sparse fine-tuning and observe a remarkableimprovement in generalization ability. Utilizing this key insight, we propose afamily of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, whichconcurrently achieve state-of-the-art fine-tuning performance, trainingefficiency, and inference scalability. S$^{2}$FT accomplishes this by"selecting sparsely and computing densely". It selects a few heads and channelsin the MHA and FFN modules for each Transformer block, respectively. Next, itco-permutes weight matrices on both sides of the coupled structures in LLMs toconnect the selected components in each layer into a dense submatrix. Finally,S$^{2}$FT performs in-place gradient updates on all submatrices. Throughtheoretical analysis and empirical results, our method prevents forgettingwhile simplifying optimization, delivers SOTA performance on both commonsenseand arithmetic reasoning with 4.6% and 1.3% average improvements compared toLoRA, and surpasses full FT by 11.5% when generalizing to various domains afterinstruction tuning. Using our partial backpropagation algorithm, S$^{2}$FTsaves training memory up to 3$\times$ and improves latency by 1.5-2.7$\times$compared to full FT, while delivering an average 10% improvement over LoRA onboth metrics. We further demonstrate that the weight updates in S$^{2}$FT canbe decoupled into adapters, enabling effective fusion, fast switch, andefficient parallelism for serving multiple fine-tuned models.</description><author>Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen</author><pubDate>Thu, 19 Dec 2024 18:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06289v3</guid></item><item><title>SqueezeMe: Efficient Gaussian Avatars for VR</title><link>http://arxiv.org/abs/2412.15171v1</link><description>Gaussian Splatting has enabled real-time 3D human avatars with unprecedentedlevels of visual quality. While previous methods require a desktop GPU forreal-time inference of a single avatar, we aim to squeeze multiple Gaussianavatars onto a portable virtual reality headset with real-time drivableinference. We begin by training a previous work, Animatable Gaussians, on ahigh quality dataset captured with 512 cameras. The Gaussians are animated bycontrolling base set of Gaussians with linear blend skinning (LBS) motion andthen further adjusting the Gaussians with a neural network decoder to correcttheir appearance. When deploying the model on a Meta Quest 3 VR headset, wefind two major computational bottlenecks: the decoder and the rendering. Toaccelerate the decoder, we train the Gaussians in UV-space instead ofpixel-space, and we distill the decoder to a single neural network layer.Further, we discover that neighborhoods of Gaussians can share a singlecorrective from the decoder, which provides an additional speedup. Toaccelerate the rendering, we develop a custom pipeline in Vulkan that runs onthe mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrentlyat 72 FPS on a VR headset. Demo videos are athttps://forresti.github.io/squeezeme.</description><author>Shunsuke Saito, Stanislav Pidhorskyi, Igor Santesteban, Forrest Iandola, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon</author><pubDate>Thu, 19 Dec 2024 18:46:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15171v1</guid></item><item><title>CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement</title><link>http://arxiv.org/abs/2411.05199v2</link><description>Large Language Models (LLMs) have revolutionized code generation but requiresignificant resources and often over-generalize, limiting their task-specificefficiency. Fine-tuning smaller, open-source LLMs provides a cost-effectivealternative. However, standard supervised approaches rely only on correctexamples, missing valuable insights from failures. We introduce CodeLutra, aframework that leverages both correct and incorrect code attempts. Instead ofusing only correct solutions, CodeLutra applies iterative preference-basedrefinement, comparing successful and failed outputs to better approximatedesired results. This approach narrows the performance gap withstate-of-the-art larger models without requiring massive datasets or auxiliarymodels. For instance, on a challenging data science coding task, using only 500samples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4'slevel. By learning from both successes and mistakes, CodeLutra provides ascalable and efficient path to high-quality code generation, making smalleropen-source models more competitive with leading closed-source alternatives.</description><author>Leitian Tao, Xiang Chen, Tong Yu, Tung Mai, Ryan Rossi, Yixuan Li, Saayan Mitra</author><pubDate>Thu, 19 Dec 2024 18:46:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05199v2</guid></item><item><title>Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration</title><link>http://arxiv.org/abs/2412.15166v1</link><description>Humanoid robots are envisioned as embodied intelligent agents capable ofperforming a wide range of human-level loco-manipulation tasks, particularly inscenarios requiring strenuous and repetitive labor. However, learning theseskills is challenging due to the high degrees of freedom of humanoid robots,and collecting sufficient training data for humanoid is a laborious process.Given the rapid introduction of new humanoid platforms, a cross-embodimentframework that allows generalizable skill transfer is becoming increasinglycritical. To address this, we propose a transferable framework that reduces thedata bottleneck by using a unified digital human model as a common prototypeand bypassing the need for re-training on every new robot platform. The modellearns behavior primitives from human demonstrations through adversarialimitation, and the complex robot structures are decomposed into functionalcomponents, each trained independently and dynamically coordinated. Taskgeneralization is achieved through a human-object interaction graph, and skillsare transferred to different robots via embodiment-specific kinematic motionretargeting and dynamic fine-tuning. Our framework is validated on fivehumanoid robots with diverse configurations, demonstrating stableloco-manipulation and highlighting its effectiveness in reducing datarequirements and increasing the efficiency of skill transfer across platforms.</description><author>Junjia Liu, Zhuo Li, Minghao Yu, Zhipeng Dong, Sylvain Calinon, Darwin Caldwell, Fei Chen</author><pubDate>Thu, 19 Dec 2024 18:41:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15166v1</guid></item><item><title>Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents</title><link>http://arxiv.org/abs/2412.15163v1</link><description>Social norms are standards of behaviour common in a society. However, whenagents make decisions without considering how others are impacted, norms canemerge that lead to the subjugation of certain agents. We present RAWL-E, amethod to create ethical norm-learning agents. RAWL-E agents operationalisemaximin, a fairness principle from Rawlsian ethics, in their decision-makingprocesses to promote ethical norms by balancing societal well-being withindividual goals. We evaluate RAWL-E agents in simulated harvesting scenarios.We find that norms emerging in RAWL-E agent societies enhance social welfare,fairness, and robustness, and yield higher minimum experience compared to thosethat emerge in agent societies that do not implement Rawlsian ethics.</description><author>Jessica Woodgate, Paul Marshall, Nirav Ajmeri</author><pubDate>Thu, 19 Dec 2024 18:38:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15163v1</guid></item><item><title>OnlineVPO: Align Video Diffusion Model with Online Video-Centric Preference Optimization</title><link>http://arxiv.org/abs/2412.15159v1</link><description>In recent years, the field of text-to-video (T2V) generation has madesignificant strides. Despite this progress, there is still a gap betweentheoretical advancements and practical application, amplified by issues likedegraded image quality and flickering artifacts. Recent advancements inenhancing the video diffusion model (VDM) through feedback learning have shownpromising results. However, these methods still exhibit notable limitations,such as misaligned feedback and inferior scalability. To tackle these issues,we introduce OnlineVPO, a more efficient preference learning approach tailoredspecifically for video diffusion models. Our method features two novel designs,firstly, instead of directly using image-based reward feedback, we leverage thevideo quality assessment (VQA) model trained on synthetic data as the rewardmodel to provide distribution and modality-aligned feedback on the videodiffusion model. Additionally, we introduce an online DPO algorithm to addressthe off-policy optimization and scalability issue in existing video preferencelearning frameworks. By employing the video reward model to offer concise videofeedback on the fly, OnlineVPO offers effective and efficient preferenceguidance. Extensive experiments on the open-source video-diffusion modeldemonstrate OnlineVPO as a simple yet effective and more importantly scalablepreference learning algorithm for video diffusion models, offering valuableinsights for future advancements in this domain.</description><author>Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, Kai Han</author><pubDate>Thu, 19 Dec 2024 18:34:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15159v1</guid></item><item><title>Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned LLM</title><link>http://arxiv.org/abs/2412.15156v1</link><description>Text-to-video models have made remarkable advancements through optimizationon high-quality text-video pairs, where the textual prompts play a pivotal rolein determining quality of output videos. However, achieving the desired outputoften entails multiple revisions and iterative inference to refineuser-provided prompts. Current automatic methods for refining prompts encounterchallenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unawarewhen applied to text-to-video diffusion models. To address these problem, weintroduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,which excels in crafting Video-Centric, Labor-Free and Preference-Alignedprompts tailored to specific video diffusion model. Our approach involves ameticulously crafted two-stage optimization and alignment system. Initially, weconduct a reward-guided prompt evolution pipeline to automatically createoptimal prompts pool and leverage them for supervised fine-tuning (SFT) of theLLM. Then multi-dimensional rewards are employed to generate pairwise data forthe SFT model, followed by the direct preference optimization (DPO) algorithmto further facilitate preference alignment. Through extensive experimentationand comparative analyses, we validate the effectiveness of Prompt-A-Videoacross diverse generation models, highlighting its potential to push theboundaries of video generation.</description><author>Yatai Ji, Jiacheng Zhang, Jie Wu, Shilong Zhang, Shoufa Chen, Chongjian GE, Peize Sun, Weifeng Chen, Wenqi Shao, Xuefeng Xiao, Weilin Huang, Ping Luo</author><pubDate>Thu, 19 Dec 2024 18:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15156v1</guid></item><item><title>Language Models as Continuous Self-Evolving Data Engineers</title><link>http://arxiv.org/abs/2412.15151v1</link><description>Large Language Models (LLMs) have demonstrated remarkable capabilities onvarious tasks, while the further evolvement is limited to the lack ofhigh-quality training data. In addition, traditional training approaches relytoo much on expert-labeled data, setting an upper limit on the performance ofLLMs. To address this issue, we propose a novel paradigm that enables LLMs totrain itself by autonomously generating, cleaning, reviewing, and annotatingdata with preference information, named LANCE. Our approach demonstrates thatLLMs can serve as continuous self-evolving data engineers, significantlyreducing the time and cost of the post-training data construction process.Through iterative fine-tuning on different variants of the Qwen2, we validatethe effectiveness of LANCE across various tasks, showing that it cancontinuously improve model performance and maintain high-quality datageneration. Across eight benchmark dimensions, LANCE resulted in an averagescore enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. Thistraining paradigm with autonomous data construction not only reduces thereliance on human experts or external models but also ensures that the dataaligns with human values and preferences, paving the way for the development offuture superintelligent systems that can exceed human capabilities.</description><author>Peidong Wang, Ming Wang, Zhiming Ma, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang</author><pubDate>Thu, 19 Dec 2024 18:28:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15151v1</guid></item><item><title>Leveraging Color Channel Independence for Improved Unsupervised Object Detection</title><link>http://arxiv.org/abs/2412.15150v1</link><description>Object-centric architectures can learn to extract distinct objectrepresentations from visual scenes, enabling downstream applications on theobject level. Similarly to autoencoder-based image models, object-centricapproaches have been trained on the unsupervised reconstruction loss of imagesencoded by RGB color spaces. In our work, we challenge the common assumptionthat RGB images are the optimal color space for unsupervised learning incomputer vision. We discuss conceptually and empirically that other colorspaces, such as HSV, bear essential characteristics for object-centricrepresentation learning, like robustness to lighting conditions. We furthershow that models improve when requiring them to predict additional colorchannels. Specifically, we propose to transform the predicted targets to theRGB-S space, which extends RGB with HSV's saturation component and leads tomarkedly better reconstruction and disentanglement for five common evaluationdatasets. The use of composite color spaces can be implemented with basicallyno computational overhead, is agnostic of the models' architecture, and isuniversally applicable across a wide range of visual computing tasks andtraining types. The findings of our approach encourage additionalinvestigations in computer vision tasks beyond object-centric learning.</description><author>Bastian Jäckl, Yannick Metz, Udo Schlegel, Daniel A. Keim, Maximilian T. Fischer</author><pubDate>Thu, 19 Dec 2024 18:28:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15150v1</guid></item><item><title>Probabilistic Strategy Logic with Degrees of Observability</title><link>http://arxiv.org/abs/2412.15135v1</link><description>There has been considerable work on reasoning about the strategic ability ofagents under imperfect information. However, existing logics such asProbabilistic Strategy Logic are unable to express properties relating toinformation transparency. Information transparency concerns the extent to whichagents' actions and behaviours are observable by other agents. Reasoning aboutinformation transparency is useful in many domains including security, privacy,and decision-making. In this paper, we present a formal framework for reasoningabout information transparency properties in stochastic multi-agent systems. Weextend Probabilistic Strategy Logic with new observability operators thatcapture the degree of observability of temporal properties by agents. We showthat the model checking problem for the resulting logic is decidable.</description><author>Chunyan Mu, Nima Motamed, Natasha Alechina, Brian Logan</author><pubDate>Thu, 19 Dec 2024 18:17:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15135v1</guid></item><item><title>Jet: A Modern Transformer-Based Normalizing Flow</title><link>http://arxiv.org/abs/2412.15129v1</link><description>In the past, normalizing generative flows have emerged as a promising classof generative models for natural images. This type of model has many modelingadvantages: the ability to efficiently compute log-likelihood of the inputdata, fast generation and simple overall structure. Normalizing flows remaineda topic of active research but later fell out of favor, as visual quality ofthe samples was not competitive with other model classes, such as GANs,VQ-VAE-based approaches or diffusion models. In this paper we revisit thedesign of the coupling-based normalizing flow models by carefully ablatingprior design choices and using computational blocks based on the VisionTransformer architecture, not convolutional neural networks. As a result, weachieve state-of-the-art quantitative and qualitative performance with a muchsimpler architecture. While the overall visual quality is still behind thecurrent state-of-the-art models, we argue that strong normalizing flow modelscan help advancing research frontier by serving as building components of morepowerful generative models.</description><author>Alexander Kolesnikov, André Susano Pinto, Michael Tschannen</author><pubDate>Thu, 19 Dec 2024 18:09:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15129v1</guid></item><item><title>Adaptive Pruning for Large Language Models with Structural Importance Awareness</title><link>http://arxiv.org/abs/2412.15127v1</link><description>The recent advancements in large language models (LLMs) have significantlyimproved language understanding and generation capabilities. However, it isdifficult to deploy LLMs on resource-constrained edge devices due to their highcomputational and storage resource demands. To address this issue, we propose anovel LLM model pruning method, namely structurally-aware adaptive pruning(SAAP), to significantly reduce the computational and memory costs whilemaintaining model performance. We first define an adaptive importance fusionmetric to evaluate the importance of all coupled structures in LLMs byconsidering their homoscedastic uncertainty. Then, we rank the importance ofall modules to determine the specific layers that should be pruned to meetparticular performance requirements. Furthermore, we develop a new groupfine-tuning strategy to improve the inference efficiency of LLMs. Finally, weevaluate the proposed SAAP method on multiple LLMs across two common tasks,i.e., zero-shot classification and text generation. Experimental results showthat our SAAP method outperforms several state-of-the-art baseline methods,achieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, andLLaMA-13B. Additionally, SAAP improves the token generation speed by 5%,showcasing its practical advantages in resource-constrained scenarios.</description><author>Haotian Zheng, Jinke Ren, Yushan Sun, Ruichen Zhang, Wenbo Zhang, Zhen Li, Dusit Niyato, Shuguang Cui, Yatong Han</author><pubDate>Thu, 19 Dec 2024 18:08:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15127v1</guid></item><item><title>Parallelized Autoregressive Visual Generation</title><link>http://arxiv.org/abs/2412.15119v1</link><description>Autoregressive models have emerged as a powerful approach for visualgeneration but suffer from slow inference speed due to their sequentialtoken-by-token prediction process. In this paper, we propose a simple yeteffective approach for parallelized autoregressive visual generation thatimproves generation efficiency while preserving the advantages ofautoregressive modeling. Our key insight is that parallel generation depends onvisual token dependencies-tokens with weak dependencies can be generated inparallel, while strongly dependent adjacent tokens are difficult to generatetogether, as their independent sampling may lead to inconsistencies. Based onthis observation, we develop a parallel generation strategy that generatesdistant tokens with weak dependencies in parallel while maintaining sequentialgeneration for strongly dependent local tokens. Our approach can be seamlesslyintegrated into standard autoregressive models without modifying thearchitecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate thatour method achieves a 3.6x speedup with comparable quality and up to 9.5xspeedup with minimal quality degradation across both image and video generationtasks. We hope this work will inspire future research in efficient visualgeneration and unified autoregressive modeling. Project page:https://epiphqny.github.io/PAR-project.</description><author>Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, Xihui Liu</author><pubDate>Thu, 19 Dec 2024 17:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15119v1</guid></item><item><title>Outcome-Refining Process Supervision for Code Generation</title><link>http://arxiv.org/abs/2412.15118v1</link><description>Large Language Models have demonstrated remarkable capabilities in codegeneration, yet they often struggle with complex programming tasks that requiredeep algorithmic reasoning. While process supervision through learned rewardmodels shows promise in guiding reasoning steps, it requires expensive trainingdata and suffers from unreliable evaluation. We propose Outcome-RefiningProcess Supervision, a novel paradigm that treats outcome refinement itself asthe process to be supervised. Our framework leverages concrete executionsignals to ground the supervision of reasoning steps, while usingtree-structured exploration to maintain multiple solution trajectoriessimultaneously. Experiments demonstrate that our approach enables even smallermodels to achieve high success accuracy and performance metrics on competitiveprogramming tasks, creates more reliable verification than traditional rewardmodels without requiring training PRMs. Our approach achieves significantimprovements across 5 models and 3 datasets: an average of 26.9% increase incorrectness and 42.2% in efficiency. The results suggest that providingstructured reasoning space with concrete verification signals is crucial forsolving complex programming tasks. We open-source all our code and data at:https://github.com/zhuohaoyu/ORPS</description><author>Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang</author><pubDate>Thu, 19 Dec 2024 17:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15118v1</guid></item><item><title>Does VLM Classification Benefit from LLM Description Semantics?</title><link>http://arxiv.org/abs/2412.11917v3</link><description>Accurately describing images with text is a foundation of explainable AI.Vision-Language Models (VLMs) like CLIP have recently addressed this byaligning images and texts in a shared embedding space, expressing semanticsimilarities between vision and language embeddings. VLM classification can beimproved with descriptions generated by Large Language Models (LLMs). However,it is difficult to determine the contribution of actual description semantics,as the performance gain may also stem from a semantic-agnostic ensemblingeffect, where multiple modified text prompts act as a noisy test-timeaugmentation for the original one. We propose an alternative evaluationscenario to decide if a performance boost of LLM-generated descriptions iscaused by such a noise augmentation effect or rather by genuine descriptionsemantics. The proposed scenario avoids noisy test-time augmentation andensures that genuine, distinctive descriptions cause the performance boost.Furthermore, we propose a training-free method for selecting discriminativedescriptions that work independently of classname-ensembling effects. Ourapproach identifies descriptions that effectively differentiate classes withina local CLIP label neighborhood, improving classification accuracy across sevendatasets. Additionally, we provide insights into the explainability ofdescription-based image classification with VLMs.</description><author>Pingchuan Ma, Lennart Rietdorf, Dmytro Kotovenko, Vincent Tao Hu, Björn Ommer</author><pubDate>Thu, 19 Dec 2024 17:57:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11917v3</guid></item><item><title>URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological and Multilingual Knowledge Base</title><link>http://arxiv.org/abs/2409.18472v2</link><description>URIEL is a knowledge base offering geographical, phylogenetic, andtypological vector representations for 7970 languages. It includes distancemeasures between these vectors for 4005 languages, which are accessible via thelang2vec tool. Despite being frequently cited, URIEL is limited in terms oflinguistic inclusion and overall usability. To tackle these challenges, weintroduce URIEL+, an enhanced version of URIEL and lang2vec that addressesthese limitations. In addition to expanding typological feature coverage for2898 languages, URIEL+ improves the user experience with robust, customizabledistance calculations to better suit the needs of users. These upgrades alsooffer competitive performance on downstream tasks and provide distances thatbetter align with linguistic distance studies.</description><author>Aditya Khan, Mason Shipton, David Anugraha, Kaiyao Duan, Phuong H. Hoang, Eric Khiu, A. Seza Doğruöz, En-Shiun Annie Lee</author><pubDate>Thu, 19 Dec 2024 17:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18472v2</guid></item><item><title>Qwen2.5 Technical Report</title><link>http://arxiv.org/abs/2412.15115v1</link><description>In this report, we introduce Qwen2.5, a comprehensive series of largelanguage models (LLMs) designed to meet diverse needs. Compared to previousiterations, Qwen 2.5 has been significantly improved during both thepre-training and post-training stages. In terms of pre-training, we have scaledthe high-quality pre-training datasets from the previous 7 trillion tokens to18 trillion tokens. This provides a strong foundation for common sense, expertknowledge, and reasoning capabilities. In terms of post-training, we implementintricate supervised finetuning with over 1 million samples, as well asmultistage reinforcement learning. Post-training techniques enhance humanpreference, and notably improve long text generation, structural data analysis,and instruction following. To handle diverse and varied use cases effectively,we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include baseand instruction-tuned models, with quantized versions available. In addition,for hosted solutions, the proprietary models currently include twomixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, bothavailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tierperformance on a wide range of benchmarks evaluating language understanding,reasoning, mathematics, coding, human preference alignment, etc. Specifically,the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open andproprietary models and demonstrates competitive performance to thestate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectivenesswhile performing competitively against GPT-4o-mini and GPT-4o respectively.Additionally, as the foundation, Qwen2.5 models have been instrumental intraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, andmultimodal models.</description><author>Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu</author><pubDate>Thu, 19 Dec 2024 17:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15115v1</guid></item><item><title>Towards Friendly AI: A Comprehensive Review and New Perspectives on Human-AI Alignment</title><link>http://arxiv.org/abs/2412.15114v1</link><description>As Artificial Intelligence (AI) continues to advance rapidly, Friendly AI(FAI) has been proposed to advocate for more equitable and fair development ofAI. Despite its importance, there is a lack of comprehensive reviews examiningFAI from an ethical perspective, as well as limited discussion on its potentialapplications and future directions. This paper addresses these gaps byproviding a thorough review of FAI, focusing on theoretical perspectives bothfor and against its development, and presenting a formal definition in a clearand accessible format. Key applications are discussed from the perspectives ofeXplainable AI (XAI), privacy, fairness and affective computing (AC).Additionally, the paper identifies challenges in current technologicaladvancements and explores future research avenues. The findings emphasise thesignificance of developing FAI and advocate for its continued advancement toensure ethical and beneficial AI development.</description><author>Qiyang Sun, Yupei Li, Emran Alturki, Sunil Munthumoduku Krishna Murthy, Björn W. Schuller</author><pubDate>Thu, 19 Dec 2024 17:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15114v1</guid></item><item><title>Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture</title><link>http://arxiv.org/abs/2412.15113v1</link><description>Large language models (LLMs) demonstrate an impressive ability to utiliseinformation within the context of their input sequences to appropriatelyrespond to data unseen by the LLM during its training procedure. This abilityis known as in-context learning (ICL). Humans and non-human animals demonstratesimilar abilities, however their neural architectures differ substantially fromLLMs. Despite this, a critical component within LLMs, the attention mechanism,resembles modern associative memory models, widely used in and influenced bythe computational neuroscience community to model biological memory systems.Using this connection, we introduce an associative memory model capable ofperforming ICL. We use this as inspiration for a novel residual streamarchitecture which allows information to directly flow between attention heads.We test this architecture during training within a two-layer Transformer andshow its ICL abilities manifest more quickly than without this modification. Wethen apply our architecture in small language models with 8 million parameters,focusing on attention head values, with results also indicating improved ICLperformance at this larger and more naturalistic scale.</description><author>Thomas F Burns, Tomoki Fukai, Christopher J Earls</author><pubDate>Thu, 19 Dec 2024 17:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15113v1</guid></item><item><title>Knowing Where to Focus: Attention-Guided Alignment for Text-based Person Search</title><link>http://arxiv.org/abs/2412.15106v1</link><description>In the realm of Text-Based Person Search (TBPS), mainstream methods aim toexplore more efficient interaction frameworks between text descriptions andvisual data. However, recent approaches encounter two principal challenges.Firstly, the widely used random-based Masked Language Modeling (MLM) considersall the words in the text equally during training. However, massivesemantically vacuous words ('with', 'the', etc.) be masked fail to contributeefficient interaction in the cross-modal MLM and hampers the representationalignment. Secondly, manual descriptions in TBPS datasets are tedious andinevitably contain several inaccuracies. To address these issues, we introducean Attention-Guided Alignment (AGA) framework featuring two innovativecomponents: Attention-Guided Mask (AGM) Modeling and Text Enrichment Module(TEM). AGM dynamically masks semantically meaningful words by aggregating theattention weight derived from the text encoding process, thereby cross-modalMLM can capture information related to the masked word from text context andimages and align their representations. Meanwhile, TEM alleviates low-qualityrepresentations caused by repetitive and erroneous text descriptions byreplacing those semantically meaningful words with MLM's prediction. It notonly enriches text descriptions but also prevents overfitting. Extensiveexperiments across three challenging benchmarks demonstrate the effectivenessof our AGA, achieving new state-of-the-art results with Rank-1 accuracyreaching 78.36%, 67.31%, and 67.4% on CUHK-PEDES, ICFG-PEDES, and RSTPReid,respectively.</description><author>Lei Tan, Weihao Li, Pingyang Dai, Jie Chen, Liujuan Cao, Rongrong Ji</author><pubDate>Thu, 19 Dec 2024 17:51:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15106v1</guid></item><item><title>Exploiting sparse structures and synergy designs to advance situational awareness of electrical power grid</title><link>http://arxiv.org/abs/2412.15105v1</link><description>The growing threats of uncertainties, anomalies, and cyberattacks on powergrids are driving a critical need to advance situational awareness which allowssystem operators to form a complete and accurate picture of the present andfuture state. Simulation and estimation are foundational tools in this process.However, existing tools lack the robustness and efficiency required to achievethe level of situational awareness needed for the ever-evolving threatlandscape. Industry-standard (steady-state) simulators are not robust toblackouts, often leading to non-converging or non-actionable results.Estimation tools lack robustness to anomalous data, returning erroneous systemstates. Efficiency is the other major concern as nonlinearities and scalabilityissues make large systems slow to converge. This thesis addresses robustness and efficiency gaps through a dual-foldcontribution. We first address the inherent limitations in the existingphysics-based and data-driven worlds; and then transcend the boundaries ofconventional algorithmic design in the direction of a new paradigm --Physics-ML Synergy -- which integrates the strengths of the two worlds. Ourapproaches are built on circuit formulation which provides a unified frameworkthat applies to both transmission and distribution. Sparse optimization acts asthe key enabler to make these tools intrinsically robust and immune to randomthreats, pinpointing dominant sources of (random) blackouts and data errors.Further, we explore sparsity-exploiting optimizations to develop lightweight MLmodels whose prediction and detection capabilities are a complement tophysics-based tools; and whose lightweight designs advance generalization andscalability. Finally, Physics-ML Synergy brings robustness and efficiencyfurther against targeted cyberthreats, by interconnecting our physics-basedtools with lightweight ML.</description><author>Shimiao Li</author><pubDate>Thu, 19 Dec 2024 17:51:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15105v1</guid></item><item><title>DepthFM: Fast Monocular Depth Estimation with Flow Matching</title><link>http://arxiv.org/abs/2403.13788v2</link><description>Current discriminative depth estimation methods often produce blurryartifacts, while generative approaches suffer from slow sampling due tocurvatures in the noise-to-depth transport. Our method addresses thesechallenges by framing depth estimation as a direct transport between image anddepth distributions. We are the first to explore flow matching in this field,and we demonstrate that its interpolation trajectories enhance both trainingand sampling efficiency while preserving high performance. While generativemodels typically require extensive training data, we mitigate this dependencyby integrating external knowledge from a pre-trained image diffusion model,enabling effective transfer even across differing objectives. To further boostour model performance, we employ synthetic data and utilize image-depth pairsgenerated by a discriminative model on an in-the-wild image dataset. As agenerative model, our model can reliably estimate depth confidence, whichprovides an additional advantage. Our approach achieves competitive zero-shotperformance on standard benchmarks of complex natural scenes while improvingsampling efficiency and only requiring minimal synthetic data for training.</description><author>Ming Gui, Johannes Schusterbauer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, Björn Ommer</author><pubDate>Thu, 19 Dec 2024 17:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13788v2</guid></item><item><title>Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization</title><link>http://arxiv.org/abs/2412.04619v3</link><description>Language models (LMs), like other neural networks, often favor shortcutheuristics based on surface-level patterns. Although LMs behave like n-grammodels early in training, they must eventually learn hierarchical syntacticrepresentations to correctly apply grammatical rules out-of-distribution (OOD).In this work, we use case studies of English grammar to explore how complex,diverse training data drives models to generalize OOD. We construct a frameworkthat unifies our understanding of random variation with training dynamics, ruleselection with memorization, and data diversity with complexity. We show thatthese factors are nuanced, and that intermediate levels of diversity andcomplexity lead to inconsistent behavior across random seeds and to unstabletraining dynamics. Our findings emphasize the critical role of training data inshaping generalization patterns and illuminate how competing model strategieslead to inconsistent generalization outcomes across random seeds. Code isavailable at https://github.com/sunnytqin/concept_comp.git.</description><author>Tian Qin, Naomi Saphra, David Alvarez-Melis</author><pubDate>Thu, 19 Dec 2024 17:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04619v3</guid></item><item><title>Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering with Temporal Adaptability</title><link>http://arxiv.org/abs/2412.15101v1</link><description>Retrieve-augmented generation (RAG) frameworks have emerged as a promisingsolution to multi-hop question answering(QA) tasks since it enables largelanguage models (LLMs) to incorporate external knowledge and mitigate theirinherent knowledge deficiencies. Despite this progress, existing RAGframeworks, which usually follows the retrieve-then-read paradigm, oftenstruggle with multi-hop QA with temporal information since it has difficultyretrieving and synthesizing accurate time-related information. To address thechallenge, this paper proposes a novel framework called review-then-refine,which aims to enhance LLM performance in multi-hop QA scenarios with temporalinformation. Our approach begins with a review phase, where decomposedsub-queries are dynamically rewritten with temporal information, allowing forsubsequent adaptive retrieval and reasoning process. In addition, we implementadaptive retrieval mechanism to minimize unnecessary retrievals, thus reducingthe potential for hallucinations. In the subsequent refine phase, the LLMsynthesizes the retrieved information from each sub-query along with itsinternal knowledge to formulate a coherent answer. Extensive experimentalresults across multiple datasets demonstrate the effectiveness of our proposedframework, highlighting its potential to significantly improve multi-hop QAcapabilities in LLMs.</description><author>Xiangsen Chen, Xuming Hu, Nan Tang</author><pubDate>Thu, 19 Dec 2024 17:48:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15101v1</guid></item><item><title>Tests for model misspecification in simulation-based inference: from local distortions to global model checks</title><link>http://arxiv.org/abs/2412.15100v1</link><description>Model misspecification analysis strategies, such as anomaly detection, modelvalidation, and model comparison are a key component of scientific modeldevelopment. Over the last few years, there has been a rapid rise in the use ofsimulation-based inference (SBI) techniques for Bayesian parameter estimation,applied to increasingly complex forward models. To move towards fullysimulation-based analysis pipelines, however, there is an urgent need for acomprehensive simulation-based framework for model misspecification analysis.In this work, we provide a solid and flexible foundation for a wide range ofmodel discrepancy analysis tasks, using distortion-driven modelmisspecification tests. From a theoretical perspective, we introduce thestatistical framework built around performing many hypothesis tests fordistortions of the simulation model. We also make explicit analytic connectionsto classical techniques: anomaly detection, model validation, andgoodness-of-fit residual analysis. Furthermore, we introduce an efficientself-calibrating training algorithm that is useful for practitioners. Wedemonstrate the performance of the framework in multiple scenarios, making theconnection to classical results where they are valid. Finally, we show how toconduct such a distortion-driven model misspecification test for realgravitational wave data, specifically on the event GW150914.</description><author>Noemi Anau Montel, James Alvey, Christoph Weniger</author><pubDate>Thu, 19 Dec 2024 17:48:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15100v1</guid></item><item><title>A Cross-Domain Study of the Use of Persuasion Techniques in Online Disinformation</title><link>http://arxiv.org/abs/2412.15098v1</link><description>Disinformation, irrespective of domain or language, aims to deceive ormanipulate public opinion, typically through employing advanced persuasiontechniques. Qualitative and quantitative research on the weaponisation ofpersuasion techniques in disinformation has been mostly topic-specific (e.g.,COVID-19) with limited cross-domain studies, resulting in a lack ofcomprehensive understanding of these strategies. This study employs astate-of-the-art persuasion technique classifier to conduct a large-scale,multi-domain analysis of the role of 16 persuasion techniques in disinformationnarratives. It shows how different persuasion techniques are employeddisproportionately in different disinformation domains. We also include adetailed case study on climate change disinformation, highlighting howlinguistic, psychological, and cultural factors shape the adaptation ofpersuasion strategies to fit unique thematic contexts.</description><author>João A. Leite, Olesya Razuvayevskaya, Carolina Scarton, Kalina Bontcheva</author><pubDate>Thu, 19 Dec 2024 17:46:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15098v1</guid></item><item><title>A Full Transformer-based Framework for Automatic Pain Estimation using Videos</title><link>http://arxiv.org/abs/2412.15095v1</link><description>The automatic estimation of pain is essential in designing an optimal painmanagement system offering reliable assessment and reducing the suffering ofpatients. In this study, we present a novel full transformer-based frameworkconsisting of a Transformer in Transformer (TNT) model and a Transformerleveraging cross-attention and self-attention blocks. Elaborating on videosfrom the BioVid database, we demonstrate state-of-the-art performances, showingthe efficacy, efficiency, and generalization capability across all the primarypain estimation tasks.</description><author>Stefanos Gkikas, Manolis Tsiknakis</author><pubDate>Thu, 19 Dec 2024 17:45:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15095v1</guid></item><item><title>Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models</title><link>http://arxiv.org/abs/2412.13702v2</link><description>This paper introduces Typhoon 2, a series of text and multimodal largelanguage models optimized for the Thai language. The series includes models fortext, vision, and audio. Typhoon2-Text builds on state-of-the-art open models,such as Llama 3 and Qwen2, and we perform continual pre-training on a mixtureof English and Thai data. We employ post-training techniques to enhance Thailanguage performance while preserving the base models' original capabilities.We release text models across a range of sizes, from 1 to 70 billionparameters, available in both base and instruction-tuned variants. To guardrailtext generation, we release Typhoon2-Safety, a classifier enhanced for Thaicultures and language. Typhoon2-Vision improves Thai document understandingwhile retaining general visual capabilities, such as image captioning.Typhoon2-Audio introduces an end-to-end speech-to-speech model architecturecapable of processing audio, speech, and text inputs and generating both textand speech outputs.</description><author>Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai</author><pubDate>Thu, 19 Dec 2024 17:36:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13702v2</guid></item><item><title>Learning Disentangled Equivariant Representation for Explicitly Controllable 3D Molecule Generation</title><link>http://arxiv.org/abs/2412.15086v1</link><description>We consider the conditional generation of 3D drug-like molecules with\textit{explicit control} over molecular properties such as drug-likeproperties (e.g., Quantitative Estimate of Druglikeness or SyntheticAccessibility score) and effectively binding to specific protein sites. Totackle this problem, we propose an E(3)-equivariant Wasserstein autoencoder andfactorize the latent space of our generative model into two disentangledaspects: molecular properties and the remaining structural context of 3Dmolecules. Our model ensures explicit control over these molecular attributeswhile maintaining equivariance of coordinate representation and invariance ofdata likelihood. Furthermore, we introduce a novel alignment-based coordinateloss to adapt equivariant networks for auto-regressive de-novo 3D moleculegeneration from scratch. Extensive experiments validate our model'seffectiveness on property-guided and context-guided molecule generation, bothfor de-novo 3D molecule design and structure-based drug discovery againstprotein targets.</description><author>Haoran Liu, Youzhi Luo, Tianxiao Li, James Caverlee, Martin Renqiang Min</author><pubDate>Thu, 19 Dec 2024 17:33:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15086v1</guid></item><item><title>AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling</title><link>http://arxiv.org/abs/2412.15084v1</link><description>In this paper, we introduce AceMath, a suite of frontier math models thatexcel in solving complex math problems, along with highly effective rewardmodels capable of evaluating generated solutions and reliably identifying thecorrect ones. To develop the instruction-tuned math models, we propose asupervised fine-tuning (SFT) process that first achieves competitiveperformance across general domains, followed by targeted fine-tuning for themath domain using a carefully curated set of prompts and syntheticallygenerated responses. The resulting model, AceMath-72B-Instruct greatlyoutperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To developmath-specialized reward model, we first construct AceMath-RewardBench, acomprehensive and robust benchmark for evaluating math reward models acrossdiverse problems and difficulty levels. After that, we present a systematicapproach to build our math reward models. The resulting model, AceMath-72B-RM,consistently outperforms state-of-the-art reward models. Furthermore, whencombining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highestaverage rm@8 score across the math reasoning benchmarks. We will release modelweights, training data, and evaluation benchmarks at:https://research.nvidia.com/labs/adlr/acemath</description><author>Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping</author><pubDate>Thu, 19 Dec 2024 17:29:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15084v1</guid></item><item><title>Till the Layers Collapse: Compressing a Deep Neural Network through the Lenses of Batch Normalization Layers</title><link>http://arxiv.org/abs/2412.15077v1</link><description>Today, deep neural networks are widely used since they can handle a varietyof complex tasks. Their generality makes them very powerful tools in moderntechnology. However, deep neural networks are often overparameterized. Theusage of these large models consumes a lot of computation resources. In thispaper, we introduce a method called \textbf{T}ill the \textbf{L}ayers\textbf{C}ollapse (TLC), which compresses deep neural networks through thelenses of batch normalization layers. By reducing the depth of these networks,our method decreases deep neural networks' computational requirements andoverall latency. We validate our method on popular models such as Swin-T,MobileNet-V2, and RoBERTa, across both image classification and naturallanguage processing (NLP) tasks.</description><author>Zhu Liao, Nour Hezbri, Victor Quétu, Van-Tam Nguyen, Enzo Tartaglione</author><pubDate>Thu, 19 Dec 2024 17:26:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15077v1</guid></item><item><title>DroughtSet: Understanding Drought Through Spatial-Temporal Learning</title><link>http://arxiv.org/abs/2412.15075v1</link><description>Drought is one of the most destructive and expensive natural disasters,severely impacting natural resources and risks by depleting water resources anddiminishing agricultural yields. Under climate change, accurately predictingdrought is critical for mitigating drought-induced risks. However, theintricate interplay among the physical and biological drivers that regulatedroughts limits the predictability and understanding of drought, particularlyat a subseasonal to seasonal (S2S) time scale. While deep learning has beendemonstrated with potential in addressing climate forecasting challenges, itsapplication to drought prediction has received relatively less attention. Inthis work, we propose a new dataset, DroughtSet, which integrates relevantpredictive features and three drought indices from multiple remote sensing andreanalysis datasets across the contiguous United States (CONUS). DroughtSetspecifically provides the machine learning community with a new real-worlddataset to benchmark drought prediction models and more generally, time-seriesforecasting methods. Furthermore, we propose a spatial-temporal model SPDroughtto predict and interpret S2S droughts. Our model learns from the spatial andtemporal information of physical and biological features to predict three typesof droughts simultaneously. Multiple strategies are employed to quantify theimportance of physical and biological features for drought prediction. Ourresults provide insights for researchers to better understand thepredictability and sensitivity of drought to biological and physicalconditions. We aim to contribute to the climate field by proposing a new toolto predict and understand the occurrence of droughts and provide the AIcommunity with a new benchmark to study deep learning applications in climatescience.</description><author>Xuwei Tan, Qian Zhao, Yanlan Liu, Xueru Zhang</author><pubDate>Thu, 19 Dec 2024 17:24:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15075v1</guid></item><item><title>Latent Ewald summation for machine learning of long-range interactions</title><link>http://arxiv.org/abs/2408.15165v2</link><description>Machine learning interatomic potentials (MLIPs) often neglect long-rangeinteractions, such as electrostatic and dispersion forces. In this work, weintroduce a straightforward and efficient method to account for long-rangeinteractions by learning a latent variable from local atomic descriptors andapplying an Ewald summation to this variable. We demonstrate that in systemsincluding charged and polar molecular dimers, bulk water, and water-vaporinterface, standard short-ranged MLIPs can lead to unphysical predictions evenwhen employing message passing. The long-range models effectively eliminatethese artifacts, with only about twice the computational cost of short-rangeMLIPs.</description><author>Bingqing Cheng</author><pubDate>Thu, 19 Dec 2024 17:11:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15165v2</guid></item><item><title>Graph-neural-network predictions of solid-state NMR parameters from spherical tensor decomposition</title><link>http://arxiv.org/abs/2412.15063v1</link><description>Nuclear magnetic resonance (NMR) is a powerful spectroscopic technique thatis sensitive to the local atomic structure of matter. Computational predictionsof NMR parameters can help to interpret experimental data and validatestructural models, and machine learning (ML) has emerged as an efficient routeto making such predictions. Here, we systematically study graph-neural-networkapproaches to representing and learning tensor quantities for solid-state NMR-- specifically, the anisotropic magnetic shielding and the electric fieldgradient. We assess how the numerical accuracy of different ML modelstranslates into prediction quality for experimentally relevant NMR properties:chemical shifts, quadrupolar coupling constants, tensor orientations, and evenstatic 1D spectra. We apply these ML models to a structurally diverse datasetof amorphous SiO$_2$ configurations, spanning a wide range of density and localorder, to larger configurations beyond the reach of traditionalfirst-principles methods, and to the dynamics of the$\alpha\unicode{x2013}\beta$ inversion in cristobalite. Our work marks a steptoward streamlining ML-driven NMR predictions for both static and dynamicbehavior of complex materials, and toward bridging the gap betweenfirst-principles modeling and real-world experimental data.</description><author>Chiheb Ben Mahmoud, Louise A. M. Rosset, Jonathan R. Yates, Volker L. Deringer</author><pubDate>Thu, 19 Dec 2024 17:11:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15063v1</guid></item><item><title>ConfliBERT: A Language Model for Political Conflict</title><link>http://arxiv.org/abs/2412.15060v1</link><description>Conflict scholars have used rule-based approaches to extract informationabout political violence from news reports and texts. Recent Natural LanguageProcessing developments move beyond rigid rule-based approaches. We review ourrecent ConfliBERT language model (Hu et al. 2022) to process political andviolence related texts. The model can be used to extract actor and actionclassifications from texts about political conflict. When fine-tuned, resultsshow that ConfliBERT has superior performance in accuracy, precision and recallover other large language models (LLM) like Google's Gemma 2 (9B), Meta's Llama3.1 (7B), and Alibaba's Qwen 2.5 (14B) within its relevant domains. It is alsohundreds of times faster than these more generalist LLMs. These results areillustrated using texts from the BBC, re3d, and the Global Terrorism Dataset(GTD).</description><author>Patrick T. Brandt, Sultan Alsarra, Vito J. D`Orazio, Dagmar Heintze, Latifur Khan, Shreyas Meher, Javier Osorio, Marcus Sianan</author><pubDate>Thu, 19 Dec 2024 17:08:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15060v1</guid></item><item><title>MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging Datasets with In-Context Guidance</title><link>http://arxiv.org/abs/2412.15058v1</link><description>Medical researchers and clinicians often need to perform novel segmentationtasks on a set of related images. Existing methods for segmenting a new datasetare either interactive, requiring substantial human effort for each image, orrequire an existing set of manually labeled images. We introduce a system,MultiverSeg, that enables practitioners to rapidly segment an entire newdataset without requiring access to any existing labeled data from that task ordomain. Along with the image to segment, the model takes user interactions suchas clicks, bounding boxes or scribbles as input, and predicts a segmentation.As the user segments more images, those images and segmentations becomeadditional inputs to the model, providing context. As the context set oflabeled images grows, the number of interactions required to segment each newimage decreases. We demonstrate that MultiverSeg enables users to interactivelysegment new datasets efficiently, by amortizing the number of interactions perimage to achieve an accurate segmentation. Compared to using a state-of-the-artinteractive segmentation method, using MultiverSeg reduced the total number ofscribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of imagesfrom unseen tasks. We release code and model weights athttps://multiverseg.csail.mit.edu</description><author>Hallee E. Wong, Jose Javier Gonzalez Ortiz, John Guttag, Adrian V. Dalca</author><pubDate>Thu, 19 Dec 2024 17:06:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15058v1</guid></item><item><title>GIRAFE: Glottal Imaging Dataset for Advanced Segmentation, Analysis, and Facilitative Playbacks Evaluation</title><link>http://arxiv.org/abs/2412.15054v1</link><description>The advances in the development of Facilitative Playbacks extracted fromHigh-Speed videoendoscopic sequences of the vocal folds are hindered by anotable lack of publicly available datasets annotated with the semanticsegmentations corresponding to the area of the glottal gap. This fact alsolimits the reproducibility and further exploration of existing research in thisfield. To address this gap, GIRAFE is a data repository designed to facilitate thedevelopment of advanced techniques for the semantic segmentation, analysis, andfast evaluation of High-Speed videoendoscopic sequences of the vocal folds. Therepository includes 65 high-speed videoendoscopic recordings from a cohort of50 patients (30 female, 20 male). The dataset comprises 15 recordings fromhealthy controls, 26 from patients with diagnosed voice disorders, and 24 withan unknown health condition. All of them were manually annotated by an expert,including the masks corresponding to the semantic segmentation of the glottalgap. The repository is also complemented with the automatic segmentation of theglottal area using different state-of-the-art approaches. This data set has already supported several studies, which demonstrates itsusefulness for the development of new glottal gap segmentation algorithms fromHigh-Speed-Videoendoscopic sequences to improve or create new FacilitativePlaybacks. Despite these advances and others in the field, the broaderchallenge of performing an accurate and completely automatic semanticsegmentation method of the glottal area remains open.</description><author>G. Andrade-Miranda, K. Chatzipapas, J. D. Arias-Londoño, J. I. Godino-Llorente</author><pubDate>Thu, 19 Dec 2024 17:02:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15054v1</guid></item><item><title>Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream Diffusion</title><link>http://arxiv.org/abs/2412.15050v1</link><description>Rendering and inverse rendering are pivotal tasks in both computer vision andgraphics. The rendering equation is the core of the two tasks, as an idealconditional distribution transfer function from intrinsic properties to RGBimages. Despite achieving promising results of existing rendering methods, theymerely approximate the ideal estimation for a specific scene and come with ahigh computational cost. Additionally, the inverse conditional distributiontransfer is intractable due to the inherent ambiguity. To address thesechallenges, we propose a data-driven method that jointly models rendering andinverse rendering as two conditional generation tasks within a single diffusionframework. Inspired by UniDiffuser, we utilize two distinct time schedules tomodel both tasks, and with a tailored dual streaming module, we achievecross-conditioning of two pre-trained diffusion models. This unified approach,named Uni-Renderer, allows the two processes to facilitate each other through acycle-consistent constrain, mitigating ambiguity by enforcing consistencybetween intrinsic properties and rendered images. Combined with a meticulouslyprepared dataset, our method effectively decomposition of intrinsic propertiesand demonstrates a strong capability to recognize changes during rendering. Wewill open-source our training and inference code to the public, fosteringfurther research and development in this area.</description><author>Zhifei Chen, Tianshuo Xu, Wenhang Ge, Leyi Wu, Dongyu Yan, Jing He, Luozhou Wang, Lu Zeng, Shunsi Zhang, Yingcong Chen</author><pubDate>Thu, 19 Dec 2024 16:57:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15050v1</guid></item><item><title>Measuring, Modeling, and Helping People Account for Privacy Risks in Online Self-Disclosures with AI</title><link>http://arxiv.org/abs/2412.15047v1</link><description>In pseudonymous online fora like Reddit, the benefits of self-disclosure areoften apparent to users (e.g., I can vent about my in-laws to understandingstrangers), but the privacy risks are more abstract (e.g., will my partner beable to tell that this is me?). Prior work has sought to develop naturallanguage processing (NLP) tools that help users identify potentially riskyself-disclosures in their text, but none have been designed for or evaluatedwith the users they hope to protect. Absent this assessment, these tools willbe limited by the social-technical gap: users need assistive tools that helpthem make informed decisions, not paternalistic tools that tell them to avoidself-disclosure altogether. To bridge this gap, we conducted a study with N =21 Reddit users; we had them use a state-of-the-art NLP disclosure detectionmodel on two of their authored posts and asked them questions to understand ifand how the model helped, where it fell short, and how it could be improved tohelp them make more informed decisions. Despite its imperfections, usersresponded positively to the model and highlighted its use as a tool that canhelp them catch mistakes, inform them of risks they were unaware of, andencourage self-reflection. However, our work also shows how, to be useful andusable, AI for supporting privacy decision-making must account for postingcontext, disclosure norms, and users' lived threat models, and provideexplanations that help contextualize detected risks.</description><author>Isadora Krsek, Anubha Kabra, Yao Dou, Tarek Naous, Laura A. Dabbish, Alan Ritter, Wei Xu, Sauvik Das</author><pubDate>Thu, 19 Dec 2024 16:53:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15047v1</guid></item><item><title>Revisiting Machine Unlearning with Dimensional Alignment</title><link>http://arxiv.org/abs/2407.17710v2</link><description>Machine unlearning, an emerging research topic focusing on compliance withdata privacy regulations, enables trained models to remove the informationlearned from specific data. While many existing methods indirectly address thisissue by intentionally injecting incorrect supervisions, they can drasticallyand unpredictably alter the decision boundaries and feature spaces, leading totraining instability and undesired side effects. To fundamentally approach thistask, we first analyze the changes in latent feature spaces between originaland retrained models, and observe that the feature representations of samplesnot involved in training are closely aligned with the feature manifolds ofpreviously seen samples in training. Based on these findings, we introduce anovel evaluation metric for machine unlearning, coined dimensional alignment,which measures the alignment between the eigenspaces of the forget and retainset samples. We employ this metric as a regularizer loss to build a robust andstable unlearning framework, which is further enhanced by integrating aself-distillation loss and an alternating training scheme. Our frameworkeffectively eliminates information from the forget set and preserves knowledgefrom the retain set. Lastly, we identify critical flaws in establishedevaluation metrics for machine unlearning, and introduce new evaluation toolsthat more accurately reflect the fundamental goals of machine unlearning.</description><author>Seonguk Seo, Dongwan Kim, Bohyung Han</author><pubDate>Thu, 19 Dec 2024 16:48:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17710v2</guid></item><item><title>LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps</title><link>http://arxiv.org/abs/2412.15035v1</link><description>Building safe Large Language Models (LLMs) across multiple languages isessential in ensuring both safe access and linguistic diversity. To this end,we introduce M-ALERT, a multilingual benchmark that evaluates the safety ofLLMs in five languages: English, French, German, Italian, and Spanish. M-ALERTincludes 15k high-quality prompts per language, totaling 75k, following thedetailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMshighlight the importance of language-specific safety analysis, revealing thatmodels often exhibit significant inconsistencies in safety across languages andcategories. For instance, Llama3.2 shows high unsafety in the categorycrime_tax for Italian but remains safe in other languages. Similar differencescan be observed across all models. In contrast, certain categories, such assubstance_cannabis and crime_propaganda, consistently trigger unsafe responsesacross models and languages. These findings underscore the need for robustmultilingual safety practices in LLMs to ensure safe and responsible usageacross diverse user communities.</description><author>Felix Friedrich, Simone Tedeschi, Patrick Schramowski, Manuel Brack, Roberto Navigli, Huu Nguyen, Bo Li, Kristian Kersting</author><pubDate>Thu, 19 Dec 2024 16:46:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15035v1</guid></item><item><title>Metric Compatible Training for Online Backfilling in Large-Scale Retrieval</title><link>http://arxiv.org/abs/2301.03767v2</link><description>Backfilling is the process of re-extracting all gallery embeddings fromupgraded models in image retrieval systems. It inevitably requires aprohibitively large amount of computational cost and even entails the downtimeof the service. Although backward-compatible learning sidesteps this challengeby tackling query-side representations, this leads to suboptimal solutions inprinciple because gallery embeddings cannot benefit from model upgrades. Weaddress this dilemma by introducing an online backfilling algorithm, whichenables us to achieve a progressive performance improvement during thebackfilling process while not sacrificing the final performance of new modelafter the completion of backfilling. To this end, we first propose a simpledistance rank merge technique for online backfilling. Then, we incorporate areverse transformation module for more effective and efficient merging, whichis further enhanced by adopting a metric-compatible contrastive learningapproach. These two components help to make the distances of old and new modelscompatible, resulting in desirable merge results during backfilling with noextra computational overhead. Extensive experiments show the effectiveness ofour framework on four standard benchmarks in various settings.</description><author>Seonguk Seo, Mustafa Gokhan Uzunbas, Bohyung Han, Sara Cao, Ser-Nam Lim</author><pubDate>Thu, 19 Dec 2024 16:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.03767v2</guid></item><item><title>DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT Space</title><link>http://arxiv.org/abs/2412.15032v1</link><description>This paper explores image modeling from the frequency space and introducesDCTdiff, an end-to-end diffusion generative paradigm that efficiently modelsimages in the discrete cosine transform (DCT) space. We investigate the designspace of DCTdiff and reveal the key design factors. Experiments on differentframeworks (UViT, DiT), generation tasks, and various diffusion samplersdemonstrate that DCTdiff outperforms pixel-based diffusion models regardinggenerative quality and training efficiency. Remarkably, DCTdiff can seamlesslyscale up to high-resolution generation without using the latent diffusionparadigm. Finally, we illustrate several intriguing properties of DCT imagemodeling. For example, we provide a theoretical proof of why `image diffusioncan be seen as spectral autoregression', bridging the gap between diffusion andautoregressive models. The effectiveness of DCTdiff and the introducedproperties suggest a promising direction for image modeling in the frequencyspace. The code is at \url{https://github.com/forever208/DCTdiff}.</description><author>Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin Beneš, Albert Ali Salah, Itir Onal Ertugrul</author><pubDate>Thu, 19 Dec 2024 16:44:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15032v1</guid></item><item><title>A Deep Learning-Based and Fully Automated Pipeline for Regurgitant Mitral Valve Anatomy Analysis from 3D Echocardiography</title><link>http://arxiv.org/abs/2302.10634v2</link><description>3D transesophageal echocardiography (3DTEE), is the recommended method fordiagnosing mitral regurgitation (MR). 3DTEE provides a high-quality 3D image ofthe mitral valve (MV), allowing for precise segmentation and measurement of theregurgitant valve anatomy. However, manual TEE segmentations are time-consumingand prone to intra-operator variability, affecting the reliability of themeasurements. To address this, we developed a fully automated pipeline using a3D convolutional neural network (CNN) to segment MV substructures (annulus,anterior leaflet, and posterior leaflet) and quantify MV anatomy. The 3D CNN,based on a multi-decoder residual U-Net architecture, was trained and tested ona dataset comprising 100 3DTEE images with corresponding segmentations. Withinthe pipeline, a custom algorithm refines the CNN-based segmentations andextracts MV models, from which anatomical landmarks and features arequantified. The accuracy of the proposed method was assessed using Dice scoreand mean surface distance (MSD) against ground truth segmentations, and theextracted anatomical parameters were compared against a semiautomatedcommercial software TomTec Image Arena. The trained 3D CNN achieved an averageDice score of 0.79 and MSD of 0.47 mm for the combined segmentation of theannulus, anterior and posterior leaflet. The proposed CNN architectureoutperformed a baseline residual U-Net architecture in MV substructuresegmentation, and the refinement of the predicted annulus segmentation improvedMSD by 8.36%. The annular and leaflet linear measurements differed by less than7.94 mm and 3.67 mm, respectively, compared to the 3D measurements obtainedwith TomTec Image Arena. The proposed pipeline was faster than the commercialsoftware, with a modeling time of 12.54 s and a quantification time of 54.42 s.</description><author>Riccardo Munafò, Simone Saitta, Giacomo Ingallina, Paolo Denti, Francesco Maisano, Eustachio Agricola, Alberto Redaelli, Emiliano Votta</author><pubDate>Thu, 19 Dec 2024 16:41:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10634v2</guid></item><item><title>Latent Variable Sequence Identification for Cognitive Models with Neural Network Estimators</title><link>http://arxiv.org/abs/2406.14742v2</link><description>Extracting time-varying latent variables from computational cognitive modelsis a key step in model-based neural analysis, which aims to understand theneural correlates of cognitive processes. However, existing methods only allowresearchers to infer latent variables that explain subjects' behavior in arelatively small class of cognitive models. For example, a broad class ofrelevant cognitive models with analytically intractable likelihood is currentlyout of reach from standard techniques, based on Maximum a Posteriori parameterestimation. Here, we present an approach that extends neural Bayes estimationto learn a direct mapping between experimental data and the targeted latentvariable space using recurrent neural networks and simulated datasets. We showthat our approach achieves competitive performance in inferring latent variablesequences in both tractable and intractable models. Furthermore, the approachis generalizable across different computational models and is adaptable forboth continuous and discrete latent spaces. We then demonstrate itsapplicability in real world datasets. Our work underscores that combiningrecurrent neural networks and simulation-based inference to identify latentvariable sequences can enable researchers to access a wider class of cognitivemodels for model-based neural analyses, and thus test a broader set oftheories.</description><author>Ti-Fen Pan, Jing-Jing Li, Bill Thompson, Anne Collins</author><pubDate>Thu, 19 Dec 2024 16:37:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14742v2</guid></item><item><title>Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and Semantic Controls</title><link>http://arxiv.org/abs/2412.15023v1</link><description>Sound designers and Foley artists usually sonorize a scene, such as from amovie or video game, by manually annotating and sonorizing each action ofinterest in the video. In our case, the intent is to leave full creativecontrol to sound designers with a tool that allows them to bypass the morerepetitive parts of their work, thus being able to focus on the creativeaspects of sound production. We achieve this presenting Stable-V2A, a two-stagemodel consisting of: an RMS-Mapper that estimates an envelope representative ofthe audio characteristics associated with the input video; and Stable-Foley, adiffusion model based on Stable Audio Open that generates audio semanticallyand temporally aligned with the target video. Temporal alignment is guaranteedby the use of the envelope as a ControlNet input, while semantic alignment isachieved through the use of sound representations chosen by the designer ascross-attention conditioning of the diffusion process. We train and test ourmodel on Greatest Hits, a dataset commonly used to evaluate V2A models. Inaddition, to test our model on a case study of interest, we introduce WalkingThe Maps, a dataset of videos extracted from video games depicting animatedcharacters walking in different locations. Samples and code available on ourdemo page at https://ispamm.github.io/Stable-V2A.</description><author>Riccardo Fosco Gramaccioni, Christian Marinoni, Emilian Postolache, Marco Comunità, Luca Cosmo, Joshua D. Reiss, Danilo Comminiello</author><pubDate>Thu, 19 Dec 2024 16:37:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15023v1</guid></item><item><title>LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings</title><link>http://arxiv.org/abs/2408.14512v3</link><description>Zero-shot graph machine learning, especially with graph neural networks(GNNs), has garnered significant interest due to the challenge of scarcelabeled data. While methods like self-supervised learning and graph promptlearning have been extensively explored, they often rely on fine-tuning withtask-specific labels, limiting their effectiveness in zero-shot scenarios.Inspired by the zero-shot capabilities of instruction-fine-tuned large languagemodels (LLMs), we introduce a novel framework named Token Embedding-AlignedGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset andcross-task zero-shot learners for graph machine learning. Concretely, wepretrain a GNN, aligning its representations with token embeddings of an LLM.We then train a linear projector that transforms the GNN's representations intoa fixed number of graph token embeddings without tuning the LLM. A unifiedinstruction is designed for various graph tasks at different levels, such asnode classification (node-level) and link prediction (edge-level). These designchoices collectively enhance our method's effectiveness in zero-shot learning,setting it apart from existing methods. Experiments show that our graph tokenembeddings help the LLM predictor achieve state-of-the-art performance onunseen datasets and tasks compared to other methods using LLMs as predictors.</description><author>Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu</author><pubDate>Thu, 19 Dec 2024 16:37:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14512v3</guid></item><item><title>Learning from Linear Algebra: A Graph Neural Network Approach to Preconditioner Design for Conjugate Gradient Solvers</title><link>http://arxiv.org/abs/2405.15557v2</link><description>Large linear systems are ubiquitous in modern computational science andengineering. The main recipe for solving them is the use of Krylov subspaceiterative methods with well-designed preconditioners. Deep learning models canbe used as nonlinear preconditioners during the iteration of linear solverssuch as the conjugate gradient (CG) method. Neural network models require anenormous number of parameters to approximate well in this setup. Anotherapproach is to take advantage of small graph neural networks (GNNs) toconstruct preconditioners with predefined sparsity patterns. Recently, GNNshave been shown to be a promising tool for designing preconditioners to reducethe overall computational cost of iterative methods by constructing them moreefficiently than with classical linear algebra techniques. However,preconditioners designed with these approaches cannot outperform those designedwith classical methods in terms of the number of iterations in CG. In our work,we recall well-established preconditioners from linear algebra and use them asa starting point for training the GNN to obtain preconditioners that reduce thecondition number of the system more significantly. Numerical experiments showthat our approach outperforms both classical and neural network-based methodsfor an important class of parametric partial differential equations. We alsoprovide a heuristic justification for the loss function used and show thatpreconditioners obtained by learning with this loss function reduce thecondition number in a more desirable way for CG.</description><author>Vladislav Trifonov, Alexander Rudikov, Oleg Iliev, Yuri M. Laevsky, Ivan Oseledets, Ekaterina Muravleva</author><pubDate>Thu, 19 Dec 2024 16:32:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15557v2</guid></item><item><title>Event-based backpropagation on the neuromorphic platform SpiNNaker2</title><link>http://arxiv.org/abs/2412.15021v1</link><description>Neuromorphic computing aims to replicate the brain's capabilities for energyefficient and parallel information processing, promising a solution to theincreasing demand for faster and more efficient computational systems.Efficient training of neural networks on neuromorphic hardware requires thedevelopment of training algorithms that retain the sparsity of spike-basedcommunication during training. Here, we report on the first implementation ofevent-based backpropagation on the SpiNNaker2 neuromorphic hardware platform.We use EventProp, an algorithm for event-based backpropagation in spikingneural networks (SNNs), to compute exact gradients using sparse communicationof error signals between neurons. Our implementation computes multi-layernetworks of leaky integrate-and-fire neurons using discretized versions of thedifferential equations and their adjoints, and uses event packets to transmitspikes and error signals between network layers. We demonstrate aproof-of-concept of batch-parallelized, on-chip training of SNNs using the YinYang dataset, and provide an off-chip implementation for efficient prototyping,hyper-parameter search, and hybrid training methods.</description><author>Béna Gabriel, Wunderlich Timo, Akl Mahmoud, Vogginger Bernhard, Mayr Christian, Andres Gonzales Hector</author><pubDate>Thu, 19 Dec 2024 16:31:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15021v1</guid></item><item><title>Robust Federated Learning in the Face of Covariate Shift: A Magnitude Pruning with Hybrid Regularization Framework for Enhanced Model Aggregation</title><link>http://arxiv.org/abs/2412.15010v1</link><description>The development of highly sophisticated neural networks has allowed for fastprogress in every field of computer vision, however, applications whereannotated data is prohibited due to privacy or security concerns remainchallenging. Federated Learning (FL) offers a promising framework forindividuals aiming to collaboratively develop a shared model while preservingdata privacy. Nevertheless, our findings reveal that variations in datadistribution among clients can profoundly affect FL methodologies, primarilydue to instabilities in the aggregation process. We also propose a novel FLframework to mitigate the adverse effects of covariate shifts among federatedclients by combining individual parameter pruning and regularization techniquesto improve the robustness of individual clients' models to aggregate. Eachclient's model is optimized through magnitude-based pruning and the addition ofdropout and noise injection layers to build more resilient decision pathways inthe networks and improve the robustness of the model's parameter aggregationstep. The proposed framework is capable of extracting robust representationseven in the presence of very large covariate shifts among client datadistributions and in the federation of a small number of clients. Empiricalfindings substantiate the effectiveness of our proposed methodology acrosscommon benchmark datasets, including CIFAR10, MNIST, SVHN, and Fashion MNIST.Furthermore, we introduce the CelebA-Gender dataset, specifically designed toevaluate performance on a more realistic domain. The proposed method is capableof extracting robust representations even in the presence of both high and lowcovariate shifts among client data distributions.</description><author>Ozgu Goksu, Nicolas Pugeault</author><pubDate>Thu, 19 Dec 2024 16:22:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15010v1</guid></item><item><title>Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts</title><link>http://arxiv.org/abs/2406.10868v4</link><description>Large Language Models (LLMs) possess vast amounts of knowledge within theirparameters, prompting research into methods for locating and editing thisknowledge. Previous work has largely focused on locating entity-related (oftensingle-token) facts in smaller models. However, several key questions remainunanswered: (1) How can we effectively locate query-relevant neurons indecoder-only LLMs, such as Llama and Mistral? (2) How can we address thechallenge of long-form (or free-form) text generation? (3) Are there localizedknowledge regions in LLMs? In this study, we introduce Query-Relevant NeuronCluster Attribution (QRNCA), a novel architecture-agnostic framework capable ofidentifying query-relevant neurons in LLMs. QRNCA allows for the examination oflong-form answers beyond triplet facts by employing the proxy task ofmulti-choice question answering. To evaluate the effectiveness of our detectedneurons, we build two multi-choice QA datasets spanning diverse domains andlanguages. Empirical evaluations demonstrate that our method outperformsbaseline methods significantly. Further, analysis of neuron distributionsreveals the presence of visible localized regions, particularly withindifferent domains. Finally, we show potential applications of our detectedneurons in knowledge editing and neuron-based prediction.</description><author>Lihu Chen, Adam Dejl, Francesca Toni</author><pubDate>Thu, 19 Dec 2024 16:22:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10868v4</guid></item><item><title>SPICA: Retrieving Scenarios for Pluralistic In-Context Alignment</title><link>http://arxiv.org/abs/2411.10912v2</link><description>When different groups' values differ, one approach to model alignment is tosteer models at inference time towards each group's preferences. However,techniques like in-context learning only consider similarity when drawingfew-shot examples and not cross-group differences in values. We propose SPICA,a framework that accounts for group-level differences during in-context exampleretrieval. SPICA introduces three designs: scenario banks, group-informedretrieval metrics, and in-context alignment prompts. From an evaluation ofSPICA on an alignment task collecting inputs from four demographic groups ($n =544$), our metrics retrieve in-context examples that more closely matchobserved preferences, with the best prompt configuration using multiplecontrastive responses to demonstrate examples. In an end-to-end evaluation ($n= 120$), we observe that SPICA is higher rated than similarity-based retrieval,with groups seeing up to a +0.16 point improvement on a 5 point scale.Additionally, gains from SPICA were more uniform, with all groups benefitingfrom alignment rather than only some. Finally, we find that while agroup-agnostic approach can align to aggregated values, it is not most suitedfor divergent groups.</description><author>Quan Ze Chen, K. J. Kevin Feng, Chan Young Park, Amy X. Zhang</author><pubDate>Thu, 19 Dec 2024 16:20:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.10912v2</guid></item><item><title>DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start Cross-Domain Recommendation</title><link>http://arxiv.org/abs/2412.15005v1</link><description>Recommender systems are widely used in various real-world applications, butthey often encounter the persistent challenge of the user cold-start problem.Cross-domain recommendation (CDR), which leverages user interactions from onedomain to improve prediction performance in another, has emerged as a promisingsolution. However, users with similar preferences in the source domain mayexhibit different interests in the target domain. Therefore, directlytransferring embeddings may introduce irrelevant source-domain collaborativeinformation. In this paper, we propose a novel graph-based disentangledcontrastive learning framework to capture fine-grained user intent and filterout irrelevant collaborative information, thereby avoiding negative transfer.Specifically, for each domain, we use a multi-channel graph encoder to capturediverse user intents. We then construct the affinity graph in the embeddingspace and perform multi-step random walks to capture high-order user similarityrelationships. Treating one domain as the target, we propose a disentangledintent-wise contrastive learning approach, guided by user similarity, to refinethe bridging of user intents across domains. Extensive experiments on fourbenchmark CDR datasets demonstrate that DisCo consistently outperforms existingstate-of-the-art baselines, thereby validating the effectiveness of both DisCoand its components.</description><author>Hourun Li, Yifan Wang, Zhiping Xiao, Jia Yang, Changling Zhou, Ming Zhang, Wei Ju</author><pubDate>Thu, 19 Dec 2024 16:20:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15005v1</guid></item><item><title>Large Language Models and Code Security: A Systematic Literature Review</title><link>http://arxiv.org/abs/2412.15004v1</link><description>Large Language Models (LLMs) have emerged as powerful tools for automatingvarious programming tasks, including security-related ones, such as detectingand fixing vulnerabilities. Despite their promising capabilities, when requiredto produce or modify pre-existing code, LLMs could introduce vulnerabilitiesunbeknown to the programmer. When analyzing code, they could miss clearvulnerabilities or signal nonexistent ones. In this Systematic LiteratureReview (SLR), we aim to investigate both the security benefits and potentialdrawbacks of using LLMs for a variety of code-related tasks. In particular,first we focus on the types of vulnerabilities that could be introduced byLLMs, when used for producing code. Second, we analyze the capabilities of LLMsto detect and fix vulnerabilities, in any given code, and how the promptingstrategy of choice impacts their performance in these two tasks. Last, weprovide an in-depth analysis on how data poisoning attacks on LLMs can impactperformance in the aforementioned tasks.</description><author>Enna Basic, Alberto Giaretta</author><pubDate>Thu, 19 Dec 2024 16:20:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15004v1</guid></item><item><title>TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients</title><link>http://arxiv.org/abs/2401.12012v5</link><description>Federated learning is a distributed collaborative machine learning paradigmthat has gained strong momentum in recent years. In federated learning, acentral server periodically coordinates models with clients and aggregates themodels trained locally by clients without necessitating access to local data.Despite its potential, the implementation of federated learning continues toencounter several challenges, predominantly the slow convergence that islargely due to data heterogeneity. The slow convergence becomes particularlyproblematic in cross-device federated learning scenarios where clients may bestrongly limited by computing power and storage space, and hence counteractingmethods that induce additional computation or memory cost on the client sidesuch as auxiliary objective terms and larger training iterations can beimpractical. In this paper, we propose a novel federated aggregation strategy,TurboSVM-FL, that poses no additional computation burden on the client side andcan significantly accelerate convergence for federated classification task,especially when clients are "lazy" and train their models solely for few epochsfor next global aggregation. TurboSVM-FL extensively utilizes support vectormachine to conduct selective aggregation and max-margin spread-outregularization on class embeddings. We evaluate TurboSVM-FL on multipledatasets including FEMNIST, CelebA, and Shakespeare using user-independentvalidation with non-iid data distribution. Our results show that TurboSVM-FLcan significantly outperform existing popular algorithms on convergence rateand reduce communication rounds while delivering better test metrics includingaccuracy, F1 score, and MCC.</description><author>Mengdi Wang, Anna Bodonhelyi, Efe Bozkir, Enkelejda Kasneci</author><pubDate>Thu, 19 Dec 2024 16:15:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12012v5</guid></item><item><title>Knowledge Tagging with Large Language Model based Multi-Agent System</title><link>http://arxiv.org/abs/2409.08406v2</link><description>Knowledge tagging for questions is vital in modern intelligent educationalapplications, including learning progress diagnosis, practice questionrecommendations, and course content organization. Traditionally, theseannotations have been performed by pedagogical experts, as the task demands notonly a deep semantic understanding of question stems and knowledge definitionsbut also a strong ability to link problem-solving logic with relevant knowledgeconcepts. With the advent of advanced natural language processing (NLP)algorithms, such as pre-trained language models and large language models(LLMs), pioneering studies have explored automating the knowledge taggingprocess using various machine learning models. In this paper, we investigatethe use of a multi-agent system to address the limitations of previousalgorithms, particularly in handling complex cases involving intricateknowledge definitions and strict numerical constraints. By demonstrating itssuperior performance on the publicly available math question knowledge taggingdataset, MathKnowCT, we highlight the significant potential of an LLM-basedmulti-agent system in overcoming the challenges that previous methods haveencountered. Finally, through an in-depth discussion of the implications ofautomating knowledge tagging, we underscore the promising results of deployingLLM-based algorithms in educational contexts.</description><author>Hang Li, Tianlong Xu, Ethan Chang, Qingsong Wen</author><pubDate>Thu, 19 Dec 2024 16:09:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08406v2</guid></item><item><title>Mitigating federated learning contribution allocation instability through randomized aggregation</title><link>http://arxiv.org/abs/2405.08044v2</link><description>Federated learning (FL) is a collaborative and privacy-preserving MachineLearning paradigm, allowing the development of robust models without the needto centralise sensitive data. A critical challenge in FL lies in fairly andaccurately allocating contributions from diverse participants. Inaccurateallocation can undermine trust, lead to unfair compensation, and thusparticipants may lack the incentive to join or actively contribute to thefederation. Various remuneration strategies have been proposed to date, includingauction-based approaches and Shapley-value based methods, the latter offering ameans to quantify the contribution of each participant. However, little to nowork has studied the stability of these contribution evaluation methods. In this paper, we focus on calculating contributions using gradient-basedmodel reconstruction techniques with Shapley values. We first show thatbaseline Shapley values do not accurately reflect clients' contributions,leading to unstable reward allocations amongst participants in a cross-silofederation. We then introduce \textsc{FedRandom}, a new method that mitigatesthese shortcomings with additional data samplings, and show its efficacy atincreasing the stability of contribution evaluation in federated learning.</description><author>Arno Geimer, Beltran Fiz, Radu State</author><pubDate>Thu, 19 Dec 2024 16:08:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08044v2</guid></item><item><title>HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony Search and Genetic Algorithm Using LLMs</title><link>http://arxiv.org/abs/2412.14995v1</link><description>Automatic Heuristic Design (AHD) is an active research area due to itsutility in solving complex search and NP-hard combinatorial optimizationproblems in the real world. The recent advancements in Large Language Models(LLMs) introduce new possibilities by coupling LLMs with evolutionarycomputation to automatically generate heuristics, known as LLM-basedEvolutionary Program Search (LLM-EPS). While previous LLM-EPS studies obtainedgreat performance on various tasks, there is still a gap in understanding theproperties of heuristic search spaces and achieving a balance betweenexploration and exploitation, which is a critical factor in large heuristicsearch spaces. In this study, we address this gap by proposing two diversitymeasurement metrics and perform an analysis on previous LLM-EPS approaches,including FunSearch, EoH, and ReEvo. Results on black-box AHD problems revealthat while EoH demonstrates higher diversity than FunSearch and ReEvo, itsobjective score is unstable. Conversely, ReEvo's reflection mechanism yieldsgood objective scores but fails to optimize diversity effectively. With thisfinding in mind, we introduce HSEvo, an adaptive LLM-EPS framework thatmaintains a balance between diversity and convergence with a harmony searchalgorithm. Through experimentation, we find that HSEvo achieved high diversityindices and good objective scores while remaining cost-effective. These resultsunderscore the importance of balancing exploration and exploitation andunderstanding heuristic search spaces in designing frameworks in LLM-EPS.</description><author>Pham Vu Tuan Dat, Long Doan, Huynh Thi Thanh Binh</author><pubDate>Thu, 19 Dec 2024 16:07:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.14995v1</guid></item><item><title>Arbitrary Polynomial Separations in Trainable Quantum Machine Learning</title><link>http://arxiv.org/abs/2402.08606v2</link><description>Recent theoretical results in quantum machine learning have demonstrated ageneral trade-off between the expressive power of quantum neural networks(QNNs) and their trainability; as a corollary of these results, practicalexponential separations in expressive power over classical machine learningmodels are believed to be infeasible as such QNNs take a time to train that isexponential in the model size. We here circumvent these negative results byconstructing a hierarchy of efficiently trainable QNNs that exhibitunconditionally provable, polynomial memory separations of arbitrary constantdegree over classical neural networks -- including state-of-the-art models,such as Transformers -- in performing a classical sequence modeling task. Thisconstruction is also computationally efficient, as each unit cell of theintroduced class of QNNs only has constant gate complexity. We show thatcontextuality -- informally, a quantitative notion of semantic ambiguity -- isthe source of the expressivity separation, suggesting that other learning taskswith this property may be a natural setting for the use of quantum learningalgorithms.</description><author>Eric R. Anschuetz, Xun Gao</author><pubDate>Thu, 19 Dec 2024 16:05:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08606v2</guid></item><item><title>Stitch Contrast and Segment_Learning a Human Action Segmentation Model Using Trimmed Skeleton Videos</title><link>http://arxiv.org/abs/2412.14988v1</link><description>Existing skeleton-based human action classification models rely onwell-trimmed action-specific skeleton videos for both training and testing,precluding their scalability to real-world applications where untrimmed videosexhibiting concatenated actions are predominant. To overcome this limitation,recently introduced skeleton action segmentation models involve un-trimmedskeleton videos into end-to-end training. The model is optimized to provideframe-wise predictions for any length of testing videos, simultaneouslyrealizing action localization and classification. Yet, achieving such animprovement im-poses frame-wise annotated skeleton videos, which remainstime-consuming in practice. This paper features a novel framework forskeleton-based action segmentation trained on short trimmed skeleton videos,but that can run on longer un-trimmed videos. The approach is implemented inthree steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poralskeleton stitching scheme that treats trimmed skeleton videos as elementaryhuman motions that compose a semantic space and can be sampled to generatemulti-action stitched se-quences. Contrast learns contrastive representationsfrom stitched sequences with a novel discrimination pretext task that enables askeleton encoder to learn meaningful action-temporal contexts to improve actionsegmentation. Finally, Segment relates the proposed method to actionsegmentation by learning a segmentation layer while handling particular da-taavailability. Experiments involve a trimmed source dataset and an untrimmedtarget dataset in an adaptation formulation for real-world skeleton-based humanaction segmentation to evaluate the effectiveness of the proposed method.</description><author>Haitao Tian, Pierre Payeur</author><pubDate>Thu, 19 Dec 2024 16:00:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.14988v1</guid></item><item><title>Optimized Gradient Clipping for Noisy Label Learning</title><link>http://arxiv.org/abs/2412.08941v3</link><description>Previous research has shown that constraining the gradient of loss functionwith respect to model-predicted probabilities can enhance the model robustnessagainst noisy labels. These methods typically specify a fixed optimal thresholdfor gradient clipping through validation data to obtain the desired robustnessagainst noise. However, this common practice overlooks the dynamic distributionof gradients from both clean and noisy-labeled samples at different stages oftraining, significantly limiting the model capability to adapt to the variablenature of gradients throughout the training process. To address this issue, wepropose a simple yet effective approach called Optimized Gradient Clipping(OGC), which dynamically adjusts the clipping threshold based on the ratio ofnoise gradients to clean gradients after clipping, estimated by modeling thedistributions of clean and noisy samples. This approach allows us to modify theclipping threshold at each training step, effectively controlling the influenceof noise gradients. Additionally, we provide statistical analysis to certifythe noise-tolerance ability of OGC. Our extensive experiments across varioustypes of label noise, including symmetric, asymmetric, instance-dependent, andreal-world noise, demonstrate the effectiveness of our approach.</description><author>Xichen Ye, Yifan Wu, Weizhong Zhang, Xiaoqiang Li, Yifan Chen, Cheng Jin</author><pubDate>Thu, 19 Dec 2024 15:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08941v3</guid></item><item><title>Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small Language Models Write Young Students Texts</title><link>http://arxiv.org/abs/2412.14986v1</link><description>Large Language Models (LLMs) have been used to generate texts in response todifferent writing tasks: reports, essays, story telling. However, languagemodels do not have a meta-representation of the text writing process, norinherent communication learning needs, comparable to those of young humanstudents. This paper introduces a fine-grained linguistic and textual analysisof multilingual Small Language Models' (SLMs) writing. With our method,Chain-of-MetaWriting, SLMs can imitate some steps of the human writing process,such as planning and evaluation. We mainly focused on short story and essaywriting tasks in French for schoolchildren and undergraduate studentsrespectively. Our results show that SLMs encounter difficulties in assistingyoung students on sensitive topics such as violence in the schoolyard, and theysometimes use words too complex for the target audience. In particular, theoutput is quite different from the human produced texts in term of textcohesion and coherence regarding temporal connectors, topic progression,reference.</description><author>Ioana Buhnila, Georgeta Cislaru, Amalia Todirascu</author><pubDate>Thu, 19 Dec 2024 15:58:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.14986v1</guid></item><item><title>Beyond Dataset Creation: Critical View of Annotation Variation and Bias Probing of a Dataset for Online Radical Content Detection</title><link>http://arxiv.org/abs/2412.11745v2</link><description>The proliferation of radical content on online platforms poses significantrisks, including inciting violence and spreading extremist ideologies. Despiteongoing research, existing datasets and models often fail to address thecomplexities of multilingual and diverse data. To bridge this gap, we introducea publicly available multilingual dataset annotated with radicalization levels,calls for action, and named entities in English, French, and Arabic. Thisdataset is pseudonymized to protect individual privacy while preservingcontextual information. Beyond presenting our freely available dataset, weanalyze the annotation process, highlighting biases and disagreements amongannotators and their implications for model performance. Additionally, we usesynthetic data to investigate the influence of socio-demographic traits onannotation patterns and model predictions. Our work offers a comprehensiveexamination of the challenges and opportunities in building robust datasets forradical content detection, emphasizing the importance of fairness andtransparency in model development.</description><author>Arij Riabi, Virginie Mouilleron, Menel Mahamdi, Wissam Antoun, Djamé Seddah</author><pubDate>Thu, 19 Dec 2024 15:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11745v2</guid></item><item><title>Clustering of timed sequences -- Application to the analysis of care pathways</title><link>http://arxiv.org/abs/2404.15379v3</link><description>Improving the future of healthcare starts by better understanding the currentactual practices in hospital settings. This motivates the objective ofdiscovering typical care pathways from patient data. Revealing typical carepathways can be achieved through clustering. The difficulty in clustering carepathways, represented by sequences of timestamped events, lies in defining asemantically appropriate metric and clustering algorithms. In this article, weadapt two methods developed for time series to the clustering of timedsequences: the drop-DTW metric and the DBA approach for the construction ofaveraged time sequences. These methods are then applied in clusteringalgorithms to propose original and sound clustering algorithms for timedsequences. This approach is experimented with and evaluated on synthetic andreal-world data.</description><author>Thomas Guyet, Pierre Pinson, Enoal Gesny</author><pubDate>Thu, 19 Dec 2024 15:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15379v3</guid></item><item><title>Task Adaptation of Reinforcement Learning-based NAS Agents through Transfer Learning</title><link>http://arxiv.org/abs/2412.01420v2</link><description>Recently, a novel paradigm has been proposed for reinforcement learning-basedNAS agents, that revolves around the incremental improvement of a givenarchitecture. We assess the abilities of such reinforcement learning agents totransfer between different tasks. We perform our evaluation using theTrans-NASBench-101 benchmark, and consider the efficacy of the transferredagents, as well as how quickly they can be trained. We find that pretraining anagent on one task benefits the performance of the agent in another task in allbut 1 task when considering final performance. We also show that the trainingprocedure for an agent can be shortened significantly by pretraining it onanother task. Our results indicate that these effects occur regardless of thesource or target task, although they are more pronounced for some tasks thanfor others. Our results show that transfer learning can be an effective tool inmitigating the computational cost of the initial training procedure forreinforcement learning-based NAS agents.</description><author>Amber Cassimon, Siegfried Mercelis, Kevin Mets</author><pubDate>Thu, 19 Dec 2024 15:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.01420v2</guid></item><item><title>LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for E-Learning Platforms</title><link>http://arxiv.org/abs/2412.13765v2</link><description>Current methods for analyzing student engagement in e-learning platforms,including automated systems, often struggle with challenges such as handlingfuzzy sentiment in text comments and relying on limited metadata. Traditionalapproaches, such as surveys and questionnaires, also face issues like smallsample sizes and scalability. In this paper, we introduce LLM-SEM (LanguageModel-Based Student Engagement Metric), a novel approach that leverages videometadata and sentiment analysis of student comments to measure engagement. Byutilizing recent Large Language Models (LLMs), we generate high-qualitysentiment predictions to mitigate text fuzziness and normalize key featuressuch as views and likes. Our holistic method combines comprehensive metadatawith sentiment polarity scores to gauge engagement at both the course andlesson levels. Extensive experiments were conducted to evaluate various LLMmodels, demonstrating the effectiveness of LLM-SEM in providing a scalable andaccurate measure of student engagement. We fine-tuned TXLM-RoBERTa usinghuman-annotated sentiment datasets to enhance prediction accuracy and utilizedLLama 3B, and Gemma 9B from Ollama.</description><author>Ali Hamdi, Ahmed Abdelmoneim Mazrou, Mohamed Shaltout</author><pubDate>Thu, 19 Dec 2024 15:50:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13765v2</guid></item><item><title>Arti-PG: A Toolbox for Procedurally Synthesizing Large-Scale and Diverse Articulated Objects with Rich Annotations</title><link>http://arxiv.org/abs/2412.14974v1</link><description>The acquisition of substantial volumes of 3D articulated object data isexpensive and time-consuming, and consequently the scarcity of 3D articulatedobject data becomes an obstacle for deep learning methods to achieve remarkableperformance in various articulated object understanding tasks. Meanwhile,pairing these object data with detailed annotations to enable training forvarious tasks is also difficult and labor-intensive to achieve. In order toexpeditiously gather a significant number of 3D articulated objects withcomprehensive and detailed annotations for training, we propose ArticulatedObject Procedural Generation toolbox, a.k.a. Arti-PG toolbox. Arti-PG toolboxconsists of i) descriptions of articulated objects by means of a generalizedstructure program along with their analytic correspondence to the objects'point cloud, ii) procedural rules about manipulations on the structure programto synthesize large-scale and diverse new articulated objects, and iii)mathematical descriptions of knowledge (e.g. affordance, semantics, etc.) toprovide annotations to the synthesized object. Arti-PG has two appealingproperties for providing training data for articulated object understandingtasks: i) objects are created with unlimited variations in shape throughprogram-oriented structure manipulation, ii) Arti-PG is widely applicable todiverse tasks by easily providing comprehensive and detailed annotations.Arti-PG now supports the procedural generation of 26 categories of articulateobjects and provides annotations across a wide range of both vision andmanipulation tasks, and we provide exhaustive experiments which fullydemonstrate its advantages. We will make Arti-PG toolbox publicly available forthe community to use.</description><author>Jianhua Sun, Yuxuan Li, Jiude Wei, Longfei Xu, Nange Wang, Yining Zhang, Cewu Lu</author><pubDate>Thu, 19 Dec 2024 15:48:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.14974v1</guid></item><item><title>PhotoHolmes: a Python library for forgery detection in digital images</title><link>http://arxiv.org/abs/2412.14969v1</link><description>In this paper, we introduce PhotoHolmes, an open-source Python librarydesigned to easily run and benchmark forgery detection methods on digitalimages. The library includes implementations of popular and state-of-the-artmethods, dataset integration tools, and evaluation metrics. Utilizing theBenchmark tool in PhotoHolmes, users can effortlessly compare various methods.This facilitates an accurate and reproducible comparison between their ownmethods and those in the existing literature. Furthermore, PhotoHolmes includesa command-line interface (CLI) to easily run the methods implemented in thelibrary on any suspicious image. As such, image forgery methods become moreaccessible to the community. The library has been built with extensibility andmodularity in mind, which makes adding new methods, datasets and metrics to thelibrary a straightforward process. The source code is available athttps://github.com/photoholmes/photoholmes.</description><author>Julián O'Flaherty, Rodrigo Paganini, Juan Pablo Sotelo, Julieta Umpiérrez, Marina Gardella, Matías Tailanian, Pablo Musé</author><pubDate>Thu, 19 Dec 2024 15:47:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.14969v1</guid></item><item><title>Movie2Story: A framework for understanding videos and telling stories in the form of novel text</title><link>http://arxiv.org/abs/2412.14965v1</link><description>Multimodal video-to-text models have made considerable progress, primarily ingenerating brief descriptions of video content. However, there is still adeficiency in generating rich long-form text descriptions that integrate bothvideo and audio. In this paper, we introduce a framework called M2S, designedto generate novel-length text by combining audio, video, and characterrecognition. M2S includes modules for video long-form text description andcomprehension, audio-based analysis of emotion, speech rate, and characteralignment, and visual-based character recognition alignment. By integratingmultimodal information using the large language model GPT4o, M2S stands out inthe field of multimodal text generation. We demonstrate the effectiveness andaccuracy of M2S through comparative experiments and human evaluation.Additionally, the model framework has good scalability and significantpotential for future research.</description><author>Kangning Li, Zheyang Jia, Anyu Ying</author><pubDate>Thu, 19 Dec 2024 15:44:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.14965v1</guid></item><item><title>Knowledge Injection via Prompt Distillation</title><link>http://arxiv.org/abs/2412.14964v1</link><description>In many practical applications, large language models (LLMs) need toincorporate new knowledge not present in their pre-training data. The primarymethods for this are fine-tuning and retrieval-augmented generation (RAG).Although RAG has emerged as the industry standard for knowledge injection,fine-tuning has not yet achieved comparable success. In this paper, we proposea new fine-tuning technique for learning new knowledge and show that it canreach the performance of RAG. The proposed method is based on theself-distillation approach, which we call prompt distillation. First, wegenerate question-answer pairs about the new knowledge. Then, we fine-tune astudent model on the question-answer pairs to imitate the output distributionsof a teacher model, which additionally receives the new knowledge in itsprompt. The student model is identical to the teacher, except it is equippedwith a LoRA adapter. This training procedure facilitates distilling the newknowledge from the teacher's prompt into the student's weights.</description><author>Kalle Kujanpää, Harri Valpola, Alexander Ilin</author><pubDate>Thu, 19 Dec 2024 15:44:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.14964v1</guid></item><item><title>IDOL: Instant Photorealistic 3D Human Creation from a Single Image</title><link>http://arxiv.org/abs/2412.14963v1</link><description>Creating a high-fidelity, animatable 3D full-body avatar from a single imageis a challenging task due to the diverse appearance and poses of humans and thelimited availability of high-quality training data. To achieve fast andhigh-quality human reconstruction, this work rethinks the task from theperspectives of dataset, model, and representation. First, we introduce alarge-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100Kdiverse, photorealistic sets of human images. Each set contains 24-view framesin specific human poses, generated using a pose-controllableimage-to-multi-view model. Next, leveraging the diversity in views, poses, andappearances within HuGe100K, we develop a scalable feed-forward transformermodel to predict a 3D human Gaussian representation in a uniform space from agiven human image. This model is trained to disentangle human pose, body shape,clothing geometry, and texture. The estimated Gaussians can be animated withoutpost-processing. We conduct comprehensive experiments to validate theeffectiveness of the proposed dataset and method. Our model demonstrates theability to efficiently reconstruct photorealistic humans at 1K resolution froma single input image using a single GPU instantly. Additionally, it seamlesslysupports various applications, as well as shape and texture editing tasks.</description><author>Yiyu Zhuang, Jiaxi Lv, Hao Wen, Qing Shuai, Ailing Zeng, Hao Zhu, Shifeng Chen, Yujiu Yang, Xun Cao, Wei Liu</author><pubDate>Thu, 19 Dec 2024 15:43:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.14963v1</guid></item><item><title>Samudra: An AI Global Ocean Emulator for Climate</title><link>http://arxiv.org/abs/2412.03795v2</link><description>AI emulators for forecasting have emerged as powerful tools that canoutperform conventional numerical predictions. The next frontier is to buildemulators for long climate simulations with skill across a range ofspatiotemporal scales, a particularly important goal for the ocean. Our workbuilds a skillful global emulator of the ocean component of a state-of-the-artclimate model. We emulate key ocean variables, sea surface height, horizontalvelocities, temperature, and salinity, across their full depth. We use amodified ConvNeXt UNet architecture trained on multidepth levels of ocean data.We show that the ocean emulator - Samudra - which exhibits no drift relative tothe truth, can reproduce the depth structure of ocean variables and theirinterannual variability. Samudra is stable for centuries and 150 times fasterthan the original ocean model. Samudra struggles to capture the correctmagnitude of the forcing trends and simultaneously remains stable, requiringfurther work.</description><author>Surya Dheeshjith, Adam Subel, Alistair Adcroft, Julius Busecke, Carlos Fernandez-Granda, Shubham Gupta, Laure Zanna</author><pubDate>Thu, 19 Dec 2024 15:43:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03795v2</guid></item></channel></rss>