<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 30 Jun 2023 14:00:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>An Efficient General-Purpose Modular Vision Model via Multi-Task Heterogeneous Training</title><link>http://arxiv.org/abs/2306.17165v1</link><description>We present a model that can perform multiple vision tasks and can be adaptedto other downstream tasks efficiently. Despite considerable progress inmulti-task learning, most efforts focus on learning from multi-label data: asingle image set with multiple task labels. Such multi-label data sets arerare, small, and expensive. We say heterogeneous to refer to image sets withdifferent task labels, or to combinations of single-task datasets. Few haveexplored training on such heterogeneous datasets. General-purpose vision modelsare still dominated by single-task pretraining, and it remains unclear how toscale up multi-task models by leveraging mainstream vision datasets designedfor different purposes. The challenges lie in managing large intrinsicdifferences among vision tasks, including data distribution, architectures,task-specific modules, dataset scales, and sampling strategies. To addressthese challenges, we propose to modify and scale up mixture-of-experts (MoE)vision transformers, so that they can simultaneously learn classification,detection, and segmentation on diverse mainstream vision datasets includingImageNet, COCO, and ADE20K. Our approach achieves comparable results tosingle-task state-of-the-art models and demonstrates strong generalization ondownstream tasks. Due to its emergent modularity, this general-purpose modeldecomposes into high-performing components, efficiently adapting to downstreamtasks. We can fine-tune it with fewer training parameters, fewer modelparameters, and less computation. Additionally, its modularity allows for easyexpansion in continual-learning-without-forgetting scenarios. Finally, thesefunctions can be controlled and combined to meet various demands of downstreamtasks.</description><author>Zitian Chen, Mingyu Ding, Yikang Shen, Wei Zhan, Masayoshi Tomizuka, Erik Learned-Miller, Chuang Gan</author><pubDate>Thu, 29 Jun 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17165v1</guid></item><item><title>Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors</title><link>http://arxiv.org/abs/2306.17156v1</link><description>Generative AI and large language models hold great promise in enhancingcomputing education by powering next-generation educational technologies forintroductory programming. Recent works have studied these models for differentscenarios relevant to programming education; however, these works are limitedfor several reasons, as they typically consider already outdated models or onlyspecific scenario(s). Consequently, there is a lack of a systematic study thatbenchmarks state-of-the-art models for a comprehensive set of programmingeducation scenarios. In our work, we systematically evaluate two models,ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with humantutors for a variety of scenarios. We evaluate using five introductory Pythonprogramming problems and real-world buggy programs from an online platform, andassess performance using expert-based annotations. Our results show that GPT-4drastically outperforms ChatGPT (based on GPT-3.5) and comes close to humantutors' performance for several scenarios. These results also highlightsettings where GPT-4 still struggles, providing exciting future directions ondeveloping techniques to improve the performance of these models.</description><author>Tung Phung, Victor-Alexandru Pădurean, José Cambronero, Sumit Gulwani, Tobias Kohn, Rupak Majumdar, Adish Singla, Gustavo Soares</author><pubDate>Thu, 29 Jun 2023 18:57:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17156v1</guid></item><item><title>Generate Anything Anywhere in Any Scene</title><link>http://arxiv.org/abs/2306.17154v1</link><description>Text-to-image diffusion models have attracted considerable interest due totheir wide applicability across diverse fields. However, challenges persist increating controllable models for personalized object generation. In this paper,we first identify the entanglement issues in existing personalized generativemodels, and then propose a straightforward and efficient data augmentationtraining strategy that guides the diffusion model to focus solely on objectidentity. By inserting the plug-and-play adapter layers from a pre-trainedcontrollable diffusion model, our model obtains the ability to control thelocation and size of each generated personalized object. During inference, wepropose a regionally-guided sampling technique to maintain the quality andfidelity of the generated images. Our method achieves comparable or superiorfidelity for personalized objects, yielding a robust, versatile, andcontrollable text-to-image diffusion model that is capable of generatingrealistic and personalized images. Our approach demonstrates significantpotential for various applications, such as those in art, entertainment, andadvertising design.</description><author>Yuheng Li, Haotian Liu, Yangming Wen, Yong Jae Lee</author><pubDate>Thu, 29 Jun 2023 18:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17154v1</guid></item><item><title>On Computational Mechanisms for Shared Intentionality, and Speculation on Rationality and Consciousness</title><link>http://arxiv.org/abs/2306.13657v2</link><description>A singular attribute of humankind is our ability to undertake novel,cooperative behavior, or teamwork. This requires that we can communicate goals,plans, and ideas between the brains of individuals to create sharedintentionality. Using the information processing model of David Marr, I derivenecessary characteristics of basic mechanisms to enable shared intentionalitybetween prelinguistic computational agents and indicate how these could beimplemented in present-day AI-based robots. More speculatively, I suggest the mechanisms derived by this thoughtexperiment apply to humans and extend to provide explanations for humanrationality and aspects of intentional and phenomenal consciousness that accordwith observation. This yields what I call the Shared Intentionality FirstTheory (SIFT) for rationality and consciousness. The significance of shared intentionality has been recognized and advocatedpreviously, but typically from a sociological or behavioral point of view. SIFTcomplements prior work by applying a computer science perspective to theunderlying mechanisms.</description><author>John Rushby</author><pubDate>Thu, 29 Jun 2023 18:54:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13657v2</guid></item><item><title>Local Risk Bounds for Statistical Aggregation</title><link>http://arxiv.org/abs/2306.17151v1</link><description>In the problem of aggregation, the aim is to combine a given class of basepredictors to achieve predictions nearly as accurate as the best one. In thisflexible framework, no assumption is made on the structure of the class or thenature of the target. Aggregation has been studied in both sequential andstatistical contexts. Despite some important differences between the twoproblems, the classical results in both cases feature the same globalcomplexity measure. In this paper, we revisit and tighten classical results inthe theory of aggregation in the statistical setting by replacing the globalcomplexity with a smaller, local one. Some of our proofs build on the PAC-Bayeslocalization technique introduced by Catoni. Among other results, we provelocalized versions of the classical bound for the exponential weights estimatordue to Leung and Barron and deviation-optimal bounds for the Q-aggregationestimator. These bounds improve over the results of Dai, Rigollet and Zhang forfixed design regression and the results of Lecu\'e and Rigollet for randomdesign regression.</description><author>Jaouad Mourtada, Tomas Vaškevičius, Nikita Zhivotovskiy</author><pubDate>Thu, 29 Jun 2023 18:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17151v1</guid></item><item><title>KDEformer: Accelerating Transformers via Kernel Density Estimation</title><link>http://arxiv.org/abs/2302.02451v2</link><description>Dot-product attention mechanism plays a crucial role in modern deeparchitectures (e.g., Transformer) for sequence modeling, however, na\"ive exactcomputation of this model incurs quadratic time and memory complexities insequence length, hindering the training of long-sequence models. Criticalbottlenecks are due to the computation of partition functions in thedenominator of softmax function as well as the multiplication of the softmaxmatrix with the matrix of values. Our key observation is that the former can bereduced to a variant of the kernel density estimation (KDE) problem, and anefficient KDE solver can be further utilized to accelerate the latter viasubsampling-based fast matrix products. Our proposed KDEformer can approximatethe attention in sub-quadratic time with provable spectral norm bounds, whileall prior results merely provide entry-wise error bounds. Empirically, weverify that KDEformer outperforms other attention approximations in terms ofaccuracy, memory, and runtime on various pre-trained models. On BigGAN imagegeneration, we achieve better generative scores than the exact computation withover $4\times$ speedup. For ImageNet classification with T2T-ViT, KDEformershows over $18\times$ speedup while the accuracy drop is less than $0.5\%$.</description><author>Amir Zandieh, Insu Han, Majid Daliri, Amin Karbasi</author><pubDate>Thu, 29 Jun 2023 18:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02451v2</guid></item><item><title>Filtered-Guided Diffusion: Fast Filter Guidance for Black-Box Diffusion Models</title><link>http://arxiv.org/abs/2306.17141v1</link><description>Recent advances in diffusion-based generative models have shown incrediblepromise for Image-to-Image translation and editing. Most recent work in thisspace relies on additional training or architecture-specific adjustments to thediffusion process. In this work, we show that much of this low-level controlcan be achieved without additional training or any access to features of thediffusion model. Our method simply applies a filter to the input of eachdiffusion step based on the output of the previous step in an adaptive manner.Notably, this approach does not depend on any specific architecture or samplerand can be done without access to internal features of the network, making iteasy to combine with other techniques, samplers, and diffusion architectures.Furthermore, it has negligible cost to performance, and allows for morecontinuous adjustment of guidance strength than other approaches. We show FGDoffers a fast and strong baseline that is competitive with recentarchitecture-dependent approaches. Furthermore, FGD can also be used as asimple add-on to enhance the structural guidance of other state-of-the-art I2Imethods. Finally, our derivation of this method helps to understand the impactof self attention, a key component of other recent architecture-specific I2Iapproaches, in a more architecture-independent way. Project page:https://github.com/jaclyngu/FilteredGuidedDiffusion</description><author>Zeqi Gu, Abe Davis</author><pubDate>Thu, 29 Jun 2023 18:44:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17141v1</guid></item><item><title>Measured Albedo in the Wild: Filling the Gap in Intrinsics Evaluation</title><link>http://arxiv.org/abs/2306.15662v2</link><description>Intrinsic image decomposition and inverse rendering are long-standingproblems in computer vision. To evaluate albedo recovery, most algorithmsreport their quantitative performance with a mean Weighted Human DisagreementRate (WHDR) metric on the IIW dataset. However, WHDR focuses only on relativealbedo values and often fails to capture overall quality of the albedo. Inorder to comprehensively evaluate albedo, we collect a new dataset, MeasuredAlbedo in the Wild (MAW), and propose three new metrics that complement WHDR:intensity, chromaticity and texture metrics. We show that existing algorithmsoften improve WHDR metric but perform poorly on other metrics. We then finetunedifferent algorithms on our MAW dataset to significantly improve the quality ofthe reconstructed albedo both quantitatively and qualitatively. Since theproposed intensity, chromaticity, and texture metrics and the WHDR are allcomplementary we further introduce a relative performance measure that capturesaverage performance. By analysing existing algorithms we show that there issignificant room for improvement. Our dataset and evaluation metrics willenable researchers to develop algorithms that improve albedo reconstruction.Code and Data available at: https://measuredalbedo.github.io/</description><author>Jiaye Wu, Sanjoy Chowdhury, Hariharmano Shanmugaraja, David Jacobs, Soumyadip Sengupta</author><pubDate>Thu, 29 Jun 2023 18:42:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15662v2</guid></item><item><title>ID-Pose: Sparse-view Camera Pose Estimation by Inverting Diffusion Models</title><link>http://arxiv.org/abs/2306.17140v1</link><description>Given sparse views of an object, estimating their camera poses is along-standing and intractable problem. We harness the pre-trained diffusionmodel of novel views conditioned on viewpoints (Zero-1-to-3). We presentID-Pose which inverses the denoising diffusion process to estimate the relativepose given two input images. ID-Pose adds a noise on one image, and predictsthe noise conditioned on the other image and a decision variable for the pose.The prediction error is used as the objective to find the optimal pose with thegradient descent method. ID-Pose can handle more than two images and estimateeach of the poses with multiple image pairs from triangular relationships.ID-Pose requires no training and generalizes to real-world images. We conductexperiments using high-quality real-scanned 3D objects, where ID-Posesignificantly outperforms state-of-the-art methods.</description><author>Weihao Cheng, Yan-Pei Cao, Ying Shan</author><pubDate>Thu, 29 Jun 2023 18:41:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17140v1</guid></item><item><title>Simulation of Human and Artificial Emotion (SHArE)</title><link>http://arxiv.org/abs/2011.02151v2</link><description>The framework for Simulation of Human and Artificial Emotion (SHArE)describes the architecture of emotion in terms of parameters transferablebetween psychology, neuroscience, and artificial intelligence. These parameterscan be defined as abstract concepts or granularized down to the voltage levelsof individual neurons. This model enables emotional trajectory design forhumans which may lead to novel therapeutic solutions for various mental healthconcerns. For artificial intelligence, this work provides a compact notationwhich can be applied to neural networks as a means to observe the emotions andmotivations of machines.</description><author>Kwadwo Opong-Mensah</author><pubDate>Thu, 29 Jun 2023 18:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.02151v2</guid></item><item><title>Orbit Classification of asteroids using implementation of radial Basis Function on Support Vector Machines</title><link>http://arxiv.org/abs/2306.17138v1</link><description>This research paper focuses on the implementation of radial Basis Function(RBF) Support Vector Machines (SVM) for classifying asteroid orbits. Asteroidsare important astronomical objects, and their orbits play a crucial role inunderstanding the dynamics of the solar system. The International AstronomicalUnion maintains data archives that provide a playground to experiment withvarious machine-learning techniques. In this study, we explore the applicationof RBF SVM algorithm to classify asteroids. The results show that the RBF SVMalgorithm provides a good efficiency and accuracy to the dataset. We alsoanalyze the impact of various parameters on the performance of the RBF SVMalgorithm and present the optimal parameter settings. Our study highlights theimportance of using machine learning techniques for classifying asteroid orbitsand the effectiveness of the RBF SVM algorithm in this regard.</description><author>Yashvir Tiberwal, Nishchal Dwivedi</author><pubDate>Thu, 29 Jun 2023 18:39:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17138v1</guid></item><item><title>Bring Your Own Data! Self-Supervised Evaluation for Large Language Models</title><link>http://arxiv.org/abs/2306.13651v2</link><description>With the rise of Large Language Models (LLMs) and their ubiquitous deploymentin diverse domains, measuring language model behavior on realistic data isimperative. For example, a company deploying a client-facing chatbot mustensure that the model will not respond to client requests with profanity.Current evaluations approach this problem using small, domain-specific datasetswith human-curated labels. These evaluation sets are often sampled from anarrow and simplified distribution, and data sources can unknowingly be leakedinto the training set which can lead to misleading evaluations. To bypass thesedrawbacks, we propose a framework for self-supervised evaluation of LLMs byanalyzing their sensitivity or invariance to transformations on the input text.Self-supervised evaluation can directly monitor LLM behavior on datasetscollected in the wild or streamed during live model deployment. We demonstrateself-supervised evaluation strategies for measuring closed-book knowledge,toxicity, and long-range context dependence, in addition to sensitivity togrammatical structure and tokenization errors. When comparisons to similarhuman-labeled benchmarks are available, we find strong correlations betweenself-supervised and human-supervised evaluations. The self-supervised paradigmcomplements current evaluation strategies that rely on labeled data.</description><author>Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein</author><pubDate>Thu, 29 Jun 2023 18:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13651v2</guid></item><item><title>PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN</title><link>http://arxiv.org/abs/2306.17123v1</link><description>Portrait synthesis creates realistic digital avatars which enable users tointeract with others in a compelling way. Recent advances in StyleGAN and itsextensions have shown promising results in synthesizing photorealistic andaccurate reconstruction of human faces. However, previous methods often focuson frontal face synthesis and most methods are not able to handle large headrotations due to the training data distribution of StyleGAN. In this work, ourgoal is to take as input a monocular video of a face, and create an editabledynamic portrait able to handle extreme head poses. The user can create novelviewpoints, edit the appearance, and animate the face. Our method utilizespivotal tuning inversion (PTI) to learn a personalized video prior from amonocular video sequence. Then we can input pose and expression coefficients toMLPs and manipulate the latent vectors to synthesize different viewpoints andexpressions of the subject. We also propose novel loss functions to furtherdisentangle pose and expression in the latent space. Our algorithm shows muchbetter performance over previous approaches on monocular video datasets, and itis also capable of running in real-time at 54 FPS on an RTX 3080.</description><author>Kai-En Lin, Alex Trevithick, Keli Cheng, Michel Sarkis, Mohsen Ghafoorian, Ning Bi, Gerhard Reitmayr, Ravi Ramamoorthi</author><pubDate>Thu, 29 Jun 2023 18:26:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17123v1</guid></item><item><title>Data Augmentation Approaches for Source Code Models: A Survey</title><link>http://arxiv.org/abs/2305.19915v3</link><description>The increasingly popular adoption of source code in many critical tasksmotivates the development of data augmentation (DA) techniques to enhancetraining data and improve various capabilities (e.g., robustness andgeneralizability) of these models. Although a series of DA methods have beenproposed and tailored for source code models, there lacks a comprehensivesurvey and examination to understand their effectiveness and implications. Thispaper fills this gap by conducting a comprehensive and integrative survey ofdata augmentation for source code, wherein we systematically compile andencapsulate existing literature to provide a comprehensive overview of thefield. We start by constructing a taxonomy of DA for source code models modelapproaches, followed by a discussion on prominent, methodologicallyillustrative approaches. Next, we highlight the general strategies andtechniques to optimize the DA quality. Subsequently, we underscore techniquesthat find utility in widely-accepted source code scenarios and downstreamtasks. Finally, we outline the prevailing challenges and potentialopportunities for future research. In essence, this paper endeavors todemystify the corpus of existing literature on DA for source code models, andfoster further exploration in this sphere. Complementing this, we present acontinually updated GitHub repository that hosts a list of update-to-datepapers on DA for source code models, accessible at\url{https://github.com/terryyz/DataAug4Code}.</description><author>Terry Yue Zhuo, Zhou Yang, Zhensu Sun, Yufei Wang, Li Li, Xiaoning Du, Zhenchang Xing, David Lo</author><pubDate>Thu, 29 Jun 2023 18:26:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19915v3</guid></item><item><title>Learning Nuclei Representations with Masked Image Modelling</title><link>http://arxiv.org/abs/2306.17116v1</link><description>Masked image modelling (MIM) is a powerful self-supervised representationlearning paradigm, whose potential has not been widely demonstrated in medicalimage analysis. In this work, we show the capacity of MIM to capture richsemantic representations of Haemotoxylin &amp; Eosin (H&amp;E)-stained images at thenuclear level. Inspired by Bidirectional Encoder representation from ImageTransformers (BEiT), we split the images into smaller patches and generatecorresponding discrete visual tokens. In addition to the regular grid-basedpatches, typically used in visual Transformers, we introduce patches ofindividual cell nuclei. We propose positional encoding of the irregulardistribution of these structures within an image. We pre-train the model in aself-supervised manner on H&amp;E-stained whole-slide images of diffuse largeB-cell lymphoma, where cell nuclei have been segmented. The pre-trainingobjective is to recover the original discrete visual tokens of the masked imageon the one hand, and to reconstruct the visual tokens of the masked objectinstances on the other. Coupling these two pre-training tasks allows us tobuild powerful, context-aware representations of nuclei. Our model generalizeswell and can be fine-tuned on downstream classification tasks, achievingimproved cell classification accuracy on PanNuke dataset by more than 5%compared to current instance segmentation methods.</description><author>Piotr Wójcik, Hussein Naji, Adrian Simon, Reinhard Büttner, Katarzyna Bożek</author><pubDate>Thu, 29 Jun 2023 18:20:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17116v1</guid></item><item><title>Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation</title><link>http://arxiv.org/abs/2306.17115v1</link><description>We present a novel alignment-before-generation approach to tackle thechallenging task of generating general 3D shapes based on 2D images or texts.Directly learning a conditional generative model from images or texts to 3Dshapes is prone to producing inconsistent results with the conditions because3D shapes have an additional dimension whose distribution significantly differsfrom that of 2D images and texts. To bridge the domain gap among the threemodalities and facilitate multi-modal-conditioned 3D shape generation, weexplore representing 3D shapes in a shape-image-text-aligned space. Ourframework comprises two models: a Shape-Image-Text-Aligned VariationalAuto-Encoder (SITA-VAE) and a conditional Aligned Shape Latent Diffusion Model(ASLDM). The former model encodes the 3D shapes into the shape latent spacealigned to the image and text and reconstructs the fine-grained 3D neuralfields corresponding to given shape embeddings via the transformer-baseddecoder. The latter model learns a probabilistic mapping function from theimage or text space to the latent shape space. Our extensive experimentsdemonstrate that our proposed approach can generate higher-quality and morediverse 3D shapes that better semantically conform to the visual or texturalconditional inputs, validating the effectiveness of theshape-image-text-aligned space for cross-modality 3D shape generation.</description><author>Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, Shenghua Gao</author><pubDate>Thu, 29 Jun 2023 18:17:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17115v1</guid></item><item><title>Gradient flows on graphons: existence, convergence, continuity equations</title><link>http://arxiv.org/abs/2111.09459v3</link><description>Wasserstein gradient flows on probability measures have found a host ofapplications in various optimization problems. They typically arise as thecontinuum limit of exchangeable particle systems evolving by some mean-fieldinteraction involving a gradient-type potential. However, in many problems,such as in multi-layer neural networks, the so-called particles are edgeweights on large graphs whose nodes are exchangeable. Such large graphs areknown to converge to continuum limits called graphons as their size grow toinfinity. We show that the Euclidean gradient flow of a suitable function ofthe edge-weights converges to a novel continuum limit given by a curve on thespace of graphons that can be appropriately described as a gradient flow or,more technically, a curve of maximal slope. Several natural functions ongraphons, such as homomorphism functions and the scalar entropy, are covered byour set-up, and the examples have been worked out in detail.</description><author>Sewoong Oh, Soumik Pal, Raghav Somani, Raghavendra Tripathi</author><pubDate>Thu, 29 Jun 2023 18:11:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.09459v3</guid></item><item><title>SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient</title><link>http://arxiv.org/abs/2301.11913v2</link><description>Many deep learning applications benefit from using large models with billionsof parameters. Training these models is notoriously expensive due to the needfor specialized HPC clusters. In this work, we consider alternative setups fortraining large models: using cheap "preemptible" instances or pooling existingresources from multiple regions. We analyze the performance of existingmodel-parallel algorithms in these conditions and find configurations wheretraining larger models becomes less communication-intensive. Based on thesefindings, we propose SWARM parallelism, a model-parallel training algorithmdesigned for poorly connected, heterogeneous and unreliable devices. SWARMcreates temporary randomized pipelines between nodes that are rebalanced incase of failure. We empirically validate our findings and compare SWARMparallelism with existing large-scale training approaches. Finally, we combineour insights with compression strategies to train a large Transformer languagemodel with 1B shared parameters (approximately 13B before sharing) onpreemptible T4 GPUs with less than 200Mb/s network.</description><author>Max Ryabinin, Tim Dettmers, Michael Diskin, Alexander Borzunov</author><pubDate>Thu, 29 Jun 2023 18:11:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11913v2</guid></item><item><title>Synthetic Demographic Data Generation for Card Fraud Detection Using GANs</title><link>http://arxiv.org/abs/2306.17109v1</link><description>Using machine learning models to generate synthetic data has become common inmany fields. Technology to generate synthetic transactions that can be used todetect fraud is also growing fast. Generally, this synthetic data contains onlyinformation about the transaction, such as the time, place, and amount ofmoney. It does not usually contain the individual user's characteristics (ageand gender are occasionally included). Using relatively complex syntheticdemographic data may improve the complexity of transaction data features, thusimproving the fraud detection performance. Benefiting from developments ofmachine learning, some deep learning models have potential to perform betterthan other well-established synthetic data generation methods, such asmicrosimulation. In this study, we built a deep-learning Generative AdversarialNetwork (GAN), called DGGAN, which will be used for demographic datageneration. Our model generates samples during model training, which we foundimportant to overcame class imbalance issues. This study can help improve thecognition of synthetic data and further explore the application of syntheticdata generation in card fraud detection.</description><author>Shuo Wang, Terrence Tricco, Xianta Jiang, Charles Robertson, John Hawkin</author><pubDate>Thu, 29 Jun 2023 18:08:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17109v1</guid></item><item><title>ManimML: Communicating Machine Learning Architectures with Animation</title><link>http://arxiv.org/abs/2306.17108v1</link><description>There has been an explosion in interest in machine learning (ML) in recentyears due to its applications to science and engineering. However, as MLtechniques have advanced, tools for explaining and visualizing novel MLalgorithms have lagged behind. Animation has been shown to be a powerful toolfor making engaging visualizations of systems that dynamically change overtime, which makes it well suited to the task of communicating ML algorithms.However, the current approach to animating ML algorithms is to handcraftapplications that highlight specific algorithms or use complex generalizedanimation software. We developed ManimML, an open-source Python library foreasily generating animations of ML algorithms directly from code. We sought toleverage ML practitioners' preexisting knowledge of programming rather thanrequiring them to learn complex animation software. ManimML has a familiarsyntax for specifying neural networks that mimics popular deep learningframeworks like Pytorch. A user can take a preexisting neural networkarchitecture and easily write a specification for an animation in ManimML,which will then automatically compose animations for different components ofthe system into a final animation of the entire neural network. ManimML is opensource and available at https://github.com/helblazer811/ManimML.</description><author>Alec Helbling, Duen Horng, Chau</author><pubDate>Thu, 29 Jun 2023 18:08:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17108v1</guid></item><item><title>LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding</title><link>http://arxiv.org/abs/2306.17107v1</link><description>Instruction tuning unlocks the superior capability of Large Language Models(LLM) to interact with humans. Furthermore, recent instruction-followingdatasets include images as visual inputs, collecting responses for image-basedinstructions. However, visual instruction-tuned models cannot comprehendtextual details within images well. This work enhances the current visualinstruction tuning pipeline with text-rich images (e.g., movie posters, bookcovers, etc.). Specifically, we first use publicly available OCR tools tocollect results on 422K text-rich images from the LAION dataset. Moreover, weprompt text-only GPT-4 with recognized texts and image captions to generate 16Kconversations, each containing question-answer pairs for text-rich images. Bycombining our collected data with previous multi-modal instruction-followingdata, our model, LLaVAR, substantially improves the LLaVA model's capability ontext-based VQA datasets (up to 20% accuracy improvement) while achieving anaccuracy of 91.42% on ScienceQA. The GPT-4-based instruction-followingevaluation also demonstrates the improvement of our model on both naturalimages and text-rich images. Through qualitative analysis, LLaVAR showspromising interaction (e.g., reasoning, writing, and elaboration) skills withhumans based on the latest real-world online content that combines text andimages. We make our code/data/models publicly available athttps://llavar.github.io/.</description><author>Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, Tong Sun</author><pubDate>Thu, 29 Jun 2023 18:08:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17107v1</guid></item><item><title>Are Neurons Actually Collapsed? On the Fine-Grained Structure in Neural Representations</title><link>http://arxiv.org/abs/2306.17105v1</link><description>Recent work has observed an intriguing ''Neural Collapse'' phenomenon inwell-trained neural networks, where the last-layer representations of trainingsamples with the same label collapse into each other. This appears to suggestthat the last-layer representations are completely determined by the labels,and do not depend on the intrinsic structure of input distribution. We provideevidence that this is not a complete description, and that the apparentcollapse hides important fine-grained structure in the representations.Specifically, even when representations apparently collapse, the small amountof remaining variation can still faithfully and accurately captures theintrinsic structure of input distribution. As an example, if we train onCIFAR-10 using only 5 coarse-grained labels (by combining two classes into onesuper-class) until convergence, we can reconstruct the original 10-class labelsfrom the learned representations via unsupervised clustering. The reconstructedlabels achieve $93\%$ accuracy on the CIFAR-10 test set, nearly matching thenormal CIFAR-10 accuracy for the same architecture. We also provide an initialtheoretical result showing the fine-grained representation structure in asimplified synthetic setting. Our results show concretely how the structure ofinput data can play a significant role in determining the fine-grainedstructure of neural representations, going beyond what Neural Collapsepredicts.</description><author>Yongyi Yang, Jacob Steinhardt, Wei Hu</author><pubDate>Thu, 29 Jun 2023 18:07:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17105v1</guid></item><item><title>Deep Ensemble for Rotorcraft Attitude Prediction</title><link>http://arxiv.org/abs/2306.17104v1</link><description>Historically, the rotorcraft community has experienced a higher fatalaccident rate than other aviation segments, including commercial and generalaviation. Recent advancements in artificial intelligence (AI) and theapplication of these technologies in different areas of our lives are bothintriguing and encouraging. When developed appropriately for the aviationdomain, AI techniques provide an opportunity to help design systems that canaddress rotorcraft safety challenges. Our recent work demonstrated that AIalgorithms could use video data from onboard cameras and correctly identifydifferent flight parameters from cockpit gauges, e.g., indicated airspeed.These AI-based techniques provide a potentially cost-effective solution,especially for small helicopter operators, to record the flight stateinformation and perform post-flight analyses. We also showed that carefullydesigned and trained AI systems could accurately predict rotorcraft attitude(i.e., pitch and yaw) from outside scenes (images or video data). Ordinaryoff-the-shelf video cameras were installed inside the rotorcraft cockpit torecord the outside scene, including the horizon. The AI algorithm couldcorrectly identify rotorcraft attitude at an accuracy in the range of 80\%. Inthis work, we combined five different onboard camera viewpoints to improveattitude prediction accuracy to 94\%. In this paper, five onboard camera viewsincluded the pilot windshield, co-pilot windshield, pilot Electronic FlightInstrument System (EFIS) display, co-pilot EFIS display, and the attitudeindicator gauge. Using video data from each camera view, we trained variousconvolutional neural networks (CNNs), which achieved prediction accuracy in therange of 79\% % to 90\% %. We subsequently ensembled the learned knowledge fromall CNNs and achieved an ensembled accuracy of 93.3\%.</description><author>Hikmat Khan, Nidhal Carla Bouaynaya, Ghulam Rasool, Tyler Travis, Lacey Thompson, Charles C. Johnson</author><pubDate>Thu, 29 Jun 2023 18:06:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17104v1</guid></item><item><title>LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT</title><link>http://arxiv.org/abs/2306.17103v1</link><description>We introduce LyricWhiz, a robust, multilingual, and zero-shot automaticlyrics transcription method achieving state-of-the-art performance on variouslyrics transcription datasets, even in challenging genres such as rock andmetal. Our novel, training-free approach utilizes Whisper, a weakly supervisedrobust speech recognition model, and GPT-4, today's most performant chat-basedlarge language model. In the proposed method, Whisper functions as the "ear" bytranscribing the audio, while GPT-4 serves as the "brain," acting as anannotator with a strong performance for contextualized output selection andcorrection. Our experiments show that LyricWhiz significantly reduces WordError Rate compared to existing methods in English and can effectivelytranscribe lyrics across multiple languages. Furthermore, we use LyricWhiz tocreate the first publicly available, large-scale, multilingual lyricstranscription dataset with a CC-BY-NC-SA copyright license, based onMTG-Jamendo, and offer a human-annotated subset for noise level estimation andevaluation. We anticipate that our proposed method and dataset will advance thedevelopment of multilingual lyrics transcription, a challenging and emergingtask.</description><author>Le Zhuo, Ruibin Yuan, Jiahao Pan, Yinghao Ma, Yizhi LI, Ge Zhang, Si Liu, Roger Dannenberg, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wenhu Chen, Wei Xue, Yike Guo</author><pubDate>Thu, 29 Jun 2023 18:01:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17103v1</guid></item><item><title>Identifying Important Sensory Feedback for Learning Locomotion Skills</title><link>http://arxiv.org/abs/2306.17101v1</link><description>Robot motor skills can be learned through deep reinforcement learning (DRL)by neural networks as state-action mappings. While the selection of stateobservations is crucial, there has been a lack of quantitative analysis todate. Here, we present a systematic saliency analysis that quantitativelyevaluates the relative importance of different feedback states for motor skillslearned through DRL. Our approach can identify the most essential feedbackstates for locomotion skills, including balance recovery, trotting, bounding,pacing and galloping. By using only key states including joint positions,gravity vector, base linear and angular velocities, we demonstrate that asimulated quadruped robot can achieve robust performance in various testscenarios across these distinct skills. The benchmarks using task performancemetrics show that locomotion skills learned with key states can achievecomparable performance to those with all states, and the task performance orlearning success rate will drop significantly if key states are missing. Thiswork provides quantitative insights into the relationship between stateobservations and specific types of motor skills, serving as a guideline forrobot motor learning. The proposed method is applicable to differentiablestate-action mapping, such as neural network based control policies, enablingthe learning of a wide range of motor skills with minimal sensing dependencies.</description><author>Wanming Yu, Chuanyu Yang, Christopher McGreavy, Eleftherios Triantafyllidis, Guillaume Bellegarda, Milad Shafiee, Auke Jan Ijspeert, Zhibin Li</author><pubDate>Thu, 29 Jun 2023 17:58:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17101v1</guid></item><item><title>RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark</title><link>http://arxiv.org/abs/2306.17100v1</link><description>We introduce RL4CO, an extensive reinforcement learning (RL) forcombinatorial optimization (CO) benchmark. RL4CO employs state-of-the-artsoftware libraries as well as best practices in implementation, such asmodularity and configuration management, to be efficient and easily modifiableby researchers for adaptations of neural network architecture, environments,and algorithms. Contrary to the existing focus on specific tasks like thetraveling salesman problem (TSP) for performance assessment, we underline theimportance of scalability and generalization capabilities for diverseoptimization tasks. We also systematically benchmark sample efficiency,zero-shot generalization, and adaptability to changes in data distributions ofvarious models. Our experiments show that some recent state-of-the-art methodsfall behind their predecessors when evaluated using these new metrics,suggesting the necessity for a more balanced view of the performance of neuralCO solvers. We hope RL4CO will encourage the exploration of novel solutions tocomplex real-world tasks, allowing to compare with existing methods through astandardized interface that decouples the science from the softwareengineering. We make our library publicly available athttps://github.com/kaist-silab/rl4co.</description><author>Federico Berto, Chuanbo Hua, Junyoung Park, Minsu Kim, Hyeonah Kim, Jiwoo Son, Haeyeon Kim, Joungho Kim, Jinkyoo Park</author><pubDate>Thu, 29 Jun 2023 17:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17100v1</guid></item><item><title>The Importance of Robust Features in Mitigating Catastrophic Forgetting</title><link>http://arxiv.org/abs/2306.17091v1</link><description>Continual learning (CL) is an approach to address catastrophic forgetting,which refers to forgetting previously learned knowledge by neural networks whentrained on new tasks or data distributions. The adversarial robustness hasdecomposed features into robust and non-robust types and demonstrated thatmodels trained on robust features significantly enhance adversarial robustness.However, no study has been conducted on the efficacy of robust features fromthe lens of the CL model in mitigating catastrophic forgetting in CL. In thispaper, we introduce the CL robust dataset and train four baseline models onboth the standard and CL robust datasets. Our results demonstrate that the CLmodels trained on the CL robust dataset experienced less catastrophicforgetting of the previously learned tasks than when trained on the standarddataset. Our observations highlight the significance of the features providedto the underlying CL models, showing that CL robust features can alleviatecatastrophic forgetting.</description><author>Hikmat Khan, Nidhal C. Bouaynaya, Ghulam Rasoom</author><pubDate>Thu, 29 Jun 2023 17:48:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17091v1</guid></item><item><title>Sparsity exploitation via discovering graphical models in multi-variate time-series forecasting</title><link>http://arxiv.org/abs/2306.17090v1</link><description>Graph neural networks (GNNs) have been widely applied in multi-variatetime-series forecasting (MTSF) tasks because of their capability in capturingthe correlations among different time-series. These graph-based learningapproaches improve the forecasting performance by discovering and understandingthe underlying graph structures, which represent the data correlation. When theexplicit prior graph structures are not available, most existing works cannotguarantee the sparsity of the generated graphs that make the overall modelcomputational expensive and less interpretable. In this work, we propose adecoupled training method, which includes a graph generating module and a GNNsforecasting module. First, we use Graphical Lasso (or GraphLASSO) to directlyexploit the sparsity pattern from data to build graph structures in both staticand time-varying cases. Second, we fit these graph structures and the inputdata into a Graph Convolutional Recurrent Network (GCRN) to train a forecastingmodel. The experimental results on three real-world datasets show that ournovel approach has competitive performance against existing state-of-the-artforecasting algorithms while providing sparse, meaningful and explainable graphstructures and reducing training time by approximately 40%. Our PyTorchimplementation is publicly available at https://github.com/HySonLab/GraphLASSO</description><author>Ngoc-Dung Do, Truong Son Hy, Duy Khuong Nguyen</author><pubDate>Thu, 29 Jun 2023 17:48:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17090v1</guid></item><item><title>Concept-Oriented Deep Learning with Large Language Models</title><link>http://arxiv.org/abs/2306.17089v1</link><description>Large Language Models (LLMs) have been successfully used in manynatural-language tasks and applications including text generation and AIchatbots. They also are a promising new technology for concept-oriented deeplearning (CODL). However, the prerequisite is that LLMs understand concepts andensure conceptual consistency. We discuss these in this paper, as well as majoruses of LLMs for CODL including concept extraction from text, concept graphextraction from text, and concept learning. Human knowledge consists of bothsymbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-onlyLLMs, however, can represent only symbolic (conceptual) knowledge. MultimodalLLMs, on the other hand, are capable of representing the full range (conceptualand sensory) of human knowledge. We discuss conceptual understanding invisual-language LLMs, the most important multimodal LLMs, and major uses ofthem for CODL including concept extraction from image, concept graph extractionfrom image, and concept learning. While uses of LLMs for CODL are valuablestandalone, they are particularly valuable as part of LLM applications such asAI chatbots.</description><author>Daniel T. Chang</author><pubDate>Thu, 29 Jun 2023 17:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17089v1</guid></item><item><title>Likelihood-free neural Bayes estimators for censored inference with peaks-over-threshold models</title><link>http://arxiv.org/abs/2306.15642v2</link><description>Inference for spatial extremal dependence models can be computationallyburdensome in moderate-to-high dimensions due to their reliance on intractableand/or censored likelihoods. Exploiting recent advances in likelihood-freeinference with neural Bayes estimators (that is, neural estimators that targetBayes estimators), we develop a novel approach to construct highly efficientestimators for censored peaks-over-threshold models by encoding censoringinformation in the neural network architecture. Our new method provides aparadigm shift that challenges traditional censored likelihood-based inferencefor spatial extremes. Our simulation studies highlight significant gains inboth computational and statistical efficiency, relative to competinglikelihood-based approaches, when applying our novel estimators for inferenceof popular extremal dependence models, such as max-stable, $r$-Pareto, andrandom scale mixture processes. We also illustrate that it is possible to traina single estimator for a general censoring level, obviating the need to retrainwhen the censoring level is changed. We illustrate the efficacy of ourestimators by making fast inference on hundreds-of-thousands ofhigh-dimensional spatial extremal dependence models to assess particulatematter 2.5 microns or less in diameter (PM2.5) concentration over the whole ofSaudi Arabia.</description><author>Jordan Richards, Matthew Sainsbury-Dale, Andrew Zammit-Mangion, Raphaël Huser</author><pubDate>Thu, 29 Jun 2023 17:42:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15642v2</guid></item><item><title>Magnitude Invariant Parametrizations Improve Hypernetwork Learning</title><link>http://arxiv.org/abs/2304.07645v2</link><description>Hypernetworks, neural networks that predict the parameters of another neuralnetwork, are powerful models that have been successfully used in diverseapplications from image generation to multi-task learning. Unfortunately,existing hypernetworks are often challenging to train. Training typicallyconverges far more slowly than for non-hypernetwork models, and the rate ofconvergence can be very sensitive to hyperparameter choices. In this work, weidentify a fundamental and previously unidentified problem that contributes tothe challenge of training hypernetworks: a magnitude proportionality betweenthe inputs and outputs of the hypernetwork. We demonstrate both analyticallyand empirically that this can lead to unstable optimization, thereby slowingdown convergence, and sometimes even preventing any learning. We present asimple solution to this problem using a revised hypernetwork formulation thatwe call Magnitude Invariant Parametrizations (MIP). We demonstrate the proposedsolution on several hypernetwork tasks, where it consistently stabilizestraining and achieves faster convergence. Furthermore, we perform acomprehensive ablation study including choices of activation function,normalization strategies, input dimensionality, and hypernetwork architecture;and find that MIP improves training in all scenarios. We provide easy-to-usecode that can turn existing networks into MIP-based hypernetworks.</description><author>Jose Javier Gonzalez Ortiz, John Guttag, Adrian Dalca</author><pubDate>Thu, 29 Jun 2023 17:38:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07645v2</guid></item><item><title>Scale-Space Hypernetworks for Efficient Biomedical Imaging</title><link>http://arxiv.org/abs/2304.05448v2</link><description>Convolutional Neural Networks (CNNs) are the predominant model used for avariety of medical image analysis tasks. At inference time, these models arecomputationally intensive, especially with volumetric data. In principle, it ispossible to trade accuracy for computational efficiency by manipulating therescaling factor in the downsample and upsample layers of CNN architectures.However, properly exploring the accuracy-efficiency trade-off is prohibitivelyexpensive with existing models. To address this, we introduce Scale-SpaceHyperNetworks (SSHN), a method that learns a spectrum of CNNs with varyinginternal rescaling factors. A single SSHN characterizes an entire Paretoaccuracy-efficiency curve of models that match, and occasionally surpass, theoutcomes of training many separate networks with fixed rescaling factors. Wedemonstrate the proposed approach in several medical image analysisapplications, comparing SSHN against strategies with both fixed and dynamicrescaling factors. We find that SSHN consistently provides a betteraccuracy-efficiency trade-off at a fraction of the training cost. Trained SSHNsenable the user to quickly choose a rescaling factor that appropriatelybalances accuracy and computational efficiency for their particular needs atinference.</description><author>Jose Javier Gonzalez Ortiz, John Guttag, Adrian Dalca</author><pubDate>Thu, 29 Jun 2023 17:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05448v2</guid></item><item><title>RAPGen: An Approach for Fixing Code Inefficiencies in Zero-Shot</title><link>http://arxiv.org/abs/2306.17077v1</link><description>Performance bugs are non-functional bugs that can even manifest inwell-tested commercial products. Fixing these performance bugs is an importantyet challenging problem. In this work, we address this challenge and present anew approach called Retrieval-Augmented Prompt Generation (RAPGen). Given acode snippet with a performance issue, RAPGen first retrieves a promptinstruction from a pre-constructed knowledge-base of previous performance bugfixes and then generates a prompt using the retrieved instruction. It then usesthis prompt on a Large Language Model (such as Codex) in zero-shot to generatea fix. We compare our approach with the various prompt variations and state ofthe art methods in the task of performance bug fixing. Our evaluation showsthat RAPGen can generate performance improvement suggestions equivalent orbetter than a developer in ~60% of the cases, getting ~39% of them verbatim, inan expert-verified dataset of past performance changes made by C# developers.</description><author>Spandan Garg, Roshanak Zilouchian Moghaddam, Neel Sundaresan</author><pubDate>Thu, 29 Jun 2023 17:28:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17077v1</guid></item><item><title>Python Wrapper for Simulating Multi-Fidelity Optimization on HPO Benchmarks without Any Wait</title><link>http://arxiv.org/abs/2305.17595v2</link><description>Hyperparameter (HP) optimization of deep learning (DL) is essential for highperformance. As DL often requires several hours to days for its training, HPoptimization (HPO) of DL is often prohibitively expensive. This boosted theemergence of tabular or surrogate benchmarks, which enable querying the(predictive) performance of DL with a specific HP configuration in a fraction.However, since the actual runtime of a DL training is significantly differentfrom its query response time, simulators of an asynchronous HPO, e.g.multi-fidelity optimization, must wait for the actual runtime at each iterationin a na\"ive implementation; otherwise, the evaluation order during simulationdoes not match with the real experiment. To ease this issue, we developed aPython wrapper and describe its usage. This wrapper forces each worker to waitso that we yield exactly the same evaluation order as in the real experimentwith only $10^{-2}$ seconds of waiting instead of waiting several hours. Ourimplementation is available athttps://github.com/nabenabe0928/mfhpo-simulator/.</description><author>Shuhei Watanabe</author><pubDate>Thu, 29 Jun 2023 17:27:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17595v2</guid></item><item><title>Detect Any Deepfakes: Segment Anything Meets Face Forgery Detection and Localization</title><link>http://arxiv.org/abs/2306.17075v1</link><description>The rapid advancements in computer vision have stimulated remarkable progressin face forgery techniques, capturing the dedicated attention of researcherscommitted to detecting forgeries and precisely localizing manipulated areas.Nonetheless, with limited fine-grained pixel-wise supervision labels, deepfakedetection models perform unsatisfactorily on precise forgery detection andlocalization. To address this challenge, we introduce the well-trained visionsegmentation foundation model, i.e., Segment Anything Model (SAM) in faceforgery detection and localization. Based on SAM, we propose the Detect AnyDeepfakes (DADF) framework with the Multiscale Adapter, which can captureshort- and long-range forgery contexts for efficient fine-tuning. Moreover, tobetter identify forged traces and augment the model's sensitivity towardsforgery regions, Reconstruction Guided Attention (RGA) module is proposed. Theproposed framework seamlessly integrates end-to-end forgery localization anddetection optimization. Extensive experiments on three benchmark datasetsdemonstrate the superiority of our approach for both forgery detection andlocalization. The codes will be released soon athttps://github.com/laiyingxin2/DADF.</description><author>Yingxin Lai, Zhiming Luo, Zitong Yu</author><pubDate>Thu, 29 Jun 2023 17:25:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17075v1</guid></item><item><title>Learning Structure-Guided Diffusion Model for 2D Human Pose Estimation</title><link>http://arxiv.org/abs/2306.17074v1</link><description>One of the mainstream schemes for 2D human pose estimation (HPE) is learningkeypoints heatmaps by a neural network. Existing methods typically improve thequality of heatmaps by customized architectures, such as high-resolutionrepresentation and vision Transformers. In this paper, we propose\textbf{DiffusionPose}, a new scheme that formulates 2D HPE as a keypointsheatmaps generation problem from noised heatmaps. During training, thekeypoints are diffused to random distribution by adding noises and thediffusion model learns to recover ground-truth heatmaps from noised heatmapswith respect to conditions constructed by image feature. During inference, thediffusion model generates heatmaps from initialized heatmaps in a progressivedenoising way. Moreover, we further explore improving the performance ofDiffusionPose with conditions from human structural information. Extensiveexperiments show the prowess of our DiffusionPose, with improvements of 1.6,1.2, and 1.2 mAP on widely-used COCO, CrowdPose, and AI Challenge datasets,respectively.</description><author>Zhongwei Qiu, Qiansheng Yang, Jian Wang, Xiyu Wang, Chang Xu, Dongmei Fu, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang</author><pubDate>Thu, 29 Jun 2023 17:24:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17074v1</guid></item><item><title>Differentially Private Algorithms for the Stochastic Saddle Point Problem with Optimal Rates for the Strong Gap</title><link>http://arxiv.org/abs/2302.12909v2</link><description>We show that convex-concave Lipschitz stochastic saddle point problems (alsoknown as stochastic minimax optimization) can be solved under the constraint of$(\epsilon,\delta)$-differential privacy with \emph{strong (primal-dual) gap}rate of $\tilde O\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$,where $n$ is the dataset size and $d$ is the dimension of the problem. Thisrate is nearly optimal, based on existing lower bounds in differentiallyprivate stochastic optimization. Specifically, we prove a tight upper bound onthe strong gap via novel implementation and analysis of the recursiveregularization technique repurposed for saddle point problems. We show thatthis rate can be attained with$O\big(\min\big\{\frac{n^2\epsilon^{1.5}}{\sqrt{d}}, n^{3/2}\big\}\big)$gradient complexity, and $\tilde{O}(n)$ gradient complexity if the lossfunction is smooth. As a byproduct of our method, we develop a generalalgorithm that, given a black-box access to a subroutine satisfying a certain$\alpha$ primal-dual accuracy guarantee with respect to the empiricalobjective, gives a solution to the stochastic saddle point problem with astrong gap of $\tilde{O}(\alpha+\frac{1}{\sqrt{n}})$. We show that this$\alpha$-accuracy condition is satisfied by standard algorithms for theempirical saddle point problem such as the proximal point method and thestochastic gradient descent ascent algorithm. Further, we show that even forsimple problems it is possible for an algorithm to have zero weak gap andsuffer from $\Omega(1)$ strong gap. We also show that there exists afundamental tradeoff between stability and accuracy. Specifically, we show thatany $\Delta$-stable algorithm has empirical gap $\Omega\big(\frac{1}{\Deltan}\big)$, and that this bound is tight. This result also holds also morespecifically for empirical risk minimization problems and may be of independentinterest.</description><author>Raef Bassily, Cristóbal Guzmán, Michael Menart</author><pubDate>Thu, 29 Jun 2023 17:22:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12909v2</guid></item><item><title>Interdisciplinary Methods in Computational Creativity: How Human Variables Shape Human-Inspired AI Research</title><link>http://arxiv.org/abs/2306.17070v1</link><description>The word creativity originally described a concept from human psychology, butin the realm of computational creativity (CC), it has become much more. Thequestion of what creativity means when it is part of a computational systemmight be considered core to CC. Pinning down the meaning of creativity, andconcepts like it, becomes salient when researchers port concepts from humanpsychology to computation, a widespread practice extending beyond CC intoartificial intelligence (AI). Yet, the human processes shaping human-inspiredcomputational systems have been little investigated. In this paper, we questionwhich human literatures (social sciences, psychology, neuroscience) enter AIscholarship and how they are translated at the port of entry. This study isbased on 22 in-depth, semi-structured interviews, primarily with human-inspiredAI researchers, half of whom focus on creativity as a major research area. Thispaper focuses on findings most relevant to CC. We suggest that which humanliterature enters AI bears greater scrutiny because ideas may becomedisconnected from context in their home discipline. Accordingly, we recommendthat CC researchers document the decisions and context of their practices,particularly those practices formalizing human concepts for machines.Publishing reflexive commentary on human elements in CC and AI would provide auseful record and permit greater dialogue with other disciplines.</description><author>Nadia M. Ady, Faun Rice</author><pubDate>Thu, 29 Jun 2023 17:17:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17070v1</guid></item><item><title>Algorithmic Censoring in Dynamic Learning Systems</title><link>http://arxiv.org/abs/2305.09035v2</link><description>Dynamic learning systems subject to selective labeling exhibit censoring,i.e. persistent negative predictions assigned to one or more subgroups ofpoints. In applications like consumer finance, this results in groups ofapplicants that are persistently denied and thus never enter into the trainingdata. In this work, we formalize censoring, demonstrate how it can arise, andhighlight difficulties in detection. We consider safeguards against censoring -recourse and randomized-exploration - both of which ensure we collect labelsfor points that would otherwise go unobserved. The resulting techniques allowexamples from censored groups to enter into the training data and correct themodel. Our results highlight the otherwise unmeasured harms of censoring anddemonstrate the effectiveness of mitigation strategies across a range of datagenerating processes.</description><author>Jennifer Chien, Margaret Roberts, Berk Ustun</author><pubDate>Thu, 29 Jun 2023 17:15:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09035v2</guid></item><item><title>On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data</title><link>http://arxiv.org/abs/2306.17066v1</link><description>Temporal Point Processes (TPPs) serve as the standard mathematical frameworkfor modeling asynchronous event sequences in continuous time. However,classical TPP models are often constrained by strong assumptions, limitingtheir ability to capture complex real-world event dynamics. To overcome thislimitation, researchers have proposed Neural TPPs, which leverage neuralnetwork parametrizations to offer more flexible and efficient modeling. Whilerecent studies demonstrate the effectiveness of Neural TPPs, they often lack aunified setup, relying on different baselines, datasets, and experimentalconfigurations. This makes it challenging to identify the key factors drivingimprovements in predictive accuracy, hindering research progress. To bridgethis gap, we present a comprehensive large-scale experimental study thatsystematically evaluates the predictive accuracy of state-of-the-art neural TPPmodels. Our study encompasses multiple real-world and synthetic event sequencedatasets, following a carefully designed unified setup. We thoroughlyinvestigate the influence of major architectural components such as eventencoding, history encoder, and decoder parametrization on both time and markprediction tasks. Additionally, we delve into the less explored area ofprobabilistic calibration for neural TPP models. By analyzing our results, wedraw insightful conclusions regarding the significance of history size and theimpact of architectural components on predictive accuracy. Furthermore, we shedlight on the miscalibration of mark distributions in neural TPP models. Ourstudy aims to provide valuable insights into the performance andcharacteristics of neural TPP models, contributing to a better understanding oftheir strengths and limitations.</description><author>Tanguy Bosser, Souhaib Ben Taieb</author><pubDate>Thu, 29 Jun 2023 17:14:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17066v1</guid></item><item><title>Gesture Recognition with mmWave Wi-Fi Access Points: Lessons Learned</title><link>http://arxiv.org/abs/2306.17062v1</link><description>In recent years, channel state information (CSI) at sub-6 GHz has been widelyexploited for Wi-Fi sensing, particularly for activity and gesture recognition.In this work, we instead explore mmWave (60 GHz) Wi-Fi signals for gesturerecognition/pose estimation. Our focus is on the mmWave Wi-Fi signals so thatthey can be used not only for high data rate communication but also forimproved sensing e.g., for extended reality (XR) applications. For this reason,we extract spatial beam signal-to-noise ratios (SNRs) from the periodic beamtraining employed by IEEE 802.11ad devices. We consider a set of 10gestures/poses motivated by XR applications. We conduct experiments in twoenvironments and with three people.As a comparison, we also collect CSI fromIEEE 802.11ac devices. To extract features from the CSI and the beam SNR, weleverage a deep neural network (DNN). The DNN classifier achieves promisingresults on the beam SNR task with state-of-the-art 96.7% accuracy in a singleenvironment, even with a limited dataset. We also investigate the robustness ofthe beam SNR against CSI across different environments. Our experiments revealthat features from the CSI generalize without additional re-training, whilethose from beam SNRs do not. Therefore, re-training is required in the lattercase.</description><author>Nabeel Nisar Bhat, Rafael Berkvens, Jeroen Famaey</author><pubDate>Thu, 29 Jun 2023 17:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17062v1</guid></item><item><title>Physics Informed Token Transformer</title><link>http://arxiv.org/abs/2305.08757v2</link><description>Solving Partial Differential Equations (PDEs) is the core of many fields ofscience and engineering. While classical approaches are often prohibitivelyslow, machine learning models often fail to incorporate complete systeminformation. Over the past few years, transformers have had a significantimpact on the field of Artificial Intelligence and have seen increased usage inPDE applications. However, despite their success, transformers currently lackintegration with physics and reasoning. This study aims to address this issueby introducing PITT: Physics Informed Token Transformer. The purpose of PITT isto incorporate the knowledge of physics by embedding partial differentialequations (PDEs) into the learning process. PITT uses an equation tokenizationmethod to learn an analytically-driven numerical update operator. By tokenizingPDEs and embedding partial derivatives, the transformer models become aware ofthe underlying knowledge behind physical processes. To demonstrate this, PITTis tested on challenging 1D and 2D PDE neural operator prediction tasks. Theresults show that PITT outperforms popular neural operator models and has theability to extract physically relevant information from governing equations.</description><author>Cooper Lorsung, Zijie Li, Amir Barati Farimani</author><pubDate>Thu, 29 Jun 2023 17:07:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08757v2</guid></item><item><title>The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps</title><link>http://arxiv.org/abs/2306.17059v1</link><description>Documents hold spatial focus and valuable locality characteristics. Forexample, descriptions of listings in real estate or travel blogs containinformation about specific local neighborhoods. This information is valuable tocharacterize how humans perceive their environment. However, the first step tomaking use of this information is to identify the spatial focus (e.g., a city)of a document. Traditional approaches for identifying the spatial focus of adocument rely on detecting and disambiguating toponyms from the document. Thisapproach requires a vocabulary set of location phrases and ad-hoc rules, whichignore important words related to location. Recent topic modeling approachesusing large language models often consider a few topics, each with broadcoverage. In contrast, the spatial focus of a document can be a country, acity, or even a neighborhood, which together, is much larger than the number oftopics considered in these approaches. Additionally, topic modeling methods areoften applied to broad topics of news articles where context is easilydistinguishable. To identify the geographic focus of a document effectively, wepresent a simple but effective Joint Embedding of multi-LocaLitY (JELLY), whichjointly learns representations with separate encoders of document and location.JELLY significantly outperforms state-of-the-art methods for identifyingspatial focus from documents from a number of sources. We also demonstrate casestudies on the arithmetic of the learned representations, including identifyingcities with similar locality characteristics and zero-shot learning to identifydocument spatial focus.</description><author>Jina Kim, Zekun Li, Yijun Lin, Min Namgung, Leeje Jang, Yao-Yi Chiang</author><pubDate>Thu, 29 Jun 2023 17:05:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17059v1</guid></item><item><title>Advances and Applications of Computer Vision Techniques in Vehicle Trajectory Generation and Surrogate Traffic Safety Indicators</title><link>http://arxiv.org/abs/2303.15231v2</link><description>The application of Computer Vision (CV) techniques massively stimulatesmicroscopic traffic safety analysis from the perspective of traffic conflictsand near misses, which is usually measured using Surrogate Safety Measures(SSM). However, as video processing and traffic safety modeling are twoseparate research domains and few research have focused on systematicallybridging the gap between them, it is necessary to provide transportationresearchers and practitioners with corresponding guidance. With this aim inmind, this paper focuses on reviewing the applications of CV techniques intraffic safety modeling using SSM and suggesting the best way forward. The CValgorithm that are used for vehicle detection and tracking from earlyapproaches to the state-of-the-art models are summarized at a high level. Then,the video pre-processing and post-processing techniques for vehicle trajectoryextraction are introduced. A detailed review of SSMs for vehicle trajectorydata along with their application on traffic safety analysis is presented.Finally, practical issues in traffic video processing and SSM-based safetyanalysis are discussed, and the available or potential solutions are provided.This review is expected to assist transportation researchers and engineers withthe selection of suitable CV techniques for video processing, and the usage ofSSMs for various traffic safety research objectives.</description><author>Mohamed Abdel-Aty, Zijin Wang, Ou Zheng, Amr Abdelraouf</author><pubDate>Thu, 29 Jun 2023 17:02:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15231v2</guid></item><item><title>Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning</title><link>http://arxiv.org/abs/2306.17052v1</link><description>Many applications, e.g., in shared mobility, require coordinating a largenumber of agents. Mean-field reinforcement learning addresses the resultingscalability challenge by optimizing the policy of a representative agent. Inthis paper, we address an important generalization where there exist globalconstraints on the distribution of agents (e.g., requiring capacity constraintsor minimum coverage requirements to be met). We propose Safe-$\text{M}^3$-UCRL,the first model-based algorithm that attains safe policies even in the case ofunknown transition dynamics. As a key ingredient, it uses epistemic uncertaintyin the transition model within a log-barrier approach to ensure pessimisticconstraints satisfaction with high probability. We showcaseSafe-$\text{M}^3$-UCRL on the vehicle repositioning problem faced by manyshared mobility operators and evaluate its performance through simulationsbuilt on Shenzhen taxi trajectory data. Our algorithm effectively meets thedemand in critical areas while ensuring service accessibility in regions withlow demand.</description><author>Matej Jusup, Barna Pásztor, Tadeusz Janik, Kenan Zhang, Francesco Corman, Andreas Krause, Ilija Bogunovic</author><pubDate>Thu, 29 Jun 2023 16:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17052v1</guid></item><item><title>Spiking Denoising Diffusion Probabilistic Models</title><link>http://arxiv.org/abs/2306.17046v1</link><description>Spiking neural networks (SNNs) have ultra-low energy consumption and highbiological plausibility due to their binary and bio-driven nature compared withartificial neural networks (ANNs). While previous research has primarilyfocused on enhancing the performance of SNNs in classification tasks, thegenerative potential of SNNs remains relatively unexplored. In our paper, weput forward Spiking Denoising Diffusion Probabilistic Models (SDDPM), a newclass of SNN-based generative models that achieve high sample quality. To fullyexploit the energy efficiency of SNNs, we propose a purely Spiking U-Netarchitecture, which achieves comparable performance to its ANN counterpartusing only 4 time steps, resulting in significantly reduced energy consumption.Extensive experimental results reveal that our approach achievesstate-of-the-art on the generative tasks and substantially outperforms otherSNN-based generative models, achieving up to $12\times$ and $6\times$improvement on the CIFAR-10 and the CelebA datasets, respectively. Moreover, wepropose a threshold-guided strategy that can further improve the performancesby 16.7% in a training-free manner. The SDDPM symbolizes a significantadvancement in the field of SNN generation, injecting new perspectives andpotential avenues of exploration.</description><author>Jiahang Cao, Ziqing Wang, Hanzhong Guo, Hao Cheng, Qiang Zhang, Renjing Xu</author><pubDate>Thu, 29 Jun 2023 16:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17046v1</guid></item><item><title>Towards Grammatical Tagging for the Legal Language of Cybersecurity</title><link>http://arxiv.org/abs/2306.17042v1</link><description>Legal language can be understood as the language typically used by thoseengaged in the legal profession and, as such, it may come both in spoken orwritten form. Recent legislation on cybersecurity obviously uses legal languagein writing, thus inheriting all its interpretative complications due to thetypical abundance of cases and sub-cases as well as to the general richness indetail. This paper faces the challenge of the essential interpretation of thelegal language of cybersecurity, namely of the extraction of the essentialParts of Speech (POS) from the legal documents concerning cybersecurity. Thechallenge is overcome by our methodology for POS tagging of legal language. Itleverages state-of-the-art open-source tools for Natural Language Processing(NLP) as well as manual analysis to validate the outcomes of the tools. As aresult, the methodology is automated and, arguably, general for any legallanguage following minor tailoring of the preprocessing step. It isdemonstrated over the most relevant EU legislation on cybersecurity, namely onthe NIS 2 directive, producing the first, albeit essential, structuredinterpretation of such a relevant document. Moreover, our findings indicatethat tools such as SpaCy and ClausIE reach their limits over the legal languageof the NIS 2.</description><author>Gianpietro Castiglione, Giampaolo Bella, Daniele Francesco Santamaria</author><pubDate>Thu, 29 Jun 2023 16:39:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17042v1</guid></item><item><title>Comparison of Single- and Multi- Objective Optimization Quality for Evolutionary Equation Discovery</title><link>http://arxiv.org/abs/2306.17038v1</link><description>Evolutionary differential equation discovery proved to be a tool to obtainequations with less a priori assumptions than conventional approaches, such assparse symbolic regression over the complete possible terms library. Theequation discovery field contains two independent directions. The first one ispurely mathematical and concerns differentiation, the object of optimizationand its relation to the functional spaces and others. The second one isdedicated purely to the optimizational problem statement. Both topics are worthinvestigating to improve the algorithm's ability to handle experimental data amore artificial intelligence way, without significant pre-processing and apriori knowledge of their nature. In the paper, we consider the prevalence ofeither single-objective optimization, which considers only the discrepancybetween selected terms in the equation, or multi-objective optimization, whichadditionally takes into account the complexity of the obtained equation. Theproposed comparison approach is shown on classical model examples -- Burgersequation, wave equation, and Korteweg - de Vries equation.</description><author>Mikhail Maslyaev, Alexander Hvatov</author><pubDate>Thu, 29 Jun 2023 16:37:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17038v1</guid></item><item><title>Exploring &amp; Exploiting High-Order Graph Structure for Sparse Knowledge Graph Completion</title><link>http://arxiv.org/abs/2306.17034v1</link><description>Sparse knowledge graph (KG) scenarios pose a challenge for previous KnowledgeGraph Completion (KGC) methods, that is, the completion performance decreasesrapidly with the increase of graph sparsity. This problem is also exacerbatedbecause of the widespread existence of sparse KGs in practical applications. Toalleviate this challenge, we present a novel framework, LR-GCN, that is able toautomatically capture valuable long-range dependency among entities tosupplement insufficient structure features and distill logical reasoningknowledge for sparse KGC. The proposed approach comprises two main components:a GNN-based predictor and a reasoning path distiller. The reasoning pathdistiller explores high-order graph structures such as reasoning paths andencodes them as rich-semantic edges, explicitly compositing long-rangedependencies into the predictor. This step also plays an essential role indensifying KGs, effectively alleviating the sparse issue. Furthermore, the pathdistiller further distills logical reasoning knowledge from these minedreasoning paths into the predictor. These two components are jointly optimizedusing a well-designed variational EM algorithm. Extensive experiments andanalyses on four sparse benchmarks demonstrate the effectiveness of ourproposed method.</description><author>Tao He, Ming Liu, Yixin Cao, Zekun Wang, Zihao Zheng, Zheng Chu, Bing Qin</author><pubDate>Thu, 29 Jun 2023 16:35:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17034v1</guid></item><item><title>Efficient and Multiply Robust Risk Estimation under General Forms of Dataset Shift</title><link>http://arxiv.org/abs/2306.16406v2</link><description>Statistical machine learning methods often face the challenge of limited dataavailable from the population of interest. One remedy is to leverage data fromauxiliary source populations, which share some conditional distributions or arelinked in other ways with the target domain. Techniques leveraging such\emph{dataset shift} conditions are known as \emph{domain adaptation} or\emph{transfer learning}. Despite extensive literature on dataset shift,limited works address how to efficiently use the auxiliary populations toimprove the accuracy of risk evaluation for a given machine learning task inthe target population. In this paper, we study the general problem of efficiently estimating targetpopulation risk under various dataset shift conditions, leveragingsemiparametric efficiency theory. We consider a general class of dataset shiftconditions, which includes three popular conditions -- covariate, label andconcept shift -- as special cases. We allow for partially non-overlappingsupport between the source and target populations. We develop efficient andmultiply robust estimators along with a straightforward specification test ofthese dataset shift conditions. We also derive efficiency bounds for two otherdataset shift conditions, posterior drift and location-scale shift. Simulationstudies support the efficiency gains due to leveraging plausible dataset shiftconditions.</description><author>Hongxiang Qiu, Eric Tchetgen Tchetgen, Edgar Dobriban</author><pubDate>Thu, 29 Jun 2023 16:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16406v2</guid></item><item><title>Safety-Aware Task Composition for Discrete and Continuous Reinforcement Learning</title><link>http://arxiv.org/abs/2306.17033v1</link><description>Compositionality is a critical aspect of scalable system design.Reinforcement learning (RL) has recently shown substantial success in tasklearning, but has only recently begun to truly leverage composition. In thispaper, we focus on Boolean composition of learned tasks as opposed tofunctional or sequential composition. Existing Boolean composition for RLfocuses on reaching a satisfying absorbing state in environments with discreteaction spaces, but does not support composable safety (i.e., avoidance)constraints. We advance the state of the art in Boolean composition of learnedtasks with three contributions: i) introduce two distinct notions of safety inthis framework; ii) show how to enforce either safety semantics, provecorrectness (under some assumptions), and analyze the trade-offs between thetwo safety notions; and iii) extend Boolean composition from discrete actionspaces to continuous action spaces. We demonstrate these techniques usingmodified versions of value iteration in a grid world, Deep Q-Network (DQN) in agrid world with image observations, and Twin Delayed DDPG (TD3) in acontinuous-observation and continuous-action Bullet physics environment. Webelieve that these contributions advance the theory of safe reinforcementlearning by allowing zero-shot composition of policies satisfying safetyproperties.</description><author>Kevin Leahy, Makai Mann, Zachary Serlin</author><pubDate>Thu, 29 Jun 2023 16:34:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17033v1</guid></item><item><title>Intriguing properties of synthetic images: from generative adversarial networks to diffusion models</title><link>http://arxiv.org/abs/2304.06408v2</link><description>Detecting fake images is becoming a major goal of computer vision. This needis becoming more and more pressing with the continuous improvement of synthesismethods based on Generative Adversarial Networks (GAN), and even more with theappearance of powerful methods based on Diffusion Models (DM). Towards thisend, it is important to gain insight into which image features betterdiscriminate fake images from real ones. In this paper we report on oursystematic study of a large number of image generators of different families,aimed at discovering the most forensically relevant characteristics of real andgenerated images. Our experiments provide a number of interesting observationsand shed light on some intriguing properties of synthetic images: (1) not onlythe GAN models but also the DM and VQ-GAN (Vector Quantized GenerativeAdversarial Networks) models give rise to visible artifacts in the Fourierdomain and exhibit anomalous regular patterns in the autocorrelation; (2) whenthe dataset used to train the model lacks sufficient variety, its biases can betransferred to the generated images; (3) synthetic and real images exhibitsignificant differences in the mid-high frequency signal content, observable intheir radial and angular spectral power distributions.</description><author>Riccardo Corvi, Davide Cozzolino, Giovanni Poggi, Koki Nagano, Luisa Verdoliva</author><pubDate>Thu, 29 Jun 2023 16:33:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06408v2</guid></item><item><title>Medoid splits for efficient random forests in metric spaces</title><link>http://arxiv.org/abs/2306.17031v1</link><description>This paper revisits an adaptation of the random forest algorithm forFr\'echet regression, addressing the challenge of regression in the context ofrandom objects in metric spaces. Recognizing the limitations of previousapproaches, we introduce a new splitting rule that circumvents thecomputationally expensive operation of Fr\'echet means by substituting with amedoid-based approach. We validate this approach by demonstrating itsasymptotic equivalence to Fr\'echet mean-based procedures and establish theconsistency of the associated regression estimator. The paper provides a soundtheoretical framework and a more efficient computational approach to Fr\'echetregression, broadening its application to non-standard data types and complexuse cases.</description><author>Matthieu Bulté, Helle Sørensen</author><pubDate>Thu, 29 Jun 2023 16:32:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17031v1</guid></item><item><title>SkiROS2: A skill-based Robot Control Platform for ROS</title><link>http://arxiv.org/abs/2306.17030v1</link><description>The need for autonomous robot systems in both the service and the industrialdomain is larger than ever. In the latter, the transition to small batches oreven "batch size 1" in production created a need for robot control systemarchitectures that can provide the required flexibility. Such architecturesmust not only have a sufficient knowledge integration framework. It must alsosupport autonomous mission execution and allow for interchangeability andinteroperability between different tasks and robot systems. We introduceSkiROS2, a skill-based robot control platform on top of ROS. SkiROS2 proposes alayered, hybrid control structure for automated task planning, and reactiveexecution, supported by a knowledge base for reasoning about the world stateand entities. The scheduling formulation builds on the extended behavior treemodel that merges task-level planning and execution. This allows for a highdegree of modularity and a fast reaction to changes in the environment. Theskill formulation based on pre-, hold- and post-conditions allows to organizerobot programs and to compose diverse skills reaching from perception tolow-level control and the incorporation of external tools. We relate SkiROS2 tothe field and outline three example use cases that cover task planning,reasoning, multisensory input, integration in a manufacturing execution systemand reinforcement learning.</description><author>Matthias Mayr, Francesco Rovida, Volker Krueger</author><pubDate>Thu, 29 Jun 2023 16:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17030v1</guid></item><item><title>Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks</title><link>http://arxiv.org/abs/2305.09671v2</link><description>Deep image classification models trained on vast amounts of web-scraped dataare susceptible to data poisoning - a mechanism for backdooring models. A smallnumber of poisoned samples seen during training can severely undermine amodel's integrity during inference. Existing work considers an effectivedefense as one that either (i) restores a model's integrity through repair or(ii) detects an attack. We argue that this approach overlooks a crucialtrade-off: Attackers can increase robustness at the expense of detectability(over-poisoning) or decrease detectability at the cost of robustness(under-poisoning). In practice, attacks should remain both undetectable androbust. Detectable but robust attacks draw human attention and rigorous modelevaluation or cause the model to be re-trained or discarded. In contrast,attacks that are undetectable but lack robustness can be repaired with minimalimpact on model accuracy. Our research points to intrinsic flaws in currentattack evaluation methods and raises the bar for all data poisoning attackerswho must delicately balance this trade-off to remain robust and undetectable.To demonstrate the existence of more potent defenders, we propose defensesdesigned to (i) detect or (ii) repair poisoned models using a limited amount oftrusted image-label pairs. Our results show that an attacker who needs to berobust and undetectable is substantially less threatening. Our defensesmitigate all tested attacks with a maximum accuracy decline of 2% using only 1%of clean data on CIFAR-10 and 2.5% on ImageNet. We demonstrate the scalabilityof our defenses by evaluating large vision-language models, such as CLIP.Attackers who can manipulate the model's parameters pose an elevated risk asthey can achieve higher robustness at low detectability compared to datapoisoning attackers.</description><author>Nils Lukas, Florian Kerschbaum</author><pubDate>Thu, 29 Jun 2023 16:23:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09671v2</guid></item><item><title>Classifying Crime Types using Judgment Documents from Social Media</title><link>http://arxiv.org/abs/2306.17020v1</link><description>The task of determining crime types based on criminal behavior facts hasbecome a very important and meaningful task in social science. But the problemfacing the field now is that the data samples themselves are unevenlydistributed, due to the nature of the crime itself. At the same time, data setsin the judicial field are less publicly available, and it is not practical toproduce large data sets for direct training. This article proposes a newtraining model to solve this problem through NLP processing methods. We firstpropose a Crime Fact Data Preprocessing Module (CFDPM), which can balance thedefects of uneven data set distribution by generating new samples. Then we usea large open source dataset (CAIL-big) as our pretraining dataset and a smalldataset collected by ourselves for Fine-tuning, giving it good generalizationability to unfamiliar small datasets. At the same time, we use the improvedBert model with dynamic masking to improve the model. Experiments show that theproposed method achieves state-of-the-art results on the present dataset. Atthe same time, the effectiveness of module CFDPM is proved by experiments. Thisarticle provides a valuable methodology contribution for classifying socialscience texts such as criminal behaviors. Extensive experiments on publicbenchmarks show that the proposed method achieves new state-of-the-art results.</description><author>Haoxuan Xu, Zeyu He, Mengfan Shen, Songning Lai, Ziqiang Han, Yifan Peng</author><pubDate>Thu, 29 Jun 2023 16:12:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17020v1</guid></item><item><title>Histopathology Slide Indexing and Search: Are We There Yet?</title><link>http://arxiv.org/abs/2306.17019v1</link><description>The search and retrieval of digital histopathology slides is an importanttask that has yet to be solved. In this case study, we investigate the clinicalreadiness of three state-of-the-art histopathology slide search engines,Yottixel, SISH, and RetCCL, on three patients with solid tumors. We provide aqualitative assessment of each model's performance in providing retrievalresults that are reliable and useful to pathologists. We found that all threeimage search engines fail to produce consistently reliable results and havedifficulties in capturing granular and subtle features of malignancy, limitingtheir diagnostic accuracy. Based on our findings, we also propose a minimal setof requirements to further advance the development of accurate and reliablehistopathology image search engines for successful clinical adoption.</description><author>Helen H. Shang, Mohammad Sadegh Nasr, Jai Prakash Veerla, Parisa Boodaghi Malidarreh, MD Jillur Rahman Saurav, Amir Hajighasemi, Manfred Huber, Chace Moleta, Jitin Makker, Jacob M. Luber</author><pubDate>Thu, 29 Jun 2023 16:11:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17019v1</guid></item><item><title>On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation</title><link>http://arxiv.org/abs/2205.16001v4</link><description>A good automatic evaluation metric for language generation ideally correlateshighly with human judgements of text quality. Yet, there is a dearth of suchmetrics, which inhibits the rapid and efficient progress of languagegenerators. One exception is the recently proposed Mauve. In theory, Mauvemeasures an information-theoretic divergence between two probabilitydistributions over strings: one representing the language generator underevaluation; the other representing the true natural language distribution.Mauve's authors argue that its success comes from the qualitative properties oftheir proposed divergence. Yet in practice, as this divergence is uncomputable,Mauve approximates it by measuring the divergence between multinomialdistributions over clusters instead, where cluster assignments are attained bygrouping strings based on a pre-trained language model's embeddings. As weshow, however, this is not a tight approximation -- in either theory orpractice. This begs the question: why does Mauve work so well? In this work, weshow that Mauve was right for the wrong reasons, and that its newly proposeddivergence is not necessary for its high performance. In fact, classicaldivergences paired with its proposed cluster-based approximation may actuallyserve as better evaluation metrics. We finish the paper with a probinganalysis; this analysis leads us to conclude that -- by encoding syntactic- andcoherence-level features of text, while ignoring surface-level features -- suchcluster-based substitutes to string distributions may simply be better forevaluating state-of-the-art language generators.</description><author>Tiago Pimentel, Clara Meister, Ryan Cotterell</author><pubDate>Thu, 29 Jun 2023 16:08:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.16001v4</guid></item><item><title>milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing</title><link>http://arxiv.org/abs/2306.17010v1</link><description>Approaching the era of ubiquitous computing, human motion sensing plays acrucial role in smart systems for decision making, user interaction, andpersonalized services. Extensive research has been conducted on human tracking,pose estimation, gesture recognition, and activity recognition, which arepredominantly based on cameras in traditional methods. However, the intrusivenature of cameras limits their use in smart home applications. To address this,mmWave radars have gained popularity due to their privacy-friendly features. Inthis work, we propose \textit{milliFlow}, a novel deep learning method forscene flow estimation as a complementary motion information for mmWave pointcloud, serving as an intermediate level of features and directly benefitingdownstream human motion sensing tasks. Experimental results demonstrate thesuperior performance of our method with an average 3D endpoint error of 4.6cm,significantly surpassing the competing approaches. Furthermore, byincorporating scene flow information, we achieve remarkable improvements inhuman activity recognition, human parsing, and human body part tracking. Tofoster further research in this area, we provide our codebase and dataset foropen access.</description><author>Fangqiang Ding, Zhen Luo, Peijun Zhao, Chris Xiaoxuan Lu</author><pubDate>Thu, 29 Jun 2023 16:06:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17010v1</guid></item><item><title>MLA-BIN: Model-level Attention and Batch-instance Style Normalization for Domain Generalization of Federated Learning on Medical Image Segmentation</title><link>http://arxiv.org/abs/2306.17008v1</link><description>The privacy protection mechanism of federated learning (FL) offers aneffective solution for cross-center medical collaboration and data sharing. Inmulti-site medical image segmentation, each medical site serves as a client ofFL, and its data naturally forms a domain. FL supplies the possibility toimprove the performance of seen domains model. However, there is a problem ofdomain generalization (DG) in the actual de-ployment, that is, the performanceof the model trained by FL in unseen domains will decrease. Hence, MLA-BIN isproposed to solve the DG of FL in this study. Specifically, the model-levelattention module (MLA) and batch-instance style normalization (BIN) block weredesigned. The MLA represents the unseen domain as a linear combination of seendomain models. The atten-tion mechanism is introduced for the weightingcoefficient to obtain the optimal coefficient ac-cording to the similarity ofinter-domain data features. MLA enables the global model to gen-eralize tounseen domain. In the BIN block, batch normalization (BN) and instancenormalization (IN) are combined to perform the shallow layers of thesegmentation network for style normali-zation, solving the influence ofinter-domain image style differences on DG. The extensive experimental resultsof two medical image seg-mentation tasks demonstrate that the proposed MLA-BINoutperforms state-of-the-art methods.</description><author>Fubao Zhu, Yanhui Tian, Chuang Han, Yanting Li, Jiaofen Nan, Ni Yao, Weihua Zhou</author><pubDate>Thu, 29 Jun 2023 16:04:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17008v1</guid></item><item><title>High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units</title><link>http://arxiv.org/abs/2306.17005v1</link><description>The goal of Automatic Voice Over (AVO) is to generate speech in sync with asilent video given its text script. Recent AVO frameworks built upontext-to-speech synthesis (TTS) have shown impressive results. However, thecurrent AVO learning objective of acoustic feature reconstruction brings inindirect supervision for inter-modal alignment learning, thus limiting thesynchronization performance and synthetic speech quality. To this end, wepropose a novel AVO method leveraging the learning objective of self-superviseddiscrete speech unit prediction, which not only provides more directsupervision for the alignment learning, but also alleviates the mismatchbetween the text-video context and acoustic features. Experimental results showthat our proposed method achieves remarkable lip-speech synchronization andhigh speech quality by outperforming baselines in both objective and subjectiveevaluations. Code and speech samples are publicly available.</description><author>Junchen Lu, Berrak Sisman, Mingyang Zhang, Haizhou Li</author><pubDate>Thu, 29 Jun 2023 16:02:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17005v1</guid></item><item><title>Learning thermodynamically constrained equations of state with uncertainty</title><link>http://arxiv.org/abs/2306.17004v1</link><description>Numerical simulations of high energy-density experiments require equation ofstate (EOS) models that relate a material's thermodynamic state variables --specifically pressure, volume/density, energy, and temperature. EOS models aretypically constructed using a semi-empirical parametric methodology, whichassumes a physics-informed functional form with many tunable parameterscalibrated using experimental/simulation data. Since there are inherentuncertainties in the calibration data (parametric uncertainty) and the assumedfunctional EOS form (model uncertainty), it is essential to perform uncertaintyquantification (UQ) to improve confidence in the EOS predictions. Modeluncertainty is challenging for UQ studies since it requires exploring the spaceof all possible physically consistent functional forms. Thus, it is oftenneglected in favor of parametric uncertainty, which is easier to quantifywithout violating thermodynamic laws. This work presents a data-driven machinelearning approach to constructing EOS models that naturally captures modeluncertainty while satisfying the necessary thermodynamic consistency andstability constraints. We propose a novel framework based on physics-informedGaussian process regression (GPR) that automatically captures total uncertaintyin the EOS and can be jointly trained on both simulation and experimental datasources. A GPR model for the shock Hugoniot is derived and its uncertaintiesare quantified using the proposed framework. We apply the proposed model tolearn the EOS for the diamond solid state of carbon, using both densityfunctional theory data and experimental shock Hugoniot data to train the modeland show that the prediction uncertainty reduces by considering thethermodynamic constraints.</description><author>Himanshu Sharma, Jim A. Gaffney, Dimitrios Tsapetis, Michael D. Shields</author><pubDate>Thu, 29 Jun 2023 16:02:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17004v1</guid></item><item><title>Motion Informed Object Detection of Small Insects in Time-lapse Camera Recordings</title><link>http://arxiv.org/abs/2212.00423v2</link><description>Insects as pollinators play a crucial role in ecosystem management and worldfood production. However, insect populations are declining, calling forefficient methods of insect monitoring. Existing methods analyze video ortime-lapse images of insects in nature, but the analysis is challenging sinceinsects are small objects in complex and dynamic scenes of natural vegetation.In this work, we provide a dataset of primary honeybees visiting threedifferent plant species during two months of the summer period. The datasetconsists of 107,387 annotated time-lapse images from multiple cameras,including 9,423 annotated insects. We present a method pipeline for detectinginsects in time-lapse RGB images. The pipeline consists of a two-step process.Firstly, the time-lapse RGB images are preprocessed to enhance insects in theimages. This Motion-Informed-Enhancement technique uses motion and colors toenhance insects in images. Secondly, the enhanced images are subsequently fedinto a Convolutional Neural network (CNN) object detector. The method improvesthe deep learning object detectors You Only Look Once (YOLO) and FasterRegion-based CNN (Faster R-CNN). Using Motion-Informed-Enhancement, theYOLO-detector improves the average micro F1-score from 0.49 to 0.71, and theFaster R-CNN-detector improves the average micro F1-score from 0.32 to 0.56 onthe dataset. Our dataset and proposed method provide a step forward to automatethe time-lapse camera monitoring of flying insects. The dataset is publishedon: https://vision.eng.au.dk/mie/</description><author>Kim Bjerge, Carsten Eie Frigaard, Henrik Karstoft</author><pubDate>Thu, 29 Jun 2023 16:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00423v2</guid></item><item><title>MotionTrack: End-to-End Transformer-based Multi-Object Tracing with LiDAR-Camera Fusion</title><link>http://arxiv.org/abs/2306.17000v1</link><description>Multiple Object Tracking (MOT) is crucial to autonomous vehicle perception.End-to-end transformer-based algorithms, which detect and track objectssimultaneously, show great potential for the MOT task. However, most existingmethods focus on image-based tracking with a single object category. In thispaper, we propose an end-to-end transformer-based MOT algorithm (MotionTrack)with multi-modality sensor inputs to track objects with multiple classes. Ourobjective is to establish a transformer baseline for the MOT in an autonomousdriving environment. The proposed algorithm consists of a transformer-baseddata association (DA) module and a transformer-based query enhancement moduleto achieve MOT and Multiple Object Detection (MOD) simultaneously. TheMotionTrack and its variations achieve better results (AMOTA score at 0.55) onthe nuScenes dataset compared with other classical baseline models, such as theAB3DMOT, the CenterTrack, and the probabilistic 3D Kalman filter. In addition,we prove that a modified attention mechanism can be utilized for DA toaccomplish the MOT, and aggregate history features to enhance the MODperformance.</description><author>Ce Zhang, Chengjie Zhang, Yiluan Guo, Lingji Chen, Michael Happold</author><pubDate>Thu, 29 Jun 2023 16:00:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17000v1</guid></item><item><title>Spectral Batch Normalization: Normalization in the Frequency Domain</title><link>http://arxiv.org/abs/2306.16999v1</link><description>Regularization is a set of techniques that are used to improve thegeneralization ability of deep neural networks. In this paper, we introducespectral batch normalization (SBN), a novel effective method to improvegeneralization by normalizing feature maps in the frequency (spectral) domain.The activations of residual networks without batch normalization (BN) tend toexplode exponentially in the depth of the network at initialization. This leadsto extremely large feature map norms even though the parameters are relativelysmall. These explosive dynamics can be very detrimental to learning. BN makesweight decay regularization on the scaling factors $\gamma, \beta$approximately equivalent to an additive penalty on the norm of the featuremaps, which prevents extremely large feature map norms to a certain degree.However, we show experimentally that, despite the approximate additive penaltyof BN, feature maps in deep neural networks (DNNs) tend to explode at thebeginning of the network and that feature maps of DNNs contain large valuesduring the whole training. This phenomenon also occurs in a weakened form innon-residual networks. SBN addresses large feature maps by normalizing them inthe frequency domain. In our experiments, we empirically show that SBN preventsexploding feature maps at initialization and large feature map values duringthe training. Moreover, the normalization of feature maps in the frequencydomain leads to more uniform distributed frequency components. This discouragesthe DNNs to rely on single frequency components of feature maps. These,together with other effects of SBN, have a regularizing effect on the trainingof residual and non-residual networks. We show experimentally that using SBN inaddition to standard regularization methods improves the performance of DNNs bya relevant margin, e.g. ResNet50 on ImageNet by 0.71%.</description><author>Rinor Cakaj, Jens Mehnert, Bin Yang</author><pubDate>Thu, 29 Jun 2023 15:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16999v1</guid></item><item><title>Computing Star Discrepancies with Numerical Black-Box Optimization Algorithms</title><link>http://arxiv.org/abs/2306.16998v1</link><description>The $L_{\infty}$ star discrepancy is a measure for the regularity of a finiteset of points taken from $[0,1)^d$. Low discrepancy point sets are highlyrelevant for Quasi-Monte Carlo methods in numerical integration and severalother applications. Unfortunately, computing the $L_{\infty}$ star discrepancyof a given point set is known to be a hard problem, with the best exactalgorithms falling short for even moderate dimensions around 8. However,despite the difficulty of finding the global maximum that defines the$L_{\infty}$ star discrepancy of the set, local evaluations at selected pointsare inexpensive. This makes the problem tractable by black-box optimizationapproaches. In this work we compare 8 popular numerical black-box optimization algorithmson the $L_{\infty}$ star discrepancy computation problem, using a wide set ofinstances in dimensions 2 to 15. We show that all used optimizers perform verybadly on a large majority of the instances and that in many cases random searchoutperforms even the more sophisticated solvers. We suspect thatstate-of-the-art numerical black-box optimization techniques fail to capturethe global structure of the problem, an important shortcoming that may guidetheir future development. We also provide a parallel implementation of the best-known algorithm tocompute the discrepancy.</description><author>François Clément, Diederick Vermetten, Jacob de Nobel, Alexandre D. Jesus, Luís Paquete, Carola Doerr</author><pubDate>Thu, 29 Jun 2023 15:57:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16998v1</guid></item><item><title>Unsupervised 3D registration through optimization-guided cyclical self-training</title><link>http://arxiv.org/abs/2306.16997v1</link><description>State-of-the-art deep learning-based registration methods employ threedifferent learning strategies: supervised learning, which requires costlymanual annotations, unsupervised learning, which heavily relies on hand-craftedsimilarity metrics designed by domain experts, or learning from synthetic data,which introduces a domain shift. To overcome the limitations of thesestrategies, we propose a novel self-supervised learning paradigm forunsupervised registration, relying on self-training. Our idea is based on twokey insights. Feature-based differentiable optimizers 1) perform reasonableregistration even from random features and 2) stabilize the training of thepreceding feature extraction network on noisy labels. Consequently, we proposecyclical self-training, where pseudo labels are initialized as the displacementfields inferred from random features and cyclically updated based on more andmore expressive features from the learning feature extractor, yielding aself-reinforcement effect. We evaluate the method for abdomen and lungregistration, consistently surpassing metric-based supervision andoutperforming diverse state-of-the-art competitors. Source code is available athttps://github.com/multimodallearning/reg-cyclical-self-train.</description><author>Alexander Bigalke, Lasse Hansen, Tony C. W. Mok, Mattias P. Heinrich</author><pubDate>Thu, 29 Jun 2023 15:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16997v1</guid></item><item><title>Weight Compander: A Simple Weight Reparameterization for Regularization</title><link>http://arxiv.org/abs/2306.16993v1</link><description>Regularization is a set of techniques that are used to improve thegeneralization ability of deep neural networks. In this paper, we introduceweight compander (WC), a novel effective method to improve generalization byreparameterizing each weight in deep neural networks using a nonlinearfunction. It is a general, intuitive, cheap and easy to implement method, whichcan be combined with various other regularization techniques. Large weights indeep neural networks are a sign of a more complex network that is overfitted tothe training data. Moreover, regularized networks tend to have a greater rangeof weights around zero with fewer weights centered at zero. We introduce aweight reparameterization function which is applied to each weight andimplicitly reduces overfitting by restricting the magnitude of the weightswhile forcing them away from zero at the same time. This leads to a moredemocratic decision-making in the network. Firstly, individual weights cannothave too much influence in the prediction process due to the restriction oftheir magnitude. Secondly, more weights are used in the prediction process,since they are forced away from zero during the training. This promotes theextraction of more features from the input data and increases the level ofweight redundancy, which makes the network less sensitive to statisticaldifferences between training and test data. We extend our method to learn thehyperparameters of the introduced weight reparameterization function. Thisavoids hyperparameter search and gives the network the opportunity to align theweight reparameterization with the training progress. We show experimentallythat using weight compander in addition to standard regularization methodsimproves the performance of neural networks.</description><author>Rinor Cakaj, Jens Mehnert, Bin Yang</author><pubDate>Thu, 29 Jun 2023 15:52:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16993v1</guid></item><item><title>Integrating Large Pre-trained Models into Multimodal Named Entity Recognition with Evidential Fusion</title><link>http://arxiv.org/abs/2306.16991v1</link><description>Multimodal Named Entity Recognition (MNER) is a crucial task for informationextraction from social media platforms such as Twitter. Most current methodsrely on attention weights to extract information from both text and images butare often unreliable and lack interpretability. To address this problem, wepropose incorporating uncertainty estimation into the MNER task, producingtrustworthy predictions. Our proposed algorithm models the distribution of eachmodality as a Normal-inverse Gamma distribution, and fuses them into a unifieddistribution with an evidential fusion mechanism, enabling hierarchicalcharacterization of uncertainties and promotion of prediction accuracy andtrustworthiness. Additionally, we explore the potential of pre-trained largefoundation models in MNER and propose an efficient fusion approach thatleverages their robust feature representations. Experiments on two datasetsdemonstrate that our proposed method outperforms the baselines and achieves newstate-of-the-art performance.</description><author>Weide Liu, Xiaoyang Zhong, Jingwen Hou, Shaohua Li, Haozhe Huang, Yuming Fang</author><pubDate>Thu, 29 Jun 2023 15:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16991v1</guid></item><item><title>Improved Privacy-Preserving PCA Using Space-optimized Homomorphic Matrix Multiplication</title><link>http://arxiv.org/abs/2305.17341v3</link><description>Principal Component Analysis (PCA) is a pivotal technique widely utilized inthe realms of machine learning and data analysis. It aims to reduce thedimensionality of a dataset while minimizing the loss of information. In recentyears, there have been endeavors to utilize homomorphic encryption inprivacy-preserving PCA algorithms for the secure cloud computing scenario.These approaches commonly employ a PCA routine known as PowerMethod, whichtakes the covariance matrix as input and generates an approximate eigenvectorcorresponding to the primary component of the dataset. However, theirperformance is constrained by the absence of an efficient homomorphiccovariance matrix computation circuit and an accurate homomorphic vectornormalization strategy in the PowerMethod algorithm. In this study, we proposea novel approach to privacy-preserving PCA that addresses these limitations,resulting in superior efficiency, accuracy, and scalability compared toprevious approaches</description><author>Xirong Ma</author><pubDate>Thu, 29 Jun 2023 15:47:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17341v3</guid></item><item><title>The State of Applying Artificial Intelligence to Tissue Imaging for Cancer Research and Early Detection</title><link>http://arxiv.org/abs/2306.16989v1</link><description>Artificial intelligence represents a new frontier in human medicine thatcould save more lives and reduce the costs, thereby increasing accessibility.As a consequence, the rate of advancement of AI in cancer medical imaging andmore particularly tissue pathology has exploded, opening it to ethical andtechnical questions that could impede its adoption into existing systems. Inorder to chart the path of AI in its application to cancer tissue imaging, wereview current work and identify how it can improve cancer pathologydiagnostics and research. In this review, we identify 5 core tasks that modelsare developed for, including regression, classification, segmentation,generation, and compression tasks. We address the benefits and challenges thatsuch methods face, and how they can be adapted for use in cancer prevention andtreatment. The studies looked at in this paper represent the beginning of thisfield and future experiments will build on the foundations that we highlight.</description><author>Michael Robben, Amir Hajighasemi, Mohammad Sadegh Nasr, Jai Prakesh Veerla, Anne M. Alsup, Biraaj Rout, Helen H. Shang, Kelli Fowlds, Parisa Boodaghi Malidarreh, Paul Koomey, MD Jillur Rahman Saurav, Jacob M. Luber</author><pubDate>Thu, 29 Jun 2023 15:47:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16989v1</guid></item><item><title>Secure and Fast Asynchronous Vertical Federated Learning via Cascaded Hybrid Optimization</title><link>http://arxiv.org/abs/2306.16077v2</link><description>Vertical Federated Learning (VFL) attracts increasing attention because itempowers multiple parties to jointly train a privacy-preserving model oververtically partitioned data. Recent research has shown that applyingzeroth-order optimization (ZOO) has many advantages in building a practical VFLalgorithm. However, a vital problem with the ZOO-based VFL is its slowconvergence rate, which limits its application in handling modern large models.To address this problem, we propose a cascaded hybrid optimization method inVFL. In this method, the downstream models (clients) are trained with ZOO toprotect privacy and ensure that no internal information is shared. Meanwhile,the upstream model (server) is updated with first-order optimization (FOO)locally, which significantly improves the convergence rate, making it feasibleto train the large models without compromising privacy and security. Wetheoretically prove that our VFL framework converges faster than the ZOO-basedVFL, as the convergence of our framework is not limited by the size of theserver model, making it effective for training large models with the major parton the server. Extensive experiments demonstrate that our method achievesfaster convergence than the ZOO-based VFL framework, while maintaining anequivalent level of privacy protection. Moreover, we show that the convergenceof our VFL is comparable to the unsafe FOO-based VFL baseline. Additionally, wedemonstrate that our method makes the training of a large model feasible.</description><author>Ganyu Wang, Qingsong Zhang, Li Xiang, Boyu Wang, Bin Gu, Charles Ling</author><pubDate>Thu, 29 Jun 2023 15:42:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16077v2</guid></item><item><title>Mathematical Foundations for a Compositional Account of the Bayesian Brain</title><link>http://arxiv.org/abs/2212.12538v2</link><description>This dissertation reports some first steps towards a compositional account ofactive inference and the Bayesian brain. Specifically, we use the tools ofcontemporary applied category theory to supply functorial semantics forapproximate inference. To do so, we define on the `syntactic' side the newnotion of Bayesian lens and show that Bayesian updating composes according tothe compositional lens pattern. Using Bayesian lenses, and inspired bycompositional game theory, we define fibrations of statistical games andclassify various problems of statistical inference as corresponding sections:the chain rule of the relative entropy is formalized as a strict section, whilemaximum likelihood estimation and the free energy give lax sections. In theprocess, we introduce a new notion of `copy-composition'. On the `semantic' side, we present a new formalization of general opendynamical systems (particularly: deterministic, stochastic, and random; anddiscrete- and continuous-time) as certain coalgebras of polynomial functors,which we show collect into monoidal opindexed categories (or, alternatively,into algebras for multicategories of generalized polynomial functors). We usethese opindexed categories to define monoidal bicategories of cilia: dynamicalsystems which control lenses, and which supply the target for our functorialsemantics. Accordingly, we construct functors which explain the bidirectionalcompositional structure of predictive coding neural circuits under the freeenergy principle, thereby giving a formal mathematical underpinning to thebidirectionality observed in the cortex. Along the way, we explain how tocompose rate-coded neural circuits using an algebra for a multicategory oflinear circuit diagrams, showing subsequently that this is subsumed by lensesand polynomial functors.</description><author>Toby St Clere Smithe</author><pubDate>Thu, 29 Jun 2023 15:34:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12538v2</guid></item><item><title>Defending Black-box Classifiers by Bayesian Boundary Correction</title><link>http://arxiv.org/abs/2306.16979v1</link><description>Classifiers based on deep neural networks have been recently challenged byAdversarial Attack, where the widely existing vulnerability has invoked theresearch in defending them from potential threats. Given a vulnerableclassifier, existing defense methods are mostly white-box and often requirere-training the victim under modified loss functions/training regimes. Whilethe model/data/training specifics of the victim are usually unavailable to theuser, re-training is unappealing, if not impossible for reasons such as limitedcomputational resources. To this end, we propose a new black-box defenseframework. It can turn any pre-trained classifier into a resilient one withlittle knowledge of the model specifics. This is achieved by new joint Bayesiantreatments on the clean data, the adversarial examples and the classifier, formaximizing their joint probability. It is further equipped with a newpost-train strategy which keeps the victim intact. We name our frameworkBayesian Boundary Correction (BBC). BBC is a general and flexible frameworkthat can easily adapt to different data types. We instantiate BBC for imageclassification and skeleton-based human activity recognition, for both staticand dynamic data. Exhaustive evaluation shows that BBC has superior robustnessand can enhance robustness without severely hurting the clean accuracy,compared with existing defense methods.</description><author>He Wang, Yunfeng Diao</author><pubDate>Thu, 29 Jun 2023 15:33:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16979v1</guid></item><item><title>End-to-end Reinforcement Learning for Online Coverage Path Planning in Unknown Environments</title><link>http://arxiv.org/abs/2306.16978v1</link><description>Coverage path planning is the problem of finding the shortest path thatcovers the entire free space of a given confined area, with applicationsranging from robotic lawn mowing and vacuum cleaning, to demining andsearch-and-rescue tasks. While offline methods can find provably complete, andin some cases optimal, paths for known environments, their value is limited inonline scenarios where the environment is not known beforehand, especially inthe presence of non-static obstacles. We propose an end-to-end reinforcementlearning-based approach in continuous state and action space, for the onlinecoverage path planning problem that can handle unknown environments. Weconstruct the observation space from both global maps and local sensory inputs,allowing the agent to plan a long-term path, and simultaneously act onshort-term obstacle detections. To account for large-scale environments, wepropose to use a multi-scale map input representation. Furthermore, we proposea novel total variation reward term for eliminating thin strips of uncoveredspace in the learned path. To validate the effectiveness of our approach, weperform extensive experiments in simulation with a distance sensor, surpassingthe performance of a recent reinforcement learning-based approach.</description><author>Arvi Jonnarth, Jie Zhao, Michael Felsberg</author><pubDate>Thu, 29 Jun 2023 15:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16978v1</guid></item><item><title>Diffusion-Jump GNNs: Homophiliation via Learnable Metric Filters</title><link>http://arxiv.org/abs/2306.16976v1</link><description>High-order Graph Neural Networks (HO-GNNs) have been developed to inferconsistent latent spaces in the heterophilic regime, where the labeldistribution is not correlated with the graph structure. However, most of theexisting HO-GNNs are hop-based, i.e., they rely on the powers of the transitionmatrix. As a result, these architectures are not fully reactive to theclassification loss and the achieved structural filters have static supports.In other words, neither the filters' supports nor their coefficients can belearned with these networks. They are confined, instead, to learn combinationsof filters. To address the above concerns, we propose Diffusion-jump GNNs amethod relying on asymptotic diffusion distances that operates on jumps. Adiffusion-pump generates pairwise distances whose projections determine boththe support and coefficients of each structural filter. These filters arecalled jumps because they explore a wide range of scales in order to find bondsbetween scattered nodes with the same label. Actually, the full process iscontrolled by the classification loss. Both the jumps and the diffusiondistances react to classification errors (i.e. they are learnable).Homophiliation, i.e., the process of learning piecewise smooth latent spaces inthe heterophilic regime, is formulated as a Dirichlet problem: the known labelsdetermine the border nodes and the diffusion-pump ensures a minimal deviationof the semi-supervised grouping from a canonical unsupervised grouping. Thistriggers the update of both the diffusion distances and, consequently, thejumps in order to minimize the classification error. The Dirichlet formulationhas several advantages. It leads to the definition of structural heterophily, anovel measure beyond edge heterophily. It also allows us to investigate linkswith (learnable) diffusion distances, absorbing random walks and stochasticdiffusion.</description><author>Ahmed Begga, Francisco Escolano, Miguel Angel Lozano, Edwin R. Hancock</author><pubDate>Thu, 29 Jun 2023 15:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16976v1</guid></item><item><title>End-to-end Autonomous Driving: Challenges and Frontiers</title><link>http://arxiv.org/abs/2306.16927v1</link><description>The autonomous driving community has witnessed a rapid growth in approachesthat embrace an end-to-end algorithm framework, utilizing raw sensor input togenerate vehicle motion plans, instead of concentrating on individual taskssuch as detection and motion prediction. End-to-end systems, in comparison tomodular pipelines, benefit from joint feature optimization for perception andplanning. This field has flourished due to the availability of large-scaledatasets, closed-loop evaluation, and the increasing need for autonomousdriving algorithms to perform effectively in challenging scenarios. In thissurvey, we provide a comprehensive analysis of more than 250 papers, coveringthe motivation, roadmap, methodology, challenges, and future trends inend-to-end autonomous driving. We delve into several critical challenges,including multi-modality, interpretability, causal confusion, robustness, andworld models, amongst others. Additionally, we discuss current advancements infoundation models and visual pre-training, as well as how to incorporate thesetechniques within the end-to-end driving framework. To facilitate futureresearch, we maintain an active repository that contains up-to-date links torelevant literature and open-source projects athttps://github.com/OpenDriveLab/End-to-end-Autonomous-Driving.</description><author>Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, Hongyang Li</author><pubDate>Thu, 29 Jun 2023 15:17:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16927v1</guid></item><item><title>Identifiability of direct effects from summary causal graphs</title><link>http://arxiv.org/abs/2306.16958v1</link><description>Dynamic structural causal models (SCMs) are a powerful framework forreasoning in dynamic systems about direct effects which measure how a change inone variable affects another variable while holding all other variablesconstant. The causal relations in a dynamic structural causal model can bequalitatively represented with a full-time causal graph. Assuming linearity andcausal sufficiency and given the full-time causal graph, the direct causaleffect is always identifiable and can be estimated from data by adjusting onany set of variables given by the so-called single-door criterion. However, inmany application such a graph is not available for various reasons butnevertheless experts have access to an abstraction of the full-time causalgraph which represents causal relations between time series while omittingtemporal information. This paper presents a complete identifiability resultwhich characterizes all cases for which the direct effect is graphicallyidentifiable from summary causal graphs and gives two sound finite adjustmentsets that can be used to estimate the direct effect whenever it isidentifiable.</description><author>Simon Ferreira, Charles K. Assaad</author><pubDate>Thu, 29 Jun 2023 15:05:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16958v1</guid></item><item><title>Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models</title><link>http://arxiv.org/abs/2305.02279v3</link><description>During the continuous evolution of one organism's ancestry, its genesaccumulate extensive experiences and knowledge, enabling newborn descendants torapidly adapt to their specific environments. Motivated by this observation, wepropose a novel machine learning paradigm Learngene to enable learning modelsto incorporate three key characteristics of genes. (i) Accumulating: theknowledge is accumulated during the continuous learning of an ancestry model.(ii) Condensing: the extensive accumulated knowledge is condensed into a muchmore compact information piece, i.e., learngene. (iii) Inheriting: thecondensed learngene is inherited to make it easier for descendant models toadapt to new environments. Since accumulating has been studied inwell-established paradigms like large-scale pre-training and lifelong learning,we focus on condensing and inheriting, which induces three key issues and weprovide the preliminary solutions to these issues in this paper: (i) LearngeneForm: the learngene is set to a few integral layers that can preservesignificance. (ii) Learngene Condensing: we identify which layers among theancestry model have the most similarity as one pseudo descendant model. (iii)Learngene Inheriting: to construct distinct descendant models for the specificdownstream tasks, we stack some randomly initialized layers to the learngenelayers. Extensive experiments across various settings, including usingdifferent network architectures like Vision Transformer (ViT) and ConvolutionalNeural Networks (CNNs) on different datasets, are carried out to confirm fouradvantages of Learngene: it makes the descendant models 1) converge morequickly, 2) exhibit less sensitivity to hyperparameters, 3) perform better, and4) require fewer training samples to converge.</description><author>Qiufeng Wang, Xu Yang, Shuxia Lin, Jing Wang, Xin Geng</author><pubDate>Thu, 29 Jun 2023 15:04:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02279v3</guid></item><item><title>Cross-Inferential Networks for Source-free Unsupervised Domain Adaptation</title><link>http://arxiv.org/abs/2306.16957v1</link><description>One central challenge in source-free unsupervised domain adaptation (UDA) isthe lack of an effective approach to evaluate the prediction results of theadapted network model in the target domain. To address this challenge, wepropose to explore a new method called cross-inferential networks (CIN). Ourmain idea is that, when we adapt the network model to predict the sample labelsfrom encoded features, we use these prediction results to construct newtraining samples with derived labels to learn a new examiner network thatperforms a different but compatible task in the target domain. Specifically, inthis work, the base network model is performing image classification while theexaminer network is tasked to perform relative ordering of triplets of sampleswhose training labels are carefully constructed from the prediction results ofthe base network model. Two similarity measures, cross-network correlationmatrix similarity and attention consistency, are then developed to provideimportant guidance for the UDA process. Our experimental results on benchmarkdatasets demonstrate that our proposed CIN approach can significantly improvethe performance of source-free UDA.</description><author>Yushun Tang, Qinghai Guo, Zhihai He</author><pubDate>Thu, 29 Jun 2023 15:04:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16957v1</guid></item><item><title>MEMD-ABSA: A Multi-Element Multi-Domain Dataset for Aspect-Based Sentiment Analysis</title><link>http://arxiv.org/abs/2306.16956v1</link><description>Aspect-based sentiment analysis is a long-standing research interest in thefield of opinion mining, and in recent years, researchers have graduallyshifted their focus from simple ABSA subtasks to end-to-end multi-element ABSAtasks. However, the datasets currently used in the research are limited toindividual elements of specific tasks, usually focusing on in-domain settings,ignoring implicit aspects and opinions, and with a small data scale. To addressthese issues, we propose a large-scale Multi-Element Multi-Domain dataset(MEMD) that covers the four elements across five domains, including nearly20,000 review sentences and 30,000 quadruples annotated with explicit andimplicit aspects and opinions for ABSA research. Meanwhile, we evaluategenerative and non-generative baselines on multiple ABSA subtasks under theopen domain setting, and the results show that open domain ABSA as well asmining implicit aspects and opinions remain ongoing challenges to be addressed.The datasets are publicly released at \url{https://github.com/NUSTM/MEMD-ABSA}.</description><author>Hongjie Cai, Nan Song, Zengzhi Wang, Qiming Xie, Qiankun Zhao, Ke Li, Siwei Wu, Shijie Liu, Jianfei Yu, Rui Xia</author><pubDate>Thu, 29 Jun 2023 15:03:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16956v1</guid></item><item><title>Predicting Music Hierarchies with a Graph-Based Neural Decoder</title><link>http://arxiv.org/abs/2306.16955v1</link><description>This paper describes a data-driven framework to parse musical sequences intodependency trees, which are hierarchical structures used in music cognitionresearch and music analysis. The parsing involves two steps. First, the inputsequence is passed through a transformer encoder to enrich it with contextualinformation. Then, a classifier filters the graph of all possible dependencyarcs to produce the dependency tree. One major benefit of this system is thatit can be easily integrated into modern deep-learning pipelines. Moreover,since it does not rely on any particular symbolic grammar, it can considermultiple musical features simultaneously, make use of sequential contextinformation, and produce partial results for noisy inputs. We test our approachon two datasets of musical trees -- time-span trees of monophonic notesequences and harmonic trees of jazz chord sequences -- and show that ourapproach outperforms previous methods.</description><author>Francesco Foscarin, Daniel Harasim, Gerhard Widmer</author><pubDate>Thu, 29 Jun 2023 14:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16955v1</guid></item><item><title>The expected sum of edge lengths in planar linearizations of trees. Theory and applications</title><link>http://arxiv.org/abs/2207.05564v3</link><description>Dependency trees have proven to be a very successful model to represent thesyntactic structure of sentences of human languages. In these structures,vertices are words and edges connect syntactically-dependent words. Thetendency of these dependencies to be short has been demonstrated using randombaselines for the sum of the lengths of the edges or its variants. A ubiquitousbaseline is the expected sum in projective orderings (wherein edges do notcross and the root word of the sentence is not covered by any edge), that canbe computed in time $O(n)$. Here we focus on a weaker formal constraint, namelyplanarity. In the theoretical domain, we present a characterization ofplanarity that, given a sentence, yields either the number of planarpermutations or an efficient algorithm to generate uniformly random planarpermutations of the words. We also show the relationship between the expectedsum in planar arrangements and the expected sum in projective arrangements. Inthe domain of applications, we derive a $O(n)$-time algorithm to calculate theexpected value of the sum of edge lengths. We also apply this research to aparallel corpus and find that the gap between actual dependency distance andthe random baseline reduces as the strength of the formal constraint ondependency structures increases, suggesting that formal constraints absorb partof the dependency distance minimization effect. Our research paves the way forreplicating past research on dependency distance minimization using randomplanar linearizations as random baseline.</description><author>Lluís Alemany-Puig, Ramon Ferrer-i-Cancho</author><pubDate>Thu, 29 Jun 2023 14:56:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.05564v3</guid></item><item><title>Alternative Telescopic Displacement: An Efficient Multimodal Alignment Method</title><link>http://arxiv.org/abs/2306.16950v1</link><description>Feature alignment is the primary means of fusing multimodal data. We proposea feature alignment method that fully fuses multimodal information, whichalternately shifts and expands feature information from different modalities tohave a consistent representation in a feature space. The proposed method canrobustly capture high-level interactions between features of differentmodalities, thus significantly improving the performance of multimodallearning. We also show that the proposed method outperforms other popularmultimodal schemes on multiple tasks. Experimental evaluation of ETT andMIT-BIH-Arrhythmia, datasets shows that the proposed method achieves state ofthe art performance.</description><author>Jiahao Qin, Yitao Xu, Zihong Luo Chengzhi Liu, Zong Lu, Xiaojun Zhang</author><pubDate>Thu, 29 Jun 2023 14:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16950v1</guid></item><item><title>SVDinsTN: An Integrated Method for Tensor Network Representation with Efficient Structure Search</title><link>http://arxiv.org/abs/2305.14912v2</link><description>Tensor network (TN) representation is a powerful technique for data analysisand machine learning. It practically involves a challenging TN structure search(TN-SS) problem, which aims to search for the optimal structure to achieve acompact representation. Existing TN-SS methods mainly adopt a bi-leveloptimization method that leads to excessive computational costs due to repeatedstructure evaluations. To address this issue, we propose an efficientintegrated (single-level) method named SVD-inspired TN decomposition(SVDinsTN), eliminating the need for repeated tedious structure evaluation. Byinserting a diagonal factor for each edge of the fully-connected TN, wecalculate TN cores and diagonal factors simultaneously, with factor sparsityrevealing the most compact TN structure. Experimental results on real-worlddata demonstrate that SVDinsTN achieves approximately $10\sim{}10^3$ timesacceleration in runtime compared to the existing TN-SS methods whilemaintaining a comparable level of representation ability.</description><author>Yu-Bang Zheng, Xi-Le Zhao, Junhua Zeng, Chao Li, Qibin Zhao, Heng-Chao Li, Ting-Zhu Huang</author><pubDate>Thu, 29 Jun 2023 14:49:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14912v2</guid></item><item><title>MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier</title><link>http://arxiv.org/abs/2209.11549v2</link><description>We offer a method for one-shot mask-guided image synthesis that allowscontrolling manipulations of a single image by inverting a quasi-robustclassifier equipped with strong regularizers. Our proposed method, entitledMAGIC, leverages structured gradients from a pre-trained quasi-robustclassifier to better preserve the input semantics while preserving itsclassification accuracy, thereby guaranteeing credibility in the synthesis.Unlike current methods that use complex primitives to supervise the process oruse attention maps as a weak supervisory signal, MAGIC aggregates gradientsover the input, driven by a guide binary mask that enforces a strong, spatialprior. MAGIC implements a series of manipulations with a single frameworkachieving shape and location control, intense non-rigid shape deformations, andcopy/move operations in the presence of repeating objects and gives users firmcontrol over the synthesis by requiring to simply specify binary guide masks.Our study and findings are supported by various qualitative comparisons withthe state-of-the-art on the same images sampled from ImageNet and quantitativeanalysis using machine perception along with a user survey of 100+ participantsthat endorse our synthesis quality. Project page athttps://mozhdehrouhsedaghat.github.io/magic.html. Code is available athttps://github.com/mozhdehrouhsedaghat/magic</description><author>Mozhdeh Rouhsedaghat, Masoud Monajatipoor, Kai-Wei Chang, Iacopo Masi</author><pubDate>Thu, 29 Jun 2023 14:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.11549v2</guid></item><item><title>ChatGPT vs State-of-the-Art Models: A Benchmarking Study in Keyphrase Generation Task</title><link>http://arxiv.org/abs/2304.14177v2</link><description>Transformer-based language models, including ChatGPT, have demonstratedexceptional performance in various natural language generation tasks. However,there has been limited research evaluating ChatGPT's keyphrase generationability, which involves identifying informative phrases that accurately reflecta document's content. This study seeks to address this gap by comparingChatGPT's keyphrase generation performance with state-of-the-art models, whilealso testing its potential as a solution for two significant challenges in thefield: domain adaptation and keyphrase generation from long documents. Weconducted experiments on six publicly available datasets from scientificarticles and news domains, analyzing performance on both short and longdocuments. Our results show that ChatGPT outperforms current state-of-the-artmodels in all tested datasets and environments, generating high-qualitykeyphrases that adapt well to diverse domains and document lengths.</description><author>Roberto Martínez-Cruz, Alvaro J. López-López, José Portela</author><pubDate>Thu, 29 Jun 2023 14:40:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14177v2</guid></item><item><title>BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion</title><link>http://arxiv.org/abs/2306.16940v1</link><description>We show, for the first time, that neural networks trained only on syntheticdata achieve state-of-the-art accuracy on the problem of 3D human pose andshape (HPS) estimation from real images. Previous synthetic datasets have beensmall, unrealistic, or lacked realistic clothing. Achieving sufficient realismis non-trivial and we show how to do this for full bodies in motion.Specifically, our BEDLAM dataset contains monocular RGB videos withground-truth 3D bodies in SMPL-X format. It includes a diversity of bodyshapes, motions, skin tones, hair, and clothing. The clothing is realisticallysimulated on the moving bodies using commercial clothing physics simulation. Werender varying numbers of people in realistic scenes with varied lighting andcamera motions. We then train various HPS regressors using BEDLAM and achievestate-of-the-art accuracy on real-image benchmarks despite training withsynthetic data. We use BEDLAM to gain insights into what model design choicesare important for accuracy. With good synthetic training data, we find that abasic method like HMR approaches the accuracy of the current SOTA method(CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truthbodies, 3D clothing, support code, and more are available for researchpurposes. Additionally, we provide detailed information about our syntheticdata generation pipeline, enabling others to generate their own datasets. Seethe project page: https://bedlam.is.tue.mpg.de/.</description><author>Michael J. Black, Priyanka Patel, Joachim Tesch, Jinlong Yang</author><pubDate>Thu, 29 Jun 2023 14:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16940v1</guid></item><item><title>Restore Translation Using Equivariant Neural Networks</title><link>http://arxiv.org/abs/2306.16938v1</link><description>Invariance to spatial transformations such as translations and rotations is adesirable property and a basic design principle for classification neuralnetworks. However, the commonly used convolutional neural networks (CNNs) areactually very sensitive to even small translations. There exist vast works toachieve exact or approximate transformation invariance by designingtransformation-invariant models or assessing the transformations. These worksusually make changes to the standard CNNs and harm the performance on standarddatasets. In this paper, rather than modifying the classifier, we propose apre-classifier restorer to recover translated (or even rotated) inputs to theoriginal ones which will be fed into any classifier for the same dataset. Therestorer is based on a theoretical result which gives a sufficient andnecessary condition for an affine operator to be translational equivariant on atensor space.</description><author>Yihan Wang, Lijia Yu, Xiao-Shan Gao</author><pubDate>Thu, 29 Jun 2023 14:34:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16938v1</guid></item><item><title>DreamDiffusion: Generating High-Quality Images from Brain EEG Signals</title><link>http://arxiv.org/abs/2306.16934v1</link><description>This paper introduces DreamDiffusion, a novel method for generatinghigh-quality images directly from brain electroencephalogram (EEG) signals,without the need to translate thoughts into text. DreamDiffusion leveragespre-trained text-to-image models and employs temporal masked signal modeling topre-train the EEG encoder for effective and robust EEG representations.Additionally, the method further leverages the CLIP image encoder to provideextra supervision to better align EEG, text, and image embeddings with limitedEEG-image pairs. Overall, the proposed method overcomes the challenges of usingEEG signals for image generation, such as noise, limited information, andindividual differences, and achieves promising results. Quantitative andqualitative results demonstrate the effectiveness of the proposed method as asignificant step towards portable and low-cost ``thoughts-to-image'', withpotential applications in neuroscience and computer vision.</description><author>Yunpeng Bai, Xintao Wang, Yanpei Cao, Yixiao Ge, Chun Yuan, Ying Shan</author><pubDate>Thu, 29 Jun 2023 14:33:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16934v1</guid></item><item><title>UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?</title><link>http://arxiv.org/abs/2306.16931v1</link><description>This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023shared task for Task-A and Task-C. We focus especially on Task-C and propose anovel LLMs cooperation system named a doctor-patient loop to generatehigh-quality conversation data sets. The experiment results demonstrate thatour approaches yield reasonable performance as evaluated by automatic metricssuch as ROUGE, medical concept recall, BLEU, and Self-BLEU. Furthermore, weconducted a comparative analysis between our proposed method and ChatGPT andGPT-4. This analysis also investigates the potential of utilizing cooperationLLMs to generate high-quality datasets.</description><author>Junda Wang, Zonghai Yao, Avijit Mitra, Samuel Osebe, Zhichao Yang, Hong Yu</author><pubDate>Thu, 29 Jun 2023 14:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16931v1</guid></item><item><title>One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization</title><link>http://arxiv.org/abs/2306.16928v1</link><description>Single image 3D reconstruction is an important but challenging task thatrequires extensive knowledge of our natural world. Many existing methods solvethis problem by optimizing a neural radiance field under the guidance of 2Ddiffusion models but suffer from lengthy optimization time, 3D inconsistencyresults, and poor geometry. In this work, we propose a novel method that takesa single image of any object as input and generates a full 360-degree 3Dtextured mesh in a single feed-forward pass. Given a single image, we first usea view-conditioned 2D diffusion model, Zero123, to generate multi-view imagesfor the input view, and then aim to lift them up to 3D space. Since traditionalreconstruction methods struggle with inconsistent multi-view predictions, webuild our 3D reconstruction module upon an SDF-based generalizable neuralsurface reconstruction method and propose several critical training strategiesto enable the reconstruction of 360-degree meshes. Without costlyoptimizations, our method reconstructs 3D shapes in significantly less timethan existing methods. Moreover, our method favors better geometry, generatesmore 3D consistent results, and adheres more closely to the input image. Weevaluate our approach on both synthetic data and in-the-wild images anddemonstrate its superiority in terms of both mesh quality and runtime. Inaddition, our approach can seamlessly support the text-to-3D task byintegrating with off-the-shelf text-to-image diffusion models.</description><author>Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, Hao Su</author><pubDate>Thu, 29 Jun 2023 14:28:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16928v1</guid></item><item><title>OSP: Boosting Distributed Model Training with 2-stage Synchronization</title><link>http://arxiv.org/abs/2306.16926v1</link><description>Distributed deep learning (DDL) is a promising research area, which aims toincrease the efficiency of training deep learning tasks with large size ofdatasets and models. As the computation capability of DDL nodes continues toincrease, the network connection between nodes is becoming a major bottleneck.Various methods of gradient compression and improved model synchronization havebeen proposed to address this bottleneck in Parameter-Server-based DDL.However, these two types of methods can result in accuracy loss due todiscarded gradients and have limited enhancement on the throughput of modelsynchronization, respectively. To address these challenges, we propose a newmodel synchronization method named Overlapped Synchronization Parallel (OSP),which achieves efficient communication with a 2-stage synchronization approachand uses Local-Gradient-based Parameter correction (LGP) to avoid accuracy losscaused by stale parameters. The prototype of OSP has been implemented usingPyTorch and evaluated on commonly used deep learning models and datasets with a9-node testbed. Evaluation results show that OSP can achieve up to 50\%improvement in throughput without accuracy loss compared to popularsynchronization models.</description><author>Zixuan Chen, Lei Shi, Xuandong Liu, Jiahui Li, Sen Liu, Yang Xu</author><pubDate>Thu, 29 Jun 2023 14:24:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16926v1</guid></item><item><title>MIS-FM: 3D Medical Image Segmentation using Foundation Models Pretrained on a Large-Scale Unannotated Dataset</title><link>http://arxiv.org/abs/2306.16925v1</link><description>Pretraining with large-scale 3D volumes has a potential for improving thesegmentation performance on a target medical image dataset where the trainingimages and annotations are limited. Due to the high cost of acquiringpixel-level segmentation annotations on the large-scale pretraining dataset,pretraining with unannotated images is highly desirable. In this work, wepropose a novel self-supervised learning strategy named Volume Fusion (VF) forpretraining 3D segmentation models. It fuses several random patches from aforeground sub-volume to a background sub-volume based on a predefined set ofdiscrete fusion coefficients, and forces the model to predict the fusioncoefficient of each voxel, which is formulated as a self-supervisedsegmentation task without manual annotations. Additionally, we propose a novelnetwork architecture based on parallel convolution and transformer blocks thatis suitable to be transferred to different downstream segmentation tasks withvarious scales of organs and lesions. The proposed model was pretrained with110k unannotated 3D CT volumes, and experiments with different downstreamsegmentation targets including head and neck organs, thoracic/abdominal organsshowed that our pretrained model largely outperformed training from scratch andseveral state-of-the-art self-supervised training methods and segmentationmodels. The code and pretrained model are available athttps://github.com/openmedlab/MIS-FM.</description><author>Guotai Wang, Jianghao Wu, Xiangde Luo, Xinglong Liu, Kang Li, Shaoting Zhang</author><pubDate>Thu, 29 Jun 2023 14:22:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16925v1</guid></item><item><title>NAUTILUS: boosting Bayesian importance nested sampling with deep learning</title><link>http://arxiv.org/abs/2306.16923v1</link><description>We introduce a novel approach to boost the efficiency of the importancenested sampling (INS) technique for Bayesian posterior and evidence estimationusing deep learning. Unlike rejection-based sampling methods such as vanillanested sampling (NS) or Markov chain Monte Carlo (MCMC) algorithms, importancesampling techniques can use all likelihood evaluations for posterior andevidence estimation. However, for efficient importance sampling, one needsproposal distributions that closely mimic the posterior distributions. We showhow to combine INS with deep learning via neural network regression toaccomplish this task. We also introduce NAUTILUS, a reference open-sourcePython implementation of this technique for Bayesian posterior and evidenceestimation. We compare NAUTILUS against popular NS and MCMC packages, includingEMCEE, DYNESTY, ULTRANEST and POCOMC, on a variety of challenging syntheticproblems and real-world applications in exoplanet detection, galaxy SED fittingand cosmology. In all applications, the sampling efficiency of NAUTILUS issubstantially higher than that of all other samplers, often by more than anorder of magnitude. Simultaneously, NAUTILUS delivers highly accurate resultsand needs fewer likelihood evaluations than all other samplers tested. We alsoshow that NAUTILUS has good scaling with the dimensionality of the likelihoodand is easily parallelizable to many CPUs.</description><author>Johannes U. Lange</author><pubDate>Thu, 29 Jun 2023 14:18:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16923v1</guid></item><item><title>Provable Advantage of Curriculum Learning on Parity Targets with Mixed Inputs</title><link>http://arxiv.org/abs/2306.16921v1</link><description>Experimental results have shown that curriculum learning, i.e., presentingsimpler examples before more complex ones, can improve the efficiency oflearning. Some recent theoretical results also showed that changing thesampling distribution can help neural networks learn parities, with formalresults only for large learning rates and one-step arguments. Here we show aseparation result in the number of training steps with standard (bounded)learning rates on a common sample distribution: if the data distribution is amixture of sparse and dense inputs, there exists a regime in which a 2-layerReLU neural network trained by a curriculum noisy-GD (or SGD) algorithm thatuses sparse examples first, can learn parities of sufficiently large degree,while any fully connected neural network of possibly larger width or depthtrained by noisy-GD on the unordered samples cannot learn without additionalsteps. We also provide experimental results supporting the qualitativeseparation beyond the specific regime of the theoretical results.</description><author>Emmanuel Abbe, Elisabetta Cornacchia, Aryo Lotfi</author><pubDate>Thu, 29 Jun 2023 14:14:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16921v1</guid></item><item><title>PCDAL: A Perturbation Consistency-Driven Active Learning Approach for Medical Image Segmentation and Classification</title><link>http://arxiv.org/abs/2306.16918v1</link><description>In recent years, deep learning has become a breakthrough technique inassisting medical image diagnosis. Supervised learning using convolutionalneural networks (CNN) provides state-of-the-art performance and has served as abenchmark for various medical image segmentation and classification. However,supervised learning deeply relies on large-scale annotated data, which isexpensive, time-consuming, and even impractical to acquire in medical imagingapplications. Active Learning (AL) methods have been widely applied in naturalimage classification tasks to reduce annotation costs by selecting morevaluable examples from the unlabeled data pool. However, their application inmedical image segmentation tasks is limited, and there is currently noeffective and universal AL-based method specifically designed for 3D medicalimage segmentation. To address this limitation, we propose an AL-based methodthat can be simultaneously applied to 2D medical image classification,segmentation, and 3D medical image segmentation tasks. We extensively validatedour proposed active learning method on three publicly available and challengingmedical image datasets, Kvasir Dataset, COVID-19 Infection SegmentationDataset, and BraTS2019 Dataset. The experimental results demonstrate that ourPCDAL can achieve significantly improved performance with fewer annotations in2D classification and segmentation and 3D segmentation tasks. The codes of thisstudy are available at https://github.com/ortonwang/PCDAL.</description><author>Tao Wang, Xinlin Zhang, Yuanbo Zhou, Junlin Lan, Tao Tan, Min Du, Qinquan Gao, Tong Tong</author><pubDate>Thu, 29 Jun 2023 14:11:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16918v1</guid></item><item><title>The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</title><link>http://arxiv.org/abs/2306.16917v1</link><description>Estimating camera motion in deformable scenes poses a complex and openresearch challenge. Most existing non-rigid structure from motion techniquesassume to observe also static scene parts besides deforming scene parts inorder to establish an anchoring reference. However, this assumption does nothold true in certain relevant application cases such as endoscopies. Deformableodometry and SLAM pipelines, which tackle the most challenging scenario ofexploratory trajectories, suffer from a lack of robustness and properquantitative evaluation methodologies. To tackle this issue with a commonbenchmark, we introduce the Drunkard's Dataset, a challenging collection ofsynthetic data targeting visual navigation and reconstruction in deformableenvironments. This dataset is the first large set of exploratory cameratrajectories with ground truth inside 3D scenes where every surface exhibitsnon-rigid deformations over time. Simulations in realistic 3D buildings lets usobtain a vast amount of data and ground truth labels, including camera poses,RGB images and depth, optical flow and normal maps at high resolution andquality. We further present a novel deformable odometry method, dubbed theDrunkard's Odometry, which decomposes optical flow estimates into rigid-bodycamera motion and non-rigid scene deformations. In order to validate our data,our work contains an evaluation of several baselines as well as a noveltracking error metric which does not require ground truth data. Dataset andcode: https://davidrecasens.github.io/TheDrunkard'sOdometry/</description><author>David Recasens, Martin R. Oswald, Marc Pollefeys, Javier Civera</author><pubDate>Thu, 29 Jun 2023 14:09:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16917v1</guid></item><item><title>Obeying the Order: Introducing Ordered Transfer Hyperparameter Optimisation</title><link>http://arxiv.org/abs/2306.16916v1</link><description>We introduce ordered transfer hyperparameter optimisation (OTHPO), a versionof transfer learning for hyperparameter optimisation (HPO) where the tasksfollow a sequential order. Unlike for state-of-the-art transfer HPO, theassumption is that each task is most correlated to those immediately before it.This matches many deployed settings, where hyperparameters are retuned as moredata is collected; for instance tuning a sequence of movie recommendationsystems as more movies and ratings are added. We propose a formal definition,outline the differences to related problems and propose a basic OTHPO methodthat outperforms state-of-the-art transfer HPO. We empirically show theimportance of taking order into account using ten benchmarks. The benchmarksare in the setting of gradually accumulating data, and span XGBoost, randomforest, approximate k-nearest neighbor, elastic net, support vector machinesand a separate real-world motivated optimisation problem. We open source thebenchmarks to foster future research on ordered transfer HPO.</description><author>Sigrid Passano Hellan, Huibin Shen, François-Xavier Aubet, David Salinas, Aaron Klein</author><pubDate>Thu, 29 Jun 2023 14:08:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16916v1</guid></item><item><title>Computationally Assisted Quality Control for Public Health Data Streams</title><link>http://arxiv.org/abs/2306.16914v1</link><description>Irregularities in public health data streams (like COVID-19 Cases) hamperdata-driven decision-making for public health stakeholders. A real-time,computer-generated list of the most important, outlying data points fromthousands of daily-updated public health data streams could assist an expertreviewer in identifying these irregularities. However, existing outlierdetection frameworks perform poorly on this task because they do not accountfor the data volume or for the statistical properties of public health streams.Accordingly, we developed FlaSH (Flagging Streams in public Health), apractical outlier detection framework for public health data users that usessimple, scalable models to capture these statistical properties explicitly. Inan experiment where human experts evaluate FlaSH and existing methods(including deep learning approaches), FlaSH scales to the data volume of thistask, matches or exceeds these other methods in mean accuracy, and identifiesthe outlier points that users empirically rate as more helpful. Based on theseresults, FlaSH has been deployed on data streams used by public healthstakeholders.</description><author>Ananya Joshi, Kathryn Mazaitis, Roni Rosenfeld, Bryan Wilder</author><pubDate>Thu, 29 Jun 2023 14:08:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16914v1</guid></item></channel></rss>