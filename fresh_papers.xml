<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 13 Dec 2023 14:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>SMERF: Streamable Memory Efficient Radiance Fields for Real-Time Large-Scene Exploration</title><link>http://arxiv.org/abs/2312.07541v1</link><description>Recent techniques for real-time view synthesis have rapidly advanced infidelity and speed, and modern methods are capable of renderingnear-photorealistic scenes at interactive frame rates. At the same time, atension has arisen between explicit scene representations amenable torasterization and neural fields built on ray marching, with state-of-the-artinstances of the latter surpassing the former in quality while beingprohibitively expensive for real-time applications. In this work, we introduceSMERF, a view synthesis approach that achieves state-of-the-art accuracy amongreal-time methods on large scenes with footprints up to 300 m$^2$ at avolumetric resolution of 3.5 mm$^3$. Our method is built upon two primarycontributions: a hierarchical model partitioning scheme, which increases modelcapacity while constraining compute and memory consumption, and a distillationtraining strategy that simultaneously yields high fidelity and internalconsistency. Our approach enables full six degrees of freedom (6DOF) navigationwithin a web browser and renders in real-time on commodity smartphones andlaptops. Extensive experiments show that our method exceeds the currentstate-of-the-art in real-time novel view synthesis by 0.78 dB on standardbenchmarks and 1.78 dB on large scenes, renders frames three orders ofmagnitude faster than state-of-the-art radiance field models, and achievesreal-time performance across a wide variety of commodity devices, includingsmartphones. We encourage the reader to explore these models in person at ourproject website: https://smerf-3d.github.io.</description><author>Daniel Duckworth, Peter Hedman, Christian Reiser, Peter Zhizhin, Jean-François Thibert, Mario Lučić, Richard Szeliski, Jonathan T. Barron</author><pubDate>Tue, 12 Dec 2023 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07541v1</guid></item><item><title>diff History for Long-Context Language Agents</title><link>http://arxiv.org/abs/2312.07540v1</link><description>Language Models (LMs) offer an exciting solution for general-purpose embodiedcontrol. However, a key technical issue arises when using an LM-basedcontroller: environment observations must be converted to text, which coupledwith history, leads to prohibitively large textual prompts. As a result, priorwork in LM agents is limited to restricted domains with either smallobservation size or minimal needs for interaction history. In this paper, weintroduce a simple and highly effective solution to these issues. We exploitthe fact that consecutive text observations have high similarity and propose tocompress them via the Unix diff command. We demonstrate our approach inNetHack, a complex rogue-like video game, that requires long-horizon reasoningfor decision-making and is far from solved, particularly for neural agents.Diff history offers an average of 4x increase in the length of the text-basedinteraction history available to the LM. This observational compression alongwith the benefits of abstraction yields a 7x improvement in game score onheld-out environment instances over state-of-the-art baselines. It alsooutperforms prior agents that use visual observations by over 40%.</description><author>Ulyana Piterbarg, Lerrel Pinto, Rob Fergus</author><pubDate>Tue, 12 Dec 2023 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07540v1</guid></item><item><title>HeadArtist: Text-conditioned 3D Head Generation with Self Score Distillation</title><link>http://arxiv.org/abs/2312.07539v1</link><description>This work presents HeadArtist for 3D head generation from text descriptions.With a landmark-guided ControlNet serving as the generative prior, we come upwith an efficient pipeline that optimizes a parameterized 3D head model underthe supervision of the prior distillation itself. We call such a process selfscore distillation (SSD). In detail, given a sampled camera pose, we firstrender an image and its corresponding landmarks from the head model, and addsome particular level of noise onto the image. The noisy image, landmarks, andtext condition are then fed into the frozen ControlNet twice for noiseprediction. Two different classifier-free guidance (CFG) weights are appliedduring these two predictions, and the prediction difference offers a directionon how the rendered image can better match the text of interest. Experimentalresults suggest that our approach delivers high-quality 3D head sculptures withadequate geometry and photorealistic appearance, significantly outperformingstate-ofthe-art methods. We also show that the same pipeline well supportsediting the generated heads, including both geometry deformation and appearancechange.</description><author>Hongyu Liu, Xuan Wang, Ziyu Wan, Yujun Shen, Yibing Song, Jing Liao, Qifeng Chen</author><pubDate>Tue, 12 Dec 2023 18:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07539v1</guid></item><item><title>Anatomically Constrained Implicit Face Models</title><link>http://arxiv.org/abs/2312.07538v1</link><description>Coordinate based implicit neural representations have gained rapid popularityin recent years as they have been successfully used in image, geometry andscene modeling tasks. In this work, we present a novel use case for suchimplicit representations in the context of learning anatomically constrainedface models. Actor specific anatomically constrained face models are the stateof the art in both facial performance capture and performance retargeting.Despite their practical success, these anatomical models are slow to evaluateand often require extensive data capture to be built. We propose the anatomicalimplicit face model; an ensemble of implicit neural networks that jointly learnto model the facial anatomy and the skin surface with high-fidelity, and canreadily be used as a drop in replacement to conventional blendshape models.Given an arbitrary set of skin surface meshes of an actor and only a neutralshape with estimated skull and jaw bones, our method can recover a denseanatomical substructure which constrains every point on the facial surface. Wedemonstrate the usefulness of our approach in several tasks ranging from shapefitting, shape editing, and performance retargeting.</description><author>Prashanth Chandran, Gaspard Zoss</author><pubDate>Tue, 12 Dec 2023 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07538v1</guid></item><item><title>FreeInit: Bridging Initialization Gap in Video Diffusion Models</title><link>http://arxiv.org/abs/2312.07537v1</link><description>Though diffusion-based video generation has witnessed rapid progress, theinference results of existing models still exhibit unsatisfactory temporalconsistency and unnatural dynamics. In this paper, we delve deep into the noiseinitialization of video diffusion models, and discover an implicittraining-inference gap that attributes to the unsatisfactory inference quality.Our key findings are: 1) the spatial-temporal frequency distribution of theinitial latent at inference is intrinsically different from that for training,and 2) the denoising process is significantly influenced by the low-frequencycomponents of the initial noise. Motivated by these observations, we propose aconcise yet effective inference sampling strategy, FreeInit, whichsignificantly improves temporal consistency of videos generated by diffusionmodels. Through iteratively refining the spatial-temporal low-frequencycomponents of the initial latent during inference, FreeInit is able tocompensate the initialization gap between training and inference, thuseffectively improving the subject appearance and temporal consistency ofgeneration results. Extensive experiments demonstrate that FreeInitconsistently enhances the generation results of various text-to-videogeneration models without additional training.</description><author>Tianxing Wu, Chenyang Si, Yuming Jiang, Ziqi Huang, Ziwei Liu</author><pubDate>Tue, 12 Dec 2023 18:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07537v1</guid></item><item><title>FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion Model with Any Condition</title><link>http://arxiv.org/abs/2312.07536v1</link><description>Recent approaches such as ControlNet offer users fine-grained spatial controlover text-to-image (T2I) diffusion models. However, auxiliary modules have tobe trained for each type of spatial condition, model architecture, andcheckpoint, putting them at odds with the diverse intents and preferences ahuman designer would like to convey to the AI models during the contentcreation process. In this work, we present FreeControl, a training-freeapproach for controllable T2I generation that supports multiple conditions,architectures, and checkpoints simultaneously. FreeControl designs structureguidance to facilitate the structure alignment with a guidance image, andappearance guidance to enable the appearance sharing between images generatedusing the same seed. Extensive qualitative and quantitative experimentsdemonstrate the superior performance of FreeControl across a variety ofpre-trained T2I models. In particular, FreeControl facilitates convenienttraining-free control over many different architectures and checkpoints, allowsthe challenging input conditions on which most of the existing training-freemethods fail, and achieves competitive synthesis quality with training-basedapproaches.</description><author>Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Bochen Guan, Yin Li, Bolei Zhou</author><pubDate>Tue, 12 Dec 2023 18:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07536v1</guid></item><item><title>Improved Frequency Estimation Algorithms with and without Predictions</title><link>http://arxiv.org/abs/2312.07535v1</link><description>Estimating frequencies of elements appearing in a data stream is a key taskin large-scale data analysis. Popular sketching approaches to this problem(e.g., CountMin and CountSketch) come with worst-case guarantees thatprobabilistically bound the error of the estimated frequencies for any possibleinput. The work of Hsu et al. (2019) introduced the idea of using machinelearning to tailor sketching algorithms to the specific data distribution theyare being run on. In particular, their learning-augmented frequency estimationalgorithm uses a learned heavy-hitter oracle which predicts which elements willappear many times in the stream. We give a novel algorithm, which in someparameter regimes, already theoretically outperforms the learning basedalgorithm of Hsu et al. without the use of any predictions. Augmenting ouralgorithm with heavy-hitter predictions further reduces the error and improvesupon the state of the art. Empirically, our algorithms achieve superiorperformance in all experiments compared to prior approaches.</description><author>Anders Aamand, Justin Y. Chen, Huy Lê Nguyen, Sandeep Silwal, Ali Vakilian</author><pubDate>Tue, 12 Dec 2023 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07535v1</guid></item><item><title>SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation</title><link>http://arxiv.org/abs/2308.16876v2</link><description>Human-centric video frame interpolation has great potential for improvingpeople's entertainment experiences and finding commercial applications in thesports analysis industry, e.g., synthesizing slow-motion videos. Although thereare multiple benchmark datasets available in the community, none of them isdedicated for human-centric scenarios. To bridge this gap, we introduceSportsSloMo, a benchmark consisting of more than 130K video clips and 1M videoframes of high-resolution ($\geq$720p) slow-motion sports videos crawled fromYouTube. We re-train several state-of-the-art methods on our benchmark, and theresults show a decrease in their accuracy compared to other datasets. Ithighlights the difficulty of our benchmark and suggests that it posessignificant challenges even for the best-performing methods, as human bodiesare highly deformable and occlusions are frequent in sports videos. To improvethe accuracy, we introduce two loss terms considering the human-aware priors,where we add auxiliary supervision to panoptic segmentation and human keypointsdetection, respectively. The loss terms are model agnostic and can be easilyplugged into any video frame interpolation approaches. Experimental resultsvalidate the effectiveness of our proposed loss terms, leading to consistentperformance improvement over 5 existing models, which establish strong baselinemodels on our benchmark. The dataset and code can be found at:https://neu-vi.github.io/SportsSlomo/.</description><author>Jiaben Chen, Huaizu Jiang</author><pubDate>Tue, 12 Dec 2023 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16876v2</guid></item><item><title>Cosmological Field Emulation and Parameter Inference with Diffusion Models</title><link>http://arxiv.org/abs/2312.07534v1</link><description>Cosmological simulations play a crucial role in elucidating the effect ofphysical parameters on the statistics of fields and on constraining parametersgiven information on density fields. We leverage diffusion generative models toaddress two tasks of importance to cosmology -- as an emulator for cold darkmatter density fields conditional on input cosmological parameters $\Omega_m$and $\sigma_8$, and as a parameter inference model that can return constraintson the cosmological parameters of an input field. We show that the model isable to generate fields with power spectra that are consistent with those ofthe simulated target distribution, and capture the subtle effect of eachparameter on modulations in the power spectrum. We additionally explore theirutility as parameter inference models and find that we can obtain tightconstraints on cosmological parameters.</description><author>Nayantara Mudur, Carolina Cuesta-Lazaro, Douglas P. Finkbeiner</author><pubDate>Tue, 12 Dec 2023 18:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07534v1</guid></item><item><title>VILA: On Pre-training for Visual Language Models</title><link>http://arxiv.org/abs/2312.07533v1</link><description>Visual language models (VLMs) rapidly progressed with the recent success oflarge language models. There have been growing efforts on visual instructiontuning to extend the LLM with visual inputs, but lacks an in-depth study of thevisual language pre-training process, where the model learns to perform jointmodeling on both modalities. In this work, we examine the design options forVLM pre-training by augmenting LLM towards VLM through step-by-stepcontrollable comparisons. We introduce three main findings: (1) freezing LLMsduring pre-training can achieve decent zero-shot performance, but lackin-context learning capability, which requires unfreezing the LLM; (2)interleaved pre-training data is beneficial whereas image-text pairs alone arenot optimal; (3) re-blending text-only instruction data to image-text dataduring instruction fine-tuning not only remedies the degradation of text-onlytasks, but also boosts VLM task accuracy. With an enhanced pre-training recipewe build VILA, a Visual Language model family that consistently outperforms thestate-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bellsand whistles. Multi-modal pre-training also helps unveil appealing propertiesof VILA, including multi-image reasoning, enhanced in-context learning, andbetter world knowledge.</description><author>Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, Song Han</author><pubDate>Tue, 12 Dec 2023 18:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07533v1</guid></item><item><title>Interfacing Foundation Models' Embeddings</title><link>http://arxiv.org/abs/2312.07532v1</link><description>We present FIND, a generalized interface for aligning foundation models'embeddings. As shown in teaser figure, a lightweight transformer interfacewithout tuning any foundation model weights is enough for a unified image(segmentation) and dataset-level (retrieval) understanding. The proposedinterface has the following favorable attributes: (1) Generalizable. It appliesto various tasks spanning retrieval, segmentation, \textit{etc.}, under thesame architecture and weights. (2) Prototypable. Different tasks are able to beimplemented through prototyping attention masks and embedding types. (3)Extendable. The proposed interface is adaptive to new tasks, and new models.(4) Interleavable. With the benefit of multi-task multi-modal training, theproposed interface creates an interleaved shared embedding space. In light ofthe interleaved embedding space, we introduce the FIND-Bench, which introducesnew training and evaluation annotations to the COCO dataset for interleavesegmentation and retrieval. Our approach achieves state-of-the-art performanceon FIND-Bench and competitive performance on standard retrieval andsegmentation settings. The training, evaluation, and demo code as well as thedataset have been released at https://github.com/UX-Decoder/FIND.</description><author>Xueyan Zou, Linjie Li, Jianfeng Wang, Jianwei Yang, Mingyu Ding, Zhengyuan Yang, Feng Li, Hao Zhang, Shilong Liu, Arul Aravinthan, Yong Jae Lee, Lijuan Wang</author><pubDate>Tue, 12 Dec 2023 18:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07532v1</guid></item><item><title>WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion</title><link>http://arxiv.org/abs/2312.07531v1</link><description>The estimation of 3D human motion from video has progressed rapidly butcurrent methods still have several key limitations. First, most methodsestimate the human in camera coordinates. Second, prior work on estimatinghumans in global coordinates often assumes a flat ground plane and producesfoot sliding. Third, the most accurate methods rely on computationallyexpensive optimization pipelines, limiting their use to offline applications.Finally, existing video-based methods are surprisingly less accurate thansingle-frame methods. We address these limitations with WHAM (World-groundedHumans with Accurate Motion), which accurately and efficiently reconstructs 3Dhuman motion in a global coordinate system from video. WHAM learns to lift 2Dkeypoint sequences to 3D using motion capture data and fuses this with videofeatures, integrating motion context and visual information. WHAM exploitscamera angular velocity estimated from a SLAM method together with human motionto estimate the body's global trajectory. We combine this with a contact-awaretrajectory refinement method that lets WHAM capture human motion in diverseconditions, such as climbing stairs. WHAM outperforms all existing 3D humanmotion recovery methods across multiple in-the-wild benchmarks. Code will beavailable for research purposes at http://wham.is.tue.mpg.de/</description><author>Soyong Shin, Juyong Kim, Eni Halilaj, Michael J. Black</author><pubDate>Tue, 12 Dec 2023 18:57:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07531v1</guid></item><item><title>Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance</title><link>http://arxiv.org/abs/2312.07530v1</link><description>Weakly supervised 3D object detection aims to learn a 3D detector with lowerannotation cost, e.g., 2D labels. Unlike prior work which still relies on fewaccurate 3D annotations, we propose a framework to study how to leverageconstraints between 2D and 3D domains without requiring any 3D labels.Specifically, we employ visual data from three perspectives to establishconnections between 2D and 3D domains. First, we design a feature-levelconstraint to align LiDAR and image features based on object-aware regions.Second, the output-level constraint is developed to enforce the overlap between2D and projected 3D box estimations. Finally, the training-level constraint isutilized by producing accurate and consistent 3D pseudo-labels that align withthe visual data. We conduct extensive experiments on the KITTI dataset tovalidate the effectiveness of the proposed three constraints. Without using any3D labels, our method achieves favorable performance against state-of-the-artapproaches and is competitive with the method that uses 500-frame 3Dannotations. Code and models will be made publicly available athttps://github.com/kuanchihhuang/VG-W3D.</description><author>Kuan-Chih Huang, Yi-Hsuan Tsai, Ming-Hsuan Yang</author><pubDate>Tue, 12 Dec 2023 18:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07530v1</guid></item><item><title>Topological Obstructions and How to Avoid Them</title><link>http://arxiv.org/abs/2312.07529v1</link><description>Incorporating geometric inductive biases into models can aid interpretabilityand generalization, but encoding to a specific geometric structure can bechallenging due to the imposed topological constraints. In this paper, wetheoretically and empirically characterize obstructions to training encoderswith geometric latent spaces. We show that local optima can arise due tosingularities (e.g. self-intersection) or due to an incorrect degree or windingnumber. We then discuss how normalizing flows can potentially circumvent theseobstructions by defining multimodal variational distributions. Inspired by thisobservation, we propose a new flow-based model that maps data points tomultimodal distributions over geometric spaces and empirically evaluate ourmodel on 2 domains. We observe improved stability during training and a higherchance of converging to a homeomorphic encoder.</description><author>Babak Esmaeili, Robin Walters, Heiko Zimmermann, Jan-Willem van de Meent</author><pubDate>Tue, 12 Dec 2023 18:56:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07529v1</guid></item><item><title>BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability</title><link>http://arxiv.org/abs/2312.07527v1</link><description>While there are numerous benchmarks comparing the performance of modernlanguage models (LMs), end-task evaluations often conflate notions of *factualaccuracy* ("truth") and *reasoning ability* ("rationality", or "honesty" in thesense of correctly reporting implications of beliefs). Our goal is a datasetthat clearly distinguishes these two notions. Our approach is to leverage andextend a collection of human-annotated *entailment trees*, engineered toexpress both good and bad chains of reasoning, and using a mixture of true andfalse facts, in particular including counterfactual examples, to avoid beliefbias (also known as the "content effect"). The resulting dataset, called BaRDa,contains 3000 entailments (1787 valid, 1213 invalid), using 6681 true and 2319false statements. Testing on four GPT-series models,GPT3(curie)/GPT3(davinici)/3.5/4, we find factual accuracy (truth) scores of74.1/80.6/82.6/87.1 and reasoning accuracy scores of 63.1/78.0/71.8/79.2. Thisshows the clear progression of models towards improved factual accuracy andentailment reasoning, and the dataset provides a new benchmark that morecleanly separates and quantifies these two notions.</description><author>Peter Clark, Bhavana Dalvi Mishra, Oyvind Tafjord</author><pubDate>Tue, 12 Dec 2023 18:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07527v1</guid></item><item><title>RTMO: Towards High-Performance One-Stage Real-Time Multi-Person Pose Estimation</title><link>http://arxiv.org/abs/2312.07526v1</link><description>Real-time multi-person pose estimation presents significant challenges inbalancing speed and precision. While two-stage top-down methods slow down asthe number of people in the image increases, existing one-stage methods oftenfail to simultaneously deliver high accuracy and real-time performance. Thispaper introduces RTMO, a one-stage pose estimation framework that seamlesslyintegrates coordinate classification by representing keypoints using dual 1-Dheatmaps within the YOLO architecture, achieving accuracy comparable totop-down methods while maintaining high speed. We propose a dynamic coordinateclassifier and a tailored loss function for heatmap learning, specificallydesigned to address the incompatibilities between coordinate classification anddense prediction models. RTMO outperforms state-of-the-art one-stage poseestimators, achieving 1.1% higher AP on COCO while operating about 9 timesfaster with the same backbone. Our largest model, RTMO-l, attains 74.8% AP onCOCO val2017 and 141 FPS on a single V100 GPU, demonstrating its efficiency andaccuracy. The code and models are available athttps://github.com/open-mmlab/mmpose/tree/dev-1.x/projects/rtmo.</description><author>Peng Lu, Tao Jiang, Yining Li, Xiangtai Li, Kai Chen, Wenming Yang</author><pubDate>Tue, 12 Dec 2023 18:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07526v1</guid></item><item><title>Learning finitely correlated states: stability of the spectral reconstruction</title><link>http://arxiv.org/abs/2312.07516v1</link><description>We show that marginals of subchains of length $t$ of any finitely correlatedtranslation invariant state on a chain can be learned, in trace distance, with$O(t^2)$ copies -- with an explicit dependence on local dimension, memorydimension and spectral properties of a certain map constructed from the state-- and computational complexity polynomial in $t$. The algorithm requires onlythe estimation of a marginal of a controlled size, in the worst case bounded bya multiple of the minimum bond dimension, from which it reconstructs atranslation invariant matrix product operator. In the analysis, a central roleis played by the theory of operator systems. A refined error bound can beproven for $C^*$-finitely correlated states, which have an operationalinterpretation in terms of sequential quantum channels applied to the memorysystem. We can also obtain an analogous error bound for a class of matrixproduct density operators reconstructible by local marginals. In this case, alinear number of marginals must be estimated, obtaining a sample complexity of$\tilde{O}(t^3)$. The learning algorithm also works for states that are onlyclose to a finitely correlated state, with the potential of providingcompetitive algorithms for other interesting families of states.</description><author>Marco Fanizza, Niklas Galke, Josep Lumbreras, Cambyse Rouzé, Andreas Winter</author><pubDate>Tue, 12 Dec 2023 18:47:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07516v1</guid></item><item><title>A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems</title><link>http://arxiv.org/abs/2312.07511v1</link><description>Recent advances in computational modelling of atomic systems, spanningmolecules, proteins, and materials, represent them as geometric graphs withatoms embedded as nodes in 3D Euclidean space. In these graphs, the geometricattributes transform according to the inherent physical symmetries of 3D atomicsystems, including rotations and translations in Euclidean space, as well asnode permutations. In recent years, Geometric Graph Neural Networks haveemerged as the preferred machine learning architecture powering applicationsranging from protein structure prediction to molecular simulations and materialgeneration. Their specificity lies in the inductive biases they leverage --such as physical symmetries and chemical properties -- to learn informativerepresentations of these geometric graphs. In this opinionated paper, weprovide a comprehensive and self-contained overview of the field of GeometricGNNs for 3D atomic systems. We cover fundamental background material andintroduce a pedagogical taxonomy of Geometric GNN architectures:(1) invariantnetworks, (2) equivariant networks in Cartesian basis, (3) equivariant networksin spherical basis, and (4) unconstrained networks. Additionally, we outlinekey datasets and application areas and suggest future research directions. Theobjective of this work is to present a structured perspective on the field,making it accessible to newcomers and aiding practitioners in gaining anintuition for its mathematical abstractions.</description><author>Alexandre Duval, Simon V. Mathis, Chaitanya K. Joshi, Victor Schmidt, Santiago Miret, Fragkiskos D. Malliaros, Taco Cohen, Pietro Lio, Yoshua Bengio, Michael Bronstein</author><pubDate>Tue, 12 Dec 2023 18:44:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07511v1</guid></item><item><title>PEEKABOO: Interactive Video Generation via Masked-Diffusion</title><link>http://arxiv.org/abs/2312.07509v1</link><description>Recently there has been a lot of progress in text-to-video generation, withstate-of-the-art models being capable of generating high quality, realisticvideos. However, these models lack the capability for users to interactivelycontrol and generate videos, which can potentially unlock new areas ofapplication. As a first step towards this goal, we tackle the problem ofendowing diffusion-based video generation models with interactivespatio-temporal control over their output. To this end, we take inspirationfrom the recent advances in segmentation literature to propose a novelspatio-temporal masked attention module - Peekaboo. This module is atraining-free, no-inference-overhead addition to off-the-shelf video generationmodels which enables spatio-temporal control. We also propose an evaluationbenchmark for the interactive video generation task. Through extensivequalitative and quantitative evaluation, we establish that Peekaboo enablescontrol video generation and even obtains a gain of upto 3.8x in mIoU overbaseline models.</description><author>Yash Jain, Anshul Nasery, Vibhav Vineet, Harkirat Behl</author><pubDate>Tue, 12 Dec 2023 18:43:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07509v1</guid></item><item><title>Non-Stationary Bandits with Auto-Regressive Temporal Dependency</title><link>http://arxiv.org/abs/2210.16386v3</link><description>Traditional multi-armed bandit (MAB) frameworks, predominantly examined understochastic or adversarial settings, often overlook the temporal dynamicsinherent in many real-world applications such as recommendation systems andonline advertising. This paper introduces a novel non-stationary MAB frameworkthat captures the temporal structure of these real-world dynamics through anauto-regressive (AR) reward structure. We propose an algorithm that integratestwo key mechanisms: (i) an alternation mechanism adept at leveraging temporaldependencies to dynamically balance exploration and exploitation, and (ii) arestarting mechanism designed to discard out-of-date information. Our algorithmachieves a regret upper bound that nearly matches the lower bound, with regretmeasured against a robust dynamic benchmark. Finally, via a real-world casestudy on tourism demand prediction, we demonstrate both the efficacy of ouralgorithm and the broader applicability of our techniques to more complex,rapidly evolving time series.</description><author>Qinyi Chen, Negin Golrezaei, Djallel Bouneffouf</author><pubDate>Tue, 12 Dec 2023 18:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16386v3</guid></item><item><title>NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention for Emotion Understanding</title><link>http://arxiv.org/abs/2312.07507v1</link><description>In the task of emotion recognition from videos, a key improvement has been tofocus on emotions over time rather than a single frame. There are manyarchitectures to address this task such as GRUs, LSTMs, Self-Attention,Transformers, and Temporal Convolutional Networks (TCNs). However, thesemethods suffer from high memory usage, large amounts of operations, or poorgradients. We propose a method known as Neighborhood Attention withConvolutions TCN (NAC-TCN) which incorporates the benefits of attention andTemporal Convolutional Networks while ensuring that causal relationships areunderstood which results in a reduction in computation and memory cost. Weaccomplish this by introducing a causal version of Dilated NeighborhoodAttention while incorporating it with convolutions. Our model achievescomparable, better, or state-of-the-art performance over TCNs, TCAN, LSTMs, andGRUs while requiring fewer parameters on standard emotion recognition datasets.We publish our code online for easy reproducibility and use in other projects.</description><author>Alexander Mehta, William Yang</author><pubDate>Tue, 12 Dec 2023 18:41:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07507v1</guid></item><item><title>COLMAP-Free 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2312.07504v1</link><description>While neural rendering has led to impressive advances in scene reconstructionand novel view synthesis, it relies heavily on accurately pre-computed cameraposes. To relax this constraint, multiple efforts have been made to trainNeural Radiance Fields (NeRFs) without pre-processed camera poses. However, theimplicit representations of NeRFs provide extra challenges to optimize the 3Dstructure and camera poses at the same time. On the other hand, the recentlyproposed 3D Gaussian Splatting provides new opportunities given its explicitpoint cloud representations. This paper leverages both the explicit geometricrepresentation and the continuity of the input video stream to perform novelview synthesis without any SfM preprocessing. We process the input frames in asequential manner and progressively grow the 3D Gaussians set by taking oneinput frame at a time, without the need to pre-compute the camera poses. Ourmethod significantly improves over previous approaches in view synthesis andcamera pose estimation under large motion changes. Our project page ishttps://oasisyang.github.io/colmap-free-3dgs</description><author>Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A. Efros, Xiaolong Wang</author><pubDate>Tue, 12 Dec 2023 18:39:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07504v1</guid></item><item><title>Local Spatiotemporal Representation Learning for Longitudinally-consistent Neuroimage Analysis</title><link>http://arxiv.org/abs/2206.04281v4</link><description>Recent self-supervised advances in medical computer vision exploit global andlocal anatomical self-similarity for pretraining prior to downstream tasks suchas segmentation. However, current methods assume i.i.d. image acquisition,which is invalid in clinical study designs where follow-up longitudinal scanstrack subject-specific temporal changes. Further, existing self-supervisedmethods for medically-relevant image-to-image architectures exploit onlyspatial or temporal self-similarity and only do so via a loss applied at asingle image-scale, with naive multi-scale spatiotemporal extensions collapsingto degenerate solutions. To these ends, this paper makes two contributions: (1)It presents a local and multi-scale spatiotemporal representation learningmethod for image-to-image architectures trained on longitudinal images. Itexploits the spatiotemporal self-similarity of learned multi-scaleintra-subject features for pretraining and develops several feature-wiseregularizations that avoid collapsed identity representations; (2) Duringfinetuning, it proposes a surprisingly simple self-supervised segmentationconsistency regularization to exploit intra-subject correlation. Benchmarked inthe one-shot segmentation setting, the proposed framework outperforms bothwell-tuned randomly-initialized baselines and current self-supervisedtechniques designed for both i.i.d. and longitudinal datasets. Theseimprovements are demonstrated across both longitudinal neurodegenerative adultMRI and developing infant brain MRI and yield both higher performance andlongitudinal consistency.</description><author>Mengwei Ren, Neel Dey, Martin A. Styner, Kelly Botteron, Guido Gerig</author><pubDate>Tue, 12 Dec 2023 18:36:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.04281v4</guid></item><item><title>NFResNet: Multi-scale and U-shaped Networks for Deblurring</title><link>http://arxiv.org/abs/2212.05909v2</link><description>Multi-Scale and U-shaped Networks are widely used in various imagerestoration problems, including deblurring. Keeping in mind the wide range ofapplications, we present a comparison of these architectures and their effectson image deblurring. We also introduce a new block called as NFResblock. Itconsists of a Fast Fourier Transformation layer and a series of modifiedNon-Linear Activation Free Blocks. Based on these architectures and additions,we introduce NFResnet and NFResnet+, which are modified multi-scale and U-Netarchitectures, respectively. We also use three different loss functions totrain these architectures: Charbonnier Loss, Edge Loss, and FrequencyReconstruction Loss. Extensive experiments on the Deep Video Deblurringdataset, along with ablation studies for each component, have been presented inthis paper. The proposed architectures achieve a considerable increase in PeakSignal to Noise (PSNR) ratio and Structural Similarity Index (SSIM) value.</description><author>Tanish Mittal, Preyansh Agrawal, Esha Pahwa, Aarya Makwana</author><pubDate>Tue, 12 Dec 2023 18:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.05909v2</guid></item><item><title>Multi-Branch Network for Imagery Emotion Prediction</title><link>http://arxiv.org/abs/2312.07500v1</link><description>For a long time, images have proved perfect at both storing and conveyingrich semantics, especially human emotions. A lot of research has been conductedto provide machines with the ability to recognize emotions in photos of people.Previous methods mostly focus on facial expressions but fail to consider thescene context, meanwhile scene context plays an important role in predictingemotions, leading to more accurate results. In addition,Valence-Arousal-Dominance (VAD) values offer a more precise quantitativeunderstanding of continuous emotions, yet there has been less emphasis onpredicting them compared to discrete emotional categories. In this paper, wepresent a novel Multi-Branch Network (MBN), which utilizes various sourceinformation, including faces, bodies, and scene contexts to predict bothdiscrete and continuous emotions in an image. Experimental results on EMOTICdataset, which contains large-scale images of people in unconstrainedsituations labeled with 26 discrete categories of emotions and VAD values, showthat our proposed method significantly outperforms state-of-the-art methodswith 28.4% in mAP and 0.93 in MAE. The results highlight the importance ofutilizing multiple contextual information in emotion prediction and illustratethe potential of our proposed method in a wide range of applications, such aseffective computing, human-computer interaction, and social robotics. Sourcecode:https://github.com/BaoNinh2808/Multi-Branch-Network-for-Imagery-Emotion-Prediction</description><author>Quoc-Bao Ninh, Hai-Chan Nguyen, Triet Huynh, Trung-Nghia Le</author><pubDate>Tue, 12 Dec 2023 18:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07500v1</guid></item><item><title>Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis</title><link>http://arxiv.org/abs/2312.05488v2</link><description>Game theory, as an analytical tool, is frequently utilized to analyze humanbehavior in social science research. With the high alignment between thebehavior of Large Language Models (LLMs) and humans, a promising researchdirection is to employ LLMs as substitutes for humans in game experiments,enabling social science research. However, despite numerous empiricalresearches on the combination of LLMs and game theory, the capabilityboundaries of LLMs in game theory remain unclear. In this research, we endeavorto systematically analyze LLMs in the context of game theory. Specifically,rationality, as the fundamental principle of game theory, serves as the metricfor evaluating players' behavior -- building a clear desire, refining beliefabout uncertainty, and taking optimal actions. Accordingly, we select threeclassical games (dictator game, Rock-Paper-Scissors, and ring-network game) toanalyze to what extent LLMs can achieve rationality in these three aspects. Theexperimental results indicate that even the current state-of-the-art LLM(GPT-4) exhibits substantial disparities compared to humans in game theory. Forinstance, LLMs struggle to build desires based on uncommon preferences, fail torefine belief from many simple patterns, and may overlook or modify refinedbelief when taking actions. Therefore, we consider that introducing LLMs intogame experiments in the field of social science should be approached withgreater caution.</description><author>Caoyun Fan, Jindou Chen, Yaohui Jin, Hao He</author><pubDate>Tue, 12 Dec 2023 18:32:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05488v2</guid></item><item><title>Exploring Plain ViT Reconstruction for Multi-class Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2312.07495v1</link><description>This work studies the recently proposed challenging and practical Multi-classUnsupervised Anomaly Detection (MUAD) task, which only requires normal imagesfor training while simultaneously testing both normal/anomaly images formultiple classes. Existing reconstruction-based methods typically adopt pyramidnetworks as encoders/decoders to obtain multi-resolution features, accompaniedby elaborate sub-modules with heavier handcraft engineering designs for moreprecise localization. In contrast, a plain Vision Transformer (ViT) with simplearchitecture has been shown effective in multiple domains, which is simpler,more effective, and elegant. Following this spirit, this paper explores plainViT architecture for MUAD. Specifically, we abstract a Meta-AD concept byinducing current reconstruction-based methods. Then, we instantiate a novel andelegant plain ViT-based symmetric ViTAD structure, effectively designed step bystep from three macro and four micro perspectives. In addition, this paperreveals several interesting findings for further exploration. Finally, wepropose a comprehensive and fair evaluation benchmark on eight metrics for theMUAD task. Based on a naive training recipe, ViTAD achieves state-of-the-art(SoTA) results and efficiency on the MVTec AD and VisA datasets without bellsand whistles, obtaining 85.4 mAD that surpasses SoTA UniAD by +3.0, and onlyrequiring 1.1 hours and 2.3G GPU memory to complete model training by a singleV100 GPU. Source code, models, and more results are available athttps://zhangzjn.github.io/projects/ViTAD.</description><author>Jiangning Zhang, Xuhai Chen, Yabiao Wang, Chengjie Wang, Yong Liu, Xiangtai Li, Ming-Hsuan Yang, Dacheng Tao</author><pubDate>Tue, 12 Dec 2023 18:28:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07495v1</guid></item><item><title>SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models</title><link>http://arxiv.org/abs/2312.07492v1</link><description>Current datasets for unwanted social bias auditing are limited to studyingprotected demographic features such as race and gender. In this work, weintroduce a comprehensive benchmark that is meant to capture the amplificationof social bias, via stigmas, in generative language models. We start with acomprehensive list of 93 stigmas documented in social science literature andcurate a question-answering (QA) dataset which involves simple socialsituations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with avariety of prompt styles, carefully constructed to systematically test for bothsocial bias and model robustness. We present results for SocialStigmaQA withtwo widely used open source generative language models and we demonstrate thatthe output generated by these models considerably amplifies existing socialbias against stigmatized groups. Specifically, we find that the proportion ofsocially biased output ranges from 45% to 59% across a variety of decodingstrategies and prompting styles. We discover that the deliberate design of thetemplates in our benchmark (e.g., by adding biasing text to the prompt orvarying the answer that indicates bias) impact the model tendencies to generatesocially biased output. Additionally, we report on patterns in the generatedchain-of-thought output, finding a variety of problems from subtle bias toevidence of a lack of reasoning. Warning: This paper contains examples of text which is toxic, biased, andharmful.</description><author>Manish Nagireddy, Lamogha Chiazor, Moninder Singh, Ioana Baldini</author><pubDate>Tue, 12 Dec 2023 18:27:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07492v1</guid></item><item><title>NearbyPatchCL: Leveraging Nearby Patches for Self-Supervised Patch-Level Multi-Class Classification in Whole-Slide Images</title><link>http://arxiv.org/abs/2312.07489v1</link><description>Whole-slide image (WSI) analysis plays a crucial role in cancer diagnosis andtreatment. In addressing the demands of this critical task, self-supervisedlearning (SSL) methods have emerged as a valuable resource, leveraging theirefficiency in circumventing the need for a large number of annotations, whichcan be both costly and time-consuming to deploy supervised methods.Nevertheless, patch-wise representation may exhibit instability in performance,primarily due to class imbalances stemming from patch selection within WSIs. Inthis paper, we introduce Nearby Patch Contrastive Learning (NearbyPatchCL), anovel self-supervised learning method that leverages nearby patches as positivesamples and a decoupled contrastive loss for robust representation learning.Our method demonstrates a tangible enhancement in performance for downstreamtasks involving patch-level multi-class classification. Additionally, we curatea new dataset derived from WSIs sourced from the Canine Cutaneous CancerHistology, thus establishing a benchmark for the rigorous evaluation ofpatch-level multi-class classification methodologies. Intensive experimentsshow that our method significantly outperforms the supervised baseline andstate-of-the-art SSL methods with top-1 classification accuracy of 87.56%. Ourmethod also achieves comparable results while utilizing a mere 1% of labeleddata, a stark contrast to the 100% labeled data requirement of otherapproaches. Source code: https://github.com/nvtien457/NearbyPatchCL</description><author>Gia-Bao Le, Van-Tien Nguyen, Trung-Nghia Le, Minh-Triet Tran</author><pubDate>Tue, 12 Dec 2023 18:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07489v1</guid></item><item><title>LMDrive: Closed-Loop End-to-End Driving with Large Language Models</title><link>http://arxiv.org/abs/2312.07488v1</link><description>Despite significant recent progress in the field of autonomous driving,modern methods still struggle and can incur serious accidents when encounteringlong-tail unforeseen events and challenging urban scenarios. On the one hand,large language models (LLM) have shown impressive reasoning capabilities thatapproach "Artificial General Intelligence". On the other hand, previousautonomous driving methods tend to rely on limited-format inputs (e.g. sensordata and navigation waypoints), restricting the vehicle's ability to understandlanguage information and interact with humans. To this end, this paperintroduces LMDrive, a novel language-guided, end-to-end, closed-loop autonomousdriving framework. LMDrive uniquely processes and integrates multi-modal sensordata with natural language instructions, enabling interaction with humans andnavigation software in realistic instructional settings. To facilitate furtherresearch in language-based closed-loop autonomous driving, we also publiclyrelease the corresponding dataset which includes approximately 64Kinstruction-following data clips, and the LangAuto benchmark that tests thesystem's ability to handle complex instructions and challenging drivingscenarios. Extensive closed-loop experiments are conducted to demonstrateLMDrive's effectiveness. To the best of our knowledge, we're the very firstwork to leverage LLMs for closed-loop end-to-end autonomous driving. Codes canbe found at https://github.com/opendilab/LMDrive</description><author>Hao Shao, Yuxuan Hu, Letian Wang, Steven L. Waslander, Yu Liu, Hongsheng Li</author><pubDate>Tue, 12 Dec 2023 18:24:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07488v1</guid></item><item><title>The Counterattack of CNNs in Self-Supervised Learning: Larger Kernel Size might be All You Need</title><link>http://arxiv.org/abs/2312.05695v2</link><description>Vision Transformers have been rapidly uprising in computer vision thanks totheir outstanding scaling trends, and gradually replacing convolutional neuralnetworks (CNNs). Recent works on self-supervised learning (SSL) introducesiamese pre-training tasks, on which Transformer backbones continue todemonstrate ever stronger results than CNNs. People come to believe thatTransformers or self-attention modules are inherently more suitable than CNNsin the context of SSL. However, it is noteworthy that most if not all priorarts of SSL with CNNs chose the standard ResNets as their backbones, whosearchitecture effectiveness is known to already lag behind advanced VisionTransformers. Therefore, it remains unclear whether the self-attentionoperation is crucial for the recent advances in SSL - or CNNs can deliver thesame excellence with more advanced designs, too? Can we close the SSLperformance gap between Transformers and CNNs? To answer these intriguingquestions, we apply self-supervised pre-training to the recently proposed,stronger lager-kernel CNN architecture and conduct an apple-to-apple comparisonwith Transformers, in their SSL performance. Our results show that we are ableto build pure CNN SSL architectures that perform on par with or better than thebest SSL-trained Transformers, by just scaling up convolutional kernel sizesbesides other small tweaks. Impressively, when transferring to the downstreamtasks \texttt{MS COCO} detection and segmentation, our SSL pre-trained CNNmodel (trained in 100 epochs) achieves the same good performance as the300-epoch pre-trained Transformer counterpart. We hope this work can help tobetter understand what is essential (or not) for self-supervised learningbackbones.</description><author>Tianjin Huang, Tianlong Chen, Zhangyang Wang, Shiwei Liu</author><pubDate>Tue, 12 Dec 2023 18:23:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05695v2</guid></item><item><title>MinD-3D: Reconstruct High-quality 3D objects in Human Brain</title><link>http://arxiv.org/abs/2312.07485v1</link><description>In this paper, we introduce Recon3DMind, a groundbreaking task focused onreconstructing 3D visuals from Functional Magnetic Resonance Imaging (fMRI)signals. This represents a major step forward in cognitive neuroscience andcomputer vision. To support this task, we present the fMRI-Shape dataset,utilizing 360-degree view videos of 3D objects for comprehensive fMRI signalcapture. Containing 55 categories of common objects from daily life, thisdataset will bolster future research endeavors. We also propose MinD-3D, anovel and effective three-stage framework that decodes and reconstructs thebrain's 3D visual information from fMRI signals. This method starts byextracting and aggregating features from fMRI frames using a neuro-fusionencoder, then employs a feature bridge diffusion model to generatecorresponding visual features, and ultimately recovers the 3D object through agenerative transformer decoder. Our experiments demonstrate that this methodeffectively extracts features that are valid and highly correlated with visualregions of interest (ROIs) in fMRI signals. Notably, it not only reconstructs3D objects with high semantic relevance and spatial similarity but alsosignificantly deepens our understanding of the human brain's 3D visualprocessing capabilities. Project page at: https://jianxgao.github.io/MinD-3D.</description><author>Jianxiong Gao, Yuqian Fu, Yun Wang, Xuelin Qian, Jianfeng Feng, Yanwei Fu</author><pubDate>Tue, 12 Dec 2023 18:21:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07485v1</guid></item><item><title>FoPro-KD: Fourier Prompted Effective Knowledge Distillation for Long-Tailed Medical Image Recognition</title><link>http://arxiv.org/abs/2305.17421v2</link><description>Representational transfer from publicly available models is a promisingtechnique for improving medical image classification, especially in long-taileddatasets with rare diseases. However, existing methods often overlook thefrequency-dependent behavior of these models, thereby limiting theireffectiveness in transferring representations and generalizations to rarediseases. In this paper, we propose FoPro-KD, a novel framework that leveragesthe power of frequency patterns learned from frozen pre-trained models toenhance their transferability and compression, presenting a few uniqueinsights: 1) We demonstrate that leveraging representations from publiclyavailable pre-trained models can substantially improve performance,specifically for rare classes, even when utilizing representations from asmaller pre-trained model. 2) We observe that pre-trained models exhibitfrequency preferences, which we explore using our proposed Fourier PromptGenerator (FPG), allowing us to manipulate specific frequencies in the inputimage, enhancing the discriminative representational transfer. 3) By amplifyingor diminishing these frequencies in the input image, we enable EffectiveKnowledge Distillation (EKD). EKD facilitates the transfer of knowledge frompre-trained models to smaller models. Through extensive experiments inlong-tailed gastrointestinal image recognition and skin lesion classification,where rare diseases are prevalent, our FoPro-KD framework outperforms existingmethods, enabling more accessible medical models for rare diseaseclassification. Code is available at https://github.com/xmed-lab/FoPro-KD.</description><author>Marawan Elbatel, Robert Martí, Xiaomeng Li</author><pubDate>Tue, 12 Dec 2023 18:14:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17421v2</guid></item><item><title>Classification of retail products: From probabilistic ranking to neural networks</title><link>http://arxiv.org/abs/2312.07482v1</link><description>Food retailing is now on an accelerated path to a success penetration intothe digital market by new ways of value creation at all stages of the consumerdecision process. One of the most important imperatives in this path is theavailability of quality data to feed all the process in digital transformation.But the quality of data is not so obvious if we consider the variety ofproducts and suppliers in the grocery market. Within this context of digitaltransformation of grocery industry, \textit{Midiadia} is Spanish data providercompany that works on converting data from the retailers' products intoknowledge with attributes and insights from the product labels, that is,maintaining quality data in a dynamic market with a high dispersion ofproducts. Currently, they manually categorize products (groceries) according tothe information extracted directly (text processing) from the product labellingand packaging. This paper introduces a solution to automatically categorize theconstantly changing product catalogue into a 3-level food taxonomy. Ourproposal studies three different approaches: a score-based ranking method,traditional machine learning algorithms, and deep neural networks. Thus, weprovide four different classifiers that support a more efficient and lesserror-prone maintenance of groceries catalogues, the main asset of the company.Finally, we have compared the performance of these three alternatives,concluding that traditional machine learning algorithms perform better, butclosely followed by the score-based approach.</description><author>Manar Mohamed Hafez, Rebeca P. Díaz Redondo, Ana Fernández-Vilas, Héctor Olivera Pazó</author><pubDate>Tue, 12 Dec 2023 18:11:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07482v1</guid></item><item><title>Convex Parameter Estimation of Perturbed Multivariate Generalized Gaussian Distributions</title><link>http://arxiv.org/abs/2312.07479v1</link><description>The multivariate generalized Gaussian distribution (MGGD), also known as themultivariate exponential power (MEP) distribution, is widely used in signal andimage processing. However, estimating MGGD parameters, which is required inpractical applications, still faces specific theoretical challenges. Inparticular, establishing convergence properties for the standard fixed-pointapproach when both the distribution mean and the scatter (or the precision)matrix are unknown is still an open problem. In robust estimation, imposingclassical constraints on the precision matrix, such as sparsity, has beenlimited by the non-convexity of the resulting cost function. This paper tacklesthese issues from an optimization viewpoint by proposing a convex formulationwith well-established convergence properties. We embed our analysis in a noisyscenario where robustness is induced by modelling multiplicative perturbations.The resulting framework is flexible as it combines a variety of regularizationsfor the precision matrix, the mean and model perturbations. This paper presentsproof of the desired theoretical properties, specifies the conditionspreserving these properties for different regularization choices and designs ageneral proximal primal-dual optimization strategy. The experiments show a moreaccurate precision and covariance matrix estimation with similar performancefor the mean vector parameter compared to Tyler's M-estimator. In ahigh-dimensional setting, the proposed method outperforms the classical GLASSO,one of its robust extensions, and the regularized Tyler's estimator.</description><author>Nora Ouzir, Frédéric Pascal, Jean-Christophe Pesquet</author><pubDate>Tue, 12 Dec 2023 18:08:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07479v1</guid></item><item><title>Double-Flow GAN model for the reconstruction of perceived faces from brain activities</title><link>http://arxiv.org/abs/2312.07478v1</link><description>Face plays an important role in human's visual perception, and reconstructingperceived faces from brain activities is challenging because of its difficultyin extracting high-level features and maintaining consistency of multiple faceattributes, such as expression, identity, gender, etc. In this study, weproposed a novel reconstruction framework, which we called Double-Flow GAN,that can enhance the capability of discriminator and handle imbalances inimages from certain domains that are too easy for generators. We also designeda pretraining process that uses features extracted from images as conditionsfor making it possible to pretrain the conditional reconstruction model fromfMRI in a larger pure image dataset. Moreover, we developed a simple pretrainedmodel to perform fMRI alignment to alleviate the problem of cross-subjectreconstruction due to the variations of brain structure among differentsubjects. We conducted experiments by using our proposed method andstate-of-the-art reconstruction models. Our results demonstrated that ourmethod showed significant reconstruction performance, outperformed the previousreconstruction models, and exhibited a good generation ability.</description><author>Zihao Wang, Jing Zhao, Hui Zhang</author><pubDate>Tue, 12 Dec 2023 18:07:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07478v1</guid></item><item><title>Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection</title><link>http://arxiv.org/abs/2312.07476v1</link><description>In-Context Learning (ICL) is an important paradigm for adapting LargeLanguage Models (LLMs) to downstream tasks through a few demonstrations.Despite the great success of ICL, the limitation of the demonstration numbermay lead to demonstration bias, i.e. the input-label mapping induced by LLMsmisunderstands the task's essence. Inspired by human experience, we attempt tomitigate such bias through the perspective of the inter-demonstrationrelationship. Specifically, we construct Comparable Demonstrations (CDs) byminimally editing the texts to flip the corresponding labels, in order tohighlight the task's essence and eliminate potential spurious correlationsthrough the inter-demonstration comparison. Through a series of experiments onCDs, we find that (1) demonstration bias does exist in LLMs, and CDs cansignificantly reduce such bias; (2) CDs exhibit good performance in ICL,especially in out-of-distribution scenarios. In summary, this study exploresthe ICL mechanisms from a novel perspective, providing a deeper insight intothe demonstration selection strategy for ICL.</description><author>Caoyun Fan, Jidong Tian, Yitian Li, Hao He, Yaohui Jin</author><pubDate>Tue, 12 Dec 2023 18:05:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07476v1</guid></item><item><title>MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception</title><link>http://arxiv.org/abs/2312.07472v1</link><description>It is a long-lasting goal to design an embodied system that can solvelong-horizon open-world tasks in human-like ways. However, existing approachesusually struggle with compound difficulties caused by the logic-awaredecomposition and context-aware execution of these tasks. To this end, weintroduce MP5, an open-ended multimodal embodied system built upon thechallenging Minecraft simulator, which can decompose feasible sub-objectives,design sophisticated situation-aware plans, and perform embodied actioncontrol, with frequent communication with a goal-conditioned active perceptionscheme. Specifically, MP5 is developed on top of recent advances in MultimodalLarge Language Models (MLLMs), and the system is modulated into functionalmodules that can be scheduled and collaborated to ultimately solve pre-definedcontext- and process-dependent tasks. Extensive experiments prove that MP5 canachieve a 22% success rate on difficult process-dependent tasks and a 91%success rate on tasks that heavily depend on the context. Moreover, MP5exhibits a remarkable ability to address many open-ended tasks that areentirely novel.</description><author>Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, Jing Shao</author><pubDate>Tue, 12 Dec 2023 17:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07472v1</guid></item><item><title>Detection of Unknown-Unknowns in Human-in-Plant Human-in-Loop Systems Using Physics Guided Process Models</title><link>http://arxiv.org/abs/2309.02603v2</link><description>Unknown-unknowns are operational scenarios in systems that are not accountedfor in the design and test phase. In such scenarios, the operational behaviorof the Human-in-loop (HIL) Human-in-Plant (HIP) systems is not guaranteed tomeet requirements such as safety and efficacy. We propose a novel framework foranalyzing the operational output characteristics of safety-critical HIL-HIPsystems that can discover unknown-unknown scenarios and evaluate potentialsafety hazards. We propose dynamics-induced hybrid recurrent neural networks(DiH-RNN) to mine a physics-guided surrogate model (PGSM) that checks fordeviation of the cyber-physical system (CPS) from safety-certified operationalcharacteristics. The PGSM enables early detection of unknown-unknowns based onthe physical laws governing the system. We demonstrate the detection ofoperational changes in an Artificial Pancreas(AP) due to unknown insulincartridge errors.</description><author>Aranyak Maity, Ayan Banerjee, Sandeep Gupta</author><pubDate>Tue, 12 Dec 2023 17:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02603v2</guid></item><item><title>Efficient Object Detection in Autonomous Driving using Spiking Neural Networks: Performance, Energy Consumption Analysis, and Insights into Open-set Object Discovery</title><link>http://arxiv.org/abs/2312.07466v1</link><description>Besides performance, efficiency is a key design driver of technologiessupporting vehicular perception. Indeed, a well-balanced trade-off betweenperformance and energy consumption is crucial for the sustainability ofautonomous vehicles. In this context, the diversity of real-world contexts inwhich autonomous vehicles can operate motivates the need for empoweringperception models with the capability to detect, characterize and identifynewly appearing objects by themselves. In this manuscript we elaborate on thisthreefold conundrum (performance, efficiency and open-world learning) forobject detection modeling tasks over image data collected from vehicularscenarios. Specifically, we show that well-performing and efficient models canbe realized by virtue of Spiking Neural Networks (SNNs), reaching competitivelevels of detection performance when compared to their non-spiking counterpartsat dramatic energy consumption savings (up to 85%) and a slightly improvedrobustness against image noise. Our experiments herein offered also exposequalitatively the complexity of detecting new objects based on the preliminaryresults of a simple approach to discriminate potential object proposals in thecaptured image.</description><author>Aitor Martinez Seras, Javier Del Ser, Pablo Garcia-Bringas</author><pubDate>Tue, 12 Dec 2023 17:47:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07466v1</guid></item><item><title>Empirical Validation of Conformal Prediction for Trustworthy Skin Lesions Classification</title><link>http://arxiv.org/abs/2312.07460v1</link><description>Uncertainty quantification is a pivotal field that contributes to therealization of reliable and robust systems. By providing complementaryinformation, it becomes instrumental in fortifying safe decisions, particularlywithin high-risk applications. Nevertheless, a comprehensive understanding ofthe advantages and limitations inherent in various methods within the medicalimaging field necessitates further research coupled with in-depth analysis. Inthis paper, we explore Conformal Prediction, an emerging distribution-freeuncertainty quantification technique, along with Monte Carlo Dropout andEvidential Deep Learning methods. Our comprehensive experiments provide acomparative performance analysis for skin lesion classification tasks acrossthe three quantification methods. Furthermore, We present insights into theeffectiveness of each method in handling Out-of-Distribution samples fromdomain-shifted datasets. Based on our experimental findings, our conclusionhighlights the robustness and consistent performance of conformal predictionacross diverse conditions. This positions it as the preferred choice fordecision-making in safety-critical applications.</description><author>Jamil Fayyad, Shadi Alijani, Homayoun Najjaran</author><pubDate>Tue, 12 Dec 2023 17:37:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07460v1</guid></item><item><title>Can LLM-Generated Misinformation Be Detected?</title><link>http://arxiv.org/abs/2309.13788v2</link><description>The advent of Large Language Models (LLMs) has made a transformative impact.However, the potential that LLMs such as ChatGPT can be exploited to generatemisinformation has posed a serious concern to online safety and public trust. Afundamental research question is: will LLM-generated misinformation cause moreharm than human-written misinformation? We propose to tackle this question fromthe perspective of detection difficulty. We first build a taxonomy ofLLM-generated misinformation. Then we categorize and validate the potentialreal-world methods for generating misinformation with LLMs. Then, throughextensive empirical investigation, we discover that LLM-generatedmisinformation can be harder to detect for humans and detectors compared tohuman-written misinformation with the same semantics, which suggests it canhave more deceptive styles and potentially cause more harm. We also discuss theimplications of our discovery on combating misinformation in the age of LLMsand the countermeasures.</description><author>Canyu Chen, Kai Shu</author><pubDate>Tue, 12 Dec 2023 17:35:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13788v2</guid></item><item><title>Dynamics Harmonic Analysis of Robotic Systems: Application in Data-Driven Koopman Modelling</title><link>http://arxiv.org/abs/2312.07457v1</link><description>We introduce the use of harmonic analysis to decompose the state space ofsymmetric robotic systems into orthogonal isotypic subspaces. These arelower-dimensional spaces that capture distinct, symmetric, and synergisticmotions. For linear dynamics, we characterize how this decomposition leads to asubdivision of the dynamics into independent linear systems on each subspace, aproperty we term dynamics harmonic analysis (DHA). To exploit this property, weuse Koopman operator theory to propose an equivariant deep-learningarchitecture that leverages the properties of DHA to learn a global linearmodel of system dynamics. Our architecture, validated on synthetic systems andthe dynamics of locomotion of a quadrupedal robot, demonstrates enhancedgeneralization, sample efficiency, and interpretability, with less trainableparameters and computational costs.</description><author>Daniel Ordoñez-Apraez, Vladimir Kostic, Giulio Turrisi, Pietro Novelli, Carlos Mastalli, Claudio Semini, Massimiliano Pontil</author><pubDate>Tue, 12 Dec 2023 17:34:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07457v1</guid></item><item><title>The Use of Multi-Scale Fiducial Markers To Aid Takeoff and Landing Navigation by Rotorcraft</title><link>http://arxiv.org/abs/2309.08769v3</link><description>This paper quantifies the performance of visual SLAM that leveragesmulti-scale fiducial markers (i.e., artificial landmarks that can be detectedat a wide range of distances) to show its potential for reliable takeoff andlanding navigation in rotorcraft. Prior work has shown that square markers witha black-and-white pattern of grid cells can be used to improve the performanceof visual SLAM with color cameras. We extend this prior work to allow nestedmarker layouts. We evaluate performance during semi-autonomous takeoff andlanding operations in a variety of environmental conditions by a DJI Matrice300 RTK rotorcraft with two FLIR Blackfly color cameras, using RTK GNSS toobtain ground truth pose estimates. Performance measures include absolutetrajectory error and the fraction of the number of estimated poses to the totalframe. We release all of our results -- our dataset and the code of theimplementation of the visual SLAM with fiducial markers -- to the public asopen-source.</description><author>Jongwon Lee, Su Yeon Choi, Timothy Bretl</author><pubDate>Tue, 12 Dec 2023 17:29:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08769v3</guid></item><item><title>Keystroke Verification Challenge (KVC): Biometric and Fairness Benchmark Evaluation</title><link>http://arxiv.org/abs/2311.06000v2</link><description>Analyzing keystroke dynamics (KD) for biometric verification has severaladvantages: it is among the most discriminative behavioral traits; keyboardsare among the most common human-computer interfaces, being the primary meansfor users to enter textual data; its acquisition does not require additionalhardware, and its processing is relatively lightweight; and it allows fortransparently recognizing subjects. However, the heterogeneity of experimentalprotocols and metrics, and the limited size of the databases adopted in theliterature impede direct comparisons between different systems, thusrepresenting an obstacle in the advancement of keystroke biometrics. Toalleviate this aspect, we present a new experimental framework to benchmarkKD-based biometric verification performance and fairness based on tweet-longsequences of variable transcript text from over 185,000 subjects, acquiredthrough desktop and mobile keyboards, extracted from the Aalto KeystrokeDatabases. The framework runs on CodaLab in the form of the KeystrokeVerification Challenge (KVC). Moreover, we also introduce a novel fairnessmetric, the Skewed Impostor Ratio (SIR), to capture inter- andintra-demographic group bias patterns in the verification scores. Wedemonstrate the usefulness of the proposed framework by employing twostate-of-the-art keystroke verification systems, TypeNet and TypeFormer, tocompare different sets of input features, achieving a less privacy-invasivesystem, by discarding the analysis of text content (ASCII codes of the keyspressed) in favor of extended features in the time domain. Our experiments showthat this approach allows to maintain satisfactory performance.</description><author>Giuseppe Stragapede, Ruben Vera-Rodriguez, Ruben Tolosana, Aythami Morales, Naser Damer, Julian Fierrez, Javier Ortega-Garcia</author><pubDate>Tue, 12 Dec 2023 17:19:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06000v2</guid></item><item><title>Highlighting Named Entities in Input for Auto-Formulation of Optimization Problems</title><link>http://arxiv.org/abs/2212.13201v3</link><description>Operations research deals with modeling and solving real-world problems asmathematical optimization problems. While solving mathematical systems isaccomplished by analytical software, formulating a problem as a set ofmathematical operations has been typically done manually by domain experts.Recent machine learning methods have shown promise in converting textualproblem descriptions to corresponding mathematical formulations. This paperpresents an approach that converts linear programming word problems intomathematical formulations. We leverage the named entities in the input andaugment the input to highlight these entities. Our approach achieves thehighest accuracy among all submissions to the NL4Opt Competition, securingfirst place in the generation track.</description><author>Neeraj Gangwar, Nickvash Kani</author><pubDate>Tue, 12 Dec 2023 17:18:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.13201v3</guid></item><item><title>BIRB: A Generalization Benchmark for Information Retrieval in Bioacoustics</title><link>http://arxiv.org/abs/2312.07439v1</link><description>The ability for a machine learning model to cope with differences in trainingand deployment conditions--e.g. in the presence of distribution shift or thegeneralization to new classes altogether--is crucial for real-world use cases.However, most empirical work in this area has focused on the image domain withartificial benchmarks constructed to measure individual aspects ofgeneralization. We present BIRB, a complex benchmark centered on the retrievalof bird vocalizations from passively-recorded datasets given focal recordingsfrom a large citizen science corpus available for training. We propose abaseline system for this collection of tasks using representation learning anda nearest-centroid search. Our thorough empirical evaluation and analysissurfaces open research directions, suggesting that BIRB fills the need for amore realistic and complex benchmark to drive progress on robustness todistribution shifts and generalization of ML models.</description><author>Jenny Hamer, Eleni Triantafillou, Bart van Merrienboer, Stefan Kahl, Holger Klinck, Tom Denton, Vincent Dumoulin</author><pubDate>Tue, 12 Dec 2023 17:06:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07439v1</guid></item><item><title>Medical Image Classification Using Transfer Learning and Chaos Game Optimization on the Internet of Medical Things</title><link>http://arxiv.org/abs/2312.07437v1</link><description>The Internet of Medical Things (IoMT) has dramatically benefited medicalprofessionals that patients and physicians can access from all regions.Although the automatic detection and prediction of diseases such as melanomaand leukemia is still being researched and studied in IoMT, existing approachesare not able to achieve a high degree of efficiency. Thus, with a new approachthat provides better results, patients would access the adequate treatmentsearlier and the death rate would be reduced. Therefore, this paper introducesan IoMT proposal for medical images classification that may be used anywhere,i.e. it is an ubiquitous approach. It was design in two stages: first, weemploy a Transfer Learning (TL)-based method for feature extraction, which iscarried out using MobileNetV3; second, we use the Chaos Game Optimization (CGO)for feature selection, with the aim of excluding unnecessary features andimproving the performance, which is key in IoMT. Our methodology was evaluatedusing ISIC-2016, PH2, and Blood-Cell datasets. The experimental resultsindicated that the proposed approach obtained an accuracy of 88.39% onISIC-2016, 97.52% on PH2, and 88.79% on Blood-cell. Moreover, our approach hadsuccessful performances for the metrics employed compared to other existingmethods.</description><author>Alhassan Mabrouk, Abdelghani Dahou, Mohamed Abd Elaziz, Rebeca P. Díaz Redondo, Mohammed Kayed</author><pubDate>Tue, 12 Dec 2023 17:04:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07437v1</guid></item><item><title>Cross-modal Contrastive Learning with Asymmetric Co-attention Network for Video Moment Retrieval</title><link>http://arxiv.org/abs/2312.07435v1</link><description>Video moment retrieval is a challenging task requiring fine-grainedinteractions between video and text modalities. Recent work in image-textpretraining has demonstrated that most existing pretrained models suffer frominformation asymmetry due to the difference in length between visual andtextual sequences. We question whether the same problem also exists in thevideo-text domain with an auxiliary need to preserve both spatial and temporalinformation. Thus, we evaluate a recently proposed solution involving theaddition of an asymmetric co-attention network for video grounding tasks.Additionally, we incorporate momentum contrastive loss for robust,discriminative representation learning in both modalities. We note that theintegration of these supplementary modules yields better performance comparedto state-of-the-art models on the TACoS dataset and comparable results onActivityNet Captions, all while utilizing significantly fewer parameters withrespect to baseline.</description><author>Love Panta, Prashant Shrestha, Brabeem Sapkota, Amrita Bhattarai, Suresh Manandhar, Anand Kumar Sah</author><pubDate>Tue, 12 Dec 2023 17:00:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07435v1</guid></item><item><title>Multi-Modal Conformal Prediction Regions by Optimizing Convex Shape Templates</title><link>http://arxiv.org/abs/2312.07434v1</link><description>Conformal prediction is a statistical tool for producing prediction regionsfor machine learning models that are valid with high probability. A keycomponent of conformal prediction algorithms is a non-conformity score functionthat quantifies how different a model's prediction is from the unknown groundtruth value. Essentially, these functions determine the shape and the size ofthe conformal prediction regions. However, little work has gone into findingnon-conformity score functions that produce prediction regions that aremulti-modal and practical, i.e., that can efficiently be used in engineeringapplications. We propose a method that optimizes parameterized shape templatefunctions over calibration data, which results in non-conformity scorefunctions that produce prediction regions with minimum volume. Our approachresults in prediction regions that are multi-modal, so they can properlycapture residuals of distributions that have multiple modes, and practical, soeach region is convex and can be easily incorporated into downstream tasks,such as a motion planner using conformal prediction regions. Our method appliesto general supervised learning tasks, while we illustrate its use intime-series prediction. We provide a toolbox and present illustrative casestudies of F16 fighter jets and autonomous vehicles, showing an up to $68\%$reduction in prediction region area.</description><author>Renukanandan Tumu, Matthew Cleaveland, Rahul Mangharam, George J. Pappas, Lars Lindemann</author><pubDate>Tue, 12 Dec 2023 17:00:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07434v1</guid></item><item><title>Non-monotone Sequential Submodular Maximization</title><link>http://arxiv.org/abs/2308.08641v2</link><description>In this paper, we study a fundamental problem in submodular optimization,which is called sequential submodular maximization. Specifically, we aim toselect and rank a group of $k$ items from a ground set $V$ such that theweighted summation of $k$ (possibly non-monotone) submodular functions $f_1,\cdots ,f_k: 2^V \rightarrow \mathbb{R}^+$ is maximized, here each function$f_j$ takes the first $j$ items from this sequence as input. The existingresearch on sequential submodular maximization has predominantly concentratedon the monotone setting, assuming that the submodular functions arenon-decreasing. However, in various real-world scenarios, like diversity-awarerecommendation systems, adding items to an existing set might negatively impactthe overall utility. In response, this paper pioneers the examination of theaforementioned problem with non-monotone submodular functions and offerseffective solutions for both flexible and fixed length constraints, as well asa special case with identical utility functions. The empirical evaluationsfurther validate the effectiveness of our proposed algorithms in the domain ofvideo recommendations. The results of this research have implications invarious fields, including recommendation systems and assortment optimization,where the ordering of items significantly impacts the overall value obtained.</description><author>Shaojie Tang, Jing Yuan</author><pubDate>Tue, 12 Dec 2023 16:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08641v2</guid></item><item><title>Ensemble Federated Learning: an approach for collaborative pneumonia diagnosis</title><link>http://arxiv.org/abs/2312.07428v1</link><description>Federated learning is a very convenient approach for scenarios where (i) theexchange of data implies privacy concerns and/or (ii) a quick reaction isneeded. In smart healthcare systems, both aspects are usually required. In thispaper, we work on the first scenario, where preserving privacy is key and,consequently, building a unique and massive medical image data set by fusingdifferent data sets from different medical institutions or research centers(computation nodes) is not an option. We propose an ensemble federated learning(EFL) approach that is based on the following characteristics: First, eachcomputation node works with a different data set (but of the same type). Theywork locally and apply an ensemble approach combining eight well-known CNNmodels (densenet169, mobilenetv2, xception, inceptionv3, vgg16, resnet50,densenet121, and resnet152v2) on Chest X-ray images. Second, the best two localmodels are used to create a local ensemble model that is shared with a centralnode. Third, the ensemble models are aggregated to obtain a global model, whichis shared with the computation nodes to continue with a new iteration. Thisprocedure continues until there are no changes in the best local models. Wehave performed different experiments to compare our approach with centralizedones (with or without an ensemble approach)\color{black}. The results concludethat our proposal outperforms these ones in Chest X-ray images (achieving anaccuracy of 96.63\%) and offers very competitive results compared to otherproposals in the literature.</description><author>Alhassan Mabrouk, Rebeca P. Díaz Redondo, Mohamed Abd Elaziz, Mohammed Kayed</author><pubDate>Tue, 12 Dec 2023 16:53:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07428v1</guid></item><item><title>Deep Internal Learning: Deep Learning from a Single Input</title><link>http://arxiv.org/abs/2312.07425v1</link><description>Deep learning in general focuses on training a neural network from largelabeled datasets. Yet, in many cases there is value in training a network justfrom the input at hand. This may involve training a network from scratch usinga single input or adapting an already trained network to a provided inputexample at inference time. This survey paper aims at covering deepinternal-learning techniques that have been proposed in the past few years forthese two important directions. While our main focus will be on imageprocessing problems, most of the approaches that we survey are derived forgeneral signals (vectors with recurring patterns that can be distinguished fromnoise) and are therefore applicable to other modalities. We believe that thetopic of internal-learning is very important in many signal and imageprocessing problems where training data is scarce and diversity is large on theone hand, and on the other, there is a lot of structure in the data that can beexploited.</description><author>Tom Tirer, Raja Giryes, Se Young Chun, Yonina C. Eldar</author><pubDate>Tue, 12 Dec 2023 16:48:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07425v1</guid></item><item><title>How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation</title><link>http://arxiv.org/abs/2312.07424v1</link><description>In machine learning, generalization against distribution shifts -- wheredeployment conditions diverge from the training scenarios -- is crucial,particularly in fields like climate modeling, biomedicine, and autonomousdriving. The emergence of foundation models, distinguished by their extensivepretraining and task versatility, has led to an increased interest in theiradaptability to distribution shifts. GPT-4V(ision) acts as the most advancedpublicly accessible multimodal foundation model, with extensive applicationsacross various domains, including anomaly detection, video understanding, imagegeneration, and medical diagnosis. However, its robustness against datadistributions remains largely underexplored. Addressing this gap, this studyrigorously evaluates GPT-4V's adaptability and generalization capabilities indynamic environments, benchmarking against prominent models like CLIP andLLaVA. We delve into GPT-4V's zero-shot generalization across 13 diversedatasets spanning natural, medical, and molecular domains. We furtherinvestigate its adaptability to controlled data perturbations and examine theefficacy of in-context learning as a tool to enhance its adaptation. Ourfindings delineate GPT-4V's capability boundaries in distribution shifts,shedding light on its strengths and limitations across various scenarios.Importantly, this investigation contributes to our understanding of how AIfoundation models generalize to distribution shifts, offering pivotal insightsinto their adaptability and robustness. Code is publicly available athttps://github.com/jameszhou-gl/gpt-4v-distribution-shift.</description><author>Zhongyi Han, Guanglin Zhou, Rundong He, Jindong Wang, Xing Xie, Tailin Wu, Yilong Yin, Salman Khan, Lina Yao, Tongliang Liu, Kun Zhang</author><pubDate>Tue, 12 Dec 2023 16:48:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07424v1</guid></item><item><title>Holoported Characters: Real-time Free-viewpoint Rendering of Humans from Sparse RGB Cameras</title><link>http://arxiv.org/abs/2312.07423v1</link><description>We present the first approach to render highly realistic free-viewpointvideos of a human actor in general apparel, from sparse multi-view recording todisplay, in real-time at an unprecedented 4K resolution. At inference, ourmethod only requires four camera views of the moving actor and the respective3D skeletal pose. It handles actors in wide clothing, and reproduces evenfine-scale dynamic detail, e.g. clothing wrinkles, face expressions, and handgestures. At training time, our learning-based approach expects densemulti-view video and a rigged static surface scan of the actor. Our methodcomprises three main stages. Stage 1 is a skeleton-driven neural approach forhigh-quality capture of the detailed dynamic mesh geometry. Stage 2 is a novelsolution to create a view-dependent texture using four test-time camera viewsas input. Finally, stage 3 comprises a new image-based refinement networkrendering the final 4K image given the output from the previous stages. Ourapproach establishes a new benchmark for real-time rendering resolution andquality using sparse input camera views, unlocking possibilities for immersivetelepresence.</description><author>Ashwath Shetty, Marc Habermann, Guoxing Sun, Diogo Luvizon, Vladislav Golyanik, Christian Theobalt</author><pubDate>Tue, 12 Dec 2023 16:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07423v1</guid></item><item><title>FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs</title><link>http://arxiv.org/abs/2312.07420v1</link><description>Training large language models (LLMs) is a costly endeavour in terms of timeand computational resources. The large amount of training data used during theunsupervised pre-training phase makes it difficult to verify all data and,unfortunately, undesirable data may be ingested during training. Re-trainingfrom scratch is impractical and has led to the creation of the 'unlearning'discipline where models are modified to "unlearn" undesirable informationwithout retraining. However, any modification can alter the behaviour of LLMs,especially on key dimensions such as fairness. This is the first work thatexamines this interplay between unlearning and fairness for LLMs. Inparticular, we focus on a popular unlearning framework known as SISA [Bourtouleet al., 2021], which creates an ensemble of models trained on disjoint shards.We evaluate the performance-fairness trade-off for SISA, and empiricallydemsontrate that SISA can indeed reduce fairness in LLMs. To remedy this, wepropose post-processing bias mitigation techniques for ensemble models producedby SISA. We adapt the post-processing fairness improvement technique from[Hardt et al., 2016] to design three methods that can handle model ensembles,and prove that one of the methods is an optimal fair predictor for ensemble ofmodels. Through experimental results, we demonstrate the efficacy of ourpost-processing framework called 'FairSISA'.</description><author>Swanand Ravindra Kadhe, Anisa Halimi, Ambrish Rawat, Nathalie Baracaldo</author><pubDate>Tue, 12 Dec 2023 16:44:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07420v1</guid></item><item><title>Risk Preferences of Learning Algorithms</title><link>http://arxiv.org/abs/2205.04619v3</link><description>Agents' learning from feedback shapes economic outcomes, and many economicdecision-makers today employ learning algorithms to make consequential choices.This note shows that a widely used learning algorithm, $\varepsilon$-Greedy,exhibits emergent risk aversion: it prefers actions with lower variance. Whenpresented with actions of the same expectation, under a wide range ofconditions, $\varepsilon$-Greedy chooses the lower-variance action withprobability approaching one. This emergent preference can have wide-rangingconsequences, ranging from concerns about fairness to homogenization, and holdstransiently even when the riskier action has a strictly higher expected payoff.We discuss two methods to correct this bias. The first method requires thealgorithm to reweight data as a function of how likely the actions were to bechosen. The second requires the algorithm to have optimistic estimates ofactions for which it has not collected much data. We show that risk-neutralityis restored with these corrections.</description><author>Andreas Haupt, Aroon Narayanan</author><pubDate>Tue, 12 Dec 2023 16:43:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.04619v3</guid></item><item><title>Towards Faster k-Nearest-Neighbor Machine Translation</title><link>http://arxiv.org/abs/2312.07419v1</link><description>Recent works have proven the effectiveness of k-nearest-neighbor machinetranslation(a.k.a kNN-MT) approaches to produce remarkable improvement incross-domain translations. However, these models suffer from heavy retrieveoverhead on the entire datastore when decoding each token. We observe thatduring the decoding phase, about 67% to 84% of tokens are unvaried aftersearching over the corpus datastore, which means most of the tokens causefutile retrievals and introduce unnecessary computational costs by initiatingk-nearest-neighbor searches. We consider this phenomenon is explainable inlinguistics and propose a simple yet effective multi-layer perceptron (MLP)network to predict whether a token should be translated jointly by the neuralmachine translation model and probabilities produced by the kNN or just by theneural model. The results show that our method succeeds in reducing redundantretrieval operations and significantly reduces the overhead of kNN retrievalsby up to 53% at the expense of a slight decline in translation quality.Moreover, our method could work together with all existing kNN-MT systems.</description><author>Xiangyu Shi, Yunlong Liang, Jinan Xu, Yufeng Chen</author><pubDate>Tue, 12 Dec 2023 16:41:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07419v1</guid></item><item><title>Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling</title><link>http://arxiv.org/abs/2308.06077v2</link><description>Generative language models (LMs) have become omnipresent across data science.For a wide variety of tasks, inputs can be phrased as natural language promptsfor an LM, from whose output the solution can then be extracted. LM performancehas consistently been increasing with model size - but so has the monetary costof querying the ever larger models. Importantly, however, not all inputs areequally hard: some require larger LMs for obtaining a satisfactory solution,whereas for others smaller LMs suffice. Based on this fact, we design aframework for Cost-Effective Language Model Choice (CELMOC). Given a set ofinputs and a set of candidate LMs, CELMOC judiciously assigns each input to anLM predicted to do well on the input according to a so-called meta-model,aiming to achieve high overall performance at low cost. The cost-performancetrade-off can be flexibly tuned by the user. Options include, among others,maximizing total expected performance (or the number of processed inputs) whilestaying within a given cost budget, or minimizing total cost while processingall inputs. We evaluate CELMOC on 14 datasets covering five natural languagetasks, using four candidate LMs of vastly different size and cost. With CELMOC,we match the performance of the largest available LM while achieving a costreduction of 63%. Via our publicly available library, researchers as well aspractitioners can thus save large amounts of money without sacrificingperformance.</description><author>Marija Šakota, Maxime Peyrard, Robert West</author><pubDate>Tue, 12 Dec 2023 16:39:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06077v2</guid></item><item><title>Attention Based Encoder Decoder Model for Video Captioning in Nepali (2023)</title><link>http://arxiv.org/abs/2312.07418v1</link><description>Video captioning in Nepali, a language written in the Devanagari script,presents a unique challenge due to the lack of existing academic work in thisdomain. This work develops a novel encoder-decoder paradigm for Nepali videocaptioning to tackle this difficulty. LSTM and GRU sequence-to-sequence modelsare used in the model to produce related textual descriptions based on featuresretrieved from video frames using CNNs. Using Google Translate and manualpost-editing, a Nepali video captioning dataset is generated from the MicrosoftResearch Video Description Corpus (MSVD) dataset created using GoogleTranslate, and manual post-editing work. The efficacy of the model forDevanagari-scripted video captioning is demonstrated by BLEU, METOR, and ROUGEmeasures, which are used to assess its performance.</description><author>Kabita Parajuli, Shashidhar Ram Joshi</author><pubDate>Tue, 12 Dec 2023 16:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07418v1</guid></item><item><title>QSMVM: QoS-aware and social-aware multimetric routing protocol for video-streaming services over MANETs</title><link>http://arxiv.org/abs/2312.07414v1</link><description>A mobile ad hoc network (MANET) is a set of autonomous mobile devicesconnected by wireless links in a distributed manner and without a fixedinfrastructure. Real-time multimedia services, such as video-streaming overMANETs, offers very promising applications, e.g. two members of a group oftourists who want to share a video transmitted through the MANET they form; avideo-streaming service deployed over a MANET where users watch a film; amongother examples. On the other hand, social web technologies, where peopleactively interact online with others through social networks, are leading to asocialization of networks. Information of interaction among users is being usedto provide socially-enhanced software. To achieve this, we need to know thestrength of the relationship between a given user and each user they interactwith. This strength of the relationship can be measured through a conceptcalled tie strength (TS), first introduced by Mark Granovetter in 1973. In thisarticle, we modify our previous proposal named multipath multimedia dynamicsource routing (MMDSR) protocol to include a social metric TS in the decisionstaken by the forwarding algorithm. We find a trade-off between the quality ofservice (QoS) and the trust level between users who form the forwarding path inthe MANET. Our goal is to increase the trust metric while the QoS is notaffected significantly.</description><author>Efraín Palacios Jara, Ahmad Mohamad Mezhe, Mónica Aguilar Igartua, Rebeca P. Díaz Redondo, Ana Fernández Vilas</author><pubDate>Tue, 12 Dec 2023 16:34:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07414v1</guid></item><item><title>AI capabilities can be significantly improved without expensive retraining</title><link>http://arxiv.org/abs/2312.07413v1</link><description>State-of-the-art AI systems can be significantly improved without expensiveretraining via "post-training enhancements"-techniques applied after initialtraining like fine-tuning the system to use a web browser. We review recentpost-training enhancements, categorizing them into five types: tool-use,prompting methods, scaffolding, solution selection, and data generation.Different enhancements improve performance on different tasks, making it hardto compare their significance. So we translate improvements from differentenhancements into a common currency, the compute-equivalent gain: how muchadditional training compute would be needed to improve performance by the sameamount as the enhancement. Our non-experimental work shows that post-trainingenhancements have significant benefits: most surveyed enhancements improvebenchmark performance by more than a 5x increase in training compute, some bymore than 20x. Post-training enhancements are relatively cheap to develop:fine-tuning costs are typically &lt;1% of the original training cost. Governingthe development of capable post-training enhancements may be challengingbecause frontier models could be enhanced by a wide range of actors.</description><author>Tom Davidson, Jean-Stanislas Denain, Pablo Villalobos, Guillem Bas</author><pubDate>Tue, 12 Dec 2023 16:34:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07413v1</guid></item><item><title>Forecasting Intraday Power Output by a Set of PV Systems using Recurrent Neural Networks and Physical Covariates</title><link>http://arxiv.org/abs/2303.08459v2</link><description>Accurate intraday forecasts of the power output by PhotoVoltaic (PV) systemsare critical to improve the operation of energy distribution grids. We describea neural autoregressive model which aims at performing such intraday forecasts.We build upon a physical, deterministic PV performance model, the output ofwhich being used as covariates in the context of the neural model. In addition,our application data relates to a geographically distributed set of PV systems.We address all PV sites with a single neural model, which embeds theinformation about the PV site in specific covariates. We use a scale-freeapproach which does rely on explicit modelling of seasonal effects. Ourproposal repurposes a model initially used in the retail sector, and disclosesa novel truncated Gaussian output distribution. An ablation study and acomparison to alternative architectures from the literature shows that thecomponents in the best performing proposed model variant work synergisticallyto reach a skill score of 15.72% with respect to the physical model, used as abaseline.</description><author>Pierrick Bruneau, David Fiorelli, Christian Braun, Daniel Koster</author><pubDate>Tue, 12 Dec 2023 16:31:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08459v2</guid></item><item><title>DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing</title><link>http://arxiv.org/abs/2312.07409v1</link><description>Diffusion models have achieved remarkable image generation quality surpassingprevious generative models. However, a notable limitation of diffusion models,in comparison to GANs, is their difficulty in smoothly interpolating betweentwo image samples, due to their highly unstructured latent space. Such a smoothinterpolation is intriguing as it naturally serves as a solution for the imagemorphing task with many applications. In this work, we present DiffMorpher, thefirst approach enabling smooth and natural image interpolation using diffusionmodels. Our key idea is to capture the semantics of the two images by fittingtwo LoRAs to them respectively, and interpolate between both the LoRAparameters and the latent noises to ensure a smooth semantic transition, wherecorrespondence automatically emerges without the need for annotation. Inaddition, we propose an attention interpolation and injection technique and anew sampling schedule to further enhance the smoothness between consecutiveimages. Extensive experiments demonstrate that DiffMorpher achieves starklybetter image morphing effects than previous methods across a variety of objectcategories, bridging a critical functional gap that distinguished diffusionmodels from GANs.</description><author>Kaiwen Zhang, Yifan Zhou, Xudong Xu, Xingang Pan, Bo Dai</author><pubDate>Tue, 12 Dec 2023 16:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07409v1</guid></item><item><title>District-scale surface temperatures generated from high-resolution longitudinal thermal infrared images</title><link>http://arxiv.org/abs/2305.01971v2</link><description>The paper describes a dataset that was collected by infrared thermography,which is a non-contact, non-intrusive technique to collect data and analyze thebuilt environment in various aspects. While most studies focus on the city andbuilding scales, the rooftop observatory provides high temporal and spatialresolution observations with dynamic interactions on the district scale. Therooftop infrared thermography observatory with a multi-modal platform that iscapable of assessing a wide range of dynamic processes in urban systems wasdeployed in Singapore. It was placed on the top of two buildings that overlookthe outdoor context of the campus of the National University of Singapore. Theplatform collects remote sensing data from tropical areas on a temporal scale,allowing users to determine the temperature trend of individual features suchas buildings, roads, and vegetation. The dataset includes 1,365,921 thermalimages collected on average at approximately 10 seconds intervals from twolocations during ten months.</description><author>Subin Lin, Vasantha Ramani, Miguel Martin, Pandarasamy Arjunan, Adrian Chong, Filip Biljecki, Marcel Ignatius, Kameshwar Poolla, Clayton Miller</author><pubDate>Tue, 12 Dec 2023 16:27:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01971v2</guid></item><item><title>Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Models</title><link>http://arxiv.org/abs/2312.07408v1</link><description>Vision-Language Large Models (VLMs) have become primary backbone of AI, dueto the impressive performance. However, their expensive computation costs,i.e., throughput and delay, impede potentials in real-world scenarios. Toachieve acceleration for VLMs, most existing methods focus on the modelperspective: pruning, distillation, quantification, but completely overlook thedata-perspective redundancy. To fill the overlook, this paper pioneers theseverity of data redundancy, and designs one plug-and-play Turbo module guidedby information degree to prune inefficient tokens from visual or textual data.In pursuit of efficiency-performance trade-offs, information degree takes twokey factors into consideration: mutual redundancy and semantic value.Concretely, the former evaluates the data duplication between sequentialtokens; while the latter evaluates each token by its contribution to theoverall semantics. As a result, tokens with high information degree carry lessredundancy and stronger semantics. For VLMs' calculation, Turbo works as auser-friendly plug-in that sorts data referring to information degree,utilizing only top-level ones to save costs. Its advantages are multifaceted,e.g., being generally compatible to various VLMs across understanding andgeneration, simple use without retraining and trivial engineering efforts. Onmultiple public VLMs benchmarks, we conduct extensive experiments to reveal thegratifying acceleration of Turbo, under negligible performance drop.</description><author>Chen Ju, Haicheng Wang, Zeqian Li, Xu Chen, Zhonghua Zhai, Weilin Huang, Shuai Xiao</author><pubDate>Tue, 12 Dec 2023 16:27:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07408v1</guid></item><item><title>ICL Markup: Structuring In-Context Learning using Soft-Token Tags</title><link>http://arxiv.org/abs/2312.07405v1</link><description>Large pretrained language models (LLMs) can be rapidly adapted to a widevariety of tasks via a text-to-text approach, where the instruction and inputare fed to the model in natural language. Combined with in-context learning(ICL), this paradigm is impressively flexible and powerful. However, it alsoburdens users with an overwhelming number of choices, many of them arbitrary.Inspired by markup languages like HTML, we contribute a method of usingsoft-token tags to compose prompt templates. This approach reduces arbitrarydecisions and streamlines the application of ICL. Our method is a form ofmeta-learning for ICL; it learns these tags in advance during aparameter-efficient fine-tuning ``warm-up'' process. The tags can subsequentlybe used in templates for ICL on new, unseen tasks without any additionalfine-tuning. Our experiments with this approach yield promising initialresults, improving LLM performance on important enterprise applications such asfew-shot and open-world intent detection, as well as text classification innews and legal domains.</description><author>Marc-Etienne Brunet, Ashton Anderson, Richard Zemel</author><pubDate>Tue, 12 Dec 2023 16:25:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07405v1</guid></item><item><title>Reconstructing Turbulent Flows Using Physics-Aware Spatio-Temporal Dynamics and Test-Time Refinement</title><link>http://arxiv.org/abs/2304.12130v3</link><description>Simulating turbulence is critical for many societally important applicationsin aerospace engineering, environmental science, the energy industry, andbiomedicine. Large eddy simulation (LES) has been widely used as an alternativeto direct numerical simulation (DNS) for simulating turbulent flows due to itsreduced computational cost. However, LES is unable to capture all of the scalesof turbulent transport accurately. Reconstructing DNS from low-resolution LESis critical for many scientific and engineering disciplines, but it poses manychallenges to existing super-resolution methods due to the spatio-temporalcomplexity of turbulent flows. In this work, we propose a new physics-guidedneural network for reconstructing the sequential DNS from low-resolution LESdata. The proposed method leverages the partial differential equation thatunderlies the flow dynamics in the design of spatio-temporal modelarchitecture. A degradation-based refinement method is also developed toenforce physical constraints and further reduce the accumulated reconstructionerrors over long periods. The results on two different types of turbulent flowdata confirm the superiority of the proposed method in reconstructing thehigh-resolution DNS data and preserving the physical characteristics of flowtransport.</description><author>Shengyu Chen, Tianshu Bao, Peyman Givi, Can Zheng, Xiaowei Jia</author><pubDate>Tue, 12 Dec 2023 16:18:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12130v3</guid></item><item><title>On Diverse Preferences for Large Language Model Alignment</title><link>http://arxiv.org/abs/2312.07401v1</link><description>The alignment of large language models (LLMs) with human values is crucialfor the development of artificial general intelligence (AGI). One promisingapproach to achieve this alignment is reinforcement learning from humanfeedback, which employs a reward model (RM) learned from human preferencedatasets to guide LLMs in generating text that aligns with human preferences.Through intensive experiments and analysis of reward distribution, this paperfinds that preference datasets are diverse from each other, even though theyare all proposed to align human preference. Hence, mixing diverse humanpreference datasets to increase data size for enhancing reward modeling couldfail. To address the issue and capture the shared human values from diversepreferences, a new training policy called MORE is introduced, which minimizespreference bias by adaptively adjusting the preference objective across diversepreferences. Experiments with the Pythia-1.4B model and five mixed preferencedatasets show that MORE achieves superior reward accuracy and lower calibrationerror, highlighting its ability to leverage diverse human preference data.</description><author>Dun Zeng, Yong Dai, Pengyu Cheng, Tianhao Hu, Wanshun Chen, Nan Du, Zenglin Xu</author><pubDate>Tue, 12 Dec 2023 16:17:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07401v1</guid></item><item><title>Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales</title><link>http://arxiv.org/abs/2312.07399v1</link><description>Machine reasoning has made great progress in recent years owing to largelanguage models (LLMs). In the clinical domain, however, most NLP-drivenprojects mainly focus on clinical classification or reading comprehension, andunder-explore clinical reasoning for disease diagnosis due to the expensiverationale annotation with clinicians. In this work, we present a``reasoning-aware'' diagnosis framework that rationalizes the diagnosticprocess via prompt-based learning in a time- and labor-efficient manner, andlearns to reason over the prompt-generated rationales. Specifically, we addressthe clinical reasoning for disease diagnosis, where the LLM generatesdiagnostic rationales providing its insight on presented patient data and thereasoning path towards the diagnosis, namely Clinical Chain-of-Thought(Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinicalreasoning via extensive experiments and analyses on both rationale generationand disease diagnosis in various settings. We further propose a novel set ofcriteria for evaluating machine-generated rationales' potential for real-worldclinical settings, facilitating and benefiting future research in this area.</description><author>Taeyoon Kwon, Kai Tzu-iunn Ong, Dongjin Kang, Seungjun Moon, Jeong Ryong Lee, Dosik Hwang, Yongsik Sim, Beomseok Sohn, Dongha Lee, Jinyoung Yeo</author><pubDate>Tue, 12 Dec 2023 16:14:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07399v1</guid></item><item><title>LLMEval: A Preliminary Study on How to Evaluate Large Language Models</title><link>http://arxiv.org/abs/2312.07398v1</link><description>Recently, the evaluation of Large Language Models has emerged as a populararea of research. The three crucial questions for LLM evaluation are ``what,where, and how to evaluate''. However, the existing research mainly focuses onthe first two questions, which are basically what tasks to give the LLM duringtesting and what kind of knowledge it should deal with. As for the thirdquestion, which is about what standards to use, the types of evaluators, how toscore, and how to rank, there hasn't been much discussion. In this paper, weanalyze evaluation methods by comparing various criteria with both manual andautomatic evaluation, utilizing onsite, crowd-sourcing, public annotators andGPT-4, with different scoring methods and ranking systems. We propose a newdataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186individuals participated, leading to the generation of 243,337 manualannotations and 57,511 automatic evaluation results. We perform comparisons andanalyses of different settings and conduct 10 conclusions that can provide someinsights for evaluating LLM in the future. The dataset and the results arepublicly available at https://github.com/llmeval .</description><author>Yue Zhang, Ming Zhang, Haipeng Yuan, Shichun Liu, Yongyao Shi, Tao Gui, Qi Zhang, Xuanjing Huang</author><pubDate>Tue, 12 Dec 2023 16:14:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07398v1</guid></item><item><title>A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames</title><link>http://arxiv.org/abs/2312.07395v1</link><description>Understanding long, real-world videos requires modeling of long-range visualdependencies. To this end, we explore video-first architectures, building onthe common paradigm of transferring large-scale, image--text models to videovia shallow temporal fusion. However, we expose two limitations to theapproach: (1) decreased spatial capabilities, likely due to poorvideo--language alignment in standard video datasets, and (2) higher memoryconsumption, bottlenecking the number of frames that can be processed. Tomitigate the memory bottleneck, we systematically analyze the memory/accuracytrade-off of various efficient methods: factorized attention,parameter-efficient image-to-video adaptation, input masking, andmulti-resolution patchification. Surprisingly, simply masking large portions ofthe video (up to 75%) during contrastive pre-training proves to be one of themost robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Oursimple approach for training long video-to-text models, which scales to 1Bparameters, does not add new architectural complexity and is able to outperformthe popular paradigm of using much larger LLMs as an information aggregatorover segment-based information on benchmarks with long-range temporaldependencies (YouCook2, EgoSchema).</description><author>Pinelopi Papalampidi, Skanda Koppula, Shreya Pathak, Justin Chiu, Joe Heyward, Viorica Patraucean, Jiajun Shen, Antoine Miech, Andrew Zisserman, Aida Nematzdeh</author><pubDate>Tue, 12 Dec 2023 16:10:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07395v1</guid></item><item><title>VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution</title><link>http://arxiv.org/abs/2306.12424v3</link><description>We introduce VisoGender, a novel dataset for benchmarking gender bias invision-language models. We focus on occupation-related biases within ahegemonic system of binary gender, inspired by Winograd and Winogender schemas,where each image is associated with a caption containing a pronoun relationshipof subjects and objects in the scene. VisoGender is balanced by genderrepresentation in professional roles, supporting bias evaluation in two ways:i) resolution bias, where we evaluate the difference between pronoun resolutionaccuracies for image subjects with gender presentations perceived as masculineversus feminine by human annotators and ii) retrieval bias, where we compareratios of professionals perceived to have masculine and feminine genderpresentations retrieved for a gender-neutral search query. We benchmark severalstate-of-the-art vision-language models and find that they demonstrate bias inresolving binary gender in complex scenes. While the direction and magnitude ofgender bias depends on the task and the model being evaluated, captioningmodels are generally less biased than Vision-Language Encoders. Dataset andcode are available at https://github.com/oxai/visogender</description><author>Siobhan Mackenzie Hall, Fernanda Gonçalves Abrantes, Hanwen Zhu, Grace Sodunke, Aleksandar Shtedritski, Hannah Rose Kirk</author><pubDate>Tue, 12 Dec 2023 16:08:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12424v3</guid></item><item><title>PathFinder: Guided Search over Multi-Step Reasoning Paths</title><link>http://arxiv.org/abs/2312.05180v2</link><description>With recent advancements in large language models, methods likechain-of-thought prompting to elicit reasoning chains have been shown toimprove results on reasoning tasks. However, tasks that require multiple stepsof reasoning still pose significant challenges to state-of-the-art models.Drawing inspiration from the beam search algorithm, we propose PathFinder, atree-search-based reasoning path generation approach. It enhances diversebranching and multi-hop reasoning through the integration of dynamic decoding,enabled by varying sampling methods and parameters. Using constrainedreasoning, PathFinder integrates novel quality constraints, pruning, andexploration methods to enhance the efficiency and the quality of generation.Moreover, it includes scoring and ranking features to improve candidateselection. Our approach outperforms competitive baselines on three complexarithmetic and commonsense reasoning tasks by 6% on average. Our modelgeneralizes well to longer, unseen reasoning chains, reflecting similarcomplexities to beam search with large branching factors.</description><author>Olga Golovneva, Sean O'Brien, Ramakanth Pasunuru, Tianlu Wang, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz</author><pubDate>Tue, 12 Dec 2023 16:06:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05180v2</guid></item><item><title>ReRoGCRL: Representation-based Robustness in Goal-Conditioned Reinforcement Learning</title><link>http://arxiv.org/abs/2312.07392v1</link><description>While Goal-Conditioned Reinforcement Learning (GCRL) has gained attention,its algorithmic robustness, particularly against adversarial perturbations,remains unexplored. Unfortunately, the attacks and robust representationtraining methods specifically designed for traditional RL are not so effectivewhen applied to GCRL. To address this challenge, we propose the\textit{Semi-Contrastive Representation} attack, a novel approach inspired bythe adversarial contrastive attack. Unlike existing attacks in RL, it onlynecessitates information from the policy function and can be seamlesslyimplemented during deployment. Furthermore, to mitigate the vulnerability ofexisting GCRL algorithms, we introduce \textit{Adversarial RepresentationTactics}. This strategy combines \textit{Semi-Contrastive AdversarialAugmentation} with \textit{Sensitivity-Aware Regularizer}. It improves theadversarial robustness of the underlying agent against various types ofperturbations. Extensive experiments validate the superior performance of ourattack and defence mechanism across multiple state-of-the-art GCRL algorithms.Our tool {\bf ReRoGCRL} is available at\url{https://github.com/TrustAI/ReRoGCRL}.</description><author>Xiangyu Yin, Sihao Wu, Jiaxu Liu, Meng Fang, Xingyu Zhao, Xiaowei Huang, Wenjie Ruan</author><pubDate>Tue, 12 Dec 2023 16:05:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07392v1</guid></item><item><title>Eroding Trust In Aerial Imagery: Comprehensive Analysis and Evaluation Of Adversarial Attacks In Geospatial Systems</title><link>http://arxiv.org/abs/2312.07389v1</link><description>In critical operations where aerial imagery plays an essential role, theintegrity and trustworthiness of data are paramount. The emergence ofadversarial attacks, particularly those that exploit control over labels oremploy physically feasible trojans, threatens to erode that trust, making theanalysis and mitigation of these attacks a matter of urgency. We demonstratehow adversarial attacks can degrade confidence in geospatial systems,specifically focusing on scenarios where the attacker's control over labels isrestricted and the use of realistic threat vectors. Proposing and evaluatingseveral innovative attack methodologies, including those tailored to overheadimages, we empirically show their threat to remote sensing systems usinghigh-quality SpaceNet datasets. Our experimentation reflects the uniquechallenges posed by aerial imagery, and these preliminary results not onlyreveal the potential risks but also highlight the non-trivial nature of theproblem compared to recent works.</description><author>Michael Lanier, Aayush Dhakal, Zhexiao Xiong, Arthur Li, Nathan Jacobs, Yevgeniy Vorobeychik</author><pubDate>Tue, 12 Dec 2023 16:05:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07389v1</guid></item><item><title>Disentanglement Learning via Topology</title><link>http://arxiv.org/abs/2308.12696v2</link><description>We propose TopDis (Topological Disentanglement), a method for learningdisentangled representations via adding multi-scale topological loss term.Disentanglement is a crucial property of data representations substantial forthe explainability and robustness of deep learning models and a step towardshigh-level cognition. The state-of-the-art method based on VAE minimizes thetotal correlation of the joint distribution of latent variables. We take adifferent perspective on disentanglement by analyzing topological properties ofdata manifolds. In particular, we optimize the topological similarity for datamanifolds traversals. To the best of our knowledge, our paper is the first oneto propose a differentiable topological loss for disentanglement. Ourexperiments have shown that the proposed topological loss improvesdisentanglement scores such as MIG, FactorVAE score, SAP score and DCIdisentanglement score with respect to state-of-the-art results. Our methodworks in an unsupervised manner, permitting to apply it for problems withoutlabeled factors of variation. Additionally, we show how to use the proposedtopological loss to find disentangled directions in a trained GAN.</description><author>Nikita Balabin, Daria Voronkova, Ilya Trofimov, Evgeny Burnaev, Serguei Barannikov</author><pubDate>Tue, 12 Dec 2023 16:03:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12696v2</guid></item><item><title>Wiener Chaos in Kernel Regression: Towards Untangling Aleatoric and Epistemic Uncertainty</title><link>http://arxiv.org/abs/2312.07387v1</link><description>Gaussian Processes (GPs) are a versatile method that enables differentapproaches towards learning for dynamics and control. Gaussianity assumptionsappear in two dimensions in GPs: The positive semi-definite kernel of theunderlying reproducing kernel Hilbert space is used to construct theco-variance of a Gaussian distribution over functions, while measurement noise(i.e. data corruption) is usually modeled as i.i.d. additive Gaussian. In thisnote, we relax the latter Gaussianity assumption, i.e., we consider kernelridge regression with additive i.i.d. non-Gaussian measurement noise. To applythe usual kernel trick, we rely on the representation of the uncertainty viapolynomial chaos expansions, which are series expansions for random variablesof finite variance introduced by Norbert Wiener. We derive and discuss theanalytic $\mathcal{L}^2$ solution to the arising Wiener kernel regression.Considering a polynomial system as numerical example, we show that our approachallows to untangle the effects of epistemic and aleatoric uncertainties.</description><author>T. Faulwasser, O. Molodchyk</author><pubDate>Tue, 12 Dec 2023 16:02:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07387v1</guid></item><item><title>GSmoothFace: Generalized Smooth Talking Face Generation via Fine Grained 3D Face Guidance</title><link>http://arxiv.org/abs/2312.07385v1</link><description>Although existing speech-driven talking face generation methods achievesignificant progress, they are far from real-world application due to theavatar-specific training demand and unstable lip movements. To address theabove issues, we propose the GSmoothFace, a novel two-stage generalized talkingface generation model guided by a fine-grained 3d face model, which cansynthesize smooth lip dynamics while preserving the speaker's identity. Ourproposed GSmoothFace model mainly consists of the Audio to ExpressionPrediction (A2EP) module and the Target Adaptive Face Translation (TAFT)module. Specifically, we first develop the A2EP module to predict expressionparameters synchronized with the driven speech. It uses a transformer tocapture the long-term audio context and learns the parameters from thefine-grained 3D facial vertices, resulting in accurate and smoothlip-synchronization performance. Afterward, the well-designed TAFT module,empowered by Morphology Augmented Face Blending (MAFB), takes the predictedexpression parameters and target video as inputs to modify the facial region ofthe target video without distorting the background content. The TAFTeffectively exploits the identity appearance and background context in thetarget video, which makes it possible to generalize to different speakerswithout retraining. Both quantitative and qualitative experiments confirm thesuperiority of our method in terms of realism, lip synchronization, and visualquality. See the project page for code, data, and request pre-trained models:https://zhanghm1995.github.io/GSmoothFace.</description><author>Haiming Zhang, Zhihao Yuan, Chaoda Zheng, Xu Yan, Baoyuan Wang, Guanbin Li, Song Wu, Shuguang Cui, Zhen Li</author><pubDate>Tue, 12 Dec 2023 16:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07385v1</guid></item><item><title>Unsupervised Temporal Action Localization via Self-paced Incremental Learning</title><link>http://arxiv.org/abs/2312.07384v1</link><description>Recently, temporal action localization (TAL) has garnered significantinterest in information retrieval community. However, existingsupervised/weakly supervised methods are heavily dependent on extensive labeledtemporal boundaries and action categories, which is labor-intensive andtime-consuming. Although some unsupervised methods have utilized the``iteratively clustering and localization'' paradigm for TAL, they still sufferfrom two pivotal impediments: 1) unsatisfactory video clustering confidence,and 2) unreliable video pseudolabels for model training. To address theselimitations, we present a novel self-paced incremental learning model toenhance clustering and localization training simultaneously, therebyfacilitating more effective unsupervised TAL. Concretely, we improve theclustering confidence through exploring the contextual feature-robust visualinformation. Thereafter, we design two (constant- and variable- speed)incremental instance learning strategies for easy-to-hard model training, thusensuring the reliability of these video pseudolabels and further improvingoverall localization performance. Extensive experiments on two public datasetshave substantiated the superiority of our model over several state-of-the-artcompetitors.</description><author>Haoyu Tang, Han Jiang, Mingzhu Xu, Yupeng Hu, Jihua Zhu, Liqiang Nie</author><pubDate>Tue, 12 Dec 2023 16:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07384v1</guid></item><item><title>ScribblePrompt: Fast and Flexible Interactive Segmentation for Any Medical Image</title><link>http://arxiv.org/abs/2312.07381v1</link><description>Semantic medical image segmentation is a crucial part of both scientificresearch and clinical care. With enough labelled data, deep learning models canbe trained to accurately automate specific medical image segmentation tasks.However, manually segmenting images to create training data is highly laborintensive. In this paper, we present ScribblePrompt, an interactivesegmentation framework for medical imaging that enables human annotators tosegment unseen structures using scribbles, clicks, and bounding boxes.Scribbles are an intuitive and effective form of user interaction for complextasks, however most existing methods focus on click-based interactions. Weintroduce algorithms for simulating realistic scribbles that enable trainingmodels that are amenable to multiple types of interaction. To achievegeneralization to new tasks, we train on a diverse collection of 65 open-accessbiomedical datasets -- using both real and synthetic labels. We testScribblePrompt on multiple network architectures and unseen datasets, anddemonstrate that it can be used in real-time on a single CPU. We evaluateScribblePrompt using manually-collected scribbles, simulated interactions, anda user study. ScribblePrompt outperforms existing methods in all ourevaluations. In the user study, ScribblePrompt reduced annotation time by 28%while improving Dice by 15% compared to existing methods. We showcaseScribblePrompt in an online demo and provide code athttps://scribbleprompt.csail.mit.edu</description><author>Hallee E. Wong, Marianne Rakic, John Guttag, Adrian V. Dalca</author><pubDate>Tue, 12 Dec 2023 15:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07381v1</guid></item><item><title>X4D-SceneFormer: Enhanced Scene Understanding on 4D Point Cloud Videos through Cross-modal Knowledge Transfer</title><link>http://arxiv.org/abs/2312.07378v1</link><description>The field of 4D point cloud understanding is rapidly developing with the goalof analyzing dynamic 3D point cloud sequences. However, it remains achallenging task due to the sparsity and lack of texture in point clouds.Moreover, the irregularity of point cloud poses a difficulty in aligningtemporal information within video sequences. To address these issues, wepropose a novel cross-modal knowledge transfer framework, calledX4D-SceneFormer. This framework enhances 4D-Scene understanding by transferringtexture priors from RGB sequences using a Transformer architecture withtemporal relationship mining. Specifically, the framework is designed with adual-branch architecture, consisting of an 4D point cloud transformer and aGradient-aware Image Transformer (GIT). During training, we employ multipleknowledge transfer techniques, including temporal consistency losses and maskedself-attention, to strengthen the knowledge transfer between modalities. Thisleads to enhanced performance during inference using single-modal 4D pointcloud inputs. Extensive experiments demonstrate the superior performance of ourframework on various 4D point cloud video understanding tasks, including actionrecognition, action segmentation and semantic segmentation. The results achieve1st places, i.e., 85.3% (+7.9%) accuracy and 47.3% (+5.0%) mIoU for 4D actionsegmentation and semantic segmentation, on the HOI4Dchallenge\footnote{\url{http://www.hoi4d.top/}.}, outperforming previousstate-of-the-art by a large margin. We release the code athttps://github.com/jinglinglingling/X4D</description><author>Linglin Jing, Ying Xue, Xu Yan, Chaoda Zheng, Dong Wang, Ruimao Zhang, Zhigang Wang, Hui Fang, Bin Zhao, Zhen Li</author><pubDate>Tue, 12 Dec 2023 15:48:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07378v1</guid></item><item><title>APG: Adaptive Parameter Generation Network for Click-Through Rate Prediction</title><link>http://arxiv.org/abs/2203.16218v3</link><description>In many web applications, deep learning-based CTR prediction models (deep CTRmodels for short) are widely adopted. Traditional deep CTR models learnpatterns in a static manner, i.e., the network parameters are the same acrossall the instances. However, such a manner can hardly characterize each of theinstances which may have different underlying distributions. It actually limitsthe representation power of deep CTR models, leading to sub-optimal results. Inthis paper, we propose an efficient, effective, and universal module, named asAdaptive Parameter Generation network (APG), which can dynamically generateparameters for deep CTR models on-the-fly based on different instances.Extensive experimental evaluation results show that APG can be applied to avariety of deep CTR models and significantly improve their performance.Meanwhile, APG can reduce the time cost by 38.7\% and memory usage by 96.6\%compared to a regular deep CTR model. We have deployed APG in the industrialsponsored search system and achieved 3\% CTR gain and 1\% RPM gainrespectively.</description><author>Bencheng Yan, Pengjie Wang, Kai Zhang, Feng Li, Hongbo Deng, Jian Xu, Bo Zheng</author><pubDate>Tue, 12 Dec 2023 15:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.16218v3</guid></item><item><title>Relax Image-Specific Prompt Requirement in SAM: A Single Generic Prompt for Segmenting Camouflaged Objects</title><link>http://arxiv.org/abs/2312.07374v1</link><description>Camouflaged object detection (COD) approaches heavily rely on pixel-levelannotated datasets. Weakly-supervised COD (WSCOD) approaches use sparseannotations like scribbles or points to reduce annotation effort, but this canlead to decreased accuracy. The Segment Anything Model (SAM) shows remarkablesegmentation ability with sparse prompts like points. However, manual prompt isnot always feasible, as it may not be accessible in real-world application.Additionally, it only provides localization information instead of semanticone, which can intrinsically cause ambiguity in interpreting the targets. Inthis work, we aim to eliminate the need for manual prompt. The key idea is toemploy Cross-modal Chains of Thought Prompting (CCTP) to reason visual promptsusing the semantic information given by a generic text prompt.To that end, weintroduce a test-time adaptation per-instance mechanism called GeneralizableSAM (GenSAM) to automatically enerate and optimize visual prompts the generictask prompt for WSCOD. In particular, CCTP maps a single generic text promptonto image-specific consensus foreground and background heatmaps usingvision-language models, acquiring reliable visual prompts. Moreover, totest-time adapt the visual prompts, we further propose Progressive MaskGeneration (PMG) to iteratively reweight the input image, guiding the model tofocus on the targets in a coarse-to-fine manner. Crucially, all networkparameters are fixed, avoiding the need for additional training. Experimentsdemonstrate the superiority of GenSAM. Experiments on three benchmarksdemonstrate that GenSAM outperforms point supervision approaches and achievescomparable results to scribble supervision ones, solely relying on general taskdescriptions as prompts. our codes is in: https://lwpyh.github.io/GenSAM/.</description><author>Jian Hu, Jiayi Lin, Weitong Cai, Shaogang Gong</author><pubDate>Tue, 12 Dec 2023 15:43:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07374v1</guid></item><item><title>Privacy-Aware Energy Consumption Modeling of Connected Battery Electric Vehicles using Federated Learning</title><link>http://arxiv.org/abs/2312.07371v1</link><description>Battery Electric Vehicles (BEVs) are increasingly significant in moderncities due to their potential to reduce air pollution. Precise and real-timeestimation of energy consumption for them is imperative for effective itineraryplanning and optimizing vehicle systems, which can reduce driving range anxietyand decrease energy costs. As public awareness of data privacy increases,adopting approaches that safeguard data privacy in the context of BEV energyconsumption modeling is crucial. Federated Learning (FL) is a promisingsolution mitigating the risk of exposing sensitive information to third partiesby allowing local data to remain on devices and only sharing model updates witha central server. Our work investigates the potential of using FL methods, suchas FedAvg, and FedPer, to improve BEV energy consumption prediction whilemaintaining user privacy. We conducted experiments using data from 10 BEVsunder simulated real-world driving conditions. Our results demonstrate that theFedAvg-LSTM model achieved a reduction of up to 67.84\% in the MAE value of theprediction results. Furthermore, we explored various real-world scenarios anddiscussed how FL methods can be employed in those cases. Our findings show thatFL methods can effectively improve the performance of BEV energy consumptionprediction while maintaining user privacy.</description><author>Sen Yan, Hongyuan Fang, Ji Li, Tomas Ward, Noel O'Connor, Mingming Liu</author><pubDate>Tue, 12 Dec 2023 15:40:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07371v1</guid></item><item><title>Adversarial Semi-Supervised Domain Adaptation for Semantic Segmentation: A New Role for Labeled Target Samples</title><link>http://arxiv.org/abs/2312.07370v1</link><description>Adversarial learning baselines for domain adaptation (DA) approaches in thecontext of semantic segmentation are under explored in semi-supervisedframework. These baselines involve solely the available labeled target samplesin the supervision loss. In this work, we propose to enhance their usefulnesson both semantic segmentation and the single domain classifier neural networks.We design new training objective losses for cases when labeled target databehave as source samples or as real target samples. The underlying rationale isthat considering the set of labeled target samples as part of source domainhelps reducing the domain discrepancy and, hence, improves the contribution ofthe adversarial loss. To support our approach, we consider a complementarymethod that mixes source and labeled target data, then applies the sameadaptation process. We further propose an unsupervised selection procedureusing entropy to optimize the choice of labeled target samples for adaptation.We illustrate our findings through extensive experiments on the benchmarksGTA5, SYNTHIA, and Cityscapes. The empirical evaluation highlights competitiveperformance of our proposed approach.</description><author>Marwa Kechaou, Mokhtar Z. Alaya, Romain Hérault, Gilles Gasso</author><pubDate>Tue, 12 Dec 2023 15:40:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07370v1</guid></item><item><title>Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding</title><link>http://arxiv.org/abs/2312.05328v2</link><description>We propose a method for accelerating large-scale pre-training with onlinedata selection policies. For the first time, we demonstrate that model-baseddata selection can reduce the total computation needed to reach the performanceof models trained with uniform sampling. The key insight which enables this"compute-positive" regime is that small models provide good proxies for theloss of much larger models, such that computation spent on scoring data can bedrastically scaled down but still significantly accelerate training of thelearner.. These data selection policies also strongly generalize acrossdatasets and tasks, opening an avenue for further amortizing the overhead ofdata scoring by re-using off-the-shelf models and training sequences. Ourmethods, ClassAct and ActiveCLIP, require 46% and 51% fewer training updatesand up to 25% less total computation when training visual classifiers on JFTand multimodal models on ALIGN, respectively. Finally, our paradigm seamlesslyapplies to the curation of large-scale image-text datasets, yielding a newstate-of-the-art in several multimodal transfer tasks and pre-training regimes.</description><author>Talfan Evans, Shreya Pathak, Hamza Merzic, Jonathan Schwarz, Ryutaro Tanno, Olivier J. Henaff</author><pubDate>Tue, 12 Dec 2023 15:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05328v2</guid></item><item><title>Sequential Planning in Large Partially Observable Environments guided by LLMs</title><link>http://arxiv.org/abs/2312.07368v1</link><description>Sequential planning in large state space and action space quickly becomesintractable due to combinatorial explosion of the search space. Heuristicmethods, like monte-carlo tree search, though effective for large state space,but struggle if action space is large. Pure reinforcement learning methods,relying only on reward signals, needs prohibitively large interactions with theenvironment to device a viable plan. If the state space, observations andactions can be represented in natural language then Large Language models (LLM)can be used to generate action plans. Recently several such goal-directedagents like Reflexion, CLIN, SayCan were able to surpass the performance ofother state-of-the-art methods with minimum or no task specific training. Butthey still struggle with exploration and get stuck in local optima. Theirplanning capabilities are limited by the limited reasoning capability of thefoundational LLMs on text data. We propose a hybrid agent "neoplanner", thatsynergizes both state space search with queries to foundational LLM to get thebest action plan. The reward signals are quantitatively used to drive thesearch. A balance of exploration and exploitation is maintained by maximizingupper confidence bounds of values of states. In places where random explorationis needed, the LLM is queried to generate an action plan. Learnings from eachtrial are stored as entity relationships in text format. Those are used infuture queries to the LLM for continual improvement. Experiments in theScienceworld environment reveals a 124% improvement from the current bestmethod in terms of average reward gained across multiple tasks.</description><author>Swarna Kamal Paul</author><pubDate>Tue, 12 Dec 2023 15:36:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07368v1</guid></item><item><title>Collapse-Oriented Adversarial Training with Triplet Decoupling for Robust Image Retrieval</title><link>http://arxiv.org/abs/2312.07364v1</link><description>Adversarial training has achieved substantial performance in defending imageretrieval systems against adversarial examples. However, existing studies stillsuffer from two major limitations: model collapse and weak adversary. Thispaper addresses these two limitations by proposing collapse-oriented (COLO)adversarial training with triplet decoupling (TRIDE). Specifically, COLOprevents model collapse by temporally orienting the perturbation updatedirection with a new collapse metric, while TRIDE yields a strong adversary byspatially decoupling the update targets of perturbation into the anchor and thetwo candidates of a triplet. Experimental results demonstrate that ourCOLO-TRIDE outperforms the current state of the art by 7% on average over 10robustness metrics and across 3 popular datasets. In addition, we identify thefairness limitations of commonly used robustness metrics in image retrieval andpropose a new metric for more meaningful robustness evaluation. Codes will bemade publicly available on GitHub.</description><author>Qiwei Tian, Chenhao Lin, Qian Li, Zhengyu Zhao, Chao Shen</author><pubDate>Tue, 12 Dec 2023 15:33:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07364v1</guid></item><item><title>On Classification-Calibration of Gamma-Phi Losses</title><link>http://arxiv.org/abs/2302.07321v2</link><description>Gamma-Phi losses constitute a family of multiclass classification lossfunctions that generalize the logistic and other common losses, and have foundapplication in the boosting literature. We establish the first generalsufficient condition for the classification-calibration (CC) of such losses. Toour knowledge, this sufficient condition gives the first family of nonconvexmulticlass surrogate losses for which CC has been fully justified. In addition,we show that a previously proposed sufficient condition is in fact notsufficient. This contribution highlights a technical issue that is important inthe study of multiclass CC but has been neglected in prior work.</description><author>Yutong Wang, Clayton D. Scott</author><pubDate>Tue, 12 Dec 2023 15:32:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07321v2</guid></item><item><title>Boosting Latent Diffusion with Flow Matching</title><link>http://arxiv.org/abs/2312.07360v1</link><description>Recently, there has been tremendous progress in visual synthesis and theunderlying generative models. Here, diffusion models (DMs) stand outparticularly, but lately, flow matching (FM) has also garnered considerableinterest. While DMs excel in providing diverse images, they suffer from longtraining and slow generation. With latent diffusion, these issues are onlypartially alleviated. Conversely, FM offers faster training and inference butexhibits less diversity in synthesis. We demonstrate that introducing FMbetween the Diffusion model and the convolutional decoder offershigh-resolution image synthesis with reduced computational cost and model size.Diffusion can then efficiently provide the necessary generation diversity. FMcompensates for the lower resolution, mapping the small latent space to ahigh-dimensional one. Subsequently, the convolutional decoder of the LDM mapsthese latents to high-resolution images. By combining the diversity of DMs, theefficiency of FMs, and the effectiveness of convolutional decoders, we achievestate-of-the-art high-resolution image synthesis at $1024^2$ with minimalcomputational cost. Importantly, our approach is orthogonal to recentapproximation and speed-up strategies for the underlying DMs, making it easilyintegrable into various DM frameworks.</description><author>Johannes S. Fischer, Ming Gui, Pingchuan Ma, Nick Stracke, Stefan A. Baumann, Björn Ommer</author><pubDate>Tue, 12 Dec 2023 15:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07360v1</guid></item><item><title>Multimodality of AI for Education: Towards Artificial General Intelligence</title><link>http://arxiv.org/abs/2312.06037v2</link><description>This paper presents a comprehensive examination of how multimodal artificialintelligence (AI) approaches are paving the way towards the realization ofArtificial General Intelligence (AGI) in educational contexts. It scrutinizesthe evolution and integration of AI in educational systems, emphasizing thecrucial role of multimodality, which encompasses auditory, visual, kinesthetic,and linguistic modes of learning. This research delves deeply into the keyfacets of AGI, including cognitive frameworks, advanced knowledgerepresentation, adaptive learning mechanisms, strategic planning, sophisticatedlanguage processing, and the integration of diverse multimodal data sources. Itcritically assesses AGI's transformative potential in reshaping educationalparadigms, focusing on enhancing teaching and learning effectiveness, fillinggaps in existing methodologies, and addressing ethical considerations andresponsible usage of AGI in educational settings. The paper also discusses theimplications of multimodal AI's role in education, offering insights intofuture directions and challenges in AGI development. This exploration aims toprovide a nuanced understanding of the intersection between AI, multimodality,and education, setting a foundation for future research and development in AGI.</description><author>Gyeong-Geon Lee, Lehong Shi, Ehsan Latif, Yizhu Gao, Arne Bewersdorff, Matthew Nyaaba, Shuchen Guo, Zihao Wu, Zhengliang Liu, Hui Wang, Gengchen Mai, Tiaming Liu, Xiaoming Zhai</author><pubDate>Tue, 12 Dec 2023 15:26:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06037v2</guid></item><item><title>Automatic coral reef fish identification and 3D measurement in the wild</title><link>http://arxiv.org/abs/2312.07357v1</link><description>In this paper we present a pipeline using stereo images in order toautomatically identify, track in 3D fish, and measure fish population.</description><author>Cyril Barrelet, Marc Chaumont, Gérard Subsol</author><pubDate>Tue, 12 Dec 2023 15:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07357v1</guid></item><item><title>Graph Neural Network-based surrogate model for granular flows</title><link>http://arxiv.org/abs/2305.05218v2</link><description>Accurate simulation of granular flow dynamics is crucial for assessingvarious geotechnical risks, including landslides and debris flows. Granularflows involve a dynamic rearrangement of particles exhibiting complextransitions from solid-like to fluid-like responses. Traditional continuum anddiscrete numerical methods are limited by their computational cost insimulating large-scale systems. Statistical or machine learning-based modelsoffer an alternative. Still, they are largely empirical, based on a limited setof parameters. Due to their permutation-dependent learning, traditional machinelearning-based models require huge training data to generalize. To resolvethese problems, we use a graph neural network, a state-of-the-art machinelearning architecture that learns local interactions. Graphs represent thestate of dynamically changing granular flows and the interaction laws, such asenergy and momentum exchange between grains. We develop a graph neuralnetwork-based simulator (GNS) that takes the current state of granular flow andpredicts the next state using Euler explicit integration by learning the localinteraction laws. We train GNS on different granular trajectories. We thenassess the performance of GNS by predicting granular column collapse. GNSaccurately predicts flow dynamics for column collapses with different aspectratios unseen during training. GNS is hundreds of times faster thanhigh-fidelity numerical simulators. The model also generalizes to domains muchlarger than the training data, handling more than twice the number of particlesthan it was trained on.</description><author>Yongjin Choi, Krishna Kumar</author><pubDate>Tue, 12 Dec 2023 15:23:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05218v2</guid></item><item><title>CLIP in Medical Imaging: A Comprehensive Survey</title><link>http://arxiv.org/abs/2312.07353v1</link><description>Contrastive Language-Image Pre-training (CLIP), a straightforward yeteffective pre-training paradigm, successfully introduces semantic-rich textsupervision to vision models and has demonstrated promising results in varioustasks due to its generalizability and interpretability. It has recently gainedincreasing interest in the medical imaging domain, either as a powerfulpre-training paradigm for medical vision language alignment or a pre-trainedkey component for various clinical tasks. With the aim of facilitating a deeperunderstanding of this promising direction, this survey offers an in-depthexploration of the CLIP paradigm within the domain of medical imaging,regarding both refined CLIP pre-training and CLIP-driven applications. Oursurvey (1) starts with a brief introduction to the fundamentals of CLIPmethodology. (2) Then, we investigate the adaptation of CLIP pre-training inthe medical domain, focusing on how to optimize CLIP given characteristics ofmedical images and reports. (3) Furthermore, we explore the practicalutilization of CLIP pre-trained models in various tasks, includingclassification, dense prediction, and cross-modal tasks. (4) Finally, wediscuss existing limitations of CLIP in the context of medical imaging andpropose forward-looking directions to address the demands of medical imagingdomain. We expect that this comprehensive survey will provide researchers inthe field of medical image analysis with a holistic understanding of the CLIPparadigm and its potential implications. The project page is available athttps://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging, which will beregularly updated.</description><author>Zihao Zhao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Xiang Li, Zhiming Cui, Qian Wang, Dinggang Shen</author><pubDate>Tue, 12 Dec 2023 15:21:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07353v1</guid></item><item><title>CholecTrack20: A Dataset for Multi-Class Multiple Tool Tracking in Laparoscopic Surgery</title><link>http://arxiv.org/abs/2312.07352v1</link><description>Tool tracking in surgical videos is vital in computer-assisted interventionfor tasks like surgeon skill assessment, safety zone estimation, andhuman-machine collaboration during minimally invasive procedures. The lack oflarge-scale datasets hampers Artificial Intelligence implementation in thisdomain. Current datasets exhibit overly generic tracking formalization, oftenlacking surgical context: a deficiency that becomes evident when tools move outof the camera's scope, resulting in rigid trajectories that hinder realisticsurgical representation. This paper addresses the need for a more precise andadaptable tracking formalization tailored to the intricacies of endoscopicprocedures by introducing CholecTrack20, an extensive dataset meticulouslyannotated for multi-class multi-tool tracking across three perspectivesrepresenting the various ways of considering the temporal duration of a tooltrajectory: (1) intraoperative, (2) intracorporeal, and (3) visibility withinthe camera's scope. The dataset comprises 20 laparoscopic videos with over35,000 frames and 65,000 annotated tool instances with details on spatiallocation, category, identity, operator, phase, and surgical visual conditions.This detailed dataset caters to the evolving assistive requirements within aprocedure.</description><author>Chinedu Innocent Nwoye, Kareem Elgohary, Anvita Srinivas, Fauzan Zaid, Joël L. Lavanchy, Nicolas Padoy</author><pubDate>Tue, 12 Dec 2023 15:18:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07352v1</guid></item><item><title>Stochastic Nonlinear Control via Finite-dimensional Spectral Dynamic Embedding</title><link>http://arxiv.org/abs/2304.03907v2</link><description>This paper presents an approach, Spectral Dynamics Embedding Control (SDEC),to optimal control for nonlinear stochastic systems. This method leverages aninfinite-dimensional feature to linearly represent the state-action valuefunction and exploits finite-dimensional truncation approximation for practicalimplementation. To characterize the effectiveness of these finite dimensionalapproximations, we provide an in-depth theoretical analysis to characterize theapproximation error induced by the finite-dimension truncation and statisticalerror induced by finite-sample approximation in both policy evaluation andpolicy optimization. Our analysis includes two prominent kernel approximationmethods: truncations onto random features and Nystrom features. We alsoempirically test the algorithm and compare the performance with Koopman-based,iLQR, and energy-based methods on a few benchmark problems.</description><author>Tongzheng Ren, Zhaolin Ren, Na Li, Haitong Ma, Bo Dai</author><pubDate>Tue, 12 Dec 2023 15:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03907v2</guid></item><item><title>MammoFL: Mammographic Breast Density Estimation using Federated Learning</title><link>http://arxiv.org/abs/2206.05575v4</link><description>In this study, we automate quantitative mammographic breast densityestimation with neural networks and show that this tool is a strong use casefor federated learning on multi-institutional datasets. Our dataset includedbilateral CC-view and MLO-view mammographic images from two separateinstitutions. Two U-Nets were separately trained on algorithm-generated labelsto perform segmentation of the breast and dense tissue from these images andsubsequently calculate breast percent density (PD). The networks were trainedwith federated learning and compared to three non-federated baselines, onetrained on each single-institution dataset and one trained on the aggregatedmulti-institution dataset. We demonstrate that training on multi-institutiondatasets is critical to algorithm generalizability. We further show thatfederated learning on multi-institutional datasets improves modelgeneralization to unseen data at nearly the same level as centralized trainingon multi-institutional datasets, indicating that federated learning can beapplied to our method to improve algorithm generalizability while maintainingpatient privacy.</description><author>Ramya Muthukrishnan, Angelina Heyler, Keshava Katti, Sarthak Pati, Walter Mankowski, Aprupa Alahari, Michael Sanborn, Emily F. Conant, Christopher Scott, Stacey Winham, Celine Vachon, Pratik Chaudhari, Despina Kontos, Spyridon Bakas</author><pubDate>Tue, 12 Dec 2023 15:10:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.05575v4</guid></item><item><title>Opening the Vocabulary of Egocentric Actions</title><link>http://arxiv.org/abs/2308.11488v2</link><description>Human actions in egocentric videos are often hand-object interactionscomposed from a verb (performed by the hand) applied to an object. Despitetheir extensive scaling up, egocentric datasets still face two limitations -sparsity of action compositions and a closed set of interacting objects. Thispaper proposes a novel open vocabulary action recognition task. Given a set ofverbs and objects observed during training, the goal is to generalize the verbsto an open vocabulary of actions with seen and novel objects. To this end, wedecouple the verb and object predictions via an object-agnostic verb encoderand a prompt-based object encoder. The prompting leverages CLIP representationsto predict an open vocabulary of interacting objects. We create open vocabularybenchmarks on the EPIC-KITCHENS-100 and Assembly101 datasets; whereasclosed-action methods fail to generalize, our proposed method is effective. Inaddition, our object encoder significantly outperforms existing open-vocabularyvisual recognition methods in recognizing novel interacting objects.</description><author>Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao</author><pubDate>Tue, 12 Dec 2023 15:10:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11488v2</guid></item><item><title>Generalization-based similarity</title><link>http://arxiv.org/abs/2302.10096v4</link><description>Detecting and exploiting similarities between seemingly distant objects iswithout doubt an important human ability. This paper develops \textit{from theground up} an abstract algebraic and qualitative justification-based notion ofsimilarity based on the observation that sets of generalizations encodeimportant properties of elements. We show that similarity defined in this wayhas appealing mathematical properties. As we construct our notion of similarityfrom first principles using only elementary concepts of universal algebra, toconvince the reader of its plausibility, we show that it can be naturallyembedded into first-order logic via model-theoretic types.</description><author>Christian Antić</author><pubDate>Tue, 12 Dec 2023 15:08:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10096v4</guid></item></channel></rss>