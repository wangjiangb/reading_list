<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 21 Jan 2024 14:00:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions</title><link>http://arxiv.org/abs/2401.10232v1</link><description>To enable machines to learn how humans interact with the physical world inour daily activities, it is crucial to provide rich data that encompasses the3D motion of humans as well as the motion of objects in a learnable 3Drepresentation. Ideally, this data should be collected in a natural setup,capturing the authentic dynamic 3D signals during human-object interactions. Toaddress this challenge, we introduce the ParaHome system, designed to captureand parameterize dynamic 3D movements of humans and objects within a commonhome environment. Our system consists of a multi-view setup with 70synchronized RGB cameras, as well as wearable motion capture devices equippedwith an IMU-based body suit and hand motion capture gloves. By leveraging theParaHome system, we collect a novel large-scale dataset of human-objectinteraction. Notably, our dataset offers key advancement over existing datasetsin three main aspects: (1) capturing 3D body and dexterous hand manipulationmotion alongside 3D object movement within a contextual home environment duringnatural activities; (2) encompassing human interaction with multiple objects invarious episodic scenarios with corresponding descriptions in texts; (3)including articulated objects with multiple parts expressed with parameterizedarticulations. Building upon our dataset, we introduce new research tasks aimedat building a generative model for learning and synthesizing human-objectinteractions in a real-world room setting.</description><author>Jeonghwan Kim, Jisoo Kim, Jeonghyeon Na, Hanbyul Joo</author><pubDate>Thu, 18 Jan 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10232v1</guid></item><item><title>OMG-Seg: Is One Model Good Enough For All Segmentation?</title><link>http://arxiv.org/abs/2401.10229v1</link><description>In this work, we address various segmentation tasks, each traditionallytackled by distinct or partially unified models. We propose OMG-Seg, One Modelthat is Good enough to efficiently and effectively handle all the segmentationtasks, including image semantic, instance, and panoptic segmentation, as wellas their video counterparts, open vocabulary settings, prompt-driven,interactive segmentation like SAM, and video object segmentation. To ourknowledge, this is the first model to handle all these tasks in one model andachieve satisfactory performance. We show that OMG-Seg, a transformer-basedencoder-decoder architecture with task-specific queries and outputs, cansupport over ten distinct segmentation tasks and yet significantly reducecomputational and parameter overhead across various tasks and datasets. Werigorously evaluate the inter-task influences and correlations duringco-training. Code and models are available at https://github.com/lxtGH/OMG-Seg.</description><author>Xiangtai Li, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen, Chen Change Loy</author><pubDate>Thu, 18 Jan 2024 18:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10229v1</guid></item><item><title>RAP-SAM: Towards Real-Time All-Purpose Segment Anything</title><link>http://arxiv.org/abs/2401.10228v1</link><description>Advanced by transformer architecture, vision foundation models (VFMs) achieveremarkable progress in performance and generalization ability. Segment AnythingModel (SAM) is one remarkable model that can achieve generalized segmentation.However, most VFMs cannot run in realtime, which makes it difficult to transferthem into several products. On the other hand, current real-time segmentationmainly has one purpose, such as semantic segmentation on the driving scene. Weargue that diverse outputs are needed for real applications. Thus, this workexplores a new real-time segmentation setting, named all-purpose segmentationin real-time, to transfer VFMs in real-time deployment. It contains threedifferent tasks, including interactive segmentation, panoptic segmentation, andvideo segmentation. We aim to use one model to achieve the above tasks inreal-time. We first benchmark several strong baselines. Then, we presentReal-Time All Purpose SAM (RAP-SAM). It contains an efficient encoder and anefficient decoupled decoder to perform prompt-driven decoding. Moreover, wefurther explore different training strategies and tuning methods to boostco-training performance further. Our code and model are available athttps://github.com/xushilin1/RAP-SAM/.</description><author>Shilin Xu, Haobo Yuan, Qingyu Shi, Lu Qi, Jingbo Wang, Yibo Yang, Yining Li, Kai Chen, Yunhai Tong, Bernard Ghanem, Xiangtai Li, Ming-Hsuan Yang</author><pubDate>Thu, 18 Jan 2024 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10228v1</guid></item><item><title>A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting</title><link>http://arxiv.org/abs/2401.10227v1</link><description>Panoptic and instance segmentation networks are often trained withspecialized object detection modules, complex loss functions, and ad-hocpost-processing steps to handle the permutation-invariance of the instancemasks. This work builds upon Stable Diffusion and proposes a latent diffusionapproach for panoptic segmentation, resulting in a simple architecture whichomits these complexities. Our training process consists of two steps: (1)training a shallow autoencoder to project the segmentation masks to latentspace; (2) training a diffusion model to allow image-conditioned sampling inlatent space. The use of a generative model unlocks the exploration of maskcompletion or inpainting, which has applications in interactive segmentation.The experimental validation yields promising results for both panopticsegmentation and mask inpainting. While not setting a new state-of-the-art, ourmodel's simplicity, generality, and mask completion capability are desirableproperties.</description><author>Wouter Van Gansbeke, Bert De Brabandere</author><pubDate>Thu, 18 Jan 2024 18:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10227v1</guid></item><item><title>AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation</title><link>http://arxiv.org/abs/2306.00977v3</link><description>During interactive segmentation, a model and a user work together todelineate objects of interest in a 3D point cloud. In an iterative process, themodel assigns each data point to an object (or the background), while the usercorrects errors in the resulting segmentation and feeds them back into themodel. The current best practice formulates the problem as binaryclassification and segments objects one at a time. The model expects the userto provide positive clicks to indicate regions wrongly assigned to thebackground and negative clicks on regions wrongly assigned to the object.Sequentially visiting objects is wasteful since it disregards synergies betweenobjects: a positive click for a given object can, by definition, serve as anegative click for nearby objects. Moreover, a direct competition betweenadjacent objects can speed up the identification of their common boundary. Weintroduce AGILE3D, an efficient, attention-based model that (1) supportssimultaneous segmentation of multiple 3D objects, (2) yields more accuratesegmentation masks with fewer user clicks, and (3) offers faster inference. Ourcore idea is to encode user clicks as spatial-temporal queries and enableexplicit interactions between click queries as well as between them and the 3Dscene through a click attention module. Every time new clicks are added, weonly need to run a lightweight decoder that produces updated segmentationmasks. In experiments with four different 3D point cloud datasets, AGILE3D setsa new state-of-the-art. Moreover, we also verify its practicality in real-worldsetups with real user studies.</description><author>Yuanwen Yue, Sabarinath Mahadevan, Jonas Schult, Francis Engelmann, Bastian Leibe, Konrad Schindler, Theodora Kontogianni</author><pubDate>Thu, 18 Jan 2024 18:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00977v3</guid></item><item><title>Towards Language-Driven Video Inpainting via Multimodal Large Language Models</title><link>http://arxiv.org/abs/2401.10226v1</link><description>We introduce a new task -- language-driven video inpainting, which usesnatural language instructions to guide the inpainting process. This approachovercomes the limitations of traditional video inpainting methods that dependon manually labeled binary masks, a process often tedious and labor-intensive.We present the Remove Objects from Videos by Instructions (ROVI) dataset,containing 5,650 videos and 9,091 inpainting results, to support training andevaluation for this task. We also propose a novel diffusion-basedlanguage-driven video inpainting framework, the first end-to-end baseline forthis task, integrating Multimodal Large Language Models to understand andexecute complex language-based inpainting requests effectively. Ourcomprehensive results showcase the dataset's versatility and the model'seffectiveness in various language-instructed inpainting scenarios. We will makedatasets, code, and models publicly available.</description><author>Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, Chen Change Loy</author><pubDate>Thu, 18 Jan 2024 18:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10226v1</guid></item><item><title>ChatQA: Building GPT-4 Level Conversational QA Models</title><link>http://arxiv.org/abs/2401.10225v1</link><description>In this work, we introduce ChatQA, a family of conversational questionanswering (QA) models, that obtain GPT-4 level accuracies. Specifically, wepropose a two-stage instruction tuning method that can significantly improvethe zero-shot conversational QA results from large language models (LLMs). Tohandle retrieval in conversational QA, we fine-tune a dense retriever on amulti-turn QA dataset, which provides comparable results to using thestate-of-the-art query rewriting model while largely reducing deployment cost.Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10conversational QA datasets (54.14 vs. 53.90), without relying on any syntheticdata from OpenAI GPT models.</description><author>Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro</author><pubDate>Thu, 18 Jan 2024 18:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10225v1</guid></item><item><title>The Manga Whisperer: Automatically Generating Transcriptions for Comics</title><link>http://arxiv.org/abs/2401.10224v1</link><description>In the past few decades, Japanese comics, commonly referred to as Manga, havetranscended both cultural and linguistic boundaries to become a true worldwidesensation. Yet, the inherent reliance on visual cues and illustration withinmanga renders it largely inaccessible to individuals with visual impairments.In this work, we seek to address this substantial barrier, with the aim ofensuring that manga can be appreciated and actively engaged by everyone.Specifically, we tackle the problem of diarisation i.e. generating atranscription of who said what and when, in a fully automatic way. To this end, we make the following contributions: (1) we present a unifiedmodel, Magi, that is able to (a) detect panels, text boxes and character boxes,(b) cluster characters by identity (without knowing the number of clustersapriori), and (c) associate dialogues to their speakers; (2) we propose a novelapproach that is able to sort the detected text boxes in their reading orderand generate a dialogue transcript; (3) we annotate an evaluation benchmark forthis task using publicly available [English] manga pages. The code, evaluationdatasets and the pre-trained model can be found at:https://github.com/ragavsachdeva/magi.</description><author>Ragav Sachdeva, Andrew Zisserman</author><pubDate>Thu, 18 Jan 2024 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10224v1</guid></item><item><title>Supervised Fine-tuning in turn Improves Visual Foundation Models</title><link>http://arxiv.org/abs/2401.10222v1</link><description>Image-text training like CLIP has dominated the pretraining of visionfoundation models in recent years. Subsequent efforts have been made tointroduce region-level visual learning into CLIP's pretraining but facescalability challenges due to the lack of large-scale region-level datasets.Drawing inspiration from supervised fine-tuning (SFT) in natural languageprocessing such as instruction tuning, we explore the potential of fine-grainedSFT in enhancing the generation of vision foundation models after theirpretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleashthe fine-grained knowledge of vision foundation models. In ViSFT, the visionfoundation model is enhanced by performing visual joint learning on somein-domain tasks and then tested on out-of-domain benchmarks. With updatingusing ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over4.4B parameters shows improvements across various out-of-domain benchmarksincluding vision and vision-linguistic scenarios.</description><author>Xiaohu Jiang, Yixiao Ge, Yuying Ge, Chun Yuan, Ying Shan</author><pubDate>Thu, 18 Jan 2024 18:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10222v1</guid></item><item><title>AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data</title><link>http://arxiv.org/abs/2401.10220v1</link><description>Foundation models encode rich representations that can be adapted to adesired task by fine-tuning on task-specific data. However, fine-tuning a modelon one particular data distribution often compromises the model's originalperformance on other distributions. Current methods for robust fine-tuningutilize hand-crafted regularization techniques to constrain the fine-tuningprocess towards the base foundation model. Yet, it is hard to precisely specifywhat characteristics of the foundation model to retain during fine-tuning, asthis depends on how the pre-training, fine-tuning, and evaluation datadistributions relate to each other. We propose AutoFT, a data-driven approachfor guiding foundation model fine-tuning. AutoFT optimizes fine-tuninghyperparameters to maximize performance on a small out-of-distribution (OOD)validation set. To guide fine-tuning in a granular way, AutoFT searches ahighly expressive hyperparameter space that includes weight coefficients formany different losses, in addition to learning rate and weight decay values. Weevaluate AutoFT on nine natural distribution shifts which include domain shiftsand subpopulation shifts. Our experiments show that AutoFT significantlyimproves generalization to new OOD data, outperforming existing robustfine-tuning methods. Notably, AutoFT achieves new state-of-the-art performanceon the WILDS-iWildCam and WILDS-FMoW benchmarks, outperforming the previousbest methods by $6.0\%$ and $1.5\%$, respectively.</description><author>Caroline Choi, Yoonho Lee, Annie Chen, Allan Zhou, Aditi Raghunathan, Chelsea Finn</author><pubDate>Thu, 18 Jan 2024 18:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10220v1</guid></item><item><title>Edit One for All: Interactive Batch Image Editing</title><link>http://arxiv.org/abs/2401.10219v1</link><description>In recent years, image editing has advanced remarkably. With increased humancontrol, it is now possible to edit an image in a plethora of ways; fromspecifying in text what we want to change, to straight up dragging the contentsof the image in an interactive point-based manner. However, most of the focushas remained on editing single images at a time. Whether and how we cansimultaneously edit large batches of images has remained understudied. With thegoal of minimizing human supervision in the editing process, this paperpresents a novel method for interactive batch image editing using StyleGAN asthe medium. Given an edit specified by users in an example image (e.g., makethe face frontal), our method can automatically transfer that edit to othertest images, so that regardless of their initial state (pose), they all arriveat the same final state (e.g., all facing front). Extensive experimentsdemonstrate that edits performed using our method have similar visual qualityto existing single-image-editing methods, while having more visual consistencyand saving significant time and human effort.</description><author>Thao Nguyen, Utkarsh Ojha, Yuheng Li, Haotian Liu, Yong Jae Lee</author><pubDate>Thu, 18 Jan 2024 18:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10219v1</guid></item><item><title>Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by Tracing their Contributions</title><link>http://arxiv.org/abs/2401.10217v1</link><description>The many variations of Implicit Neural Representations (INRs), where a neuralnetwork is trained as a continuous representation of a signal, have tremendouspractical utility for downstream tasks including novel view synthesis, videocompression, and image superresolution. Unfortunately, the inner workings ofthese networks are seriously under-studied. Our work, eXplaining the ImplicitNeural Canvas (XINC), is a unified framework for explaining properties of INRsby examining the strength of each neuron's contribution to each output pixel.We call the aggregate of these contribution maps the Implicit Neural Canvas andwe use this concept to demonstrate that the INRs which we study learn to''see'' the frames they represent in surprising ways. For example, INRs tend tohave highly distributed representations. While lacking high-level objectsemantics, they have a significant bias for color and edges, and are almostentirely space-agnostic. We arrive at our conclusions by examining how objectsare represented across time in video INRs, using clustering to visualizesimilar neurons across layers and architectures, and show that this isdominated by motion. These insights demonstrate the general usefulness of ouranalysis framework. Our project page is available athttps://namithap10.github.io/xinc.</description><author>Namitha Padmanabhan, Matthew Gwilliam, Pulkit Kumar, Shishira R Maiya, Max Ehrlich, Abhinav Shrivastava</author><pubDate>Thu, 18 Jan 2024 18:57:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10217v1</guid></item><item><title>Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products</title><link>http://arxiv.org/abs/2401.10216v1</link><description>Developing equivariant neural networks for the E(3) group plays an importantrole in modeling 3D data across real-world applications. Enforcing thisequivariance primarily involves the tensor products of irreduciblerepresentations (irreps). However, the computational complexity of suchoperations increases significantly as higher-order tensors are used. In thiswork, we propose a systematic approach to substantially accelerate thecomputation of the tensor products of irreps. We mathematically connect thecommonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which areintegrals of products of three spherical harmonics. Through Gaunt coefficients,the tensor product of irreps becomes equivalent to the multiplication betweenspherical functions represented by spherical harmonics. This perspectivefurther allows us to change the basis for the equivariant operations fromspherical harmonics to a 2D Fourier basis. Consequently, the multiplicationbetween spherical functions represented by a 2D Fourier basis can beefficiently computed via the convolution theorem and Fast Fourier Transforms.This transformation reduces the complexity of full tensor products of irrepsfrom $\mathcal{O}(L^6)$ to $\mathcal{O}(L^3)$, where $L$ is the max degree ofirreps. Leveraging this approach, we introduce the Gaunt Tensor Product, whichserves as a new method to construct efficient equivariant operations acrossdifferent model architectures. Our experiments on the Open Catalyst Project and3BPA datasets demonstrate both the increased efficiency and improvedperformance of our approach.</description><author>Shengjie Luo, Tianlang Chen, Aditi S. Krishnapriyan</author><pubDate>Thu, 18 Jan 2024 18:57:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10216v1</guid></item><item><title>MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use</title><link>http://arxiv.org/abs/2310.03128v4</link><description>Large language models (LLMs) have garnered significant attention due to theirimpressive natural language processing (NLP) capabilities. Recently, manystudies have focused on the tool utilization ability of LLMs. They primarilyinvestigated how LLMs effectively collaborate with given specific tools.However, in scenarios where LLMs serve as intelligent agents, as seen inapplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricatedecision-making processes that involve deciding whether to employ a tool andselecting the most suitable tool(s) from a collection of available tools tofulfill user requests. Therefore, in this paper, we introduce MetaTool, abenchmark designed to evaluate whether LLMs have tool usage awareness and cancorrectly choose tools. Specifically, we create a dataset called ToolE withinthe benchmark. This dataset contains various types of user queries in the formof prompts that trigger LLMs to use tools, including both single-tool andmulti-tool scenarios. Subsequently, we set the tasks for both tool usageawareness and tool selection. We define four subtasks from differentperspectives in tool selection, including tool selection with similar choices,tool selection in specific scenarios, tool selection with possible reliabilityissues, and multi-tool selection. We conduct experiments involving eightpopular LLMs and find that the majority of them still struggle to effectivelyselect tools, highlighting the existing gaps between LLMs and genuineintelligent agents. However, through the error analysis, we found there isstill significant room for improvement. Finally, we conclude with insights fortool developers -- we strongly recommend that tool developers choose anappropriate rewrite model for generating new descriptions based on thedownstream LLM the tool will apply to. Our code is in\href{https://github.com/HowieHwong/MetaTool}{Github}.</description><author>Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, Lichao Sun</author><pubDate>Thu, 18 Jan 2024 18:57:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03128v4</guid></item><item><title>GPAvatar: Generalizable and Precise Head Avatar from Image(s)</title><link>http://arxiv.org/abs/2401.10215v1</link><description>Head avatar reconstruction, crucial for applications in virtual reality,online meetings, gaming, and film industries, has garnered substantialattention within the computer vision community. The fundamental objective ofthis field is to faithfully recreate the head avatar and precisely controlexpressions and postures. Existing methods, categorized into 2D-based warping,mesh-based, and neural rendering approaches, present challenges in maintainingmulti-view consistency, incorporating non-facial information, and generalizingto new identities. In this paper, we propose a framework named GPAvatar thatreconstructs 3D head avatars from one or several images in a single forwardpass. The key idea of this work is to introduce a dynamic point-basedexpression field driven by a point cloud to precisely and effectively captureexpressions. Furthermore, we use a Multi Tri-planes Attention (MTA) fusionmodule in the tri-planes canonical field to leverage information from multipleinput images. The proposed method achieves faithful identity reconstruction,precise expression control, and multi-view consistency, demonstrating promisingresults for free-viewpoint rendering and novel view synthesis.</description><author>Xuangeng Chu, Yu Li, Ailing Zeng, Tianyu Yang, Lijian Lin, Yunfei Liu, Tatsuya Harada</author><pubDate>Thu, 18 Jan 2024 18:56:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10215v1</guid></item><item><title>Functional Invariants to Watermark Large Transformers</title><link>http://arxiv.org/abs/2310.11446v2</link><description>The rapid growth of transformer-based models increases the concerns abouttheir integrity and ownership insurance. Watermarking addresses this issue byembedding a unique identifier into the model, while preserving its performance.However, most existing approaches require to optimize the weights to imprintthe watermark signal, which is not suitable at scale due to the computationalcost. This paper explores watermarks with virtually no computational cost,applicable to a non-blind white-box setting (assuming access to both theoriginal and watermarked networks). They generate functionally equivalentcopies by leveraging the models' invariance, via operations like dimensionpermutations or scaling/unscaling. This enables to watermark models without anychange in their outputs and remains stealthy. Experiments demonstrate theeffectiveness of the approach and its robustness against various modeltransformations (fine-tuning, quantization, pruning), making it a practicalsolution to protect the integrity of large models.</description><author>Pierre Fernandez, Guillaume Couairon, Teddy Furon, Matthijs Douze</author><pubDate>Thu, 18 Jan 2024 18:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11446v2</guid></item><item><title>MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer</title><link>http://arxiv.org/abs/2401.10208v1</link><description>Developing generative models for interleaved image-text data has bothresearch and practical value. It requires models to understand the interleavedsequences and subsequently generate images and text. However, existing attemptsare limited by the issue that the fixed number of visual tokens cannotefficiently capture image details, which is particularly problematic in themulti-image scenarios. To address this, this paper presents MM-Interleaved, anend-to-end generative model for interleaved image-text data. It introduces amulti-scale and multi-image feature synchronizer module, allowing direct accessto fine-grained image features in the previous context during the generationprocess. MM-Interleaved is end-to-end pre-trained on both paired andinterleaved image-text corpora. It is further enhanced through a supervisedfine-tuning phase, wherein the model improves its ability to follow complexmulti-modal instructions. Experiments demonstrate the versatility ofMM-Interleaved in recognizing visual details following multi-modal instructionsand generating consistent images following both textual and visual conditions.Code and models are available at\url{https://github.com/OpenGVLab/MM-Interleaved}.</description><author>Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, Hongsheng Li, Yu Qiao, Jifeng Dai</author><pubDate>Thu, 18 Jan 2024 18:50:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10208v1</guid></item><item><title>Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems</title><link>http://arxiv.org/abs/2401.10207v1</link><description>This paper addresses trust issues created from the ubiquity of black boxalgorithms and surrogate explainers in Explainable Intrusion Detection Systems(X-IDS). While Explainable Artificial Intelligence (XAI) aims to enhancetransparency, black box surrogate explainers, such as Local InterpretableModel-Agnostic Explanation (LIME) and SHapley Additive exPlanation (SHAP), aredifficult to trust. The black box nature of these surrogate explainers makesthe process behind explanation generation opaque and difficult to understand.To avoid this problem, one can use transparent white box algorithms such asRule Extraction (RE). There are three types of RE algorithms: pedagogical,decompositional, and eclectic. Pedagogical methods offer fast but untrustworthywhite-box explanations, while decompositional RE provides trustworthyexplanations with poor scalability. This work explores eclectic ruleextraction, which strikes a balance between scalability and trustworthiness. Bycombining techniques from pedagogical and decompositional approaches, eclecticrule extraction leverages the advantages of both, while mitigating some oftheir drawbacks. The proposed Hybrid X-IDS architecture features eclectic RE asa white box surrogate explainer for black box Deep Neural Networks (DNN). Thepresented eclectic RE algorithm extracts human-readable rules from hiddenlayers, facilitating explainable and trustworthy rulesets. Evaluations onUNSW-NB15 and CIC-IDS-2017 datasets demonstrate the algorithm's ability togenerate rulesets with 99.9% accuracy, mimicking DNN outputs. The contributionsof this work include the hybrid X-IDS architecture, the eclectic ruleextraction algorithm applicable to intrusion detection datasets, and a thoroughanalysis of performance and explainability, demonstrating the trade-offsinvolved in rule extraction speed and accuracy.</description><author>Jesse Ables, Nathaniel Childers, William Anderson, Sudip Mittal, Shahram Rahimi, Ioana Banicescu, Maria Seale</author><pubDate>Thu, 18 Jan 2024 18:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10207v1</guid></item><item><title>Maximal-Capacity Discrete Memoryless Channel Identification</title><link>http://arxiv.org/abs/2401.10204v1</link><description>The problem of identifying the channel with the highest capacity amongseveral discrete memoryless channels (DMCs) is considered. The problem is castas a pure-exploration multi-armed bandit problem, which follows the practicaluse of training sequences to sense the communication channel statistics. Acapacity estimator is proposed and tight confidence bounds on the estimatorerror are derived. Based on this capacity estimator, a gap-eliminationalgorithm termed BestChanID is proposed, which is oblivious to thecapacity-achieving input distribution and is guaranteed to output the DMC withthe largest capacity, with a desired confidence. Furthermore, two additionalalgorithms NaiveChanSel and MedianChanEl, that output with certain confidence aDMC with capacity close to the maximal, are introduced. Each of thosealgorithms is beneficial in a different regime and can be used as a subroutinein BestChanID. The sample complexity of all algorithms is analyzed as afunction of the desired confidence parameter, the number of channels, and thechannels' input and output alphabet sizes. The cost of best channelidentification is shown to scale quadratically with the alphabet size, and afundamental lower bound for the required number of channel senses to identifythe best channel with a certain confidence is derived.</description><author>Maximilian Egger, Rawad Bitar, Antonia Wachter-Zeh, Deniz Gündüz, Nir Weinberger</author><pubDate>Thu, 18 Jan 2024 18:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10204v1</guid></item><item><title>Unboxing Tree Ensembles for interpretability: a hierarchical visualization tool and a multivariate optimal re-built tree</title><link>http://arxiv.org/abs/2302.07580v2</link><description>The interpretability of models has become a crucial issue in Machine Learningbecause of algorithmic decisions' growing impact on real-world applications.Tree ensemble methods, such as Random Forests or XgBoost, are powerful learningtools for classification tasks. However, while combining multiple trees mayprovide higher prediction quality than a single one, it sacrifices theinterpretability property resulting in "black-box" models. In light of this, weaim to develop an interpretable representation of a tree-ensemble model thatcan provide valuable insights into its behavior. First, given a targettree-ensemble model, we develop a hierarchical visualization tool based on aheatmap representation of the forest's feature use, considering the frequencyof a feature and the level at which it is selected as an indicator ofimportance. Next, we propose a mixed-integer linear programming (MILP)formulation for constructing a single optimal multivariate tree that accuratelymimics the target model predictions. The goal is to provide an interpretablesurrogate model based on oblique hyperplane splits, which uses only the mostrelevant features according to the defined forest's importance indicators. TheMILP model includes a penalty on feature selection based on their frequency inthe forest to further induce sparsity of the splits. The natural formulationhas been strengthened to improve the computational performance of{mixed-integer} software. Computational experience is carried out on benchmarkdatasets from the UCI repository using a state-of-the-art off-the-shelf solver.Results show that the proposed model is effective in yielding a shallowinterpretable tree approximating the tree-ensemble decision function.</description><author>Giulia Di Teodoro, Marta Monaci, Laura Palagi</author><pubDate>Thu, 18 Jan 2024 18:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07580v2</guid></item><item><title>Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4</title><link>http://arxiv.org/abs/2312.16171v2</link><description>This paper introduces 26 guiding principles designed to streamline theprocess of querying and prompting large language models. Our goal is tosimplify the underlying concepts of formulating questions for various scales oflarge language models, examining their abilities, and enhancing usercomprehension on the behaviors of different scales of large language modelswhen feeding into different prompts. Extensive experiments are conducted onLLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of theproposed principles on instructions and prompts design. We hope that this workcan provide a better guide for researchers working on the prompting of largelanguage models. Project page is available athttps://github.com/VILA-Lab/ATLAS.</description><author>Sondos Mahmoud Bsharat, Aidar Myrzakhan, Zhiqiang Shen</author><pubDate>Thu, 18 Jan 2024 18:41:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16171v2</guid></item><item><title>Compositional Program Generation for Few-Shot Systematic Generalization</title><link>http://arxiv.org/abs/2309.16467v2</link><description>Compositional generalization is a key ability of humans that enables us tolearn new concepts from only a handful examples. Neural machine learningmodels, including the now ubiquitous Transformers, struggle to generalize inthis way, and typically require thousands of examples of a concept duringtraining in order to generalize meaningfully. This difference in abilitybetween humans and artificial neural architectures, motivates this study on aneuro-symbolic architecture called the Compositional Program Generator (CPG).CPG has three key features: \textit{modularity}, \textit{composition}, and\textit{abstraction}, in the form of grammar rules, that enable it togeneralize both systematically to new concepts in a few-shot manner, as well asproductively by length on various sequence-to-sequence language tasks. For eachinput, CPG uses a grammar of the input language and a parser to generate aparse in which each grammar rule is assigned its own unique semantic module, aprobabilistic copy or substitution program. Instances with the same parse arealways processed with the same composed modules, while those with differentparses may be processed with different modules. CPG learns parameters for themodules and is able to learn the semantics for new rules and typesincrementally, without forgetting or retraining on rules it's already seen. Itachieves perfect generalization on both the SCAN and COGS benchmarks using just14 examples for SCAN and 22 examples for COGS -- state-of-the-art accuracy witha 1000x improvement in sample efficiency.</description><author>Tim Klinger, Luke Liu, Soham Dan, Maxwell Crouse, Parikshit Ram, Alexander Gray</author><pubDate>Thu, 18 Jan 2024 18:25:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16467v2</guid></item><item><title>Divide and not forget: Ensemble of selectively trained experts in Continual Learning</title><link>http://arxiv.org/abs/2401.10191v1</link><description>Class-incremental learning is becoming more popular as it helps models widentheir applicability while not forgetting what they already know. A trend inthis area is to use a mixture-of-expert technique, where different models worktogether to solve the task. However, the experts are usually trained all atonce using whole task data, which makes them all prone to forgetting andincreasing computational burden. To address this limitation, we introduce anovel approach named SEED. SEED selects only one, the most optimal expert for aconsidered task, and uses data from this task to fine-tune only this expert.For this purpose, each expert represents each class with a Gaussiandistribution, and the optimal expert is selected based on the similarity ofthose distributions. Consequently, SEED increases diversity and heterogeneitywithin the experts while maintaining the high stability of this ensemblemethod. The extensive experiments demonstrate that SEED achievesstate-of-the-art performance in exemplar-free settings across variousscenarios, showing the potential of expert diversification through data incontinual learning.</description><author>Grzegorz Rypeść, Sebastian Cygert, Valeriya Khan, Tomasz Trzciński, Bartosz Zieliński, Bartłomiej Twardowski</author><pubDate>Thu, 18 Jan 2024 18:25:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10191v1</guid></item><item><title>Less is More for Long Document Summary Evaluation by LLMs</title><link>http://arxiv.org/abs/2309.07382v2</link><description>Large Language Models (LLMs) have shown promising performance in summaryevaluation tasks, yet they face challenges such as high computational costs andthe Lost-in-the-Middle problem where important information in the middle oflong documents is often overlooked. To address these issues, this paperintroduces a novel approach, Extract-then-Evaluate, which involves extractingkey sentences from a long source document and then evaluating the summary byprompting LLMs. The results reveal that the proposed method not onlysignificantly reduces evaluation costs but also exhibits a higher correlationwith human evaluations. Furthermore, we provide practical recommendations foroptimal document length and sentence extraction methods, contributing to thedevelopment of cost-effective yet more accurate methods for LLM-based textgeneration evaluation.</description><author>Yunshu Wu, Hayate Iso, Pouya Pezeshkpour, Nikita Bhutani, Estevam Hruschka</author><pubDate>Thu, 18 Jan 2024 18:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07382v2</guid></item><item><title>A Kaczmarz-inspired approach to accelerate the optimization of neural network wavefunctions</title><link>http://arxiv.org/abs/2401.10190v1</link><description>Neural network wavefunctions optimized using the variational Monte Carlomethod have been shown to produce highly accurate results for the electronicstructure of atoms and small molecules, but the high cost of optimizing suchwavefunctions prevents their application to larger systems. We propose theSubsampled Projected-Increment Natural Gradient Descent (SPRING) optimizer toreduce this bottleneck. SPRING combines ideas from the recently introducedminimum-step stochastic reconfiguration optimizer (MinSR) and the classicalrandomized Kaczmarz method for solving linear least-squares problems. Wedemonstrate that SPRING outperforms both MinSR and the popularKronecker-Factored Approximate Curvature method (KFAC) across a number of smallatoms and molecules, given that the learning rates of all methods are optimallytuned. For example, on the oxygen atom, SPRING attains chemical accuracy afterforty thousand training iterations, whereas both MinSR and KFAC fail to do soeven after one hundred thousand iterations.</description><author>Gil Goldshlager, Nilin Abrahamsen, Lin Lin</author><pubDate>Thu, 18 Jan 2024 18:23:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10190v1</guid></item><item><title>Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction</title><link>http://arxiv.org/abs/2401.10189v1</link><description>Fine-grained few-shot entity extraction in the chemical domain faces twounique challenges. First, compared with entity extraction tasks in the generaldomain, sentences from chemical papers usually contain more entities. Moreover,entity extraction models usually have difficulty extracting entities oflong-tailed types. In this paper, we propose Chem-FINESE, a novelsequence-to-sequence (seq2seq) based few-shot entity extraction approach, toaddress these two challenges. Our Chem-FINESE has two components: a seq2seqentity extractor to extract named entities from the input sentence and aseq2seq self-validation module to reconstruct the original input sentence fromextracted entities. Inspired by the fact that a good entity extraction systemneeds to extract entities faithfully, our new self-validation module leveragesentity extraction results to reconstruct the original input sentence. Besides,we design a new contrastive loss to reduce excessive copying during theextraction process. Finally, we release ChemNER+, a new fine-grained chemicalentity extraction dataset that is annotated by domain experts with the ChemNERschema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasetsshow that our newly proposed framework has contributed up to 8.26% and 6.84%absolute F1-score gains respectively.</description><author>Qingyun Wang, Zixuan Zhang, Hongxiang Li, Xuan Liu, Jiawei Han, Heng Ji, Huimin Zhao</author><pubDate>Thu, 18 Jan 2024 18:20:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10189v1</guid></item><item><title>On Error Propagation of Diffusion Models</title><link>http://arxiv.org/abs/2308.05021v3</link><description>Although diffusion models (DMs) have shown promising performances in a numberof tasks (e.g., speech synthesis and image generation), they might suffer fromerror propagation because of their sequential structure. However, this is notcertain because some sequential models, such as Conditional Random Field (CRF),are free from this problem. To address this issue, we develop a theoreticalframework to mathematically formulate error propagation in the architecture ofDMs, The framework contains three elements, including modular error, cumulativeerror, and propagation equation. The modular and cumulative errors are relatedby the equation, which interprets that DMs are indeed affected by errorpropagation. Our theoretical study also suggests that the cumulative error isclosely related to the generation quality of DMs. Based on this finding, weapply the cumulative error as a regularization term to reduce errorpropagation. Because the term is computationally intractable, we derive itsupper bound and design a bootstrap algorithm to efficiently estimate the boundfor optimization. We have conducted extensive experiments on multiple imagedatasets, showing that our proposed regularization reduces error propagation,significantly improves vanilla DMs, and outperforms previous baselines.</description><author>Yangming Li, Mihaela van der Schaar</author><pubDate>Thu, 18 Jan 2024 18:18:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05021v3</guid></item><item><title>Benchmarking Robustness of Multimodal Image-Text Models under Distribution Shift</title><link>http://arxiv.org/abs/2212.08044v2</link><description>Multimodal image-text models have shown remarkable performance in the pastfew years. However, evaluating robustness against distribution shifts iscrucial before adopting them in real-world applications. In this work, weinvestigate the robustness of 12 popular open-sourced image-text models undercommon perturbations on five tasks (image-text retrieval, visual reasoning,visual entailment, image captioning, and text-to-image generation). Inparticular, we propose several new multimodal robustness benchmarks by applying17 image perturbation and 16 text perturbation techniques on top of existingdatasets. We observe that multimodal models are not robust to image and textperturbations, especially to image perturbations. Among the tested perturbationmethods, character-level perturbations constitute the most severe distributionshift for text, and zoom blur is the most severe shift for image data. We alsointroduce two new robustness metrics (\textbf{MMI} for MultiModal Impact scoreand \textbf{MOR} for Missing Object Rate) for proper evaluations of multimodalmodels. We hope our extensive study sheds light on new directions for thedevelopment of robust multimodal models. More details can be found on theproject webpage: \url{https://MMRobustness.github.io}.</description><author>Jielin Qiu, Yi Zhu, Xingjian Shi, Florian Wenzel, Zhiqiang Tang, Ding Zhao, Bo Li, Mu Li</author><pubDate>Thu, 18 Jan 2024 18:16:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08044v2</guid></item><item><title>Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models</title><link>http://arxiv.org/abs/2309.14068v3</link><description>Because diffusion models have shown impressive performances in a number oftasks, such as image synthesis, there is a trend in recent works to prove (withcertain assumptions) that these models have strong approximation capabilities.In this paper, we show that current diffusion models actually have anexpressive bottleneck in backward denoising and some assumption made byexisting theoretical guarantees is too strong. Based on this finding, we provethat diffusion models have unbounded errors in both local and global denoising.In light of our theoretical studies, we introduce soft mixture denoising (SMD),an expressive and efficient model for backward denoising. SMD not only permitsdiffusion models to well approximate any Gaussian mixture distributions intheory, but also is simple and efficient for implementation. Our experiments onmultiple image datasets show that SMD significantly improves different types ofdiffusion models (e.g., DDPM), espeically in the situation of few backwarditerations.</description><author>Yangming Li, Boris van Breugel, Mihaela van der Schaar</author><pubDate>Thu, 18 Jan 2024 18:16:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14068v3</guid></item><item><title>Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation</title><link>http://arxiv.org/abs/2401.10186v1</link><description>We investigate to which extent open large language models (LLMs) can generatecoherent and relevant text from structured data. To prevent bias frombenchmarks leaked into LLM training data, we collect Quintd-1: an ad-hocbenchmark for five data-to-text (D2T) generation tasks, consisting ofstructured data records in standard formats gathered from public APIs. Weleverage reference-free evaluation metrics and LLMs' in-context learningcapabilities, allowing us to test the models with no human-written references.Our evaluation focuses on annotating semantic accuracy errors on token-level,combining human annotators and a metric based on GPT-4. Our systematicexamination of the models' behavior across domains and tasks suggests thatstate-of-the-art open LLMs with 7B parameters can generate fluent and coherenttext from various standard data formats in zero-shot settings. However, we alsoshow that semantic accuracy of the outputs remains a major issue: on ourbenchmark, 80% of outputs of open LLMs contain a semantic error according tohuman annotators (91% according to GPT-4). Our code, data, and model outputsare available at https://d2t-llm.github.io.</description><author>Zdeněk Kasner, Ondřej Dušek</author><pubDate>Thu, 18 Jan 2024 18:15:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10186v1</guid></item><item><title>Transfer Learning in Human Activity Recognition: A Survey</title><link>http://arxiv.org/abs/2401.10185v1</link><description>Sensor-based human activity recognition (HAR) has been an active researcharea, owing to its applications in smart environments, assisted living,fitness, healthcare, etc. Recently, deep learning based end-to-end training hasresulted in state-of-the-art performance in domains such as computer vision andnatural language, where large amounts of annotated data are available. However,large quantities of annotated data are not available for sensor-based HAR.Moreover, the real-world settings on which the HAR is performed differ in termsof sensor modalities, classification tasks, and target users. To address thisproblem, transfer learning has been employed extensively. In this survey, wefocus on these transfer learning methods in the application domains of smarthome and wearables-based HAR. In particular, we provide a problem-solutionperspective by categorizing and presenting the works in terms of theircontributions and the challenges they address. We also present an updated viewof the state-of-the-art for both application domains. Based on our analysis of205 papers, we highlight the gaps in the literature and provide a roadmap foraddressing them. This survey provides a reference to the HAR community, bysummarizing the existing works and providing a promising research agenda.</description><author>Sourish Gunesh Dhekane, Thomas Ploetz</author><pubDate>Thu, 18 Jan 2024 18:12:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10185v1</guid></item><item><title>Uncovering local aggregated air quality index with smartphone captured images leveraging efficient deep convolutional neural network</title><link>http://arxiv.org/abs/2308.03200v3</link><description>The prevalence and mobility of smartphones make these a widely used tool forenvironmental health research. However, their potential for determiningaggregated air quality index (AQI) based on PM2.5 concentration in specificlocations remains largely unexplored in the existing literature. In this paper,we thoroughly examine the challenges associated with predictinglocation-specific PM2.5 concentration using images taken with smartphonecameras. The focus of our study is on Dhaka, the capital of Bangladesh, due toits significant air pollution levels and the large population exposed to it.Our research involves the development of a Deep Convolutional Neural Network(DCNN), which we train using over a thousand outdoor images taken andannotated. These photos are captured at various locations in Dhaka, and theirlabels are based on PM2.5 concentration data obtained from the local USconsulate, calculated using the NowCast algorithm. Through supervised learning,our model establishes a correlation index during training, enhancing itsability to function as a Picture-based Predictor of PM2.5 Concentration (PPPC).This enables the algorithm to calculate an equivalent daily averaged AQI indexfrom a smartphone image. Unlike, popular overly parameterized models, our modelshows resource efficiency since it uses fewer parameters. Furthermore, testresults indicate that our model outperforms popular models like ViT and INN, aswell as popular CNN-based models such as VGG19, ResNet50, and MobileNetV2, inpredicting location-specific PM2.5 concentration. Our dataset is the firstpublicly available collection that includes atmospheric images andcorresponding PM2.5 measurements from Dhaka. Our codes and dataset areavailable at https://github.com/lepotatoguy/aqi.</description><author>Joyanta Jyoti Mondal, Md. Farhadul Islam, Raima Islam, Nowsin Kabir Rhidi, Sarfaraz Newaz, Meem Arafat Manab, A. B. M. Alim Al Islam, Jannatun Noor</author><pubDate>Thu, 18 Jan 2024 18:10:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03200v3</guid></item><item><title>Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields</title><link>http://arxiv.org/abs/2401.10178v1</link><description>In this study, we present evidence suggesting that depthwise convolutionalkernels are effectively replicating the structural intricacies of thebiological receptive fields observed in the mammalian retina. We provideanalytics of trained kernels from various state-of-the-art modelssubstantiating this evidence. Inspired by this intriguing discovery, we proposean initialization scheme that draws inspiration from the biological receptivefields. Experimental analysis of the ImageNet dataset with multiple CNNarchitectures featuring depthwise convolutions reveals a marked enhancement inthe accuracy of the learned model when initialized with biologically derivedweights. This underlies the potential for biologically inspired computationalmodels to further our understanding of vision processing systems and to improvethe efficacy of convolutional networks.</description><author>Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu</author><pubDate>Thu, 18 Jan 2024 18:06:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10178v1</guid></item><item><title>Comprehensive OOD Detection Improvements</title><link>http://arxiv.org/abs/2401.10176v1</link><description>As machine learning becomes increasingly prevalent in impactful decisions,recognizing when inference data is outside the model's expected inputdistribution is paramount for giving context to predictions.Out-of-distribution (OOD) detection methods have been created for this task.Such methods can be split into representation-based or logit-based methods fromwhether they respectively utilize the model's embeddings or predictions for OODdetection. In contrast to most papers which solely focus on one such group, weaddress both. We employ dimensionality reduction on feature embeddings inrepresentation-based methods for both time speedups and improved performance.Additionally, we propose DICE-COL, a modification of the popular logit-basedmethod Directed Sparsification (DICE) that resolves an unnoticed flaw. Wedemonstrate the effectiveness of our methods on the OpenOODv1.5 benchmarkframework, where they significantly improve performance and setstate-of-the-art results.</description><author>Anish Lakkapragada, Amol Khanna, Edward Raff, Nathan Inkawhich</author><pubDate>Thu, 18 Jan 2024 18:05:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10176v1</guid></item><item><title>SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild</title><link>http://arxiv.org/abs/2401.10171v1</link><description>We present SHINOBI, an end-to-end framework for the reconstruction of shape,material, and illumination from object images captured with varying lighting,pose, and background. Inverse rendering of an object based on unconstrainedimage collections is a long-standing challenge in computer vision and graphicsand requires a joint optimization over shape, radiance, and pose. We show thatan implicit shape representation based on a multi-resolution hash encodingenables faster and robust shape reconstruction with joint camera alignmentoptimization that outperforms prior work. Further, to enable the editing ofillumination and object reflectance (i.e. material) we jointly optimize BRDFand illumination together with the object's shape. Our method is class-agnosticand works on in-the-wild image collections of objects to produce relightable 3Dassets for several use cases such as AR/VR, movies, games, etc. Project page:https://shinobi.aengelhardt.com Video:https://www.youtube.com/watch?v=iFENQ6AcYd8&amp;feature=youtu.be</description><author>Andreas Engelhardt, Amit Raj, Mark Boss, Yunzhi Zhang, Abhishek Kar, Yuanzhen Li, Deqing Sun, Ricardo Martin Brualla, Jonathan T. Barron, Hendrik P. A. Lensch, Varun Jampani</author><pubDate>Thu, 18 Jan 2024 18:01:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10171v1</guid></item><item><title>VMamba: Visual State Space Model</title><link>http://arxiv.org/abs/2401.10166v1</link><description>Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) stand asthe two most popular foundation models for visual representation learning.While CNNs exhibit remarkable scalability with linear complexity w.r.t. imageresolution, ViTs surpass them in fitting capabilities despite contending withquadratic complexity. A closer inspection reveals that ViTs achieve superiorvisual modeling performance through the incorporation of global receptivefields and dynamic weights. This observation motivates us to propose a novelarchitecture that inherits these components while enhancing computationalefficiency. To this end, we draw inspiration from the recently introduced statespace model and propose the Visual State Space Model (VMamba), which achieveslinear complexity without sacrificing global receptive fields. To address theencountered direction-sensitive issue, we introduce the Cross-Scan Module (CSM)to traverse the spatial domain and convert any non-causal visual image intoorder patch sequences. Extensive experimental results substantiate that VMambanot only demonstrates promising capabilities across various visual perceptiontasks, but also exhibits more pronounced advantages over established benchmarksas the image resolution increases. Source code has been available athttps://github.com/MzeroMiko/VMamba.</description><author>Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, Yunfan Liu</author><pubDate>Thu, 18 Jan 2024 17:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10166v1</guid></item><item><title>Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery</title><link>http://arxiv.org/abs/2310.19776v3</link><description>In the quest for unveiling novel categories at test time, we confront theinherent limitations of traditional supervised recognition models that arerestricted by a predefined category set. While strides have been made in therealms of self-supervised and open-world learning towards test-time categorydiscovery, a crucial yet often overlooked question persists: what exactlydelineates a category? In this paper, we conceptualize a category through thelens of optimization, viewing it as an optimal solution to a well-definedproblem. Harnessing this unique conceptualization, we propose a novel,efficient and self-supervised method capable of discovering previously unknowncategories at test time. A salient feature of our approach is the assignment ofminimum length category codes to individual data instances, which encapsulatesthe implicit category hierarchy prevalent in real-world datasets. Thismechanism affords us enhanced control over category granularity, therebyequipping our model to handle fine-grained categories adeptly. Experimentalevaluations, bolstered by state-of-the-art benchmark comparisons, testify tothe efficacy of our solution in managing unknown categories at test time.Furthermore, we fortify our proposition with a theoretical foundation,providing proof of its optimality. Our code is available athttps://github.com/SarahRastegar/InfoSieve.</description><author>Sarah Rastegar, Hazel Doughty, Cees G. M. Snoek</author><pubDate>Thu, 18 Jan 2024 17:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19776v3</guid></item><item><title>Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values</title><link>http://arxiv.org/abs/2311.09262v2</link><description>The potential impact of an academic paper is determined by various factors,including its popularity and contribution. Existing models usually estimateoriginal citation counts based on static graphs and fail to differentiatevalues from nuanced perspectives. In this study, we propose a novel graphneural network to Disentangle the Potential impacts of Papers into Diffusion,Conformity, and Contribution values (called DPPDCC). Given a target paper,DPPDCC encodes temporal and structural features within the constructed dynamicheterogeneous graph. Particularly, to capture the knowledge flow, we emphasizethe importance of comparative and co-cited/citing information between papersand aggregate snapshots evolutionarily. To unravel popularity, we contrastaugmented graphs to extract the essence of diffusion and predict theaccumulated citation binning to model conformity. We further apply orthogonalconstraints to encourage distinct modeling of each perspective and preserve theinherent value of contribution. To evaluate models' generalization for paperspublished at various times, we reformulate the problem by partitioning databased on specific time points to mirror real-world conditions. Extensiveexperimental results on three datasets demonstrate that DPPDCC significantlyoutperforms baselines for previously, freshly, and immediately publishedpapers. Further analyses confirm its robust capabilities. We will make ourdatasets and codes publicly available.</description><author>Zhikai Xue, Guoxiu He, Zhuoren Jiang, Sichen Gu, Yangyang Kang, Star Zhao, Wei Lu</author><pubDate>Thu, 18 Jan 2024 17:38:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09262v2</guid></item><item><title>Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation</title><link>http://arxiv.org/abs/2401.10150v1</link><description>Recent large-scale pre-trained diffusion models have demonstrated a powerfulgenerative ability to produce high-quality videos from detailed textdescriptions. However, exerting control over the motion of objects in videosgenerated by any video diffusion model is a challenging problem. In this paper,we propose a novel zero-shot moving object trajectory control framework,Motion-Zero, to enable a bounding-box-trajectories-controlled text-to-videodiffusion model.To this end, an initial noise prior module is designed toprovide a position-based prior to improve the stability of the appearance ofthe moving object and the accuracy of position. In addition, based on theattention map of the U-net, spatial constraints are directly applied to thedenoising process of diffusion models, which further ensures the positional andspatial consistency of moving objects during the inference. Furthermore,temporal consistency is guaranteed with a proposed shift temporal attentionmechanism. Our method can be flexibly applied to various state-of-the-art videodiffusion models without any training process. Extensive experimentsdemonstrate our proposed method can control the motion trajectories of objectsand generate high-quality videos.</description><author>Changgu Chen, Junwei Shu, Lianggangxu Chen, Gaoqi He, Changbo Wang, Yang Li</author><pubDate>Thu, 18 Jan 2024 17:22:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10150v1</guid></item><item><title>Multi-Agent Reinforcement Learning for Maritime Operational Technology Cyber Security</title><link>http://arxiv.org/abs/2401.10149v1</link><description>This paper demonstrates the potential for autonomous cyber defence to beapplied on industrial control systems and provides a baseline environment tofurther explore Multi-Agent Reinforcement Learning's (MARL) application to thisproblem domain. It introduces a simulation environment, IPMSRL, of a genericIntegrated Platform Management System (IPMS) and explores the use of MARL forautonomous cyber defence decision-making on generic maritime based IPMSOperational Technology (OT). OT cyber defensive actions are less mature thanthey are for Enterprise IT. This is due to the relatively brittle nature of OTinfrastructure originating from the use of legacy systems, design-timeengineering assumptions, and lack of full-scale modern security controls. Thereare many obstacles to be tackled across the cyber landscape due to continuallyincreasing cyber-attack sophistication and the limitations of traditionalIT-centric cyber defence solutions. Traditional IT controls are rarely deployedon OT infrastructure, and where they are, some threats aren't fully addressed.In our experiments, a shared critic implementation of Multi Agent ProximalPolicy Optimisation (MAPPO) outperformed Independent Proximal PolicyOptimisation (IPPO). MAPPO reached an optimal policy (episode outcome mean of1) after 800K timesteps, whereas IPPO was only able to reach an episode outcomemean of 0.966 after one million timesteps. Hyperparameter tuning greatlyimproved training performance. Across one million timesteps the tunedhyperparameters reached an optimal policy whereas the default hyperparametersonly managed to win sporadically, with most simulations resulting in a draw. Wetested a real-world constraint, attack detection alert success, and found thatwhen alert success probability is reduced to 0.75 or 0.9, the MARL defenderswere still able to win in over 97.5% or 99.5% of episodes, respectively.</description><author>Alec Wilson, Ryan Menzies, Neela Morarji, David Foster, Marco Casassa Mont, Esin Turkbeyler, Lisa Gralewski</author><pubDate>Thu, 18 Jan 2024 17:22:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10149v1</guid></item><item><title>Explicitly Disentangled Representations in Object-Centric Learning</title><link>http://arxiv.org/abs/2401.10148v1</link><description>Extracting structured representations from raw visual data is an importantand long-standing challenge in machine learning. Recently, techniques forunsupervised learning of object-centric representations have raised growinginterest. In this context, enhancing the robustness of the latent features canimprove the efficiency and effectiveness of the training of downstream tasks. Apromising step in this direction is to disentangle the factors that causevariation in the data. Previously, Invariant Slot Attention disentangledposition, scale, and orientation from the remaining features. Extending thisapproach, we focus on separating the shape and texture components. Inparticular, we propose a novel architecture that biases object-centric modelstoward disentangling shape and texture components into two non-overlappingsubsets of the latent space dimensions. These subsets are known a priori, hencebefore the training process. Experiments on a range of object-centricbenchmarks reveal that our approach achieves the desired disentanglement whilealso numerically improving baseline performance in most cases. In addition, weshow that our method can generate novel textures for a specific object ortransfer textures between objects with distinct shapes.</description><author>Riccardo Majellaro, Jonathan Collu, Aske Plaat, Thomas M. Moerland</author><pubDate>Thu, 18 Jan 2024 17:22:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10148v1</guid></item><item><title>ICML 2023 Topological Deep Learning Challenge : Design and Results</title><link>http://arxiv.org/abs/2309.15188v4</link><description>This paper presents the computational challenge on topological deep learningthat was hosted within the ICML 2023 Workshop on Topology and Geometry inMachine Learning. The competition asked participants to provide open-sourceimplementations of topological neural networks from the literature bycontributing to the python packages TopoNetX (data processing) and TopoModelX(deep learning). The challenge attracted twenty-eight qualifying submissions inits two-month duration. This paper describes the design of the challenge andsummarizes its main findings.</description><author>Mathilde Papillon, Mustafa Hajij, Helen Jenne, Johan Mathe, Audun Myers, Theodore Papamarkou, Tolga Birdal, Tamal Dey, Tim Doster, Tegan Emerson, Gurusankar Gopalakrishnan, Devendra Govil, Aldo Guzmán-Sáenz, Henry Kvinge, Neal Livesay, Soham Mukherjee, Shreyas N. Samaga, Karthikeyan Natesan Ramamurthy, Maneel Reddy Karri, Paul Rosen, Sophia Sanborn, Robin Walters, Jens Agerberg, Sadrodin Barikbin, Claudio Battiloro, Gleb Bazhenov, Guillermo Bernardez, Aiden Brent, Sergio Escalera, Simone Fiorellino, Dmitrii Gavrilev, Mohammed Hassanin, Paul Häusner, Odin Hoff Gardaa, Abdelwahed Khamis, Manuel Lecha, German Magai, Tatiana Malygina, Rubén Ballester, Kalyan Nadimpalli, Alexander Nikitin, Abraham Rabinowitz, Alessandro Salatiello, Simone Scardapane, Luca Scofano, Suraj Singh, Jens Sjölund, Pav</author><pubDate>Thu, 18 Jan 2024 17:21:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15188v4</guid></item><item><title>Hyperbolic Image-Text Representations</title><link>http://arxiv.org/abs/2304.09172v3</link><description>Visual and linguistic concepts naturally organize themselves in a hierarchy,where a textual concept "dog" entails all images that contain dogs. Despitebeing intuitive, current large-scale vision and language models such as CLIP donot explicitly capture such hierarchy. We propose MERU, a contrastive modelthat yields hyperbolic representations of images and text. Hyperbolic spaceshave suitable geometric properties to embed tree-like data, so MERU can bettercapture the underlying hierarchy in image-text datasets. Our results show thatMERU learns a highly interpretable and structured representation space whilebeing competitive with CLIP's performance on standard multi-modal tasks likeimage classification and image-text retrieval. Our code and models areavailable at https://www.github.com/facebookresearch/meru</description><author>Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, Ramakrishna Vedantam</author><pubDate>Thu, 18 Jan 2024 17:13:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09172v3</guid></item><item><title>Model Compression Techniques in Biometrics Applications: A Survey</title><link>http://arxiv.org/abs/2401.10139v1</link><description>The development of deep learning algorithms has extensively empoweredhumanity's task automatization capacity. However, the huge improvement in theperformance of these models is highly correlated with their increasing level ofcomplexity, limiting their usefulness in human-oriented applications, which areusually deployed in resource-constrained devices. This led to the developmentof compression techniques that drastically reduce the computational and memorycosts of deep learning models without significant performance degradation. Thispaper aims to systematize the current literature on this topic by presenting acomprehensive survey of model compression techniques in biometricsapplications, namely quantization, knowledge distillation and pruning. Weconduct a critical analysis of the comparative value of these techniques,focusing on their advantages and disadvantages and presenting suggestions forfuture work directions that can potentially improve the current methods.Additionally, we discuss and analyze the link between model bias and modelcompression, highlighting the need to direct compression research toward modelfairness in future works.</description><author>Eduarda Caldeira, Pedro C. Neto, Marco Huber, Naser Damer, Ana F. Sequeira</author><pubDate>Thu, 18 Jan 2024 17:06:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10139v1</guid></item><item><title>Spatial-Temporal Large Language Model for Traffic Prediction</title><link>http://arxiv.org/abs/2401.10134v1</link><description>Traffic prediction, a critical component for intelligent transportationsystems, endeavors to foresee future traffic at specific locations usinghistorical data. Although existing traffic prediction models often emphasizedeveloping complex neural network structures, their accuracy has not seenimprovements accordingly. Recently, Large Language Models (LLMs) have shownoutstanding capabilities in time series analysis. Differing from existingmodels, LLMs progress mainly through parameter expansion and extensivepre-training while maintaining their fundamental structures. In this paper, wepropose a Spatial-Temporal Large Language Model (ST-LLM) for trafficprediction. Specifically, ST-LLM redefines the timesteps at each location astokens and incorporates a spatial-temporal embedding module to learn thespatial location and global temporal representations of tokens. Then theserepresentations are fused to provide each token with unified spatial andtemporal information. Furthermore, we propose a novel partially frozenattention strategy of the LLM, which is designed to capture spatial-temporaldependencies for traffic prediction. Comprehensive experiments on real trafficdatasets offer evidence that ST-LLM outperforms state-of-the-art models.Notably, the ST-LLM also exhibits robust performance in both few-shot andzero-shot prediction scenarios.</description><author>Chenxi Liu, Sun Yang, Qianxiong Xu, Zhishuai Li, Cheng Long, Ziyue Li, Rui Zhao</author><pubDate>Thu, 18 Jan 2024 17:03:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10134v1</guid></item><item><title>Few-shot learning for COVID-19 Chest X-Ray Classification with Imbalanced Data: An Inter vs. Intra Domain Study</title><link>http://arxiv.org/abs/2401.10129v1</link><description>Medical image datasets are essential for training models used incomputer-aided diagnosis, treatment planning, and medical research. However,some challenges are associated with these datasets, including variability indata distribution, data scarcity, and transfer learning issues when usingmodels pre-trained from generic images. This work studies the effect of thesechallenges at the intra- and inter-domain level in few-shot learning scenarioswith severe data imbalance. For this, we propose a methodology based on Siameseneural networks in which a series of techniques are integrated to mitigate theeffects of data scarcity and distribution imbalance. Specifically, differentinitialization and data augmentation methods are analyzed, and four adaptationsto Siamese networks of solutions to deal with imbalanced data are introduced,including data balancing and weighted loss, both separately and combined, andwith a different balance of pairing ratios. Moreover, we also assess theinference process considering four classifiers, namely Histogram, $k$NN, SVM,and Random Forest. Evaluation is performed on three chest X-ray datasets withannotated cases of both positive and negative COVID-19 diagnoses. The accuracyof each technique proposed for the Siamese architecture is analyzed separatelyand their results are compared to those obtained using equivalent methods on astate-of-the-art CNN. We conclude that the introduced techniques offerpromising improvements over the baseline in almost all cases, and that theselection of the technique may vary depending on the amount of data availableand the level of imbalance.</description><author>Alejandro Galán-Cuenca, Antonio Javier Gallego, Marcelo Saval-Calvo, Antonio Pertusa</author><pubDate>Thu, 18 Jan 2024 16:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10129v1</guid></item><item><title>Sub2Full: split spectrum to boost OCT despeckling without clean data</title><link>http://arxiv.org/abs/2401.10128v1</link><description>Optical coherence tomography (OCT) suffers from speckle noise, causing thedeterioration of image quality, especially in high-resolution modalities likevisible light OCT (vis-OCT). The potential of conventional supervised deeplearning denoising methods is limited by the difficulty of obtaining cleandata. Here, we proposed an innovative self-supervised strategy called Sub2Full(S2F) for OCT despeckling without clean data. This approach works by acquiringtwo repeated B-scans, splitting the spectrum of the first repeat as alow-resolution input, and utilizing the full spectrum of the second repeat asthe high-resolution target. The proposed method was validated on vis-OCTretinal images visualizing sublaminar structures in outer retina anddemonstrated superior performance over conventional Noise2Noise and Noise2Voidschemes. The code is available athttps://github.com/PittOCT/Sub2Full-OCT-Denoising.</description><author>Lingyun Wang, Jose A Sahel, Shaohua Pi</author><pubDate>Thu, 18 Jan 2024 16:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10128v1</guid></item><item><title>From Procedures, Objects, Actors, Components, Services, to Agents -- A Comparative Analysis of the History and Evolution of Programming Abstractions</title><link>http://arxiv.org/abs/2112.12508v4</link><description>The objective of this chapter is to propose some retrospective analysis ofthe evolution of programming abstractions, from {\em procedures}, {\emobjects}, {\em actors}, {\em components}, {\em services}, up to {\em agents},%have some compare concepts of software component and of agent (and multi-agentsystem), %The method chosen is to by replacing them within a general historicalperspective. Some common referential with three axes/dimensions is chosen: {\emaction selection} at the level of one entity, {\em coupling flexibility}between entities, and {\em abstraction level}. We indeed may observe somecontinuous quest for higher flexibility (through notions such as {\em latebinding}, or {\em reification} of {\em connections}) and higher level of {\emabstraction}. Concepts of components, services and agents have some commonobjectives (notably, {\em software modularity and reconfigurability}), withmulti-agent systems raising further concepts of {\em autonomy} and {\emcoordination}. notably through the notion of {\em auto-organization} and theuse of {\em knowledge}. We hope that this analysis helps at highlighting someof the basic forces motivating the progress of programming abstractions andtherefore that it may provide some seeds for the reflection about futureprogramming abstractions.</description><author>Jean-Pierre Briot</author><pubDate>Thu, 18 Jan 2024 16:53:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.12508v4</guid></item><item><title>Assertion Enhanced Few-Shot Learning: Instructive Technique for Large Language Models to Generate Educational Explanations</title><link>http://arxiv.org/abs/2312.03122v2</link><description>Human educators possess an intrinsic ability to anticipate and seekeducational explanations from students, which drives them to posethought-provoking questions when students cannot articulate these explanationsindependently. We aim to imbue Intelligent Tutoring Systems with this abilityusing few-shot learning capability of Large Language Models. Our work proposesa novel prompting technique, Assertion Enhanced Few-Shot Learning, tofacilitate the generation of accurate, detailed oriented educationalexplanations. Our central hypothesis is that, in educational domain, few-shotdemonstrations are necessary but not a sufficient condition for qualityexplanation generation. We conducted a study involving 12 in-service teachers,comparing our approach to Traditional Few-Shot Learning. The results show thatAssertion Enhanced Few-Shot Learning improves explanation accuracy by 15% andyields higher-quality explanations, as evaluated by teachers. We also conduct aqualitative ablation study to factor the impact of assertions to provideeducator-friendly prompting guidelines for generating explanations in theirdomain of interest.</description><author>Tasmia Shahriar, Noboru Matsuda, Kelly Ramos</author><pubDate>Thu, 18 Jan 2024 16:53:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03122v2</guid></item><item><title>BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis</title><link>http://arxiv.org/abs/2310.20496v2</link><description>Bases have become an integral part of modern deep learning-based models fortime series forecasting due to their ability to act as feature extractors orfuture references. To be effective, a basis must be tailored to the specificset of time series data and exhibit distinct correlation with each time serieswithin the set. However, current state-of-the-art methods are limited in theirability to satisfy both of these requirements simultaneously. To address thischallenge, we propose BasisFormer, an end-to-end time series forecastingarchitecture that leverages learnable and interpretable bases. Thisarchitecture comprises three components: First, we acquire bases throughadaptive self-supervised learning, which treats the historical and futuresections of the time series as two distinct views and employs contrastivelearning. Next, we design a Coef module that calculates the similaritycoefficients between the time series and bases in the historical view viabidirectional cross-attention. Finally, we present a Forecast module thatselects and consolidates the bases in the future view based on the similaritycoefficients, resulting in accurate future predictions. Through extensiveexperiments on six datasets, we demonstrate that BasisFormer outperformsprevious state-of-the-art methods by 11.04\% and 15.78\% respectively forunivariate and multivariate forecasting tasks. Code is available at:\url{https://github.com/nzl5116190/Basisformer}</description><author>Zelin Ni, Hang Yu, Shizhan Liu, Jianguo Li, Weiyao Lin</author><pubDate>Thu, 18 Jan 2024 16:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.20496v2</guid></item><item><title>Towards Principled Graph Transformers</title><link>http://arxiv.org/abs/2401.10119v1</link><description>Graph learning architectures based on the k-dimensional Weisfeiler-Leman(k-WL) hierarchy offer a theoretically well-understood expressive power.However, such architectures often fail to deliver solid predictive performanceon real-world tasks, limiting their practical impact. In contrast, globalattention-based models such as graph transformers demonstrate strongperformance in practice, but comparing their expressive power with the k-WLhierarchy remains challenging, particularly since these architectures rely onpositional or structural encodings for their expressivity and predictiveperformance. To address this, we show that the recently proposed EdgeTransformer, a global attention model operating on node pairs instead of nodes,has at least 3-WL expressive power. Empirically, we demonstrate that the EdgeTransformer surpasses other theoretically aligned architectures regardingpredictive performance while not relying on positional or structural encodings.</description><author>Luis Müller, Christopher Morris</author><pubDate>Thu, 18 Jan 2024 16:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10119v1</guid></item><item><title>FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel Identification</title><link>http://arxiv.org/abs/2311.10359v4</link><description>Highly parallelized workloads like machine learning training, inferences andgeneral HPC tasks are greatly accelerated using GPU devices. In a cloudcomputing cluster, serving a GPU's computation power through multi-taskssharing is highly demanded since there are always more task requests than thenumber of GPU available. Existing GPU sharing solutions focus on reducingtask-level waiting time or task-level switching costs when multiple jobscompeting for a single GPU. Non-stopped computation requests come withdifferent priorities, having non-symmetric impact on QoS for sharing a GPUdevice. Existing work missed the kernel-level optimization opportunity broughtby this setting. To address this problem, we present a novel kernel-levelscheduling strategy called FIKIT: Filling Inter-kernel Idle Time. FIKITincorporates task-level priority information, fine-grained kernelidentification, and kernel measurement, allowing low priorities task'sexecution during high priority task's inter-kernel idle time. Thereby, fillingthe GPU's device runtime fully, and reduce overall GPU sharing impact to cloudservices. Across a set of ML models, the FIKIT based inference systemaccelerated high priority tasks by 1.33 to 14.87 times compared to the JCT inGPU sharing mode, and more than half of the cases are accelerated by more than3.5 times. Alternatively, under preemptive sharing, the low-priority tasks havea comparable to default GPU sharing mode JCT, with a 0.84 to 1 times ratio. Wefurther limit the kernel measurement and runtime fine-grained kernel schedulingoverhead to less than 5%.</description><author>Wenqing Wu</author><pubDate>Thu, 18 Jan 2024 16:47:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10359v4</guid></item><item><title>Recovering Simultaneously Structured Data via Non-Convex Iteratively Reweighted Least Squares</title><link>http://arxiv.org/abs/2306.04961v2</link><description>We propose a new algorithm for the problem of recovering data that adheres tomultiple, heterogeneous low-dimensional structures from linear observations.Focusing on data matrices that are simultaneously row-sparse and low-rank, wepropose and analyze an iteratively reweighted least squares (IRLS) algorithmthat is able to leverage both structures. In particular, it optimizes acombination of non-convex surrogates for row-sparsity and rank, a balancing ofwhich is built into the algorithm. We prove locally quadratic convergence ofthe iterates to a simultaneously structured data matrix in a regime of minimalsample complexity (up to constants and a logarithmic factor), which is known tobe impossible for a combination of convex surrogates. In experiments, we showthat the IRLS method exhibits favorable empirical convergence, identifyingsimultaneously row-sparse and low-rank matrices from fewer measurements thanstate-of-the-art methods. Code is available athttps://github.com/ckuemmerle/simirls.</description><author>Christian Kümmerle, Johannes Maly</author><pubDate>Thu, 18 Jan 2024 16:47:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04961v2</guid></item><item><title>UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World Understanding</title><link>http://arxiv.org/abs/2401.06397v2</link><description>Vision-language foundation models, represented by Contrastive language-imagepre-training (CLIP), have gained increasing attention for jointly understandingboth vision and textual tasks. However, existing approaches primarily focus ontraining models to match global image representations with textualdescriptions, thereby overlooking the critical alignment between local regionsand corresponding text tokens. This paper extends CLIP with multi-granularityalignment. Notably, we deliberately construct a new dataset comprising pseudoannotations at various levels of granularities, encompassing image-level,region-level, and pixel-level captions/tags. Accordingly, we develop a unifiedmulti-granularity learning framework, named UMG-CLIP, that simultaneouslyempowers the model with versatile perception abilities across different levelsof detail. Equipped with parameter efficient tuning, UMG-CLIP surpasses currentwidely used CLIP models and achieves state-of-the-art performance on diverseimage understanding benchmarks, including open-world recognition, retrieval,semantic segmentation, and panoptic segmentation tasks. We hope UMG-CLIP canserve as a valuable option for advancing vision-language foundation models.</description><author>Bowen Shi, Peisen Zhao, Zichen Wang, Yuhang Zhang, Yaoming Wang, Jin Li, Wenrui Dai, Junni Zou, Hongkai Xiong, Qi Tian, Xiaopeng Zhang</author><pubDate>Thu, 18 Jan 2024 16:40:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06397v2</guid></item><item><title>Exposing Lip-syncing Deepfakes from Mouth Inconsistencies</title><link>http://arxiv.org/abs/2401.10113v1</link><description>A lip-syncing deepfake is a digitally manipulated video in which a person'slip movements are created convincingly using AI models to match altered orentirely new audio. Lip-syncing deepfakes are a dangerous type of deepfakes asthe artifacts are limited to the lip region and more difficult to discern. Inthis paper, we describe a novel approach, LIP-syncing detection based on mouthINConsistency (LIPINC), for lip-syncing deepfake detection by identifyingtemporal inconsistencies in the mouth region. These inconsistencies are seen inthe adjacent frames and throughout the video. Our model can successfullycapture these irregularities and outperforms the state-of-the-art methods onseveral benchmark deepfake datasets.</description><author>Soumyya Kanti Datta, Shan Jia, Siwei Lyu</author><pubDate>Thu, 18 Jan 2024 16:35:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10113v1</guid></item><item><title>Marrying Adapters and Mixup to Efficiently Enhance the Adversarial Robustness of Pre-Trained Language Models for Text Classification</title><link>http://arxiv.org/abs/2401.10111v1</link><description>Existing works show that augmenting training data of neural networks usingboth clean and adversarial examples can enhance their generalizability underadversarial attacks. However, this training approach often leads to performancedegradation on clean inputs. Additionally, it requires frequent re-training ofthe entire model to account for new attack types, resulting in significant andcostly computations. Such limitations make adversarial training mechanisms lesspractical, particularly for complex Pre-trained Language Models (PLMs) withmillions or even billions of parameters. To overcome these challenges whilestill harnessing the theoretical benefits of adversarial training, this studycombines two concepts: (1) adapters, which enable parameter-efficientfine-tuning, and (2) Mixup, which train NNs via convex combinations of pairsdata pairs. Intuitively, we propose to fine-tune PLMs through convexcombinations of non-data pairs of fine-tuned adapters, one trained with cleanand another trained with adversarial examples. Our experiments show that theproposed method achieves the best trade-off between training efficiency andpredictive performance, both with and without attacks compared to otherbaselines on a variety of downstream tasks.</description><author>Tuc Nguyen, Thai Le</author><pubDate>Thu, 18 Jan 2024 16:27:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10111v1</guid></item><item><title>VIPTR: A Vision Permutable Extractor for Fast and Efficient Scene Text Recognition</title><link>http://arxiv.org/abs/2401.10110v1</link><description>Scene Text Recognition (STR) is a challenging task that involves recognizingtext within images of natural scenes. Although current state-of-the-art modelsfor STR exhibit high performance, they typically suffer from low inferenceefficiency due to their reliance on hybrid architectures comprised of visualencoders and sequence decoders. In this work, we propose the VIsion Permutableextractor for fast and efficient scene Text Recognition (VIPTR), which achievesan impressive balance between high performance and rapid inference speeds inthe domain of STR. Specifically, VIPTR leverages a visual-semantic extractorwith a pyramid structure, characterized by multiple self-attention layers,while eschewing the traditional sequence decoder. This design choice results ina lightweight and efficient model capable of handling inputs of varying sizes.Extensive experimental results on various standard datasets for both Chineseand English scene text recognition validate the superiority of VIPTR. Notably,the VIPTR-T (Tiny) variant delivers highly competitive accuracy on par withother lightweight models and achieves SOTA inference speeds. Meanwhile, theVIPTR-L (Large) variant attains greater recognition accuracy, while maintaininga low parameter count and favorable inference speed. Our proposed methodprovides a compelling solution for the STR challenge, which blends highaccuracy with efficiency and greatly benefits real-world applications requiringfast and reliable text recognition. The code is publicly available athttps://github.com/cxfyxl/VIPTR.</description><author>Xianfu Cheng, Weixiao Zhou, Xiang Li, Xiaoming Chen, Jian Yang, Tongliang Li, Zhoujun Li</author><pubDate>Thu, 18 Jan 2024 16:27:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10110v1</guid></item><item><title>A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem</title><link>http://arxiv.org/abs/2305.17198v2</link><description>Training multiple agents to coordinate is an essential problem withapplications in robotics, game theory, economics, and social sciences. However,most existing Multi-Agent Reinforcement Learning (MARL) methods are online andthus impractical for real-world applications in which collecting newinteractions is costly or dangerous. While these algorithms should leverageoffline data when available, doing so gives rise to what we call the offlinecoordination problem. Specifically, we identify and formalize the strategyagreement (SA) and the strategy fine-tuning (SFT) coordination challenges, twoissues at which current offline MARL algorithms fail. Concretely, we revealthat the prevalent model-free methods are severely deficient and cannot handlecoordination-intensive offline multi-agent tasks in either toy or MuJoCodomains. To address this setback, we emphasize the importance of inter-agentinteractions and propose the very first model-based offline MARL method. Ourresulting algorithm, Model-based Offline Multi-Agent Proximal PolicyOptimization (MOMA-PPO) generates synthetic interaction data and enables agentsto converge on a strategy while fine-tuning their policies accordingly. Thissimple model-based solution solves the coordination-intensive offline tasks,significantly outperforming the prevalent model-free methods even under severepartial observability and with learned world models.</description><author>Paul Barde, Jakob Foerster, Derek Nowrouzezahrai, Amy Zhang</author><pubDate>Thu, 18 Jan 2024 16:25:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17198v2</guid></item><item><title>FactCHD: Benchmarking Fact-Conflicting Hallucination Detection</title><link>http://arxiv.org/abs/2310.12086v2</link><description>Despite their impressive generative capabilities, LLMs are hindered byfact-conflicting hallucinations in real-world applications. The accurateidentification of hallucinations in texts generated by LLMs, especially incomplex inferential scenarios, is a relatively unexplored area. To address thisgap, we present FactCHD, a dedicated benchmark designed for the detection offact-conflicting hallucinations from LLMs. FactCHD features a diverse datasetthat spans various factuality patterns, including vanilla, multi-hop,comparison, and set operation. A distinctive element of FactCHD is itsintegration of fact-based evidence chains, significantly enhancing the depth ofevaluating the detectors' explanations. Experiments on different LLMs exposethe shortcomings of current approaches in detecting factual errors accurately.Furthermore, we introduce Truth-Triangulator that synthesizes reflectiveconsiderations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aimingto yield more credible detection through the amalgamation of predictive resultsand evidence. The benchmark dataset is available athttps://github.com/zjunlp/FactCHD.</description><author>Xiang Chen, Duanzheng Song, Honghao Gui, Chenxi Wang, Ningyu Zhang, Jiang Yong, Fei Huang, Chengfei Lv, Dan Zhang, Huajun Chen</author><pubDate>Thu, 18 Jan 2024 16:20:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12086v2</guid></item><item><title>Comparison analysis between standard polysomnographic data and in-ear-EEG signals: A preliminary study</title><link>http://arxiv.org/abs/2401.10107v1</link><description>Study Objectives: Polysomnography (PSG) currently serves as the benchmark forevaluating sleep disorders. Its discomfort, impracticality for home-use, andintroduction of bias in sleep quality assessment necessitate the exploration ofless invasive, cost-effective, and portable alternatives. One promisingcontender is the in-ear-EEG sensor, which offers advantages in terms ofcomfort, fixed electrode positions, resistance to electromagnetic interference,and user-friendliness. This study aims to establish a methodology to assess thesimilarity between the in-ear-EEG signal and standard PSG. Methods: We assess the agreement between the PSG and in-ear-EEG derivedhypnograms. We extract features in the time- and frequency- domain from PSG andin-ear-EEG 30-second epochs. We only consider the epochs where the PSG-scorersand the in-ear-EEG-scorers were in agreement. We introduce a methodology toquantify the similarity between PSG derivations and the single-channelin-ear-EEG. The approach relies on a comparison of distributions of selectedfeatures -- extracted for each sleep stage and subject on both PSG and thein-ear-EEG signals -- via a Jensen-Shannon Divergence Feature-based SimilarityIndex (JSD-FSI). Results: We found a high intra-scorer variability, mainly due to theuncertainty the scorers had in evaluating the in-ear-EEG signals. We show thatthe similarity between PSG and in-ear-EEG signals is high (JSD-FSI: 0.61 +/-0.06 in awake, 0.60 +/- 0.07 in NREM and 0.51 +/- 0.08 in REM), and in linewith the similarity values computed independently on standardPSG-channel-combinations. Conclusions: In-ear-EEG is a valuable solution for home-based sleepmonitoring, however further studies with a larger and more heterogeneousdataset are needed.</description><author>Gianpaolo Palo, Luigi Fiorillo, Giuliana Monachino, Michal Bechny, Mark Melnykowycz, Athina Tzovara, Valentina Agostini, Francesca Dalia Faraci</author><pubDate>Thu, 18 Jan 2024 16:18:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10107v1</guid></item><item><title>CodeKGC: Code Language Model for Generative Knowledge Graph Construction</title><link>http://arxiv.org/abs/2304.09048v2</link><description>Current generative knowledge graph construction approaches usually fail tocapture structural knowledge by simply flattening natural language intoserialized texts or a specification language. However, large generativelanguage model trained on structured data such as code has demonstratedimpressive capability in understanding natural language for structuralprediction and reasoning tasks. Intuitively, we address the task of generativeknowledge graph construction with code language model: given a code-formatnatural language input, the target is to generate triples which can berepresented as code completion tasks. Specifically, we develop schema-awareprompts that effectively utilize the semantic structure within the knowledgegraph. As code inherently possesses structure, such as class and functiondefinitions, it serves as a useful model for prior semantic structuralknowledge. Furthermore, we employ a rationale-enhanced generation method toboost the performance. Rationales provide intermediate steps, thereby improvingknowledge extraction abilities. Experimental results indicate that the proposedapproach can obtain better performance on benchmark datasets compared withbaselines. Code and datasets are available inhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.</description><author>Zhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo, Huajun Chen, Ningyu Zhang</author><pubDate>Thu, 18 Jan 2024 16:14:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09048v2</guid></item><item><title>Counterfactual Reasoning with Probabilistic Graphical Models for Analyzing Socioecological Systems</title><link>http://arxiv.org/abs/2401.10101v1</link><description>Causal and counterfactual reasoning are emerging directions in data sciencethat allow us to reason about hypothetical scenarios. This is particularlyuseful in domains where experimental data are usually not available. In thecontext of environmental and ecological sciences, causality enables us, forexample, to predict how an ecosystem would respond to hypotheticalinterventions. A structural causal model is a class of probabilistic graphicalmodels for causality, which, due to its intuitive nature, can be easilyunderstood by experts in multiple fields. However, certain queries, calledunidentifiable, cannot be calculated in an exact and precise manner. This paperproposes applying a novel and recent technique for bounding unidentifiablequeries within the domain of socioecological systems. Our findings indicatethat traditional statistical analysis, including probabilistic graphicalmodels, can identify the influence between variables. However, such methods donot offer insights into the nature of the relationship, specifically whether itinvolves necessity or sufficiency. This is where counterfactual reasoningbecomes valuable.</description><author>Rafael Cabañas, Ana D. Maldonado, María Morales, Pedro A. Aguilera, Antonio Salmerón</author><pubDate>Thu, 18 Jan 2024 16:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10101v1</guid></item><item><title>Learning shallow quantum circuits</title><link>http://arxiv.org/abs/2401.10095v1</link><description>Despite fundamental interests in learning quantum circuits, the existence ofa computationally efficient algorithm for learning shallow quantum circuitsremains an open question. Because shallow quantum circuits can generatedistributions that are classically hard to sample from, existing learningalgorithms do not apply. In this work, we present a polynomial-time classicalalgorithm for learning the description of any unknown $n$-qubit shallow quantumcircuit $U$ (with arbitrary unknown architecture) within a small diamonddistance using single-qubit measurement data on the output states of $U$. Wealso provide a polynomial-time classical algorithm for learning the descriptionof any unknown $n$-qubit state $\lvert \psi \rangle = U \lvert 0^n \rangle$prepared by a shallow quantum circuit $U$ (on a 2D lattice) within a smalltrace distance using single-qubit measurements on copies of $\lvert \psi\rangle$. Our approach uses a quantum circuit representation based on localinversions and a technique to combine these inversions. This circuitrepresentation yields an optimization landscape that can be efficientlynavigated and enables efficient learning of quantum circuits that areclassically hard to simulate.</description><author>Hsin-Yuan Huang, Yunchao Liu, Michael Broughton, Isaac Kim, Anurag Anshu, Zeph Landau, Jarrod R. McClean</author><pubDate>Thu, 18 Jan 2024 16:05:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10095v1</guid></item><item><title>Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation and Regression</title><link>http://arxiv.org/abs/2306.00788v3</link><description>Data augmentation is critical to the empirical success of modernself-supervised representation learning, such as contrastive learning andmasked language modeling. However, a theoretical understanding of the exactrole of augmentation remains limited. Recent work has built the connectionbetween self-supervised learning and the approximation of the top eigenspace ofa graph Laplacian operator, suggesting that learning a linear probe atop suchrepresentation can be connected to RKHS regression. Building on this insight,this work delves into a statistical analysis of augmentation-based pretraining.Starting from the isometry property, a geometric characterization of the targetfunction given by the augmentation, we disentangle the effects of the model andthe augmentation, and prove two generalization bounds that are free of modelcomplexity. Our first bound works for an arbitrary encoder, where theprediction error is decomposed as the sum of an estimation error incurred byfitting a linear probe with RKHS regression, and an approximation errorentailed by RKHS approximation. Our second bound specifically addresses thecase where the encoder is near-optimal, that is it approximates the top-deigenspace of the RKHS induced by the augmentation. A key ingredient in ouranalysis is the augmentation complexity, which we use to quantitatively comparedifferent augmentations and analyze their impact on downstream performance.</description><author>Runtian Zhai, Bingbin Liu, Andrej Risteski, Zico Kolter, Pradeep Ravikumar</author><pubDate>Thu, 18 Jan 2024 16:00:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00788v3</guid></item><item><title>Power in Numbers: Robust reading comprehension by finetuning with four adversarial sentences per example</title><link>http://arxiv.org/abs/2401.10091v1</link><description>Recent models have achieved human level performance on the Stanford QuestionAnswering Dataset when using F1 scores to evaluate the reading comprehensiontask. Yet, teaching machines to comprehend text has not been solved in thegeneral case. By appending one adversarial sentence to the context paragraph,past research has shown that the F1 scores from reading comprehension modelsdrop almost in half. In this paper, I replicate past adversarial research witha new model, ELECTRA-Small, and demonstrate that the new model's F1 score dropsfrom 83.9% to 29.2%. To improve ELECTRA-Small's resistance to this attack, Ifinetune the model on SQuAD v1.1 training examples with one to five adversarialsentences appended to the context paragraph. Like past research, I find thatthe finetuned model on one adversarial sentence does not generalize well acrossevaluation datasets. However, when finetuned on four or five adversarialsentences the model attains an F1 score of more than 70% on most evaluationdatasets with multiple appended and prepended adversarial sentences. Theresults suggest that with enough examples we can make models robust toadversarial attacks.</description><author>Ariel Marcus</author><pubDate>Thu, 18 Jan 2024 15:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10091v1</guid></item><item><title>Hierarchical Masked 3D Diffusion Model for Video Outpainting</title><link>http://arxiv.org/abs/2309.02119v2</link><description>Video outpainting aims to adequately complete missing areas at the edges ofvideo frames. Compared to image outpainting, it presents an additionalchallenge as the model should maintain the temporal consistency of the filledarea. In this paper, we introduce a masked 3D diffusion model for videooutpainting. We use the technique of mask modeling to train the 3D diffusionmodel. This allows us to use multiple guide frames to connect the results ofmultiple video clip inferences, thus ensuring temporal consistency and reducingjitter between adjacent frames. Meanwhile, we extract the global frames of thevideo as prompts and guide the model to obtain information other than thecurrent video clip using cross-attention. We also introduce a hybridcoarse-to-fine inference pipeline to alleviate the artifact accumulationproblem. The existing coarse-to-fine pipeline only uses the infilling strategy,which brings degradation because the time interval of the sparse frames is toolarge. Our pipeline benefits from bidirectional learning of the mask modelingand thus can employ a hybrid strategy of infilling and interpolation whengenerating sparse frames. Experiments show that our method achievesstate-of-the-art results in video outpainting tasks. More results and codes areprovided at our https://fanfanda.github.io/M3DDM/.</description><author>Fanda Fan, Chaoxu Guo, Litong Gong, Biao Wang, Tiezheng Ge, Yuning Jiang, Chunjie Luo, Jianfeng Zhan</author><pubDate>Thu, 18 Jan 2024 15:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02119v2</guid></item><item><title>Conversational Process Modeling: Can Generative AI Empower Domain Experts in Creating and Redesigning Process Models?</title><link>http://arxiv.org/abs/2304.11065v2</link><description>AI-driven chatbots such as ChatGPT have caused a tremendous hype lately. ForBPM applications, several applications for AI-driven chatbots have beenidentified to be promising to generate business value, including explanation ofprocess mining outcomes and preparation of input data. However, a systematicanalysis of chatbots for their support of conversational process modeling as aprocess-oriented capability is missing. This work aims at closing this gap byproviding a systematic analysis of existing chatbots. Application scenarios areidentified along the process life cycle. Then a systematic literature review onconversational process modeling is performed, resulting in a taxonomy ofapplication scenarios for conversational process modeling, includingparaphrasing and improvement of process descriptions. In addition, this worksuggests and applies an evaluation method for the output of AI-driven chatbotswith respect to completeness and correctness of the process models. This methodconsists of a set of KPIs on a test set, a set of prompts for task and controlflow extraction, as well as a survey with users. Based on the literature andthe evaluation, recommendations for the usage (practical implications) andfurther development (research directions) of conversational process modelingare derived.</description><author>Nataliia Klievtsova, Janik-Vasily Benzin, Timotheus Kampik, Juergen Mangler, Stefanie Rinderle-Ma</author><pubDate>Thu, 18 Jan 2024 15:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11065v2</guid></item><item><title>Cross-Modality Perturbation Synergy Attack for Person Re-identification</title><link>http://arxiv.org/abs/2401.10090v1</link><description>In recent years, there has been significant research focusing on addressingsecurity concerns in single-modal person re-identification (ReID) systems thatare based on RGB images. However, the safety of cross-modality scenarios, whichare more commonly encountered in practical applications involving imagescaptured by infrared cameras, has not received adequate attention. The mainchallenge in cross-modality ReID lies in effectively dealing with visualdifferences between different modalities. For instance, infrared images aretypically grayscale, unlike visible images that contain color information.Existing attack methods have primarily focused on the characteristics of thevisible image modality, overlooking the features of other modalities and thevariations in data distribution among different modalities. This oversight canpotentially undermine the effectiveness of these methods in image retrievalacross diverse modalities. This study represents the first exploration into thesecurity of cross-modality ReID models and proposes a universal perturbationattack specifically designed for cross-modality ReID. This attack optimizesperturbations by leveraging gradients from diverse modality data, therebydisrupting the discriminator and reinforcing the differences betweenmodalities. We conducted experiments on two widely used cross-modalitydatasets, namely RegDB and SYSU, which not only demonstrated the effectivenessof our method but also provided insights for future enhancements in therobustness of cross-modality ReID systems.</description><author>Yunpeng Gong, others</author><pubDate>Thu, 18 Jan 2024 15:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10090v1</guid></item><item><title>Determinantal Point Process Attention Over Grid Cell Code Supports Out of Distribution Generalization</title><link>http://arxiv.org/abs/2305.18417v2</link><description>Deep neural networks have made tremendous gains in emulating human-likeintelligence, and have been used increasingly as ways of understanding how thebrain may solve the complex computational problems on which this relies.However, these still fall short of, and therefore fail to provide insight intohow the brain supports strong forms of generalization of which humans arecapable. One such case is out-of-distribution (OOD) generalization-successfulperformance on test examples that lie outside the distribution of the trainingset. Here, we identify properties of processing in the brain that maycontribute to this ability. We describe a two-part algorithm that draws onspecific features of neural computation to achieve OOD generalization, andprovide a proof of concept by evaluating performance on two challengingcognitive tasks. First we draw on the fact that the mammalian brain representsmetric spaces using grid cell code (e.g., in entorhinal cortex): abstractrepresentations of relational structure, organized in recurring motifs thatcover the representational space. Second, we propose an attentional mechanismthat operates over the grid cell code using Determinantal Point Process (DPP),that we call DPP attention (DPP-A) -- a transformation that ensures maximumsparseness in the coverage of that space. We show that a loss function thatcombines standard task-optimized error with DPP-A can exploit the recurringmotifs in the grid cell code, and can be integrated with common architecturesto achieve strong OOD generalization performance on analogy and arithmetictasks. This provides both an interpretation of how the grid cell code in themammalian brain may contribute to generalization performance, and at the sametime a potential means for improving such capabilities in artificial neuralnetworks.</description><author>Shanka Subhra Mondal, Steven Frankland, Taylor Webb, Jonathan D. Cohen</author><pubDate>Thu, 18 Jan 2024 15:50:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18417v2</guid></item><item><title>GIVT: Generative Infinite-Vocabulary Transformers</title><link>http://arxiv.org/abs/2312.02116v2</link><description>We introduce generative infinite-vocabulary transformers (GIVT) whichgenerate vector sequences with real-valued entries, instead of discrete tokensfrom a finite vocabulary. To this end, we propose two surprisingly simplemodifications to decoder-only transformers: 1) at the input, we replace thefinite-vocabulary lookup table with a linear projection of the input vectors;and 2) at the output, we replace the logits prediction (usually mapped to acategorical distribution) with the parameters of a multivariate Gaussianmixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT,where transformers are used to model the discrete latent sequences of a VQ-VAE,we use GIVT to model the unquantized real-valued latent sequences of a VAE.When applying GIVT to class-conditional image generation with iterative maskedmodeling, we show competitive results with MaskGIT, while our approachoutperforms both VQ-GAN and MaskGIT when using it for causal modeling. Finally,we obtain competitive results outside of image generation when applying ourapproach to panoptic segmentation and depth estimation with a VAE-based variantof the UViM framework.</description><author>Michael Tschannen, Cian Eastwood, Fabian Mentzer</author><pubDate>Thu, 18 Jan 2024 15:47:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02116v2</guid></item><item><title>Communication-Efficient Personalized Federated Learning for Speech-to-Text Tasks</title><link>http://arxiv.org/abs/2401.10070v1</link><description>To protect privacy and meet legal regulations, federated learning (FL) hasgained significant attention for training speech-to-text (S2T) systems,including automatic speech recognition (ASR) and speech translation (ST).However, the commonly used FL approach (i.e., \textsc{FedAvg}) in S2T taskstypically suffers from extensive communication overhead due to multi-roundinteractions based on the whole model and performance degradation caused bydata heterogeneity among clients.To address these issues, we propose apersonalized federated S2T framework that introduces \textsc{FedLoRA}, alightweight LoRA module for client-side tuning and interaction with the serverto minimize communication overhead, and \textsc{FedMem}, a global modelequipped with a $k$-nearest-neighbor ($k$NN) classifier that capturesclient-specific distributional shifts to achieve personalization and overcomedata heterogeneity. Extensive experiments based on Conformer and Whisperbackbone models on CoVoST and GigaSpeech benchmarks show that our approachsignificantly reduces the communication overhead on all S2T tasks andeffectively personalizes the global model to overcome data heterogeneity.</description><author>Yichao Du, Zhirui Zhang, Linan Yue, Xu Huang, Yuqing Zhang, Tong Xu, Linli Xu, Enhong Chen</author><pubDate>Thu, 18 Jan 2024 15:39:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10070v1</guid></item><item><title>Labeling Neural Representations with Inverse Recognition</title><link>http://arxiv.org/abs/2311.13594v2</link><description>Deep Neural Networks (DNNs) demonstrate remarkable capabilities in learningcomplex hierarchical data representations, but the nature of theserepresentations remains largely unknown. Existing global explainabilitymethods, such as Network Dissection, face limitations such as reliance onsegmentation masks, lack of statistical significance testing, and highcomputational demands. We propose Inverse Recognition (INVERT), a scalableapproach for connecting learned representations with human-understandableconcepts by leveraging their capacity to discriminate between these concepts.In contrast to prior work, INVERT is capable of handling diverse types ofneurons, exhibits less computational complexity, and does not rely on theavailability of segmentation masks. Moreover, INVERT provides an interpretablemetric assessing the alignment between the representation and its correspondingexplanation and delivering a measure of statistical significance. Wedemonstrate the applicability of INVERT in various scenarios, including theidentification of representations affected by spurious correlations, and theinterpretation of the hierarchical structure of decision-making within themodels.</description><author>Kirill Bykov, Laura Kopf, Shinichi Nakajima, Marius Kloft, Marina M. -C. Höhne</author><pubDate>Thu, 18 Jan 2024 15:39:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13594v2</guid></item><item><title>Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models</title><link>http://arxiv.org/abs/2308.10462v2</link><description>Large Language Models (LLMs) demonstrate impressive capabilities to generateaccurate code snippets given natural language intents in zero-shot, i.e.,without the need for specific fine-tuning. While prior studies have highlightedthe advantages of fine-tuning LLMs, this process incurs high computationalcosts, making it impractical in resource-scarce environments, particularly formodels with billions of parameters. To address these challenges, previousresearch explored In-Context Learning (ICL) as a strategy to guide the LLMgenerative process with task-specific prompt examples. However, ICL introducesinconveniences, such as the need for designing contextually relevant promptsand the absence of learning task-specific parameters, thereby limitingdownstream task performance. In this context, we foresee Parameter-EfficientFine-Tuning (PEFT) techniques as a promising approach to efficiently specializeLLMs to task-specific data while maintaining reasonable resource consumption.In this paper, we deliver a comprehensive study of PEFT techniques for LLMsunder the automated code generation scenario. Our comprehensive investigationof PEFT techniques for LLMs reveals their superiority and potential over ICLacross a diverse set of LLMs. Additionally, we demonstrate the extendedcapabilities of PEFT, showcasing its ability to learn from two distinctdatasets jointly without compromising performance. Furthermore, our studyhighlights the potential for tuning larger LLMs and significant reductions inmemory usage by combining PEFT with quantization. Therefore, this study opensopportunities for broader applications of PEFT in software engineeringscenarios. Our code is available athttps://github.com/martin-wey/peft-llm-code/.</description><author>Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, Houari Sahraoui</author><pubDate>Thu, 18 Jan 2024 15:37:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10462v2</guid></item><item><title>Chat Failures and Troubles: Reasons and Solutions</title><link>http://arxiv.org/abs/2309.03708v2</link><description>This paper examines some common problems in Human-Robot Interaction (HRI)causing failures and troubles in Chat. A given use case's design decisionsstart with the suitable robot, the suitable chatting model, identifying commonproblems that cause failures, identifying potential solutions, and planningcontinuous improvement. In conclusion, it is recommended to use a closed-loopcontrol algorithm that guides the use of trained Artificial Intelligence (AI)pre-trained models and provides vocabulary filtering, re-train batched modelson new datasets, learn online from data streams, and/or use reinforcementlearning models to self-update the trained models and reduce errors.</description><author>Manal Helal, Patrick Holthaus, Gabriella Lakatos, Farshid Amirabdollahian</author><pubDate>Thu, 18 Jan 2024 15:35:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03708v2</guid></item><item><title>Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs</title><link>http://arxiv.org/abs/2401.10065v1</link><description>Reasoning is a fundamental component for achieving language understanding.Among the multiple types of reasoning, conditional reasoning, the ability todraw different conclusions depending on some condition, has been understudiedin large language models (LLMs). Recent prompting methods, such as chain ofthought, have significantly improved LLMs on reasoning tasks. Nevertheless,there is still little understanding of what triggers reasoning abilities inLLMs. We hypothesize that code prompts can trigger conditional reasoning inLLMs trained on text and code. We propose a chain of prompts that transforms anatural language problem into code and prompts the LLM with the generated code.Our experiments find that code prompts exhibit a performance boost between 2.6and 7.7 points on GPT 3.5 across multiple datasets requiring conditionalreasoning. We then conduct experiments to discover how code prompts elicitconditional reasoning abilities and through which features. We observe thatprompts need to contain natural language text accompanied by high-quality codethat closely represents the semantics of the instance text. Furthermore, weshow that code prompts are more efficient, requiring fewer demonstrations, andthat they trigger superior state tracking of variables or key entities.</description><author>Haritz Puerto, Martin Tutek, Somak Aditya, Xiaodan Zhu, Iryna Gurevych</author><pubDate>Thu, 18 Jan 2024 15:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10065v1</guid></item><item><title>DiffusionGPT: LLM-Driven Text-to-Image Generation System</title><link>http://arxiv.org/abs/2401.10061v1</link><description>Diffusion models have opened up new avenues for the field of imagegeneration, resulting in the proliferation of high-quality models shared onopen-source platforms. However, a major challenge persists in currenttext-to-image systems are often unable to handle diverse inputs, or are limitedto single model results. Current unified attempts often fall into twoorthogonal aspects: i) parse Diverse Prompts in input stage; ii) activateexpert model to output. To combine the best of both worlds, we proposeDiffusionGPT, which leverages Large Language Models (LLM) to offer a unifiedgeneration system capable of seamlessly accommodating various types of promptsand integrating domain-expert models. DiffusionGPT constructs domain-specificTrees for various generative models based on prior knowledge. When providedwith an input, the LLM parses the prompt and employs the Trees-of-Thought toguide the selection of an appropriate model, thereby relaxing input constraintsand ensuring exceptional performance across diverse domains. Moreover, weintroduce Advantage Databases, where the Tree-of-Thought is enriched with humanfeedback, aligning the model selection process with human preferences. Throughextensive experiments and comparisons, we demonstrate the effectiveness ofDiffusionGPT, showcasing its potential for pushing the boundaries of imagesynthesis in diverse domains.</description><author>Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, Shilei Wen</author><pubDate>Thu, 18 Jan 2024 15:30:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10061v1</guid></item><item><title>FedA3I: Annotation Quality-Aware Aggregation for Federated Medical Image Segmentation against Heterogeneous Annotation Noise</title><link>http://arxiv.org/abs/2312.12838v2</link><description>Federated learning (FL) has emerged as a promising paradigm for trainingsegmentation models on decentralized medical data, owing to itsprivacy-preserving property. However, existing research overlooks the prevalentannotation noise encountered in real-world medical datasets, which limits theperformance ceilings of FL. In this paper, we, for the first time, identify andtackle this problem. For problem formulation, we propose a contour evolutionfor modeling non-independent and identically distributed (Non-IID) noise acrosspixels within each client and then extend it to the case of multi-source datato form a heterogeneous noise model (i.e., Non-IID annotation noise acrossclients). For robust learning from annotations with such two-level Non-IIDnoise, we emphasize the importance of data quality in model aggregation,allowing high-quality clients to have a greater impact on FL. To achieve this,we propose Federated learning with Annotation quAlity-aware AggregatIon, namedFedA3I, by introducing a quality factor based on client-wise noise estimation.Specifically, noise estimation at each client is accomplished through theGaussian mixture model and then incorporated into model aggregation in alayer-wise manner to up-weight high-quality clients. Extensive experiments ontwo real-world medical image segmentation datasets demonstrate the superiorperformance of FedA$^3$I against the state-of-the-art approaches in dealingwith cross-client annotation noise. The code is available athttps://github.com/wnn2000/FedAAAI.</description><author>Nannan Wu, Zhaobin Sun, Zengqiang Yan, Li Yu</author><pubDate>Thu, 18 Jan 2024 15:27:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12838v2</guid></item><item><title>ContextMix: A context-aware data augmentation method for industrial visual inspection systems</title><link>http://arxiv.org/abs/2401.10050v1</link><description>While deep neural networks have achieved remarkable performance, dataaugmentation has emerged as a crucial strategy to mitigate overfitting andenhance network performance. These techniques hold particular significance inindustrial manufacturing contexts. Recently, image mixing-based methods havebeen introduced, exhibiting improved performance on public benchmark datasets.However, their application to industrial tasks remains challenging. Themanufacturing environment generates massive amounts of unlabeled data on adaily basis, with only a few instances of abnormal data occurrences. This leadsto severe data imbalance. Thus, creating well-balanced datasets is notstraightforward due to the high costs associated with labeling. Nonetheless,this is a crucial step for enhancing productivity. For this reason, weintroduce ContextMix, a method tailored for industrial applications andbenchmark datasets. ContextMix generates novel data by resizing entire imagesand integrating them into other images within the batch. This approach enablesour method to learn discriminative features based on varying sizes from resizedimages and train informative secondary features for object recognition usingoccluded images. With the minimal additional computation cost of imageresizing, ContextMix enhances performance compared to existing augmentationtechniques. We evaluate its effectiveness across classification, detection, andsegmentation tasks using various network architectures on public benchmarkdatasets. Our proposed method demonstrates improved results across a range ofrobustness tasks. Its efficacy in real industrial environments is particularlynoteworthy, as demonstrated using the passive component dataset.</description><author>Hyungmin Kim, Donghun Kim, Pyunghwan Ahn, Sungho Suh, Hansang Cho, Junmo Kim</author><pubDate>Thu, 18 Jan 2024 15:15:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10050v1</guid></item><item><title>CTAGE: Curvature-Based Topology-Aware Graph Embedding for Learning Molecular Representations</title><link>http://arxiv.org/abs/2307.13275v2</link><description>AI-driven drug design relies significantly on predicting molecularproperties, which is a complex task. In current approaches, the most commonlyused feature representations for training deep neural network models are basedon SMILES and molecular graphs. While these methods are concise and efficient,they have limitations in capturing complex spatial information. Recently,researchers have recognized the importance of incorporating three-dimensionalinformation of molecular structures into models. However, capturing spatialinformation requires the introduction of additional units in the generator,bringing additional design and computational costs. Therefore, it is necessaryto develop a method for predicting molecular properties that effectivelycombines spatial structural information while maintaining the simplicity andefficiency of graph neural networks. In this work, we propose an embeddingapproach CTAGE, utilizing $k$-hop discrete Ricci curvature to extractstructural insights from molecular graph data. This effectively integratesspatial structural information while preserving the training complexity of thenetwork. Experimental results indicate that introducing node curvaturesignificantly improves the performance of current graph neural networkframeworks, validating that the information from k-hop node curvatureeffectively reflects the relationship between molecular structure and function.</description><author>Yili Chen, Zhengyu Li, Zheng Wan, Hui Yu, Xian Wei</author><pubDate>Thu, 18 Jan 2024 15:14:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13275v2</guid></item><item><title>Antonym vs Synonym Distinction using InterlaCed Encoder NETworks (ICE-NET)</title><link>http://arxiv.org/abs/2401.10045v1</link><description>Antonyms vs synonyms distinction is a core challenge in lexico-semanticanalysis and automated lexical resource construction. These pairs share asimilar distributional context which makes it harder to distinguish them.Leading research in this regard attempts to capture the properties of therelation pairs, i.e., symmetry, transitivity, and trans-transitivity. However,the inability of existing research to appropriately model the relation-specificproperties limits their end performance. In this paper, we propose InterlaCedEncoder NETworks (i.e., ICE-NET) for antonym vs synonym distinction, that aimto capture and model the relation-specific properties of the antonyms andsynonyms pairs in order to perform the classification task in aperformance-enhanced manner. Experimental evaluation using the benchmarkdatasets shows that ICE-NET outperforms the existing research by a relativescore of upto 1.8% in F1-measure. We release the codes for ICE-NET athttps://github.com/asif6827/ICENET.</description><author>Muhammad Asif Ali, Yan Hu, Jianbin Qin, Di Wang</author><pubDate>Thu, 18 Jan 2024 15:08:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10045v1</guid></item><item><title>Deep spatial context: when attention-based models meet spatial regression</title><link>http://arxiv.org/abs/2401.10044v1</link><description>We propose 'Deep spatial context' (DSCon) method, which serves forinvestigation of the attention-based vision models using the concept of spatialcontext. It was inspired by histopathologists, however, the method can beapplied to various domains. The DSCon allows for a quantitative measure of thespatial context's role using three Spatial Context Measures: $SCM_{features}$,$SCM_{targets}$, $SCM_{residuals}$ to distinguish whether the spatial contextis observable within the features of neighboring regions, their target values(attention scores) or residuals, respectively. It is achieved by integratingspatial regression into the pipeline. The DSCon helps to verify researchquestions. The experiments reveal that spatial relationships are much bigger inthe case of the classification of tumor lesions than normal tissues. Moreover,it turns out that the larger the size of the neighborhood taken into accountwithin spatial regression, the less valuable contextual information is.Furthermore, it is observed that the spatial context measure is the largestwhen considered within the feature space as opposed to the targets andresiduals.</description><author>Paulina Tomaszewska, Elżbieta Sienkiewicz, Mai P. Hoang, Przemysław Biecek</author><pubDate>Thu, 18 Jan 2024 15:08:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10044v1</guid></item><item><title>RTFS-Net: Recurrent time-frequency modelling for efficient audio-visual speech separation</title><link>http://arxiv.org/abs/2309.17189v2</link><description>Audio-visual speech separation methods aim to integrate different modalitiesto generate high-quality separated speech, thereby enhancing the performance ofdownstream tasks such as speech recognition. Most existing state-of-the-art(SOTA) models operate in the time domain. However, their overly simplisticapproach to modeling acoustic features often necessitates larger and morecomputationally intensive models in order to achieve SOTA performance. In thispaper, we present a novel time-frequency domain audio-visual speech separationmethod: Recurrent Time-Frequency Separation Network (RTFS-Net), which appliesits algorithms on the complex time-frequency bins yielded by the Short-TimeFourier Transform. We model and capture the time and frequency dimensions ofthe audio independently using a multi-layered RNN along each dimension.Furthermore, we introduce a unique attention-based fusion technique for theefficient integration of audio and visual information, and a new maskseparation approach that takes advantage of the intrinsic spectral nature ofthe acoustic features for a clearer separation. RTFS-Net outperforms theprevious SOTA method using only 10% of the parameters and 18% of the MACs. Thisis the first time-frequency domain audio-visual speech separation method tooutperform all contemporary time-domain counterparts.</description><author>Samuel Pegg, Kai Li, Xiaolin Hu</author><pubDate>Thu, 18 Jan 2024 15:06:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17189v2</guid></item><item><title>CMFN: Cross-Modal Fusion Network for Irregular Scene Text Recognition</title><link>http://arxiv.org/abs/2401.10041v1</link><description>Scene text recognition, as a cross-modal task involving vision and text, isan important research topic in computer vision. Most existing methods uselanguage models to extract semantic information for optimizing visualrecognition. However, the guidance of visual cues is ignored in the process ofsemantic mining, which limits the performance of the algorithm in recognizingirregular scene text. To tackle this issue, we propose a novel cross-modalfusion network (CMFN) for irregular scene text recognition, which incorporatesvisual cues into the semantic mining process. Specifically, CMFN consists of aposition self-enhanced encoder, a visual recognition branch and an iterativesemantic recognition branch. The position self-enhanced encoder providescharacter sequence position encoding for both the visual recognition branch andthe iterative semantic recognition branch. The visual recognition branchcarries out visual recognition based on the visual features extracted by CNNand the position encoding information provided by the position self-enhancedencoder. The iterative semantic recognition branch, which consists of alanguage recognition module and a cross-modal fusion gate, simulates the waythat human recognizes scene text and integrates cross-modal visual cues fortext recognition. The experiments demonstrate that the proposed CMFN algorithmachieves comparable performance to state-of-the-art algorithms, indicating itseffectiveness.</description><author>Jinzhi Zheng, Ruyi Ji, Libo Zhang, Yanjun Wu, Chen Zhao</author><pubDate>Thu, 18 Jan 2024 15:05:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10041v1</guid></item><item><title>Large Language Models for Scientific Information Extraction: An Empirical Study for Virology</title><link>http://arxiv.org/abs/2401.10040v1</link><description>In this paper, we champion the use of structured and semantic contentrepresentation of discourse-based scholarly communication, inspired by toolslike Wikipedia infoboxes or structured Amazon product descriptions. Theserepresentations provide users with a concise overview, aiding scientists innavigating the dense academic landscape. Our novel automated approach leveragesthe robust text generation capabilities of LLMs to produce structured scholarlycontribution summaries, offering both a practical solution and insights intoLLMs' emergent abilities. For LLMs, the prime focus is on improving their general intelligence asconversational agents. We argue that these models can also be appliedeffectively in information extraction (IE), specifically in complex IE taskswithin terse domains like Science. This paradigm shift replaces the traditionalmodular, pipelined machine learning approach with a simpler objective expressedthrough instructions. Our results show that finetuned FLAN-T5 with 1000x fewerparameters than the state-of-the-art GPT-davinci is competitive for the task.</description><author>Mahsa Shamsabadi, Jennifer D'Souza, Sören Auer</author><pubDate>Thu, 18 Jan 2024 15:04:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10040v1</guid></item><item><title>GPT4Ego: Unleashing the Potential of Pre-trained Models for Zero-Shot Egocentric Action Recognition</title><link>http://arxiv.org/abs/2401.10039v1</link><description>Vision-Language Models (VLMs), pre-trained on large-scale datasets, haveshown impressive performance in various visual recognition tasks. Thisadvancement paves the way for notable performance in Zero-Shot EgocentricAction Recognition (ZS-EAR). Typically, VLMs handle ZS-EAR as a globalvideo-text matching task, which often leads to suboptimal alignment of visionand linguistic knowledge. We propose a refined approach for ZS-EAR using VLMs,emphasizing fine-grained concept-description alignment that capitalizes on therich semantic and contextual details in egocentric videos. In this paper, weintroduce GPT4Ego, a straightforward yet remarkably potent VLM framework forZS-EAR, designed to enhance the fine-grained alignment of concept anddescription between vision and language. Extensive experiments demonstrateGPT4Ego significantly outperforms existing VLMs on three large-scale egocentricvideo benchmarks, i.e., EPIC-KITCHENS-100 (33.2%, +9.4%), EGTEA (39.6%, +5.5%),and CharadesEgo (31.5%, +2.6%).</description><author>Guangzhao Dai, Xiangbo Shu, Wenhao Wu</author><pubDate>Thu, 18 Jan 2024 15:04:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10039v1</guid></item><item><title>HaGRID - HAnd Gesture Recognition Image Dataset</title><link>http://arxiv.org/abs/2206.08219v2</link><description>This paper introduces an enormous dataset, HaGRID (HAnd Gesture RecognitionImage Dataset), to build a hand gesture recognition (HGR) system concentratingon interaction with devices to manage them. That is why all 18 chosen gesturesare endowed with the semiotic function and can be interpreted as a specificaction. Although the gestures are static, they were picked up, especially forthe ability to design several dynamic gestures. It allows the trained model torecognize not only static gestures such as "like" and "stop" but also "swipes"and "drag and drop" dynamic gestures. The HaGRID contains 554,800 images andbounding box annotations with gesture labels to solve hand detection andgesture classification tasks. The low variability in context and subjects ofother datasets was the reason for creating the dataset without suchlimitations. Utilizing crowdsourcing platforms allowed us to collect samplesrecorded by 37,583 subjects in at least as many scenes with subject-to-cameradistances from 0.5 to 4 meters in various natural light conditions. Theinfluence of the diversity characteristics was assessed in ablation studyexperiments. Also, we demonstrate the HaGRID ability to be used for pretrainingmodels in HGR tasks. The HaGRID and pretrained models are publicly available.</description><author>Alexander Kapitanov, Karina Kvanchiani, Alexander Nagaev, Roman Kraynov, Andrei Makhliarchuk</author><pubDate>Thu, 18 Jan 2024 15:02:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.08219v2</guid></item><item><title>Depth Over RGB: Automatic Evaluation of Open Surgery Skills Using Depth Camera</title><link>http://arxiv.org/abs/2401.10037v1</link><description>Purpose: In this paper, we present a novel approach to the automaticevaluation of open surgery skills using depth cameras. This work is intended toshow that depth cameras achieve similar results to RGB cameras, which is thecommon method in the automatic evaluation of open surgery skills. Moreover,depth cameras offer advantages such as robustness to lighting variations,camera positioning, simplified data compression, and enhanced privacy, makingthem a promising alternative to RGB cameras. Methods: Experts and novice surgeons completed two simulators of opensuturing. We focused on hand and tool detection, and action segmentation insuturing procedures. YOLOv8 was used for tool detection in RGB and depthvideos. Furthermore, UVAST and MSTCN++ were used for action segmentation. Ourstudy includes the collection and annotation of a dataset recorded with AzureKinect. Results: We demonstrated that using depth cameras in object detection andaction segmentation achieves comparable results to RGB cameras. Furthermore, weanalyzed 3D hand path length, revealing significant differences between expertsand novice surgeons, emphasizing the potential of depth cameras in capturingsurgical skills. We also investigated the influence of camera angles onmeasurement accuracy, highlighting the advantages of 3D cameras in providing amore accurate representation of hand movements. Conclusion: Our research contributes to advancing the field of surgical skillassessment by leveraging depth cameras for more reliable and privacyevaluations. The findings suggest that depth cameras can be valuable inassessing surgical skills and provide a foundation for future research in thisarea.</description><author>Ido Zuckerman, Nicole Werner, Jonathan Kouchly, Emma Huston, Shannon DiMarco, Paul DiMusto, Shlomi Laufer</author><pubDate>Thu, 18 Jan 2024 15:00:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10037v1</guid></item><item><title>LOCALINTEL: Generating Organizational Threat Intelligence from Global and Local Cyber Knowledge</title><link>http://arxiv.org/abs/2401.10036v1</link><description>Security Operations Center (SoC) analysts gather threat reports from openlyaccessible global threat databases and customize them manually to suit aparticular organization's needs. These analysts also depend on internalrepositories, which act as private local knowledge database for anorganization. Credible cyber intelligence, critical operational details, andrelevant organizational information are all stored in these local knowledgedatabases. Analysts undertake a labor intensive task utilizing these global andlocal knowledge databases to manually create organization's unique threatresponse and mitigation strategies. Recently, Large Language Models (LLMs) haveshown the capability to efficiently process large diverse knowledge sources. Weleverage this ability to process global and local knowledge databases toautomate the generation of organization-specific threat intelligence. In this work, we present LOCALINTEL, a novel automated knowledgecontextualization system that, upon prompting, retrieves threat reports fromthe global threat repositories and uses its local knowledge database tocontextualize them for a specific organization. LOCALINTEL comprises of threekey phases: global threat intelligence retrieval, local knowledge retrieval,and contextualized completion generation. The former retrieves intelligencefrom global threat repositories, while the second retrieves pertinent knowledgefrom the local knowledge database. Finally, the fusion of these knowledgesources is orchestrated through a generator to produce a contextualizedcompletion.</description><author>Shaswata Mitra, Subash Neupane, Trisha Chakraborty, Sudip Mittal, Aritran Piplai, Manas Gaur, Shahram Rahimi</author><pubDate>Thu, 18 Jan 2024 15:00:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10036v1</guid></item><item><title>Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap</title><link>http://arxiv.org/abs/2401.10034v1</link><description>Large Language Models (LLMs), built upon Transformer-based architectures withmassive pretraining on diverse data, have not only revolutionized naturallanguage processing but also extended their prowess to various domains, markinga significant stride towards artificial general intelligence. The interplaybetween LLMs and Evolutionary Algorithms (EAs), despite differing in objectivesand methodologies, reveals intriguing parallels, especially in their sharedoptimization nature, black-box characteristics, and proficiency in handlingcomplex problems. Meanwhile, EA can not only provide an optimization frameworkfor LLM's further enhancement under black-box settings but also empower LLMwith flexible global search and iterative mechanism in applications. On theother hand, LLM's abundant domain knowledge enables EA to perform smartersearches, while its text processing capability assist in deploying EA acrossvarious tasks. Based on their complementary advantages, this paper presents acomprehensive review and forward-looking roadmap, categorizing their mutualinspiration into LLM-enhanced evolutionary optimization and EA-enhanced LLM.Some integrated synergy methods are further introduced to exemplify theamalgamation of LLMs and EAs in various application scenarios, including neuralarchitecture search, code generation, software engineering, and textgeneration. As the first comprehensive review specifically focused on the EAresearch in the era of LLMs, this paper provides a foundational stepping stonefor understanding and harnessing the collaborative potential of LLMs and EAs.By presenting a comprehensive review, categorization, and critical analysis, wecontribute to the ongoing discourse on the cross-disciplinary study of thesetwo powerful paradigms. The identified challenges and future directions offerguidance to unlock the full potential of this innovative collaboration.</description><author>Xingyu Wu, Sheng-hao Wu, Jibin Wu, Liang Feng, Kay Chen Tan</author><pubDate>Thu, 18 Jan 2024 14:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10034v1</guid></item><item><title>FreGrad: Lightweight and Fast Frequency-aware Diffusion Vocoder</title><link>http://arxiv.org/abs/2401.10032v1</link><description>The goal of this paper is to generate realistic audio with a lightweight andfast diffusion-based vocoder, named FreGrad. Our framework consists of thefollowing three key components: (1) We employ discrete wavelet transform thatdecomposes a complicated waveform into sub-band wavelets, which helps FreGradto operate on a simple and concise feature space, (2) We design afrequency-aware dilated convolution that elevates frequency awareness,resulting in generating speech with accurate frequency information, and (3) Weintroduce a bag of tricks that boosts the generation quality of the proposedmodel. In our experiments, FreGrad achieves 3.7 times faster training time and2.2 times faster inference speed compared to our baseline while reducing themodel size by 0.6 times (only 1.78M parameters) without sacrificing the outputquality. Audio samples are available at:https://mm.kaist.ac.kr/projects/FreGrad.</description><author>Tan Dat Nguyen, Ji-Hoon Kim, Youngjoon Jang, Jaehun Kim, Joon Son Chung</author><pubDate>Thu, 18 Jan 2024 14:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10032v1</guid></item><item><title>Framing Analysis of Health-Related Narratives: Conspiracy versus Mainstream Media</title><link>http://arxiv.org/abs/2401.10030v1</link><description>Understanding how online media frame issues is crucial due to their impact onpublic opinion. Research on framing using natural language processingtechniques mainly focuses on specific content features in messages and neglectstheir narrative elements. Also, the distinction between framing in differentsources remains an understudied problem. We address those issues andinvestigate how the framing of health-related topics, such as COVID-19 andother diseases, differs between conspiracy and mainstream websites. Weincorporate narrative information into the framing analysis by introducing anovel frame extraction approach based on semantic graphs. We find thathealth-related narratives in conspiracy media are predominantly framed in termsof beliefs, while mainstream media tend to present them in terms of science. Wehope our work offers new ways for a more nuanced frame analysis.</description><author>Markus Reiter-Haas, Beate Klösch, Markus Hadler, Elisabeth Lex</author><pubDate>Thu, 18 Jan 2024 14:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10030v1</guid></item><item><title>Nearly $d$-Linear Convergence Bounds for Diffusion Models via Stochastic Localization</title><link>http://arxiv.org/abs/2308.03686v2</link><description>Denoising diffusions are a powerful method to generate approximate samplesfrom high-dimensional data distributions. Recent results provide polynomialbounds on their convergence rate, assuming $L^2$-accurate scores. Until now,the tightest bounds were either superlinear in the data dimension or requiredstrong smoothness assumptions. We provide the first convergence bounds whichare linear in the data dimension (up to logarithmic factors) assuming onlyfinite second moments of the data distribution. We show that diffusion modelsrequire at most $\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$ steps toapproximate an arbitrary distribution on $\mathbb{R}^d$ corrupted with Gaussiannoise of variance $\delta$ to within $\varepsilon^2$ in KL divergence. Ourproof extends the Girsanov-based methods of previous works. We introduce arefined treatment of the error from discretizing the reverse SDE inspired bystochastic localization.</description><author>Joe Benton, Valentin De Bortoli, Arnaud Doucet, George Deligiannidis</author><pubDate>Thu, 18 Jan 2024 14:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03686v2</guid></item><item><title>Self-Rewarding Language Models</title><link>http://arxiv.org/abs/2401.10020v1</link><description>We posit that to achieve superhuman agents, future models require superhumanfeedback in order to provide an adequate training signal. Current approachescommonly train reward models from human preferences, which may then bebottlenecked by human performance level, and secondly these separate frozenreward models cannot then learn to improve during LLM training. In this work,we study Self-Rewarding Language Models, where the language model itself isused via LLM-as-a-Judge prompting to provide its own rewards during training.We show that during Iterative DPO training that not only does instructionfollowing ability improve, but also the ability to provide high-quality rewardsto itself. Fine-tuning Llama 2 70B on three iterations of our approach yields amodel that outperforms many existing systems on the AlpacaEval 2.0 leaderboard,including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study,this work opens the door to the possibility of models that can continuallyimprove in both axes.</description><author>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston</author><pubDate>Thu, 18 Jan 2024 14:43:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10020v1</guid></item><item><title>R-Judge: Benchmarking Safety Risk Awareness for LLM Agents</title><link>http://arxiv.org/abs/2401.10019v1</link><description>Large language models (LLMs) have exhibited great potential in autonomouslycompleting tasks across real-world applications. Despite this, these LLM agentsintroduce unexpected safety risks when operating in interactive environments.Instead of centering on LLM-generated content safety in most prior studies,this work addresses the imperative need for benchmarking the behavioral safetyof LLM agents within diverse environments. We introduce R-Judge, a benchmarkcrafted to evaluate the proficiency of LLMs in judging safety risks given agentinteraction records. R-Judge comprises 162 agent interaction records,encompassing 27 key risk scenarios among 7 application categories and 10 risktypes. It incorporates human consensus on safety with annotated safety risklabels and high-quality risk descriptions. Utilizing R-Judge, we conduct acomprehensive evaluation of 8 prominent LLMs commonly employed as the backbonefor agents. The best-performing model, GPT-4, achieves 72.29% in contrast tothe human score of 89.38%, showing considerable room for enhancing the riskawareness of LLMs. Notably, leveraging risk descriptions as environmentfeedback significantly improves model performance, revealing the importance ofsalient safety risk feedback. Furthermore, we design an effective chain ofsafety analysis technique to help the judgment of safety risks and conduct anin-depth case study to facilitate future research. R-Judge is publiclyavailable at https://github.com/Lordog/R-Judge.</description><author>Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, Rui Wang, Gongshen Liu</author><pubDate>Thu, 18 Jan 2024 14:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10019v1</guid></item><item><title>An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised Learning</title><link>http://arxiv.org/abs/2211.11086v2</link><description>Semi-supervised learning (SSL) has shown great promise in leveragingunlabeled data to improve model performance. While standard SSL assumes uniformdata distribution, we consider a more realistic and challenging setting calledimbalanced SSL, where imbalanced class distributions occur in both labeled andunlabeled data. Although there are existing endeavors to tackle this challenge,their performance degenerates when facing severe imbalance since they can notreduce the class imbalance sufficiently and effectively. In this paper, westudy a simple yet overlooked baseline -- SimiS -- which tackles data imbalanceby simply supplementing labeled data with pseudo-labels, according to thedifference in class distribution from the most frequent class. Such a simplebaseline turns out to be highly effective in reducing class imbalance. Itoutperforms existing methods by a significant margin, e.g., 12.8%, 13.6%, and16.7% over previous SOTA on CIFAR100-LT, FOOD101-LT, and ImageNet127respectively. The reduced imbalance results in faster convergence and betterpseudo-label accuracy of SimiS. The simplicity of our method also makes itpossible to be combined with other re-balancing techniques to improve theperformance further. Moreover, our method shows great robustness to a widerange of data distributions, which holds enormous potential in practice. Codewill be publicly available.</description><author>Hao Chen, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Marios Savvides, Bhiksha Raj</author><pubDate>Thu, 18 Jan 2024 14:40:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11086v2</guid></item><item><title>Upper and lower bounds for the Lipschitz constant of random neural networks</title><link>http://arxiv.org/abs/2311.01356v3</link><description>Empirical studies have widely demonstrated that neural networks are highlysensitive to small, adversarial perturbations of the input. The worst-caserobustness against these so-called adversarial examples can be quantified bythe Lipschitz constant of the neural network. In this paper, we study upper andlower bounds for the Lipschitz constant of random ReLU neural networks.Specifically, we assume that the weights and biases follow a generalization ofthe He initialization, where general symmetric distributions for the biases arepermitted. For shallow neural networks, we characterize the Lipschitz constantup to an absolute numerical constant. For deep networks with fixed depth andsufficiently large width, our established upper bound is larger than the lowerbound by a factor that is logarithmic in the width.</description><author>Paul Geuchen, Thomas Heindl, Dominik Stöger, Felix Voigtlaender</author><pubDate>Thu, 18 Jan 2024 14:39:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01356v3</guid></item><item><title>Text Region Multiple Information Perception Network for Scene Text Detection</title><link>http://arxiv.org/abs/2401.10017v1</link><description>Segmentation-based scene text detection algorithms can handle arbitrary shapescene texts and have strong robustness and adaptability, so it has attractedwide attention. Existing segmentation-based scene text detection algorithmsusually only segment the pixels in the center region of the text, whileignoring other information of the text region, such as edge information,distance information, etc., thus limiting the detection accuracy of thealgorithm for scene text. This paper proposes a plug-and-play module called theRegion Multiple Information Perception Module (RMIPM) to enhance the detectionperformance of segmentation-based algorithms. Specifically, we design animproved module that can perceive various types of information about scene textregions, such as text foreground classification maps, distance maps, directionmaps, etc. Experiments on MSRA-TD500 and TotalText datasets show that ourmethod achieves comparable performance with current state-of-the-artalgorithms.</description><author>Jinzhi Zheng, Libo Zhang, Yanjun Wu, Chen Zhao</author><pubDate>Thu, 18 Jan 2024 14:36:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10017v1</guid></item><item><title>Gender Bias in Machine Translation and The Era of Large Language Models</title><link>http://arxiv.org/abs/2401.10016v1</link><description>This chapter examines the role of Machine Translation in perpetuating genderbias, highlighting the challenges posed by cross-linguistic settings andstatistical dependencies. A comprehensive overview of relevant existing workrelated to gender bias in both conventional Neural Machine Translationapproaches and Generative Pretrained Transformer models employed as MachineTranslation systems is provided. Through an experiment using ChatGPT (based onGPT-3.5) in an English-Italian translation context, we further assess ChatGPT'scurrent capacity to address gender bias. The findings emphasize the ongoingneed for advancements in mitigating bias in Machine Translation systems andunderscore the importance of fostering fairness and inclusivity in languagetechnologies.</description><author>Eva Vanmassenhove</author><pubDate>Thu, 18 Jan 2024 14:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10016v1</guid></item><item><title>On Mitigating the Utility-Loss in Differentially Private Learning: A new Perspective by a Geometrically Inspired Kernel Approach</title><link>http://arxiv.org/abs/2304.01300v3</link><description>Privacy-utility tradeoff remains as one of the fundamental issues ofdifferentially private machine learning. This paper introduces a geometricallyinspired kernel-based approach to mitigate the accuracy-loss issue inclassification. In this approach, a representation of the affine hull of givendata points is learned in Reproducing Kernel Hilbert Spaces (RKHS). This leadsto a novel distance measure that hides privacy-sensitive information aboutindividual data points and improves the privacy-utility tradeoff viasignificantly reducing the risk of membership inference attacks. Theeffectiveness of the approach is demonstrated through experiments on MNISTdataset, Freiburg groceries dataset, and a real biomedical dataset. It isverified that the approach remains computationally practical. The applicationof the approach to federated learning is considered and it is observed that theaccuracy-loss due to data being distributed is either marginal or notsignificantly high.</description><author>Mohit Kumar, Bernhard A. Moser, Lukas Fischer</author><pubDate>Thu, 18 Jan 2024 14:33:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01300v3</guid></item><item><title>Towards Hierarchical Spoken Language Dysfluency Modeling</title><link>http://arxiv.org/abs/2401.10015v1</link><description>Speech dysfluency modeling is the bottleneck for both speech therapy andlanguage learning. However, there is no AI solution to systematically tacklethis problem. We first propose to define the concept of dysfluent speech anddysfluent speech modeling. We then present Hierarchical UnconstrainedDysfluency Modeling (H-UDM) approach that addresses both dysfluencytranscription and detection to eliminate the need for extensive manualannotation. Furthermore, we introduce a simulated dysfluent dataset calledVCTK++ to enhance the capabilities of H-UDM in phonetic transcription. Ourexperimental results demonstrate the effectiveness and robustness of ourproposed methods in both transcription and detection tasks.</description><author>Jiachen Lian, Gopala Anumanchipalli</author><pubDate>Thu, 18 Jan 2024 14:33:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10015v1</guid></item></channel></rss>