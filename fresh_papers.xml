<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 14 Jan 2025 01:00:08 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Multi-subject Open-set Personalization in Video Generation</title><link>http://arxiv.org/abs/2501.06187v1</link><description>Video personalization methods allow us to synthesize videos with specificconcepts such as people, pets, and places. However, existing methods oftenfocus on limited domains, require time-consuming optimization per subject, orsupport only a single subject. We present Video Alchemist $-$ a video modelwith built-in multi-subject, open-set personalization capabilities for bothforeground objects and background, eliminating the need for time-consumingtest-time optimization. Our model is built on a new Diffusion Transformermodule that fuses each conditional reference image and its correspondingsubject-level text prompt with cross-attention layers. Developing such a largemodel presents two main challenges: dataset and evaluation. First, as paireddatasets of reference images and videos are extremely hard to collect, wesample selected video frames as reference images and synthesize a clip of thetarget video. However, while models can easily denoise training videos givenreference frames, they fail to generalize to new contexts. To mitigate thisissue, we design a new automatic data construction pipeline with extensiveimage augmentations. Second, evaluating open-set video personalization is achallenge in itself. To address this, we introduce a personalization benchmarkthat focuses on accurate subject fidelity and supports diverse personalizationscenarios. Finally, our extensive experiments show that our methodsignificantly outperforms existing personalization methods in both quantitativeand qualitative evaluations.</description><author>Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, Sergey Tulyakov</author><pubDate>Fri, 10 Jan 2025 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06187v1</guid></item><item><title>LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs</title><link>http://arxiv.org/abs/2501.06186v1</link><description>Reasoning is a fundamental capability for solving complex multi-stepproblems, particularly in visual contexts where sequential step-wiseunderstanding is essential. Existing approaches lack a comprehensive frameworkfor evaluating visual reasoning and do not emphasize step-wise problem-solving.To this end, we propose a comprehensive framework for advancing step-by-stepvisual reasoning in large language models (LMMs) through three keycontributions. First, we introduce a visual reasoning benchmark specificallydesigned to evaluate multi-step reasoning tasks. The benchmark presents adiverse set of challenges with eight different categories ranging from complexvisual perception to scientific reasoning with over 4k reasoning steps intotal, enabling robust evaluation of LLMs' abilities to perform accurate andinterpretable visual reasoning across multiple steps. Second, we propose anovel metric that assesses visual reasoning quality at the granularity ofindividual steps, emphasizing both correctness and logical coherence. Theproposed metric offers deeper insights into reasoning performance compared totraditional end-task accuracy metrics. Third, we present a new multimodalvisual reasoning model, named LlamaV-o1, trained using a multi-step curriculumlearning approach, where tasks are progressively organized to facilitateincremental skill acquisition and problem-solving. The proposed LlamaV-o1 isdesigned for multi-step reasoning and learns step-by-step through a structuredtraining paradigm. Extensive experiments show that our LlamaV-o1 outperformsexisting open-source models and performs favorably against close-sourceproprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves anaverage score of 67.3 with an absolute gain of 3.8\% across six benchmarkswhile being 5 times faster during inference scaling. Our benchmark, model, andcode are publicly available.</description><author>Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, Hisham Cholakkal, Ivan Laptev, Mubarak Shah, Fahad Shahbaz Khan, Salman Khan</author><pubDate>Fri, 10 Jan 2025 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06186v1</guid></item><item><title>PEACE: Empowering Geologic Map Holistic Understanding with MLLMs</title><link>http://arxiv.org/abs/2501.06184v1</link><description>Geologic map, as a fundamental diagram in geology science, provides criticalinsights into the structure and composition of Earth's subsurface and surface.These maps are indispensable in various fields, including disaster detection,resource exploration, and civil engineering. Despite their significance,current Multimodal Large Language Models (MLLMs) often fall short in geologicmap understanding. This gap is primarily due to the challenging nature ofcartographic generalization, which involves handling high-resolution map,managing multiple associated components, and requiring domain-specificknowledge. To quantify this gap, we construct GeoMap-Bench, the first-everbenchmark for evaluating MLLMs in geologic map understanding, which assessesthe full-scale abilities in extracting, referring, grounding, reasoning, andanalyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agentdesigned for geologic map understanding, which features three modules:Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),and Prompt-enhanced Question Answering (PEQA). Inspired by theinterdisciplinary collaboration among human scientists, an AI expert group actsas consultants, utilizing a diverse tool pool to comprehensively analyzequestions. Through comprehensive experiments, GeoMap-Agent achieves an overallscore of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,paves the way for advanced AI applications in geology, enhancing the efficiencyand accuracy of geological investigations.</description><author>Yangyu Huang, Tianyi Gao, Haoran Xu, Qihao Zhao, Yang Song, Zhipeng Gui, Tengchao Lv, Hao Chen, Lei Cui, Scarlett Li, Furu Wei</author><pubDate>Fri, 10 Jan 2025 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06184v1</guid></item><item><title>Decentralized Diffusion Models</title><link>http://arxiv.org/abs/2501.05450v2</link><description>Large-scale AI model training divides work across thousands of GPUs, thensynchronizes gradients across them at each step. This incurs a significantnetwork burden that only centralized, monolithic clusters can support, drivingup infrastructure costs and straining power systems. We propose DecentralizedDiffusion Models, a scalable framework for distributing diffusion modeltraining across independent clusters or datacenters by eliminating thedependence on a centralized, high-bandwidth networking fabric. Our methodtrains a set of expert diffusion models over partitions of the dataset, each infull isolation from one another. At inference time, the experts ensemblethrough a lightweight router. We show that the ensemble collectively optimizesthe same objective as a single model trained over the whole dataset. This meanswe can divide the training burden among a number of "compute islands," loweringinfrastructure costs and improving resilience to localized GPU failures.Decentralized diffusion models empower researchers to take advantage ofsmaller, more cost-effective and more readily available compute like on-demandGPU nodes rather than central integrated systems. We conduct extensiveexperiments on ImageNet and LAION Aesthetics, showing that decentralizeddiffusion models FLOP-for-FLOP outperform standard diffusion models. We finallyscale our approach to 24 billion parameters, demonstrating that high-qualitydiffusion models can now be trained with just eight individual GPU nodes inless than a week.</description><author>David McAllister, Matthew Tancik, Jiaming Song, Angjoo Kanazawa</author><pubDate>Fri, 10 Jan 2025 18:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05450v2</guid></item><item><title>VideoAuteur: Towards Long Narrative Video Generation</title><link>http://arxiv.org/abs/2501.06173v1</link><description>Recent video generation models have shown promising results in producinghigh-quality video clips lasting several seconds. However, these models facechallenges in generating long sequences that convey clear and informativeevents, limiting their ability to support coherent narrations. In this paper,we present a large-scale cooking video dataset designed to advance long-formnarrative generation in the cooking domain. We validate the quality of ourproposed dataset in terms of visual fidelity and textual caption accuracy usingstate-of-the-art Vision-Language Models (VLMs) and video generation models,respectively. We further introduce a Long Narrative Video Director to enhanceboth visual and semantic coherence in generated videos and emphasize the roleof aligning visual embeddings to achieve improved overall video quality. Ourmethod demonstrates substantial improvements in generating visually detailedand semantically aligned keyframes, supported by finetuning techniques thatintegrate text and image embeddings within the video generation process.Project page: https://videoauteur.github.io/</description><author>Junfei Xiao, Feng Cheng, Lu Qi, Liangke Gui, Jiepeng Cen, Zhibei Ma, Alan Yuille, Lu Jiang</author><pubDate>Fri, 10 Jan 2025 18:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06173v1</guid></item><item><title>Machine Learning Force-Field Approach for Itinerant Electron Magnets</title><link>http://arxiv.org/abs/2501.06171v1</link><description>We review the recent development of machine-learning (ML) force-fieldframeworks for Landau-Lifshitz-Gilbert (LLG) dynamics simulations of itinerantelectron magnets, focusing on the general theory and implementations ofsymmetry-invariant representations of spin configurations. The crucialproperties that such magnetic descriptors must satisfy are differentiabilitywith respect to spin rotations and invariance to both lattice point-groupsymmetry and internal spin rotation symmetry. We propose an efficientimplementation based on the concept of reference irreducible representations,modified from the group-theoretical power-spectrum and bispectrum methods. TheML framework is demonstrated using the s-d models, which are widely applied inspintronics research. We show that LLG simulations based on local fieldspredicted by the trained ML models successfully reproduce representativenon-collinear spin structures, including 120$^\circ$, tetrahedral, and skyrmioncrystal orders of the triangular-lattice s-d models. Large-scale thermal quenchsimulations enabled by ML models further reveal intriguing freezing dynamicsand glassy stripe states consisting of skyrmions and bi-merons. Our workhighlights the utility of ML force-field approach to dynamical modeling ofcomplex spin orders in itinerant electron magnets.</description><author>Sheng Zhang, Yunhao Fan, Kotaro Shimizu, Gia-Wei Chern</author><pubDate>Fri, 10 Jan 2025 18:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06171v1</guid></item><item><title>Meta-Learning for Physically-Constrained Neural System Identification</title><link>http://arxiv.org/abs/2501.06167v1</link><description>We present a gradient-based meta-learning framework for rapid adaptation ofneural state-space models (NSSMs) for black-box system identification. Whenapplicable, we also incorporate domain-specific physical constraints to improvethe accuracy of the NSSM. The major benefit of our approach is that instead ofrelying solely on data from a single target system, our framework utilizes datafrom a diverse set of source systems, enabling learning from limited targetdata, as well as with few online training iterations. Through benchmarkexamples, we demonstrate the potential of our approach, study the effect offine-tuning subnetworks rather than full fine-tuning, and report real-worldcase studies to illustrate the practical application and generalizability ofthe approach to practical problems with physical-constraints. Specifically, weshow that the meta-learned models result in improved downstream performance inmodel-based state estimation in indoor localization and energy systems.</description><author>Ankush Chakrabarty, Gordon Wichern, Vedang M. Deshpande, Abraham P. Vinod, Karl Berntorp, Christopher R. Laughman</author><pubDate>Fri, 10 Jan 2025 18:46:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06167v1</guid></item><item><title>S2-Attention: Hardware-Aware Context Sharding Among Attention Heads</title><link>http://arxiv.org/abs/2407.17678v6</link><description>Sparse attention, which selectively attends to a subset of tokens in thecontext was supposed to be efficient. However, its theoretical reduction inFLOPs has rarely translated into wall-clock speed-up over its dense attentioncounterparts due to the lack of hardware-aware optimizations likeFlashAttention. Meanwhile, it remains unclear whether sparse attention canmaintain the model's quality at a scale of today's large language models (LLMs)and how. This paper presents Sparsely-Sharded(S2) Attention, a Triton librarythat provides kernel optimization for sparse attention customizable at bothper-head and per-context-range levels. S2-Attention enables the exploration ofnovel and high-performance sparse attention techniques, which we demonstratethrough extensive ablations across a wide range of sparse attention designs atvarious model scales. From these insights, we present several basic guidelinesto design sparse attention that can achieve not only practical efficiencyimprovements, but also strong downstream performance. To achieve highparallelization and optimized memory IO, sparse attention should shard thecontext heterogeneously across attention heads, where each head attends to adifferent subset of tokens while collectively covering the full context.Meanwhile, we find hybrid architectures combining sparse and dense attentionparticularly beneficial in practice. S2-Attention achieves wall-clock speedupof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline withstrong downstream performance on-par with full attention and perfect retrievalperformance at a 128k context length. At inference, for 7B models, our model,with the help of our S2-Attention kernel, achieves 4.5x speed-up compared todense counterparts. S2-Attention is released with easy-to-customize APIs fordirect usage in Megatron and vLLM.</description><author>Xihui Lin, Yunan Zhang, Suyu Ge, Liliang Ren, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song</author><pubDate>Fri, 10 Jan 2025 18:45:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17678v6</guid></item><item><title>Model Alignment Search</title><link>http://arxiv.org/abs/2501.06164v1</link><description>When can we say that two neural systems are the same? The answer to thisquestion is goal-dependent, and it is often addressed through correlativemethods such as Representational Similarity Analysis (RSA) and Centered KernelAlignment (CKA). What do we miss when we forgo causal explorations, and how canwe target specific types of similarity? In this work, we introduce ModelAlignment Search (MAS), a method for causally exploring distributedrepresentational similarity. The method learns invertible lineartransformations that align a subspace between two distributed networks'representations where causal information can be freely interchanged. We firstshow that the method can be used to transfer specific causal variables, such asthe number of items in a counting task, between networks with differenttraining seeds. We then explore open questions in number cognition by comparingdifferent types of numeric representations in models trained on structurallydifferent numeric tasks. We then explore differences between MAS vs preexistingcausal similarity methods, showing MAS to be more resistant to unwantedexchanges. Lastly, we introduce a counterfactual latent auxiliary loss functionthat helps shape causally relevant alignments even in cases where we do nothave causal access to one of the two models for training.</description><author>Satchel Grant</author><pubDate>Fri, 10 Jan 2025 18:39:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06164v1</guid></item><item><title>Efficient Transition State Searches by Freezing String Method with Graph Neural Network Potentials</title><link>http://arxiv.org/abs/2501.06159v1</link><description>Transition states are a critical bottleneck in chemical transformations.Significant efforts have been made to develop algorithms that efficientlylocate transition states on potential energy surfaces. However, thecomputational cost of ab-initio potential energy surface evaluation limits thesize of chemical systems that can routinely studied. In this work, we developand fine-tune a graph neural network potential energy function suitable fordescribing organic chemical reactions and use it to rapidly identify transitionstate guess structures. We successfully refine guess structures and locate atransition state in each test system considered and reduce the average numberof ab-initio calculations by 47% though use of the graph neural networkpotential energy function. Our results show that modern machine learning modelshave reached levels of reliability whereby they can be used to accelerateroutine computational chemistry tasks.</description><author>Jonah Marks, Joseph Gomes</author><pubDate>Fri, 10 Jan 2025 18:32:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06159v1</guid></item><item><title>Beyond Item Dissimilarities: Diversifying by Intent in Recommender Systems</title><link>http://arxiv.org/abs/2405.12327v3</link><description>It has become increasingly clear that recommender systems that overly focuson short-term engagement prevents users from exploring diverse interests,ultimately hurting long-term user experience. To tackle this challenge,numerous diversification algorithms have been proposed. These algorithmstypically rely on measures of item similarity, aiming to maximize thedissimilarity across items in the final set of recommendations. However, inthis work, we demonstrate the benefits of going beyond item-level similaritiesby utilizing higher-level user understanding--specifically, user intents thatpersist across multiple interactions--in diversification. Our approach ismotivated by the observation that user behaviors on online platforms arelargely driven by their underlying intents. Therefore, recommendations shouldensure that diverse user intents are accurately represented. While intent hasprimarily been studied in the context of search, it is less clear how toincorporate real-time dynamic intent predictions into recommender systems. Toaddress this gap, we develop a probabilistic intent-based whole-pagediversification framework for the final stage of a recommender system. Startingwith a prior belief of user intents, the proposed framework sequentiallyselects items for each position based on these beliefs and subsequently updatesposterior beliefs about the intents. This approach ensures that different userintents are represented on a page, towards optimizing long-term userexperience. We experiment with the intent diversification framework on YouTube,the world's largest video recommendation platform, serving billions of usersdaily. Live experiments on a diverse set of intents show that the proposedframework increases Daily Active Users (DAU) and overall user enjoyment,validating its effectiveness in facilitating long-term planning.</description><author>Yuyan Wang, Cheenar Banerjee, Samer Chucri, Fabio Soldo, Sriraj Badam, Ed H. Chi, Minmin Chen</author><pubDate>Fri, 10 Jan 2025 18:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12327v3</guid></item><item><title>GenMol: A Drug Discovery Generalist with Discrete Diffusion</title><link>http://arxiv.org/abs/2501.06158v1</link><description>Drug discovery is a complex process that involves multiple scenarios andstages, such as fragment-constrained molecule generation, hit generation andlead optimization. However, existing molecular generative models can onlytackle one or two of these scenarios and lack the flexibility to addressvarious aspects of the drug discovery pipeline. In this paper, we presentGeneralist Molecular generative model (GenMol), a versatile framework thataddresses these limitations by applying discrete diffusion to the SequentialAttachment-based Fragment Embedding (SAFE) molecular representation. GenMolgenerates SAFE sequences through non-autoregressive bidirectional paralleldecoding, thereby allowing utilization of a molecular context that does notrely on the specific token ordering and enhanced computational efficiency.Moreover, under the discrete diffusion framework, we introduce fragmentremasking, a strategy that optimizes molecules by replacing fragments withmasked tokens and regenerating them, enabling effective exploration of chemicalspace. GenMol significantly outperforms the previous GPT-based model trained onSAFE representations in de novo generation and fragment-constrained generation,and achieves state-of-the-art performance in goal-directed hit generation andlead optimization. These experimental results demonstrate that GenMol cantackle a wide range of drug discovery tasks, providing a unified and versatileapproach for molecular design.</description><author>Seul Lee, Karsten Kreis, Srimukh Prasad Veccham, Meng Liu, Danny Reidenbach, Yuxing Peng, Saee Paliwal, Weili Nie, Arash Vahdat</author><pubDate>Fri, 10 Jan 2025 18:30:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06158v1</guid></item><item><title>PySpatial: A High-Speed Whole Slide Image Pathomics Toolkit</title><link>http://arxiv.org/abs/2501.06151v1</link><description>Whole Slide Image (WSI) analysis plays a crucial role in modern digitalpathology, enabling large-scale feature extraction from tissue samples.However, traditional feature extraction pipelines based on tools likeCellProfiler often involve lengthy workflows, requiring WSI segmentation intopatches, feature extraction at the patch level, and subsequent mapping back tothe original WSI. To address these challenges, we present PySpatial, ahigh-speed pathomics toolkit specifically designed for WSI-level analysis.PySpatial streamlines the conventional pipeline by directly operating oncomputational regions of interest, reducing redundant processing steps.Utilizing rtree-based spatial indexing and matrix-based computation, PySpatialefficiently maps and processes computational regions, significantlyaccelerating feature extraction while maintaining high accuracy. Ourexperiments on two datasets-Perivascular Epithelioid Cell (PEC) and data fromthe Kidney Precision Medicine Project (KPMP)-demonstrate substantialperformance improvements. For smaller and sparse objects in PEC datasets,PySpatial achieves nearly a 10-fold speedup compared to standard CellProfilerpipelines. For larger objects, such as glomeruli and arteries in KPMP datasets,PySpatial achieves a 2-fold speedup. These results highlight PySpatial'spotential to handle large-scale WSI analysis with enhanced efficiency andaccuracy, paving the way for broader applications in digital pathology.</description><author>Yuechen Yang, Yu Wang, Tianyuan Yao, Ruining Deng, Mengmeng Yin, Shilin Zhao, Haichun Yang, Yuankai Huo</author><pubDate>Fri, 10 Jan 2025 18:24:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06151v1</guid></item><item><title>From discrete-time policies to continuous-time diffusion samplers: Asymptotic equivalences and faster training</title><link>http://arxiv.org/abs/2501.06148v1</link><description>We study the problem of training neural stochastic differential equations, ordiffusion models, to sample from a Boltzmann distribution without access totarget samples. Existing methods for training such models enforce time-reversalof the generative and noising processes, using either differentiable simulationor off-policy reinforcement learning (RL). We prove equivalences betweenfamilies of objectives in the limit of infinitesimal discretization steps,linking entropic RL methods (GFlowNets) with continuous-time objects (partialdifferential equations and path space measures). We further show that anappropriate choice of coarse time discretization during training allows greatlyimproved sample efficiency and the use of time-local objectives, achievingcompetitive performance on standard sampling benchmarks with reducedcomputational cost.</description><author>Julius Berner, Lorenz Richter, Marcin Sendera, Jarrid Rector-Brooks, Nikolay Malkin</author><pubDate>Fri, 10 Jan 2025 18:18:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06148v1</guid></item><item><title>Guess What I Think: Streamlined EEG-to-Image Generation with Latent Diffusion Models</title><link>http://arxiv.org/abs/2410.02780v2</link><description>Generating images from brain waves is gaining increasing attention due to itspotential to advance brain-computer interface (BCI) systems by understandinghow brain signals encode visual cues. Most of the literature has focused onfMRI-to-Image tasks as fMRI is characterized by high spatial resolution.However, fMRI is an expensive neuroimaging modality and does not allow forreal-time BCI. On the other hand, electroencephalography (EEG) is a low-cost,non-invasive, and portable neuroimaging technique, making it an attractiveoption for future real-time applications. Nevertheless, EEG presents inherentchallenges due to its low spatial resolution and susceptibility to noise andartifacts, which makes generating images from EEG more difficult. In thispaper, we address these problems with a streamlined framework based on theControlNet adapter for conditioning a latent diffusion model (LDM) through EEGsignals. We conduct experiments and ablation studies on popular benchmarks todemonstrate that the proposed method beats other state-of-the-art models.Unlike these methods, which often require extensive preprocessing, pretraining,different losses, and captioning models, our approach is efficient andstraightforward, requiring only minimal preprocessing and a few components. Thecode is available at https://github.com/LuigiSigillo/GWIT.</description><author>Eleonora Lopez, Luigi Sigillo, Federica Colonnese, Massimo Panella, Danilo Comminiello</author><pubDate>Fri, 10 Jan 2025 18:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02780v2</guid></item><item><title>xLSTM-SENet: xLSTM for Single-Channel Speech Enhancement</title><link>http://arxiv.org/abs/2501.06146v1</link><description>While attention-based architectures, such as Conformers, excel in speechenhancement, they face challenges such as scalability with respect to inputsequence length. In contrast, the recently proposed Extended Long Short-TermMemory (xLSTM) architecture offers linear scalability. However, xLSTM-basedmodels remain unexplored for speech enhancement. This paper introducesxLSTM-SENet, the first xLSTM-based single-channel speech enhancement system. Acomparative analysis reveals that xLSTM-and notably, even LSTM-can match oroutperform state-of-the-art Mamba- and Conformer-based systems across variousmodel sizes in speech enhancement on the VoiceBank+Demand dataset. Throughablation studies, we identify key architectural design choices such asexponential gating and bidirectionality contributing to its effectiveness. Ourbest xLSTM-based model, xLSTM-SENet2, outperforms state-of-the-art Mamba- andConformer-based systems on the Voicebank+DEMAND dataset.</description><author>Nikolai Lund Kühne, Jan Østergaard, Jesper Jensen, Zheng-Hua Tan</author><pubDate>Fri, 10 Jan 2025 18:10:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06146v1</guid></item><item><title>Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories</title><link>http://arxiv.org/abs/2501.06143v1</link><description>We investigate the multilingual and multimodal performance of a largelanguage model-based artificial intelligence (AI) system, GPT-4o, on a diverseset of physics concept inventories spanning multiple languages and subjectareas. The inventories taken from the PhysPort website cover the classicalphysics topics of mechanics, electromagnetism, optics, and thermodynamics aswell as relativity, quantum mechanics, astronomy, mathematics, and laboratoryskills. Unlike previous text-only studies, we uploaded the inventories asimages mirroring what a student would see on paper, assessing the system'smultimodal functionality. The AI is prompted in English and autonomouslychooses the language of its response - either remaining in the nominal languageof the test, switching entirely to English, or mixing languages - revealingadaptive behavior dependent on linguistic complexity and data availability. Ourresults indicate some variation in performance across subject areas, withlaboratory skills standing out as the area of poorest performance. Furthermore,the AI's performance on questions that require visual interpretation of imagesis worse than on purely text-based questions. Questions that are difficult forthe AI tend to be that way invariably of the inventory language. We also findlarge variations in performance across languages, with some appearing tobenefit substantially from language switching, a phenomenon similar tocode-switching ofhuman speakers. Overall, comparing the obtained AI results tothe existing literature, we find that the AI system outperforms averageundergraduate students post-instruction in all subject areas but laboratoryskills.</description><author>Gerd Kortemeyer, Marina Babayeva, Giulia Polverini, Bor Gregorcic, Ralf Widenhorn</author><pubDate>Fri, 10 Jan 2025 18:08:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06143v1</guid></item><item><title>Emergent Symbol-like Number Variables in Artificial Neural Networks</title><link>http://arxiv.org/abs/2501.06141v1</link><description>What types of numeric representations emerge in Neural Networks (NNs)? Towhat degree do NNs induce abstract, mutable, slot-like numeric variables, andin what situations do these representations emerge? How do theserepresentations change over learning, and how can we understand the neuralimplementations in ways that are unified across different NNs? In this work, weapproach these questions by first training sequence based neural systems usingNext Token Prediction (NTP) objectives on numeric tasks. We then seek tounderstand the neural solutions through the lens of causal abstractions orsymbolic algorithms. We use a combination of causal interventions andvisualization methods to find that artificial neural models do indeed developanalogs of interchangeable, mutable, latent number variables purely from theNTP objective. We then ask how variations on the tasks and model architecturesaffect the models' learned solutions to find that these symbol-like numericrepresentations do not form for every variant of the task, and transformerssolve the problem in a notably different way than their recurrent counterparts.We then show how the symbol-like variables change over the course of trainingto find a strong correlation between the models' task performance and thealignment of their symbol-like representations. Lastly, we show that in allcases, some degree of gradience exists in these neural symbols, highlightingthe difficulty of finding simple, interpretable symbolic stories of how neuralnetworks perform numeric tasks. Taken together, our results are consistent withthe view that neural networks can approximate interpretable symbolic programsof number cognition, but the particular program they approximate and the extentto which they approximate it can vary widely, depending on the networkarchitecture, training data, extent of training, and network size.</description><author>Satchel Grant, Noah D. Goodman, James L. McClelland</author><pubDate>Fri, 10 Jan 2025 18:03:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06141v1</guid></item><item><title>Two Stage Segmentation of Cervical Tumors using PocketNet</title><link>http://arxiv.org/abs/2409.11456v2</link><description>Cervical cancer remains the fourth most common malignancy amongst womenworldwide.1 Concurrent chemoradiotherapy (CRT) serves as the mainstaydefinitive treatment regimen for locally advanced cervical cancers and includesexternal beam radiation followed by brachytherapy.2 Integral to radiotherapytreatment planning is the routine contouring of both the target tumor at thelevel of the cervix, associated gynecologic anatomy and the adjacent organs atrisk (OARs). However, manual contouring of these structures is both time andlabor intensive and associated with known interobserver variability that canimpact treatment outcomes. While multiple tools have been developed toautomatically segment OARs and the high-risk clinical tumor volume (HR-CTV)using computed tomography (CT) images,3,4,5,6 the development of deeplearning-based tumor segmentation tools using routine T2-weighted (T2w)magnetic resonance imaging (MRI) addresses an unmet clinical need to improvethe routine contouring of both anatomical structures and cervical cancers,thereby increasing quality and consistency of radiotherapy planning. This workapplied a novel deep-learning model (PocketNet) to segment the cervix, vagina,uterus, and tumor(s) on T2w MRI. The performance of the PocketNet architecturewas evaluated, when trained on data via 5-fold cross validation. PocketNetachieved a mean Dice-Sorensen similarity coefficient (DSC) exceeding 70% fortumor segmentation and 80% for organ segmentation. These results suggest thatPocketNet is robust to variations in contrast protocols, providing reliablesegmentation of the regions of interest.</description><author>Awj Twam, Megan Jacobsen, Rachel Glenn, Peng Wei, Jia Sun, Ann Klopp, Aradhana M. Venkatesan, David Fuentes</author><pubDate>Fri, 10 Jan 2025 17:54:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11456v2</guid></item><item><title>MS-Temba : Multi-Scale Temporal Mamba for Efficient Temporal Action Detection</title><link>http://arxiv.org/abs/2501.06138v1</link><description>Action detection in real-world scenarios is particularly challenging due todensely distributed actions in hour-long untrimmed videos. It requires modelingboth short- and long-term temporal relationships while handling significantintra-class temporal variations. Previous state-of-the-art (SOTA)Transformer-based architectures, though effective, are impractical forreal-world deployment due to their high parameter count, GPU memory usage, andlimited throughput, making them unsuitable for very long videos. In this work,we innovatively adapt the Mamba architecture for action detection and proposeMulti-scale Temporal Mamba (MS-Temba), comprising two key components: TemporalMamba (Temba) Blocks and the Temporal Mamba Fuser. Temba Blocks include theTemporal Local Module (TLM) for short-range temporal modeling and the DilatedTemporal SSM (DTS) for long-range dependencies. By introducing dilations, anovel concept for Mamba, TLM and DTS capture local and global features atmultiple scales. The Temba Fuser aggregates these scale-specific features usingMamba to learn comprehensive multi-scale representations of untrimmed videos.MS-Temba is validated on three public datasets, outperforming SOTA methods onlong videos and matching prior methods on short videos while using onlyone-eighth of the parameters.</description><author>Arkaprava Sinha, Monish Soundar Raj, Pu Wang, Ahmed Helmy, Srijan Das</author><pubDate>Fri, 10 Jan 2025 17:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06138v1</guid></item><item><title>Supervision policies can shape long-term risk management in general-purpose AI models</title><link>http://arxiv.org/abs/2501.06137v1</link><description>The rapid proliferation and deployment of General-Purpose AI (GPAI) models,including large language models (LLMs), present unprecedented challenges for AIsupervisory entities. We hypothesize that these entities will need to navigatean emergent ecosystem of risk and incident reporting, likely to exceed theirsupervision capacity. To investigate this, we develop a simulation frameworkparameterized by features extracted from the diverse landscape of risk,incident, or hazard reporting ecosystems, including community-driven platforms,crowdsourcing initiatives, and expert assessments. We evaluate four supervisionpolicies: non-prioritized (first-come, first-served), random selection,priority-based (addressing the highest-priority risks first), anddiversity-prioritized (balancing high-priority risks with comprehensivecoverage across risk types). Our results indicate that while priority-based anddiversity-prioritized policies are more effective at mitigating high-impactrisks, particularly those identified by experts, they may inadvertently neglectsystemic issues reported by the broader community. This oversight can createfeedback loops that amplify certain types of reporting while discouragingothers, leading to a skewed perception of the overall risk landscape. Wevalidate our simulation results with several real-world datasets, including onewith over a million ChatGPT interactions, of which more than 150,000conversations were identified as risky. This validation underscores the complextrade-offs inherent in AI risk supervision and highlights how the choice ofrisk management policies can shape the future landscape of AI risks acrossdiverse GPAI models used in society.</description><author>Manuel Cebrian, Emilia Gomez, David Fernandez Llorca</author><pubDate>Fri, 10 Jan 2025 17:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06137v1</guid></item><item><title>CoDriveVLM: VLM-Enhanced Urban Cooperative Dispatching and Motion Planning for Future Autonomous Mobility on Demand Systems</title><link>http://arxiv.org/abs/2501.06132v1</link><description>The increasing demand for flexible and efficient urban transportationsolutions has spotlighted the limitations of traditional Demand ResponsiveTransport (DRT) systems, particularly in accommodating diverse passenger needsand dynamic urban environments. Autonomous Mobility-on-Demand (AMoD) systemshave emerged as a promising alternative, leveraging connected and autonomousvehicles (CAVs) to provide responsive and adaptable services. However, existingmethods primarily focus on either vehicle scheduling or path planning, whichoften simplify complex urban layouts and neglect the necessity for simultaneouscoordination and mutual avoidance among CAVs. This oversimplification posessignificant challenges to the deployment of AMoD systems in real-worldscenarios. To address these gaps, we propose CoDriveVLM, a novel framework thatintegrates high-fidelity simultaneous dispatching and cooperative motionplanning for future AMoD systems. Our method harnesses Vision-Language Models(VLMs) to enhance multi-modality information processing, and this enablescomprehensive dispatching and collision risk evaluation. The VLM-enhanced CAVdispatching coordinator is introduced to effectively manage complex andunforeseen AMoD conditions, thus supporting efficient schedulingdecision-making. Furthermore, we propose a scalable decentralized cooperativemotion planning method via consensus alternating direction method ofmultipliers (ADMM) focusing on collision risk evaluation and decentralizedtrajectory optimization. Simulation results demonstrate the feasibility androbustness of CoDriveVLM in various traffic conditions, showcasing itspotential to significantly improve the fidelity and effectiveness of AMoDsystems in future urban transportation networks. The code is available athttps://github.com/henryhcliu/CoDriveVLM.git.</description><author>Haichao Liu, Ruoyu Yao, Wenru Liu, Zhenmin Huang, Shaojie Shen, Jun Ma</author><pubDate>Fri, 10 Jan 2025 17:44:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06132v1</guid></item><item><title>Benchmark Evaluations, Applications, and Challenges of Large Vision Language Models: A Survey</title><link>http://arxiv.org/abs/2501.02189v2</link><description>Multimodal Vision Language Models (VLMs) have emerged as a transformativetechnology at the intersection of computer vision and natural languageprocessing, enabling machines to perceive and reason about the world throughboth visual and textual modalities. For example, models such as CLIP, Claude,and GPT-4V demonstrate strong reasoning and understanding abilities on visualand textual data and beat classical single modality vision models on zero-shotclassification. Despite their rapid advancements in research and growingpopularity in applications, a comprehensive survey of existing studies on VLMsis notably lacking, particularly for researchers aiming to leverage VLMs intheir specific domains. To this end, we provide a systematic overview of VLMsin the following aspects: model information of the major VLMs developed overthe past five years (2019-2024); the main architectures and training methods ofthese VLMs; summary and categorization of the popular benchmarks and evaluationmetrics of VLMs; the applications of VLMs including embodied agents, robotics,and video generation; the challenges and issues faced by current VLMs such ashallucination, fairness, and safety. Detailed collections including papers andmodel repository links are listed inhttps://github.com/zli12321/Awesome-VLM-Papers-And-Models.git.</description><author>Zongxia Li, Xiyang Wu, Hongyang Du, Huy Nghiem, Guangyao Shi</author><pubDate>Fri, 10 Jan 2025 17:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.02189v2</guid></item><item><title>Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented Conversational AI</title><link>http://arxiv.org/abs/2501.06129v1</link><description>General-purpose automatic speech recognition (ASR) systems do not alwaysperform well in goal-oriented dialogue. Existing ASR correction methods rely onprior user data or named entities. We extend correction to tasks that have noprior user data and exhibit linguistic flexibility such as lexical andsyntactic variations. We propose a novel context augmentation with a largelanguage model and a ranking strategy that incorporates contextual informationfrom the dialogue states of a goal-oriented conversational AI and its tasks.Our method ranks (1) n-best ASR hypotheses by their lexical and semanticsimilarity with context and (2) context by phonetic correspondence with ASRhypotheses. Evaluated in home improvement and cooking domains with real-worldusers, our method improves recall and F1 of correction by 34% and 16%,respectively, while maintaining precision and false positive rate. Users rated.8-1 point (out of 5) higher when our correction method worked properly, withno decrease due to false positives.</description><author>Yuya Asano, Sabit Hassan, Paras Sharma, Anthony Sicilia, Katherine Atwell, Diane Litman, Malihe Alikhani</author><pubDate>Fri, 10 Jan 2025 17:35:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06129v1</guid></item><item><title>Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain Diffusion Models</title><link>http://arxiv.org/abs/2408.11810v2</link><description>Diffusion Models have emerged as powerful generative models for high-qualityimage synthesis, with many subsequent image editing techniques based on them.However, the ease of text-based image editing introduces significant risks,such as malicious editing for scams or intellectual property infringement.Previous works have attempted to safeguard images from diffusion-based editingby adding imperceptible perturbations. These methods are costly andspecifically target prevalent Latent Diffusion Models (LDMs), whilePixel-domain Diffusion Models (PDMs) remain largely unexplored and robustagainst such attacks. Our work addresses this gap by proposing a novel attackframework, AtkPDM. AtkPDM is mainly composed of a feature representationattacking loss that exploits vulnerabilities in denoising UNets and a latentoptimization strategy to enhance the naturalness of adversarial images.Extensive experiments demonstrate the effectiveness of our approach inattacking dominant PDM-based editing methods (e.g., SDEdit) while maintainingreasonable fidelity and robustness against common defense methods.Additionally, our framework is extensible to LDMs, achieving comparableperformance to existing approaches.</description><author>Chun-Yen Shih, Li-Xuan Peng, Jia-Wei Liao, Ernie Chu, Cheng-Fu Chou, Jun-Cheng Chen</author><pubDate>Fri, 10 Jan 2025 17:29:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11810v2</guid></item><item><title>Merging Feed-Forward Sublayers for Compressed Transformers</title><link>http://arxiv.org/abs/2501.06126v1</link><description>With the rise and ubiquity of larger deep learning models, the need forhigh-quality compression techniques is growing in order to deploy these modelswidely. The sheer parameter count of these models makes it difficult to fitthem into the memory constraints of different hardware. In this work, wepresent a novel approach to model compression by merging similar parametergroups within a model, rather than pruning away less important parameters.Specifically, we select, align, and merge separate feed-forward sublayers inTransformer models, and test our method on language modeling, imageclassification, and machine translation. With our method, we demonstrateperformance comparable to the original models while combining more than a thirdof model feed-forward sublayers, and demonstrate improved performance over astrong layer-pruning baseline. For instance, we can remove over 21% of totalparameters from a Vision Transformer, while maintaining 99% of its originalperformance. Additionally, we observe that some groups of feed-forwardsublayers exhibit high activation similarity, which may help explain theirsurprising mergeability.</description><author>Neha Verma, Kenton Murray, Kevin Duh</author><pubDate>Fri, 10 Jan 2025 17:25:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06126v1</guid></item><item><title>Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding</title><link>http://arxiv.org/abs/2501.06117v1</link><description>While recent multilingual automatic speech recognition models claim tosupport thousands of languages, ASR for low-resource languages remains highlyunreliable due to limited bimodal speech and text training data. Bettermultilingual spoken language understanding (SLU) can strengthen massively therobustness of multilingual ASR by levering language semantics to compensate forscarce training data, such as disambiguating utterances via context orexploiting semantic similarities across languages. Even more so, SLU isindispensable for inclusive speech technology in roughly half of all livinglanguages that lack a formal writing system. However, the evaluation ofmultilingual SLU remains limited to shallower tasks such as intentclassification or language identification. To address this, we presentFleurs-SLU, a multilingual SLU benchmark that encompasses topical speechclassification in 102 languages and multiple-choice question answering throughlistening comprehension in 92 languages. We extensively evaluate bothend-to-end speech classification models and cascaded systems that combinespeech-to-text transcription with subsequent classification by large languagemodels on Fleurs-SLU. Our results show that cascaded systems exhibit greaterrobustness in multilingual SLU tasks, though speech encoders can achievecompetitive performance in topical speech classification when appropriatelypre-trained. We further find a strong correlation between robust multilingualASR, effective speech-to-text translation, and strong multilingual SLU,highlighting the mutual benefits between acoustic and semantic speechrepresentations.</description><author>Fabian David Schmidt, Ivan Vulić, Goran Glavaš, David Ifeoluwa Adelani</author><pubDate>Fri, 10 Jan 2025 17:15:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06117v1</guid></item><item><title>Self-Supervised Masked Mesh Learning for Unsupervised Anomaly Detection on 3D Cortical Surfaces</title><link>http://arxiv.org/abs/2412.05580v2</link><description>Unsupervised anomaly detection in brain imaging is challenging. In thispaper, we propose a self-supervised masked mesh learning for unsupervisedanomaly detection in 3D cortical surfaces. Our framework leverages theintrinsic geometry of the cortical surface to learn a self-supervisedrepresentation that captures the underlying structure of the brain. Weintroduce a masked mesh convolutional neural network (MMN) that learns topredict masked regions of the cortical surface. By training the MMN on a largedataset of healthy subjects, we learn a representation that captures the normalvariation in the cortical surface. We then use this representation to detectanomalies in unseen individuals by calculating anomaly scores based on thereconstruction error of the MMN. We evaluate our framework by training onpopulation-scale dataset UKB and HCP-Aging and testing on two datasets ofAlzheimer's disease patients ADNI and OASIS3. Our results show that ourframework can detect anomalies in cortical thickness, cortical volume, andcortical sulcus features, which are known to be sensitive biomarkers forAlzheimer's disease. Our proposed framework provides a promising approach forunsupervised anomaly detection based on normative variation of corticalfeatures.</description><author>Hao-Chun Yang, Sicheng Dai, Saige Rutherford, Christian Gaser, Andre F Marquand, Christian F Beckmann, Thomas Wolfers</author><pubDate>Fri, 10 Jan 2025 17:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05580v2</guid></item><item><title>Conformalised data synthesis</title><link>http://arxiv.org/abs/2312.08999v2</link><description>With the proliferation of increasingly complicated Deep Learningarchitectures, data synthesis is a highly promising technique to address thedemand of data-hungry models. However, reliably assessing the quality of a'synthesiser' model's output is an open research question with significantassociated risks for high-stake domains. To address this challenge, we proposea unique synthesis algorithm that generates data from high-confidence featurespace regions based on the Conformal Prediction framework. We support ourproposed algorithm with a comprehensive exploration of the core parameter'sinfluence, an in-depth discussion of practical advice, and an extensiveempirical evaluation of five benchmark datasets. To show our approach'sversatility on ubiquitous real-world challenges, the datasets were carefullyselected for their variety of difficult characteristics: low sample count,class imbalance, and non-separability. In all trials, training sets extendedwith our confident synthesised data performed at least as well as the originalset and frequently significantly improved Deep Learning performance by up to 61percentage points F1-score.</description><author>Julia A. Meister, Khuong An Nguyen</author><pubDate>Fri, 10 Jan 2025 17:04:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08999v2</guid></item><item><title>Inferring High-Order Couplings with Neural Networks</title><link>http://arxiv.org/abs/2501.06108v1</link><description>Maximum-entropy methods, rooted in the inverse Ising/Potts problem fromstatistical mechanics, have become indispensable tools for modeling pairwiseinteractions in disciplines such as bioinformatics, ecology, and neuroscience.Despite their remarkable success, these methods often overlook high-orderinteractions that may be crucial in complex systems. Conversely, while modernmachine learning approaches can capture such interactions, existinginterpretable frameworks are computationally expensive, making it impracticalto assess the relevance of high-order interactions in real-world scenarios.Restricted Boltzmann Machines (RBMs) offer a computationally efficientalternative by encoding statistical correlations via hidden nodes in abipartite neural network. Here, we present a method that maps RBMs exactly ontogeneralized Potts models with interactions of arbitrary high order. Thisapproach leverages large-$N$ approximations, facilitated by the simplearchitecture of the RBM, to enable the efficient extraction of effectivemany-body couplings with minimal computational cost. This mapping also enablesthe development of a general formal framework for the extraction of effectivehigher-order interactions in arbitrarily complex probabilistic models.Additionally, we introduce a robust formalism for gauge fixing within thegeneralized Potts model. We validate our method by accurately recovering two-and three-body interactions from synthetic datasets. Additionally, applying ourframework to protein sequence data demonstrates its effectiveness inreconstructing protein contact maps, achieving performance comparable tostate-of-the-art inverse Potts models. These results position RBMs as apowerful and efficient tool for investigating high-order interactions incomplex systems.</description><author>Aurélien Decelle, Alfonso de Jesús Navas Gómez, Beatriz Seoane</author><pubDate>Fri, 10 Jan 2025 17:01:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06108v1</guid></item><item><title>Closing the Gap: A User Study on the Real-world Usefulness of AI-powered Vulnerability Detection &amp; Repair in the IDE</title><link>http://arxiv.org/abs/2412.14306v2</link><description>This paper presents the first empirical study of a vulnerability detectionand fix tool with professional software developers on real projects that theyown. We implemented DeepVulGuard, an IDE-integrated tool based onstate-of-the-art detection and fix models, and show that it has promisingperformance on benchmarks of historic vulnerability data. DeepVulGuard scanscode for vulnerabilities (including identifying the vulnerability type andvulnerable region of code), suggests fixes, provides natural-languageexplanations for alerts and fixes, leveraging chat interfaces. We recruited 17professional software developers at Microsoft, observed their usage of the toolon their code, and conducted interviews to assess the tool's usefulness, speed,trust, relevance, and workflow integration. We also gathered detailedqualitative feedback on users' perceptions and their desired features. Studyparticipants scanned a total of 24 projects, 6.9k files, and over 1.7 millionlines of source code, and generated 170 alerts and 50 fix suggestions. We findthat although state-of-the-art AI-powered detection and fix tools show promise,they are not yet practical for real-world use due to a high rate of falsepositives and non-applicable fixes. User feedback reveals several actionablepain points, ranging from incomplete context to lack of customization for theuser's codebase. Additionally, we explore how AI features, including confidencescores, explanations, and chat interaction, can apply to vulnerabilitydetection and fixing. Based on these insights, we offer practicalrecommendations for evaluating and deploying AI detection and fix models. Ourcode and data are available at https://doi.org/10.6084/m9.figshare.26367139.</description><author>Benjamin Steenhoek, Kalpathy Sivaraman, Renata Saldivar Gonzalez, Yevhen Mohylevskyy, Roshanak Zilouchian Moghaddam, Wei Le</author><pubDate>Fri, 10 Jan 2025 17:00:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.14306v2</guid></item><item><title>Atlas: A Novel Pathology Foundation Model by Mayo Clinic, Charité, and Aignostics</title><link>http://arxiv.org/abs/2501.05409v2</link><description>Recent advances in digital pathology have demonstrated the effectiveness offoundation models across diverse applications. In this report, we presentAtlas, a novel vision foundation model based on the RudolfV approach. Our modelwas trained on a dataset comprising 1.2 million histopathology whole slideimages, collected from two medical institutions: Mayo Clinic and Charit\'e -Universt\"atsmedizin Berlin. Comprehensive evaluations show that Atlas achievesstate-of-the-art performance across twenty-one public benchmark datasets, eventhough it is neither the largest model by parameter count nor by trainingdataset size.</description><author>Maximilian Alber, Stephan Tietz, Jonas Dippel, Timo Milbich, Timothée Lesort, Panos Korfiatis, Moritz Krügener, Beatriz Perez Cancer, Neelay Shah, Alexander Möllers, Philipp Seegerer, Alexandra Carpen-Amarie, Kai Standvoss, Gabriel Dernbach, Edwin de Jong, Simon Schallenberg, Andreas Kunft, Helmut Hoffer von Ankershoffen, Gavin Schaeferle, Patrick Duffy, Matt Redlon, Philipp Jurmeister, David Horst, Lukas Ruff, Klaus-Robert Müller, Frederick Klauschen, Andrew Norgan</author><pubDate>Fri, 10 Jan 2025 16:58:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05409v2</guid></item><item><title>Finite-Horizon Single-Pull Restless Bandits: An Efficient Index Policy For Scarce Resource Allocation</title><link>http://arxiv.org/abs/2501.06103v1</link><description>Restless multi-armed bandits (RMABs) have been highly successful inoptimizing sequential resource allocation across many domains. However, in manypractical settings with highly scarce resources, where each agent can onlyreceive at most one resource, such as healthcare intervention programs, thestandard RMAB framework falls short. To tackle such scenarios, we introduceFinite-Horizon Single-Pull RMABs (SPRMABs), a novel variant in which each armcan only be pulled once. This single-pull constraint introduces additionalcomplexity, rendering many existing RMAB solutions suboptimal or ineffective.%To address this, we propose using dummy states to duplicate the system,ensuring that once an arm is activated, it transitions exclusively within thedummy states. To address this shortcoming, we propose using \textit{dummystates} that expand the system and enforce the one-pull constraint. We thendesign a lightweight index policy for this expanded system. For the first time,we demonstrate that our index policy achieves a sub-linearly decaying averageoptimality gap of $\tilde{\mathcal{O}}\left(\frac{1}{\rho^{1/2}}\right)$ for afinite number of arms, where $\rho$ is the scaling factor for each arm cluster.Extensive simulations validate the proposed method, showing robust performanceacross various domains compared to existing benchmarks.</description><author>Guojun Xiong, Haichuan Wang, Yuqi Pan, Saptarshi Mandal, Sanket Shah, Niclas Boehmer, Milind Tambe</author><pubDate>Fri, 10 Jan 2025 16:54:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06103v1</guid></item><item><title>From Conversation to Automation: Leveraging Large Language Models to Analyze Strategies in Problem Solving Therapy</title><link>http://arxiv.org/abs/2501.06101v1</link><description>Problem-solving therapy (PST) is a structured psychological approach thathelps individuals manage stress and resolve personal issues by guiding themthrough problem identification, solution brainstorming, decision-making, andoutcome evaluation. As mental health care increasingly integrates technologieslike chatbots and large language models (LLMs), understanding how PST can beeffectively automated is important. This study leverages anonymized therapytranscripts to analyze and classify therapeutic interventions using variousLLMs and transformer-based models. Our results show that GPT-4o achieved thehighest accuracy (0.76) in identifying PST strategies, outperforming othermodels. Additionally, we introduced a new dimension of communication strategiesthat enhances the current PST framework, offering deeper insights intotherapist-client interactions. This research demonstrates the potential of LLMsto automate complex therapeutic dialogue analysis, providing a scalable,efficient tool for mental health interventions. Our annotation framework canenhance the accessibility, effectiveness, and personalization of PST,supporting therapists in real-time with more precise, targeted interventions.</description><author>Elham Aghakhani, Lu Wang, Karla T. Washington, George Demiris, Jina Huh-Yoo, Rezvaneh Rezapour</author><pubDate>Fri, 10 Jan 2025 16:54:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06101v1</guid></item><item><title>Explaining Deep Learning-based Anomaly Detection in Energy Consumption Data by Focusing on Contextually Relevant Data</title><link>http://arxiv.org/abs/2501.06099v1</link><description>Detecting anomalies in energy consumption data is crucial for identifyingenergy waste, equipment malfunction, and overall, for ensuring efficient energymanagement. Machine learning, and specifically deep learning approaches, havebeen greatly successful in anomaly detection; however, they are black-boxapproaches that do not provide transparency or explanations. SHAP and itsvariants have been proposed to explain these models, but they suffer from highcomputational complexity (SHAP) or instability and inconsistency (e.g., KernelSHAP). To address these challenges, this paper proposes an explainabilityapproach for anomalies in energy consumption data that focuses oncontext-relevant information. The proposed approach leverages existingexplainability techniques, focusing on SHAP variants, together with globalfeature importance and weighted cosine similarity to select background datasetbased on the context of each anomaly point. By focusing on the context and mostrelevant features, this approach mitigates the instability of explainabilityalgorithms. Experimental results across 10 different machine learning models,five datasets, and five XAI techniques, demonstrate that our method reduces thevariability of explanations providing consistent explanations. Statisticalanalyses confirm the robustness of our approach, showing an average reductionin variability of approximately 38% across multiple datasets.</description><author>Mohammad Noorchenarboo, Katarina Grolinger</author><pubDate>Fri, 10 Jan 2025 16:53:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06099v1</guid></item><item><title>Improving Medical Visual Representations via Radiology Report Generation</title><link>http://arxiv.org/abs/2310.19635v2</link><description>Vision-language pretraining has been shown to produce high-quality visualencoders which transfer efficiently to downstream computer vision tasks.Contrastive learning approaches have increasingly been adopted for medicalvision language pretraining (MVLP), yet recent developments in generative AIoffer new modeling alternatives. This paper introduces RadTex, a CNN-encodertransformer-decoder architecture optimized for radiology. We explorebidirectional captioning as an alternative MVLP strategy and demonstrate thatRadTex's captioning pretraining is competitive with established contrastivemethods, achieving a CheXpert macro-AUC of 89.4%. Additionally, RadTex'slightweight text decoder not only generates clinically relevant radiologyreports (macro-F1 score of 0.349), but also provides targeted, interactiveresponses, highlighting the utility of bidirectional captioning in advancingmedical image analysis.</description><author>Keegan Quigley, Miriam Cha, Josh Barua, Geeticka Chauhan, Seth Berkowitz, Steven Horng, Polina Golland</author><pubDate>Fri, 10 Jan 2025 16:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19635v2</guid></item><item><title>ZeroComp: Zero-shot Object Compositing from Image Intrinsics via Diffusion</title><link>http://arxiv.org/abs/2410.08168v2</link><description>We present ZeroComp, an effective zero-shot 3D object compositing approachthat does not require paired composite-scene images during training. Our methodleverages ControlNet to condition from intrinsic images and combines it with aStable Diffusion model to utilize its scene priors, together operating as aneffective rendering engine. During training, ZeroComp uses intrinsic imagesbased on geometry, albedo, and masked shading, all without the need for pairedimages of scenes with and without composite objects. Once trained, itseamlessly integrates virtual 3D objects into scenes, adjusting shading tocreate realistic composites. We developed a high-quality evaluation dataset anddemonstrate that ZeroComp outperforms methods using explicit lightingestimations and generative techniques in quantitative and human perceptionbenchmarks. Additionally, ZeroComp extends to real and outdoor imagecompositing, even when trained solely on synthetic indoor data, showcasing itseffectiveness in image compositing.</description><author>Zitian Zhang, Frédéric Fortier-Chouinard, Mathieu Garon, Anand Bhattad, Jean-François Lalonde</author><pubDate>Fri, 10 Jan 2025 16:44:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08168v2</guid></item><item><title>Towards Developing Socially Compliant Automated Vehicles: State of the Art, Experts Expectations, and A Conceptual Framework</title><link>http://arxiv.org/abs/2501.06089v1</link><description>Automated Vehicles (AVs) hold promise for revolutionizing transportation byimproving road safety, traffic efficiency, and overall mobility. Despite thesteady advancement in high-level AVs in recent years, the transition to fullautomation entails a period of mixed traffic, where AVs of varying automationlevels coexist with human-driven vehicles (HDVs). Making AVs socially compliantand understood by human drivers is expected to improve the safety andefficiency of mixed traffic. Thus, ensuring AVs compatibility with HDVs andsocial acceptance is crucial for their successful and seamless integration intomixed traffic. However, research in this critical area of developing SociallyCompliant AVs (SCAVs) remains sparse. This study carries out the firstcomprehensive scoping review to assess the current state of the art indeveloping SCAVs, identifying key concepts, methodological approaches, andresearch gaps. An expert interview was also conducted to identify criticalresearch gaps and expectations towards SCAVs. Based on the scoping review andexpert interview input, a conceptual framework is proposed for the developmentof SCAVs. The conceptual framework is evaluated using an online surveytargeting researchers, technicians, policymakers, and other relevantprofessionals worldwide. The survey results provide valuable validation andinsights, affirming the significance of the proposed conceptual framework intackling the challenges of integrating AVs into mixed-traffic environments.Additionally, future research perspectives and suggestions are discussed,contributing to the research and development agenda of SCAVs.</description><author>Yongqi Dong, Bart van Arem, Haneen Farah</author><pubDate>Fri, 10 Jan 2025 16:39:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06089v1</guid></item><item><title>All AI Models are Wrong, but Some are Optimal</title><link>http://arxiv.org/abs/2501.06086v1</link><description>AI models that predict the future behavior of a system (a.k.a. predictive AImodels) are central to intelligent decision-making. However, decision-makingusing predictive AI models often results in suboptimal performance. This isprimarily because AI models are typically constructed to best fit the data, andhence to predict the most likely future rather than to enable high-performancedecision-making. The hope that such prediction enables high-performancedecisions is neither guaranteed in theory nor established in practice. In fact,there is increasing empirical evidence that predictive models must be tailoredto decision-making objectives for performance. In this paper, we establishformal (necessary and sufficient) conditions that a predictive model (AI-basedor not) must satisfy for a decision-making policy established using that modelto be optimal. We then discuss their implications for building predictive AImodels for sequential decision-making.</description><author>Akhil S Anand, Shambhuraj Sawant, Dirk Reinhardt, Sebastien Gros</author><pubDate>Fri, 10 Jan 2025 16:34:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06086v1</guid></item><item><title>Self-supervised video pretraining yields robust and more human-aligned visual representations</title><link>http://arxiv.org/abs/2210.06433v3</link><description>Humans learn powerful representations of objects and scenes by observing howthey evolve over time. Yet, outside of specific tasks that require explicittemporal understanding, static image pretraining remains the dominant paradigmfor learning visual foundation models. We question this mismatch, and askwhether video pretraining can yield visual representations that bear thehallmarks of human perception: generalisation across tasks, robustness toperturbations, and consistency with human judgements. To that end we propose anovel procedure for curating videos, and develop a contrastive framework whichlearns from the complex transformations therein. This simple paradigm fordistilling knowledge from videos, called VITO, yields general representationsthat far outperform prior video pretraining methods on image understandingtasks, and image pretraining methods on video understanding tasks. Moreover,VITO representations are significantly more robust to natural and syntheticdeformations than image-, video-, and adversarially-trained ones. Finally,VITO's predictions are strongly aligned with human judgements, surpassingmodels that were specifically trained for that purpose. Together, these resultssuggest that video pretraining could be a simple way of learning unified,robust, and human-aligned representations of the visual world.</description><author>Nikhil Parthasarathy, S. M. Ali Eslami, João Carreira, Olivier J. Hénaff</author><pubDate>Fri, 10 Jan 2025 16:26:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06433v3</guid></item><item><title>Averaged Adam accelerates stochastic optimization in the training of deep neural network approximations for partial differential equation and optimal control problems</title><link>http://arxiv.org/abs/2501.06081v1</link><description>Deep learning methods - usually consisting of a class of deep neural networks(DNNs) trained by a stochastic gradient descent (SGD) optimization method - arenowadays omnipresent in data-driven learning problems as well as in scientificcomputing tasks such as optimal control (OC) and partial differential equation(PDE) problems. In practically relevant learning tasks, often not theplain-vanilla standard SGD optimization method is employed to train theconsidered class of DNNs but instead more sophisticated adaptive andaccelerated variants of the standard SGD method such as the popular Adamoptimizer are used. Inspired by the classical Polyak-Ruppert averagingapproach, in this work we apply averaged variants of the Adam optimizer totrain DNNs to approximately solve exemplary scientific computing problems inthe form of PDEs and OC problems. We test the averaged variants of Adam in aseries of learning problems including physics-informed neural network (PINN),deep backward stochastic differential equation (deep BSDE), and deep Kolmogorovapproximations for PDEs (such as heat, Black-Scholes, Burgers, and Allen-CahnPDEs), including DNN approximations for OC problems, and including DNNapproximations for image classification problems (ResNet for CIFAR-10). In eachof the numerical examples the employed averaged variants of Adam outperform thestandard Adam and the standard SGD optimizers, particularly, in the situationof the scientific machine learning problems. The Python source codes for thenumerical experiments associated to this work can be found on GitHub athttps://github.com/deeplearningmethods/averaged-adam.</description><author>Steffen Dereich, Arnulf Jentzen, Adrian Riekert</author><pubDate>Fri, 10 Jan 2025 16:15:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06081v1</guid></item><item><title>Scale-up Unlearnable Examples Learning with High-Performance Computing</title><link>http://arxiv.org/abs/2501.06080v1</link><description>Recent advancements in AI models are structured to retain user interactions,which could inadvertently include sensitive healthcare data. In the healthcarefield, particularly when radiologists use AI-driven diagnostic tools hosted ononline platforms, there is a risk that medical imaging data may be repurposedfor future AI training without explicit consent, spotlighting critical privacyand intellectual property concerns around healthcare data usage. Addressingthese privacy challenges, a novel approach known as Unlearnable Examples (UEs)has been introduced, aiming to make data unlearnable to deep learning models. Aprominent method within this area, called Unlearnable Clustering (UC), hasshown improved UE performance with larger batch sizes but was previouslylimited by computational resources. To push the boundaries of UE performancewith theoretically unlimited resources, we scaled up UC learning across variousdatasets using Distributed Data Parallel (DDP) training on the Summitsupercomputer. Our goal was to examine UE efficacy at high-performancecomputing (HPC) levels to prevent unauthorized learning and enhance datasecurity, particularly exploring the impact of batch size on UE'sunlearnability. Utilizing the robust computational capabilities of the Summit,extensive experiments were conducted on diverse datasets such as Pets,MedMNist, Flowers, and Flowers102. Our findings reveal that both overly largeand overly small batch sizes can lead to performance instability and affectaccuracy. However, the relationship between batch size and unlearnabilityvaried across datasets, highlighting the necessity for tailored batch sizestrategies to achieve optimal data protection. Our results underscore thecritical role of selecting appropriate batch sizes based on the specificcharacteristics of each dataset to prevent learning and ensure data security indeep learning applications.</description><author>Yanfan Zhu, Issac Lyngaas, Murali Gopalakrishnan Meena, Mary Ellen I. Koran, Bradley Malin, Daniel Moyer, Shunxing Bao, Anuj Kapadia, Xiao Wang, Bennett Landman, Yuankai Huo</author><pubDate>Fri, 10 Jan 2025 16:15:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06080v1</guid></item><item><title>Explaining k-Nearest Neighbors: Abductive and Counterfactual Explanations</title><link>http://arxiv.org/abs/2501.06078v1</link><description>Despite the wide use of $k$-Nearest Neighbors as classification models, theirexplainability properties remain poorly understood from a theoreticalperspective. While nearest neighbors classifiers offer interpretability from a"data perspective", in which the classification of an input vector $\bar{x}$ isexplained by identifying the vectors $\bar{v}_1, \ldots, \bar{v}_k$ in thetraining set that determine the classification of $\bar{x}$, we argue that suchexplanations can be impractical in high-dimensional applications, where eachvector has hundreds or thousands of features and it is not clear what theirrelative importance is. Hence, we focus on understanding nearest neighborclassifications through a "feature perspective", in which the goal is toidentify how the values of the features in $\bar{x}$ affect its classification.Concretely, we study abductive explanations such as "minimum sufficientreasons", which correspond to sets of features in $\bar{x}$ that are enough toguarantee its classification, and "counterfactual explanations" based on theminimum distance feature changes one would have to perform in $\bar{x}$ tochange its classification. We present a detailed landscape of positive andnegative complexity results for counterfactual and abductive explanations,distinguishing between discrete and continuous feature spaces, and consideringthe impact of the choice of distance function involved. Finally, we show thatdespite some negative complexity results, Integer Quadratic Programming and SATsolving allow for computing explanations in practice.</description><author>Pablo Barceló, Alexander Kozachinskiy, Miguel Romero Orth, Bernardo Subercaseaux, José Verschae</author><pubDate>Fri, 10 Jan 2025 16:14:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06078v1</guid></item><item><title>Explainable Federated Bayesian Causal Inference and Its Application in Advanced Manufacturing</title><link>http://arxiv.org/abs/2501.06077v1</link><description>Causal inference has recently gained notable attention across various fieldslike biology, healthcare, and environmental science, especially withinexplainable artificial intelligence (xAI) systems, for uncovering the causalrelationships among multiple variables and outcomes. Yet, it has not been fullyrecognized and deployed in the manufacturing systems. In this paper, weintroduce an explainable, scalable, and flexible federated Bayesian learningframework, \texttt{xFBCI}, designed to explore causality through treatmenteffect estimation in distributed manufacturing systems. By leveraging federatedBayesian learning, we efficiently estimate posterior of local parameters toderive the propensity score for each client without accessing local privatedata. These scores are then used to estimate the treatment effect usingpropensity score matching (PSM). Through simulations on various datasets and areal-world Electrohydrodynamic (EHD) printing data, we demonstrate that ourapproach outperforms standard Bayesian causal inference methods and severalstate-of-the-art federated learning benchmarks.</description><author>Xiaofeng Xiao, Khawlah Alharbi, Pengyu Zhang, Hantang Qin, Xubo Yue</author><pubDate>Fri, 10 Jan 2025 16:14:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06077v1</guid></item><item><title>A monthly sub-national Harmonized Food Insecurity Dataset for comprehensive analysis and predictive modeling</title><link>http://arxiv.org/abs/2501.06076v1</link><description>Food security is a complex, multidimensional concept challenging to measurecomprehensively. Effective anticipation, monitoring, and mitigation of foodcrises require timely and comprehensive global data. This paper introduces theHarmonized Food Insecurity Dataset (HFID), an open-source resourceconsolidating four key data sources: the Integrated Food Security PhaseClassification (IPC)/Cadre Harmonis\'e (CH) phases, the Famine Early WarningSystems Network (FEWS NET) IPC-compatible phases, and the World Food Program's(WFP) Food Consumption Score (FCS) and reduced Coping Strategy Index (rCSI).Updated monthly and using a common reference system for administrative units,the HFID offers extensive spatial and temporal coverage. It serves as a vitaltool for food security experts and humanitarian agencies, providing a unifiedresource for analyzing food security conditions and highlighting global datadisparities. The scientific community can also leverage the HFID to developdata-driven predictive models, enhancing the capacity to forecast and preventfuture food crises.</description><author>Machefer Mélissande, Michele Ronco, Anne-Claire Thomas, Michael Assouline, Melanie Rabier, Christina Corbane, Felix Rembold</author><pubDate>Fri, 10 Jan 2025 16:13:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06076v1</guid></item><item><title>Geometry and Optimization of Shallow Polynomial Networks</title><link>http://arxiv.org/abs/2501.06074v1</link><description>We study shallow neural networks with polynomial activations. The functionspace for these models can be identified with a set of symmetric tensors withbounded rank. We describe general features of these networks, focusing on therelationship between width and optimization. We then consider teacher-studentproblems, that can be viewed as a problem of low-rank tensor approximation withrespect to a non-standard inner product that is induced by the datadistribution. In this setting, we introduce a teacher-metric discriminant whichencodes the qualitative behavior of the optimization as a function of thetraining data distribution. Finally, we focus on networks with quadraticactivations, presenting an in-depth analysis of the optimization landscape. Inparticular, we present a variation of the Eckart-Young Theorem characterizingall critical points and their Hessian signatures for teacher-student problemswith quadratic networks and Gaussian training data.</description><author>Yossi Arjevani, Joan Bruna, Joe Kileel, Elzbieta Polak, Matthew Trager</author><pubDate>Fri, 10 Jan 2025 16:11:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06074v1</guid></item><item><title>The Expressive Power of Graph Neural Networks: A Survey</title><link>http://arxiv.org/abs/2308.08235v2</link><description>Graph neural networks (GNNs) are effective machine learning models for manygraph-related applications. Despite their empirical success, many researchefforts focus on the theoretical limitations of GNNs, i.e., the GNNs expressivepower. Early works in this domain mainly focus on studying the graphisomorphism recognition ability of GNNs, and recent works try to leverage theproperties such as subgraph counting and connectivity learning to characterizethe expressive power of GNNs, which are more practical and closer toreal-world. However, no survey papers and open-source repositoriescomprehensively summarize and discuss models in this important direction. Tofill the gap, we conduct a first survey for models for enhancing expressivepower under different forms of definition. Concretely, the models are reviewedbased on three categories, i.e., Graph feature enhancement, Graph topologyenhancement, and GNNs architecture enhancement.</description><author>Bingxu Zhang, Changjun Fan, Shixuan Liu, Kuihua Huang, Xiang Zhao, Jincai Huang, Zhong Liu</author><pubDate>Fri, 10 Jan 2025 16:02:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08235v2</guid></item><item><title>Distilling Calibration via Conformalized Credal Inference</title><link>http://arxiv.org/abs/2501.06066v1</link><description>Deploying artificial intelligence (AI) models on edge devices involves adelicate balance between meeting stringent complexity constraints, such aslimited memory and energy resources, and ensuring reliable performance insensitive decision-making tasks. One way to enhance reliability is throughuncertainty quantification via Bayesian inference. This approach, however,typically necessitates maintaining and running multiple models in an ensemble,which may exceed the computational limits of edge devices. This paperintroduces a low-complexity methodology to address this challenge by distillingcalibration information from a more complex model. In an offline phase,predictive probabilities generated by a high-complexity cloud-based model areleveraged to determine a threshold based on the typical divergence between thecloud and edge models. At run time, this threshold is used to construct credalsets -- ranges of predictive probabilities that are guaranteed, with auser-selected confidence level, to include the predictions of the cloud model.The credal sets are obtained through thresholding of a divergence measure inthe simplex of predictive probabilities. Experiments on visual and languagetasks demonstrate that the proposed approach, termed Conformalized Distillationfor Credal Inference (CD-CI), significantly improves calibration performancecompared to low-complexity Bayesian methods, such as Laplace approximation,making it a practical and efficient solution for edge AI deployments.</description><author>Jiayi Huang, Sangwoo Park, Nicola Paoletti, Osvaldo Simeone</author><pubDate>Fri, 10 Jan 2025 15:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06066v1</guid></item><item><title>High-dimensional classification problems with Barron regular boundaries under margin conditions</title><link>http://arxiv.org/abs/2412.07312v2</link><description>We prove that a classifier with a Barron-regular decision boundary can beapproximated with a rate of high polynomial degree by ReLU neural networks withthree hidden layers when a margin condition is assumed. In particular, forstrong margin conditions, high-dimensional discontinuous classifiers can beapproximated with a rate that is typically only achievable when approximating alow-dimensional smooth function. We demonstrate how these expression ratebounds imply fast-rate learning bounds that are close to $n^{-1}$ where $n$ isthe number of samples. In addition, we carry out comprehensive numericalexperimentation on binary classification problems with various margins. Westudy three different dimensions, with the highest dimensional problemcorresponding to images from the MNIST data set.</description><author>Jonathan García, Philipp Petersen</author><pubDate>Fri, 10 Jan 2025 15:46:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07312v2</guid></item><item><title>Personalized Language Model Learning on Text Data Without User Identifiers</title><link>http://arxiv.org/abs/2501.06062v1</link><description>In many practical natural language applications, user data are highlysensitive, requiring anonymous uploads of text data from mobile devices to thecloud without user identifiers. However, the absence of user identifiersrestricts the ability of cloud-based language models to provide personalizedservices, which are essential for catering to diverse user needs. The trivialmethod of replacing an explicit user identifier with a static user embedding asmodel input still compromises data anonymization. In this work, we propose tolet each mobile device maintain a user-specific distribution to dynamicallygenerate user embeddings, thereby breaking the one-to-one mapping between anembedding and a specific user. We further theoretically demonstrate that toprevent the cloud from tracking users via uploaded embeddings, the localdistributions of different users should either be derived from a linearlydependent space to avoid identifiability or be close to each other to preventaccurate attribution. Evaluation on both public and industrial datasets usingdifferent language models reveals a remarkable improvement in accuracy fromincorporating anonymous user embeddings, while preserving real-time inferencerequirement.</description><author>Yucheng Ding, Yangwenjian Tan, Xiangyu Liu, Chaoyue Niu, Fandong Meng, Jie Zhou, Ning Liu, Fan Wu, Guihai Chen</author><pubDate>Fri, 10 Jan 2025 15:46:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06062v1</guid></item><item><title>FaceMe: Robust Blind Face Restoration with Personal Identification</title><link>http://arxiv.org/abs/2501.05177v2</link><description>Blind face restoration is a highly ill-posed problem due to the lack ofnecessary context. Although existing methods produce high-quality outputs, theyoften fail to faithfully preserve the individual's identity. In this paper, wepropose a personalized face restoration method, FaceMe, based on a diffusionmodel. Given a single or a few reference images, we use an identity encoder toextract identity-related features, which serve as prompts to guide thediffusion model in restoring high-quality and identity-consistent facialimages. By simply combining identity-related features, we effectively minimizethe impact of identity-irrelevant features during training and support anynumber of reference image inputs during inference. Additionally, thanks to therobustness of the identity encoder, synthesized images can be used as referenceimages during training, and identity changing during inference does not requirefine-tuning the model. We also propose a pipeline for constructing a referenceimage training pool that simulates the poses and expressions that may appear inreal-world scenarios. Experimental results demonstrate that our FaceMe canrestore high-quality facial images while maintaining identity consistency,achieving excellent performance and robustness.</description><author>Siyu Liu, Zheng-Peng Duan, Jia OuYang, Jiayi Fu, Hyunhee Park, Zikun Liu, Chun-Le Guo, Chongyi Li</author><pubDate>Fri, 10 Jan 2025 15:44:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05177v2</guid></item><item><title>COMIX: Compositional Explanations using Prototypes</title><link>http://arxiv.org/abs/2501.06059v1</link><description>Aligning machine representations with human understanding is key to improvinginterpretability of machine learning (ML) models. When classifying a new image,humans often explain their decisions by decomposing the image into concepts andpointing to corresponding regions in familiar images. Current ML explanationtechniques typically either trace decision-making processes to referenceprototypes, generate attribution maps highlighting feature importance, orincorporate intermediate bottlenecks designed to align with human-interpretableconcepts. The proposed method, named COMIX, classifies an image by decomposingit into regions based on learned concepts and tracing each region tocorresponding ones in images from the training dataset, assuring thatexplanations fully represent the actual decision-making process. We dissect thetest image into selected internal representations of a neural network to deriveprototypical parts (primitives) and match them with the correspondingprimitives derived from the training data. In a series of qualitative andquantitative experiments, we theoretically prove and demonstrate that ourmethod, in contrast to post hoc analysis, provides fidelity of explanations andshows that the efficiency is competitive with other inherently interpretablearchitectures. Notably, it shows substantial improvements in fidelity andsparsity metrics, including 48.82% improvement in the C-insertion score on theImageNet dataset over the best state-of-the-art baseline.</description><author>Sarath Sivaprasad, Dmitry Kangin, Plamen Angelov, Mario Fritz</author><pubDate>Fri, 10 Jan 2025 15:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06059v1</guid></item><item><title>Learning Flexible Heterogeneous Coordination with Capability-Aware Shared Hypernetworks</title><link>http://arxiv.org/abs/2501.06058v1</link><description>Cooperative heterogeneous multi-agent tasks require agents to effectivelycoordinate their behaviors while accounting for their relative capabilities.Learning-based solutions to this challenge span between two extremes: i)shared-parameter methods, which encode diverse behaviors within a singlearchitecture by assigning an ID to each agent, and are sample-efficient butresult in limited behavioral diversity; ii) independent methods, which learn aseparate policy for each agent, and show greater behavioral diversity but lacksample-efficiency. Prior work has also explored selective parameter-sharing,allowing for a compromise between diversity and efficiency. None of theseapproaches, however, effectively generalize to unseen agents or teams. Wepresent Capability-Aware Shared Hypernetworks (CASH), a novel architecture forheterogeneous multi-agent coordination that generates sufficient diversitywhile maintaining sample-efficiency via soft parameter-sharing hypernetworks.Intuitively, CASH allows the team to learn common strategies using a sharedencoder, which are then adapted according to the team's individual andcollective capabilities with a hypernetwork, allowing for zero-shotgeneralization to unseen teams and agents. We present experiments across twoheterogeneous coordination tasks and three standard learning paradigms(imitation learning, on- and off-policy reinforcement learning). CASH is ableto outperform baseline architectures in success rate and sample efficiency whenevaluated on unseen teams and agents despite using less than half of thelearnable parameters.</description><author>Kevin Fu, Pierce Howell, Shalin Jain, Harish Ravichandar</author><pubDate>Fri, 10 Jan 2025 15:39:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06058v1</guid></item><item><title>BIV-Priv-Seg: Locating Private Content in Images Taken by People With Visual Impairments</title><link>http://arxiv.org/abs/2407.18243v3</link><description>Individuals who are blind or have low vision (BLV) are at a heightened riskof sharing private information if they share photographs they have taken. Tofacilitate developing technologies that can help them preserve privacy, weintroduce BIV-Priv-Seg, the first localization dataset originating from peoplewith visual impairments that shows private content. It contains 1,028 imageswith segmentation annotations for 16 private object categories. We firstcharacterize BIV-Priv-Seg and then evaluate modern models' performance forlocating private content in the dataset. We find modern models struggle mostwith locating private objects that are not salient, small, and lack text aswell as recognizing when private content is absent from an image. We facilitatefuture extensions by sharing our new dataset with the evaluation server athttps://vizwiz.org/tasks-and-datasets/object-localization.</description><author>Yu-Yun Tseng, Tanusree Sharma, Lotus Zhang, Abigale Stangl, Leah Findlater, Yang Wang, Danna Gurari</author><pubDate>Fri, 10 Jan 2025 15:37:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18243v3</guid></item><item><title>Advances in Diffusion Models for Image Data Augmentation: A Review of Methods, Models, Evaluation Metrics and Future Research Directions</title><link>http://arxiv.org/abs/2407.04103v2</link><description>Image data augmentation constitutes a critical methodology in modern computervision tasks, since it can facilitate towards enhancing the diversity andquality of training datasets; thereby, improving the performance and robustnessof machine learning models in downstream tasks. In parallel, augmentationapproaches can also be used for editing/modifying a given image in a context-and semantics-aware way. Diffusion Models (DMs), which comprise one of the mostrecent and highly promising classes of methods in the field of generativeArtificial Intelligence (AI), have emerged as a powerful tool for image dataaugmentation, capable of generating realistic and diverse images by learningthe underlying data distribution. The current study realizes a systematic,comprehensive and in-depth review of DM-based approaches for imageaugmentation, covering a wide range of strategies, tasks and applications. Inparticular, a comprehensive analysis of the fundamental principles, modelarchitectures and training strategies of DMs is initially performed.Subsequently, a taxonomy of the relevant image augmentation methods isintroduced, focusing on techniques regarding semantic manipulation,personalization and adaptation, and application-specific augmentation tasks.Then, performance assessment methodologies and respective evaluation metricsare analyzed. Finally, current challenges and future research directions in thefield are discussed.</description><author>Panagiotis Alimisis, Ioannis Mademlis, Panagiotis Radoglou-Grammatikis, Panagiotis Sarigiannidis, Georgios Th. Papadopoulos</author><pubDate>Fri, 10 Jan 2025 15:37:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04103v2</guid></item><item><title>Uncovering the Genetic Basis of Glioblastoma Heterogeneity through Multimodal Analysis of Whole Slide Images and RNA Sequencing Data</title><link>http://arxiv.org/abs/2410.18710v2</link><description>Glioblastoma is a highly aggressive form of brain cancer characterized byrapid progression and poor prognosis. Despite advances in treatment, theunderlying genetic mechanisms driving this aggressiveness remain poorlyunderstood. In this study, we employed multimodal deep learning approaches toinvestigate glioblastoma heterogeneity using joint image/RNA-seq analysis. Ourresults reveal novel genes associated with glioblastoma. By leveraging acombination of whole-slide images and RNA-seq, as well as introducing novelmethods to encode RNA-seq data, we identified specific genetic profiles thatmay explain different patterns of glioblastoma progression. These findingsprovide new insights into the genetic mechanisms underlying glioblastomaheterogeneity and highlight potential targets for therapeutic intervention.</description><author>Ahmad Berjaoui, Louis Roussel, Eduardo Hugo Sanchez, Elizabeth Cohen-Jonathan Moyal</author><pubDate>Fri, 10 Jan 2025 15:37:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18710v2</guid></item><item><title>Dr. Tongue: Sign-Oriented Multi-label Detection for Remote Tongue Diagnosis</title><link>http://arxiv.org/abs/2501.03053v2</link><description>Tongue diagnosis is a vital tool in Western and Traditional Chinese Medicine,providing key insights into a patient's health by analyzing tongue attributes.The COVID-19 pandemic has heightened the need for accurate remote medicalassessments, emphasizing the importance of precise tongue attribute recognitionvia telehealth. To address this, we propose a Sign-Oriented multi-labelAttributes Detection framework. Our approach begins with an adaptive tonguefeature extraction module that standardizes tongue images and mitigatesenvironmental factors. This is followed by a Sign-oriented Network (SignNet)that identifies specific tongue attributes, emulating the diagnostic process ofexperienced practitioners and enabling comprehensive health evaluations. Tovalidate our methodology, we developed an extensive tongue image datasetspecifically designed for telemedicine. Unlike existing datasets, ours istailored for remote diagnosis, with a comprehensive set of attribute labels.This dataset will be openly available, providing a valuable resource forresearch. Initial tests have shown improved accuracy in detecting varioustongue attributes, highlighting our framework's potential as an essential toolfor remote medical assessments.</description><author>Yiliang Chen, Steven SC Ho, Cheng Xu, Yao Jie Xie, Wing-Fai Yeung, Shengfeng He, Jing Qin</author><pubDate>Fri, 10 Jan 2025 15:35:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.03053v2</guid></item><item><title>Enhancing, Refining, and Fusing: Towards Robust Multi-Scale and Dense Ship Detection</title><link>http://arxiv.org/abs/2501.06053v1</link><description>Synthetic aperture radar (SAR) imaging, celebrated for its high resolution,all-weather capability, and day-night operability, is indispensable formaritime applications. However, ship detection in SAR imagery faces significantchallenges, including complex backgrounds, densely arranged targets, and largescale variations. To address these issues, we propose a novel framework,Center-Aware SAR Ship Detector (CASS-Det), designed for robust multi-scale anddensely packed ship detection. CASS-Det integrates three key innovations: (1) acenter enhancement module (CEM) that employs rotational convolution toemphasize ship centers, improving localization while suppressing backgroundinterference; (2) a neighbor attention module (NAM) that leverages cross-layerdependencies to refine ship boundaries in densely populated scenes; and (3) across-connected feature pyramid network (CC-FPN) that enhances multi-scalefeature fusion by integrating shallow and deep features. Extensive experimentson the SSDD, HRSID, and LS-SSDD-v1.0 datasets demonstrate the state-of-the-artperformance of CASS-Det, excelling at detecting multi-scale and denselyarranged ships.</description><author>Congxia Zhao, Xiongjun Fu, Jian Dong, Shen Cao, Chunyan Zhang</author><pubDate>Fri, 10 Jan 2025 15:33:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06053v1</guid></item><item><title>Benchmarking Rotary Position Embeddings for Automatic Speech Recognition</title><link>http://arxiv.org/abs/2501.06051v1</link><description>Rotary Position Embedding (RoPE) encodes relative and absolute positionalinformation in Transformer-based models through rotation matrices applied toinput vectors within sequences. While RoPE has demonstrated superiorperformance compared to other positional embedding technologies in naturallanguage processing tasks, its effectiveness in speech processing applicationsremains understudied. In this work, we conduct a comprehensive evaluation ofRoPE across diverse automatic speech recognition (ASR) tasks. Our experimentalresults demonstrate that for ASR tasks, RoPE consistently achieves lower errorrates compared to the currently widely used relative positional embedding. Tofacilitate further research, we release the implementation and all experimentalrecipes through the SpeechBrain toolkit.</description><author>Shucong Zhang, Titouan Parcollet, Rogier van Dalen, Sourav Bhattacharya</author><pubDate>Fri, 10 Jan 2025 15:30:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06051v1</guid></item><item><title>Theoretical Error Analysis of Entropy Approximation for Gaussian Mixture</title><link>http://arxiv.org/abs/2202.13059v5</link><description>Gaussian mixture distributions are commonly employed to represent generalprobability distributions. Despite the importance of using Gaussian mixturesfor uncertainty estimation, the entropy of a Gaussian mixture cannot becalculated analytically. In this paper, we study the approximate entropyrepresented as the sum of the entropies of unimodal Gaussian distributions withmixing coefficients. This approximation is easy to calculate analyticallyregardless of dimension, but there is a lack of theoretical guarantees. Wetheoretically analyze the approximation error between the true and theapproximate entropy to reveal when this approximation works effectively. Thiserror is essentially controlled by how far apart each Gaussian component of theGaussian mixture is. To measure such separation, we introduce the ratios of thedistances between the means to the sum of the variances of each Gaussiancomponent of the Gaussian mixture, and we reveal that the error converges tozero as the ratios tend to infinity. In addition, the probabilistic estimateindicates that this convergence situation is more likely to occur inhigher-dimensional spaces. Therefore, our results provide a guarantee that thisapproximation works well for high-dimensional problems, such as neural networksthat involve a large number of parameters.</description><author>Takashi Furuya, Hiroyuki Kusumoto, Koichi Taniguchi, Naoya Kanno, Kazuma Suetake</author><pubDate>Fri, 10 Jan 2025 15:30:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.13059v5</guid></item><item><title>MARS: A neurosymbolic approach for interpretable drug discovery</title><link>http://arxiv.org/abs/2410.05289v3</link><description>Neurosymbolic (NeSy) artificial intelligence describes the combination oflogic or rule-based techniques with neural networks. Compared to neuralapproaches, NeSy methods often possess enhanced interpretability, which isparticularly promising for biomedical applications like drug discovery.However, since interpretability is broadly defined, there are no clearguidelines for assessing the biological plausibility of model interpretations.To assess interpretability in the context of drug discovery, we devise a novelprediction task, called drug mechanism-of-action (MoA) deconvolution, with anassociated, tailored knowledge graph (KG), MoA-net. We then develop the MoARetrieval System (MARS), a NeSy approach for drug discovery which leverageslogical rules with learned rule weights. Using this interpretable featurealongside domain knowledge, we find that MARS and other NeSy approaches on KGsare susceptible to reasoning shortcuts, in which the prediction of true labelsis driven by "degree-bias" rather than the domain-based rules. Subsequently, wedemonstrate ways to identify and mitigate this. Thereafter, MARS achievesperformance on par with current state-of-the-art models while producing modelinterpretations aligned with known MoAs.</description><author>Lauren Nicole DeLong, Yojana Gadiya, Paola Galdi, Jacques D. Fleuriot, Daniel Domingo-Fernández</author><pubDate>Fri, 10 Jan 2025 15:25:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05289v3</guid></item><item><title>MSCViT: A Small-size ViT architecture with Multi-Scale Self-Attention Mechanism for Tiny Datasets</title><link>http://arxiv.org/abs/2501.06040v1</link><description>Vision Transformer (ViT) has demonstrated significant potential in variousvision tasks due to its strong ability in modelling long-range dependencies.However, such success is largely fueled by training on massive samples. In realapplications, the large-scale datasets are not always available, and ViTperforms worse than Convolutional Neural Networks (CNNs) if it is only trainedon small scale dataset (called tiny dataset), since it requires large amount oftraining data to ensure its representational capacity. In this paper, asmall-size ViT architecture with multi-scale self-attention mechanism andconvolution blocks is presented (dubbed MSCViT) to model different scales ofattention at each layer. Firstly, we introduced wavelet convolution, whichselectively combines the high-frequency components obtained by frequencydivision with our convolution channel to extract local features. Then, alightweight multi-head attention module is developed to reduce the number oftokens and computational costs. Finally, the positional encoding (PE) in thebackbone is replaced by a local feature extraction module. Compared with theoriginal ViT, it is parameter-efficient and is particularly suitable for tinydatasets. Extensive experiments have been conducted on tiny datasets, in whichour model achieves an accuracy of 84.68% on CIFAR-100 with 14.0M parameters and2.5 GFLOPs, without pre-training on large datasets.</description><author>Bowei Zhang, Yi Zhang</author><pubDate>Fri, 10 Jan 2025 15:18:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06040v1</guid></item><item><title>AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery</title><link>http://arxiv.org/abs/2501.06039v1</link><description>Spatial proteomics technologies have transformed our understanding of complextissue architectures by enabling simultaneous analysis of multiple molecularmarkers and their spatial organization. The high dimensionality of these data,varying marker combinations across experiments and heterogeneous study designspose unique challenges for computational analysis. Here, we present VirtualTissues (VirTues), a foundation model framework for biological tissues thatoperates across the molecular, cellular and tissue scale. VirTues introducesinnovations in transformer architecture design, including a novel tokenizationscheme that captures both spatial and marker dimensions, and attentionmechanisms that scale to high-dimensional multiplex data while maintaininginterpretability. Trained on diverse cancer and non-cancer tissue datasets,VirTues demonstrates strong generalization capabilities without task-specificfine-tuning, enabling cross-study analysis and novel marker integration. As ageneralist model, VirTues outperforms existing approaches across clinicaldiagnostics, biological discovery and patient case retrieval tasks, whileproviding insights into tissue function and disease mechanisms.</description><author>Johann Wenckstern, Eeshaan Jain, Kiril Vasilev, Matteo Pariset, Andreas Wicki, Gabriele Gut, Charlotte Bunne</author><pubDate>Fri, 10 Jan 2025 15:17:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06039v1</guid></item><item><title>A Holistically Point-guided Text Framework for Weakly-Supervised Camouflaged Object Detection</title><link>http://arxiv.org/abs/2501.06038v1</link><description>Weakly-Supervised Camouflaged Object Detection (WSCOD) has gained popularityfor its promise to train models with weak labels to segment objects thatvisually blend into their surroundings. Recently, some methods usingsparsely-annotated supervision shown promising results through scribbling inWSCOD, while point-text supervision remains underexplored. Hence, this paperintroduces a novel holistically point-guided text framework for WSCOD bydecomposing into three phases: segment, choose, train. Specifically, we proposePoint-guided Candidate Generation (PCG), where the point's foreground serves asa correction for the text path to explicitly correct and rejuvenate the lossdetection object during the mask generation process (SEGMENT). We alsointroduce a Qualified Candidate Discriminator (QCD) to choose the optimal maskfrom a given text prompt using CLIP (CHOOSE), and employ the chosen pseudo maskfor training with a self-supervised Vision Transformer (TRAIN). Additionally,we developed a new point-supervised dataset (P2C-COD) and a text-superviseddataset (T-COD). Comprehensive experiments on four benchmark datasetsdemonstrate our method outperforms state-of-the-art methods by a large margin,and also outperforms some existing fully-supervised camouflaged objectdetection methods.</description><author>Tsui Qin Mok, Shuyong Gao, Haozhe Xing, Miaoyang He, Yan Wang, Wenqiang Zhang</author><pubDate>Fri, 10 Jan 2025 15:17:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06038v1</guid></item><item><title>Nonisotropic Gaussian Diffusion for Realistic 3D Human Motion Prediction</title><link>http://arxiv.org/abs/2501.06035v1</link><description>Probabilistic human motion prediction aims to forecast multiple possiblefuture movements from past observations. While current approaches report highdiversity and realism, they often generate motions with undetected limbstretching and jitter. To address this, we introduce SkeletonDiffusion, alatent diffusion model that embeds an explicit inductive bias on the human bodywithin its architecture and training. Our model is trained with a novelnonisotropic Gaussian diffusion formulation that aligns with the naturalkinematic structure of the human skeleton. Results show that our approachoutperforms conventional isotropic alternatives, consistently generatingrealistic predictions while avoiding artifacts such as limb distortion.Additionally, we identify a limitation in commonly used diversity metrics,which may inadvertently favor models that produce inconsistent limb lengthswithin the same sequence. SkeletonDiffusion sets a new benchmark on threereal-world datasets, outperforming various baselines across multiple evaluationmetrics. Visit our project page:https://ceveloper.github.io/publications/skeletondiffusion/</description><author>Cecilia Curreli, Dominik Muhle, Abhishek Saroha, Zhenzhang Ye, Riccardo Marin, Daniel Cremers</author><pubDate>Fri, 10 Jan 2025 15:13:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06035v1</guid></item><item><title>LUMIA: Linear probing for Unimodal and MultiModal Membership Inference Attacks leveraging internal LLM states</title><link>http://arxiv.org/abs/2411.19876v3</link><description>Large Language Models (LLMs) are increasingly used in a variety ofapplications, but concerns around membership inference have grown in parallel.Previous efforts focus on black-to-grey-box models, thus neglecting thepotential benefit from internal LLM information. To address this, we proposethe use of Linear Probes (LPs) as a method to detect Membership InferenceAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbedLUMIA, applies LPs layer-by-layer to get fine-grained data on the model innerworkings. We test this method across several model architectures, sizes anddatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIAachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previoustechniques. Remarkably, LUMIA reaches AUC&gt;60% in 65.33% of cases -- anincrement of 46.80% against the state of the art. Furthermore, our approachreveals key insights, such as the model layers where MIAs are most detectable.In multimodal models, LPs indicate that visual inputs can significantlycontribute to detect MIAs -- AUC&gt;60% is reached in 85.90% of experiments.</description><author>Luis Ibanez-Lissen, Lorena Gonzalez-Manzano, Jose Maria de Fuentes, Nicolas Anciaux, Joaquin Garcia-Alfaro</author><pubDate>Fri, 10 Jan 2025 15:08:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19876v3</guid></item><item><title>Generate, Transduct, Adapt: Iterative Transduction with VLMs</title><link>http://arxiv.org/abs/2501.06031v1</link><description>Transductive zero-shot learning with vision-language models leveragesimage-image similarities within the dataset to achieve better classificationaccuracy compared to the inductive setting. However, there is little work thatexplores the structure of the language space in this context. We proposeGTA-CLIP, a novel technique that incorporates supervision from language modelsfor joint transduction in language and vision spaces. Our approach is iterativeand consists of three steps: (i) incrementally exploring the attribute space byquerying language models, (ii) an attribute-augmented transductive inferenceprocedure, and (iii) fine-tuning the language and vision encoders based oninferred labels within the dataset. Through experiments with CLIP encoders, wedemonstrate that GTA-CLIP, yields an average performance improvement of 8.6%and 3.7% across 12 datasets and 3 encoders, over CLIP and transductive CLIPrespectively in the zero-shot setting. We also observe similar improvements ina few-shot setting. We present ablation studies that demonstrate the value ofeach step and visualize how the vision and language spaces evolve overiterations driven by the transductive learning.</description><author>Oindrila Saha, Logan Lawrence, Grant Van Horn, Subhransu Maji</author><pubDate>Fri, 10 Jan 2025 15:07:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06031v1</guid></item><item><title>Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent</title><link>http://arxiv.org/abs/2401.11940v3</link><description>This paper considers the problem of recovering a tensor with an underlyinglow-tubal-rank structure from a small number of corrupted linear measurements.Traditional approaches tackling such a problem require the computation oftensor Singular Value Decomposition (t-SVD), that is a computationallyintensive process, rendering them impractical for dealing with large-scaletensors. Aim to address this challenge, we propose an efficient and effectivelow-tubal-rank tensor recovery method based on a factorization procedure akinto the Burer-Monteiro (BM) method. Precisely, our fundamental approach involvesdecomposing a large tensor into two smaller factor tensors, followed by solvingthe problem through factorized gradient descent (FGD). This strategy eliminatesthe need for t-SVD computation, thereby reducing computational costs andstorage requirements. We provide rigorous theoretical analysis to ensure theconvergence of FGD under both noise-free and noisy situations. Additionally, itis worth noting that our method does not require the precise estimation of thetensor tubal-rank. Even in cases where the tubal-rank is slightlyoverestimated, our approach continues to demonstrate robust performance. Aseries of experiments have been carried out to demonstrate that, as compared toother popular ones, our approach exhibits superior performance in multiplescenarios, in terms of the faster computational speed and the smallerconvergence error.</description><author>Zhiyu Liu, Zhi Han, Yandong Tang, Xi-Le Zhao, Yao Wang</author><pubDate>Fri, 10 Jan 2025 15:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11940v3</guid></item><item><title>Geometric-Based Nail Segmentation for Clinical Measurements</title><link>http://arxiv.org/abs/2501.06027v1</link><description>A robust segmentation method that can be used to perform measurements ontoenails is presented. The proposed method is used as the first step in aclinical trial to objectively quantify the incidence of a particular pathology.For such an assessment, it is necessary to distinguish a nail, which locallyappears to be similar to the skin. Many algorithms have been used, each ofwhich leverages different aspects of toenail appearance. We used the Houghtransform to locate the tip of the toe and estimate the nail location and size.Subsequently, we classified the super-pixels of the image based on theirgeometric and photometric information. Thereafter, the watershed transformdelineated the border of the nail. The method was validated using a 348-imagemedical dataset, achieving an accuracy of 0.993 and an F-measure of 0.925. Theproposed method is considerably robust across samples, with respect to factorssuch as nail shape, skin pigmentation, illumination conditions, and appearanceof large regions affected by a medical condition</description><author>Bernat Galmés, Gabriel Moyà-Alcover, Pedro Bibiloni, Javier Varona, Antoni Jaume-i-Capó</author><pubDate>Fri, 10 Jan 2025 15:04:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06027v1</guid></item><item><title>A unified cross-attention model for predicting antigen binding specificity to both HLA and TCR molecules</title><link>http://arxiv.org/abs/2405.06653v2</link><description>The immune checkpoint inhibitors have demonstrated promising clinicalefficacy across various tumor types, yet the percentage of patients who benefitfrom them remains low. The bindings between tumor antigens and HLA-I/TCRmolecules determine the antigen presentation and T-cell activation, therebyplaying an important role in the immunotherapy response. In this paper, wepropose UnifyImmun, a unified cross-attention transformer model designed tosimultaneously predict the bindings of peptides to both receptors, providingmore comprehensive evaluation of antigen immunogenicity. We devise a two-phasestrategy using virtual adversarial training that enables these two tasks toreinforce each other mutually, by compelling the encoders to extract moreexpressive features. Our method demonstrates superior performance in predictingboth pHLA and pTCR binding on multiple independent and external test sets.Notably, on a large-scale COVID-19 pTCR binding test set without any seenpeptide in training set, our method outperforms the current state-of-the-artmethods by more than 10\%. The predicted binding scores significantly correlatewith the immunotherapy response and clinical outcomes on two clinical cohorts.Furthermore, the cross-attention scores and integrated gradients reveal theamino-acid sites critical for peptide binding to receptors. In essence, ourapproach marks a significant step toward comprehensive evaluation of antigenimmunogenicity.</description><author>Chenpeng Yu, Xing Fang, Hui Liu</author><pubDate>Fri, 10 Jan 2025 15:02:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06653v2</guid></item><item><title>How to Tune a Multilingual Encoder Model for Germanic Languages: A Study of PEFT, Full Fine-Tuning, and Language Adapters</title><link>http://arxiv.org/abs/2501.06025v1</link><description>This paper investigates the optimal use of the multilingual encoder modelmDeBERTa for tasks in three Germanic languages -- German, Swedish, andIcelandic -- representing varying levels of presence and likely data quality inmDeBERTas pre-training data. We compare full fine-tuning with theparameter-efficient fine-tuning (PEFT) methods LoRA and Pfeiffer bottleneckadapters, finding that PEFT is more effective for the higher-resource language,German. However, results for Swedish and Icelandic are less consistent. We alsoobserve differences between tasks: While PEFT tends to work better for questionanswering, full fine-tuning is preferable for named entity recognition.Inspired by previous research on modular approaches that combine task andlanguage adapters, we evaluate the impact of adding PEFT modules trained onunstructured text, finding that this approach is not beneficial.</description><author>Romina Oji, Jenny Kunz</author><pubDate>Fri, 10 Jan 2025 15:01:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06025v1</guid></item><item><title>A Steerable Deep Network for Model-Free Diffusion MRI Registration</title><link>http://arxiv.org/abs/2501.04794v2</link><description>Nonrigid registration is vital to medical image analysis but remainschallenging for diffusion MRI (dMRI) due to its high-dimensional,orientation-dependent nature. While classical methods are accurate, they arecomputationally demanding, and deep neural networks, though efficient, havebeen underexplored for nonrigid dMRI registration compared to structuralimaging. We present a novel, deep learning framework for model-free, nonrigidregistration of raw diffusion MRI data that does not require explicitreorientation. Unlike previous methods relying on derived representations suchas diffusion tensors or fiber orientation distribution functions, in ourapproach, we formulate the registration as an equivariant diffeomorphism ofposition-and-orientation space. Central to our method is an$\mathsf{SE}(3)$-equivariant UNet that generates velocity fields whilepreserving the geometric properties of a raw dMRI's domain. We introduce a newloss function based on the maximum mean discrepancy in Fourier space,implicitly matching ensemble average propagators across images. Experimentalresults on Human Connectome Project dMRI data demonstrate competitiveperformance compared to state-of-the-art approaches, with the added advantageof bypassing the overhead for estimating derived representations. This workestablishes a foundation for data-driven, geometry-aware dMRI registrationdirectly in the acquisition space.</description><author>Gianfranco Cortes, Xiaoda Qu, Baba C. Vemuri</author><pubDate>Fri, 10 Jan 2025 14:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.04794v2</guid></item><item><title>"Stupid robot, I want to speak to a human!" User Frustration Detection in Task-Oriented Dialog Systems</title><link>http://arxiv.org/abs/2411.17437v2</link><description>Detecting user frustration in modern-day task-oriented dialog (TOD) systemsis imperative for maintaining overall user satisfaction, engagement, andretention. However, most recent research is focused on sentiment and emotiondetection in academic settings, thus failing to fully encapsulate implicationsof real-world user data. To mitigate this gap, in this work, we focus on userfrustration in a deployed TOD system, assessing the feasibility ofout-of-the-box solutions for user frustration detection. Specifically, wecompare the performance of our deployed keyword-based approach, open-sourceapproaches to sentiment analysis, dialog breakdown detection methods, andemerging in-context learning LLM-based detection. Our analysis highlights thelimitations of open-source methods for real-world frustration detection, whiledemonstrating the superior performance of the LLM-based approach, achieving a16\% relative improvement in F1 score on an internal benchmark. Finally, weanalyze advantages and limitations of our methods and provide an insight intouser frustration detection task for industry practitioners.</description><author>Mireia Hernandez Caralt, Ivan Sekulić, Filip Carević, Nghia Khau, Diana Nicoleta Popa, Bruna Guedes, Victor Guimarães, Zeyu Yang, Andre Manso, Meghana Reddy, Paolo Rosso, Roland Mathis</author><pubDate>Fri, 10 Jan 2025 14:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17437v2</guid></item><item><title>BRIGHT: A globally distributed multimodal building damage assessment dataset with very-high-resolution for all-weather disaster response</title><link>http://arxiv.org/abs/2501.06019v1</link><description>Disaster events occur around the world and cause significant damage to humanlife and property. Earth observation (EO) data enables rapid and comprehensivebuilding damage assessment (BDA), an essential capability in the aftermath of adisaster to reduce human casualties and to inform disaster relief efforts.Recent research focuses on the development of AI models to achieve accuratemapping of unseen disaster events, mostly using optical EO data. However,solutions based on optical data are limited to clear skies and daylight hours,preventing a prompt response to disasters. Integrating multimodal (MM) EO data,particularly the combination of optical and SAR imagery, makes it possible toprovide all-weather, day-and-night disaster responses. Despite this potential,the development of robust multimodal AI models has been constrained by the lackof suitable benchmark datasets. In this paper, we present a BDA dataset usingveRy-hIGH-resoluTion optical and SAR imagery (BRIGHT) to support AI-basedall-weather disaster response. To the best of our knowledge, BRIGHT is thefirst open-access, globally distributed, event-diverse MM dataset specificallycurated to support AI-based disaster response. It covers five types of naturaldisasters and two types of man-made disasters across 12 regions worldwide, witha particular focus on developing countries where external assistance is mostneeded. The optical and SAR imagery in BRIGHT, with a spatial resolutionbetween 0.3-1 meters, provides detailed representations of individualbuildings, making it ideal for precise BDA. In our experiments, we have testedseven advanced AI models trained with our BRIGHT to validate thetransferability and robustness. The dataset and code are available athttps://github.com/ChenHongruixuan/BRIGHT. BRIGHT also serves as the officialdataset for the 2025 IEEE GRSS Data Fusion Contest.</description><author>Hongruixuan Chen, Jian Song, Olivier Dietrich, Clifford Broni-Bediako, Weihao Xuan, Junjue Wang, Xinlei Shao, Yimin Wei, Junshi Xia, Cuiling Lan, Konrad Schindler, Naoto Yokoya</author><pubDate>Fri, 10 Jan 2025 14:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06019v1</guid></item><item><title>Investigating the Impact of Observation Space Design Choices On Training Reinforcement Learning Solutions for Spacecraft Problems</title><link>http://arxiv.org/abs/2501.06016v1</link><description>Recent research using Reinforcement Learning (RL) to learn autonomous controlfor spacecraft operations has shown great success. However, a recent studyshowed their performance could be improved by changing the action space, i.e.control outputs, used in the learning environment. This has opened the door forfinding more improvements through further changes to the environment. The workin this paper focuses on how changes to the environment's observation space canimpact the training and performance of RL agents learning the spacecraftinspection task. The studies are split into two groups. The first looks at theimpact of sensors that were designed to help agents learn the task. The secondlooks at the impact of reference frames, reorienting the agent to see the worldfrom a different perspective. The results show the sensors are not necessary,but most of them help agents learn more optimal behavior, and that thereference frame does not have a large impact, but is best kept consistent.</description><author>Nathaniel Hamilton, Kyle Dunlap, Kerianne L Hobbs</author><pubDate>Fri, 10 Jan 2025 14:53:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06016v1</guid></item><item><title>Convergence analysis of wide shallow neural operators within the framework of Neural Tangent Kernel</title><link>http://arxiv.org/abs/2412.05545v3</link><description>Neural operators are aiming at approximating operators mapping between Banachspaces of functions, achieving much success in the field of scientificcomputing. Compared to certain deep learning-based solvers, such asPhysics-Informed Neural Networks (PINNs), Deep Ritz Method (DRM), neuraloperators can solve a class of Partial Differential Equations (PDEs). Althoughmuch work has been done to analyze the approximation and generalization errorof neural operators, there is still a lack of analysis on their training error.In this work, we conduct the convergence analysis of gradient descent for thewide shallow neural operators and physics-informed shallow neural operatorswithin the framework of Neural Tangent Kernel (NTK). The core idea lies on thefact that over-parameterization and random initialization together ensure thateach weight vector remains near its initialization throughout all iterations,yielding the linear convergence of gradient descent. In this work, wedemonstrate that under the setting of over-parametrization, gradient descentcan find the global minimum regardless of whether it is in continuous time ordiscrete time.</description><author>Xianliang Xu, Ye Li, Zhongyi Huang</author><pubDate>Fri, 10 Jan 2025 14:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05545v3</guid></item><item><title>Pose-independent 3D Anthropometry from Sparse Data</title><link>http://arxiv.org/abs/2501.06014v1</link><description>3D digital anthropometry is the study of estimating human body measurementsfrom 3D scans. Precise body measurements are important health indicators in themedical industry, and guiding factors in the fashion, ergonomic andentertainment industries. The measuring protocol consists of scanning the wholesubject in the static A-pose, which is maintained without breathing or movementduring the scanning process. However, the A-pose is not easy to maintain duringthe whole scanning process, which can last even up to a couple of minutes. Thisconstraint affects the final quality of the scan, which in turn affects theaccuracy of the estimated body measurements obtained from methods that rely ondense geometric data. Additionally, this constraint makes it impossible todevelop a digital anthropometry method for subjects unable to assume theA-pose, such as those with injuries or disabilities. We propose a method thatcan obtain body measurements from sparse landmarks acquired in any pose. Wemake use of the sparse landmarks of the posed subject to createpose-independent features, and train a network to predict the body measurementsas taken from the standard A-pose. We show that our method achieves comparableresults to competing methods that use dense geometry in the standard A-pose,but has the capability of estimating the body measurements from any pose usingsparse landmarks only. Finally, we address the lack of open-source 3Danthropometry methods by making our method available to the research communityat https://github.com/DavidBoja/pose-independent-anthropometry.</description><author>David Bojanić, Stefanie Wuhrer, Tomislav Petković, Tomislav Pribanić</author><pubDate>Fri, 10 Jan 2025 14:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06014v1</guid></item><item><title>A Neural Operator for Forecasting Carbon Monoxide Evolution in Cities</title><link>http://arxiv.org/abs/2501.06007v1</link><description>Real-time forecasting of carbon monoxide (CO) concentrations is essential forenabling timely interventions to improve urban air quality. Conventional airquality models often require extensive computational resources for accurate,multi-scale predictions, limiting their practicality for rapid, real-timeapplication. To address this challenge, we introduce the Complex NeuralOperator for Air Quality (CoNOAir), a machine learning model that forecast COconcentrations efficiently. CoNOAir demonstrates superior performance overstate-of-theart models, such as the Fourier Neural Operator (FNO), in bothshort-term (hourly) and extended (72-hour) forecasts at a national scale. Itexcels in capturing extreme pollution events and performs consistently acrossmultiple Indian cities, achieving an R2 above 0.95 for hourly CO predictionsacross all evaluated locations. CoNOAir equips authorities with an effectivetool for issuing early warnings and designing targeted intervention strategies.This work marks a step forward in achieving dependable, real-time CO pollutionpredictions for densely populated urban centres.</description><author>Sanchit Bedi, Karn Tiwari, Prathosh A. P., Sri Harsha Kota, N. M. Anoop Krishnan</author><pubDate>Fri, 10 Jan 2025 14:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06007v1</guid></item><item><title>ViM-Disparity: Bridging the Gap of Speed, Accuracy and Memory for Disparity Map Generation</title><link>http://arxiv.org/abs/2412.16745v2</link><description>In this work we propose a Visual Mamba (ViM) based architecture, to dissolvethe existing trade-off for real-time and accurate model with low computationoverhead for disparity map generation (DMG). Moreover, we proposed aperformance measure that can jointly evaluate the inference speed, computationoverhead and the accurateness of a DMG model. The code implementation andcorresponding models are available at: https://github.com/MBora/ViM-Disparity.</description><author>Maheswar Bora, Tushar Anand, Saurabh Atreya, Aritra Mukherjee, Abhijit Das</author><pubDate>Fri, 10 Jan 2025 14:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.16745v2</guid></item><item><title>CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control</title><link>http://arxiv.org/abs/2501.06006v1</link><description>We propose a method for generating fly-through videos of a scene, from asingle image and a given camera trajectory. We build upon an image-to-videolatent diffusion model. We condition its UNet denoiser on the cameratrajectory, using four techniques. (1) We condition the UNet's temporal blockson raw camera extrinsics, similar to MotionCtrl. (2) We use images containingcamera rays and directions, similar to CameraCtrl. (3) We reproject the initialimage to subsequent frames and use the resulting video as a condition. (4) Weuse 2D&lt;=&gt;3D transformers to introduce a global 3D representation, whichimplicitly conditions on the camera poses. We combine all conditions in aContolNet-style architecture. We then propose a metric that evaluates overallvideo quality and the ability to preserve details with view changes, which weuse to analyze the trade-offs of individual and combined conditions. Finally,we identify an optimal combination of conditions. We calibrate camera positionsin our datasets for scale consistency across scenes, and we train our sceneexploration model, CamCtrl3D, demonstrating state-of-theart results.</description><author>Stefan Popov, Amit Raj, Michael Krainin, Yuanzhen Li, William T. Freeman, Michael Rubinstein</author><pubDate>Fri, 10 Jan 2025 14:37:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06006v1</guid></item><item><title>CURing Large Models: Compression via CUR Decomposition</title><link>http://arxiv.org/abs/2501.04211v2</link><description>Large deep learning models have achieved remarkable success but areresource-intensive, posing challenges such as memory usage. We introduceCURing, a novel model compression method based on CUR matrix decomposition,which approximates weight matrices as the product of selected columns (C) androws (R), and a small linking matrix (U). We apply this decomposition toweights chosen based on the combined influence of their magnitudes andactivations. By identifying and retaining informative rows and columns, CURingsignificantly reduces model size with minimal performance loss. For example, itreduces Llama3.1-8B's parameters to 7.32B (-9%) in just 129 seconds, over 20times faster than prior compression methods.</description><author>Sanghyeon Park, Soo-Mook Moon</author><pubDate>Fri, 10 Jan 2025 14:36:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.04211v2</guid></item><item><title>SeMi: When Imbalanced Semi-Supervised Learning Meets Mining Hard Examples</title><link>http://arxiv.org/abs/2501.06004v1</link><description>Semi-Supervised Learning (SSL) can leverage abundant unlabeled data to boostmodel performance. However, the class-imbalanced data distribution inreal-world scenarios poses great challenges to SSL, resulting in performancedegradation. Existing class-imbalanced semi-supervised learning (CISSL) methodsmainly focus on rebalancing datasets but ignore the potential of using hardexamples to enhance performance, making it difficult to fully harness the powerof unlabeled data even with sophisticated algorithms. To address this issue, wepropose a method that enhances the performance of Imbalanced Semi-SupervisedLearning by Mining Hard Examples (SeMi). This method distinguishes the entropydifferences among logits of hard and easy examples, thereby identifying hardexamples and increasing the utility of unlabeled data, better addressing theimbalance problem in CISSL. In addition, we maintain a class-balanced memorybank with confidence decay for storing high-confidence embeddings to enhancethe pseudo-labels' reliability. Although our method is simple, it is effectiveand seamlessly integrates with existing approaches. We perform comprehensiveexperiments on standard CISSL benchmarks and experimentally demonstrate thatour proposed SeMi outperforms existing state-of-the-art methods on multiplebenchmarks, especially in reversed scenarios, where our best result showsapproximately a 54.8\% improvement over the baseline methods.</description><author>Yin Wang, Zixuan Wang, Hao Lu, Zhen Qin, Hailiang Zhao, Guanjie Cheng, Ge Su, Li Kuang, Mengchu Zhou, Shuiguang Deng</author><pubDate>Fri, 10 Jan 2025 14:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06004v1</guid></item><item><title>Learning to generate feasible graphs using graph grammars</title><link>http://arxiv.org/abs/2501.06003v1</link><description>Generative methods for graphs need to be sufficiently flexible to modelcomplex dependencies between sets of nodes. At the same time, the generatedgraphs need to satisfy domain-dependent feasibility conditions, that is, theyshould not violate certain constraints that would make their interpretationimpossible within the given application domain (e.g. a molecular graph where anatom has a very large number of chemical bounds). Crucially, constraints caninvolve not only local but also long-range dependencies: for example, themaximal length of a cycle can be bounded. Currently, a large class of generative approaches for graphs, such as methodsbased on artificial neural networks, is based on message passing schemes. Theseapproaches suffer from information 'dilution' issues that severely limit themaximal range of the dependencies that can be modeled. To address this problem,we propose a generative approach based on the notion of graph grammars. The keynovel idea is to introduce a domain-dependent coarsening procedure to provideshort-cuts for long-range dependencies. We show the effectiveness of our proposal in two domains: 1) small drugs and2) RNA secondary structures. In the first case, we compare the quality of thegenerated molecular graphs via the Molecular Sets (MOSES) benchmark suite,which evaluates the distance between generated and real molecules, theirlipophilicity, synthesizability, and drug-likeness. In the second case, we showthat the approach can generate very large graphs (with hundreds of nodes) thatare accepted as valid examples for a desired RNA family by the "Infernal"covariance model, a state-of-the-art RNA classifier. Our implementation is available on github:github.com/fabriziocosta/GraphLearn</description><author>Stefan Mautner, Rolf Backofen, Fabrizio Costa</author><pubDate>Fri, 10 Jan 2025 14:34:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06003v1</guid></item><item><title>DeltaGNN: Graph Neural Network with Information Flow Control</title><link>http://arxiv.org/abs/2501.06002v1</link><description>Graph Neural Networks (GNNs) are popular deep learning models designed toprocess graph-structured data through recursive neighborhood aggregations inthe message passing process. When applied to semi-supervised nodeclassification, the message-passing enables GNNs to understand short-rangespatial interactions, but also causes them to suffer from over-smoothing andover-squashing. These challenges hinder model expressiveness and prevent theuse of deeper models to capture long-range node interactions (LRIs) within thegraph. Popular solutions for LRIs detection are either too expensive to processlarge graphs due to high time complexity or fail to generalize across diversegraph structures. To address these limitations, we propose a mechanism called\emph{information flow control}, which leverages a novel connectivity measure,called \emph{information flow score}, to address over-smoothing andover-squashing with linear computational overhead, supported by theoreticalevidence. Finally, to prove the efficacy of our methodology we design DeltaGNN,the first scalable and generalizable approach for detecting long-range andshort-range interactions. We benchmark our model across 10 real-world datasets,including graphs with varying sizes, topologies, densities, and homophilicratios, showing superior performance with limited computational complexity. Theimplementation of the proposed methods are publicly available athttps://github.com/basiralab/DeltaGNN.</description><author>Kevin Mancini, Islem Rekik</author><pubDate>Fri, 10 Jan 2025 14:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06002v1</guid></item><item><title>Self-Supervised Partial Cycle-Consistency for Multi-View Matching</title><link>http://arxiv.org/abs/2501.06000v1</link><description>Matching objects across partially overlapping camera views is crucial inmulti-camera systems and requires a view-invariant feature extraction network.Training such a network with cycle-consistency circumvents the need forlabor-intensive labeling. In this paper, we extend the mathematical formulationof cycle-consistency to handle partial overlap. We then introduce a pseudo-maskwhich directs the training loss to take partial overlap into account. Weadditionally present several new cycle variants that complement each other andpresent a time-divergent scene sampling scheme that improves the data input forthis self-supervised setting. Cross-camera matching experiments on thechallenging DIVOTrack dataset show the merits of our approach. Compared to theself-supervised state-of-the-art, we achieve a 4.3 percentage point higher F1score with our combined contributions. Our improvements are robust to reducedoverlap in the training data, with substantial improvements in challengingscenes that need to make few matches between many people. Self-supervisedfeature networks trained with our method are effective at matching objects in arange of multi-camera settings, providing opportunities for complex tasks likelarge-scale multi-camera scene understanding.</description><author>Fedor Taggenbrock, Gertjan Burghouts, Ronald Poppe</author><pubDate>Fri, 10 Jan 2025 14:32:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06000v1</guid></item><item><title>Are We Done with MMLU?</title><link>http://arxiv.org/abs/2406.04127v3</link><description>Maybe not. We identify and analyse errors in the popular Massive MultitaskLanguage Understanding (MMLU) benchmark. Even though MMLU is widely adopted,our analysis demonstrates numerous ground truth errors that obscure the truecapabilities of LLMs. For example, we find that 57% of the analysed questionsin the Virology subset contain errors. To address this issue, we introduce acomprehensive framework for identifying dataset errors using a novel errorannotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700manually re-annotated questions across all 57 MMLU subjects. We estimate that6.49% of MMLU questions contain errors. Using MMLU-Redux, we demonstratesignificant discrepancies with the model performance metrics that wereoriginally reported. Our results strongly advocate for revising MMLU'serror-ridden questions to enhance its future utility and reliability as abenchmark. https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0.</description><author>Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, Pasquale Minervini</author><pubDate>Fri, 10 Jan 2025 14:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04127v3</guid></item><item><title>Minimizing Occlusion Effect on Multi-View Camera Perception in BEV with Multi-Sensor Fusion</title><link>http://arxiv.org/abs/2501.05997v1</link><description>Autonomous driving technology is rapidly evolving, offering the potential forsafer and more efficient transportation. However, the performance of thesesystems can be significantly compromised by the occlusion on sensors due toenvironmental factors like dirt, dust, rain, and fog. These occlusions severelyaffect vision-based tasks such as object detection, vehicle segmentation, andlane recognition. In this paper, we investigate the impact of various kinds ofocclusions on camera sensor by projecting their effects from multi-view cameraimages of the nuScenes dataset into the Bird's-Eye View (BEV) domain. Thisapproach allows us to analyze how occlusions spatially distribute and influencevehicle segmentation accuracy within the BEV domain. Despite significantadvances in sensor technology and multi-sensor fusion, a gap remains in theexisting literature regarding the specific effects of camera occlusions onBEV-based perception systems. To address this gap, we use a multi-sensor fusiontechnique that integrates LiDAR and radar sensor data to mitigate theperformance degradation caused by occluded cameras. Our findings demonstratethat this approach significantly enhances the accuracy and robustness ofvehicle segmentation tasks, leading to more reliable autonomous drivingsystems.</description><author>Sanjay Kumar, Hiep Truong, Sushil Sharma, Ganesh Sistu, Tony Scanlan, Eoin Grua, Ciarán Eising</author><pubDate>Fri, 10 Jan 2025 14:29:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05997v1</guid></item><item><title>DUET: Dual Clustering Enhanced Multivariate Time Series Forecasting</title><link>http://arxiv.org/abs/2412.10859v3</link><description>Multivariate time series forecasting is crucial for various applications,such as financial investment, energy management, weather forecasting, andtraffic optimization. However, accurate forecasting is challenging due to twomain factors. First, real-world time series often show heterogeneous temporalpatterns caused by distribution shifts over time. Second, correlations amongchannels are complex and intertwined, making it hard to model the interactionsamong channels precisely and flexibly. In this study, we address these challenges by proposing a general frameworkcalled DUET, which introduces dual clustering on the temporal and channeldimensions to enhance multivariate time series forecasting. First, we design aTemporal Clustering Module (TCM) that clusters time series into fine-graineddistributions to handle heterogeneous temporal patterns. For differentdistribution clusters, we design various pattern extractors to capture theirintrinsic temporal patterns, thus modeling the heterogeneity. Second, weintroduce a novel Channel-Soft-Clustering strategy and design a ChannelClustering Module (CCM), which captures the relationships among channels in thefrequency domain through metric learning and applies sparsification to mitigatethe adverse effects of noisy channels. Finally, DUET combines TCM and CCM toincorporate both the temporal and channel dimensions. Extensive experiments on25 real-world datasets from 10 application domains, demonstrate thestate-of-the-art performance of DUET.</description><author>Xiangfei Qiu, Xingjian Wu, Yan Lin, Chenjuan Guo, Jilin Hu, Bin Yang</author><pubDate>Fri, 10 Jan 2025 14:28:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10859v3</guid></item><item><title>An Attention-Guided Deep Learning Approach for Classifying 39 Skin Lesion Types</title><link>http://arxiv.org/abs/2501.05991v1</link><description>The skin, as the largest organ of the human body, is vulnerable to a diversearray of conditions collectively known as skin lesions, which encompass variousdermatoses. Diagnosing these lesions presents significant challenges formedical practitioners due to the subtle visual differences that are oftenimperceptible to the naked eye. While not all skin lesions arelife-threatening, certain types can act as early indicators of severe diseases,including skin cancers, underscoring the critical need for timely and accuratediagnostic methods. Deep learning algorithms have demonstrated remarkablepotential in facilitating the early detection and prognosis of skin lesions.This study advances the field by curating a comprehensive and diverse datasetcomprising 39 categories of skin lesions, synthesized from five publiclyavailable datasets. Using this dataset, the performance of fivestate-of-the-art deep learning models -- MobileNetV2, Xception, InceptionV3,EfficientNetB1, and Vision Transformer - is rigorously evaluated. To enhancethe accuracy and robustness of these models, attention mechanisms such as theEfficient Channel Attention (ECA) and the Convolutional Block Attention Module(CBAM) are incorporated into their architectures. Comprehensive evaluationacross multiple performance metrics reveals that the Vision Transformer modelintegrated with CBAM outperforms others, achieving an accuracy of 93.46%,precision of 94%, recall of 93%, F1-score of 93%, and specificity of 93.67%.These results underscore the significant potential of the proposed system insupporting medical professionals with accurate and efficient prognostic toolsfor diagnosing a broad spectrum of skin lesions. The dataset and code used inthis study can be found athttps://github.com/akabircs/Skin-Lesions-Classification.</description><author>Sauda Adiv Hanum, Ashim Dey, Muhammad Ashad Kabir</author><pubDate>Fri, 10 Jan 2025 14:25:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05991v1</guid></item><item><title>u-$μ$P: The Unit-Scaled Maximal Update Parametrization</title><link>http://arxiv.org/abs/2407.17465v3</link><description>The Maximal Update Parametrization ($\mu$P) aims to make the optimalhyperparameters (HPs) of a model independent of its size, allowing them to beswept using a cheap proxy model rather than the full-size target model. Wepresent a new scheme, u-$\mu$P, which improves upon $\mu$P by combining it withUnit Scaling, a method for designing models that makes them easy to train inlow-precision. The two techniques have a natural affinity: $\mu$P ensures thatthe scale of activations is independent of model size, and Unit Scaling ensuresthat activations, weights and gradients begin training with a scale of one.This synthesis opens the door to a simpler scheme, whose default values arenear-optimal. This in turn facilitates a more efficient sweeping strategy, withu-$\mu$P models reaching a loss that is equal to or lower than comparable$\mu$P models and working out-of-the-box in FP8.</description><author>Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y. Prince, Björn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach, Douglas Orr</author><pubDate>Fri, 10 Jan 2025 14:22:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17465v3</guid></item><item><title>Constraining constructions with WordNet: pros and cons for the semantic annotation of fillers in the Italian Constructicon</title><link>http://arxiv.org/abs/2501.05990v1</link><description>The paper discusses the role of WordNet-based semantic classification in theformalization of constructions, and more specifically in the semanticannotation of schematic fillers, in the Italian Constructicon. We outline howthe Italian Constructicon project uses Open Multilingual WordNet topics torepresent semantic features and constraints of constructions.</description><author>Flavio Pisciotta, Ludovica Pannitto, Lucia Busso, Beatrice Bernasconi, Francesca Masini</author><pubDate>Fri, 10 Jan 2025 14:21:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05990v1</guid></item><item><title>Addressing speaker gender bias in large scale speech translation systems</title><link>http://arxiv.org/abs/2501.05989v1</link><description>This study addresses the issue of speaker gender bias in Speech Translation(ST) systems, which can lead to offensive and inaccurate translations. Themasculine bias often found in large-scale ST systems is typically perpetuatedthrough training data derived from Machine Translation (MT) systems. Ourapproach involves two key steps. First, we employ Large Language Models (LLMs)to rectify translations based on the speaker's gender in a cost-effectivemanner. Second, we fine-tune the ST model with the corrected data, enabling themodel to generate gender-specific translations directly from audio cues,without the need for explicit gender input. Additionally, we propose athree-mode fine-tuned model for scenarios where the speaker's gender is eitherpredefined or should not be inferred from speech cues. We demonstrate a 70%improvement in translations for female speakers compared to our baseline andother large-scale ST systems, such as Seamless M4T and Canary, on the MuST-SHEtest set.</description><author>Shubham Bansal, Vikas Joshi, Harveen Chadha, Rupeshkumar Mehta, Jinyu Li</author><pubDate>Fri, 10 Jan 2025 14:20:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05989v1</guid></item><item><title>Comparing Self-Supervised Learning Models Pre-Trained on Human Speech and Animal Vocalizations for Bioacoustics Processing</title><link>http://arxiv.org/abs/2501.05987v1</link><description>Self-supervised learning (SSL) foundation models have emerged as powerful,domain-agnostic, general-purpose feature extractors applicable to a wide rangeof tasks. Such models pre-trained on human speech have demonstrated hightransferability for bioacoustic processing. This paper investigates (i) whetherSSL models pre-trained directly on animal vocalizations offer a significantadvantage over those pre-trained on speech, and (ii) whether fine-tuningspeech-pretrained models on automatic speech recognition (ASR) tasks canenhance bioacoustic classification. We conduct a comparative analysis usingthree diverse bioacoustic datasets and two different bioacoustic tasks. Resultsindicate that pre-training on bioacoustic data provides only marginalimprovements over speech-pretrained models, with comparable performance in mostscenarios. Fine-tuning on ASR tasks yields mixed outcomes, suggesting that thegeneral-purpose representations learned during SSL pre-training are alreadywell-suited for bioacoustic tasks. These findings highlight the robustness ofspeech-pretrained SSL models for bioacoustics and imply that extensivefine-tuning may not be necessary for optimal performance.</description><author>Eklavya Sarkar, Mathew Magimai. -Doss</author><pubDate>Fri, 10 Jan 2025 14:18:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05987v1</guid></item><item><title>Deep Variational Sequential Monte Carlo for High-Dimensional Observations</title><link>http://arxiv.org/abs/2501.05982v1</link><description>Sequential Monte Carlo (SMC), or particle filtering, is widely used innonlinear state-space systems, but its performance often suffers from poorlyapproximated proposal and state-transition distributions. This work introducesa differentiable particle filter that leverages the unsupervised variationalSMC objective to parameterize the proposal and transition distributions with aneural network, designed to learn from high-dimensional observations.Experimental results demonstrate that our approach outperforms establishedbaselines in tracking the challenging Lorenz attractor from high-dimensionaland partial observations. Furthermore, an evidence lower bound based evaluationindicates that our method offers a more accurate representation of theposterior distribution.</description><author>Wessel L. van Nierop, Nir Shlezinger, Ruud J. G. van Sloun</author><pubDate>Fri, 10 Jan 2025 14:10:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05982v1</guid></item><item><title>Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study of LLM Hallucination on North Korea</title><link>http://arxiv.org/abs/2501.05981v1</link><description>Hallucination in large language models (LLMs) remains a significant challengefor their safe deployment, particularly due to its potential to spreadmisinformation. Most existing solutions address this challenge by focusing onaligning the models with credible sources or by improving how modelscommunicate their confidence (or lack thereof) in their outputs. While thesemeasures may be effective in most contexts, they may fall short in scenariosrequiring more nuanced approaches, especially in situations where access toaccurate data is limited or determining credible sources is challenging. Inthis study, we take North Korea - a country characterised by an extreme lack ofreliable sources and the prevalence of sensationalist falsehoods - as a casestudy. We explore and evaluate how some of the best-performing multilingualLLMs and specific language-based models generate information about North Koreain three languages spoken in countries with significant geo-politicalinterests: English (United States, United Kingdom), Korean (South Korea), andMandarin Chinese (China). Our findings reveal significant differences,suggesting that the choice of model and language can lead to vastly differentunderstandings of North Korea, which has important implications given theglobal security challenges the country poses.</description><author>Eunjung Cho, Won Ik Cho, Soomin Seo</author><pubDate>Fri, 10 Jan 2025 14:08:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05981v1</guid></item><item><title>A Brain Age Residual Biomarker (BARB): Leveraging MRI-Based Models to Detect Latent Health Conditions in U.S. Veterans</title><link>http://arxiv.org/abs/2501.05970v1</link><description>Age prediction using brain imaging, such as MRIs, has achieved promisingresults, with several studies identifying the model's residual as a potentialbiomarker for chronic disease states. In this study, we developed a brain agepredictive model using a dataset of 1,220 U.S. veterans (18--80 years) andconvolutional neural networks (CNNs) trained on two-dimensional slices of axialT2-weighted fast spin-echo and T2-weighted fluid attenuated inversion recoveryMRI images. The model, incorporating a degree-3 polynomial ensemble, achievedan $R^{2}$ of 0.816 on the testing set. Images were acquired at the level ofthe anterior commissure and the frontal horns of the lateral ventricles.Residual analysis was performed to assess its potential as a biomarker for fiveICD-coded conditions: hypertension (HTN), diabetes mellitus (DM), mildtraumatic brain injury (mTBI), illicit substance abuse/dependence (SAD), andalcohol abuse/dependence (AAD). Residuals grouped by the number of ICD-codedconditions demonstrated different trends that were statistically significant($p = 0.002$), suggesting a relationship between disease states and predictedbrain age. This association was particularly pronounced in patients over 49years, where negative residuals (indicating advanced brain aging) correlatedwith the presence of multiple ICD codes. These findings support the potentialof residuals as biomarkers for detecting latent health conditions.</description><author>Arthur Bousquet, Sugata Banerji, Mark F. Conneely, Shahrzad Jamshidi</author><pubDate>Fri, 10 Jan 2025 13:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05970v1</guid></item><item><title>A stochastic first-order method with multi-extrapolated momentum for highly smooth unconstrained optimization</title><link>http://arxiv.org/abs/2412.14488v2</link><description>In this paper, we consider an unconstrained stochastic optimization problemwhere the objective function exhibits high-order smoothness. Specifically, wepropose a new stochastic first-order method (SFOM) with multi-extrapolatedmomentum, in which multiple extrapolations are performed in each iteration,followed by a momentum update based on these extrapolations. We demonstratethat the proposed SFOM can accelerate optimization by exploiting the high-ordersmoothness of the objective function $f$. Assuming that the $p$th-orderderivative of $f$ is Lipschitz continuous for some $p\ge2$, and underadditional mild assumptions, we establish that our method achieves a samplecomplexity of $\widetilde{\mathcal{O}}(\epsilon^{-(3p+1)/p})$ for finding apoint $x$ such that $\mathbb{E}[\|\nabla f(x)\|]\le\epsilon$. To the best ofour knowledge, this is the first SFOM to leverage arbitrary-order smoothness ofthe objective function for acceleration, resulting in a sample complexity thatimproves upon the best-known results without assuming the mean-squaredsmoothness condition. Preliminary numerical experiments validate the practicalperformance of our method and support our theoretical findings.</description><author>Chuan He</author><pubDate>Fri, 10 Jan 2025 13:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.14488v2</guid></item><item><title>Towards Early Prediction of Self-Supervised Speech Model Performance</title><link>http://arxiv.org/abs/2501.05966v1</link><description>In Self-Supervised Learning (SSL), pre-training and evaluation are resourceintensive. In the speech domain, current indicators of the quality of SSLmodels during pre-training, such as the loss, do not correlate well withdownstream performance. Consequently, it is often difficult to gauge the finaldownstream performance in a cost efficient manner during pre-training. In thiswork, we propose unsupervised efficient methods that give insights into thequality of the pre-training of SSL speech models, namely, measuring the clusterquality and rank of the embeddings of the SSL model. Results show that measuresof cluster quality and rank correlate better with downstream performance thanthe pre-training loss with only one hour of unlabeled audio, reducing the needfor GPU hours and labeled data in SSL model evaluation.</description><author>Ryan Whetten, Lucas Maison, Titouan Parcollet, Marco Dinarelli, Yannick Estève</author><pubDate>Fri, 10 Jan 2025 13:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05966v1</guid></item><item><title>Model Inversion in Split Learning for Personalized LLMs: New Insights from Information Bottleneck Theory</title><link>http://arxiv.org/abs/2501.05965v1</link><description>Personalized Large Language Models (LLMs) have become increasingly prevalent,showcasing the impressive capabilities of models like GPT-4. This trend hasalso catalyzed extensive research on deploying LLMs on mobile devices. Feasibleapproaches for such edge-cloud deployment include using split learning.However, previous research has largely overlooked the privacy leakageassociated with intermediate representations transmitted from devices toservers. This work is the first to identify model inversion attacks in thesplit learning framework for LLMs, emphasizing the necessity of secure defense.For the first time, we introduce mutual information entropy to understand theinformation propagation of Transformer-based LLMs and assess privacy attackperformance for LLM blocks. To address the issue of representations beingsparser and containing less information than embeddings, we propose a two-stageattack system in which the first part projects representations into theembedding space, and the second part uses a generative model to recover textfrom these embeddings. This design breaks down the complexity and achievesattack scores of 38%-75% in various scenarios, with an over 60% improvementover the SOTA. This work comprehensively highlights the potential privacy risksduring the deployment of personalized LLMs on the edge side.</description><author>Yunmeng Shu, Shaofeng Li, Tian Dong, Yan Meng, Haojin Zhu</author><pubDate>Fri, 10 Jan 2025 13:47:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05965v1</guid></item><item><title>Finnish SQuAD: A Simple Approach to Machine Translation of Span Annotations</title><link>http://arxiv.org/abs/2501.05963v1</link><description>We apply a simple method to machine translate datasets with span-levelannotation using the DeepL MT service and its ability to translate formatteddocuments. Using this method, we produce a Finnish version of the SQuAD2.0question answering dataset and train QA retriever models on this new dataset.We evaluate the quality of the dataset and more generally the MT method throughdirect evaluation, indirect comparison to other similar datasets, abacktranslation experiment, as well as through the performance of downstreamtrained QA models. In all these evaluations, we find that the method oftransfer is not only simple to use but produces consistently better translateddata. Given its good performance on the SQuAD dataset, it is likely the methodcan be used to translate other similar span-annotated datasets for other tasksand languages as well. All code and data is available under an open license:data at HuggingFace TurkuNLP/squad_v2_fi, code on GitHub TurkuNLP/squad2-fi,and model at HuggingFace TurkuNLP/bert-base-finnish-cased-squad2.</description><author>Emil Nuutinen, Iiro Rastas, Filip Ginter</author><pubDate>Fri, 10 Jan 2025 13:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05963v1</guid></item></channel></rss>