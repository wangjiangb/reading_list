<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 03 Dec 2023 14:00:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Just Add $π$! Pose Induced Video Transformers for Understanding Activities of Daily Living</title><link>http://arxiv.org/abs/2311.18840v1</link><description>Video transformers have become the de facto standard for human actionrecognition, yet their exclusive reliance on the RGB modality still limitstheir adoption in certain domains. One such domain is Activities of DailyLiving (ADL), where RGB alone is not sufficient to distinguish between visuallysimilar actions, or actions observed from multiple viewpoints. To facilitatethe adoption of video transformers for ADL, we hypothesize that theaugmentation of RGB with human pose information, known for its sensitivity tofine-grained motion and multiple viewpoints, is essential. Consequently, weintroduce the first Pose Induced Video Transformer: PI-ViT (or $\pi$-ViT), anovel approach that augments the RGB representations learned by videotransformers with 2D and 3D pose information. The key elements of $\pi$-ViT aretwo plug-in modules, 2D Skeleton Induction Module and 3D Skeleton InductionModule, that are responsible for inducing 2D and 3D pose information into theRGB representations. These modules operate by performing pose-aware auxiliarytasks, a design choice that allows $\pi$-ViT to discard the modules duringinference. Notably, $\pi$-ViT achieves the state-of-the-art performance onthree prominent ADL datasets, encompassing both real-world and large-scaleRGB-D datasets, without requiring poses or additional computational overhead atinference.</description><author>Dominick Reilly, Srijan Das</author><pubDate>Thu, 30 Nov 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18840v1</guid></item><item><title>TrafficMOT: A Challenging Dataset for Multi-Object Tracking in Complex Traffic Scenarios</title><link>http://arxiv.org/abs/2311.18839v1</link><description>Multi-object tracking in traffic videos is a crucial research area, offeringimmense potential for enhancing traffic monitoring accuracy and promoting roadsafety measures through the utilisation of advanced machine learningalgorithms. However, existing datasets for multi-object tracking in trafficvideos often feature limited instances or focus on single classes, which cannotwell simulate the challenges encountered in complex traffic scenarios. Toaddress this gap, we introduce TrafficMOT, an extensive dataset designed toencompass diverse traffic situations with complex scenarios. To validate thecomplexity and challenges presented by TrafficMOT, we conducted comprehensiveempirical studies using three different settings: fully-supervised,semi-supervised, and a recent powerful zero-shot foundation model TrackingAnything Model (TAM). The experimental results highlight the inherentcomplexity of this dataset, emphasising its value in driving advancements inthe field of traffic monitoring and multi-object tracking.</description><author>Lihao Liu, Yanqi Cheng, Zhongying Deng, Shujun Wang, Dongdong Chen, Xiaowei Hu, Pietro Liò, Carola-Bibiane Schönlieb, Angelica Aviles-Rivero</author><pubDate>Thu, 30 Nov 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18839v1</guid></item><item><title>Dataset Distillation in Large Data Era</title><link>http://arxiv.org/abs/2311.18838v1</link><description>Dataset distillation aims to generate a smaller but representative subsetfrom a large dataset, which allows a model to be trained efficiently, meanwhileevaluating on the original testing data distribution to achieve decentperformance. Many prior works have aimed to align with diverse aspects of theoriginal datasets, such as matching the training weight trajectories, gradient,feature/BatchNorm distributions, etc. In this work, we show how to distillvarious large-scale datasets such as full ImageNet-1K/21K under a conventionalinput resolution of 224$\times$224 to achieve the best accuracy over allprevious approaches, including SRe$^2$L, TESLA and MTT. To achieve this, weintroduce a simple yet effective ${\bf C}$urriculum ${\bf D}$ata ${\bfA}$ugmentation ($\texttt{CDA}$) during data synthesis that obtains the accuracyon large-scale ImageNet-1K and 21K with 63.2% under IPC (Images Per Class) 50and 36.1% under IPC 20, respectively. Finally, we show that, by integrating allour enhancements together, the proposed model beats the currentstate-of-the-art by more than 4% Top-1 accuracy on ImageNet-1K/21K and for thefirst time, reduces the gap to its full-data training counterpart to less thanabsolute 15%. Moreover, this work represents the inaugural success in datasetdistillation on larger-scale ImageNet-21K under the standard 224$\times$224resolution. Our code and distilled ImageNet-21K dataset of 20 IPC, 2K recoverybudget are available at https://github.com/VILA-Lab/SRe2L/tree/main/CDA.</description><author>Zeyuan Yin, Zhiqiang Shen</author><pubDate>Thu, 30 Nov 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18838v1</guid></item><item><title>PoseGPT: Chatting about 3D Human Pose</title><link>http://arxiv.org/abs/2311.18836v1</link><description>We introduce PoseGPT, a framework employing Large Language Models (LLMs) tounderstand and reason about 3D human poses from images or textual descriptions.Our work is motivated by the human ability to intuitively understand posturesfrom a single image or a brief description, a process that intertwines imageinterpretation, world knowledge, and an understanding of body language.Traditional human pose estimation methods, whether image-based or text-based,often lack holistic scene comprehension and nuanced reasoning, leading to adisconnect between visual data and its real-world implications. PoseGPTaddresses these limitations by embedding SMPL poses as a distinct signal tokenwithin a multi-modal LLM, enabling direct generation of 3D body poses from bothtextual and visual inputs. This approach not only simplifies pose predictionbut also empowers LLMs to apply their world knowledge in reasoning about humanposes, fostering two advanced tasks: speculative pose generation and reasoningabout pose estimation. These tasks involve reasoning about humans to generate3D poses from subtle text queries, possibly accompanied by images. We establishbenchmarks for these tasks, moving beyond traditional 3D pose generation andestimation methods. Our results show that PoseGPT outperforms existingmultimodal LLMs and task-sepcific methods on these newly proposed tasks.Furthermore, PoseGPT's ability to understand and generate 3D human poses basedon complex reasoning opens new directions in human pose analysis.</description><author>Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Michael J. Black</author><pubDate>Thu, 30 Nov 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18836v1</guid></item><item><title>VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models</title><link>http://arxiv.org/abs/2311.18837v1</link><description>Diffusion models have achieved significant success in image and videogeneration. This motivates a growing interest in video editing tasks, wherevideos are edited according to provided text descriptions. However, mostexisting approaches only focus on video editing for short clips and rely ontime-consuming tuning or inference. We are the first to propose VideoInstruction Diffusion (VIDiff), a unified foundation model designed for a widerange of video tasks. These tasks encompass both understanding tasks (such aslanguage-guided video object segmentation) and generative tasks (video editingand enhancement). Our model can edit and translate the desired results withinseconds based on user instructions. Moreover, we design an iterativeauto-regressive method to ensure consistency in editing and enhancing longvideos. We provide convincing generative results for diverse input videos andwritten instructions, both qualitatively and quantitatively. More examples canbe found at our website https://ChenHsing.github.io/VIDiff.</description><author>Zhen Xing, Qi Dai, Zihao Zhang, Hui Zhang, Han Hu, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Thu, 30 Nov 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18837v1</guid></item><item><title>InstructSeq: Unifying Vision Tasks with Instruction-conditioned Multi-modal Sequence Generation</title><link>http://arxiv.org/abs/2311.18835v1</link><description>Empowering models to dynamically accomplish tasks specified through naturallanguage instructions represents a promising path toward more capable andgeneral artificial intelligence. In this work, we introduce InstructSeq, aninstruction-conditioned multi-modal modeling framework that unifies diversevision tasks through flexible natural language control and handling of bothvisual and textual data. InstructSeq employs a multimodal transformerarchitecture encompassing visual, language, and sequential modeling. We utilizea visual encoder to extract image features and a text encoder to encodeinstructions. An autoregressive transformer fuses the representations andgenerates sequential task outputs. By training with LLM-generated naturallanguage instructions, InstructSeq acquires a strong comprehension of free-forminstructions for specifying visual tasks. This provides an intuitive interfacefor directing capabilities using flexible natural instructions. Without anytask-specific tuning, InstructSeq achieves compelling performance on semanticsegmentation, referring expression segmentation/comprehension, and imagecaptioning. The flexible control and multi-task unification empower the modelwith more human-like versatility and generalizability for computer vision. Thecode will be released soon at https://github.com/rongyaofang/InstructSeq.</description><author>Rongyao Fang, Shilin Yan, Zhaoyang Huang, Jingqiu Zhou, Hao Tian, Jifeng Dai, Hongsheng Li</author><pubDate>Thu, 30 Nov 2023 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18835v1</guid></item><item><title>ART$\boldsymbol{\cdot}$V: Auto-Regressive Text-to-Video Generation with Diffusion Models</title><link>http://arxiv.org/abs/2311.18834v1</link><description>We present ART$\boldsymbol{\cdot}$V, an efficient framework forauto-regressive video generation with diffusion models. Unlike existing methodsthat generate entire videos in one-shot, ART$\boldsymbol{\cdot}$V generates asingle frame at a time, conditioned on the previous ones. The framework offersthree distinct advantages. First, it only learns simple continual motionsbetween adjacent frames, therefore avoiding modeling complex long-range motionsthat require huge training data. Second, it preserves the high-fidelitygeneration ability of the pre-trained image diffusion models by making onlyminimal network modifications. Third, it can generate arbitrarily long videosconditioned on a variety of prompts such as text, image or their combinations,making it highly versatile and flexible. To combat the common drifting issue inAR models, we propose masked diffusion model which implicitly learns whichinformation can be drawn from reference images rather than network predictions,in order to reduce the risk of generating inconsistent appearances that causedrifting. Moreover, we further enhance generation coherence by conditioning iton the initial frame, which typically contains minimal noise. This isparticularly useful for long video generation. When trained for only two weekson four GPUs, ART$\boldsymbol{\cdot}$V already can generate videos with naturalmotions, rich details and a high level of aesthetic quality. Besides, itenables various appealing applications, e.g., composing a long video frommultiple text prompts.</description><author>Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu, Jianmin Bao, Yuhui Yuan, Chong Luo, Yueyi Zhang, Zhiwei Xiong</author><pubDate>Thu, 30 Nov 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18834v1</guid></item><item><title>Exploiting Diffusion Prior for Generalizable Pixel-Level Semantic Prediction</title><link>http://arxiv.org/abs/2311.18832v1</link><description>Contents generated by recent advanced Text-to-Image (T2I) diffusion modelsare sometimes too imaginative for existing off-the-shelf property semanticpredictors to estimate due to the immitigable domain gap. We introduce DMP, apipeline utilizing pre-trained T2I models as a prior for pixel-level semanticprediction tasks. To address the misalignment between deterministic predictiontasks and stochastic T2I models, we reformulate the diffusion process through asequence of interpolations, establishing a deterministic mapping between inputRGB images and output prediction distributions. To preserve generalizability,we use low-rank adaptation to fine-tune pre-trained models. Extensiveexperiments across five tasks, including 3D property estimation, semanticsegmentation, and intrinsic image decomposition, showcase the efficacy of theproposed method. Despite limited-domain training data, the approach yieldsfaithful estimations for arbitrary images, surpassing existing state-of-the-artalgorithms.</description><author>Hsin-Ying Lee, Hung-Yu Tseng, Hsin-Ying Lee, Ming-Hsuan Yang</author><pubDate>Thu, 30 Nov 2023 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18832v1</guid></item><item><title>MotionEditor: Editing Video Motion via Content-Aware Diffusion</title><link>http://arxiv.org/abs/2311.18830v1</link><description>Existing diffusion-based video editing models have made gorgeous advances forediting attributes of a source video over time but struggle to manipulate themotion information while preserving the original protagonist's appearance andbackground. To address this, we propose MotionEditor, a diffusion model forvideo motion editing. MotionEditor incorporates a novel content-aware motionadapter into ControlNet to capture temporal motion correspondence. WhileControlNet enables direct generation based on skeleton poses, it encounterschallenges when modifying the source motion in the inverted noise due tocontradictory signals between the noise (source) and the condition (reference).Our adapter complements ControlNet by involving source content to transferadapted control signals seamlessly. Further, we build up a two-brancharchitecture (a reconstruction branch and an editing branch) with ahigh-fidelity attention injection mechanism facilitating branch interaction.This mechanism enables the editing branch to query the key and value from thereconstruction branch in a decoupled manner, making the editing branch retainthe original background and protagonist appearance. We also propose a skeletonalignment algorithm to address the discrepancies in pose size and position.Experiments demonstrate the promising motion editing ability of MotionEditor,both qualitatively and quantitatively.</description><author>Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Thu, 30 Nov 2023 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18830v1</guid></item><item><title>MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation</title><link>http://arxiv.org/abs/2311.18829v1</link><description>We present MicroCinema, a straightforward yet effective framework forhigh-quality and coherent text-to-video generation. Unlike existing approachesthat align text prompts with video directly, MicroCinema introduces aDivide-and-Conquer strategy which divides the text-to-video into a two-stageprocess: text-to-image generation and image\&amp;text-to-video generation. Thisstrategy offers two significant advantages. a) It allows us to take fulladvantage of the recent advances in text-to-image models, such as StableDiffusion, Midjourney, and DALLE, to generate photorealistic and highlydetailed images. b) Leveraging the generated image, the model can allocate lessfocus to fine-grained appearance details, prioritizing the efficient learningof motion dynamics. To implement this strategy effectively, we introduce twocore designs. First, we propose the Appearance Injection Network, enhancing thepreservation of the appearance of the given image. Second, we introduce theAppearance Noise Prior, a novel mechanism aimed at maintaining the capabilitiesof pre-trained 2D diffusion models. These design elements empower MicroCinemato generate high-quality videos with precise motion, guided by the providedtext prompts. Extensive experiments demonstrate the superiority of the proposedframework. Concretely, MicroCinema achieves SOTA zero-shot FVD of 342.86 onUCF-101 and 377.40 on MSR-VTT. Seehttps://wangyanhui666.github.io/MicroCinema.github.io/ for video samples.</description><author>Yanhui Wang, Jianmin Bao, Wenming Weng, Ruoyu Feng, Dacheng Yin, Tao Yang, Jingxu Zhang, Qi Dai Zhiyuan Zhao, Chunyu Wang, Kai Qiu, Yuhui Yuan, Xiaoyan Sun, Chong Luo, Baining Guo</author><pubDate>Thu, 30 Nov 2023 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18829v1</guid></item><item><title>One-step Diffusion with Distribution Matching Distillation</title><link>http://arxiv.org/abs/2311.18828v1</link><description>Diffusion models generate high-quality images but require dozens of forwardpasses. We introduce Distribution Matching Distillation (DMD), a procedure totransform a diffusion model into a one-step image generator with minimal impacton image quality. We enforce the one-step image generator match the diffusionmodel at distribution level, by minimizing an approximate KL divergence whosegradient can be expressed as the difference between 2 score functions, one ofthe target distribution and the other of the synthetic distribution beingproduced by our one-step generator. The score functions are parameterized astwo diffusion models trained separately on each distribution. Combined with asimple regression loss matching the large-scale structure of the multi-stepdiffusion outputs, our method outperforms all published few-step diffusionapproaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shotCOCO-30k, comparable to Stable Diffusion but orders of magnitude faster.Utilizing FP16 inference, our model can generate images at 20 FPS on modernhardware.</description><author>Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Frédo Durand, William T. Freeman, Taesung Park</author><pubDate>Thu, 30 Nov 2023 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18828v1</guid></item><item><title>Motion-Conditioned Image Animation for Video Editing</title><link>http://arxiv.org/abs/2311.18827v1</link><description>We introduce MoCA, a Motion-Conditioned Image Animation approach for videoediting. It leverages a simple decomposition of the video editing problem intoimage editing followed by motion-conditioned image animation. Furthermore,given the lack of robust evaluation datasets for video editing, we introduce anew benchmark that measures edit capability across a wide variety of tasks,such as object replacement, background changes, style changes, and motionedits. We present a comprehensive human evaluation of the latest video editingmethods along with MoCA, on our proposed benchmark. MoCA establishes a newstate-of-the-art, demonstrating greater human preference win-rate, andoutperforming notable recent approaches including Dreamix (63%), MasaCtrl(75%), and Tune-A-Video (72%), with especially significant improvements formotion edits.</description><author>Wilson Yan, Andrew Brown, Pieter Abbeel, Rohit Girdhar, Samaneh Azadi</author><pubDate>Thu, 30 Nov 2023 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18827v1</guid></item><item><title>Geometry-Aware Normalizing Wasserstein Flows for Optimal Causal Inference</title><link>http://arxiv.org/abs/2311.18826v1</link><description>This manuscript enriches the framework of continuous normalizing flows (CNFs)within causal inference, primarily to augment the geometric properties ofparametric submodels used in targeted maximum likelihood estimation (TMLE). Byintroducing an innovative application of CNFs, we construct a refined series ofparametric submodels that enable a directed interpolation between the priordistribution $p_0$ and the empirical distribution $p_1$. This proposedmethodology serves to optimize the semiparametric efficiency bound in causalinference by orchestrating CNFs to align with Wasserstein gradient flows. Ourapproach not only endeavors to minimize the mean squared error in theestimation but also imbues the estimators with geometric sophistication,thereby enhancing robustness against misspecification. This robustness iscrucial, as it alleviates the dependence on the standard $n^{\frac{1}{4}}$ ratefor a doubly-robust perturbation direction in TMLE. By incorporating robustoptimization principles and differential geometry into the estimators, thedeveloped geometry-aware CNFs represent a significant advancement in thepursuit of doubly robust causal inference.</description><author>Kaiwen Hou</author><pubDate>Thu, 30 Nov 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18826v1</guid></item><item><title>Knowledge Pursuit Prompting for Zero-Shot Multimodal Synthesis</title><link>http://arxiv.org/abs/2311.17898v2</link><description>Hallucinations and unfaithful synthesis due to inaccurate prompts withinsufficient semantic details are widely observed in multimodal generativemodels. A prevalent strategy to align multiple modalities is to fine-tune thegenerator with a large number of annotated text-image pairs. However, such aprocedure is labor-consuming and resource-draining. The key question we ask is:can we enhance the quality and faithfulness of text-driven generative modelsbeyond extensive text-image pair annotations? To address this question, wepropose Knowledge Pursuit Prompting (KPP), a zero-shot framework thatiteratively incorporates external knowledge to help generators produce reliablevisual content. Instead of training generators to handle generic prompts, KPPemploys a recursive knowledge query process to gather informative externalfacts from the knowledge base, instructs a language model to compress theacquired knowledge for prompt refinement, and utilizes text-driven generatorsfor visual synthesis. The entire process is zero-shot, without accessing thearchitectures and parameters of generative models. We evaluate the frameworkacross multiple text-driven generative tasks (image, 3D rendering, and video)on datasets of different domains. We further demonstrate the extensibility andadaptability of KPP through varying foundation model bases and instructions.Our results show that KPP is capable of generating faithful and semanticallyrich content across diverse visual domains, offering a promising solution toimprove multimodal generative models.</description><author>Jinqi Luo, Kwan Ho Ryan Chan, Dimitris Dimos, René Vidal</author><pubDate>Thu, 30 Nov 2023 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17898v2</guid></item><item><title>CAST: Cross-Attention in Space and Time for Video Action Recognition</title><link>http://arxiv.org/abs/2311.18825v1</link><description>Recognizing human actions in videos requires spatial and temporalunderstanding. Most existing action recognition models lack a balancedspatio-temporal understanding of videos. In this work, we propose a noveltwo-stream architecture, called Cross-Attention in Space and Time (CAST), thatachieves a balanced spatio-temporal understanding of videos using only RGBinput. Our proposed bottleneck cross-attention mechanism enables the spatialand temporal expert models to exchange information and make synergisticpredictions, leading to improved performance. We validate the proposed methodwith extensive experiments on public benchmarks with different characteristics:EPIC-KITCHENS-100, Something-Something-V2, and Kinetics-400. Our methodconsistently shows favorable performance across these datasets, while theperformance of existing methods fluctuates depending on the datasetcharacteristics.</description><author>Dongho Lee, Jongseo Lee, Jinwoo Choi</author><pubDate>Thu, 30 Nov 2023 18:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18825v1</guid></item><item><title>An Adaptive Framework for Generalizing Network Traffic Prediction towards Uncertain Environments</title><link>http://arxiv.org/abs/2311.18824v1</link><description>We have developed a new framework using time-series analysis for dynamicallyassigning mobile network traffic prediction models in previously unseenwireless environments. Our framework selectively employs learned behaviors,outperforming any single model with over a 50% improvement relative to currentstudies. More importantly, it surpasses traditional approaches without needingprior knowledge of a cell. While this paper focuses on network trafficprediction using our adaptive forecasting framework, this framework can also beapplied to other machine learning applications in uncertain environments. The framework begins with unsupervised clustering of time-series data toidentify unique trends and seasonal patterns. Subsequently, we apply supervisedlearning for traffic volume prediction within each cluster. This specializationtowards specific traffic behaviors occurs without penalties from spatial andtemporal variations. Finally, the framework adaptively assigns trained modelsto new, previously unseen cells. By analyzing real-time measurements of a cell,our framework intelligently selects the most suitable cluster for that cell atany given time, with cluster assignment dynamically adjusting tospatio-temporal fluctuations.</description><author>Alexander Downey, Evren Tuna, Alkan Soysal</author><pubDate>Thu, 30 Nov 2023 18:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18824v1</guid></item><item><title>Initializing Models with Larger Ones</title><link>http://arxiv.org/abs/2311.18823v1</link><description>Weight initialization plays an important role in neural network training.Widely used initialization methods are proposed and evaluated for networks thatare trained from scratch. However, the growing number of pretrained models nowoffers new opportunities for tackling this classical problem of weightinitialization. In this work, we introduce weight selection, a method forinitializing smaller models by selecting a subset of weights from a pretrainedlarger model. This enables the transfer of knowledge from pretrained weights tosmaller models. Our experiments demonstrate that weight selection cansignificantly enhance the performance of small models and reduce their trainingtime. Notably, it can also be used together with knowledge distillation. Weightselection offers a new approach to leverage the power of pretrained models inresource-constrained settings, and we hope it can be a useful tool for trainingsmall models in the large-model era. Code is available athttps://github.com/OscarXZQ/weight-selection.</description><author>Zhiqiu Xu, Yanjie Chen, Kirill Vishniakov, Yida Yin, Zhiqiang Shen, Trevor Darrell, Lingjie Liu, Zhuang Liu</author><pubDate>Thu, 30 Nov 2023 18:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18823v1</guid></item><item><title>ElasticDiffusion: Training-free Arbitrary Size Image Generation</title><link>http://arxiv.org/abs/2311.18822v1</link><description>Diffusion models have revolutionized image generation in recent years, yetthey are still limited to a few sizes and aspect ratios. We proposeElasticDiffusion, a novel training-free decoding method that enables pretrainedtext-to-image diffusion models to generate images with various sizes.ElasticDiffusion attempts to decouple the generation trajectory of a pretrainedmodel into local and global signals. The local signal controls low-level pixelinformation and can be estimated on local patches, while the global signal isused to maintain overall structural consistency and is estimated with areference image. We test our method on CelebA-HQ (faces) and LAION-COCO(objects/indoor/outdoor scenes). Our experiments and qualitative results showsuperior image coherence quality across aspect ratios compared toMultiDiffusion and the standard decoding strategy of Stable Diffusion. Code:https://github.com/MoayedHajiAli/ElasticDiffusion-official.git</description><author>Moayed Haji-Ali, Guha Balakrishnan, Vicente Ordonez</author><pubDate>Thu, 30 Nov 2023 18:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18822v1</guid></item><item><title>Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs</title><link>http://arxiv.org/abs/2311.14656v2</link><description>Multimodal large language models (MLLMs) have shown remarkable capabilitiesacross a broad range of tasks but their knowledge and abilities in thegeographic and geospatial domains are yet to be explored, despite potentialwide-ranging benefits to navigation, environmental research, urban development,and disaster response. We conduct a series of experiments exploring variousvision capabilities of MLLMs within these domains, particularly focusing on thefrontier model GPT-4V, and benchmark its performance against open-sourcecounterparts. Our methodology involves challenging these models with asmall-scale geographic benchmark consisting of a suite of visual tasks, testingtheir abilities across a spectrum of complexity. The analysis uncovers not onlywhere such models excel, including instances where they outperform humans, butalso where they falter, providing a balanced view of their capabilities in thegeographic domain. To enable the comparison and evaluation of future models,our benchmark will be publicly released.</description><author>Jonathan Roberts, Timo Lüddecke, Rehan Sheikh, Kai Han, Samuel Albanie</author><pubDate>Thu, 30 Nov 2023 18:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14656v2</guid></item><item><title>Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking</title><link>http://arxiv.org/abs/2311.18817v1</link><description>Recent work by Power et al. (2022) highlighted a surprising "grokking"phenomenon in learning arithmetic tasks: a neural net first "memorizes" thetraining set, resulting in perfect training accuracy but near-random testaccuracy, and after training for sufficiently longer, it suddenly transitionsto perfect test accuracy. This paper studies the grokking phenomenon intheoretical setups and shows that it can be induced by a dichotomy of early andlate phase implicit biases. Specifically, when training homogeneous neural netswith large initialization and small weight decay on both classification andregression tasks, we prove that the training process gets trapped at a solutioncorresponding to a kernel predictor for a long time, and then a very sharptransition to min-norm/max-margin predictors occurs, leading to a dramaticchange in test accuracy.</description><author>Kaifeng Lyu, Jikai Jin, Zhiyuan Li, Simon S. Du, Jason D. Lee, Wei Hu</author><pubDate>Thu, 30 Nov 2023 18:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18817v1</guid></item><item><title>IMMA: Immunizing text-to-image Models against Malicious Adaptation</title><link>http://arxiv.org/abs/2311.18815v1</link><description>Advancements in text-to-image models and fine-tuning methods have led to theincreasing risk of malicious adaptation, i.e., fine-tuning to generate harmfulunauthorized content. Recent works, e.g., Glaze or MIST, have developeddata-poisoning techniques which protect the data against adaptation methods. Inthis work, we consider an alternative paradigm for protection. We propose to``immunize'' the model by learning model parameters that are difficult for theadaptation methods when fine-tuning malicious content; in short IMMA. Empiricalresults show IMMA's effectiveness against malicious adaptations, includingmimicking the artistic style and learning of inappropriate/unauthorizedcontent, over three adaptation methods: LoRA, Textual-Inversion, andDreamBooth.</description><author>Yijia Zheng, Raymond A. Yeh</author><pubDate>Thu, 30 Nov 2023 18:55:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18815v1</guid></item><item><title>Is Underwater Image Enhancement All Object Detectors Need?</title><link>http://arxiv.org/abs/2311.18814v1</link><description>Underwater object detection is a crucial and challenging problem in marineengineering and aquatic robot. The difficulty is partly because of thedegradation of underwater images caused by light selective absorption andscattering. Intuitively, enhancing underwater images can benefit high-levelapplications like underwater object detection. However, it is still unclearwhether all object detectors need underwater image enhancement aspre-processing. We therefore pose the questions "Does underwater imageenhancement really improve underwater object detection?" and "How doesunderwater image enhancement contribute to underwater object detection?". Withthese two questions, we conduct extensive studies. Specifically, we use 18state-of-the-art underwater image enhancement algorithms, covering traditional,CNN-based, and GAN-based algorithms, to pre-process underwater object detectiondata. Then, we retrain 7 popular deep learning-based object detectors using thecorresponding results enhanced by different algorithms, obtaining 126underwater object detection models. Coupled with 7 object detection modelsretrained using raw underwater images, we employ these 133 models tocomprehensively analyze the effect of underwater image enhancement onunderwater object detection. We expect this study can provide sufficientexploration to answer the aforementioned questions and draw more attention ofthe community to the joint problem of underwater image enhancement andunderwater object detection. The pre-trained models and results are publiclyavailable and will be regularly updated. Project page:https://github.com/BIGWangYuDong/lqit/tree/main/configs/detection/uw_enhancement_affect_detection.</description><author>Yudong Wang, Jichang Guo, Wanru He, Huan Gao, Huihui Yue, Zenan Zhang, Chongyi Li</author><pubDate>Thu, 30 Nov 2023 18:54:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18814v1</guid></item><item><title>What Do Llamas Really Think? Revealing Preference Biases in Language Model Representations</title><link>http://arxiv.org/abs/2311.18812v1</link><description>Do large language models (LLMs) exhibit sociodemographic biases, even whenthey decline to respond? To bypass their refusal to "speak," we study thisresearch question by probing contextualized embeddings and exploring whetherthis bias is encoded in its latent representations. We propose a logisticBradley-Terry probe which predicts word pair preferences of LLMs from thewords' hidden vectors. We first validate our probe on three pair preferencetasks and thirteen LLMs, where we outperform the word embedding associationtest (WEAT), a standard approach in testing for implicit association, by arelative 27% in error rate. We also find that word pair preferences are bestrepresented in the middle layers. Next, we transfer probes trained on harmlesstasks (e.g., pick the larger number) to controversial ones (compareethnicities) to examine biases in nationality, politics, religion, and gender.We observe substantial bias for all target classes: for instance, the Mistralmodel implicitly prefers Europe to Africa, Christianity to Judaism, andleft-wing to right-wing politics, despite declining to answer. This suggeststhat instruction fine-tuning does not necessarily debias contextualizedembeddings. Our codebase is at https://github.com/castorini/biasprobe.</description><author>Raphael Tang, Xinyu Zhang, Jimmy Lin, Ferhan Ture</author><pubDate>Thu, 30 Nov 2023 18:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18812v1</guid></item><item><title>Convergence of Nonconvex PnP-ADMM with MMSE Denoisers</title><link>http://arxiv.org/abs/2311.18810v1</link><description>Plug-and-Play Alternating Direction Method of Multipliers (PnP-ADMM) is awidely-used algorithm for solving inverse problems by integrating physicalmeasurement models and convolutional neural network (CNN) priors. PnP-ADMM hasbeen theoretically proven to converge for convex data-fidelity terms andnonexpansive CNNs. It has however been observed that PnP-ADMM often empiricallyconverges even for expansive CNNs. This paper presents a theoreticalexplanation for the observed stability of PnP-ADMM based on the interpretationof the CNN prior as a minimum mean-squared error (MMSE) denoiser. Ourexplanation parallels a similar argument recently made for the iterativeshrinkage/thresholding algorithm variant of PnP (PnP-ISTA) and relies on theconnection between MMSE denoisers and proximal operators. We also numericallyevaluate the performance gap between PnP-ADMM using a nonexpansive DnCNNdenoiser and expansive DRUNet denoiser, thus motivating the use of expansiveCNNs.</description><author>Chicago Park, Shirin Shoushtari, Weijie Gan, Ulugbek S. Kamilov</author><pubDate>Thu, 30 Nov 2023 18:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18810v1</guid></item><item><title>FoundPose: Unseen Object Pose Estimation with Foundation Features</title><link>http://arxiv.org/abs/2311.18809v1</link><description>We propose FoundPose, a method for 6D pose estimation of unseen rigid objectsfrom a single RGB image. The method assumes that 3D models of the objects areavailable but does not require any object-specific training. This is achievedby building upon DINOv2, a recent vision foundation model with impressivegeneralization capabilities. An online pose estimation stage is supported by aminimal object representation that is built during a short onboarding stagefrom DINOv2 patch features extracted from rendered object templates. Given aquery image with an object segmentation mask, FoundPose first rapidly retrievesa handful of similarly looking templates by a DINOv2-based bag-of-wordsapproach. Pose hypotheses are then generated from 2D-3D correspondencesestablished by matching DINOv2 patch features between the query image and aretrieved template, and finally optimized by featuremetric refinement. Themethod can handle diverse objects, including challenging ones with symmetriesand without any texture, and noticeably outperforms existing RGB methods forcoarse pose estimation in both accuracy and speed on the standard BOPbenchmark. With the featuremetric and additional MegaPose refinement, which aredemonstrated complementary, the method outperforms all RGB competitors. Sourcecode is at: evinpinar.github.io/foundpose.</description><author>Evin Pınar Örnek, Yann Labbé, Bugra Tekin, Lingni Ma, Cem Keskin, Christian Forster, Tomas Hodan</author><pubDate>Thu, 30 Nov 2023 18:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18809v1</guid></item><item><title>Pre-registration for Predictive Modeling</title><link>http://arxiv.org/abs/2311.18807v1</link><description>Amid rising concerns of reproducibility and generalizability in predictivemodeling, we explore the possibility and potential benefits of introducingpre-registration to the field. Despite notable advancements in predictivemodeling, spanning core machine learning tasks to various scientificapplications, challenges such as overlooked contextual factors, data-dependentdecision-making, and unintentional re-use of test data have raised questionsabout the integrity of results. To address these issues, we propose adaptingpre-registration practices from explanatory modeling to predictive modeling. Wediscuss current best practices in predictive modeling and their limitations,introduce a lightweight pre-registration template, and present a qualitativestudy with machine learning researchers to gain insight into the effectivenessof pre-registration in preventing biased estimates and promoting more reliableresearch outcomes. We conclude by exploring the scope of problems thatpre-registration can address in predictive modeling and acknowledging itslimitations within this context.</description><author>Jake M. Hofman, Angelos Chatzimparmpas, Amit Sharma, Duncan J. Watts, Jessica Hullman</author><pubDate>Thu, 30 Nov 2023 18:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18807v1</guid></item><item><title>Efficient Baseline for Quantitative Precipitation Forecasting in Weather4cast 2023</title><link>http://arxiv.org/abs/2311.18806v1</link><description>Accurate precipitation forecasting is indispensable for informeddecision-making across various industries. However, the computational demandsof current models raise environmental concerns. We address the critical needfor accurate precipitation forecasting while considering the environmentalimpact of computational resources and propose a minimalist U-Net architectureto be used as a baseline for future weather forecasting initiatives.</description><author>Akshay Punjabi, Pablo Izquierdo Ayala</author><pubDate>Thu, 30 Nov 2023 18:51:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18806v1</guid></item><item><title>Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text</title><link>http://arxiv.org/abs/2311.18805v1</link><description>While Large Language Models (LLMs) have achieved remarkable performance inmany tasks, much about their inner workings remains unclear. In this study, wepresent novel experimental insights into the resilience of LLMs, particularlyGPT-4, when subjected to extensive character-level permutations. To investigatethis, we first propose the Scrambled Bench, a suite designed to measure thecapacity of LLMs to handle scrambled input, in terms of both recoveringscrambled sentences and answering questions given scrambled context. Theexperimental results indicate that most powerful LLMs demonstrate thecapability akin to typoglycemia, a phenomenon where humans can understand themeaning of words even when the letters within those words are scrambled, aslong as the first and last letters remain in place. More surprisingly, we foundthat only GPT-4 nearly flawlessly processes inputs with unnatural errors, evenunder the extreme condition, a task that poses significant challenges for otherLLMs and often even for humans. Specifically, GPT-4 can almost perfectlyreconstruct the original sentences from scrambled ones, decreasing the editdistance by 95%, even when all letters within each word are entirely scrambled.It is counter-intuitive that LLMs can exhibit such resilience despite severedisruption to input tokenization caused by scrambled text.</description><author>Qi Cao, Takeshi Kojima, Yutaka Matsuo, Yusuke Iwasawa</author><pubDate>Thu, 30 Nov 2023 18:51:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18805v1</guid></item><item><title>BIOCLIP: A Vision Foundation Model for the Tree of Life</title><link>http://arxiv.org/abs/2311.18803v1</link><description>Images of the natural world, collected by a variety of cameras, from dronesto individual phones, are increasingly abundant sources of biologicalinformation. There is an explosion of computational methods and tools,particularly computer vision, for extracting biologically relevant informationfrom images for science and conservation. Yet most of these are bespokeapproaches designed for a specific task and are not easily adaptable orextendable to new questions, contexts, and datasets. A vision model for generalorganismal biology questions on images is of timely need. To approach this, wecurate and release TreeOfLife-10M, the largest and most diverse ML-readydataset of biology images. We then develop BioCLIP, a foundation model for thetree of life, leveraging the unique properties of biology captured byTreeOfLife-10M, namely the abundance and variety of images of plants, animals,and fungi, together with the availability of rich structured biologicalknowledge. We rigorously benchmark our approach on diverse fine-grained biologyclassification tasks, and find that BioCLIP consistently and substantiallyoutperforms existing baselines (by 17% to 20% absolute). Intrinsic evaluationreveals that BioCLIP has learned a hierarchical representation conforming tothe tree of life, shedding light on its strong generalizability. Our code,models and data will be made available athttps://github.com/Imageomics/bioclip.</description><author>Samuel Stevens, Jiaman Wu, Matthew J Thompson, Elizabeth G Campolongo, Chan Hee Song, David Edward Carlyn, Li Dong, Wasila M Dahdul, Charles Stewart, Tanya Berger-Wolf, Wei-Lun Chao, Yu Su</author><pubDate>Thu, 30 Nov 2023 18:49:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18803v1</guid></item><item><title>A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift</title><link>http://arxiv.org/abs/2311.14743v3</link><description>Foundation models, specifically Large Language Models (LLM's), have latelygained wide-spread attention and adoption. Reinforcement Learning with HumanFeedback (RLHF) involves training a reward model to capture desired behaviors,which is then used to align an LLM. These reward models are additionally usedat inference-time to estimate how well LLM responses adhere to those desiredbehaviors. However, there is little work measuring how robust these rewardmodels are to distribution shifts. In this work, we evaluate how reward modelperformance - measured via accuracy and calibration (i.e. alignment betweenaccuracy and confidence) - is affected by distribution shift. We show novelcalibration patterns and accuracy drops due to OOD prompts and responses, andthat the reward model is more sensitive to shifts in responses than prompts.Additionally, we adapt an OOD detection technique commonly used inclassification to the reward model setting in order to detect thesedistribution shifts in prompts and responses.</description><author>Ben Pikus, Will LeVine, Tony Chen, Sean Hendryx</author><pubDate>Thu, 30 Nov 2023 18:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14743v3</guid></item><item><title>Distributed Global Structure-from-Motion with a Deep Front-End</title><link>http://arxiv.org/abs/2311.18801v1</link><description>While initial approaches to Structure-from-Motion (SfM) revolved around bothglobal and incremental methods, most recent applications rely on incrementalsystems to estimate camera poses due to their superior robustness. Though therehas been tremendous progress in SfM `front-ends' powered by deep models learnedfrom data, the state-of-the-art (incremental) SfM pipelines still rely onclassical SIFT features, developed in 2004. In this work, we investigatewhether leveraging the developments in feature extraction and matching helpsglobal SfM perform on par with the SOTA incremental SfM approach (COLMAP). Todo so, we design a modular SfM framework that allows us to easily combinedevelopments in different stages of the SfM pipeline. Our experiments show thatwhile developments in deep-learning based two-view correspondence estimation dotranslate to improvements in point density for scenes reconstructed with globalSfM, none of them outperform SIFT when comparing with incremental SfM resultson a range of datasets. Our SfM system is designed from the ground up toleverage distributed computation, enabling us to parallelize computation onmultiple machines and scale to large scenes.</description><author>Ayush Baid, John Lambert, Travis Driver, Akshay Krishnan, Hayk Stepanyan, Frank Dellaert</author><pubDate>Thu, 30 Nov 2023 18:47:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18801v1</guid></item><item><title>X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning</title><link>http://arxiv.org/abs/2311.18799v1</link><description>Vision-language pre-training and instruction tuning have demonstratedgeneral-purpose capabilities in 2D visual reasoning tasks by aligning visualencoders with state-of-the-art large language models (LLMs). In this paper, weintroduce a simple, yet effective, cross-modality framework built atop frozenLLMs that allows the integration of various modalities without extensivemodality-specific customization. To facilitate instruction-modalityfine-tuning, we collect high-quality instruction tuning data in an automaticand scalable manner, composed of 24K QA samples for audio and 250K QA samplesfor 3D. Leveraging instruction-aware representations, our model performscomparably with leading-edge counterparts without the need of extensivemodality-specific pre-training or customization. Furthermore, our approachdemonstrates cross-modal reasoning abilities across two or more inputmodalities, despite each modality projection being trained individually. Tostudy the model's cross-modal abilities, we contribute a novel DiscriminativeCross-modal Reasoning (DisCRn) evaluation task, comprising 9K audio-video QAsamples and 28K image-3D QA samples that require the model to reasondiscriminatively across disparate input modalities.</description><author>Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongxu Li, Shafiq Joty, Ran Xu, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles</author><pubDate>Thu, 30 Nov 2023 18:43:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18799v1</guid></item><item><title>Unsupervised learning architecture based on neural Darwinism and Hopfield networks recognizes symbols with high accuracy</title><link>http://arxiv.org/abs/2311.18789v1</link><description>This paper introduces a novel unsupervised learning paradigm inspired byGerald Edelman's theory of neuronal group selection ("Neural Darwinism"). Thepresented automaton learns to recognize arbitrary symbols (e.g., letters of analphabet) when they are presented repeatedly, as they are when children learnto read. On a second hierarchical level, the model creates abstract categoriesrepresenting the learnt symbols. The fundamental computational unit are simpleMcCulloch-Pitts neurons arranged into fully-connected groups (Hopfield networkswith randomly initialized weights), which are "selected", in an evolutionarysense, through symbol presentation. The learning process is fully tractable andeasily interpretable for humans, in contrast to most neural networkarchitectures. Computational properties of Hopfield networks enabling patternrecognition are discussed. In simulations, the model achieves high accuracy inlearning the letters of the Latin alphabet, presented as binary patterns on agrid. This paper is a proof of concept with no claims to state-of-the-artperformance in letter recognition, but hopefully inspires new thinking inbio-inspired machine learning.</description><author>Mario Stepanik</author><pubDate>Thu, 30 Nov 2023 18:39:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18789v1</guid></item><item><title>Automated interpretation of congenital heart disease from multi-view echocardiograms</title><link>http://arxiv.org/abs/2311.18788v1</link><description>Congenital heart disease (CHD) is the most common birth defect and theleading cause of neonate death in China. Clinical diagnosis can be based on theselected 2D key-frames from five views. Limited by the availability ofmulti-view data, most methods have to rely on the insufficient single viewanalysis. This study proposes to automatically analyze the multi-viewechocardiograms with a practical end-to-end framework. We collect the five-viewechocardiograms video records of 1308 subjects (including normal controls,ventricular septal defect (VSD) patients and atrial septal defect (ASD)patients) with both disease labels and standard-view key-frame labels.Depthwise separable convolution-based multi-channel networks are adopted tolargely reduce the network parameters. We also approach the imbalanced classproblem by augmenting the positive training samples. Our 2D key-frame model candiagnose CHD or negative samples with an accuracy of 95.4\%, and in negative,VSD or ASD classification with an accuracy of 92.3\%. To further alleviate thework of key-frame selection in real-world implementation, we propose anadaptive soft attention scheme to directly explore the raw video data. Fourkinds of neural aggregation methods are systematically investigated to fuse theinformation of an arbitrary number of frames in a video. Moreover, with a viewdetection module, the system can work without the view records. Our video-basedmodel can diagnose with an accuracy of 93.9\% (binary classification), and92.1\% (3-class classification) in a collected 2D video testing set, which doesnot need key-frame selection and view annotation in testing. The detailedablation study and the interpretability analysis are provided.</description><author>Jing Wang, Xiaofeng Liu, Fangyun Wang, Lin Zheng, Fengqiao Gao, Hanwen Zhang, Xin Zhang, Wanqing Xie, Binbin Wang</author><pubDate>Thu, 30 Nov 2023 18:37:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18788v1</guid></item><item><title>Communication-Efficient Federated Optimization over Semi-Decentralized Networks</title><link>http://arxiv.org/abs/2311.18787v1</link><description>In large-scale federated and decentralized learning, communication efficiencyis one of the most challenging bottlenecks. While gossip communication -- whereagents can exchange information with their connected neighbors -- is morecost-effective than communicating with the remote server, it often requires agreater number of communication rounds, especially for large and sparsenetworks. To tackle the trade-off, we examine the communication efficiencyunder a semi-decentralized communication protocol, in which agents can performboth agent-to-agent and agent-to-server communication in a probabilisticmanner. We design a tailored communication-efficient algorithm oversemi-decentralized networks, referred to as PISCO, which inherits therobustness to data heterogeneity thanks to gradient tracking and allowsmultiple local updates for saving communication. We establish the convergencerate of PISCO for nonconvex problems and show that PISCO enjoys a linearspeedup in terms of the number of agents and local updates. Our numericalresults highlight the superior communication efficiency of PISCO and itsresilience to data heterogeneity and various network topologies.</description><author>He Wang, Yuejie Chi</author><pubDate>Thu, 30 Nov 2023 18:37:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18787v1</guid></item><item><title>ID-Pose: Sparse-view Camera Pose Estimation by Inverting Diffusion Models</title><link>http://arxiv.org/abs/2306.17140v2</link><description>Given sparse views of a 3D object, estimating their camera poses is along-standing and intractable problem. Toward this goal, we consider harnessingthe pre-trained diffusion model of novel views conditioned on viewpoints(Zero-1-to-3). We present ID-Pose which inverses the denoising diffusionprocess to estimate the relative pose given two input images. ID-Pose adds anoise to one image, and predicts the noise conditioned on the other image and ahypothesis of the relative pose. The prediction error is used as theminimization objective to find the optimal pose with the gradient descentmethod. We extend ID-Pose to handle more than two images and estimate each posewith multiple image pairs from triangular relations. ID-Pose requires notraining and generalizes to open-world images. We conduct extensive experimentsusing casually captured photos and rendered images with random viewpoints. Theresults demonstrate that ID-Pose significantly outperforms state-of-the-artmethods.</description><author>Weihao Cheng, Yan-Pei Cao, Ying Shan</author><pubDate>Thu, 30 Nov 2023 18:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17140v2</guid></item><item><title>Learning by Aligning 2D Skeleton Sequences in Time</title><link>http://arxiv.org/abs/2305.19480v4</link><description>This paper presents a self-supervised temporal video alignment frameworkwhich is useful for several fine-grained human activity understandingapplications. In contrast with the state-of-the-art method of CASA, wheresequences of 3D skeleton coordinates are taken directly as input, our key ideais to use sequences of 2D skeleton heatmaps as input. Unlike CASA whichperforms self-attention in the temporal domain only, we feed 2D skeletonheatmaps to a video transformer which performs self-attention both in thespatial and temporal domains for extracting effective spatiotemporal andcontextual features. In addition, we introduce simple heatmap augmentationtechniques based on 2D skeletons for self-supervised learning. Despite the lackof 3D information, our approach achieves not only higher accuracy but alsobetter robustness against missing and noisy keypoints than CASA. Furthermore,extensive evaluations on three public datasets, i.e., Penn Action, IKEA ASM,and H2O, demonstrate that our approach outperforms previous methods indifferent fine-grained human activity understanding tasks. Finally, fusing 2Dskeleton heatmaps with RGB videos yields the state-of-the-art on all metricsand datasets. To our best knowledge, our work is the first to utilize 2Dskeleton heatmap inputs and the first to explore multi-modality fusion fortemporal video alignment.</description><author>Quoc-Huy Tran, Muhammad Ahmed, Murad Popattia, M. Hassan Ahmed, Andrey Konin, M. Zeeshan Zia</author><pubDate>Thu, 30 Nov 2023 18:32:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19480v4</guid></item><item><title>A Fully Unsupervised Instance Segmentation Technique for White Blood Cell Images</title><link>http://arxiv.org/abs/2306.14875v2</link><description>White blood cells, also known as leukocytes are group of heterogeneouslynucleated cells which act as salient immune system cells. These are originatedin the bone marrow and are found in blood, plasma, and lymph tissues.Leukocytes kill the bacteria, virus and other kind of pathogens which invadehuman body through phagocytosis that in turn results immunity. Detection of awhite blood cell count can reveal camouflaged infections and warn doctors aboutchronic medical conditions such as autoimmune diseases, immune deficiencies,and blood disorders. Segmentation plays an important role in identification ofwhite blood cells (WBC) from microscopic image analysis. The goal ofsegmentation in a microscopic image is to divide the image into differentdistinct regions. In our paper, we tried to propose a novel instancesegmentation method for segmenting the WBCs containing both the nucleus and thecytoplasm, from bone marrow images.</description><author>Shrijeet Biswas, Amartya Bhattacharya</author><pubDate>Thu, 30 Nov 2023 18:28:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.14875v2</guid></item><item><title>MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for General Time Series Forecasting</title><link>http://arxiv.org/abs/2311.18780v1</link><description>Transformer-based models have greatly pushed the boundaries of time seriesforecasting recently. Existing methods typically encode time series data into$\textit{patches}$ using one or a fixed set of patch lengths. This, however,could result in a lack of ability to capture the variety of intricate temporaldependencies present in real-world multi-periodic time series. In this paper,we propose MultiResFormer, which dynamically models temporal variations byadaptively choosing optimal patch lengths. Concretely, at the beginning of eachlayer, time series data is encoded into several parallel branches, each using adetected periodicity, before going through the transformer encoder block. Weconduct extensive evaluations on long- and short-term forecasting datasetscomparing MultiResFormer with state-of-the-art baselines. MultiResFormeroutperforms patch-based Transformer baselines on long-term forecasting tasksand also consistently outperforms CNN baselines by a large margin, while usingmuch fewer parameters than these baselines.</description><author>Linfeng Du, Ji Xin, Alex Labach, Saba Zuberi, Maksims Volkovs, Rahul G. Krishnan</author><pubDate>Thu, 30 Nov 2023 18:24:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18780v1</guid></item><item><title>Mavericks at BLP-2023 Task 1: Ensemble-based Approach Using Language Models for Violence Inciting Text Detection</title><link>http://arxiv.org/abs/2311.18778v1</link><description>This paper presents our work for the Violence Inciting Text Detection sharedtask in the First Workshop on Bangla Language Processing. Social media hasaccelerated the propagation of hate and violence-inciting speech in society. Itis essential to develop efficient mechanisms to detect and curb the propagationof such texts. The problem of detecting violence-inciting texts is furtherexacerbated in low-resource settings due to sparse research and less data. Thedata provided in the shared task consists of texts in the Bangla language,where each example is classified into one of the three categories defined basedon the types of violence-inciting texts. We try and evaluate several BERT-basedmodels, and then use an ensemble of the models as our final submission. Oursubmission is ranked 10th in the final leaderboard of the shared task with amacro F1 score of 0.737.</description><author>Saurabh Page, Sudeep Mangalvedhekar, Kshitij Deshpande, Tanmay Chavan, Sheetal Sonawane</author><pubDate>Thu, 30 Nov 2023 18:23:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18778v1</guid></item><item><title>CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation</title><link>http://arxiv.org/abs/2311.18775v1</link><description>We present CoDi-2, a versatile and interactive Multimodal Large LanguageModel (MLLM) that can follow complex multimodal interleaved instructions,conduct in-context learning (ICL), reason, chat, edit, etc., in an any-to-anyinput-output modality paradigm. By aligning modalities with language for bothencoding and generation, CoDi-2 empowers Large Language Models (LLMs) to notonly understand complex modality-interleaved instructions and in-contextexamples, but also autoregressively generate grounded and coherent multimodaloutputs in the continuous feature space. To train CoDi-2, we build alarge-scale generation dataset encompassing in-context multimodal instructionsacross text, vision, and audio. CoDi-2 demonstrates a wide range of zero-shotcapabilities for multimodal generation, such as in-context learning, reasoning,and compositionality of any-to-any modality generation through multi-roundinteractive conversation. CoDi-2 surpasses previous domain-specific models ontasks such as subject-driven image generation, vision transformation, and audioediting. CoDi-2 signifies a substantial breakthrough in developing acomprehensive multimodal foundation model adept at interpreting in-contextlanguage-vision-audio interleaved instructions and producing multimodaloutputs.</description><author>Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, Mohit Bansal</author><pubDate>Thu, 30 Nov 2023 18:21:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18775v1</guid></item><item><title>Spacewalk-18: A Benchmark for Multimodal and Long-form Procedural Video Understanding in Novel Domains</title><link>http://arxiv.org/abs/2311.18773v1</link><description>Learning from videos is an emerging research area that enables robots toacquire skills from human demonstrations, such as procedural videos. To dothis, video-language models must be able to obtain structured understandings,such as the temporal segmentation of a demonstration into sequences of actionsand skills, and to generalize the understandings to novel domains. In pursuitof this goal, we introduce Spacewalk-18, a benchmark containing two tasks: (1)step recognition and (2) intra-video retrieval over a dataset of temporallysegmented and labeled tasks in International Space Station spacewalkrecordings. In tandem, the two tasks quantify a model's ability to make use of:(1) out-of-domain visual information; (2) a high temporal context window; and(3) multimodal (text + video) domains. This departs from existing benchmarksfor procedural video understanding, which typically deal with short contextlengths and can be solved with a single modality. Spacewalk-18, with itsinherent multimodal and long-form complexity, exposes the high difficulty oftask recognition and segmentation. We find that state-of-the-art methodsperform poorly on our benchmark, demonstrating that the goal of generalizableprocedural video understanding models is far out and underscoring the need todevelop new approaches to these tasks. Data, model, and code will be publiclyreleased.</description><author>Rohan Myer Krishnan, Zitian Tang, Zhiqiu Yu, Chen Sun</author><pubDate>Thu, 30 Nov 2023 18:19:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18773v1</guid></item><item><title>TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting</title><link>http://arxiv.org/abs/2307.14680v2</link><description>Time series forecasting lies at the core of important real-world applicationsin many fields of science and engineering. The abundance of large time seriesdatasets that consist of complex patterns and long-term dependencies has led tothe development of various neural network architectures. Graph neural networkapproaches, which jointly learn a graph structure based on the correlation ofraw values of multivariate time series while forecasting, have recently seengreat success. However, such solutions are often costly to train and difficultto scale. In this paper, we propose TimeGNN, a method that learns dynamictemporal graph representations that can capture the evolution of inter-seriespatterns along with the correlations of multiple series. TimeGNN achievesinference times 4 to 80 times faster than other state-of-the-art graph-basedmethods while achieving comparable forecasting performance</description><author>Nancy Xu, Chrysoula Kosma, Michalis Vazirgiannis</author><pubDate>Thu, 30 Nov 2023 18:18:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14680v2</guid></item><item><title>Benchmarking Robustness of Text-Image Composed Retrieval</title><link>http://arxiv.org/abs/2311.14837v2</link><description>Text-image composed retrieval aims to retrieve the target image through thecomposed query, which is specified in the form of an image plus some text thatdescribes desired modifications to the input image. It has recently attractedattention due to its ability to leverage both information-rich images andconcise language to precisely express the requirements for target images.However, the robustness of these approaches against real-world corruptions orfurther text understanding has never been studied. In this paper, we performthe first robustness study and establish three new diversified benchmarks forsystematic analysis of text-image composed retrieval against naturalcorruptions in both vision and text and further probe textural understanding.For natural corruption analysis, we introduce two new large-scale benchmarkdatasets, CIRR-C and FashionIQ-C for testing in open domain and fashion domainrespectively, both of which apply 15 visual corruptions and 7 texturalcorruptions. For textural understanding analysis, we introduce a new diagnosticdataset CIRR-D by expanding the original raw data with synthetic data, whichcontains modified text to better probe textual understanding ability includingnumerical variation, attribute variation, object removal, background variation,and fine-grained evaluation. The code and benchmark datasets are available athttps://github.com/SunTongtongtong/Benchmark-Robustness-Text-Image-Compose-Retrieval.</description><author>Shitong Sun, Jindong Gu, Shaogang Gong</author><pubDate>Thu, 30 Nov 2023 18:14:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14837v2</guid></item><item><title>Locally Differentially Private Document Generation Using Zero Shot Prompting</title><link>http://arxiv.org/abs/2310.16111v2</link><description>Numerous studies have highlighted the privacy risks associated withpretrained large language models. In contrast, our research offers a uniqueperspective by demonstrating that pretrained large language models caneffectively contribute to privacy preservation. We propose a locallydifferentially private mechanism called DP-Prompt, which leverages the power ofpretrained large language models and zero-shot prompting to counter authorde-anonymization attacks while minimizing the impact on downstream utility.When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5),we observe a notable reduction in the success rate of de-anonymization attacks,showing that it surpasses existing approaches by a considerable margin despiteits simpler design. For instance, in the case of the IMDB dataset, DP-Prompt(with ChatGPT) perfectly recovers the clean sentiment F1 score while achievinga 46\% reduction in author identification F1 score against static attackers anda 26\% reduction against adaptive attackers. We conduct extensive experimentsacross six open-source large language models, ranging up to 7 billionparameters, to analyze various effects of the privacy-utility tradeoff.</description><author>Saiteja Utpala, Sara Hooker, Pin Yu Chen</author><pubDate>Thu, 30 Nov 2023 18:13:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16111v2</guid></item><item><title>Online Change Points Detection for Linear Dynamical Systems with Finite Sample Guarantees</title><link>http://arxiv.org/abs/2311.18769v1</link><description>The problem of online change point detection is to detect abrupt changes inproperties of time series, ideally as soon as possible after those changesoccur. Existing work on online change point detection either assumes i.i.ddata, focuses on asymptotic analysis, does not present theoretical guaranteeson the trade-off between detection accuracy and detection delay, or is onlysuitable for detecting single change points. In this work, we study the onlinechange point detection problem for linear dynamical systems with unknowndynamics, where the data exhibits temporal correlations and the system couldhave multiple change points. We develop a data-dependent threshold that can beused in our test that allows one to achieve a pre-specified upper bound on theprobability of making a false alarm. We further provide a finite-sample-basedbound for the probability of detecting a change point. Our bound demonstrateshow parameters used in our algorithm affect the detection probability anddelay, and provides guidance on the minimum required time between changes toguarantee detection.</description><author>Lei Xin, George Chiu, Shreyas Sundaram</author><pubDate>Thu, 30 Nov 2023 18:08:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18769v1</guid></item><item><title>Evaluating the Impact of Flaky Simulators on Testing Autonomous Driving Systems</title><link>http://arxiv.org/abs/2311.18768v1</link><description>Simulators are widely used to test Autonomous Driving Systems (ADS), buttheir potential flakiness can lead to inconsistent test results. We investigatetest flakiness in simulation-based testing of ADS by addressing two keyquestions: (1) How do flaky ADS simulations impact automated testing thatrelies on randomized algorithms? and (2) Can machine learning (ML) effectivelyidentify flaky ADS tests while decreasing the required number of test reruns?Our empirical results, obtained from two widely-used open-source ADS simulatorsand five diverse ADS test setups, show that test flakiness in ADS is a commonoccurrence and can significantly impact the test results obtained by randomizedalgorithms. Further, our ML classifiers effectively identify flaky ADS testsusing only a single test run, achieving F1-scores of $85$%, $82$% and $96$% forthree different ADS test setups. Our classifiers significantly outperform ournon-ML baseline, which requires executing tests at least twice, by $31$%,$21$%, and $13$% in F1-score performance, respectively. We conclude with adiscussion on the scope, implications and limitations of our study. We provideour complete replication package in a Github repository.</description><author>Mohammad Hossein Amini, Shervin Naseri, Shiva Nejati</author><pubDate>Thu, 30 Nov 2023 18:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18768v1</guid></item><item><title>Two-step reinforcement learning for model-free redesign of nonlinear optimal regulator</title><link>http://arxiv.org/abs/2103.03808v4</link><description>In many practical control applications, the performance level of aclosed-loop system degrades over time due to the change of plantcharacteristics. Thus, there is a strong need for redesigning a controllerwithout going through the system modeling process, which is often difficult forclosed-loop systems. Reinforcement learning (RL) is one of the promisingapproaches that enable model-free redesign of optimal controllers for nonlineardynamical systems based only on the measurement of the closed-loop system.However, the learning process of RL usually requires a considerable number oftrial-and-error experiments using the poorly controlled system that mayaccumulate wear on the plant. To overcome this limitation, we propose amodel-free two-step design approach that improves the transient learningperformance of RL in an optimal regulator redesign problem for unknownnonlinear systems. Specifically, we first design a linear control law thatattains some degree of control performance in a model-free manner, and then,train the nonlinear optimal control law with online RL by using the designedlinear control law in parallel. We introduce an offline RL algorithm for thedesign of the linear control law and theoretically guarantee its convergence tothe LQR controller under mild assumptions. Numerical simulations show that theproposed approach improves the transient learning performance and efficiency inhyperparameter tuning of RL.</description><author>Mei Minami, Yuka Masumoto, Yoshihiro Okawa, Tomotake Sasaki, Yutaka Hori</author><pubDate>Thu, 30 Nov 2023 18:07:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.03808v4</guid></item><item><title>MLLMs-Augmented Visual-Language Representation Learning</title><link>http://arxiv.org/abs/2311.18765v1</link><description>Visual-language pre-training (VLP) have achieved remarkable success inmulti-modal tasks, largely attributed to the availability of large-scaleimage-text datasets. In this work, we demonstrate that multi-modal largelanguage models (MLLMs) can enhance visual-language representation learning byimproving data quality. Our approach is simple, utilizing MLLMs to extendmultiple captions for each image. To prevent the bias that introduced by MLLMs'hallucinations and intrinsic caption styles, we propose a "text shearing" tokeep the lengths of extended captions identical to the originals. In image-textretrieval, our method consistently obtains 5.6 ~ 35.0% and 16.8 ~ 46.1%improvement on R@1 under the fine-tuning and zero-shot settings, respectively.Notably, our zero-shot results are comparable to fine-tuning on targetdatasets, which encourages more exploration on the versatile use of MLLMs.</description><author>Yanqing Liu, Kai Wang, Wenqi Shao, Ping Luo, Yu Qiao, Mike Zheng Shou, Kaipeng Zhang, Yang You</author><pubDate>Thu, 30 Nov 2023 18:05:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18765v1</guid></item><item><title>MMOTU: A Multi-Modality Ovarian Tumor Ultrasound Image Dataset for Unsupervised Cross-Domain Semantic Segmentation</title><link>http://arxiv.org/abs/2207.06799v4</link><description>Ovarian cancer is one of the most harmful gynecological diseases. Detectingovarian tumors in early stage with computer-aided techniques can efficientlydecrease the mortality rate. With the improvement of medical treatmentstandard, ultrasound images are widely applied in clinical treatment. However,recent notable methods mainly focus on single-modality ultrasound ovarian tumorsegmentation or recognition, which means there still lacks researches onexploring the representation capability of multi-modality ultrasound ovariantumor images. To solve this problem, we propose a Multi-Modality Ovarian TumorUltrasound (MMOTU) image dataset containing 1469 2d ultrasound images and 170contrast enhanced ultrasonography (CEUS) images with pixel-wise and global-wiseannotations. Based on MMOTU, we mainly focus on unsupervised cross-domainsemantic segmentation task. To solve the domain shift problem, we propose afeature alignment based architecture named Dual-Scheme Domain-Selected Network(DS2Net). Specifically, we first design source-encoder and target-encoder toextract two-style features of source and target images. Then, we proposeDomain-Distinct Selected Module (DDSM) and Domain-Universal Selected Module(DUSM) to extract the distinct and universal features in two styles(source-style or target-style). Finally, we fuse these two kinds of featuresand feed them into the source-decoder and target-decoder to generate finalpredictions. Extensive comparison experiments and analysis on MMOTU imagedataset show that DS2Net can boost the segmentation performance forbidirectional cross-domain adaptation of 2d ultrasound images and CEUS images.Our proposed dataset and code are all available athttps://github.com/cv516Buaa/MMOTU_DS2Net.</description><author>Qi Zhao, Shuchang Lyu, Wenpei Bai, Linghan Cai, Binghao Liu, Guangliang Cheng, Meijing Wu, Xiubo Sang, Min Yang, Lijiang Chen</author><pubDate>Thu, 30 Nov 2023 18:05:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.06799v4</guid></item><item><title>Continual Diffusion with STAMINA: STack-And-Mask INcremental Adapters</title><link>http://arxiv.org/abs/2311.18763v1</link><description>Recent work has demonstrated a remarkable ability to customize text-to-imagediffusion models to multiple, fine-grained concepts in a sequential (i.e.,continual) manner while only providing a few example images for each concept.This setting is known as continual diffusion. Here, we ask the question: Can wescale these methods to longer concept sequences without forgetting? Althoughprior work mitigates the forgetting of previously learned concepts, we showthat its capacity to learn new tasks reaches saturation over longer sequences.We address this challenge by introducing a novel method, STack-And-MaskINcremental Adapters (STAMINA), which is composed of low-rankedattention-masked adapters and customized MLP tokens. STAMINA is designed toenhance the robust fine-tuning properties of LoRA for sequential conceptlearning via learnable hard-attention masks parameterized with low rank MLPs,enabling precise, scalable learning via sparse adaptation. Notably, allintroduced trainable parameters can be folded back into the model aftertraining, inducing no additional inference parameter costs. We show thatSTAMINA outperforms the prior SOTA for the setting of text-to-image continualcustomization on a 50-concept benchmark composed of landmarks and human faces,with no stored replay data. Additionally, we extended our method to the settingof continual learning for image classification, demonstrating that our gainsalso translate to state-of-the-art performance in this standard benchmark.</description><author>James Seale Smith, Yen-Chang Hsu, Zsolt Kira, Yilin Shen, Hongxia Jin</author><pubDate>Thu, 30 Nov 2023 18:04:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18763v1</guid></item><item><title>Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?</title><link>http://arxiv.org/abs/2311.18761v1</link><description>The use of neural language models to model human behavior has met with mixedsuccess. While some work has found that the surprisal estimates from thesemodels can be used to predict a wide range of human neural and behavioralresponses, other work studying more complex syntactic phenomena has found thatthese surprisal estimates generate incorrect behavioral predictions. This paperexplores the extent to which the misalignment between empirical andmodel-predicted behavior can be minimized by training models on moredevelopmentally plausible data, such as in the BabyLM Challenge. We trainedteacher language models on the BabyLM "strict-small" dataset and used sentencelevel surprisal estimates from these teacher models to create a curriculum. Wefound tentative evidence that our curriculum made it easier for models toacquire linguistic knowledge from the training data: on the subset of tasks inthe BabyLM challenge suite evaluating models' grammatical knowledge of English,models first trained on the BabyLM data curriculum and then on a few randomlyordered training epochs performed slightly better than models trained onrandomly ordered epochs alone. This improved linguistic knowledge acquisitiondid not result in better alignment with human reading behavior, however: modelstrained on the BabyLM dataset (with or without a curriculum) generatedpredictions that were as misaligned with human behavior as models trained onlarger less curated datasets. This suggests that training on developmentallyplausible datasets alone is likely insufficient to generate language modelscapable of accurately predicting human language processing.</description><author>Aryaman Chobey, Oliver Smith, Anzi Wang, Grusha Prasad</author><pubDate>Thu, 30 Nov 2023 18:03:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18761v1</guid></item><item><title>TaskBench: Benchmarking Large Language Models for Task Automation</title><link>http://arxiv.org/abs/2311.18760v1</link><description>Recently, the incredible progress of large language models (LLMs) has ignitedthe spark of task automation, which decomposes the complex tasks described byuser instructions into sub-tasks, and invokes external tools to execute them,and plays a central role in autonomous agents. However, there lacks asystematic and standardized benchmark to foster the development of LLMs in taskautomation. To this end, we introduce TaskBench to evaluate the capability ofLLMs in task automation. Specifically, task automation can be formulated intothree critical stages: task decomposition, tool invocation, and parameterprediction to fulfill user intent. This complexity makes data collection andevaluation more challenging compared to common NLP tasks. To generatehigh-quality evaluation datasets, we introduce the concept of Tool Graph torepresent the decomposed tasks in user intent, and adopt a back-instruct methodto simulate user instruction and annotations. Furthermore, we propose TaskEvalto evaluate the capability of LLMs from different aspects, including taskdecomposition, tool invocation, and parameter prediction. Experimental resultsdemonstrate that TaskBench can effectively reflects the capability of LLMs intask automation. Benefiting from the mixture of automated data construction andhuman verification, TaskBench achieves a high consistency compared to the humanevaluation, which can be utilized as a comprehensive and faithful benchmark forLLM-based autonomous agents.</description><author>Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, Yueting Zhuang</author><pubDate>Thu, 30 Nov 2023 18:02:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18760v1</guid></item><item><title>Semi-supervised Semantic Segmentation via Boosting Uncertainty on Unlabeled Data</title><link>http://arxiv.org/abs/2311.18758v1</link><description>We bring a new perspective to semi-supervised semantic segmentation byproviding an analysis on the labeled and unlabeled distributions in trainingdatasets. We first figure out that the distribution gap between labeled andunlabeled datasets cannot be ignored, even though the two datasets are sampledfrom the same distribution. To address this issue, we theoretically analyze andexperimentally prove that appropriately boosting uncertainty on unlabeled datacan help minimize the distribution gap, which benefits the generalization ofthe model. We propose two strategies and design an uncertainty boosteralgorithm, specially for semi-supervised semantic segmentation. Extensiveexperiments are carried out based on these theories, and the results confirmthe efficacy of the algorithm and strategies. Our plug-and-play uncertaintybooster is tiny, efficient, and robust to hyperparameters but can significantlypromote performance. Our approach achieves state-of-the-art performance in ourexperiments compared to the current semi-supervised semantic segmentationmethods on the popular benchmarks: Cityscapes and PASCAL VOC 2012 withdifferent train settings.</description><author>Daoan Zhang, Yunhao Luo, Jianguo Zhang</author><pubDate>Thu, 30 Nov 2023 18:01:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18758v1</guid></item><item><title>Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation</title><link>http://arxiv.org/abs/2311.15841v2</link><description>This study focuses on a novel task in text-to-image (T2I) generation, namelyaction customization. The objective of this task is to learn the co-existingaction from limited data and generalize it to unseen humans or even animals.Experimental results show that existing subject-driven customization methodsfail to learn the representative characteristics of actions and struggle indecoupling actions from context features, including appearance. To overcome thepreference for low-level features and the entanglement of high-level features,we propose an inversion-based method Action-Disentangled Identifier (ADI) tolearn action-specific identifiers from the exemplar images. ADI first expandsthe semantic conditioning space by introducing layer-wise identifier tokens,thereby increasing the representational richness while distributing theinversion across different features. Then, to block the inversion ofaction-agnostic features, ADI extracts the gradient invariance from theconstructed sample triples and masks the updates of irrelevant channels. Tocomprehensively evaluate the task, we present an ActionBench that includes avariety of actions, each accompanied by meticulously selected samples. Bothquantitative and qualitative results show that our ADI outperforms existingbaselines in action-customized T2I generation. Our project page is athttps://adi-t2i.github.io/ADI.</description><author>Siteng Huang, Biao Gong, Yutong Feng, Xi Chen, Yuqian Fu, Yu Liu, Donglin Wang</author><pubDate>Thu, 30 Nov 2023 17:51:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15841v2</guid></item><item><title>Language Model Agents Suffer from Compositional Generalization in Web Automation</title><link>http://arxiv.org/abs/2311.18751v1</link><description>Language model agents (LMA) recently emerged as a promising paradigm onmuti-step decision making tasks, often outperforming humans and otherreinforcement learning agents. Despite the promise, their performance onreal-world applications that often involve combinations of tasks is stillunderexplored. In this work, we introduce a new benchmark, called CompWoB -- 50new compositional web automation tasks reflecting more realistic assumptions.We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve94.0% average success rate on base tasks, their performance degrades to 24.9%success rate on compositional tasks. On the other hand, transferred LMAs(finetuned only on base tasks) show less generalization gap, dropping from85.4% to 54.8%. By balancing data distribution across tasks, we train a newmodel, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB,and achieves the best zero-shot performance on CompWoB (61.5%). While thesehighlight the promise of small-scale finetuned and transferred models forcompositional generalization, their performance further degrades underdifferent instruction compositions changing combinational order. In contrast tothe recent remarkable success of LMA, our benchmark and detailed analysisemphasize the necessity of building LMAs that are robust and generalizable totask compositionality for real-world deployment.</description><author>Hiroki Furuta, Yutaka Matsuo, Aleksandra Faust, Izzeddin Gur</author><pubDate>Thu, 30 Nov 2023 17:50:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18751v1</guid></item><item><title>LocoMuJoCo: A Comprehensive Imitation Learning Benchmark for Locomotion</title><link>http://arxiv.org/abs/2311.02496v2</link><description>Imitation Learning (IL) holds great promise for enabling agile locomotion inembodied agents. However, many existing locomotion benchmarks primarily focuson simplified toy tasks, often failing to capture the complexity of real-worldscenarios and steering research toward unrealistic domains. To advance researchin IL for locomotion, we present a novel benchmark designed to facilitaterigorous evaluation and comparison of IL algorithms. This benchmark encompassesa diverse set of environments, including quadrupeds, bipeds, andmusculoskeletal human models, each accompanied by comprehensive datasets, suchas real noisy motion capture data, ground truth expert data, and ground truthsub-optimal data, enabling evaluation across a spectrum of difficulty levels.To increase the robustness of learned agents, we provide an easy interface fordynamics randomization and offer a wide range of partially observable tasks totrain agents across different embodiments. Finally, we provide handcraftedmetrics for each task and ship our benchmark with state-of-the-art baselinealgorithms to ease evaluation and enable fast benchmarking.</description><author>Firas Al-Hafez, Guoping Zhao, Jan Peters, Davide Tateo</author><pubDate>Thu, 30 Nov 2023 17:47:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02496v2</guid></item><item><title>TransCORALNet: A Two-Stream Transformer CORAL Networks for Supply Chain Credit Assessment Cold Start</title><link>http://arxiv.org/abs/2311.18749v1</link><description>This paper proposes an interpretable two-stream transformer CORAL networks(TransCORALNet) for supply chain credit assessment under the segment industryand cold start problem. The model aims to provide accurate credit assessmentprediction for new supply chain borrowers with limited historical data. Here,the two-stream domain adaptation architecture with correlation alignment(CORAL) loss is used as a core model and is equipped with transformer, whichprovides insights about the learned features and allow efficientparallelization during training. Thanks to the domain adaptation capability ofthe proposed model, the domain shift between the source and target domain isminimized. Therefore, the model exhibits good generalization where the sourceand target do not follow the same distribution, and a limited amount of targetlabeled instances exist. Furthermore, we employ Local InterpretableModel-agnostic Explanations (LIME) to provide more insight into the modelprediction and identify the key features contributing to supply chain creditassessment decisions. The proposed model addresses four significant supplychain credit assessment challenges: domain shift, cold start, imbalanced-classand interpretability. Experimental results on a real-world data set demonstratethe superiority of TransCORALNet over a number of state-of-the-art baselines interms of accuracy. The code is available on GitHubhttps://github.com/JieJieNiu/TransCORALN .</description><author>Jie Shi, Arno P. J. M. Siebes, Siamak Mehrkanoon</author><pubDate>Thu, 30 Nov 2023 17:47:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18749v1</guid></item><item><title>A data-science pipeline to enable the Interpretability of Many-Objective Feature Selection</title><link>http://arxiv.org/abs/2311.18746v1</link><description>Many-Objective Feature Selection (MOFS) approaches use four or moreobjectives to determine the relevance of a subset of features in a supervisedlearning task. As a consequence, MOFS typically returns a large set ofnon-dominated solutions, which have to be assessed by the data scientist inorder to proceed with the final choice. Given the multi-variate nature of theassessment, which may include criteria (e.g. fairness) not related topredictive accuracy, this step is often not straightforward and suffers fromthe lack of existing tools. For instance, it is common to make use of a tabularpresentation of the solutions, which provide little information about thetrade-offs and the relations between criteria over the set of solutions. This paper proposes an original methodology to support data scientists in theinterpretation and comparison of the MOFS outcome by combining post-processingand visualisation of the set of solutions. The methodology supports the datascientist in the selection of an optimal feature subset by providing her withhigh-level information at three different levels: objectives, solutions, andindividual features. The methodology is experimentally assessed on two feature selection tasksadopting a GA-based MOFS with six objectives (number of selected features,balanced accuracy, F1-Score, variance inflation factor, statistical parity, andequalised odds). The results show the added value of the methodology in theselection of the final subset of features.</description><author>Uchechukwu F. Njoku, Alberto Abelló, Besim Bilalli, Gianluca Bontempi</author><pubDate>Thu, 30 Nov 2023 17:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18746v1</guid></item><item><title>$\mathbb{Z}_2\times \mathbb{Z}_2$ Equivariant Quantum Neural Networks: Benchmarking against Classical Neural Networks</title><link>http://arxiv.org/abs/2311.18744v1</link><description>This paper presents a comprehensive comparative analysis of the performanceof Equivariant Quantum Neural Networks (EQNN) and Quantum Neural Networks(QNN), juxtaposed against their classical counterparts: Equivariant NeuralNetworks (ENN) and Deep Neural Networks (DNN). We evaluate the performance ofeach network with two toy examples for a binary classification task, focusingon model complexity (measured by the number of parameters) and the size of thetraining data set. Our results show that the $\mathbb{Z}_2\times \mathbb{Z}_2$EQNN and the QNN provide superior performance for smaller parameter sets andmodest training data samples.</description><author>Zhongtian Dong, Marçal Comajoan Cara, Gopal Ramesh Dahale, Roy T. Forestano, Sergei Gleyzer, Daniel Justice, Kyoungchul Kong, Tom Magorsch, Konstantin T. Matchev, Katia Matcheva, Eyup B. Unlu</author><pubDate>Thu, 30 Nov 2023 17:41:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18744v1</guid></item><item><title>AlignBench: Benchmarking Chinese Alignment of Large Language Models</title><link>http://arxiv.org/abs/2311.18743v1</link><description>Alignment has become a critical step for instruction-tuned Large LanguageModels (LLMs) to become helpful assistants. However, effective evaluation ofalignment for emerging Chinese LLMs is still significantly lacking, calling forreal-scenario grounded, open-ended, challenging and automatic evaluationstailored for alignment. To fill in this gap, we introduce AlignBench, acomprehensive multi-dimensional benchmark for evaluating LLMs' alignment inChinese. Equipped with a human-in-the-loop data curation pipeline, ourbenchmark employs a rule-calibrated multi-dimensional LLM-as-Judge withChain-of-Thought to generate explanations and final ratings as evaluations,ensuring high reliability and interpretability. Furthermore, we developed adedicated companion evaluator LLM -- CritiqueLLM, which recovers 95\% ofGPT-4's evaluation ability and will be provided via public APIs to researchersfor evaluation of alignment in Chinese LLMs. All evaluation codes, data, andLLM generations are available at \url{https://github.com/THUDM/AlignBench}.</description><author>Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, Jie Tang</author><pubDate>Thu, 30 Nov 2023 17:41:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18743v1</guid></item><item><title>DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models</title><link>http://arxiv.org/abs/2304.00916v3</link><description>We present DreamAvatar, a text-and-shape guided framework for generatinghigh-quality 3D human avatars with controllable poses. While encouragingresults have been reported by recent methods on text-guided 3D common objectgeneration, generating high-quality human avatars remains an open challenge dueto the complexity of the human body's shape, pose, and appearance. We proposeDreamAvatar to tackle this challenge, which utilizes a trainable NeRF forpredicting density and color for 3D points and pretrained text-to-imagediffusion models for providing 2D self-supervision. Specifically, we leveragethe SMPL model to provide shape and pose guidance for the generation. Weintroduce a dual-observation-space design that involves the joint optimizationof a canonical space and a posed space that are related by a learnabledeformation field. This facilitates the generation of more complete texturesand geometry faithful to the target pose. We also jointly optimize the lossescomputed from the full body and from the zoomed-in 3D head to alleviate thecommon multi-face ''Janus'' problem and improve facial details in the generatedavatars. Extensive evaluations demonstrate that DreamAvatar significantlyoutperforms existing methods, establishing a new state-of-the-art fortext-and-shape guided 3D human avatar generation.</description><author>Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong</author><pubDate>Thu, 30 Nov 2023 17:40:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.00916v3</guid></item><item><title>VREM-FL: Mobility-Aware Computation-Scheduling Co-Design for Vehicular Federated Learning</title><link>http://arxiv.org/abs/2311.18741v1</link><description>Assisted and autonomous driving are rapidly gaining momentum, and will soonbecome a reality. Among their key enablers, artificial intelligence and machinelearning are expected to play a prominent role, also thanks to the massiveamount of data that smart vehicles will collect from their onboard sensors. Inthis domain, federated learning is one of the most effective and promisingtechniques for training global machine learning models, while preserving dataprivacy at the vehicles and optimizing communications resource usage. In thiswork, we propose VREM-FL, a computation-scheduling co-design for vehicularfederated learning that leverages mobility of vehicles in conjunction withestimated 5G radio environment maps. VREM-FL jointly optimizes the global modellearned at the server while wisely allocating communication resources. This isachieved by orchestrating local computations at the vehicles in conjunctionwith the transmission of their local model updates in an adaptive andpredictive fashion, by exploiting radio channel maps. The proposed algorithmcan be tuned to trade model training time for radio resource usage.Experimental results demonstrate the efficacy of utilizing radio maps. VREM-FLoutperforms literature benchmarks for both a linear regression model (learningtime reduced by 28%) and a deep neural network for a semantic imagesegmentation task (doubling the number of model updates within the same timewindow).</description><author>Luca Ballotta, Nicolò Dal Fabbro, Giovanni Perin, Luca Schenato, Michele Rossi, Giuseppe Piro</author><pubDate>Thu, 30 Nov 2023 17:38:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18741v1</guid></item><item><title>Mavericks at NADI 2023 Shared Task: Unravelling Regional Nuances through Dialect Identification using Transformer-based Approach</title><link>http://arxiv.org/abs/2311.18739v1</link><description>In this paper, we present our approach for the "Nuanced Arabic DialectIdentification (NADI) Shared Task 2023". We highlight our methodology forsubtask 1 which deals with country-level dialect identification. Recognizingdialects plays an instrumental role in enhancing the performance of variousdownstream NLP tasks such as speech recognition and translation. The task usesthe Twitter dataset (TWT-2023) that encompasses 18 dialects for the multi-classclassification problem. Numerous transformer-based models, pre-trained onArabic language, are employed for identifying country-level dialects. Wefine-tune these state-of-the-art models on the provided dataset. The ensemblingmethod is leveraged to yield improved performance of the system. We achieved anF1-score of 76.65 (11th rank on the leaderboard) on the test dataset.</description><author>Vedant Deshpande, Yash Patwardhan, Kshitij Deshpande, Sudeep Mangalvedhekar, Ravindra Murumkar</author><pubDate>Thu, 30 Nov 2023 17:37:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18739v1</guid></item><item><title>Controlgym: Large-Scale Safety-Critical Control Environments for Benchmarking Reinforcement Learning Algorithms</title><link>http://arxiv.org/abs/2311.18736v1</link><description>We introduce controlgym, a library of thirty-six safety-critical industrialcontrol settings, and ten infinite-dimensional partial differential equation(PDE)-based control problems. Integrated within the OpenAI Gym/Gymnasium (Gym)framework, controlgym allows direct applications of standard reinforcementlearning (RL) algorithms like stable-baselines3. Our control environmentscomplement those in Gym with continuous, unbounded action and observationspaces, motivated by real-world control applications. Moreover, the PDE controlenvironments uniquely allow the users to extend the state dimensionality of thesystem to infinity while preserving the intrinsic dynamics. This feature iscrucial for evaluating the scalability of RL algorithms for control. Thisproject serves the learning for dynamics &amp; control (L4DC) community, aiming toexplore key questions: the convergence of RL algorithms in learning controlpolicies; the stability and robustness issues of learning-based controllers;and the scalability of RL algorithms to high- and potentiallyinfinite-dimensional systems. We open-source the controlgym project athttps://github.com/xiangyuan-zhang/controlgym.</description><author>Xiangyuan Zhang, Weichao Mao, Saviz Mowlavi, Mouhacine Benosman, Tamer Başar</author><pubDate>Thu, 30 Nov 2023 17:34:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18736v1</guid></item><item><title>Dimension Mixer: A Generalized Method for Structured Sparsity in Deep Neural Networks</title><link>http://arxiv.org/abs/2311.18735v1</link><description>The recent success of multiple neural architectures like CNNs, Transformers,and MLP-Mixers motivated us to look for similarities and differences betweenthem. We found that these architectures can be interpreted through the lens ofa general concept of dimension mixing. Research on coupling flows and thebutterfly transform shows that partial and hierarchical signal mixing schemesare sufficient for efficient and expressive function approximation. In thiswork, we study group-wise sparse, non-linear, multi-layered and learnablemixing schemes of inputs and find that they are complementary to many standardneural architectures. Following our observations and drawing inspiration fromthe Fast Fourier Transform, we generalize Butterfly Structure to use non-linearmixer function allowing for MLP as mixing function called Butterfly MLP. Wewere also able to mix along sequence dimension for Transformer-basedarchitectures called Butterfly Attention. Experiments on CIFAR and LRA datasetsdemonstrate that the proposed Non-Linear Butterfly Mixers are efficient andscale well when the host architectures are used as mixing function.Additionally, we propose Patch-Only MLP-Mixer for processing spatial 2D signalsdemonstrating a different dimension mixing strategy.</description><author>Suman Sapkota, Binod Bhattarai</author><pubDate>Thu, 30 Nov 2023 17:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18735v1</guid></item><item><title>Indoor Millimeter Wave Localization using Multiple Self-Supervised Tiny Neural Networks</title><link>http://arxiv.org/abs/2311.18732v1</link><description>We consider the localization of a mobile millimeter-wave client in a largeindoor environment using multilayer perceptron neural networks (NNs). Insteadof training and deploying a single deep model, we proceed by choosing amongmultiple tiny NNs trained in a self-supervised manner. The main challenge thenbecomes to determine and switch to the best NN among the available ones, as anincorrect NN will fail to localize the client. In order to upkeep thelocalization accuracy, we propose two switching schemes: one based on a Kalmanfilter, and one based on the statistical distribution of the training data. Weanalyze the proposed schemes via simulations, showing that our approachoutperforms both geometric localization schemes and the use of a single NN.</description><author>Anish Shastri, Andres Garcia-Saavedra, Paolo Casari</author><pubDate>Thu, 30 Nov 2023 17:27:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18732v1</guid></item><item><title>Mavericks at ArAIEval Shared Task: Towards a Safer Digital Space -- Transformer Ensemble Models Tackling Deception and Persuasion</title><link>http://arxiv.org/abs/2311.18730v1</link><description>In this paper, we highlight our approach for the "Arabic AI Tasks Evaluation(ArAiEval) Shared Task 2023". We present our approaches for task 1-A and task2-A of the shared task which focus on persuasion technique detection anddisinformation detection respectively. Detection of persuasion techniques anddisinformation has become imperative to avoid distortion of authenticinformation. The tasks use multigenre snippets of tweets and news articles forthe given binary classification problem. We experiment with severaltransformer-based models that were pre-trained on the Arabic language. Wefine-tune these state-of-the-art models on the provided dataset. Ensembling isemployed to enhance the performance of the systems. We achieved a microF1-score of 0.742 on task 1-A (8th rank on the leaderboard) and 0.901 on task2-A (7th rank on the leaderboard) respectively.</description><author>Sudeep Mangalvedhekar, Kshitij Deshpande, Yash Patwardhan, Vedant Deshpande, Ravindra Murumkar</author><pubDate>Thu, 30 Nov 2023 17:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18730v1</guid></item><item><title>Learning One-Shot 4D Head Avatar Synthesis using Synthetic Data</title><link>http://arxiv.org/abs/2311.18729v1</link><description>Existing one-shot 4D head synthesis methods usually learn from monocularvideos with the aid of 3DMM reconstruction, yet the latter is evenlychallenging which restricts them from reasonable 4D head synthesis. We presenta method to learn one-shot 4D head synthesis via large-scale synthetic data.The key is to first learn a part-wise 4D generative model from monocular imagesvia adversarial learning, to synthesize multi-view images of diverse identitiesand full motions as training data; then leverage a transformer-based animatabletriplane reconstructor to learn 4D head reconstruction using the syntheticdata. A novel learning strategy is enforced to enhance the generalizability toreal images by disentangling the learning process of 3D reconstruction andreenactment. Experiments demonstrate our superiority over the prior art.</description><author>Yu Deng, Duomin Wang, Xiaohang Ren, Xingyu Chen, Baoyuan Wang</author><pubDate>Thu, 30 Nov 2023 17:26:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18729v1</guid></item><item><title>Automatic Functional Differentiation in JAX</title><link>http://arxiv.org/abs/2311.18727v1</link><description>We extend JAX with the capability to automatically differentiate higher-orderfunctions (functionals and operators). By representing functions as ageneralization of arrays, we seamlessly use JAX's existing primitive system toimplement higher-order functions. We present a set of primitive operators thatserve as foundational building blocks for constructing several key types offunctionals. For every introduced primitive operator, we derive and implementboth linearization and transposition rules, aligning with JAX's internalprotocols for forward and reverse mode automatic differentiation. Thisenhancement allows for functional differentiation in the same syntaxtraditionally use for functions. The resulting functional gradients arethemselves functions ready to be invoked in python. We showcase this tool'sefficacy and simplicity through applications where functional derivatives areindispensable. The source code of this work is released athttps://github.com/sail-sg/autofd .</description><author>Min Lin</author><pubDate>Thu, 30 Nov 2023 17:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18727v1</guid></item><item><title>AI in Pharma for Personalized Sequential Decision-Making: Methods, Applications and Opportunities</title><link>http://arxiv.org/abs/2311.18725v1</link><description>In the pharmaceutical industry, the use of artificial intelligence (AI) hasseen consistent growth over the past decade. This rise is attributed to majoradvancements in statistical machine learning methodologies, computationalcapabilities and the increased availability of large datasets. AI techniquesare applied throughout different stages of drug development, ranging from drugdiscovery to post-marketing benefit-risk assessment. Kolluri et al. provided areview of several case studies that span these stages, featuring keyapplications such as protein structure prediction, success probabilityestimation, subgroup identification, and AI-assisted clinical trial monitoring.From a regulatory standpoint, there was a notable uptick in submissionsincorporating AI components in 2021. The most prevalent therapeutic areasleveraging AI were oncology (27%), psychiatry (15%), gastroenterology (12%),and neurology (11%). The paradigm of personalized or precision medicine hasgained significant traction in recent research, partly due to advancements inAI techniques \cite{hamburg2010path}. This shift has had a transformativeimpact on the pharmaceutical industry. Departing from the traditional"one-size-fits-all" model, personalized medicine incorporates variousindividual factors, such as environmental conditions, lifestyle choices, andhealth histories, to formulate customized treatment plans. By utilizingsophisticated machine learning algorithms, clinicians and researchers arebetter equipped to make informed decisions in areas such as disease prevention,diagnosis, and treatment selection, thereby optimizing health outcomes for eachindividual.</description><author>Yuhan Li, Hongtao Zhang, Keaven Anderson, Songzi Li, Ruoqing Zhu</author><pubDate>Thu, 30 Nov 2023 17:23:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18725v1</guid></item><item><title>Understanding Sample Generation Strategies for Learning Heuristic Functions in Classical Planning</title><link>http://arxiv.org/abs/2211.13316v2</link><description>We study the problem of learning good heuristic functions for classicalplanning tasks with neural networks based on samples represented by states withtheir cost-to-goal estimates. The heuristic function is learned for a statespace and goal condition with the number of samples limited to a fraction ofthe size of the state space, and must generalize well for all states of thestate space with the same goal condition. Our main goal is to better understandthe influence of sample generation strategies on the performance of a greedybest-first heuristic search (GBFS) guided by a learned heuristic function. In aset of controlled experiments, we find that two main factors determine thequality of the learned heuristic: which states are included in the sample setand the quality of the cost-to-goal estimates. These two factors are dependent:having perfect cost-to-goal estimates is insufficient if the samples are notwell distributed across the state space. We also study other effects, such asadding samples with high-value estimates. Based on our findings, we proposepractical strategies to improve the quality of learned heuristics: threestrategies that aim to generate more representative states and two strategiesthat improve the cost-to-goal estimates. Our practical strategies almost doublethe mean coverage of a GBFS algorithm guided by a learned heuristic.</description><author>R. V. Bettker, P. P. Minini, A. G. Pereira, M. Ritt</author><pubDate>Thu, 30 Nov 2023 17:20:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13316v2</guid></item><item><title>Steering Deep Feature Learning with Backward Aligned Feature Updates</title><link>http://arxiv.org/abs/2311.18718v1</link><description>Deep learning succeeds by doing hierarchical feature learning, yet tuningHyper-Parameters (HP) such as initialization scales, learning rates etc., onlygive indirect control over this behavior. In this paper, we propose thealignment between the feature updates and the backward pass as a key notion topredict, measure and control feature learning. On the one hand, we show thatwhen alignment holds, the magnitude of feature updates after one SGD step isrelated to the magnitude of the forward and backward passes by a simple andgeneral formula. This leads to techniques to automatically adjust HPs(initialization scales and learning rates) at initialization and throughouttraining to attain a desired feature learning behavior. On the other hand, weshow that, at random initialization, this alignment is determined by thespectrum of a certain kernel, and that well-conditioned layer-to-layerJacobians (aka dynamical isometry) implies alignment. Finally, we investigateReLU MLPs and ResNets in the large width-then-depth limit. Combining hints fromrandom matrix theory and numerical experiments, we show that (i) in MLP withiid initializations, alignment degenerates with depth, making it impossible tostart training, and that (ii) in ResNets, the branch scale$1/\sqrt{\text{depth}}$ is the only one maintaining non-trivial alignment atinfinite depth.</description><author>Lénaïc Chizat, Praneeth Netrapalli</author><pubDate>Thu, 30 Nov 2023 17:19:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18718v1</guid></item><item><title>Focused Transformer: Contrastive Training for Context Scaling</title><link>http://arxiv.org/abs/2307.03170v2</link><description>Large language models have an exceptional capability to incorporate newinformation in a contextual manner. However, the full potential of such anapproach is often restrained due to a limitation in the effective contextlength. One solution to this issue is to endow an attention layer with accessto an external memory, which comprises of (key, value) pairs. Yet, as thenumber of documents increases, the proportion of relevant keys to irrelevantones decreases, leading the model to focus more on the irrelevant keys. Weidentify a significant challenge, dubbed the distraction issue, where keyslinked to different semantic values might overlap, making them hard todistinguish. To tackle this problem, we introduce the Focused Transformer(FoT), a technique that employs a training process inspired by contrastivelearning. This novel approach enhances the structure of the (key, value) space,enabling an extension of the context length. Our method allows for fine-tuningpre-existing, large-scale models to lengthen their effective context. This isdemonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. Theresulting models, which we name LongLLaMA, exhibit advancements in tasksrequiring a long context. We further illustrate that our LongLLaMA modelsadeptly manage a $256 k$ context length for passkey retrieval.</description><author>Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr Miłoś</author><pubDate>Thu, 30 Nov 2023 17:15:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03170v2</guid></item><item><title>CoRec: An Easy Approach for Coordination Recognition</title><link>http://arxiv.org/abs/2311.18712v1</link><description>In this paper, we observe and address the challenges of the coordinationrecognition task. Most existing methods rely on syntactic parsers to identifythe coordinators in a sentence and detect the coordination boundaries. However,state-of-the-art syntactic parsers are slow and suffer from errors, especiallyfor long and complicated sentences. To better solve the problems, we propose apipeline model COordination RECognizer (CoRec). It consists of two components:coordinator identifier and conjunct boundary detector. The experimental resultson datasets from various domains demonstrate the effectiveness and efficiencyof the proposed method. Further experiments show that CoRec positively impactsdownstream tasks, improving the yield of state-of-the-art Open IE models.</description><author>Qing Wang, Haojie Jia, Wenfei Song, Qi Li</author><pubDate>Thu, 30 Nov 2023 17:11:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18712v1</guid></item><item><title>Hessian-Aware Bayesian Optimization for Decision Making Systems</title><link>http://arxiv.org/abs/2308.00629v3</link><description>Many approaches for optimizing decision making systems rely on gradient basedmethods requiring informative feedback from the environment. However, in thecase where such feedback is sparse or uninformative, such approaches may resultin poor performance. Derivative-free approaches such as Bayesian Optimizationmitigate the dependency on the quality of gradient feedback, but are known toscale poorly in the high-dimension setting of complex decision making systems.This problem is exacerbated if the system requires interactions between severalactors cooperating to accomplish a shared goal. To address the dimensionalitychallenge, we propose a compact multi-layered architecture modeling thedynamics of actor interactions through the concept of role. Additionally, weintroduce Hessian-aware Bayesian Optimization to efficiently optimize themulti-layered architecture parameterized by a large number of parameters.Experimental results demonstrate that our method (HA-GP-UCB) works effectivelyon several benchmarks under resource constraints and malformed feedbacksettings.</description><author>Mohit Rajpal, Lac Gia Tran, Yehong Zhang, Bryan Kian Hsiang Low</author><pubDate>Thu, 30 Nov 2023 17:07:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00629v3</guid></item><item><title>Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling</title><link>http://arxiv.org/abs/2311.18711v1</link><description>We present GEST -- a new dataset for measuring gender-stereotypical reasoningin masked LMs and English-to-X machine translation systems. GEST containssamples that are compatible with 9 Slavic languages and English for 16 genderstereotypes about men and women (e.g., Women are beautiful, Men are leaders).The definition of said stereotypes was informed by gender experts. We used GESTto evaluate 11 masked LMs and 4 machine translation systems. We discoveredsignificant and consistent amounts of stereotypical reasoning in almost all theevaluated models and languages.</description><author>Matúš Pikuliak, Andrea Hrckova, Stefan Oresko, Marián Šimko</author><pubDate>Thu, 30 Nov 2023 17:06:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18711v1</guid></item><item><title>Meta-Prior: Meta learning for Adaptive Inverse Problem Solvers</title><link>http://arxiv.org/abs/2311.18710v1</link><description>Deep neural networks have become a foundational tool for addressing imaginginverse problems. They are typically trained for a specific task, with asupervised loss to learn a mapping from the observations to the image torecover. However, real-world imaging challenges often lack ground truth data,rendering traditional supervised approaches ineffective. Moreover, for each newimaging task, a new model needs to be trained from scratch, wasting time andresources. To overcome these limitations, we introduce a novel approach basedon meta-learning. Our method trains a meta-model on a diverse set of imagingtasks that allows the model to be efficiently fine-tuned for specific taskswith few fine-tuning steps. We show that the proposed method extends to theunsupervised setting, where no ground truth data is available. In its bilevelformulation, the outer level uses a supervised loss, that evaluates how wellthe fine-tuned model performs, while the inner loss can be either supervised orunsupervised, relying only on the measurement operator. This allows themeta-model to leverage a few ground truth samples for each task while beingable to generalize to new imaging tasks. We show that in simple settings, thisapproach recovers the Bayes optimal estimator, illustrating the soundness ofour approach. We also demonstrate our method's effectiveness on various tasks,including image processing and magnetic resonance imaging.</description><author>Matthieu Terris, Thomas Moreau</author><pubDate>Thu, 30 Nov 2023 17:02:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18710v1</guid></item><item><title>Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization</title><link>http://arxiv.org/abs/2311.18703v1</link><description>In Reinforcement Learning (RL), agents have no incentive to exhibitpredictable behaviors, and are often pushed (through e.g. policy entropyregularization) to randomize their actions in favor of exploration. From ahuman perspective, this makes RL agents hard to interpret and predict, and froma safety perspective, even harder to formally verify. We propose a novel methodto induce predictable behavior in RL agents, referred to asPredictability-Aware RL (PA-RL), which employs the state sequence entropy rateas a predictability measure. We show how the entropy rate can be formulated asan average reward objective, and since its entropy reward function ispolicy-dependent, we introduce an action-dependent surrogate entropy enablingthe use of PG methods. We prove that deterministic policies minimizing theaverage surrogate reward exist and also minimize the actual entropy rate, andshow how, given a learned dynamical model, we are able to approximate the valuefunction associated to the true entropy rate. Finally, we demonstrate theeffectiveness of the approach in RL tasks inspired by human-robot use-cases,and show how it produces agents with more predictable behavior while achievingnear-optimal rewards.</description><author>Daniel Jarne Ornia, Giannis Delimpaltadakis, Jens Kober, Javier Alonso-Mora</author><pubDate>Thu, 30 Nov 2023 16:53:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18703v1</guid></item><item><title>CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation</title><link>http://arxiv.org/abs/2311.18702v1</link><description>Since the natural language processing (NLP) community started to make largelanguage models (LLMs), such as GPT-4, act as a critic to evaluate the qualityof generated texts, most of them only train a critique generation model of aspecific scale on specific datasets. We argue that a comprehensiveinvestigation on the key factor of LLM-based evaluation models, such as scalingproperties, is lacking, so that it is still inconclusive whether these modelshave potential to replace GPT-4's evaluation in practical scenarios. In thispaper, we propose a new critique generation model called CritiqueLLM, whichincludes a dialogue-based prompting method for high-quality referenced /reference-free evaluation data. Experimental results show that our model canachieve comparable evaluation performance to GPT-4 especially in system-levelcorrelations, and even outperform GPT-4 in 3 out of 8 tasks in a challengingreference-free setting. We conduct detailed analysis to show promising scalingproperties of our model in the quality of generated critiques. We alsodemonstrate that our generated critiques can act as scalable feedback todirectly improve the generation quality of LLMs.</description><author>Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang</author><pubDate>Thu, 30 Nov 2023 16:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18702v1</guid></item><item><title>Bayesian CART models for insurance claims frequency</title><link>http://arxiv.org/abs/2303.01923v2</link><description>Accuracy and interpretability of a (non-life) insurance pricing model areessential qualities to ensure fair and transparent premiums for policy-holders,that reflect their risk. In recent years, the classification and regressiontrees (CARTs) and their ensembles have gained popularity in the actuarialliterature, since they offer good prediction performance and are relativelyeasily interpretable. In this paper, we introduce Bayesian CART models forinsurance pricing, with a particular focus on claims frequency modelling.Additionally to the common Poisson and negative binomial (NB) distributionsused for claims frequency, we implement Bayesian CART for the zero-inflatedPoisson (ZIP) distribution to address the difficulty arising from theimbalanced insurance claims data. To this end, we introduce a general MCMCalgorithm using data augmentation methods for posterior tree exploration. Wealso introduce the deviance information criterion (DIC) for the tree modelselection. The proposed models are able to identify trees which can betterclassify the policy-holders into risk groups. Some simulations and realinsurance data will be discussed to illustrate the applicability of thesemodels.</description><author>Yaojun Zhang, Lanpeng Ji, Georgios Aivaliotis, Charles Taylor</author><pubDate>Thu, 30 Nov 2023 16:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01923v2</guid></item><item><title>Seg2Reg: Differentiable 2D Segmentation to 1D Regression Rendering for 360 Room Layout Reconstruction</title><link>http://arxiv.org/abs/2311.18695v1</link><description>State-of-the-art single-view 360-degree room layout reconstruction methodsformulate the problem as a high-level 1D (per-column) regression task. On theother hand, traditional low-level 2D layout segmentation is simpler to learnand can represent occluded regions, but it requires complex post-processing forthe targeting layout polygon and sacrifices accuracy. We present Seg2Reg torender 1D layout depth regression from the 2D segmentation map in adifferentiable and occlusion-aware way, marrying the merits of both sides.Specifically, our model predicts floor-plan density for the inputequirectangular 360-degree image. Formulating the 2D layout representation as adensity field enables us to employ `flattened' volume rendering to form 1Dlayout depth regression. In addition, we propose a novel 3D warpingaugmentation on layout to improve generalization. Finally, we re-implementrecent room layout reconstruction methods into our codebase for benchmarkingand explore modern backbones and training techniques to serve as the strongbaseline. Our model significantly outperforms previous arts. The code will bemade available upon publication.</description><author>Cheng Sun, Wei-En Tai, Yu-Lin Shih, Kuan-Wei Chen, Yong-Jing Syu, Kent Selwyn The, Yu-Chiang Frank Wang, Hwann-Tzong Chen</author><pubDate>Thu, 30 Nov 2023 16:42:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18695v1</guid></item><item><title>Balancing Summarization and Change Detection in Graph Streams</title><link>http://arxiv.org/abs/2311.18694v1</link><description>This study addresses the issue of balancing graph summarization and graphchange detection. Graph summarization compresses large-scale graphs into asmaller scale. However, the question remains: To what extent should theoriginal graph be compressed? This problem is solved from the perspective ofgraph change detection, aiming to detect statistically significant changesusing a stream of summary graphs. If the compression rate is extremely high,important changes can be ignored, whereas if the compression rate is extremelylow, false alarms may increase with more memory. This implies that there is atrade-off between compression rate in graph summarization and accuracy inchange detection. We propose a novel quantitative methodology to balance thistrade-off to simultaneously realize reliable graph summarization and changedetection. We introduce a probabilistic structure of hierarchical latentvariable model into a graph, thereby designing a parameterized summary graph onthe basis of the minimum description length principle. The parameter specifyingthe summary graph is then optimized so that the accuracy of change detection isguaranteed to suppress Type I error probability (probability of raising falsealarms) to be less than a given confidence level. First, we provide atheoretical framework for connecting graph summarization with change detection.Then, we empirically demonstrate its effectiveness on synthetic and realdatasets.</description><author>Shintaro Fukushima, Kenji Yamanishi</author><pubDate>Thu, 30 Nov 2023 16:39:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18694v1</guid></item><item><title>Handling Cost and Constraints with Off-Policy Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2311.18684v1</link><description>By reusing data throughout training, off-policy deep reinforcement learningalgorithms offer improved sample efficiency relative to on-policy approaches.For continuous action spaces, the most popular methods for off-policy learninginclude policy improvement steps where a learned state-action ($Q$) valuefunction is maximized over selected batches of data. These updates are oftenpaired with regularization to combat associated overestimation of $Q$ values.With an eye toward safety, we revisit this strategy in environments with"mixed-sign" reward functions; that is, with reward functions that includeindependent positive (incentive) and negative (cost) terms. This setting iscommon in real-world applications, and may be addressed with or withoutconstraints on the cost terms. We find the combination of functionapproximation and a term that maximizes $Q$ in the policy update to beproblematic in such environments, because systematic errors in value estimationimpact the contributions from the competing terms asymmetrically. This resultsin overemphasis of either incentives or costs and may severely limit learning.We explore two remedies to this issue. First, consistent with prior work, wefind that periodic resetting of $Q$ and policy networks can be used to reducevalue estimation error and improve learning in this setting. Second, weformulate novel off-policy actor-critic methods for both unconstrained andconstrained learning that do not explicitly maximize $Q$ in the policy update.We find that this second approach, when applied to continuous action spaceswith mixed-sign rewards, consistently and significantly outperformsstate-of-the-art methods augmented by resetting. We further find that ourapproach produces agents that are both competitive with popular methods overalland more reliably competent on frequently-studied control problems that do nothave mixed-sign rewards.</description><author>Jared Markowitz, Jesse Silverberg, Gary Collins</author><pubDate>Thu, 30 Nov 2023 16:31:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18684v1</guid></item><item><title>Supporting Human-AI Collaboration in Auditing LLMs with LLMs</title><link>http://arxiv.org/abs/2304.09991v3</link><description>Large language models are becoming increasingly pervasive and ubiquitous insociety via deployment in sociotechnical systems. Yet these language models, beit for classification or generation, have been shown to be biased and behaveirresponsibly, causing harm to people at scale. It is crucial to audit theselanguage models rigorously. Existing auditing tools leverage either or bothhumans and AI to find failures. In this work, we draw upon literature inhuman-AI collaboration and sensemaking, and conduct interviews with researchexperts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiroand Lundberg, 2022), which is powered by a generative large language model(LLM). Through the design process we highlight the importance of sensemakingand human-AI communication to leverage complementary strengths of humans andgenerative models in collaborative auditing. To evaluate the effectiveness ofthe augmented tool, AdaTest++, we conduct user studies with participantsauditing two commercial language models: OpenAI's GPT-3 and Azure's sentimentanalysis model. Qualitative analysis shows that AdaTest++ effectively leverageshuman strengths such as schematization, hypothesis formation and testing.Further, with our tool, participants identified a variety of failures modes,covering 26 different topics over 2 tasks, that have been shown before informal audits and also those previously under-reported.</description><author>Charvi Rastogi, Marco Tulio Ribeiro, Nicholas King, Harsha Nori, Saleema Amershi</author><pubDate>Thu, 30 Nov 2023 16:30:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09991v3</guid></item><item><title>RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance</title><link>http://arxiv.org/abs/2311.18681v1</link><description>Conversational AI tools that can generate and discuss clinically correctradiology reports for a given medical image have the potential to transformradiology. Such a human-in-the-loop radiology assistant could facilitate acollaborative diagnostic process, thus saving time and improving the quality ofreports. Towards this goal, we introduce RaDialog, the first thoroughlyevaluated and publicly available large vision-language model for radiologyreport generation and interactive dialog. RaDialog effectively integratesvisual image features and structured pathology findings with a large languagemodel (LLM) while simultaneously adapting it to a specialized domain usingparameter-efficient fine-tuning. To keep the conversational abilities of theunderlying LLM, we propose a comprehensive, semi-automatically labeled,image-grounded instruct dataset for chest X-ray radiology tasks. By trainingwith this dataset, our method achieves state-of-the-art clinical correctness inreport generation and shows impressive abilities in interactive tasks such ascorrecting reports and answering questions, serving as a foundational steptoward clinical dialog systems. Our code is available on github:https://github.com/ChantalMP/RaDialog.</description><author>Chantal Pellegrini, Ege Özsoy, Benjamin Busam, Nassir Navab, Matthias Keicher</author><pubDate>Thu, 30 Nov 2023 16:28:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18681v1</guid></item><item><title>Some Intriguing Aspects about Lipschitz Continuity of Neural Networks</title><link>http://arxiv.org/abs/2302.10886v3</link><description>Lipschitz continuity is a crucial functional property of any predictivemodel, that naturally governs its robustness, generalisation, as well asadversarial vulnerability. Contrary to other works that focus on obtainingtighter bounds and developing different practical strategies to enforce certainLipschitz properties, we aim to thoroughly examine and characterise theLipschitz behaviour of Neural Networks. Thus, we carry out an empiricalinvestigation in a range of different settings (namely, architectures,datasets, label noise, and more) by exhausting the limits of the simplest andthe most general lower and upper bounds. As a highlight of this investigation,we showcase a remarkable fidelity of the lower Lipschitz bound, identify astriking Double Descent trend in both upper and lower bounds to the Lipschitzand explain the intriguing effects of label noise on function smoothness andgeneralisation.</description><author>Grigory Khromov, Sidak Pal Singh</author><pubDate>Thu, 30 Nov 2023 16:27:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10886v3</guid></item><item><title>DQSSA: A Quantum-Inspired Solution for Maximizing Influence in Online Social Networks (Student Abstract)</title><link>http://arxiv.org/abs/2311.18676v1</link><description>Influence Maximization is the task of selecting optimal nodes maximising theinfluence spread in social networks. This study proposes a DiscretizedQuantum-based Salp Swarm Algorithm (DQSSA) for optimizing influence diffusionin social networks. By discretizing meta-heuristic algorithms and infusing themwith quantum-inspired enhancements, we address issues like prematureconvergence and low efficacy. The proposed method, guided by quantumprinciples, offers a promising solution for Influence Maximisation. Experimentson four real-world datasets reveal DQSSA's superior performance as compared toestablished cutting-edge algorithms.</description><author>Aryaman Rao, Parth Singh, Dinesh Kumar Vishwakarma, Mukesh Prasad</author><pubDate>Thu, 30 Nov 2023 16:23:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18676v1</guid></item><item><title>Top-Down Knowledge Compilation for Counting Modulo Theories</title><link>http://arxiv.org/abs/2306.04541v2</link><description>Propositional model counting (#SAT) can be solved efficiently when the inputformula is in deterministic decomposable negation normal form (d-DNNF).Translating an arbitrary formula into a representation that allows inferencetasks, such as counting, to be performed efficiently, is called knowledgecompilation. Top-down knowledge compilation is a state-of-the-art technique forsolving #SAT problems that leverages the traces of exhaustive DPLL search toobtain d-DNNF representations. While knowledge compilation is well studied forpropositional approaches, knowledge compilation for the (quantifier free)counting modulo theory setting (#SMT) has been studied to a much lesser degree.In this paper, we discuss compilation strategies for #SMT. We specificallyadvocate for a top-down compiler based on the traces of exhaustive DPLL(T)search.</description><author>Vincent Derkinderen, Pedro Zuidberg Dos Martires, Samuel Kolb, Paolo Morettin</author><pubDate>Thu, 30 Nov 2023 16:21:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04541v2</guid></item><item><title>Cascaded Interaction with Eroded Deep Supervision for Salient Object Detection</title><link>http://arxiv.org/abs/2311.18675v1</link><description>Deep convolutional neural networks have been widely applied in salient objectdetection and have achieved remarkable results in this field. However, existingmodels suffer from information distortion caused by interpolation duringup-sampling and down-sampling. In response to this drawback, this articlestarts from two directions in the network: feature and label. On the one hand,a novel cascaded interaction network with a guidance module named global-localaligned attention (GAA) is designed to reduce the negative impact ofinterpolation on the feature side. On the other hand, a deep supervisionstrategy based on edge erosion is proposed to reduce the negative guidance oflabel interpolation on lateral output. Extensive experiments on five populardatasets demonstrate the superiority of our method.</description><author>Hewen Xiao, Jie Mei, Guangfu Ma, Weiren Wu</author><pubDate>Thu, 30 Nov 2023 16:20:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18675v1</guid></item><item><title>A Comparison Between Invariant and Equivariant Classical and Quantum Graph Neural Networks</title><link>http://arxiv.org/abs/2311.18672v1</link><description>Machine learning algorithms are heavily relied on to understand the vastamounts of data from high-energy particle collisions at the CERN Large HadronCollider (LHC). The data from such collision events can naturally berepresented with graph structures. Therefore, deep geometric methods, such asgraph neural networks (GNNs), have been leveraged for various data analysistasks in high-energy physics. One typical task is jet tagging, where jets areviewed as point clouds with distinct features and edge connections betweentheir constituent particles. The increasing size and complexity of the LHCparticle datasets, as well as the computational models used for their analysis,greatly motivate the development of alternative fast and efficientcomputational paradigms such as quantum computation. In addition, to enhancethe validity and robustness of deep networks, one can leverage the fundamentalsymmetries present in the data through the use of invariant inputs andequivariant layers. In this paper, we perform a fair and comprehensivecomparison between classical graph neural networks (GNNs) and equivariant graphneural networks (EGNNs) and their quantum counterparts: quantum graph neuralnetworks (QGNNs) and equivariant quantum graph neural networks (EQGNN). Thefour architectures were benchmarked on a binary classification task to classifythe parton-level particle initiating the jet. Based on their AUC scores, thequantum networks were shown to outperform the classical networks. However,seeing the computational advantage of the quantum networks in practice may haveto wait for the further development of quantum technology and its associatedAPIs.</description><author>Roy T. Forestano, Marçal Comajoan Cara, Gopal Ramesh Dahale, Zhongtian Dong, Sergei Gleyzer, Daniel Justice, Kyoungchul Kong, Tom Magorsch, Konstantin T. Matchev, Katia Matcheva, Eyup B. Unlu</author><pubDate>Thu, 30 Nov 2023 16:19:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18672v1</guid></item><item><title>Action Recognition in Video Recordings from Gynecologic Laparoscopy</title><link>http://arxiv.org/abs/2311.18666v1</link><description>Action recognition is a prerequisite for many applications in laparoscopicvideo analysis including but not limited to surgical training, operation roomplanning, follow-up surgery preparation, post-operative surgical assessment,and surgical outcome estimation. However, automatic action recognition inlaparoscopic surgeries involves numerous challenges such as (I) cross-actionand intra-action duration variation, (II) relevant content distortion due tosmoke, blood accumulation, fast camera motions, organ movements, objectocclusion, and (III) surgical scene variations due to different illuminationsand viewpoints. Besides, action annotations in laparoscopy surgeries arelimited and expensive due to requiring expert knowledge. In this study, wedesign and evaluate a CNN-RNN architecture as well as a customizedtraining-inference framework to deal with the mentioned challenges inlaparoscopic surgery action recognition. Using stacked recurrent layers, ourproposed network takes advantage of inter-frame dependencies to negate thenegative effect of content distortion and variation in action recognition.Furthermore, our proposed frame sampling strategy effectively manages theduration variations in surgical actions to enable action recognition with hightemporal resolution. Our extensive experiments confirm the superiority of ourproposed method in action recognition compared to static CNNs.</description><author>Sahar Nasirihaghighi, Negin Ghamsarian, Daniela Stefanics, Klaus Schoeffmann, Heinrich Husslein</author><pubDate>Thu, 30 Nov 2023 16:15:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18666v1</guid></item><item><title>Pose Estimation and Tracking for ASIST</title><link>http://arxiv.org/abs/2311.18665v1</link><description>Aircraft Ship Integrated Secure and Traverse (ASIST) is a system designed toarrest helicopters safely and efficiently on ships. Originally, a precisionHelicopter Position Sensing Equipment (HPSE) tracked and monitored the positionof the helicopter relative to the Rapid Securing Device (RSD). However, usingthe HPSE component was determined to be infeasible in the transition of theASIST system due to the hardware installation requirements. As a result,sailors track the position of the helicopters with their eyes with no sensor orartificially intelligent decision aid. Manually tracking the helicopter takesadditional time and makes recoveries more difficult, especially at high seastates. Performing recoveries without the decision aid leads to higheruncertainty and cognitive load. PETA (Pose Estimation and Tracking for ASIST)is a research effort to create a helicopter tracking system prototype withouthardware installation requirements for ASIST system operators. Its overall goalis to improve situational awareness and reduce operator uncertainty withrespect to the aircrafts position relative to the RSD, and consequentlyincrease the allowable landing area. The authors produced a prototype systemcapable of tracking helicopters with respect to the RSD. The software includeda helicopter pose estimation component, camera pose estimation component, and auser interface component. PETA demonstrated the potential for state-of-the-artcomputer vision algorithms Faster R-CNN and HRNet (High-Resolution Network) tobe used to estimate the pose of helicopters in real-time, returning ASIST toits originally intended capability. PETA also demonstrated that traditionalmethods of encoder-decoders could be used to estimate the orientation of thehelicopter and could be used to confirm the output from HRNet.</description><author>Ari Goodman, Gurpreet Singh, Ryan O'Shea, Peter Teague, James Hing</author><pubDate>Thu, 30 Nov 2023 16:15:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18665v1</guid></item><item><title>Multi-task learning with cross-task consistency for improved depth estimation in colonoscopy</title><link>http://arxiv.org/abs/2311.18664v1</link><description>Colonoscopy screening is the gold standard procedure for assessingabnormalities in the colon and rectum, such as ulcers and cancerous polyps.Measuring the abnormal mucosal area and its 3D reconstruction can help quantifythe surveyed area and objectively evaluate disease burden. However, due to thecomplex topology of these organs and variable physical conditions, for example,lighting, large homogeneous texture, and image modality estimating distancefrom the camera aka depth) is highly challenging. Moreover, most colonoscopicvideo acquisition is monocular, making the depth estimation a non-trivialproblem. While methods in computer vision for depth estimation have beenproposed and advanced on natural scene datasets, the efficacy of thesetechniques has not been widely quantified on colonoscopy datasets. As thecolonic mucosa has several low-texture regions that are not well pronounced,learning representations from an auxiliary task can improve salient featureextraction, allowing estimation of accurate camera depths. In this work, wepropose to develop a novel multi-task learning (MTL) approach with a sharedencoder and two decoders, namely a surface normal decoder and a depth estimatordecoder. Our depth estimator incorporates attention mechanisms to enhanceglobal context awareness. We leverage the surface normal prediction to improvegeometric feature extraction. Also, we apply a cross-task consistency lossamong the two geometrically related tasks, surface normal and camera depth. Wedemonstrate an improvement of 14.17% on relative error and 10.4% improvement on$\delta_{1}$ accuracy over the most accurate baseline state-of-the-art BTSapproach. All experiments are conducted on a recently released C3VD dataset;thus, we provide a first benchmark of state-of-the-art methods.</description><author>Pedro Esteban Chavarrias Solano, Andrew Bulpitt, Venkataraman Subramanian, Sharib Ali</author><pubDate>Thu, 30 Nov 2023 16:13:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18664v1</guid></item><item><title>Choosing the parameter of the Fermat distance: navigating geometry and noise</title><link>http://arxiv.org/abs/2311.18663v1</link><description>The Fermat distance has been recently established as a useful tool formachine learning tasks when a natural distance is not directly available to thepractitioner or to improve the results given by Euclidean distances byexploding the geometrical and statistical properties of the dataset. Thisdistance depends on a parameter $\alpha$ that greatly impacts the performanceof subsequent tasks. Ideally, the value of $\alpha$ should be large enough tonavigate the geometric intricacies inherent to the problem. At the same, itshould remain restrained enough to sidestep any deleterious ramificationsstemming from noise during the process of distance estimation. We study boththeoretically and through simulations how to select this parameter.</description><author>Frédéric Chazal, Laure Ferraris, Pablo Groisman, Matthieu Jonckheere, Frédéric Pascal, Facundo Sapienza</author><pubDate>Thu, 30 Nov 2023 16:11:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18663v1</guid></item><item><title>Solving the Team Orienteering Problem with Transformers</title><link>http://arxiv.org/abs/2311.18662v1</link><description>Route planning for a fleet of vehicles is an important task in applicationssuch as package delivery, surveillance, or transportation. This problem isusually modeled as a Combinatorial Optimization problem named as TeamOrienteering Problem. The most popular Team Orienteering Problem solvers aremainly based on either linear programming, which provides accurate solutions byemploying a large computation time that grows with the size of the problem, orheuristic methods, which usually find suboptimal solutions in a shorter amountof time. In this paper, a multi-agent route planning system capable of solvingthe Team Orienteering Problem in a very fast and accurate manner is presented.The proposed system is based on a centralized Transformer neural network thatcan learn to encode the scenario (modeled as a graph) and the context of theagents to provide fast and accurate solutions. Several experiments have beenperformed to demonstrate that the presented system can outperform most of thestate-of-the-art works in terms of computation speed. In addition, the code ispublicly available at \url{http://gti.ssr.upm.es/data}.</description><author>Daniel Fuertes, Carlos R. del-Blanco, Fernando Jaureguizar, Narciso García</author><pubDate>Thu, 30 Nov 2023 16:10:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18662v1</guid></item><item><title>Learning Part Segmentation from Synthetic Animals</title><link>http://arxiv.org/abs/2311.18661v1</link><description>Semantic part segmentation provides an intricate and interpretableunderstanding of an object, thereby benefiting numerous downstream tasks.However, the need for exhaustive annotations impedes its usage across diverseobject types. This paper focuses on learning part segmentation from syntheticanimals, leveraging the Skinned Multi-Animal Linear (SMAL) models to scale upexisting synthetic data generated by computer-aided design (CAD) animal models.Compared to CAD models, SMAL models generate data with a wider range of posesobserved in real-world scenarios. As a result, our first contribution is toconstruct a synthetic animal dataset of tigers and horses with more posediversity, termed Synthetic Animal Parts (SAP). We then benchmark Syn-to-Realanimal part segmentation from SAP to PartImageNet, namely SynRealPart, withexisting semantic segmentation domain adaptation methods and further improvethem as our second contribution. Concretely, we examine three Syn-to-Realadaptation methods but observe relative performance drop due to the innatedifference between the two tasks. To address this, we propose a simple yeteffective method called Class-Balanced Fourier Data Mixing (CB-FDM). FourierData Mixing aligns the spectral amplitudes of synthetic images with realimages, thereby making the mixed images have more similar frequency content toreal images. We further use Class-Balanced Pseudo-Label Re-Weighting toalleviate the imbalanced class distribution. We demonstrate the efficacy ofCB-FDM on SynRealPart over previous methods with significant performanceimprovements. Remarkably, our third contribution is to reveal that the learnedparts from synthetic tiger and horse are transferable across all quadrupeds inPartImageNet, further underscoring the utility and potential applications ofanimal part segmentation.</description><author>Jiawei Peng, Ju He, Prakhar Kaushik, Zihao Xiao, Jiteng Mu, Alan Yuille</author><pubDate>Thu, 30 Nov 2023 16:10:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18661v1</guid></item><item><title>ANPL: Towards Natural Programming with Interactive Decomposition</title><link>http://arxiv.org/abs/2305.18498v2</link><description>Though LLMs are capable of generating plausible programs, it's challenging tointeract with the LLMs further to revise the program, especially if the user'sspecific requirements are different from the initial proposal. In this paper,we introduce ANPL, an interactive programming system that ensures users canalways refine the generated code towards their specific programmatic intentsvia structured decompositions. Borrowing the paradigm of sketching from programsynthesis, an ANPL program consists of a set of input-outputs that it mustsatisfy, a ``sketch'' -- control/data flow expressed in precise code (e.g.Python), and ``holes'' -- sub-modules to be implemented by the LLM specifiedwith natural language. The user revises an ANPL program by either modifying thesketch, changing the language used to describe the holes, or providingadditional input-outputs to a particular hole, turning it into a sub-ANPLprogram that can be solved recursively. This workflow allows the users tooffload programming burdens to the LLM as much as possible while retaining theability to pinpoint and resolve bugs locally, without exposing the rest of theprogram to the LLM. We deploy ANPL on the Abstraction and Reasoning Corpus(ARC), a set of unique tasks that are challenging for state-of-the-art AIsystems, showing it outperforms baseline programming systems that (a) withoutthe ability to decompose tasks interactively and (b) without the guarantee thatthe modules can be correctly composed together. Additional evaluations on APPS,HumanEval, and real-world programming tasks have validated that the ANPLframework is applicable to multiple programming domains. We release the ANPLsolutions to the ARC tasks as a dataset, providing insights into how humansdecompose novel tasks programmatically. See our code athttps://iprc-dip.github.io/ANPL/.</description><author>Di Huang, Ziyuan Nan, Xing Hu, Pengwei Jin, Shaohui Peng, Yuanbo Wen, Rui Zhang, Zidong Du, Qi Guo, Yewen Pu, Yunji Chen</author><pubDate>Thu, 30 Nov 2023 16:08:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18498v2</guid></item><item><title>ArcMMLU: A Library and Information Science Benchmark for Large Language Models</title><link>http://arxiv.org/abs/2311.18658v1</link><description>In light of the rapidly evolving capabilities of large language models(LLMs), it becomes imperative to develop rigorous domain-specific evaluationbenchmarks to accurately assess their capabilities. In response to this need,this paper introduces ArcMMLU, a specialized benchmark tailored for the Library&amp; Information Science (LIS) domain in Chinese. This benchmark aims to measurethe knowledge and reasoning capability of LLMs within four key sub-domains:Archival Science, Data Science, Library Science, and Information Science.Following the format of MMLU/CMMLU, we collected over 6,000 high-qualityquestions for the compilation of ArcMMLU. This extensive compilation canreflect the diverse nature of the LIS domain and offer a robust foundation forLLM evaluation. Our comprehensive evaluation reveals that while most mainstreamLLMs achieve an average accuracy rate above 50% on ArcMMLU, there remains anotable performance gap, suggesting substantial headroom for refinement in LLMcapabilities within the LIS domain. Further analysis explores the effectivenessof few-shot examples on model performance and highlights challenging questionswhere models consistently underperform, providing valuable insights fortargeted improvements. ArcMMLU fills a critical gap in LLM evaluations withinthe Chinese LIS domain and paves the way for future development of LLMstailored to this specialized area.</description><author>Shitou Zhang, Zuchao Li, Xingshen Liu, Liming Yang, Ping Wang</author><pubDate>Thu, 30 Nov 2023 16:08:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18658v1</guid></item><item><title>Detailed Human-Centric Text Description-Driven Large Scene Synthesis</title><link>http://arxiv.org/abs/2311.18654v1</link><description>Text-driven large scene image synthesis has made significant progress withdiffusion models, but controlling it is challenging. While using additionalspatial controls with corresponding texts has improved the controllability oflarge scene synthesis, it is still challenging to faithfully reflect detailedtext descriptions without user-provided controls. Here, we proposeDetText2Scene, a novel text-driven large-scale image synthesis with highfaithfulness, controllability, and naturalness in a global context for thedetailed human-centric text description. Our DetText2Scene consists of 1)hierarchical keypoint-box layout generation from the detailed description byleveraging large language model (LLM), 2) view-wise conditioned joint diffusionprocess to synthesize a large scene from the given detailed text withLLM-generated grounded keypoint-box layout and 3) pixel perturbation-basedpyramidal interpolation to progressively refine the large scene for globalcoherence. Our DetText2Scene significantly outperforms prior arts intext-to-large scene synthesis qualitatively and quantitatively, demonstratingstrong faithfulness with detailed descriptions, superior controllability, andexcellent naturalness in a global context.</description><author>Gwanghyun Kim, Dong Un Kang, Hoigi Seo, Hayeon Kim, Se Young Chun</author><pubDate>Thu, 30 Nov 2023 16:04:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18654v1</guid></item></channel></rss>