<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 18 Sep 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Sparse Autoencoders Find Highly Interpretable Features in Language Models</title><link>http://arxiv.org/abs/2309.08600v1</link><description>One of the roadblocks to a better understanding of neural networks' internalsis \textit{polysemanticity}, where neurons appear to activate in multiple,semantically distinct contexts. Polysemanticity prevents us from identifyingconcise, human-understandable explanations for what neural networks are doinginternally. One hypothesised cause of polysemanticity is\textit{superposition}, where neural networks represent more features than theyhave neurons by assigning features to an overcomplete set of directions inactivation space, rather than to individual neurons. Here, we attempt toidentify those directions, using sparse autoencoders to reconstruct theinternal activations of a language model. These autoencoders learn sets ofsparsely activating features that are more interpretable and monosemantic thandirections identified by alternative approaches, where interpretability ismeasured by automated methods. Ablating these features enables precise modelediting, for example, by removing capabilities such as pronoun prediction,while disrupting model behaviour less than prior techniques. This workindicates that it is possible to resolve superposition in language models usinga scalable, unsupervised method. Our method may serve as a foundation forfuture mechanistic interpretability work, which we hope will enable greatermodel transparency and steerability.</description><author>Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, Lee Sharkey</author><pubDate>Fri, 15 Sep 2023 18:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08600v1</guid></item><item><title>Robust e-NeRF: NeRF from Sparse &amp; Noisy Events under Non-Uniform Motion</title><link>http://arxiv.org/abs/2309.08596v1</link><description>Event cameras offer many advantages over standard cameras due to theirdistinctive principle of operation: low power, low latency, high temporalresolution and high dynamic range. Nonetheless, the success of many downstreamvisual applications also hinges on an efficient and effective scenerepresentation, where Neural Radiance Field (NeRF) is seen as the leadingcandidate. Such promise and potential of event cameras and NeRF inspired recentworks to investigate on the reconstruction of NeRF from moving event cameras.However, these works are mainly limited in terms of the dependence on dense andlow-noise event streams, as well as generalization to arbitrary contrastthreshold values and camera speed profiles. In this work, we propose Robuste-NeRF, a novel method to directly and robustly reconstruct NeRFs from movingevent cameras under various real-world conditions, especially from sparse andnoisy events generated under non-uniform motion. It consists of two keycomponents: a realistic event generation model that accounts for variousintrinsic parameters (e.g. time-independent, asymmetric threshold andrefractory period) and non-idealities (e.g. pixel-to-pixel thresholdvariation), as well as a complementary pair of normalized reconstruction lossesthat can effectively generalize to arbitrary speed profiles and intrinsicparameter values without such prior knowledge. Experiments on real and novelrealistically simulated sequences verify our effectiveness. Our code, syntheticdataset and improved event simulator are public.</description><author>Weng Fei Low, Gim Hee Lee</author><pubDate>Fri, 15 Sep 2023 18:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08596v1</guid></item><item><title>"Merge Conflicts!" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs</title><link>http://arxiv.org/abs/2309.08594v1</link><description>Large language models (LLMs) acquire extensive knowledge during pre-training,known as their parametric knowledge. However, in order to remain up-to-date andalign with human instructions, LLMs inevitably require external knowledgeduring their interactions with users. This raises a crucial question: How willLLMs respond when external knowledge interferes with their parametricknowledge? To investigate this question, we propose a framework thatsystematically elicits LLM parametric knowledge and introduces externalknowledge. Specifically, we uncover the impacts by constructing a parametricknowledge graph to reveal the different knowledge structures of LLMs, andintroduce external knowledge through distractors of varying degrees, methods,positions, and formats. Our experiments on both black-box and open-sourcemodels demonstrate that LLMs tend to produce responses that deviate from theirparametric knowledge, particularly when they encounter direct conflicts orconfounding changes of information within detailed contexts. We also find thatwhile LLMs are sensitive to the veracity of external knowledge, they can stillbe distracted by unrelated information. These findings highlight the risk ofhallucination when integrating external knowledge, even indirectly, duringinteractions with current LLMs. All the data and results are publiclyavailable.</description><author>Cheng Qian, Xinran Zhao, Sherry Tongshuang Wu</author><pubDate>Fri, 15 Sep 2023 18:47:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08594v1</guid></item><item><title>Attention-Only Transformers and Implementing MLPs with Attention Heads</title><link>http://arxiv.org/abs/2309.08593v1</link><description>The transformer architecture is widely used in machine learning models andconsists of two alternating sublayers: attention heads and MLPs. We prove thatan MLP neuron can be implemented by a masked attention head with internaldimension 1 so long as the MLP's activation function comes from a restrictedclass including SiLU and close approximations of ReLU and GeLU. This allows oneto convert an MLP-and-attention transformer into an attention-only transformerat the cost of greatly increasing the number of attention heads. We also provethat attention heads can perform the components of an MLP (lineartransformations and activation functions) separately. Finally, we prove thatattention heads can encode arbitrary masking patterns in their weight matricesto within arbitrarily small error.</description><author>Robert Huben, Valerie Morris</author><pubDate>Fri, 15 Sep 2023 18:47:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08593v1</guid></item><item><title>Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings</title><link>http://arxiv.org/abs/2309.08591v1</link><description>Large language models (LLMs) are highly adept at question answering andreasoning tasks, but when reasoning in situational context, human expectationsvary depending on the relevant cultural common ground. As human languages areassociated with diverse cultures, LLMs should also be culturally-diversereasoners. In this paper, we study the ability of a wide range ofstate-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and sayingsin a conversational context. Our experiments reveal that: (1) mLLMs 'knows'limited proverbs and memorizing proverbs does not mean understanding themwithin a conversational context; (2) mLLMs struggle to reason with figurativeproverbs and sayings, and when asked to select the wrong answer (instead ofasking it to select the correct answer); and (3) there is a "culture gap" inmLLMs when reasoning about proverbs and sayings translated from otherlanguages. We construct and release our evaluation dataset MAPS (MulticultrAlProverbs and Sayings) for proverb understanding with conversational context forsix different languages.</description><author>Chen Cecilia Liu, Fajri Koto, Timothy Baldwin, Iryna Gurevych</author><pubDate>Fri, 15 Sep 2023 18:45:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08591v1</guid></item><item><title>Neural Machine Translation Models Can Learn to be Few-shot Learners</title><link>http://arxiv.org/abs/2309.08590v1</link><description>The emergent ability of Large Language Models to use a small number ofexamples to learn to perform in novel domains and tasks, also called in-contextlearning (ICL). In this work, we show that a much smaller model can be trainedto perform ICL by fine-tuning towards a specialized training objective,exemplified on the task of domain adaptation for neural machine translation.With this capacity for ICL, the model can take advantage of relevant few-shotexamples to adapt its output towards the domain. We compare the quality of thisdomain adaptation to traditional supervised techniques and ICL with a40B-parameter Large Language Model. Our approach allows efficient batchinference on a mix of domains and outperforms state-of-the-art baselines interms of both translation quality and immediate adaptation rate, i.e. theability to reproduce a specific term after being shown a single example.</description><author>Raphael Reinauer, Patrick Simianer, Kaden Uhlig, Johannes E. M. Mosig, Joern Wuebker</author><pubDate>Fri, 15 Sep 2023 18:44:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08590v1</guid></item><item><title>Chain-of-Thought Reasoning is a Policy Improvement Operator</title><link>http://arxiv.org/abs/2309.08589v1</link><description>Large language models have astounded the world with fascinating newcapabilities. However, they currently lack the ability to teach themselves newskills, relying instead on being trained on large amounts of human-generateddata. We introduce SECToR (Self-Education via Chain-of-Thought Reasoning), aproof-of-concept demonstration that language models can successfully teachthemselves new skills using chain-of-thought reasoning. Inspired by previouswork in both reinforcement learning (Silver et al., 2017) and human cognition(Kahneman, 2011), SECToR first uses chain-of-thought reasoning to slowly thinkits way through problems. SECToR then fine-tunes the model to generate thosesame answers, this time without using chain-of-thought reasoning. Languagemodels trained via SECToR autonomously learn to add up to 29-digit numberswithout any access to any ground truth examples beyond an initial supervisedfine-tuning phase consisting only of numbers with 6 or fewer digits. Ourcentral hypothesis is that chain-of-thought reasoning can act as a policyimprovement operator, analogously to how Monte-Carlo Tree Search is used inAlphaZero. We hope that this research can lead to new directions in whichlanguage models can learn to teach themselves without the need for humandemonstrations.</description><author>Hugh Zhang, David C. Parkes</author><pubDate>Fri, 15 Sep 2023 18:44:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08589v1</guid></item><item><title>Robust Frame-to-Frame Camera Rotation Estimation in Crowded Scenes</title><link>http://arxiv.org/abs/2309.08588v1</link><description>We present an approach to estimating camera rotation in crowded, real-worldscenes from handheld monocular video. While camera rotation estimation is awell-studied problem, no previous methods exhibit both high accuracy andacceptable speed in this setting. Because the setting is not addressed well byother datasets, we provide a new dataset and benchmark, with high-accuracy,rigorously verified ground truth, on 17 video sequences. Methods developed forwide baseline stereo (e.g., 5-point methods) perform poorly on monocular video.On the other hand, methods used in autonomous driving (e.g., SLAM) leveragespecific sensor setups, specific motion models, or local optimizationstrategies (lagging batch processing) and do not generalize well to handheldvideo. Finally, for dynamic scenes, commonly used robustification techniqueslike RANSAC require large numbers of iterations, and become prohibitively slow.We introduce a novel generalization of the Hough transform on SO(3) toefficiently and robustly find the camera rotation most compatible with opticalflow. Among comparably fast methods, ours reduces error by almost 50\% over thenext best, and is more accurate than any method, irrespective of speed. Thisrepresents a strong new performance point for crowded scenes, an importantsetting for computer vision. The code and the dataset are available athttps://fabiendelattre.com/robust-rotation-estimation.</description><author>Fabien Delattre, David Dirnfeld, Phat Nguyen, Stephen Scarano, Michael J. Jones, Pedro Miraldo, Erik Learned-Miller</author><pubDate>Fri, 15 Sep 2023 18:44:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08588v1</guid></item><item><title>Compositional Foundation Models for Hierarchical Planning</title><link>http://arxiv.org/abs/2309.08587v1</link><description>To make effective decisions in novel environments with long-horizon goals, itis crucial to engage in hierarchical reasoning across spatial and temporalscales. This entails planning abstract subgoal sequences, visually reasoningabout the underlying plans, and executing actions in accordance with thedevised plan through visual-motor control. We propose Compositional FoundationModels for Hierarchical Planning (HiP), a foundation model which leveragesmultiple expert foundation model trained on language, vision and action dataindividually jointly together to solve long-horizon tasks. We use a largelanguage model to construct symbolic plans that are grounded in the environmentthrough a large video diffusion model. Generated video plans are then groundedto visual-motor control, through an inverse dynamics model that infers actionsfrom generated videos. To enable effective reasoning within this hierarchy, weenforce consistency between the models via iterative refinement. We illustratethe efficacy and adaptability of our approach in three different long-horizontable-top manipulation tasks.</description><author>Anurag Ajay, Seungwook Han, Yilun Du, Shaung Li, Abhi Gupta, Tommi Jaakkola, Josh Tenenbaum, Leslie Kaelbling, Akash Srivastava, Pulkit Agrawal</author><pubDate>Fri, 15 Sep 2023 18:44:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08587v1</guid></item><item><title>Replacing softmax with ReLU in Vision Transformers</title><link>http://arxiv.org/abs/2309.08586v1</link><description>Previous research observed accuracy degradation when replacing the attentionsoftmax with a point-wise activation such as ReLU. In the context of visiontransformers, we find that this degradation is mitigated when dividing bysequence length. Our experiments training small to large vision transformers onImageNet-21k indicate that ReLU-attention can approach or match the performanceof softmax-attention in terms of scaling behavior as a function of compute.</description><author>Mitchell Wortsman, Jaehoon Lee, Justin Gilmer, Simon Kornblith</author><pubDate>Fri, 15 Sep 2023 18:43:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08586v1</guid></item><item><title>Viewpoint Integration and Registration with Vision Language Foundation Model for Image Change Understanding</title><link>http://arxiv.org/abs/2309.08585v1</link><description>Recently, the development of pre-trained vision language foundation models(VLFMs) has led to remarkable performance in many tasks. However, these modelstend to have strong single-image understanding capability but lack the abilityto understand multiple images. Therefore, they cannot be directly applied tocope with image change understanding (ICU), which requires models to captureactual changes between multiple images and describe them in language. In thispaper, we discover that existing VLFMs perform poorly when applied directly toICU because of the following problems: (1) VLFMs generally learn the globalrepresentation of a single image, while ICU requires capturing nuances betweenmultiple images. (2) The ICU performance of VLFMs is significantly affected byviewpoint variations, which is caused by the altered relationships betweenobjects when viewpoint changes. To address these problems, we propose aViewpoint Integration and Registration method. Concretely, we introduce a fusedadapter image encoder that fine-tunes pre-trained encoders by insertingdesigned trainable adapters and fused adapters, to effectively capture nuancesbetween image pairs. Additionally, a viewpoint registration flow and a semanticemphasizing module are designed to reduce the performance degradation caused byviewpoint variations in the visual and semantic space, respectively.Experimental results on CLEVR-Change and Spot-the-Diff demonstrate that ourmethod achieves state-of-the-art performance in all metrics.</description><author>Xiaonan Lu, Jianlong Yuan, Ruigang Niu, Yuan Hu, Fan Wang</author><pubDate>Fri, 15 Sep 2023 18:41:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08585v1</guid></item><item><title>ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer</title><link>http://arxiv.org/abs/2309.08583v1</link><description>While state-of-the-art language models excel at the style transfer task,current work does not address explainability of style transfer systems.Explanations could be generated using large language models such as GPT-3.5 andGPT-4, but the use of such complex systems is inefficient when smaller, widelydistributed, and transparent alternatives are available. We propose a frameworkto augment and improve a formality style transfer dataset with explanations viamodel distillation from ChatGPT. To further refine the generated explanations,we propose a novel way to incorporate scarce expert human feedback usingin-context learning (ICLEF: In-Context Learning from Expert Feedback) byprompting ChatGPT to act as a critic to its own outputs. We use the resultingdataset of 9,960 explainable formality style transfer instances (e-GYAFC) toshow that current openly distributed instruction-tuned models (and, in somesettings, ChatGPT) perform poorly on the task, and that fine-tuning on ourhigh-quality dataset leads to significant improvements as shown by automaticevaluation. In human evaluation, we show that models much smaller than ChatGPTfine-tuned on our data align better with expert preferences. Finally, wediscuss two potential applications of models fine-tuned on the explainablestyle transfer task: interpretable authorship verification and interpretableadversarial attacks on AI-generated text detectors.</description><author>Arkadiy Saakyan, Smaranda Muresan</author><pubDate>Fri, 15 Sep 2023 18:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08583v1</guid></item><item><title>Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West</title><link>http://arxiv.org/abs/2309.08573v1</link><description>Large Language Models (LLMs), now used daily by millions of users, can encodesocietal biases, exposing their users to representational harms. A large bodyof scholarship on LLM bias exists but it predominantly adopts a Western-centricframe and attends comparatively less to bias levels and potential harms in theGlobal South. In this paper, we quantify stereotypical bias in popular LLMsaccording to an Indian-centric frame and compare bias levels between the Indianand Western contexts. To do this, we develop a novel dataset which we callIndian-BhED (Indian Bias Evaluation Dataset), containing stereotypical andanti-stereotypical examples for caste and religion contexts. We find that themajority of LLMs tested are strongly biased towards stereotypes in the Indiancontext, especially as compared to the Western context. We finally investigateInstruction Prompting as a simple intervention to mitigate such bias and findthat it significantly reduces both stereotypical and anti-stereotypical biasesin the majority of cases for GPT-3.5. The findings of this work highlight theneed for including more diverse voices when evaluating LLMs.</description><author>Khyati Khandelwal, Manuel Tonneau, Andrew M. Bean, Hannah Rose Kirk, Scott A. Hale</author><pubDate>Fri, 15 Sep 2023 18:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08573v1</guid></item><item><title>A Bayesian Approach to Robust Inverse Reinforcement Learning</title><link>http://arxiv.org/abs/2309.08571v1</link><description>We consider a Bayesian approach to offline model-based inverse reinforcementlearning (IRL). The proposed framework differs from existing offlinemodel-based IRL approaches by performing simultaneous estimation of theexpert's reward function and subjective model of environment dynamics. We makeuse of a class of prior distributions which parameterizes how accurate theexpert's model of the environment is to develop efficient algorithms toestimate the expert's reward and subjective dynamics in high-dimensionalsettings. Our analysis reveals a novel insight that the estimated policyexhibits robust performance when the expert is believed (a priori) to have ahighly accurate model of the environment. We verify this observation in theMuJoCo environments and show that our algorithms outperform state-of-the-artoffline IRL algorithms.</description><author>Ran Wei, Siliang Zeng, Chenliang Li, Alfredo Garcia, Anthony McDonald, Mingyi Hong</author><pubDate>Fri, 15 Sep 2023 18:37:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08571v1</guid></item><item><title>Neural Network Driven, Interactive Design for Nonlinear Optical Molecules Based on Group Contribution Method</title><link>http://arxiv.org/abs/2309.08570v1</link><description>A Lewis-mode group contribution method (LGC) -- multi-stage Bayesian neuralnetwork (msBNN) -- evolutionary algorithm (EA) framework is reported forrational design of D-Pi-A type organic small-molecule nonlinear opticalmaterials is presented. Upon combination of msBNN and corrected Lewis-modegroup contribution method (cLGC), different optical properties of molecules areafforded accurately and efficiently - by using only a small data set fortraining. Moreover, by employing the EA model designed specifically for LGC,structural search is well achievable. The logical origins of the wellperformance of the framework are discussed in detail. Considering that such atheory guided, machine learning framework combines chemical principles anddata-driven tools, most likely, it will be proven efficient to solve moleculardesign related problems in wider fields.</description><author>Jinming Fan, Chao Qian, Shaodong Zhou</author><pubDate>Fri, 15 Sep 2023 18:36:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08570v1</guid></item><item><title>Local Differential Privacy in Graph Neural Networks: a Reconstruction Approach</title><link>http://arxiv.org/abs/2309.08569v1</link><description>Graph Neural Networks have achieved tremendous success in modeling complexgraph data in a variety of applications. However, there are limited studiesinvestigating privacy protection in GNNs. In this work, we propose a learningframework that can provide node privacy at the user level, while incurring lowutility loss. We focus on a decentralized notion of Differential Privacy,namely Local Differential Privacy, and apply randomization mechanisms toperturb both feature and label data at the node level before the data iscollected by a central server for model training. Specifically, we investigatethe application of randomization mechanisms in high-dimensional featuresettings and propose an LDP protocol with strict privacy guarantees. Based onfrequency estimation in statistical analysis of randomized data, we developreconstruction methods to approximate features and labels from perturbed data.We also formulate this learning framework to utilize frequency estimates ofgraph clusters to supervise the training procedure at a sub-graph level.Extensive experiments on real-world and semi-synthetic datasets demonstrate thevalidity of our proposed model.</description><author>Karuna Bhaila, Wen Huang, Yongkai Wu, Xintao Wu</author><pubDate>Fri, 15 Sep 2023 18:35:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08569v1</guid></item><item><title>How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?</title><link>http://arxiv.org/abs/2309.08565v1</link><description>Customizing machine translation models to comply with fine-grained attributessuch as formality has seen tremendous progress recently. However, currentapproaches mostly rely on at least some supervised data with attributeannotation. Data scarcity therefore remains a bottleneck to democratizing suchcustomization possibilities to a wider range of languages, lower-resource onesin particular. Given recent progress in pretrained massively multilingualtranslation models, we use them as a foundation to transfer the attributecontrolling capabilities to languages without supervised data. In this work, wepresent a comprehensive analysis of transferring attribute controllers based ona pretrained NLLB-200 model. We investigate both training- and inference-timecontrol techniques under various data scenarios, and uncover their relativestrengths and weaknesses in zero-shot performance and domain robustness. Weshow that both paradigms are complementary, as shown by consistent improvementson 5 zero-shot directions. Moreover, a human evaluation on a real low-resourcelanguage, Bengali, confirms our findings on zero-shot transfer to new targetlanguages. The code is$\href{https://github.com/dannigt/attribute-controller-transfer}{\text{here}}$.</description><author>Danni Liu, Jan Niehues</author><pubDate>Fri, 15 Sep 2023 18:33:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08565v1</guid></item><item><title>The Impact of Different Backbone Architecture on Autonomous Vehicle Dataset</title><link>http://arxiv.org/abs/2309.08564v1</link><description>Object detection is a crucial component of autonomous driving, and manydetection applications have been developed to address this task. Theseapplications often rely on backbone architectures, which extract representationfeatures from inputs to perform the object detection task. The quality of thefeatures extracted by the backbone architecture can have a significant impacton the overall detection performance. Many researchers have focused ondeveloping new and improved backbone architectures to enhance the efficiencyand accuracy of object detection applications. While these backbonearchitectures have shown state-of-the-art performance on generic objectdetection datasets like MS-COCO and PASCAL-VOC, evaluating their performanceunder an autonomous driving environment has not been previously explored. Toaddress this, our study evaluates three well-known autonomous vehicle datasets,namely KITTI, NuScenes, and BDD, to compare the performance of differentbackbone architectures on object detection tasks.</description><author>Ning Ding, Azim Eskandarian</author><pubDate>Fri, 15 Sep 2023 18:32:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08564v1</guid></item><item><title>Deep Reinforcement Learning for Efficient and Fair Allocation of Health Care Resources</title><link>http://arxiv.org/abs/2309.08560v1</link><description>Scarcity of health care resources could result in the unavoidable consequenceof rationing. For example, ventilators are often limited in supply, especiallyduring public health emergencies or in resource-constrained health caresettings, such as amid the pandemic of COVID-19. Currently, there is nouniversally accepted standard for health care resource allocation protocols,resulting in different governments prioritizing patients based on variouscriteria and heuristic-based protocols. In this study, we investigate the useof reinforcement learning for critical care resource allocation policyoptimization to fairly and effectively ration resources. We propose atransformer-based deep Q-network to integrate the disease progression ofindividual patients and the interaction effects among patients during thecritical care resource allocation. We aim to improve both fairness ofallocation and overall patient outcomes. Our experiments demonstrate that ourmethod significantly reduces excess deaths and achieves a more equitabledistribution under different levels of ventilator shortage, when compared toexisting severity-based and comorbidity-based methods in use by differentgovernments. Our source code is included in the supplement and will be releasedon Github upon publication.</description><author>Yikuan Li, Chengsheng Mao, Kaixuan Huang, Hanyin Wang, Zheng Yu, Mengdi Wang, Yuan Luo</author><pubDate>Fri, 15 Sep 2023 18:28:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08560v1</guid></item><item><title>CDFSL-V: Cross-Domain Few-Shot Learning for Videos</title><link>http://arxiv.org/abs/2309.03989v2</link><description>Few-shot video action recognition is an effective approach to recognizing newcategories with only a few labeled examples, thereby reducing the challengesassociated with collecting and annotating large-scale video datasets. Existingmethods in video action recognition rely on large labeled datasets from thesame domain. However, this setup is not realistic as novel categories may comefrom different data domains that may have different spatial and temporalcharacteristics. This dissimilarity between the source and target domains canpose a significant challenge, rendering traditional few-shot action recognitiontechniques ineffective. To address this issue, in this work, we propose a novelcross-domain few-shot video action recognition method that leveragesself-supervised learning and curriculum learning to balance the informationfrom the source and target domains. To be particular, our method employs amasked autoencoder-based self-supervised training objective to learn from bothsource and target data in a self-supervised manner. Then a progressivecurriculum balances learning the discriminative information from the sourcedataset with the generic information learned from the target domain. Initially,our curriculum utilizes supervised learning to learn class discriminativefeatures from the source data. As the training progresses, we transition tolearning target-domain-specific features. We propose a progressive curriculumto encourage the emergence of rich features in the target domain based on classdiscriminative supervised features in the source domain. We evaluate our methodon several challenging benchmark datasets and demonstrate that our approachoutperforms existing cross-domain few-shot learning techniques. Our code isavailable at https://github.com/Sarinda251/CDFSL-V</description><author>Sarinda Samarasinghe, Mamshad Nayeem Rizve, Navid Kardan, Mubarak Shah</author><pubDate>Fri, 15 Sep 2023 18:24:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03989v2</guid></item><item><title>Convergence of Gradient-based MAML in LQR</title><link>http://arxiv.org/abs/2309.06588v2</link><description>The main objective of this research paper is to investigate the localconvergence characteristics of Model-agnostic Meta-learning (MAML) when appliedto linear system quadratic optimal control (LQR). MAML and its variations havebecome popular techniques for quickly adapting to new tasks by leveragingprevious learning knowledge in areas like regression, classification, andreinforcement learning. However, its theoretical guarantees remain unknown dueto non-convexity and its structure, making it even more challenging to ensurestability in the dynamic system setting. This study focuses on exploring MAMLin the LQR setting, providing its local convergence guarantees whilemaintaining the stability of the dynamical system. The paper also presentssimple numerical results to demonstrate the convergence properties of MAML inLQR tasks.</description><author>Negin Musavi, Geir E. Dullerud</author><pubDate>Fri, 15 Sep 2023 18:22:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06588v2</guid></item><item><title>DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System</title><link>http://arxiv.org/abs/2308.06378v2</link><description>A key challenge in eXplainable Artificial Intelligence is the well-knowntradeoff between the transparency of an algorithm (i.e., how easily a human candirectly understand the algorithm, as opposed to receiving a post-hocexplanation), and its accuracy. We report on the design of a new deep networkthat achieves improved transparency without sacrificing accuracy. We design adeep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzylogic and deep learning models and show that DCNFIS performs as accurately asthree existing convolutional neural networks on four well-known datasets. Wefurthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. Wethen exploit the transparency of fuzzy logic by deriving explanations, in theform of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigatethe properties of these explanations in greater depth using the Fashion-MNISTdataset.</description><author>Mojtaba Yeganejou, Kimia Honari, Ryan Kluzinski, Scott Dick, Michael Lipsett, James Miller</author><pubDate>Fri, 15 Sep 2023 18:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06378v2</guid></item><item><title>Leveraging Memory Effects and Gradient Information in Consensus-Based Optimization: On Global Convergence in Mean-Field Law</title><link>http://arxiv.org/abs/2211.12184v2</link><description>In this paper we study consensus-based optimization (CBO), a versatile,flexible and customizable optimization method suitable for performing nonconvexand nonsmooth global optimizations in high dimensions. CBO is a multi-particlemetaheuristic, which is effective in various applications and at the same timeamenable to theoretical analysis thanks to its minimalistic design. Theunderlying dynamics, however, is flexible enough to incorporate differentmechanisms widely used in evolutionary computation and machine learning, as weshow by analyzing a variant of CBO which makes use of memory effects andgradient information. We rigorously prove that this dynamics converges to aglobal minimizer of the objective function in mean-field law for a vast classof functions under minimal assumptions on the initialization of the method. Theproof in particular reveals how to leverage further, in some applicationsadvantageous, forces in the dynamics without loosing provable globalconvergence. To demonstrate the benefit of the herein investigated memoryeffects and gradient information in certain applications, we present numericalevidence for the superiority of this CBO variant in applications such asmachine learning and compressed sensing, which en passant widen the scope ofapplications of CBO.</description><author>Konstantin Riedl</author><pubDate>Fri, 15 Sep 2023 18:14:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.12184v2</guid></item><item><title>Augmenting conformers with structured state space models for online speech recognition</title><link>http://arxiv.org/abs/2309.08551v1</link><description>Online speech recognition, where the model only accesses context to the left,is an important and challenging use case for ASR systems. In this work, weinvestigate augmenting neural encoders for online ASR by incorporatingstructured state-space sequence models (S4), which are a family of models thatprovide a parameter-efficient way of accessing arbitrarily long left context.We perform systematic ablation studies to compare variants of S4 models andpropose two novel approaches that combine them with convolutions. We find thatthe most effective design is to stack a small S4 using real-valued recurrentweights with a local convolution, allowing them to work complementarily. Ourbest model achieves WERs of 4.01%/8.53% on test sets from Librispeech,outperforming Conformers with extensively tuned convolution.</description><author>Haozhe Shan, Albert Gu, Zhong Meng, Weiran Wang, Krzysztof Choromanski, Tara Sainath</author><pubDate>Fri, 15 Sep 2023 18:14:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08551v1</guid></item><item><title>Feature Enforcing PINN (FE-PINN): A Framework for Learning the Underlying-Physics to Resolve Unbalancing in the Objective Function Terms</title><link>http://arxiv.org/abs/2308.08873v2</link><description>In this study, we propose a new data-free framework, Feature EnforcingPhysics Informed Neural Network (FE-PINN), to overcome the challenge of animbalanced loss function in vanilla PINNs. The imbalance is caused by thepresence of two terms in the loss function: the partial differential loss andthe boundary condition mean squared error. A standard solution is to use lossweighting, but it requires hyperparameter tuning. To address this challenge, weintroduce a process called smart initialization to force the neural network tolearn only the boundary conditions before the final training in a designedprocess. In this method, clustered domain points are used to train a neuralnetwork with designed weights, resulting in the creation of a neural networkcalled Foundation network. This results in a network with unique weights thatunderstand boundary conditions. Then, additional layers are used to improve theaccuracy. This solves the problem of an imbalanced loss function withoutfurther need for hyperparameter tuning. For 2D flow over a cylinder as abenchmark, smart initialization in FE-PINN is 574 times faster thanhyperparameter tuning in vanilla PINN. Even with the optimal loss weight value,FE-PINN outperforms vanilla PINN by speeding up the average training time by1.98. Also, the ability of the proposed approach is shown for an inverseproblem. To find the inlet velocity for a 2D flow over a cylinder, FE-PINN istwice faster than vanilla PINN with the knowledge of optimal weight loss valuefor vanilla PINN. Our results show that FE-PINN not only eliminates thetime-consuming process of loss weighting but also improves convergence speedcompared to vanilla PINN, even when the optimal weight value is used in itsloss function. In conclusion, this framework can be used as a fast and accuratetool for solving a wide range of Partial Differential Equations across variousfields.</description><author>Mahyar Jahaninasab, Mohamad Ali Bijarchi</author><pubDate>Fri, 15 Sep 2023 18:13:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08873v2</guid></item><item><title>HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks</title><link>http://arxiv.org/abs/2309.08549v1</link><description>While numerous defense methods have been proposed to prohibit potentialpoisoning attacks from untrusted data sources, most research works only defendagainst specific attacks, which leaves many avenues for an adversary toexploit. In this work, we propose an efficient and robust training approach todefend against data poisoning attacks based on influence functions, namedHealthy Influential-Noise based Training. Using influence functions, we crafthealthy noise that helps to harden the classification model against poisoningattacks without significantly affecting the generalization ability on testdata. In addition, our method can perform effectively when only a subset of thetraining data is modified, instead of the current method of adding noise to allexamples that has been used in several previous works. We conduct comprehensiveevaluations over two image datasets with state-of-the-art poisoning attacksunder different realistic attack scenarios. Our empirical results show thatHINT can efficiently protect deep learning models against the effect of bothuntargeted and targeted poisoning attacks.</description><author>Minh-Hao Van, Alycia N. Carey, Xintao Wu</author><pubDate>Fri, 15 Sep 2023 18:12:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08549v1</guid></item><item><title>Towards Robust Continual Learning with Bayesian Adaptive Moment Regularization</title><link>http://arxiv.org/abs/2309.08546v1</link><description>The pursuit of long-term autonomy mandates that robotic agents mustcontinuously adapt to their changing environments and learn to solve new tasks.Continual learning seeks to overcome the challenge of catastrophic forgetting,where learning to solve new tasks causes a model to forget previously learntinformation. Prior-based continual learning methods are appealing for roboticapplications as they are space efficient and typically do not increase incomputational complexity as the number of tasks grows. Despite these desirableproperties, prior-based approaches typically fail on important benchmarks andconsequently are limited in their potential applications compared to theirmemory-based counterparts. We introduce Bayesian adaptive moment regularization(BAdam), a novel prior-based method that better constrains parameter growth,leading to lower catastrophic forgetting. Our method boasts a range ofdesirable properties for robotic applications such as being lightweight andtask label-free, converging quickly, and offering calibrated uncertainty thatis important for safe real-world deployment. Results show that BAdam achievesstate-of-the-art performance for prior-based methods on challengingsingle-headed class-incremental experiments such as Split MNIST and SplitFashionMNIST, and does so without relying on task labels or discrete taskboundaries.</description><author>Jack Foster, Alexandra Brintrup</author><pubDate>Fri, 15 Sep 2023 18:10:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08546v1</guid></item><item><title>Efficient and robust Sensor Placement in Complex Environments</title><link>http://arxiv.org/abs/2309.08545v1</link><description>We address the problem of efficient and unobstructed surveillance orcommunication in complex environments. On one hand, one wishes to use a minimalnumber of sensors to cover the environment. On the other hand, it is oftenimportant to consider solutions that are robust against sensor failure oradversarial attacks. This paper addresses these challenges of designing minimalsensor sets that achieve multi-coverage constraints -- every point in theenvironment is covered by a prescribed number of sensors. We propose a greedyalgorithm to achieve the objective. Further, we explore deep learningtechniques to accelerate the evaluation of the objective function formulated inthe greedy algorithm. The training of the neural network reveals that thegeometric properties of the data significantly impact the network'sperformance, particularly at the end stage. By taking into account theseproperties, we discuss the differences in using greedy and $\epsilon$-greedyalgorithms to generate data and their impact on the robustness of the network.</description><author>Lukas Taus, Yen-Hsi Richard Tsai</author><pubDate>Fri, 15 Sep 2023 18:10:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08545v1</guid></item><item><title>When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets</title><link>http://arxiv.org/abs/2309.08541v1</link><description>Using large language models (LMs) for query or document expansion can improvegeneralization in information retrieval. However, it is unknown whether thesetechniques are universally beneficial or only effective in specific settings,such as for particular retrieval models, dataset domains, or query types. Toanswer this, we conduct the first comprehensive analysis of LM-based expansion.We find that there exists a strong negative correlation between retrieverperformance and gains from expansion: expansion improves scores for weakermodels, but generally harms stronger models. We show this trend holds across aset of eleven expansion techniques, twelve datasets with diverse distributionshifts, and twenty-four retrieval models. Through qualitative error analysis,we hypothesize that although expansions provide extra information (potentiallyimproving recall), they add additional noise that makes it difficult to discernbetween the top relevant documents (thus introducing false positives). Ourresults suggest the following recipe: use expansions for weaker models or whenthe target dataset significantly differs from training corpus in format;otherwise, avoid expansions to keep the relevance signal clear.</description><author>Orion Weller, Kyle Lo, David Wadden, Dawn Lawrie, Benjamin Van Durme, Arman Cohan, Luca Soldaini</author><pubDate>Fri, 15 Sep 2023 18:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08541v1</guid></item><item><title>Diversity in deep generative models and generative AI</title><link>http://arxiv.org/abs/2202.09573v2</link><description>The machine learning generative algorithms such as Generative AdversarialNetworks (GAN) and Variational Auto-Encoders (VAE) show impressive results whenconstructing objects similar to those in a training ensemble. However, thegeneration of new objects builds mainly on the understanding of the hiddenstructure of the training dataset followed by a sampling from amulti-dimensional normal variable. In particular each sample is independentfrom the others and can repeatedly propose same type of objects. To cure thisdrawback we introduce a kernel-based measure quantization method that canproduce new objects from a given target measure by approximating it as a wholeand even staying away from elements already drawn from that distribution. Thisensures a better diversity of the produced objects. The method is tested onclassic machine learning benchmarks.</description><author>Gabriel Turinici</author><pubDate>Fri, 15 Sep 2023 17:55:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.09573v2</guid></item><item><title>Visual Speech Recognition for Low-resource Languages with Automatic Labels From Whisper Model</title><link>http://arxiv.org/abs/2309.08535v1</link><description>This paper proposes a powerful Visual Speech Recognition (VSR) method formultiple languages, especially for low-resource languages that have a limitednumber of labeled data. Different from previous methods that tried to improvethe VSR performance for the target language by using knowledge learned fromother languages, we explore whether we can increase the amount of training dataitself for the different languages without human intervention. To this end, weemploy a Whisper model which can conduct both language identification andaudio-based speech recognition. It serves to filter data of the desiredlanguages and transcribe labels from the unannotated, multilingual audio-visualdata pool. By comparing the performances of VSR models trained on automaticlabels and the human-annotated labels, we show that we can achieve similar VSRperformance to that of human-annotated labels even without utilizing humanannotations. Through the automated labeling process, we label large-scaleunlabeled multilingual databases, VoxCeleb2 and AVSpeech, producing 1,002 hoursof data for four low VSR resource languages, French, Italian, Spanish, andPortuguese. With the automatic labels, we achieve new state-of-the-artperformance on mTEDx in four languages, significantly surpassing the previousmethods. The automatic labels are available online:https://github.com/JeongHun0716/Visual-Speech-Recognition-for-Low-Resource-Languages</description><author>Jeong Hun Yeo, Minsu Kim, Shinji Watanabe, Yong Man Ro</author><pubDate>Fri, 15 Sep 2023 17:53:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08535v1</guid></item><item><title>Towards Last-layer Retraining for Group Robustness with Fewer Annotations</title><link>http://arxiv.org/abs/2309.08534v1</link><description>Empirical risk minimization (ERM) of neural networks is prone toover-reliance on spurious correlations and poor generalization on minoritygroups. The recent deep feature reweighting (DFR) technique achievesstate-of-the-art group robustness via simple last-layer retraining, but itrequires held-out group and class annotations to construct a group-balancedreweighting dataset. In this work, we examine this impractical requirement andfind that last-layer retraining can be surprisingly effective with no groupannotations (other than for model selection) and only a handful of classannotations. We first show that last-layer retraining can greatly improveworst-group accuracy even when the reweighting dataset has only a smallproportion of worst-group data. This implies a "free lunch" where holding out asubset of training data to retrain the last layer can substantially outperformERM on the entire dataset with no additional data or annotations. To furtherimprove group robustness, we introduce a lightweight method called selectivelast-layer finetuning (SELF), which constructs the reweighting dataset usingmisclassifications or disagreements. Our empirical and theoretical resultspresent the first evidence that model disagreement upsamples worst-group data,enabling SELF to nearly match DFR on four well-established benchmarks acrossvision and language tasks with no group annotations and less than 3% of theheld-out class annotations. Our code is available athttps://github.com/tmlabonte/last-layer-retraining.</description><author>Tyler LaBonte, Vidya Muthukumar, Abhishek Kumar</author><pubDate>Fri, 15 Sep 2023 17:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08534v1</guid></item><item><title>Automated dermatoscopic pattern discovery by clustering neural network output for human-computer interaction</title><link>http://arxiv.org/abs/2309.08533v1</link><description>Background: As available medical image datasets increase in size, it becomesinfeasible for clinicians to review content manually for knowledge extraction.The objective of this study was to create an automated clustering resulting inhuman-interpretable pattern discovery. Methods: Images from the public HAM10000 dataset, including 7 commonpigmented skin lesion diagnoses, were tiled into 29420 tiles and clustered viak-means using neural network-extracted image features. The final number ofclusters per diagnosis was chosen by either the elbow method or a compactnessmetric balancing intra-lesion variance and cluster numbers. The amount ofresulting non-informative clusters, defined as those containing less than siximage tiles, was compared between the two methods. Results: Applying k-means, the optimal elbow cutoff resulted in a mean of24.7 (95%-CI: 16.4-33) clusters for every included diagnosis, including 14.9%(95% CI: 0.8-29.0) non-informative clusters. The optimal cutoff, as estimatedby the compactness metric, resulted in significantly fewer clusters (13.4;95%-CI 11.8-15.1; p=0.03) and less non-informative ones (7.5%; 95% CI: 0-19.5;p=0.017). The majority of clusters (93.6%) from the compactness metric could bemanually mapped to previously described dermatoscopic diagnostic patterns. Conclusions: Automatically constraining unsupervised clustering can producean automated extraction of diagnostically relevant and human-interpretableclusters of visual patterns from a large image dataset.</description><author>Lidia Talavera-Martinez, Philipp Tschandl</author><pubDate>Fri, 15 Sep 2023 17:50:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08533v1</guid></item><item><title>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</title><link>http://arxiv.org/abs/2309.08532v1</link><description>Large Language Models (LLMs) excel in various tasks, but they rely oncarefully crafted prompts that often demand substantial human effort. Toautomate this process, in this paper, we propose a novel framework for discreteprompt optimization, called EvoPrompt, which borrows the idea of evolutionaryalgorithms (EAs) as they exhibit good performance and fast convergence. Toenable EAs to work on discrete prompts, which are natural language expressionsthat need to be coherent and human-readable, we connect LLMs with EAs. Thisapproach allows us to simultaneously leverage the powerful language processingcapabilities of LLMs and the efficient optimization performance of EAs.Specifically, abstaining from any gradients or parameters, EvoPrompt startsfrom a population of prompts and iteratively generates new prompts with LLMsbased on the evolutionary operators, improving the population based on thedevelopment set. We optimize prompts for both closed- and open-source LLMsincluding GPT-3.5 and Alpaca, on 9 datasets spanning language understanding andgeneration tasks. EvoPrompt significantly outperforms human-engineered promptsand existing methods for automatic prompt generation by up to 25% and 14%respectively. Furthermore, EvoPrompt demonstrates that connecting LLMs with EAscreates synergies, which could inspire further research on the combination ofLLMs and conventional algorithms.</description><author>Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang</author><pubDate>Fri, 15 Sep 2023 17:50:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08532v1</guid></item><item><title>Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens</title><link>http://arxiv.org/abs/2309.08531v1</link><description>In this paper, we propose methods to build a powerful and efficientImage-to-Speech captioning (Im2Sp) model. To this end, we start with importingthe rich knowledge related to image comprehension and language modeling from alarge-scale pre-trained vision-language model into Im2Sp. We set the output ofthe proposed Im2Sp as discretized speech units, i.e., the quantized speechfeatures of a self-supervised speech model. The speech units mainly containlinguistic information while suppressing other characteristics of speech. Thisallows us to incorporate the language modeling capability of the pre-trainedvision-language model into the spoken language modeling of Im2Sp. With thevision-language pre-training strategy, we set new state-of-the-art Im2Spperformances on two widely used benchmark databases, COCO and Flickr8k. Then,we further improve the efficiency of the Im2Sp model. Similar to the speechunit case, we convert the original image into image units, which are derivedthrough vector quantization of the raw image. With these image units, we candrastically reduce the required data storage for saving image data to just 0.8%when compared to the original image data in terms of bits. Demo page:https://ms-dot-k.github.io/Image-to-Speech-Captioning.</description><author>Minsu Kim, Jeongsoo Choi, Soumi Maiti, Jeong Hun Yeo, Shinji Watanabe, Yong Man Ro</author><pubDate>Fri, 15 Sep 2023 17:48:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08531v1</guid></item><item><title>MixStyle Neural Networks for Domain Generalization and Adaptation</title><link>http://arxiv.org/abs/2107.02053v2</link><description>Neural networks do not generalize well to unseen data with domain shifts -- alongstanding problem in machine learning and AI. To overcome the problem, wepropose MixStyle, a simple plug-and-play, parameter-free module that canimprove domain generalization performance without the need to collect more dataor increase model capacity. The design of MixStyle is simple: it mixes thefeature statistics of two random instances in a single forward pass duringtraining. The idea is grounded by the finding from recent style transferresearch that feature statistics capture image style information, whichessentially defines visual domains. Therefore, mixing feature statistics can beseen as an efficient way to synthesize new domains in the feature space, thusachieving data augmentation. MixStyle is easy to implement with a few lines ofcode, does not require modification to training objectives, and can fit avariety of learning paradigms including supervised domain generalization,semi-supervised domain generalization, and unsupervised domain adaptation. Ourexperiments show that MixStyle can significantly boost out-of-distributiongeneralization performance across a wide range of tasks including imagerecognition, instance retrieval and reinforcement learning.</description><author>Kaiyang Zhou, Yongxin Yang, Yu Qiao, Tao Xiang</author><pubDate>Fri, 15 Sep 2023 17:42:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.02053v2</guid></item><item><title>Text-Driven Foley Sound Generation With Latent Diffusion Model</title><link>http://arxiv.org/abs/2306.10359v4</link><description>Foley sound generation aims to synthesise the background sound for multimediacontent. Previous models usually employ a large development set with labels asinput (e.g., single numbers or one-hot vector). In this work, we propose adiffusion model based system for Foley sound generation with text conditions.To alleviate the data scarcity issue, our model is initially pre-trained withlarge-scale datasets and fine-tuned to this task via transfer learning usingthe contrastive language-audio pertaining (CLAP) technique. We have observedthat the feature embedding extracted by the text encoder can significantlyaffect the performance of the generation model. Hence, we introduce a trainablelayer after the encoder to improve the text embedding produced by the encoder.In addition, we further refine the generated waveform by generating multiplecandidate audio clips simultaneously and selecting the best one, which isdetermined in terms of the similarity score between the embedding of thecandidate clips and the embedding of the target text label. Using the proposedmethod, our system ranks ${1}^{st}$ among the systems submitted to DCASEChallenge 2023 Task 7. The results of the ablation studies illustrate that theproposed techniques significantly improve sound generation performance. Thecodes for implementing the proposed system are available online.</description><author>Yi Yuan, Haohe Liu, Xubo Liu, Xiyuan Kang, Peipei Wu, Mark D. Plumbley, Wenwu Wang</author><pubDate>Fri, 15 Sep 2023 17:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10359v4</guid></item><item><title>Breathing New Life into 3D Assets with Generative Repainting</title><link>http://arxiv.org/abs/2309.08523v1</link><description>Diffusion-based text-to-image models ignited immense attention from thevision community, artists, and content creators. Broad adoption of these modelsis due to significant improvement in the quality of generations and efficientconditioning on various modalities, not just text. However, lifting the richgenerative priors of these 2D models into 3D is challenging. Recent works haveproposed various pipelines powered by the entanglement of diffusion models andneural fields. We explore the power of pretrained 2D diffusion models andstandard 3D neural radiance fields as independent, standalone tools anddemonstrate their ability to work together in a non-learned fashion. Suchmodularity has the intrinsic advantage of eased partial upgrades, which becamean important property in such a fast-paced domain. Our pipeline accepts anylegacy renderable geometry, such as textured or untextured meshes, orchestratesthe interaction between 2D generative refinement and 3D consistency enforcementtools, and outputs a painted input geometry in several formats. We conduct alarge-scale study on a wide range of objects and categories from theShapeNetSem dataset and demonstrate the advantages of our approach, bothqualitatively and quantitatively. Project page:https://www.obukhov.ai/repainting_3d_assets</description><author>Tianfu Wang, Menelaos Kanakis, Konrad Schindler, Luc Van Gool, Anton Obukhov</author><pubDate>Fri, 15 Sep 2023 17:34:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08523v1</guid></item><item><title>Scaling Laws for Sparsely-Connected Foundation Models</title><link>http://arxiv.org/abs/2309.08520v1</link><description>We explore the impact of parameter sparsity on the scaling behavior ofTransformers trained on massive datasets (i.e., "foundation models"), in bothvision and language domains. In this setting, we identify the first scaling lawdescribing the relationship between weight sparsity, number of non-zeroparameters, and amount of training data, which we validate empirically acrossmodel and data scales; on ViT/JFT-4B and T5/C4. These results allow us tocharacterize the "optimal sparsity", the sparsity level which yields the bestperformance for a given effective model size and training budget. For a fixednumber of non-zero parameters, we identify that the optimal sparsity increaseswith the amount of data used for training. We also extend our study todifferent sparsity structures (such as the hardware-friendly n:m pattern) andstrategies (such as starting from a pretrained dense model). Our findings shedlight on the power and limitations of weight sparsity across various parameterand computational settings, offering both theoretical understanding andpractical implications for leveraging sparsity towards computational efficiencyimprovements.</description><author>Elias Frantar, Carlos Riquelme, Neil Houlsby, Dan Alistarh, Utku Evci</author><pubDate>Fri, 15 Sep 2023 17:29:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08520v1</guid></item><item><title>SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels</title><link>http://arxiv.org/abs/2309.08513v1</link><description>Pre-trained vision transformers have strong representation benefits tovarious downstream tasks. Recently, many parameter-efficient fine-tuning (PEFT)methods have been proposed, and their experiments demonstrate that tuning only1% of extra parameters could surpass full fine-tuning in low-data resourcescenarios. However, these methods overlook the task-specific information whenfine-tuning diverse downstream tasks. In this paper, we propose a simple yeteffective method called "Salient Channel Tuning" (SCT) to leverage thetask-specific information by forwarding the model with the task images toselect partial channels in a feature map that enables us to tune only 1/8channels leading to significantly lower parameter costs. Experiments outperformfull fine-tuning on 18 out of 19 tasks in the VTAB-1K benchmark by adding only0.11M parameters of the ViT-B, which is 780$\times$ fewer than its fullfine-tuning counterpart. Furthermore, experiments on domain generalization andfew-shot learning surpass other PEFT methods with lower parameter costs,demonstrating our proposed tuning technique's strong capability andeffectiveness in the low-data regime.</description><author>Henry Hengyuan Zhao, Pichao Wang, Yuyang Zhao, Hao Luo, Fan Wang, Mike Zheng Shou</author><pubDate>Fri, 15 Sep 2023 17:19:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08513v1</guid></item><item><title>Generalised Probabilistic Diffusion Scale-Spaces</title><link>http://arxiv.org/abs/2309.08511v1</link><description>Probabilistic diffusion models excel at sampling new images from learneddistributions. Originally motivated by drift-diffusion concepts from physics,they apply image perturbations such as noise and blur in a forward process thatresults in a tractable probability distribution. A corresponding learnedreverse process generates images and can be conditioned on side information,which leads to a wide variety of practical applications. Most of the researchfocus currently lies on practice-oriented extensions. In contrast, thetheoretical background remains largely unexplored, in particular the relationsto drift-diffusion. In order to shed light on these connections to classicalimage filtering, we propose a generalised scale-space theory for probabilisticdiffusion models. Moreover, we show conceptual and empirical connections todiffusion and osmosis filters.</description><author>Pascal Peter</author><pubDate>Fri, 15 Sep 2023 17:17:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08511v1</guid></item><item><title>Self-Correlation and Cross-Correlation Learning for Few-Shot Remote Sensing Image Semantic Segmentation</title><link>http://arxiv.org/abs/2309.05840v2</link><description>Remote sensing image semantic segmentation is an important problem for remotesensing image interpretation. Although remarkable progress has been achieved,existing deep neural network methods suffer from the reliance on massivetraining data. Few-shot remote sensing semantic segmentation aims at learningto segment target objects from a query image using only a few annotated supportimages of the target class. Most existing few-shot learning methods stemprimarily from their sole focus on extracting information from support images,thereby failing to effectively address the large variance in appearance andscales of geographic objects. To tackle these challenges, we propose aSelf-Correlation and Cross-Correlation Learning Network for the few-shot remotesensing image semantic segmentation. Our model enhances the generalization byconsidering both self-correlation and cross-correlation between support andquery images to make segmentation predictions. To further explore theself-correlation with the query image, we propose to adopt a classical spectralmethod to produce a class-agnostic segmentation mask based on the basic visualinformation of the image. Extensive experiments on two remote sensing imagedatasets demonstrate the effectiveness and superiority of our model in few-shotremote sensing image semantic segmentation. Code and models will be accessed athttps://github.com/linhanwang/SCCNet.</description><author>Linhan Wang, Shuo Lei, Jianfeng He, Shengkun Wang, Min Zhang, Chang-Tien Lu</author><pubDate>Fri, 15 Sep 2023 17:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05840v2</guid></item><item><title>OccupancyDETR: Making Semantic Scene Completion as Straightforward as Object Detection</title><link>http://arxiv.org/abs/2309.08504v1</link><description>Visual-based 3D semantic occupancy perception (also known as 3D semanticscene completion) is a new perception paradigm for robotic applications likeautonomous driving. Compared with Bird's Eye View (BEV) perception, it extendsthe vertical dimension, significantly enhancing the ability of robots tounderstand their surroundings. However, due to this very reason, thecomputational demand for current 3D semantic occupancy perception methodsgenerally surpasses that of BEV perception methods and 2D perception methods.We propose a novel 3D semantic occupancy perception method, OccupancyDETR,which consists of a DETR-like object detection module and a 3D occupancydecoder module. The integration of object detection simplifies our methodstructurally - instead of predicting the semantics of each voxels, itidentifies objects in the scene and their respective 3D occupancy grids. Thisspeeds up our method, reduces required resources, and leverages objectdetection algorithm, giving our approach notable performance on small objects.We demonstrate the effectiveness of our proposed method on the SemanticKITTIdataset, showcasing an mIoU of 23 and a processing speed of 6 frames persecond, thereby presenting a promising solution for real-time 3D semantic scenecompletion.</description><author>Yupeng Jia, Jie He, Runze Chen, Fang Zhao, Haiyong Luo</author><pubDate>Fri, 15 Sep 2023 17:06:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08504v1</guid></item><item><title>HealthFC: A Dataset of Health Claims for Evidence-Based Medical Fact-Checking</title><link>http://arxiv.org/abs/2309.08503v1</link><description>Seeking health-related advice on the internet has become a common practice inthe digital era. Determining the trustworthiness of medical claims found onlineand finding appropriate evidence for this information is increasinglychallenging. Fact-checking has emerged as an approach to assess the veracity offactual claims using evidence from credible knowledge sources. To help advancethe automation of this task, in this paper, we introduce a novel dataset of 750health-related claims, labeled for veracity by medical experts and backed withevidence from appropriate clinical studies. We provide an analysis of thedataset, highlighting its characteristics and challenges. The dataset can beused for Machine Learning tasks related to automated fact-checking such asevidence retrieval, veracity prediction, and explanation generation. For thispurpose, we provide baseline models based on different approaches, examinetheir performance, and discuss the findings.</description><author>Juraj Vladika, Phillip Schneider, Florian Matthes</author><pubDate>Fri, 15 Sep 2023 17:05:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08503v1</guid></item><item><title>Deep-learning-powered data analysis in plankton ecology</title><link>http://arxiv.org/abs/2309.08500v1</link><description>The implementation of deep learning algorithms has brought new perspectivesto plankton ecology. Emerging as an alternative approach to establishedmethods, deep learning offers objective schemes to investigate planktonorganisms in diverse environments. We provide an overview ofdeep-learning-based methods including detection and classification of phyto-and zooplankton images, foraging and swimming behaviour analysis, and finallyecological modelling. Deep learning has the potential to speed up the analysisand reduce the human experimental bias, thus enabling data acquisition atrelevant temporal and spatial scales with improved reproducibility. We alsodiscuss shortcomings and show how deep learning architectures have evolved tomitigate imprecise readouts. Finally, we suggest opportunities where deeplearning is particularly likely to catalyze plankton research. The examples areaccompanied by detailed tutorials and code samples that allow readers to applythe methods described in this review to their own data.</description><author>Harshith Bachimanchi, Matthew I. M. Pinder, Chloé Robert, Pierre De Wit, Jonathan Havenhand, Alexandra Kinnby, Daniel Midtvedt, Erik Selander, Giovanni Volpe</author><pubDate>Fri, 15 Sep 2023 17:04:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08500v1</guid></item><item><title>P-ROCKET: Pruning Random Convolution Kernels for Time Series Classification</title><link>http://arxiv.org/abs/2309.08499v1</link><description>In recent years, two time series classification models, ROCKET andMINIROCKET, have attracted much attention for their low training cost andstate-of-the-art accuracy. Utilizing random 1-D convolutional kernels withouttraining, ROCKET and MINIROCKET can rapidly extract features from time seriesdata, allowing for the efficient fitting of linear classifiers. However, tocomprehensively capture useful features, a large number of random kernels arerequired, which is incompatible for resource-constrained devices. Therefore, aheuristic evolutionary algorithm named S-ROCKET is devised to recognize andprune redundant kernels. Nevertheless, the inherent nature of evolutionaryalgorithms renders the evaluation of kernels within S-ROCKET an unacceptabletime-consuming process. In this paper, diverging from S-ROCKET, which directlyevaluates random kernels with nonsignificant differences, we remove kernelsfrom a feature selection perspective by eliminating associating connections inthe sequential classification layer. To this end, we start by formulating thepruning challenge as a Group Elastic Net classification problem and employ theADMM method to arrive at a solution. Sequentially, we accelerate theaforementioned time-consuming solving process by bifurcating the $l_{2,1}$ and$l_2$ regularizations into two sequential stages and solve them separately,which ultimately forms our core algorithm, named P-ROCKET. Stage 1 of P-ROCKETemploys group-wise regularization similarly to our initial ADMM-basedAlgorithm, but introduces dynamically varying penalties to greatly acceleratethe process. To mitigate overfitting, Stage 2 of P-ROCKET implementselement-wise regularization to refit a linear classifier, utilizing theretained features.</description><author>Shaowu Chen, Weize Sun, Lei Huang, Xiaopeng Li, Qingyuan Wang, Deepu John</author><pubDate>Fri, 15 Sep 2023 17:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08499v1</guid></item><item><title>Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata</title><link>http://arxiv.org/abs/2309.08491v1</link><description>In this work, we explore the use of Large Language Models (LLMs) forknowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge.For this task, given subject and relation pairs sourced from Wikidata, weutilize pre-trained LLMs to produce the relevant objects in string format andlink them to their respective Wikidata QIDs. We developed a pipeline using LLMsfor Knowledge Engineering (LLMKE), combining knowledge probing and Wikidataentity mapping. The method achieved a macro-averaged F1-score of 0.701 acrossthe properties, with the scores varying from 1.00 to 0.328. These resultsdemonstrate that the knowledge of LLMs varies significantly depending on thedomain and that further experimentation is required to determine thecircumstances under which LLMs can be used for automatic Knowledge Base (e.g.,Wikidata) completion and correction. The investigation of the results alsosuggests the promising contribution of LLMs in collaborative knowledgeengineering. LLMKE won Track 2 of the challenge. The implementation isavailable at https://github.com/bohuizhang/LLMKE.</description><author>Bohui Zhang, Ioannis Reklos, Nitisha Jain, Albert Meroño Peñuela, Elena Simperl</author><pubDate>Fri, 15 Sep 2023 16:51:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08491v1</guid></item><item><title>Towards Word-Level End-to-End Neural Speaker Diarization with Auxiliary Network</title><link>http://arxiv.org/abs/2309.08489v1</link><description>While standard speaker diarization attempts to answer the question "whospoken when", most of relevant applications in reality are more interested indetermining "who spoken what". Whether it is the conventional modularizedapproach or the more recent end-to-end neural diarization (EEND), an additionalautomatic speech recognition (ASR) model and an orchestration algorithm arerequired to associate the speaker labels with recognized words. In this paper,we propose Word-level End-to-End Neural Diarization (WEEND) with auxiliarynetwork, a multi-task learning algorithm that performs end-to-end ASR andspeaker diarization in the same neural architecture. That is, while speech isbeing recognized, speaker labels are predicted simultaneously for eachrecognized word. Experimental results demonstrate that WEEND outperforms theturn-based diarization baseline system on all 2-speaker short-form scenariosand has the capability to generalize to audio lengths of 5 minutes. Although3+speaker conversations are harder, we find that with enough in-domain trainingdata, WEEND has the potential to deliver high quality diarized text.</description><author>Yiling Huang, Weiran Wang, Guanlong Zhao, Hank Liao, Wei Xia, Quan Wang</author><pubDate>Fri, 15 Sep 2023 16:48:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08489v1</guid></item><item><title>XFedHunter: An Explainable Federated Learning Framework for Advanced Persistent Threat Detection in SDN</title><link>http://arxiv.org/abs/2309.08485v1</link><description>Advanced Persistent Threat (APT) attacks are highly sophisticated and employa multitude of advanced methods and techniques to target organizations andsteal sensitive and confidential information. APT attacks consist of multiplestages and have a defined strategy, utilizing new and innovative techniques andtechnologies developed by hackers to evade security software monitoring. Toeffectively protect against APTs, detecting and predicting APT indicators withan explanation from Machine Learning (ML) prediction is crucial to reveal thecharacteristics of attackers lurking in the network system. Meanwhile,Federated Learning (FL) has emerged as a promising approach for buildingintelligent applications without compromising privacy. This is particularlyimportant in cybersecurity, where sensitive data and high-quality labeling playa critical role in constructing effective machine learning models for detectingcyber threats. Therefore, this work proposes XFedHunter, an explainablefederated learning framework for APT detection in Software-Defined Networking(SDN) leveraging local cyber threat knowledge from many training collaborators.In XFedHunter, Graph Neural Network (GNN) and Deep Learning model are utilizedto reveal the malicious events effectively in the large number of normal onesin the network system. The experimental results on NF-ToN-IoT and DARPA TCE3datasets indicate that our framework can enhance the trust and accountabilityof ML-based systems utilized for cybersecurity purposes without privacyleakage.</description><author>Huynh Thai Thi, Ngo Duc Hoang Son, Phan The Duy, Nghi Hoang Khoa, Khoa Ngo-Khanh, Van-Hau Pham</author><pubDate>Fri, 15 Sep 2023 16:44:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08485v1</guid></item><item><title>Policy Gradient Optimal Correlation Search for Variance Reduction in Monte Carlo simulation and Maximum Optimal Transport</title><link>http://arxiv.org/abs/2307.12703v2</link><description>We propose a new algorithm for variance reduction when estimating $f(X_T)$where $X$ is the solution to some stochastic differential equation and $f$ is atest function. The new estimator is $(f(X^1_T) + f(X^2_T))/2$, where $X^1$ and$X^2$ have same marginal law as $X$ but are pathwise correlated so that toreduce the variance. The optimal correlation function $\rho$ is approximated bya deep neural network and is calibrated along the trajectories of $(X^1, X^2)$by policy gradient and reinforcement learning techniques. Finding an optimalcoupling given marginal laws has links with maximum optimal transport.</description><author>Pierre Bras, Gilles Pagès</author><pubDate>Fri, 15 Sep 2023 16:43:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12703v2</guid></item><item><title>Spoken Humanoid Embodied Conversational Agents in Mobile Serious Games: A Usability Assessment</title><link>http://arxiv.org/abs/2309.07773v2</link><description>This paper presents an empirical investigation of the extent to which spokenHumanoid Embodied Conversational Agents (HECAs) can foster usability in mobileserious game (MSG) applications. The aim of the research is to assess theimpact of multiple agents and illusion of humanness on the quality of theinteraction. The experiment investigates two styles of agent presentation: anagent of high human-likeness (HECA) and an agent of low human-likeness (text).The purpose of the experiment is to assess whether and how agents of highhumanlikeness can evoke the illusion of humanness and affect usability. Agentsof high human-likeness were designed by following the ECA design model that isa proposed guide for ECA development. The results of the experiment with 90participants show that users prefer to interact with the HECAs. The differencebetween the two versions is statistically significant with a large effect size(d=1.01), with many of the participants justifying their choice by saying thatthe human-like characteristics of the HECA made the version more appealing.This research provides key information on the potential effect of HECAs onserious games, which can provide insight into the design of future mobileserious games.</description><author>Danai Korre, Judy Robertson</author><pubDate>Fri, 15 Sep 2023 16:42:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07773v2</guid></item><item><title>YCB-Ev: Event-vision dataset for 6DoF object pose estimation</title><link>http://arxiv.org/abs/2309.08482v1</link><description>Our work introduces the YCB-Ev dataset, which contains synchronized RGB-Dframes and event data that enables evaluating 6DoF object pose estimationalgorithms using these modalities. This dataset provides ground truth 6DoF object poses for the same 21 YCBobjects \cite{calli2017yale} that were used in the YCB-Video (YCB-V) dataset,enabling the evaluation of algorithm performance when transferred acrossdatasets. The dataset consists of 21 synchronized event and RGB-D sequences, amountingto a total of 7:43 minutes of video. Notably, 12 of these sequences feature thesame object arrangement as the YCB-V subset used in the BOP challenge. Our dataset is the first to provide ground truth 6DoF pose data for eventstreams. Furthermore, we evaluate the generalization capabilities of twostate-of-the-art algorithms, which were pre-trained for the BOP challenge,using our novel YCB-V sequences. The proposed dataset is available at https://github.com/paroj/ycbev.</description><author>Pavel Rojtberg, Thomas Pöllabauer</author><pubDate>Fri, 15 Sep 2023 16:42:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08482v1</guid></item><item><title>3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images</title><link>http://arxiv.org/abs/2309.08481v1</link><description>Automated segmentation of the blood vessels in 3D volumes is an essentialstep for the quantitative diagnosis and treatment of many vascular diseases. 3Dvessel segmentation is being actively investigated in existing works, mostly indeep learning approaches. However, training 3D deep networks requires largeamounts of manual 3D annotations from experts, which are laborious to obtain.This is especially the case for 3D vessel segmentation, as vessels are sparseyet spread out over many slices and disconnected when visualized in 2D slices.In this work, we propose a novel method to segment the 3D peripancreaticarteries solely from one annotated 2D projection per training image with depthsupervision. We perform extensive experiments on the segmentation ofperipancreatic arteries on 3D contrast-enhanced CT images and demonstrate howwell we capture the rich depth information from 2D projections. We demonstratethat by annotating a single, randomly chosen projection for each trainingsample, we obtain comparable performance to annotating multiple 2D projections,thereby reducing the annotation effort. Furthermore, by mapping the 2D labelsto the 3D space using depth information and incorporating this into training,we almost close the performance gap between 3D supervision and 2D supervision.Our code is available at: https://github.com/alinafdima/3Dseg-mip-depth.</description><author>Alina F. Dima, Veronika A. Zimmer, Martin J. Menten, Hongwei Bran Li, Markus Graf, Tristan Lemke, Philipp Raffler, Robert Graf, Jan S. Kirschke, Rickmer Braren, Daniel Rueckert</author><pubDate>Fri, 15 Sep 2023 16:41:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08481v1</guid></item><item><title>PoseFix: Correcting 3D Human Poses with Natural Language</title><link>http://arxiv.org/abs/2309.08480v1</link><description>Automatically producing instructions to modify one's posture could open thedoor to endless applications, such as personalized coaching and in-homephysical therapy. Tackling the reverse problem (i.e., refining a 3D pose basedon some natural language feedback) could help for assisted 3D characteranimation or robot teaching, for instance. Although a few recent works explorethe connections between natural language and 3D human pose, none focus ondescribing 3D body pose differences. In this paper, we tackle the problem ofcorrecting 3D human poses with natural language. To this end, we introduce thePoseFix dataset, which consists of several thousand paired 3D poses and theircorresponding text feedback, that describe how the source pose needs to bemodified to obtain the target pose. We demonstrate the potential of thisdataset on two tasks: (1) text-based pose editing, that aims at generatingcorrected 3D body poses given a query pose and a text modifier; and (2)correctional text generation, where instructions are generated based on thedifferences between two body poses.</description><author>Ginger Delmas, Philippe Weinzaepfel, Francesc Moreno-Noguer, Grégory Rogez</author><pubDate>Fri, 15 Sep 2023 16:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08480v1</guid></item><item><title>A Spiking Binary Neuron -- Detector of Causal Links</title><link>http://arxiv.org/abs/2309.08476v1</link><description>Causal relationship recognition is a fundamental operation in neural networksaimed at learning behavior, action planning, and inferring external worlddynamics. This operation is particularly crucial for reinforcement learning(RL). In the context of spiking neural networks (SNNs), events are representedas spikes emitted by network neurons or input nodes. Detecting causalrelationships within these events is essential for effective RL implementation.This research paper presents a novel approach to realize causal relationshiprecognition using a simple spiking binary neuron. The proposed method leveragesspecially designed synaptic plasticity rules, which are both straightforwardand efficient. Notably, our approach accounts for the temporal aspects ofdetected causal links and accommodates the representation of spiking signals assingle spikes or tight spike sequences (bursts), as observed in biologicalbrains. Furthermore, this study places a strong emphasis on thehardware-friendliness of the proposed models, ensuring their efficientimplementation on modern and future neuroprocessors. Being compared withprecise machine learning techniques, such as decision tree algorithms andconvolutional neural networks, our neuron demonstrates satisfactory accuracydespite its simplicity. In conclusion, we introduce a multi-neuron structurecapable of operating in more complex environments with enhanced accuracy,making it a promising candidate for the advancement of RL applications in SNNs.</description><author>Mikhail Kiselev, Denis Larionov, Andrey Urusov</author><pubDate>Fri, 15 Sep 2023 16:34:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08476v1</guid></item><item><title>Unconstrained Parametrization of Dissipative and Contracting Neural Ordinary Differential Equations</title><link>http://arxiv.org/abs/2304.02976v2</link><description>In this work, we introduce and study a class of Deep Neural Networks (DNNs)in continuous-time. The proposed architecture stems from the combination ofNeural Ordinary Differential Equations (Neural ODEs) with the model structureof recently introduced Recurrent Equilibrium Networks (RENs). We show how toendow our proposed NodeRENs with contractivity and dissipativity -- crucialproperties for robust learning and control. Most importantly, as for RENs, wederive parametrizations of contractive and dissipative NodeRENs which areunconstrained, hence enabling their learning for a large number of parameters.We validate the properties of NodeRENs, including the possibility of handlingirregularly sampled data, in a case study in nonlinear system identification.</description><author>Daniele Martinelli, Clara Lucía Galimberti, Ian R. Manchester, Luca Furieri, Giancarlo Ferrari-Trecate</author><pubDate>Fri, 15 Sep 2023 16:28:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02976v2</guid></item><item><title>VulnSense: Efficient Vulnerability Detection in Ethereum Smart Contracts by Multimodal Learning with Graph Neural Network and Language Model</title><link>http://arxiv.org/abs/2309.08474v1</link><description>This paper presents VulnSense framework, a comprehensive approach toefficiently detect vulnerabilities in Ethereum smart contracts using amultimodal learning approach on graph-based and natural language processing(NLP) models. Our proposed framework combines three types of features fromsmart contracts comprising source code, opcode sequences, and control flowgraph (CFG) extracted from bytecode. We employ Bidirectional EncoderRepresentations from Transformers (BERT), Bidirectional Long Short-Term Memory(BiLSTM) and Graph Neural Network (GNN) models to extract and analyze thesefeatures. The final layer of our multimodal approach consists of a fullyconnected layer used to predict vulnerabilities in Ethereum smart contracts.Addressing limitations of existing vulnerability detection methods relying onsingle-feature or single-model deep learning techniques, our method surpassesaccuracy and effectiveness constraints. We assess VulnSense using a collectionof 1.769 smart contracts derived from the combination of three datasets:Curated, SolidiFI-Benchmark, and Smartbugs Wild. We then make a comparison withvarious unimodal and multimodal learning techniques contributed by GNN, BiLSTMand BERT architectures. The experimental outcomes demonstrate the superiorperformance of our proposed approach, achieving an average accuracy of 77.96\%across all three categories of vulnerable smart contracts.</description><author>Phan The Duy, Nghi Hoang Khoa, Nguyen Huu Quyen, Le Cong Trinh, Vu Trung Kien, Trinh Minh Hoang, Van-Hau Pham</author><pubDate>Fri, 15 Sep 2023 16:26:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08474v1</guid></item><item><title>Don't Get Me Wrong: How to Apply Deep Visual Interpretations to Time Series</title><link>http://arxiv.org/abs/2203.07861v2</link><description>The correct interpretation and understanding of deep learning models areessential in many applications. Explanatory visual interpretation approachesfor image, and natural language processing allow domain experts to validate andunderstand almost any deep learning model. However, they fall short whengeneralizing to arbitrary time series, which is inherently less intuitive andmore diverse. Whether a visualization explains valid reasoning or captures theactual features is difficult to judge. Hence, instead of blind trust, we needan objective evaluation to obtain trustworthy quality metrics. We propose aframework of six orthogonal metrics for gradient-, propagation- orperturbation-based post-hoc visual interpretation methods for time seriesclassification and segmentation tasks. An experimental study includes popularneural network architectures for time series and nine visual interpretationmethods. We evaluate the visual interpretation methods with diverse datasetsfrom the UCR repository and a complex, real-world dataset and study theinfluence of standard regularization techniques during training. We show thatnone of the methods consistently outperforms others on all metrics, while someare sometimes ahead. Our insights and recommendations allow experts to choosesuitable visualization techniques for the model and task.</description><author>Christoffer Loeffler, Wei-Cheng Lai, Bjoern Eskofier, Dario Zanca, Lukas Schmidt, Christopher Mutschler</author><pubDate>Fri, 15 Sep 2023 16:25:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.07861v2</guid></item><item><title>On the limitations of data-driven weather forecasting models</title><link>http://arxiv.org/abs/2309.08473v1</link><description>As in many other areas of engineering and applied science, Machine Learning(ML) is having a profound impact in the domain of Weather and ClimatePrediction. A very recent development in this area has been the emergence offully data-driven ML prediction models which routinely claim superiorperformance to that of traditional physics-based models. In this work, weexamine some aspects of the forecasts produced by an exemplar of the currentgeneration of ML models, Pangu-Weather, with a focus on the fidelity andphysical consistency of those forecasts and how these characteristics relate toperceived forecast performance. The main conclusion is that Pangu-Weatherforecasts, and by extension those of similar ML models, do not have thefidelity and physical consistency of physics-based models and their advantagein accuracy on traditional deterministic metrics of forecast skill can beattributed, to a large extent, to these peculiarities. Similarly to othercurrent post-processing technologies, ML models appear to be able to add valueto standard NWP outputs for specific forecast applications and combined withtheir extremely low computational cost during deployment, will likely providean additional, useful source of forecast information.</description><author>Massimo Bonavita</author><pubDate>Fri, 15 Sep 2023 16:21:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08473v1</guid></item><item><title>Representing Edge Flows on Graphs via Sparse Cell Complexes</title><link>http://arxiv.org/abs/2309.01632v2</link><description>Obtaining sparse, interpretable representations of observable data is crucialin many machine learning and signal processing tasks. For data representingflows along the edges of a graph, an intuitively interpretable way to obtainsuch representations is to lift the graph structure to a simplicial complex:The eigenvectors of the associated Hodge-Laplacian, respectively the incidencematrices of the corresponding simplicial complex then induce a Hodgedecomposition, which can be used to represent the observed data in terms ofgradient, curl, and harmonic flows. In this paper, we generalize this approachto cellular complexes and introduce the cell inference optimization problem,i.e., the problem of augmenting the observed graph by a set of cells, such thatthe eigenvectors of the associated Hodge Laplacian provide a sparse,interpretable representation of the observed edge flows on the graph. We showthat this problem is NP-hard and introduce an efficient approximation algorithmfor its solution. Experiments on real-world and synthetic data demonstrate thatour algorithm outperforms current state-of-the-art methods while beingcomputationally efficient.</description><author>Josef Hoppe, Michael T. Schaub</author><pubDate>Fri, 15 Sep 2023 16:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01632v2</guid></item><item><title>TreeLearn: A Comprehensive Deep Learning Method for Segmenting Individual Trees from Forest Point Clouds</title><link>http://arxiv.org/abs/2309.08471v1</link><description>Laser-scanned point clouds of forests make it possible to extract valuableinformation for forest management. To consider single trees, a forest pointcloud needs to be segmented into individual tree point clouds. Existingsegmentation methods are usually based on hand-crafted algorithms, such asidentifying trunks and growing trees from them, and face difficulties in denseforests with overlapping tree crowns. In this study, we propose\mbox{TreeLearn}, a deep learning-based approach for semantic and instancesegmentation of forest point clouds. Unlike previous methods, TreeLearn istrained on already segmented point clouds in a data-driven manner, making itless reliant on predefined features and algorithms. Additionally, we introducea new manually segmented benchmark forest dataset containing 156 full trees,and 79 partial trees, that have been cleanly segmented by hand. This enablesthe evaluation of instance segmentation performance going beyond justevaluating the detection of individual trees. We trained TreeLearn on forestpoint clouds of 6665 trees, labeled using the Lidar360 software. An evaluationon the benchmark dataset shows that TreeLearn performs equally well or betterthan the algorithm used to generate its training data. Furthermore, themethod's performance can be vastly improved by fine-tuning on the cleanlylabeled benchmark dataset. The TreeLearn code is availabe fromhttps://github.com/ecker-lab/TreeLearn. The data as well as trained models canbe found at https://doi.org/10.25625/VPMPID.</description><author>Jonathan Henrich, Jan van Delden, Dominik Seidel, Thomas Kneib, Alexander Ecker</author><pubDate>Fri, 15 Sep 2023 16:20:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08471v1</guid></item><item><title>SilverRetriever: Advancing Neural Passage Retrieval for Polish Question Answering</title><link>http://arxiv.org/abs/2309.08469v1</link><description>Modern open-domain question answering systems often rely on accurate andefficient retrieval components to find passages containing the facts necessaryto answer the question. Recently, neural retrievers have gained popularity overlexical alternatives due to their superior performance. However, most of thework concerns popular languages such as English or Chinese. For others, such asPolish, few models are available. In this work, we present SilverRetriever, aneural retriever for Polish trained on a diverse collection of manually orweakly labeled datasets. SilverRetriever achieves much better results thanother Polish models and is competitive with larger multilingual models.Together with the model, we open-source five new passage retrieval datasets.</description><author>Piotr Rybak, Maciej Ogrodniczuk</author><pubDate>Fri, 15 Sep 2023 16:19:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08469v1</guid></item><item><title>Explaining Search Result Stances to Opinionated People</title><link>http://arxiv.org/abs/2309.08460v1</link><description>People use web search engines to find information before forming opinions,which can lead to practical decisions with different levels of impact. Thecognitive effort of search can leave opinionated users vulnerable to cognitivebiases, e.g., the confirmation bias. In this paper, we investigate whetherstance labels and their explanations can help users consume more diverse searchresults. We automatically classify and label search results on three topics(i.e., intellectual property rights, school uniforms, and atheism) as against,neutral, and in favor, and generate explanations for these labels. In a userstudy (N =203), we then investigate whether search result stance bias (balancedvs biased) and the level of explanation (plain text, label only, label andexplanation) influence the diversity of search results clicked. We find thatstance labels and explanations lead to a more diverse search resultconsumption. However, we do not find evidence for systematic opinion changeamong users in this context. We believe these results can help designers ofsearch engines to make more informed design decisions.</description><author>Z. Wu, T. Draws, F. Cau, F. Barile, A. Rieger, N. Tintarev</author><pubDate>Fri, 15 Sep 2023 16:08:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08460v1</guid></item><item><title>Mixture Encoder Supporting Continuous Speech Separation for Meeting Recognition</title><link>http://arxiv.org/abs/2309.08454v1</link><description>Many real-life applications of automatic speech recognition (ASR) requireprocessing of overlapped speech. A commonmethod involves first separating thespeech into overlap-free streams and then performing ASR on the resultingsignals. Recently, the inclusion of a mixture encoder in the ASR model has beenproposed. This mixture encoder leverages the original overlapped speech tomitigate the effect of artifacts introduced by the speech separation.Previously, however, the method only addressed two-speaker scenarios. In thiswork, we extend this approach to more natural meeting contexts featuring anarbitrary number of speakers and dynamic overlaps. We evaluate the performanceusing different speech separators, including the powerful TF-GridNet model. Ourexperiments show state-of-the-art performance on the LibriCSS dataset andhighlight the advantages of the mixture encoder. Furthermore, they demonstratethe strong separation of TF-GridNet which largely closes the gap betweenprevious methods and oracle separation.</description><author>Peter Vieting, Simon Berger, Thilo von Neumann, Christoph Boeddeker, Ralf Schlüter, Reinhold Haeb-Umbach</author><pubDate>Fri, 15 Sep 2023 15:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08454v1</guid></item><item><title>Do Random and Chaotic Sequences Really Cause Different PSO Performance? Further Results</title><link>http://arxiv.org/abs/2309.08449v1</link><description>Empirical results show that PSO performance may be different if using eitherchaotic or random sequences to drive the algorithm's search dynamics. Weanalyze the phenomenon by evaluating the performance based on a benchmark oftest functions and comparing random and chaotic sequences according to equalityor difference in underlying distribution or density. Our results show that theunderlying distribution is the main influential factor in performance and thusthe assumption of general and systematic performance differences between chaosand random appears not plausible.</description><author>{Paul Moritz Nörenberg, Hendrik Richter</author><pubDate>Fri, 15 Sep 2023 15:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08449v1</guid></item><item><title>Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite</title><link>http://arxiv.org/abs/2309.08448v1</link><description>The evaluation of large language models is an essential task in the field oflanguage understanding and generation. As language models continue to advance,the need for effective benchmarks to assess their performance has becomeimperative. In the context of Traditional Chinese, there is a scarcity ofcomprehensive and diverse benchmarks to evaluate the capabilities of languagemodels, despite the existence of certain benchmarks such as DRCD, TTQA, CMDQA,and FGC dataset. To address this gap, we propose a novel set of benchmarks thatleverage existing English datasets and are tailored to evaluate language modelsin Traditional Chinese. These benchmarks encompass a wide range of tasks,including contextual question-answering, summarization, classification, andtable understanding. The proposed benchmarks offer a comprehensive evaluationframework, enabling the assessment of language models' capabilities acrossdifferent tasks. In this paper, we evaluate the performance of GPT-3.5,Taiwan-LLaMa-v1.0, and Model 7-C, our proprietary model, on these benchmarks.The evaluation results highlight that our model, Model 7-C, achievesperformance comparable to GPT-3.5 with respect to a part of the evaluatedcapabilities. In an effort to advance the evaluation of language models inTraditional Chinese and stimulate further research in this field, we haveopen-sourced our benchmark and opened the model for trial.</description><author>Chan-Jan Hsu, Chang-Le Liu, Feng-Ting Liao, Po-Chun Hsu, Yi-Chang Chen, Da-shan Shiu</author><pubDate>Fri, 15 Sep 2023 15:52:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08448v1</guid></item><item><title>Neural Network Exemplar Parallelization with Go</title><link>http://arxiv.org/abs/2309.08444v1</link><description>This paper presents a case for exemplar parallelism of neural networks usingGo as parallelization framework. Further it is shown that also limitedmulti-core hardware systems are feasible for these parallelization tasks, asnotebooks and single board computer systems. The main question was how muchspeedup can be generated when using concurrent Go goroutines specifically. Asimple concurrent feedforward network for MNIST digit recognition with theprogramming language Go was created to find the answer. The first findings whenusing a notebook (Lenovo Yoga 2) showed a speedup of 252% when utilizing 4goroutines. Testing a single board computer (Banana Pi M3) delivered moreconvincing results: 320% with 4 goroutines, and 432% with 8 goroutines.</description><author>Georg Wiesinger, Erich Schikuta</author><pubDate>Fri, 15 Sep 2023 15:46:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08444v1</guid></item><item><title>TIDE: Time Derivative Diffusion for Deep Learning on Graphs</title><link>http://arxiv.org/abs/2212.02483v3</link><description>A prominent paradigm for graph neural networks is based on themessage-passing framework. In this framework, information communication isrealized only between neighboring nodes. The challenge of approaches that usethis paradigm is to ensure efficient and accurate long-distance communicationbetween nodes, as deep convolutional networks are prone to oversmoothing. Inthis paper, we present a novel method based on time derivative graph diffusion(TIDE) to overcome these structural limitations of the message-passingframework. Our approach allows for optimizing the spatial extent of diffusionacross various tasks and network channels, thus enabling medium andlong-distance communication efficiently. Furthermore, we show that ourarchitecture design also enables local message-passing and thus inherits fromthe capabilities of local message-passing approaches. We show that on bothwidely used graph benchmarks and synthetic mesh and graph datasets, theproposed framework outperforms state-of-the-art methods by a significant margin</description><author>Maysam Behmanesh, Maximilian Krahn, Maks Ovsjanikov</author><pubDate>Fri, 15 Sep 2023 15:45:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02483v3</guid></item><item><title>Toward responsible face datasets: modeling the distribution of a disentangled latent space for sampling face images from demographic groups</title><link>http://arxiv.org/abs/2309.08442v1</link><description>Recently, it has been exposed that some modern facial recognition systemscould discriminate specific demographic groups and may lead to unfair attentionwith respect to various facial attributes such as gender and origin. The mainreason are the biases inside datasets, unbalanced demographics, used to traintheses models. Unfortunately, collecting a large-scale balanced dataset withrespect to various demographics is impracticable. In this paper, we investigate as an alternative the generation of a balancedand possibly bias-free synthetic dataset that could be used to train, toregularize or to evaluate deep learning-based facial recognition models. Wepropose to use a simple method for modeling and sampling a disentangledprojection of a StyleGAN latent space to generate any combination ofdemographic groups (e.g. $hispanic-female$). Our experiments show that we cansynthesis any combination of demographic groups effectively and the identitiesare different from the original training dataset. We also released the sourcecode.</description><author>Parsa Rahimi, Christophe Ecabert, Sebastien Marcel</author><pubDate>Fri, 15 Sep 2023 15:42:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08442v1</guid></item><item><title>Chunked Attention-based Encoder-Decoder Model for Streaming Speech Recognition</title><link>http://arxiv.org/abs/2309.08436v1</link><description>We study a streamable attention-based encoder-decoder model in which eitherthe decoder, or both the encoder and decoder, operate on pre-defined,fixed-size windows called chunks. A special end-of-chunk (EOC) symbol advancesfrom one chunk to the next chunk, effectively replacing the conventionalend-of-sequence symbol. This modification, while minor, situates our model asequivalent to a transducer model that operates on chunks instead of frames,where EOC corresponds to the blank symbol. We further explore the remainingdifferences between a standard transducer and our model. Additionally, weexamine relevant aspects such as long-form speech generalization, beam size,and length normalization. Through experiments on Librispeech and TED-LIUM-v2,and by concatenating consecutive sequences for long-form trials, we find thatour streamable model maintains competitive performance compared to thenon-streamable variant and generalizes very well to long-form speech.</description><author>Mohammad Zeineldeen, Albert Zeyer, Ralf Schlüter, Hermann Ney</author><pubDate>Fri, 15 Sep 2023 15:36:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08436v1</guid></item><item><title>Segment Anything Model for Brain Tumor Segmentation</title><link>http://arxiv.org/abs/2309.08434v1</link><description>Glioma is a prevalent brain tumor that poses a significant health risk toindividuals. Accurate segmentation of brain tumor is essential for clinicaldiagnosis and treatment. The Segment Anything Model(SAM), released by Meta AI,is a fundamental model in image segmentation and has excellent zero-samplegeneralization capabilities. Thus, it is interesting to apply SAM to the taskof brain tumor segmentation. In this study, we evaluated the performance of SAMon brain tumor segmentation and found that without any model fine-tuning, thereis still a gap between SAM and the current state-of-the-art(SOTA) model.</description><author>Peng Zhang, Yaping Wang</author><pubDate>Fri, 15 Sep 2023 15:33:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08434v1</guid></item><item><title>IHT-Inspired Neural Network for Single-Snapshot DOA Estimation with Sparse Linear Arrays</title><link>http://arxiv.org/abs/2309.08429v1</link><description>Single-snapshot direction-of-arrival (DOA) estimation using sparse lineararrays (SLAs) has gained significant attention in the field of automotive MIMOradars. This is due to the dynamic nature of automotive settings, wheremultiple snapshots aren't accessible, and the importance of minimizing hardwarecosts. Low-rank Hankel matrix completion has been proposed to interpolate themissing elements in SLAs. However, the solvers of matrix completion, such asiterative hard thresholding (IHT), heavily rely on expert knowledge ofhyperparameter tuning and lack task-specificity. Besides, IHT involvestruncated-singular value decomposition (t-SVD), which has high computationalcost in each iteration. In this paper, we propose an IHT-inspired neuralnetwork for single-snapshot DOA estimation with SLAs, termed IHT-Net. Weutilize a recurrent neural network structure to parameterize the IHT algorithm.Additionally, we integrate shallow-layer autoencoders to replace t-SVD,reducing computational overhead while generating a novel optimizer throughsupervised learning. IHT-Net maintains strong interpretability as its networklayer operations align with the iterations of the IHT algorithm. The learnedoptimizer exhibits fast convergence and higher accuracy in the full arraysignal reconstruction followed by single-snapshot DOA estimation. Numericalresults validate the effectiveness of the proposed method.</description><author>Yunqiao Hu, Shunqiao Sun</author><pubDate>Fri, 15 Sep 2023 15:30:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08429v1</guid></item><item><title>3D-Spatiotemporal Forecasting the Expansion of Supernova Shells Using Deep Learning toward High-Resolution Galaxy Simulations</title><link>http://arxiv.org/abs/2302.00026v2</link><description>Supernova (SN) plays an important role in galaxy formation and evolution. Inhigh-resolution galaxy simulations using massively parallel computing, shortintegration timesteps for SNe are serious bottlenecks. This is an urgent issuethat needs to be resolved for future higher-resolution galaxy simulations. Onepossible solution would be to use the Hamiltonian splitting method, in whichregions requiring short timesteps are integrated separately from the entiresystem. To apply this method to the particles affected by SNe in asmoothed-particle hydrodynamics simulation, we need to detect the shape of theshell on and within which such SN-affected particles reside during thesubsequent global step in advance. In this paper, we develop a deep learningmodel, 3D-MIM, to predict a shell expansion after a SN explosion. Trained onturbulent cloud simulations with particle mass $m_{\rm gas}$~=~1 M$_\odot$, themodel accurately reproduces the anisotropic shell shape, where densitiesdecrease by over 10 per cent by the explosion. We also demonstrate that themodel properly predicts the shell radius in the uniform medium beyond thetraining dataset of inhomogeneous turbulent clouds. We conclude that our modelenables the forecast of the shell and its interior where SN-affected particleswill be present.</description><author>Keiya Hirashima, Kana Moriwaki, Michiko S. Fujii, Yutaka Hirai, Takayuki R. Saitoh, Junichiro Makino</author><pubDate>Fri, 15 Sep 2023 15:29:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00026v2</guid></item><item><title>X-PDNet: Accurate Joint Plane Instance Segmentation and Monocular Depth Estimation with Cross-Task Distillation and Boundary Correction</title><link>http://arxiv.org/abs/2309.08424v1</link><description>Segmentation of planar regions from a single RGB image is a particularlyimportant task in the perception of complex scenes. To utilize both visual andgeometric properties in images, recent approaches often formulate the problemas a joint estimation of planar instances and dense depth through featurefusion mechanisms and geometric constraint losses. Despite promising results,these methods do not consider cross-task feature distillation and performpoorly in boundary regions. To overcome these limitations, we propose X-PDNet,a framework for the multitask learning of plane instance segmentation and depthestimation with improvements in the following two aspects. Firstly, weconstruct the cross-task distillation design which promotes early informationsharing between dual-tasks for specific task improvements. Secondly, wehighlight the current limitations of using the ground truth boundary to developboundary regression loss, and propose a novel method that exploits depthinformation to support precise boundary region segmentation. Finally, wemanually annotate more than 3000 images from Stanford 2D-3D-Semantics datasetand make available for evaluation of plane instance segmentation. Through theexperiments, our proposed methods prove the advantages, outperforming thebaseline with large improvement margins in the quantitative results on theScanNet and the Stanford 2D-3D-S dataset, demonstrating the effectiveness ofour proposals.</description><author>Duc Cao Dinh, J Lim</author><pubDate>Fri, 15 Sep 2023 15:27:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08424v1</guid></item><item><title>MIML: Multiplex Image Machine Learning for High Precision Cell Classification via Mechanical Traits within Microfluidic Systems</title><link>http://arxiv.org/abs/2309.08421v1</link><description>Label-free cell classification is advantageous for supplying pristine cellsfor further use or examination, yet existing techniques frequently fall shortin terms of specificity and speed. In this study, we address these limitationsthrough the development of a novel machine learning framework, Multiplex ImageMachine Learning (MIML). This architecture uniquely combines label-free cellimages with biomechanical property data, harnessing the vast, oftenunderutilized morphological information intrinsic to each cell. By integratingboth types of data, our model offers a more holistic understanding of thecellular properties, utilizing morphological information typically discarded intraditional machine learning models. This approach has led to a remarkable98.3\% accuracy in cell classification, a substantial improvement over modelsthat only consider a single data type. MIML has been proven effective inclassifying white blood cells and tumor cells, with potential for broaderapplication due to its inherent flexibility and transfer learning capability.It's particularly effective for cells with similar morphology but distinctbiomechanical properties. This innovative approach has significant implicationsacross various fields, from advancing disease diagnostics to understandingcellular behavior.</description><author>Khayrul Islam, Ratul Paul, Shen Wang, Yaling Liu</author><pubDate>Fri, 15 Sep 2023 15:23:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08421v1</guid></item><item><title>FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning</title><link>http://arxiv.org/abs/2309.08420v1</link><description>Cross-domain Sequential Recommendation (CSR) which leverages user sequencedata from multiple domains has received extensive attention in recent years.However, the existing CSR methods require sharing origin user data acrossdomains, which violates the General Data Protection Regulation (GDPR). Thus, itis necessary to combine federated learning (FL) and CSR to fully utilizeknowledge from different domains while preserving data privacy. Nonetheless,the sequence feature heterogeneity across different domains significantlyimpacts the overall performance of FL. In this paper, we propose FedDCSR, anovel federated cross-domain sequential recommendation framework viadisentangled representation learning. Specifically, to address the sequencefeature heterogeneity across domains, we introduce an approach calledinter-intra domain sequence representation disentanglement (SRD) to disentanglethe user sequence features into domain-shared and domain-exclusive features. Inaddition, we design an intra domain contrastive infomax (CIM) strategy to learnricher domain-exclusive features of users by performing data augmentation onuser sequences. Extensive experiments on three real-world scenarios demonstratethat FedDCSR achieves significant improvements over existing baselines.</description><author>Hongyu Zhang, Dongyi Zheng, Xu Yang, Jiyuan Feng, Qing Liao</author><pubDate>Fri, 15 Sep 2023 15:23:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08420v1</guid></item><item><title>A Comprehensive Review and a Taxonomy of Edge Machine Learning: Requirements, Paradigms, and Techniques</title><link>http://arxiv.org/abs/2302.08571v2</link><description>The union of Edge Computing (EC) and Artificial Intelligence (AI) has broughtforward the Edge AI concept to provide intelligent solutions close to theend-user environment, for privacy preservation, low latency to real-timeperformance, and resource optimization. Machine Learning (ML), as the mostadvanced branch of AI in the past few years, has shown encouraging results andapplications in the edge environment. Nevertheless, edge-powered ML solutionsare more complex to realize due to the joint constraints from both edgecomputing and AI domains, and the corresponding solutions are expected to beefficient and adapted in technologies such as data processing, modelcompression, distributed inference, and advanced learning paradigms for Edge MLrequirements. Despite the fact that a great deal of the attention garnered byEdge ML is gained in both the academic and industrial communities, we noticedthe lack of a complete survey on existing Edge ML technologies to provide acommon understanding of this concept. To tackle this, this paper aims atproviding a comprehensive taxonomy and a systematic review of Edge MLtechniques, focusing on the soft computing aspects of existing paradigms andtechniques. We start by identifying the Edge ML requirements driven by thejoint constraints. We then extensively survey more than twenty paradigms andtechniques along with their representative work, covering two main parts: edgeinference, and edge learning. In particular, we analyze how each technique fitsinto Edge ML by meeting a subset of the identified requirements. We alsosummarize Edge ML frameworks and open issues to shed light on future directionsfor Edge ML.</description><author>Wenbin Li, Hakim Hacid, Ebtesam Almazrouei, Merouane Debbah</author><pubDate>Fri, 15 Sep 2023 15:20:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08571v2</guid></item><item><title>Deformable Neural Radiance Fields using RGB and Event Cameras</title><link>http://arxiv.org/abs/2309.08416v1</link><description>Modeling Neural Radiance Fields for fast-moving deformable objects fromvisual data alone is a challenging problem. A major issue arises due to thehigh deformation and low acquisition rates. To address this problem, we proposeto use event cameras that offer very fast acquisition of visual change in anasynchronous manner. In this work, we develop a novel method to model thedeformable neural radiance fields using RGB and event cameras. The proposedmethod uses the asynchronous stream of events and calibrated sparse RGB frames.In our setup, the camera pose at the individual events required to integratethem into the radiance fields remains unknown. Our method jointly optimizesthese poses and the radiance field. This happens efficiently by leveraging thecollection of events at once and actively sampling the events during learning.Experiments conducted on both realistically rendered graphics and real-worlddatasets demonstrate a significant benefit of the proposed method over thestate-of-the-art and the compared baseline. This shows a promising direction for modeling deformable neural radiancefields in real-world dynamic scenes.</description><author>Qi Ma, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool</author><pubDate>Fri, 15 Sep 2023 15:19:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08416v1</guid></item><item><title>ISLE: A Framework for Image Level Semantic Segmentation Ensemble</title><link>http://arxiv.org/abs/2303.07898v4</link><description>One key bottleneck of employing state-of-the-art semantic segmentationnetworks in the real world is the availability of training labels. Conventionalsemantic segmentation networks require massive pixel-wise annotated labels toreach state-of-the-art prediction quality. Hence, several works focus onsemantic segmentation networks trained with only image-level annotations.However, when scrutinizing the results of state-of-the-art in more detail, wenotice that they are remarkably close to each other on average predictionquality, different approaches perform better in different classes whileproviding low quality in others. To address this problem, we propose a novelframework, ISLE, which employs an ensemble of the "pseudo-labels" for a givenset of different semantic segmentation techniques on a class-wise level.Pseudo-labels are the pixel-wise predictions of the image-level semanticsegmentation frameworks used to train the final segmentation model. Ourpseudo-labels seamlessly combine the strong points of multiple segmentationtechniques approaches to reach superior prediction quality. We reach up to 2.4%improvement over ISLE's individual components. An exhaustive analysis wasperformed to demonstrate ISLE's effectiveness over state-of-the-art frameworksfor image-level semantic segmentation.</description><author>Erik Ostrowski, Muhammad Shafique</author><pubDate>Fri, 15 Sep 2023 15:18:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07898v4</guid></item><item><title>A new method of modeling the multi-stage decision-making process of CRT using machine learning with uncertainty quantification</title><link>http://arxiv.org/abs/2309.08415v1</link><description>Aims. The purpose of this study is to create a multi-stage machine learningmodel to predict cardiac resynchronization therapy (CRT) response for heartfailure (HF) patients. This model exploits uncertainty quantification torecommend additional collection of single-photon emission computed tomographymyocardial perfusion imaging (SPECT MPI) variables if baseline clinicalvariables and features from electrocardiogram (ECG) are not sufficient.Methods. 218 patients who underwent rest-gated SPECT MPI were enrolled in thisstudy. CRT response was defined as an increase in left ventricular ejectionfraction (LVEF) &gt; 5% at a 6 month follow-up. A multi-stage ML model was createdby combining two ensemble models. Results. The response rate for CRT was 55.5%(n = 121) with overall male gender 61.0% (n = 133), an average age of 62.0, andLVEF of 27.7. The multi-stage model performed similarly to Ensemble 2 (whichutilized the additional SPECT data) with AUC of 0.75 vs. 0.77, accuracy of 0.71vs. 0.69, sensitivity of 0.70 vs. 0.72, and specificity 0.72 vs. 0.65,respectively. However, the multi-stage model only required SPECT MPI data for52.7% of the patients across all folds. Conclusions. By using rule-based logicstemming from uncertainty quantification, the multi-stage model was able toreduce the need for additional SPECT MPI data acquisition without sacrificingperformance.</description><author>Kristoffer Larsena, Chen Zhao, Joyce Keyak, Qiuying Sha, Diana Paezd, Xinwei Zhang, Jiangang Zou, Amalia Peixf, Weihua Zhou</author><pubDate>Fri, 15 Sep 2023 15:18:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08415v1</guid></item><item><title>Make Deep Networks Shallow Again</title><link>http://arxiv.org/abs/2309.08414v1</link><description>Deep neural networks have a good success record and are thus viewed as thebest architecture choice for complex applications. Their main shortcoming hasbeen, for a long time, the vanishing gradient which prevented the numericaloptimization algorithms from acceptable convergence. A breakthrough has beenachieved by the concept of residual connections -- an identity mapping parallelto a conventional layer. This concept is applicable to stacks of layers of thesame dimension and substantially alleviates the vanishing gradient problem. Astack of residual connection layers can be expressed as an expansion of termssimilar to the Taylor expansion. This expansion suggests the possibility oftruncating the higher-order terms and receiving an architecture consisting of asingle broad layer composed of all initially stacked layers in parallel. Inother words, a sequential deep architecture is substituted by a parallelshallow one. Prompted by this theory, we investigated the performancecapabilities of the parallel architecture in comparison to the sequential one.The computer vision datasets MNIST and CIFAR10 were used to train botharchitectures for a total of 6912 combinations of varying numbers ofconvolutional layers, numbers of filters, kernel sizes, and other metaparameters. Our findings demonstrate a surprising equivalence between the deep(sequential) and shallow (parallel) architectures. Both layouts producedsimilar results in terms of training and validation set loss. This discoveryimplies that a wide, shallow architecture can potentially replace a deepnetwork without sacrificing performance. Such substitution has the potential tosimplify network architectures, improve optimization efficiency, and acceleratethe training process.</description><author>Bernhard Bermeitinger, Tomas Hrycej, Siegfried Handschuh</author><pubDate>Fri, 15 Sep 2023 15:18:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08414v1</guid></item><item><title>ReFit: A Framework for Refinement of Weakly Supervised Semantic Segmentation using Object Border Fitting for Medical Images</title><link>http://arxiv.org/abs/2303.07853v3</link><description>Weakly Supervised Semantic Segmentation (WSSS) relying only on image-levelsupervision is a promising approach to deal with the need for Segmentationnetworks, especially for generating a large number of pixel-wise masks in agiven dataset. However, most state-of-the-art image-level WSSS techniques lackan understanding of the geometric features embedded in the images since thenetwork cannot derive any object boundary information from just image-levellabels. We define a boundary here as the line separating an object and itsbackground, or two different objects. To address this drawback, we areproposing our novel ReFit framework, which deploys state-of-the-art classactivation maps combined with various post-processing techniques in order toachieve fine-grained higher-accuracy segmentation masks. To achieve this, weinvestigate a state-of-the-art unsupervised segmentation network that can beused to construct a boundary map, which enables ReFit to predict objectlocations with sharper boundaries. By applying our method to WSSS predictions,we achieved up to 10% improvement over the current state-of-the-art WSSSmethods for medical imaging. The framework is open-source, to ensure that ourresults are reproducible, and accessible online athttps://github.com/bharathprabakaran/ReFit.</description><author>Bharath Srinivas Prabakaran, Erik Ostrowski, Muhammad Shafique</author><pubDate>Fri, 15 Sep 2023 15:17:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07853v3</guid></item><item><title>Deep learning for bias-correcting CMIP6-class Earth system models</title><link>http://arxiv.org/abs/2301.01253v2</link><description>The accurate representation of precipitation in Earth system models (ESMs) iscrucial for reliable projections of the ecological and socioeconomic impacts inresponse to anthropogenic global warming. The complex cross-scale interactionsof processes that produce precipitation are challenging to model, however,inducing potentially strong biases in ESM fields, especially regardingextremes. State-of-the-art bias correction methods only address errors in thesimulated frequency distributions locally at every individual grid cell.Improving unrealistic spatial patterns of the ESM output, which would requirespatial context, has not been possible so far. Here, we show that apost-processing method based on physically constrained generative adversarialnetworks (cGANs) can correct biases of a state-of-the-art, CMIP6-class ESM bothin local frequency distributions and in the spatial patterns at once. While ourmethod improves local frequency distributions equally well as gold-standardbias-adjustment frameworks, it strongly outperforms any existing methods in thecorrection of spatial patterns, especially in terms of the characteristicspatial intermittency of precipitation extremes.</description><author>Philipp Hess, Stefan Lange, Christof Schötz, Niklas Boers</author><pubDate>Fri, 15 Sep 2023 15:15:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.01253v2</guid></item><item><title>Constraint-Free Structure Learning with Smooth Acyclic Orientations</title><link>http://arxiv.org/abs/2309.08406v1</link><description>The structure learning problem consists of fitting data generated by aDirected Acyclic Graph (DAG) to correctly reconstruct its arcs. In thiscontext, differentiable approaches constrain or regularize the optimizationproblem using a continuous relaxation of the acyclicity property. Thecomputational cost of evaluating graph acyclicity is cubic on the number ofnodes and significantly affects scalability. In this paper we introduce COSMO,a constraint-free continuous optimization scheme for acyclic structurelearning. At the core of our method, we define a differentiable approximationof an orientation matrix parameterized by a single priority vector. Differentlyfrom previous work, our parameterization fits a smooth orientation matrix andthe resulting acyclic adjacency matrix without evaluating acyclicity at anystep. Despite the absence of explicit constraints, we prove that COSMO alwaysconverges to an acyclic solution. In addition to being asymptotically faster,our empirical analysis highlights how COSMO performance on graph reconstructioncompares favorably with competing structure learning methods.</description><author>Riccardo Massidda, Francesco Landolfi, Martina Cinquini, Davide Bacciu</author><pubDate>Fri, 15 Sep 2023 15:08:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08406v1</guid></item><item><title>A Comprehensive Study on the Robustness of Image Classification and Object Detection in Remote Sensing: Surveying and Benchmarking</title><link>http://arxiv.org/abs/2306.12111v2</link><description>Deep neural networks (DNNs) have found widespread applications ininterpreting remote sensing (RS) imagery. However, it has been demonstrated inprevious works that DNNs are vulnerable to different types of noises,particularly adversarial noises. Surprisingly, there has been a lack ofcomprehensive studies on the robustness of RS tasks, prompting us to undertakea thorough survey and benchmark on the robustness of image classification andobject detection in RS. To our best knowledge, this study represents the firstcomprehensive examination of both natural robustness and adversarial robustnessin RS tasks. Specifically, we have curated and made publicly available datasetsthat contain natural and adversarial noises. These datasets serve as valuableresources for evaluating the robustness of DNNs-based models. To provide acomprehensive assessment of model robustness, we conducted meticulousexperiments with numerous different classifiers and detectors, encompassing awide range of mainstream methods. Through rigorous evaluation, we haveuncovered insightful and intriguing findings, which shed light on therelationship between adversarial noise crafting and model training, yielding adeeper understanding of the susceptibility and limitations of various models,and providing guidance for the development of more resilient and robust models</description><author>Shaohui Mei, Jiawei Lian, Xiaofei Wang, Yuru Su, Mingyang Ma, Lap-Pui Chau</author><pubDate>Fri, 15 Sep 2023 15:00:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12111v2</guid></item><item><title>Refining time-space traffic diagrams: A simple multiple linear regression model</title><link>http://arxiv.org/abs/2204.04457v4</link><description>A time-space traffic (TS) diagram, which presents traffic states intime-space cells with color, is an important traffic analysis and visualizationtool. Despite its importance for transportation research and engineering, mostTS diagrams that have already existed or are being produced are too coarse toexhibit detailed traffic dynamics due to the limitations of existinginformation technology and traffic infrastructure investment. To increase theresolution of a TS diagram and enable it to present ample traffic details, thispaper introduces the TS diagram refinement problem and proposes a multiplelinear regression-based model to solve the problem. Two tests, which attempt toincrease the resolution of a TS diagram 4 and 16 times, are carried out toevaluate the performance of the proposed model. Data collected at differenttimes, in different locations and even in different countries are employed tothoroughly evaluate the accuracy and transferability of the proposed model.Strict tests with diverse data show that the proposed model, despite itssimplicity, is able to refine a TS diagram with promising accuracy and reliabletransferability. The proposed refinement model will "save" widely existing TSdiagrams from their blurry "faces" and enable TS diagrams to show more trafficdetails.</description><author>Zhengbing He</author><pubDate>Fri, 15 Sep 2023 14:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.04457v4</guid></item><item><title>3D SA-UNet: 3D Spatial Attention UNet with 3D ASPP for White Matter Hyperintensities Segmentation</title><link>http://arxiv.org/abs/2309.08402v1</link><description>White Matter Hyperintensity (WMH) is an imaging feature related to variousdiseases such as dementia and stroke. Accurately segmenting WMH using computertechnology is crucial for early disease diagnosis. However, this task remainschallenging due to the small lesions with low contrast and high discontinuityin the images, which contain limited contextual and spatial information. Toaddress this challenge, we propose a deep learning model called 3D SpatialAttention U-Net (3D SA-UNet) for automatic WMH segmentation using only FluidAttenuation Inversion Recovery (FLAIR) scans. The 3D SA-UNet introduces a 3DSpatial Attention Module that highlights important lesion features, such asWMH, while suppressing unimportant regions. Additionally, to capture featuresat different scales, we extend the Atrous Spatial Pyramid Pooling (ASPP) moduleto a 3D version, enhancing the segmentation performance of the network. Weevaluate our method on publicly available dataset and demonstrate theeffectiveness of 3D spatial attention module and 3D ASPP in WMH segmentation.Through experimental results, it has been demonstrated that our proposed 3DSA-UNet model achieves higher accuracy compared to other state-of-the-art 3Dconvolutional neural networks.</description><author>Changlu Guo</author><pubDate>Fri, 15 Sep 2023 14:54:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08402v1</guid></item><item><title>PottsMGNet: A Mathematical Explanation of Encoder-Decoder Based Neural Networks</title><link>http://arxiv.org/abs/2307.09039v2</link><description>For problems in image processing and many other fields, a large class ofeffective neural networks has encoder-decoder-based architectures. Althoughthese networks have made impressive performances, mathematical explanations oftheir architectures are still underdeveloped. In this paper, we study theencoder-decoder-based network architecture from the algorithmic perspective andprovide a mathematical explanation. We use the two-phase Potts model for imagesegmentation as an example for our explanations. We associate the segmentationproblem with a control problem in the continuous setting. Then, multigridmethod and operator splitting scheme, the PottsMGNet, are used to discretizethe continuous control model. We show that the resulting discrete PottsMGNet isequivalent to an encoder-decoder-based network. With minor modifications, it isshown that a number of the popular encoder-decoder-based neural networks arejust instances of the proposed PottsMGNet. By incorporating theSoft-Threshold-Dynamics into the PottsMGNet as a regularizer, the PottsMGNethas shown to be robust with the network parameters such as network width anddepth and achieved remarkable performance on datasets with very large noise. Innearly all our experiments, the new network always performs better or as goodon accuracy and dice score than existing networks for image segmentation.</description><author>Xue-Cheng Tai, Hao Liu, Raymond Chan</author><pubDate>Fri, 15 Sep 2023 14:53:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09039v2</guid></item><item><title>Optimizing Modular Robot Composition: A Lexicographic Genetic Algorithm Approach</title><link>http://arxiv.org/abs/2309.08399v1</link><description>Industrial robots are designed as general-purpose hardware, which limitstheir ability to adapt to changing task requirements or environments. Modularrobots, on the other hand, offer flexibility and can be easily customized tosuit diverse needs. The morphology, i.e., the form and structure of a robot,significantly impacts the primary performance metrics acquisition cost, cycletime, and energy efficiency. However, identifying an optimal module compositionfor a specific task remains an open problem, presenting a substantial hurdle indeveloping task-tailored modular robots. Previous approaches either lackadequate exploration of the design space or the possibility to adapt to complextasks. We propose combining a genetic algorithm with a lexicographic evaluationof solution candidates to overcome this problem and navigate search spacesexceeding those in prior work by magnitudes in the number of possiblecompositions. We demonstrate that our approach outperforms a state-of-the-artbaseline and is able to synthesize modular robots for industrial tasks incluttered environments.</description><author>Jonathan Külz, Matthias Althoff</author><pubDate>Fri, 15 Sep 2023 14:50:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08399v1</guid></item><item><title>Exploring Meta Information for Audio-based Zero-shot Bird Classification</title><link>http://arxiv.org/abs/2309.08398v1</link><description>Advances in passive acoustic monitoring and machine learning have led to theprocurement of vast datasets for computational bioacoustic research.Nevertheless, data scarcity is still an issue for rare and underrepresentedspecies. This study investigates how meta-information can improve zero-shotaudio classification, utilising bird species as an example case study due tothe availability of rich and diverse metadata. We investigate three differentsources of metadata: textual bird sound descriptions encoded via (S)BERT,functional traits (AVONET), and bird life-history (BLH) characteristics. Asaudio features, we extract audio spectrogram transformer (AST) embeddings andproject them to the dimension of the auxiliary information by adopting a singlelinear layer. Then, we employ the dot product as compatibility function and astandard zero-shot learning ranking hinge loss to determine the correct class.The best results are achieved by concatenating the AVONET and BLH featuresattaining a mean F1-score of .233 over five different test sets with 8 to 10classes.</description><author>Alexander Gebhard, Andreas Triantafyllopoulos, Teresa Bez, Lukas Christ, Alexander Kathan, Björn W. Schuller</author><pubDate>Fri, 15 Sep 2023 14:50:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08398v1</guid></item><item><title>HGCN-GJS: Hierarchical Graph Convolutional Network with Groupwise Joint Sampling for Trajectory Prediction</title><link>http://arxiv.org/abs/2009.07140v3</link><description>Accurate pedestrian trajectory prediction is of great importance fordownstream tasks such as autonomous driving and mobile robot navigation. Fullyinvestigating the social interactions within the crowd is crucial for accuratepedestrian trajectory prediction. However, most existing methods do not capturegroup level interactions well, focusing only on pairwise interactions andneglecting group-wise interactions. In this work, we propose a hierarchicalgraph convolutional network, HGCN-GJS, for trajectory prediction which wellleverages group level interactions within the crowd. Furthermore, we introducea novel joint sampling scheme for modeling the joint distribution of multiplepedestrians in the future trajectories. Based on the group information, thisscheme associates the trajectory of one person with the trajectory of otherpeople in the group, but maintains the independence of the trajectories ofoutsiders. We demonstrate the performance of our network on several trajectoryprediction datasets, achieving state-of-the-art results on all datasetsconsidered.</description><author>Yuying Chen, Congcong Liu, Xiaodong Mei, Bertram E. Shi, Ming Liu</author><pubDate>Fri, 15 Sep 2023 14:44:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2009.07140v3</guid></item><item><title>Learning by Self-Explaining</title><link>http://arxiv.org/abs/2309.08395v1</link><description>Artificial intelligence (AI) research has a long track record of drawinginspirations from findings from biology, in particular human intelligence. Incontrast to current AI research that mainly treats explanations as a means formodel inspection, a somewhat neglected finding from human psychology is thebenefit of self-explaining in an agents' learning process. Motivated by this,we introduce a novel learning paradigm, termed Learning by Self-Explaining(LSX). The underlying idea is that a learning module (learner) performs a basetask, e.g. image classification, and provides explanations to its decisions. Aninternal critic module next evaluates the quality of these explanations giventhe original task. Finally, the learner is refined with the critic's feedbackand the loop is repeated as required. The intuition behind this is that anexplanation is considered "good" if the critic can perform the same task giventhe respective explanation. Despite many implementation possibilities thestructure of any LSX instantiation can be taxonomized based on four learningmodules which we identify as: Fit, Explain, Reflect and Revise. In our work, weprovide distinct instantiations of LSX for two different learner models, eachillustrating different choices for the various LSX components. We broadlyevaluate these on several datasets and show that Learning by Self-Explainingnot only boosts the generalization abilities of AI models, particularly insmall-data regimes, but also aids in mitigating the influence of confoundingfactors, as well as leading to more task specific and faithful modelexplanations. Overall, our results provide experimental evidence of thepotential of self-explaining within the learning phase of an AI model.</description><author>Wolfgang Stammer, Felix Friedrich, David Steinmann, Hikaru Shindo, Kristian Kersting</author><pubDate>Fri, 15 Sep 2023 14:41:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08395v1</guid></item><item><title>DeCUR: decoupling common &amp; unique representations for multimodal self-supervision</title><link>http://arxiv.org/abs/2309.05300v2</link><description>The increasing availability of multi-sensor data sparks interest inmultimodal self-supervised learning. However, most existing approaches learnonly common representations across modalities while ignoring intra-modaltraining and modality-unique representations. We propose Decoupling Common andUnique Representations (DeCUR), a simple yet effective method for multimodalself-supervised learning. By distinguishing inter- and intra-modal embeddings,DeCUR is trained to integrate complementary information across differentmodalities. We evaluate DeCUR in three common multimodal scenarios(radar-optical, RGB-elevation, and RGB-depth), and demonstrate its consistentbenefits on scene classification and semantic segmentation downstream tasks.Notably, we get straightforward improvements by transferring our pretrainedbackbones to state-of-the-art supervised multimodal methods without anyhyperparameter tuning. Furthermore, we conduct a comprehensive explainabilityanalysis to shed light on the interpretation of common and unique features inour multimodal approach. Codes are available at\url{https://github.com/zhu-xlab/DeCUR}.</description><author>Yi Wang, Conrad M Albrecht, Nassim Ait Ali Braham, Chenying Liu, Zhitong Xiong, Xiao Xiang Zhu</author><pubDate>Fri, 15 Sep 2023 14:39:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05300v2</guid></item><item><title>Engression: Extrapolation for Nonlinear Regression?</title><link>http://arxiv.org/abs/2307.00835v2</link><description>Extrapolation is crucial in many statistical and machine learningapplications, as it is common to encounter test data outside the trainingsupport. However, extrapolation is a considerable challenge for nonlinearmodels. Conventional models typically struggle in this regard: while treeensembles provide a constant prediction beyond the support, neural networkpredictions tend to become uncontrollable. This work aims at providing anonlinear regression methodology whose reliability does not break downimmediately at the boundary of the training support. Our primary contributionis a new method called `engression' which, at its core, is a distributionalregression technique for pre-additive noise models, where the noise is added tothe covariates before applying a nonlinear transformation. Our experimentalresults indicate that this model is typically suitable for many real data sets.We show that engression can successfully perform extrapolation under someassumptions such as a strictly monotone function class, whereas traditionalregression approaches such as least-squares regression and quantile regressionfall short under the same assumptions. We establish the advantages ofengression over existing approaches in terms of extrapolation, showing thatengression consistently provides a meaningful improvement. Our empiricalresults, from both simulated and real data, validate these findings,highlighting the effectiveness of the engression method. The softwareimplementations of engression are available in both R and Python.</description><author>Xinwei Shen, Nicolai Meinshausen</author><pubDate>Fri, 15 Sep 2023 14:38:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00835v2</guid></item><item><title>Predictive change point detection for heterogeneous data</title><link>http://arxiv.org/abs/2305.06630v2</link><description>A change point detection (CPD) framework assisted by a predictive machinelearning model called "Predict and Compare" is introduced and characterised inrelation to other state-of-the-art online CPD routines which it outperforms interms of false positive rate and out-of-control average run length. Themethod's focus is on improving standard methods from sequential analysis suchas the CUSUM rule in terms of these quality measures. This is achieved by replacing typically used trend estimation functionalssuch as the running mean with more sophisticated predictive models (Predictstep), and comparing their prognosis with actual data (Compare step). The twomodels used in the Predict step are the ARIMA model and the LSTM recursiveneural network. However, the framework is formulated in general terms, so as toallow the use of other prediction or comparison methods than those tested here.The power of the method is demonstrated in a tribological case study in whichchange points separating the run-in, steady-state, and divergent wear phasesare detected in the regime of very few false positives.</description><author>Anna-Christina Glock, Florian Sobieczky, Johannes Fürnkranz, Peter Filzmoser, Martin Jech</author><pubDate>Fri, 15 Sep 2023 14:38:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06630v2</guid></item><item><title>Wasserstein Dictionaries of Persistence Diagrams</title><link>http://arxiv.org/abs/2304.14852v2</link><description>This paper presents a computational framework for the concise encoding of anensemble of persistence diagrams, in the form of weighted Wassersteinbarycenters [100], [102] of a dictionary of atom diagrams. We introduce amulti-scale gradient descent approach for the efficient resolution of thecorresponding minimization problem, which interleaves the optimization of thebarycenter weights with the optimization of the atom diagrams. Our approachleverages the analytic expressions for the gradient of both sub-problems toensure fast iterations and it additionally exploits shared-memory parallelism.Extensive experiments on public ensembles demonstrate the efficiency of ourapproach, with Wasserstein dictionary computations in the orders of minutes forthe largest examples. We show the utility of our contributions in twoapplications. First, we apply Wassserstein dictionaries to data reduction andreliably compress persistence diagrams by concisely representing them withtheir weights in the dictionary. Second, we present a dimensionality reductionframework based on a Wasserstein dictionary defined with a small number ofatoms (typically three) and encode the dictionary as a low dimensional simplexembedded in a visual space (typically in 2D). In both applications,quantitative experiments assess the relevance of our framework. Finally, weprovide a C++ implementation that can be used to reproduce our results.</description><author>Keanu Sisouk, Julie Delon, Julien Tierny</author><pubDate>Fri, 15 Sep 2023 14:31:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14852v2</guid></item><item><title>A Unified View Between Tensor Hypergraph Neural Networks And Signal Denoising</title><link>http://arxiv.org/abs/2309.08385v1</link><description>Hypergraph Neural networks (HyperGNNs) and hypergraph signal denoising(HyperGSD) are two fundamental topics in higher-order network modeling.Understanding the connection between these two domains is particularly usefulfor designing novel HyperGNNs from a HyperGSD perspective, and vice versa. Inparticular, the tensor-hypergraph convolutional network (T-HGCN) has emerged asa powerful architecture for preserving higher-order interactions onhypergraphs, and this work shows an equivalence relation between a HyperGSDproblem and the T-HGCN. Inspired by this intriguing result, we further design atensor-hypergraph iterative network (T-HGIN) based on the HyperGSD problem,which takes advantage of a multi-step updating scheme in every single layer.Numerical experiments are conducted to show the promising applications of theproposed T-HGIN approach.</description><author>Fuli Wang, Karelia Pena-Pena, Wei Qian, Gonzalo R. Arce</author><pubDate>Fri, 15 Sep 2023 14:19:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08385v1</guid></item><item><title>Double Domain Guided Real-Time Low-Light Image Enhancement for Ultra-High-Definition Transportation Surveillance</title><link>http://arxiv.org/abs/2309.08382v1</link><description>Real-time transportation surveillance is an essential part of the intelligenttransportation system (ITS). However, images captured under low-lightconditions often suffer the poor visibility with types of degradation, such asnoise interference and vague edge features, etc. With the development ofimaging devices, the quality of the visual surveillance data is continuallyincreasing, like 2K and 4K, which has more strict requirements on theefficiency of image processing. To satisfy the requirements on both enhancementquality and computational speed, this paper proposes a double domain guidedreal-time low-light image enhancement network (DDNet) for ultra-high-definition(UHD) transportation surveillance. Specifically, we design an encoder-decoderstructure as the main architecture of the learning network. In particular, theenhancement processing is divided into two subtasks (i.e., color enhancementand gradient enhancement) via the proposed coarse enhancement module (CEM) andLoG-based gradient enhancement module (GEM), which are embedded in theencoder-decoder structure. It enables the network to enhance the color and edgefeatures simultaneously. Through the decomposition and reconstruction on bothcolor and gradient domains, our DDNet can restore the detailed featureinformation concealed by the darkness with better visual quality andefficiency. The evaluation experiments on standard and transportation-relateddatasets demonstrate that our DDNet provides superior enhancement quality andefficiency compared with the state-of-the-art methods. Besides, the objectdetection and scene segmentation experiments indicate the practical benefitsfor higher-level image analysis under low-light environments in ITS.</description><author>Jingxiang Qu, Ryan Wen Liu, Yuan Gao, Yu Guo, Fenghua Zhu, Fei-yue Wang</author><pubDate>Fri, 15 Sep 2023 14:16:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08382v1</guid></item><item><title>Reconsidering evaluation practices in modular systems: On the propagation of errors in MRI prostate cancer detection</title><link>http://arxiv.org/abs/2309.08381v1</link><description>Magnetic resonance imaging has evolved as a key component for prostate cancer(PCa) detection, substantially increasing the radiologist workload. Artificialintelligence (AI) systems can support radiological assessment by segmenting andclassifying lesions in clinically significant (csPCa) and non-clinicallysignificant (ncsPCa). Commonly, AI systems for PCa detection involve anautomatic prostate segmentation followed by the lesion detection using theextracted prostate. However, evaluation reports are typically presented interms of detection under the assumption of the availability of a highlyaccurate segmentation and an idealistic scenario, omitting the propagation oferrors between modules. For that purpose, we evaluate the effect of twodifferent segmentation networks (s1 and s2) with heterogeneous performances inthe detection stage and compare it with an idealistic setting (s1:89.90+-2.23vs 88.97+-3.06 ncsPCa, P&lt;.001, 89.30+-4.07 and 88.12+-2.71 csPCa, P&lt;.001). Ourresults depict the relevance of a holistic evaluation, accounting for all thesub-modules involved in the system.</description><author>Erlend Sortland Rolfsnes, Philip Thangngat, Trygve Eftestøl, Tobias Nordström, Fredrik Jäderling, Martin Eklund, Alvaro Fernandez-Quilez</author><pubDate>Fri, 15 Sep 2023 14:15:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08381v1</guid></item><item><title>Unleashing Potential of Evidence in Knowledge-Intensive Dialogue Generation</title><link>http://arxiv.org/abs/2309.08380v1</link><description>Incorporating external knowledge into dialogue generation (KIDG) is crucialfor improving the correctness of response, where evidence fragments serve asknowledgeable snippets supporting the factual dialogue replies. However,introducing irrelevant content often adversely impacts reply quality and easilyleads to hallucinated responses. Prior work on evidence retrieval andintegration in dialogue systems falls short of fully leveraging existingevidence since the model fails to locate useful fragments accurately andoverlooks hidden evidence labels within the KIDG dataset. To fully Unleash thepotential of evidence, we propose a framework to effectively incorporateEvidence in knowledge-Intensive Dialogue Generation (u-EIDG). Specifically, weintroduce an automatic evidence generation framework that harnesses the powerof Large Language Models (LLMs) to mine reliable evidence veracity labels fromunlabeled data. By utilizing these evidence labels, we train a reliableevidence indicator to effectively identify relevant evidence from retrievedpassages. Furthermore, we propose an evidence-augmented generator with anevidence-focused attention mechanism, which allows the model to concentrate onevidenced segments. Experimental results on MultiDoc2Dial demonstrate theefficacy of evidential label augmentation and refined attention mechanisms inimproving model performance. Further analysis confirms that the proposed methodoutperforms other baselines (+3~+5 points) regarding coherence and factualconsistency.</description><author>Xianjie Wu, Jian Yang, Tongliang Li, Di Liang, Shiwei Zhang, Yiyang Du, Zhoujun Li</author><pubDate>Fri, 15 Sep 2023 14:13:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08380v1</guid></item></channel></rss>