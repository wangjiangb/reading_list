<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 26 Aug 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>NeuralClothSim: Neural Deformation Fields Meet the Kirchhoff-Love Thin Shell Theory</title><link>http://arxiv.org/abs/2308.12970v1</link><description>Cloth simulation is an extensively studied problem, with a plethora ofsolutions available in computer graphics literature. Existing cloth simulatorsproduce realistic cloth deformations that obey different types of boundaryconditions. Nevertheless, their operational principle remains limited inseveral ways: They operate on explicit surface representations with a fixedspatial resolution, perform a series of discretised updates (which bounds theirtemporal resolution), and require comparably large amounts of storage.Moreover, back-propagating gradients through the existing solvers is often notstraightforward, which poses additional challenges when integrating them intomodern neural architectures. In response to the limitations mentioned above,this paper takes a fundamentally different perspective on physically-plausiblecloth simulation and re-thinks this long-standing problem: We proposeNeuralClothSim, i.e., a new cloth simulation approach using thin shells, inwhich surface evolution is encoded in neural network weights. Ourmemory-efficient and differentiable solver operates on a new continuouscoordinate-based representation of dynamic surfaces, i.e., neural deformationfields (NDFs); it supervises NDF evolution with the rules of the non-linearKirchhoff-Love shell theory. NDFs are adaptive in the sense that they 1)allocate their capacity to the deformation details as the latter arise duringthe cloth evolution and 2) allow surface state queries at arbitrary spatial andtemporal resolutions without retraining. We show how to train ourNeuralClothSim solver while imposing hard boundary conditions and demonstratemultiple applications, such as material interpolation and simulation editing.The experimental results highlight the effectiveness of our formulation and itspotential impact.</description><author>Navami Kairanda, Marc Habermann, Christian Theobalt, Vladislav Golyanik</author><pubDate>Thu, 24 Aug 2023 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12970v1</guid></item><item><title>ROAM: Robust and Object-aware Motion Generation using Neural Pose Descriptors</title><link>http://arxiv.org/abs/2308.12969v1</link><description>Existing automatic approaches for 3D virtual character motion synthesissupporting scene interactions do not generalise well to new objects outsidetraining distributions, even when trained on extensive motion capture datasetswith diverse objects and annotated interactions. This paper addresses thislimitation and shows that robustness and generalisation to novel scene objectsin 3D object-aware character synthesis can be achieved by training a motionmodel with as few as one reference object. We leverage an implicit featurerepresentation trained on object-only datasets, which encodes anSE(3)-equivariant descriptor field around the object. Given an unseen objectand a reference pose-object pair, we optimise for the object-aware pose that isclosest in the feature space to the reference pose. Finally, we use l-NSM,i.e., our motion generation model that is trained to seamlessly transition fromlocomotion to object interaction with the proposed bidirectional pose blendingscheme. Through comprehensive numerical comparisons to state-of-the-art methodsand in a user study, we demonstrate substantial improvements in 3D virtualcharacter motion and interaction quality and robustness to scenarios withunseen objects. Our project page is available athttps://vcai.mpi-inf.mpg.de/projects/ROAM/.</description><author>Wanyue Zhang, Rishabh Dabral, Thomas Leimk√ºhler, Vladislav Golyanik, Marc Habermann, Christian Theobalt</author><pubDate>Thu, 24 Aug 2023 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12969v1</guid></item><item><title>NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes</title><link>http://arxiv.org/abs/2308.12967v1</link><description>Recent implicit neural representations have shown great results for novelview synthesis. However, existing methods require expensive per-sceneoptimization from many views hence limiting their application to real-worldunbounded urban settings where the objects of interest or backgrounds areobserved from very few views. To mitigate this challenge, we introduce a newapproach called NeO 360, Neural fields for sparse view synthesis of outdoorscenes. NeO 360 is a generalizable method that reconstructs 360{\deg} scenesfrom a single or a few posed RGB images. The essence of our approach is incapturing the distribution of complex real-world outdoor 3D scenes and using ahybrid image-conditional triplanar representation that can be queried from anyworld point. Our representation combines the best of both voxel-based andbird's-eye-view (BEV) representations and is more effective and expressive thaneach. NeO 360's representation allows us to learn from a large collection ofunbounded 3D scenes while offering generalizability to new views and novelscenes from as few as a single image during inference. We demonstrate ourapproach on the proposed challenging 360{\deg} unbounded dataset, called NeRDS360, and show that NeO 360 outperforms state-of-the-art generalizable methodsfor novel view synthesis while also offering editing and compositioncapabilities. Project page:https://zubair-irshad.github.io/projects/neo360.html</description><author>Muhammad Zubair Irshad, Sergey Zakharov, Katherine Liu, Vitor Guizilini, Thomas Kollar, Adrien Gaidon, Zsolt Kira, Rares Ambrus</author><pubDate>Thu, 24 Aug 2023 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12967v1</guid></item><item><title>Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation</title><link>http://arxiv.org/abs/2308.12968v1</link><description>Automatic high-quality rendering of anime scenes from complex real-worldimages is of significant practical value. The challenges of this task lie inthe complexity of the scenes, the unique features of anime style, and the lackof high-quality datasets to bridge the domain gap. Despite promising attempts,previous efforts are still incompetent in achieving satisfactory results withconsistent semantic preservation, evident stylization, and fine details. Inthis study, we propose Scenimefy, a novel semi-supervised image-to-imagetranslation framework that addresses these challenges. Our approach guides thelearning with structure-consistent pseudo paired data, simplifying the pureunsupervised setting. The pseudo data are derived uniquely from asemantic-constrained StyleGAN leveraging rich model priors like CLIP. Wefurther apply segmentation-guided data selection to obtain high-quality pseudosupervision. A patch-wise contrastive style loss is introduced to improvestylization and fine details. Besides, we contribute a high-resolution animescene dataset to facilitate future research. Our extensive experimentsdemonstrate the superiority of our method over state-of-the-art baselines interms of both perceptual quality and quantitative performance.</description><author>Yuxin Jiang, Liming Jiang, Shuai Yang, Chen Change Loy</author><pubDate>Thu, 24 Aug 2023 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12968v1</guid></item><item><title>Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities</title><link>http://arxiv.org/abs/2308.12966v1</link><description>We introduce the Qwen-VL series, a set of large-scale vision-language modelsdesigned to perceive and understand both text and images. Comprising Qwen-VLand Qwen-VL-Chat, these models exhibit remarkable performance in tasks likeimage captioning, question answering, visual localization, and flexibleinteraction. The evaluation covers a wide range of tasks including zero-shotcaptioning, visual or document visual question answering, and grounding. Wedemonstrate the Qwen-VL outperforms existing Large Vision Language Models(LVLMs). We present their architecture, training, capabilities, andperformance, highlighting their contributions to advancing multimodalartificial intelligence. Code, demo and models are available athttps://github.com/QwenLM/Qwen-VL.</description><author>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou</author><pubDate>Thu, 24 Aug 2023 18:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12966v1</guid></item><item><title>POCO: 3D Pose and Shape Estimation with Confidence</title><link>http://arxiv.org/abs/2308.12965v1</link><description>The regression of 3D Human Pose and Shape (HPS) from an image is becomingincreasingly accurate. This makes the results useful for downstream tasks likehuman action recognition or 3D graphics. Yet, no regressor is perfect, andaccuracy can be affected by ambiguous image evidence or by poses and appearancethat are unseen during training. Most current HPS regressors, however, do notreport the confidence of their outputs, meaning that downstream tasks cannotdifferentiate accurate estimates from inaccurate ones. To address this, wedevelop POCO, a novel framework for training HPS regressors to estimate notonly a 3D human body, but also their confidence, in a single feed-forward pass.Specifically, POCO estimates both the 3D body pose and a per-sample variance.The key idea is to introduce a Dual Conditioning Strategy (DCS) for regressinguncertainty that is highly correlated to pose reconstruction quality. The POCOframework can be applied to any HPS regressor and here we evaluate it bymodifying HMR, PARE, and CLIFF. In all cases, training the network to reasonabout uncertainty helps it learn to more accurately estimate 3D pose. Whilethis was not our goal, the improvement is modest but consistent. Our mainmotivation is to provide uncertainty estimates for downstream tasks; wedemonstrate this in two ways: (1) We use the confidence estimates to bootstrapHPS training. Given unlabelled image data, we take the confident estimates of aPOCO-trained regressor as pseudo ground truth. Retraining with thisautomatically-curated data improves accuracy. (2) We exploit uncertainty invideo pose estimation by automatically identifying uncertain frames (e.g. dueto occlusion) and inpainting these from confident frames. Code and models willbe available for research at https://poco.is.tue.mpg.de.</description><author>Sai Kumar Dwivedi, Cordelia Schmid, Hongwei Yi, Michael J. Black, Dimitrios Tzionas</author><pubDate>Thu, 24 Aug 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12965v1</guid></item><item><title>Dense Text-to-Image Generation with Attention Modulation</title><link>http://arxiv.org/abs/2308.12964v1</link><description>Existing text-to-image diffusion models struggle to synthesize realisticimages given dense captions, where each text prompt provides a detaileddescription for a specific image region. To address this, we proposeDenseDiffusion, a training-free method that adapts a pre-trained text-to-imagemodel to handle such dense captions while offering control over the scenelayout. We first analyze the relationship between generated images' layouts andthe pre-trained model's intermediate attention maps. Next, we develop anattention modulation method that guides objects to appear in specific regionsaccording to layout guidance. Without requiring additional fine-tuning ordatasets, we improve image generation performance given dense captionsregarding both automatic and human evaluation scores. In addition, we achievesimilar-quality visual results with models specifically trained with layoutconditions.</description><author>Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, Jun-Yan Zhu</author><pubDate>Thu, 24 Aug 2023 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12964v1</guid></item><item><title>MapPrior: Bird's-Eye View Map Layout Estimation with Generative Models</title><link>http://arxiv.org/abs/2308.12963v1</link><description>Despite tremendous advancements in bird's-eye view (BEV) perception, existingmodels fall short in generating realistic and coherent semantic map layouts,and they fail to account for uncertainties arising from partial sensorinformation (such as occlusion or limited coverage). In this work, we introduceMapPrior, a novel BEV perception framework that combines a traditionaldiscriminative BEV perception model with a learned generative model forsemantic map layouts. Our MapPrior delivers predictions with better accuracy,realism, and uncertainty awareness. We evaluate our model on the large-scalenuScenes benchmark. At the time of submission, MapPrior outperforms thestrongest competing method, with significantly improved MMD and ECE scores incamera- and LiDAR-based BEV perception.</description><author>Xiyue Zhu, Vlas Zyrianov, Zhijian Liu, Shenlong Wang</author><pubDate>Thu, 24 Aug 2023 18:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12963v1</guid></item><item><title>Motion-Guided Masking for Spatiotemporal Representation Learning</title><link>http://arxiv.org/abs/2308.12962v1</link><description>Several recent works have directly extended the image masked autoencoder(MAE) with random masking into video domain, achieving promising results.However, unlike images, both spatial and temporal information are important forvideo understanding. This suggests that the random masking strategy that isinherited from the image MAE is less effective for video MAE. This motivatesthe design of a novel masking algorithm that can more efficiently make use ofvideo saliency. Specifically, we propose a motion-guided masking algorithm(MGM) which leverages motion vectors to guide the position of each mask overtime. Crucially, these motion-based correspondences can be directly obtainedfrom information stored in the compressed format of the video, which makes ourmethod efficient and scalable. On two challenging large-scale video benchmarks(Kinetics-400 and Something-Something V2), we equip video MAE with our MGM andachieve up to +$1.3\%$ improvement compared to previous state-of-the-artmethods. Additionally, our MGM achieves equivalent performance to previousvideo MAE using up to $66\%$ fewer training epochs. Lastly, we show that MGMgeneralizes better to downstream transfer learning and domain adaptation taskson the UCF101, HMDB51, and Diving48 datasets, achieving up to +$4.9\%$improvement compared to baseline methods.</description><author>David Fan, Jue Wang, Shuai Liao, Yi Zhu, Vimal Bhat, Hector Santos-Villalobos, Rohith MV, Xinyu Li</author><pubDate>Thu, 24 Aug 2023 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12962v1</guid></item><item><title>Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via Training-free Networks</title><link>http://arxiv.org/abs/2308.12961v1</link><description>To reduce the reliance on large-scale datasets, recent works in 3Dsegmentation resort to few-shot learning. Current 3D few-shot semanticsegmentation methods first pre-train the models on `seen' classes, and thenevaluate their generalization performance on `unseen' classes. However, theprior pre-training stage not only introduces excessive time overhead, but alsoincurs a significant domain gap on `unseen' classes. To tackle these issues, wepropose an efficient Training-free Few-shot 3D Segmentation netwrok, TFS3D, anda further training-based variant, TFS3D-T. Without any learnable parameters,TFS3D extracts dense representations by trigonometric positional encodings, andachieves comparable performance to previous training-based methods. Due to theelimination of pre-training, TFS3D can alleviate the domain gap issue and savea substantial amount of time. Building upon TFS3D, TFS3D-T only requires totrain a lightweight query-support transferring attention (QUEST), whichenhances the interaction between the few-shot query and support data.Experiments demonstrate TFS3D-T improves previous state-of-the-art methods by+6.93% and +17.96% mIoU respectively on S3DIS and ScanNet, while reducing thetraining time by -90%, indicating superior effectiveness and efficiency.</description><author>Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Jiaming Liu, Hao Dong, Peng Gao</author><pubDate>Thu, 24 Aug 2023 18:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12961v1</guid></item><item><title>Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models</title><link>http://arxiv.org/abs/2308.11764v2</link><description>Large Language Models (LLMs) have revolutionized Natural Language Processing(NLP). Although convenient for research and practical applications, open-sourceLLMs with fewer parameters often suffer from severe hallucinations compared totheir larger counterparts. This paper focuses on measuring and reducinghallucinations in BLOOM 7B, a representative of such weaker open-source LLMsthat are publicly available for research and commercial applications. Weintroduce HaloCheck, a lightweight BlackBox knowledge-free framework designedto quantify the severity of hallucinations in LLMs. Additionally, we exploretechniques like knowledge injection and teacher-student approaches to alleviatehallucinations in low-parameter LLMs. Our experiments effectively demonstratethe reduction of hallucinations in challenging domains for these LLMs.</description><author>Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, Shizhu Liu</author><pubDate>Thu, 24 Aug 2023 18:57:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11764v2</guid></item><item><title>Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment</title><link>http://arxiv.org/abs/2308.12960v1</link><description>Large-scale pre-trained Vision Language Models (VLMs) have proven effectivefor zero-shot classification. Despite the success, most traditional VLMs-basedmethods are restricted by the assumption of partial source supervision or idealvocabularies, which rarely satisfy the open-world scenario. In this paper, weaim at a more challenging setting, Realistic Zero-Shot Classification, whichassumes no annotation but instead a broad vocabulary. To address thischallenge, we propose the Self Structural Semantic Alignment (S^3A) framework,which extracts the structural semantic information from unlabeled data whilesimultaneously self-learning. Our S^3A framework adopts a uniqueCluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groupsunlabeled data to derive structural semantics for pseudo-supervision. Our CVPRprocess includes iterative clustering on images, voting within each cluster toidentify initial class candidates from the vocabulary, generatingdiscriminative prompts with large language models to discern confusingcandidates, and realigning images and the vocabulary as structural semanticalignment. Finally, we propose to self-learn the CLIP image encoder with bothindividual and structural semantic alignment through a teacher-student learningstrategy. Our comprehensive experiments across various generic and fine-grainedbenchmarks demonstrate that the S^3A method offers substantial improvementsover existing VLMs-based approaches, achieving a more than 15% accuracyimprovement over CLIP on average. Our codes, models, and prompts are publiclyreleased at https://github.com/sheng-eatamath/S3A.</description><author>Sheng Zhang, Muzammal Naseer, Guangyi Chen, Zhiqiang Shen, Salman Khan, Kun Zhang, Fahad Khan</author><pubDate>Thu, 24 Aug 2023 18:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12960v1</guid></item><item><title>DLIP: Distilling Language-Image Pre-training</title><link>http://arxiv.org/abs/2308.12956v1</link><description>Vision-Language Pre-training (VLP) shows remarkable progress with theassistance of extremely heavy parameters, which challenges deployment in realapplications. Knowledge distillation is well recognized as the essentialprocedure in model compression. However, existing knowledge distillationtechniques lack an in-depth investigation and analysis of VLP, and practicalguidelines for VLP-oriented distillation are still not yet explored. In thispaper, we present DLIP, a simple yet efficient Distilling Language-ImagePre-training framework, through which we investigate how to distill a light VLPmodel. Specifically, we dissect the model distillation from multipledimensions, such as the architecture characteristics of different modules andthe information transfer of different modalities. We conduct comprehensiveexperiments and provide insights on distilling a light but performant VLPmodel. Experimental results reveal that DLIP can achieve a state-of-the-artaccuracy/efficiency trade-off across diverse cross-modal tasks, e.g.,image-text retrieval, image captioning and visual question answering. Forexample, DLIP compresses BLIP by 1.9x, from 213M to 108M parameters, whileachieving comparable or better performance. Furthermore, DLIP succeeds inretaining more than 95% of the performance with 22.4% parameters and 24.8%FLOPs compared to the teacher model and accelerates inference speed by 2.7x.</description><author>Huafeng Kuang, Jie Wu, Xiawu Zheng, Ming Li, Xuefeng Xiao, Rui Wang, Min Zheng, Rongrong Ji</author><pubDate>Thu, 24 Aug 2023 18:50:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12956v1</guid></item><item><title>Differentiable Microscopy Designs an All Optical Phase Retrieval Microscope</title><link>http://arxiv.org/abs/2203.14944v4</link><description>Since the late 16th century, scientists have continuously innovated anddeveloped new microscope types for various applications. Creating a newarchitecture from the ground up requires substantial scientific expertise andcreativity, often spanning years or even decades. In this study, we propose analternative approach called "Differentiable Microscopy," which introduces atop-down design paradigm for optical microscopes. Using all-optical phaseretrieval as an illustrative example, we demonstrate the effectiveness ofdata-driven microscopy design through $\partial\mu$. Furthermore, we conductcomprehensive comparisons with competing methods, showcasing the consistentsuperiority of our learned designs across multiple datasets, includingbiological samples. To substantiate our ideas, we experimentally validate thefunctionality of one of the learned designs, providing a proof of concept. Theproposed differentiable microscopy framework supplements the creative processof designing new optical systems and would perhaps lead to unconventional butbetter optical designs.</description><author>Kithmini Herath, Udith Haputhanthri, Ramith Hettiarachchi, Hasindu Kariyawasam, Raja N. Ahmad, Azeem Ahmad, Balpreet S. Ahluwalia, Chamira U. S. Edussooriya, Dushan N. Wadduwage</author><pubDate>Thu, 24 Aug 2023 18:46:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.14944v4</guid></item><item><title>Anderson Acceleration For Bioinformatics-Based Machine Learning</title><link>http://arxiv.org/abs/2302.00347v2</link><description>Anderson acceleration (AA) is a well-known method for accelerating theconvergence of iterative algorithms, with applications in various fieldsincluding deep learning and optimization. Despite its popularity in theseareas, the effectiveness of AA in classical machine learning classifiers hasnot been thoroughly studied. Tabular data, in particular, presents a uniquechallenge for deep learning models, and classical machine learning models areknown to perform better in these scenarios. However, the convergence analysisof these models has received limited attention. To address this gap inresearch, we implement a support vector machine (SVM) classifier variant thatincorporates AA to speed up convergence. We evaluate the performance of our SVMwith and without Anderson acceleration on several datasets from the biologydomain and demonstrate that the use of AA significantly improves convergenceand reduces the training loss as the number of iterations increases. Ourfindings provide a promising perspective on the potential of Andersonacceleration in the training of simple machine learning classifiers andunderscore the importance of further research in this area. By showing theeffectiveness of AA in this setting, we aim to inspire more studies thatexplore the applications of AA in classical machine learning.</description><author>Sarwan Ali, Prakash Chourasia, Murray Patterson</author><pubDate>Thu, 24 Aug 2023 18:42:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00347v2</guid></item><item><title>BridgeData V2: A Dataset for Robot Learning at Scale</title><link>http://arxiv.org/abs/2308.12952v1</link><description>We introduce BridgeData V2, a large and diverse dataset of roboticmanipulation behaviors designed to facilitate research on scalable robotlearning. BridgeData V2 contains 60,096 trajectories collected across 24environments on a publicly available low-cost robot. BridgeData V2 providesextensive task and environment variability, leading to skills that cangeneralize across environments, domains, and institutions, making the dataset auseful resource for a broad range of researchers. Additionally, the dataset iscompatible with a wide variety of open-vocabulary, multi-task learning methodsconditioned on goal images or natural language instructions. In ourexperiments, we train 6 state-of-the-art imitation learning and offlinereinforcement learning methods on our dataset, and find that they succeed on asuite of tasks requiring varying amounts of generalization. We also demonstratethat the performance of these methods improves with more data and highercapacity models, and that training on a greater variety of skills leads toimproved generalization. By publicly sharing BridgeData V2 and our pre-trainedmodels, we aim to accelerate research in scalable robot learning methods.Project page at https://rail-berkeley.github.io/bridgedata</description><author>Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-Estruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, Sergey Levine</author><pubDate>Thu, 24 Aug 2023 18:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12952v1</guid></item><item><title>Code Llama: Open Foundation Models for Code</title><link>http://arxiv.org/abs/2308.12950v1</link><description>We release Code Llama, a family of large language models for code based onLlama 2 providing state-of-the-art performance among open models, infillingcapabilities, support for large input contexts, and zero-shot instructionfollowing ability for programming tasks. We provide multiple flavors to cover awide range of applications: foundation models (Code Llama), Pythonspecializations (Code Llama - Python), and instruction-following models (CodeLlama - Instruct) with 7B, 13B and 34B parameters each. All models are trainedon sequences of 16k tokens and show improvements on inputs with up to 100ktokens. 7B and 13B Code Llama and Code Llama - Instruct variants supportinfilling based on surrounding content. Code Llama reaches state-of-the-artperformance among open models on several code benchmarks, with scores of up to53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperformevery other publicly available model on MultiPL-E. We release Code Llama undera permissive license that allows for both research and commercial use.</description><author>Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D√©fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve</author><pubDate>Thu, 24 Aug 2023 18:39:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12950v1</guid></item><item><title>Label Budget Allocation in Multi-Task Learning</title><link>http://arxiv.org/abs/2308.12949v1</link><description>The cost of labeling data often limits the performance of machine learningsystems. In multi-task learning, related tasks provide information to eachother and improve overall performance, but the label cost can vary among tasks.How should the label budget (i.e. the amount of money spent on labeling) beallocated among different tasks to achieve optimal multi-task performance? Weare the first to propose and formally define the label budget allocationproblem in multi-task learning and to empirically show that different budgetallocation strategies make a big difference to its performance. We propose aTask-Adaptive Budget Allocation algorithm to robustly generate the optimalbudget allocation adaptive to different multi-task learning settings.Specifically, we estimate and then maximize the extent of new informationobtained from the allocated budget as a proxy for multi-task learningperformance. Experiments on PASCAL VOC and Taskonomy demonstrate the efficacyof our approach over other widely used heuristic labeling strategies.</description><author>Ximeng Sun, Kihyuk Sohn, Kate Saenko, Clayton Mellina, Xiao Bian</author><pubDate>Thu, 24 Aug 2023 18:38:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12949v1</guid></item><item><title>LOPR: Latent Occupancy PRediction using Generative Models</title><link>http://arxiv.org/abs/2210.01249v3</link><description>Environment prediction frameworks are integral for autonomous vehicles,enabling safe navigation in dynamic environments. LiDAR generated occupancygrid maps (L-OGMs) offer a robust bird's eye-view scene representation thatfacilitates joint scene predictions without relying on manual labeling unlikecommonly used trajectory prediction frameworks. Prior approaches have optimizeddeterministic L-OGM prediction architectures directly in grid cell space. Whilethese methods have achieved some degree of success in prediction, theyoccasionally grapple with unrealistic and incorrect predictions. We claim thatthe quality and realism of the forecasted occupancy grids can be enhanced withthe use of generative models. We propose a framework that decouples occupancyprediction into: representation learning and stochastic prediction within thelearned latent space. Our approach allows for conditioning the model on otheravailable sensor modalities such as RGB-cameras and high definition maps. Wedemonstrate that our approach achieves state-of-the-art performance and isreadily transferable between different robotic platforms on the real-worldNuScenes, Waymo Open, and a custom dataset we collected on an experimentalvehicle platform.</description><author>Bernard Lange, Masha Itkina, Mykel J. Kochenderfer</author><pubDate>Thu, 24 Aug 2023 18:30:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.01249v3</guid></item><item><title>Learning Only On Boundaries: a Physics-Informed Neural operator for Solving Parametric Partial Differential Equations in Complex Geometries</title><link>http://arxiv.org/abs/2308.12939v1</link><description>Recently deep learning surrogates and neural operators have shown promise insolving partial differential equations (PDEs). However, they often require alarge amount of training data and are limited to bounded domains. In this work,we present a novel physics-informed neural operator method to solveparametrized boundary value problems without labeled data. By reformulating thePDEs into boundary integral equations (BIEs), we can train the operator networksolely on the boundary of the domain. This approach reduces the number ofrequired sample points from $O(N^d)$ to $O(N^{d-1})$, where $d$ is the domain'sdimension, leading to a significant acceleration of the training process.Additionally, our method can handle unbounded problems, which are unattainablefor existing physics-informed neural networks (PINNs) and neural operators. Ournumerical experiments show the effectiveness of parametrized complex geometriesand unbounded problems.</description><author>Zhiwei Fang, Sifan Wang, Paris Perdikaris</author><pubDate>Thu, 24 Aug 2023 18:29:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12939v1</guid></item><item><title>FIESTA: Autoencoders for accurate fiber segmentation in tractography</title><link>http://arxiv.org/abs/2212.00143v3</link><description>White matter bundle segmentation is a cornerstone of modern tractography tostudy the brain's structural connectivity in domains such as neurologicaldisorders, neurosurgery, and aging. In this study, we present FIESTA (FIbErSegmentation in Tractography using Autoencoders), a reliable and robust, fullyautomated, and easily semi-automatically calibrated pipeline based on deepautoencoders that can dissect and fully populate white matter bundles. Thispipeline is built upon previous works that demonstrated how autoencoders can beused successfully for streamline filtering, bundle segmentation, and streamlinegeneration in tractography. Our proposed method improves bundle segmentationcoverage by recovering hard-to-track bundles with generative sampling throughthe latent space seeding of the subject bundle and the atlas bundle. A latentspace of streamlines is learned using autoencoder-based modeling combined withcontrastive learning. Using an atlas of bundles in standard space (MNI), ourproposed method segments new tractograms using the autoencoder latent distancebetween each tractogram streamline and its closest neighbor bundle in the atlasof bundles. Intra-subject bundle reliability is improved by recoveringhard-to-track streamlines, using the autoencoder to generate new streamlinesthat increase the spatial coverage of each bundle while remaining anatomicallycorrect. Results show that our method is more reliable than state-of-the-artautomated virtual dissection methods such as RecoBundles, RecoBundlesX,TractSeg, White Matter Analysis and XTRACT. Our framework allows for thetransition from one anatomical bundle definition to another with marginalcalibration efforts. Overall, these results show that our framework improvesthe practicality and usability of current state-of-the-art bundle segmentationframework.</description><author>F√©lix Dumais, Jon Haitz Legarreta, Carl Lemaire, Philippe Poulin, Fran√ßois Rheault, Laurent Petit, Muhamed Barakovic, Stefano Magon, Maxime Descoteaux, Pierre-Marc Jodoin</author><pubDate>Thu, 24 Aug 2023 18:29:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00143v3</guid></item><item><title>Perspective-aware Convolution for Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2308.12938v1</link><description>Monocular 3D object detection is a crucial and challenging task forautonomous driving vehicle, while it uses only a single camera image to infer3D objects in the scene. To address the difficulty of predicting depth usingonly pictorial clue, we propose a novel perspective-aware convolutional layerthat captures long-range dependencies in images. By enforcing convolutionalkernels to extract features along the depth axis of every image pixel, weincorporates perspective information into network architecture. We integrateour perspective-aware convolutional layer into a 3D object detector anddemonstrate improved performance on the KITTI3D dataset, achieving a 23.9\%average precision in the easy benchmark. These results underscore theimportance of modeling scene clues for accurate depth inference and highlightthe benefits of incorporating scene structure in network design. Ourperspective-aware convolutional layer has the potential to enhance objectdetection accuracy by providing more precise and context-aware featureextraction.</description><author>Jia-Quan Yu, Soo-Chang Pei</author><pubDate>Thu, 24 Aug 2023 18:25:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12938v1</guid></item><item><title>Panoptic-Depth Color Map for Combination of Depth and Image Segmentation</title><link>http://arxiv.org/abs/2308.12937v1</link><description>Image segmentation and depth estimation are crucial tasks in computer vision,especially in autonomous driving scenarios. Although these tasks are typicallyaddressed separately, we propose an innovative approach to combine them in ournovel deep learning network, Panoptic-DepthLab. By incorporating an additionaldepth estimation branch into the segmentation network, it can predict the depthof each instance segment. Evaluating on Cityscape dataset, we demonstrate theeffectiveness of our method in achieving high-quality segmentation results withdepth and visualize it with a color map. Our proposed method demonstrates a newpossibility of combining different tasks and networks to generate a morecomprehensive image recognition result to facilitate the safety of autonomousdriving vehicles.</description><author>Jia-Quan Yu, Soo-Chang Pei</author><pubDate>Thu, 24 Aug 2023 18:25:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12937v1</guid></item><item><title>CNOS: A Strong Baseline for CAD-based Novel Object Segmentation</title><link>http://arxiv.org/abs/2307.11067v3</link><description>We propose a simple three-stage approach to segment unseen objects in RGBimages using their CAD models. Leveraging recent powerful foundation models,DINOv2 and Segment Anything, we create descriptors and generate proposals,including binary masks for a given input RGB image. By matching proposals withreference descriptors created from CAD models, we achieve precise object IDassignment along with modal masks. We experimentally demonstrate that ourmethod achieves state-of-the-art results in CAD-based novel objectsegmentation, surpassing existing approaches on the seven core datasets of theBOP challenge by 19.8% AP using the same BOP evaluation protocol. Our sourcecode is available at https://github.com/nv-nguyen/cnos.</description><author>Van Nguyen Nguyen, Tomas Hodan, Georgy Ponimatkin, Thibault Groueix, Vincent Lepetit</author><pubDate>Thu, 24 Aug 2023 18:17:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11067v3</guid></item><item><title>A Survey on Blood Pressure Measurement Technologies: Addressing Potential Sources of Bias</title><link>http://arxiv.org/abs/2306.08451v2</link><description>Regular blood pressure (BP) monitoring in clinical and ambulatory settingsplays a crucial role in the prevention, diagnosis, treatment, and management ofcardiovascular diseases. Recently, the widespread adoption of ambulatory BPmeasurement devices has been driven predominantly by the increased prevalenceof hypertension and its associated risks and clinical conditions. Recentguidelines advocate for regular BP monitoring as part of regular clinicalvisits or even at home. This increased utilization of BP measurementtechnologies has brought up significant concerns, regarding the accuracy ofreported BP values across settings. In this survey, focusing mainly on cuff-based BP monitoring technologies, wehighlight how BP measurements can demonstrate substantial biases and variancesdue to factors such as measurement and device errors, demographics, and bodyhabitus. With these inherent biases, the development of a new generation ofcuff-based BP devices which use artificial-intelligence (AI) has significantpotential. We present future avenues where AI-assisted technologies canleverage the extensive clinical literature on BP-related studies together withthe large collections of BP records available in electronic health records.These resources can be combined with machine learning approaches, includingdeep learning and Bayesian inference, to remove BP measurement biases and toprovide individualized BP-related cardiovascular risk indexes.</description><author>Seyedeh Somayyeh Mousavi, Matthew A. Reyna, Gari D. Clifford, Reza Sameni</author><pubDate>Thu, 24 Aug 2023 18:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08451v2</guid></item><item><title>Low-count Time Series Anomaly Detection</title><link>http://arxiv.org/abs/2308.12925v1</link><description>Low-count time series describe sparse or intermittent events, which areprevalent in large-scale online platforms that capture and monitor diverse datatypes. Several distinct challenges surface when modelling low-count timeseries, particularly low signal-to-noise ratios (when anomaly signatures areprovably undetectable), and non-uniform performance (when average metrics arenot representative of local behaviour). The time series anomaly detectioncommunity currently lacks explicit tooling and processes to model and reliablydetect anomalies in these settings. We address this gap by introducing a novelgenerative procedure for creating benchmark datasets comprising of low-counttime series with anomalous segments. Via a mixture of theoretical and empiricalanalysis, our work explains how widely-used algorithms struggle with thedistribution overlap between normal and anomalous segments. In order tomitigate this shortcoming, we then leverage our findings to demonstrate howanomaly score smoothing consistently improves performance. The practicalutility of our analysis and recommendation is validated on a real-world datasetcontaining sales data for retail stores.</description><author>Philipp Renz, Kurt Cutajar, Niall Twomey, Gavin K. C. Cheung, Hanting Xie</author><pubDate>Thu, 24 Aug 2023 17:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12925v1</guid></item><item><title>Self-regulating Prompts: Foundational Model Adaptation without Forgetting</title><link>http://arxiv.org/abs/2307.06948v2</link><description>Prompt learning has emerged as an efficient alternative for fine-tuningfoundational models, such as CLIP, for various downstream tasks. Conventionallytrained using the task-specific objective, i.e., cross-entropy loss, promptstend to overfit downstream data distributions and find it challenging tocapture task-agnostic general features from the frozen CLIP. This leads to theloss of the model's original generalization capability. To address this issue,our work introduces a self-regularization framework for prompting calledPromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides theprompts to optimize for both task-specific and task-agnostic generalrepresentations using a three-pronged approach by: (a) regulating promptedrepresentations via mutual agreement maximization with the frozen model, (b)regulating with self-ensemble of prompts over the training trajectory to encodetheir complementary strengths, and (c) regulating with textual diversity tomitigate sample diversity imbalance with the visual branch. To the best of ourknowledge, this is the first regularization framework for prompt learning thatavoids overfitting by jointly attending to pre-trained model features, thetraining trajectory during prompting, and the textual diversity. PromptSRCexplicitly steers the prompts to learn a representation space that maximizesperformance on downstream tasks without compromising CLIP generalization. Weperform extensive experiments on 4 benchmarks where PromptSRC overall performsfavorably well compared to the existing methods. Our code and pre-trainedmodels are publicly available at: https://github.com/muzairkhattak/PromptSRC.</description><author>Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan</author><pubDate>Thu, 24 Aug 2023 17:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06948v2</guid></item><item><title>An Efficient Distributed Multi-Agent Reinforcement Learning for EV Charging Network Control</title><link>http://arxiv.org/abs/2308.12921v1</link><description>The increasing trend in adopting electric vehicles (EVs) will significantlyimpact the residential electricity demand, which results in an increased riskof transformer overload in the distribution grid. To mitigate such risks, thereare urgent needs to develop effective EV charging controllers. Currently, themajority of the EV charge controllers are based on a centralized approach formanaging individual EVs or a group of EVs. In this paper, we introduce adecentralized Multi-agent Reinforcement Learning (MARL) charging framework thatprioritizes the preservation of privacy for EV owners. We employ theCentralized Training Decentralized Execution-Deep Deterministic Policy Gradient(CTDE-DDPG) scheme, which provides valuable information to users duringtraining while maintaining privacy during execution. Our results demonstratethat the CTDE framework improves the performance of the charging network byreducing the network costs. Moreover, we show that the Peak-to-Average Ratio(PAR) of the total demand is reduced, which, in turn, reduces the risk oftransformer overload during the peak hours.</description><author>Amin Shojaeighadikolaei, Morteza Hashemi</author><pubDate>Thu, 24 Aug 2023 17:53:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12921v1</guid></item><item><title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition</title><link>http://arxiv.org/abs/2307.06947v3</link><description>Recent video recognition models utilize Transformer models for long-rangespatio-temporal context modeling. Video transformer designs are based onself-attention that can model global context at a high computational cost. Incomparison, convolutional designs for videos offer an efficient alternative butlack long-range dependency modeling. Towards achieving the best of bothdesigns, this work proposes Video-FocalNet, an effective and efficientarchitecture for video recognition that models both local and global contexts.Video-FocalNet is based on a spatio-temporal focal modulation architecture thatreverses the interaction and aggregation steps of self-attention for betterefficiency. Further, the aggregation step and the interaction step are bothimplemented using efficient convolution and element-wise multiplicationoperations that are computationally less expensive than their self-attentioncounterparts on video representations. We extensively explore the design spaceof focal modulation-based spatio-temporal context modeling and demonstrate ourparallel spatial and temporal encoding design to be the optimal choice.Video-FocalNets perform favorably well against the state-of-the-arttransformer-based models for video recognition on five large-scale datasets(Kinetics-400, Kinetics-600, SS-v2, Diving-48, and ActivityNet-1.3) at a lowercomputational cost. Our code/models are released athttps://github.com/TalalWasim/Video-FocalNets.</description><author>Syed Talal Wasim, Muhammad Uzair Khattak, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan</author><pubDate>Thu, 24 Aug 2023 17:48:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06947v3</guid></item><item><title>Towards Realistic Unsupervised Fine-tuning with CLIP</title><link>http://arxiv.org/abs/2308.12919v1</link><description>The emergence of vision-language models (VLMs), such as CLIP, has spurred asignificant research effort towards their application for downstream supervisedlearning tasks. Although some previous studies have explored the unsupervisedfine-tuning of CLIP, they often rely on prior knowledge in the form of classnames associated with ground truth labels. In this paper, we delve into arealistic unsupervised fine-tuning scenario by assuming that the unlabeled datamight contain out-of-distribution samples from unknown classes. Furthermore, weemphasize the importance of simultaneously enhancing out-of-distributiondetection capabilities alongside the recognition of instances associated withpredefined class labels. To tackle this problem, we present a simple, efficient, and effectivefine-tuning approach called Universal Entropy Optimization (UEO). UEO leveragessample-level confidence to approximately minimize the conditional entropy ofconfident instances and maximize the marginal entropy of less confidentinstances. Apart from optimizing the textual prompts, UEO also incorporatesoptimization of channel-wise affine transformations within the visual branch ofCLIP. Through extensive experiments conducted across 15 domains and 4 differenttypes of prior knowledge, we demonstrate that UEO surpasses baseline methods interms of both generalization and out-of-distribution detection.</description><author>Jian Liang, Lijun Sheng, Zhengbo Wang, Ran He, Tieniu Tan</author><pubDate>Thu, 24 Aug 2023 17:47:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12919v1</guid></item><item><title>Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks</title><link>http://arxiv.org/abs/2308.12918v1</link><description>There have been recent adversarial attacks that are difficult to find. Thesenew adversarial attacks methods may pose challenges to current deep learningcyber defense systems and could influence the future defense of cyberattacks.The authors focus on this domain in this research paper. They explore theconsequences of vulnerabilities in AI systems. This includes discussing howthey might arise, differences between randomized and adversarial examples andalso potential ethical implications of vulnerabilities. Moreover, it isimportant to train the AI systems appropriately when they are in testing phaseand getting them ready for broader use.</description><author>John Harshith, Mantej Singh Gill, Madhan Jothimani</author><pubDate>Thu, 24 Aug 2023 17:46:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12918v1</guid></item><item><title>Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach</title><link>http://arxiv.org/abs/2305.17058v2</link><description>We present an exact Bayesian inference method for discrete statisticalmodels, which can find exact solutions to many discrete inference problems,even with infinite support and continuous priors. To express such models, weintroduce a probabilistic programming language that supports discrete andcontinuous sampling, discrete observations, affine functions, (stochastic)branching, and conditioning on events. Our key tool is probability generatingfunctions: they provide a compact closed-form representation of distributionsthat are definable by programs, thus enabling the exact computation ofposterior probabilities, expectation, variance, and higher moments. Ourinference method is provably correct, fully automated and uses automaticdifferentiation (specifically, Taylor polynomials), but does not requirecomputer algebra. Our experiments show that its performance on a range ofreal-world examples is competitive with approximate Monte Carlo methods, whileavoiding approximation errors.</description><author>Fabian Zaiser, Andrzej S. Murawski, Luke Ong</author><pubDate>Thu, 24 Aug 2023 17:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17058v2</guid></item><item><title>Language as Reality: A Co-Creative Storytelling Game Experience in 1001 Nights using Generative AI</title><link>http://arxiv.org/abs/2308.12915v1</link><description>In this paper, we present "1001 Nights", an AI-native game that allowsplayers lead in-game reality through co-created storytelling with the characterdriven by large language model. The concept is inspired by Wittgenstein's ideaof the limits of one's world being determined by the bounds of their language.Using advanced AI tools like GPT-4 and Stable Diffusion, the second iterationof the game enables the protagonist, Shahrzad, to realize words and stories inher world. The player can steer the conversation with the AI King towardsspecific keywords, which then become battle equipment in the game. This blendof interactive narrative and text-to-image transformation challenges theconventional border between the game world and reality through a dualperspective. We focus on Shahrzad, who seeks to alter her fate compared to theoriginal folklore, and the player, who collaborates with AI to craft narrativesand shape the game world. We explore the technical and design elements ofimplementing such a game with an objective to enhance the narrative game genrewith AI-generated content and to delve into AI-native gameplay possibilities.</description><author>Yuqian Sun, Zhouyi Li, Ke Fang, Chang Hee Lee, Ali Asadipour</author><pubDate>Thu, 24 Aug 2023 17:42:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12915v1</guid></item><item><title>Robot Pose Nowcasting: Forecast the Future to Improve the Present</title><link>http://arxiv.org/abs/2308.12914v1</link><description>In recent years, the effective and safe collaboration between humans andmachines has gained significant importance, particularly in the Industry 4.0scenario. A critical prerequisite for realizing this collaborative paradigm isprecisely understanding the robot's 3D pose within its environment. Therefore,in this paper, we introduce a novel vision-based system leveraging depth datato accurately establish the 3D locations of robotic joints. Specifically, weprove the ability of the proposed system to enhance its current pose estimationaccuracy by jointly learning to forecast future poses. Indeed, we introduce theconcept of Pose Nowcasting, denoting the capability of a system to exploit thelearned knowledge of the future to improve the estimation of the present. Theexperimental evaluation is conducted on two different datasets, providingstate-of-the-art and real-time performance and confirming the validity of theproposed method on both the robotic and human scenarios.</description><author>Alessandro Simoni, Francesco Marchetti, Guido Borghi, Federico Becattini, Lorenzo Seidenari, Roberto Vezzani, Alberto Del Bimbo</author><pubDate>Thu, 24 Aug 2023 17:40:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12914v1</guid></item><item><title>Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning</title><link>http://arxiv.org/abs/2301.06496v3</link><description>Retrieving images transmitted through multi-mode fibers is of growinginterest, thanks to their ability to confine and transport light efficiently ina compact system. Here, we demonstrate machine-learning-based decoding oflarge-scale digital images (pages), maximizing page capacity for opticalstorage applications. Using a millimeter-sized square cross-section waveguide,we image an 8-bit spatial light modulator, presenting data as a matrix ofsymbols. Normally, decoders will incur a prohibitive O(n^2) computationalscaling to decode n symbols in spatially scrambled data. However, by combininga digital twin of the setup with a U-Net, we can retrieve up to 66 kB usingefficient convolutional operations only. We compare trainable ray-tracing-basedwith eigenmode-based twins and show the former to be superior thanks to itsability to overcome the simulation-to-experiment gap by adjusting to opticalimperfections. We train the pipeline end-to-end using a differentiablemutual-information estimator based on the von-Mises distribution, generallyapplicable to phase-coding channels.</description><author>Joowon Lim, Jannes Gladrow, Douglas Kelly, Greg O'Shea, Govert Verkes, Ioan Stefanovici, Sebastian Nowozin, Benn Thomsen</author><pubDate>Thu, 24 Aug 2023 17:39:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.06496v3</guid></item><item><title>SCoRD: Subject-Conditional Relation Detection with Text-Augmented Data</title><link>http://arxiv.org/abs/2308.12910v1</link><description>We propose Subject-Conditional Relation Detection SCoRD, where conditioned onan input subject, the goal is to predict all its relations to other objects ina scene along with their locations. Based on the Open Images dataset, wepropose a challenging OIv6-SCoRD benchmark such that the training and testingsplits have a distribution shift in terms of the occurrence statistics of$\langle$subject, relation, object$\rangle$ triplets. To solve this problem, wepropose an auto-regressive model that given a subject, it predicts itsrelations, objects, and object locations by casting this output as a sequenceof tokens. First, we show that previous scene-graph prediction methods fail toproduce as exhaustive an enumeration of relation-object pairs when conditionedon a subject on this benchmark. Particularly, we obtain a recall@3 of 83.8% forour relation-object predictions compared to the 49.75% obtained by a recentscene graph detector. Then, we show improved generalization on bothrelation-object and object-box predictions by leveraging during trainingrelation-object pairs obtained automatically from textual captions and forwhich no object-box annotations are available. Particularly, for$\langle$subject, relation, object$\rangle$ triplets for which no objectlocations are available during training, we are able to obtain a recall@3 of42.59% for relation-object pairs and 32.27% for their box locations.</description><author>Ziyan Yang, Kushal Kafle, Zhe Lin, Scott Cohen, Zhihong Ding, Vicente Ordonez</author><pubDate>Thu, 24 Aug 2023 17:35:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12910v1</guid></item><item><title>POLCA: Power Oversubscription in LLM Cloud Providers</title><link>http://arxiv.org/abs/2308.12908v1</link><description>Recent innovation in large language models (LLMs), and their myriad use-caseshave rapidly driven up the compute capacity demand for datacenter GPUs. Severalcloud providers and other enterprises have made substantial plans of growth intheir datacenters to support these new workloads. One of the key bottleneckresources in datacenters is power, and given the increasing model sizes ofLLMs, they are becoming increasingly power intensive. In this paper, we showthat there is a significant opportunity to oversubscribe power in LLM clusters.Power oversubscription improves the power efficiency of these datacenters,allowing more deployable servers per datacenter, and reduces the deploymenttime, since building new datacenters is slow. We extensively characterize the power consumption patterns of a variety ofLLMs and their configurations. We identify the differences between theinference and training power consumption patterns. Based on our analysis ofthese LLMs, we claim that the average and peak power utilization in LLMclusters for inference should not be very high. Our deductions align with thedata from production LLM clusters, revealing that inference workloads offersubstantial headroom for power oversubscription. However, the stringent set oftelemetry and controls that GPUs offer in a virtualized environment, makes itchallenging to have a reliable and robust power oversubscription mechanism. We propose POLCA, our framework for power oversubscription that is robust,reliable, and readily deployable for GPU clusters. Using open-source models toreplicate the power patterns observed in production, we simulate POLCA anddemonstrate that we can deploy 30% more servers in the same GPU cluster forinference, with minimal performance loss</description><author>Pratyush Patel, Esha Choukse, Chaojie Zhang, √ç√±igo Goiri, Brijesh Warrier, Nithish Mahalingam, Ricardo Bianchini</author><pubDate>Thu, 24 Aug 2023 17:32:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12908v1</guid></item><item><title>Improving Sample Quality of Diffusion Models Using Self-Attention Guidance</title><link>http://arxiv.org/abs/2210.00939v6</link><description>Denoising diffusion models (DDMs) have attracted attention for theirexceptional generation quality and diversity. This success is largelyattributed to the use of class- or text-conditional diffusion guidance methods,such as classifier and classifier-free guidance. In this paper, we present amore comprehensive perspective that goes beyond the traditional guidancemethods. From this generalized perspective, we introduce novel condition- andtraining-free strategies to enhance the quality of generated images. As asimple solution, blur guidance improves the suitability of intermediate samplesfor their fine-scale information and structures, enabling diffusion models togenerate higher quality samples with a moderate guidance scale. Improving uponthis, Self-Attention Guidance (SAG) uses the intermediate self-attention mapsof diffusion models to enhance their stability and efficacy. Specifically, SAGadversarially blurs only the regions that diffusion models attend to at eachiteration and guides them accordingly. Our experimental results show that ourSAG improves the performance of various diffusion models, including ADM, IDDPM,Stable Diffusion, and DiT. Moreover, combining SAG with conventional guidancemethods leads to further improvement.</description><author>Susung Hong, Gyuseong Lee, Wooseok Jang, Seungryong Kim</author><pubDate>Thu, 24 Aug 2023 17:26:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.00939v6</guid></item><item><title>CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement</title><link>http://arxiv.org/abs/2308.12902v1</link><description>Low-light images, characterized by inadequate illumination, pose challengesof diminished clarity, muted colors, and reduced details. Low-light imageenhancement, an essential task in computer vision, aims to rectify these issuesby improving brightness, contrast, and overall perceptual quality, therebyfacilitating accurate analysis and interpretation. This paper introduces theConvolutional Dense Attention-guided Network (CDAN), a novel solution forenhancing low-light images. CDAN integrates an autoencoder-based architecturewith convolutional and dense blocks, complemented by an attention mechanism andskip connections. This architecture ensures efficient information propagationand feature learning. Furthermore, a dedicated post-processing phase refinescolor balance and contrast. Our approach demonstrates notable progress comparedto state-of-the-art results in low-light image enhancement, showcasing itsrobustness across a wide range of challenging scenarios. Our model performsremarkably on benchmark datasets, effectively mitigating under-exposure andproficiently restoring textures and colors in diverse low-light scenarios. Thisachievement underscores CDAN's potential for diverse computer vision tasks,notably enabling robust object detection and recognition in challenginglow-light conditions.</description><author>Hossein Shakibania, Sina Raoufi, Hassan Khotanlou</author><pubDate>Thu, 24 Aug 2023 17:22:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12902v1</guid></item><item><title>Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis &amp; Benchmark]</title><link>http://arxiv.org/abs/2308.12899v1</link><description>The field of urban spatial-temporal prediction is advancing rapidly with thedevelopment of deep learning techniques and the availability of large-scaledatasets. However, challenges persist in accessing and utilizing diverse urbanspatial-temporal datasets from different sources and stored in differentformats, as well as determining effective model structures and components withthe proliferation of deep learning models. This work addresses these challengesand provides three significant contributions. Firstly, we introduce "atomicfiles", a unified storage format designed for urban spatial-temporal big data,and validate its effectiveness on 40 diverse datasets, simplifying datamanagement. Secondly, we present a comprehensive overview of technologicaladvances in urban spatial-temporal prediction models, guiding the developmentof robust models. Thirdly, we conduct extensive experiments using diversemodels and datasets, establishing a performance leaderboard and identifyingpromising research directions. Overall, this work effectively manages urbanspatial-temporal data, guides future efforts, and facilitates the developmentof accurate and efficient urban spatial-temporal prediction models. It canpotentially make long-term contributions to urban spatial-temporal datamanagement and prediction, ultimately leading to improved urban livingstandards.</description><author>Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, Jingyuan Wang</author><pubDate>Thu, 24 Aug 2023 17:20:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12899v1</guid></item><item><title>Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh Reconstruction</title><link>http://arxiv.org/abs/2303.13796v3</link><description>As it is hard to calibrate single-view RGB images in the wild, existing 3Dhuman mesh reconstruction (3DHMR) methods either use a constant large focallength or estimate one based on the background environment context, which cannot tackle the problem of the torso, limb, hand or face distortion caused byperspective camera projection when the camera is close to the human body. Thenaive focal length assumptions can harm this task with the incorrectlyformulated projection matrices. To solve this, we propose Zolly, the first3DHMR method focusing on perspective-distorted images. Our approach begins withanalysing the reason for perspective distortion, which we find is mainly causedby the relative location of the human body to the camera center. We propose anew camera model and a novel 2D representation, termed distortion image, whichdescribes the 2D dense distortion scale of the human body. We then estimate thedistance from distortion scale features rather than environment contextfeatures. Afterwards, we integrate the distortion feature with image featuresto reconstruct the body mesh. To formulate the correct projection matrix andlocate the human body position, we simultaneously use perspective andweak-perspective projection loss. Since existing datasets could not handle thistask, we propose the first synthetic dataset PDHuman and extend two real-worlddatasets tailored for this task, all containing perspective-distorted humanimages. Extensive experiments show that Zolly outperforms existingstate-of-the-art methods on both perspective-distorted datasets and thestandard benchmark (3DPW).</description><author>Wenjia Wang, Yongtao Ge, Haiyi Mei, Zhongang Cai, Qingping Sun, Yanjun Wang, Chunhua Shen, Lei Yang, Taku Komura</author><pubDate>Thu, 24 Aug 2023 17:18:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13796v3</guid></item><item><title>Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?</title><link>http://arxiv.org/abs/2308.12898v1</link><description>The multimedia community has shown a significant interest in perceiving andrepresenting the physical world with multimodal pretrained neural networkmodels, and among them, the visual-language pertaining (VLP) is, currently, themost captivating topic. However, there have been few endeavors dedicated to theexploration of 1) whether essential linguistic knowledge (e.g., semantics andsyntax) can be extracted during VLP, and 2) how such linguistic knowledgeimpact or enhance the multimodal alignment. In response, here we aim toelucidate the impact of comprehensive linguistic knowledge, including semanticexpression and syntactic structure, on multimodal alignment. Specifically, wedesign and release the SNARE, the first large-scale multimodal alignmentprobing benchmark, to detect the vital linguistic components, e.g., lexical,semantic, and syntax knowledge, containing four tasks: Semantic structure,Negation logic, Attribute ownership, and Relationship composition. Based on ourproposed probing benchmarks, our holistic analyses of five advanced VLP modelsillustrate that the VLP model: i) shows insensitivity towards complex syntaxstructures and relies on content words for sentence comprehension; ii)demonstrates limited comprehension of combinations between sentences andnegations; iii) faces challenges in determining the presence of actions orspatial relationships within visual information and struggles with verifyingthe correctness of triple combinations. We make our benchmark and codeavailable at \url{https://github.com/WangFei-2019/SNARE/}.</description><author>Fei Wang, Liang Ding, Jun Rao, Ye Liu, Li Shen, Changxing Ding</author><pubDate>Thu, 24 Aug 2023 17:17:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12898v1</guid></item><item><title>Multimodal Image Synthesis and Editing: The Generative AI Era</title><link>http://arxiv.org/abs/2112.13592v6</link><description>As information exists in various modalities in real world, effectiveinteraction and fusion among multimodal information plays a key role for thecreation and perception of multimodal data in computer vision and deep learningresearch. With superb power in modeling the interaction among multimodalinformation, multimodal image synthesis and editing has become a hot researchtopic in recent years. Instead of providing explicit guidance for networktraining, multimodal guidance offers intuitive and flexible means for imagesynthesis and editing. On the other hand, this field is also facing severalchallenges in alignment of multimodal features, synthesis of high-resolutionimages, faithful evaluation metrics, etc. In this survey, we comprehensivelycontextualize the advance of the recent multimodal image synthesis and editingand formulate taxonomies according to data modalities and model types. We startwith an introduction to different guidance modalities in image synthesis andediting, and then describe multimodal image synthesis and editing approachesextensively according to their model types. After that, we describe benchmarkdatasets and evaluation metrics as well as corresponding experimental results.Finally, we provide insights about the current research challenges and possibledirections for future research. A project associated with this survey isavailable at https://github.com/fnzhan/Generative-AI.</description><author>Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Shijian Lu, Lingjie Liu, Adam Kortylewski, Christian Theobalt, Eric Xing</author><pubDate>Thu, 24 Aug 2023 17:17:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.13592v6</guid></item><item><title>Beyond Document Page Classification: Design, Datasets, and Challenges</title><link>http://arxiv.org/abs/2308.12896v1</link><description>This paper highlights the need to bring document classification benchmarkingcloser to real-world applications, both in the nature of data tested ($X$:multi-channel, multi-paged, multi-industry; $Y$: class distributions and labelset variety) and in classification tasks considered ($f$: multi-page document,page stream, and document bundle classification, ...). We identify the lack ofpublic multi-page document classification datasets, formalize differentclassification tasks arising in application scenarios, and motivate the valueof targeting efficient multi-page document representations. An experimentalstudy on proposed multi-page document classification datasets demonstrates thatcurrent benchmarks have become irrelevant and need to be updated to evaluatecomplete documents, as they naturally occur in practice. This reality checkalso calls for more mature evaluation methodologies, covering calibrationevaluation, inference complexity (time-memory), and a range of realisticdistribution shifts (e.g., born-digital vs. scanning noise, shifting pageorder). Our study ends on a hopeful note by recommending concrete avenues forfuture improvements.}</description><author>Jordy Van Landeghem, Sanket Biswas, Matthew B. Blaschko, Marie-Francine Moens</author><pubDate>Thu, 24 Aug 2023 17:16:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12896v1</guid></item><item><title>Boosting Semantic Segmentation from the Perspective of Explicit Class Embeddings</title><link>http://arxiv.org/abs/2308.12894v1</link><description>Semantic segmentation is a computer vision task that associates a label witheach pixel in an image. Modern approaches tend to introduce class embeddingsinto semantic segmentation for deeply utilizing category semantics, and regardsupervised class masks as final predictions. In this paper, we explore themechanism of class embeddings and have an insight that more explicit andmeaningful class embeddings can be generated based on class masks purposely.Following this observation, we propose ECENet, a new segmentation paradigm, inwhich class embeddings are obtained and enhanced explicitly during interactingwith multi-stage image features. Based on this, we revisit the traditionaldecoding process and explore inverted information flow between segmentationmasks and class embeddings. Furthermore, to ensure the discriminability andinformativity of features from backbone, we propose a Feature Reconstructionmodule, which combines intrinsic and diverse branches together to ensure theconcurrence of diversity and redundancy in features. Experiments show that ourECENet outperforms its counterparts on the ADE20K dataset with much lesscomputational cost and achieves new state-of-the-art results on PASCAL-Contextdataset. The code will be released at https://gitee.com/mindspore/models andhttps://github.com/Carol-lyh/ECENet.</description><author>Yuhe Liu, Chuanjian Liu, Kai Han, Quan Tang, Zengchang Qin</author><pubDate>Thu, 24 Aug 2023 17:16:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12894v1</guid></item><item><title>Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark</title><link>http://arxiv.org/abs/2304.14343v5</link><description>As deep learning technology advances and more urban spatial-temporal dataaccumulates, an increasing number of deep learning models are being proposed tosolve urban spatial-temporal prediction problems. However, there arelimitations in the existing field, including open-source data being in variousformats and difficult to use, few papers making their code and data openlyavailable, and open-source models often using different frameworks andplatforms, making comparisons challenging. A standardized framework is urgentlyneeded to implement and evaluate these methods. To address these issues, weprovide a comprehensive review of urban spatial-temporal prediction and proposea unified storage format for spatial-temporal data called atomic files. We alsopropose LibCity, an open-source library that offers researchers a credibleexperimental tool and a convenient development framework. In this library, wehave reproduced 65 spatial-temporal prediction models and collected 55spatial-temporal datasets, allowing researchers to conduct comprehensiveexperiments conveniently. Using LibCity, we conducted a series of experimentsto validate the effectiveness of different models and components, and wesummarized promising future technology developments and research directions forspatial-temporal prediction. By enabling fair model comparisons, designing aunified data storage format, and simplifying the process of developing newmodels, LibCity is poised to make significant contributions to thespatial-temporal prediction field.</description><author>Jiawei Jiang, Chengkai Han, Wenjun Jiang, Wayne Xin Zhao, Jingyuan Wang</author><pubDate>Thu, 24 Aug 2023 17:09:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14343v5</guid></item><item><title>Large Language Models Vote: Prompting for Rare Disease Identification</title><link>http://arxiv.org/abs/2308.12890v1</link><description>The emergence of generative Large Language Models (LLMs) emphasizes the needfor accurate and efficient prompting approaches. LLMs are often applied inFew-Shot Learning (FSL) contexts, where tasks are executed with minimaltraining data. FSL has become popular in many Artificial Intelligence (AI)subdomains, including AI for health. Rare diseases, affecting a small fractionof the population, inherently require FSL techniques due to limited dataavailability, though manual data collection and annotation is costly andtime-consuming. In this paper, we propose Models-Vote Prompting (MVP), aflexible prompting approach for improving the performance of LLM queries in FSLsettings. MVP works by prompting numerous LLMs to perform the same tasks andthen conducting a majority vote on the resulting outputs. This method achievesimproved results to any one model in the ensemble on one-shot rare diseaseidentification and classification tasks. We also release a novel rare diseasedataset for FSL, available to those who agreed to the MIMIC-IV Data UseAgreement (DUA). Furthermore, in using MVP, each model is prompted multipletimes, substantially increasing the time needed for manual annotation, and toaddress this, we assess the feasibility of using JSON for automating generativeLLM evaluation.</description><author>David Oniani, Jordan Hilsman, Hang Dong, Fengyi Gao, Shiven Verma, Yanshan Wang</author><pubDate>Thu, 24 Aug 2023 17:09:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12890v1</guid></item><item><title>Inducing Causal Structure for Abstractive Text Summarization</title><link>http://arxiv.org/abs/2308.12888v1</link><description>The mainstream of data-driven abstractive summarization models tends toexplore the correlations rather than the causal relationships. Among suchcorrelations, there can be spurious ones which suffer from the language priorlearned from the training corpus and therefore undermine the overalleffectiveness of the learned model. To tackle this issue, we introduce aStructural Causal Model (SCM) to induce the underlying causal structure of thesummarization data. We assume several latent causal factors and non-causalfactors, representing the content and style of the document and summary.Theoretically, we prove that the latent factors in our SCM can be identified byfitting the observed training data under certain conditions. On the basis ofthis, we propose a Causality Inspired Sequence-to-Sequence model (CI-Seq2Seq)to learn the causal representations that can mimic the causal factors, guidingus to pursue causal information for summary generation. The key idea is toreformulate the Variational Auto-encoder (VAE) to fit the joint distribution ofthe document and summary variables from the training corpus. Experimentalresults on two widely used text summarization datasets demonstrate theadvantages of our approach.</description><author>Lu Chen, Ruqing Zhang, Wei Huang, Wei Chen, Jiafeng Guo, Xueqi Cheng</author><pubDate>Thu, 24 Aug 2023 17:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12888v1</guid></item><item><title>Multi-stage feature decorrelation constraints for improving CNN classification performance</title><link>http://arxiv.org/abs/2308.12880v1</link><description>For the convolutional neural network (CNN) used for pattern classification,the training loss function is usually applied to the final output of thenetwork, except for some regularization constraints on the network parameters.However, with the increasing of the number of network layers, the influence ofthe loss function on the network front layers gradually decreases, and thenetwork parameters tend to fall into local optimization. At the same time, itis found that the trained network has significant information redundancy at allstages of features, which reduces the effectiveness of feature mapping at allstages and is not conducive to the change of the subsequent parameters of thenetwork in the direction of optimality. Therefore, it is possible to obtain amore optimized solution of the network and further improve the classificationaccuracy of the network by designing a loss function for restraining the frontstage features and eliminating the information redundancy of the front stagefeatures .For CNN, this article proposes a multi-stage feature decorrelationloss (MFD Loss), which refines effective features and eliminates informationredundancy by constraining the correlation of features at all stages.Considering that there are many layers in CNN, through experimental comparisonand analysis, MFD Loss acts on multiple front layers of CNN, constrains theoutput features of each layer and each channel, and performs supervisiontraining jointly with classification loss function during network training.Compared with the single Softmax Loss supervised learning, the experiments onseveral commonly used datasets on several typical CNNs prove that theclassification performance of Softmax Loss+MFD Loss is significantly better.Meanwhile, the comparison experiments before and after the combination of MFDLoss and some other typical loss functions verify its good universality.</description><author>Qiuyu Zhu, Xuewen Zu, Chengfei Liu</author><pubDate>Thu, 24 Aug 2023 17:00:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12880v1</guid></item><item><title>Easy attention: A simple self-attention mechanism for Transformers</title><link>http://arxiv.org/abs/2308.12874v1</link><description>To improve the robustness of transformer neural networks used fortemporal-dynamics prediction of chaotic systems, we propose a novel attentionmechanism called easy attention. Due to the fact that self attention only makesusage of the inner product of queries and keys, it is demonstrated that thekeys, queries and softmax are not necessary for obtaining the attention scorerequired to capture long-term dependencies in temporal sequences. Throughimplementing singular-value decomposition (SVD) on the softmax attention score,we further observe that the self attention compresses contribution from bothqueries and keys in the spanned space of the attention score. Therefore, ourproposed easy-attention method directly treats the attention scores aslearnable parameters. This approach produces excellent results whenreconstructing and predicting the temporal dynamics of chaotic systemsexhibiting more robustness and less complexity than the self attention or thewidely-used long short-term memory (LSTM) network. Our results show greatpotential for applications in more complex high-dimensional dynamical systems.</description><author>Marcial Sanchis-Agudo, Yuning Wang, Karthik Duraisamy, Ricardo Vinuesa</author><pubDate>Thu, 24 Aug 2023 16:54:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12874v1</guid></item><item><title>Near Optimal Adversarial Attack on UCB Bandits</title><link>http://arxiv.org/abs/2008.09312v6</link><description>I study a stochastic multi-arm bandit problem where rewards are subject toadversarial corruption. I propose a novel attack strategy that manipulates alearner employing the UCB algorithm into pulling some non-optimal target arm $T- o(T)$ times with a cumulative cost that scales as $\widehat{O}(\sqrt{\logT})$, where $T$ is the number of rounds. I also prove the first lower bound onthe cumulative attack cost. The lower bound matches the upper bound up to$O(\log \log T)$ factors, showing the proposed attack strategy to be nearoptimal.</description><author>Shiliang Zuo</author><pubDate>Thu, 24 Aug 2023 16:51:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2008.09312v6</guid></item><item><title>Transforming to Yoked Neural Networks to Improve ANN Structure</title><link>http://arxiv.org/abs/2306.02157v3</link><description>Most existing classical artificial neural networks (ANN) are designed as atree structure to imitate neural networks. In this paper, we argue that theconnectivity of a tree is not sufficient to characterize a neural network. Thenodes of the same level of a tree cannot be connected with each other, i.e.,these neural unit cannot share information with each other, which is a majordrawback of ANN. Although ANN has been significantly improved in recent yearsto more complex structures, such as the directed acyclic graph (DAG), thesemethods also have unidirectional and acyclic bias for ANN. In this paper, wepropose a method to build a bidirectional complete graph for the nodes in thesame level of an ANN, which yokes the nodes of the same level to formulate aneural module. We call our model as YNN in short. YNN promotes the informationtransfer significantly which obviously helps in improving the performance ofthe method. Our YNN can imitate neural networks much better compared with thetraditional ANN. In this paper, we analyze the existing structural bias of ANNand propose a model YNN to efficiently eliminate such structural bias. In ourmodel, nodes also carry out aggregation and transformation of features, andedges determine the flow of information. We further impose auxiliary sparsityconstraint to the distribution of connectedness, which promotes the learnedstructure to focus on critical connections. Finally, based on the optimizedstructure, we also design small neural module structure based on the minimumcut technique to reduce the computational burden of the YNN model. Thislearning process is compatible with the existing networks and different tasks.The obtained quantitative experimental results reflect that the learnedconnectivity is superior to the traditional NN structure.</description><author>Xinshun Liu, Yizhi Fang, Yichao Jiang</author><pubDate>Thu, 24 Aug 2023 16:51:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02157v3</guid></item><item><title>IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency</title><link>http://arxiv.org/abs/2308.12871v1</link><description>Efficiently optimizing multi-model inference pipelines for fast, accurate,and cost-effective inference is a crucial challenge in ML production systems,given their tight end-to-end latency requirements. To simplify the explorationof the vast and intricate trade-off space of accuracy and cost in inferencepipelines, providers frequently opt to consider one of them. However, thechallenge lies in reconciling accuracy and cost trade-offs. To address thischallenge and propose a solution to efficiently manage model variants ininference pipelines, we present IPA, an online deep-learning Inference PipelineAdaptation system that efficiently leverages model variants for each deeplearning task. Model variants are different versions of pre-trained models forthe same deep learning task with variations in resource requirements, latency,and accuracy. IPA dynamically configures batch size, replication, and modelvariants to optimize accuracy, minimize costs, and meet user-defined latencySLAs using Integer Programming. It supports multi-objective settings forachieving different trade-offs between accuracy and cost objectives whileremaining adaptable to varying workloads and dynamic traffic patterns.Extensive experiments on a Kubernetes implementation with five real-worldinference pipelines demonstrate that IPA improves normalized accuracy by up to35% with a minimal cost increase of less than 5%.</description><author>Saeid Ghafouri, Kamran Razavi, Mehran Salmani, Alireza Sanaee, Tania Lorido-Botran, Lin Wang, Joseph Doyle, Pooyan Jamshidi</author><pubDate>Thu, 24 Aug 2023 16:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12871v1</guid></item><item><title>VNI-Net: Vector Neurons-based Rotation-Invariant Descriptor for LiDAR Place Recognition</title><link>http://arxiv.org/abs/2308.12870v1</link><description>LiDAR-based place recognition plays a crucial role in SimultaneousLocalization and Mapping (SLAM) and LiDAR localization. Despite the emergence of various deep learning-based and hand-crafting-basedmethods, rotation-induced place recognition failure remains a criticalchallenge. Existing studies address this limitation through specific training strategiesor network structures. However, the former does not produce satisfactory results, while the latterfocuses mainly on the reduced problem of SO(2) rotation invariance. Methodstargeting SO(3) rotation invariance suffer from limitations in discriminationcapability. In this paper, we propose a new method that employs Vector Neurons Network(VNN) to achieve SO(3) rotation invariance. We first extract rotation-equivariant features from neighboring points andmap low-dimensional features to a high-dimensional space through VNN. Afterwards, we calculate the Euclidean and Cosine distance in therotation-equivariant feature space as rotation-invariant feature descriptors. Finally, we aggregate the features using GeM pooling to obtain globaldescriptors. To address the significant information loss when formulatingrotation-invariant descriptors, we propose computing distances between featuresat different layers within the Euclidean space neighborhood. This greatly improves the discriminability of the point cloud descriptorswhile ensuring computational efficiency. Experimental results on public datasets show that our approach significantlyoutperforms other baseline methods implementing rotation invariance, whileachieving comparable results with current state-of-the-art place recognitionmethods that do not consider rotation issues.</description><author>Gengxuan Tian, Junqiao Zhao, Yingfeng Cai, Fenglin Zhang, Wenjie Mu, Chen Ye</author><pubDate>Thu, 24 Aug 2023 16:47:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12870v1</guid></item><item><title>MIPS-Fusion: Multi-Implicit-Submaps for Scalable and Robust Online Neural RGB-D Reconstruction</title><link>http://arxiv.org/abs/2308.08741v2</link><description>We introduce MIPS-Fusion, a robust and scalable online RGB-D reconstructionmethod based on a novel neural implicit representation --multi-implicit-submap. Different from existing neural RGB-D reconstructionmethods lacking either flexibility with a single neural map or scalability dueto extra storage of feature grids, we propose a pure neural representationtackling both difficulties with a divide-and-conquer design. In our method,neural submaps are incrementally allocated alongside the scanning trajectoryand efficiently learned with local neural bundle adjustments. The submaps canbe refined individually in a back-end optimization and optimized jointly torealize submap-level loop closure. Meanwhile, we propose a hybrid trackingapproach combining randomized and gradient-based pose optimizations. For thefirst time, randomized optimization is made possible in neural tracking withseveral key designs to the learning process, enabling efficient and robusttracking even under fast camera motions. The extensive evaluation demonstratesthat our method attains higher reconstruction quality than the state of thearts for large-scale scenes and under fast camera motions.</description><author>Yijie Tang, Jiazhao Zhang, Zhinan Yu, He Wang, Kai Xu</author><pubDate>Thu, 24 Aug 2023 16:43:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08741v2</guid></item><item><title>ToonTalker: Cross-Domain Face Reenactment</title><link>http://arxiv.org/abs/2308.12866v1</link><description>We target cross-domain face reenactment in this paper, i.e., driving acartoon image with the video of a real person and vice versa. Recently, manyworks have focused on one-shot talking face generation to drive a portrait witha real video, i.e., within-domain reenactment. Straightforwardly applying thosemethods to cross-domain animation will cause inaccurate expression transfer,blur effects, and even apparent artifacts due to the domain shift betweencartoon and real faces. Only a few works attempt to settle cross-domain facereenactment. The most related work AnimeCeleb requires constructing a datasetwith pose vector and cartoon image pairs by animating 3D characters, whichmakes it inapplicable anymore if no paired data is available. In this paper, wepropose a novel method for cross-domain reenactment without paired data.Specifically, we propose a transformer-based framework to align the motionsfrom different domains into a common latent space where motion transfer isconducted via latent code addition. Two domain-specific motion encoders and twolearnable motion base memories are used to capture domain properties. A sourcequery transformer and a driving one are exploited to project domain-specificmotion to the canonical space. The edited motion is projected back to thedomain of the source with a transformer. Moreover, since no paired data isprovided, we propose a novel cross-domain training scheme using data from twodomains with the designed analogy constraint. Besides, we contribute a cartoondataset in Disney style. Extensive evaluations demonstrate the superiority ofour method over competing methods.</description><author>Yuan Gong, Yong Zhang, Xiaodong Cun, Fei Yin, Yanbo Fan, Xuan Wang, Baoyuan Wu, Yujiu Yang</author><pubDate>Thu, 24 Aug 2023 16:43:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12866v1</guid></item><item><title>Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution</title><link>http://arxiv.org/abs/2308.12864v1</link><description>In this article, we present a novel data assimilation strategy in pore-scaleimaging and demonstrate that this makes it possible to robustly addressreactive inverse problems incorporating Uncertainty Quantification (UQ).Pore-scale modeling of reactive flow offers a valuable opportunity toinvestigate the evolution of macro-scale properties subject to dynamicprocesses. Yet, they suffer from imaging limitations arising from theassociated X-ray microtomography (X-ray microCT) process, which inducesdiscrepancies in the properties estimates. Assessment of the kinetic parametersalso raises challenges, as reactive coefficients are critical parameters thatcan cover a wide range of values. We account for these two issues and ensurereliable calibration of pore-scale modeling, based on dynamical microCT images,by integrating uncertainty quantification in the workflow. The present method is based on a multitasking formulation of reactive inverseproblems combining data-driven and physics-informed techniques in calcitedissolution. This allows quantifying morphological uncertainties on theporosity field and estimating reactive parameter ranges through prescribed PDEmodels with a latent concentration field and dynamical microCT. The dataassimilation strategy relies on sequential reinforcement incorporatingsuccessively additional PDE constraints. We guarantee robust and unbiaseduncertainty quantification by straightforward adaptive weighting of BayesianPhysics-Informed Neural Networks (BPINNs), ensuring reliable micro-porositychanges during geochemical transformations. We demonstrate successful BayesianInference in 1D+Time and 2D+Time calcite dissolution based on synthetic microCTimages with meaningful posterior distribution on the reactive parameters anddimensionless numbers.</description><author>Sarah Perez, Philippe Poncet</author><pubDate>Thu, 24 Aug 2023 16:39:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12864v1</guid></item><item><title>Farm-wide virtual load monitoring for offshore wind structures via Bayesian neural networks</title><link>http://arxiv.org/abs/2211.00642v2</link><description>Offshore wind structures are subject to deterioration mechanisms throughouttheir operational lifetime. Even if the deterioration evolution of structuralelements can be estimated through physics-based deterioration models, theuncertainties involved in the process hurdle the selection of lifecyclemanagement decisions. In this scenario, the collection of relevant informationthrough an efficient monitoring system enables the reduction of uncertainties,ultimately driving more optimal lifecycle decisions. However, a full monitoringinstrumentation implemented on all wind turbines in a farm might becomeunfeasible due to practical and economical constraints. Besides, certain loadmonitoring systems often become defective after a few years of marineenvironment exposure. Addressing the aforementioned concerns, a farm-widevirtual load monitoring scheme directed by a fleet-leader wind turbine offersan attractive solution. Fetched with data retrieved from a fully-instrumentedwind turbine, a model can be trained and then deployed, thus yielding loadpredictions of non-fully monitored wind turbines, from which only standard dataremains available. In this paper, we propose a virtual load monitoringframework formulated via Bayesian neural networks (BNNs) and we providerelevant implementation details needed for the construction, training, anddeployment of BNN data-based virtual monitoring models. As opposed to theirdeterministic counterparts, BNNs intrinsically announce the uncertaintiesassociated with generated load predictions and allow to detect inaccurate loadestimations generated for non-fully monitored wind turbines. The proposedvirtual load monitoring is thoroughly tested through an experimental campaignin an operational offshore wind farm and the results demonstrate theeffectiveness of BNN models for fleet-leader-based farm-wide virtualmonitoring.</description><author>N. Hlaing, Pablo G. Morato, F. d. N. Santos, W. Weijtjens, C. Devriendt, P. Rigo</author><pubDate>Thu, 24 Aug 2023 16:35:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.00642v2</guid></item><item><title>SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection</title><link>http://arxiv.org/abs/2308.12863v1</link><description>Multi-modal fusion is increasingly being used for autonomous driving tasks,as images from different modalities provide unique information for featureextraction. However, the existing two-stream networks are only fused at aspecific network layer, which requires a lot of manual attempts to set up. Asthe CNN goes deeper, the two modal features become more and more advanced andabstract, and the fusion occurs at the feature level with a large gap, whichcan easily hurt the performance. In this study, we propose a novel fusionarchitecture called skip-cross networks (SkipcrossNets), which combinesadaptively LiDAR point clouds and camera images without being bound to acertain fusion epoch. Specifically, skip-cross connects each layer to eachlayer in a feed-forward manner, and for each layer, the feature maps of allprevious layers are used as input and its own feature maps are used as input toall subsequent layers for the other modality, enhancing feature propagation andmulti-modal features fusion. This strategy facilitates selection of the mostsimilar feature layers from two data pipelines, providing a complementaryeffect for sparse point cloud features during fusion processes. The network isalso divided into several blocks to reduce the complexity of feature fusion andthe number of model parameters. The advantages of skip-cross fusion weredemonstrated through application to the KITTI and A2D2 datasets, achieving aMaxF score of 96.85% on KITTI and an F1 score of 84.84% on A2D2. The modelparameters required only 2.33 MB of memory at a speed of 68.24 FPS, which couldbe viable for mobile terminals and embedded devices.</description><author>Xinyu Zhang, Yan Gong, Zhiwei Li, Xin Gao, Dafeng Jin, Jun Li, Huaping Liu</author><pubDate>Thu, 24 Aug 2023 16:34:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12863v1</guid></item><item><title>Learned Local Attention Maps for Synthesising Vessel Segmentations</title><link>http://arxiv.org/abs/2308.12861v1</link><description>Magnetic resonance angiography (MRA) is an imaging modality for visualisingblood vessels. It is useful for several diagnostic applications and forassessing the risk of adverse events such as haemorrhagic stroke (resultingfrom the rupture of aneurysms in blood vessels). However, MRAs are not acquiredroutinely, hence, an approach to synthesise blood vessel segmentations frommore routinely acquired MR contrasts such as T1 and T2, would be useful. Wepresent an encoder-decoder model for synthesising segmentations of the maincerebral arteries in the circle of Willis (CoW) from only T2 MRI. We propose atwo-phase multi-objective learning approach, which captures both global andlocal features. It uses learned local attention maps generated by dilating thesegmentation labels, which forces the network to only extract information fromthe T2 MRI relevant to synthesising the CoW. Our synthetic vessel segmentationsgenerated from only T2 MRI achieved a mean Dice score of $0.79 \pm 0.03$ intesting, compared to state-of-the-art segmentation networks such as transformerU-Net ($0.71 \pm 0.04$) and nnU-net($0.68 \pm 0.05$), while using only afraction of the parameters. The main qualitative difference between oursynthetic vessel segmentations and the comparative models was in the sharperresolution of the CoW vessel segments, especially in the posterior circulation.</description><author>Yash Deo, Rodrigo Bonazzola, Haoran Dou, Yan Xia, Tianyou Wei, Nishant Ravikumar, Alejandro F. Frangi, Toni Lassila</author><pubDate>Thu, 24 Aug 2023 16:32:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12861v1</guid></item><item><title>Towards Automated Animal Density Estimation with Acoustic Spatial Capture-Recapture</title><link>http://arxiv.org/abs/2308.12859v1</link><description>Passive acoustic monitoring can be an effective way of monitoring wildlifepopulations that are acoustically active but difficult to survey visually.Digital recorders allow surveyors to gather large volumes of data at low cost,but identifying target species vocalisations in these data is non-trivial.Machine learning (ML) methods are often used to do the identification. They canprocess large volumes of data quickly, but they do not detect all vocalisationsand they do generate some false positives (vocalisations that are not from thetarget species). Existing wildlife abundance survey methods have been designedspecifically to deal with the first of these mistakes, but current methods ofdealing with false positives are not well-developed. They do not take accountof features of individual vocalisations, some of which are more likely to befalse positives than others. We propose three methods for acoustic spatialcapture-recapture inference that integrate individual-level measures ofconfidence from ML vocalisation identification into the likelihood and henceintegrate ML uncertainty into inference. The methods include a mixture model inwhich species identity is a latent variable. We test the methods by simulationand find that in a scenario based on acoustic data from Hainan gibbons, inwhich ignoring false positives results in 17% positive bias, our methods givenegligible bias and coverage probabilities that are close to the nominal 95%level.</description><author>Yuheng Wang, Juan Ye, David L. Borchers</author><pubDate>Thu, 24 Aug 2023 16:29:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12859v1</guid></item><item><title>Fast Adversarial Training with Smooth Convergence</title><link>http://arxiv.org/abs/2308.12857v1</link><description>Fast adversarial training (FAT) is beneficial for improving the adversarialrobustness of neural networks. However, previous FAT work has encountered asignificant issue known as catastrophic overfitting when dealing with largeperturbation budgets, \ie the adversarial robustness of models declines to nearzero during training. To address this, we analyze the training process of prior FAT work andobserve that catastrophic overfitting is accompanied by the appearance of lossconvergence outliers. Therefore, we argue a moderately smooth loss convergence process will be astable FAT process that solves catastrophic overfitting. To obtain a smooth loss convergence process, we propose a novel oscillatoryconstraint (dubbed ConvergeSmooth) to limit the loss difference betweenadjacent epochs. The convergence stride of ConvergeSmooth is introduced tobalance convergence and smoothing. Likewise, we design weight centralizationwithout introducing additional hyperparameters other than the loss balancecoefficient. Our proposed methods are attack-agnostic and thus can improve the trainingstability of various FAT techniques. Extensive experiments on popular datasets show that the proposed methodsefficiently avoid catastrophic overfitting and outperform all previous FATmethods. Code is available at \url{https://github.com/FAT-CS/ConvergeSmooth}.</description><author>Mengnan Zhao, Lihe Zhang, Yuqiu Kong, Baocai Yin</author><pubDate>Thu, 24 Aug 2023 16:28:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12857v1</guid></item><item><title>Algorithmic progress in computer vision</title><link>http://arxiv.org/abs/2212.05153v4</link><description>We investigate algorithmic progress in image classification on ImageNet,perhaps the most well-known test bed for computer vision. We estimate a model,informed by work on neural scaling laws, and infer a decomposition of progressinto the scaling of compute, data, and algorithms. Using Shapley values toattribute performance improvements, we find that algorithmic improvements havebeen roughly as important as the scaling of compute for progress computervision. Our estimates indicate that algorithmic innovations mostly take theform of compute-augmenting algorithmic advances (which enable researchers toget better performance from less compute), not data-augmenting algorithmicadvances. We find that compute-augmenting algorithmic advances are made at apace more than twice as fast as the rate usually associated with Moore's law.In particular, we estimate that compute-augmenting innovations halve computerequirements every nine months (95\% confidence interval: 4 to 25 months).</description><author>Ege Erdil, Tamay Besiroglu</author><pubDate>Thu, 24 Aug 2023 16:15:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.05153v4</guid></item><item><title>Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle Avoidance</title><link>http://arxiv.org/abs/2308.12845v1</link><description>Robust obstacle avoidance is one of the critical steps for successfulgoal-driven indoor navigation tasks.Due to the obstacle missing in the visualimage and the possible missed detection issue, visual image-based obstacleavoidance techniques still suffer from unsatisfactory robustness. To mitigateit, in this paper, we propose a novel implicit obstacle map-driven indoornavigation framework for robust obstacle avoidance, where an implicit obstaclemap is learned based on the historical trial-and-error experience rather thanthe visual image. In order to further improve the navigation efficiency, anon-local target memory aggregation module is designed to leverage a non-localnetwork to model the intrinsic relationship between the target semantic and thetarget orientation clues during the navigation process so as to mine the mosttarget-correlated object clues for the navigation decision. Extensiveexperimental results on AI2-Thor and RoboTHOR benchmarks verify the excellentobstacle avoidance and navigation efficiency of our proposed method. The coresource code is available at https://github.com/xwaiyy123/object-navigation.</description><author>Wei Xie, Haobo Jiang, Shuo Gu, Jin Xie</author><pubDate>Thu, 24 Aug 2023 16:10:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12845v1</guid></item><item><title>Symplectic model reduction of Hamiltonian systems using data-driven quadratic manifolds</title><link>http://arxiv.org/abs/2305.15490v2</link><description>This work presents two novel approaches for the symplectic model reduction ofhigh-dimensional Hamiltonian systems using data-driven quadratic manifolds.Classical symplectic model reduction approaches employ linear symplecticsubspaces for representing the high-dimensional system states in areduced-dimensional coordinate system. While these approximations respect thesymplectic nature of Hamiltonian systems, linear basis approximations cansuffer from slowly decaying Kolmogorov $N$-width, especially in wave-typeproblems, which then requires a large basis size. We propose two differentmodel reduction methods based on recently developed quadratic manifolds, eachpresenting its own advantages and limitations. The addition of quadratic termsto the state approximation, which sits at the heart of the proposedmethodologies, enables us to better represent intrinsic low-dimensionality inthe problem at hand. Both approaches are effective for issuing predictions insettings well outside the range of their training data while providing moreaccurate solutions than the linear symplectic reduced-order models.</description><author>Harsh Sharma, Hongliang Mu, Patrick Buchfink, Rudy Geelen, Silke Glas, Boris Kramer</author><pubDate>Thu, 24 Aug 2023 16:08:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15490v2</guid></item><item><title>Probabilistic load forecasting with Reservoir Computing</title><link>http://arxiv.org/abs/2308.12844v1</link><description>Some applications of deep learning require not only to provide accurateresults but also to quantify the amount of confidence in their prediction. Themanagement of an electric power grid is one of these cases: to avoid riskyscenarios, decision-makers need both precise and reliable forecasts of, forexample, power loads. For this reason, point forecasts are not enough hence itis necessary to adopt methods that provide an uncertainty quantification. This work focuses on reservoir computing as the core time series forecastingmethod, due to its computational efficiency and effectiveness in predictingtime series. While the RC literature mostly focused on point forecasting, thiswork explores the compatibility of some popular uncertainty quantificationmethods with the reservoir setting. Both Bayesian and deterministic approachesto uncertainty assessment are evaluated and compared in terms of theirprediction accuracy, computational resource efficiency and reliability of theestimated uncertainty, based on a set of carefully chosen performance metrics.</description><author>Michele Guerra, Simone Scardapane, Filippo Maria Bianchi</author><pubDate>Thu, 24 Aug 2023 16:07:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12844v1</guid></item><item><title>Actuator Trajectory Planning for UAVs with Overhead Manipulator using Reinforcement Learning</title><link>http://arxiv.org/abs/2308.12843v1</link><description>In this paper, we investigate the operation of an aerial manipulator system,namely an Unmanned Aerial Vehicle (UAV) equipped with a controllable arm withtwo degrees of freedom to carry out actuation tasks on the fly. Our solution isbased on employing a Q-learning method to control the trajectory of the tip ofthe arm, also called \textit{end-effector}. More specifically, we develop amotion planning model based on Time To Collision (TTC), which enables aquadrotor UAV to navigate around obstacles while ensuring the manipulator'sreachability. Additionally, we utilize a model-based Q-learning model toindependently track and control the desired trajectory of the manipulator'send-effector, given an arbitrary baseline trajectory for the UAV platform. Sucha combination enables a variety of actuation tasks such as high-altitudewelding, structural monitoring and repair, battery replacement, guttercleaning, sky scrapper cleaning, and power line maintenance in hard-to-reachand risky environments while retaining compatibility with flight controlfirmware. Our RL-based control mechanism results in a robust control strategythat can handle uncertainties in the motion of the UAV, offering promisingperformance. Specifically, our method achieves 92\% accuracy in terms ofaverage displacement error (i.e. the mean distance between the target andobtained trajectory points) using Q-learning with 15,000 episodes</description><author>Hazim Alzorgan, Abolfazl Razi, Ata Jahangir Moshayedi</author><pubDate>Thu, 24 Aug 2023 16:06:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12843v1</guid></item><item><title>Text Similarity from Image Contents using Statistical and Semantic Analysis Techniques</title><link>http://arxiv.org/abs/2308.12842v1</link><description>Plagiarism detection is one of the most researched areas among the NaturalLanguage Processing(NLP) community. A good plagiarism detection covers all theNLP methods including semantics, named entities, paraphrases etc. and producesdetailed plagiarism reports. Detection of Cross Lingual Plagiarism requiresdeep knowledge of various advanced methods and algorithms to perform effectivetext similarity checking. Nowadays the plagiarists are also advancingthemselves from hiding the identity from being catch in such offense. Theplagiarists are bypassed from being detected with techniques like paraphrasing,synonym replacement, mismatching citations, translating one language toanother. Image Content Plagiarism Detection (ICPD) has gained importance,utilizing advanced image content processing to identify instances of plagiarismto ensure the integrity of image content. The issue of plagiarism extendsbeyond textual content, as images such as figures, graphs, and tables also havethe potential to be plagiarized. However, image content plagiarism detectionremains an unaddressed challenge. Therefore, there is a critical need todevelop methods and systems for detecting plagiarism in image content. In thispaper, the system has been implemented to detect plagiarism form contents ofImages such as Figures, Graphs, Tables etc. Along with statistical algorithmssuch as Jaccard and Cosine, introducing semantic algorithms such as LSA, BERT,WordNet outperformed in detecting efficient and accurate plagiarism.</description><author>Sagar Kulkarni, Sharvari Govilkar, Dhiraj Amin</author><pubDate>Thu, 24 Aug 2023 16:06:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12842v1</guid></item><item><title>Boosting Convolution with Efficient MLP-Permutation for Volumetric Medical Image Segmentation</title><link>http://arxiv.org/abs/2303.13111v3</link><description>Recently, the advent of vision Transformer (ViT) has brought substantialadvancements in 3D dataset benchmarks, particularly in 3D volumetric medicalimage segmentation (Vol-MedSeg). Concurrently, multi-layer perceptron (MLP)network has regained popularity among researchers due to their comparableresults to ViT, albeit with the exclusion of the resource-intensiveself-attention module. In this work, we propose a novel permutable hybridnetwork for Vol-MedSeg, named PHNet, which capitalizes on the strengths of bothconvolution neural networks (CNNs) and MLP. PHNet addresses the intrinsicisotropy problem of 3D volumetric data by employing a combination of 2D and 3DCNNs to extract local features. Besides, we propose an efficient multi-layerpermute perceptron (MLPP) module that captures long-range dependence whilepreserving positional information. This is achieved through an axisdecomposition operation that permutes the input tensor along different axes,thereby enabling the separate encoding of the positional information.Furthermore, MLPP tackles the resolution sensitivity issue of MLP in Vol-MedSegwith a token segmentation operation, which divides the feature into smallertokens and processes them individually. Extensive experimental results validatethat PHNet outperforms the state-of-the-art methods with lower computationalcosts on the widely-used yet challenging COVID-19-20 and Synapse benchmarks.The ablation study also demonstrates the effectiveness of PHNet in harnessingthe strengths of both CNNs and MLP.</description><author>Yi Lin, Xiao Fang, Dong Zhang, Kwang-Ting Cheng, Hao Chen</author><pubDate>Thu, 24 Aug 2023 16:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13111v3</guid></item><item><title>Leveraging Global Binary Masks for Structure Segmentation in Medical Images</title><link>http://arxiv.org/abs/2205.09107v2</link><description>Deep learning (DL) models for medical image segmentation are highlyinfluenced by intensity variations of input images and lack generalization dueto primarily utilizing pixels' intensity information for inference. Acquiringsufficient training data is another challenge limiting models' applications. Weproposed to leverage the consistency of organs' anatomical shape and positioninformation in medical images. We introduced a framework leveraging recurringanatomical patterns through global binary masks for organ segmentation. Twoscenarios were studied.1) Global binary masks were the only model's (i.e.U-Net) input, forcing exclusively encoding organs' position and shapeinformation for segmentation/localization.2) Global binary masks wereincorporated as an additional channel functioning as position/shape clues tomitigate training data scarcity. Two datasets of the brain and heart CT imageswith their ground-truth were split into (26:10:10) and (12:3:5) for training,validation, and test respectively. Training exclusively on global binary masksled to Dice scores of 0.77(0.06) and 0.85(0.04), with the average Euclidiandistance of 3.12(1.43)mm and 2.5(0.93)mm relative to the center of mass of theground truth for the brain and heart structures respectively. The outcomesindicate that a surprising degree of position and shape information is encodedthrough global binary masks. Incorporating global binary masks led tosignificantly higher accuracy relative to the model trained on only CT imagesin small subsets of training data; the performance improved by 4.3-125.3% and1.3-48.1% for 1-8 training cases of the brain and heart datasets respectively.The findings imply the advantages of utilizing global binary masks for buildinggeneralizable models and to compensate for training data scarcity.</description><author>Mahdieh Kazemimoghadam, Zi Yang, Lin Ma, Mingli Chen, Weiguo Lu, Xuejun Gu</author><pubDate>Thu, 24 Aug 2023 16:00:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.09107v2</guid></item><item><title>FaceTouch: Detecting hand-to-face touch with supervised contrastive learning to assist in tracing infectious disease</title><link>http://arxiv.org/abs/2308.12840v1</link><description>Through our respiratory system, many viruses and diseases frequently spreadand pass from one person to another. Covid-19 served as an example of howcrucial it is to track down and cut back on contacts to stop its spread. Thereis a clear gap in finding automatic methods that can detect hand-to-facecontact in complex urban scenes or indoors. In this paper, we introduce acomputer vision framework, called FaceTouch, based on deep learning. Itcomprises deep sub-models to detect humans and analyse their actions. FaceTouchseeks to detect hand-to-face touches in the wild, such as through video chats,bus footage, or CCTV feeds. Despite partial occlusion of faces, the introducedsystem learns to detect face touches from the RGB representation of a givenscene by utilising the representation of the body gestures such as armmovement. This has been demonstrated to be useful in complex urban scenariosbeyond simply identifying hand movement and its closeness to faces. Relying onSupervised Contrastive Learning, the introduced model is trained on ourcollected dataset, given the absence of other benchmark datasets. The frameworkshows a strong validation in unseen datasets which opens the door for potentialdeployment.</description><author>Mohamed R. Ibrahim, Terry Lyons</author><pubDate>Thu, 24 Aug 2023 15:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12840v1</guid></item><item><title>A Survey on Dataset Distillation: Approaches, Applications and Future Directions</title><link>http://arxiv.org/abs/2305.01975v3</link><description>Dataset distillation is attracting more attention in machine learning astraining sets continue to grow and the cost of training state-of-the-art modelsbecomes increasingly high. By synthesizing datasets with high informationdensity, dataset distillation offers a range of potential applications,including support for continual learning, neural architecture search, andprivacy protection. Despite recent advances, we lack a holistic understandingof the approaches and applications. Our survey aims to bridge this gap by firstproposing a taxonomy of dataset distillation, characterizing existingapproaches, and then systematically reviewing the data modalities, and relatedapplications. In addition, we summarize the challenges and discuss futuredirections for this field of research.</description><author>Jiahui Geng, Zongxiong Chen, Yuandou Wang, Herbert Woisetschlaeger, Sonja Schimmler, Ruben Mayer, Zhiming Zhao, Chunming Rong</author><pubDate>Thu, 24 Aug 2023 15:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01975v3</guid></item><item><title>Positive AI: Key Challenges for Designing Wellbeing-aligned Artificial Intelligence</title><link>http://arxiv.org/abs/2304.12241v2</link><description>Artificial Intelligence (AI) is rapidly transforming society, creating anurgent need to ensure its positive impact. In this article, we take a positivedesign approach towards this issue, viewing it as a matter of designing AIsystems that actively support human wellbeing. However, designingwellbeing-aligned AI systems is difficult. This article adopts a cyberneticperspective to identify twelve key challenges across two categories: lack ofknowledge and lack of motivation. Knowledge barriers include challenges inconceptualizing, measuring, and optimizing for wellbeing, then designingappropriate AI actions. Motivation barriers include misaligned incentives,financial and publicity risks, and a lack of data access preventing(third-party) research on wellbeing. To address these challenges we havecaptured our key takeaways in a research agenda related to 1) advancing thescientific understanding of the impact of AI systems on wellbeing, and 2)guiding design actions on how AI systems might be intentionally designed topromote and sustain wellbeing.</description><author>Willem van der Maden, Derek Lomas, Malak Sadek, Paul Hekkert</author><pubDate>Thu, 24 Aug 2023 15:47:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12241v2</guid></item><item><title>Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities</title><link>http://arxiv.org/abs/2308.12833v1</link><description>Spurred by the recent rapid increase in the development and distribution oflarge language models (LLMs) across industry and academia, much recent work hasdrawn attention to safety- and security-related threats and vulnerabilities ofLLMs, including in the context of potentially criminal activities.Specifically, it has been shown that LLMs can be misused for fraud,impersonation, and the generation of malware; while other authors haveconsidered the more general problem of AI alignment. It is important thatdevelopers and practitioners alike are aware of security-related problems withsuch models. In this paper, we provide an overview of existing - predominantlyscientific - efforts on identifying and mitigating threats and vulnerabilitiesarising from LLMs. We present a taxonomy describing the relationship betweenthreats caused by the generative capabilities of LLMs, prevention measuresintended to address such threats, and vulnerabilities arising from imperfectprevention measures. With our work, we hope to raise awareness of thelimitations of LLMs in light of such security concerns, among both experienceddevelopers and novel users of such technologies.</description><author>Maximilian Mozes, Xuanli He, Bennett Kleinberg, Lewis D. Griffin</author><pubDate>Thu, 24 Aug 2023 15:45:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12833v1</guid></item><item><title>EFormer: Enhanced Transformer towards Semantic-Contour Features of Foreground for Portraits Matting</title><link>http://arxiv.org/abs/2308.12831v1</link><description>The portrait matting task aims to extract an alpha matte with completesemantics and finely-detailed contours. In comparison to CNN-based approaches,transformers with self-attention allow a larger receptive field, enabling it tobetter capture long-range dependencies and low-frequency semantic informationof a portrait. However, the recent research shows that self-attention mechanismstruggle with modeling high-frequency information and capturing fine contourdetails, which can lead to bias while predicting the portrait's contours. Toaddress the problem, we propose EFormer to enhance the model's attentiontowards semantic and contour features. Especially the latter, which issurrounded by a large amount of high-frequency details. We build a semantic andcontour detector (SCD) to accurately capture the distribution of semantic andcontour features. And we further design contour-edge extraction branch andsemantic extraction branch for refining contour features and complete semanticinformation. Finally, we fuse the two kinds of features and leverage thesegmentation head to generate the predicted portrait matte. Remarkably, EFormeris an end-to-end trimap-free method and boasts a simple structure. Experimentsconducted on VideoMatte240K-JPEGSD and AIM datasets demonstrate that EFormeroutperforms previous portrait matte methods.</description><author>Zitao Wang, Qiguang Miao, Yue Xi</author><pubDate>Thu, 24 Aug 2023 15:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12831v1</guid></item><item><title>Short Run Transit Route Planning Decision Support System Using a Deep Learning-Based Weighted Graph</title><link>http://arxiv.org/abs/2308.12828v1</link><description>Public transport routing plays a crucial role in transit network design,ensuring a satisfactory level of service for passengers. However, currentrouting solutions rely on traditional operational research heuristics, whichcan be time-consuming to implement and lack the ability to provide quicksolutions. Here, we propose a novel deep learning-based methodology for adecision support system that enables public transport (PT) planners to identifyshort-term route improvements rapidly. By seamlessly adjusting specificsections of routes between two stops during specific times of the day, ourmethod effectively reduces times and enhances PT services. Leveraging diversedata sources such as GTFS and smart card data, we extract features and modelthe transportation network as a directed graph. Using self-supervision, wetrain a deep learning model for predicting lateness values for road segments. These lateness values are then utilized as edge weights in the transportationgraph, enabling efficient path searching. Through evaluating the method on TelAviv, we are able to reduce times on more than 9\% of the routes. The improvedroutes included both intraurban and suburban routes showcasing a facthighlighting the model's versatility. The findings emphasize the potential ofour data-driven decision support system to enhance public transport and citylogistics, promoting greater efficiency and reliability in PT services.</description><author>Nadav Shalit, Michael Fire, Dima Kagan, Eran Ben-Elia</author><pubDate>Thu, 24 Aug 2023 15:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12828v1</guid></item><item><title>VeriCompress: A Tool to Streamline the Synthesis of Verified Robust Compressed Neural Networks from Scratch</title><link>http://arxiv.org/abs/2211.09945v6</link><description>AI's widespread integration has led to neural networks (NNs) deployment onedge and similar limited-resource platforms for safety-critical scenarios. Yet,NN's fragility raises concerns about reliable inference. Moreover, constrainedplatforms demand compact networks. This study introduces VeriCompress, a toolthat automates the search and training of compressed models with robustnessguarantees. These models are well-suited for safety-critical applications andadhere to predefined architecture and size limitations, making them deployableon resource-restricted platforms. The method trains models 2-3 times fasterthan the state-of-the-art approaches, surpassing relevant baseline approachesby average accuracy and robustness gains of 15.1 and 9.8 percentage points,respectively. When deployed on a resource-restricted generic platform, thesemodels require 5-8 times less memory and 2-4 times less inference time thanmodels used in verified robustness literature. Our comprehensive evaluationacross various model architectures and datasets, including MNIST, CIFAR, SVHN,and a relevant pedestrian detection dataset, showcases VeriCompress's capacityto identify compressed verified robust models with reduced computation overheadcompared to current standards. This underscores its potential as a valuabletool for end users, such as developers of safety-critical applications on edgeor Internet of Things platforms, empowering them to create suitable models forsafety-critical, resource-constrained platforms in their respective domains.</description><author>Sawinder Kaur, Yi Xiao, Asif Salekin</author><pubDate>Thu, 24 Aug 2023 15:35:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09945v6</guid></item><item><title>Prediction without Preclusion: Recourse Verification with Reachable Sets</title><link>http://arxiv.org/abs/2308.12820v1</link><description>Machine learning models are often used to decide who will receive a loan, ajob interview, or a public benefit. Standard techniques to build these modelsuse features about people but overlook their actionability. In turn, models canassign predictions that are fixed, meaning that consumers who are denied loans,interviews, or benefits may be permanently locked out from access to credit,employment, or assistance. In this work, we introduce a formal testingprocedure to flag models that assign fixed predictions that we call recourseverification. We develop machinery to reliably determine if a given model canprovide recourse to its decision subjects from a set of user-specifiedactionability constraints. We demonstrate how our tools can ensure recourse andadversarial robustness in real-world datasets and use them to study theinfeasibility of recourse in real-world lending datasets. Our results highlighthow models can inadvertently assign fixed predictions that permanently baraccess, and we provide tools to design algorithms that account foractionability when developing models.</description><author>Avni Kothari, Bogdan Kulynych, Tsui-Wei Weng, Berk Ustun</author><pubDate>Thu, 24 Aug 2023 15:24:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12820v1</guid></item><item><title>Unifying Gradients to Improve Real-world Robustness for Deep Networks</title><link>http://arxiv.org/abs/2208.06228v2</link><description>The wide application of deep neural networks (DNNs) demands an increasingamount of attention to their real-world robustness, i.e., whether a DNN resistsblack-box adversarial attacks, among which score-based query attacks (SQAs) aremost threatening since they can effectively hurt a victim network with the onlyaccess to model outputs. Defending against SQAs requires a slight but artfulvariation of outputs due to the service purpose for users, who share the sameoutput information with SQAs. In this paper, we propose a real-world defense byUnifying Gradients (UniG) of different data so that SQAs could only probe amuch weaker attack direction that is similar for different samples. Since suchuniversal attack perturbations have been validated as less aggressive than theinput-specific perturbations, UniG protects real-world DNNs by indicatingattackers a twisted and less informative attack direction. We implement UniGefficiently by a Hadamard product module which is plug-and-play. According toextensive experiments on 5 SQAs, 2 adaptive attacks and 7 defense baselines,UniG significantly improves real-world robustness without hurting cleanaccuracy on CIFAR10 and ImageNet. For instance, UniG maintains a model of77.80% accuracy under 2500-query Square attack while the state-of-the-artadversarially-trained model only has 67.34% on CIFAR10. Simultaneously, UniGoutperforms all compared baselines in terms of clean accuracy and achieves thesmallest modification of the model output. The code is released athttps://github.com/snowien/UniG-pytorch.</description><author>Yingwen Wu, Sizhe Chen, Kun Fang, Xiaolin Huang</author><pubDate>Thu, 24 Aug 2023 15:22:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.06228v2</guid></item><item><title>Min-Max Optimization under Delays</title><link>http://arxiv.org/abs/2307.06886v2</link><description>Delays and asynchrony are inevitable in large-scale machine-learning problemswhere communication plays a key role. As such, several works have extensivelyanalyzed stochastic optimization with delayed gradients. However, as far as weare aware, no analogous theory is available for min-max optimization, a topicthat has gained recent popularity due to applications in adversarialrobustness, game theory, and reinforcement learning. Motivated by this gap, weexamine the performance of standard min-max optimization algorithms withdelayed gradient updates. First, we show (empirically) that even small delayscan cause prominent algorithms like Extra-gradient (\texttt{EG}) to diverge onsimple instances for which \texttt{EG} guarantees convergence in the absence ofdelays. Our empirical study thus suggests the need for a careful analysis ofdelayed versions of min-max optimization algorithms. Accordingly, undersuitable technical assumptions, we prove that Gradient Descent-Ascent(\texttt{GDA}) and \texttt{EG} with delayed updates continue to guaranteeconvergence to saddle points for convex-concave and strongly convex-stronglyconcave settings. Our complexity bounds reveal, in a transparent manner, theslow-down in convergence caused by delays.</description><author>Arman Adibi, Aritra Mitra, Hamed Hassani</author><pubDate>Thu, 24 Aug 2023 15:22:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06886v2</guid></item><item><title>An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization</title><link>http://arxiv.org/abs/2308.12126v2</link><description>We propose an accelerated block proximal linear framework with adaptivemomentum (ABPL$^+$) for nonconvex and nonsmooth optimization. We analyze thepotential causes of the extrapolation step failing in some algorithms, andresolve this issue by enhancing the comparison process that evaluates thetrade-off between the proximal gradient step and the linear extrapolation stepin our algorithm. Furthermore, we extends our algorithm to any scenarioinvolving updating block variables with positive integers, allowing each cycleto randomly shuffle the update order of the variable blocks. Additionally,under mild assumptions, we prove that ABPL$^+$ can monotonically decrease thefunction value without strictly restricting the extrapolation parameters andstep size, demonstrates the viability and effectiveness of updating theseblocks in a random order, and we also more obviously and intuitivelydemonstrate that the derivative set of the sequence generated by our algorithmis a critical point set. Moreover, we demonstrate the global convergence aswell as the linear and sublinear convergence rates of our algorithm byutilizing the Kurdyka-Lojasiewicz (K{\L}) condition. To enhance theeffectiveness and flexibility of our algorithm, we also expand the study to theimprecise version of our algorithm and construct an adaptive extrapolationparameter strategy, which improving its overall performance. We apply ouralgorithm to multiple non-negative matrix factorization with the $\ell_0$ norm,nonnegative tensor decomposition with the $\ell_0$ norm, and perform extensivenumerical experiments to validate its effectiveness and efficiency.</description><author>Weifeng Yang, Wenwen Min</author><pubDate>Thu, 24 Aug 2023 15:16:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12126v2</guid></item><item><title>Individual Privacy Accounting with Gaussian Differential Privacy</title><link>http://arxiv.org/abs/2209.15596v2</link><description>Individual privacy accounting enables bounding differential privacy (DP) lossindividually for each participant involved in the analysis. This can beinformative as often the individual privacy losses are considerably smallerthan those indicated by the DP bounds that are based on considering worst-casebounds at each data access. In order to account for the individual privacylosses in a principled manner, we need a privacy accountant for adaptivecompositions of randomised mechanisms, where the loss incurred at a given dataaccess is allowed to be smaller than the worst-case loss. This kind of analysishas been carried out for the R\'enyi differential privacy (RDP) by Feldman andZrnic (2021), however not yet for the so-called optimal privacy accountants. Wemake first steps in this direction by providing a careful analysis using theGaussian differential privacy which gives optimal bounds for the Gaussianmechanism, one of the most versatile DP mechanisms. This approach is based ondetermining a certain supermartingale for the hockey-stick divergence and onextending the R\'enyi divergence-based fully adaptive composition results byFeldman and Zrnic. We also consider measuring the individual$(\varepsilon,\delta)$-privacy losses using the so-called privacy lossdistributions. With the help of the Blackwell theorem, we can then make use ofthe RDP analysis to construct an approximative individual$(\varepsilon,\delta)$-accountant.</description><author>Antti Koskela, Marlon Tobaben, Antti Honkela</author><pubDate>Thu, 24 Aug 2023 15:00:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.15596v2</guid></item><item><title>Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods</title><link>http://arxiv.org/abs/2308.12794v1</link><description>We introduce an open-source GitHub repository containing comprehensivebenchmarks for a wide range of machine scheduling problems, including Job ShopScheduling (JSP), Flow Shop Scheduling (FSP), Flexible Job Shop Scheduling(FJSP), FJSP with Assembly constraints (FAJSP), FJSP with Sequence-DependentSetup Times (FJSP-SDST), and the online FJSP (with online job arrivals). Ourprimary goal is to provide a centralized hub for researchers, practitioners,and enthusiasts interested in tackling machine scheduling challenges.</description><author>Robbert Reijnen, Kjell van Straaten, Zaharah Bukhsh, Yingqian Zhang</author><pubDate>Thu, 24 Aug 2023 14:49:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12794v1</guid></item><item><title>Interneurons accelerate learning dynamics in recurrent neural networks for statistical adaptation</title><link>http://arxiv.org/abs/2209.10634v2</link><description>Early sensory systems in the brain rapidly adapt to fluctuating inputstatistics, which requires recurrent communication between neurons.Mechanistically, such recurrent communication is often indirect and mediated bylocal interneurons. In this work, we explore the computational benefits ofmediating recurrent communication via interneurons compared with directrecurrent connections. To this end, we consider two mathematically tractablerecurrent linear neural networks that statistically whiten their inputs -- onewith direct recurrent connections and the other with interneurons that mediaterecurrent communication. By analyzing the corresponding continuous synapticdynamics and numerically simulating the networks, we show that the network withinterneurons is more robust to initialization than the network with directrecurrent connections in the sense that the convergence time for the synapticdynamics in the network with interneurons (resp. direct recurrent connections)scales logarithmically (resp. linearly) with the spectrum of theirinitialization. Our results suggest that interneurons are computationallyuseful for rapid adaptation to changing input statistics. Interestingly, thenetwork with interneurons is an overparameterized solution of the whiteningobjective for the network with direct recurrent connections, so our results canbe viewed as a recurrent linear neural network analogue of the implicitacceleration phenomenon observed in overparameterized feedforward linear neuralnetworks.</description><author>David Lipshutz, Cengiz Pehlevan, Dmitri B. Chklovskii</author><pubDate>Thu, 24 Aug 2023 14:46:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.10634v2</guid></item><item><title>Robotic Scene Segmentation with Memory Network for Runtime Surgical Context Inference</title><link>http://arxiv.org/abs/2308.12789v1</link><description>Surgical context inference has recently garnered significant attention inrobot-assisted surgery as it can facilitate workflow analysis, skillassessment, and error detection. However, runtime context inference ischallenging since it requires timely and accurate detection of the interactionsamong the tools and objects in the surgical scene based on the segmentation ofvideo data. On the other hand, existing state-of-the-art video segmentationmethods are often biased against infrequent classes and fail to providetemporal consistency for segmented masks. This can negatively impact thecontext inference and accurate detection of critical states. In this study, wepropose a solution to these challenges using a Space Time CorrespondenceNetwork (STCN). STCN is a memory network that performs binary segmentation andminimizes the effects of class imbalance. The use of a memory bank in STCNallows for the utilization of past image and segmentation information, therebyensuring consistency of the masks. Our experiments using the publicly availableJIGSAWS dataset demonstrate that STCN achieves superior segmentationperformance for objects that are difficult to segment, such as needle andthread, and improves context inference compared to the state-of-the-art. Wealso demonstrate that segmentation and context inference can be performed atruntime without compromising performance.</description><author>Zongyu Li, Ian Reyes, Homa Alemzadeh</author><pubDate>Thu, 24 Aug 2023 14:44:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12789v1</guid></item><item><title>Single-shot Bayesian approximation for neural networks</title><link>http://arxiv.org/abs/2308.12785v1</link><description>Deep neural networks (NNs) are known for their high-prediction performances.However, NNs are prone to yield unreliable predictions when encounteringcompletely new situations without indicating their uncertainty. Bayesianvariants of NNs (BNNs), such as Monte Carlo (MC) dropout BNNs, do provideuncertainty measures and simultaneously increase the prediction performance.The only disadvantage of BNNs is their higher computation time during test timebecause they rely on a sampling approach. Here we present a single-shot MCdropout approximation that preserves the advantages of BNNs while being as fastas NNs. Our approach is based on moment propagation (MP) and allows toanalytically approximate the expected value and the variance of the MC dropoutsignal for commonly used layers in NNs, i.e. convolution, max pooling, dense,softmax, and dropout layers. The MP approach can convert an NN into a BNNwithout re-training given the NN has been trained with standard dropout. Weevaluate our approach on different benchmark datasets and a simulated toyexample in a classification and regression setting. We demonstrate that oursingle-shot MC dropout approximation resembles the point estimate and theuncertainty estimate of the predictive distribution that is achieved with an MCapproach, while being fast enough for real-time deployments of BNNs. We showthat using part of the saved time to combine our MP approach with deep ensembletechniques does further improve the uncertainty measures.</description><author>Kai Brach, Beate Sick, Oliver D√ºrr</author><pubDate>Thu, 24 Aug 2023 14:40:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12785v1</guid></item><item><title>Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers</title><link>http://arxiv.org/abs/2306.04504v3</link><description>ChatGPT is a large language model developed by OpenAI. Despite its impressiveperformance across various tasks, no prior work has investigated its capabilityin the biomedical domain yet. To this end, this paper aims to evaluate theperformance of ChatGPT on various benchmark biomedical tasks, such as relationextraction, document classification, question answering, and summarization. Tothe best of our knowledge, this is the first work that conducts an extensiveevaluation of ChatGPT in the biomedical domain. Interestingly, we find based onour evaluation that in biomedical datasets that have smaller training sets,zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generativetransformer models, such as BioGPT and BioBART. This suggests that ChatGPT'spre-training on large text corpora makes it quite specialized even in thebiomedical domain. Our findings demonstrate that ChatGPT has the potential tobe a valuable tool for various tasks in the biomedical domain that lack largeannotated data.</description><author>Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang</author><pubDate>Thu, 24 Aug 2023 14:39:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04504v3</guid></item><item><title>Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces</title><link>http://arxiv.org/abs/2303.00028v4</link><description>The sensor placement problem is a common problem that arises when monitoringcorrelated phenomena, such as temperature and precipitation. Existingapproaches to this problem typically use discrete optimization methods, whichare computationally expensive and cannot scale to large problems. We addressthe sensor placement problem in correlated environments by reducing it to aregression problem that can be efficiently solved using sparse Gaussianprocesses (SGPs). Our approach can handle both discrete sensor placementproblems-where sensors are limited to a subset of a given set of locations-andcontinuous sensor placement problems-where sensors can be placed anywhere in abounded continuous region. We further generalize our approach to handle sensorswith a non-point field of view and integrated observations. Our experimentalresults on three real-world datasets show that our approach generates sensorplacements that result in reconstruction quality that is consistently on par orbetter than the prior state-of-the-art approach while being significantlyfaster. Our computationally efficient approach enables both large-scale sensorplacement and fast robotic sensor placement for informative path planningalgorithms.</description><author>Kalvik Jakkala, Srinivas Akella</author><pubDate>Thu, 24 Aug 2023 14:33:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00028v4</guid></item><item><title>On Offline Evaluation of 3D Object Detection for Autonomous Driving</title><link>http://arxiv.org/abs/2308.12779v1</link><description>Prior work in 3D object detection evaluates models using offline metrics likeaverage precision since closed-loop online evaluation on the downstream drivingtask is costly. However, it is unclear how indicative offline results are ofdriving performance. In this work, we perform the first empirical evaluationmeasuring how predictive different detection metrics are of driving performancewhen detectors are integrated into a full self-driving stack. We conductextensive experiments on urban driving in the CARLA simulator using 16 objectdetection models. We find that the nuScenes Detection Score has a highercorrelation to driving performance than the widely used average precisionmetric. In addition, our results call for caution on the exclusive reliance onthe emerging class of `planner-centric' metrics.</description><author>Tim Schreier, Katrin Renz, Andreas Geiger, Kashyap Chitta</author><pubDate>Thu, 24 Aug 2023 14:31:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12779v1</guid></item><item><title>Universal Soldier: Using Universal Adversarial Perturbations for Detecting Backdoor Attacks</title><link>http://arxiv.org/abs/2302.00747v3</link><description>Deep learning models achieve excellent performance in numerous machinelearning tasks. Yet, they suffer from security-related issues such asadversarial examples and poisoning (backdoor) attacks. A deep learning modelmay be poisoned by training with backdoored data or by modifying inner networkparameters. Then, a backdoored model performs as expected when receiving aclean input, but it misclassifies when receiving a backdoored input stampedwith a pre-designed pattern called "trigger". Unfortunately, it is difficult todistinguish between clean and backdoored models without prior knowledge of thetrigger. This paper proposes a backdoor detection method by utilizing a specialtype of adversarial attack, universal adversarial perturbation (UAP), and itssimilarities with a backdoor trigger. We observe an intuitive phenomenon: UAPsgenerated from backdoored models need fewer perturbations to mislead the modelthan UAPs from clean models. UAPs of backdoored models tend to exploit theshortcut from all classes to the target class, built by the backdoor trigger.We propose a novel method called Universal Soldier for Backdoor detection (USB)and reverse engineering potential backdoor triggers via UAPs. Experiments on345 models trained on several datasets show that USB effectively detects theinjected backdoor and provides comparable or better results thanstate-of-the-art methods.</description><author>Xiaoyun Xu, Oguzhan Ersoy, Stjepan Picek</author><pubDate>Thu, 24 Aug 2023 14:27:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00747v3</guid></item><item><title>LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition</title><link>http://arxiv.org/abs/2308.12774v1</link><description>The diversity in length constitutes a significant characteristic of text. Dueto the long-tail distribution of text lengths, most existing methods for scenetext recognition (STR) only work well on short or seen-length text, lacking thecapability of recognizing longer text or performing length extrapolation. Thisis a crucial issue, since the lengths of the text to be recognized are usuallynot given in advance in real-world applications, but it has not been adequatelyinvestigated in previous works. Therefore, we propose in this paper a methodcalled Length-Insensitive Scene TExt Recognizer (LISTER), which remedies thelimitation regarding the robustness to various text lengths. Specifically, aNeighbor Decoder is proposed to obtain accurate character attention maps withthe assistance of a novel neighbor matrix regardless of the text lengths.Besides, a Feature Enhancement Module is devised to model the long-rangedependency with low computation cost, which is able to perform iterations withthe neighbor decoder to enhance the feature map progressively. To the best ofour knowledge, we are the first to achieve effective length-insensitive scenetext recognition. Extensive experiments demonstrate that the proposed LISTERalgorithm exhibits obvious superiority on long text recognition and the abilityfor length extrapolation, while comparing favourably with the previousstate-of-the-art methods on standard benchmarks for STR (mainly short text).</description><author>Changxu Cheng, Peng Wang, Cheng Da, Qi Zheng, Cong Yao</author><pubDate>Thu, 24 Aug 2023 14:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12774v1</guid></item><item><title>HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials</title><link>http://arxiv.org/abs/2308.11787v2</link><description>Robotics and automation offer massive accelerations for solving intractable,multivariate scientific problems such as materials discovery, but the availablesearch spaces can be dauntingly large. Bayesian optimization (BO) has emergedas a popular sample-efficient optimization engine, thriving in tasks where noanalytic form of the target function/property is known. Here we exploit experthuman knowledge in the form of hypotheses to direct Bayesian searches morequickly to promising regions of chemical space. Previous methods have usedunderlying distributions derived from existing experimental measurements, whichis unfeasible for new, unexplored scientific tasks. Also, such distributionscannot capture intricate hypotheses. Our proposed method, which we call HypBO,uses expert human hypotheses to generate an improved seed of samples.Unpromising seeds are automatically discounted, while promising seeds are usedto augment the surrogate model data, thus achieving better-informed sampling.This process continues in a global versus local search fashion, organized in abilevel optimization framework. We validate the performance of our method on arange of synthetic functions and demonstrate its practical utility on a realchemical design task where the use of expert hypotheses accelerates the searchperformance significantly.</description><author>Abdoulatif Cisse, Xenophon Evangelopoulos, Sam Carruthers, Vladimir V. Gusev, Andrew I. Cooper</author><pubDate>Thu, 24 Aug 2023 14:24:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11787v2</guid></item><item><title>Intentionally-underestimated Value Function at Terminal State for Temporal-difference Learning with Mis-designed Reward</title><link>http://arxiv.org/abs/2308.12772v1</link><description>Robot control using reinforcement learning has become popular, but itslearning process generally terminates halfway through an episode for safety andtime-saving reasons. This study addresses the problem of the most popularexception handling that temporal-difference (TD) learning performs at suchtermination. That is, by forcibly assuming zero value after termination,unintentionally implicit underestimation or overestimation occurs, depending onthe reward design in the normal states. When the episode is terminated due totask failure, the failure may be highly valued with the unintentionaloverestimation, and the wrong policy may be acquired. Although this problem canbe avoided by paying attention to the reward design, it is essential inpractical use of TD learning to review the exception handling at termination.This paper therefore proposes a method to intentionally underestimate the valueafter termination to avoid learning failures due to the unintentionaloverestimation. In addition, the degree of underestimation is adjustedaccording to the degree of stationarity at termination, thereby preventingexcessive exploration due to the intentional underestimation. Simulations andreal robot experiments showed that the proposed method can stably obtain theoptimal policies for various tasks and reward designs.https://youtu.be/AxXr8uFOe7M</description><author>Taisuke Kobayashi</author><pubDate>Thu, 24 Aug 2023 14:21:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12772v1</guid></item><item><title>WavMark: Watermarking for Audio Generation</title><link>http://arxiv.org/abs/2308.12770v1</link><description>Recent breakthroughs in zero-shot voice synthesis have enabled imitating aspeaker's voice using just a few seconds of recording while maintaining a highlevel of realism. Alongside its potential benefits, this powerful technologyintroduces notable risks, including voice fraud and speaker impersonation.Unlike the conventional approach of solely relying on passive methods fordetecting synthetic data, watermarking presents a proactive and robust defencemechanism against these looming risks. This paper introduces an innovativeaudio watermarking framework that encodes up to 32 bits of watermark within amere 1-second audio snippet. The watermark is imperceptible to human senses andexhibits strong resilience against various attacks. It can serve as aneffective identifier for synthesized voices and holds potential for broaderapplications in audio copyright protection. Moreover, this framework boastshigh flexibility, allowing for the combination of multiple watermark segmentsto achieve heightened robustness and expanded capacity. Utilizing 10 to20-second audio as the host, our approach demonstrates an average Bit ErrorRate (BER) of 0.48\% across ten common attacks, a remarkable reduction of over2800\% in BER compared to the state-of-the-art watermarking tool. Seehttps://aka.ms/wavmark for demos of our work.</description><author>Guangyu Chen, Yu Wu, Shujie Liu, Tao Liu, Xiaoyong Du, Furu Wei</author><pubDate>Thu, 24 Aug 2023 14:17:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12770v1</guid></item><item><title>On the Consistency of Average Embeddings for Item Recommendation</title><link>http://arxiv.org/abs/2308.12767v1</link><description>A prevalent practice in recommender systems consists of averaging itemembeddings to represent users or higher-level concepts in the same embeddingspace. This paper investigates the relevance of such a practice. For thispurpose, we propose an expected precision score, designed to measure theconsistency of an average embedding relative to the items used for itsconstruction. We subsequently analyze the mathematical expression of this scorein a theoretical setting with specific assumptions, as well as its empiricalbehavior on real-world data from music streaming services. Our resultsemphasize that real-world averages are less consistent for recommendation,which paves the way for future research to better align real-world embeddingswith assumptions from our theoretical setting.</description><author>Walid Bendada, Guillaume Salha-Galvan, Romain Hennequin, Thomas Bouab√ßa, Tristan Cazenave</author><pubDate>Thu, 24 Aug 2023 14:14:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12767v1</guid></item><item><title>Reinforcement learning informed evolutionary search for autonomous systems testing</title><link>http://arxiv.org/abs/2308.12762v1</link><description>Evolutionary search-based techniques are commonly used for testing autonomousrobotic systems. However, these approaches often rely on computationallyexpensive simulator-based models for test scenario evaluation. To improve thecomputational efficiency of the search-based testing, we propose augmenting theevolutionary search (ES) with a reinforcement learning (RL) agent trained usingsurrogate rewards derived from domain knowledge. In our approach, known asRIGAA (Reinforcement learning Informed Genetic Algorithm for Autonomous systemstesting), we first train an RL agent to learn useful constraints of the problemand then use it to produce a certain part of the initial population of thesearch algorithm. By incorporating an RL agent into the search process, we aimto guide the algorithm towards promising regions of the search space from thestart, enabling more efficient exploration of the solution space. We evaluateRIGAA on two case studies: maze generation for an autonomous ant robot and roadtopology generation for an autonomous vehicle lane keeping assist system. Inboth case studies, RIGAA converges faster to fitter solutions and produces abetter test suite (in terms of average test scenario fitness and diversity).RIGAA also outperforms the state-of-the-art tools for vehicle lane keepingassist system testing, such as AmbieGen and Frenetic.</description><author>Dmytro Humeniuk, Foutse Khomh, Giuliano Antoniol</author><pubDate>Thu, 24 Aug 2023 14:11:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12762v1</guid></item><item><title>Reliable Multimodality Eye Disease Screening via Mixture of Student's t Distributions</title><link>http://arxiv.org/abs/2303.09790v3</link><description>Multimodality eye disease screening is crucial in ophthalmology as itintegrates information from diverse sources to complement their respectiveperformances. However, the existing methods are weak in assessing thereliability of each unimodality, and directly fusing an unreliable modality maycause screening errors. To address this issue, we introduce a novelmultimodality evidential fusion pipeline for eye disease screening, EyeMoSt,which provides a measure of confidence for unimodality and elegantly integratesthe multimodality information from a multi-distribution fusion perspective.Specifically, our model estimates both local uncertainty for unimodality andglobal uncertainty for the fusion modality to produce reliable classificationresults. More importantly, the proposed mixture of Student's $t$ distributionsadaptively integrates different modalities to endow the model with heavy-tailedproperties, increasing robustness and reliability. Our experimental findings onboth public and in-house datasets show that our model is more reliable thancurrent methods. Additionally, EyeMost has the potential ability to serve as adata quality discriminator, enabling reliable decision-making for multimodalityeye disease screening.</description><author>Ke Zou, Tian Lin, Xuedong Yuan, Haoyu Chen, Xiaojing Shen, Meng Wang, Huazhu Fu</author><pubDate>Thu, 24 Aug 2023 14:11:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09790v3</guid></item><item><title>IP-UNet: Intensity Projection UNet Architecture for 3D Medical Volume Segmentation</title><link>http://arxiv.org/abs/2308.12761v1</link><description>CNNs have been widely applied for medical image analysis. However, limitedmemory capacity is one of the most common drawbacks of processinghigh-resolution 3D volumetric data. 3D volumes are usually cropped or downsizedfirst before processing, which can result in a loss of resolution, increaseclass imbalance, and affect the performance of the segmentation algorithms. Inthis paper, we propose an end-to-end deep learning approach called IP-UNet.IP-UNet is a UNet-based model that performs multi-class segmentation onIntensity Projection (IP) of 3D volumetric data instead of the memory-consuming3D volumes. IP-UNet uses limited memory capability for training without losingthe original 3D image resolution. We compare the performance of three models interms of segmentation accuracy and computational cost: 1) Slice-by-slice 2Dsegmentation of the CT scan images using a conventional 2D UNet model. 2)IP-UNet that operates on data obtained by merging the extracted MaximumIntensity Projection (MIP), Closest Vessel Projection (CVP), and AverageIntensity Projection (AvgIP) representations of the source 3D volumes, thenapplying the UNet model on the output IP images. 3) 3D-UNet model directlyreads the 3D volumes constructed from a series of CT scan images and outputsthe 3D volume of the predicted segmentation. We test the performance of thesemethods on 3D volumetric images for automatic breast calcification detection.Experimental results show that IP-Unet can achieve similar segmentationaccuracy with 3D-Unet but with much better performance. It reduces the trainingtime by 70\% and memory consumption by 92\%.</description><author>Nyothiri Aung, Tahar Kechadi, Liming Chen, Sahraoui Dhelim</author><pubDate>Thu, 24 Aug 2023 14:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12761v1</guid></item><item><title>On the Generalization of PINNs outside the training domain and the Hyperparameters influencing it</title><link>http://arxiv.org/abs/2302.07557v2</link><description>Physics-Informed Neural Networks (PINNs) are Neural Network architecturestrained to emulate solutions of differential equations without the necessity ofsolution data. They are currently ubiquitous in the scientific literature dueto their flexible and promising settings. However, very little of the availableresearch provides practical studies that aim for a better quantitativeunderstanding of such architecture and its functioning. In this paper, weperform an empirical analysis of the behavior of PINN predictions outside theirtraining domain. The primary goal is to investigate the scenarios in which aPINN can provide consistent predictions outside the training area.Thereinafter, we assess whether the algorithmic setup of PINNs can influencetheir potential for generalization and showcase the respective effect on theprediction. The results obtained in this study returns insightful and at timescounterintuitive perspectives which can be highly relevant for architectureswhich combines PINNs with domain decomposition and/or adaptive trainingstrategies.</description><author>Andrea Bonfanti, Roberto Santana, Marco Ellero, Babak Gholami</author><pubDate>Thu, 24 Aug 2023 14:07:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07557v2</guid></item><item><title>Edge-aware Hard Clustering Graph Pooling for Brain Imaging Data</title><link>http://arxiv.org/abs/2308.11909v2</link><description>Graph Convolutional Networks (GCNs) can capture non-Euclidean spatialdependence between different brain regions, and the graph pooling operator inGCNs is key to enhancing the representation learning capability and acquiringabnormal brain maps. However, the majority of existing research designs graphpooling operators only from the perspective of nodes while disregarding theoriginal edge features, in a way that not only confines graph poolingapplication scenarios, but also diminishes its ability to capture criticalsubstructures. In this study, a clustering graph pooling method that firstsupports multidimensional edge features, called Edge-aware hard clusteringgraph pooling (EHCPool), is developed. EHCPool proposes the first'Edge-to-node' score evaluation criterion based on edge features to assess nodefeature significance. To more effectively capture the critical subgraphs, anovel Iteration n-top strategy is further designed to adaptively learn sparsehard clustering assignments for graphs. Subsequently, an innovative N-EAggregation strategy is presented to aggregate node and edge featureinformation in each independent subgraph. The proposed model was evaluated onmulti-site brain imaging public datasets and yielded state-of-the-artperformance. We believe this method is the first deep learning tool with thepotential to probe different types of abnormal functional brain networks fromdata-driven perspective.</description><author>Cheng Zhu, Jiayi Zhu, Lijuan Zhang, Xi Wu, Shuqi Yang, Ping Liang, Honghan Chen, Ying Tan</author><pubDate>Thu, 24 Aug 2023 14:05:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11909v2</guid></item></channel></rss>