<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 01 Mar 2024 06:00:40 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</title><link>http://arxiv.org/abs/2402.19481v1</link><description>Diffusion models have achieved great success in synthesizing high-qualityimages. However, generating high-resolution images with diffusion models isstill challenging due to the enormous computational costs, resulting in aprohibitive latency for interactive applications. In this paper, we proposeDistriFusion to tackle this problem by leveraging parallelism across multipleGPUs. Our method splits the model input into multiple patches and assigns eachpatch to a GPU. However, na\"{\i}vely implementing such an algorithm breaks theinteraction between patches and loses fidelity, while incorporating such aninteraction will incur tremendous communication overhead. To overcome thisdilemma, we observe the high similarity between the input from adjacentdiffusion steps and propose displaced patch parallelism, which takes advantageof the sequential nature of the diffusion process by reusing the pre-computedfeature maps from the previous timestep to provide context for the currentstep. Therefore, our method supports asynchronous communication, which can bepipelined by computation. Extensive experiments show that our method can beapplied to recent Stable Diffusion XL with no quality degradation and achieveup to a 6.1$\times$ speedup on eight NVIDIA A100s compared to one. Our code ispublicly available at https://github.com/mit-han-lab/distrifuser.</description><author>Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, Song Han</author><pubDate>Thu, 29 Feb 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19481v1</guid></item><item><title>Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</title><link>http://arxiv.org/abs/2402.19479v1</link><description>The quality of the data and annotation upper-bounds the quality of adownstream model. While there exist large text corpora and image-text pairs,high-quality video-text data is much harder to collect. First of all, manuallabeling is more time-consuming, as it requires an annotator to watch an entirevideo. Second, videos have a temporal dimension, consisting of several scenesstacked together, and showing multiple actions. Accordingly, to establish avideo dataset with high-quality captions, we propose an automatic approachleveraging multimodal inputs, such as textual video description, subtitles, andindividual video frames. Specifically, we curate 3.8M high-resolution videosfrom the publicly available HD-VILA-100M dataset. We then split them intosemantically consistent video clips, and apply multiple cross-modality teachermodels to obtain captions for each video. Next, we finetune a retrieval modelon a small subset where the best caption of each video is manually selected andthen employ the model in the whole dataset to select the best caption as theannotation. In this way, we get 70M videos paired with high-quality textcaptions. We dub the dataset as Panda-70M. We show the value of the proposeddataset on three downstream tasks: video captioning, video and text retrieval,and text-driven video generation. The models trained on the proposed data scoresubstantially better on the majority of metrics across all the tasks.</description><author>Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, Sergey Tulyakov</author><pubDate>Thu, 29 Feb 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19479v1</guid></item><item><title>Learning a Generalized Physical Face Model From Data</title><link>http://arxiv.org/abs/2402.19477v1</link><description>Physically-based simulation is a powerful approach for 3D facial animation asthe resulting deformations are governed by physical constraints, allowing toeasily resolve self-collisions, respond to external forces and performrealistic anatomy edits. Today's methods are data-driven, where the actuationsfor finite elements are inferred from captured skin geometry. Unfortunately,these approaches have not been widely adopted due to the complexity ofinitializing the material space and learning the deformation model for eachcharacter separately, which often requires a skilled artist followed by lengthynetwork training. In this work, we aim to make physics-based facial animationmore accessible by proposing a generalized physical face model that we learnfrom a large 3D face dataset in a simulation-free manner. Once trained, ourmodel can be quickly fit to any unseen identity and produce a ready-to-animatephysical face model automatically. Fitting is as easy as providing a single 3Dface scan, or even a single face image. After fitting, we offer intuitiveanimation controls, as well as the ability to retarget animations acrosscharacters. All the while, the resulting animations allow for physical effectslike collision avoidance, gravity, paralysis, bone reshaping and more.</description><author>Lingchen Yang, Gaspard Zoss, Prashanth Chandran, Markus Gross, Barbara Solenthaler, Eftychios Sifakis, Derek Bradley</author><pubDate>Thu, 29 Feb 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19477v1</guid></item><item><title>The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?</title><link>http://arxiv.org/abs/2402.19475v1</link><description>While language models are increasingly more proficient at code generation,they still frequently generate incorrect programs. Many of these programs areobviously wrong, but others are more subtle and pass weaker correctness checkssuch as being able to compile. In this work, we focus on these counterfeitsamples: programs sampled from a language model that 1) have a high enoughlog-probability to be generated at a moderate temperature and 2) pass weakcorrectness checks. Overall, we discover that most models have a very shallowunderstanding of counterfeits through three clear failure modes. First, modelsmistakenly classify them as correct. Second, models are worse at reasoningabout the execution behaviour of counterfeits and often predict their executionresults as if they were correct. Third, when asking models to fix counterfeits,the likelihood of a model successfully repairing a counterfeit is often evenlower than that of sampling a correct program from scratch. Counterfeits alsohave very unexpected properties: first, counterfeit programs for problems thatare easier for a model to solve are not necessarily easier to detect and onlyslightly easier to execute and repair. Second, counterfeits from a given modelare just as confusing to the model itself as they are to other models. Finally,both strong and weak models are able to generate counterfeit samples thatequally challenge all models. In light of our findings, we recommend that careand caution be taken when relying on models to understand their own samples,especially when no external feedback is incorporated.</description><author>Alex Gu, Wen-Ding Li, Naman Jain, Theo X. Olausson, Celine Lee, Koushik Sen, Armando Solar-Lezama</author><pubDate>Thu, 29 Feb 2024 18:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19475v1</guid></item><item><title>The All-Seeing Project V2: Towards General Relation Comprehension of the Open World</title><link>http://arxiv.org/abs/2402.19474v1</link><description>We present the All-Seeing Project V2: a new model and dataset designed forunderstanding object relations in images. Specifically, we propose theAll-Seeing Model V2 (ASMv2) that integrates the formulation of text generation,object localization, and relation comprehension into a relation conversation(ReC) task. Leveraging this unified task, our model excels not only inperceiving and recognizing all objects within the image but also in graspingthe intricate relation graph between them, diminishing the relationhallucination often encountered by Multi-modal Large Language Models (MLLMs).To facilitate training and evaluation of MLLMs in relation understanding, wecreated the first high-quality ReC dataset ({AS-V2) which is aligned with theformat of standard instruction tuning data. In addition, we design a newbenchmark, termed Circular-based Relation Probing Evaluation (CRPE) forcomprehensively evaluating the relation comprehension capabilities of MLLMs.Notably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-awarebenchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope thatour work can inspire more future research and contribute to the evolutiontowards artificial general intelligence. Our project is released athttps://github.com/OpenGVLab/all-seeing.</description><author>Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, Yu Qiao, Jifeng Dai</author><pubDate>Thu, 29 Feb 2024 18:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19474v1</guid></item><item><title>Retrieval-Augmented Generation for AI-Generated Content: A Survey</title><link>http://arxiv.org/abs/2402.19473v1</link><description>The development of Artificial Intelligence Generated Content (AIGC) has beenfacilitated by advancements in model algorithms, scalable foundation modelarchitectures, and the availability of ample high-quality datasets. While AIGChas achieved remarkable performance, it still faces challenges, such as thedifficulty of maintaining up-to-date and long-tail knowledge, the risk of dataleakage, and the high costs associated with training and inference.Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm toaddress such challenges. In particular, RAG introduces the informationretrieval process, which enhances AIGC results by retrieving relevant objectsfrom available data stores, leading to greater accuracy and robustness. In thispaper, we comprehensively review existing efforts that integrate RAG techniqueinto AIGC scenarios. We first classify RAG foundations according to how theretriever augments the generator. We distill the fundamental abstractions ofthe augmentation methodologies for various retrievers and generators. Thisunified perspective encompasses all RAG scenarios, illuminating advancementsand pivotal technologies that help with potential future progress. We alsosummarize additional enhancements methods for RAG, facilitating effectiveengineering and implementation of RAG systems. Then from another view, wesurvey on practical applications of RAG across different modalities and tasks,offering valuable references for researchers and practitioners. Furthermore, weintroduce the benchmarks for RAG, discuss the limitations of current RAGsystems, and suggest potential directions for future research. Project:https://github.com/hymie122/RAG-Survey</description><author>Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Bin Cui</author><pubDate>Thu, 29 Feb 2024 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19473v1</guid></item><item><title>Real-time Traffic Object Detection for Autonomous Driving</title><link>http://arxiv.org/abs/2402.00128v2</link><description>With recent advances in computer vision, it appears that autonomous drivingwill be part of modern society sooner rather than later. However, there arestill a significant number of concerns to address. Although modern computervision techniques demonstrate superior performance, they tend to prioritizeaccuracy over efficiency, which is a crucial aspect of real-time applications.Large object detection models typically require higher computational power,which is achieved by using more sophisticated onboard hardware. For autonomousdriving, these requirements translate to increased fuel costs and, ultimately,a reduction in mileage. Further, despite their computational demands, theexisting object detectors are far from being real-time. In this research, weassess the robustness of our previously proposed, highly efficient pedestriandetector LSFM on well-established autonomous driving benchmarks, includingdiverse weather conditions and nighttime scenes. Moreover, we extend our LSFMmodel for general object detection to achieve real-time object detection intraffic scenes. We evaluate its performance, low latency, and generalizabilityon traffic object detection datasets. Furthermore, we discuss the inadequacy ofthe current key performance indicator employed by object detection systems inthe context of autonomous driving and propose a more suitable alternative thatincorporates real-time requirements.</description><author>Abdul Hannan Khan, Syed Tahseen Raza Rizvi, Andreas Dengel</author><pubDate>Thu, 29 Feb 2024 18:58:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00128v2</guid></item><item><title>Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress</title><link>http://arxiv.org/abs/2402.19472v1</link><description>Standardized benchmarks drive progress in machine learning. However, withrepeated testing, the risk of overfitting grows as algorithms over-exploitbenchmark idiosyncrasies. In our work, we seek to mitigate this challenge bycompiling ever-expanding large-scale benchmarks called Lifelong Benchmarks. Asexemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet,containing (for now) 1.69M and 1.98M test samples, respectively. While reducingoverfitting, lifelong benchmarks introduce a key challenge: the high cost ofevaluating a growing number of models across an ever-expanding sample set. Toaddress this challenge, we also introduce an efficient evaluation framework:Sort \&amp; Search (S&amp;S), which reuses previously evaluated models by leveragingdynamic programming algorithms to selectively rank and sub-select test samples,enabling cost-effective lifelong benchmarking. Extensive empirical evaluationsacross 31,000 models demonstrate that S&amp;S achieves highly-efficient approximateaccuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours(1000x reduction) on a single A100 GPU, with low approximation error. As such,lifelong benchmarks offer a robust, practical solution to the "benchmarkexhaustion" problem.</description><author>Ameya Prabhu, Vishaal Udandarao, Philip Torr, Matthias Bethge, Adel Bibi, Samuel Albanie</author><pubDate>Thu, 29 Feb 2024 18:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19472v1</guid></item><item><title>Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling</title><link>http://arxiv.org/abs/2402.19471v1</link><description>Questions combine our mastery of language with our remarkable facility forreasoning about uncertainty. How do people navigate vast hypothesis spaces topose informative questions given limited cognitive resources? We study thesetradeoffs in a classic grounded question-asking task based on the board gameBattleship. Our language-informed program sampling (LIPS) model uses largelanguage models (LLMs) to generate natural language questions, translate theminto symbolic programs, and evaluate their expected information gain. We findthat with a surprisingly modest resource budget, this simple Monte Carlooptimization strategy yields informative questions that mirror humanperformance across varied Battleship board scenarios. In contrast, LLM-onlybaselines struggle to ground questions in the board state; notably, GPT-4Vprovides no improvement over non-visual baselines. Our results illustrate howBayesian models of question-asking can leverage the statistics of language tocapture human priors, while highlighting some shortcomings of pure LLMs asgrounded reasoners.</description><author>Gabriel Grand, Valerio Pepe, Jacob Andreas, Joshua B. Tenenbaum</author><pubDate>Thu, 29 Feb 2024 18:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19471v1</guid></item><item><title>Towards Generalizable Tumor Synthesis</title><link>http://arxiv.org/abs/2402.19470v1</link><description>Tumor synthesis enables the creation of artificial tumors in medical images,facilitating the training of AI models for tumor detection and segmentation.However, success in tumor synthesis hinges on creating visually realistictumors that are generalizable across multiple organs and, furthermore, theresulting AI models being capable of detecting real tumors in images sourcedfrom different domains (e.g., hospitals). This paper made a progressive stridetoward generalizable tumor synthesis by leveraging a critical observation:early-stage tumors (&lt; 2cm) tend to have similar imaging characteristics incomputed tomography (CT), whether they originate in the liver, pancreas, orkidneys. We have ascertained that generative AI models, e.g., Diffusion Models,can create realistic tumors generalized to a range of organs even when trainedon a limited number of tumor examples from only one organ. Moreover, we haveshown that AI models trained on these synthetic tumors can be generalized todetect and segment real tumors from CT volumes, encompassing a broad spectrumof patient demographics, imaging protocols, and healthcare facilities.</description><author>Qi Chen, Xiaoxi Chen, Haorui Song, Zhiwei Xiong, Alan Yuille, Chen Wei, Zongwei Zhou</author><pubDate>Thu, 29 Feb 2024 18:57:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19470v1</guid></item><item><title>Humanoid Locomotion as Next Token Prediction</title><link>http://arxiv.org/abs/2402.19469v1</link><description>We cast real-world humanoid control as a next token prediction problem, akinto predicting the next word in language. Our model is a causal transformertrained via autoregressive prediction of sensorimotor trajectories. To accountfor the multi-modal nature of the data, we perform prediction in amodality-aligned way, and for each input token predict the next token from thesame modality. This general formulation enables us to leverage data withmissing modalities, like video trajectories without actions. We train our modelon a collection of simulated trajectories coming from prior neural networkpolicies, model-based controllers, motion capture data, and YouTube videos ofhumans. We show that our model enables a full-sized humanoid to walk in SanFrancisco zero-shot. Our model can transfer to the real world even when trainedon only 27 hours of walking data, and can generalize to commands not seenduring training like walking backward. These findings suggest a promising pathtoward learning challenging real-world control tasks by generative modeling ofsensorimotor trajectories.</description><author>Ilija Radosavovic, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran, Sarthak Kamat, Trevor Darrell, Koushil Sreenath, Jitendra Malik</author><pubDate>Thu, 29 Feb 2024 18:57:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19469v1</guid></item><item><title>TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning</title><link>http://arxiv.org/abs/2402.19467v1</link><description>It is challenging to perform question-answering over complex, multimodalcontent such as television clips. This is in part because currentvideo-language models rely on single-modality reasoning, have loweredperformance on long inputs, and lack interpetability. We propose TV-TREES, thefirst multimodal entailment tree generator. TV-TREES serves as an approach tovideo understanding that promotes interpretable joint-modality reasoning byproducing trees of entailment relationships between simple premises directlyentailed by the videos and higher-level conclusions. We then introduce the taskof multimodal entailment tree generation to evaluate the reasoning quality ofsuch methods. Our method's experimental results on the challenging TVQA datasetdemonstrate intepretable, state-of-the-art zero-shot performance on full videoclips, illustrating a best of both worlds contrast to black-box methods.</description><author>Kate Sanders, Nathaniel Weir, Benjamin Van Durme</author><pubDate>Thu, 29 Feb 2024 18:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19467v1</guid></item><item><title>Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models</title><link>http://arxiv.org/abs/2402.19465v1</link><description>Ensuring the trustworthiness of large language models (LLMs) is crucial. Moststudies concentrate on fully pre-trained LLMs to better understand and improveLLMs' trustworthiness. In this paper, to reveal the untapped potential ofpre-training, we pioneer the exploration of LLMs' trustworthiness during thisperiod, focusing on five key dimensions: reliability, privacy, toxicity,fairness, and robustness. To begin with, we apply linear probing to LLMs. Thehigh probing accuracy suggests that \textit{LLMs in early pre-training canalready distinguish concepts in each trustworthiness dimension}. Therefore, tofurther uncover the hidden possibilities of pre-training, we extract steeringvectors from a LLM's pre-training checkpoints to enhance the LLM'strustworthiness. Finally, inspired by~\citet{choi2023understanding} that mutualinformation estimation is bounded by linear probing accuracy, we also probeLLMs with mutual information to investigate the dynamics of trustworthinessduring pre-training. We are the first to observe a similar two-phasephenomenon: fitting and compression~\citep{shwartz2017opening}. This researchprovides an initial exploration of trustworthiness modeling during LLMpre-training, seeking to unveil new insights and spur further developments inthe field. We will make our code publicly accessible at\url{https://github.com/ChnQ/TracingLLM}.</description><author>Chen Qian, Jie Zhang, Wei Yao, Dongrui Liu, Zhenfei Yin, Yu Qiao, Yong Liu, Jing Shao</author><pubDate>Thu, 29 Feb 2024 18:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19465v1</guid></item><item><title>Curiosity-driven Red-teaming for Large Language Models</title><link>http://arxiv.org/abs/2402.19464v1</link><description>Large language models (LLMs) hold great potential for many natural languageapplications but risk generating incorrect or toxic content. To probe when anLLM generates unwanted content, the current paradigm is to recruit a\textit{red team} of human testers to design input prompts (i.e., test cases)that elicit undesirable responses from LLMs. However, relying solely on humantesters is expensive and time-consuming. Recent works automate red teaming bytraining a separate red team LLM with reinforcement learning (RL) to generatetest cases that maximize the chance of eliciting undesirable responses from thetarget LLM. However, current RL methods are only able to generate a smallnumber of effective test cases resulting in a low coverage of the span ofprompts that elicit undesirable responses from the target LLM. To overcome thislimitation, we draw a connection between the problem of increasing the coverageof generated test cases and the well-studied approach of curiosity-drivenexploration that optimizes for novelty. Our method of curiosity-driven redteaming (CRT) achieves greater coverage of test cases while mantaining orincreasing their effectiveness compared to existing methods. Our method, CRTsuccessfully provokes toxic responses from LLaMA2 model that has been heavilyfine-tuned using human preferences to avoid toxic outputs. Code is available at\url{https://github.com/Improbable-AI/curiosity_redteam}</description><author>Zhang-Wei Hong, Idan Shenfeld, Tsun-Hsuan Wang, Yung-Sung Chuang, Aldo Pareja, James Glass, Akash Srivastava, Pulkit Agrawal</author><pubDate>Thu, 29 Feb 2024 18:55:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19464v1</guid></item><item><title>SeMoLi: What Moves Together Belongs Together</title><link>http://arxiv.org/abs/2402.19463v1</link><description>We tackle semi-supervised object detection based on motion cues. Recentresults suggest that heuristic-based clustering methods in conjunction withobject trackers can be used to pseudo-label instances of moving objects and usethese as supervisory signals to train 3D object detectors in Lidar data withoutmanual supervision. We re-think this approach and suggest that both, objectdetection, as well as motion-inspired pseudo-labeling, can be tackled in adata-driven manner. We leverage recent advances in scene flow estimation toobtain point trajectories from which we extract long-term, class-agnosticmotion patterns. Revisiting correlation clustering in the context of messagepassing networks, we learn to group those motion patterns to cluster points toobject instances. By estimating the full extent of the objects, we obtainper-scan 3D bounding boxes that we use to supervise a Lidar object detectionnetwork. Our method not only outperforms prior heuristic-based approaches (57.5AP, +14 improvement over prior work), more importantly, we show we canpseudo-label and train object detectors across datasets.</description><author>Jenny Seidenschwarz, Aljoša Ošep, Francesco Ferroni, Simon Lucey, Laura Leal-Taixé</author><pubDate>Thu, 29 Feb 2024 18:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19463v1</guid></item><item><title>Accelerating materials discovery for polymer solar cells: Data-driven insights enabled by natural language processing</title><link>http://arxiv.org/abs/2402.19462v1</link><description>We present a natural language processing pipeline that was used to extractpolymer solar cell property data from the literature and simulate variousactive learning strategies. While data-driven methods have been wellestablished to discover novel materials faster than Edisonian trial-and-errorapproaches, their benefits have not been quantified. Our approach demonstratesa potential reduction in discovery time by approximately 75 %, equivalent to a15 year acceleration in material innovation. Our pipeline enables us to extractdata from more than 3300 papers which is ~5 times larger than similar data setsreported by others. We also trained machine learning models to predict thepower conversion efficiency and used our model to identify promisingdonor-acceptor combinations that are as yet unreported. We thus demonstrate aworkflow that goes from published literature to extracted material propertydata which in turn is used to obtain data-driven insights. Our insights includeactive learning strategies that can simultaneously optimize the material systemand train strong predictive models of material properties. This work provides avaluable framework for research in material science.</description><author>Pranav Shetty, Aishat Adeboye, Sonakshi Gupta, Chao Zhang, Rampi Ramprasad</author><pubDate>Thu, 29 Feb 2024 18:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19462v1</guid></item><item><title>Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks</title><link>http://arxiv.org/abs/2402.19460v1</link><description>Uncertainty quantification, once a singular task, has evolved into a spectrumof tasks, including abstained prediction, out-of-distribution detection, andaleatoric uncertainty quantification. The latest goal is disentanglement: theconstruction of multiple estimators that are each tailored to one and only onetask. Hence, there is a plethora of recent advances with different intentions -that often entirely deviate from practical behavior. This paper conducts acomprehensive evaluation of numerous uncertainty estimators across diversetasks on ImageNet. We find that, despite promising theoretical endeavors,disentanglement is not yet achieved in practice. Additionally, we reveal whichuncertainty estimators excel at which specific tasks, providing insights forpractitioners and guiding future research toward task-centric and disentangleduncertainty estimation methods. Our code is available athttps://github.com/bmucsanyi/bud.</description><author>Bálint Mucsányi, Michael Kirchhof, Seong Joon Oh</author><pubDate>Thu, 29 Feb 2024 18:52:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19460v1</guid></item><item><title>$\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation</title><link>http://arxiv.org/abs/2402.19457v1</link><description>Assessing the quality of summarizers poses significant challenges. Inresponse, we propose a novel task-oriented evaluation approach that assessessummarizers based on their capacity to produce summaries that are useful fordownstream tasks, while preserving task outcomes. We theoretically establish adirect relationship between the resulting error probability of these tasks andthe mutual information between source texts and generated summaries. Weintroduce $\texttt{COSMIC}$ as a practical implementation of this metric,demonstrating its strong correlation with human judgment-based metrics and itseffectiveness in predicting downstream task performance. Comparative analysesagainst established metrics like $\texttt{BERTScore}$ and $\texttt{ROUGE}$highlight the competitive performance of $\texttt{COSMIC}$.</description><author>Maxime Darrin, Philippe Formont, Jackie Chi Kit Cheung, Pablo Piantanida</author><pubDate>Thu, 29 Feb 2024 18:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19457v1</guid></item><item><title>Listening to the Noise: Blind Denoising with Gibbs Diffusion</title><link>http://arxiv.org/abs/2402.19455v1</link><description>In recent years, denoising problems have become intertwined with thedevelopment of deep generative models. In particular, diffusion models aretrained like denoisers, and the distribution they model coincide with denoisingpriors in the Bayesian picture. However, denoising through diffusion-basedposterior sampling requires the noise level and covariance to be known,preventing blind denoising. We overcome this limitation by introducing GibbsDiffusion (GDiff), a general methodology addressing posterior sampling of boththe signal and the noise parameters. Assuming arbitrary parametric Gaussiannoise, we develop a Gibbs algorithm that alternates sampling steps from aconditional diffusion model trained to map the signal prior to the family ofnoise distributions, and a Monte Carlo sampler to infer the noise parameters.Our theoretical analysis highlights potential pitfalls, guides diagnosticusage, and quantifies errors in the Gibbs stationary distribution caused by thediffusion model. We showcase our method for 1) blind denoising of naturalimages involving colored noises with unknown amplitude and spectral index, and2) a cosmology problem, namely the analysis of cosmic microwave backgrounddata, where Bayesian inference of "noise" parameters means constraining modelsof the evolution of the Universe.</description><author>David Heurtel-Depeiges, Charles C. Margossian, Ruben Ohana, Bruno Régaldo-Saint Blancard</author><pubDate>Thu, 29 Feb 2024 18:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19455v1</guid></item><item><title>Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding</title><link>http://arxiv.org/abs/2402.12374v2</link><description>As the usage of large language models (LLMs) grows, performing efficientinference with these models becomes increasingly important. While speculativedecoding has recently emerged as a promising direction for speeding upinference, existing methods are limited in their ability to scale to largerspeculation budgets, and adapt to different hyperparameters and hardware. Thispaper introduces Sequoia, a scalable, robust, and hardware-aware algorithm forspeculative decoding. To attain better scalability, Sequoia introduces adynamic programming algorithm to find the optimal tree structure for thespeculated tokens. To achieve robust speculative performance, Sequoia uses anovel sampling and verification method that outperforms prior work acrossdifferent decoding temperatures. Finally, Sequoia introduces a hardware-awaretree optimizer that maximizes speculative performance by automaticallyselecting the token tree size and depth for a given hardware platform.Evaluation shows that Sequoia improves the decoding speed of Llama2-7B,Llama2-13B, and Vicuna-33B on an A100 by up to $4.04\times$, $3.73\times$, and$2.27\times$. For offloading setting on L40, Sequoia achieves as low as 0.56s/token for exact Llama2-70B inference latency, which is $9.96\times$ on ouroptimized offloading system (5.6 s/token), $9.7\times$ thanDeepSpeed-Zero-Inference, $19.5\times$ than Huggingface Accelerate.</description><author>Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, Beidi Chen</author><pubDate>Thu, 29 Feb 2024 18:48:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12374v2</guid></item><item><title>Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap</title><link>http://arxiv.org/abs/2402.19450v1</link><description>We propose a framework for robust evaluation of reasoning capabilities oflanguage models, using functional variants of benchmarks. Models that solve areasoning test should exhibit no difference in performance over the staticversion of a problem compared to a snapshot of the functional variant. We haverewritten the relevant fragment of the MATH benchmark into its functionalvariant MATH(), with functionalization of other benchmarks to follow. Whenevaluating current state-of-the-art models over snapshots of MATH(), we find areasoning gap -- the percentage difference between the static and functionalaccuracies. We find reasoning gaps from 58.35% to 80.31% among thestate-of-the-art closed and open weights models that perform well on staticbenchmarks, with the caveat that the gaps are likely to be smaller with moresophisticated prompting strategies. Here we show that models which anecdotallyhave good reasoning performance over real-world tasks, have quantifiable lowergaps, motivating the open problem of building "gap 0" models. Code forevaluation and new evaluation datasets, three MATH() snapshots, are publiclyavailable at https://github.com/consequentai/fneval/.</description><author>Saurabh Srivastava, Annarose M B, Anto P V, Shashank Menon, Ajay Sukumar, Adwaith Samod T, Alan Philipose, Stevin Prince, Sooraj Thomas</author><pubDate>Thu, 29 Feb 2024 18:48:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19450v1</guid></item><item><title>Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models</title><link>http://arxiv.org/abs/2402.19449v1</link><description>Adam has been shown to outperform gradient descent in optimizing largelanguage transformers empirically, and by a larger margin than on other tasks,but it is unclear why this happens. We show that the heavy-tailed classimbalance found in language modeling tasks leads to difficulties in theoptimization dynamics. When training with gradient descent, the loss associatedwith infrequent words decreases slower than the loss associated with frequentones. As most samples come from relatively infrequent words, the average lossdecreases slowly with gradient descent. On the other hand, Adam and sign-basedmethods do not suffer from this problem and improve predictions on all classes.To establish that this behavior is indeed caused by class imbalance, we showempirically that it persist through different architectures and data types, onlanguage transformers, vision CNNs, and linear models. We further study thisphenomenon on a linear classification with cross-entropy loss, showing thatheavy-tailed class imbalance leads to ill-conditioning, and that thenormalization used by Adam can counteract it.</description><author>Frederik Kunstner, Robin Yadav, Alan Milligan, Mark Schmidt, Alberto Bietti</author><pubDate>Thu, 29 Feb 2024 18:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19449v1</guid></item><item><title>Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?</title><link>http://arxiv.org/abs/2310.08540v4</link><description>The emergence of In-Context Learning (ICL) in LLMs remains a significantphenomenon with little understanding. To explain ICL, recent studies try totheoretically connect it to Gradient Descent (GD). We ask, does this connectionhold up in actual pre-trained models? We highlight the limiting assumptions in prior works that make their contextconsiderably different from the practical context in which language models aretrained. For example, the theoretical hand-constructed weights used in thesestudies have properties that don't match those of real LLMs. Furthermore, theirexperimental verification uses ICL objective (training models explicitly forICL), which differs from the emergent ICL in the wild. We also look for evidence in real models. We observe that ICL and GD havedifferent sensitivity to the order in which they observe demonstrations.Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting.We conduct comprehensive empirical analyses on language models pre-trained onnatural data (LLaMa-7B). Our comparisons of three performance metrics highlightthe inconsistent behavior of ICL and GD as a function of various factors suchas datasets, models, and the number of demonstrations. We observe that ICL andGD modify the output distribution of language models differently. These resultsindicate that the equivalence between ICL and GD remains an open hypothesis andcalls for further studies.</description><author>Lingfeng Shen, Aayush Mishra, Daniel Khashabi</author><pubDate>Thu, 29 Feb 2024 18:47:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08540v4</guid></item><item><title>ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL</title><link>http://arxiv.org/abs/2402.19446v1</link><description>A broad use case of large language models (LLMs) is in goal-directeddecision-making tasks (or "agent" tasks), where an LLM needs to not justgenerate completions for a given prompt, but rather make intelligent decisionsover a multi-turn interaction to accomplish a task (e.g., when interacting withthe web, using tools, or providing customer support). Reinforcement learning(RL) provides a general paradigm to address such agent tasks, but current RLmethods for LLMs largely focus on optimizing single-turn rewards. Byconstruction, most single-turn RL methods cannot endow LLMs with the ability tointelligently seek information over multiple turns, perform credit assignment,or reason about their past actions -- all of which are critical in agent tasks.This raises the question: how can we design effective and efficient multi-turnRL algorithms for LLMs? In this paper, we develop a framework for buildingmulti-turn RL algorithms for fine-tuning LLMs, that preserves the flexibilityof existing single-turn RL methods for LLMs (e.g., proximal policyoptimization), while accommodating multiple turns, long horizons, and delayedrewards effectively. To do this, our framework adopts a hierarchical RLapproach and runs two RL algorithms in parallel: a high-level off-policyvalue-based RL algorithm to aggregate reward over utterances, and a low-levelRL algorithm that utilizes this high-level value function to train a tokenpolicy within each utterance or turn. Our hierarchical framework, Actor-CriticFramework with a Hierarchical Structure (ArCHer), can also give rise to otherRL methods. Empirically, we find that ArCHer significantly improves efficiencyand performance on agent tasks, attaining a sample efficiency of about 100xover existing methods, while also improving with larger model capacity (uptothe 7 billion scale that we tested on).</description><author>Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, Aviral Kumar</author><pubDate>Thu, 29 Feb 2024 18:45:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19446v1</guid></item><item><title>Probing the Information Encoded in Neural-based Acoustic Models of Automatic Speech Recognition Systems</title><link>http://arxiv.org/abs/2402.19443v1</link><description>Deep learning architectures have made significant progress in terms ofperformance in many research areas. The automatic speech recognition (ASR)field has thus benefited from these scientific and technological advances,particularly for acoustic modeling, now integrating deep neural networkarchitectures. However, these performance gains have translated into increasedcomplexity regarding the information learned and conveyed through theseblack-box architectures. Following many researches in neural networksinterpretability, we propose in this article a protocol that aims to determinewhich and where information is located in an ASR acoustic model (AM). To do so,we propose to evaluate AM performance on a determined set of tasks usingintermediate representations (here, at different layer levels). Regarding theperformance variation and targeted tasks, we can emit hypothesis about whichinformation is enhanced or perturbed at different architecture steps.Experiments are performed on both speaker verification, acoustic environmentclassification, gender classification, tempo-distortion detection systems andspeech sentiment/emotion identification. Analysis showed that neural-based AMshold heterogeneous information that seems surprisingly uncorrelated withphoneme recognition, such as emotion, sentiment or speaker identity. Thelow-level hidden layers globally appears useful for the structuring ofinformation while the upper ones would tend to delete useless information forphoneme recognition.</description><author>Quentin Raymondaud, Mickael Rouvier, Richard Dufour</author><pubDate>Thu, 29 Feb 2024 18:43:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19443v1</guid></item><item><title>Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality</title><link>http://arxiv.org/abs/2402.19442v1</link><description>We study the dynamics of gradient flow for training a multi-head softmaxattention model for in-context learning of multi-task linear regression. Weestablish the global convergence of gradient flow under suitable choices ofinitialization. In addition, we prove that an interesting "task allocation"phenomenon emerges during the gradient flow dynamics, where each attention headfocuses on solving a single task of the multi-task model. Specifically, weprove that the gradient flow dynamics can be split into three phases -- awarm-up phase where the loss decreases rather slowly and the attention headsgradually build up their inclination towards individual tasks, an emergencephase where each head selects a single task and the loss rapidly decreases, anda convergence phase where the attention parameters converge to a limit.Furthermore, we prove the optimality of gradient flow in the sense that thelimiting model learned by gradient flow is on par with the best possiblemulti-head softmax attention model up to a constant factor. Our analysis alsodelineates a strict separation in terms of the prediction accuracy of ICLbetween single-head and multi-head attention models. The key technique for ourconvergence analysis is to map the gradient flow dynamics in the parameterspace to a set of ordinary differential equations in the spectral domain, wherethe relative magnitudes of the semi-singular values of the attention weightsdetermines task allocation. To our best knowledge, our work provides the firstconvergence result for the multi-head softmax attention model.</description><author>Siyu Chen, Heejune Sheen, Tianhao Wang, Zhuoran Yang</author><pubDate>Thu, 29 Feb 2024 18:43:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19442v1</guid></item><item><title>Differentially Private Worst-group Risk Minimization</title><link>http://arxiv.org/abs/2402.19437v1</link><description>We initiate a systematic study of worst-group risk minimization under$(\epsilon, \delta)$-differential privacy (DP). The goal is to privately find amodel that approximately minimizes the maximal risk across $p$ sub-populations(groups) with different distributions, where each group distribution isaccessed via a sample oracle. We first present a new algorithm that achievesexcess worst-group population risk of $\tilde{O}(\frac{p\sqrt{d}}{K\epsilon} +\sqrt{\frac{p}{K}})$, where $K$ is the total number of samples drawn from allgroups and $d$ is the problem dimension. Our rate is nearly optimal when eachdistribution is observed via a fixed-size dataset of size $K/p$. Our result isbased on a new stability-based analysis for the generalization error. Inparticular, we show that $\Delta$-uniform argument stability implies$\tilde{O}(\Delta + \frac{1}{\sqrt{n}})$ generalization error w.r.t. theworst-group risk, where $n$ is the number of samples drawn from each sampleoracle. Next, we propose an algorithmic framework for worst-group populationrisk minimization using any DP online convex optimization algorithm as asubroutine. Hence, we give another excess risk bound of $\tilde{O}\left(\sqrt{\frac{d^{1/2}}{\epsilon K}} +\sqrt{\frac{p}{K\epsilon^2}} \right)$.Assuming the typical setting of $\epsilon=\Theta(1)$, this bound is morefavorable than our first bound in a certain range of $p$ as a function of $K$and $d$. Finally, we study differentially private worst-group empirical riskminimization in the offline setting, where each group distribution is observedby a fixed-size dataset. We present a new algorithm with nearly optimal excessrisk of $\tilde{O}(\frac{p\sqrt{d}}{K\epsilon})$.</description><author>Xinyu Zhou, Raef Bassily</author><pubDate>Thu, 29 Feb 2024 18:38:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19437v1</guid></item><item><title>Redefining Digital Health Interfaces with Large Language Models</title><link>http://arxiv.org/abs/2310.03560v3</link><description>Digital health tools have the potential to significantly improve the deliveryof healthcare services. However, their adoption remains comparatively limiteddue, in part, to challenges surrounding usability and trust. Large LanguageModels (LLMs) have emerged as general-purpose models with the ability toprocess complex information and produce human-quality text, presenting a wealthof potential applications in healthcare. Directly applying LLMs in clinicalsettings is not straightforward, however, with LLMs susceptible to providinginconsistent or nonsensical answers. We demonstrate how LLM-based systems canutilize external tools and provide a novel interface between clinicians anddigital technologies. This enhances the utility and practical impact of digitalhealthcare tools and AI models while addressing current issues with using LLMsin clinical settings such as hallucinations. We illustrate LLM-based interfaceswith the example of cardiovascular disease risk prediction. We develop a newprognostic tool using automated machine learning and demonstrate how LLMs canprovide a unique interface to both our model and existing risk scores,highlighting the benefit compared to traditional interfaces for digital tools.</description><author>Fergus Imrie, Paulius Rauba, Mihaela van der Schaar</author><pubDate>Thu, 29 Feb 2024 18:37:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03560v3</guid></item><item><title>Generalizability Under Sensor Failure: Tokenization + Transformers Enable More Robust Latent Spaces</title><link>http://arxiv.org/abs/2402.18546v2</link><description>A major goal in neuroscience is to discover neural data representations thatgeneralize. This goal is challenged by variability along recording sessions(e.g. environment), subjects (e.g. varying neural structures), and sensors(e.g. sensor noise), among others. Recent work has begun to addressgeneralization across sessions and subjects, but few study robustness to sensorfailure which is highly prevalent in neuroscience experiments. In order toaddress these generalizability dimensions we first collect our ownelectroencephalography dataset with numerous sessions, subjects, and sensors,then study two time series models: EEGNet (Lawhern et al., 2018) and TOTEM(Talukder et al., 2024). EEGNet is a widely used convolutional neural network,while TOTEM is a discrete time series tokenizer and transformer model. We findthat TOTEM outperforms or matches EEGNet across all generalizability cases.Finally through analysis of TOTEM's latent codebook we observe thattokenization enables generalization</description><author>Geeling Chau, Yujin An, Ahamed Raffey Iqbal, Soon-Jo Chung, Yisong Yue, Sabera Talukder</author><pubDate>Thu, 29 Feb 2024 18:35:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18546v2</guid></item><item><title>Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation</title><link>http://arxiv.org/abs/2309.13192v2</link><description>Fine-tuning is the most effective way of adapting pre-trained large languagemodels (LLMs) to downstream applications. With the fast growth of LLM-enabledAI applications and democratization of open-souced LLMs, fine-tuning has becomepossible for non-expert individuals, but intensively performed LLM fine-tuningworldwide could result in significantly high energy consumption and carbonfootprint, which may bring large environmental impact. Mitigating suchenvironmental impact towards Green AI directly correlates to reducing the FLOPsof fine-tuning, but existing techniques on efficient LLM fine-tuning can onlyachieve limited reduction of such FLOPs, due to their ignorance of thebackpropagation cost in fine-tuning. To address this limitation, in this paperwe present GreenTrainer, a new LLM fine-tuning technique that adaptivelyevaluates different tensors' backpropagation costs and contributions to thefine-tuned model accuracy, to minimize the fine-tuning cost by selecting themost appropriate set of tensors in training. Such selection in GreenTrainer ismade based on a given objective of FLOPs reduction, which can flexibly adapt tothe carbon footprint in energy supply and the need in Green AI. Experimentresults over multiple open-sourced LLM models and abstractive summarizationdatasets show that, compared to fine-tuning the whole LLM model, GreenTrainercan save up to 64% FLOPs in fine-tuning without any noticeable model accuracyloss. Compared to the existing fine-tuning techniques such as LoRa,GreenTrainer can achieve up to 4% improvement on model accuracy with on-parFLOPs reduction.</description><author>Kai Huang, Hanyun Yin, Heng Huang, Wei Gao</author><pubDate>Thu, 29 Feb 2024 18:27:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13192v2</guid></item><item><title>Compositional API Recommendation for Library-Oriented Code Generation</title><link>http://arxiv.org/abs/2402.19431v1</link><description>Large language models (LLMs) have achieved exceptional performance in codegeneration. However, the performance remains unsatisfactory in generatinglibrary-oriented code, especially for the libraries not present in the trainingdata of LLMs. Previous work utilizes API recommendation technology to help LLMsuse libraries: it retrieves APIs related to the user requirements, thenleverages them as context to prompt LLMs. However, developmental requirementscan be coarse-grained, requiring a combination of multiple fine-grained APIs.This granularity inconsistency makes API recommendation a challenging task. Toaddress this, we propose CAPIR (Compositional API Recommendation), which adoptsa "divide-and-conquer" strategy to recommend APIs for coarse-grainedrequirements. Specifically, CAPIR employs an LLM-based Decomposer to break downa coarse-grained task description into several detailed subtasks. Then, CAPIRapplies an embedding-based Retriever to identify relevant APIs corresponding toeach subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter outredundant APIs and provides the final recommendation. To facilitate theevaluation of API recommendation methods on coarse-grained requirements, wepresent two challenging benchmarks, RAPID (Recommend APIs based onDocumentation) and LOCG (Library-Oriented Code Generation). Experimentalresults on these benchmarks, demonstrate the effectiveness of CAPIR incomparison to existing baselines. Specifically, on RAPID's Torchdata-ARdataset, compared to the state-of-the-art API recommendation approach, CAPIRimproves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%. OnLOCG's Torchdata-Code dataset, compared to code generation without APIrecommendation, CAPIR improves pass@100 from 16.0% to 28.0%.</description><author>Zexiong Ma, Shengnan An, Bing Xie, Zeqi Lin</author><pubDate>Thu, 29 Feb 2024 18:27:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19431v1</guid></item><item><title>Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models</title><link>http://arxiv.org/abs/2402.19427v1</link><description>Recurrent neural networks (RNNs) have fast inference and scale efficiently onlong sequences, but they are difficult to train and hard to scale. We proposeHawk, an RNN with gated linear recurrences, and Griffin, a hybrid model thatmixes gated linear recurrences with local attention. Hawk exceeds the reportedperformance of Mamba on downstream tasks, while Griffin matches the performanceof Llama-2 despite being trained on over 6 times fewer tokens. We also showthat Griffin can extrapolate on sequences significantly longer than those seenduring training. Our models match the hardware efficiency of Transformersduring training, and during inference they have lower latency and significantlyhigher throughput. We scale Griffin up to 14B parameters, and explain how toshard our models for efficient distributed training.</description><author>Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, Caglar Gulcehre</author><pubDate>Thu, 29 Feb 2024 18:24:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19427v1</guid></item><item><title>Leveraging AI Predicted and Expert Revised Annotations in Interactive Segmentation: Continual Tuning or Full Training?</title><link>http://arxiv.org/abs/2402.19423v1</link><description>Interactive segmentation, an integration of AI algorithms and humanexpertise, premises to improve the accuracy and efficiency of curatinglarge-scale, detailed-annotated datasets in healthcare. Human experts revisethe annotations predicted by AI, and in turn, AI improves its predictions bylearning from these revised annotations. This interactive process continues toenhance the quality of annotations until no major revision is needed fromexperts. The key challenge is how to leverage AI predicted and expert revisedannotations to iteratively improve the AI. Two problems arise: (1) The risk ofcatastrophic forgetting--the AI tends to forget the previously learned classesif it is only retrained using the expert revised classes. (2) Computationalinefficiency when retraining the AI using both AI predicted and expert revisedannotations; moreover, given the dominant AI predicted annotations in thedataset, the contribution of newly revised annotations--often account for avery small fraction--to the AI training remains marginal. This paper proposesContinual Tuning to address the problems from two perspectives: network designand data reuse. Firstly, we design a shared network for all classes followed byclass-specific networks dedicated to individual classes. To mitigateforgetting, we freeze the shared network for previously learned classes andonly update the class-specific network for revised classes. Secondly, we reusea small fraction of data with previous annotations to avoid over-computing. Theselection of such data relies on the importance estimate of each data. Theimportance score is computed by combining the uncertainty and consistency of AIpredictions. Our experiments demonstrate that Continual Tuning achieves a speed16x greater than repeatedly training AI from scratch without compromising theperformance.</description><author>Tiezheng Zhang, Xiaoxi Chen, Chongyu Qu, Alan Yuille, Zongwei Zhou</author><pubDate>Thu, 29 Feb 2024 18:22:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19423v1</guid></item><item><title>PEM: Prototype-based Efficient MaskFormer for Image Segmentation</title><link>http://arxiv.org/abs/2402.19422v1</link><description>Recent transformer-based architectures have shown impressive results in thefield of image segmentation. Thanks to their flexibility, they obtainoutstanding performance in multiple segmentation tasks, such as semantic andpanoptic, under a single unified framework. To achieve such impressiveperformance, these architectures employ intensive operations and requiresubstantial computational resources, which are often not available, especiallyon edge devices. To fill this gap, we propose Prototype-based EfficientMaskFormer (PEM), an efficient transformer-based architecture that can operatein multiple segmentation tasks. PEM proposes a novel prototype-basedcross-attention which leverages the redundancy of visual features to restrictthe computation and improve the efficiency without harming the performance. Inaddition, PEM introduces an efficient multi-scale feature pyramid network,capable of extracting features that have high semantic content in an efficientway, thanks to the combination of deformable convolutions and context-basedself-modulation. We benchmark the proposed PEM architecture on two tasks,semantic and panoptic segmentation, evaluated on two different datasets,Cityscapes and ADE20K. PEM demonstrates outstanding performance on every taskand dataset, outperforming task-specific architectures while being comparableand even better than computationally-expensive baselines.</description><author>Niccolò Cavagnero, Gabriele Rosi, Claudia Ruttano, Francesca Pistilli, Marco Ciccone, Giuseppe Averta, Fabio Cermelli</author><pubDate>Thu, 29 Feb 2024 18:21:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19422v1</guid></item><item><title>Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines</title><link>http://arxiv.org/abs/2402.19421v1</link><description>In the domain of digital information dissemination, search engines act aspivotal conduits linking information seekers with providers. The advent ofchat-based search engines utilizing Large Language Models (LLMs) and RetrievalAugmented Generation (RAG), exemplified by Bing Chat, marks an evolutionaryleap in the search ecosystem. They demonstrate metacognitive abilities ininterpreting web information and crafting responses with human-likeunderstanding and creativity. Nonetheless, the intricate nature of LLMs renderstheir "cognitive" processes opaque, challenging even their designers'understanding. This research aims to dissect the mechanisms through which anLLM-powered chat-based search engine, specifically Bing Chat, selectsinformation sources for its responses. To this end, an extensive dataset hasbeen compiled through engagements with New Bing, documenting the websites itcites alongside those listed by the conventional search engine. Employingnatural language processing (NLP) techniques, the research reveals that BingChat exhibits a preference for content that is not only readable and formallystructured, but also demonstrates lower perplexity levels, indicating a uniqueinclination towards text that is predictable by the underlying LLM. Furtherenriching our analysis, we procure an additional dataset through interactionswith the GPT-4 based knowledge retrieval API, unveiling a congruent textpreference between the RAG API and Bing Chat. This consensus suggests thatthese text preferences intrinsically emerge from the underlying languagemodels, rather than being explicitly crafted by Bing Chat's developers.Moreover, our investigation documents a greater similarity among websites citedby RAG technologies compared to those ranked highest by conventional searchengines.</description><author>Lijia Ma, Xingchen Xu, Yong Tan</author><pubDate>Thu, 29 Feb 2024 18:20:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19421v1</guid></item><item><title>Inherent Diverse Redundant Safety Mechanisms for AI-based Software Elements in Automotive Applications</title><link>http://arxiv.org/abs/2402.08208v2</link><description>This paper explores the role and challenges of Artificial Intelligence (AI)algorithms, specifically AI-based software elements, in autonomous drivingsystems. These AI systems are fundamental in executing real-time criticalfunctions in complex and high-dimensional environments. They handle vital taskslike multi-modal perception, cognition, and decision-making tasks such asmotion planning, lane keeping, and emergency braking. A primary concern relatesto the ability (and necessity) of AI models to generalize beyond their initialtraining data. This generalization issue becomes evident in real-timescenarios, where models frequently encounter inputs not represented in theirtraining or validation data. In such cases, AI systems must still functioneffectively despite facing distributional or domain shifts. This paperinvestigates the risk associated with overconfident AI models insafety-critical applications like autonomous driving. To mitigate these risks,methods for training AI models that help maintain performance withoutoverconfidence are proposed. This involves implementing certainty reportingarchitectures and ensuring diverse training data. While variousdistribution-based methods exist to provide safety mechanisms for AI models,there is a noted lack of systematic assessment of these methods, especially inthe context of safety-critical automotive applications. Many methods in theliterature do not adapt well to the quick response times required insafety-critical edge applications. This paper reviews these methods, discussestheir suitability for safety-critical applications, and highlights theirstrengths and limitations. The paper also proposes potential improvements toenhance the safety and reliability of AI algorithms in autonomous vehicles inthe context of rapid and accurate decision-making processes.</description><author>Mandar Pitale, Alireza Abbaspour, Devesh Upadhyay</author><pubDate>Thu, 29 Feb 2024 18:18:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08208v2</guid></item><item><title>Understanding Iterative Combinatorial Auction Designs via Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2402.19420v1</link><description>Iterative combinatorial auctions are widely used in high stakes settings suchas spectrum auctions. Such auctions can be hard to understand analytically,making it difficult for bidders to determine how to behave and for designers tooptimize auction rules to ensure desirable outcomes such as high revenue orwelfare. In this paper, we investigate whether multi-agent reinforcementlearning (MARL) algorithms can be used to understand iterative combinatorialauctions, given that these algorithms have recently shown empirical success inseveral other domains. We find that MARL can indeed benefit auction analysis,but that deploying it effectively is nontrivial. We begin by describingmodelling decisions that keep the resulting game tractable without sacrificingimportant features such as imperfect information or asymmetry between bidders.We also discuss how to navigate pitfalls of various MARL algorithms, how toovercome challenges in verifying convergence, and how to generate and interpretmultiple equilibria. We illustrate the promise of our resulting approach byusing it to evaluate a specific rule change to a clock auction, findingsubstantially different auction outcomes due to complex changes in bidders'behavior.</description><author>Greg d'Eon, Neil Newman, Kevin Leyton-Brown</author><pubDate>Thu, 29 Feb 2024 18:16:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19420v1</guid></item><item><title>The NPU-ASLP-LiAuto System Description for Visual Speech Recognition in CNVSRC 2023</title><link>http://arxiv.org/abs/2401.06788v2</link><description>This paper delineates the visual speech recognition (VSR) system introducedby the NPU-ASLP-LiAuto (Team 237) in the first Chinese Continuous Visual SpeechRecognition Challenge (CNVSRC) 2023, engaging in the fixed and open tracks ofSingle-Speaker VSR Task, and the open track of Multi-Speaker VSR Task. In termsof data processing, we leverage the lip motion extractor from the baseline1 toproduce multi-scale video data. Besides, various augmentation techniques areapplied during training, encompassing speed perturbation, random rotation,horizontal flipping, and color transformation. The VSR model adopts anend-to-end architecture with joint CTC/attention loss, comprising a ResNet3Dvisual frontend, an E-Branchformer encoder, and a Transformer decoder.Experiments show that our system achieves 34.76% CER for the Single-SpeakerTask and 41.06% CER for the Multi-Speaker Task after multi-system fusion,ranking first place in all three tracks we participate.</description><author>He Wang, Pengcheng Guo, Wei Chen, Pan Zhou, Lei Xie</author><pubDate>Thu, 29 Feb 2024 18:09:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06788v2</guid></item><item><title>PaECTER: Patent-level Representation Learning using Citation-informed Transformers</title><link>http://arxiv.org/abs/2402.19411v1</link><description>PaECTER is a publicly available, open-source document-level encoder specificfor patents. We fine-tune BERT for Patents with examiner-added citationinformation to generate numerical representations for patent documents. PaECTERperforms better in similarity tasks than current state-of-the-art models usedin the patent domain. More specifically, our model outperforms the next-bestpatent specific pre-trained language model (BERT for Patents) on our patentcitation prediction test dataset on two different rank evaluation metrics.PaECTER predicts at least one most similar patent at a rank of 1.32 on averagewhen compared against 25 irrelevant patents. Numerical representationsgenerated by PaECTER from patent text can be used for downstream tasks such asclassification, tracing knowledge flows, or semantic similarity search.Semantic similarity search is especially relevant in the context of prior artsearch for both inventors and patent examiners. PaECTER is available on HuggingFace.</description><author>Mainak Ghosh, Sebastian Erhardt, Michael E. Rose, Erik Buunk, Dietmar Harhoff</author><pubDate>Thu, 29 Feb 2024 18:09:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19411v1</guid></item><item><title>On the Scaling Laws of Geographical Representation in Language Models</title><link>http://arxiv.org/abs/2402.19406v1</link><description>Language models have long been shown to embed geographical information intheir hidden representations. This line of work has recently been revisited byextending this result to Large Language Models (LLMs). In this paper, wepropose to fill the gap between well-established and recent literature byobserving how geographical knowledge evolves when scaling language models. Weshow that geographical knowledge is observable even for tiny models, and thatit scales consistently as we increase the model size. Notably, we observe thatlarger language models cannot mitigate the geographical bias that is inherentto the training data.</description><author>Nathan Godey, Éric de la Clergerie, Benoît Sagot</author><pubDate>Thu, 29 Feb 2024 18:04:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19406v1</guid></item><item><title>Navigating Hallucinations for Reasoning of Unintentional Activities</title><link>http://arxiv.org/abs/2402.19405v1</link><description>In this work we present a novel task of understanding unintentional humanactivities in videos. We formalize this problem as a reasoning task underzero-shot scenario, where given a video of an unintentional activity we want toknow why it transitioned from intentional to unintentional. We first evaluatethe effectiveness of current state-of-the-art Large Multimodal Models on thisreasoning task and observe that they suffer from hallucination. We furtherpropose a novel prompting technique,termed as Dream of Thoughts (DoT), whichallows the model to navigate through hallucinated thoughts to achieve betterreasoning. To evaluate the performance on this task, we also introduce threedifferent specialized metrics designed to quantify the models reasoningcapability. We perform our experiments on two different datasets, OOPs andUCF-Crimes, and our findings show that DOT prompting technique is able tooutperform standard prompting, while minimizing hallucinations.</description><author>Shresth Grover, Vibhav Vineet, Yogesh S Rawat</author><pubDate>Thu, 29 Feb 2024 18:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19405v1</guid></item><item><title>Entity-Aware Multimodal Alignment Framework for News Image Captioning</title><link>http://arxiv.org/abs/2402.19404v1</link><description>News image captioning task is a variant of image captioning task whichrequires model to generate a more informative caption with news image and theassociated news article. Multimodal Large Language models have developedrapidly in recent years and is promising in news image captioning task.However, according to our experiments, common MLLMs are not good at generatingthe entities in zero-shot setting. Their abilities to deal with the entitiesinformation are still limited after simply fine-tuned on news image captioningdataset. To obtain a more powerful model to handle the multimodal entityinformation, we design two multimodal entity-aware alignment tasks and analignment framework to align the model and generate the news image captions.Our method achieves better results than previous state-of-the-art models inCIDEr score (72.33 -&gt; 86.29) on GoodNews dataset and (70.83 -&gt; 85.61) onNYTimes800k dataset.</description><author>Junzhe Zhang, Huixuan Zhang, Xiaojun Wan</author><pubDate>Thu, 29 Feb 2024 18:03:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19404v1</guid></item><item><title>A Scalable and Transferable Time Series Prediction Framework for Demand Forecasting</title><link>http://arxiv.org/abs/2402.19402v1</link><description>Time series forecasting is one of the most essential and ubiquitous tasks inmany business problems, including demand forecasting and logisticsoptimization. Traditional time series forecasting methods, however, haveresulted in small models with limited expressive power because they havedifficulty in scaling their model size up while maintaining high accuracy. Inthis paper, we propose Forecasting orchestra (Forchestra), a simple butpowerful framework capable of accurately predicting future demand for a diverserange of items. We empirically demonstrate that the model size is scalable toup to 0.8 billion parameters. The proposed method not only outperforms existingforecasting models with a significant margin, but it could generalize well tounseen data points when evaluated in a zero-shot fashion on downstreamdatasets. Last but not least, we present extensive qualitative and quantitativestudies to analyze how the proposed model outperforms baseline models anddiffers from conventional approaches. The original paper was presented as afull paper at ICDM 2022 and is available at:https://ieeexplore.ieee.org/document/10027662.</description><author>Young-Jin Park, Donghyun Kim, Frédéric Odermatt, Juho Lee, Kyung-Min Kim</author><pubDate>Thu, 29 Feb 2024 18:01:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19402v1</guid></item><item><title>Assessing Visually-Continuous Corruption Robustness of Neural Networks Relative to Human Performance</title><link>http://arxiv.org/abs/2402.19401v1</link><description>While Neural Networks (NNs) have surpassed human accuracy in imageclassification on ImageNet, they often lack robustness against imagecorruption, i.e., corruption robustness. Yet such robustness is seeminglyeffortless for human perception. In this paper, we propose visually-continuouscorruption robustness (VCR) -- an extension of corruption robustness to allowassessing it over the wide and continuous range of changes that correspond tothe human perceptive quality (i.e., from the original image to the fulldistortion of all perceived visual information), along with two novelhuman-aware metrics for NN evaluation. To compare VCR of NNs with humanperception, we conducted extensive experiments on 14 commonly used imagecorruptions with 7,718 human participants and state-of-the-art robust NN modelswith different training objectives (e.g., standard, adversarial, corruptionrobustness), different architectures (e.g., convolution NNs, visiontransformers), and different amounts of training data augmentation. Our studyshowed that: 1) assessing robustness against continuous corruption can revealinsufficient robustness undetected by existing benchmarks; as a result, 2) thegap between NN and human robustness is larger than previously known; andfinally, 3) some image corruptions have a similar impact on human perception,offering opportunities for more cost-effective robustness assessments. Ourvalidation set with 14 image corruptions, human robustness data, and theevaluation code is provided as a toolbox and a benchmark.</description><author>Huakun Shen, Boyue Caroline Hu, Krzysztof Czarnecki, Lina Marsso, Marsha Chechik</author><pubDate>Thu, 29 Feb 2024 18:00:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19401v1</guid></item><item><title>Efficient Online Scheduling and Routing for Automated Guided Vehicles In Loop-Based Graphs</title><link>http://arxiv.org/abs/2310.02195v2</link><description>Automated guided vehicles (AGVs) are widely used in various industries, andscheduling and routing them in a conflict-free manner is crucial to theirefficient operation. We propose a loop-based algorithm that solves the online,conflict-free scheduling and routing problem for AGVs with any capacity andordered jobs in loop-based graphs. The proposed algorithm is compared againstan exact method, a greedy heuristic and a metaheuristic. We experimentallyshow, using theoretical and real instances on a model representing a realmanufacturing plant, that this algorithm either outperforms the otheralgorithms or gets an equally good solution in less computing time.</description><author>Louis Stubbe, Jens Goemaere, Jan Goedgebeur</author><pubDate>Thu, 29 Feb 2024 17:56:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02195v2</guid></item><item><title>SeD: Semantic-Aware Discriminator for Image Super-Resolution</title><link>http://arxiv.org/abs/2402.19387v1</link><description>Generative Adversarial Networks (GANs) have been widely used to recover vividtextures in image super-resolution (SR) tasks. In particular, one discriminatoris utilized to enable the SR network to learn the distribution of real-worldhigh-quality images in an adversarial training manner. However, thedistribution learning is overly coarse-grained, which is susceptible to virtualtextures and causes counter-intuitive generation results. To mitigate this, wepropose the simple and effective Semantic-aware Discriminator (denoted as SeD),which encourages the SR network to learn the fine-grained distributions byintroducing the semantics of images as a condition. Concretely, we aim toexcavate the semantics of images from a well-trained semantic extractor. Underdifferent semantics, the discriminator is able to distinguish the real-fakeimages individually and adaptively, which guides the SR network to learn themore fine-grained semantic-aware textures. To obtain accurate and abundantsemantics, we take full advantage of recently popular pretrained vision models(PVMs) with extensive datasets, and then incorporate its semantic features intothe discriminator through a well-designed spatial cross-attention module. Inthis way, our proposed semantic-aware discriminator empowered the SR network toproduce more photo-realistic and pleasing images. Extensive experiments on twotypical tasks, i.e., SR and Real SR have demonstrated the effectiveness of ourproposed methods.</description><author>Bingchen Li, Xin Li, Hanxin Zhu, Yeying Jin, Ruoyu Feng, Zhizheng Zhang, Zhibo Chen</author><pubDate>Thu, 29 Feb 2024 17:38:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19387v1</guid></item><item><title>Towards Safe and Reliable Autonomous Driving: Dynamic Occupancy Set Prediction</title><link>http://arxiv.org/abs/2402.19385v1</link><description>In the rapidly evolving field of autonomous driving, accurate trajectoryprediction is pivotal for vehicular safety. However, trajectory predictionsoften deviate from actual paths, particularly in complex and challengingenvironments, leading to significant errors. To address this issue, our studyintroduces a novel method for Dynamic Occupancy Set (DOS) prediction, enhancingtrajectory prediction capabilities. This method effectively combines advancedtrajectory prediction networks with a DOS prediction module, overcoming theshortcomings of existing models. It provides a comprehensive and adaptableframework for predicting the potential occupancy sets of traffic participants.The main contributions of this research include: 1) A novel DOS predictionmodel tailored for complex scenarios, augmenting traditional trajectoryprediction; 2) The development of unique DOS representations and evaluationmetrics; 3) Extensive validation through experiments, demonstrating enhancedperformance and adaptability. This research contributes to the advancement ofsafer and more efficient intelligent vehicle and transportation systems.</description><author>Wenbo Shao, Jiahui Xu, Wenhao Yu, Jun Li, Hong Wang</author><pubDate>Thu, 29 Feb 2024 17:36:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19385v1</guid></item><item><title>Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy</title><link>http://arxiv.org/abs/2402.19379v1</link><description>Human forecasting accuracy in practice relies on the 'wisdom of the crowd'effect, in which predictions about future events are significantly improved byaggregating across a crowd of individual forecasters. Past work on theforecasting ability of large language models (LLMs) suggests that frontierLLMs, as individual forecasters, underperform compared to the gold standard ofa human crowd forecasting tournament aggregate. In Study 1, we expand thisresearch by using an LLM ensemble approach consisting of a crowd of twelveLLMs. We compare the aggregated LLM predictions on 31 binary questions to thatof a crowd of 925 human forecasters from a three-month forecasting tournament.Our main analysis shows that the LLM crowd outperforms a simple no-informationbenchmark and is statistically equivalent to the human crowd. We also observean acquiescence effect, with mean model predictions being significantly above50%, despite an almost even split of positive and negative resolutions.Moreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2)can be improved by drawing on human cognitive output. We find that both models'forecasting accuracy benefits from exposure to the median human prediction asinformation, improving accuracy by between 17% and 28%: though this leads toless accurate predictions than simply averaging human and machine forecasts.Our results suggest that LLMs can achieve forecasting accuracy rivaling that ofhuman crowd forecasting tournaments: via the simple, practically applicablemethod of forecast aggregation. This replicates the 'wisdom of the crowd'effect for LLMs, and opens up their use for a variety applications throughoutsociety.</description><author>Philipp Schoenegger, Indre Tuminauskaite, Peter S. Park, Philip E. Tetlock</author><pubDate>Thu, 29 Feb 2024 17:27:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19379v1</guid></item><item><title>Single and bi-layered 2-D acoustic soft tactile skin (AST2)</title><link>http://arxiv.org/abs/2401.14292v2</link><description>This paper aims to present an innovative and cost-effective design forAcoustic Soft Tactile (AST) Skin, with the primary goal of significantlyenhancing the accuracy of 2-D tactile feature estimation. The existingchallenge lies in achieving precise tactile feature estimation, especiallyconcerning contact geometry characteristics, using cost-effective solutions. Wehypothesise that by harnessing acoustic energy through dedicated acousticchannels in 2 layers beneath the sensing surface and analysing amplitudemodulation, we can effectively decode interactions on the sensory surface,thereby improving tactile feature estimation. Our approach involves thedistinct separation of hardware components responsible for emitting andreceiving acoustic signals, resulting in a modular and highly customizable skindesign. Practical tests demonstrate the effectiveness of this novel design,achieving remarkable precision in estimating contact normal forces (MAE &lt; 0.8N), 2D contact localisation (MAE &lt; 0.7 mm), and contact surface diameter (MAE &lt;0.3 mm). In conclusion, the AST skin, with its innovative design and modulararchitecture, successfully addresses the challenge of tactile featureestimation. The presented results showcase its ability to precisely estimatevarious tactile features, making it a practical and cost-effective solution forrobotic applications.</description><author>Vishnu Rajendran, Simon Parsons, Amir Ghalamzan E</author><pubDate>Thu, 29 Feb 2024 17:23:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14292v2</guid></item><item><title>Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)</title><link>http://arxiv.org/abs/2402.04140v3</link><description>This study consists of a novel approach toward the analysis of courtjudgments spanning five countries, including the United States, the UnitedKingdom, Rwanda, Sweden and Hong Kong. This study also explores theintersection of the latest advancements in artificial intelligence (AI) andlegal analysis, emphasizing the role of AI (specifically generative AI) inidentifying human biases and facilitating automated, valid, and coherentmultisided argumentation of court judgments with the goal of ensuringconsistent application of laws in and across various jurisdictions. Byincorporating Advanced Language Models (ALMs) and a newly introduced human-AIcollaborative framework, this paper seeks to analyze Grounded Theory-basedresearch design with Advanced Language Models (ALMs) in the practice of law.SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPTtechnology), focusing on detecting logical inconsistencies and biases acrossvarious legal decisions. SHIRLEY analysis is aggregated and is accompanied by acomparison-oriented AI-based application called SAM (also an ALM) to identifyrelative deviations in SHIRLEY bias detections. Further, a CRITIC is generatedwithin semi-autonomous arbitration process via the ALM, SARA. A novel approachis introduced in the utilization of an AI arbitrator to critically evaluatebiases and qualitative-in-nature nuances identified by the aforementioned AIapplications (SAM in concert with SHIRLEY), based on the Hague Rules onBusiness and Human Rights Arbitration. This Semi-Automated Arbitration Process(SAAP) aims to uphold the integrity and fairness of legal judgments by ensuringa nuanced debate-resultant "understanding" through a hybrid system of AI andhuman-based collaborative analysis.</description><author>Michael De'Shazer</author><pubDate>Thu, 29 Feb 2024 17:23:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04140v3</guid></item><item><title>OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics</title><link>http://arxiv.org/abs/2401.12202v2</link><description>Remarkable progress has been made in recent years in the fields of vision,language, and robotics. We now have vision models capable of recognizingobjects based on language queries, navigation systems that can effectivelycontrol mobile systems, and grasping models that can handle a wide range ofobjects. Despite these advancements, general-purpose applications of roboticsstill lag behind, even though they rely on these fundamental capabilities ofrecognition, navigation, and grasping. In this paper, we adopt a systems-firstapproach to develop a new Open Knowledge-based robotics framework calledOK-Robot. By combining Vision-Language Models (VLMs) for object detection,navigation primitives for movement, and grasping primitives for objectmanipulation, OK-Robot offers a integrated solution for pick-and-dropoperations without requiring any training. To evaluate its performance, we runOK-Robot in 10 real-world home environments. The results demonstrate thatOK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks,representing a new state-of-the-art in Open Vocabulary Mobile Manipulation(OVMM) with nearly 1.8x the performance of prior work. On cleaner, unclutteredenvironments, OK-Robot's performance increases to 82%. However, the mostimportant insight gained from OK-Robot is the critical role of nuanced detailswhen combining Open Knowledge systems like VLMs with robotic modules. Videos ofour experiments and code are available on our website:https://ok-robot.github.io</description><author>Peiqi Liu, Yaswanth Orru, Jay Vakil, Chris Paxton, Nur Muhammad Mahi Shafiullah, Lerrel Pinto</author><pubDate>Thu, 29 Feb 2024 17:20:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12202v2</guid></item><item><title>OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models</title><link>http://arxiv.org/abs/2402.19371v1</link><description>LLMs have become increasingly capable at accomplishing a range ofspecialized-tasks and can be utilized to expand equitable access to medicalknowledge. Most medical LLMs have involved extensive fine-tuning, leveragingspecialized medical data and significant, thus costly, amounts of computationalpower. Many of the top performing LLMs are proprietary and their access islimited to very few research groups. However, open-source (OS) models representa key area of growth for medical LLMs due to significant improvements inperformance and an inherent ability to provide the transparency and compliancerequired in healthcare. We present OpenMedLM, a prompting platform whichdelivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks.We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks(MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series ofprompting strategies, including zero-shot, few-shot, chain-of-thought (randomselection and kNN selection), and ensemble/self-consistency voting. We foundthat OpenMedLM delivers OS SOTA results on three common medical LLM benchmarks,surpassing the previous best performing OS models that leveragedcomputationally costly extensive fine-tuning. The model delivers a 72.6%accuracy on the MedQA benchmark, outperforming the previous SOTA by 2.4%, andachieves 81.7% accuracy on the MMLU medical-subset, establishing itself as thefirst OS LLM to surpass 80% accuracy on this benchmark. Our results highlightmedical-specific emergent properties in OS LLMs which have not yet beendocumented to date elsewhere, and showcase the benefits of further leveragingprompt engineering to improve the performance of accessible LLMs for medicalapplications.</description><author>Jenish Maharjan, Anurag Garikipati, Navan Preet Singh, Leo Cyrus, Mayank Sharma, Madalina Ciobanu, Gina Barnes, Rahul Thapa, Qingqing Mao, Ritankar Das</author><pubDate>Thu, 29 Feb 2024 17:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19371v1</guid></item><item><title>Edge-aware Hard Clustering Graph Pooling for Brain Imaging</title><link>http://arxiv.org/abs/2308.11909v7</link><description>Graph Convolutional Networks (GCNs) can capture non-Euclidean spatialdependence between different brain regions. The graph pooling operator, acrucial element of GCNs, enhances the representation learning capability andfacilitates the acquisition of abnormal brain maps. However, most existingresearch designs graph pooling operators solely from the perspective of nodeswhile disregarding the original edge features. This confines graph poolingapplication scenarios and diminishes its ability to capture criticalsubstructures. In this paper, we propose a novel edge-aware hard clusteringgraph pool (EHCPool), which is tailored to dominant edge features and redefinesthe clustering process. EHCPool initially introduced the 'Edge-to-Node' scorecriterion which utilized edge information to evaluate the significance ofnodes. An innovative Iteration n-top strategy was then developed, guided byedge scores, to adaptively learn sparse hard clustering assignments for graphs.Additionally, a N-E Aggregation strategy is designed to aggregate node and edgefeatures in each independent subgraph. Extensive experiments on the multi-sitepublic datasets demonstrate the superiority and robustness of the proposedmodel. More notably, EHCPool has the potential to probe different types ofdysfunctional brain networks from a data-driven perspective. Method code:https://github.com/swfen/EHCPool</description><author>Cheng Zhu, Jiayi Zhu, Xi Wu, Lijuan Zhang, Shuqi Yang, Ping Liang, Honghan Chen, Ying Tan</author><pubDate>Thu, 29 Feb 2024 17:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11909v7</guid></item><item><title>Structure Preserving Diffusion Models</title><link>http://arxiv.org/abs/2402.19369v1</link><description>Diffusion models have become the leading distribution-learning method inrecent years. Herein, we introduce structure-preserving diffusion processes, afamily of diffusion processes for learning distributions that possessadditional structure, such as group symmetries, by developing theoreticalconditions under which the diffusion transition steps preserve said symmetry.While also enabling equivariant data sampling trajectories, we exemplify theseresults by developing a collection of different symmetry equivariant diffusionmodels capable of learning distributions that are inherently symmetric.Empirical studies, over both synthetic and real-world datasets, are used tovalidate the developed models adhere to the proposed theory and are capable ofachieving improved performance over existing methods in terms of sampleequality. We also show how the proposed models can be used to achievetheoretically guaranteed equivariant image noise reduction without priorknowledge of the image orientation.</description><author>Haoye Lu, Spencer Szabados, Yaoliang Yu</author><pubDate>Thu, 29 Feb 2024 17:16:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19369v1</guid></item><item><title>SoK: Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency</title><link>http://arxiv.org/abs/2402.19366v1</link><description>The growing number of cases requiring digital forensic analysis raisesconcerns about law enforcement's ability to conduct investigations promptly.Consequently, this systemisation of knowledge paper delves into the potentialand effectiveness of integrating Large Language Models (LLMs) into digitalforensic investigation to address these challenges. A thorough literaturereview is undertaken, encompassing existing digital forensic models, tools,LLMs, deep learning techniques, and the utilisation of LLMs in investigations.The review identifies current challenges within existing digital forensicprocesses and explores both the obstacles and possibilities of incorporatingLLMs. In conclusion, the study asserts that the adoption of LLMs in digitalforensics, with appropriate constraints, holds the potential to enhanceinvestigation efficiency, improve traceability, and alleviate technical andjudicial barriers faced by law enforcement entities.</description><author>Akila Wickramasekara, Frank Breitinger, Mark Scanlon</author><pubDate>Thu, 29 Feb 2024 17:13:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19366v1</guid></item><item><title>Watermark Stealing in Large Language Models</title><link>http://arxiv.org/abs/2402.19361v1</link><description>LLM watermarking has attracted attention as a promising way to detectAI-generated content, with some works suggesting that current schemes mayalready be fit for deployment. In this work we dispute this claim, identifyingwatermark stealing (WS) as a fundamental vulnerability of these schemes. Weshow that querying the API of the watermarked LLM to approximatelyreverse-engineer a watermark enables practical spoofing attacks, as suggestedin prior work, but also greatly boosts scrubbing attacks, which was previouslyunnoticed. We are the first to propose an automated WS algorithm and use it inthe first comprehensive study of spoofing and scrubbing in realistic settings.We show that for under $50 an attacker can both spoof and scrubstate-of-the-art schemes previously considered safe, with average success rateof over 80%. Our findings challenge common beliefs about LLM watermarking,stressing the need for more robust schemes. We make all our code and additionalexamples available at https://watermark-stealing.org.</description><author>Nikola Jovanović, Robin Staab, Martin Vechev</author><pubDate>Thu, 29 Feb 2024 17:12:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19361v1</guid></item><item><title>LLM Inference Unveiled: Survey and Roofline Model Insights</title><link>http://arxiv.org/abs/2402.16363v3</link><description>The field of efficient Large Language Model (LLM) inference is rapidlyevolving, presenting a unique blend of opportunities and challenges. Althoughthe field has expanded and is vibrant, there hasn't been a concise frameworkthat analyzes the various methods of LLM Inference to provide a clearunderstanding of this domain. Our survey stands out from traditional literaturereviews by not only summarizing the current state of research but also byintroducing a framework based on roofline model for systematic analysis of LLMinference techniques. This framework identifies the bottlenecks when deployingLLMs on hardware devices and provides a clear understanding of practicalproblems, such as why LLMs are memory-bound, how much memory and computationthey need, and how to choose the right hardware. We systematically collate thelatest advancements in efficient LLM inference, covering crucial areas such asmodel compression (e.g., Knowledge Distillation and Quantization), algorithmimprovements (e.g., Early Exit and Mixture-of-Expert), and both hardware andsystem-level enhancements. Our survey stands out by analyzing these methodswith roofline model, helping us understand their impact on memory access andcomputation. This distinctive approach not only showcases the current researchlandscape but also delivers valuable insights for practical implementation,positioning our work as an indispensable resource for researchers new to thefield as well as for those seeking to deepen their understanding of efficientLLM deployment. The analyze tool, LLM-Viewer, is open-sourced.</description><author>Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen, Guangyu Sun, Kurt Keutzer</author><pubDate>Thu, 29 Feb 2024 17:08:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16363v3</guid></item><item><title>Unraveling Adversarial Examples against Speaker Identification -- Techniques for Attack Detection and Victim Model Classification</title><link>http://arxiv.org/abs/2402.19355v1</link><description>Adversarial examples have proven to threaten speaker identification systems,and several countermeasures against them have been proposed. In this paper, wepropose a method to detect the presence of adversarial examples, i.e., a binaryclassifier distinguishing between benign and adversarial examples. We buildupon and extend previous work on attack type classification by exploring newarchitectures. Additionally, we introduce a method for identifying the victimmodel on which the adversarial attack is carried out. To achieve this, wegenerate a new dataset containing multiple attacks performed against variousvictim models. We achieve an AUC of 0.982 for attack detection, with no morethan a 0.03 drop in performance for unknown attacks. Our attack classificationaccuracy (excluding benign) reaches 86.48% across eight attack types using ourLightResNet34 architecture, while our victim model classification accuracyreaches 72.28% across four victim models.</description><author>Sonal Joshi, Thomas Thebaud, Jesús Villalba, Najim Dehak</author><pubDate>Thu, 29 Feb 2024 17:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19355v1</guid></item><item><title>BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning of SAM</title><link>http://arxiv.org/abs/2402.16338v2</link><description>The Segment Anything Model (SAM), a foundation model pretrained on millionsof images and segmentation masks, has significantly advanced semanticsegmentation, a fundamental task in computer vision. Despite its strengths, SAMencounters two major challenges. Firstly, it struggles with segmenting specificobjects autonomously, as it relies on users to manually input prompts likepoints or bounding boxes to identify targeted objects. Secondly, SAM faceschallenges in excelling at specific downstream tasks, like medical imaging, dueto a disparity between the distribution of its pretraining data, whichpredominantly consists of general-domain images, and the data used indownstream tasks. Current solutions to these problems, which involve finetuningSAM, often lead to overfitting, a notable issue in scenarios with very limiteddata, like in medical imaging. To overcome these limitations, we introduceBLO-SAM, which finetunes SAM based on bi-level optimization (BLO). Our approachallows for automatic image segmentation without the need for manual prompts, byoptimizing a learnable prompt embedding. Furthermore, it significantly reducesthe risk of overfitting by training the model's weight parameters and theprompt embedding on two separate subsets of the training dataset, each at adifferent level of optimization. We apply BLO-SAM to diverse semanticsegmentation tasks in general and medical domains. The results demonstrateBLO-SAM's superior performance over various state-of-the-art image semanticsegmentation methods.</description><author>Li Zhang, Youwei Liang, Ruiyi Zhang, Pengtao Xie</author><pubDate>Thu, 29 Feb 2024 17:04:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16338v2</guid></item><item><title>Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process</title><link>http://arxiv.org/abs/2402.19350v1</link><description>Pre-trained language models (PLMs) leverage chains-of-thought (CoT) tosimulate human reasoning and inference processes, achieving proficientperformance in multi-hop QA. However, a gap persists between PLMs' reasoningabilities and those of humans when tackling complex problems. Psychologicalstudies suggest a vital connection between explicit information in passages andhuman prior knowledge during reading. Nevertheless, current research has giveninsufficient attention to linking input passages and PLMs' pre-training-basedknowledge from the perspective of human cognition studies. In this study, weintroduce a \textbf{P}rompting \textbf{E}xplicit and \textbf{I}mplicitknowledge (PEI) framework, which uses prompts to connect explicit and implicitknowledge, aligning with human reading process for multi-hop QA. We considerthe input passages as explicit knowledge, employing them to elicit implicitknowledge through unified prompt reasoning. Furthermore, our model incorporatestype-specific reasoning via prompts, a form of implicit knowledge. Experimentalresults show that PEI performs comparably to the state-of-the-art on HotpotQA.Ablation studies confirm the efficacy of our model in bridging and integratingexplicit and implicit knowledge.</description><author>Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun</author><pubDate>Thu, 29 Feb 2024 16:56:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19350v1</guid></item><item><title>Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook</title><link>http://arxiv.org/abs/2402.19348v1</link><description>As cities continue to burgeon, Urban Computing emerges as a pivotaldiscipline for sustainable development by harnessing the power of cross-domaindata fusion from diverse sources (e.g., geographical, traffic, social media,and environmental data) and modalities (e.g., spatio-temporal, visual, andtextual modalities). Recently, we are witnessing a rising trend that utilizesvarious deep-learning methods to facilitate cross-domain data fusion in smartcities. To this end, we propose the first survey that systematically reviewsthe latest advancements in deep learning-based data fusion methods tailored forurban computing. Specifically, we first delve into data perspective tocomprehend the role of each modality and data source. Secondly, we classify themethodology into four primary categories: feature-based, alignment-based,contrast-based, and generation-based fusion methods. Thirdly, we furthercategorize multi-modal urban applications into seven types: urban planning,transportation, economy, public safety, society, environment, and energy.Compared with previous surveys, we focus more on the synergy of deep learningmethods with urban computing applications. Furthermore, we shed light on theinterplay between Large Language Models (LLMs) and urban computing, postulatingfuture research directions that could revolutionize the field. We firmlybelieve that the taxonomy, progress, and prospects delineated in our surveystand poised to significantly enrich the research community. The summary of thecomprehensive and up-to-date paper list can be found athttps://github.com/yoshall/Awesome-Multimodal-Urban-Computing.</description><author>Xingchen Zou, Yibo Yan, Xixuan Hao, Yuehong Hu, Haomin Wen, Erdong Liu, Junbo Zhang, Yong Li, Tianrui Li, Yu Zheng, Yuxuan Liang</author><pubDate>Thu, 29 Feb 2024 16:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19348v1</guid></item><item><title>MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large Multimodal Model</title><link>http://arxiv.org/abs/2402.16749v2</link><description>With the evolution of storage and communication protocols, ultra-low bitrateimage compression has become a highly demanding topic. However, existingcompression algorithms must sacrifice either consistency with the ground truthor perceptual quality at ultra-low bitrate. In recent years, the rapiddevelopment of the Large Multimodal Model (LMM) has made it possible to balancethese two goals. To solve this problem, this paper proposes a method calledMultimodal Image Semantic Compression (MISC), which consists of an LMM encoderfor extracting the semantic information of the image, a map encoder to locatethe region corresponding to the semantic, an image encoder generates anextremely compressed bitstream, and a decoder reconstructs the image based onthe above information. Experimental results show that our proposed MISC issuitable for compressing both traditional Natural Sense Images (NSIs) andemerging AI-Generated Images (AIGIs) content. It can achieve optimalconsistency and perception results while saving 50% bitrate, which has strongpotential applications in the next generation of storage and communication. Thecode will be released on https://github.com/lcysyzxdxc/MISC.</description><author>Chunyi Li, Guo Lu, Donghui Feng, Haoning Wu, Zicheng Zhang, Xiaohong Liu, Guangtao Zhai, Weisi Lin, Wenjun Zhang</author><pubDate>Thu, 29 Feb 2024 16:53:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16749v2</guid></item><item><title>The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition</title><link>http://arxiv.org/abs/2402.19344v1</link><description>This paper describes the 6th Affective Behavior Analysis in-the-wild (ABAW)Competition, which is part of the respective Workshop held in conjunction withIEEE CVPR 2024. The 6th ABAW Competition addresses contemporary challenges inunderstanding human emotions and behaviors, crucial for the development ofhuman-centered technologies. In more detail, the Competition focuses on affectrelated benchmarking tasks and comprises of five sub-challenges: i)Valence-Arousal Estimation (the target is to estimate two continuous affectdimensions, valence and arousal), ii) Expression Recognition (the target is torecognise between the mutually exclusive classes of the 7 basic expressions and'other'), iii) Action Unit Detection (the target is to detect 12 action units),iv) Compound Expression Recognition (the target is to recognise between the 7mutually exclusive compound expression classes), and v) Emotional MimicryIntensity Estimation (the target is to estimate six continuous emotiondimensions). In the paper, we present these Challenges, describe theirrespective datasets and challenge protocols (we outline the evaluation metrics)and present the baseline systems as well as their obtained performance. Moreinformation for the Competition can be found in:\url{https://affective-behavior-analysis-in-the-wild.github.io/6th}.</description><author>Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Stefanos Zafeiriou, Chunchang Shao, Guanyu Hu</author><pubDate>Thu, 29 Feb 2024 16:49:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19344v1</guid></item><item><title>RoadRunner -- Learning Traversability Estimation for Autonomous Off-road Driving</title><link>http://arxiv.org/abs/2402.19341v1</link><description>Autonomous navigation at high speeds in off-road environments necessitatesrobots to comprehensively understand their surroundings using onboard sensingonly. The extreme conditions posed by the off-road setting can cause degradedcamera image quality due to poor lighting and motion blur, as well as limitedsparse geometric information available from LiDAR sensing when driving at highspeeds. In this work, we present RoadRunner, a novel framework capable ofpredicting terrain traversability and an elevation map directly from camera andLiDAR sensor inputs. RoadRunner enables reliable autonomous navigation, byfusing sensory information, handling of uncertainty, and generation ofcontextually informed predictions about the geometry and traversability of theterrain while operating at low latency. In contrast to existing methods relyingon classifying handcrafted semantic classes and using heuristics to predicttraversability costs, our method is trained end-to-end in a self-supervisedfashion. The RoadRunner network architecture builds upon popular sensor fusionnetwork architectures from the autonomous driving domain, which embed LiDAR andcamera information into a common Bird's Eye View perspective. Training isenabled by utilizing an existing traversability estimation stack to generatetraining data in hindsight in a scalable manner from real-world off-roaddriving datasets. Furthermore, RoadRunner improves the system latency by afactor of roughly 4, from 500 ms to 140 ms, while improving the accuracy fortraversability costs and elevation map predictions. We demonstrate theeffectiveness of RoadRunner in enabling safe and reliable off-road navigationat high speeds in multiple real-world driving scenarios through unstructureddesert environments.</description><author>Jonas Frey, Shehryar Khattak, Manthan Patel, Deegan Atha, Julian Nubert, Curtis Padgett, Marco Hutter, Patrick Spieler</author><pubDate>Thu, 29 Feb 2024 16:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19341v1</guid></item><item><title>One model to use them all: Training a segmentation model with complementary datasets</title><link>http://arxiv.org/abs/2402.19340v1</link><description>Understanding a surgical scene is crucial for computer-assisted surgerysystems to provide any intelligent assistance functionality. One way ofachieving this scene understanding is via scene segmentation, where every pixelof a frame is classified and therefore identifies the visible structures andtissues. Progress on fully segmenting surgical scenes has been made usingmachine learning. However, such models require large amounts of annotatedtraining data, containing examples of all relevant object classes. Such fullyannotated datasets are hard to create, as every pixel in a frame needs to beannotated by medical experts and, therefore, are rarely available. In thiswork, we propose a method to combine multiple partially annotated datasets,which provide complementary annotations, into one model, enabling better scenesegmentation and the use of multiple readily available datasets. Our methodaims to combine available data with complementary labels by leveraging mutualexclusive properties to maximize information. Specifically, we propose to usepositive annotations of other classes as negative samples and to excludebackground pixels of binary annotations, as we cannot tell if they contain aclass not annotated but predicted by the model. We evaluate our method bytraining a DeepLabV3 on the publicly available Dresden Surgical AnatomyDataset, which provides multiple subsets of binary segmented anatomicalstructures. Our approach successfully combines 6 classes into one model,increasing the overall Dice Score by 4.4% compared to an ensemble of modelstrained on the classes individually. By including information on multipleclasses, we were able to reduce confusion between stomach and colon by 24%. Ourresults demonstrate the feasibility of training a model on multiple datasets.This paves the way for future work further alleviating the need for one large,fully segmented datasets.</description><author>Alexander C. Jenke, Sebastian Bodenstedt, Fiona R. Kolbinger, Marius Distler, Jürgen Weitz, Stefanie Speidel</author><pubDate>Thu, 29 Feb 2024 16:46:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19340v1</guid></item><item><title>Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers for High-Level Image Classification</title><link>http://arxiv.org/abs/2402.19339v1</link><description>The increasing demand for automatic high-level image understanding,particularly in detecting abstract concepts (AC) within images, underscores thenecessity for innovative and more interpretable approaches. These approachesneed to harmonize traditional deep vision methods with the nuanced,context-dependent knowledge humans employ to interpret images at intricatesemantic levels. In this work, we leverage situated perceptual knowledge ofcultural images to enhance performance and interpretability in AC imageclassification. We automatically extract perceptual semantic units from images,which we then model and integrate into the ARTstract Knowledge Graph (AKG).This resource captures situated perceptual semantics gleaned from over 14,000cultural images labeled with ACs. Additionally, we enhance the AKG withhigh-level linguistic frames. We compute KG embeddings and experiment withrelative representations and hybrid approaches that fuse these embeddings withvisual transformer embeddings. Finally, for interpretability, we conductposthoc qualitative analyses by examining model similarities with traininginstances. Our results show that our hybrid KGE-ViT methods outperform existingtechniques in AC image classification. The posthoc interpretability analysesreveal the visual transformer's proficiency in capturing pixel-level visualattributes, contrasting with our method's efficacy in representing moreabstract and semantic scene elements. We demonstrate the synergy andcomplementarity between KGE embeddings' situated perceptual knowledge and deepvisual model's sensory-perceptual understanding for AC image classification.This work suggests a strong potential of neuro-symbolic methods for knowledgeintegration and robust image representation for use in downstream intricatevisual comprehension tasks. All the materials and code are available online.</description><author>Delfina Sol Martinez Pandiani, Nicolas Lazzari, Valentina Presutti</author><pubDate>Thu, 29 Feb 2024 16:46:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19339v1</guid></item><item><title>Exploring a new machine learning based probabilistic model for high-resolution indoor radon mapping, using the German indoor radon survey data</title><link>http://arxiv.org/abs/2310.11143v2</link><description>Radon is a carcinogenic, radioactive gas that can accumulate indoors.Therefore, accurate knowledge of indoor radon concentration is crucial forassessing radon-related health effects or identifying radon-prone areas. Indoorradon concentration at the national scale is usually estimated on the basis ofextensive measurement campaigns. However, characteristics of the sample oftendiffer from the characteristics of the population due to the large number ofrelevant factors that control the indoor radon concentration such as theavailability of geogenic radon or floor level. Furthermore, the sample sizeusually does not allow estimation with high spatial resolution. We propose amodel-based approach that allows a more realistic estimation of indoor radondistribution with a higher spatial resolution than a purely data-basedapproach. A two-stage modelling approach was applied: 1) a quantile regressionforest using environmental and building data as predictors was applied toestimate the probability distribution function of indoor radon for each floorlevel of each residential building in Germany; (2) a probabilistic Monte Carlosampling technique enabled the combination and population weighting offloor-level predictions. In this way, the uncertainty of the individualpredictions is effectively propagated into the estimate of variability at theaggregated level. The results show an approximate lognormal distribution withan arithmetic mean of 63 Bq/m3, a geometric mean of 41 Bq/m3 and a 95 %ile of180 Bq/m3. The exceedance probability for 100 Bq/m3 and 300 Bq/m3 are 12.5 %(10.5 million people) and 2.2 % (1.9 million people), respectively.</description><author>Eric Petermann, Peter Bossew, Joachim Kemski, Valeria Gruber, Nils Suhr, Bernd Hoffmann</author><pubDate>Thu, 29 Feb 2024 16:38:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11143v2</guid></item><item><title>Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge</title><link>http://arxiv.org/abs/2402.19334v1</link><description>The democratization of pre-trained language models through open-sourceinitiatives has rapidly advanced innovation and expanded access to cutting-edgetechnologies. However, this openness also brings significant security risks,including backdoor attacks, where hidden malicious behaviors are triggered byspecific inputs, compromising natural language processing (NLP) systemintegrity and reliability. This paper suggests that merging a backdoored modelwith other homogeneous models can remediate backdoor vulnerabilities even ifsuch models are not entirely secure. In our experiments, we explore variousmodels (BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets(SST-2, OLID, AG News, and QNLI). Compared to multiple advanced defensiveapproaches, our method offers an effective and efficient inference-stagedefense against backdoor attacks without additional resources or specificknowledge. Our approach consistently outperforms the other advanced baselines,leading to an average of 75% reduction in the attack success rate. Since modelmerging has been an established approach for improving model performance, theextra advantage it provides regarding defense can be seen as a cost-free bonus.</description><author>Ansh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras, Qiongkai Xu</author><pubDate>Thu, 29 Feb 2024 16:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19334v1</guid></item><item><title>Compact Speech Translation Models via Discrete Speech Units Pretraining</title><link>http://arxiv.org/abs/2402.19333v1</link><description>Using Self-Supervised Learning (SSL) as model initialization is now common toobtain strong results in Speech Translation (ST). However, they also impose alarge memory footprint, hindering on-device deployment. In this paper, weleverage the SSL models by pretraining smaller models on their Discrete SpeechUnits (DSU). We pretrain encoder-decoder models on 1) Filterbank-to-DSU and 2)DSU-to-Translation data, and take the encoder from 1) and the decoder from 2)to initialise a new model, finetuning this on limited speech-translation data.The final model becomes compact by using the DSU pretraining to distil theknowledge of the SSL model. Our method has several benefits over using DSU asmodel inputs, such as shorter inference pipeline and robustness over (DSU)tokenization. In contrast to ASR pretraining, it does not require transcripts,making it applicable to low-resource settings. Evaluation on CoVoST-2 X-Enshows that our method is &gt;$0.5$ BLEU better than a ST model that directlyfinetune the SSL model, given only half the model size, and on a par with ASRpretraining.</description><author>Tsz Kin Lam, Alexandra Birch, Barry Haddow</author><pubDate>Thu, 29 Feb 2024 16:36:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19333v1</guid></item><item><title>A Novel Approach to Industrial Defect Generation through Blended Latent Diffusion Model with Online Adaptation</title><link>http://arxiv.org/abs/2402.19330v1</link><description>Effectively addressing the challenge of industrial Anomaly Detection (AD)necessitates an ample supply of defective samples, a constraint often hinderedby their scarcity in industrial contexts. This paper introduces a novelalgorithm designed to augment defective samples, thereby enhancing ADperformance. The proposed method tailors the blended latent diffusion model fordefect sample generation, employing a diffusion model to generate defectivesamples in the latent space. A feature editing process, controlled by a"trimap" mask and text prompts, refines the generated samples. The imagegeneration inference process is structured into three stages: a free diffusionstage, an editing diffusion stage, and an online decoder adaptation stage. Thissophisticated inference strategy yields high-quality synthetic defectivesamples with diverse pattern variations, leading to significantly improved ADaccuracies based on the augmented training set. Specifically, on the widelyrecognized MVTec AD dataset, the proposed method elevates the state-of-the-art(SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for ADmetrics AP, IAP, and IAP90, respectively. The implementation code of this workcan be found at the GitHub repositoryhttps://github.com/GrandpaXun242/AdaBLDM.git</description><author>Hanxi Li, Zhengxun Zhang, Hao Chen, Lin Wu, Bo Li, Deyin Liu, Mingwen Wang</author><pubDate>Thu, 29 Feb 2024 16:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19330v1</guid></item><item><title>Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions</title><link>http://arxiv.org/abs/2402.18060v2</link><description>LLMs have demonstrated impressive performance in answering medical questions,such as passing scores on medical licensing examinations. However, medicalboard exam questions or general clinical questions do not capture thecomplexity of realistic clinical cases. Moreover, the lack of referenceexplanations means we cannot easily evaluate the reasoning of model decisions,a crucial component of supporting doctors in making complex medical decisions.To address these challenges, we construct two new datasets: JAMA ClinicalChallenge and Medbullets. JAMA Clinical Challenge consists of questions basedon challenging clinical cases, while Medbullets comprises USMLE Step 2&amp;3 styleclinical questions. Both datasets are structured as multiple-choicequestion-answering tasks, where each question is accompanied by anexpert-written explanation. We evaluate four LLMs on the two datasets usingvarious prompts. Experiments demonstrate that our datasets are harder thanprevious benchmarks. The inconsistency between automatic and human evaluationsof model-generated explanations highlights the need to develop new metrics tosupport future research on explainable medical QA.</description><author>Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze</author><pubDate>Thu, 29 Feb 2024 16:31:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18060v2</guid></item><item><title>Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction</title><link>http://arxiv.org/abs/2402.19326v1</link><description>Whole Slide Image (WSI) classification is often formulated as a MultipleInstance Learning (MIL) problem. Recently, Vision-Language Models (VLMs) havedemonstrated remarkable performance in WSI classification. However, existingmethods leverage coarse-grained pathogenetic descriptions for visualrepresentation supervision, which are insufficient to capture the complexvisual appearance of pathogenetic images, hindering the generalizability ofmodels on diverse downstream tasks. Additionally, processing high-resolutionWSIs can be computationally expensive. In this paper, we propose a novel"Fine-grained Visual-Semantic Interaction" (FiVE) framework for WSIclassification. It is designed to enhance the model's generalizability byleveraging the interplay between localized visual patterns and fine-grainedpathological semantics. Specifically, with meticulously designed queries, westart by utilizing a large language model to extract fine-grained pathologicaldescriptions from various non-standardized raw reports. The output descriptionsare then reconstructed into fine-grained labels used for training. Byintroducing a Task-specific Fine-grained Semantics (TFS) module, we enableprompts to capture crucial visual information in WSIs, which enhancesrepresentation learning and augments generalization capabilities significantly.Furthermore, given that pathological visual patterns are redundantlydistributed across tissue slices, we sample a subset of visual instances duringtraining. Our method demonstrates robust generalizability and strongtransferability, dominantly outperforming the counterparts on the TCGA LungCancer dataset with at least 9.19% higher accuracy in few-shot experiments.</description><author>Hao Li, Ying Chen, Yifei Chen, Wenxian Yang, Bowen Ding, Yuchen Han, Liansheng Wang, Rongshan Yu</author><pubDate>Thu, 29 Feb 2024 16:29:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19326v1</guid></item><item><title>Verification of Neural Networks' Global Robustness</title><link>http://arxiv.org/abs/2402.19322v1</link><description>Neural networks are successful in various applications but are alsosusceptible to adversarial attacks. To show the safety of network classifiers,many verifiers have been introduced to reason about the local robustness of agiven input to a given perturbation. While successful, local robustness cannotgeneralize to unseen inputs. Several works analyze global robustnessproperties, however, neither can provide a precise guarantee about the caseswhere a network classifier does not change its classification. In this work, wepropose a new global robustness property for classifiers aiming at finding theminimal globally robust bound, which naturally extends the popular localrobustness property for classifiers. We introduce VHAGaR, an anytime verifierfor computing this bound. VHAGaR relies on three main ideas: encoding theproblem as a mixed-integer programming and pruning the search space byidentifying dependencies stemming from the perturbation or network computationand generalizing adversarial attacks to unknown inputs. We evaluate VHAGaR onseveral datasets and classifiers and show that, given a three hour timeout, theaverage gap between the lower and upper bound on the minimal globally robustbound computed by VHAGaR is 1.9, while the gap of an existing global robustnessverifier is 154.7. Moreover, VHAGaR is 130.6x faster than this verifier. Ourresults further indicate that leveraging dependencies and adversarial attacksmakes VHAGaR 78.6x faster.</description><author>Anan Kabaha, Dana Drachsler-Cohen</author><pubDate>Thu, 29 Feb 2024 16:27:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19322v1</guid></item><item><title>Attacks Against Mobility Prediction in 5G Networks</title><link>http://arxiv.org/abs/2402.19319v1</link><description>The $5^{th}$ generation of mobile networks introduces a new Network Function(NF) that was not present in previous generations, namely the Network DataAnalytics Function (NWDAF). Its primary objective is to provide advancedanalytics services to various entities within the network and also towardsexternal application services in the 5G ecosystem. One of the key use cases ofNWDAF is mobility trajectory prediction, which aims to accurately supportefficient mobility management of User Equipment (UE) in the network byallocating ``just in time'' necessary network resources. In this paper, we showthat there are potential mobility attacks that can compromise the accuracy ofthese predictions. In a semi-realistic scenario with 10,000 subscribers, wedemonstrate that an adversary equipped with the ability to hijack cellularmobile devices and clone them can significantly reduce the prediction accuracyfrom 75\% to 40\% using just 100 adversarial UEs. While a defense mechanismlargely depends on the attack and the mobility types in a particular area, weprove that a basic KMeans clustering is effective in distinguishing legitimateand adversarial UEs.</description><author>Syafiq Al Atiiq, Yachao Yuan, Christian Gehrmann, Jakob Sternby, Luis Barriga</author><pubDate>Thu, 29 Feb 2024 16:24:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19319v1</guid></item><item><title>UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical Neural Radiance Fields</title><link>http://arxiv.org/abs/2311.05836v6</link><description>In the field of clinical medicine, computed tomography (CT) is an effectivemedical imaging modality for the diagnosis of various pathologies. Comparedwith X-ray images, CT images can provide more information, includingmulti-planar slices and three-dimensional structures for clinical diagnosis.However, CT imaging requires patients to be exposed to large doses of ionizingradiation for a long time, which may cause irreversible physical harm. In thispaper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based ongenerated radiation fields. The network can learn a continuous representationof CT projections from 2D X-ray images by obtaining the internal structure anddepth information and using adaptive loss weights to ensure the quality of thegenerated images. Our model is trained on publicly available knee and chestdatasets, and we show the results of CT projection rendering with a singleX-ray and compare our method with other methods based on generated radiationfields.</description><author>Jing Hu, Qinrui Fan, Shu Hu, Siwei Lyu, Xi Wu, Xin Wang</author><pubDate>Thu, 29 Feb 2024 16:21:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05836v6</guid></item><item><title>Imputation of missing values in multi-view data</title><link>http://arxiv.org/abs/2210.14484v3</link><description>Data for which a set of objects is described by multiple distinct featuresets (called views) is known as multi-view data. When missing values occur inmulti-view data, all features in a view are likely to be missingsimultaneously. This leads to very large quantities of missing data which,especially when combined with high-dimensionality, makes the application ofconditional imputation methods computationally infeasible. We introduce a newimputation method based on the existing stacked penalized logistic regression(StaPLR) algorithm for multi-view learning. It performs imputation in adimension-reduced space to address computational challenges inherent to themulti-view context. We compare the performance of the new imputation methodwith several existing imputation algorithms in simulated data sets. The resultsshow that the new imputation method leads to competitive results at a muchlower computational cost, and makes the use of advanced imputation algorithmssuch as missForest and predictive mean matching possible in settings where theywould otherwise be computationally infeasible.</description><author>Wouter van Loon, Marjolein Fokkema, Frank de Vos, Marisa Koini, Reinhold Schmidt, Mark de Rooij</author><pubDate>Thu, 29 Feb 2024 16:21:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.14484v3</guid></item><item><title>Generative Adversarial Networks for Weakly Supervised Generation and Evaluation of Brain Tumor Segmentations on MR Images</title><link>http://arxiv.org/abs/2211.05269v3</link><description>Segmentation of regions of interest (ROIs) for identifying abnormalities is aleading problem in medical imaging. Using machine learning for this problemgenerally requires manually annotated ground-truth segmentations, demandingextensive time and resources from radiologists. This work presents a weaklysupervised approach that utilizes binary image-level labels, which are muchsimpler to acquire, to effectively segment anomalies in 2D magnetic resonanceimages without ground truth annotations. We train a generative adversarialnetwork (GAN) that converts cancerous images to healthy variants, which areused along with localization seeds as priors to generate improved weaklysupervised segmentations. The non-cancerous variants can also be used toevaluate the segmentations in a weakly supervised fashion, which allows for themost effective segmentations to be identified and then applied to downstreamclinical classification tasks. On the Multimodal Brain Tumor Segmentation(BraTS) 2020 dataset, our proposed method generates and identifiessegmentations that achieve test Dice coefficients of 83.91%. Using thesesegmentations for pathology classification results with a test AUC of 93.32%which is comparable to the test AUC of 95.80% achieved when using truesegmentations.</description><author>Jay J. Yoo, Khashayar Namdar, Matthias W. Wagner, Liana Nobre, Uri Tabori, Cynthia Hawkins, Birgit B. Ertl-Wagner, Farzad Khalvati</author><pubDate>Thu, 29 Feb 2024 16:19:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.05269v3</guid></item><item><title>Seeing the Intangible: Survey of Image Classification into High-Level and Abstract Categories</title><link>http://arxiv.org/abs/2308.10562v2</link><description>The field of Computer Vision (CV) is increasingly shifting towards``high-level'' visual sensemaking tasks, yet the exact nature of these tasksremains unclear and tacit. This survey paper addresses this ambiguity bysystematically reviewing research on high-level visual understanding, focusingparticularly on Abstract Concepts (ACs) in automatic image classification. Oursurvey contributes in three main ways: Firstly, it clarifies the tacitunderstanding of high-level semantics in CV through a multidisciplinaryanalysis, and categorization into distinct clusters, including commonsense,emotional, aesthetic, and inductive interpretative semantics. Secondly, itidentifies and categorizes computer vision tasks associated with high-levelvisual sensemaking, offering insights into the diverse research areas withinthis domain. Lastly, it examines how abstract concepts such as values andideologies are handled in CV, revealing challenges and opportunities inAC-based image classification. Notably, our survey of AC image classificationtasks highlights persistent challenges, such as the limited efficacy of massivedatasets and the importance of integrating supplementary information andmid-level features. We emphasize the growing relevance of hybrid AI systems inaddressing the multifaceted nature of AC image classification tasks. Overall,this survey enhances our understanding of high-level visual reasoning in CV andlays the groundwork for future research endeavors.</description><author>Delfina Sol Martinez Pandiani, Valentina Presutti</author><pubDate>Thu, 29 Feb 2024 16:18:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10562v2</guid></item><item><title>Loss-Free Machine Unlearning</title><link>http://arxiv.org/abs/2402.19308v1</link><description>We present a machine unlearning approach that is both retraining- andlabel-free. Most existing machine unlearning approaches require a model to befine-tuned to remove information while preserving performance. This iscomputationally expensive and necessitates the storage of the whole dataset forthe lifetime of the model. Retraining-free approaches often utilise Fisherinformation, which is derived from the loss and requires labelled data whichmay not be available. Thus, we present an extension to the Selective SynapticDampening algorithm, substituting the diagonal of the Fisher information matrixfor the gradient of the l2 norm of the model output to approximate sensitivity.We evaluate our method in a range of experiments using ResNet18 and VisionTransformer. Results show our label-free method is competitive with existingstate-of-the-art approaches.</description><author>Jack Foster, Stefan Schoepf, Alexandra Brintrup</author><pubDate>Thu, 29 Feb 2024 16:15:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19308v1</guid></item><item><title>Convex Hulls of Reachable Sets</title><link>http://arxiv.org/abs/2303.17674v3</link><description>We study the convex hulls of reachable sets of nonlinear systems with boundeddisturbances and uncertain initial conditions. Reachable sets play a criticalrole in control, but remain notoriously challenging to compute, and existingover-approximation tools tend to be conservative or computationally expensive.In this work, we characterize the convex hulls of reachable sets as the convexhulls of solutions of an ordinary differential equation with initial conditionson the sphere. This finite-dimensional characterization unlocks an efficientsampling-based estimation algorithm to accurately over-approximate reachablesets. We also study the structure of the boundary of the reachable convex hullsand derive error bounds for the estimation algorithm. We give applications toneural feedback loop analysis and robust MPC.</description><author>Thomas Lew, Riccardo Bonalli, Marco Pavone</author><pubDate>Thu, 29 Feb 2024 16:14:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17674v3</guid></item><item><title>The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun</title><link>http://arxiv.org/abs/2402.10311v3</link><description>The word order of a sentence is shaped by multiple principles. The principleof syntactic dependency distance minimization is in conflict with the principleof surprisal minimization (or predictability maximization) in single headsyntactic dependency structures: while the former predicts that the head shouldbe placed at the center of the linear arrangement, the latter predicts that thehead should be placed at one of the ends (either first or last). A criticalquestion is when surprisal minimization (or predictability maximization) shouldsurpass syntactic dependency distance minimization. In the context of singlehead structures, it has been predicted that this is more likely to happen whentwo conditions are met, i.e. (a) fewer words are involved and (b) words areshorter. Here we test the prediction on the noun phrase when it is composed ofa demonstrative, a numeral, an adjective and a noun. We find that, acrosspreferred orders in languages, the noun tends to be placed at one of the ends,confirming the theoretical prediction. We also show evidence of anti localityeffects: syntactic dependency distances in preferred orders are longer thanexpected by chance.</description><author>Ramon Ferrer-i-Cancho</author><pubDate>Thu, 29 Feb 2024 16:13:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10311v3</guid></item><item><title>HyenaPixel: Global Image Context with Convolutions</title><link>http://arxiv.org/abs/2402.19305v1</link><description>In vision tasks, a larger effective receptive field (ERF) is associated withbetter performance. While attention natively supports global context,convolution requires multiple stacked layers and a hierarchical structure forlarge context. In this work, we extend Hyena, a convolution-based attentionreplacement, from causal sequences to the non-causal two-dimensional imagespace. We scale the Hyena convolution kernels beyond the feature map size up to191$\times$191 to maximize the ERF while maintaining sub-quadratic complexityin the number of pixels. We integrate our two-dimensional Hyena, HyenaPixel,and bidirectional Hyena into the MetaFormer framework. For imagecategorization, HyenaPixel and bidirectional Hyena achieve a competitiveImageNet-1k top-1 accuracy of 83.0% and 83.5%, respectively, whileoutperforming other large-kernel networks. Combining HyenaPixel with attentionfurther increases accuracy to 83.6%. We attribute the success of attention tothe lack of spatial bias in later stages and support this finding withbidirectional Hyena.</description><author>Julian Spravil, Sebastian Houben, Sven Behnke</author><pubDate>Thu, 29 Feb 2024 16:10:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19305v1</guid></item><item><title>RelayAttention for Efficient Large Language Model Serving with Long System Prompts</title><link>http://arxiv.org/abs/2402.14808v2</link><description>Practical large language model (LLM) services may involve a long systemprompt, which specifies the instructions, examples, and knowledge documents ofthe task and is reused across numerous requests. However, the long systemprompt causes throughput/latency bottlenecks as the cost of generating the nexttoken grows w.r.t. the sequence length. This paper aims to improve theefficiency of LLM services that involve long system prompts. Our keyobservation is that handling these system prompts requires heavily redundantmemory accesses in existing causal attention computation algorithms.Specifically, for batched requests, the cached hidden states (i.e., key-valuepairs) of system prompts are transferred from off-chip DRAM to on-chip SRAMmultiple times, each corresponding to an individual request. To eliminate sucha redundancy, we propose RelayAttention, an attention algorithm that allowsreading these hidden states from DRAM exactly once for a batch of input tokens.RelayAttention is a free lunch: it maintains the generation quality whilerequiring no model retraining, as it is based on a mathematical reformulationof causal attention. Code is available at\url{https://github.com/rayleizhu/vllm-ra}.</description><author>Lei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W. H. Lau</author><pubDate>Thu, 29 Feb 2024 16:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14808v2</guid></item><item><title>Learnability Gaps of Strategic Classification</title><link>http://arxiv.org/abs/2402.19303v1</link><description>In contrast with standard classification tasks, strategic classificationinvolves agents strategically modifying their features in an effort to receivefavorable predictions. For instance, given a classifier determining loanapproval based on credit scores, applicants may open or close their creditcards to fool the classifier. The learning goal is to find a classifier robustagainst strategic manipulations. Various settings, based on what and wheninformation is known, have been explored in strategic classification. In thiswork, we focus on addressing a fundamental question: the learnability gapsbetween strategic classification and standard learning. We essentially show that any learnable class is also strategically learnable:we first consider a fully informative setting, where the manipulation structure(which is modeled by a manipulation graph $G^\star$) is known and duringtraining time the learner has access to both the pre-manipulation data andpost-manipulation data. We provide nearly tight sample complexity and regretbounds, offering significant improvements over prior results. Then, we relaxthe fully informative setting by introducing two natural types of uncertainty.First, following Ahmadi et al. (2023), we consider the setting in which thelearner only has access to the post-manipulation data. We improve the resultsof Ahmadi et al. (2023) and close the gap between mistake upper bound and lowerbound raised by them. Our second relaxation of the fully informative settingintroduces uncertainty to the manipulation structure. That is, we assume thatthe manipulation graph is unknown but belongs to a known class of graphs. Weprovide nearly tight bounds on the learning complexity in various unknownmanipulation graph settings. Notably, our algorithm in this setting is ofindependent interest and can be applied to other problems such as multi-labellearning.</description><author>Lee Cohen, Yishay Mansour, Shay Moran, Han Shao</author><pubDate>Thu, 29 Feb 2024 16:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19303v1</guid></item><item><title>DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly</title><link>http://arxiv.org/abs/2402.19302v1</link><description>Reassembly tasks play a fundamental role in many fields and multipleapproaches exist to solve specific reassembly problems. In this context, weposit that a general unified model can effectively address them all,irrespective of the input data type (images, 3D, etc.). We introduceDiffAssemble, a Graph Neural Network (GNN)-based architecture that learns tosolve reassembly tasks using a diffusion model formulation. Our method treatsthe elements of a set, whether pieces of 2D patch or 3D object fragments, asnodes of a spatial graph. Training is performed by introducing noise into theposition and rotation of the elements and iteratively denoising them toreconstruct the coherent initial pose. DiffAssemble achieves state-of-the-art(SOTA) results in most 2D and 3D reassembly tasks and is the firstlearning-based approach that solves 2D puzzles for both rotation andtranslation. Furthermore, we highlight its remarkable reduction in run-time,performing 11 times faster than the quickest optimization-based method forpuzzle solving. Code available at https://github.com/IIT-PAVIS/DiffAssemble</description><author>Gianluca Scarpellini, Stefano Fiorini, Francesco Giuliari, Pietro Morerio, Alessio Del Bue</author><pubDate>Thu, 29 Feb 2024 16:09:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19302v1</guid></item><item><title>RL-GPT: Integrating Reinforcement Learning and Code-as-policy</title><link>http://arxiv.org/abs/2402.19299v1</link><description>Large Language Models (LLMs) have demonstrated proficiency in utilizingvarious tools by coding, yet they face limitations in handling intricate logicand precise control. In embodied tasks, high-level planning is amenable todirect coding, while low-level actions often necessitate task-specificrefinement, such as Reinforcement Learning (RL). To seamlessly integrate bothmodalities, we introduce a two-level hierarchical framework, RL-GPT, comprisinga slow agent and a fast agent. The slow agent analyzes actions suitable forcoding, while the fast agent executes coding tasks. This decompositioneffectively focuses each agent on specific tasks, proving highly efficientwithin our pipeline. Our approach outperforms traditional RL methods andexisting GPT agents, demonstrating superior efficiency. In the Minecraft game,it rapidly obtains diamonds within a single day on an RTX3090. Additionally, itachieves SOTA performance across all designated MineDojo tasks.</description><author>Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, Jiaya Jia</author><pubDate>Thu, 29 Feb 2024 16:07:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19299v1</guid></item><item><title>Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing</title><link>http://arxiv.org/abs/2402.19298v1</link><description>Face Anti-Spoofing (FAS) is crucial for securing face recognition systemsagainst presentation attacks. With advancements in sensor manufacture andmulti-modal learning techniques, many multi-modal FAS approaches have emerged.However, they face challenges in generalizing to unseen attacks and deploymentconditions. These challenges arise from (1) modality unreliability, where somemodality sensors like depth and infrared undergo significant domain shifts invarying environments, leading to the spread of unreliable information duringcross-modal feature fusion, and (2) modality imbalance, where training overlyrelies on a dominant modality hinders the convergence of others, reducingeffectiveness against attack types that are indistinguishable sorely using thedominant modality. To address modality unreliability, we propose theUncertainty-Guided Cross-Adapter (U-Adapter) to recognize unreliably detectedregions within each modality and suppress the impact of unreliable regions onother modalities. For modality imbalance, we propose a Rebalanced ModalityGradient Modulation (ReGrad) strategy to rebalance the convergence speed of allmodalities by adaptively adjusting their gradients. Besides, we provide thefirst large-scale benchmark for evaluating multi-modal FAS performance underdomain generalization scenarios. Extensive experiments demonstrate that ourmethod outperforms state-of-the-art methods. Source code and protocols will bereleased on https://github.com/OMGGGGG/mmdg.</description><author>Xun Lin, Shuai Wang, Rizhao Cai, Yizhong Liu, Ying Fu, Zitong Yu, Wenzhong Tang, Alex Kot</author><pubDate>Thu, 29 Feb 2024 16:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19298v1</guid></item><item><title>Unveiling Molecular Moieties through Hierarchical Graph Explainability</title><link>http://arxiv.org/abs/2402.01744v2</link><description>Background: Graph Neural Networks (GNN) have emerged in very recent years asa powerful tool for supporting in silico Virtual Screening. In this work wepresent a GNN which uses Graph Convolutional architectures to achieve veryaccurate multi-target screening. We also devised a hierarchical ExplainableArtificial Intelligence (XAI) technique to catch information directly at atom,ring, and whole molecule level by leveraging the message passing mechanism. Inthis way, we find the most relevant moieties involved in bioactivityprediction. Results: We report a state-of-the-art GNN classifier on twentyCyclin-dependent Kinase targets in support of VS. Our classifier outperformsprevious SOTA approaches proposed by the authors. Moreover, a CDK1-onlyhigh-sensitivity version of the GNN has been designed to use our explainer inorder to avoid the inherent bias of multi-class models. The hierarchicalexplainer has been validated by an expert chemist on 19 approved drugs on CDK1.Our explainer provided information in accordance to the docking analysis for 17out of the 19 test drugs. Conclusion: Our approach is a valid support forshortening both the screening and the hit-to-lead phase. Detailed knowledgeabout the molecular substructures that play a role in the inhibitory action,can help the computational chemist to gain insights into the pharmacophoricfunction of the molecule also for repurposing purposes.</description><author>Paolo Sortino, Salvatore Contino, Ugo Perricone, Roberto Pirrone</author><pubDate>Thu, 29 Feb 2024 16:05:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01744v2</guid></item><item><title>An AI based Digital Score of Tumour-Immune Microenvironment Predicts Benefit to Maintenance Immunotherapy in Advanced Oesophagogastric Adenocarcinoma</title><link>http://arxiv.org/abs/2402.19296v1</link><description>Gastric and oesophageal (OG) cancers are the leading causes of cancermortality worldwide. In OG cancers, recent studies have showed that PDL1 immunecheckpoint inhibitors (ICI) in combination with chemotherapy improves patientsurvival. However, our understanding of the tumour immune microenvironment inOG cancers remains limited. In this study, we interrogate multipleximmunofluorescence (mIF) images taken from patients with advancedOesophagogastric Adenocarcinoma (OGA) who received first-line fluoropyrimidineand platinum-based chemotherapy in the PLATFORM trial (NCT02678182) to predictthe efficacy of the treatment and to explore the biological basis of patientsresponding to maintenance durvalumab (PDL1 inhibitor). Our proposed ArtificialIntelligence (AI) based marker successfully identified responder fromnon-responder (p &lt; 0.05) as well as those who could potentially benefit fromICI with statistical significance (p &lt; 0.05) for both progression free andoverall survival. Our findings suggest that T cells that express FOXP3 seem toheavily influence the patient treatment response and survival outcome. We alsoobserved that higher levels of CD8+PD1+ cells are consistently linked to poorprognosis for both OS and PFS, regardless of ICI.</description><author>Quoc Dang Vu, Caroline Fong, Anderley Gordon, Tom Lund, Tatiany L Silveira, Daniel Rodrigues, Katharina von Loga, Shan E Ahmed Raza, David Cunningham, Nasir Rajpoot</author><pubDate>Thu, 29 Feb 2024 15:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19296v1</guid></item><item><title>Anomaly Detection in Offshore Wind Turbine Structures using Hierarchical Bayesian Modelling</title><link>http://arxiv.org/abs/2402.19295v1</link><description>Population-based structural health monitoring (PBSHM), aims to shareinformation between members of a population. An offshore wind (OW) farm couldbe considered as a population of nominally-identical wind-turbine structures.However, benign variations exist among members, such as geometry, sea-bedconditions and temperature differences. These factors could influencestructural properties and therefore the dynamic response, making it moredifficult to detect structural problems via traditional SHM techniques. Thispaper explores the use of a hierarchical Bayesian model to infer expected soilstiffness distributions at both population and local levels, as a basis toperform anomaly detection, in the form of scour, for new and existing turbines.To do this, observations of natural frequency will be generated as though theyare from a small population of wind turbines. Differences between individualobservations will be introduced by postulating distributions over the soilstiffness and measurement noise, as well as reducing soil depth (to representscour), in the case of anomaly detection.</description><author>S. M. Smith, A. J. Hughes, T. A. Dardeno, L. A. Bull, N. Dervilis, K. Worden</author><pubDate>Thu, 29 Feb 2024 15:58:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19295v1</guid></item><item><title>Degradation Modeling and Prognostic Analysis Under Unknown Failure Modes</title><link>http://arxiv.org/abs/2402.19294v1</link><description>Operating units often experience various failure modes in complex systems,leading to distinct degradation paths. Relying on a prognostic model trained ona single failure mode may lead to poor generalization performance acrossmultiple failure modes. Therefore, accurately identifying the failure mode isof critical importance. Current prognostic approaches either ignore failuremodes during degradation or assume known failure mode labels, which can bechallenging to acquire in practice. Moreover, the high dimensionality andcomplex relations of sensor signals make it challenging to identify the failuremodes accurately. To address these issues, we propose a novel failure modediagnosis method that leverages a dimension reduction technique called UMAP(Uniform Manifold Approximation and Projection) to project and visualize eachunit's degradation trajectory into a lower dimension. Then, using thesedegradation trajectories, we develop a time series-based clustering method toidentify the training units' failure modes. Finally, we introduce amonotonically constrained prognostic model to predict the failure mode labelsand RUL of the test units simultaneously using the obtained failure modes ofthe training units. The proposed prognostic model provides failuremode-specific RUL predictions while preserving the monotonic property of theRUL predictions across consecutive time steps. We evaluate the proposed modelusing a case study with the aircraft gas turbine engine dataset.</description><author>Ying Fu, Ye Kwon Huh, Kaibo Liu</author><pubDate>Thu, 29 Feb 2024 15:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19294v1</guid></item><item><title>Active propulsion noise shaping for multi-rotor aircraft localization</title><link>http://arxiv.org/abs/2402.17289v2</link><description>Multi-rotor aerial autonomous vehicles (MAVs) primarily rely on vision fornavigation purposes. However, visual localization and odometry techniquessuffer from poor performance in low or direct sunlight, a limited field ofview, and vulnerability to occlusions. Acoustic sensing can serve as acomplementary or even alternative modality for vision in many situations, andit also has the added benefits of lower system cost and energy footprint, whichis especially important for micro aircraft. This paper proposes activelycontrolling and shaping the aircraft propulsion noise generated by the rotorsto benefit localization tasks, rather than considering it a harmful nuisance.We present a neural network architecture for selfnoise-based localization in aknown environment. We show that training it simultaneously with learningtime-varying rotor phase modulation achieves accurate and robust localization.The proposed methods are evaluated using a computationally affordablesimulation of MAV rotor noise in 2D acoustic environments that is fitted toreal recordings of rotor pressure fields.</description><author>Gabriele Serussi, Tamir Shor, Tom Hirshberg, Chaim Baskin, Alex Bronstein</author><pubDate>Thu, 29 Feb 2024 15:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17289v2</guid></item><item><title>Estimation and Deconvolution of Second Order Cyclostationary Signals</title><link>http://arxiv.org/abs/2402.19290v1</link><description>This method solves the dual problem of blind deconvolution and estimation ofthe time waveform of noisy second-order cyclo-stationary (CS2) signals thattraverse a Transfer Function (TF) en route to a sensor. We have proven that thedeconvolution filter exists and eliminates the TF effect from signals whosestatistics vary over time. This method is blind, meaning it does not requireprior knowledge about the signals or TF. Simulations demonstrate the algorithmhigh precision across various signal types, TFs, and Signal-to-Noise Ratios(SNRs). In this study, the CS2 signals family is restricted to the product of adeterministic periodic function and white noise. Furthermore, this method hasthe potential to improve the training of Machine Learning models where theaggregation of signals from identical systems but with different TFs isrequired.</description><author>Igor Makienko, Michael Grebshtein, Eli Gildish</author><pubDate>Thu, 29 Feb 2024 15:53:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19290v1</guid></item><item><title>Does resistance to style-transfer equal Global Shape Bias? Measuring network sensitivity to global shape configuration</title><link>http://arxiv.org/abs/2310.07555v3</link><description>Deep learning models are known to exhibit a strong texture bias, while humantends to rely heavily on global shape structure for object recognition. Thecurrent benchmark for evaluating a model's global shape bias is a set ofstyle-transferred images with the assumption that resistance to the attack ofstyle transfer is related to the development of global structure sensitivity inthe model. In this work, we show that networks trained with style-transferimages indeed learn to ignore style, but its shape bias arises primarily fromlocal detail. We provide a \textbf{Disrupted Structure Testbench (DiST)} as adirect measurement of global structure sensitivity. Our test includes 2400original images from ImageNet-1K, each of which is accompanied by two imageswith the global shapes of the original image disrupted while preserving itstexture via the texture synthesis program. We found that \textcolor{black}{(1)models that performed well on the previous cue-conflict dataset do not farewell in the proposed DiST; (2) the supervised trained Vision Transformer (ViT)lose its global spatial information from positional embedding, leading to nosignificant advantages over Convolutional Neural Networks (CNNs) on DiST. Whileself-supervised learning methods, especially mask autoencoder significantlyimproves the global structure sensitivity of ViT. (3) Improving the globalstructure sensitivity is orthogonal to resistance to style-transfer, indicatingthat the relationship between global shape structure and local texture detailis not an either/or relationship. Training with DiST images andstyle-transferred images are complementary, and can be combined to trainnetwork together to enhance the global shape sensitivity and robustness oflocal features.} Our code will be hosted in github:https://github.com/leelabcnbc/DiST</description><author>Ziqi Wen, Tianqin Li, Zhi Jing, Tai Sing Lee</author><pubDate>Thu, 29 Feb 2024 15:53:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07555v3</guid></item><item><title>"It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models</title><link>http://arxiv.org/abs/2307.10811v3</link><description>Prewriting is the process of discovering and developing ideas before a firstdraft, which requires divergent thinking and often implies unstructuredstrategies such as diagramming, outlining, free-writing, etc. Although largelanguage models (LLMs) have been demonstrated to be useful for a variety oftasks including creative writing, little is known about how users wouldcollaborate with LLMs to support prewriting. The preferred collaborative roleand initiative of LLMs during such a creativity process is also unclear. Toinvestigate human-LLM collaboration patterns and dynamics during prewriting, weconducted a three-session qualitative study with 15 participants in twocreative tasks: story writing and slogan writing. The findings indicated thatduring collaborative prewriting, there appears to be a three-stage iterativeHuman-AI Co-creativity process that includes Ideation, Illumination, andImplementation stages. This collaborative process champions the human in adominant role, in addition to mixed and shifting levels of initiative thatexist between humans and LLMs. This research also reports on collaborationbreakdowns that occur during this process, user perceptions of using existingLLMs during Human-AI Co-creativity, and discusses design implications tosupport this co-creativity process.</description><author>Qian Wan, Siying Hu, Yu Zhang, Piaohong Wang, Bo Wen, Zhicong Lu</author><pubDate>Thu, 29 Feb 2024 15:53:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10811v3</guid></item><item><title>CAMixerSR: Only Details Need More "Attention"</title><link>http://arxiv.org/abs/2402.19289v1</link><description>To satisfy the rapidly increasing demands on the large image (2K-8K)super-resolution (SR), prevailing methods follow two independent tracks: 1)accelerate existing networks by content-aware routing, and 2) design bettersuper-resolution networks via token mixer refining. Despite directness, theyencounter unavoidable defects (e.g., inflexible route or non-discriminativeprocessing) limiting further improvements of quality-complexity trade-off. Toerase the drawbacks, we integrate these schemes by proposing a content-awaremixer (CAMixer), which assigns convolution for simple contexts and additionaldeformable window-attention for sparse textures. Specifically, the CAMixer usesa learnable predictor to generate multiple bootstraps, including offsets forwindows warping, a mask for classifying windows, and convolutional attentionsfor endowing convolution with the dynamic property, which modulates attentionto include more useful textures self-adaptively and improves the representationcapability of convolution. We further introduce a global classification loss toimprove the accuracy of predictors. By simply stacking CAMixers, we obtainCAMixerSR which achieves superior performance on large-image SR, lightweightSR, and omnidirectional-image SR.</description><author>Yan Wang, Shijie Zhao, Yi Liu, Junlin Li, Li Zhang</author><pubDate>Thu, 29 Feb 2024 15:52:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19289v1</guid></item><item><title>StiefelGen: A Simple, Model Agnostic Approach for Time Series Data Augmentation over Riemannian Manifolds</title><link>http://arxiv.org/abs/2402.19287v1</link><description>Data augmentation is an area of research which has seen active development inmany machine learning fields, such as in image-based learning models,reinforcement learning for self driving vehicles, and general noise injectionfor point cloud data. However, convincing methods for general time series dataaugmentation still leaves much to be desired, especially since the methodsdeveloped for these models do not readily cross-over. Three common approachesfor time series data augmentation include: (i) Constructing a physics-basedmodel and then imbuing uncertainty over the coefficient space (for example),(ii) Adding noise to the observed data set(s), and, (iii) Having access toample amounts of time series data sets from which a robust generative neuralnetwork model can be trained. However, for many practical problems that workwith time series data in the industry: (i) One usually does not have access toa robust physical model, (ii) The addition of noise can in of itself requirelarge or difficult assumptions (for example, what probability distributionshould be used? Or, how large should the noise variance be?), and, (iii) Inpractice, it can be difficult to source a large representative time series database with which to train the neural network model for the underlying problem.In this paper, we propose a methodology which attempts to simultaneously tackleall three of these previous limitations to a large extent. The method reliesupon the well-studied matrix differential geometry of the Stiefel manifold, asit proposes a simple way in which time series signals can placed on, and thensmoothly perturbed over the manifold. We attempt to clarify how this methodworks by showcasing several potential use cases which in particular work totake advantage of the unique properties of this underlying manifold.</description><author>Prasad Cheema, Mahito Sugiyama</author><pubDate>Thu, 29 Feb 2024 15:52:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19287v1</guid></item><item><title>PrPSeg: Universal Proposition Learning for Panoramic Renal Pathology Segmentation</title><link>http://arxiv.org/abs/2402.19286v1</link><description>Understanding the anatomy of renal pathology is crucial for advancing diseasediagnostics, treatment evaluation, and clinical research. The complex kidneysystem comprises various components across multiple levels, including regions(cortex, medulla), functional units (glomeruli, tubules), and cells (podocytes,mesangial cells in glomerulus). Prior studies have predominantly overlooked theintricate spatial interrelations among objects from clinical knowledge. In thisresearch, we introduce a novel universal proposition learning approach, calledpanoramic renal pathology segmentation (PrPSeg), designed to segmentcomprehensively panoramic structures within kidney by integrating extensiveknowledge of kidney anatomy. In this paper, we propose (1) the design of a comprehensive universalproposition matrix for renal pathology, facilitating the incorporation ofclassification and spatial relationships into the segmentation process; (2) atoken-based dynamic head single network architecture, with the improvement ofthe partial label image segmentation and capability for future dataenlargement; and (3) an anatomy loss function, quantifying the inter-objectrelationships across the kidney.</description><author>Ruining Deng, Quan Liu, Can Cui, Tianyuan Yao, Jialin Yue, Juming Xiong, Lining Yu, Yifei Wu, Mengmeng Yin, Yu Wang, Shilin Zhao, Yucheng Tang, Haichun Yang, Yuankai Huo</author><pubDate>Thu, 29 Feb 2024 15:51:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19286v1</guid></item><item><title>WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset</title><link>http://arxiv.org/abs/2402.19282v1</link><description>This paper presents WanJuan-CC, a safe and high-quality open-sourced Englishwebtext dataset derived from Common Crawl data. The study addresses thechallenges of constructing large-scale pre-training datasets for languagemodels, which require vast amounts of high-quality data. A comprehensiveprocess was designed to handle Common Crawl data, including extraction,heuristic rule filtering, fuzzy deduplication, content safety filtering, anddata quality filtering. From approximately 68 billion original Englishdocuments, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens ofhigh-quality data as part of WanJuan-CC. We have open-sourced 300B Tokens fromthis dataset. The paper also provides statistical information related to dataquality, enabling users to select appropriate data according to their needs. Toevaluate the quality and utility of the dataset, we trained 1B-parameter and3B-parameter models using WanJuan-CC and another dataset, RefinedWeb. Resultsshow that WanJuan-CC performs better on validation datasets and downstreamtasks.</description><author>Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia Yu, ChaoBin Zhang, Pei Chu, Yuan Qu, Runyu Peng, Zhiyuan Zeng, Huanze Tang, Ruiliang Xu, Wei Li, Hang Yan, Conghui He</author><pubDate>Thu, 29 Feb 2024 15:49:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19282v1</guid></item><item><title>SIFT-Aided Rectified 2D-DIC for Displacement and Strain Measurements in Asphalt Concrete Testing</title><link>http://arxiv.org/abs/2402.19279v1</link><description>Two-dimensional digital image correlation (2D-DIC) is a widely used opticaltechnique to measure displacement and strain during asphalt concrete (AC)testing. An accurate 2-D DIC measurement can only be achieved when the camera'sprincipal axis is perpendicular to the planar specimen surface. However, thisrequirement may not be met during testing due to device constraints. This paperproposes a simple and reliable method to correct errors induced bynon-perpendicularity. The method is based on image feature matching andrectification. No additional equipment is needed. A theoretical error analysiswas conducted to quantify the effect of a non-perpendicular camera alignment onmeasurement accuracy. The proposed method was validated numerically usingsynthetic images and experimentally in an AC fracture test. It achievedrelatively high accuracy, even under considerable camera rotation angle andlarge deformation. As a pre-processing technique, the proposed method showedpromising performance in assisting the recently developed CrackPropNet forautomated crack propagation measurement under a non-perpendicular cameraalignment.</description><author>Zehui Zhu, Imad L. Al-Qadi</author><pubDate>Thu, 29 Feb 2024 15:46:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19279v1</guid></item></channel></rss>