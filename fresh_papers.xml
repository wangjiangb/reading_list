<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 27 Sep 2024 01:00:40 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>LingoQA: Visual Question Answering for Autonomous Driving</title><link>http://arxiv.org/abs/2312.14115v4</link><description>We introduce LingoQA, a novel dataset and benchmark for visual questionanswering in autonomous driving. The dataset contains 28K unique short videoscenarios, and 419K annotations. Evaluating state-of-the-art vision-languagemodels on our benchmark shows that their performance is below humancapabilities, with GPT-4V responding truthfully to 59.6% of the questionscompared to 96.6% for humans. For evaluation, we propose a truthfulnessclassifier, called Lingo-Judge, that achieves a 0.95 Spearman correlationcoefficient to human evaluations, surpassing existing techniques like METEOR,BLEU, CIDEr, and GPT-4. We establish a baseline vision-language model and runextensive ablation studies to understand its performance. We release ourdataset and benchmark as an evaluation platform for vision-language models inautonomous driving.</description><author>Ana-Maria Marcu, Long Chen, Jan HÃ¼nermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, Elahe Arani, Oleg Sinavski</author><pubDate>Thu, 26 Sep 2024 15:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14115v4</guid></item><item><title>A Comprehensive Framework for Evaluating API-oriented Code Generation in Large Language Models</title><link>http://arxiv.org/abs/2409.15228v3</link><description>Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged aspowerful tools for code generation, significantly enhancing productivity andaccelerating software development. However, existing benchmarks primarily focuson general code generation without considering API-oriented code generation,i.e., generating code that invokes APIs from specific libraries. Given thegrowing demand for API-oriented code generation, there is a pressing need for asystematic and automated approach to evaluate LLM on API-oriented codegeneration. To address this gap, we propose AutoAPIEval, a lightweight andautomated framework designed to evaluate the capabilities of LLMs inAPI-oriented code generation. Our framework works with any library thatprovides API documentation and focuses on two unit tasks: API recommendationand code example generation, along with four metrics to evaluate the generatedAPIs and code examples, such as the proportion of incorrect API recommendationsfor Task 1, and the proportion of code examples where no specific API isinvoked and uncompilable/unexecutable code examples for Task 2. In addition, weconducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder)and Java Runtime Environment 8 to demonstrate the framework's effectiveness.Our findings reveal substantial variability in LLM performance across tasks,with ChatGPT adhering better to instructions, while sharing similareffectiveness in code example generation with its counterparts (i.e., MagiCoderand DeekSeek Coder). We also identify key factors associated with code quality,such as API popularity and model confidence, and build classifiers that achievehigh accuracy in detecting incorrect API recommendations and erroneous codeexamples. Retrieval-augmented generation enhances the quality of code generatedby LLMs, though its effectiveness varies across different LLMs.</description><author>Yixi Wu, Pengfei He, Zehao Wang, Shaowei Wang, Yuan Tian, Tse-Hsun Chen</author><pubDate>Thu, 26 Sep 2024 14:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15228v3</guid></item><item><title>Opponent Shaping for Antibody Development</title><link>http://arxiv.org/abs/2409.10588v5</link><description>Anti-viral therapies are typically designed to target the current strains ofa virus. Game theoretically, this corresponds to a short-sighted, or myopic,response. However, therapy-induced selective pressures act on viral antigens todrive the emergence of mutated strains, against which initial therapies havereduced efficacy. Building on a computational model of binding betweenantibodies and viral antigens (the Absolut! framework), we design and implementa genetic simulation of such viral evolutionary escape. Crucially, this allowsour antibody optimisation algorithm to consider and influence the entire escapecurve of the virus, i.e. to guide (or ''shape'') the viral evolution. This isinspired by opponent shaping which, in general-sum learning, accounts for theadaptation of the co-player rather than playing a myopic best response. Hencewe call the optimised antibodies shapers. Within our simulations, wedemonstrate that our shapers target both current and simulated future viralvariants, outperforming the antibodies chosen in a myopic way. Furthermore, weshow that shapers exert specific evolutionary pressure on the virus compared tomyopic antibodies. Altogether, shapers modify the evolutionary trajectories ofviral strains and minimise the viral escape compared to their myopiccounterparts. While this is a simplified model, we hope that our proposedparadigm will enable the discovery of better long-lived vaccines and antibodytherapies in the future, enabled by rapid advancements in the capabilities ofsimulation tools. Our code is available athttps://github.com/olakalisz/antibody-shapers.</description><author>Sebastian Towers, Aleksandra Kalisz, Philippe A. Robert, Alicia Higueruelo, Francesca Vianello, Ming-Han Chloe Tsai, Harrison Steel, Jakob N. Foerster</author><pubDate>Thu, 26 Sep 2024 14:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10588v5</guid></item><item><title>Characterizing stable regions in the residual stream of LLMs</title><link>http://arxiv.org/abs/2409.17113v2</link><description>We identify "stable regions" in the residual stream of Transformers, wherethe model's output remains insensitive to small activation changes, butexhibits high sensitivity at region boundaries. These regions emerge duringtraining and become more defined as training progresses or model sizeincreases. The regions appear to be much larger than previously studiedpolytopes. Our analysis suggests that these stable regions align with semanticdistinctions, where similar prompts cluster within regions, and activationsfrom the same region lead to similar next token predictions. This work providesa promising research direction for understanding the complexity of neuralnetworks, shedding light on training dynamics, and advancing interpretability.</description><author>Jett Janiak, Jacek Karwowski, Chatrik Singh Mangat, Giorgi Giglemiani, Nora Petrova, Stefan Heimersheim</author><pubDate>Thu, 26 Sep 2024 13:30:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17113v2</guid></item><item><title>Investigating OCR-Sensitive Neurons to Improve Entity Recognition in Historical Documents</title><link>http://arxiv.org/abs/2409.16934v2</link><description>This paper investigates the presence of OCR-sensitive neurons within theTransformer architecture and their influence on named entity recognition (NER)performance on historical documents. By analysing neuron activation patterns inresponse to clean and noisy text inputs, we identify and then neutraliseOCR-sensitive neurons to improve model performance. Based on two open accesslarge language models (Llama2 and Mistral), experiments demonstrate theexistence of OCR-sensitive regions and show improvements in NER performance onhistorical newspapers and classical commentaries, highlighting the potential oftargeted neuron modulation to improve models' performance on noisy text.</description><author>Emanuela Boros, Maud Ehrmann</author><pubDate>Thu, 26 Sep 2024 13:22:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16934v2</guid></item><item><title>Enhanced Unsupervised Image-to-Image Translation Using Contrastive Learning and Histogram of Oriented Gradients</title><link>http://arxiv.org/abs/2409.16042v2</link><description>Image-to-Image Translation is a vital area of computer vision that focuses ontransforming images from one visual domain to another while preserving theircore content and structure. However, this field faces two major challenges:first, the data from the two domains are often unpaired, making it difficult totrain generative adversarial networks effectively; second, existing methodstend to produce artifacts or hallucinations during image generation, leading toa decline in image quality. To address these issues, this paper proposes anenhanced unsupervised image-to-image translation method based on theContrastive Unpaired Translation (CUT) model, incorporating Histogram ofOriented Gradients (HOG) features. This novel approach ensures the preservationof the semantic structure of images, even without semantic labels, byminimizing the loss between the HOG features of input and generated images. Themethod was tested on translating synthetic game environments from GTA5 datasetto realistic urban scenes in cityscapes dataset, demonstrating significantimprovements in reducing hallucinations and enhancing image quality.</description><author>Wanchen Zhao</author><pubDate>Thu, 26 Sep 2024 13:18:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16042v2</guid></item><item><title>Scenario of Use Scheme: Threat Model Specification for Speaker Privacy Protection in the Medical Domain</title><link>http://arxiv.org/abs/2409.16106v2</link><description>Speech recordings are being more frequently used to detect and monitordisease, leading to privacy concerns. Beyond cryptography, protection of speechcan be addressed by approaches, such as perturbation, disentanglement, andre-synthesis, that eliminate sensitive information of the speaker, leaving theinformation necessary for medical analysis purposes. In order for such privacyprotective approaches to be developed, clear and systematic specifications ofassumptions concerning medical settings and the needs of medical professionalsare necessary. In this paper, we propose a Scenario of Use Scheme thatincorporates an Attacker Model, which characterizes the adversary against whomthe speaker's privacy must be defended, and a Protector Model, which specifiesthe defense. We discuss the connection of the scheme with previous work onspeech privacy. Finally, we present a concrete example of a specified Scenarioof Use and a set of experiments about protecting speaker data against genderinference attacks while maintaining utility for Parkinson's detection.</description><author>Mehtab Ur Rahman, Martha Larson, Louis ten Bosch, Cristian Tejedor-GarcÃ­a</author><pubDate>Thu, 26 Sep 2024 13:05:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16106v2</guid></item><item><title>Interpretable Vision-Language Survival Analysis with Ordinal Inductive Bias for Computational Pathology</title><link>http://arxiv.org/abs/2409.09369v3</link><description>Histopathology Whole-Slide Images (WSIs) provide an important tool to assesscancer prognosis in computational pathology (CPATH). While existing survivalanalysis (SA) approaches have made exciting progress, they are generallylimited to adopting highly-expressive architectures and only coarse-grainedpatient-level labels to learn prognostic visual representations from gigapixelWSIs. Such learning paradigm suffers from important performance bottlenecks,when facing present scarce training data and standard multi-instance learning(MIL) framework in CPATH. To overcome it, this paper, for the first time,proposes a new Vision-Language-based SA (VLSA) paradigm. Concretely, (1) VLSAis driven by pathology VL foundation models. It no longer relies onhigh-capability networks and shows the advantage of data efficiency. (2) Invision-end, VLSA encodes prognostic language prior and then employs it asauxiliary signals to guide the aggregating of prognostic visual features atinstance level, thereby compensating for the weak supervision in MIL. Moreover,given the characteristics of SA, we propose i) ordinal survival prompt learningto transform continuous survival labels into textual prompts; and ii) ordinalincidence function as prediction target to make SA compatible with VL-basedprediction. Notably, VLSA's predictions can be interpreted intuitively by ourShapley values-based method. The extensive experiments on five datasets confirmthe effectiveness of our scheme. Our VLSA could pave a new way for SA in CPATHby offering weakly-supervised MIL an effective means to learn valuableprognostic clues from gigapixel WSIs. Our source code is available athttps://github.com/liupei101/VLSA.</description><author>Pei Liu, Luping Ji, Jiaxiang Gou, Bo Fu, Mao Ye</author><pubDate>Thu, 26 Sep 2024 11:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09369v3</guid></item><item><title>In-Context Ensemble Improves Video-Language Models for Low-Level Workflow Understanding from Human Demonstrations</title><link>http://arxiv.org/abs/2409.15867v3</link><description>A Standard Operating Procedure (SOP) defines a low-level, step-by-stepwritten guide for a business software workflow based on a video demonstration.SOPs are a crucial step toward automating end-to-end software workflows.Manually creating SOPs can be time-consuming. Recent advancements in largevideo-language models offer the potential for automating SOP generation byanalyzing recordings of human demonstrations. However, current largevideo-language models face challenges with zero-shot SOP generation. We explorein-context learning with video-language models for SOP generation. We reportthat in-context learning sometimes helps video-language models at SOPgeneration. We then propose an in-context ensemble learning to further enhancethe capabilities of the models in SOP generation.</description><author>Moucheng Xu, Evangelos Chatzaroulas, Luc McCutcheon, Abdul Ahad, Hamzah Azeem, Janusz Marecki, Ammar Anwar</author><pubDate>Thu, 26 Sep 2024 09:56:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15867v3</guid></item><item><title>EAGLE: Towards Efficient Arbitrary Referring Visual Prompts Comprehension for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2409.16723v2</link><description>Recently, Multimodal Large Language Models (MLLMs) have sparked greatresearch interests owing to their exceptional content-reasoning andinstruction-following capabilities. To effectively instruct an MLLM, inaddition to conventional language expressions, the practice of referring toobjects by painting with brushes on images has emerged as a prevalent tool(referred to as "referring visual prompts") due to its efficacy in aligning theuser's intention with specific image regions. To accommodate the most commonreferring visual prompts, namely points, boxes, and masks, existing approachesinitially utilize specialized feature encoding modules to capture the semanticsof the highlighted areas indicated by these prompts. Subsequently, theseencoded region features are adapted to MLLMs through fine-tuning on ameticulously curated multimodal instruction dataset. However, such designssuffer from redundancy in architecture. Moreover, they face challenges ineffectively generalizing when encountering a diverse range of arbitraryreferring visual prompts in real-life scenarios. To address the above issues,we propose EAGLE, a novel MLLM that empowers comprehension of arbitraryreferring visual prompts with less training efforts than existing approaches.Specifically, our EAGLE maintains the innate format of the referring visualprompts as colored patches rendered on the given image for conducting theinstruction tuning. Our approach embeds referring visual prompts as spatialconcepts conveying specific spatial areas comprehensible to the MLLM, with thesemantic comprehension of these regions originating from the MLLM itself.Besides, we also propose a Geometry-Agnostic Learning paradigm (GAL) to furtherdisentangle the MLLM's region-level comprehension with the specific formats ofreferring visual prompts. Extensive experiments are conducted to prove theeffectiveness of our proposed method.</description><author>Jiacheng Zhang, Yang Jiao, Shaoxiang Chen, Jingjing Chen, Yu-Gang Jiang</author><pubDate>Thu, 26 Sep 2024 08:28:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16723v2</guid></item><item><title>Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs</title><link>http://arxiv.org/abs/2409.16341v2</link><description>Training large language models (LLMs) for external tool usage is a rapidlyexpanding field, with recent research focusing on generating synthetic data toaddress the shortage of available data. However, the absence of systematic dataquality checks poses complications for properly training and testing models. Tothat end, we propose two approaches for assessing the reliability of data fortraining LLMs to use external tools. The first approach uses intuitive,human-defined correctness criteria. The second approach uses a model-drivenassessment with in-context evaluation. We conduct a thorough evaluation of dataquality on two popular benchmarks, followed by an extrinsic evaluation thatshowcases the impact of data quality on model performance. Our resultsdemonstrate that models trained on high-quality data outperform those trainedon unvalidated data, even when trained with a smaller quantity of data. Thesefindings empirically support the significance of assessing and ensuring thereliability of training data for tool-using LLMs.</description><author>Shadi Iskander, Nachshon Cohen, Zohar Karnin, Ori Shapira, Sofia Tolmach</author><pubDate>Thu, 26 Sep 2024 07:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16341v2</guid></item><item><title>INT-FlashAttention: Enabling Flash Attention for INT8 Quantization</title><link>http://arxiv.org/abs/2409.16997v2</link><description>As the foundation of large language models (LLMs), self-attention modulefaces the challenge of quadratic time and memory complexity with respect tosequence length. FlashAttention accelerates attention computation and reducesits memory usage by leveraging the GPU memory hierarchy. A promising researchdirection is to integrate FlashAttention with quantization methods. This paperintroduces INT-FlashAttention, the first INT8 quantization architecturecompatible with the forward workflow of FlashAttention, which significantlyimproves the inference speed of FlashAttention on Ampere GPUs. We implement ourINT-FlashAttention prototype with fully INT8 activations and generalmatrix-multiplication (GEMM) kernels, making it the first attention operatorwith fully INT8 input. As a general token-level post-training quantizationframework, INT-FlashAttention is also compatible with other data formats likeINT4, etc. Experimental results show INT-FlashAttention achieves 72% fasterinference speed and 82% smaller quantization error compared to standardFlashAttention with FP16 and FP8 data format.</description><author>Shimao Chen, Zirui Liu, Zhiying Wu, Ce Zheng, Peizhuang Cong, Zihan Jiang, Yuhan Wu, Lei Su, Tong Yang</author><pubDate>Thu, 26 Sep 2024 06:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16997v2</guid></item><item><title>VideoPatchCore: An Effective Method to Memorize Normality for Video Anomaly Detection</title><link>http://arxiv.org/abs/2409.16225v2</link><description>Video anomaly detection (VAD) is a crucial task in video analysis andsurveillance within computer vision. Currently, VAD is gaining attention withmemory techniques that store the features of normal frames. The stored featuresare utilized for frame reconstruction, identifying an abnormality when asignificant difference exists between the reconstructed and input frames.However, this approach faces several challenges due to the simultaneousoptimization required for both the memory and encoder-decoder model. Thesechallenges include increased optimization difficulty, complexity ofimplementation, and performance variability depending on the memory size. Toaddress these challenges,we propose an effective memory method for VAD, calledVideoPatchCore. Inspired by PatchCore, our approach introduces a structure thatprioritizes memory optimization and configures three types of memory tailoredto the characteristics of video data. This method effectively addresses thelimitations of existing memory-based methods, achieving good performancecomparable to state-of-the-art methods. Furthermore, our method requires notraining and is straightforward to implement, making VAD tasks more accessible.Our code is available online at github.com/SkiddieAhn/Paper-VideoPatchCore.</description><author>Sunghyun Ahn, Youngwan Jo, Kijung Lee, Sanghyun Park</author><pubDate>Thu, 26 Sep 2024 06:04:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16225v2</guid></item><item><title>Regional quality estimation for echocardiography using deep learning</title><link>http://arxiv.org/abs/2408.00591v4</link><description>Automatic estimation of cardiac ultrasound image quality can be beneficialfor guiding operators and ensuring the accuracy of clinical measurements.Previous work often fails to distinguish the view correctness of theechocardiogram from the image quality. Additionally, previous studies onlyprovide a global image quality value, which limits their practical utility. Inthis work, we developed and compared three methods to estimate image quality:1) classic pixel-based metrics like the generalized contrast-to-noise ratio(gCNR) on myocardial segments as region of interest and left ventricle lumen asbackground, obtained using a U-Net segmentation 2) local image coherencederived from a U-Net model that predicts coherence from B-Mode images 3) a deepconvolutional network that predicts the quality of each region directly in anend-to-end fashion. We evaluate each method against manual regional imagequality annotations by three experienced cardiologists. The results indicatepoor performance of the gCNR metric, with Spearman correlation to theannotations of rho = 0.24. The end-to-end learning model obtains the bestresult, rho = 0.69, comparable to the inter-observer correlation, rho = 0.63.Finally, the coherence-based method, with rho = 0.58, outperformed theclassical metrics and is more generic than the end-to-end approach. The imagequality prediction tool is available as an open source Python library athttps://github.com/GillesVanDeVyver/arqee.</description><author>Gilles Van De Vyver, Svein-Erik MÃ¥sÃ¸y, HÃ¥vard Dalen, BjÃ¸rnar Leangen Grenne, Espen Holte, Sindre Hellum Olaisen, John Nyberg, Andreas Ãstvik, Lasse LÃ¸vstakken, Erik Smistad</author><pubDate>Thu, 26 Sep 2024 03:34:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00591v4</guid></item><item><title>Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving Human-AI Alignment in the Writing Process through Edits</title><link>http://arxiv.org/abs/2409.14509v3</link><description>LLM-based applications are helping people write, and LLM-generated text ismaking its way into social media, journalism, and our classrooms. However, thedifferences between LLM-generated and human-written text remain unclear. Toexplore this, we hired professional writers to edit paragraphs in severalcreative domains. We first found these writers agree on undesirableidiosyncrasies in LLM-generated text, formalizing it into a seven-categorytaxonomy (e.g. cliches, unnecessary exposition). Second, we curated the LAMPcorpus: 1,057 LLM-generated paragraphs edited by professional writers accordingto our taxonomy. Analysis of LAMP reveals that none of the LLMs used in ourstudy (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in termsof writing quality, revealing common limitations across model families. Third,we explored automatic editing methods to improve LLM-generated text. Alarge-scale preference annotation confirms that although experts largely prefertext edited by other experts, automatic editing methods show promise inimproving alignment between LLM-generated and human-written text.</description><author>Tuhin Chakrabarty, Philippe Laban, Chien-Sheng Wu</author><pubDate>Thu, 26 Sep 2024 03:15:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.14509v3</guid></item><item><title>Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models</title><link>http://arxiv.org/abs/2409.16663v2</link><description>We propose the use of latent space generative world models to address thecovariate shift problem in autonomous driving. A world model is a neuralnetwork capable of predicting an agent's next state given past states andactions. By leveraging a world model during training, the driving policyeffectively mitigates covariate shift without requiring an excessive amount oftraining data. During end-to-end training, our policy learns how to recoverfrom errors by aligning with states observed in human demonstrations, so thatat runtime it can recover from perturbations outside the training distribution.Additionally, we introduce a novel transformer-based perception encoder thatemploys multi-view cross-attention and a learned scene query. We presentqualitative and quantitative results, demonstrating significant improvementsupon prior state of the art in closed-loop testing in the CARLA simulator, aswell as showing the ability to handle perturbations in both CARLA and NVIDIA'sDRIVE Sim.</description><author>Alexander Popov, Alperen Degirmenci, David Wehr, Shashank Hegde, Ryan Oldja, Alexey Kamenev, Bertrand Douillard, David NistÃ©r, Urs Muller, Ruchi Bhargava, Stan Birchfield, Nikolai Smolyanskiy</author><pubDate>Thu, 26 Sep 2024 02:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16663v2</guid></item><item><title>HVT: A Comprehensive Vision Framework for Learning in Non-Euclidean Space</title><link>http://arxiv.org/abs/2409.16897v2</link><description>Data representation in non-Euclidean spaces has proven effective forcapturing hierarchical and complex relationships in real-world datasets.Hyperbolic spaces, in particular, provide efficient embeddings for hierarchicalstructures. This paper introduces the Hyperbolic Vision Transformer (HVT), anovel extension of the Vision Transformer (ViT) that integrates hyperbolicgeometry. While traditional ViTs operate in Euclidean space, our methodenhances the self-attention mechanism by leveraging hyperbolic distance andM\"obius transformations. This enables more effective modeling of hierarchicaland relational dependencies in image data. We present rigorous mathematicalformulations, showing how hyperbolic geometry can be incorporated intoattention layers, feed-forward networks, and optimization. We offer improvedperformance for image classification using the ImageNet dataset.</description><author>Jacob Fein-Ashley, Ethan Feng, Minh Pham</author><pubDate>Thu, 26 Sep 2024 02:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16897v2</guid></item><item><title>Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models</title><link>http://arxiv.org/abs/2409.17146v1</link><description>Today's most advanced multimodal models remain proprietary. The strongestopen-weight models rely heavily on synthetic data from proprietary VLMs toachieve good performance, effectively distilling these closed models into openones. As a result, the community is still missing foundational knowledge abouthow to build performant VLMs from scratch. We present Molmo, a new family ofVLMs that are state-of-the-art in their class of openness. Our key innovationis a novel, highly detailed image caption dataset collected entirely from humanannotators using speech-based descriptions. To enable a wide array of userinteractions, we also introduce a diverse dataset mixture for fine-tuning thatincludes in-the-wild Q&amp;A and innovative 2D pointing data. The success of ourapproach relies on careful choices for the model architecture details, awell-tuned training pipeline, and, most critically, the quality of our newlycollected datasets, all of which will be released. The best-in-class 72B modelwithin the Molmo family not only outperforms others in the class of open weightand data models but also compares favorably against proprietary systems likeGPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and humanevaluation. We will be releasing all of our model weights, captioning and fine-tuningdata, and source code in the near future. Select model weights, inference code,and demo are available at https://molmo.allenai.org.</description><author>Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi</author><pubDate>Wed, 25 Sep 2024 17:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17146v1</guid></item><item><title>DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion</title><link>http://arxiv.org/abs/2409.17145v1</link><description>Leveraging pretrained 2D diffusion models and score distillation sampling(SDS), recent methods have shown promising results for text-to-3D avatargeneration. However, generating high-quality 3D avatars capable of expressiveanimation remains challenging. In this work, we present DreamWaltz-G, a novellearning framework for animatable 3D avatar generation from text. The core ofthis framework lies in Skeleton-guided Score Distillation and Hybrid 3DGaussian Avatar representation. Specifically, the proposed skeleton-guidedscore distillation integrates skeleton controls from 3D human templates into 2Ddiffusion models, enhancing the consistency of SDS supervision in terms of viewand human pose. This facilitates the generation of high-quality avatars,mitigating issues such as multiple faces, extra limbs, and blurring. Theproposed hybrid 3D Gaussian avatar representation builds on the efficient 3DGaussians, combining neural implicit fields and parameterized 3D meshes toenable real-time rendering, stable SDS optimization, and expressive animation.Extensive experiments demonstrate that DreamWaltz-G is highly effective ingenerating and animating 3D avatars, outperforming existing methods in bothvisual quality and animation expressiveness. Our framework further supportsdiverse applications, including human video reenactment and multi-subject scenecomposition.</description><author>Yukun Huang, Jianan Wang, Ailing Zeng, Zheng-Jun Zha, Lei Zhang, Xihui Liu</author><pubDate>Wed, 25 Sep 2024 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17145v1</guid></item><item><title>Differential Privacy Regularization: Protecting Training Data Through Loss Function Regularization</title><link>http://arxiv.org/abs/2409.17144v1</link><description>Training machine learning models based on neural networks requires largedatasets, which may contain sensitive information. The models, however, shouldnot expose private information from these datasets. Differentially private SGD[DP-SGD] requires the modification of the standard stochastic gradient descent[SGD] algorithm for training new models. In this short paper, a novelregularization strategy is proposed to achieve the same goal in a moreefficient manner.</description><author>Francisco Aguilera-MartÃ­nez, Fernando Berzal</author><pubDate>Wed, 25 Sep 2024 17:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17144v1</guid></item><item><title>A Concise Mathematical Description of Active Inference in Discrete Time</title><link>http://arxiv.org/abs/2406.07726v2</link><description>In this paper we present a concise mathematical description of activeinference in discrete time. The main part of the paper serves as a basicintroduction to the topic, including a detailed example illustrating the theoryon action selection. In the appendix the more subtle mathematical details arediscussed. This part is aimed at readers who have already studied the activeinference literature but struggle to make sense of the mathematical details andderivations. Throughout the whole manuscript, special attention has been paidto adopting notation that is both precise and in line with standardmathematical texts. All equations and derivations are linked to specificequation numbers in other popular text on the topic. Furthermore, Python codeis provided that implements the action selection mechanism described in thispaper and is compatible with pymdp environments.</description><author>Jesse van Oostrum, Carlotta Langer, Nihat Ay</author><pubDate>Wed, 25 Sep 2024 17:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07726v2</guid></item><item><title>Attention Prompting on Image for Large Vision-Language Models</title><link>http://arxiv.org/abs/2409.17143v1</link><description>Compared with Large Language Models (LLMs), Large Vision-Language Models(LVLMs) can also accept images as input, thus showcasing more interestingemergent capabilities and demonstrating impressive performance on variousvision-language tasks. Motivated by text prompting in LLMs, visual promptinghas been explored to enhance LVLMs' capabilities of perceiving visualinformation. However, previous visual prompting techniques solely processvisual inputs without considering text queries, limiting the models' ability tofollow text instructions to complete tasks. To fill this gap, in this work, wepropose a new prompting technique named Attention Prompting on Image, whichjust simply overlays a text-query-guided attention heatmap on the originalinput image and effectively enhances LVLM on various tasks. Specifically, wegenerate an attention heatmap for the input image dependent on the text querywith an auxiliary model like CLIP. Then the heatmap simply multiplies the pixelvalues of the original image to obtain the actual input image for the LVLM.Extensive experiments on various vison-language benchmarks verify theeffectiveness of our technique. For example, Attention Prompting on Imageimproves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks,respectively.</description><author>Runpeng Yu, Weihao Yu, Xinchao Wang</author><pubDate>Wed, 25 Sep 2024 17:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17143v1</guid></item><item><title>FineZip : Pushing the Limits of Large Language Models for Practical Lossless Text Compression</title><link>http://arxiv.org/abs/2409.17141v1</link><description>While the language modeling objective has been shown to be deeply connectedwith compression, it is surprising that modern LLMs are not employed inpractical text compression systems. In this paper, we provide an in-depthanalysis of neural network and transformer-based compression techniques toanswer this question. We compare traditional text compression systems withneural network and LLM-based text compression methods. Although LLM-basedsystems significantly outperform conventional compression methods, they arehighly impractical. Specifically, LLMZip, a recent text compression systemusing Llama3-8B requires 9.5 days to compress just 10 MB of text, although withhuge improvements in compression ratios. To overcome this, we present FineZip -a novel LLM-based text compression system that combines ideas of onlinememorization and dynamic context to reduce the compression time immensely.FineZip can compress the above corpus in approximately 4 hours compared to 9.5days, a 54 times improvement over LLMZip and comparable performance. FineZipoutperforms traditional algorithmic compression methods with a large margin,improving compression ratios by approximately 50\%. With this work, we take thefirst step towards making lossless text compression with LLMs a reality. WhileFineZip presents a significant step in that direction, LLMs are still not aviable solution for large-scale text compression. We hope our work paves theway for future research and innovation to solve this problem.</description><author>Fazal Mittu, Yihuan Bu, Akshat Gupta, Ashok Devireddy, Alp Eren Ozdarendeli, Anant Singh, Gopala Anumanchipalli</author><pubDate>Wed, 25 Sep 2024 17:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17141v1</guid></item><item><title>Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation</title><link>http://arxiv.org/abs/2311.16201v2</link><description>Recent advances in image tokenizers, such as VQ-VAE, have enabledtext-to-image generation using auto-regressive methods, similar to languagemodeling. However, these methods have yet to leverage pre-trained languagemodels, despite their adaptability to various downstream tasks. In this work,we explore this gap by adapting a pre-trained language model forauto-regressive text-to-image generation, and find that pre-trained languagemodels offer limited help. We provide a two-fold explanation by analyzingtokens from each modality. First, we demonstrate that image tokens possesssignificantly different semantics compared to text tokens, renderingpre-trained language models no more effective in modeling them than randomlyinitialized ones. Second, the text tokens in the image-text datasets are toosimple compared to normal language model pre-training data, which causes thecatastrophic degradation of language models' capability.</description><author>Yuhui Zhang, Brandon McKinzie, Zhe Gan, Vaishaal Shankar, Alexander Toshev</author><pubDate>Wed, 25 Sep 2024 17:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16201v2</guid></item><item><title>Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents</title><link>http://arxiv.org/abs/2409.17140v1</link><description>Multimodal large language models (MLLMs) have enabled LLM-based agents todirectly interact with application user interfaces (UIs), enhancing agents'performance in complex tasks. However, these agents often suffer from highlatency and low reliability due to the extensive sequential UI interactions. Toaddress this issue, we propose AXIS, a novel LLM-based agents frameworkprioritize actions through application programming interfaces (APIs) over UIactions. This framework also facilitates the creation and expansion of APIsthrough automated exploration of applications. Our experiments on Office Worddemonstrate that AXIS reduces task completion time by 65%-70% and cognitiveworkload by 38%-53%, while maintaining accuracy of 97%-98% compare to humans.Our work contributes to a new human-agent-computer interaction (HACI) frameworkand a fresh UI design principle for application providers in the era of LLMs.It also explores the possibility of turning every applications into agents,paving the way towards an agent-centric operating system (Agent OS).</description><author>Junting Lu, Zhiyang Zhang, Fangkai Yang, Jue Zhang, Lu Wang, Chao Du, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang</author><pubDate>Wed, 25 Sep 2024 17:58:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17140v1</guid></item><item><title>Learning with Dynamics: Autonomous Regulation of UAV Based Communication Networks with Dynamic UAV Crew</title><link>http://arxiv.org/abs/2409.17139v1</link><description>Unmanned Aerial Vehicle (UAV) based communication networks (UCNs) are a keycomponent in future mobile networking. To handle the dynamic environments inUCNs, reinforcement learning (RL) has been a promising solution attributed toits strong capability of adaptive decision-making free of the environmentmodels. However, most existing RL-based research focus on control strategydesign assuming a fixed set of UAVs. Few works have investigated how UCNsshould be adaptively regulated when the serving UAVs change dynamically. Thisarticle discusses RL-based strategy design for adaptive UCN regulation given adynamic UAV set, addressing both reactive strategies in general UCNs andproactive strategies in solar-powered UCNs. An overview of the UCN and the RLframework is first provided. Potential research directions with key challengesand possible solutions are then elaborated. Some of our recent works arepresented as case studies to inspire innovative ways to handle dynamic UAV crewwith different RL algorithms.</description><author>Ran Zhang, Bowei Li, Liyuan Zhang, Jiang, Xie, Miao Wang</author><pubDate>Wed, 25 Sep 2024 17:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17139v1</guid></item><item><title>Landscape of Policy Optimization for Finite Horizon MDPs with General State and Action</title><link>http://arxiv.org/abs/2409.17138v1</link><description>Policy gradient methods are widely used in reinforcement learning. Yet, thenonconvexity of policy optimization imposes significant challenges inunderstanding the global convergence of policy gradient methods. For a class offinite-horizon Markov Decision Processes (MDPs) with general state and actionspaces, we develop a framework that provides a set of easily verifiableassumptions to ensure the Kurdyka-Lojasiewicz (KL) condition of the policyoptimization. Leveraging the KL condition, policy gradient methods converge tothe globally optimal policy with a non-asymptomatic rate despite nonconvexity.Our results find applications in various control and operations models,including entropy-regularized tabular MDPs, Linear Quadratic Regulator (LQR)problems, stochastic inventory models, and stochastic cash balance problems,for which we show an $\epsilon$-optimal policy can be obtained using a samplesize in $\tilde{\mathcal{O}}(\epsilon^{-1})$ and polynomial in terms of theplanning horizon by stochastic policy gradient methods. Our result establishesthe first sample complexity for multi-period inventory systems withMarkov-modulated demands and stochastic cash balance problems in theliterature.</description><author>Xin Chen, Yifan Hu, Minda Zhao</author><pubDate>Wed, 25 Sep 2024 17:56:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17138v1</guid></item><item><title>PACE: marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization</title><link>http://arxiv.org/abs/2409.17137v1</link><description>Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained visiontransformers to downstream tasks. However, the optimization for tasksperformance often comes at the cost of generalizability in fine-tuned models.To address this issue, we theoretically connect smaller weight gradient normsduring training and larger datasets to the improved model generalization.Motivated by this connection, we propose reducing gradient norms for enhancedgeneralization and aligning fine-tuned model with the pre-trained counterpartto retain knowledge from large-scale pre-training data. Yet, naive alignmentdoes not guarantee gradient reduction and can potentially cause gradientexplosion, complicating efforts to manage gradients. To address such issues, wepropose PACE, marrying generalization of PArameter-efficient fine-tuning withConsistency rEgularization. We perturb features learned from the adapter withthe multiplicative noise and ensure the fine-tuned model remains consistent forsame sample under different perturbations. Theoretical analysis shows that PACEnot only implicitly regularizes gradients for enhanced generalization, but alsoimplicitly aligns the fine-tuned and pre-trained models to retain knowledge.Experimental evidence supports our theories. PACE outperforms existing PEFTmethods in four visual adaptation tasks: VTAB-1k, FGVC, few-shot learning anddomain adaptation. Code will be available athttps://github.com/MaxwellYaoNi/PACE</description><author>Yao Ni, Shan Zhang, Piotr Koniusz</author><pubDate>Wed, 25 Sep 2024 17:56:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17137v1</guid></item><item><title>Simple Image Signal Processing using Global Context Guidance</title><link>http://arxiv.org/abs/2404.11569v2</link><description>In modern smartphone cameras, the Image Signal Processor (ISP) is the coreelement that converts the RAW readings from the sensor into perceptuallypleasant RGB images for the end users. The ISP is typically proprietary andhandcrafted and consists of several blocks such as white balance, colorcorrection, and tone mapping. Deep learning-based ISPs aim to transform RAWimages into DSLR-like RGB images using deep neural networks. However, mostlearned ISPs are trained using patches (small regions) due to computationallimitations. Such methods lack global context, which limits their efficacy onfull-resolution images and harms their ability to capture global propertiessuch as color constancy or illumination. First, we propose a novel module thatcan be integrated into any neural ISP to capture the global context informationfrom the full RAW images. Second, we propose an efficient and simple neural ISPthat utilizes our proposed module. Our model achieves state-of-the-art resultson different benchmarks using diverse and real smartphone images.</description><author>Omar Elezabi, Marcos V. Conde, Radu Timofte</author><pubDate>Wed, 25 Sep 2024 17:53:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11569v2</guid></item><item><title>Streaming Neural Images</title><link>http://arxiv.org/abs/2409.17134v1</link><description>Implicit Neural Representations (INRs) are a novel paradigm for signalrepresentation that have attracted considerable interest for image compression.INRs offer unprecedented advantages in signal resolution and memory efficiency,enabling new possibilities for compression techniques. However, the existinglimitations of INRs for image compression have not been sufficiently addressedin the literature. In this work, we explore the critical yet overlookedlimiting factors of INRs, such as computational cost, unstable performance, androbustness. Through extensive experiments and empirical analysis, we provide adeeper and more nuanced understanding of implicit neural image compressionmethods such as Fourier Feature Networks and Siren. Our work also offersvaluable insights for future research in this area.</description><author>Marcos V. Conde, Andy Bigos, Radu Timofte</author><pubDate>Wed, 25 Sep 2024 17:51:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17134v1</guid></item><item><title>Assessing the Level of Toxicity Against Distinct Groups in Bangla Social Media Comments: A Comprehensive Investigation</title><link>http://arxiv.org/abs/2409.17130v1</link><description>Social media platforms have a vital role in the modern world, serving asconduits for communication, the exchange of ideas, and the establishment ofnetworks. However, the misuse of these platforms through toxic comments, whichcan range from offensive remarks to hate speech, is a concerning issue. Thisstudy focuses on identifying toxic comments in the Bengali language targetingthree specific groups: transgender people, indigenous people, and migrantpeople, from multiple social media sources. The study delves into the intricateprocess of identifying and categorizing toxic language while considering thevarying degrees of toxicity: high, medium, and low. The methodology involvescreating a dataset, manual annotation, and employing pre-trained transformermodels like Bangla-BERT, bangla-bert-base, distil-BERT, andBert-base-multilingual-cased for classification. Diverse assessment metricssuch as accuracy, recall, precision, and F1-score are employed to evaluate themodel's effectiveness. The experimental findings reveal that Bangla-BERTsurpasses alternative models, achieving an F1-score of 0.8903. This researchexposes the complexity of toxicity in Bangla social media dialogues, revealingits differing impacts on diverse demographic groups.</description><author>Mukaffi Bin Moin, Pronay Debnath, Usafa Akther Rifa, Rijeet Bin Anis</author><pubDate>Wed, 25 Sep 2024 17:48:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17130v1</guid></item><item><title>Blox-Net: Generative Design-for-Robot-Assembly Using VLM Supervision, Physics Simulation, and a Robot with Reset</title><link>http://arxiv.org/abs/2409.17126v1</link><description>Generative AI systems have shown impressive capabilities in creating text,code, and images. Inspired by the rich history of research in industrial''Design for Assembly'', we introduce a novel problem: GenerativeDesign-for-Robot-Assembly (GDfRA). The task is to generate an assembly based ona natural language prompt (e.g., ''giraffe'') and an image of availablephysical components, such as 3D-printed blocks. The output is an assembly, aspatial arrangement of these components, and instructions for a robot to buildthis assembly. The output must 1) resemble the requested object and 2) bereliably assembled by a 6 DoF robot arm with a suction gripper. We then presentBlox-Net, a GDfRA system that combines generative vision language models withwell-established methods in computer vision, simulation, perturbation analysis,motion planning, and physical robot experimentation to solve a class of GDfRAproblems with minimal human supervision. Blox-Net achieved a Top-1 accuracy of63.5% in the ''recognizability'' of its designed assemblies (eg, resemblinggiraffe as judged by a VLM). These designs, after automated perturbationredesign, were reliably assembled by a robot, achieving near-perfect successacross 10 consecutive assembly iterations with human intervention only duringreset prior to assembly. Surprisingly, this entire design process from textualword (''giraffe'') to reliable physical assembly is performed with zero humanintervention.</description><author>Andrew Goldberg, Kavish Kondap, Tianshuang Qiu, Zehan Ma, Letian Fu, Justin Kerr, Huang Huang, Kaiyuan Chen, Kuan Fang, Ken Goldberg</author><pubDate>Wed, 25 Sep 2024 17:42:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17126v1</guid></item><item><title>On-orbit Servicing for Spacecraft Collision Avoidance With Autonomous Decision Making</title><link>http://arxiv.org/abs/2409.17125v1</link><description>This study develops an AI-based implementation of autonomous On-OrbitServicing (OOS) mission to assist with spacecraft collision avoidance maneuvers(CAMs). We propose an autonomous `servicer' trained with Reinforcement Learning(RL) to autonomously detect potential collisions between a target satellite andspace debris, rendezvous and dock with endangered satellites, and executeoptimal CAM. The RL model integrates collision risk estimates, satellitespecifications, and debris data to generate an optimal maneuver matrix for OOSrendezvous and collision prevention. We employ the Cross-Entropy algorithm tofind optimal decision policies efficiently. Initial results demonstrate thefeasibility of autonomous robotic OOS for collision avoidance services,focusing on one servicer spacecraft to one endangered satellite scenario.However, merging spacecraft rendezvous and optimal CAM presents significantcomplexities. We discuss design challenges and critical parameters for thesuccessful implementation of the framework presented through a case study.</description><author>Susmitha Patnala, Adam Abdin</author><pubDate>Wed, 25 Sep 2024 17:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17125v1</guid></item><item><title>Classification of Gleason Grading in Prostate Cancer Histopathology Images Using Deep Learning Techniques: YOLO, Vision Transformers, and Vision Mamba</title><link>http://arxiv.org/abs/2409.17122v1</link><description>Prostate cancer ranks among the leading health issues impacting men, with theGleason scoring system serving as the primary method for diagnosis andprognosis. This system relies on expert pathologists to evaluate samples ofprostate tissue and assign a Gleason grade, a task that requires significanttime and manual effort. To address this challenge, artificial intelligence (AI)solutions have been explored to automate the grading process. In light of thesechallenges, this study evaluates and compares the effectiveness of three deeplearning methodologies, YOLO, Vision Transformers, and Vision Mamba, inaccurately classifying Gleason grades from histopathology images. The goal isto enhance diagnostic precision and efficiency in prostate cancer management.This study utilized two publicly available datasets, Gleason2019 and SICAPv2,to train and test the performance of YOLO, Vision Transformers, and VisionMamba models. Each model was assessed based on its ability to classify Gleasongrades accurately, considering metrics such as false positive rate, falsenegative rate, precision, and recall. The study also examined the computationalefficiency and applicability of each method in a clinical setting. Vision Mambademonstrated superior performance across all metrics, achieving high precisionand recall rates while minimizing false positives and negatives. YOLO showedpromise in terms of speed and efficiency, particularly beneficial for real-timeanalysis. Vision Transformers excelled in capturing long-range dependencieswithin images, although they presented higher computational complexity comparedto the other models. Vision Mamba emerges as the most effective model forGleason grade classification in histopathology images, offering a balancebetween accuracy and computational efficiency.</description><author>Amin Malekmohammadi, Ali Badiezadeh, Seyed Mostafa Mirhassani, Parisa Gifani, Majid Vafaeezadeh</author><pubDate>Wed, 25 Sep 2024 17:36:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17122v1</guid></item><item><title>Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Handy Appetizer</title><link>http://arxiv.org/abs/2409.17120v1</link><description>This book explores the role of Artificial Intelligence (AI), Machine Learning(ML), and Deep Learning (DL) in driving the progress of big data analytics andmanagement. The book focuses on simplifying the complex mathematical conceptsbehind deep learning, offering intuitive visualizations and practical casestudies to help readers understand how neural networks and technologies likeConvolutional Neural Networks (CNNs) work. It introduces several classic modelsand technologies such as Transformers, GPT, ResNet, BERT, and YOLO,highlighting their applications in fields like natural language processing,image recognition, and autonomous driving. The book also emphasizes theimportance of pre-trained models and how they can enhance model performance andaccuracy, with instructions on how to apply these models in various real-worldscenarios. Additionally, it provides an overview of key big data managementtechnologies like SQL and NoSQL databases, as well as distributed computingframeworks such as Apache Hadoop and Spark, explaining their importance inmanaging and processing vast amounts of data. Ultimately, the book underscoresthe value of mastering deep learning and big data management skills as criticaltools for the future workforce, making it an essential resource for bothbeginners and experienced professionals.</description><author>Benji Peng, Xuanhe Pan, Yizhu Wen, Ziqian Bi, Keyu Chen, Ming Li, Ming Liu, Qian Niu, Junyu Liu, Jinlang Wang, Sen Zhang, Jiawei Xu, Pohsun Feng</author><pubDate>Wed, 25 Sep 2024 17:31:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17120v1</guid></item><item><title>Small data deep learning methodology for in-field disease detection</title><link>http://arxiv.org/abs/2409.17119v1</link><description>Early detection of diseases in crops is essential to prevent harvest lossesand improve the quality of the final product. In this context, the combinationof machine learning and proximity sensors is emerging as a technique capable ofachieving this detection efficiently and effectively. For example, this machinelearning approach has been applied to potato crops -- to detect late blight(Phytophthora infestans) -- and grapevine crops -- to detect downy mildew.However, most of these AI models found in the specialised literature have beendeveloped using leaf-by-leaf images taken in the lab, which does not representfield conditions and limits their applicability. In this study, we present the first machine learning model capable ofdetecting mild symptoms of late blight in potato crops through the analysis ofhigh-resolution RGB images captured directly in the field, overcoming thelimitations of other publications in the literature and presenting real-worldapplicability. Our proposal exploits the availability of high-resolution imagesvia the concept of patching, and is based on deep convolutional neural networkswith a focal loss function, which makes the model to focus on the complexpatterns that arise in field conditions. Additionally, we present a dataaugmentation scheme that facilitates the training of these neural networks withfew high-resolution images, which allows for development of models under thesmall data paradigm. Our model correctly detects all cases of late blight in the test dataset,demonstrating a high level of accuracy and effectiveness in identifying earlysymptoms. These promising results reinforce the potential use of machinelearning for the early detection of diseases and pests in agriculture, enablingbetter treatment and reducing their impact on crops.</description><author>David Herrera-Poyato, Jacinto DomÃ­nguez-Rull, Rosana Montes, InÃ©s HernÃ¡nde, Ignacio Barrio, Carlos Poblete-Echeverria, Javier Tardaguila, Francisco Herrera, AndrÃ©s Herrera-Poyatos</author><pubDate>Wed, 25 Sep 2024 17:31:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17119v1</guid></item><item><title>Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale</title><link>http://arxiv.org/abs/2409.17115v1</link><description>Large language model pre-training has traditionally relied on human expertsto craft heuristics for improving the corpora quality, resulting in numerousrules developed to date. However, these rules lack the flexibility to addressthe unique characteristics of individual example effectively. Meanwhile,applying tailored rules to every example is impractical for human experts. Inthis paper, we demonstrate that even small language models, with as few as 0.3Bparameters, can exhibit substantial data refining capabilities comparable tothose of human experts. We introduce Programming Every Example (ProX), a novelframework that treats data refinement as a programming task, enabling models torefine corpora by generating and executing fine-grained operations, such asstring normalization, for each individual example at scale. Experimentalresults show that models pre-trained on ProX-curated data outperform eitheroriginal data or data filtered by other selection methods by more than 2%across various downstream benchmarks. Its effectiveness spans various modelsizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb.Furthermore, ProX exhibits significant potential in domain-specific continualpre-training: without domain specific design, models trained on OpenWebMathrefined by ProX outperform human-crafted rule-based methods, improving averageaccuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% forCodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7Btrained on 200B tokens. Further analysis highlights that ProX significantlysaves training FLOPs, offering a promising path for efficient LLMpre-training.We are open-sourcing ProX with &gt;100B corpus, models, and sharingall training and implementation details for reproducible research and futureinnovation. Code: https://github.com/GAIR-NLP/ProX</description><author>Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, Pengfei Liu</author><pubDate>Wed, 25 Sep 2024 17:28:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17115v1</guid></item><item><title>Characterizing stable regions in the residual stream of LLMs</title><link>http://arxiv.org/abs/2409.17113v1</link><description>We identify "stable regions" in the residual stream of Transformers, wherethe model's output remains insensitive to small activation changes, butexhibits high sensitivity at region boundaries. These regions emerge duringtraining and become more defined as training progresses or model sizeincreases. The regions appear to be much larger than previously studiedpolytopes. Our analysis suggests that these stable regions align with semanticdistinctions, where similar prompts cluster within regions, and activationsfrom the same region lead to similar next token predictions.</description><author>Jett Janiak, Jacek Karwowski, Chatrik Singh Mangat, Giorgi Giglemiani, Nora Petrova, Stefan Heimersheim</author><pubDate>Wed, 25 Sep 2024 17:27:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17113v1</guid></item><item><title>MorphoSeg: An Uncertainty-Aware Deep Learning Method for Biomedical Segmentation of Complex Cellular Morphologies</title><link>http://arxiv.org/abs/2409.17110v1</link><description>Deep learning has revolutionized medical and biological imaging, particularlyin segmentation tasks. However, segmenting biological cells remains challengingdue to the high variability and complexity of cell shapes. Addressing thischallenge requires high-quality datasets that accurately represent the diversemorphologies found in biological cells. Existing cell segmentation datasets areoften limited by their focus on regular and uniform shapes. In this paper, weintroduce a novel benchmark dataset of Ntera-2 (NT2) cells, a pluripotentcarcinoma cell line, exhibiting diverse morphologies across multiple stages ofdifferentiation, capturing the intricate and heterogeneous cellular structuresthat complicate segmentation tasks. To address these challenges, we propose anuncertainty-aware deep learning framework for complex cellular morphologysegmentation (MorphoSeg) by incorporating sampling of virtual outliers fromlow-likelihood regions during training. Our comprehensive experimentalevaluations against state-of-the-art baselines demonstrate that MorphoSegsignificantly enhances segmentation accuracy, achieving up to a 7.74% increasein the Dice Similarity Coefficient (DSC) and a 28.36% reduction in theHausdorff Distance. These findings highlight the effectiveness of our datasetand methodology in advancing cell segmentation capabilities, especially forcomplex and variable cell morphologies. The dataset and source code is publiclyavailable at https://github.com/RanchoGoose/MorphoSeg.</description><author>Tianhao Zhang, Heather J. McCourty, Berardo M. Sanchez-Tafolla, Anton Nikolaev, Lyudmila S. Mihaylova</author><pubDate>Wed, 25 Sep 2024 17:25:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17110v1</guid></item><item><title>Unveiling Ontological Commitment in Multi-Modal Foundation Models</title><link>http://arxiv.org/abs/2409.17109v1</link><description>Ontological commitment, i.e., used concepts, relations, and assumptions, area corner stone of qualitative reasoning (QR) models. The state-of-the-art forprocessing raw inputs, though, are deep neural networks (DNNs), nowadays oftenbased off from multimodal foundation models. These automatically learn richrepresentations of concepts and respective reasoning. Unfortunately, thelearned qualitative knowledge is opaque, preventing easy inspection,validation, or adaptation against available QR models. So far, it is possibleto associate pre-defined concepts with latent representations of DNNs, butextractable relations are mostly limited to semantic similarity. As a next steptowards QR for validation and verification of DNNs: Concretely, we propose amethod that extracts the learned superclass hierarchy from a multimodal DNN fora given set of leaf concepts. Under the hood we (1) obtain leaf conceptembeddings using the DNN's textual input modality; (2) apply hierarchicalclustering to them, using that DNNs encode semantic similarities via vectordistances; and (3) label the such-obtained parent concepts using search inavailable ontologies from QR. An initial evaluation study shows that meaningfulontological class hierarchies can be extracted from state-of-the-art foundationmodels. Furthermore, we demonstrate how to validate and verify a DNN's learnedrepresentations against given ontologies. Lastly, we discuss potential futureapplications in the context of QR.</description><author>Mert Keser, Gesina Schwalbe, Niki Amini-Naieni, Matthias Rottmann, Alois Knoll</author><pubDate>Wed, 25 Sep 2024 17:24:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17109v1</guid></item><item><title>Non-asymptotic convergence analysis of the stochastic gradient Hamiltonian Monte Carlo algorithm with discontinuous stochastic gradient with applications to training of ReLU neural networks</title><link>http://arxiv.org/abs/2409.17107v1</link><description>In this paper, we provide a non-asymptotic analysis of the convergence of thestochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm to a targetmeasure in Wasserstein-1 and Wasserstein-2 distance. Crucially, compared to theexisting literature on SGHMC, we allow its stochastic gradient to bediscontinuous. This allows us to provide explicit upper bounds, which can becontrolled to be arbitrarily small, for the expected excess risk of non-convexstochastic optimization problems with discontinuous stochastic gradients,including, among others, the training of neural networks with ReLU activationfunction. To illustrate the applicability of our main results, we considernumerical experiments on quantile estimation and on several optimizationproblems involving ReLU neural networks relevant in finance and artificialintelligence.</description><author>Luxu Liang, Ariel Neufeld, Ying Zhang</author><pubDate>Wed, 25 Sep 2024 17:21:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17107v1</guid></item><item><title>Text2CAD: Generating Sequential CAD Models from Beginner-to-Expert Level Text Prompts</title><link>http://arxiv.org/abs/2409.17106v1</link><description>Prototyping complex computer-aided design (CAD) models in modern softwarescan be very time-consuming. This is due to the lack of intelligent systems thatcan quickly generate simpler intermediate parts. We propose Text2CAD, the firstAI framework for generating text-to-parametric CAD models usingdesigner-friendly instructions for all skill levels. Furthermore, we introducea data annotation pipeline for generating text prompts based on naturallanguage instructions for the DeepCAD dataset using Mistral and LLaVA-NeXT. Thedataset contains $\sim170$K models and $\sim660$K text annotations, fromabstract CAD descriptions (e.g., generate two concentric cylinders) to detailedspecifications (e.g., draw two circles with center $(x,y)$ and radius $r_{1}$,$r_{2}$, and extrude along the normal by $d$...). Within the Text2CADframework, we propose an end-to-end transformer-based auto-regressive networkto generate parametric CAD models from input texts. We evaluate the performanceof our model through a mixture of metrics, including visual quality, parametricprecision, and geometrical accuracy. Our proposed framework shows greatpotential in AI-aided design applications. Our source code and annotations willbe publicly available.</description><author>Mohammad Sadil Khan, Sankalp Sinha, Talha Uddin Sheikh, Didier Stricker, Sk Aziz Ali, Muhammad Zeshan Afzal</author><pubDate>Wed, 25 Sep 2024 17:19:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17106v1</guid></item><item><title>Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay Networks With Learnable Delay Lines</title><link>http://arxiv.org/abs/2404.00082v3</link><description>Over the past few decades, extensive research has been devoted to the designof artificial reverberation algorithms aimed at emulating the room acoustics ofphysical environments. Despite significant advancements, automatic parametertuning of delay-network models remains an open challenge. We introduce a novelmethod for finding the parameters of a Feedback Delay Network (FDN) such thatits output renders target attributes of a measured room impulse response. Theproposed approach involves the implementation of a differentiable FDN withtrainable delay lines, which, for the first time, allows us to simultaneouslylearn each and every delay-network parameter via backpropagation. The iterativeoptimization process seeks to minimize a perceptually-motivated time-domainloss function incorporating differentiable terms accounting for energy decayand echo density. Through experimental validation, we show that the proposedmethod yields time-invariant frequency-independent FDNs capable of closelymatching the desired acoustical characteristics, and outperforms existingmethods based on genetic algorithms and analytical FDN design.</description><author>Alessandro Ilic Mezza, Riccardo Giampiccolo, Enzo De Sena, Alberto Bernardini</author><pubDate>Wed, 25 Sep 2024 17:16:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00082v3</guid></item><item><title>LingoQA: Video Question Answering for Autonomous Driving</title><link>http://arxiv.org/abs/2312.14115v3</link><description>We introduce LingoQA, a novel dataset and benchmark for visual questionanswering in autonomous driving. The dataset contains 28K unique short videoscenarios, and 419K annotations. Evaluating state-of-the-art vision-languagemodels on our benchmark shows that their performance is below humancapabilities, with GPT-4V responding truthfully to 59.6% of the questionscompared to 96.6% for humans. For evaluation, we propose a truthfulnessclassifier, called Lingo-Judge, that achieves a 0.95 Spearman correlationcoefficient to human evaluations, surpassing existing techniques like METEOR,BLEU, CIDEr, and GPT-4. We establish a baseline vision-language model and runextensive ablation studies to understand its performance. We release ourdataset and benchmark https://github.com/wayveai/LingoQA as an evaluationplatform for vision-language models in autonomous driving.</description><author>Ana-Maria Marcu, Long Chen, Jan HÃ¼nermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, Elahe Arani, Oleg Sinavski</author><pubDate>Wed, 25 Sep 2024 17:14:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14115v3</guid></item><item><title>General Detection-based Text Line Recognition</title><link>http://arxiv.org/abs/2409.17095v1</link><description>We introduce a general detection-based approach to text line recognition, beit printed (OCR) or handwritten (HTR), with Latin, Chinese, or cipheredcharacters. Detection-based approaches have until now been largely discardedfor HTR because reading characters separately is often challenging, andcharacter-level annotation is difficult and expensive. We overcome thesechallenges thanks to three main insights: (i) synthetic pre-training withsufficiently diverse data enables learning reasonable character localizationfor any script; (ii) modern transformer-based detectors can jointly detect alarge number of instances, and, if trained with an adequate masking strategy,leverage consistency between the different detections; (iii) once a pre-traineddetection model with approximate character localization is available, it ispossible to fine-tune it with line-level annotation on real data, even with adifferent alphabet. Our approach, dubbed DTLR, builds on a completely differentparadigm than state-of-the-art HTR methods, which rely on autoregressivedecoding, predicting character values one by one, while we treat a completeline in parallel. Remarkably, we demonstrate good performance on a large rangeof scripts, usually tackled with specialized approaches. In particular, weimprove state-of-the-art performances for Chinese script recognition on theCASIA v2 dataset, and for cipher recognition on the Borg and Copiale datasets.Our code and models are available at https://github.com/raphael-baena/DTLR.</description><author>Raphael Baena, Syrine Kalleli, Mathieu Aubry</author><pubDate>Wed, 25 Sep 2024 17:05:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17095v1</guid></item><item><title>BitQ: Tailoring Block Floating Point Precision for Improved DNN Efficiency on Resource-Constrained Devices</title><link>http://arxiv.org/abs/2409.17093v1</link><description>Deep neural networks (DNNs) are powerful for cognitive tasks such as imageclassification, object detection, and scene segmentation. One drawback howeveris the significant high computational complexity and memory consumption, whichmakes them unfeasible to run real-time on embedded platforms because of thelimited hardware resources. Block floating point (BFP) quantization is one ofthe representative compression approaches for reducing the memory andcomputational burden owing to their capability to effectively capture the broaddata distribution of DNN models. Unfortunately, prior works on BFP-basedquantization empirically choose the block size and the precision that preserveaccuracy. In this paper, we develop a BFP-based bitwidth-aware analyticalmodeling framework (called ``BitQ'') for the best BFP implementation of DNNinference on embedded platforms. We formulate and resolve an optimizationproblem to identify the optimal BFP block size and bitwidth distribution by thetrade-off of both accuracy and performance loss. Experimental results show thatcompared with an equal bitwidth setting, the BFP DNNs with optimized bitwidthallocation provide efficient computation, preserving accuracy on famousbenchmarks. The source code and data are available athttps://github.com/Cheliosoops/BitQ.</description><author>Yongqi Xu, Yujian Lee, Gao Yi, Bosheng Liu, Yucong Chen, Peng Liu, Jigang Wu, Xiaoming Chen, Yinhe Han</author><pubDate>Wed, 25 Sep 2024 17:03:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17093v1</guid></item><item><title>Unified Embedding Based Personalized Retrieval in Etsy Search</title><link>http://arxiv.org/abs/2306.04833v2</link><description>Embedding-based neural retrieval is a prevalent approach to address thesemantic gap problem which often arises in product search on tail queries. Incontrast, popular queries typically lack context and have a broad intent whereadditional context from users historical interaction can be helpful. In thispaper, we share our novel approach to address both: the semantic gap problemfollowed by an end to end trained model for personalized semantic retrieval. Wepropose learning a unified embedding model incorporating graph, transformer andterm-based embeddings end to end and share our design choices for optimaltradeoff between performance and efficiency. We share our learnings in featureengineering, hard negative sampling strategy, and application of transformermodel, including a novel pre-training strategy and other tricks for improvingsearch relevance and deploying such a model at industry scale. Our personalizedretrieval model significantly improves the overall search experience, asmeasured by a 5.58% increase in search purchase rate and a 2.63% increase insite-wide conversion rate, aggregated across multiple A/B tests - on livetraffic.</description><author>Rishikesh Jha, Siddharth Subramaniyam, Ethan Benjamin, Thrivikrama Taula</author><pubDate>Wed, 25 Sep 2024 17:01:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04833v2</guid></item><item><title>Accumulator-Aware Post-Training Quantization</title><link>http://arxiv.org/abs/2409.17092v1</link><description>Several recent studies have investigated low-precision accumulation,reporting improvements in throughput, power, and area across various platforms.However, the accompanying proposals have only considered the quantization-awaretraining (QAT) paradigm, in which models are fine-tuned or trained from scratchwith quantization in the loop. As models continue to grow in size, QATtechniques become increasingly more expensive, which has motivated the recentsurge in post-training quantization (PTQ) research. To the best of ourknowledge, ours marks the first formal study of accumulator-aware quantizationin the PTQ setting. To bridge this gap, we introduce AXE, a practical frameworkof accumulator-aware extensions designed to endow overflow avoidance guaranteesto existing layer-wise PTQ algorithms. We theoretically motivate AXE anddemonstrate its flexibility by implementing it on top of two state-of-the-artPTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stageaccumulation for the first time, opening the door for full datapathoptimization and scaling to large language models (LLMs). We evaluate AXEacross image classification and language generation models, and observesignificant improvements in the trade-off between accumulator bit width andmodel accuracy over baseline methods.</description><author>Ian Colbert, Fabian Grob, Giuseppe Franco, Jinjie Zhang, Rayan Saab</author><pubDate>Wed, 25 Sep 2024 16:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17092v1</guid></item><item><title>Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence Classification</title><link>http://arxiv.org/abs/2409.17091v1</link><description>In the medical field, the limited availability of large-scale datasets andlabor-intensive annotation processes hinder the performance of deep models.Diffusion-based generative augmentation approaches present a promising solutionto this issue, having been proven effective in advancing downstream medicalrecognition tasks. Nevertheless, existing works lack sufficient semantic andsequential steerability for challenging video/3D sequence generation, andneglect quality control of noisy synthesized samples, resulting in unreliablesynthetic databases and severely limiting the performance of downstream tasks.In this work, we present Ctrl-GenAug, a novel and general generativeaugmentation framework that enables highly semantic- and sequential-customizedsequence synthesis and suppresses incorrectly synthesized samples, to aidmedical sequence classification. Specifically, we first design a multimodalconditions-guided sequence generator for controllably synthesizingdiagnosis-promotive samples. A sequential augmentation module is integrated toenhance the temporal/stereoscopic coherence of generated samples. Then, wepropose a noisy synthetic data filter to suppress unreliable cases at semanticand sequential levels. Extensive experiments on 3 medical datasets, using 11networks trained on 3 paradigms, comprehensively analyze the effectiveness andgenerality of Ctrl-GenAug, particularly in underrepresented high-riskpopulations and out-domain conditions.</description><author>Xinrui Zhou, Yuhao Huang, Haoran Dou, Shijing Chen, Ao Chang, Jia Liu, Weiran Long, Jian Zheng, Erjiao Xu, Jie Ren, Ruobing Huang, Jun Cheng, Wufeng Xue, Dong Ni</author><pubDate>Wed, 25 Sep 2024 16:58:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17091v1</guid></item><item><title>Locally Regularized Sparse Graph by Fast Proximal Gradient Descent</title><link>http://arxiv.org/abs/2409.17090v1</link><description>Sparse graphs built by sparse representation has been demonstrated to beeffective in clustering high-dimensional data. Albeit the compelling empiricalperformance, the vanilla sparse graph ignores the geometric information of thedata by performing sparse representation for each datum separately. In order toobtain a sparse graph aligned with the local geometric structure of data, wepropose a novel Support Regularized Sparse Graph, abbreviated as SRSG, for dataclustering. SRSG encourages local smoothness on the neighborhoods of nearbydata points by a well-defined support regularization term. We propose a fastproximal gradient descent method to solve the non-convex optimization problemof SRSG with the convergence matching the Nesterov's optimal convergence rateof first-order methods on smooth and convex objective function with Lipschitzcontinuous gradient. Extensive experimental results on various real data setsdemonstrate the superiority of SRSG over other competing clustering methods.</description><author>Dongfang Sun, Yingzhen Yang</author><pubDate>Wed, 25 Sep 2024 16:57:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17090v1</guid></item><item><title>Benchmarking Cognitive Biases in Large Language Models as Evaluators</title><link>http://arxiv.org/abs/2309.17012v3</link><description>Large Language Models are cognitively biased judges. Large Language Models(LLMs) have recently been shown to be effective as automatic evaluators withsimple prompting and in-context learning. In this work, we assemble 15 LLMs offour different size ranges and evaluate their output responses by preferenceranking from the other LLMs as evaluators, such as System Star is better thanSystem Square. We then evaluate the quality of ranking outputs introducing theCognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark tomeasure six different cognitive biases in LLM evaluation outputs, such as theEgocentric bias where a model prefers to rank its own outputs highly inevaluation. We find that LLMs are biased text quality evaluators, exhibitingstrong indications on our bias benchmark (average of 40% of comparisons acrossall models) within each of their evaluations that question their robustness asevaluators. Furthermore, we examine the correlation between human and machinepreferences and calculate the average Rank-Biased Overlap (RBO) score to be49.6%, indicating that machine preferences are misaligned with humans.According to our findings, LLMs may still be unable to be utilized forautomatic annotation aligned with human preferences. Our project page is at:https://minnesotanlp.github.io/cobbler.</description><author>Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, Dongyeop Kang</author><pubDate>Wed, 25 Sep 2024 16:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17012v3</guid></item><item><title>Stochastic Multi-round Submodular Optimization with Budget</title><link>http://arxiv.org/abs/2404.13737v4</link><description>In this work, we study the Stochastic Budgeted Multi-round SubmodularMaximization (SBMSm) problem, where we aim to adaptively maximize the sum, overmultiple rounds, of a monotone and submodular objective function defined onsubsets of items. The objective function also depends on the realization ofstochastic events, and the total number of items we can select over all roundsis bounded by a limited budget. This problem extends, and generalizes tomultiple round settings, well-studied problems such as (adaptive) influencemaximization and stochastic probing. We show that, if the number of items and stochastic events is somehowbounded, there is a polynomial time dynamic programming algorithm for SBMSm.Then, we provide a simple greedy $1/2(1-1/e-\epsilon)\approx0.316$-approximation algorithm for SBMSm, that first non-adaptively allocatesthe budget to be spent at each round, and then greedily and adaptivelymaximizes the objective function by using the budget assigned at each round.Finally, we introduce the {\em budget-adaptivity gap}, by which we measure howmuch an adaptive policy for SBMSm is better than an optimal partially adaptiveone that, as in our greedy algorithm, determines the budget allocation inadvance. We show that the budget-adaptivity gap lies between $e/(e-1)\approx1.582$ and $2$.</description><author>Vincenzo Auletta, Diodato Ferraioli, Cosimo Vinci</author><pubDate>Wed, 25 Sep 2024 16:53:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13737v4</guid></item><item><title>SEN12-WATER: A New Dataset for Hydrological Applications and its Benchmarking</title><link>http://arxiv.org/abs/2409.17087v1</link><description>Climate change and increasing droughts pose significant challenges to waterresource management around the world. These problems lead to severe watershortages that threaten ecosystems, agriculture, and human communities. Toadvance the fight against these challenges, we present a new dataset,SEN12-WATER, along with a benchmark using a novel end-to-end Deep Learning (DL)framework for proactive drought-related analysis. The dataset, identified as aspatiotemporal datacube, integrates SAR polarization, elevation, slope, andmultispectral optical bands. Our DL framework enables the analysis andestimation of water losses over time in reservoirs of interest, revealingsignificant insights into water dynamics for drought analysis by examiningtemporal changes in physical quantities such as water volume. Our methodologytakes advantage of the multitemporal and multimodal characteristics of theproposed dataset, enabling robust generalization and advancing understanding ofdrought, contributing to climate change resilience and sustainable waterresource management. The proposed framework involves, among the severalcomponents, speckle noise removal from SAR data, a water body segmentationthrough a U-Net architecture, the time series analysis, and the predictivecapability of a Time-Distributed-Convolutional Neural Network (TD-CNN). Resultsare validated through ground truth data acquired on-ground via dedicatedsensors and (tailored) metrics, such as Precision, Recall, Intersection overUnion, Mean Squared Error, Structural Similarity Index Measure and PeakSignal-to-Noise Ratio.</description><author>Luigi Russo, Francesco Mauro, Alessandro Sebastianelli, Paolo Gamba, Silvia Liberata Ullo</author><pubDate>Wed, 25 Sep 2024 16:50:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17087v1</guid></item><item><title>Scalable Learning of Segment-Level Traffic Congestion Functions</title><link>http://arxiv.org/abs/2405.06080v2</link><description>We propose and study a data-driven framework for identifying trafficcongestion functions (numerical relationships between observations of trafficvariables) at global scale and segment-level granularity. In contrast tomethods that estimate a separate set of parameters for each roadway, ourslearns a single black-box function over all roadways in a metropolitan area.First, we pool traffic data from all segments into one dataset, combiningstatic attributes with dynamic time-dependent features. Second, we train afeed-forward neural network on this dataset, which we can then use on anysegment in the area. We evaluate how well our framework identifies congestionfunctions on observed segments and how it generalizes to unobserved segmentsand predicts segment attributes on a large dataset covering multiple citiesworldwide. For identification error on observed segments, our singledata-driven congestion function compares favorably to segment-specificmodel-based functions on highway roads, but has room to improve on arterialroads. For generalization, our approach shows strong performance across citiesand road types: both on unobserved segments in the same city and on zero-shottransfer learning between cities. Finally, for predicting segment attributes,we find that our approach can approximate critical densities for individualsegments using their static properties.</description><author>Shushman Choudhury, Abdul Rahman Kreidieh, Iveel Tsogsuren, Neha Arora, Carolina Osorio, Alexandre Bayen</author><pubDate>Wed, 25 Sep 2024 16:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06080v2</guid></item><item><title>Parameter-efficient Bayesian Neural Networks for Uncertainty-aware Depth Estimation</title><link>http://arxiv.org/abs/2409.17085v1</link><description>State-of-the-art computer vision tasks, like monocular depth estimation(MDE), rely heavily on large, modern Transformer-based architectures. However,their application in safety-critical domains demands reliable predictiveperformance and uncertainty quantification. While Bayesian neural networksprovide a conceptually simple approach to serve those requirements, they sufferfrom the high dimensionality of the parameter space. Parameter-efficientfine-tuning (PEFT) methods, in particular low-rank adaptations (LoRA), haveemerged as a popular strategy for adapting large-scale models to down-streamtasks by performing parameter inference on lower-dimensional subspaces. In thiswork, we investigate the suitability of PEFT methods for subspace Bayesianinference in large-scale Transformer-based vision models. We show that, indeed,combining BitFit, DiffFit, LoRA, and CoLoRA, a novel LoRA-inspired PEFT method,with Bayesian inference enables more robust and reliable predictive performancein MDE.</description><author>Richard D. Paul, Alessio Quercia, Vincent Fortuin, Katharina NÃ¶h, Hanno Scharr</author><pubDate>Wed, 25 Sep 2024 16:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17085v1</guid></item><item><title>Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?</title><link>http://arxiv.org/abs/2409.17080v1</link><description>Large vision-language models (VLMs) have become state-of-the-art for manycomputer vision tasks, with in-context learning (ICL) as a popular adaptationstrategy for new ones. But can VLMs learn novel concepts purely from visualdemonstrations, or are they limited to adapting to the output format of ICLexamples? We propose a new benchmark we call Spatial Visual Ambiguity Tasks(SVAT) that challenges state-of-the-art VLMs to learn new visuospatial tasksin-context. We find that VLMs fail to do this zero-shot, and sometimes continueto fail after finetuning. However, adding simpler data to the training bycurriculum learning leads to improved ICL performance.</description><author>Bowen Zhao, Leo Parker Dirac, Paulina Varshavskaya</author><pubDate>Wed, 25 Sep 2024 16:45:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17080v1</guid></item><item><title>Projective Proximal Gradient Descent for A Class of Nonconvex Nonsmooth Optimization Problems: Fast Convergence Without Kurdyka-Lojasiewicz (KL) Property</title><link>http://arxiv.org/abs/2304.10499v2</link><description>Nonconvex and nonsmooth optimization problems are important and challengingfor statistics and machine learning. In this paper, we propose ProjectedProximal Gradient Descent (PPGD) which solves a class of nonconvex andnonsmooth optimization problems, where the nonconvexity and nonsmoothness comefrom a nonsmooth regularization term which is nonconvex but piecewise convex.In contrast with existing convergence analysis of accelerated PGD methods fornonconvex and nonsmooth problems based on the Kurdyka-\L{}ojasiewicz (K\L{})property, we provide a new theoretical analysis showing local fast convergenceof PPGD. It is proved that PPGD achieves a fast convergence rate of$\cO(1/k^2)$ when the iteration number $k \ge k_0$ for a finite $k_0$ on aclass of nonconvex and nonsmooth problems under mild assumptions, which islocally Nesterov's optimal convergence rate of first-order methods on smoothand convex objective function with Lipschitz continuous gradient. Experimentalresults demonstrate the effectiveness of PPGD.</description><author>Yingzhen Yang, Ping Li</author><pubDate>Wed, 25 Sep 2024 16:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10499v2</guid></item><item><title>Adaptive Error-Bounded Hierarchical Matrices for Efficient Neural Network Compression</title><link>http://arxiv.org/abs/2409.07028v2</link><description>This paper introduces a dynamic, error-bounded hierarchical matrix (H-matrix)compression method tailored for Physics-Informed Neural Networks (PINNs). Theproposed approach reduces the computational complexity and memory demands oflarge-scale physics-based models while preserving the essential properties ofthe Neural Tangent Kernel (NTK). By adaptively refining hierarchical matrixapproximations based on local error estimates, our method ensures efficienttraining and robust model performance. Empirical results demonstrate that thistechnique outperforms traditional compression methods, such as Singular ValueDecomposition (SVD), pruning, and quantization, by maintaining high accuracyand improving generalization capabilities. Additionally, the dynamic H-matrixmethod enhances inference speed, making it suitable for real-time applications.This approach offers a scalable and efficient solution for deploying PINNs incomplex scientific and engineering domains, bridging the gap betweencomputational feasibility and real-world applicability.</description><author>John Mango, Ronald Katende</author><pubDate>Wed, 25 Sep 2024 16:41:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07028v2</guid></item><item><title>Efficient Feature Interactions with Transformers: Improving User Spending Propensity Predictions in Gaming</title><link>http://arxiv.org/abs/2409.17077v1</link><description>Dream11 is a fantasy sports platform that allows users to create their ownvirtual teams for real-life sports events. We host multiple sports and matchesfor our 200M+ user base. In this RMG (real money gaming) setting, users pay anentry amount to participate in various contest products that we provide tousers. In our current work, we discuss the problem of predicting the user'spropensity to spend in a gaming round, so it can be utilized for variousdownstream applications. e.g. Upselling users by incentivizing them marginallyas per their spending propensity, or personalizing the product listing based onthe user's propensity to spend. We aim to model the spending propensity of each user based on pasttransaction data. In this paper, we benchmark tree-based and deep-learningmodels that show good results on structured data, and we propose a newarchitecture change that is specifically designed to capture the richinteractions among the input features. We show that our proposed architectureoutperforms the existing models on the task of predicting the user's propensityto spend in a gaming round. Our new transformer model surpasses thestate-of-the-art FT-Transformer, improving MAE by 2.5\% and MSE by 21.8\%.</description><author>Ved Prakash, Kartavya Kothari</author><pubDate>Wed, 25 Sep 2024 16:40:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17077v1</guid></item><item><title>Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition</title><link>http://arxiv.org/abs/2409.17073v1</link><description>Accurately attributing answer text to its source document is crucial fordeveloping a reliable question-answering system. However, attribution for longdocuments remains largely unexplored. Post-hoc attribution systems are designedto map answer text back to the source document, yet the granularity of thismapping has not been addressed. Furthermore, a critical question arises: Whatprecisely should be attributed, with an emphasis on identifying the informationunits within an answer that necessitate grounding? In this paper, we proposeand investigate a novel approach to the factual decomposition of generatedanswers for attribution, employing template-based in-context learning. Toaccomplish this, we utilize the question and integrate negative sampling duringfew-shot in-context learning for decomposition. This approach enhances thesemantic understanding of both abstractive and extractive answers. We examinethe impact of answer decomposition by providing a thorough examination ofvarious attribution approaches, ranging from retrieval-based techniques toLLM-based attributors.</description><author>Pritika Ramu, Koustava Goswami, Apoorv Saxena, Balaji Vasan Srinivavsan</author><pubDate>Wed, 25 Sep 2024 16:32:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17073v1</guid></item><item><title>The Effect of Perceptual Metrics on Music Representation Learning for Genre Classification</title><link>http://arxiv.org/abs/2409.17069v1</link><description>The subjective quality of natural signals can be approximated with objectiveperceptual metrics. Designed to approximate the perceptual behaviour of humanobservers, perceptual metrics often reflect structures found in natural signalsand neurological pathways. Models trained with perceptual metrics as lossfunctions can capture perceptually meaningful features from the structures heldwithin these metrics. We demonstrate that using features extracted fromautoencoders trained with perceptual losses can improve performance on musicunderstanding tasks, i.e. genre classification, over using these metricsdirectly as distances when learning a classifier. This result suggests improvedgeneralisation to novel signals when using perceptual metrics as loss functionsfor representation learning.</description><author>Tashi Namgyal, Alexander Hepburn, Raul Santos-Rodriguez, Valero Laparra, Jesus Malo</author><pubDate>Wed, 25 Sep 2024 16:29:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17069v1</guid></item><item><title>Is This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text</title><link>http://arxiv.org/abs/2406.14829v2</link><description>Understanding whether a generated table is of good quality is important to beable to use it in creating or editing documents using automatic methods. Inthis work, we underline that existing measures for table quality evaluationfail to capture the overall semantics of the tables, and sometimes unfairlypenalize good tables and reward bad ones. We propose TabEval, a novel tableevaluation strategy that captures table semantics by first breaking down atable into a list of natural language atomic statements and then compares themwith ground truth statements using entailment-based measures. To validate ourapproach, we curate a dataset comprising of text descriptions for 1,250 diverseWikipedia tables, covering a range of topics and structures, in contrast to thelimited scope of existing datasets. We compare TabEval with existing metricsusing unsupervised and supervised text-to-table generation methods,demonstrating its stronger correlation with human judgments of table qualityacross four datasets.</description><author>Pritika Ramu, Aparna Garimella, Sambaran Bandyopadhyay</author><pubDate>Wed, 25 Sep 2024 16:27:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14829v2</guid></item><item><title>VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models</title><link>http://arxiv.org/abs/2409.17066v1</link><description>Scaling model size significantly challenges the deployment and inference ofLarge Language Models (LLMs). Due to the redundancy in LLM weights, recentresearch has focused on pushing weight-only quantization to extremely low-bit(even down to 2 bits). It reduces memory requirements, optimizes storage costs,and decreases memory bandwidth needs during inference. However, due tonumerical representation limitations, traditional scalar-based weightquantization struggles to achieve such extreme low-bit. Recent research onVector Quantization (VQ) for LLMs has demonstrated the potential for extremelylow-bit model quantization by compressing vectors into indices using lookuptables. In this paper, we introduce Vector Post-Training Quantization (VPTQ) forextremely low-bit quantization of LLMs. We use Second-Order Optimization toformulate the LLM VQ problem and guide our quantization algorithm design bysolving the optimization. We further refine the weights usingChannel-Independent Second-Order Optimization for a granular VQ. In addition,by decomposing the optimization problem, we propose a brief and effectivecodebook initialization algorithm. We also extend VPTQ to support residual andoutlier quantization, which enhances model accuracy and further compresses themodel. Our experimental results show that VPTQ reduces model quantizationperplexity by $0.01$-$0.34$ on LLaMA-2, $0.38$-$0.68$ on Mistral-7B,$4.41$-$7.34$ on LLaMA-3 over SOTA at 2-bit, with an average accuracyimprovement of $0.79$-$1.5\%$ on LLaMA-2, $1\%$ on Mistral-7B, $11$-$22\%$ onLLaMA-3 on QA tasks on average. We only utilize $10.4$-$18.6\%$ of thequantization algorithm execution time, resulting in a $1.6$-$1.8\times$increase in inference throughput compared to SOTA.</description><author>Yifei Liu, Jicheng Wen, Yang Wang, Shengyu Ye, Li Lyna Zhang, Ting Cao, Cheng Li, Mao Yang</author><pubDate>Wed, 25 Sep 2024 16:25:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17066v1</guid></item><item><title>Benchmarking Domain Generalization Algorithms in Computational Pathology</title><link>http://arxiv.org/abs/2409.17063v1</link><description>Deep learning models have shown immense promise in computational pathology(CPath) tasks, but their performance often suffers when applied to unseen datadue to domain shifts. Addressing this requires domain generalization (DG)algorithms. However, a systematic evaluation of DG algorithms in the CPathcontext is lacking. This study aims to benchmark the effectiveness of 30 DGalgorithms on 3 CPath tasks of varying difficulty through 7,560cross-validation runs. We evaluate these algorithms using a unified and robustplatform, incorporating modality-specific techniques and recent advances likepretrained foundation models. Our extensive cross-validation experimentsprovide insights into the relative performance of various DG strategies. Weobserve that self-supervised learning and stain augmentation consistentlyoutperform other methods, highlighting the potential of pretrained models anddata augmentation. Furthermore, we introduce a new pan-cancer tumor detectiondataset (HISTOPANTUM) as a benchmark for future research. This study offersvaluable guidance to researchers in selecting appropriate DG approaches forCPath tasks.</description><author>Neda Zamanitajeddin, Mostafa Jahanifar, Kesi Xu, Fouzia Siraj, Nasir Rajpoot</author><pubDate>Wed, 25 Sep 2024 16:21:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17063v1</guid></item><item><title>Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors</title><link>http://arxiv.org/abs/2409.17058v1</link><description>Diffusion-based image super-resolution (SR) methods have achieved remarkablesuccess by leveraging large pre-trained text-to-image diffusion models aspriors. However, these methods still face two challenges: the requirement fordozens of sampling steps to achieve satisfactory results, which limitsefficiency in real scenarios, and the neglect of degradation models, which arecritical auxiliary information in solving the SR problem. In this work, weintroduced a novel one-step SR model, which significantly addresses theefficiency issue of diffusion-based SR methods. Unlike existing fine-tuningstrategies, we designed a degradation-guided Low-Rank Adaptation (LoRA) modulespecifically for SR, which corrects the model parameters based on thepre-estimated degradation information from low-resolution images. This modulenot only facilitates a powerful data-dependent or degradation-dependent SRmodel but also preserves the generative prior of the pre-trained diffusionmodel as much as possible. Furthermore, we tailor a novel training pipeline byintroducing an online negative sample generation strategy. Combined with theclassifier-free guidance strategy during inference, it largely improves theperceptual quality of the super-resolution results. Extensive experiments havedemonstrated the superior efficiency and effectiveness of the proposed modelcompared to recent state-of-the-art methods.</description><author>Aiping Zhang, Zongsheng Yue, Renjing Pei, Wenqi Ren, Xiaochun Cao</author><pubDate>Wed, 25 Sep 2024 16:15:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17058v1</guid></item><item><title>DRIM: Learning Disentangled Representations from Incomplete Multimodal Healthcare Data</title><link>http://arxiv.org/abs/2409.17055v1</link><description>Real-life medical data is often multimodal and incomplete, fueling thegrowing need for advanced deep learning models capable of integrating themefficiently. The use of diverse modalities, including histopathology slides,MRI, and genetic data, offers unprecedented opportunities to improve prognosisprediction and to unveil new treatment pathways. Contrastive learning, widelyused for deriving representations from paired data in multimodal tasks, assumesthat different views contain the same task-relevant information and leveragesonly shared information. This assumption becomes restrictive when handlingmedical data since each modality also harbors specific knowledge relevant todownstream tasks. We introduce DRIM, a new multimodal method for capturingthese shared and unique representations, despite data sparsity. Morespecifically, given a set of modalities, we aim to encode a representation foreach one that can be divided into two components: one encapsulatingpatient-related information common across modalities and the other,encapsulating modality-specific details. This is achieved by increasing theshared information among different patient modalities while minimizing theoverlap between shared and unique components within each modality. Our methodoutperforms state-of-the-art algorithms on glioma patients survival predictiontasks, while being robust to missing modalities. To promote reproducibility,the code is made publicly available at https://github.com/Lucas-rbnt/DRIM</description><author>Lucas Robinet, Ahmad Berjaoui, Ziad Kheil, Elizabeth Cohen-Jonathan Moyal</author><pubDate>Wed, 25 Sep 2024 16:13:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17055v1</guid></item><item><title>Using LLM for Real-Time Transcription and Summarization of Doctor-Patient Interactions into ePuskesmas in Indonesia</title><link>http://arxiv.org/abs/2409.17054v1</link><description>One of the key issues contributing to inefficiency in Puskesmas is thetime-consuming nature of doctor-patient interactions. Doctors need to conductthorough consultations, which include diagnosing the patient's condition,providing treatment advice, and transcribing detailed notes into medicalrecords. In regions with diverse linguistic backgrounds, doctors often have toask clarifying questions, further prolonging the process. While diagnosing isessential, transcription and summarization can often be automated using AI toimprove time efficiency and help doctors enhance care quality and enable earlydiagnosis and intervention. This paper proposes a solution using a localizedlarge language model (LLM) to transcribe, translate, and summarizedoctor-patient conversations. We utilize the Whisper model for transcriptionand GPT-3 to summarize them into the ePuskemas medical records format. Thissystem is implemented as an add-on to an existing web browser extension,allowing doctors to fill out patient forms while talking. By leveraging thissolution for real-time transcription, translation, and summarization, doctorscan improve the turnaround time for patient care while enhancing the quality ofrecords, which become more detailed and insightful for future visits. Thisinnovation addresses challenges like overcrowded facilities and theadministrative burden on healthcare providers in Indonesia. We believe thissolution will help doctors save time, provide better care, and produce moreaccurate medical records, representing a significant step toward modernizinghealthcare and ensuring patients receive timely, high-quality care, even inresource-constrained settings.</description><author>Azmul Asmar Irfan, Nur Ahmad Khatim, Mansur M. Arief</author><pubDate>Wed, 25 Sep 2024 16:13:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17054v1</guid></item><item><title>TFT-multi: simultaneous forecasting of vital sign trajectories in the ICU</title><link>http://arxiv.org/abs/2409.15586v2</link><description>Trajectory forecasting in healthcare data has been an important area ofresearch in precision care and clinical integration for computational methods.In recent years, generative AI models have demonstrated promising results incapturing short and long range dependencies in time series data. While thesemodels have also been applied in healthcare, most of them only predict onevalue at a time, which is unrealistic in a clinical setting where multiplemeasures are taken at once. In this work, we extend the framework temporalfusion transformer (TFT), a multi-horizon time series prediction tool, andpropose TFT-multi, an end-to-end framework that can predict multiple vitaltrajectories simultaneously. We apply TFT-multi to forecast 5 vital signsrecorded in the intensive care unit: blood pressure, pulse, SpO2, temperatureand respiratory rate. We hypothesize that by jointly predicting these measures,which are often correlated with one another, we can make more accuratepredictions, especially in variables with large missingness. We validate ourmodel on the public MIMIC dataset and an independent institutional dataset, anddemonstrate that this approach outperforms state-of-the-art univariateprediction tools including the original TFT and Prophet, as well as vectorregression modeling for multivariate prediction. Furthermore, we perform astudy case analysis by applying our pipeline to forecast blood pressure changesin response to actual and hypothetical pressor administration.</description><author>Rosemary Y. He, Jeffrey N. Chiang</author><pubDate>Wed, 25 Sep 2024 16:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15586v2</guid></item><item><title>ControlCity: A Multimodal Diffusion Model Based Approach for Accurate Geospatial Data Generation and Urban Morphology Analysis</title><link>http://arxiv.org/abs/2409.17049v1</link><description>Volunteer Geographic Information (VGI), with its rich variety, large volume,rapid updates, and diverse sources, has become a critical source of geospatialdata. However, VGI data from platforms like OSM exhibit significant qualityheterogeneity across different data types, particularly with urban buildingdata. To address this, we propose a multi-source geographic data transformationsolution, utilizing accessible and complete VGI data to assist in generatingurban building footprint data. We also employ a multimodal data generationframework to improve accuracy. First, we introduce a pipeline for constructingan 'image-text-metadata-building footprint' dataset, primarily based on roadnetwork data and supplemented by other multimodal data. We then presentControlCity, a geographic data transformation method based on a multimodaldiffusion model. This method first uses a pre-trained text-to-image model toalign text, metadata, and building footprint data. An improved ControlNetfurther integrates road network and land-use imagery, producing refinedbuilding footprint data. Experiments across 22 global cities demonstrate thatControlCity successfully simulates real urban building patterns, achievingstate-of-the-art performance. Specifically, our method achieves an average FIDscore of 50.94, reducing error by 71.01% compared to leading methods, and aMIoU score of 0.36, an improvement of 38.46%. Additionally, our model excels intasks like urban morphology transfer, zero-shot city generation, and spatialdata completeness assessment. In the zero-shot city task, our method accuratelypredicts and generates similar urban structures, demonstrating stronggeneralization. This study confirms the effectiveness of our approach ingenerating urban building footprint data and capturing complex citycharacteristics.</description><author>Fangshuo Zhou, Huaxia Li, Rui Hu, Sensen Wu, Hailin Feng, Zhenhong Du, Liuchang Xu</author><pubDate>Wed, 25 Sep 2024 16:03:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17049v1</guid></item><item><title>Predictive Covert Communication Against Multi-UAV Surveillance Using Graph Koopman Autoencoder</title><link>http://arxiv.org/abs/2409.17048v1</link><description>Low Probability of Detection (LPD) communication aims to obscure the presenceof radio frequency (RF) signals to evade surveillance. In the context of mobilesurveillance utilizing unmanned aerial vehicles (UAVs), achieving LPDcommunication presents significant challenges due to the UAVs' rapid andcontinuous movements, which are characterized by unknown nonlinear dynamics.Therefore, accurately predicting future locations of UAVs is essential forenabling real-time LPD communication. In this paper, we introduce a novelframework termed predictive covert communication, aimed at minimizingdetectability in terrestrial ad-hoc networks under multi-UAV surveillance. Ourdata-driven method synergistically integrates graph neural networks (GNN) withKoopman theory to model the complex interactions within a multi-UAV network andfacilitating long-term predictions by linearizing the dynamics, even withlimited historical data. Extensive simulation results substantiate that thepredicted trajectories using our method result in at least 63%-75% lowerprobability of detection when compared to well-known state-of-the-art baselineapproaches, showing promise in enabling low-latency covert operations inpractical scenarios.</description><author>Sivaram Krishnan, Jihong Park, Gregory Sherman, Benjamin Campbell, Jinho Choi</author><pubDate>Wed, 25 Sep 2024 16:02:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17048v1</guid></item><item><title>Detecting Temporal Ambiguity in Questions</title><link>http://arxiv.org/abs/2409.17046v1</link><description>Detecting and answering ambiguous questions has been a challenging task inopen-domain question answering. Ambiguous questions have different answersdepending on their interpretation and can take diverse forms. Temporallyambiguous questions are one of the most common types of such questions. In thispaper, we introduce TEMPAMBIQA, a manually annotated temporally ambiguous QAdataset consisting of 8,162 open-domain questions derived from existingdatasets. Our annotations focus on capturing temporal ambiguity to study thetask of detecting temporally ambiguous questions. We propose a novel approachby using diverse search strategies based on disambiguated versions of thequestions. We also introduce and test non-search, competitive baselines fordetecting temporal ambiguity using zero-shot and few-shot approaches.</description><author>Bhawna Piryani, Abdelrahman Abdallah, Jamshid Mozafari, Adam Jatowt</author><pubDate>Wed, 25 Sep 2024 15:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17046v1</guid></item><item><title>GeoBiked: A Dataset with Geometric Features and Automated Labeling Techniques to Enable Deep Generative Models in Engineering Design</title><link>http://arxiv.org/abs/2409.17045v1</link><description>We provide a dataset for enabling Deep Generative Models (DGMs) inengineering design and propose methods to automate data labeling by utilizinglarge-scale foundation models. GeoBiked is curated to contain 4 355 bicycleimages, annotated with structural and technical features and is used toinvestigate two automated labeling techniques: The utilization of consolidatedlatent features (Hyperfeatures) from image-generation models to detectgeometric correspondences (e.g. the position of the wheel center) in structuralimages and the generation of diverse text descriptions for structural images.GPT-4o, a vision-language-model (VLM), is instructed to analyze images andproduce diverse descriptions aligned with the system-prompt. By representingtechnical images as Diffusion-Hyperfeatures, drawing geometric correspondencesbetween them is possible. The detection accuracy of geometric points in unseensamples is improved by presenting multiple annotated source images. GPT-4o hassufficient capabilities to generate accurate descriptions of technical images.Grounding the generation only on images leads to diverse descriptions butcauses hallucinations, while grounding it on categorical labels restricts thediversity. Using both as input balances creativity and accuracy. Successfullyusing Hyperfeatures for geometric correspondence suggests that this approachcan be used for general point-detection and annotation tasks in technicalimages. Labeling such images with text descriptions using VLMs is possible, butdependent on the models detection capabilities, careful prompt-engineering andthe selection of input information. Applying foundation models in engineeringdesign is largely unexplored. We aim to bridge this gap with a dataset toexplore training, finetuning and conditioning DGMs in this field and suggestingapproaches to bootstrap foundation models to process technical images.</description><author>Phillip Mueller, Sebastian Mueller, Lars Mikelsons</author><pubDate>Wed, 25 Sep 2024 15:57:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17045v1</guid></item><item><title>DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation</title><link>http://arxiv.org/abs/2409.14307v2</link><description>Diffusion models have shown excellent performance on various image generationtasks, but the substantial computational costs and huge memory footprint hindertheir low-latency applications in real-world scenarios. Quantization is apromising way to compress and accelerate models. Nevertheless, due to the widerange and time-varying activations in diffusion models, existing methods cannotmaintain both accuracy and efficiency simultaneously for low-bit quantization.To tackle this issue, we propose DilateQuant, a novel quantization frameworkfor diffusion models that offers comparable accuracy and high efficiency.Specifically, we keenly aware of numerous unsaturated in-channel weights, whichcan be cleverly exploited to reduce the range of activations without additionalcomputation cost. Based on this insight, we propose Weight Dilation (WD) thatmaximally dilates the unsaturated in-channel weights to a constrained rangethrough a mathematically equivalent scaling. WD costlessly absorbs theactivation quantization errors into weight quantization. The range ofactivations decreases, which makes activations quantization easy. The range ofweights remains constant, which makes model easy to converge in training stage.Considering the temporal network leads to time-varying activations, we design aTemporal Parallel Quantizer (TPQ), which sets time-step quantization parametersand supports parallel quantization for different time steps, significantlyimproving the performance and reducing time cost. To further enhanceperformance while preserving efficiency, we introduce a Block-wise KnowledgeDistillation (BKD) to align the quantized models with the full-precision modelsat a block level. The simultaneous training of time-step quantizationparameters and weights minimizes the time required, and the shorterbackpropagation paths decreases the memory footprint of the quantizationprocess.</description><author>Xuewen Liu, Zhikai Li, Qingyi Gu</author><pubDate>Wed, 25 Sep 2024 15:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.14307v2</guid></item><item><title>How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not</title><link>http://arxiv.org/abs/2409.17044v1</link><description>The remarkable performance achieved by Large Language Models (LLM) has drivenresearch efforts to leverage them for a wide range of tasks and inputmodalities. In speech-to-text (S2T) tasks, the emerging solution consists ofprojecting the output of the encoder of a Speech Foundational Model (SFM) intothe LLM embedding space through an adapter module. However, no work has yetinvestigated how much the downstream-task performance depends on each component(SFM, adapter, LLM) nor whether the best design of the adapter depends on thechosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adaptermodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) ontwo widespread S2T tasks, namely Automatic Speech Recognition and SpeechTranslation. Our results demonstrate that the SFM plays a pivotal role indownstream performance, while the adapter choice has moderate impact anddepends on the SFM and LLM.</description><author>Francesco Verdini, Pierfrancesco Melucci, Stefano Perna, Francesco Cariaggi, Marco Gaido, Sara Papi, Szymon Mazurek, Marek Kasztelnik, Luisa Bentivogli, SÃ©bastien BratiÃ¨res, Paolo Merialdo, Simone Scardapane</author><pubDate>Wed, 25 Sep 2024 15:54:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17044v1</guid></item><item><title>Looped Transformers for Length Generalization</title><link>http://arxiv.org/abs/2409.15647v2</link><description>Recent work has shown that Transformers trained from scratch can successfullysolve various arithmetic and algorithmic tasks, such as adding numbers andcomputing parity. While these Transformers generalize well on unseen inputs ofthe same length, they struggle with length generalization, i.e., handlinginputs of unseen lengths. In this work, we demonstrate that looped Transformerswith an adaptive number of steps significantly improve length generalization.We focus on tasks with a known iterative solution, involving multipleiterations of a RASP-L operation - a length-generalizable operation that can beexpressed by a finite-sized Transformer. We train looped Transformers using ourproposed learning algorithm and observe that they learn highlylength-generalizable solutions for various tasks.</description><author>Ying Fan, Yilun Du, Kannan Ramchandran, Kangwook Lee</author><pubDate>Wed, 25 Sep 2024 15:52:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15647v2</guid></item><item><title>A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders</title><link>http://arxiv.org/abs/2409.14507v3</link><description>Sparse Autoencoders (SAEs) have emerged as a promising approach to decomposethe activations of Large Language Models (LLMs) into human-interpretablelatents. In this paper, we pose two questions. First, to what extent do SAEsextract monosemantic and interpretable latents? Second, to what extent doesvarying the sparsity or the size of the SAE affect monosemanticity /interpretability? By investigating these questions in the context of a simplefirst-letter identification task where we have complete access to ground truthlabels for all tokens in the vocabulary, we are able to provide more detailthan prior investigations. Critically, we identify a problematic form offeature-splitting we call feature absorption where seemingly monosemanticlatents fail to fire in cases where they clearly should. Our investigationsuggests that varying SAE size or sparsity is insufficient to solve this issue,and that there are deeper conceptual issues in need of resolution.</description><author>David Chanin, James Wilken-Smith, TomÃ¡Å¡ Dulka, Hardik Bhatnagar, Joseph Bloom</author><pubDate>Wed, 25 Sep 2024 15:50:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.14507v3</guid></item><item><title>Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of Peptides</title><link>http://arxiv.org/abs/2408.15126v4</link><description>Molecular Dynamics (MD) is crucial in various fields such as materialsscience, chemistry, and pharmacology to name a few. Conventional MD softwarestruggles with the balance between time cost and prediction accuracy, whichrestricts its wider application. Recently, data-driven approaches based on deepgenerative models have been devised for time-coarsened dynamics, which aim atlearning dynamics of diverse molecular systems over a long timestep, enjoyingboth universality and efficiency. Nevertheless, most current methods aredesigned solely to learn from the data distribution regardless of theunderlying Boltzmann distribution, and the physics priors such as energies andforces are constantly overlooked. In this work, we propose a conditionalgenerative model called Force-guided Bridge Matching (FBM), which learnsfull-atom time-coarsened dynamics and targets the Boltzmann-constraineddistribution. With the guidance of our delicately-designed intermediate forcefield, FBM leverages favourable physics priors into the generation process,giving rise to enhanced simulations. Experiments on two datasets consisting ofpeptides verify our superiority in terms of comprehensive metrics anddemonstrate transferability to unseen systems.</description><author>Ziyang Yu, Wenbing Huang, Yang Liu</author><pubDate>Wed, 25 Sep 2024 15:47:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15126v4</guid></item><item><title>Real-time estimation of overt attention from dynamic features of the face using deep-learning</title><link>http://arxiv.org/abs/2409.13084v2</link><description>Students often drift in and out of focus during class. Effective teachersrecognize this and re-engage them when necessary. With the shift to remotelearning, teachers have lost the visual feedback needed to adapt to varyingstudent engagement. We propose using readily available front-facing video toinfer attention levels based on movements of the eyes, head, and face. We traina deep learning model to predict a measure of attention based on overt eyemovements. Specifically, we measure Inter-Subject Correlation of eye movementsin ten-second intervals while students watch the same educational videos. In 3different experiments (N=83) we show that the trained model predicts thisobjective metric of attention on unseen data with $R^2$=0.38, and on unseensubjects with $R^2$=0.26-0.30. The deep network relies mostly on a student'seye movements, but to some extent also on movements of the brows, cheeks, andhead. In contrast to Inter-Subject Correlation of the eyes, the model canestimate attentional engagement from individual students' movements withoutneeding reference data from an attentive group. This enables a much broader setof online applications. The solution is lightweight and can operate on theclient side, which mitigates some of the privacy concerns associated withonline attention monitoring. GitHub implementation is available athttps://github.com/asortubay/timeISC</description><author>Aimar Silvan Ortubay, Lucas C. Parra, Jens Madsen</author><pubDate>Wed, 25 Sep 2024 15:34:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13084v2</guid></item><item><title>EventHDR: from Event to High-Speed HDR Videos and Beyond</title><link>http://arxiv.org/abs/2409.17029v1</link><description>Event cameras are innovative neuromorphic sensors that asynchronously capturethe scene dynamics. Due to the event-triggering mechanism, such cameras recordevent streams with much shorter response latency and higher intensitysensitivity compared to conventional cameras. On the basis of these features,previous works have attempted to reconstruct high dynamic range (HDR) videosfrom events, but have either suffered from unrealistic artifacts or failed toprovide sufficiently high frame rates. In this paper, we present a recurrentconvolutional neural network that reconstruct high-speed HDR videos from eventsequences, with a key frame guidance to prevent potential error accumulationcaused by the sparse event data. Additionally, to address the problem ofseverely limited real dataset, we develop a new optical system to collect areal-world dataset with paired high-speed HDR videos and event streams,facilitating future research in this field. Our dataset provides the first realpaired dataset for event-to-HDR reconstruction, avoiding potential inaccuraciesfrom simulation strategies. Experimental results demonstrate that our methodcan generate high-quality, high-speed HDR videos. We further explore thepotential of our work in cross-camera reconstruction and downstream computervision tasks, including object detection, panoramic segmentation, optical flowestimation, and monocular depth estimation under HDR scenarios.</description><author>Yunhao Zou, Ying Fu, Tsuyoshi Takatani, Yinqiang Zheng</author><pubDate>Wed, 25 Sep 2024 15:32:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17029v1</guid></item><item><title>Counterfactual Token Generation in Large Language Models</title><link>http://arxiv.org/abs/2409.17027v1</link><description>"Sure, I am happy to generate a story for you: Captain Lyra stood at the helmof her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]Lyra's eyes welled up with tears as she realized the bitter truth - she hadsacrificed everything for fleeting riches, and lost the love of her crew, herfamily, and herself." Although this story, generated by a large language model,is captivating, one may wonder -- how would the story have unfolded if themodel had chosen "Captain Maeve" as the protagonist instead? We cannot know.State-of-the-art large language models are stateless -- they maintain nointernal memory or state. Given a prompt, they generate a sequence of tokens asan output using an autoregressive process. As a consequence, they cannot reasonabout counterfactual alternatives to tokens they have generated in the past. Inthis work, our goal is to enhance them with this functionality. To this end, wedevelop a causal model of token generation that builds upon the Gumbel-Maxstructural causal model. Our model allows any large language model to performcounterfactual token generation at almost no cost in comparison with vanillatoken generation, it is embarrassingly simple to implement, and it does notrequire any fine-tuning nor prompt engineering. We implement our model on Llama3 8B-instruct and conduct both qualitative and quantitative analyses ofcounterfactually generated text. We conclude with a demonstrative applicationof counterfactual token generation for bias detection, unveiling interestinginsights about the model of the world constructed by large language models.</description><author>Ivi Chatzi, Nina Corvelo Benz, Eleni Straitouri, Stratis Tsirtsis, Manuel Gomez-Rodriguez</author><pubDate>Wed, 25 Sep 2024 15:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17027v1</guid></item><item><title>Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom</title><link>http://arxiv.org/abs/2409.17025v1</link><description>Improved surgical skill is generally associated with improved patientoutcomes, although assessment is subjective; labour-intensive; and requiresdomain specific expertise. Automated data driven metrics can alleviate thesedifficulties, as demonstrated by existing machine learning instrument trackingmodels in minimally invasive surgery. However, these models have been tested onlimited datasets of laparoscopic surgery, with a focus on isolated tasks androbotic surgery. In this paper, a new public dataset is introduced, focusing onsimulated surgery, using the nasal phase of endoscopic pituitary surgery as anexemplar. Simulated surgery allows for a realistic yet repeatable environment,meaning the insights gained from automated assessment can be used by novicesurgeons to hone their skills on the simulator before moving to real surgery.PRINTNet (Pituitary Real-time INstrument Tracking Network) has been created asa baseline model for this automated assessment. Consisting of DeepLabV3 forclassification and segmentation; StrongSORT for tracking; and the NVIDIAHoloscan SDK for real-time performance, PRINTNet achieved 71.9% Multiple ObjectTracking Precision running at 22 Frames Per Second. Using this tracking output,a Multilayer Perceptron achieved 87% accuracy in predicting surgical skilllevel (novice or expert), with the "ratio of total procedure time to instrumentvisible time" correlated with higher surgical skill. This thereforedemonstrates the feasibility of automated surgical skill assessment insimulated endoscopic pituitary surgery. The new publicly available dataset canbe found here: https://doi.org/10.5522/04/26511049.</description><author>Adrito Das, Bilal Sidiqi, Laurent Mennillo, Zhehua Mao, Mikael Brudfors, Miguel Xochicale, Danyal Z. Khan, Nicola Newall, John G. Hanrahan, Matthew J. Clarkson, Danail Stoyanov, Hani J. Marcus, Sophia Bano</author><pubDate>Wed, 25 Sep 2024 15:27:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17025v1</guid></item><item><title>Enhanced Wavelet Scattering Network for image inpainting detection</title><link>http://arxiv.org/abs/2409.17023v1</link><description>The rapid advancement of image inpainting tools, especially those aimed atremoving artifacts, has made digital image manipulation alarmingly accessible.This paper proposes several innovative ideas for detecting inpainting forgeriesbased on low level noise analysis by combining Dual-Tree Complex WaveletTransform (DT-CWT) for feature extraction with convolutional neural networks(CNN) for forged area detection and localization, and lastly by employing aninnovative combination of texture segmentation with noise variance estimations.The DT-CWT offers significant advantages due to its shift-invariance, enhancingits robustness against subtle manipulations during the inpainting process.Furthermore, its directional selectivity allows for the detection of subtleartifacts introduced by inpainting within specific frequency bands andorientations. Various neural network architectures were evaluated and proposed.Lastly, we propose a fusion detection module that combines texture analysiswith noise variance estimation to give the forged area. Our approach wasbenchmarked against state-of-the-art methods and demonstrated superiorperformance over all cited alternatives. The training code (with pretrainedmodel weights) as long as the dataset will be available athttps://github.com/jmaba/Deep-dual-tree-complex-neural-network-for-image-inpainting-detection</description><author>Barglazan Adrian-Alin, Brad Remus</author><pubDate>Wed, 25 Sep 2024 15:27:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17023v1</guid></item><item><title>CombU: A Combined Unit Activation for Fitting Mathematical Expressions with Neural Networks</title><link>http://arxiv.org/abs/2409.17021v1</link><description>The activation functions are fundamental to neural networks as they introducenon-linearity into data relationships, thereby enabling deep networks toapproximate complex data relations. Existing efforts to enhance neural networkperformance have predominantly focused on developing new mathematicalfunctions. However, we find that a well-designed combination of existingactivation functions within a neural network can also achieve this objective.In this paper, we introduce the Combined Units activation (CombU), whichemploys different activation functions at various dimensions across differentlayers. This approach can be theoretically proven to fit most mathematicalexpressions accurately. The experiments conducted on four mathematicalexpression datasets, compared against six State-Of-The-Art (SOTA) activationfunction algorithms, demonstrate that CombU outperforms all SOTA algorithms in10 out of 16 metrics and ranks in the top three for the remaining six metrics.</description><author>Jiayu Li, Zilong Zhao, Kevin Yee, Uzair Javaid, Biplab Sikdar</author><pubDate>Wed, 25 Sep 2024 15:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17021v1</guid></item><item><title>PTQ4RIS: Post-Training Quantization for Referring Image Segmentation</title><link>http://arxiv.org/abs/2409.17020v1</link><description>Referring Image Segmentation (RIS), aims to segment the object referred by agiven sentence in an image by understanding both visual and linguisticinformation. However, existing RIS methods tend to explore top-performancemodels, disregarding considerations for practical applications onresources-limited edge devices. This oversight poses a significant challengefor on-device RIS inference. To this end, we propose an effective and efficientpost-training quantization framework termed PTQ4RIS. Specifically, we firstconduct an in-depth analysis of the root causes of performance degradation inRIS model quantization and propose dual-region quantization (DRQ) andreorder-based outlier-retained quantization (RORQ) to address the quantizationdifficulties in visual and text encoders. Extensive experiments on threebenchmarks with different bits settings (from 8 to 4 bits) demonstrates itssuperior performance. Importantly, we are the first PTQ method specificallydesigned for the RIS task, highlighting the feasibility of PTQ in RISapplications. Code will be available at {https://github.com/gugu511yy/PTQ4RIS}.</description><author>Xiaoyan Jiang, Hang Yang, Kaiying Zhu, Xihe Qiu, Shibo Zhao, Sifan Zhou</author><pubDate>Wed, 25 Sep 2024 15:23:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17020v1</guid></item><item><title>The $Î¼\mathcal{G}$ Language for Programming Graph Neural Networks</title><link>http://arxiv.org/abs/2407.09441v3</link><description>Graph neural networks form a class of deep learning architecturesspecifically designed to work with graph-structured data. As such, they sharethe inherent limitations and problems of deep learning, especially regardingthe issues of explainability and trustworthiness. We propose $\mu\mathcal{G}$,an original domain-specific language for the specification of graph neuralnetworks that aims to overcome these issues. The language's syntax isintroduced, and its meaning is rigorously defined by a denotational semantics.An equivalent characterization in the form of an operational semantics is alsoprovided and, together with a type system, is used to prove the type soundnessof $\mu\mathcal{G}$. We show how $\mu\mathcal{G}$ programs can be representedin a more user-friendly graphical visualization, and provide examples of itsgenerality by showing how it can be used to define some of the most populargraph neural network models, or to develop any custom graph processingapplication.</description><author>Matteo Belenchia, Flavio Corradini, Michela Quadrini, Michele Loreti</author><pubDate>Wed, 25 Sep 2024 15:23:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09441v3</guid></item><item><title>CNN Mixture-of-Depths</title><link>http://arxiv.org/abs/2409.17016v1</link><description>We introduce Mixture-of-Depths (MoD) for Convolutional Neural Networks(CNNs), a novel approach that enhances the computational efficiency of CNNs byselectively processing channels based on their relevance to the currentprediction. This method optimizes computational resources by dynamicallyselecting key channels in feature maps for focused processing within theconvolutional blocks (Conv-Blocks), while skipping less relevant channels.Unlike conditional computation methods that require dynamic computation graphs,CNN MoD uses a static computation graph with fixed tensor sizes which improvehardware efficiency. It speeds up the training and inference processes withoutthe need for customized CUDA kernels, unique loss functions, or finetuning. CNNMoD either matches the performance of traditional CNNs with reduced inferencetimes, GMACs, and parameters, or exceeds their performance while maintainingsimilar inference times, GMACs, and parameters. For example, on ImageNet,ResNet86-MoD exceeds the performance of the standard ResNet50 by 0.45% with a6% speedup on CPU and 5% on GPU. Moreover, ResNet75-MoD achieves the sameperformance as ResNet50 with a 25% speedup on CPU and 15% on GPU.</description><author>Rinor Cakaj, Jens Mehnert, Bin Yang</author><pubDate>Wed, 25 Sep 2024 15:19:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17016v1</guid></item><item><title>Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code</title><link>http://arxiv.org/abs/2409.07368v3</link><description>This paper introduces SGCode, a flexible prompt-optimizing system to generatesecure code with large language models (LLMs). SGCode integrates recentprompt-optimization approaches with LLMs in a unified system accessible throughfront-end and back-end APIs, enabling users to 1) generate secure code, whichis free of vulnerabilities, 2) review and share security analysis, and 3)easily switch from one prompt optimization approach to another, while providinginsights on model and system performance. We populated SGCode on an AWS serverwith PromSec, an approach that optimizes prompts by combining an LLM andsecurity tools with a lightweight generative adversarial graph neural networkto detect and fix security vulnerabilities in the generated code. Extensiveexperiments show that SGCode is practical as a public tool to gain insightsinto the trade-offs between model utility, secure code generation, and systemcost. SGCode has only a marginal cost compared with prompting LLMs. SGCode isavailable at: https://sgcode.codes/.</description><author>Khiem Ton, Nhi Nguyen, Mahmoud Nazzal, Abdallah Khreishah, Cristian Borcea, NhatHai Phan, Ruoming Jin, Issa Khalil, Yelong Shen</author><pubDate>Wed, 25 Sep 2024 15:17:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07368v3</guid></item><item><title>AI-Driven Risk-Aware Scheduling for Active Debris Removal Missions</title><link>http://arxiv.org/abs/2409.17012v1</link><description>The proliferation of debris in Low Earth Orbit (LEO) represents a significantthreat to space sustainability and spacecraft safety. Active Debris Removal(ADR) has emerged as a promising approach to address this issue, utilisingOrbital Transfer Vehicles (OTVs) to facilitate debris deorbiting, therebyreducing future collision risks. However, ADR missions are substantiallycomplex, necessitating accurate planning to make the missions economicallyviable and technically effective. Moreover, these servicing missions require ahigh level of autonomous capability to plan under evolving orbital conditionsand changing mission requirements. In this paper, an autonomousdecision-planning model based on Deep Reinforcement Learning (DRL) is developedto train an OTV to plan optimal debris removal sequencing. It is shown thatusing the proposed framework, the agent can find optimal mission plans andlearn to update the planning autonomously to include risk handling of debriswith high collision risk.</description><author>Antoine Poupon, Hugo de Rohan Willner, Pierre Nikitits, Adam Abdin</author><pubDate>Wed, 25 Sep 2024 15:16:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17012v1</guid></item><item><title>LLM-CARD: Towards a Description and Landscape of Large Language Models</title><link>http://arxiv.org/abs/2409.17011v1</link><description>With the rapid growth of the Natural Language Processing (NLP) field, a vastvariety of Large Language Models (LLMs) continue to emerge for diverse NLPtasks. As an increasing number of papers are presented, researchers anddevelopers face the challenge of information overload. Thus, it is particularlyimportant to develop a system that can automatically extract and organise keyinformation about LLMs from academic papers (\textbf{LLM model card}). Thiswork is to develop such a pioneer system by using Named Entity Recognition(\textbf{NER}) and Relation Extraction (\textbf{RE}) methods that automaticallyextract key information about large language models from the papers, helpingresearchers to efficiently access information about LLMs. These featuresinclude model \textit{licence}, model \textit{name}, and model\textit{application}. With these features, we can form a model card for eachpaper. \textbf{Data-contribution} wise, 106 academic papers were processed bydefining three dictionaries - LLMs name, licence, and application. 11,051sentences were extracted through dictionary lookup, and the dataset wasconstructed through manual review of the final selection of 129 sentences thathave a link between the name and the licence, and 106 sentences that have alink between the model name and the application.</description><author>Shengwei Tian, Lifeng Han, Erick Mendez Guzman, Goran Nenadic</author><pubDate>Wed, 25 Sep 2024 15:15:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17011v1</guid></item><item><title>Models Can and Should Embrace the Communicative Nature of Human-Generated Math</title><link>http://arxiv.org/abs/2409.17005v1</link><description>Math is constructed by people for people: just as natural language corporareflect not just propositions but the communicative goals of language users,the math data that models are trained on reflects not just idealizedmathematical entities but rich communicative intentions. While there areimportant advantages to treating math in a purely symbolic manner, we herehypothesize that there are benefits to treating math as situated linguisticcommunication and that language models are well suited for this goal, in waysthat are not fully appreciated. We illustrate these points with two casestudies. First, we ran an experiment in which we found that language modelsinterpret the equals sign in a humanlike way -- generating systematicallydifferent word problems for the same underlying equation arranged in differentways. Second, we found that language models prefer proofs to be ordered innaturalistic ways, even though other orders would be logically equivalent. Weadvocate for AI systems that learn from and represent the communicativeintentions latent in human-generated math.</description><author>Sasha Boguraev, Ben Lipkin, Leonie Weissweiler, Kyle Mahowald</author><pubDate>Wed, 25 Sep 2024 15:08:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17005v1</guid></item><item><title>Adverse Weather Optical Flow: Cumulative Homogeneous-Heterogeneous Adaptation</title><link>http://arxiv.org/abs/2409.17001v1</link><description>Optical flow has made great progress in clean scenes, while suffersdegradation under adverse weather due to the violation of the brightnessconstancy and gradient continuity assumptions of optical flow. Typically,existing methods mainly adopt domain adaptation to transfer motion knowledgefrom clean to degraded domain through one-stage adaptation. However, thisdirect adaptation is ineffective, since there exists a large gap due to adverseweather and scene style between clean and real degraded domains. Moreover, evenwithin the degraded domain itself, static weather (e.g., fog) and dynamicweather (e.g., rain) have different impacts on optical flow. To address aboveissues, we explore synthetic degraded domain as an intermediate bridge betweenclean and real degraded domains, and propose a cumulativehomogeneous-heterogeneous adaptation framework for real adverse weather opticalflow. Specifically, for clean-degraded transfer, our key insight is that staticweather possesses the depth-association homogeneous feature which does notchange the intrinsic motion of the scene, while dynamic weather additionallyintroduces the heterogeneous feature which results in a significant boundarydiscrepancy in warp errors between clean and degraded domains. Forsynthetic-real transfer, we figure out that cost volume correlation shares asimilar statistical histogram between synthetic and real degraded domains,benefiting to holistically aligning the homogeneous correlation distributionfor synthetic-real knowledge distillation. Under this unified framework, theproposed method can progressively and explicitly transfer knowledge from cleanscenes to real adverse weather. In addition, we further collect a real adverseweather dataset with manually annotated optical flow labels and performextensive experiments to verify the superiority of the proposed method.</description><author>Hanyu Zhou, Yi Chang, Zhiwei Shi, Wending Yan, Gang Chen, Yonghong Tian, Luxin Yan</author><pubDate>Wed, 25 Sep 2024 15:05:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17001v1</guid></item><item><title>WasteGAN: Data Augmentation for Robotic Waste Sorting through Generative Adversarial Networks</title><link>http://arxiv.org/abs/2409.16999v1</link><description>Robotic waste sorting poses significant challenges in both perception andmanipulation, given the extreme variability of objects that should berecognized on a cluttered conveyor belt. While deep learning has proveneffective in solving complex tasks, the necessity for extensive data collectionand labeling limits its applicability in real-world scenarios like wastesorting. To tackle this issue, we introduce a data augmentation method based ona novel GAN architecture called wasteGAN. The proposed method allows toincrease the performance of semantic segmentation models, starting from a verylimited bunch of labeled examples, such as few as 100. The key innovations ofwasteGAN include a novel loss function, a novel activation function, and alarger generator block. Overall, such innovations helps the network to learnfrom limited number of examples and synthesize data that better mirrorsreal-world distributions. We then leverage the higher-quality segmentationmasks predicted from models trained on the wasteGAN synthetic data to computesemantic-aware grasp poses, enabling a robotic arm to effectively recognizingcontaminants and separating waste in a real-world scenario. Throughcomprehensive evaluation encompassing dataset-based assessments and real-worldexperiments, our methodology demonstrated promising potential for robotic wastesorting, yielding performance gains of up to 5.8\% in picking contaminants. Theproject page is available at https://github.com/bach05/wasteGAN.git</description><author>Alberto Bacchin, Leonardo Barcellona, Matteo Terreran, Stefano Ghidoni, Emanuele Menegatti, Takuya Kiyokawa</author><pubDate>Wed, 25 Sep 2024 15:04:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16999v1</guid></item><item><title>PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in Endoscopic Pituitary Surgery</title><link>http://arxiv.org/abs/2409.16998v1</link><description>Accurate intra-operative Remaining Surgery Duration (RSD) predictions allowfor anaesthetists to more accurately decide when to administer anaestheticagents and drugs, as well as to notify hospital staff to send in the nextpatient. Therefore RSD plays an important role in improving patient care andminimising surgical theatre costs via efficient scheduling. In endoscopicpituitary surgery, it is uniquely challenging due to variable workflowsequences with a selection of optional steps contributing to high variabilityin surgery duration. This paper presents PitRSDNet for predicting RSD duringpituitary surgery, a spatio-temporal neural network model that learns fromhistorical data focusing on workflow sequences. PitRSDNet integrates workflowknowledge into RSD prediction in two forms: 1) multi-task learning forconcurrently predicting step and RSD; and 2) incorporating prior steps ascontext in temporal learning and inference. PitRSDNet is trained and evaluatedon a new endoscopic pituitary surgery dataset with 88 videos to showcompetitive performance improvements over previous statistical and machinelearning methods. The findings also highlight how PitRSDNet improve RSDprecision on outlier cases utilising the knowledge of prior steps.</description><author>Anjana Wijekoon, Adrito Das, Roxana R. Herrera, Danyal Z. Khan, John Hanrahan, Eleanor Carter, Valpuri Luoma, Danail Stoyanov, Hani J. Marcus, Sophia Bano</author><pubDate>Wed, 25 Sep 2024 15:03:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16998v1</guid></item><item><title>INT-FlashAttention: Enabling Flash Attention for INT8 Quantization</title><link>http://arxiv.org/abs/2409.16997v1</link><description>As the foundation of large language models (LLMs), self-attention modulefaces the challenge of quadratic time and memory complexity with respect tosequence length. FlashAttention accelerates attention computation and reducesits memory usage by leveraging the GPU memory hierarchy. A promising researchdirection is to integrate FlashAttention with quantization methods. This paperintroduces INT-FlashAttention, the first INT8 quantization architecturecompatible with the forward workflow of FlashAttention, which significantlyimproves the inference speed of FlashAttention on Ampere GPUs. We implement ourINT-FlashAttention prototype with fully INT8 activations and generalmatrix-multiplication (GEMM) kernels, making it the first attention operatorwith fully INT8 input. As a general token-level post-training quantizationframework, INT-FlashAttention is also compatible with other data formats likeINT4, etc. Experimental results show INT-FlashAttention achieves 72% fasterinference speed and 82% smaller quantization error compared to standardFlashAttention with FP16 and FP8 data format.</description><author>Shimao Chen, Zirui Liu, Zhiying Wu, Ce Zheng, Peizhuang Cong, Zihan Jiang, Lei Su, Tong Yang</author><pubDate>Wed, 25 Sep 2024 15:02:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16997v1</guid></item><item><title>Ranking Manipulation for Conversational Search Engines</title><link>http://arxiv.org/abs/2406.03589v3</link><description>Major search engine providers are rapidly incorporating Large Language Model(LLM)-generated content in response to user queries. These conversationalsearch engines operate by loading retrieved website text into the LLM contextfor summarization and interpretation. Recent research demonstrates that LLMsare highly vulnerable to jailbreaking and prompt injection attacks, whichdisrupt the safety and quality goals of LLMs using adversarial strings. Thiswork investigates the impact of prompt injections on the ranking order ofsources referenced by conversational search engines. To this end, we introducea focused dataset of real-world consumer product websites and formalizeconversational search ranking as an adversarial problem. Experimentally, weanalyze conversational search rankings in the absence of adversarial injectionsand show that different LLMs vary significantly in prioritizing product name,document content, and context position. We then present a tree-of-attacks-basedjailbreaking technique which reliably promotes low-ranked products.Importantly, these attacks transfer effectively to state-of-the-artconversational search engines such as perplexity$.$ai. Given the strongfinancial incentive for website owners to boost their search ranking, we arguethat our problem formulation is of critical importance for future robustnesswork.</description><author>Samuel Pfrommer, Yatong Bai, Tanmay Gautam, Somayeh Sojoudi</author><pubDate>Wed, 25 Sep 2024 14:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03589v3</guid></item><item><title>What is the relationship between Slow Feature Analysis and the Successor Representation?</title><link>http://arxiv.org/abs/2409.16991v1</link><description>(This is a work in progress. Feedback is welcome) An analytical comparison ismade between slow feature analysis (SFA) and the successor representation (SR).While SFA and the SR stem from distinct areas of machine learning, they shareimportant properties, both in terms of their mathematics and the types ofinformation they are sensitive to. This work studies their connection alongthese two axes. In particular, multiple variants of the SFA algorithm areexplored analytically and then applied to the setting of an MDP, leading to afamily of eigenvalue problems involving the SR and other related quantities.These resulting eigenvalue problems are then illustrated in the toy setting ofa gridworld, where it is demonstrated that the place- and grid-like fieldsoften associated to the SR can equally be generated using SFA.</description><author>Eddie Seabrook, Laurenz Wiskott</author><pubDate>Wed, 25 Sep 2024 14:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16991v1</guid></item><item><title>Single Image, Any Face: Generalisable 3D Face Generation</title><link>http://arxiv.org/abs/2409.16990v1</link><description>The creation of 3D human face avatars from a single unconstrained image is afundamental task that underlies numerous real-world vision and graphicsapplications. Despite the significant progress made in generative models,existing methods are either less suited in design for human faces or fail togeneralise from the restrictive training domain to unconstrained facial images.To address these limitations, we propose a novel model, Gen3D-Face, whichgenerates 3D human faces with unconstrained single image input within amulti-view consistent diffusion framework. Given a specific input image, ourmodel first produces multi-view images, followed by neural surfaceconstruction. To incorporate face geometry information in a generalisablemanner, we utilise input-conditioned mesh estimation instead of ground-truthmesh along with synthetic multi-view training data. Importantly, we introduce amulti-view joint generation scheme to enhance appearance consistency amongdifferent views. To the best of our knowledge, this is the first attempt andbenchmark for creating photorealistic 3D human face avatars from single imagesfor generic human subject across domains. Extensive experiments demonstrate thesuperiority of our method over previous alternatives for out-of-domain singeimage 3D face generation and top competition for in-domain setting.</description><author>Wenqing Wang, Haosen Yang, Josef Kittler, Xiatian Zhu</author><pubDate>Wed, 25 Sep 2024 14:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16990v1</guid></item><item><title>Harnessing Diversity for Important Data Selection in Pretraining Large Language Models</title><link>http://arxiv.org/abs/2409.16986v1</link><description>Data selection is of great significance in pre-training large languagemodels, given the variation in quality within the large-scale availabletraining corpora. To achieve this, researchers are currently investigating theuse of data influence to measure the importance of data instances, $i.e.,$ ahigh influence score indicates that incorporating this instance to the trainingset is likely to enhance the model performance. Consequently, they select thetop-$k$ instances with the highest scores. However, this approach has severallimitations. (1) Computing the influence of all available data istime-consuming. (2) The selected data instances are not diverse enough, whichmay hinder the pre-trained model's ability to generalize effectively to variousdownstream tasks. In this paper, we introduce \texttt{Quad}, a data selectionapproach that considers both quality and diversity by using data influence toachieve state-of-the-art pre-training results. In particular, noting thatattention layers capture extensive semantic details, we have adapted theaccelerated $iHVP$ computation methods for attention layers, enhancing ourability to evaluate the influence of data, $i.e.,$ its quality. For thediversity, \texttt{Quad} clusters the dataset into similar data instanceswithin each cluster and diverse instances across different clusters. For eachcluster, if we opt to select data from it, we take some samples to evaluate theinfluence to prevent processing all instances. To determine which clusters toselect, we utilize the classic Multi-Armed Bandit method, treating each clusteras an arm. This approach favors clusters with highly influential instances(ensuring high quality) or clusters that have been selected less frequently(ensuring diversity), thereby well balancing between quality and diversity.</description><author>Chi Zhang, Huaping Zhong, Kuan Zhang, Chengliang Chai, Rui Wang, Xinlin Zhuang, Tianyi Bai, Jiantao Qiu, Lei Cao, Ye Yuan, Guoren Wang, Conghui He</author><pubDate>Wed, 25 Sep 2024 14:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16986v1</guid></item><item><title>AXCEL: Automated eXplainable Consistency Evaluation using LLMs</title><link>http://arxiv.org/abs/2409.16984v1</link><description>Large Language Models (LLMs) are widely used in both industry and academiafor various tasks, yet evaluating the consistency of generated text responsescontinues to be a challenge. Traditional metrics like ROUGE and BLEU show aweak correlation with human judgment. More sophisticated metrics using NaturalLanguage Inference (NLI) have shown improved correlations but are complex toimplement, require domain-specific training due to poor cross-domaingeneralization, and lack explainability. More recently, prompt-based metricsusing LLMs as evaluators have emerged; while they are easier to implement, theystill lack explainability and depend on task-specific prompts, which limitstheir generalizability. This work introduces Automated eXplainable ConsistencyEvaluation using LLMs (AXCEL), a prompt-based consistency metric which offersexplanations for the consistency scores by providing detailed reasoning andpinpointing inconsistent text spans. AXCEL is also a generalizable metric whichcan be adopted to multiple tasks without changing the prompt. AXCEL outperformsboth non-prompt and prompt-based state-of-the-art (SOTA) metrics in detectinginconsistencies across summarization by 8.7%, free text generation by 6.2%, anddata-to-text conversion tasks by 29.4%. We also evaluate the influence ofunderlying LLMs on prompt based metric performance and recalibrate the SOTAprompt-based metrics with the latest LLMs for fair comparison. Further, we showthat AXCEL demonstrates strong performance using open source LLMs.</description><author>P Aditya Sreekar, Sahil Verma, Suransh Chopra, Sarik Ghazarian, Abhishek Persad, Narayanan Sadagopan</author><pubDate>Wed, 25 Sep 2024 14:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16984v1</guid></item><item><title>Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free Real Image Editing</title><link>http://arxiv.org/abs/2409.01322v3</link><description>Despite recent advances in large-scale text-to-image generative models,manipulating real images with these models remains a challenging problem. Themain limitations of existing editing methods are that they either fail toperform with consistent quality on a wide range of image edits or requiretime-consuming hyperparameter tuning or fine-tuning of the diffusion model topreserve the image-specific appearance of the input image. We propose a novelapproach that is built upon a modified diffusion sampling process via theguidance mechanism. In this work, we explore the self-guidance technique topreserve the overall structure of the input image and its local regionsappearance that should not be edited. In particular, we explicitly introducelayout-preserving energy functions that are aimed to save local and globalstructures of the source image. Additionally, we propose a noise rescalingmechanism that allows to preserve noise distribution by balancing the norms ofclassifier-free guidance and our proposed guiders during generation. Such aguiding approach does not require fine-tuning the diffusion model and exactinversion process. As a result, the proposed method provides a fast andhigh-quality editing mechanism. In our experiments, we show through humanevaluation and quantitative analysis that the proposed method allows to producedesired editing which is more preferable by humans and also achieves a bettertrade-off between editing quality and preservation of the original image. Ourcode is available at https://github.com/MACderRu/Guide-and-Rescale.</description><author>Vadim Titov, Madina Khalmatova, Alexandra Ivanova, Dmitry Vetrov, Aibek Alanov</author><pubDate>Wed, 25 Sep 2024 14:44:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.01322v3</guid></item></channel></rss>