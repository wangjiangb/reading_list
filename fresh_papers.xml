<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 17 Aug 2023 06:00:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>TeCH: Text-guided Reconstruction of Lifelike Clothed Humans</title><link>http://arxiv.org/abs/2308.08545v1</link><description>Despite recent research advancements in reconstructing clothed humans from asingle image, accurately restoring the "unseen regions" with high-level detailsremains an unsolved challenge that lacks attention. Existing methods oftengenerate overly smooth back-side surfaces with a blurry texture. But how toeffectively capture all visual attributes of an individual from a single image,which are sufficient to reconstruct unseen areas (e.g., the back view)?Motivated by the power of foundation models, TeCH reconstructs the 3D human byleveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles)which are automatically generated via a garment parsing model and VisualQuestion Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusionmodel (T2I) which learns the "indescribable" appearance. To representhigh-resolution 3D clothed humans at an affordable cost, we propose a hybrid 3Drepresentation based on DMTet, which consists of an explicit body shape gridand an implicit distance field. Guided by the descriptive prompts +personalized T2I diffusion model, the geometry and texture of the 3D humans areoptimized through multi-view Score Distillation Sampling (SDS) andreconstruction losses based on the original observation. TeCH produceshigh-fidelity 3D clothed humans with consistent &amp; delicate texture, anddetailed full-body geometry. Quantitative and qualitative experimentsdemonstrate that TeCH outperforms the state-of-the-art methods in terms ofreconstruction accuracy and rendering quality. The code will be publiclyavailable for research purposes at https://huangyangyi.github.io/tech</description><author>Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Jiaxiang Tang, Deng Cai, Justus Thies</author><pubDate>Wed, 16 Aug 2023 18:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08545v1</guid></item><item><title>LLM-Rec: Personalized Recommendation via Prompting Large Language Models</title><link>http://arxiv.org/abs/2307.15780v2</link><description>We investigate various prompting strategies for enhancing personalizedrecommendation performance with large language models (LLMs) through inputaugmentation. Our proposed approach, termed LLM-Rec, encompasses four distinctprompting strategies: (1) basic prompting, (2) recommendation-driven prompting,(3) engagement-guided prompting, and (4) recommendation-driven +engagement-guided prompting. Our empirical experiments show that incorporatingthe augmented input text generated by LLM leads to improved recommendationperformance. Recommendation-driven and engagement-guided prompting strategiesare found to elicit LLM's understanding of global and local itemcharacteristics. This finding highlights the importance of leveraging diverseprompts and input augmentation techniques to enhance the recommendationcapabilities with LLMs.</description><author>Hanjia Lyu, Song Jiang, Hanqing Zeng, Qifan Wang, Si Zhang, Ren Chen, Chris Leung, Jiajie Tang, Yinglong Xia, Jiebo Luo</author><pubDate>Wed, 16 Aug 2023 18:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15780v2</guid></item><item><title>SHERF: Generalizable Human NeRF from a Single Image</title><link>http://arxiv.org/abs/2303.12791v2</link><description>Existing Human NeRF methods for reconstructing 3D humans typically rely onmultiple 2D images from multi-view cameras or monocular videos captured fromfixed camera views. However, in real-world scenarios, human images are oftencaptured from random camera angles, presenting challenges for high-quality 3Dhuman reconstruction. In this paper, we propose SHERF, the first generalizableHuman NeRF model for recovering animatable 3D humans from a single input image.SHERF extracts and encodes 3D human representations in canonical space,enabling rendering and animation from free views and poses. To achievehigh-fidelity novel view and pose synthesis, the encoded 3D humanrepresentations should capture both global appearance and local fine-grainedtextures. To this end, we propose a bank of 3D-aware hierarchical features,including global, point-level, and pixel-aligned features, to facilitateinformative encoding. Global features enhance the information extracted fromthe single input image and complement the information missing from the partial2D observation. Point-level features provide strong clues of 3D humanstructure, while pixel-aligned features preserve more fine-grained details. Toeffectively integrate the 3D-aware hierarchical feature bank, we design afeature fusion transformer. Extensive experiments on THuman, RenderPeople,ZJU_MoCap, and HuMMan datasets demonstrate that SHERF achieves state-of-the-artperformance, with better generalizability for novel view and pose synthesis.</description><author>Shoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei Yang, Ziwei Liu</author><pubDate>Wed, 16 Aug 2023 18:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12791v2</guid></item><item><title>MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions</title><link>http://arxiv.org/abs/2308.08544v1</link><description>This paper strives for motion expressions guided video segmentation, whichfocuses on segmenting objects in video content based on a sentence describingthe motion of the objects. Existing referring video object datasets typicallyfocus on salient objects and use language expressions that contain excessivestatic attributes that could potentially enable the target object to beidentified in a single frame. These datasets downplay the importance of motionin video content for language-guided video object segmentation. To investigatethe feasibility of using motion expressions to ground and segment objects invideos, we propose a large-scale dataset called MeViS, which contains numerousmotion expressions to indicate target objects in complex environments. Webenchmarked 5 existing referring video object segmentation (RVOS) methods andconducted a comprehensive comparison on the MeViS dataset. The results showthat current RVOS methods cannot effectively address motion expression-guidedvideo segmentation. We further analyze the challenges and propose a baselineapproach for the proposed MeViS dataset. The goal of our benchmark is toprovide a platform that enables the development of effective language-guidedvideo segmentation algorithms that leverage motion expressions as a primary cuefor object segmentation in complex video scenes. The proposed MeViS dataset hasbeen released at https://henghuiding.github.io/MeViS.</description><author>Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Chen Change Loy</author><pubDate>Wed, 16 Aug 2023 18:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08544v1</guid></item><item><title>InsightMapper: A Closer Look at Inner-instance Information for Vectorized High-Definition Mapping</title><link>http://arxiv.org/abs/2308.08543v1</link><description>Vectorized high-definition (HD) maps contain detailed information aboutsurrounding road elements, which are crucial for various downstream tasks inmodern autonomous driving vehicles, such as vehicle planning and control.Recent works have attempted to directly detect the vectorized HD map as a pointset prediction task, resulting in significant improvements in detectionperformance. However, these approaches fail to analyze and exploit theinner-instance correlations between predicted points, impeding furtheradvancements. To address these challenges, we investigate the utilization ofinner-$\textbf{INS}$tance information for vectorized h$\textbf{IGH}$-definitionmapping through $\textbf{T}$ransformers and introduce InsightMapper. This paperpresents three novel designs within InsightMapper that leverage inner-instanceinformation in distinct ways, including hybrid query generation, inner-instancequery fusion, and inner-instance feature aggregation. Comparative experimentsare conducted on the NuScenes dataset, showcasing the superiority of ourproposed method. InsightMapper surpasses previous state-of-the-art (SOTA)methods by 5.78 mAP and 5.12 TOPO, which assess topology correctness.Simultaneously, InsightMapper maintains high efficiency during both trainingand inference phases, resulting in remarkable comprehensive performance. Theproject page for this work is available athttps://tonyxuqaq.github.io/projects/InsightMapper .</description><author>Zhenhua Xu, Kenneth K. Y. Wong, Hengshuang Zhao</author><pubDate>Wed, 16 Aug 2023 18:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08543v1</guid></item><item><title>Normalizing Flows for Human Pose Anomaly Detection</title><link>http://arxiv.org/abs/2211.10946v2</link><description>Video anomaly detection is an ill-posed problem because it relies on manyparameters such as appearance, pose, camera angle, background, and more. Wedistill the problem to anomaly detection of human pose, thus decreasing therisk of nuisance parameters such as appearance affecting the result. Focusingon pose alone also has the side benefit of reducing bias against distinctminority groups. Our model works directly on human pose graph sequences and isexceptionally lightweight (~1K parameters), capable of running on any machineable to run the pose estimation with negligible additional resources. Weleverage the highly compact pose representation in a normalizing flowsframework, which we extend to tackle the unique characteristics ofspatio-temporal pose data and show its advantages in this use case. Thealgorithm is quite general and can handle training data of only normal examplesas well as a supervised setting that consists of labeled normal and abnormalexamples. We report state-of-the-art results on two anomaly detectionbenchmarks - the unsupervised ShanghaiTech dataset and the recent supervisedUBnormal dataset.</description><author>Or Hirschorn, Shai Avidan</author><pubDate>Wed, 16 Aug 2023 18:54:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10946v2</guid></item><item><title>DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars</title><link>http://arxiv.org/abs/2303.09375v3</link><description>We present DINAR, an approach for creating realistic rigged fullbody avatarsfrom single RGB images. Similarly to previous works, our method uses neuraltextures combined with the SMPL-X body model to achieve photo-realistic qualityof avatars while keeping them easy to animate and fast to infer. To restore thetexture, we use a latent diffusion model and show how such model can be trainedin the neural texture space. The use of the diffusion model allows us torealistically reconstruct large unseen regions such as the back of a persongiven the frontal view. The models in our pipeline are trained using 2D imagesand videos only. In the experiments, our approach achieves state-of-the-artrendering quality and good generalization to new poses and viewpoints. Inparticular, the approach improves state-of-the-art on the SnapshotPeople publicbenchmark.</description><author>David Svitov, Dmitrii Gudkov, Renat Bashirov, Victor Lempitsky</author><pubDate>Wed, 16 Aug 2023 18:54:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09375v3</guid></item><item><title>Proprioceptive Learning with Soft Polyhedral Networks</title><link>http://arxiv.org/abs/2308.08538v1</link><description>Proprioception is the "sixth sense" that detects limb postures with motorneurons. It requires a natural integration between the musculoskeletal systemsand sensory receptors, which is challenging among modern robots that aim forlightweight, adaptive, and sensitive designs at a low cost. Here, we presentthe Soft Polyhedral Network with an embedded vision for physical interactions,capable of adaptive kinesthesia and viscoelastic proprioception by learningkinetic features. This design enables passive adaptations to omni-directionalinteractions, visually captured by a miniature high-speed motion trackingsystem embedded inside for proprioceptive learning. The results show that thesoft network can infer real-time 6D forces and torques with accuracies of0.25/0.24/0.35 N and 0.025/0.034/0.006 Nm in dynamic interactions. We alsoincorporate viscoelasticity in proprioception during static adaptation byadding a creep and relaxation modifier to refine the predicted results. Theproposed soft network combines simplicity in design, omni-adaptation, andproprioceptive sensing with high accuracy, making it a versatile solution forrobotics at a low cost with more than 1 million use cycles for tasks such assensitive and competitive grasping, and touch-based geometry reconstruction.This study offers new insights into vision-based proprioception for soft robotsin adaptive grasping, soft manipulation, and human-robot interaction.</description><author>Xiaobo Liu, Xudong Han, Wei Hong, Fang Wan, Chaoyang Song</author><pubDate>Wed, 16 Aug 2023 18:53:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08538v1</guid></item><item><title>Can Transformers Learn Optimal Filtering for Unknown Systems?</title><link>http://arxiv.org/abs/2308.08536v1</link><description>Transformers have demonstrated remarkable success in natural languageprocessing; however, their potential remains mostly unexplored for problemsarising in dynamical systems. In this work, we investigate the optimal outputestimation problem using transformers, which generate output predictions usingall the past ones. We train the transformer using various systems drawn from aprior distribution and then evaluate its performance on previously unseensystems from the same distribution. As a result, the obtained transformer actslike a prediction algorithm that learns in-context and quickly adapts to andpredicts well for different systems - thus we call it meta-output-predictor(MOP). MOP matches the performance of the optimal output estimator, based onKalman filter, for most linear dynamical systems even though it does not haveaccess to a model. We observe via extensive numerical experiments that MOP alsoperforms well in challenging scenarios with non-i.i.d. noise, time-varyingdynamics, and nonlinear dynamics like a quadrotor system with unknownparameters. To further support this observation, in the second part of thepaper, we provide statistical guarantees on the performance of MOP and quantifythe required amount of training to achieve a desired excess risk duringtest-time. Finally, we point out some limitations of MOP by identifying twoclasses of problems MOP fails to perform well, highlighting the need forcaution when using transformers for control and estimation.</description><author>Haldun Balim, Zhe Du, Samet Oymak, Necmiye Ozay</author><pubDate>Wed, 16 Aug 2023 18:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08536v1</guid></item><item><title>AI-Assisted Discovery of Quantitative and Formal Models in Social Science</title><link>http://arxiv.org/abs/2210.00563v3</link><description>In social science, formal and quantitative models, such as ones describingeconomic growth and collective action, are used to formulate mechanisticexplanations, provide predictions, and uncover questions about observedphenomena. Here, we demonstrate the use of a machine learning system to aid thediscovery of symbolic models that capture nonlinear and dynamical relationshipsin social science datasets. By extending neuro-symbolic methods to find compactfunctions and differential equations in noisy and longitudinal data, we showthat our system can be used to discover interpretable models from real-worlddata in economics and sociology. Augmenting existing workflows with symbolicregression can help uncover novel relationships and explore counterfactualmodels during the scientific process. We propose that this AI-assistedframework can bridge parametric and non-parametric models commonly employed insocial science research by systematically exploring the space of nonlinearmodels and enabling fine-grained control over expressivity andinterpretability.</description><author>Julia Balla, Sihao Huang, Owen Dugan, Rumen Dangovski, Marin Soljacic</author><pubDate>Wed, 16 Aug 2023 18:45:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.00563v3</guid></item><item><title>CoDeF: Content Deformation Fields for Temporally Consistent Video Processing</title><link>http://arxiv.org/abs/2308.07926v1</link><description>We present the content deformation field CoDeF as a new type of videorepresentation, which consists of a canonical content field aggregating thestatic contents in the entire video and a temporal deformation field recordingthe transformations from the canonical image (i.e., rendered from the canonicalcontent field) to each individual frame along the time axis.Given a targetvideo, these two fields are jointly optimized to reconstruct it through acarefully tailored rendering pipeline.We advisedly introduce someregularizations into the optimization process, urging the canonical contentfield to inherit semantics (e.g., the object shape) from the video.With such adesign, CoDeF naturally supports lifting image algorithms for video processing,in the sense that one can apply an image algorithm to the canonical image andeffortlessly propagate the outcomes to the entire video with the aid of thetemporal deformation field.We experimentally show that CoDeF is able to liftimage-to-image translation to video-to-video translation and lift keypointdetection to keypoint tracking without any training.More importantly, thanks toour lifting strategy that deploys the algorithms on only one image, we achievesuperior cross-frame consistency in processed videos compared to existingvideo-to-video translation approaches, and even manage to track non-rigidobjects like water and smog.Project page can be found athttps://qiuyu96.github.io/CoDeF/.</description><author>Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, Yujun Shen</author><pubDate>Tue, 15 Aug 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07926v1</guid></item><item><title>RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models</title><link>http://arxiv.org/abs/2308.07922v1</link><description>In this paper, we investigate the in-context learning ability ofretrieval-augmented encoder-decoder language models. We first conduct acomprehensive analysis of the state-of-the-art ATLAS model and identify itslimitations in in-context learning, primarily due to a mismatch betweenpretraining and testing, as well as a restricted context length. To addressthese issues, we propose RAVEN, a model that combines retrieval-augmentedmasked language modeling and prefix language modeling. We further introduceFusion-in-Context Learning to enhance the few-shot performance by enabling themodel to leverage more in-context examples without requiring additionaltraining or model modifications. Through extensive experiments, we demonstratethat RAVEN significantly outperforms ATLAS and achieves results comparable tothe most advanced language models in certain scenarios, despite havingsubstantially fewer parameters. Our work underscores the potential ofretrieval-augmented encoder-decoder language models for in-context learning andencourages further research in this direction.</description><author>Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang, Bryan Catanzaro</author><pubDate>Tue, 15 Aug 2023 18:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07922v1</guid></item><item><title>Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification</title><link>http://arxiv.org/abs/2308.07921v1</link><description>Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 hasbrought significant advancements in addressing math reasoning problems. Inparticular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter,shows remarkable performance on challenging math datasets. In this paper, weexplore the effect of code on enhancing LLMs' reasoning capability byintroducing different constraints on the \textit{Code Usage Frequency} of GPT-4Code Interpreter. We found that its success can be largely attributed to itspowerful skills in generating and executing code, evaluating the output of codeexecution, and rectifying its solution when receiving unreasonable outputs.Based on this insight, we propose a novel and effective prompting method,explicit \uline{c}ode-based \uline{s}elf-\uline{v}erification~(CSV), to furtherboost the mathematical reasoning potential of GPT-4 Code Interpreter. Thismethod employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it touse code to self-verify its answers. In instances where the verification stateregisters as ``False'', the model shall automatically amend its solution,analogous to our approach of rectifying errors during a mathematicsexamination. Furthermore, we recognize that the states of the verificationresult indicate the confidence of a solution, which can improve theeffectiveness of majority voting. With GPT-4 Code Interpreter and CSV, weachieve an impressive zero-shot accuracy on MATH dataset \textbf{(53.9\% $\to$84.3\%)}.</description><author>Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li</author><pubDate>Tue, 15 Aug 2023 18:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07921v1</guid></item><item><title>Helping Hands: An Object-Aware Ego-Centric Video Recognition Model</title><link>http://arxiv.org/abs/2308.07918v1</link><description>We introduce an object-aware decoder for improving the performance ofspatio-temporal representations on ego-centric videos. The key idea is toenhance object-awareness during training by tasking the model to predict handpositions, object positions, and the semantic label of the objects using pairedcaptions when available. At inference time the model only requires RGB framesas inputs, and is able to track and ground objects (although it has not beentrained explicitly for this). We demonstrate the performance of theobject-aware representations learnt by our model, by: (i) evaluating it forstrong transfer, i.e. through zero-shot testing, on a number of downstreamvideo-text retrieval and classification benchmarks; and (ii) by using therepresentations learned as input for long-term video understanding tasks (e.g.Episodic Memory in Ego4D). In all cases the performance improves over the stateof the art -- even compared to networks trained with far larger batch sizes. Wealso show that by using noisy image-level detection as pseudo-labels intraining, the model learns to provide better bounding boxes using videoconsistency, as well as grounding the words in the associated textdescriptions. Overall, we show that the model can act as a drop-in replacementfor an ego-centric video model to improve performance through visual-textgrounding.</description><author>Chuhan Zhang, Ankush Gupta, Andrew Zisserman</author><pubDate>Tue, 15 Aug 2023 18:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07918v1</guid></item><item><title>Relightable and Animatable Neural Avatar from Sparse-View Video</title><link>http://arxiv.org/abs/2308.07903v1</link><description>This paper tackles the challenge of creating relightable and animatableneural avatars from sparse-view (or even monocular) videos of dynamic humansunder unknown illumination. Compared to studio environments, this setting ismore practical and accessible but poses an extremely challenging ill-posedproblem. Previous neural human reconstruction methods are able to reconstructanimatable avatars from sparse views using deformed Signed Distance Fields(SDF) but cannot recover material parameters for relighting. Whiledifferentiable inverse rendering-based methods have succeeded in materialrecovery of static objects, it is not straightforward to extend them to dynamichumans as it is computationally intensive to compute pixel-surface intersectionand light visibility on deformed SDFs for inverse rendering. To solve thischallenge, we propose a Hierarchical Distance Query (HDQ) algorithm toapproximate the world space distances under arbitrary human poses.Specifically, we estimate coarse distances based on a parametric human modeland compute fine distances by exploiting the local deformation invariance ofSDF. Based on the HDQ algorithm, we leverage sphere tracing to efficientlyestimate the surface intersection and light visibility. This allows us todevelop the first system to recover animatable and relightable neural avatarsfrom sparse view (or monocular) inputs. Experiments demonstrate that ourapproach is able to produce superior results compared to state-of-the-artmethods. Our code will be released for reproducibility.</description><author>Zhen Xu, Sida Peng, Chen Geng, Linzhan Mou, Zihan Yan, Jiaming Sun, Hujun Bao, Xiaowei Zhou</author><pubDate>Tue, 15 Aug 2023 18:42:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07903v1</guid></item><item><title>Through the Lens of Core Competency: Survey on Evaluation of Large Language Models</title><link>http://arxiv.org/abs/2308.07902v1</link><description>From pre-trained language model (PLM) to large language model (LLM), thefield of natural language processing (NLP) has witnessed steep performancegains and wide practical uses. The evaluation of a research field guides itsdirection of improvement. However, LLMs are extremely hard to thoroughlyevaluate for two reasons. First of all, traditional NLP tasks become inadequatedue to the excellent performance of LLM. Secondly, existing evaluation tasksare difficult to keep up with the wide range of applications in real-worldscenarios. To tackle these problems, existing works proposed various benchmarksto better evaluate LLMs. To clarify the numerous evaluation tasks in bothacademia and industry, we investigate multiple papers concerning LLMevaluations. We summarize 4 core competencies of LLM, including reasoning,knowledge, reliability, and safety. For every competency, we introduce itsdefinition, corresponding benchmarks, and metrics. Under this competencyarchitecture, similar tasks are combined to reflect corresponding ability,while new tasks can also be easily added into the system. Finally, we give oursuggestions on the future direction of LLM's evaluation.</description><author>Ziyu Zhuang, Qiguang Chen, Longxuan Ma, Mingda Li, Yi Han, Yushan Qian, Haopeng Bai, Zixian Feng, Weinan Zhang, Ting Liu</author><pubDate>Tue, 15 Aug 2023 18:40:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07902v1</guid></item><item><title>The Regular Expression Inference Challenge</title><link>http://arxiv.org/abs/2308.07899v1</link><description>We propose \emph{regular expression inference (REI)} as a challenge forcode/language modelling, and the wider machine learning community. REI is asupervised machine learning (ML) and program synthesis task, and poses theproblem of finding minimal regular expressions from examples: Given two finitesets of strings $P$ and $N$ and a cost function $\text{cost}(\cdot)$, the taskis to generate an expression $r$ that accepts all strings in $P$ and rejectsall strings in $N$, while no other such expression $r'$ exists with$\text{cost}(r')&lt;\text{cost}(r)$. REI has advantages as a challenge problem: (i) regular expressions arewell-known, widely used, and a natural idealisation of code; (ii) REI'sasymptotic worst-case complexity is well understood; (iii) REI has a smallnumber of easy to understand parameters (e.g.~$P$ or $N$ cardinality, stringlengths of examples, or the cost function); this lets us easily finetuneREI-hardness; (iv) REI is an unsolved problem for deep learning based ML. Recently, an REI solver was implemented on GPUs, using program synthesistechniques. This enabled, for the first time, fast generation of minimalexpressions for complex REI instances. Building on this advance, we generateand publish the first large-scale datasets for REI, and devise and evaluateseveral initial heuristic and machine learning baselines. We invite the community to participate and explore ML methods that learn tosolve REI problems. We believe that progress in REI directly translates tocode/language modelling.</description><author>Mojtaba Valizadeh, Philip John Gorinski, Ignacio Iacobacci, Martin Berger</author><pubDate>Tue, 15 Aug 2023 18:40:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07899v1</guid></item><item><title>A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert knowledge in text supervision</title><link>http://arxiv.org/abs/2308.07898v1</link><description>Foundation vision-language models are currently transforming computer vision,and are on the rise in medical imaging fueled by their very promisinggeneralization capabilities. However, the initial attempts to transfer this newparadigm to medical imaging have shown less impressive performances than thoseobserved in other domains, due to the significant domain shift and the complex,expert domain knowledge inherent to medical-imaging tasks. Motivated by theneed for domain-expert foundation models, we present FLAIR, a pre-trainedvision-language model for universal retinal fundus image understanding. To thisend, we compiled 37 open-access, mostly categorical fundus imaging datasetsfrom various sources, with up to 97 different target conditions and 284,660images. We integrate the expert's domain knowledge in the form of descriptivetextual prompts, during both pre-training and zero-shot inference, enhancingthe less-informative categorical supervision of the data. Such a textualexpert's knowledge, which we compiled from the relevant clinical literature andcommunity standards, describes the fine-grained features of the pathologies aswell as the hierarchies and dependencies between them. We report comprehensiveevaluations, which illustrate the benefit of integrating expert knowledge andthe strong generalization capabilities of FLAIR under difficult scenarios withdomain shifts or unseen categories. When adapted with a lightweight linearprobe, FLAIR outperforms fully-trained, dataset-focused models, more so in thefew-shot regimes. Interestingly, FLAIR outperforms by a large margin moregeneralist, larger-scale image-language models, which emphasizes the potentialof embedding experts' domain knowledge and the limitations of generalist modelsin medical imaging.</description><author>Julio Silva-Rodriguez, Hadi Chakor, Riadh Kobbi, Jose Dolz, Ismail Ben Ayed</author><pubDate>Tue, 15 Aug 2023 18:39:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07898v1</guid></item><item><title>Data augmentation and refinement for recommender system: A semi-supervised approach using maximum margin matrix factorization</title><link>http://arxiv.org/abs/2306.13050v2</link><description>Collaborative filtering (CF) has become a popular method for developingrecommender systems (RSs) where ratings of a user for new items are predictedbased on her past preferences and available preference information of otherusers. Despite the popularity of CF-based methods, their performance is oftengreatly limited by the sparsity of observed entries. In this study, we explorethe data augmentation and refinement aspects of Maximum Margin MatrixFactorization (MMMF), a widely accepted CF technique for rating predictions,which has not been investigated before. We exploit the inherent characteristicsof CF algorithms to assess the confidence level of individual ratings andpropose a semi-supervised approach for rating augmentation based onself-training. We hypothesize that any CF algorithm's predictions with lowconfidence are due to some deficiency in the training data and hence, theperformance of the algorithm can be improved by adopting a systematic dataaugmentation strategy. We iteratively use some of the ratings predicted withhigh confidence to augment the training data and remove low-confidence entriesthrough a refinement process. By repeating this process, the system learns toimprove prediction accuracy. Our method is experimentally evaluated on severalstate-of-the-art CF algorithms and leads to informative rating augmentation,improving the performance of the baseline approaches.</description><author>Shamal Shaikh, Venkateswara Rao Kagita, Vikas Kumar, Arun K Pujari</author><pubDate>Tue, 15 Aug 2023 18:39:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13050v2</guid></item><item><title>Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</title><link>http://arxiv.org/abs/2308.01246v2</link><description>Digital preservation of Cultural Heritage (CH) sites is crucial to protectthem against damage from natural disasters or human activities. Creating 3Dmodels of CH sites has become a popular method of digital preservation thanksto advancements in computer vision and photogrammetry. However, the process istime-consuming, expensive, and typically requires specialized equipment andexpertise, posing challenges in resource-limited developing countries.Additionally, the lack of an open repository for 3D models hinders research andpublic engagement with their heritage. To address these issues, we proposeTirtha, a web platform for crowdsourcing images of CH sites and creating their3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) andMulti-View Stereo (MVS) techniques. It is modular, extensible andcost-effective, allowing for the incorporation of new techniques asphotogrammetry advances. Tirtha is accessible through a web interface athttps://tirtha.niser.ac.in and can be deployed on-premise or in a cloudenvironment. In our case studies, we demonstrate the pipeline's effectivenessby creating 3D models of temples in Odisha, India, using crowdsourced images.These models are available for viewing, interaction, and download on the Tirthawebsite. Our work aims to provide a dataset of crowdsourced images and 3Dreconstructions for research in computer vision, heritage conservation, andrelated domains. Overall, Tirtha is a step towards democratizing digitalpreservation, primarily in resource-limited developing countries.</description><author>Jyotirmaya Shivottam, Subhankar Mishra</author><pubDate>Tue, 15 Aug 2023 18:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01246v2</guid></item><item><title>Probabilistic Phase Labeling and Lattice Refinement for Autonomous Material Research</title><link>http://arxiv.org/abs/2308.07897v1</link><description>X-ray diffraction (XRD) is an essential technique to determine a material'scrystal structure in high-throughput experimentation, and has recently beenincorporated in artificially intelligent agents in autonomous scientificdiscovery processes. However, rapid, automated and reliable analysis method ofXRD data matching the incoming data rate remains a major challenge. To addressthese issues, we present CrystalShift, an efficient algorithm for probabilisticXRD phase labeling that employs symmetry-constrained pseudo-refinementoptimization, best-first tree search, and Bayesian model comparison to estimateprobabilities for phase combinations without requiring phase space informationor training. We demonstrate that CrystalShift provides robust probabilityestimates, outperforming existing methods on synthetic and experimentaldatasets, and can be readily integrated into high-throughput experimentalworkflows. In addition to efficient phase-mapping, CrystalShift offersquantitative insights into materials' structural parameters, which facilitateboth expert evaluation and AI-based modeling of the phase space, ultimatelyaccelerating materials identification and discovery.</description><author>Ming-Chiang Chang, Sebastian Ament, Maximilian Amsler, Duncan R. Sutherland, Lan Zhou, John M. Gregoire, Carla P. Gomes, R. Bruce van Dover, Michael O. Thompson</author><pubDate>Tue, 15 Aug 2023 18:38:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07897v1</guid></item><item><title>Subset-Based Instance Optimality in Private Estimation</title><link>http://arxiv.org/abs/2303.01262v2</link><description>We propose a new definition of instance optimality for differentially privateestimation algorithms. Our definition requires an optimal algorithm to compete,simultaneously for every dataset $D$, with the best private benchmark algorithmthat (a) knows $D$ in advance and (b) is evaluated by its worst-caseperformance on large subsets of $D$. That is, the benchmark algorithm need notperform well when potentially extreme points are added to $D$; it only has tohandle the removal of a small number of real data points that already exist.This makes our benchmark significantly stronger than those proposed in priorwork. We nevertheless show, for real-valued datasets, how to construct privatealgorithms that achieve our notion of instance optimality when estimating abroad class of dataset properties, including means, quantiles, and$\ell_p$-norm minimizers. For means in particular, we provide a detailedanalysis and show that our algorithm simultaneously matches or exceeds theasymptotic performance of existing algorithms under a range of distributionalassumptions.</description><author>Travis Dick, Alex Kulesza, Ziteng Sun, Ananda Theertha Suresh</author><pubDate>Tue, 15 Aug 2023 18:38:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01262v2</guid></item><item><title>SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation</title><link>http://arxiv.org/abs/2308.07896v1</link><description>Diffusion probabilistic models (DPMs) are a powerful class of generativemodels known for their ability to generate high-fidelity image samples. A majorchallenge in the implementation of DPMs is the slow sampling process. In thiswork, we bring a high-efficiency sampler for DPMs. Specifically, we propose ascore-based exact solution paradigm for the diffusion ODEs corresponding to thesampling process of DPMs, which introduces a new perspective on developingnumerical algorithms for solving diffusion ODEs. To achieve an efficientsampler, we propose a recursive derivative estimation (RDE) method to reducethe estimation error. With our proposed solution paradigm and RDE method, wepropose the score-integrand solver with the convergence order guarantee asefficient solver (SciRE-Solver) for solving diffusion ODEs. The SciRE-Solverattains state-of-the-art (SOTA) sampling performance with a limited number ofscore function evaluations (NFE) on both discrete-time and continuous-time DPMsin comparison to existing training-free sampling algorithms. Such as, weachieve $3.48$ FID with $12$ NFE and $2.42$ FID with $20$ NFE forcontinuous-time DPMs on CIFAR10, respectively. Different from other samplers,SciRE-Solver has the promising potential to surpass the FIDs achieved in theoriginal papers of some pre-trained models with just fewer NFEs. For example,we reach SOTA value of $2.40$ FID with $100$ NFE for continuous-time DPM and of$3.15$ FID with $84$ NFE for discrete-time DPM on CIFAR-10, as well as of$2.17$ ($2.02$) FID with $18$ ($50$) NFE for discrete-time DPM on CelebA64$\times$64.</description><author>Shigui Li, Wei Chen, Delu Zeng</author><pubDate>Tue, 15 Aug 2023 18:37:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07896v1</guid></item><item><title>Memory-and-Anticipation Transformer for Online Action Understanding</title><link>http://arxiv.org/abs/2308.07893v1</link><description>Most existing forecasting systems are memory-based methods, which attempt tomimic human forecasting ability by employing various memory mechanisms and haveprogressed in temporal modeling for memory dependency. Nevertheless, an obviousweakness of this paradigm is that it can only model limited historicaldependence and can not transcend the past. In this paper, we rethink thetemporal dependence of event evolution and propose a novelmemory-anticipation-based paradigm to model an entire temporal structure,including the past, present, and future. Based on this idea, we presentMemory-and-Anticipation Transformer (MAT), a memory-anticipation-basedapproach, to address the online action detection and anticipation tasks. Inaddition, owing to the inherent superiority of MAT, it can process onlineaction detection and anticipation tasks in a unified manner. The proposed MATmodel is tested on four challenging benchmarks TVSeries, THUMOS'14, HDD, andEPIC-Kitchens-100, for online action detection and anticipation tasks, and itsignificantly outperforms all existing methods. Code is available athttps://github.com/Echo0125/Memory-and-Anticipation-Transformer.</description><author>Jiahao Wang, Guo Chen, Yifei Huang, Limin Wang, Tong Lu</author><pubDate>Tue, 15 Aug 2023 18:34:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07893v1</guid></item><item><title>Whose Emotion Matters? Speaking Activity Localisation without Prior Knowledge</title><link>http://arxiv.org/abs/2211.15377v4</link><description>The task of emotion recognition in conversations (ERC) benefits from theavailability of multiple modalities, as provided, for example, in thevideo-based Multimodal EmotionLines Dataset (MELD). However, only a fewresearch approaches use both acoustic and visual information from the MELDvideos. There are two reasons for this: First, label-to-video alignments inMELD are noisy, making those videos an unreliable source of emotional speechdata. Second, conversations can involve several people in the same scene, whichrequires the localisation of the utterance source. In this paper, we introduceMELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by usingrecent active speaker detection and automatic speech recognition models, we areable to realign the videos of MELD and capture the facial expressions fromspeakers in 96.92% of the utterances provided in MELD. Experiments with aself-supervised voice recognition model indicate that the realigned MELD-FAIRvideos more closely match the transcribed utterances given in the MELD dataset.Finally, we devise a model for emotion recognition in conversations trained onthe realigned MELD-FAIR videos, which outperforms state-of-the-art models forERC based on vision alone. This indicates that localising the source ofspeaking activities is indeed effective for extracting facial expressions fromthe uttering speakers and that faces provide more informative visual cues thanthe visual features state-of-the-art models have been using so far. TheMELD-FAIR realignment data, and the code of the realignment procedure and ofthe emotional recognition, are available athttps://github.com/knowledgetechnologyuhh/MELD-FAIR.</description><author>Hugo Carneiro, Cornelius Weber, Stefan Wermter</author><pubDate>Tue, 15 Aug 2023 18:33:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15377v4</guid></item><item><title>Link-Context Learning for Multimodal LLMs</title><link>http://arxiv.org/abs/2308.07891v1</link><description>The ability to learn from context with novel concepts, and deliverappropriate responses are essential in human conversations. Despite currentMultimodal Large Language Models (MLLMs) and Large Language Models (LLMs) beingtrained on mega-scale datasets, recognizing unseen images or understandingnovel concepts in a training-free manner remains a challenge. In-ContextLearning (ICL) explores training-free few-shot learning, where models areencouraged to ``learn to learn" from limited tasks and generalize to unseentasks. In this work, we propose link-context learning (LCL), which emphasizes"reasoning from cause and effect" to augment the learning capabilities ofMLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causalrelationship between the support set and the query set. By providingdemonstrations with causal links, LCL guides the model to discern not only theanalogy but also the underlying causal associations between data points, whichempowers MLLMs to recognize unseen images and understand novel concepts moreeffectively. To facilitate the evaluation of this novel approach, we introducethe ISEKAI dataset, comprising exclusively of unseen generated image-labelpairs designed for link-context learning. Extensive experiments show that ourLCL-MLLM exhibits strong link-context learning capabilities to novel conceptsover vanilla MLLMs. Code and data will be released athttps://github.com/isekai-portal/Link-Context-Learning.</description><author>Yan Tai, Weichen Fan, Zhao Zhang, Feng Zhu, Rui Zhao, Ziwei Liu</author><pubDate>Tue, 15 Aug 2023 18:33:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07891v1</guid></item><item><title>EduSAT: A Pedagogical Tool for Theory and Applications of Boolean Satisfiability</title><link>http://arxiv.org/abs/2308.07890v1</link><description>Boolean Satisfiability (SAT) and Satisfiability Modulo Theories (SMT) arewidely used in automated verification, but there is a lack of interactive toolsdesigned for educational purposes in this field. To address this gap, wepresent EduSAT, a pedagogical tool specifically developed to support learningand understanding of SAT and SMT solving. EduSAT offers implementations of keyalgorithms such as the Davis-Putnam-Logemann-Loveland (DPLL) algorithm and theReduced Order Binary Decision Diagram (ROBDD) for SAT solving. Additionally,EduSAT provides solver abstractions for five NP-complete problems beyond SATand SMT. Users can benefit from EduSAT by experimenting, analyzing, andvalidating their understanding of SAT and SMT solving techniques. Our tool isaccompanied by comprehensive documentation and tutorials, extensive testing,and practical features such as a natural language interface and SAT and SMTformula generators, which also serve as a valuable opportunity for learners todeepen their understanding. Our evaluation of EduSAT demonstrates its highaccuracy, achieving 100% correctness across all the implemented SAT and SMTsolvers. We release EduSAT as a python package in .whl file, and the source canbe identified at https://github.com/zhaoy37/SAT_Solver.</description><author>Yiqi Zhao, Ziyan An, Meiyi Ma, Taylor Johnson</author><pubDate>Tue, 15 Aug 2023 18:31:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07890v1</guid></item><item><title>A Comprehensive Study on Knowledge Graph Embedding over Relational Patterns Based on Rule Learning</title><link>http://arxiv.org/abs/2308.07889v1</link><description>Knowledge Graph Embedding (KGE) has proven to be an effective approach tosolving the Knowledge Graph Completion (KGC) task. Relational patterns whichrefer to relations with specific semantics exhibiting graph patterns are animportant factor in the performance of KGE models. Though KGE models'capabilities are analyzed over different relational patterns in theory and arough connection between better relational patterns modeling and betterperformance of KGC has been built, a comprehensive quantitative analysis on KGEmodels over relational patterns remains absent so it is uncertain how thetheoretical support of KGE to a relational pattern contributes to theperformance of triples associated to such a relational pattern. To address thischallenge, we evaluate the performance of 7 KGE models over 4 common relationalpatterns on 2 benchmarks, then conduct an analysis in theory, entity frequency,and part-to-whole three aspects and get some counterintuitive conclusions.Finally, we introduce a training-free method Score-based Patterns Adaptation(SPA) to enhance KGE models' performance over various relational patterns. Thisapproach is simple yet effective and can be applied to KGE models withoutadditional training. Our experimental results demonstrate that our methodgenerally enhances performance over specific relational patterns. Our sourcecode is available from GitHub athttps://github.com/zjukg/Comprehensive-Study-over-Relational-Patterns.</description><author>Long Jin, Zhen Yao, Mingyang Chen, Huajun Chen, Wen Zhang</author><pubDate>Tue, 15 Aug 2023 18:30:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07889v1</guid></item><item><title>On regularized Radon-Nikodym differentiation</title><link>http://arxiv.org/abs/2308.07887v1</link><description>We discuss the problem of estimating Radon-Nikodym derivatives. This problemappears in various applications, such as covariate shift adaptation,likelihood-ratio testing, mutual information estimation, and conditionalprobability estimation. To address the above problem, we employ the generalregularization scheme in reproducing kernel Hilbert spaces. The convergencerate of the corresponding regularized algorithm is established by taking intoaccount both the smoothness of the derivative and the capacity of the space inwhich it is estimated. This is done in terms of general source conditions andthe regularized Christoffel functions. We also find that the reconstruction ofRadon-Nikodym derivatives at any particular point can be done with high orderof accuracy. Our theoretical results are illustrated by numerical simulations.</description><author>Duc Hoan Nguyen, Werner Zellinger, Sergei V. Pereverzyev</author><pubDate>Tue, 15 Aug 2023 18:27:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07887v1</guid></item><item><title>Back to Basics: A Sanity Check on Modern Time Series Classification Algorithms</title><link>http://arxiv.org/abs/2308.07886v1</link><description>The state-of-the-art in time series classification has come a long way, fromthe 1NN-DTW algorithm to the ROCKET family of classifiers. However, in thecurrent fast-paced development of new classifiers, taking a step back andperforming simple baseline checks is essential. These checks are oftenoverlooked, as researchers are focused on establishing new state-of-the-artresults, developing scalable algorithms, and making models explainable.Nevertheless, there are many datasets that look like time series at firstglance, but classic algorithms such as tabular methods with no time orderingmay perform better on such problems. For example, for spectroscopy datasets,tabular methods tend to significantly outperform recent time series methods. Inthis study, we compare the performance of tabular models using classic machinelearning approaches (e.g., Ridge, LDA, RandomForest) with the ROCKET family ofclassifiers (e.g., Rocket, MiniRocket, MultiRocket). Tabular models are simpleand very efficient, while the ROCKET family of classifiers are more complex andhave state-of-the-art accuracy and efficiency among recent time seriesclassifiers. We find that tabular models outperform the ROCKET family ofclassifiers on approximately 19% of univariate and 28% of multivariate datasetsin the UCR/UEA benchmark and achieve accuracy within 10 percentage points onabout 50% of datasets. Our results suggest that it is important to considersimple tabular models as baselines when developing time series classifiers.These models are very fast, can be as effective as more complex methods and maybe easier to understand and deploy.</description><author>Bhaskar Dhariyal, Thach Le Nguyen, Georgiana Ifrim</author><pubDate>Tue, 15 Aug 2023 18:23:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07886v1</guid></item><item><title>The Challenge of Fetal Cardiac MRI Reconstruction Using Deep Learning</title><link>http://arxiv.org/abs/2308.07885v1</link><description>Dynamic free-breathing fetal cardiac MRI is one of the most challengingmodalities, which requires high temporal and spatial resolution to depict rapidchanges in a small fetal heart. The ability of deep learning methods to recoverundersampled data could help to optimise the kt-SENSE acquisition strategy andimprove non-gated kt-SENSE reconstruction quality. In this work, we exploresupervised deep learning networks for reconstruction of kt-SENSE style acquireddata using an extensive in vivo dataset. Having access to fully-sampledlow-resolution multi-coil fetal cardiac MRI, we study the performance of thenetworks to recover fully-sampled data from undersampled data. We considermodel architectures together with training strategies taking into account theirapplication in the real clinical setup used to collect the dataset to enablenetworks to recover prospectively undersampled data. We explore a set ofmodifications to form a baseline performance evaluation for dynamic fetalcardiac MRI on real data. We systematically evaluate the models oncoil-combined data to reveal the effect of the suggested changes to thearchitecture in the context of fetal heart properties. We show that thebest-performers recover a detailed depiction of the maternal anatomy on a largescale, but the dynamic properties of the fetal heart are under-represented.Training directly on multi-coil data improves the performance of the models,allows their prospective application to undersampled data and makes themoutperform CTFNet introduced for adult cardiac cine MRI. However, these modelsdeliver similar qualitative performances recovering the maternal body very wellbut underestimating the dynamic properties of fetal heart. This dynamic featureof fast change of fetal heart that is highly localised suggests both moretargeted training and evaluation methods might be needed for fetal heartapplication.</description><author>Denis Prokopenko, Kerstin Hammernik, Thomas Roberts, David F A Lloyd, Daniel Rueckert, Joseph V Hajnal</author><pubDate>Tue, 15 Aug 2023 18:22:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07885v1</guid></item><item><title>Towards Temporal Edge Regression: A Case Study on Agriculture Trade Between Nations</title><link>http://arxiv.org/abs/2308.07883v1</link><description>Recently, Graph Neural Networks (GNNs) have shown promising performance intasks on dynamic graphs such as node classification, link prediction and graphregression. However, few work has studied the temporal edge regression taskwhich has important real-world applications. In this paper, we explore theapplication of GNNs to edge regression tasks in both static and dynamicsettings, focusing on predicting food and agriculture trade values betweennations. We introduce three simple yet strong baselines and comprehensivelyevaluate one static and three dynamic GNN models using the UN Trade dataset.Our experimental results reveal that the baselines exhibit remarkably strongperformance across various settings, highlighting the inadequacy of existingGNNs. We also find that TGN outperforms other GNN models, suggesting TGN is amore appropriate choice for edge regression tasks. Moreover, we note that theproportion of negative edges in the training samples significantly affects thetest performance. The companion source code can be found at:https://github.com/scylj1/GNN_Edge_Regression.</description><author>Lekang Jiang, Caiqi Zhang, Farimah Poursafaei, Shenyang Huang</author><pubDate>Tue, 15 Aug 2023 18:13:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07883v1</guid></item><item><title>GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking</title><link>http://arxiv.org/abs/2305.18144v2</link><description>Retrieval-enhanced text generation has shown remarkable progress onknowledge-intensive language tasks, such as open-domain question answering andknowledge-enhanced dialogue generation, by leveraging passages retrieved from alarge passage corpus for delivering a proper answer given the input query.However, the retrieved passages are not ideal for guiding answer generationbecause of the discrepancy between retrieval and generation, i.e., thecandidate passages are all treated equally during the retrieval procedurewithout considering their potential to generate a proper answer. Thisdiscrepancy makes a passage retriever deliver a sub-optimal collection ofcandidate passages to generate the answer. In this paper, we propose theGeneRative Knowledge Improved Passage Ranking (GripRank) approach, addressingthe above challenge by distilling knowledge from a generative passage estimator(GPE) to a passage ranker, where the GPE is a generative language model used tomeasure how likely the candidate passages can generate the proper answer. Werealize the distillation procedure by teaching the passage ranker learning torank the passages ordered by the GPE. Furthermore, we improve the distillationquality by devising a curriculum knowledge distillation mechanism, which allowsthe knowledge provided by the GPE can be progressively distilled to the rankerthrough an easy-to-hard curriculum, enabling the passage ranker to correctlyrecognize the provenance of the answer from many plausible candidates. Weconduct extensive experiments on four datasets across three knowledge-intensivelanguage tasks. Experimental results show advantages over the state-of-the-artmethods for both passage ranking and answer generation on the KILT benchmark.</description><author>Jiaqi Bai, Hongcheng Guo, Jiaheng Liu, Jian Yang, Xinnian Liang, Zhao Yan, Zhoujun Li</author><pubDate>Tue, 15 Aug 2023 18:10:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18144v2</guid></item><item><title>SEMI-CenterNet: A Machine Learning Facilitated Approach for Semiconductor Defect Inspection</title><link>http://arxiv.org/abs/2308.07180v2</link><description>Continual shrinking of pattern dimensions in the semiconductor domain ismaking it increasingly difficult to inspect defects due to factors such as thepresence of stochastic noise and the dynamic behavior of defect patterns andtypes. Conventional rule-based methods and non-parametric supervised machinelearning algorithms like KNN mostly fail at the requirements of semiconductordefect inspection at these advanced nodes. Deep Learning (DL)-based methodshave gained popularity in the semiconductor defect inspection domain becausethey have been proven robust towards these challenging scenarios. In thisresearch work, we have presented an automated DL-based approach for efficientlocalization and classification of defects in SEM images. We have proposedSEMI-CenterNet (SEMI-CN), a customized CN architecture trained on SEM images ofsemiconductor wafer defects. The use of the proposed CN approach allowsimproved computational efficiency compared to previously studied DL models.SEMI-CN gets trained to output the center, class, size, and offset of a defectinstance. This is different from the approach of most object detection modelsthat use anchors for bounding box prediction. Previous methods predictredundant bounding boxes, most of which are discarded in postprocessing. CNmitigates this by only predicting boxes for likely defect center points. Wetrain SEMI-CN on two datasets and benchmark two ResNet backbones for theframework. Initially, ResNet models pretrained on the COCO dataset undergotraining using two datasets separately. Primarily, SEMI-CN shows significantimprovement in inference time against previous research works. Finally,transfer learning (using weights of custom SEM dataset) is applied from ADIdataset to AEI dataset and vice-versa, which reduces the required training timefor both backbones to reach the best mAP against conventional training method.</description><author>Vic De Ridder, Bappaditya Dey, Enrique Dehaerne, Sandip Halder, Stefan De Gendt, Bartel Van Waeyenberge</author><pubDate>Tue, 15 Aug 2023 17:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07180v2</guid></item><item><title>The $10 Million ANA Avatar XPRIZE Competition Advanced Immersive Telepresence Systems</title><link>http://arxiv.org/abs/2308.07878v1</link><description>The $10M ANA Avatar XPRIZE aimed to create avatar systems that can transporthuman presence to remote locations in real time. The participants of thismulti-year competition developed robotic systems that allow operators to see,hear, and interact with a remote environment in a way that feels as if they aretruly there. On the other hand, people in the remote environment were given theimpression that the operator was present inside the avatar robot. At thecompetition finals, held in November 2022 in Long Beach, CA, USA, the avatarsystems were evaluated on their support for remotely interacting with humans,exploring new environments, and employing specialized skills. This articledescribes the competition stages with tasks and evaluation procedures, reportsthe results, presents the winning teams' approaches, and discusses lessonslearned.</description><author>Sven Behnke, Julie A. Adams, David Locke</author><pubDate>Tue, 15 Aug 2023 17:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07878v1</guid></item><item><title>Undersampling and Cumulative Class Re-decision Methods to Improve Detection of Agitation in People with Dementia</title><link>http://arxiv.org/abs/2302.03224v3</link><description>Agitation is one of the most prevalent symptoms in people with dementia (PwD)that can place themselves and the caregiver's safety at risk. Developingobjective agitation detection approaches is important to support health andsafety of PwD living in a residential setting. In a previous study, wecollected multimodal wearable sensor data from 17 participants for 600 days anddeveloped machine learning models for detecting agitation in one-minutewindows. However, there are significant limitations in the dataset, such asimbalance problem and potential imprecise labelsas the occurrence of agitationis much rarer in comparison to the normal behaviours. In this paper, we firstimplemented different undersampling methods to eliminate the imbalance problem,and came to the conclusion that only 20% of normal behaviour data were adequateto train a competitive agitation detection model. Then, we designed a weightedundersampling method to evaluate the manual labeling mechanism given theambiguous time interval assumption. After that, the postprocessing method ofcumulative class re-decision (CCR) was proposed based on the historicalsequential information and continuity characteristic of agitation, improvingthe decision-making performance for the potential application of agitationdetection system. The results showed that a combination of undersampling andCCR improved F1-score and other metrics to varying degrees with less trainingtime and data.</description><author>Zhidong Meng, Andrea Iaboni, Bing Ye, Kristine Newman, Alex Mihailidis, Zhihong Deng, Shehroz S. Khan</author><pubDate>Tue, 15 Aug 2023 17:44:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03224v3</guid></item><item><title>DIG In: Evaluating Disparities in Image Generations with Indicators for Geographic Diversity</title><link>http://arxiv.org/abs/2308.06198v2</link><description>The unprecedented photorealistic results achieved by recent text-to-imagegenerative systems and their increasing use as plug-and-play content creationsolutions make it crucial to understand their potential biases. In this work,we introduce three indicators to evaluate the realism, diversity andprompt-generation consistency of text-to-image generative systems when promptedto generate objects from across the world. Our indicators complementqualitative analysis of the broader impact of such systems by enablingautomatic and efficient benchmarking of geographic disparities, an importantstep towards building responsible visual content creation systems. We use ourproposed indicators to analyze potential geographic biases in state-of-the-artvisual content creation systems and find that: (1) models have less realism anddiversity of generations when prompting for Africa and West Asia than Europe,(2) prompting with geographic information comes at a cost to prompt-consistencyand diversity of generated images, and (3) models exhibit more region-leveldisparities for some objects than others. Perhaps most interestingly, ourindicators suggest that progress in image generation quality has come at thecost of real-world geographic representation. Our comprehensive evaluationconstitutes a crucial step towards ensuring a positive experience of visualcontent creation for everyone.</description><author>Melissa Hall, Candace Ross, Adina Williams, Nicolas Carion, Michal Drozdzal, Adriana Romero Soriano</author><pubDate>Tue, 15 Aug 2023 17:42:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06198v2</guid></item><item><title>Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT</title><link>http://arxiv.org/abs/2308.07876v1</link><description>Recent supervised models for event coding vastly outperform pattern-matchingmethods. However, their reliance solely on new annotations disregards the vastknowledge within expert databases, hindering their applicability tofine-grained classification. To address these limitations, we explore zero-shotapproaches for political event ontology relation classification, by leveragingknowledge from established annotation codebooks. Our study encompasses bothChatGPT and a novel natural language inference (NLI) based approach named ZSP.ZSP adopts a tree-query framework that deconstructs the task into context,modality, and class disambiguation levels. This framework improvesinterpretability, efficiency, and adaptability to schema changes. By conductingextensive experiments on our newly curated datasets, we pinpoint theinstability issues within ChatGPT and highlight the superior performance ofZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grainedRootcode classification. ZSP demonstrates competitive performance compared tosupervised BERT models, positioning it as a valuable tool for event recordvalidation and ontology development. Our work underscores the potential ofleveraging transfer learning and existing expertise to enhance the efficiencyand scalability of research in the field.</description><author>Yibo Hu, Erick Skorupa Parolin, Latifur Khan, Patrick T. Brandt, Javier Osorio, Vito J. D'Orazio</author><pubDate>Tue, 15 Aug 2023 17:41:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07876v1</guid></item><item><title>Stack More Layers Differently: High-Rank Training Through Low-Rank Updates</title><link>http://arxiv.org/abs/2307.05695v3</link><description>Despite the dominance and effectiveness of scaling, resulting in largenetworks with hundreds of billions of parameters, the necessity to trainoverparametrized models remains poorly understood, and alternative approachesdo not necessarily make it cheaper to train high-performance models. In thispaper, we explore low-rank training techniques as an alternative approach totraining large neural networks. We introduce a novel method called ReLoRA,which utilizes low-rank updates to train high-rank networks. We apply ReLoRA topre-training transformer language models with up to 350M parameters anddemonstrate comparable performance to regular neural network training.Furthermore, we observe that the efficiency of ReLoRA increases with modelsize, making it a promising approach for training multi-billion-parameternetworks efficiently. Our findings shed light on the potential of low-ranktraining techniques and their implications for scaling laws.</description><author>Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, Anna Rumshisky</author><pubDate>Tue, 15 Aug 2023 17:41:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05695v3</guid></item><item><title>SEDA: Self-Ensembling ViT with Defensive Distillation and Adversarial Training for robust Chest X-rays Classification</title><link>http://arxiv.org/abs/2308.07874v1</link><description>Deep Learning methods have recently seen increased adoption in medicalimaging applications. However, elevated vulnerabilities have been explored inrecent Deep Learning solutions, which can hinder future adoption. Particularly,the vulnerability of Vision Transformer (ViT) to adversarial, privacy, andconfidentiality attacks raise serious concerns about their reliability inmedical settings. This work aims to enhance the robustness of self-ensemblingViTs for the tuberculosis chest x-ray classification task. We proposeSelf-Ensembling ViT with defensive Distillation and Adversarial training(SEDA). SEDA utilizes efficient CNN blocks to learn spatial features withvarious levels of abstraction from feature representations extracted fromintermediate ViT blocks, that are largely unaffected by adversarialperturbations. Furthermore, SEDA leverages adversarial training in combinationwith defensive distillation for improved robustness against adversaries.Training using adversarial examples leads to better model generalizability andimproves its ability to handle perturbations. Distillation using softprobabilities introduces uncertainty and variation into the outputprobabilities, making it more difficult for adversarial and privacy attacks.Extensive experiments performed with the proposed architecture and trainingparadigm on publicly available Tuberculosis x-ray dataset shows SOTA efficacyof SEDA compared to SEViT in terms of computational efficiency with 70x timeslighter framework and enhanced robustness of +9%.</description><author>Raza Imam, Ibrahim Almakky, Salma Alrashdi, Baketah Alrashdi, Mohammad Yaqub</author><pubDate>Tue, 15 Aug 2023 17:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07874v1</guid></item><item><title>Emotion Embeddings $\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets</title><link>http://arxiv.org/abs/2308.07871v1</link><description>Human emotion is expressed in many communication modalities and media formatsand so their computational study is equally diversified into natural languageprocessing, audio signal analysis, computer vision, etc. Similarly, the largevariety of representation formats used in previous research to describeemotions (polarity scales, basic emotion categories, dimensional approaches,appraisal theory, etc.) have led to an ever proliferating diversity ofdatasets, predictive models, and software tools for emotion analysis. Becauseof these two distinct types of heterogeneity, at the expressional andrepresentational level, there is a dire need to unify previous work onincreasingly diverging data and label types. This article presents such aunifying computational model. We propose a training procedure that learns ashared latent representation for emotions, so-called emotion embeddings,independent of different natural languages, communication modalities, media orrepresentation label formats, and even disparate model architectures.Experiments on a wide range of heterogeneous affective datasets indicate thatthis approach yields the desired interoperability for the sake of reusability,interpretability and flexibility, without penalizing prediction quality. Codeand data are archived under https://doi.org/10.5281/zenodo.7405327 .</description><author>Sven Buechel, Udo Hahn</author><pubDate>Tue, 15 Aug 2023 17:39:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07871v1</guid></item><item><title>Brain-Inspired Computational Intelligence via Predictive Coding</title><link>http://arxiv.org/abs/2308.07870v1</link><description>Artificial intelligence (AI) is rapidly becoming one of the key technologiesof this century. The majority of results in AI thus far have been achievedusing deep neural networks trained with the error backpropagation learningalgorithm. However, the ubiquitous adoption of this approach has highlightedsome important limitations such as substantial computational cost, difficultyin quantifying uncertainty, lack of robustness, unreliability, and biologicalimplausibility. It is possible that addressing these limitations may requireschemes that are inspired and guided by neuroscience theories. One such theory,called predictive coding (PC), has shown promising performance in machineintelligence tasks, exhibiting exciting properties that make it potentiallyvaluable for the machine learning community: PC can model informationprocessing in different brain areas, can be used in cognitive control androbotics, and has a solid mathematical grounding in variational inference,offering a powerful inversion scheme for a specific class of continuous-stategenerative models. With the hope of foregrounding research in this direction,we survey the literature that has contributed to this perspective, highlightingthe many ways that PC might play a role in the future of machine learning andcomputational intelligence at large.</description><author>Tommaso Salvatori, Ankur Mali, Christopher L. Buckley, Thomas Lukasiewicz, Rajesh P. N. Rao, Karl Friston, Alexander Ororbia</author><pubDate>Tue, 15 Aug 2023 17:37:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07870v1</guid></item><item><title>ObjectSDF++: Improved Object-Compositional Neural Implicit Surfaces</title><link>http://arxiv.org/abs/2308.07868v1</link><description>In recent years, neural implicit surface reconstruction has emerged as apopular paradigm for multi-view 3D reconstruction. Unlike traditionalmulti-view stereo approaches, the neural implicit surface-based methodsleverage neural networks to represent 3D scenes as signed distance functions(SDFs). However, they tend to disregard the reconstruction of individualobjects within the scene, which limits their performance and practicalapplications. To address this issue, previous work ObjectSDF introduced a niceframework of object-composition neural implicit surfaces, which utilizes 2Dinstance masks to supervise individual object SDFs. In this paper, we propose anew framework called ObjectSDF++ to overcome the limitations of ObjectSDF.First, in contrast to ObjectSDF whose performance is primarily restricted byits converted semantic field, the core component of our model is anocclusion-aware object opacity rendering formulation that directlyvolume-renders object opacity to be supervised with instance masks. Second, wedesign a novel regularization term for object distinction, which caneffectively mitigate the issue that ObjectSDF may result in unexpectedreconstruction in invisible regions due to the lack of constraint to preventcollisions. Our extensive experiments demonstrate that our novel framework notonly produces superior object reconstruction results but also significantlyimproves the quality of scene reconstruction. Code and more resources can befound in \url{https://qianyiwu.github.io/objectsdf++}</description><author>Qianyi Wu, Kaisiyuan Wang, Kejie Li, Jianmin Zheng, Jianfei Cai</author><pubDate>Tue, 15 Aug 2023 17:35:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07868v1</guid></item><item><title>Graph-Structured Kernel Design for Power Flow Learning using Gaussian Processes</title><link>http://arxiv.org/abs/2308.07867v1</link><description>This paper presents a physics-inspired graph-structured kernel designed forpower flow learning using Gaussian Process (GP). The kernel, named thevertex-degree kernel (VDK), relies on latent decomposition of voltage-injectionrelationship based on the network graph or topology. Notably, VDK design avoidsthe need to solve optimization problems for kernel search. To enhanceefficiency, we also explore a graph-reduction approach to obtain a VDKrepresentation with lesser terms. Additionally, we propose a novelnetwork-swipe active learning scheme, which intelligently selects sequentialtraining inputs to accelerate the learning of VDK. Leveraging the additivestructure of VDK, the active learning algorithm performs a block-descent typeprocedure on GP's predictive variance, serving as a proxy for information gain.Simulations demonstrate that the proposed VDK-GP achieves more than two foldsample complexity reduction, compared to full GP on medium scale 500-Bus andlarge scale 1354-Bus power systems. The network-swipe algorithm outperformsmean performance of 500 random trials on test predictions by two fold formedium-sized 500-Bus systems and best performance of 25 random trials forlarge-scale 1354-Bus systems by 10%. Moreover, we demonstrate that the proposedmethod's performance for uncertainty quantification applications withdistributionally shifted testing data sets.</description><author>Parikshit Pareek, Deepjyoti Deka, Sidhant Misra</author><pubDate>Tue, 15 Aug 2023 17:34:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07867v1</guid></item><item><title>StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models</title><link>http://arxiv.org/abs/2308.07863v1</link><description>Content and style (C-S) disentanglement is a fundamental problem and criticalchallenge of style transfer. Existing approaches based on explicit definitions(e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretablenor easy to control, resulting in entangled representations and less satisfyingresults. In this paper, we propose a new C-S disentangled framework for styletransfer without using previous assumptions. The key insight is to explicitlyextract the content information and implicitly learn the complementary styleinformation, yielding interpretable and controllable C-S disentanglement andstyle transfer. A simple yet effective CLIP-based style disentanglement losscoordinated with a style reconstruction prior is introduced to disentangle C-Sin the CLIP image space. By further leveraging the powerful style removal andgenerative ability of diffusion models, our framework achieves superior resultsthan state of the art and flexible C-S disentanglement and trade-off control.Our work provides new insights into the C-S disentanglement in style transferand demonstrates the potential of diffusion models for learningwell-disentangled C-S characteristics.</description><author>Zhizhong Wang, Lei Zhao, Wei Xing</author><pubDate>Tue, 15 Aug 2023 17:30:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07863v1</guid></item><item><title>PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in Poetry Generation</title><link>http://arxiv.org/abs/2306.08456v2</link><description>Controllable text generation is a challenging and meaningful field in naturallanguage generation (NLG). Especially, poetry generation is a typical one withwell-defined and strict conditions for text generation which is an idealplayground for the assessment of current methodologies. While prior workssucceeded in controlling either semantic or metrical aspects of poetrygeneration, simultaneously addressing both remains a challenge. In this paper,we pioneer the use of the Diffusion model for generating sonnets and ChineseSongCi poetry to tackle such challenges. In terms of semantics, ourPoetryDiffusion model, built upon the Diffusion model, generates entiresentences or poetry by comprehensively considering the entirety of sentenceinformation. This approach enhances semantic expression, distinguishing it fromautoregressive and large language models (LLMs). For metrical control, theseparation feature of diffusion generation and its constraint control moduleenable us to flexibly incorporate a novel metrical controller to manipulate andevaluate metrics (format and rhythm). The denoising process in PoetryDiffusionallows for gradual enhancement of semantics and flexible integration of themetrical controller which can calculate and impose penalties on states thatstray significantly from the target control distribution. Experimental resultson two datasets demonstrate that our model outperforms existing models inautomatic evaluation of semantic, metrical, and overall performance as well ashuman evaluation.</description><author>Zhiyuan Hu, Chumin Liu, Yue Feng, Anh Tuan Luu, Bryan Hooi</author><pubDate>Tue, 15 Aug 2023 17:21:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08456v2</guid></item><item><title>Impression-Aware Recommender Systems</title><link>http://arxiv.org/abs/2308.07857v1</link><description>Novel data sources bring new opportunities to improve the quality ofrecommender systems. Impressions are a novel data source containing pastrecommendations (shown items) and traditional interactions. Researchers may useimpressions to refine user preferences and overcome the current limitations inrecommender systems research. The relevance and interest of impressions haveincreased over the years; hence, the need for a review of relevant work on thistype of recommenders. We present a systematic literature review on recommendersystems using impressions, focusing on three fundamental angles in research:recommenders, datasets, and evaluation methodologies. We provide threecategorizations of papers describing recommenders using impressions, presenteach reviewed paper in detail, describe datasets with impressions, and analyzethe existing evaluation methodologies. Lastly, we present open questions andfuture directions of interest, highlighting aspects missing in the literaturethat can be addressed in future works.</description><author>Fernando B. Pérez Maurera, Maurizio Ferrari Dacrema, Pablo Castells, Paolo Cremonesi</author><pubDate>Tue, 15 Aug 2023 17:16:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07857v1</guid></item><item><title>Policy Regularization with Dataset Constraint for Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2306.06569v2</link><description>We consider the problem of learning the best possible policy from a fixeddataset, known as offline Reinforcement Learning (RL). A common taxonomy ofexisting offline RL works is policy regularization, which typically constrainsthe learned policy by distribution or support of the behavior policy. However,distribution and support constraints are overly conservative since they bothforce the policy to choose similar actions as the behavior policy whenconsidering particular states. It will limit the learned policy's performance,especially when the behavior policy is sub-optimal. In this paper, we find thatregularizing the policy towards the nearest state-action pair can be moreeffective and thus propose Policy Regularization with Dataset Constraint(PRDC). When updating the policy in a given state, PRDC searches the entiredataset for the nearest state-action sample and then restricts the policy withthe action of this sample. Unlike previous works, PRDC can guide the policywith proper behaviors from the dataset, allowing it to choose actions that donot appear in the dataset along with the given state. It is a softer constraintbut still keeps enough conservatism from out-of-distribution actions. Empiricalevidence and theoretical analysis show that PRDC can alleviate offline RL'sfundamentally challenging value overestimation issue with a bounded performancegap. Moreover, on a set of locomotion and navigation tasks, PRDC achievesstate-of-the-art performance compared with existing methods. Code is availableat https://github.com/LAMDA-RL/PRDC</description><author>Yuhang Ran, Yi-Chen Li, Fuxiang Zhang, Zongzhang Zhang, Yang Yu</author><pubDate>Tue, 15 Aug 2023 17:14:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06569v2</guid></item><item><title>A Framework For Refining Text Classification and Object Recognition from Academic Articles</title><link>http://arxiv.org/abs/2305.17401v3</link><description>With the widespread use of the internet, it has become increasingly crucialto extract specific information from vast amounts of academic articlesefficiently. Data mining techniques are generally employed to solve this issue.However, data mining for academic articles is challenging since it requiresautomatically extracting specific patterns in complex and unstructured layoutdocuments. Current data mining methods for academic articles employrule-based(RB) or machine learning(ML) approaches. However, using rule-basedmethods incurs a high coding cost for complex typesetting articles. On theother hand, simply using machine learning methods requires annotation work forcomplex content types within the paper, which can be costly. Furthermore, onlyusing machine learning can lead to cases where patterns easily recognized byrule-based methods are mistakenly extracted. To overcome these issues, from theperspective of analyzing the standard layout and typesetting used in thespecified publication, we emphasize implementing specific methods for specificcharacteristics in academic articles. We have developed a novel Text BlockRefinement Framework (TBRF), a machine learning and rule-based scheme hybrid.We used the well-known ACL proceeding articles as experimental data for thevalidation experiment. The experiment shows that our approach achieved over 95%classification accuracy and 90% detection accuracy for tables and figures.</description><author>Jinghong Li, Koichi Ota, Wen Gu, Shinobu Hasegawa</author><pubDate>Tue, 15 Aug 2023 17:02:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17401v3</guid></item><item><title>A Recipe for Well-behaved Graph Neural Approximations of Complex Dynamics</title><link>http://arxiv.org/abs/2301.04900v2</link><description>Data-driven approximations of ordinary differential equations offer apromising alternative to classical methods in discovering a dynamical systemmodel, particularly in complex systems lacking explicit first principles. Thispaper focuses on a complex system whose dynamics is described with a system ofordinary differential equations, coupled via a network adjacency matrix.Numerous real-world systems, including financial, social, and neural systems,belong to this class of dynamical models. We propose essential elements forapproximating such dynamical systems using neural networks, including necessarybiases and an appropriate neural architecture. Emphasizing the differences fromstatic supervised learning, we advocate for evaluating generalization beyondclassical assumptions of statistical learning theory. To estimate confidence inprediction during inference time, we introduce a dedicated null model. Bystudying various complex network dynamics, we demonstrate the neural network'sability to approximate various dynamics, generalize across complex networkstructures, sizes, and statistical properties of inputs. Our comprehensiveframework enables deep learning approximations of high-dimensional,non-linearly coupled complex dynamical systems.</description><author>Vaiva Vasiliauskaite, Nino Antulov-Fantulin</author><pubDate>Tue, 15 Aug 2023 16:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.04900v2</guid></item><item><title>Why Batch Normalization Damage Federated Learning on Non-IID Data?</title><link>http://arxiv.org/abs/2301.02982v2</link><description>As a promising distributed learning paradigm, federated learning (FL)involves training deep neural network (DNN) models at the network edge whileprotecting the privacy of the edge clients. To train a large-scale DNN model,batch normalization (BN) has been regarded as a simple and effective means toaccelerate the training and improve the generalization capability. However,recent findings indicate that BN can significantly impair the performance of FLin the presence of non-i.i.d. data. While several FL algorithms have beenproposed to address this issue, their performance still falls significantlywhen compared to the centralized scheme. Furthermore, none of them haveprovided a theoretical explanation of how the BN damages the FL convergence. Inthis paper, we present the first convergence analysis to show that under thenon-i.i.d. data, the mismatch between the local and global statisticalparameters in BN causes the gradient deviation between the local and globalmodels, which, as a result, slows down and biases the FL convergence. In viewof this, we develop a new FL algorithm that is tailored to BN, called FedTAN,which is capable of achieving robust FL performance under a variety of datadistributions via iterative layer-wise parameter aggregation. Comprehensiveexperimental results demonstrate the superiority of the proposed FedTAN overexisting baselines for training BN-based DNN models.</description><author>Yanmeng Wang, Qingjiang Shi, Tsung-Hui Chang</author><pubDate>Tue, 15 Aug 2023 16:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.02982v2</guid></item><item><title>Source-free Domain Adaptive Human Pose Estimation</title><link>http://arxiv.org/abs/2308.03202v3</link><description>Human Pose Estimation (HPE) is widely used in various fields, includingmotion analysis, healthcare, and virtual reality. However, the great expensesof labeled real-world datasets present a significant challenge for HPE. Toovercome this, one approach is to train HPE models on synthetic datasets andthen perform domain adaptation (DA) on real-world data. Unfortunately, existingDA methods for HPE neglect data privacy and security by using both source andtarget data in the adaptation process. To this end, we propose a new task,named source-free domain adaptive HPE, which aims to address the challenges ofcross-domain learning of HPE without access to source data during theadaptation process. We further propose a novel framework that consists of threemodels: source model, intermediate model, and target model, which explores thetask from both source-protect and target-relevant perspectives. Thesource-protect module preserves source information more effectively whileresisting noise, and the target-relevant module reduces the sparsity of spatialrepresentations by building a novel spatial probability space, andpose-specific contrastive learning and information maximization are proposed onthe basis of this space. Comprehensive experiments on several domain adaptiveHPE benchmarks show that the proposed method outperforms existing approaches bya considerable margin. The codes are available athttps://github.com/davidpengucf/SFDAHPE.</description><author>Qucheng Peng, Ce Zheng, Chen Chen</author><pubDate>Tue, 15 Aug 2023 16:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03202v3</guid></item><item><title>Dyadic Reinforcement Learning</title><link>http://arxiv.org/abs/2308.07843v1</link><description>Mobile health aims to enhance health outcomes by delivering interventions toindividuals as they go about their daily life. The involvement of care partnersand social support networks often proves crucial in helping individualsmanaging burdensome medical conditions. This presents opportunities in mobilehealth to design interventions that target the dyadic relationship -- therelationship between a target person and their care partner -- with the aim ofenhancing social support. In this paper, we develop dyadic RL, an onlinereinforcement learning algorithm designed to personalize intervention deliverybased on contextual factors and past responses of a target person and theircare partner. Here, multiple sets of interventions impact the dyad acrossmultiple time intervals. The developed dyadic RL is Bayesian and hierarchical.We formally introduce the problem setup, develop dyadic RL and establish aregret bound. We demonstrate dyadic RL's empirical performance throughsimulation studies on both toy scenarios and on a realistic test bedconstructed from data collected in a mobile health study.</description><author>Shuangning Li, Lluis Salvat Niell, Sung Won Choi, Inbal Nahum-Shani, Guy Shani, Susan Murphy</author><pubDate>Tue, 15 Aug 2023 16:43:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07843v1</guid></item><item><title>CCD-3DR: Consistent Conditioning in Diffusion for Single-Image 3D Reconstruction</title><link>http://arxiv.org/abs/2308.07837v1</link><description>In this paper, we present a novel shape reconstruction method leveragingdiffusion model to generate 3D sparse point cloud for the object captured in asingle RGB image. Recent methods typically leverage global embedding or localprojection-based features as the condition to guide the diffusion model.However, such strategies fail to consistently align the denoised point cloudwith the given image, leading to unstable conditioning and inferiorperformance. In this paper, we present CCD-3DR, which exploits a novel centereddiffusion probabilistic model for consistent local feature conditioning. Weconstrain the noise and sampled point cloud from the diffusion model into asubspace where the point cloud center remains unchanged during the forwarddiffusion process and reverse process. The stable point cloud center furtherserves as an anchor to align each point with its corresponding localprojection-based features. Extensive experiments on synthetic benchmarkShapeNet-R2N2 demonstrate that CCD-3DR outperforms all competitors by a largemargin, with over 40% improvement. We also provide results on real-worlddataset Pix3D to thoroughly demonstrate the potential of CCD-3DR in real-worldapplications. Codes will be released soon</description><author>Yan Di, Chenyangguang Zhang, Pengyuan Wang, Guangyao Zhai, Ruida Zhang, Fabian Manhardt, Benjamin Busam, Xiangyang Ji, Federico Tombari</author><pubDate>Tue, 15 Aug 2023 16:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07837v1</guid></item><item><title>Simple and Efficient Partial Graph Adversarial Attack: A New Perspective</title><link>http://arxiv.org/abs/2308.07834v1</link><description>As the study of graph neural networks becomes more intensive andcomprehensive, their robustness and security have received great researchinterest. The existing global attack methods treat all nodes in the graph astheir attack targets. Although existing methods have achieved excellentresults, there is still considerable space for improvement. The key problem isthat the current approaches rigidly follow the definition of global attacks.They ignore an important issue, i.e., different nodes have different robustnessand are not equally resilient to attacks. From a global attacker's view, weshould arrange the attack budget wisely, rather than wasting them on highlyrobust nodes. To this end, we propose a totally new method named partial graphattack (PGA), which selects the vulnerable nodes as attack targets. First, toselect the vulnerable items, we propose a hierarchical target selection policy,which allows attackers to only focus on easy-to-attack nodes. Then, we proposea cost-effective anchor-picking policy to pick the most promising anchors foradding or removing edges, and a more aggressive iterative greedy-based attackmethod to perform more efficient attacks. Extensive experimental resultsdemonstrate that PGA can achieve significant improvements in both attack effectand attack efficiency compared to other existing graph global attack methods.</description><author>Guanghui Zhu, Mengyu Chen, Chunfeng Yuan, Yihua Huang</author><pubDate>Tue, 15 Aug 2023 16:23:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07834v1</guid></item><item><title>REFORMS: Reporting Standards for Machine Learning Based Science</title><link>http://arxiv.org/abs/2308.07832v1</link><description>Machine learning (ML) methods are proliferating in scientific research.However, the adoption of these methods has been accompanied by failures ofvalidity, reproducibility, and generalizability. These failures can hinderscientific progress, lead to false consensus around invalid claims, andundermine the credibility of ML-based science. ML methods are often applied andfail in similar ways across disciplines. Motivated by this observation, ourgoal is to provide clear reporting standards for ML-based science. Drawing froman extensive review of past literature, we present the REFORMS checklist($\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine LearningBased $\textbf{S}$cience). It consists of 32 questions and a paired set ofguidelines. REFORMS was developed based on a consensus of 19 researchers acrosscomputer science, data science, mathematics, social sciences, and biomedicalsciences. REFORMS can serve as a resource for researchers when designing andimplementing a study, for referees when reviewing papers, and for journals whenenforcing standards for transparency and reproducibility.</description><author>Sayash Kapoor, Emily Cantrell, Kenny Peng, Thanh Hien Pham, Christopher A. Bail, Odd Erik Gundersen, Jake M. Hofman, Jessica Hullman, Michael A. Lones, Momin M. Malik, Priyanka Nanayakkara, Russell A. Poldrack, Inioluwa Deborah Raji, Michael Roberts, Matthew J. Salganik, Marta Serra-Garcia, Brandon M. Stewart, Gilles Vandewiele, Arvind Narayanan</author><pubDate>Tue, 15 Aug 2023 16:21:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07832v1</guid></item><item><title>A Genetic Algorithm Meta-Heuristic for a Generalized Quadratic Assignment Problem</title><link>http://arxiv.org/abs/2308.07828v1</link><description>The generalized quadratic assignment problem (GQAP) is one of the hardestproblems to solve in the operations research area. The GQAP addressed in thiswork is defined as the task of minimizing the assignment and transportationcosts of assigning a set of facilities to a set of locations. The facilitieshave different space requirements, and the locations have different spacecapacities. Multiple facilities can be assigned to each location if the spacecapacity is not violated. In this work, three instances of GQAP in differentsituations are presented. Then, a genetic algorithm is developed to solve theGQAP instances. Finally, the local neighborhood search with the steepestdescend strategy is constructed and applied to the final solution obtained bythe GA, and the final solution is compared with the best solution found byMPL/CPLEX software and reference papers. The results show that the developed GAheuristic is effective for solving the GQAP.</description><author>Mojtaba A. Farahani</author><pubDate>Tue, 15 Aug 2023 16:13:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07828v1</guid></item><item><title>Learning Better Keypoints for Multi-Object 6DoF Pose Estimation</title><link>http://arxiv.org/abs/2308.07827v1</link><description>We investigate the impact of pre-defined keypoints for pose estimation, andfound that accuracy and efficiency can be improved by training a graph networkto select a set of disperse keypoints with similarly distributed votes. Thesevotes, learned by a regression network to accumulate evidence for the keypointlocations, can be regressed more accurately compared to previous heuristickeypoint algorithms. The proposed KeyGNet, supervised by a combined lossmeasuring both Wassserstein distance and dispersion, learns the color andgeometry features of the target objects to estimate optimal keypoint locations.Experiments demonstrate the keypoints selected by KeyGNet improved the accuracyfor all evaluation metrics of all seven datasets tested, for three keypointvoting methods. The challenging Occlusion LINEMOD dataset notably improvedADD(S) by +16.4% on PVN3D, and all core BOP datasets showed an AR improvementfor all objects, of between +1% and +21.5%. There was also a notable increasein performance when transitioning from single object to multiple objecttraining using KeyGNet keypoints, essentially eliminating the SISO-MIMO gap forOcclusion LINEMOD.</description><author>Yangzheng Wu, Michael Greenspan</author><pubDate>Tue, 15 Aug 2023 16:11:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07827v1</guid></item><item><title>ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification</title><link>http://arxiv.org/abs/2305.04003v3</link><description>Verification of machine learning models used in Natural Language Processing(NLP) is known to be a hard problem. In particular, many known neural networkverification methods that work for computer vision and other numeric datasetsdo not work for NLP. Here, we study technical reasons that underlie thisproblem. Based on this analysis, we propose practical methods and heuristicsfor preparing NLP datasets and models in a way that renders them amenable toknown verification methods based on abstract interpretation. We implement thesemethods as a Python library called ANTONIO that links to the neural networkverifiers ERAN and Marabou. We perform evaluation of the tool using an NLPdataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLPapplications. We hope that, thanks to its general applicability, this work willopen novel possibilities for including NLP verification problems into neuralnetwork verification competitions, and will popularise NLP problems within thiscommunity.</description><author>Marco Casadio, Luca Arnaboldi, Matthew L. Daggitt, Omri Isac, Tanvi Dinkar, Daniel Kienitz, Verena Rieser, Ekaterina Komendantskaya</author><pubDate>Tue, 15 Aug 2023 16:09:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04003v3</guid></item><item><title>Cerberus: A Deep Learning Hybrid Model for Lithium-Ion Battery Aging Estimation and Prediction Based on Relaxation Voltage Curves</title><link>http://arxiv.org/abs/2308.07824v1</link><description>The degradation process of lithium-ion batteries is intricately linked totheir entire lifecycle as power sources and energy storage devices,encompassing aspects such as performance delivery and cycling utilization.Consequently, the accurate and expedient estimation or prediction of the agingstate of lithium-ion batteries has garnered extensive attention. Nonetheless,prevailing research predominantly concentrates on either aging estimation orprediction, neglecting the dynamic fusion of both facets. This paper proposes ahybrid model for capacity aging estimation and prediction based on deeplearning, wherein salient features highly pertinent to aging are extracted fromcharge and discharge relaxation processes. By amalgamating historical capacitydecay data, the model dynamically furnishes estimations of the present capacityand forecasts of future capacity for lithium-ion batteries. Our approach isvalidated against a novel dataset involving charge and discharge cycles atvarying rates. Specifically, under a charging condition of 0.25C, a meanabsolute percentage error (MAPE) of 0.29% is achieved. This outcome underscoresthe model's adeptness in harnessing relaxation processes commonly encounteredin the real world and synergizing with historical capacity records withinbattery management systems (BMS), thereby affording estimations andprognostications of capacity decline with heightened precision.</description><author>Yue Xiang, Bo Jiang, Haifeng Dai</author><pubDate>Tue, 15 Aug 2023 16:07:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07824v1</guid></item><item><title>Feature Embedding by Template Matching as a ResNet Block</title><link>http://arxiv.org/abs/2210.00992v2</link><description>Convolution blocks serve as local feature extractors and are the key tosuccess of the neural networks. To make local semantic feature embedding ratherexplicit, we reformulate convolution blocks as feature selection according tothe best matching kernel. In this manner, we show that typical ResNet blocksindeed perform local feature embedding via template matching once batchnormalization (BN) followed by a rectified linear unit (ReLU) is interpreted asarg-max optimizer. Following this perspective, we tailor a residual block thatexplicitly forces semantically meaningful local feature embedding through usinglabel information. Specifically, we assign a feature vector to each localregion according to the classes that the corresponding region matches. Weevaluate our method on three popular benchmark datasets with severalarchitectures for image classification and consistently show that our approachsubstantially improves the performance of the baseline architectures.</description><author>Ada Gorgun, Yeti Z. Gurbuz, A. Aydin Alatan</author><pubDate>Tue, 15 Aug 2023 16:06:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.00992v2</guid></item><item><title>Quantum Image Denoising: A Framework via Boltzmann Machines, QUBO, and Quantum Annealing</title><link>http://arxiv.org/abs/2307.06542v2</link><description>We investigate a framework for binary image denoising via restrictedBoltzmann machines (RBMs) that introduces a denoising objective in quadraticunconstrained binary optimization (QUBO) form and is well-suited for quantumannealing. The denoising objective is attained by balancing the distributionlearned by a trained RBM with a penalty term for derivations from the noisyimage. We derive the statistically optimal choice of the penalty parameterassuming the target distribution has been well-approximated, and furthersuggest an empirically supported modification to make the method robust to thatidealistic assumption. We also show under additional assumptions that thedenoised images attained by our method are, in expectation, strictly closer tothe noise-free images than the noisy images are. While we frame the model as animage denoising model, it can be applied to any binary data. As the QUBOformulation is well-suited for implementation on quantum annealers, we test themodel on a D-Wave Advantage machine, and also test on data too large forcurrent quantum annealers by approximating QUBO solutions through classicalheuristics.</description><author>Phillip Kerger, Ryoji Miyazaki</author><pubDate>Tue, 15 Aug 2023 16:06:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06542v2</guid></item><item><title>Towards Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction</title><link>http://arxiv.org/abs/2303.18125v3</link><description>This paper addresses the problem of rolling shutter correction in complexnonlinear and dynamic scenes with extreme occlusion. Existing methods sufferfrom two main drawbacks. Firstly, they face challenges in estimating theaccurate correction field due to the uniform velocity assumption, leading tosignificant image correction errors under complex motion. Secondly, the drasticocclusion in dynamic scenes prevents current solutions from achieving betterimage quality because of the inherent difficulties in aligning and aggregatingmultiple frames. To tackle these challenges, we model the curvilineartrajectory of pixels analytically and propose a geometry-based QuadraticRolling Shutter (QRS) motion solver, which precisely estimates the high-ordercorrection field of individual pixels. Besides, to reconstruct high-qualityocclusion frames in dynamic scenes, we present a 3D video architecture thateffectively Aligns and Aggregates multi-frame context, namely, RSA2-Net. Weevaluate our method across a broad range of cameras and video sequences,demonstrating its significant superiority. Specifically, our method surpassesthe state-of-the-art by +4.98, +0.77, and +4.33 of PSNR on Carla-RS, Fastec-RS,and BS-RSC datasets, respectively. Code is available athttps://github.com/DelinQu/qrsc.</description><author>Delin Qu, Yizhen Lao, Zhigang Wang, Dong Wang, Bin Zhao, Xuelong Li</author><pubDate>Tue, 15 Aug 2023 16:06:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.18125v3</guid></item><item><title>Deep reinforcement learning for process design: Review and perspective</title><link>http://arxiv.org/abs/2308.07822v1</link><description>The transformation towards renewable energy and feedstock supply in thechemical industry requires new conceptual process design approaches. Recently,breakthroughs in artificial intelligence offer opportunities to accelerate thistransition. Specifically, deep reinforcement learning, a subclass of machinelearning, has shown the potential to solve complex decision-making problems andaid sustainable process design. We survey state-of-the-art research inreinforcement learning for process design through three major elements: (i)information representation, (ii) agent architecture, and (iii) environment andreward. Moreover, we discuss perspectives on underlying challenges andpromising future works to unfold the full potential of reinforcement learningfor process design in chemical engineering.</description><author>Qinghe Gao, Artur M. Schweidtmann</author><pubDate>Tue, 15 Aug 2023 15:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07822v1</guid></item><item><title>Quantifying the Cost of Learning in Queueing Systems</title><link>http://arxiv.org/abs/2308.07817v1</link><description>Queueing systems are widely applicable stochastic models with use cases incommunication networks, healthcare, service systems, etc. Although theiroptimal control has been extensively studied, most existing approaches assumeperfect knowledge of system parameters. Of course, this assumption rarely holdsin practice where there is parameter uncertainty, thus motivating a recent lineof work on bandit learning for queueing systems. This nascent stream ofresearch focuses on the asymptotic performance of the proposed algorithms. In this paper, we argue that an asymptotic metric, which focuses onlate-stage performance, is insufficient to capture the intrinsic statisticalcomplexity of learning in queueing systems which typically occurs in the earlystage. Instead, we propose the Cost of Learning in Queueing (CLQ), a new metricthat quantifies the maximum increase in time-averaged queue length caused byparameter uncertainty. We characterize the CLQ of a single-queue multi-serversystem, and then extend these results to multi-queue multi-server systems andnetworks of queues. In establishing our results, we propose a unified analysisframework for CLQ that bridges Lyapunov and bandit analysis, which could be ofindependent interest.</description><author>Daniel Freund, Thodoris Lykouris, Wentao Weng</author><pubDate>Tue, 15 Aug 2023 15:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07817v1</guid></item><item><title>Mixed Regression via Approximate Message Passing</title><link>http://arxiv.org/abs/2304.02229v2</link><description>We study the problem of regression in a generalized linear model (GLM) withmultiple signals and latent variables. This model, which we call a matrix GLM,covers many widely studied problems in statistical learning, including mixedlinear regression, max-affine regression, and mixture-of-experts. In mixedlinear regression, each observation comes from one of $L$ signal vectors(regressors), but we do not know which one; in max-affine regression, eachobservation comes from the maximum of $L$ affine functions, each defined via adifferent signal vector. The goal in all these problems is to estimate thesignals, and possibly some of the latent variables, from the observations. Wepropose a novel approximate message passing (AMP) algorithm for estimation in amatrix GLM and rigorously characterize its performance in the high-dimensionallimit. This characterization is in terms of a state evolution recursion, whichallows us to precisely compute performance measures such as the asymptoticmean-squared error. The state evolution characterization can be used to tailorthe AMP algorithm to take advantage of any structural information known aboutthe signals. Using state evolution, we derive an optimal choice of AMP`denoising' functions that minimizes the estimation error in each iteration. The theoretical results are validated by numerical simulations for mixedlinear regression, max-affine regression, and mixture-of-experts. Formax-affine regression, we propose an algorithm that combines AMP withexpectation-maximization to estimate intercepts of the model along with thesignals. The numerical results show that AMP significantly outperforms otherestimators for mixed linear regression and max-affine regression in mostparameter regimes.</description><author>Nelvin Tan, Ramji Venkataramanan</author><pubDate>Tue, 15 Aug 2023 15:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02229v2</guid></item><item><title>ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition</title><link>http://arxiv.org/abs/2308.07815v1</link><description>Class imbalance is a common challenge in real-world recognition tasks, wherethe majority of classes have few samples, also known as tail classes. Weaddress this challenge with the perspective of generalization and empiricallyfind that the promising Sharpness-Aware Minimization (SAM) fails to addressgeneralization issues under the class-imbalanced setting. Through investigatingthis specific type of task, we identify that its generalization bottleneckprimarily lies in the severe overfitting for tail classes with limited trainingdata. To overcome this bottleneck, we leverage class priors to restrict thegeneralization scope of the class-agnostic SAM and propose a class-awaresmoothness optimization algorithm named Imbalanced-SAM (ImbSAM). With theguidance of class priors, our ImbSAM specifically improves generalizationtargeting tail classes. We also verify the efficacy of ImbSAM on twoprototypical applications of class-imbalanced recognition: long-tailedclassification and semi-supervised anomaly detection, where our ImbSAMdemonstrates remarkable performance improvements for tail classes and anomaly.Our code implementation is available athttps://github.com/cool-xuan/Imbalanced_SAM.</description><author>Yixuan Zhou, Yi Qu, Xing Xu, Hengtao Shen</author><pubDate>Tue, 15 Aug 2023 15:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07815v1</guid></item><item><title>FedICT: Federated Multi-task Distillation for Multi-access Edge Computing</title><link>http://arxiv.org/abs/2301.00389v2</link><description>The growing interest in intelligent services and privacy protection formobile devices has given rise to the widespread application of federatedlearning in Multi-access Edge Computing (MEC). Diverse user behaviors call forpersonalized services with heterogeneous Machine Learning (ML) models ondifferent devices. Federated Multi-task Learning (FMTL) is proposed to trainrelated but personalized ML models for different devices, whereas previousworks suffer from excessive communication overhead during training and neglectthe model heterogeneity among devices in MEC. Introducing knowledgedistillation into FMTL can simultaneously enable efficient communication andmodel heterogeneity among clients, whereas existing methods rely on a publicdataset, which is impractical in reality. To tackle this dilemma, FederatedMultI-task Distillation for Multi-access Edge CompuTing (FedICT) is proposed.FedICT direct local-global knowledge aloof during bi-directional distillationprocesses between clients and the server, aiming to enable multi-task clientswhile alleviating client drift derived from divergent optimization directionsof client-side local models. Specifically, FedICT includes Federated PriorKnowledge Distillation (FPKD) and Local Knowledge Adjustment (LKA). FPKD isproposed to reinforce the clients' fitting of local data by introducing priorknowledge of local data distributions. Moreover, LKA is proposed to correct thedistillation loss of the server, making the transferred local knowledge bettermatch the generalized representation. Experiments on three datasets show thatFedICT significantly outperforms all compared benchmarks in various dataheterogeneous and model architecture settings, achieving improved accuracy withless than 1.2% training communication overhead compared with FedAvg and no morethan 75% training communication round compared with FedGKT.</description><author>Zhiyuan Wu, Sheng Sun, Yuwei Wang, Min Liu, Quyang Pan, Xuefeng Jiang, Bo Gao</author><pubDate>Tue, 15 Aug 2023 15:33:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00389v2</guid></item><item><title>Grasp Transfer based on Self-Aligning Implicit Representations of Local Surfaces</title><link>http://arxiv.org/abs/2308.07807v1</link><description>Objects we interact with and manipulate often share similar parts, such ashandles, that allow us to transfer our actions flexibly due to their sharedfunctionality. This work addresses the problem of transferring a graspexperience or a demonstration to a novel object that shares shape similaritieswith objects the robot has previously encountered. Existing approaches forsolving this problem are typically restricted to a specific object category ora parametric shape. Our approach, however, can transfer grasps associated withimplicit models of local surfaces shared across object categories.Specifically, we employ a single expert grasp demonstration to learn animplicit local surface representation model from a small dataset of objectmeshes. At inference time, this model is used to transfer grasps to novelobjects by identifying the most geometrically similar surfaces to the one onwhich the expert grasp is demonstrated. Our model is trained entirely insimulation and is evaluated on simulated and real-world objects that are notseen during training. Evaluations indicate that grasp transfer to unseen objectcategories using this approach can be successfully performed both in simulationand real-world experiments. The simulation results also show that the proposedapproach leads to better spatial precision and grasp accuracy compared to abaseline approach.</description><author>Ahmet Tekden, Marc Peter Deisenroth, Yasemin Bekiroglu</author><pubDate>Tue, 15 Aug 2023 15:33:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07807v1</guid></item><item><title>Fairness and Privacy in Federated Learning and Their Implications in Healthcare</title><link>http://arxiv.org/abs/2308.07805v1</link><description>Currently, many contexts exist where distributed learning is difficult orotherwise constrained by security and communication limitations. One commondomain where this is a consideration is in Healthcare where data is oftengoverned by data-use-ordinances like HIPAA. On the other hand, larger samplesizes and shared data models are necessary to allow models to better generalizeon account of the potential for more variability and balancing underrepresentedclasses. Federated learning is a type of distributed learning model that allowsdata to be trained in a decentralized manner. This, in turn, addresses datasecurity, privacy, and vulnerability considerations as data itself is notshared across a given learning network nodes. Three main challenges tofederated learning include node data is not independent and identicallydistributed (iid), clients requiring high levels of communication overheadbetween peers, and there is the heterogeneity of different clients within anetwork with respect to dataset bias and size. As the field has grown, thenotion of fairness in federated learning has also been introduced through novelimplementations. Fairness approaches differ from the standard form of federatedlearning and also have distinct challenges and considerations for thehealthcare domain. This paper endeavors to outline the typical lifecycle offair federated learning in research as well as provide an updated taxonomy toaccount for the current state of fairness in implementations. Lastly, thispaper provides added insight into the implications and challenges ofimplementing and supporting fairness in federated learning in the healthcaredomain.</description><author>Navya Annapareddy, Jade Preston, Judy Fox</author><pubDate>Tue, 15 Aug 2023 15:32:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07805v1</guid></item><item><title>Investigating and Improving Latent Density Segmentation Models for Aleatoric Uncertainty Quantification in Medical Imaging</title><link>http://arxiv.org/abs/2307.16694v2</link><description>Data uncertainties, such as sensor noise or occlusions, can introduceirreducible ambiguities in images, which result in varying, yet plausible,semantic hypotheses. In Machine Learning, this ambiguity is commonly referredto as aleatoric uncertainty. Latent density models can be utilized to addressthis problem in image segmentation. The most popular approach is theProbabilistic U-Net (PU-Net), which uses latent Normal densities to optimizethe conditional data log-likelihood Evidence Lower Bound. In this work, wedemonstrate that the PU- Net latent space is severely inhomogenous. As aresult, the effectiveness of gradient descent is inhibited and the modelbecomes extremely sensitive to the localization of the latent space samples,resulting in defective predictions. To address this, we present the SinkhornPU-Net (SPU-Net), which uses the Sinkhorn Divergence to promote homogeneityacross all latent dimensions, effectively improving gradient-descent updatesand model robustness. Our results show that by applying this on public datasetsof various clinical segmentation problems, the SPU-Net receives up to 11%performance gains compared against preceding latent variable models forprobabilistic segmentation on the Hungarian-Matched metric. The resultsindicate that by encouraging a homogeneous latent space, one can significantlyimprove latent density modeling for medical image segmentation.</description><author>M. M. Amaan Valiuddin, Christiaan G. A. Viviers, Ruud J. G. van Sloun, Peter H. N. de With, Fons van der Sommen</author><pubDate>Tue, 15 Aug 2023 15:28:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16694v2</guid></item><item><title>Neuromorphic Seatbelt State Detection for In-Cabin Monitoring with Event Cameras</title><link>http://arxiv.org/abs/2308.07802v1</link><description>Neuromorphic vision sensors, or event cameras, differ from conventionalcameras in that they do not capture images at a specified rate. Instead, theyasynchronously log local brightness changes at each pixel. As a result, eventcameras only record changes in a given scene, and do so with very high temporalresolution, high dynamic range, and low power requirements. Recent research hasdemonstrated how these characteristics make event cameras extremely practicalsensors in driver monitoring systems (DMS), enabling the tracking of high-speedeye motion and blinks. This research provides a proof of concept to expandevent-based DMS techniques to include seatbelt state detection. Using an eventsimulator, a dataset of 108,691 synthetic neuromorphic frames of car occupantswas generated from a near-infrared (NIR) dataset, and split into training,validation, and test sets for a seatbelt state detection algorithm based on arecurrent convolutional neural network (CNN). In addition, a smaller set ofreal event data was collected and reserved for testing. In a binaryclassification task, the fastened/unfastened frames were identified with an F1score of 0.989 and 0.944 on the simulated and real test sets respectively. Whenthe problem extended to also classify the action of fastening/unfastening theseatbelt, respective F1 scores of 0.964 and 0.846 were achieved.</description><author>Paul Kielty, Cian Ryan, Mehdi Sefidgar Dilmaghani, Waseem Shariff, Joe Lemley, Peter Corcoran</author><pubDate>Tue, 15 Aug 2023 15:27:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07802v1</guid></item><item><title>Handwritten Stenography Recognition and the LION Dataset</title><link>http://arxiv.org/abs/2308.07799v1</link><description>Purpose: In this paper, we establish a baseline for handwritten stenographyrecognition, using the novel LION dataset, and investigate the impact ofincluding selected aspects of stenographic theory into the recognition process.We make the LION dataset publicly available with the aim of encouraging futureresearch in handwritten stenography recognition. Methods: A state-of-the-art text recognition model is trained to establish abaseline. Stenographic domain knowledge is integrated by applying fourdifferent encoding methods that transform the target sequence intorepresentations, which approximate selected aspects of the writing system.Results are further improved by integrating a pre-training scheme, based onsynthetic data. Results: The baseline model achieves an average test character error rate(CER) of 29.81% and a word error rate (WER) of 55.14%. Test error rates arereduced significantly by combining stenography-specific target sequenceencodings with pre-training and fine-tuning, yielding CERs in the range of24.5% - 26% and WERs of 44.8% - 48.2%. Conclusion: The obtained results demonstrate the challenging nature ofstenography recognition. Integrating stenography-specific knowledge, inconjunction with pre-training and fine-tuning on synthetic data, yieldsconsiderable improvements. Together with our precursor study on the subject,this is the first work to apply modern handwritten text recognition tostenography. The dataset and our code are publicly available via Zenodo.</description><author>Raphaela Heil, Malin Nauwerck</author><pubDate>Tue, 15 Aug 2023 15:25:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07799v1</guid></item><item><title>Adaptive Noise Covariance Estimation under Colored Noise using Dynamic Expectation Maximization</title><link>http://arxiv.org/abs/2308.07797v1</link><description>The accurate estimation of the noise covariance matrix (NCM) in a dynamicsystem is critical for state estimation and control, as it has a majorinfluence in their optimality. Although a large number of NCM estimationmethods have been developed, most of them assume the noises to be white.However, in many real-world applications, the noises are colored (e.g., theyexhibit temporal autocorrelations), resulting in suboptimal solutions. Here, weintroduce a novel brain-inspired algorithm that accurately and adaptivelyestimates the NCM for dynamic systems subjected to colored noise. Particularly,we extend the Dynamic Expectation Maximization algorithm to perform both onlinenoise covariance and state estimation by optimizing the free energy objective.We mathematically prove that our NCM estimator converges to the global optimumof this free energy objective. Using randomized numerical simulations, we showthat our estimator outperforms nine baseline methods with minimal noisecovariance estimation error under colored noise conditions. Notably, we showthat our method outperforms the best baseline (Variational Bayes) in jointnoise and state estimation for high colored noise. We foresee that the accuracyand the adaptive nature of our estimator make it suitable for online estimationin real-world applications.</description><author>Ajith Anil Meera, Pablo Lanillos</author><pubDate>Tue, 15 Aug 2023 15:21:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07797v1</guid></item><item><title>Learning to Identify Critical States for Reinforcement Learning from Videos</title><link>http://arxiv.org/abs/2308.07795v1</link><description>Recent work on deep reinforcement learning (DRL) has pointed out thatalgorithmic information about good policies can be extracted from offline datawhich lack explicit information about executed actions. For example, videos ofhumans or robots may convey a lot of implicit information about rewardingaction sequences, but a DRL machine that wants to profit from watching suchvideos must first learn by itself to identify and recognize relevantstates/actions/rewards. Without relying on ground-truth annotations, our newmethod called Deep State Identifier learns to predict returns from episodesencoded as videos. Then it uses a kind of mask-based sensitivity analysis toextract/identify important critical states. Extensive experiments showcase ourmethod's potential for understanding and improving agent behavior. The sourcecode and the generated datasets are available athttps://github.com/AI-Initiative-KAUST/VideoRLCS.</description><author>Haozhe Liu, Mingchen Zhuge, Bing Li, Yuhui Wang, Francesco Faccio, Bernard Ghanem, Jürgen Schmidhuber</author><pubDate>Tue, 15 Aug 2023 15:21:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07795v1</guid></item><item><title>Informed Named Entity Recognition Decoding for Generative Language Models</title><link>http://arxiv.org/abs/2308.07791v1</link><description>Ever-larger language models with ever-increasing capabilities are by nowwell-established text processing tools. Alas, information extraction tasks suchas named entity recognition are still largely unaffected by this progress asthey are primarily based on the previous generation of encoder-only transformermodels. Here, we propose a simple yet effective approach, Informed Named EntityRecognition Decoding (iNERD), which treats named entity recognition as agenerative process. It leverages the language understanding capabilities ofrecent generative models in a future-proof manner and employs an informeddecoding scheme incorporating the restricted nature of information extractioninto open-ended text generation, improving performance and eliminating any riskof hallucinations. We coarse-tune our model on a merged named entity corpus tostrengthen its performance, evaluate five generative language models on eightnamed entity recognition datasets, and achieve remarkable results, especiallyin an environment with an unknown entity class set, demonstrating theadaptability of the approach.</description><author>Tobias Deußer, Lars Hillebrand, Christian Bauckhage, Rafet Sifa</author><pubDate>Tue, 15 Aug 2023 15:16:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07791v1</guid></item><item><title>DAC: Detector-Agnostic Spatial Covariances for Deep Local Features</title><link>http://arxiv.org/abs/2305.12250v2</link><description>Current deep visual local feature detectors do not model the spatialuncertainty of detected features, producing suboptimal results in downstreamapplications. In this work, we propose two post-hoc covariance estimates thatcan be plugged into any pretrained deep feature detector: a simple, isotropiccovariance estimate that uses the predicted score at a given pixel location,and a full covariance estimate via the local structure tensor of the learnedscore maps. Both methods are easy to implement and can be applied to any deepfeature detector. We show that these covariances are directly related to errorsin feature matching, leading to improvements in downstream tasks, includingsolving the perspective-n-point problem and motion-only bundle adjustment. Codeis available at https://github.com/javrtg/DAC</description><author>Javier Tirado-Garín, Frederik Warburg, Javier Civera</author><pubDate>Tue, 15 Aug 2023 15:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12250v2</guid></item><item><title>Fréchet Statistics Based Change Point Detection in Multivariate Hawkes Process</title><link>http://arxiv.org/abs/2308.06769v2</link><description>This paper proposes a new approach for change point detection in causalnetworks of multivariate Hawkes processes using Frechet statistics. Our methodsplits the point process into overlapping windows, estimates kernel matrices ineach window, and reconstructs the signed Laplacians by treating the kernelmatrices as the adjacency matrices of the causal network. We demonstrate theeffectiveness of our method through experiments on both simulated andreal-world cryptocurrency datasets. Our results show that our method is capableof accurately detecting and characterizing changes in the causal structure ofmultivariate Hawkes processes, and may have potential applications in fieldssuch as finance and neuroscience. The proposed method is an extension ofprevious work on Frechet statistics in point process settings and represents animportant contribution to the field of change point detection in multivariatepoint processes.</description><author>Rui Luo, Vikram Krishnamurthy</author><pubDate>Tue, 15 Aug 2023 15:08:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06769v2</guid></item><item><title>DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding</title><link>http://arxiv.org/abs/2308.07787v1</link><description>Recent research has demonstrated impressive results in video-to-speechsynthesis which involves reconstructing speech solely from visual input.However, previous works have struggled to accurately synthesize speech due to alack of sufficient guidance for the model to infer the correct content with theappropriate sound. To resolve the issue, they have adopted an extra speakerembedding as a speaking style guidance from a reference auditory information.Nevertheless, it is not always possible to obtain the audio information fromthe corresponding video input, especially during the inference time. In thispaper, we present a novel vision-guided speaker embedding extractor using aself-supervised pre-trained model and prompt tuning technique. In doing so, therich speaker embedding information can be produced solely from input visualinformation, and the extra audio information is not necessary during theinference time. Using the extracted vision-guided speaker embeddingrepresentations, we further develop a diffusion-based video-to-speech synthesismodel, so called DiffV2S, conditioned on those speaker embeddings and thevisual representation extracted from the input video. The proposed DiffV2S notonly maintains phoneme details contained in the input video frames, but alsocreates a highly intelligible mel-spectrogram in which the speaker identitiesof the multiple speakers are all preserved. Our experimental results show thatDiffV2S achieves the state-of-the-art performance compared to the previousvideo-to-speech synthesis technique.</description><author>Jeongsoo Choi, Joanna Hong, Yong Man Ro</author><pubDate>Tue, 15 Aug 2023 15:07:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07787v1</guid></item><item><title>Future Video Prediction from a Single Frame for Video Anomaly Detection</title><link>http://arxiv.org/abs/2308.07783v1</link><description>Video anomaly detection (VAD) is an important but challenging task incomputer vision. The main challenge rises due to the rarity of training samplesto model all anomaly cases. Hence, semi-supervised anomaly detection methodshave gotten more attention, since they focus on modeling normals and theydetect anomalies by measuring the deviations from normal patterns. Despiteimpressive advances of these methods in modeling normal motion and appearance,long-term motion modeling has not been effectively explored so far. Inspired bythe abilities of the future frame prediction proxy-task, we introduce the taskof future video prediction from a single frame, as a novel proxy-task for videoanomaly detection. This proxy-task alleviates the challenges of previousmethods in learning longer motion patterns. Moreover, we replace the initialand future raw frames with their corresponding semantic segmentation map, whichnot only makes the method aware of object class but also makes the predictiontask less complex for the model. Extensive experiments on the benchmarkdatasets (ShanghaiTech, UCSD-Ped1, and UCSD-Ped2) show the effectiveness of themethod and the superiority of its performance compared to SOTA prediction-basedVAD methods.</description><author>Mohammad Baradaran, Robert Bergevin</author><pubDate>Tue, 15 Aug 2023 15:04:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07783v1</guid></item><item><title>Formal Modelling for Multi-Robot Systems Under Uncertainty</title><link>http://arxiv.org/abs/2305.17018v2</link><description>Purpose of Review: To effectively synthesise and analyse multi-robotbehaviour, we require formal task-level models which accurately capturemulti-robot execution. In this paper, we review modelling formalisms formulti-robot systems under uncertainty, and discuss how they can be used forplanning, reinforcement learning, model checking, and simulation. Recent Findings: Recent work has investigated models which more accuratelycapture multi-robot execution by considering different forms of uncertainty,such as temporal uncertainty and partial observability, and modelling theeffects of robot interactions on action execution. Other strands of work havepresented approaches for reducing the size of multi-robot models to admit moreefficient solution methods. This can be achieved by decoupling the robots underindependence assumptions, or reasoning over higher level macro actions. Summary: Existing multi-robot models demonstrate a trade off betweenaccurately capturing robot dependencies and uncertainty, and being small enoughto tractably solve real world problems. Therefore, future research shouldexploit realistic assumptions over multi-robot behaviour to develop smallermodels which retain accurate representations of uncertainty and robotinteractions; and exploit the structure of multi-robot problems, such asfactored state spaces, to develop scalable solution methods.</description><author>Charlie Street, Masoumeh Mansouri, Bruno Lacerda</author><pubDate>Tue, 15 Aug 2023 15:01:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17018v2</guid></item><item><title>Learning Image Deraining Transformer Network with Dynamic Dual Self-Attention</title><link>http://arxiv.org/abs/2308.07781v1</link><description>Recently, Transformer-based architecture has been introduced into singleimage deraining task due to its advantage in modeling non-local information.However, existing approaches tend to integrate global features based on a denseself-attention strategy since it tend to uses all similarities of the tokensbetween the queries and keys. In fact, this strategy leads to ignoring the mostrelevant information and inducing blurry effect by the irrelevantrepresentations during the feature aggregation. To this end, this paperproposes an effective image deraining Transformer with dynamic dualself-attention (DDSA), which combines both dense and sparse attentionstrategies to better facilitate clear image reconstruction. Specifically, weonly select the most useful similarity values based on top-k approximatecalculation to achieve sparse attention. In addition, we also develop a novelspatial-enhanced feed-forward network (SEFN) to further obtain a more accuraterepresentation for achieving high-quality derained results. Extensiveexperiments on benchmark datasets demonstrate the effectiveness of our proposedmethod.</description><author>Zhentao Fan, Hongming Chen, Yufeng Li</author><pubDate>Tue, 15 Aug 2023 14:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07781v1</guid></item><item><title>BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer</title><link>http://arxiv.org/abs/2307.00360v2</link><description>BatGPT is a large-scale language model designed and trained jointly by WuhanUniversity and Shanghai Jiao Tong University. It is capable of generatinghighly natural and fluent text in response to various types of input, includingtext prompts, images, and audio. In the modeling level, we employ abidirectional autoregressive architecture that allows the model to efficientlycapture the complex dependencies of natural language, making it highlyeffective in tasks such as language generation, dialog systems, and questionanswering. Moreover, the bidirectional autoregressive modeling not onlyoperates from left to right but also from right to left, effectively reducingfixed memory effects and alleviating model hallucinations. In the training aspect, we propose a novel parameter expansion method forleveraging the pre-training of smaller models and employ reinforcement learningfrom both AI and human feedback, aimed at improving the model's alignmentperformance. Overall, these approaches significantly improve the effectivenessof BatGPT, and the model can be utilized for a wide range of natural languageapplications.</description><author>Zuchao Li, Shitou Zhang, Hai Zhao, Yifei Yang, Dongjie Yang</author><pubDate>Tue, 15 Aug 2023 14:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00360v2</guid></item><item><title>Do We Fully Understand Students' Knowledge States? Identifying and Mitigating Answer Bias in Knowledge Tracing</title><link>http://arxiv.org/abs/2308.07779v1</link><description>Knowledge tracing (KT) aims to monitor students' evolving knowledge statesthrough their learning interactions with concept-related questions, and can beindirectly evaluated by predicting how students will perform on futurequestions. In this paper, we observe that there is a common phenomenon ofanswer bias, i.e., a highly unbalanced distribution of correct and incorrectanswers for each question. Existing models tend to memorize the answer bias asa shortcut for achieving high prediction performance in KT, thereby failing tofully understand students' knowledge states. To address this issue, we approachthe KT task from a causality perspective. A causal graph of KT is firstestablished, from which we identify that the impact of answer bias lies in thedirect causal effect of questions on students' responses. A novelCOunterfactual REasoning (CORE) framework for KT is further proposed, whichseparately captures the total causal effect and direct causal effect duringtraining, and mitigates answer bias by subtracting the latter from the formerin testing. The CORE framework is applicable to various existing KT models, andwe implement it based on the prevailing DKT, DKVMN, and AKT models,respectively. Extensive experiments on three benchmark datasets demonstrate theeffectiveness of CORE in making the debiased inference for KT.</description><author>Chaoran Cui, Hebo Ma, Chen Zhang, Chunyun Zhang, Yumo Yao, Meng Chen, Yuling Ma</author><pubDate>Tue, 15 Aug 2023 14:56:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07779v1</guid></item><item><title>An Interpretable Machine Learning Model with Deep Learning-based Imaging Biomarkers for Diagnosis of Alzheimer's Disease</title><link>http://arxiv.org/abs/2308.07778v1</link><description>Machine learning methods have shown large potential for the automatic earlydiagnosis of Alzheimer's Disease (AD). However, some machine learning methodsbased on imaging data have poor interpretability because it is usually unclearhow they make their decisions. Explainable Boosting Machines (EBMs) areinterpretable machine learning models based on the statistical framework ofgeneralized additive modeling, but have so far only been used for tabular data.Therefore, we propose a framework that combines the strength of EBM withhigh-dimensional imaging data using deep learning-based feature extraction. Theproposed framework is interpretable because it provides the importance of eachfeature. We validated the proposed framework on the Alzheimer's DiseaseNeuroimaging Initiative (ADNI) dataset, achieving accuracy of 0.883 andarea-under-the-curve (AUC) of 0.970 on AD and control classification.Furthermore, we validated the proposed framework on an external testing set,achieving accuracy of 0.778 and AUC of 0.887 on AD and subjective cognitivedecline (SCD) classification. The proposed framework significantly outperformedan EBM model using volume biomarkers instead of deep learning-based features,as well as an end-to-end convolutional neural network (CNN) with optimizedarchitecture.</description><author>Wenjie Kang, Bo Li, Janne M. Papma, Lize C. Jiskoot, Peter Paul De Deyn, Geert Jan Biessels, Jurgen A. H. R. Claassen, Huub A. M. Middelkoop, Wiesje M. van der Flier, Inez H. G. B. Ramakers, Stefan Klein, Esther E. Bron</author><pubDate>Tue, 15 Aug 2023 14:54:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07778v1</guid></item><item><title>Enhancing Visually-Rich Document Understanding via Layout Structure Modeling</title><link>http://arxiv.org/abs/2308.07777v1</link><description>In recent years, the use of multi-modal pre-trained Transformers has led tosignificant advancements in visually-rich document understanding. However,existing models have mainly focused on features such as text and vision whileneglecting the importance of layout relationship between text nodes. In thispaper, we propose GraphLayoutLM, a novel document understanding model thatleverages the modeling of layout structure graph to inject document layoutknowledge into the model. GraphLayoutLM utilizes a graph reordering algorithmto adjust the text sequence based on the graph structure. Additionally, ourmodel uses a layout-aware multi-head self-attention layer to learn documentlayout knowledge. The proposed model enables the understanding of the spatialarrangement of text elements, improving document comprehension. We evaluate ourmodel on various benchmarks, including FUNSD, XFUND and CORD, and achievestate-of-the-art results among these datasets. Our experimental resultsdemonstrate that our proposed method provides a significant improvement overexisting approaches and showcases the importance of incorporating layoutinformation into document understanding models. We also conduct an ablationstudy to investigate the contribution of each component of our model. Theresults show that both the graph reordering algorithm and the layout-awaremulti-head self-attention layer play a crucial role in achieving the bestperformance.</description><author>Qiwei Li, Zuchao Li, Xiantao Cai, Bo Du, Hai Zhao</author><pubDate>Tue, 15 Aug 2023 14:53:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07777v1</guid></item><item><title>Hierarchical generative modelling for autonomous robots</title><link>http://arxiv.org/abs/2308.07775v1</link><description>Humans can produce complex whole-body motions when interacting with theirsurroundings, by planning, executing and combining individual limb movements.We investigated this fundamental aspect of motor control in the setting ofautonomous robotic operations. We approach this problem by hierarchicalgenerative modelling equipped with multi-level planning-for autonomous taskcompletion-that mimics the deep temporal architecture of human motor control.Here, temporal depth refers to the nested time scales at which successivelevels of a forward or generative model unfold, for example, delivering anobject requires a global plan to contextualise the fast coordination ofmultiple local movements of limbs. This separation of temporal scales alsomotivates robotics and control. Specifically, to achieve versatile sensorimotorcontrol, it is advantageous to hierarchically structure the planning andlow-level motor control of individual limbs. We use numerical and physicalsimulation to conduct experiments and to establish the efficacy of thisformulation. Using a hierarchical generative model, we show how a humanoidrobot can autonomously complete a complex task that necessitates a holistic useof locomotion, manipulation, and grasping. Specifically, we demonstrate theability of a humanoid robot that can retrieve and transport a box, open andwalk through a door to reach the destination, approach and kick a football,while showing robust performance in presence of body damage and groundirregularities. Our findings demonstrated the effectiveness of usinghuman-inspired motor control algorithms, and our method provides a viablehierarchical architecture for the autonomous completion of challenginggoal-directed tasks.</description><author>Kai Yuan, Noor Sajid, Karl Friston, Zhibin Li</author><pubDate>Tue, 15 Aug 2023 14:51:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07775v1</guid></item><item><title>A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2308.07774v1</link><description>A key component of many graph neural networks (GNNs) is the poolingoperation, which seeks to reduce the size of a graph while preserving importantstructural information. However, most existing graph pooling strategies rely onan assignment matrix obtained by employing a GNN layer, which is characterizedby trainable parameters, often leading to significant computational complexityand a lack of interpretability in the pooling process. In this paper, wepropose an unsupervised graph encoder-decoder model to detect abnormal nodesfrom graphs by learning an anomaly scoring function to rank nodes based ontheir degree of abnormality. In the encoding stage, we design a novel poolingmechanism, named LCPool, which leverages locality-constrained linear coding forfeature encoding to find a cluster assignment matrix by solving a least-squaresoptimization problem with a locality regularization term. By enforcing localityconstraints during the coding process, LCPool is designed to be free fromlearnable parameters, capable of efficiently handling large graphs, and caneffectively generate a coarser graph representation while retaining the mostsignificant structural characteristics of the graph. In the decoding stage, wepropose an unpooling operation, called LCUnpool, to reconstruct both thestructure and nodal features of the original graph. We conduct empiricalevaluations of our method on six benchmark datasets using several evaluationmetrics, and the results demonstrate its superiority over state-of-the-artanomaly detection approaches.</description><author>Mahsa Mesgaran, A. Ben Hamza</author><pubDate>Tue, 15 Aug 2023 14:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07774v1</guid></item><item><title>MOLE: MOdular Learning FramEwork via Mutual Information Maximization</title><link>http://arxiv.org/abs/2308.07772v1</link><description>This paper is to introduce an asynchronous and local learning framework forneural networks, named Modular Learning Framework (MOLE). This frameworkmodularizes neural networks by layers, defines the training objective viamutual information for each module, and sequentially trains each module bymutual information maximization. MOLE makes the training become localoptimization with gradient-isolated across modules, and this scheme is morebiologically plausible than BP. We run experiments on vector-, grid- andgraph-type data. In particular, this framework is capable of solving bothgraph- and node-level tasks for graph-type data. Therefore, MOLE has beenexperimentally proven to be universally applicable to different types of data.</description><author>Tianchao Li, Yulong Pei</author><pubDate>Tue, 15 Aug 2023 14:48:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07772v1</guid></item><item><title>Dual-path TokenLearner for Remote Photoplethysmography-based Physiological Measurement with Facial Videos</title><link>http://arxiv.org/abs/2308.07771v1</link><description>Remote photoplethysmography (rPPG) based physiological measurement is anemerging yet crucial vision task, whose challenge lies in exploring accuraterPPG prediction from facial videos accompanied by noises of illuminationvariations, facial occlusions, head movements, \etc, in a non-contact manner.Existing mainstream CNN-based models make efforts to detect physiologicalsignals by capturing subtle color changes in facial regions of interest (ROI)caused by heartbeats. However, such models are constrained by the limited localspatial or temporal receptive fields in the neural units. Unlike them, a nativeTransformer-based framework called Dual-path TokenLearner (Dual-TL) is proposedin this paper, which utilizes the concept of learnable tokens to integrate bothspatial and temporal informative contexts from the global perspective of thevideo. Specifically, the proposed Dual-TL uses a Spatial TokenLearner (S-TL) toexplore associations in different facial ROIs, which promises the rPPGprediction far away from noisy ROI disturbances. Complementarily, a TemporalTokenLearner (T-TL) is designed to infer the quasi-periodic pattern ofheartbeats, which eliminates temporal disturbances such as head movements. Thetwo TokenLearners, S-TL and T-TL, are executed in a dual-path mode. Thisenables the model to reduce noise disturbances for final rPPG signalprediction. Extensive experiments on four physiological measurement benchmarkdatasets are conducted. The Dual-TL achieves state-of-the-art performances inboth intra- and cross-dataset testings, demonstrating its immense potential asa basic backbone for rPPG measurement. The source code is available at\href{https://github.com/VUT-HFUT/Dual-TL}{https://github.com/VUT-HFUT/Dual-TL}</description><author>Wei Qian, Dan Guo, Kun Li, Xilan Tian, Meng Wang</author><pubDate>Tue, 15 Aug 2023 14:45:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07771v1</guid></item><item><title>Multi-scale Promoted Self-adjusting Correlation Learning for Facial Action Unit Detection</title><link>http://arxiv.org/abs/2308.07770v1</link><description>Facial Action Unit (AU) detection is a crucial task in affective computingand social robotics as it helps to identify emotions expressed through facialexpressions. Anatomically, there are innumerable correlations between AUs,which contain rich information and are vital for AU detection. Previous methodsused fixed AU correlations based on expert experience or statistical rules onspecific benchmarks, but it is challenging to comprehensively reflect complexcorrelations between AUs via hand-crafted settings. There are alternativemethods that employ a fully connected graph to learn these dependenciesexhaustively. However, these approaches can result in a computational explosionand high dependency with a large dataset. To address these challenges, thispaper proposes a novel self-adjusting AU-correlation learning (SACL) methodwith less computation for AU detection. This method adaptively learns andupdates AU correlation graphs by efficiently leveraging the characteristics ofdifferent levels of AU motion and emotion representation information extractedin different stages of the network. Moreover, this paper explores the role ofmulti-scale learning in correlation information extraction, and design a simpleyet effective multi-scale feature learning (MSFL) method to promote betterperformance in AU detection. By integrating AU correlation information withmulti-scale features, the proposed method obtains a more robust featurerepresentation for the final AU detection. Extensive experiments show that theproposed method outperforms the state-of-the-art methods on widely used AUdetection benchmark datasets, with only 28.7\% and 12.0\% of the parameters andFLOPs of the best method, respectively. The code for this method is availableat \url{https://github.com/linuxsino/Self-adjusting-AU}.</description><author>Xin Liu, Kaishen Yuan, Xuesong Niu, Jingang Shi, Zitong Yu, Huanjing Yue, Jingyu Yang</author><pubDate>Tue, 15 Aug 2023 14:43:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07770v1</guid></item><item><title>Whale Detection Enhancement through Synthetic Satellite Images</title><link>http://arxiv.org/abs/2308.07766v1</link><description>With a number of marine populations in rapid decline, collecting andanalyzing data about marine populations has become increasingly important todevelop effective conservation policies for a wide range of marine animals,including whales. Modern computer vision algorithms allow us to detect whalesin images in a wide range of domains, further speeding up and enhancing themonitoring process. However, these algorithms heavily rely on large trainingdatasets, which are challenging and time-consuming to collect particularly inmarine or aquatic environments. Recent advances in AI however have made itpossible to synthetically create datasets for training machine learningalgorithms, thus enabling new solutions that were not possible before. In thiswork, we present a solution - SeaDroneSim2 benchmark suite, which addressesthis challenge by generating aerial, and satellite synthetic image datasets toimprove the detection of whales and reduce the effort required for trainingdata collection. We show that we can achieve a 15% performance boost on whaledetection compared to using the real data alone for training, by augmenting a10% real data. We open source both the code of the simulation platformSeaDroneSim2 and the dataset generated through it.</description><author>Akshaj Gaur, Cheng Liu, Xiaomin Lin, Nare Karapetyan, Yiannis Aloimonos</author><pubDate>Tue, 15 Aug 2023 14:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07766v1</guid></item><item><title>RIGID: Recurrent GAN Inversion and Editing of Real Face Videos</title><link>http://arxiv.org/abs/2308.06097v2</link><description>GAN inversion is indispensable for applying the powerful editability of GANto real images. However, existing methods invert video frames individuallyoften leading to undesired inconsistent results over time. In this paper, wepropose a unified recurrent framework, named \textbf{R}ecurrent v\textbf{I}deo\textbf{G}AN \textbf{I}nversion and e\textbf{D}iting (RIGID), to explicitly andsimultaneously enforce temporally coherent GAN inversion and facial editing ofreal videos. Our approach models the temporal relations between current andprevious frames from three aspects. To enable a faithful real videoreconstruction, we first maximize the inversion fidelity and consistency bylearning a temporal compensated latent code. Second, we observe incoherentnoises lie in the high-frequency domain that can be disentangled from thelatent space. Third, to remove the inconsistency after attribute manipulation,we propose an \textit{in-between frame composition constraint} such that thearbitrary frame must be a direct composite of its neighboring frames. Ourunified framework learns the inherent coherence between input frames in anend-to-end manner, and therefore it is agnostic to a specific attribute and canbe applied to arbitrary editing of the same video without re-training.Extensive experiments demonstrate that RIGID outperforms state-of-the-artmethods qualitatively and quantitatively in both inversion and editing tasks.The deliverables can be found in \url{https://cnnlstm.github.io/RIGID}</description><author>Yangyang Xu, Shengfeng He, Kwan-Yee K. Wong, Ping Luo</author><pubDate>Tue, 15 Aug 2023 14:34:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06097v2</guid></item><item><title>SuS-X: Training-Free Name-Only Transfer of Vision-Language Models</title><link>http://arxiv.org/abs/2211.16198v4</link><description>Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yeteffective way to train large-scale vision-language models. CLIP demonstratesimpressive zero-shot classification and retrieval on diverse downstream tasks.However, to leverage its full potential, fine-tuning still appears to benecessary. Fine-tuning the entire CLIP model can be resource-intensive andunstable. Moreover, recent methods that aim to circumvent this need forfine-tuning still require access to images from the target distribution. Inthis paper, we pursue a different approach and explore the regime oftraining-free "name-only transfer" in which the only knowledge we possess aboutthe downstream task comprises the names of downstream target categories. Wepropose a novel method, SuS-X, consisting of two key building blocks -- SuS andTIP-X, that requires neither intensive fine-tuning nor costly labelled data.SuS-X achieves state-of-the-art zero-shot classification results on 19benchmark datasets. We further show the utility of TIP-X in the training-freefew-shot setting, where we again achieve state-of-the-art results over strongtraining-free baselines. Code is available athttps://github.com/vishaal27/SuS-X.</description><author>Vishaal Udandarao, Ankush Gupta, Samuel Albanie</author><pubDate>Tue, 15 Aug 2023 14:31:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16198v4</guid></item><item><title>NeFL: Nested Federated Learning for Heterogeneous Clients</title><link>http://arxiv.org/abs/2308.07761v1</link><description>Federated learning (FL) is a promising approach in distributed learningkeeping privacy. However, during the training pipeline of FL, slow or incapableclients (i.e., stragglers) slow down the total training time and degradeperformance. System heterogeneity, including heterogeneous computing andnetwork bandwidth, has been addressed to mitigate the impact of stragglers.Previous studies split models to tackle the issue, but with lessdegree-of-freedom in terms of model architecture. We propose nested federatedlearning (NeFL), a generalized framework that efficiently divides a model intosubmodels using both depthwise and widthwise scaling. NeFL is implemented byinterpreting models as solving ordinary differential equations (ODEs) withadaptive step sizes. To address the inconsistency that arises when trainingmultiple submodels with different architecture, we decouple a few parameters.NeFL enables resource-constrained clients to effectively join the FL pipelineand the model to be trained with a larger amount of data. Through a series ofexperiments, we demonstrate that NeFL leads to significant gains, especiallyfor the worst-case submodel (e.g., 8.33 improvement on CIFAR-10). Furthermore,we demonstrate NeFL aligns with recent studies in FL.</description><author>Honggu Kang, Seohyeon Cha, Jinwoo Shin, Jongmyeong Lee, Joonhyuk Kang</author><pubDate>Tue, 15 Aug 2023 14:29:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07761v1</guid></item><item><title>Dynamic Embedding Size Search with Minimum Regret for Streaming Recommender System</title><link>http://arxiv.org/abs/2308.07760v1</link><description>With the continuous increase of users and items, conventional recommendersystems trained on static datasets can hardly adapt to changing environments.The high-throughput data requires the model to be updated in a timely mannerfor capturing the user interest dynamics, which leads to the emergence ofstreaming recommender systems. Due to the prevalence of deep learning-basedrecommender systems, the embedding layer is widely adopted to represent thecharacteristics of users, items, and other features in low-dimensional vectors.However, it has been proved that setting an identical and static embedding sizeis sub-optimal in terms of recommendation performance and memory cost,especially for streaming recommendations. To tackle this problem, we firstrethink the streaming model update process and model the dynamic embedding sizesearch as a bandit problem. Then, we analyze and quantify the factors thatinfluence the optimal embedding sizes from the statistics perspective. Based onthis, we propose the \textbf{D}ynamic \textbf{E}mbedding \textbf{S}ize\textbf{S}earch (\textbf{DESS}) method to minimize the embedding size selectionregret on both user and item sides in a non-stationary manner. Theoretically,we obtain a sublinear regret upper bound superior to previous methods.Empirical results across two recommendation tasks on four public datasets alsodemonstrate that our approach can achieve better streaming recommendationperformance with lower memory cost and higher time efficiency.</description><author>Bowei He, Xu He, Renrui Zhang, Yingxue Zhang, Ruiming Tang, Chen Ma</author><pubDate>Tue, 15 Aug 2023 14:27:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07760v1</guid></item><item><title>Backward Reasoning in Large Language Models for Verification</title><link>http://arxiv.org/abs/2308.07758v1</link><description>Chain-of-Though (CoT) prompting has shown promising performance in variousreasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency}proposes to sample a diverse set of reasoning chains which may lead todifferent answers while the answer that receives the most votes is selected. Inthis paper, we propose a novel method to use backward reasoning in verifyingcandidate answers. We mask a token in the question by ${\bf x}$ and ask the LLMto predict the masked token when a candidate answer is provided by \textit{asimple template}, i.e., ``\textit{\textbf{If we know the answer of the abovequestion is \{a candidate answer\}, what is the value of unknown variable ${\bfx}$?}}'' Intuitively, the LLM is expected to predict the masked tokensuccessfully if the provided candidate answer is correct. We further proposeFOBAR to combine forward and backward reasoning for estimating the probabilityof candidate answers. We conduct extensive experiments on six data sets andthree LLMs. Experimental results demonstrate that FOBAR achievesstate-of-the-art performance on various reasoning benchmarks.</description><author>Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, James T. Kwok</author><pubDate>Tue, 15 Aug 2023 14:19:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07758v1</guid></item><item><title>CASPNet++: Joint Multi-Agent Motion Prediction</title><link>http://arxiv.org/abs/2308.07751v1</link><description>The prediction of road users' future motion is a critical task in supportingadvanced driver-assistance systems (ADAS). It plays an even more crucial rolefor autonomous driving (AD) in enabling the planning and execution of safedriving maneuvers. Based on our previous work, Context-Aware Scene PredictionNetwork (CASPNet), an improved system, CASPNet++, is proposed. In this work, wefocus on further enhancing the interaction modeling and scene understanding tosupport the joint prediction of all road users in a scene using spatiotemporalgrids to model future occupancy. Moreover, an instance-based output head isintroduced to provide multi-modal trajectories for agents of interest. Inextensive quantitative and qualitative analysis, we demonstrate the scalabilityof CASPNet++ in utilizing and fusing diverse environmental input sources suchas HD maps, Radar detection, and Lidar segmentation. Tested on theurban-focused prediction dataset nuScenes, CASPNet++ reaches state-of-the-artperformance. The model has been deployed in a testing vehicle, running inreal-time with moderate computational resources.</description><author>Maximilian Schäfer, Kun Zhao, Anton Kummert</author><pubDate>Tue, 15 Aug 2023 14:09:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07751v1</guid></item><item><title>Common Limitations of Image Processing Metrics: A Picture Story</title><link>http://arxiv.org/abs/2104.05642v7</link><description>While the importance of automatic image analysis is continuously increasing,recent meta-research revealed major flaws with respect to algorithm validation.Performance metrics are particularly key for meaningful, objective, andtransparent performance assessment and validation of the used automaticalgorithms, but relatively little attention has been given to the practicalpitfalls when using specific metrics for a given image analysis task. These aretypically related to (1) the disregard of inherent metric properties, such asthe behaviour in the presence of class imbalance or small target structures,(2) the disregard of inherent data set properties, such as the non-independenceof the test cases, and (3) the disregard of the actual biomedical domaininterest that the metrics should reflect. This living dynamically document hasthe purpose to illustrate important limitations of performance metrics commonlyapplied in the field of image analysis. In this context, it focuses onbiomedical image analysis problems that can be phrased as image-levelclassification, semantic segmentation, instance segmentation, or objectdetection task. The current version is based on a Delphi process on metricsconducted by an international consortium of image analysis experts from morethan 60 institutions worldwide.</description><author>Annika Reinke, Minu D. Tizabi, Carole H. Sudre, Matthias Eisenmann, Tim Rädsch, Michael Baumgartner, Laura Acion, Michela Antonelli, Tal Arbel, Spyridon Bakas, Peter Bankhead, Arriel Benis, Matthew Blaschko, Florian Büttner, M. Jorge Cardoso, Jianxu Chen, Veronika Cheplygina, Evangelia Christodoulou, Beth Cimini, Gary S. Collins, Sandy Engelhardt, Keyvan Farahani, Luciana Ferrer, Adrian Galdran, Bram van Ginneken, Ben Glocker, Patrick Godau, Robert Haase, Fred Hamprecht, Daniel A. Hashimoto, Doreen Heckmann-Nötzel, Peter Hirsch, Michael M. Hoffman, Merel Huisman, Fabian Isensee, Pierre Jannin, Charles E. Kahn, Dagmar Kainmueller, Bernhard Kainz, Alexandros Karargyris, Alan Karthikesalingam, A. Emre Kavur, Hannes Kenngott, Jens Kleesiek, Andreas Kleppe, Sven Kohler, Florian Kofler, Annette </author><pubDate>Tue, 15 Aug 2023 14:03:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.05642v7</guid></item><item><title>Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model</title><link>http://arxiv.org/abs/2308.07749v1</link><description>The rising demand for creating lifelike avatars in the digital realm has ledto an increased need for generating high-quality human videos guided by textualdescriptions and poses. We propose Dancing Avatar, designed to fabricate humanmotion videos driven by poses and textual cues. Our approach employs apretrained T2I diffusion model to generate each video frame in anautoregressive fashion. The crux of innovation lies in our adept utilization ofthe T2I diffusion model for producing video frames successively whilepreserving contextual relevance. We surmount the hurdles posed by maintaininghuman character and clothing consistency across varying poses, along withupholding the background's continuity amidst diverse human movements. To ensureconsistent human appearances across the entire video, we devise an intra-framealignment module. This module assimilates text-guided synthesized humancharacter knowledge into the pretrained T2I diffusion model, synergizinginsights from ChatGPT. For preserving background continuity, we put forth abackground alignment pipeline, amalgamating insights from segment anything andimage inpainting techniques. Furthermore, we propose an inter-frame alignmentmodule that draws inspiration from an auto-regressive pipeline to augmenttemporal consistency between adjacent frames, where the preceding frame guidesthe synthesis process of the current frame. Comparisons with state-of-the-artmethods demonstrate that Dancing Avatar exhibits the capacity to generate humanvideos with markedly superior quality, both in terms of human and backgroundfidelity, as well as temporal coherence compared to existing state-of-the-artapproaches.</description><author>Bosheng Qin, Wentao Ye, Qifan Yu, Siliang Tang, Yueting Zhuang</author><pubDate>Tue, 15 Aug 2023 14:00:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07749v1</guid></item></channel></rss>