<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 01 Sep 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>3D Whole-body Grasp Synthesis with Directional Controllability</title><link>http://arxiv.org/abs/2408.16770v1</link><description>Synthesizing 3D whole-bodies that realistically grasp objects is useful foranimation, mixed reality, and robotics. This is challenging, because the handsand body need to look natural w.r.t. each other, the grasped object, as well asthe local scene (i.e., a receptacle supporting the object). Only recent worktackles this, with a divide-and-conquer approach; it first generates a"guiding" right-hand grasp, and then searches for bodies that match this.However, the guiding-hand synthesis lacks controllability and receptacleawareness, so it likely has an implausible direction (i.e., a body can't matchthis without penetrating the receptacle) and needs corrections through majorpost-processing. Moreover, the body search needs exhaustive sampling and isexpensive. These are strong limitations. We tackle these with a novel methodcalled CWGrasp. Our key idea is that performing geometry-based reasoning "earlyon," instead of "too late," provides rich "control" signals for inference. Tothis end, CWGrasp first samples a plausible reaching-direction vector (usedlater for both the arm and hand) from a probabilistic model built viaraycasting from the object and collision checking. Then, it generates areaching body with a desired arm direction, as well as a "guiding" graspinghand with a desired palm direction that complies with the arm's one.Eventually, CWGrasp refines the body to match the "guiding" hand, whileplausibly contacting the scene. Notably, generating already-compatible "parts"greatly simplifies the "whole." Moreover, CWGrasp uniquely tackles both right-and left-hand grasps. We evaluate on the GRAB and ReplicaGrasp datasets.CWGrasp outperforms baselines, at lower runtime and budget, while allcomponents help performance. Code and models will be released.</description><author>Georgios Paschalidis, Romana Wilschut, Dimitrije Antić, Omid Taheri, Dimitrios Tzionas</author><pubDate>Thu, 29 Aug 2024 17:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16770v1</guid></item><item><title>PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning</title><link>http://arxiv.org/abs/2408.16769v1</link><description>Medical vision-language models (Med-VLMs) trained on large datasets ofmedical image-text pairs and later fine-tuned for specific tasks have emergedas a mainstream paradigm in medical image analysis. However, recent studieshave highlighted the susceptibility of these Med-VLMs to adversarial attacks,raising concerns about their safety and robustness. Randomized smoothing is awell-known technique for turning any classifier into a model that iscertifiably robust to adversarial perturbations. However, this approachrequires retraining the Med-VLM-based classifier so that it classifies wellunder Gaussian noise, which is often infeasible in practice. In this paper, wepropose a novel framework called PromptSmooth to achieve efficient certifiedrobustness of Med-VLMs by leveraging the concept of prompt learning. Given anypre-trained Med-VLM, PromptSmooth adapts it to handle Gaussian noise bylearning textual prompts in a zero-shot or few-shot manner, achieving adelicate balance between accuracy and robustness, while minimizing thecomputational overhead. Moreover, PromptSmooth requires only a single model tohandle multiple noise levels, which substantially reduces the computationalcost compared to traditional methods that rely on training a separate model foreach noise level. Comprehensive experiments based on three Med-VLMs and acrosssix downstream datasets of various imaging modalities demonstrate the efficacyof PromptSmooth. Our code and models are available athttps://github.com/nhussein/promptsmooth.</description><author>Noor Hussein, Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar</author><pubDate>Thu, 29 Aug 2024 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16769v1</guid></item><item><title>SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners</title><link>http://arxiv.org/abs/2408.16768v1</link><description>We introduce SAM2Point, a preliminary exploration adapting Segment AnythingModel 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Pointinterprets any 3D data as a series of multi-directional videos, and leveragesSAM 2 for 3D-space segmentation, without further training or 2D-3D projection.Our framework supports various prompt types, including 3D points, boxes, andmasks, and can generalize across diverse scenarios, such as 3D objects, indoorscenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlightthe robust generalization capabilities of SAM2Point. To our best knowledge, wepresent the most faithful implementation of SAM in 3D, which may serve as astarting point for future research in promptable 3D segmentation. Online Demo:https://huggingface.co/spaces/ZiyuG/SAM2Point . Code:https://github.com/ZiyuGuo99/SAM2Point .</description><author>Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Chengzhuo Tong, Peng Gao, Chunyuan Li, Pheng-Ann Heng</author><pubDate>Thu, 29 Aug 2024 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16768v1</guid></item><item><title>ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model</title><link>http://arxiv.org/abs/2408.16767v1</link><description>Advancements in 3D scene reconstruction have transformed 2D images from thereal world into 3D models, producing realistic 3D results from hundreds ofinput photos. Despite great success in dense-view reconstruction scenarios,rendering a detailed scene from insufficient captured views is still anill-posed optimization problem, often resulting in artifacts and distortions inunseen areas. In this paper, we propose ReconX, a novel 3D scene reconstructionparadigm that reframes the ambiguous reconstruction challenge as a temporalgeneration task. The key insight is to unleash the strong generative prior oflarge pre-trained video diffusion models for sparse-view reconstruction.However, 3D view consistency struggles to be accurately preserved in directlygenerated video frames from pre-trained models. To address this, given limitedinput views, the proposed ReconX first constructs a global point cloud andencodes it into a contextual space as the 3D structure condition. Guided by thecondition, the video diffusion model then synthesizes video frames that areboth detail-preserved and exhibit a high degree of 3D consistency, ensuring thecoherence of the scene from various perspectives. Finally, we recover the 3Dscene from the generated video through a confidence-aware 3D Gaussian Splattingoptimization scheme. Extensive experiments on various real-world datasets showthe superiority of our ReconX over state-of-the-art methods in terms of qualityand generalizability.</description><author>Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, Yueqi Duan</author><pubDate>Thu, 29 Aug 2024 17:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16767v1</guid></item><item><title>CSGO: Content-Style Composition in Text-to-Image Generation</title><link>http://arxiv.org/abs/2408.16766v1</link><description>The diffusion model has shown exceptional capabilities in controlled imagegeneration, which has further fueled interest in image style transfer. Existingworks mainly focus on training free-based methods (e.g., image inversion) dueto the scarcity of specific data. In this study, we present a data constructionpipeline for content-style-stylized image triplets that generates andautomatically cleanses stylized data triplets. Based on this pipeline, weconstruct a dataset IMAGStyle, the first large-scale style transfer datasetcontaining 210k image triplets, available for the community to explore andresearch. Equipped with IMAGStyle, we propose CSGO, a style transfer modelbased on end-to-end training, which explicitly decouples content and stylefeatures employing independent feature injection. The unified CSGO implementsimage-driven style transfer, text-driven stylized synthesis, and textediting-driven stylized synthesis. Extensive experiments demonstrate theeffectiveness of our approach in enhancing style control capabilities in imagegeneration. Additional visualization and access to the source code can belocated on the project page: \url{https://csgo-gen.github.io/}.</description><author>Peng Xing, Haofan Wang, Yanpeng Sun, Qixun Wang, Xu Bai, Hao Ai, Renyuan Huang, Zechao Li</author><pubDate>Thu, 29 Aug 2024 17:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16766v1</guid></item><item><title>A Score-Based Density Formula, with Applications in Diffusion Generative Models</title><link>http://arxiv.org/abs/2408.16765v1</link><description>Score-based generative models (SGMs) have revolutionized the field ofgenerative modeling, achieving unprecedented success in generating realisticand diverse content. Despite empirical advances, the theoretical basis for whyoptimizing the evidence lower bound (ELBO) on the log-likelihood is effectivefor training diffusion generative models, such as DDPMs, remains largelyunexplored. In this paper, we address this question by establishing a densityformula for a continuous-time diffusion process, which can be viewed as thecontinuous-time limit of the forward process in an SGM. This formula revealsthe connection between the target density and the score function associatedwith each step of the forward process. Building on this, we demonstrate thatthe minimizer of the optimization objective for training DDPMs nearly coincideswith that of the true objective, providing a theoretical foundation foroptimizing DDPMs using the ELBO. Furthermore, we offer new insights into therole of score-matching regularization in training GANs, the use of ELBO indiffusion classifiers, and the recently proposed diffusion loss.</description><author>Gen Li, Yuling Yan</author><pubDate>Thu, 29 Aug 2024 17:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16765v1</guid></item><item><title>Batched Stochastic Bandit for Nondegenerate Functions</title><link>http://arxiv.org/abs/2405.05733v2</link><description>This paper studies batched bandit learning problems for nondegeneratefunctions. We introduce an algorithm that solves the batched bandit problem fornondegenerate functions near-optimally. More specifically, we introduce analgorithm, called Geometric Narrowing (GN), whose regret bound is of order$\widetilde{{\mathcal{O}}} ( A_{+}^d \sqrt{T} )$. In addition, GN only needs$\mathcal{O} (\log \log T)$ batches to achieve this regret. We also providelower bound analysis for this problem. More specifically, we prove that oversome (compact) doubling metric space of doubling dimension $d$: 1. For anypolicy $\pi$, there exists a problem instance on which $\pi$ admits a regret oforder ${\Omega} ( A_-^d \sqrt{T})$; 2. No policy can achieve a regret of order$ A_-^d \sqrt{T} $ over all problem instances, using less than $ \Omega ( \log\log T ) $ rounds of communications. Our lower bound analysis shows that the GNalgorithm achieves near optimal regret with minimal number of batches.</description><author>Yu Liu, Yunlu Shu, Tianyu Wang</author><pubDate>Thu, 29 Aug 2024 17:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05733v2</guid></item><item><title>UV-free Texture Generation with Denoising and Geodesic Heat Diffusions</title><link>http://arxiv.org/abs/2408.16762v1</link><description>Seams, distortions, wasted UV space, vertex-duplication, and varyingresolution over the surface are the most prominent issues of the standardUV-based texturing of meshes. These issues are particularly acute whenautomatic UV-unwrapping techniques are used. For this reason, instead ofgenerating textures in automatically generated UV-planes like moststate-of-the-art methods, we propose to represent textures as colouredpoint-clouds whose colours are generated by a denoising diffusion probabilisticmodel constrained to operate on the surface of 3D objects. Our sampling andresolution agnostic generative model heavily relies on heat diffusion over thesurface of the meshes for spatial communication between points. To enableprocessing of arbitrarily sampled point-cloud textures and ensure long-distancetexture consistency we introduce a fast re-sampling of the mesh spectralproperties used during the heat diffusion and introduce a novelheat-diffusion-based self-attention mechanism. Our code and pre-trained modelsare available at github.com/simofoti/UV3-TeD.</description><author>Simone Foti, Stefanos Zafeiriou, Tolga Birdal</author><pubDate>Thu, 29 Aug 2024 17:57:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16762v1</guid></item><item><title>OmniRe: Omni Urban Scene Reconstruction</title><link>http://arxiv.org/abs/2408.16760v1</link><description>We introduce OmniRe, a holistic approach for efficiently reconstructinghigh-fidelity dynamic urban scenes from on-device logs. Recent methods formodeling driving sequences using neural radiance fields or Gaussian Splattinghave demonstrated the potential of reconstructing challenging dynamic scenes,but often overlook pedestrians and other non-vehicle dynamic actors, hinderinga complete pipeline for dynamic urban scene reconstruction. To that end, wepropose a comprehensive 3DGS framework for driving scenes, named OmniRe, thatallows for accurate, full-length reconstruction of diverse dynamic objects in adriving log. OmniRe builds dynamic neural scene graphs based on Gaussianrepresentations and constructs multiple local canonical spaces that modelvarious dynamic actors, including vehicles, pedestrians, and cyclists, amongmany others. This capability is unmatched by existing methods. OmniRe allows usto holistically reconstruct different objects present in the scene,subsequently enabling the simulation of reconstructed scenarios with all actorsparticipating in real-time (~60Hz). Extensive evaluations on the Waymo datasetshow that our approach outperforms prior state-of-the-art methodsquantitatively and qualitatively by a large margin. We believe our work fills acritical gap in driving reconstruction.</description><author>Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, Li Song, Yue Wang</author><pubDate>Thu, 29 Aug 2024 17:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16760v1</guid></item><item><title>VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation</title><link>http://arxiv.org/abs/2407.10972v2</link><description>In the realm of vision models, the primary mode of representation is usingpixels to rasterize the visual world. Yet this is not always the best or uniqueway to represent visual content, especially for designers and artists whodepict the world using geometry primitives such as polygons. Vector graphics(VG), on the other hand, offer a textual representation of visual content,which can be more concise and powerful for content like cartoons, sketches andscientific figures. Recent studies have shown promising results on processingvector graphics with capable Large Language Models (LLMs). However, such worksfocus solely on qualitative results, understanding, or a specific type ofvector graphics. We propose VGBench, a comprehensive benchmark for LLMs onhandling vector graphics through diverse aspects, including (a) both visualunderstanding and generation, (b) evaluation of various vector graphicsformats, (c) diverse question types, (d) wide range of prompting techniques,(e) under multiple LLMs and (f) comparison with VLMs on rasterizedrepresentations. Evaluating on our collected 4279 understanding and 5845generation samples, we find that LLMs show strong capability on both aspectswhile exhibiting less desirable performance on low-level formats (SVG). Bothdata and evaluation pipeline will be open-sourced at https://vgbench.github.io.</description><author>Bocheng Zou, Mu Cai, Jianrui Zhang, Yong Jae Lee</author><pubDate>Thu, 29 Aug 2024 17:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10972v2</guid></item><item><title>Dissecting Out-of-Distribution Detection and Open-Set Recognition: A Critical Analysis of Methods and Benchmarks</title><link>http://arxiv.org/abs/2408.16757v1</link><description>Detecting test-time distribution shift has emerged as a key capability forsafely deployed machine learning models, with the question being tackled undervarious guises in recent years. In this paper, we aim to provide a consolidatedview of the two largest sub-fields within the community: out-of-distribution(OOD) detection and open-set recognition (OSR). In particular, we aim toprovide rigorous empirical analysis of different methods across settings andprovide actionable takeaways for practitioners and researchers. Concretely, wemake the following contributions: (i) We perform rigorous cross-evaluationbetween state-of-the-art methods in the OOD detection and OSR settings andidentify a strong correlation between the performances of methods for them;(ii) We propose a new, large-scale benchmark setting which we suggest betterdisentangles the problem tackled by OOD detection and OSR, re-evaluatingstate-of-the-art OOD detection and OSR methods in this setting; (iii) Wesurprisingly find that the best performing method on standard benchmarks(Outlier Exposure) struggles when tested at scale, while scoring rules whichare sensitive to the deep feature magnitude consistently show promise; and (iv)We conduct empirical analysis to explain these phenomena and highlightdirections for future research. Code:\url{https://github.com/Visual-AI/Dissect-OOD-OSR}</description><author>Hongjun Wang, Sagar Vaze, Kai Han</author><pubDate>Thu, 29 Aug 2024 17:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16757v1</guid></item><item><title>How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models</title><link>http://arxiv.org/abs/2408.16756v1</link><description>The rapid evolution of large language models (LLMs) has transformed thecompetitive landscape in natural language processing (NLP), particularly forEnglish and other data-rich languages. However, underrepresented languages likeCantonese, spoken by over 85 million people, face significant development gaps,which is particularly concerning given the economic significance of theGuangdong-Hong Kong-Macau Greater Bay Area, and in substantialCantonese-speaking populations in places like Singapore and North America.Despite its wide use, Cantonese has scant representation in NLP research,especially compared to other languages from similarly developed regions. Tobridge these gaps, we outline current Cantonese NLP methods and introduce newbenchmarks designed to evaluate LLM performance in factual generation,mathematical logic, complex reasoning, and general knowledge in Cantonese,which aim to advance open-source Cantonese LLM technology. We also proposefuture research directions and recommended models to enhance Cantonese LLMdevelopment.</description><author>Jiyue Jiang, Liheng Chen, Pengan Chen, Sheng Wang, Qinghang Bao, Lingpeng Kong, Yu Li, Chuan Wu</author><pubDate>Thu, 29 Aug 2024 17:54:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16756v1</guid></item><item><title>Reinforcement Learning without Human Feedback for Last Mile Fine-Tuning of Large Language Models</title><link>http://arxiv.org/abs/2408.16753v1</link><description>Reinforcement learning is used to align language models with human preferencesignals after first pre-training the model to predict the next token of textwithin a large corpus using likelihood maximization. Before being deployed in aspecific domain, models are often further fine-tuned on task specific data.Since human preferences are often unavailable for the last step, it isperformed using likelihood maximization as that is the typical default method.However, reinforcement learning has other advantages besides facilitatingalignment to a human derived reward function. For one, whereas likelihoodmaximization is a form of imitation learning in which the model is trained onwhat to do under ideal conditions, reinforcement learning is not limited todemonstrating actions just for optimally reached states and trains a model whatto do under a range of scenarios as it explores the policy space. In addition,it also trains a model what not to do, suppressing competitive but pooractions. This work develops a framework for last-mile fine-tuning usingreinforcement learning and tests whether it garners performance gains. Theexperiments center on abstractive summarization, but the framework is generaland broadly applicable. Use of the procedure produced significantly betterresults than likelihood maximization when comparing raw predictions. For thespecific data tested, the gap could be bridged by employing post-processing ofthe maximum likelihood outputs. Nonetheless, the framework offers a new avenuefor model optimization in situations where post-processing may be lessstraightforward or effective, and it can be extended to include more complexclasses of undesirable outputs to penalize and train against, such ashallucinations.</description><author>Alec Solway</author><pubDate>Thu, 29 Aug 2024 17:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16753v1</guid></item><item><title>GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models</title><link>http://arxiv.org/abs/2408.11817v2</link><description>Large multimodal models (LMMs) have exhibited proficiencies across manyvisual tasks. Although numerous well-known benchmarks exist to evaluate modelperformance, they increasingly have insufficient headroom. As such, there is apressing need for a new generation of benchmarks challenging enough for thenext generation of LMMs. One area that LMMs show potential is graph analysis,specifically, the tasks an analyst might typically perform when interpretingfigures such as estimating the mean, intercepts or correlations of functionsand data series. In this work, we introduce GRAB, a graph analysis benchmark,fit for current and future frontier LMMs. Our benchmark is entirely synthetic,ensuring high-quality, noise-free questions. GRAB is comprised of 2170questions, covering four tasks and 23 graph properties. We evaluate 20 LMMs onGRAB, finding it to be a challenging benchmark, with the highest performingmodel attaining a score of just 21.7%. Finally, we conduct various ablations toinvestigate where the models succeed and struggle. We release GRAB to encourageprogress in this important, growing domain.</description><author>Jonathan Roberts, Kai Han, Samuel Albanie</author><pubDate>Thu, 29 Aug 2024 17:47:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11817v2</guid></item><item><title>Conditional score-based diffusion models for solving inverse problems in mechanics</title><link>http://arxiv.org/abs/2406.13154v3</link><description>We propose a framework to perform Bayesian inference using conditionalscore-based diffusion models to solve a class of inverse problems in mechanicsinvolving the inference of a specimen's spatially varying material propertiesfrom noisy measurements of its mechanical response to loading. Conditionalscore-based diffusion models are generative models that learn to approximatethe score function of a conditional distribution using samples from the jointdistribution. More specifically, the score functions corresponding to multiplerealizations of the measurement are approximated using a single neural network,the so-called score network, which is subsequently used to sample the posteriordistribution using an appropriate Markov chain Monte Carlo scheme based onLangevin dynamics. Training the score network only requires simulating theforward model. Hence, the proposed approach can accommodate black-box forwardmodels and complex measurement noise. Moreover, once the score network has beentrained, it can be re-used to solve the inverse problem for differentrealizations of the measurements. We demonstrate the efficacy of the proposedapproach on a suite of high-dimensional inverse problems in mechanics thatinvolve inferring heterogeneous material properties from noisy measurements.Some examples we consider involve synthetic data, while others include datacollected from actual elastography experiments. Further, our applicationsdemonstrate that the proposed approach can handle different measurementmodalities, complex patterns in the inferred quantities, non-Gaussian andnon-additive noise models, and nonlinear black-box forward models. The resultsshow that the proposed framework can solve large-scale physics-based inverseproblems efficiently.</description><author>Agnimitra Dasgupta, Harisankar Ramaswamy, Javier Murgoitio-Esandi, Ken Foo, Runze Li, Qifa Zhou, Brendan Kennedy, Assad Oberai</author><pubDate>Thu, 29 Aug 2024 17:47:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13154v3</guid></item><item><title>A Gradient Analysis Framework for Rewarding Good and Penalizing Bad Examples in Language Models</title><link>http://arxiv.org/abs/2408.16751v1</link><description>Beyond maximum likelihood estimation (MLE), the standard objective of alanguage model (LM) that optimizes good examples probabilities, many studieshave explored ways that also penalize bad examples for enhancing the quality ofoutput distribution, including unlikelihood training, exponential maximizingaverage treatment effect (ExMATE), and direct preference optimization (DPO). Tosystematically compare these methods and further provide a unified recipe forLM optimization, in this paper, we present a unique angle of gradient analysisof loss functions that simultaneously reward good examples and penalize badones in LMs. Through both mathematical results and experiments onCausalDialogue and Anthropic HH-RLHF datasets, we identify distinct functionalcharacteristics among these methods. We find that ExMATE serves as a superiorsurrogate for MLE, and that combining DPO with ExMATE instead of MLE furtherenhances both the statistical (5-7%) and generative (+18% win rate)performance.</description><author>Yi-Lin Tuan, William Yang Wang</author><pubDate>Thu, 29 Aug 2024 17:46:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16751v1</guid></item><item><title>Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge</title><link>http://arxiv.org/abs/2408.16749v1</link><description>The United States has experienced a significant increase in violentextremism, prompting the need for automated tools to detect and limit thespread of extremist ideology online. This study evaluates the performance ofBidirectional Encoder Representations from Transformers (BERT) and GenerativePre-Trained Transformers (GPT) in detecting and classifying online domesticextremist posts. We collected social media posts containing "far-right" and"far-left" ideological keywords and manually labeled them as extremist ornon-extremist. Extremist posts were further classified into one or more of fivecontributing elements of extremism based on a working definitional framework.The BERT model's performance was evaluated based on training data size andknowledge transfer between categories. We also compared the performance of GPT3.5 and GPT 4 models using different prompts: na\"ive, layperson-definition,role-playing, and professional-definition. Results showed that the bestperforming GPT models outperformed the best performing BERT models, with moredetailed prompts generally yielding better results. However, overly complexprompts may impair performance. Different versions of GPT have uniquesensitives to what they consider extremist. GPT 3.5 performed better atclassifying far-left extremist posts, while GPT 4 performed better atclassifying far-right extremist posts. Large language models, represented byGPT models, hold significant potential for online extremism classificationtasks, surpassing traditional BERT models in a zero-shot setting. Futureresearch should explore human-computer interactions in optimizing GPT modelsfor extremist detection and classification tasks to develop more efficient(e.g., quicker, less effort) and effective (e.g., fewer errors or mistakes)methods for identifying extremist content.</description><author>Beidi Dong, Jin R. Lee, Ziwei Zhu, Balassubramanian Srinivasan</author><pubDate>Thu, 29 Aug 2024 17:43:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16749v1</guid></item><item><title>Theoretical and Methodological Framework for Studying Texts Produced by Large Language Models</title><link>http://arxiv.org/abs/2408.16740v1</link><description>This paper addresses the conceptual, methodological and technical challengesin studying large language models (LLMs) and the texts they produce from aquantitative linguistics perspective. It builds on a theoretical framework thatdistinguishes between the LLM as a substrate and the entities the modelsimulates. The paper advocates for a strictly non-anthropomorphic approach tomodels while cautiously applying methodologies used in studying humanlinguistic behavior to the simulated entities. While natural languageprocessing researchers focus on the models themselves, their architecture,evaluation, and methods for improving performance, we as quantitative linguistsshould strive to build a robust theory concerning the characteristics of textsproduced by LLMs, how they differ from human-produced texts, and the propertiesof simulated entities. Additionally, we should explore the potential of LLMs asan instrument for studying human culture, of which language is an integralpart.</description><author>Jiří Milička</author><pubDate>Thu, 29 Aug 2024 17:34:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16740v1</guid></item><item><title>Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling</title><link>http://arxiv.org/abs/2408.16737v1</link><description>Training on high-quality synthetic data from strong language models (LMs) isa common strategy to improve the reasoning performance of LMs. In this work, werevisit whether this strategy is compute-optimal under a fixed inference budget(e.g., FLOPs). To do so, we investigate the trade-offs between generatingsynthetic data using a stronger but more expensive (SE) model versus a weakerbut cheaper (WC) model. We evaluate the generated data across three keymetrics: coverage, diversity, and false positive rate, and show that the datafrom WC models may have higher coverage and diversity, but also exhibit higherfalse positive rates. We then finetune LMs on data from SE and WC models indifferent settings: knowledge distillation, self-improvement, and a novelweak-to-strong improvement setup where a weaker LM teaches reasoning to astronger LM. Our findings reveal that models finetuned on WC-generated dataconsistently outperform those trained on SE-generated data across multiplebenchmarks and multiple choices of WC and SE models. These results challengethe prevailing practice of relying on SE models for synthetic data generation,suggesting that WC may be the compute-optimal approach for training advanced LMreasoners.</description><author>Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Q. Tran, Mehran Kazemi</author><pubDate>Thu, 29 Aug 2024 17:32:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16737v1</guid></item><item><title>FilFL: Client Filtering for Optimized Client Participation in Federated Learning</title><link>http://arxiv.org/abs/2302.06599v3</link><description>Federated learning, an emerging machine learning paradigm, enables clients tocollaboratively train a model without exchanging local data. Clientsparticipating in the training process significantly impact the convergencerate, learning efficiency, and model generalization. We propose a novelapproach, client filtering, to improve model generalization and optimize clientparticipation and training. The proposed method periodically filters availableclients to identify a subset that maximizes a combinatorial objective functionwith an efficient greedy filtering algorithm. Thus, the clients are assessed asa combination rather than individually. We theoretically analyze theconvergence of federated learning with client filtering in heterogeneoussettings and evaluate its performance across diverse vision and language tasks,including realistic scenarios with time-varying client availability. Ourempirical results demonstrate several benefits of our approach, includingimproved learning efficiency, faster convergence, and up to 10% higher testaccuracy than training without client filtering.</description><author>Fares Fourati, Salma Kharrat, Vaneet Aggarwal, Mohamed-Slim Alouini, Marco Canini</author><pubDate>Thu, 29 Aug 2024 17:31:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06599v3</guid></item><item><title>Learning to Prompt Your Domain for Vision-Language Models</title><link>http://arxiv.org/abs/2310.03103v5</link><description>Prompt learning has recently become a very efficient transfer learningparadigm for Contrastive Language Image Pretraining (CLIP) models. Comparedwith fine-tuning the entire encoder, prompt learning can obtain highlycompetitive results by optimizing only a small number of parameters, whichpresents considerably exciting benefits for federated learning applicationsthat prioritizes communication efficiency. However, in this work, we identifythat directly transferring prompt learning approaches into federated learningdoes not yield favorable results since the model often suffers fromconsiderable domain gaps across different clients. To address this issue, wepropose ADAPT, a novel domain-aware prompt learning approach that facilitatesboth intra- and inter-domain prompts across federated participants. The basicidea of ADAPT is that the prompted CLIP should detect the input image's domaincorrespondence and before making the prediction of its category. Extensiveexperiments of ADAPT demonstrate its significant efficiency and effectivenessin federated learning. For example, by learning and sharing only 0.08Mparameters, our ADAPT attains a 68.4% average accuracy over six domains in theDomainNet dataset, which improves the original CLIP by a large margin of 14.8%.</description><author>Guoyizhe Wei, Feng Wang, Anshul Shah, Rama Chellappa</author><pubDate>Thu, 29 Aug 2024 17:24:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03103v5</guid></item><item><title>VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation</title><link>http://arxiv.org/abs/2408.16730v1</link><description>A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) isthat while increasing the number of vision tokens generally enhances visualunderstanding, it also significantly raises memory and computational costs,especially in long-term, dense video frame streaming scenarios. Althoughlearnable approaches like Q-Former and Perceiver Resampler have been developedto reduce the vision token burden, they overlook the context causally modeledby LLMs (i.e., key-value cache), potentially leading to missed visual cues whenaddressing user queries. In this paper, we introduce a novel approach to reducevision compute by leveraging redundant vision tokens "skipping layers" ratherthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, isinspired by mixture-of-depths LLMs and addresses the challenge of numerousvision tokens in long-term or streaming video. Specifically, for eachtransformer layer, we learn to skip the computation for a high proportion(e.g., 80\%) of vision tokens, passing them directly to the next layer. Thisapproach significantly enhances model efficiency, achieving approximately\textasciitilde42\% time and \textasciitilde30\% memory savings for the entiretraining. Moreover, our method reduces the computation in the context and avoiddecreasing the vision tokens, thus preserving or even improving performancecompared to the vanilla model. We conduct extensive experiments to demonstratethe effectiveness of VideoLLM-MoD, showing its state-of-the-art results onmultiple benchmarks, including narration, forecasting, and summarization tasksin COIN, Ego4D, and Ego-Exo4D datasets.</description><author>Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli Xu, Tong Xu, Yao Hu, Enhong Chen, Mike Zheng Shou</author><pubDate>Thu, 29 Aug 2024 17:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16730v1</guid></item><item><title>Evaluation Framework for Feedback Generation Methods in Skeletal Movement Assessment</title><link>http://arxiv.org/abs/2404.09359v4</link><description>The application of machine-learning solutions to movement assessment fromskeleton videos has attracted significant research attention in recent years.This advancement has made rehabilitation at home more accessible, utilizingmovement assessment algorithms that can operate on affordable equipment forhuman pose detection and analysis from 2D or 3D videos. While the primaryobjective of automatic assessment tasks is to score movements, the automaticgeneration of feedback highlighting key movement issues has the potential tosignificantly enhance and accelerate the rehabilitation process. While numerousresearch works exist in the field of automatic movement assessment, only ahandful address feedback generation. In this study, we propose terminology andcriteria for the classification, evaluation, and comparison of feedbackgeneration solutions. We discuss the challenges associated with each feedbackgeneration approach and use our proposed criteria to classify existingsolutions. To our knowledge, this is the first work that formulates feedbackgeneration in skeletal movement assessment.</description><author>Tal Hakim</author><pubDate>Thu, 29 Aug 2024 17:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09359v4</guid></item><item><title>Prediction-Feedback DETR for Temporal Action Detection</title><link>http://arxiv.org/abs/2408.16729v1</link><description>Temporal Action Detection (TAD) is fundamental yet challenging for real-worldvideo applications. Leveraging the unique benefits of transformers, variousDETR-based approaches have been adopted in TAD. However, it has recently beenidentified that the attention collapse in self-attention causes the performancedegradation of DETR for TAD. Building upon previous research, this paper newlyaddresses the attention collapse problem in cross-attention within DETR-basedTAD methods. Moreover, our findings reveal that cross-attention exhibitspatterns distinct from predictions, indicating a short-cut phenomenon. Toresolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR),which utilizes predictions to restore the collapse and align the cross- andself-attention with predictions. Specifically, we devise novelprediction-feedback objectives using guidance from the relations of thepredictions. As a result, Pred-DETR significantly alleviates the collapse andachieves state-of-the-art performance among DETR-based methods on variouschallenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, andFineAction.</description><author>Jihwan Kim, Miso Lee, Cheol-Ho Cho, Jihyun Lee, Jae-Pil Heo</author><pubDate>Thu, 29 Aug 2024 17:20:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16729v1</guid></item><item><title>Adaptive Log-Euclidean Metrics for SPD Matrix Learning</title><link>http://arxiv.org/abs/2303.15477v5</link><description>Symmetric Positive Definite (SPD) matrices have received wide attention inmachine learning due to their intrinsic capacity to encode underlyingstructural correlation in data. Many successful Riemannian metrics have beenproposed to reflect the non-Euclidean geometry of SPD manifolds. However, mostexisting metric tensors are fixed, which might lead to sub-optimal performancefor SPD matrix learning, especially for deep SPD neural networks. To remedythis limitation, we leverage the commonly encountered pullback techniques andpropose Adaptive Log-Euclidean Metrics (ALEMs), which extend the widely usedLog-Euclidean Metric (LEM). Compared with the previous Riemannian metrics, ourmetrics contain learnable parameters, which can better adapt to the complexdynamics of Riemannian neural networks with minor extra computations. We alsopresent a complete theoretical analysis to support our ALEMs, includingalgebraic and Riemannian properties. The experimental and theoretical resultsdemonstrate the merit of the proposed metrics in improving the performance ofSPD neural networks. The efficacy of our metrics is further showcased on a setof recently developed Riemannian building blocks, including Riemannian batchnormalization, Riemannian Residual blocks, and Riemannian classifiers.</description><author>Ziheng Chen, Yue Song, Tianyang Xu, Zhiwu Huang, Xiao-Jun Wu, Nicu Sebe</author><pubDate>Thu, 29 Aug 2024 17:20:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15477v5</guid></item><item><title>Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming</title><link>http://arxiv.org/abs/2408.16725v1</link><description>Recent advances in language models have achieved significant progress.GPT-4o, as a new milestone, has enabled real-time conversations with humans,demonstrating near-human natural fluency. Such human-computer interactionnecessitates models with the capability to perform reasoning directly with theaudio modality and generate output in streaming. However, this remains beyondthe reach of current academic models, as they typically depend on extra TTSsystems for speech synthesis, resulting in undesirable latency. This paperintroduces the Mini-Omni, an audio-based end-to-end conversational model,capable of real-time speech interaction. To achieve this capability, we proposea text-instructed speech generation method, along with batch-parallelstrategies during inference to further boost the performance. Our method alsohelps to retain the original model's language capabilities with minimaldegradation, enabling other works to establish real-time interactioncapabilities. We call this training method "Any Model Can Talk". We alsointroduce the VoiceAssistant-400K dataset to fine-tune models optimized forspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,open-source model for real-time speech interaction, offering valuable potentialfor future research.</description><author>Zhifei Xie, Changqiao Wu</author><pubDate>Thu, 29 Aug 2024 17:18:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16725v1</guid></item><item><title>OpticalRS-4M: Scaling Efficient Masked Autoencoder Learning on Large Remote Sensing Dataset</title><link>http://arxiv.org/abs/2406.11933v2</link><description>Masked Image Modeling (MIM) has become an essential method for buildingfoundational visual models in remote sensing (RS). However, the limitations insize and diversity of existing RS datasets restrict the ability of MIM methodsto learn generalizable representations. Additionally, conventional MIMtechniques, which require reconstructing all tokens, introduce unnecessarycomputational overhead. To address these issues, we present a new pre-trainingpipeline for RS models, featuring the creation of a large-scale RS dataset andan efficient MIM approach. We curated a high-quality dataset named OpticalRS-4Mby collecting publicly available RS datasets and processing them throughexclusion, slicing, and deduplication. OpticalRS-4M comprises 4 million opticalimages covering various RS tasks, such as object detection and pixelsegmentation. To enhance efficiency, we propose SelectiveMAE, a pre-trainingmethod that dynamically encodes and reconstructs semantically rich patchtokens, thereby reducing the inefficiencies of traditional MIM models caused byredundant background pixels in RS images. Extensive experiments demonstratethat OpticalRS-4M significantly improves classification, detection, andsegmentation performance, while SelectiveMAE increases training efficiency over2 times. This highlights the effectiveness and scalability of our pipeline indeveloping RS foundational models.</description><author>Fengxiang Wang, Hongzhen Wang, Di Wang, Zonghao Guo, Zhenyu Zhong, Long Lan, Jing Zhang, Zhiyuan Liu, Maosong Sun</author><pubDate>Thu, 29 Aug 2024 17:16:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11933v2</guid></item><item><title>Normalized mutual information is a biased measure for classification and community detection</title><link>http://arxiv.org/abs/2307.01282v2</link><description>Normalized mutual information is widely used as a similarity measure forevaluating the performance of clustering and classification algorithms. In thispaper, we argue that results returned by the normalized mutual information arebiased for two reasons: first, because they ignore the information content ofthe contingency table and, second, because their symmetric normalizationintroduces spurious dependence on algorithm output. We introduce a modifiedversion of the mutual information that remedies both of these shortcomings. Asa practical demonstration of the importance of using an unbiased measure, weperform extensive numerical tests on a basket of popular algorithms for networkcommunity detection and show that one's conclusions about which algorithm isbest are significantly affected by the biases in the traditional mutualinformation.</description><author>Maximilian Jerdee, Alec Kirkley, M. E. J. Newman</author><pubDate>Thu, 29 Aug 2024 17:13:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01282v2</guid></item><item><title>H-SGANet: Hybrid Sparse Graph Attention Network for Deformable Medical Image Registration</title><link>http://arxiv.org/abs/2408.16719v1</link><description>The integration of Convolutional Neural Network (ConvNet) and Transformer hasemerged as a strong candidate for image registration, leveraging the strengthsof both models and a large parameter space. However, this hybrid model,treating brain MRI volumes as grid or sequence structures, faces challenges inaccurately representing anatomical connectivity, diverse brain regions, andvital connections contributing to the brain's internal architecture. Concernsalso arise regarding the computational expense and GPU memory usage associatedwith this model. To tackle these issues, a lightweight hybrid sparse graphattention network (H-SGANet) has been developed. This network incorporates acentral mechanism, Sparse Graph Attention (SGA), based on a Vision Graph NeuralNetwork (ViG) with predetermined anatomical connections. The SGA module expandsthe model's receptive field and seamlessly integrates into the network. Tofurther amplify the advantages of the hybrid network, the SeparableSelf-Attention (SSA) is employed as an enhanced token mixer, integrated withdepth-wise convolution to constitute SSAFormer. This strategic integration isdesigned to more effectively extract long-range dependencies. As a hybridConvNet-ViG-Transformer model, H-SGANet offers threefold benefits forvolumetric medical image registration. It optimizes fixed and moving imagesconcurrently through a hybrid feature fusion layer and an end-to-end learningframework. Compared to VoxelMorph, a model with a similar parameter count,H-SGANet demonstrates significant performance enhancements of 3.5% and 1.5% inDice score on the OASIS dataset and LPBA40 dataset, respectively.</description><author>Yufeng Zhou, Wenming Cao</author><pubDate>Thu, 29 Aug 2024 17:11:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16719v1</guid></item><item><title>A GREAT Architecture for Edge-Based Graph Problems Like TSP</title><link>http://arxiv.org/abs/2408.16717v1</link><description>In the last years, many neural network-based approaches have been proposed totackle combinatorial optimization problems such as routing problems. Many ofthese approaches are based on graph neural networks (GNNs) or relatedtransformers, operating on the Euclidean coordinates representing the routingproblems. However, GNNs are inherently not well suited to operate on densegraphs, such as in routing problems. Furthermore, models operating on Euclideancoordinates cannot be applied to non-Euclidean versions of routing problemsthat are often found in real-world settings. To overcome these limitations, wepropose a novel GNN-related edge-based neural model called Graph Edge AttentionNetwork (GREAT). We evaluate the performance of GREAT in theedge-classification task to predict optimal edges in the Traveling SalesmanProblem (TSP). We can use such a trained GREAT model to produce sparse TSPgraph instances, keeping only the edges GREAT finds promising. Compared toother, non-learning-based methods to sparsify TSP graphs, GREAT can producevery sparse graphs while keeping most of the optimal edges. Furthermore, webuild a reinforcement learning-based GREAT framework which we apply toEuclidean and non-Euclidean asymmetric TSP. This framework achievesstate-of-the-art results.</description><author>Attila Lischka, Jiaming Wu, Morteza Haghir Chehreghani, Balázs Kulcsár</author><pubDate>Thu, 29 Aug 2024 17:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16717v1</guid></item><item><title>Wasserstein Gradient Boosting: A Framework for Distribution-Valued Supervised Learning</title><link>http://arxiv.org/abs/2405.09536v2</link><description>Gradient boosting is a sequential ensemble method that fits a new weakerlearner to pseudo residuals at each iteration. We propose Wasserstein gradientboosting, a novel extension of gradient boosting that fits a new weak learnerto alternative pseudo residuals that are Wasserstein gradients of lossfunctionals of probability distributions assigned at each input. It solvesdistribution-valued supervised learning, where the output values of thetraining dataset are probability distributions for each input. Inclassification and regression, a model typically returns, for each input, apoint estimate of a parameter of a noise distribution specified for a responsevariable, such as the class probability parameter of a categorical distributionspecified for a response label. A main application of Wasserstein gradientboosting in this paper is tree-based evidential learning, which returns adistributional estimate of the response parameter for each input. Weempirically demonstrate the superior performance of the probabilisticprediction by Wasserstein gradient boosting in comparison with existinguncertainty quantification methods.</description><author>Takuo Matsubara</author><pubDate>Thu, 29 Aug 2024 17:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09536v2</guid></item><item><title>Enhanced forecasting of stock prices based on variational mode decomposition, PatchTST, and adaptive scale-weighted layer</title><link>http://arxiv.org/abs/2408.16707v1</link><description>The significant fluctuations in stock index prices in recent years highlightthe critical need for accurate forecasting to guide investment and financialstrategies. This study introduces a novel composite forecasting framework thatintegrates variational mode decomposition (VMD), PatchTST, and adaptivescale-weighted layer (ASWL) to address these challenges. Utilizing datasets offour major stock indices--SP500, DJI, SSEC, and FTSE--from 2000 to 2024, theproposed method first decomposes the raw price series into intrinsic modefunctions (IMFs) using VMD. Each IMF is then modeled with PatchTST to capturetemporal patterns effectively. The ASWL module is applied to incorporate scaleinformation, enhancing prediction accuracy. The final forecast is derived byaggregating predictions from all IMFs. The VMD-PatchTST-ASWL frameworkdemonstrates significant improvements in forecasting accuracy compared totraditional models, showing robust performance across different indices. Thisinnovative approach provides a powerful tool for stock index price forecasting,with potential applications in various financial analysis and investmentdecision-making contexts.</description><author>Xiaorui Xue, Shaofang Li, Xiaonan Wang</author><pubDate>Thu, 29 Aug 2024 17:00:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16707v1</guid></item><item><title>Awes, Laws, and Flaws From Today's LLM Research</title><link>http://arxiv.org/abs/2408.15409v2</link><description>We perform a critical examination of the scientific methodology behindcontemporary large language model (LLM) research. For this we assess over 2,000research works based on criteria typical of what is considered good research(e.g. presence of statistical tests and reproducibility) and cross-validate itwith arguments that are at the centre of controversy (e.g., claims of emergentbehaviour, the use of LLMs as evaluators). We find multiple trends, such asdeclines in claims of emergent behaviour and ethics disclaimers; the rise ofLLMs as evaluators in spite of a lack of consensus from the community abouttheir useability; and an increase of claims of LLM reasoning abilities,typically without leveraging human evaluation. This paper underscores the needfor more scrutiny and rigour by and from this field to live up to thefundamentals of a responsible scientific method that is ethical, reproducible,systematic, and open to criticism.</description><author>Adrian de Wynter</author><pubDate>Thu, 29 Aug 2024 17:00:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15409v2</guid></item><item><title>One-Shot Learning Meets Depth Diffusion in Multi-Object Videos</title><link>http://arxiv.org/abs/2408.16704v1</link><description>Creating editable videos that depict complex interactions between multipleobjects in various artistic styles has long been a challenging task infilmmaking. Progress is often hampered by the scarcity of data sets thatcontain paired text descriptions and corresponding videos that showcase theseinteractions. This paper introduces a novel depth-conditioning approach thatsignificantly advances this field by enabling the generation of coherent anddiverse videos from just a single text-video pair using a pre-traineddepth-aware Text-to-Image (T2I) model. Our method fine-tunes the pre-trainedmodel to capture continuous motion by employing custom-designed spatial andtemporal attention mechanisms. During inference, we use the DDIM inversion toprovide structural guidance for video generation. This innovative techniqueallows for continuously controllable depth in videos, facilitating thegeneration of multiobject interactions while maintaining the concept generationand compositional strengths of the original T2I model across various artisticstyles, such as photorealism, animation, and impressionism.</description><author>Anisha Jain</author><pubDate>Thu, 29 Aug 2024 16:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16704v1</guid></item><item><title>CC-GPX: Extracting High-Quality Annotated Geospatial Data from Common Crawl</title><link>http://arxiv.org/abs/2405.11039v3</link><description>The Common Crawl (CC) corpus is the largest open web crawl dataset containing9.5+ petabytes of data captured since 2008. The dataset is instrumental intraining large language models, and as such it has been studied for(un)desirable content, and distilled for smaller, domain-specific datasets.However, to our knowledge, no research has been dedicated to using CC as asource of annotated geospatial data. In this paper, we introduce an efficientpipeline to extract annotated user-generated tracks from GPX files found in CC,and the resulting multimodal dataset with 1,416 pairings of human-writtendescriptions and MultiLineString vector data from the 6 most recent CCreleases. The dataset can be used to study people's outdoor activity patterns,the way people talk about their outdoor experiences, as well as for developingtrajectory generation or track annotation models, or for various other problemsin place of synthetically generated routes. Our reproducible code is availableon GitHub: https://github.com/ilyankou/cc-gpx</description><author>Ilya Ilyankou, Meihui Wang, Stefano Cavazzi, James Haworth</author><pubDate>Thu, 29 Aug 2024 16:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11039v3</guid></item><item><title>GradBias: Unveiling Word Influence on Bias in Text-to-Image Generative Models</title><link>http://arxiv.org/abs/2408.16700v1</link><description>Recent progress in Text-to-Image (T2I) generative models has enabledhigh-quality image generation. As performance and accessibility increase, thesemodels are gaining significant attraction and popularity: ensuring theirfairness and safety is a priority to prevent the dissemination and perpetuationof biases. However, existing studies in bias detection focus on closed sets ofpredefined biases (e.g., gender, ethnicity). In this paper, we propose ageneral framework to identify, quantify, and explain biases in an open setsetting, i.e. without requiring a predefined set. This pipeline leverages aLarge Language Model (LLM) to propose biases starting from a set of captions.Next, these captions are used by the target generative model for generating aset of images. Finally, Vision Question Answering (VQA) is leveraged for biasevaluation. We show two variations of this framework: OpenBias and GradBias.OpenBias detects and quantifies biases, while GradBias determines thecontribution of individual prompt words on biases. OpenBias effectively detectsboth well-known and novel biases related to people, objects, and animals andhighly aligns with existing closed-set bias detection methods and humanjudgment. GradBias shows that neutral words can significantly influence biasesand it outperforms several baselines, including state-of-the-art foundationmodels. Code available here: https://github.com/Moreno98/GradBias.</description><author>Moreno D'Incà, Elia Peruzzo, Massimiliano Mancini, Xingqian Xu, Humphrey Shi, Nicu Sebe</author><pubDate>Thu, 29 Aug 2024 16:51:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16700v1</guid></item><item><title>Quantifying Geospatial in the Common Crawl Corpus</title><link>http://arxiv.org/abs/2406.04952v2</link><description>Large language models (LLMs) exhibit emerging geospatial capabilities,stemming from their pre-training on vast unlabelled text datasets that areoften derived from the Common Crawl (CC) corpus. However, the geospatialcontent within CC remains largely unexplored, impacting our understanding ofLLMs' spatial reasoning. This paper investigates the prevalence of geospatialdata in recent Common Crawl releases using Gemini 1.5, a powerful languagemodel. By analyzing a sample of documents and manually revising the results, weestimate that 18.7% of web documents in CC contain geospatial information suchas coordinates and addresses. We find little difference in prevalence betweenEnlgish- and non-English-language documents. Our findings provide quantitativeinsights into the nature and extent of geospatial data in CC, and lay thegroundwork for future studies of geospatial biases of LLMs.</description><author>Ilya Ilyankou, Meihui Wang, Stefano Cavazzi, James Haworth</author><pubDate>Thu, 29 Aug 2024 16:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04952v2</guid></item><item><title>GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM</title><link>http://arxiv.org/abs/2403.05527v3</link><description>Key-value (KV) caching has become the de-facto to accelerate generation speedfor large language models (LLMs) inference. However, the growing cache demandwith increasing sequence length has transformed LLM inference to be a memorybound problem, significantly constraining the system throughput. Existingmethods rely on dropping unimportant tokens or quantizing all entriesuniformly. Such methods, however, often incur high approximation errors torepresent the compressed matrices. The autoregressive decoding process furthercompounds the error of each step, resulting in critical deviation in modelgeneration and deterioration of performance. To tackle this challenge, wepropose GEAR, an efficient KV cache compression framework that achievesnear-lossless high-ratio compression. GEAR first applies quantization tomajority of entries of similar magnitudes to ultra-low precision. It thenemploys a low rank matrix to approximate the quantization error, and a sparsematrix to remedy individual errors from outlier entries. By adeptly integratingthree techniques, GEAR is able to fully exploit their synergistic potentials.Our experiments demonstrate that compared to alternatives, GEAR achievesnear-lossless 4-bit KV cache compression with up to 2.38x throughputimprovement, while reducing peak-memory size up to 2.29x. Our code is publiclyavailable at https://github.com/HaoKang-Timmy/GEAR.</description><author>Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao</author><pubDate>Thu, 29 Aug 2024 16:48:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05527v3</guid></item><item><title>SympGNNs: Symplectic Graph Neural Networks for identifiying high-dimensional Hamiltonian systems and node classification</title><link>http://arxiv.org/abs/2408.16698v1</link><description>Existing neural network models to learn Hamiltonian systems, such asSympNets, although accurate in low-dimensions, struggle to learn the correctdynamics for high-dimensional many-body systems. Herein, we introduceSymplectic Graph Neural Networks (SympGNNs) that can effectively handle systemidentification in high-dimensional Hamiltonian systems, as well as nodeclassification. SympGNNs combines symplectic maps with permutationequivariance, a property of graph neural networks. Specifically, we propose twovariants of SympGNNs: i) G-SympGNN and ii) LA-SympGNN, arising from differentparameterizations of the kinetic and potential energy. We demonstrate thecapabilities of SympGNN on two physical examples: a 40-particle coupledHarmonic oscillator, and a 2000-particle molecular dynamics simulation in atwo-dimensional Lennard-Jones potential. Furthermore, we demonstrate theperformance of SympGNN in the node classification task, achieving accuracycomparable to the state-of-the-art. We also empirically show that SympGNN canovercome the oversmoothing and heterophily problems, two key challenges in thefield of graph neural networks.</description><author>Alan John Varghese, Zhen Zhang, George Em Karniadakis</author><pubDate>Thu, 29 Aug 2024 16:47:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16698v1</guid></item><item><title>Misam: Using ML in Dataflow Selection of Sparse-Sparse Matrix Multiplication</title><link>http://arxiv.org/abs/2406.10166v2</link><description>Sparse matrix-matrix multiplication (SpGEMM) is a critical operation innumerous fields, including scientific computing, graph analytics, and deeplearning. These applications exploit the sparsity of matrices to reduce storageand computational demands. However, the irregular structure of sparse matricesposes significant challenges for performance optimization. Traditional hardwareaccelerators are tailored for specific sparsity patterns with fixed dataflowschemes - inner, outer, and row-wise but often perform suboptimally when theactual sparsity deviates from these predetermined patterns. As the use ofSpGEMM expands across various domains, each with distinct sparsitycharacteristics, the demand for hardware accelerators that can efficientlyhandle a range of sparsity patterns is increasing. This paper presents amachine learning based approach for adaptively selecting the most appropriatedataflow scheme for SpGEMM tasks with diverse sparsity patterns. By employingdecision trees and deep reinforcement learning, we explore the potential ofthese techniques to surpass heuristic-based methods in identifying optimaldataflow schemes. We evaluate our models by comparing their performance withthat of a heuristic, highlighting the strengths and weaknesses of eachapproach. Our findings suggest that using machine learning for dynamic dataflowselection in hardware accelerators can provide upto 28 times gains.</description><author>Sanjali Yadav, Bahar Asgari</author><pubDate>Thu, 29 Aug 2024 16:44:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10166v2</guid></item><item><title>Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models</title><link>http://arxiv.org/abs/2310.12000v2</link><description>Latent Gaussian process (GP) models are flexible probabilistic non-parametricfunction models. Vecchia approximations are accurate approximations for GPs toovercome computational bottlenecks for large data, and the Laplaceapproximation is a fast method with asymptotic convergence guarantees toapproximate marginal likelihoods and posterior predictive distributions fornon-Gaussian likelihoods. Unfortunately, the computational complexity ofcombined Vecchia-Laplace approximations grows faster than linearly in thesample size when used in combination with direct solver methods such as theCholesky decomposition. Computations with Vecchia-Laplace approximations canthus become prohibitively slow precisely when the approximations are usuallythe most accurate, i.e., on large data sets. In this article, we presentiterative methods to overcome this drawback. Among other things, we introduceand analyze several preconditioners, derive new convergence results, andpropose novel methods for accurately approximating predictive variances. Weanalyze our proposed methods theoretically and in experiments with simulatedand real-world data. In particular, we obtain a speed-up of an order ofmagnitude compared to Cholesky-based calculations and a threefold increase inprediction accuracy in terms of the continuous ranked probability scorecompared to a state-of-the-art method on a large satellite data set. Allmethods are implemented in a free C++ software library with high-level Pythonand R packages.</description><author>Pascal Kündig, Fabio Sigrist</author><pubDate>Thu, 29 Aug 2024 16:40:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12000v2</guid></item><item><title>Methods for Recovering Conditional Independence Graphs: A Survey</title><link>http://arxiv.org/abs/2211.06829v3</link><description>Conditional Independence (CI) graphs are a type of probabilistic graphicalmodels that are primarily used to gain insights about feature relationships.Each edge represents the partial correlation between the connected featureswhich gives information about their direct dependence. In this survey, we listout different methods and study the advances in techniques developed to recoverCI graphs. We cover traditional optimization methods as well as recentlydeveloped deep learning architectures along with their recommendedimplementations. To facilitate wider adoption, we include preliminaries thatconsolidate associated operations, for example techniques to obtain covariancematrix for mixed datatypes.</description><author>Harsh Shrivastava, Urszula Chajewska</author><pubDate>Thu, 29 Aug 2024 16:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.06829v3</guid></item><item><title>Generic Objects as Pose Probes for Few-Shot View Synthesis</title><link>http://arxiv.org/abs/2408.16690v1</link><description>Radiance fields including NeRFs and 3D Gaussians demonstrate great potentialin high-fidelity rendering and scene reconstruction, while they require asubstantial number of posed images as inputs. COLMAP is frequently employed forpreprocessing to estimate poses, while it necessitates a large number offeature matches to operate effectively, and it struggles with scenescharacterized by sparse features, large baselines between images, or a limitednumber of input images. We aim to tackle few-view NeRF reconstruction usingonly 3 to 6 unposed scene images. Traditional methods often use calibrationboards but they are not common in images. We propose a novel idea of utilizingeveryday objects, commonly found in both images and real life, as "poseprobes". The probe object is automatically segmented by SAM, whose shape isinitialized from a cube. We apply a dual-branch volume rendering optimization(object NeRF and scene NeRF) to constrain the pose optimization and jointlyrefine the geometry. Specifically, object poses of two views are firstestimated by PnP matching in an SDF representation, which serves as initialposes. PnP matching, requiring only a few features, is suitable forfeature-sparse scenes. Additional views are incrementally incorporated torefine poses from preceding views. In experiments, PoseProbe achievesstate-of-the-art performance in both pose estimation and novel view synthesisacross multiple datasets. We demonstrate its effectiveness, particularly infew-view and large-baseline scenes where COLMAP struggles. In ablations, usingdifferent objects in a scene yields comparable performance.</description><author>Zhirui Gao, Renjiao Yi, Chenyang Zhu, Ke Zhuang, Wei Chen, Kai Xu</author><pubDate>Thu, 29 Aug 2024 16:37:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16690v1</guid></item><item><title>CW-CNN &amp; CW-AN: Convolutional Networks and Attention Networks for CW-Complexes</title><link>http://arxiv.org/abs/2408.16686v1</link><description>We present a novel framework for learning on CW-complex structured datapoints. Recent advances have discussed CW-complexes as ideal learningrepresentations for problems in cheminformatics. However, there is a lack ofavailable machine learning methods suitable for learning on CW-complexes. Inthis paper we develop notions of convolution and attention that are welldefined for CW-complexes. These notions enable us to create the first neuralnetwork that can receive a CW-complex as input. We illustrate and interpretthis framework in the context of supervised prediction.</description><author>Rahul Khorana</author><pubDate>Thu, 29 Aug 2024 16:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16686v1</guid></item><item><title>PartFormer: Awakening Latent Diverse Representation from Vision Transformer for Object Re-Identification</title><link>http://arxiv.org/abs/2408.16684v1</link><description>Extracting robust feature representation is critical for objectre-identification to accurately identify objects across non-overlappingcameras. Although having a strong representation ability, the VisionTransformer (ViT) tends to overfit on most distinct regions of training data,limiting its generalizability and attention to holistic object features.Meanwhile, due to the structural difference between CNN and ViT, fine-grainedstrategies that effectively address this issue in CNN do not continue to besuccessful in ViT. To address this issue, by observing the latent diverserepresentation hidden behind the multi-head attention, we present PartFormer,an innovative adaptation of ViT designed to overcome the granularitylimitations in object Re-ID tasks. The PartFormer integrates a HeadDisentangling Block (HDB) that awakens the diverse representation of multi-headself-attention without the typical loss of feature richness induced byconcatenation and FFN layers post-attention. To avoid the homogenization ofattention heads and promote robust part-based feature learning, two headdiversity constraints are imposed: attention diversity constraint andcorrelation diversity constraint. These constraints enable the model to exploitdiverse and discriminative feature representations from different attentionheads. Comprehensive experiments on various object Re-ID benchmarks demonstratethe superiority of the PartFormer. Specifically, our framework significantlyoutperforms state-of-the-art by 2.4\% mAP scores on the most challenging MSMT17dataset.</description><author>Lei Tan, Pingyang Dai, Jie Chen, Liujuan Cao, Yongjian Wu, Rongrong Ji</author><pubDate>Thu, 29 Aug 2024 16:31:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16684v1</guid></item><item><title>A Catalog of Fairness-Aware Practices in Machine Learning Engineering</title><link>http://arxiv.org/abs/2408.16683v1</link><description>Machine learning's widespread adoption in decision-making processes raisesconcerns about fairness, particularly regarding the treatment of sensitivefeatures and potential discrimination against minorities. The softwareengineering community has responded by developing fairness-oriented metrics,empirical studies, and approaches. However, there remains a gap inunderstanding and categorizing practices for engineering fairness throughoutthe machine learning lifecycle. This paper presents a novel catalog ofpractices for addressing fairness in machine learning derived from a systematicmapping study. The study identifies and categorizes 28 practices from existingliterature, mapping them onto different stages of the machine learninglifecycle. From this catalog, the authors extract actionable items andimplications for both researchers and practitioners in software engineering.This work aims to provide a comprehensive resource for integrating fairnessconsiderations into the development and deployment of machine learning systems,enhancing their reliability, accountability, and credibility.</description><author>Gianmario Voria, Giulia Sellitto, Carmine Ferrara, Francesco Abate, Andrea De Lucia, Filomena Ferrucci, Gemma Catolino, Fabio Palomba</author><pubDate>Thu, 29 Aug 2024 16:28:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16683v1</guid></item><item><title>Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity</title><link>http://arxiv.org/abs/2408.16673v1</link><description>Large language models rely on Supervised Fine-Tuning (SFT) to specialize indownstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but itoften leads to overfitting and limited output diversity due to its aggressiveupdates to the data distribution. This paper aim to address these issues byintroducing the maximum entropy principle, which favors models with flatterdistributions that still effectively capture the data. Specifically, we developa new distribution matching method called GEM, which solves reverseKullback-Leibler divergence minimization with an entropy regularizer. For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects.First, when applied to the UltraFeedback dataset to develop generalinstruction-following abilities, GEM exhibits reduced overfitting, evidenced bylower perplexity and better performance on the IFEval benchmark. Furthermore,GEM enhances output diversity, leading to performance gains of up to 7 pointson math reasoning and code generation tasks using best-of-n sampling, evenwithout domain-specific data. Second, when fine-tuning with domain-specificdatasets for math reasoning and code generation, GEM also shows lessoverfitting and improvements of up to 10 points compared with CE.</description><author>Ziniu Li, Congliang Chen, Tian Xu, Zeyu Qin, Jiancong Xiao, Ruoyu Sun, Zhi-Quan Luo</author><pubDate>Thu, 29 Aug 2024 16:21:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16673v1</guid></item><item><title>Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever</title><link>http://arxiv.org/abs/2408.16672v1</link><description>Multi-vector dense models, such as ColBERT, have proven highly effective ininformation retrieval. ColBERT's late interaction scoring approximates thejoint query-document attention seen in cross-encoders while maintaininginference efficiency closer to traditional dense retrieval models, thanks toits bi-encoder architecture and recent optimizations in indexing and search. Inthis paper, we introduce several improvements to the ColBERT model architectureand training pipeline, leveraging techniques successful in the more establishedsingle-vector embedding model paradigm, particularly those suited forheterogeneous multilingual data. Our new model, Jina-ColBERT-v2, demonstratesstrong performance across a range of English and multilingual retrieval tasks,while also cutting storage requirements by up to 50% compared to previousmodels.</description><author>Rohan Jha, Bo Wang, Michael Günther, Saba Sturua, Mohammad Kalim Akram, Han Xiao</author><pubDate>Thu, 29 Aug 2024 16:21:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16672v1</guid></item><item><title>Iterative Graph Alignment</title><link>http://arxiv.org/abs/2408.16667v1</link><description>By compressing diverse narratives, LLMs go beyond memorization, achievingintelligence by capturing generalizable causal relationships. However, theysuffer from local 'representation gaps' due to insufficient training datadiversity, limiting their real-world utility, especially in tasks requiringstrict alignment to rules. Traditional alignment methods relying on heavy humanannotations are inefficient and unscalable. Recent self-alignment techniquesalso fall short, as they often depend on self-selection based prompting andmemorization-based learning. To address these issues, we introduce IterativeGraph Alignment (IGA), an annotation-free rule-based alignment algorithm. Ateacher model (VLM) employs Iterative Graph Prompting (IGP) to create logicalgraphs and reference answers. The student model (LLM) identifies localknowledge gaps by attempting to align its responses with these references,collaborating with helper models to generate diverse answers. These alignedresponses are then used for iterative supervised fine-tuning (SFT). Ourevaluations across five rule-based scenarios demonstrate IGP's effectiveness,with a 73.12\% alignment improvement in Claude Sonnet 3.5, andLlama3-8B-Instruct achieving an 86.20\% improvement, outperforming ClaudeSonnet 3.5 in rule-based alignment.</description><author>Fangyuan Yu, Hardeep Singh Arora, Matt Johnson</author><pubDate>Thu, 29 Aug 2024 16:15:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16667v1</guid></item><item><title>Manipulate-Anything: Automating Real-World Robots using Vision-Language Models</title><link>http://arxiv.org/abs/2406.18915v3</link><description>Large-scale endeavors like and widespread community efforts such asOpen-X-Embodiment have contributed to growing the scale of robot demonstrationdata. However, there is still an opportunity to improve the quality, quantity,and diversity of robot demonstration data. Although vision-language models havebeen shown to automatically generate demonstration data, their utility has beenlimited to environments with privileged state information, they requirehand-designed skills, and are limited to interactions with few objectinstances. We propose Manipulate-Anything, a scalable automated generationmethod for real-world robotic manipulation. Unlike prior work, our method canoperate in real-world environments without any privileged state information,hand-designed skills, and can manipulate any static object. We evaluate ourmethod using two setups. First, Manipulate-Anything successfully generatestrajectories for all 7 real-world and 14 simulation tasks, significantlyoutperforming existing methods like VoxPoser. Second, Manipulate-Anything'sdemonstrations can train more robust behavior cloning policies than trainingwith human demonstrations, or from data generated by VoxPoser, Scaling-up, andCode-As-Policies. We believe Manipulate-Anything can be a scalable method forboth generating data for robotics and solving novel tasks in a zero-shotsetting. Project page: https://robot-ma.github.io/.</description><author>Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, Ranjay Krishna</author><pubDate>Thu, 29 Aug 2024 16:07:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18915v3</guid></item><item><title>Space3D-Bench: Spatial 3D Question Answering Benchmark</title><link>http://arxiv.org/abs/2408.16662v1</link><description>Answering questions about the spatial properties of the environment poseschallenges for existing language and vision foundation models due to a lack ofunderstanding of the 3D world notably in terms of relationships betweenobjects. To push the field forward, multiple 3D Q&amp;A datasets were proposedwhich, overall, provide a variety of questions, but they individually focus onparticular aspects of 3D reasoning or are limited in terms of data modalities.To address this, we present Space3D-Bench - a collection of 1000 generalspatial questions and answers related to scenes of the Replica dataset whichoffers a variety of data modalities: point clouds, posed RGB-D images,navigation meshes and 3D object detections. To ensure that the questions covera wide range of 3D objectives, we propose an indoor spatial questions taxonomyinspired by geographic information systems and use it to balance the datasetaccordingly. Moreover, we provide an assessment system that grades naturallanguage responses based on predefined ground-truth answers by leveraging aVision Language Model's comprehension of both text and images to compare theresponses with ground-truth textual information or relevant visual data.Finally, we introduce a baseline called RAG3D-Chat integrating the worldunderstanding of foundation models with rich context retrieval, achieving anaccuracy of 67% on the proposed dataset.</description><author>Emilia Szymanska, Mihai Dusmanu, Jan-Willem Buurlage, Mahdi Rad, Marc Pollefeys</author><pubDate>Thu, 29 Aug 2024 16:05:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16662v1</guid></item><item><title>Eigen-Cluster VIS: Improving Weakly-supervised Video Instance Segmentation by Leveraging Spatio-temporal Consistency</title><link>http://arxiv.org/abs/2408.16661v1</link><description>The performance of Video Instance Segmentation (VIS) methods has improvedsignificantly with the advent of transformer networks. However, these networksoften face challenges in training due to the high annotation cost. To addressthis, unsupervised and weakly-supervised methods have been developed to reducethe dependency on annotations. This work introduces a novel weakly-supervisedmethod called Eigen-cluster VIS that, without requiring any mask annotations,achieves competitive accuracy compared to other VIS approaches. This method isbased on two key innovations: a Temporal Eigenvalue Loss (TEL) and a clip-levelQuality Cluster Coefficient (QCC). The TEL ensures temporal coherence byleveraging the eigenvalues of the Laplacian matrix derived from graph adjacencymatrices. By minimizing the mean absolute error (MAE) between the eigenvaluesof adjacent frames, this loss function promotes smooth transitions and stablesegmentation boundaries over time, reducing temporal discontinuities andimproving overall segmentation quality. The QCC employs the K-means method toensure the quality of spatio-temporal clusters without relying on ground truthmasks. Using the Davies-Bouldin score, the QCC provides an unsupervised measureof feature discrimination, allowing the model to self-evaluate and adapt tovarying object distributions, enhancing robustness during the testing phase.These enhancements are computationally efficient and straightforward, offeringsignificant performance gains without additional annotated data. The proposedEigen-Cluster VIS method is evaluated on the YouTube-VIS 2019/2021 and OVISdatasets, demonstrating that it effectively narrows the performance gap betweenthe fully-supervised and weakly-supervised VIS approaches. The code isavailable on: https://github.com/farnooshar/EigenClusterVIS</description><author>Farnoosh Arefi, Amir M. Mansourian, Shohreh Kasaei</author><pubDate>Thu, 29 Aug 2024 16:05:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16661v1</guid></item><item><title>Post-processing fairness with minimal changes</title><link>http://arxiv.org/abs/2408.15096v2</link><description>In this paper, we introduce a novel post-processing algorithm that is bothmodel-agnostic and does not require the sensitive attribute at test time. Inaddition, our algorithm is explicitly designed to enforce minimal changesbetween biased and debiased predictions; a property that, while highlydesirable, is rarely prioritized as an explicit objective in fairnessliterature. Our approach leverages a multiplicative factor applied to the logitvalue of probability scores produced by a black-box classifier. We demonstratethe efficacy of our method through empirical evaluations, comparing itsperformance against other four debiasing algorithms on two widely used datasetsin fairness research.</description><author>Federico Di Gennaro, Thibault Laugel, Vincent Grari, Xavier Renard, Marcin Detyniecki</author><pubDate>Thu, 29 Aug 2024 15:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15096v2</guid></item><item><title>Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition</title><link>http://arxiv.org/abs/2407.04559v2</link><description>Visual storytelling consists in generating a natural language story given atemporally ordered sequence of images. This task is not only challenging formodels, but also very difficult to evaluate with automatic metrics since thereis no consensus about what makes a story 'good'. In this paper, we introduce anovel method that measures story quality in terms of human likeness regardingthree key aspects highlighted in previous work: visual grounding, coherence,and repetitiveness. We then use this method to evaluate the stories generatedby several models, showing that the foundation model LLaVA obtains the bestresult, but only slightly so compared to TAPM, a 50-times smaller visualstorytelling model. Upgrading the visual and language components of TAPMresults in a model that yields competitive performance with a relatively lownumber of parameters. Finally, we carry out a human evaluation study, whoseresults suggest that a 'good' story may require more than a human-like level ofvisual grounding, coherence, and repetition.</description><author>Aditya K Surikuchi, Raquel Fernández, Sandro Pezzelle</author><pubDate>Thu, 29 Aug 2024 15:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04559v2</guid></item><item><title>Optimal Parallelization of Boosting</title><link>http://arxiv.org/abs/2408.16653v1</link><description>Recent works on the parallel complexity of Boosting have established stronglower bounds on the tradeoff between the number of training rounds $p$ and thetotal parallel work per round $t$. These works have also presented highlynon-trivial parallel algorithms that shed light on different regions of thistradeoff. Despite these advancements, a significant gap persists between thetheoretical lower bounds and the performance of these algorithms across much ofthe tradeoff space. In this work, we essentially close this gap by providingboth improved lower bounds on the parallel complexity of weak-to-stronglearners, and a parallel Boosting algorithm whose performance matches thesebounds across the entire $p$ vs.~$t$ compromise spectrum, up to logarithmicfactors. Ultimately, this work settles the true parallel complexity of Boostingalgorithms that are nearly sample-optimal.</description><author>Arthur da Cunha, Mikael Møller Høgsgaard, Kasper Green Larsen</author><pubDate>Thu, 29 Aug 2024 15:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16653v1</guid></item><item><title>Towards Efficient Modelling of String Dynamics: A Comparison of State Space and Koopman based Deep Learning Methods</title><link>http://arxiv.org/abs/2408.16650v1</link><description>This paper presents an examination of State Space Models (SSM) andKoopman-based deep learning methods for modelling the dynamics of both linearand non-linear stiff strings. Through experiments with datasets generated underdifferent initial conditions and sample rates, we assess the capacity of thesemodels to accurately model the complex behaviours observed in string dynamics.Our findings indicate that our proposed Koopman-based model performs as well asor better than other existing approaches in non-linear cases for long-sequencemodelling. We inform the design of these architectures with the structure of theproblems at hand. Although challenges remain in extending model predictionsbeyond the training horizon (i.e., extrapolation), the focus of ourinvestigation lies in the models' ability to generalise across differentinitial conditions within the training time interval. This research contributesinsights into the physical modelling of dynamical systems (in particular thoseaddressing musical acoustics) by offering a comparative overview of these andprevious methods and introducing innovative strategies for model improvement.Our results highlight the efficacy of these models in simulating non-lineardynamics and emphasise their wide-ranging applicability in accurately modellingdynamical systems over extended sequences.</description><author>Rodrigo Diaz, Carlos De La Vega Martin, Mark Sandler</author><pubDate>Thu, 29 Aug 2024 15:55:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16650v1</guid></item><item><title>Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble Approach Using Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2408.15886v2</link><description>In recent years, the evolution of machine learning techniques hassignificantly impacted the field of intrusion detection, particularly withinthe context of the Internet of Things (IoT). As IoT networks expand, the needfor robust security measures to counteract potential threats has becomeincreasingly critical. This paper introduces a hybrid Intrusion DetectionSystem (IDS) that synergistically combines Kolmogorov-Arnold Networks (KANs)with the XGBoost algorithm. Our proposed IDS leverages the unique capabilitiesof KANs, which utilize learnable activation functions to model complexrelationships within data, alongside the powerful ensemble learning techniquesof XGBoost, known for its high performance in classification tasks. This hybridapproach not only enhances the detection accuracy but also improves theinterpretability of the model, making it suitable for dynamic and intricate IoTenvironments. Experimental evaluations demonstrate that our hybrid IDS achievesan impressive detection accuracy exceeding 99% in distinguishing between benignand malicious activities. Additionally, we were able to achieve F1 scores,precision, and recall that exceeded 98%. Furthermore, we conduct a comparativeanalysis against traditional Multi-Layer Perceptron (MLP) networks, assessingperformance metrics such as Precision, Recall, and F1-score. The resultsunderscore the efficacy of integrating KANs with XGBoost, highlighting thepotential of this innovative approach to significantly strengthen the securityframework of IoT networks.</description><author>Amar Amouri, Mohamad Mahmoud Al Rahhal, Yakoub Bazi, Ismail Butun, Imad Mahgoub</author><pubDate>Thu, 29 Aug 2024 15:54:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15886v2</guid></item><item><title>Gameplay Filters: Robust Zero-Shot Safety through Adversarial Imagination</title><link>http://arxiv.org/abs/2405.00846v3</link><description>Despite the impressive recent advances in learning-based robot control,ensuring robustness to out-of-distribution conditions remains an openchallenge. Safety filters can, in principle, keep arbitrary control policiesfrom incurring catastrophic failures by overriding unsafe actions, but existingsolutions for complex (e.g., legged) robot dynamics do not span the full motionenvelope and instead rely on local, reduced-order models. These filters tend tooverly restrict agility and can still fail when perturbed away from nominalconditions. This paper presents the gameplay filter, a new class of predictivesafety filter that continually plays out hypothetical matches between itssimulation-trained safety strategy and a virtual adversary co-trained to invokeworst-case events and sim-to-real error, and precludes actions that would causeit to fail down the line. We demonstrate the scalability and robustness of theapproach with a first-of-its-kind full-order safety filter for (36-D)quadrupedal dynamics. Physical experiments on two different quadruped platformsdemonstrate the superior zero-shot effectiveness of the gameplay filter underlarge perturbations such as tugging and unmodeled terrain.</description><author>Duy P. Nguyen, Kai-Chieh Hsu, Wenhao Yu, Jie Tan, Jaime F. Fisac</author><pubDate>Thu, 29 Aug 2024 15:53:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00846v3</guid></item><item><title>DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving</title><link>http://arxiv.org/abs/2408.16647v1</link><description>The advancement of autonomous driving technologies necessitates increasinglysophisticated methods for understanding and predicting real-world scenarios.Vision language models (VLMs) are emerging as revolutionary tools withsignificant potential to influence autonomous driving. In this paper, wepropose the DriveGenVLM framework to generate driving videos and use VLMs tounderstand them. To achieve this, we employ a video generation frameworkgrounded in denoising diffusion probabilistic models (DDPM) aimed at predictingreal-world video sequences. We then explore the adequacy of our generatedvideos for use in VLMs by employing a pre-trained model known as EfficientIn-context Learning on Egocentric Videos (EILEV). The diffusion model istrained with the Waymo open dataset and evaluated using the Fr\'echet VideoDistance (FVD) score to ensure the quality and realism of the generated videos.Corresponding narrations are provided by EILEV for these generated videos,which may be beneficial in the autonomous driving domain. These narrations canenhance traffic scene understanding, aid in navigation, and improve planningcapabilities. The integration of video generation with VLMs in the DriveGenVLMframework represents a significant step forward in leveraging advanced AImodels to address complex challenges in autonomous driving.</description><author>Yongjie Fu, Anmol Jain, Xuan Di, Xu Chen, Zhaobin Mo</author><pubDate>Thu, 29 Aug 2024 15:52:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16647v1</guid></item><item><title>SODAWideNet++: Combining Attention and Convolutions for Salient Object Detection</title><link>http://arxiv.org/abs/2408.16645v1</link><description>Salient Object Detection (SOD) has traditionally relied on feature refinementmodules that utilize the features of an ImageNet pre-trained backbone. However,this approach limits the possibility of pre-training the entire network becauseof the distinct nature of SOD and image classification. Additionally, thearchitecture of these backbones originally built for Image classification issub-optimal for a dense prediction task like SOD. To address these issues, wepropose a novel encoder-decoder-style neural network called SODAWideNet++ thatis designed explicitly for SOD. Inspired by the vision transformers ability toattain a global receptive field from the initial stages, we introduce theAttention Guided Long Range Feature Extraction (AGLRFE) module, which combineslarge dilated convolutions and self-attention. Specifically, we use attentionfeatures to guide long-range information extracted by multiple dilatedconvolutions, thus taking advantage of the inductive biases of a convolutionoperation and the input dependency brought by self-attention. In contrast tothe current paradigm of ImageNet pre-training, we modify 118K annotated imagesfrom the COCO semantic segmentation dataset by binarizing the annotations topre-train the proposed model end-to-end. Further, we supervise the backgroundpredictions along with the foreground to push our model to generate accuratesaliency predictions. SODAWideNet++ performs competitively on five differentdatasets while only containing 35% of the trainable parameters compared to thestate-of-the-art models. The code and pre-computed saliency maps are providedat https://github.com/VimsLab/SODAWideNetPlusPlus.</description><author>Rohit Venkata Sai Dulam, Chandra Kambhamettu</author><pubDate>Thu, 29 Aug 2024 15:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16645v1</guid></item><item><title>3D Pose-Based Temporal Action Segmentation for Figure Skating: A Fine-Grained and Jump Procedure-Aware Annotation Approach</title><link>http://arxiv.org/abs/2408.16638v1</link><description>Understanding human actions from videos is essential in many domains,including sports. In figure skating, technical judgments are performed bywatching skaters' 3D movements, and its part of the judging procedure can beregarded as a Temporal Action Segmentation (TAS) task. TAS tasks in figureskating that automatically assign temporal semantics to video are activelyresearched. However, there is a lack of datasets and effective methods for TAStasks requiring 3D pose data. In this study, we first created the FS-Jump3Ddataset of complex and dynamic figure skating jumps using optical markerlessmotion capture. We also propose a new fine-grained figure skating jump TASdataset annotation method with which TAS models can learn jump procedures. Inthe experimental results, we validated the usefulness of 3D pose features asinput and the fine-grained dataset for the TAS model in figure skating.FS-Jump3D Dataset is available at https://github.com/ryota-skating/FS-Jump3D.</description><author>Ryota Tanaka, Tomohiro Suzuki, Keisuke Fujii</author><pubDate>Thu, 29 Aug 2024 15:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16638v1</guid></item><item><title>Generalization of Hamiltonian algorithms</title><link>http://arxiv.org/abs/2405.14469v2</link><description>The paper proves generalization results for a class of stochastic learningalgorithms. The method applies whenever the algorithm generates an absolutelycontinuous distribution relative to some a-priori measure and the Radon Nikodymderivative has subgaussian concentration. Applications are bounds for the Gibbsalgorithm and randomizations of stable deterministic algorithms as well asPAC-Bayesian bounds with data-dependent priors.</description><author>Andreas Maurer</author><pubDate>Thu, 29 Aug 2024 15:41:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14469v2</guid></item><item><title>RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model</title><link>http://arxiv.org/abs/2408.16634v1</link><description>The increasing sophistication of text-to-image generative models has led tocomplex challenges in defining and enforcing copyright infringement criteriaand protection. Existing methods, such as watermarking and datasetdeduplication, fail to provide comprehensive solutions due to the lack ofstandardized metrics and the inherent complexity of addressing copyrightinfringement in diffusion models. To deal with these challenges, we propose aReinforcement Learning-based Copyright Protection(RLCP) method forText-to-Image Diffusion Model, which minimizes the generation ofcopyright-infringing content while maintaining the quality of themodel-generated dataset. Our approach begins with the introduction of a novelcopyright metric grounded in copyright law and court precedents oninfringement. We then utilize the Denoising Diffusion Policy Optimization(DDPO) framework to guide the model through a multi-step decision-makingprocess, optimizing it using a reward function that incorporates our proposedcopyright metric. Additionally, we employ KL divergence as a regularizationterm to mitigate some failure modes and stabilize RL fine-tuning. Experimentsconducted on 3 mixed datasets of copyright and non-copyright images demonstratethat our approach significantly reduces copyright infringement risk whilemaintaining image quality.</description><author>Zhuan Shi, Jing Yan, Xiaoli Tang, Lingjuan Lyu, Boi Faltings</author><pubDate>Thu, 29 Aug 2024 15:39:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16634v1</guid></item><item><title>Optimizing Automated Picking Systems in Warehouse Robots Using Machine Learning</title><link>http://arxiv.org/abs/2408.16633v1</link><description>With the rapid growth of global e-commerce, the demand for automation in thelogistics industry is increasing. This study focuses on automated pickingsystems in warehouses, utilizing deep learning and reinforcement learningtechnologies to enhance picking efficiency and accuracy while reducing systemfailure rates. Through empirical analysis, we demonstrate the effectiveness ofthese technologies in improving robot picking performance and adaptability tocomplex environments. The results show that the integrated machine learningmodel significantly outperforms traditional methods, effectively addressing thechallenges of peak order processing, reducing operational errors, and improvingoverall logistics efficiency. Additionally, by analyzing environmental factors,this study further optimizes system design to ensure efficient and stableoperation under variable conditions. This research not only provides innovativesolutions for logistics automation but also offers a theoretical and empiricalfoundation for future technological development and application.</description><author>Keqin Li, Jin Wang, Xubo Wu, Xirui Peng, Runmian Chang, Xiaoyu Deng, Yiwen Kang, Yue Yang, Fanghao Ni, Bo Hong</author><pubDate>Thu, 29 Aug 2024 15:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16633v1</guid></item><item><title>Maelstrom Networks</title><link>http://arxiv.org/abs/2408.16632v1</link><description>Artificial Neural Networks has struggled to devise a way to incorporateworking memory into neural networks. While the ``long term'' memory can be seenas the learned weights, the working memory consists likely more of dynamicalactivity, that is missing from feed-forward models. Current state of the artmodels such as transformers tend to ``solve'' this by ignoring working memoryentirely and simply process the sequence as an entire piece of data; howeverthis means the network cannot process the sequence in an online fashion, andleads to an immense explosion in memory requirements. Here, inspired by acombination of controls, reservoir computing, deep learning, and recurrentneural networks, we offer an alternative paradigm that combines the strength ofrecurrent networks, with the pattern matching capability of feed-forward neuralnetworks, which we call the \textit{Maelstrom Networks} paradigm. This paradigmleaves the recurrent component - the \textit{Maelstrom} - unlearned, andoffloads the learning to a powerful feed-forward network. This allows thenetwork to leverage the strength of feed-forward training without unrolling thenetwork, and allows for the memory to be implemented in new neuromorphichardware. It endows a neural network with a sequential memory that takesadvantage of the inductive bias that data is organized causally in the temporaldomain, and imbues the network with a state that represents the agent's``self'', moving through the environment. This could also lead the way tocontinual learning, with the network modularized and ``'protected'' fromoverwrites that come with new data. In addition to aiding in solving theseperformance problems that plague current non-temporal deep networks, this alsocould finally lead towards endowing artificial networks with a sense of``self''.</description><author>Matthew Evanusa, Cornelia Fermüller, Yiannis Aloimonos</author><pubDate>Thu, 29 Aug 2024 15:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16632v1</guid></item><item><title>LLMs generate structurally realistic social networks but overestimate political homophily</title><link>http://arxiv.org/abs/2408.16629v1</link><description>Generating social networks is essential for many applications, such asepidemic modeling and social simulations. Prior approaches either involve deeplearning models, which require many observed networks for training, or stylizedmodels, which are limited in their realism and flexibility. In contrast, LLMsoffer the potential for zero-shot and flexible network generation. However, twokey questions are: (1) are LLM's generated networks realistic, and (2) what arerisks of bias, given the importance of demographics in forming social ties? Toanswer these questions, we develop three prompting methods for networkgeneration and compare the generated networks to real social networks. We findthat more realistic networks are generated with "local" methods, where the LLMconstructs relations for one persona at a time, compared to "global" methodsthat construct the entire network at once. We also find that the generatednetworks match real networks on many characteristics, including density,clustering, community structure, and degree. However, we find that LLMsemphasize political homophily over all other types of homophily andoverestimate political homophily relative to real-world measures.</description><author>Serina Chang, Alicja Chaszczewicz, Emma Wang, Maya Josifovska, Emma Pierson, Jure Leskovec</author><pubDate>Thu, 29 Aug 2024 15:36:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16629v1</guid></item><item><title>Trajectory Forecasting through Low-Rank Adaptation of Discrete Latent Codes</title><link>http://arxiv.org/abs/2405.20743v2</link><description>Trajectory forecasting is crucial for video surveillance analytics, as itenables the anticipation of future movements for a set of agents, e.g.basketball players engaged in intricate interactions with long-term intentions.Deep generative models offer a natural learning approach for trajectoryforecasting, yet they encounter difficulties in achieving an optimal balancebetween sampling fidelity and diversity. We address this challenge byleveraging Vector Quantized Variational Autoencoders (VQ-VAEs), which utilize adiscrete latent space to tackle the issue of posterior collapse. Specifically,we introduce an instance-based codebook that allows tailored latentrepresentations for each example. In a nutshell, the rows of the codebook aredynamically adjusted to reflect contextual information (i.e., past motionpatterns extracted from the observed trajectories). In this way, thediscretization process gains flexibility, leading to improved reconstructions.Notably, instance-level dynamics are injected into the codebook throughlow-rank updates, which restrict the customization of the codebook to a lowerdimension space. The resulting discrete space serves as the basis of thesubsequent step, which regards the training of a diffusion-based predictivemodel. We show that such a two-fold framework, augmented with instance-leveldiscretization, leads to accurate and diverse forecasts, yieldingstate-of-the-art performance on three established benchmarks.</description><author>Riccardo Benaglia, Angelo Porrello, Pietro Buzzega, Simone Calderara, Rita Cucchiara</author><pubDate>Thu, 29 Aug 2024 15:31:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20743v2</guid></item><item><title>Turbulence Strength $C_n^2$ Estimation from Video using Physics-based Deep Learning</title><link>http://arxiv.org/abs/2408.16623v1</link><description>Images captured from a long distance suffer from dynamic image distortion dueto turbulent flow of air cells with random temperatures, and thus refractiveindices. This phenomenon, known as image dancing, is commonly characterized byits refractive-index structure constant $C_n^2$ as a measure of the turbulencestrength. For many applications such as atmospheric forecast model,long-range/astronomy imaging, and aviation safety, optical communicationtechnology, $C_n^2$ estimation is critical for accurately sensing the turbulentenvironment. Previous methods for $C_n^2$ estimation include estimation frommeteorological data (temperature, relative humidity, wind shear, etc.) forsingle-point measurements, two-ended pathlength measurements from opticalscintillometer for path-averaged $C_n^2$, and more recently estimating $C_n^2$from passive video cameras for low cost and hardware complexity. In this paper,we present a comparative analysis of classical image gradient methods for$C_n^2$ estimation and modern deep learning-based methods leveragingconvolutional neural networks. To enable this, we collect a dataset of videocapture along with reference scintillometer measurements for ground truth, andwe release this unique dataset to the scientific community. We observe thatdeep learning methods can achieve higher accuracy when trained on similar data,but suffer from generalization errors to other, unseen imagery as compared toclassical methods. To overcome this trade-off, we present a novel physics-basednetwork architecture that combines learned convolutional layers with adifferentiable image gradient method that maintains high accuracy while beinggeneralizable across image datasets.</description><author>Ripon Kumar Saha, Esen Salcin, Jihoo Kim, Joseph Smith, Suren Jayasuriya</author><pubDate>Thu, 29 Aug 2024 15:31:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16623v1</guid></item><item><title>Sparse Signal Reconstruction for Overdispersed Low-photon Count Biomedical Imaging Using $\ell_p$ Total Variation</title><link>http://arxiv.org/abs/2408.16622v1</link><description>The negative binomial model, which generalizes the Poisson distributionmodel, can be found in applications involving low-photon signal recovery,including medical imaging. Recent studies have explored several regularizationterms for the negative binomial model, such as the $\ell_p$ quasi-norm with $0&lt; p &lt; 1$, $\ell_1$ norm, and the total variation (TV) quasi-seminorm forpromoting sparsity in signal recovery. These penalty terms have been shown toimprove image reconstruction outcomes. In this paper, we investigate the$\ell_p$ quasi-seminorm, both isotropic and anisotropic $\ell_p$ TVquasi-seminorms, within the framework of the negative binomial statisticalmodel. This problem can be formulated as an optimization problem, which wesolve using a gradient-based approach. We present comparisons between thenegative binomial and Poisson statistical models using the $\ell_p$ TVquasi-seminorm as well as common penalty terms. Our experimental resultshighlight the efficacy of the proposed method.</description><author>Yu Lu, Roummel F. Marcia</author><pubDate>Thu, 29 Aug 2024 15:31:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16622v1</guid></item><item><title>Verification of Geometric Robustness of Neural Networks via Piecewise Linear Approximation and Lipschitz Optimisation</title><link>http://arxiv.org/abs/2408.13140v2</link><description>We address the problem of verifying neural networks against geometrictransformations of the input image, including rotation, scaling, shearing, andtranslation. The proposed method computes provably sound piecewise linearconstraints for the pixel values by using sampling and linear approximations incombination with branch-and-bound Lipschitz optimisation. The method obtainsprovably tighter over-approximations of the perturbation region than thepresent state-of-the-art. We report results from experiments on a comprehensiveset of verification benchmarks on MNIST and CIFAR10. We show that our proposedimplementation resolves up to 32% more verification cases than presentapproaches.</description><author>Ben Batten, Yang Zheng, Alessandro De Palma, Panagiotis Kouvaros, Alessio Lomuscio</author><pubDate>Thu, 29 Aug 2024 15:31:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13140v2</guid></item><item><title>Towards Infusing Auxiliary Knowledge for Distracted Driver Detection</title><link>http://arxiv.org/abs/2408.16621v1</link><description>Distracted driving is a leading cause of road accidents globally.Identification of distracted driving involves reliably detecting andclassifying various forms of driver distraction (e.g., texting, eating, orusing in-car devices) from in-vehicle camera feeds to enhance road safety. Thistask is challenging due to the need for robust models that can generalize to adiverse set of driver behaviors without requiring extensive annotated datasets.In this paper, we propose KiD3, a novel method for distracted driver detection(DDD) by infusing auxiliary knowledge about semantic relations between entitiesin a scene and the structural configuration of the driver's pose. Specifically,we construct a unified framework that integrates the scene graphs, and driverpose information with the visual cues in video frames to create a holisticrepresentation of the driver's actions.Our results indicate that KiD3 achievesa 13.64% accuracy improvement over the vision-only baseline by incorporatingsuch auxiliary knowledge with visual information.</description><author>Ishwar B Balappanawar, Ashmit Chamoli, Ruwan Wickramarachchi, Aditya Mishra, Ponnurangam Kumaraguru, Amit P. Sheth</author><pubDate>Thu, 29 Aug 2024 15:28:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16621v1</guid></item><item><title>Hyperdimensional Vector Tsetlin Machines with Applications to Sequence Learning and Generation</title><link>http://arxiv.org/abs/2408.16620v1</link><description>We construct a two-layered model for learning and generating sequential datathat is both computationally fast and competitive with vanilla Tsetlinmachines, adding numerous advantages. Through the use of hyperdimensionalvector computing (HVC) algebras and Tsetlin machine clause structures, wedemonstrate that the combination of both inherits the generality of dataencoding and decoding of HVC with the fast interpretable nature of Tsetlinmachines to yield a powerful machine learning model. We apply the approach intwo areas, namely in forecasting, generating new sequences, and classification.For the latter, we derive results for the entire UCR Time Series Archive andcompare with the standard benchmarks to see how well the method competes intime series classification.</description><author>Christian D. Blakely</author><pubDate>Thu, 29 Aug 2024 15:28:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16620v1</guid></item><item><title>Blending Low and High-Level Semantics of Time Series for Better Masked Time Series Generation</title><link>http://arxiv.org/abs/2408.16613v1</link><description>State-of-the-art approaches in time series generation (TSG), such asTimeVQVAE, utilize vector quantization-based tokenization to effectively modelcomplex distributions of time series. These approaches first learn to transformtime series into a sequence of discrete latent vectors, and then a prior modelis learned to model the sequence. The discrete latent vectors, however, onlycapture low-level semantics (\textit{e.g.,} shapes). We hypothesize thathigher-fidelity time series can be generated by training a prior model on moreinformative discrete latent vectors that contain both low and high-levelsemantics (\textit{e.g.,} characteristic dynamics). In this paper, we introducea novel framework, termed NC-VQVAE, to integrate self-supervised learning intothose TSG methods to derive a discrete latent space where low and high-levelsemantics are captured. Our experimental results demonstrate that NC-VQVAEresults in a considerable improvement in the quality of synthetic samples.</description><author>Johan Vik Mathisen, Erlend Lokna, Daesoo Lee, Erlend Aune</author><pubDate>Thu, 29 Aug 2024 15:20:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16613v1</guid></item><item><title>Data Quality Monitoring through Transfer Learning on Anomaly Detection for the Hadron Calorimeters</title><link>http://arxiv.org/abs/2408.16612v1</link><description>The proliferation of sensors brings an immense volume of spatio-temporal (ST)data in many domains for various purposes, including monitoring, diagnostics,and prognostics applications. Data curation is a time-consuming process for alarge volume of data, making it challenging and expensive to deploy dataanalytics platforms in new environments. Transfer learning (TL) mechanismspromise to mitigate data sparsity and model complexity by utilizing pre-trainedmodels for a new task. Despite the triumph of TL in fields like computer visionand natural language processing, efforts on complex ST models for anomalydetection (AD) applications are limited. In this study, we present thepotential of TL within the context of AD for the Hadron Calorimeter of theCompact Muon Solenoid experiment at CERN. We have transferred the ST AD modelstrained on data collected from one part of a calorimeter to another. We haveinvestigated different configurations of TL on semi-supervised autoencoders ofthe ST AD models -- transferring convolutional, graph, and recurrent neuralnetworks of both the encoder and decoder networks. The experiment resultsdemonstrate that TL effectively enhances the model learning accuracy on atarget subdetector. The TL achieves promising data reconstruction and ADperformance while substantially reducing the trainable parameters of the ADmodels. It also improves robustness against anomaly contamination in thetraining data sets of the semi-supervised AD models.</description><author>Mulugeta Weldezgina Asres, Christian Walter Omlin, Long Wang, Pavel Parygin, David Yu, Jay Dittmann, The CMS-HCAL Collaboration</author><pubDate>Thu, 29 Aug 2024 15:19:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16612v1</guid></item><item><title>Subspace Representation Learning for Sparse Linear Arrays to Localize More Sources than Sensors: A Deep Learning Methodology</title><link>http://arxiv.org/abs/2408.16605v1</link><description>Localizing more sources than sensors with a sparse linear array (SLA) haslong relied on minimizing a distance between two covariance matrices and recentalgorithms often utilize semidefinite programming (SDP). Although deep neuralnetwork (DNN)-based methods offer new alternatives, they still depend oncovariance matrix fitting. In this paper, we develop a novel methodology thatestimates the co-array subspaces from a sample covariance for SLAs. Ourmethodology trains a DNN to learn signal and noise subspace representationsthat are invariant to the selection of bases. To learn such representations, wepropose loss functions that gauge the separation between the desired and theestimated subspace. In particular, we propose losses that measure the length ofthe shortest path between subspaces viewed on a union of Grassmannians, andprove that it is possible for a DNN to approximate signal subspaces. Thecomputation of learning subspaces of different dimensions is accelerated by anew batch sampling strategy called consistent rank sampling. The methodology isrobust to array imperfections due to its geometry-agnostic and data-drivennature. In addition, we propose a fully end-to-end gridless approach thatdirectly learns angles to study the possibility of bypassing subspace methods.Numerical results show that learning such subspace representations is morebeneficial than learning covariances or angles. It outperforms conventionalSDP-based methods such as the sparse and parametric approach (SPA) and existingDNN-based covariance reconstruction methods for a wide range of signal-to-noiseratios (SNRs), snapshots, and source numbers for both perfect and imperfectarrays.</description><author>Kuan-Lin Chen, Bhaskar D. Rao</author><pubDate>Thu, 29 Aug 2024 15:14:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16605v1</guid></item><item><title>Smart Multi-Modal Search: Contextual Sparse and Dense Embedding Integration in Adobe Express</title><link>http://arxiv.org/abs/2408.14698v2</link><description>As user content and queries become increasingly multi-modal, the need foreffective multi-modal search systems has grown. Traditional search systemsoften rely on textual and metadata annotations for indexed images, whilemulti-modal embeddings like CLIP enable direct search using text and imageembeddings. However, embedding-based approaches face challenges in integratingcontextual features such as user locale and recency. Building a scalablemulti-modal search system requires fine-tuning several components. This paperpresents a multi-modal search architecture and a series of AB tests thatoptimize embeddings and multi-modal technologies in Adobe Express templatesearch. We address considerations such as embedding model selection, the rolesof embeddings in matching and ranking, and the balance between dense and sparseembeddings. Our iterative approach demonstrates how utilizing sparse, dense,and contextual features enhances short and long query search, significantlyreduces null rates (over 70\%), and increases click-through rates (CTR). Ourfindings provide insights into developing robust multi-modal search systems,thereby enhancing relevance for complex queries.</description><author>Cherag Aroraa, Tracy Holloway King, Jayant Kumar, Yi Lu, Sanat Sharma, Arvind Srikantan, David Uvalle, Josep Valls-Vargas, Harsha Vardhan</author><pubDate>Thu, 29 Aug 2024 15:14:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14698v2</guid></item><item><title>Examination of Code generated by Large Language Models</title><link>http://arxiv.org/abs/2408.16601v1</link><description>Large language models (LLMs), such as ChatGPT and Copilot, are transformingsoftware development by automating code generation and, arguably, enable rapidprototyping, support education, and boost productivity. Therefore, correctnessand quality of the generated code should be on par with manually written code.To assess the current state of LLMs in generating correct code of high quality,we conducted controlled experiments with ChatGPT and Copilot: we let the LLMsgenerate simple algorithms in Java and Python along with the corresponding unittests and assessed the correctness and the quality (coverage) of the generated(test) codes. We observed significant differences between the LLMs, between thelanguages, between algorithm and test codes, and over time. The present paperreports these results together with the experimental methods allowing repeatedand comparable assessments for more algorithms, languages, and LLMs over time.</description><author>Robin Beer, Alexander Feix, Tim Guttzeit, Tamara Muras, Vincent Müller, Maurice Rauscher, Florian Schäffler, Welf Löwe</author><pubDate>Thu, 29 Aug 2024 15:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16601v1</guid></item><item><title>On the Efficacy of Text-Based Input Modalities for Action Anticipation</title><link>http://arxiv.org/abs/2401.12972v3</link><description>Anticipating future actions is a highly challenging task due to the diversityand scale of potential future actions; yet, information from differentmodalities help narrow down plausible action choices. Each modality can providediverse and often complementary context for the model to learn from. Whileprevious multi-modal methods leverage information from modalities such as videoand audio, we primarily explore how text descriptions of actions and objectscan also lead to more accurate action anticipation by providing additionalcontextual cues, e.g., about the environment and its contents. We propose aMulti-modal Contrastive Anticipative Transformer (M-CAT), a video transformerarchitecture that jointly learns from multi-modal features and textdescriptions of actions and objects. We train our model in two stages, wherethe model first learns to align video clips with descriptions of futureactions, and is subsequently fine-tuned to predict future actions. Compared toexisting methods, M-CAT has the advantage of learning additional context fromtwo types of text inputs: rich descriptions of future actions duringpre-training, and, text descriptions for detected objects and actions duringmodality feature fusion. Through extensive experimental evaluation, wedemonstrate that our model outperforms previous methods on the EpicKitchensdatasets, and show that using simple text descriptions of actions and objectsaid in more effective action anticipation. In addition, we examine the impactof object and action information obtained via text, and perform extensiveablations.</description><author>Apoorva Beedu, Harish Haresamudram, Karan Samel, Irfan Essa</author><pubDate>Thu, 29 Aug 2024 15:11:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12972v3</guid></item><item><title>sEMG-Driven Physics-Informed Gated Recurrent Networks for Modeling Upper Limb Multi-Joint Movement Dynamics</title><link>http://arxiv.org/abs/2408.16599v1</link><description>Exoskeletons and rehabilitation systems offer great potential for enhancinghuman strength and recovery through advanced human-machine interfaces (HMIs)that adapt to movement dynamics. However, the real-time application ofphysics-informed neural networks (PINNs) is limited by their reliance on fixedinput lengths and surrogate models. This study introduces a novelphysics-informed Gated Recurrent Network (PiGRN) designed to predictmulti-joint torques using surface electromyography (sEMG) data. The PiGRN modelemploys a Gated Recurrent Unit (GRU) to convert time-series sEMG inputs intomulti-joint kinematics and external loads, which are then integrated into anequation of motion to ensure consistency with physical laws. Experimentalvalidation with sEMG data from five participants performing elbowflexion-extension tasks showed that the PiGRN model accurately predicted jointtorques for 10 unfamiliar movements, with RMSE values between 4.02\% and11.40\% and correlation coefficients ranging from 0.87 to 0.98. These findingshighlight the PiGRN's potential for real-time exoskeleton and rehabilitationapplications. Future research will explore more diverse datasets, improvemusculoskeletal models, and investigate unsupervised learning methods.</description><author>Rajnish Kumar, Anand Gupta, Suriya Prakash Muthukrishnan, Lalan Kumar, Sitikantha Roy</author><pubDate>Thu, 29 Aug 2024 15:09:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16599v1</guid></item><item><title>High-Dimensional Sparse Data Low-rank Representation via Accelerated Asynchronous Parallel Stochastic Gradient Descent</title><link>http://arxiv.org/abs/2408.16592v1</link><description>Data characterized by high dimensionality and sparsity are commonly used todescribe real-world node interactions. Low-rank representation (LR) can maphigh-dimensional sparse (HDS) data to low-dimensional feature spaces and infernode interactions via modeling data latent associations. Unfortunately,existing optimization algorithms for LR models are computationally inefficientand slowly convergent on large-scale datasets. To address this issue, thispaper proposes an Accelerated Asynchronous Parallel Stochastic Gradient DescentA2PSGD for High-Dimensional Sparse Data Low-rank Representation with threefold-ideas: a) establishing a lock-free scheduler to simultaneously respond toscheduling requests from multiple threads; b) introducing a greedyalgorithm-based load balancing strategy for balancing the computational loadamong threads; c) incorporating Nesterov's accelerated gradient into thelearning scheme to accelerate model convergence. Empirical studies show thatA2PSGD outperforms existing optimization algorithms for HDS data LR in bothaccuracy and training time.</description><author>Qicong Hu, Hao Wu</author><pubDate>Thu, 29 Aug 2024 14:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16592v1</guid></item><item><title>CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions</title><link>http://arxiv.org/abs/2408.16589v1</link><description>We demonstrate that carefully adjusting the tokenizer of the Whisper speechrecognition model significantly improves the precision of word-level timestampswhen applying dynamic time warping to the decoder's cross-attention scores. Wefine-tune the model to produce more verbatim speech transcriptions and employseveral techniques to increase robustness against multiple speakers andbackground noise. These adjustments achieve state-of-the-art performance onbenchmarks for verbatim speech transcription, word segmentation, and the timeddetection of filler events, and can further mitigate transcriptionhallucinations. The code is available openhttps://github.com/nyrahealth/CrisperWhisper.</description><author>Laurin Wagner, Bernhard Thallinger, Mario Zusag</author><pubDate>Thu, 29 Aug 2024 14:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16589v1</guid></item><item><title>Mitigating Exaggerated Safety in Large Language Models</title><link>http://arxiv.org/abs/2405.05418v2</link><description>As the popularity of Large Language Models (LLMs) grow, combining modelsafety with utility becomes increasingly important. The challenge is makingsure that LLMs can recognize and decline dangerous prompts without sacrificingtheir ability to be helpful. The problem of "exaggerated safety" demonstrateshow difficult this can be. To reduce excessive safety behaviours -- which wasdiscovered to be 26.1% of safe prompts being misclassified as dangerous andrefused -- we use a combination of XSTest dataset prompts as well asinteractive, contextual, and few-shot prompting to examine the decision boundsof LLMs such as Llama2, Gemma Command R+, and Phi-3. We find that few-shotprompting works best for Llama2, interactive prompting works best Gemma, andcontextual prompting works best for Command R+ and Phi-3. Using a combinationof these prompting strategies, we are able to mitigate exaggerated safetybehaviors by an overall 92.9% across all LLMs. Our work presents a multipleprompting strategies to jailbreak LLMs' decision-making processes, allowingthem to navigate the tight line between refusing unsafe prompts and remaininghelpful.</description><author>Ruchira Ray, Ruchi Bhalani</author><pubDate>Thu, 29 Aug 2024 14:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05418v2</guid></item><item><title>Enhancing Dialogue Generation in Werewolf Game Through Situation Analysis and Persuasion Strategies</title><link>http://arxiv.org/abs/2408.16586v1</link><description>Recent advancements in natural language processing, particularly with largelanguage models (LLMs) like GPT-4, have significantly enhanced dialoguesystems, enabling them to generate more natural and fluent conversations.Despite these improvements, challenges persist, such as managing continuousdialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024addresses these challenges by employing the Werewolf Game, an incompleteinformation game, to test the capabilities of LLMs in complex interactiveenvironments. This paper introduces a LLM-based Werewolf Game AI, where eachrole is supported by situation analysis to aid response generation.Additionally, for the werewolf role, various persuasion strategies, includinglogical appeal, credibility appeal, and emotional appeal, are employed toeffectively persuade other players to align with its actions.</description><author>Zhiyang Qi, Michimasa Inaba</author><pubDate>Thu, 29 Aug 2024 14:49:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16586v1</guid></item><item><title>Adaptive Reinforcement Learning Planning: Harnessing Large Language Models for Complex Information Extraction</title><link>http://arxiv.org/abs/2406.11455v2</link><description>Existing research on large language models (LLMs) shows that they can solveinformation extraction tasks through multi-step planning. However, theirextraction behavior on complex sentences and tasks is unstable, emerging issuessuch as false positives and missing elements. We observe that decomposingcomplex extraction tasks and extracting them step by step can effectivelyimprove LLMs' performance, and the extraction orders of entities significantlyaffect the final results of LLMs. This paper proposes a two-stage multi-stepmethod for LLM-based information extraction and adopts the RL framework toexecute the multi-step planning. We regard sequential extraction as a Markovdecision process, build an LLM-based extraction environment, design a decisionmodule to adaptively provide the optimal order for sequential entity extractionon different sentences, and utilize the DDQN algorithm to train the decisionmodel. We also design the rewards and evaluation metrics suitable for theextraction results of LLMs. We conduct extensive experiments on multiple publicdatasets to demonstrate the effectiveness of our method in improving theinformation extraction capabilities of LLMs.</description><author>Zepeng Ding, Ruiyang Ke, Wenhao Huang, Guochao Jiang, Yanda Li, Deqing Yang, Jiaqing Liang</author><pubDate>Thu, 29 Aug 2024 14:48:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11455v2</guid></item><item><title>FastForensics: Efficient Two-Stream Design for Real-Time Image Manipulation Detection</title><link>http://arxiv.org/abs/2408.16582v1</link><description>With the rise in popularity of portable devices, the spread of falsifiedmedia on social platforms has become rampant. This necessitates the timelyidentification of authentic content. However, most advanced detection methodsare computationally heavy, hindering their real-time application. In thispaper, we describe an efficient two-stream architecture for real-time imagemanipulation detection. Our method consists of two-stream branches targetingthe cognitive and inspective perspectives. In the cognitive branch, we proposeefficient wavelet-guided Transformer blocks to capture the global manipulationtraces related to frequency. This block contains an interactive wavelet-guidedself-attention module that integrates wavelet transformation with efficientattention design, interacting with the knowledge from the inspective branch.The inspective branch consists of simple convolutions that capture fine-grainedtraces and interact bidirectionally with Transformer blocks to provide mutualsupport. Our method is lightweight ($\sim$ 8M) but achieves competitiveperformance compared to many other counterparts, demonstrating its efficacy inimage manipulation detection and its potential for portable integration.</description><author>Yangxiang Zhang, Yuezun Li, Ao Luo, Jiaran Zhou, Junyu Dong</author><pubDate>Thu, 29 Aug 2024 14:48:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16582v1</guid></item><item><title>Conan-embedding: General Text Embedding with More and Better Negative Samples</title><link>http://arxiv.org/abs/2408.15710v2</link><description>With the growing popularity of RAG, the capabilities of embedding models aregaining increasing attention. Embedding models are primarily trained throughcontrastive loss learning, with negative examples being a key component.Previous work has proposed various hard negative mining strategies, but thesestrategies are typically employed as preprocessing steps. In this paper, wepropose the conan-embedding model, which maximizes the utilization of more andhigher-quality negative examples. Specifically, since the model's ability tohandle preprocessed negative examples evolves during training, we proposedynamic hard negative mining method to expose the model to more challengingnegative examples throughout the training process. Secondly, contrastivelearning requires as many negative examples as possible but is limited by GPUmemory constraints. Therefore, we use a Cross-GPU balancing Loss to providemore negative examples for embedding training and balance the batch size acrossmultiple tasks. Moreover, we also discovered that the prompt-response pairsfrom LLMs can be used for embedding training. Our approach effectively enhancesthe capabilities of embedding models, currently ranking first on the Chineseleaderboard of Massive text embedding benchmark</description><author>Shiyu Li, Yang Tang, Shizhe Chen, Xi Chen</author><pubDate>Thu, 29 Aug 2024 14:47:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15710v2</guid></item><item><title>Standardized Interpretable Fairness Measures for Continuous Risk Scores</title><link>http://arxiv.org/abs/2308.11375v2</link><description>We propose a standardized version of fairness measures for continuous scoreswith a reasonable interpretation based on the Wasserstein distance. Ourmeasures are easily computable and well suited for quantifying and interpretingthe strength of group disparities as well as for comparing biases acrossdifferent models, datasets, or time points. We derive a link between thedifferent families of existing fairness measures for scores and show that theproposed standardized fairness measures outperform ROC-based fairness measuresbecause they are more explicit and can quantify significant biases thatROC-based fairness measures miss.</description><author>Ann-Kristin Becker, Oana Dumitrasc, Klaus Broelemann</author><pubDate>Thu, 29 Aug 2024 14:45:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11375v2</guid></item><item><title>Transformers Meet ACT-R: Repeat-Aware and Sequential Listening Session Recommendation</title><link>http://arxiv.org/abs/2408.16578v1</link><description>Music streaming services often leverage sequential recommender systems topredict the best music to showcase to users based on past sequences oflistening sessions. Nonetheless, most sequential recommendation methods ignoreor insufficiently account for repetitive behaviors. This is a cruciallimitation for music recommendation, as repeatedly listening to the same songover time is a common phenomenon that can even change the way users perceivethis song. In this paper, we introduce PISA (Psychology-Informed Sessionembedding using ACT-R), a session-level sequential recommender system thatovercomes this limitation. PISA employs a Transformer architecture learningembedding representations of listening sessions and users using attentionmechanisms inspired by Anderson's ACT-R (Adaptive Control of Thought-Rational),a cognitive architecture modeling human information access and memory dynamics.This approach enables us to capture dynamic and repetitive patterns from userbehaviors, allowing us to effectively predict the songs they will listen to insubsequent sessions, whether they are repeated or new ones. We demonstrate theempirical relevance of PISA using both publicly available listening data fromLast.fm and proprietary data from Deezer, a global music streaming service,confirming the critical importance of repetition modeling for sequentiallistening session recommendation. Along with this paper, we publicly releaseour proprietary dataset to foster future research in this field, as well as thesource code of PISA to facilitate its future use.</description><author>Viet-Anh Tran, Guillaume Salha-Galvan, Bruno Sguerra, Romain Hennequin</author><pubDate>Thu, 29 Aug 2024 14:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16578v1</guid></item><item><title>Seeking the Sufficiency and Necessity Causal Features in Multimodal Representation Learning</title><link>http://arxiv.org/abs/2408.16577v1</link><description>Learning representations with a high Probability of Necessary and SufficientCauses (PNS) has been shown to enhance deep learning models' ability. This taskinvolves identifying causal features that are both sufficient (guaranteeing theoutcome) and necessary (without which the outcome cannot occur). However,current research predominantly focuses on unimodal data, and extending PNSlearning to multimodal settings presents significant challenges. The challengesarise as the conditions for PNS identifiability, Exogeneity and Monotonicity,need to be reconsidered in a multimodal context, where sufficient and necessarycausal features are distributed across different modalities. To address this,we first propose conceptualizing multimodal representations as comprisingmodality-invariant and modality-specific components. We then analyze PNSidentifiability for each component, while ensuring non-trivial PNS estimation.Finally, we formulate tractable optimization objectives that enable multimodalmodels to learn high-PNS representations, thereby enhancing their predictiveperformance. Experiments demonstrate the effectiveness of our method on bothsynthetic and real-world data.</description><author>Boyu Chen, Junjie Liu, Zhu Li, Mengyue yang</author><pubDate>Thu, 29 Aug 2024 14:43:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16577v1</guid></item><item><title>Mumpy: Multilateral Temporal-view Pyramid Transformer for Video Inpainting Detection</title><link>http://arxiv.org/abs/2404.11054v3</link><description>The task of video inpainting detection is to expose the pixel-level inpaintedregions within a video sequence. Existing methods usually focus on leveragingspatial and temporal inconsistencies. However, these methods typically employfixed operations to combine spatial and temporal clues, limiting theirapplicability in different scenarios. In this paper, we introduce a novelMultilateral Temporal-view Pyramid Transformer ({\em MumPy}) that collaboratesspatial-temporal clues flexibly. Our method utilizes a newly designedmultilateral temporal-view encoder to extract various collaborations ofspatial-temporal clues and introduces a deformable window-based temporal-viewinteraction module to enhance the diversity of these collaborations.Subsequently, we develop a multi-pyramid decoder to aggregate the various typesof features and generate detection maps. By adjusting the contribution strengthof spatial and temporal clues, our method can effectively identify inpaintedregions. We validate our method on existing datasets and also introduce a newchallenging and large-scale Video Inpainting dataset based on the YouTube-VOSdataset, which employs several more recent inpainting methods. The resultsdemonstrate the superiority of our method in both in-domain and cross-domainevaluation scenarios.</description><author>Ying Zhang, Yuezun Li, Bo Peng, Jiaran Zhou, Huiyu Zhou, Junyu Dong</author><pubDate>Thu, 29 Aug 2024 14:43:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11054v3</guid></item><item><title>An Adaptive Latent Factorization of Tensors Model for Embedding Dynamic Communication Network</title><link>http://arxiv.org/abs/2408.16573v1</link><description>The Dynamic Communication Network (DCN) describes the interactions over timeamong various communication nodes, and it is widely used in Big-dataapplications as a data source. As the number of communication nodes increasesand temporal slots accumulate, each node interacts in with only a few nodes ina given temporal slot, the DCN can be represented by an High-Dimensional Sparse(HDS) tensor. In order to extract rich behavioral patterns from an HDS tensorin DCN, this paper proposes an Adaptive Temporal-dependent Tensor low-rankrepresentation (ATT) model. It adopts a three-fold approach: a) designing atemporal-dependent method to reconstruct temporal feature matrix, therebyprecisely represent the data by capturing the temporal patterns; b) achievinghyper-parameters adaptation of the model via the Differential EvolutionaryAlgorithms (DEA) to avoid tedious hyper-parameters tuning; c) employingnonnegative learning schemes for the model parameters to effectively handle anthe nonnegativity inherent in HDS data. The experimental results on fourreal-world DCNs demonstrate that the proposed ATT model significantlyoutperforms several state-of-the-art models in both prediction errors andconvergence rounds.</description><author>Xin Liao, Qicong Hu, Peng Tang</author><pubDate>Thu, 29 Aug 2024 14:40:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16573v1</guid></item><item><title>Generalist Segmentation Algorithm for Photoreceptors Analysis in Adaptive Optics Imaging</title><link>http://arxiv.org/abs/2408.14810v2</link><description>Analyzing the cone photoreceptor pattern in images obtained from the livinghuman retina using quantitative methods can be crucial for the early detectionand management of various eye conditions. Confocal adaptive optics scanninglight ophthalmoscope (AOSLO) imaging enables visualization of the cones fromreflections of waveguiding cone photoreceptors. While there have beensignificant improvements in automated algorithms for segmenting cones inconfocal AOSLO images, the process of labelling data remains labor-intensiveand manual. This paper introduces a method based on deep learning (DL) fordetecting and segmenting cones in AOSLO images. The models were trained on asemi-automatically labelled dataset of 20 AOSLO batches of images of 18participants for 0$^{\circ}$, 1$^{\circ}$, and 2$^{\circ}$ from the fovealcenter. F1 scores were 0.968, 0.958, and 0.954 for 0$^{\circ}$, 1$^{\circ}$,and 2$^{\circ}$, respectively, which is better than previously reported DLapproaches. Our method minimizes the need for labelled data by onlynecessitating a fraction of labelled cones, which is especially beneficial inthe field of ophthalmology, where labelled data can often be limited.</description><author>Mikhail Kulyabin, Aline Sindel, Hilde Pedersen, Stuart Gilson, Rigmor Baraas, Andreas Maier</author><pubDate>Thu, 29 Aug 2024 14:38:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14810v2</guid></item><item><title>Predictability maximization and the origins of word order harmony</title><link>http://arxiv.org/abs/2408.16570v1</link><description>We address the linguistic problem of the sequential arrangement of a head andits dependents from an information theoretic perspective. In particular, weconsider the optimal placement of a head that maximizes the predictability ofthe sequence. We assume that dependents are statistically independent given ahead, in line with the open-choice principle and the core assumptions ofdependency grammar. We demonstrate the optimality of harmonic order, i.e.,placing the head last maximizes the predictability of the head whereas placingthe head first maximizes the predictability of dependents. We also show thatpostponing the head is the optimal strategy to maximize its predictabilitywhile bringing it forward is the optimal strategy to maximize thepredictability of dependents. We unravel the advantages of the strategy ofmaximizing the predictability of the head over maximizing the predictability ofdependents. Our findings shed light on the placements of the head adopted byreal languages or emerging in different kinds of experiments.</description><author>Ramon Ferrer-i-Cancho</author><pubDate>Thu, 29 Aug 2024 14:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16570v1</guid></item><item><title>Follow-up Attention: An Empirical Study of Developer and Neural Model Code Exploration</title><link>http://arxiv.org/abs/2210.05506v2</link><description>Recent neural models of code, such as OpenAI Codex and AlphaCode, havedemonstrated remarkable proficiency at code generation due to the underlyingattention mechanism. However, it often remains unclear how the models actuallyprocess code, and to what extent their reasoning and the way their attentionmechanism scans the code matches the patterns of developers. A poorunderstanding of the model reasoning process limits the way in which currentneural models are leveraged today, so far mostly for their raw prediction. Tofill this gap, this work studies how the processed attention signal of threeopen large language models - CodeGen, InCoder and GPT-J - agrees with howdevelopers look at and explore code when each answers the same sensemakingquestions about code. Furthermore, we contribute an open-source eye-trackingdataset comprising 92 manually-labeled sessions from 25 developers engaged insensemaking tasks. We empirically evaluate five heuristics that do not use theattention and ten attention-based post-processing approaches of the attentionsignal of CodeGen against our ground truth of developers exploring code,including the novel concept of follow-up attention which exhibits the highestagreement between model and human attention. Our follow-up attention method canpredict the next line a developer will look at with 47% accuracy. Thisoutperforms the baseline prediction accuracy of 42.3%, which uses the sessionhistory of other developers to recommend the next line. These resultsdemonstrate the potential of leveraging the attention signal of pre-trainedmodels for effective code exploration.</description><author>Matteo Paltenghi, Rahul Pandita, Austin Z. Henley, Albert Ziegler</author><pubDate>Thu, 29 Aug 2024 14:36:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.05506v2</guid></item><item><title>Identifying Terrain Physical Parameters from Vision -- Towards Physical-Parameter-Aware Locomotion and Navigation</title><link>http://arxiv.org/abs/2408.16567v1</link><description>Identifying the physical properties of the surrounding environment isessential for robotic locomotion and navigation to deal with non-geometrichazards, such as slippery and deformable terrains. It would be of great benefitfor robots to anticipate these extreme physical properties before contact;however, estimating environmental physical parameters from vision is still anopen challenge. Animals can achieve this by using their prior experience andknowledge of what they have seen and how it felt. In this work, we propose across-modal self-supervised learning framework for vision-based environmentalphysical parameter estimation, which paves the way for futurephysical-property-aware locomotion and navigation. We bridge the gap betweenexisting policies trained in simulation and identification of physical terrainparameters from vision. We propose to train a physical decoder in simulation topredict friction and stiffness from multi-modal input. The trained networkallows the labeling of real-world images with physical parameters in aself-supervised manner to further train a visual network during deployment,which can densely predict the friction and stiffness from image data. Wevalidate our physical decoder in simulation and the real world using aquadruped ANYmal robot, outperforming an existing baseline method. We show thatour visual network can predict the physical properties in indoor and outdoorexperiments while allowing fast adaptation to new environments.</description><author>Jiaqi Chen, Jonas Frey, Ruyi Zhou, Takahiro Miki, Georg Martius, Marco Hutter</author><pubDate>Thu, 29 Aug 2024 14:35:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16567v1</guid></item><item><title>Unified Convergence Theory of Stochastic and Variance-Reduced Cubic Newton Methods</title><link>http://arxiv.org/abs/2302.11962v3</link><description>We study stochastic Cubic Newton methods for solving general possiblynon-convex minimization problems. We propose a new framework, which we call thehelper framework, that provides a unified view of the stochastic andvariance-reduced second-order algorithms equipped with global complexityguarantees. It can also be applied to learning with auxiliary information. Ourhelper framework offers the algorithm designer high flexibility forconstructing and analyzing the stochastic Cubic Newton methods, allowingarbitrary size batches, and the use of noisy and possibly biased estimates ofthe gradients and Hessians, incorporating both the variance reduction and thelazy Hessian updates. We recover the best-known complexities for the stochasticand variance-reduced Cubic Newton, under weak assumptions on the noise. Adirect consequence of our theory is the new lazy stochastic second-ordermethod, which significantly improves the arithmetic complexity for largedimension problems. We also establish complexity bounds for the classes ofgradient-dominated objectives, that include convex and strongly convexproblems. For Auxiliary Learning, we show that using a helper (auxiliaryfunction) can outperform training alone if a given similarity measure is small.</description><author>El Mahdi Chayti, Nikita Doikov, Martin Jaggi</author><pubDate>Thu, 29 Aug 2024 14:31:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11962v3</guid></item><item><title>MST-KD: Multiple Specialized Teachers Knowledge Distillation for Fair Face Recognition</title><link>http://arxiv.org/abs/2408.16563v1</link><description>As in school, one teacher to cover all subjects is insufficient to distillequally robust information to a student. Hence, each subject is taught by ahighly specialised teacher. Following a similar philosophy, we propose amultiple specialized teacher framework to distill knowledge to a studentnetwork. In our approach, directed at face recognition use cases, we train fourteachers on one specific ethnicity, leading to four highly specialized andbiased teachers. Our strategy learns a project of these four teachers into acommon space and distill that information to a student network. Our resultshighlighted increased performance and reduced bias for all our experiments. Inaddition, we further show that having biased/specialized teachers is crucial byshowing that our approach achieves better results than when knowledge isdistilled from four teachers trained on balanced datasets. Our approachrepresents a step forward to the understanding of the importance ofethnicity-specific features.</description><author>Eduarda Caldeira, Jaime S. Cardoso, Ana F. Sequeira, Pedro C. Neto</author><pubDate>Thu, 29 Aug 2024 14:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16563v1</guid></item><item><title>No Regrets: Investigating and Improving Regret Approximations for Curriculum Discovery</title><link>http://arxiv.org/abs/2408.15099v2</link><description>What data or environments to use for training to improve downstreamperformance is a longstanding and very topical question in reinforcementlearning. In particular, Unsupervised Environment Design (UED) methods havegained recent attention as their adaptive curricula enable agents to be robustto in- and out-of-distribution tasks. We ask to what extent these methods arethemselves robust when applied to a novel setting, closely inspired by areal-world robotics problem. Surprisingly, we find that the state-of-the-artUED methods either do not improve upon the na\"{i}ve baseline of DomainRandomisation (DR), or require substantial hyperparameter tuning to do so. Ouranalysis shows that this is due to their underlying scoring functions failingto predict intuitive measures of ``learnability'', i.e., in finding thesettings that the agent sometimes solves, but not always. Based on this, weinstead directly train on levels with high learnability and find that thissimple and intuitive approach outperforms UED methods and DR in severalbinary-outcome environments, including on our domain and the standard UEDdomain of Minigrid. We further introduce a new adversarial evaluation procedurefor directly measuring robustness, closely mirroring the conditional value atrisk (CVaR). We open-source all our code and present visualisations of finalpolicies here: https://github.com/amacrutherford/sampling-for-learnability.</description><author>Alexander Rutherford, Michael Beukman, Timon Willi, Bruno Lacerda, Nick Hawes, Jakob Foerster</author><pubDate>Thu, 29 Aug 2024 14:20:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15099v2</guid></item><item><title>Android Malware Detection Based on RGB Images and Multi-feature Fusion</title><link>http://arxiv.org/abs/2408.16555v1</link><description>With the widespread adoption of smartphones, Android malware has become asignificant challenge in the field of mobile device security. Current Androidmalware detection methods often rely on feature engineering to constructdynamic or static features, which are then used for learning. However, staticfeature-based methods struggle to counter code obfuscation, packing, andsigning techniques, while dynamic feature-based methods involve time-consumingfeature extraction. Image-based methods for Android malware detection offerbetter resilience against malware variants and polymorphic malware. This paperproposes an end-to-end Android malware detection technique based on RGB imagesand multi-feature fusion. The approach involves extracting Dalvik Executable(DEX) files, AndroidManifest.xml files, and API calls from APK files,converting them into grayscale images, and enhancing their texture featuresusing Canny edge detection, histogram equalization, and adaptive thresholdingtechniques. These grayscale images are then combined into an RGB imagecontaining multi-feature fusion information, which is analyzed using mainstreamimage classification models for Android malware detection. Extensiveexperiments demonstrate that the proposed method effectively captures Androidmalware characteristics, achieving an accuracy of up to 97.25%, outperformingexisting detection methods that rely solely on DEX files as classificationfeatures. Additionally, ablation experiments confirm the effectiveness of usingthe three key files for feature representation in the proposed approach.</description><author>Zhiqiang Wang, Qiulong Yu, Sicheng Yuan</author><pubDate>Thu, 29 Aug 2024 14:18:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16555v1</guid></item><item><title>Super-Resolution works for coastal simulations</title><link>http://arxiv.org/abs/2408.16553v1</link><description>Learning fine-scale details of a coastal ocean simulation from a coarserepresentation is a challenging task. For real-world applications,high-resolution simulations are necessary to advance understanding of manycoastal processes, specifically, to predict flooding resulting from tsunamisand storm surges. We propose a Deep Network for Coastal Super-Resolution(DNCSR) for spatiotemporal enhancement to efficiently learn the high-resolutionnumerical solution. Given images of coastal simulations produced onlow-resolution computational meshes using low polynomial order discontinuousGalerkin discretizations and a coarse temporal resolution, the proposed DNCSRlearns to produce high-resolution free surface elevation and velocityvisualizations in both time and space. To efficiently model the dynamic changesover time and space, we propose grid-aware spatiotemporal attention to projectthe temporal features to the spatial domain for non-local feature matching. Thecoordinate information is also utilized via positional encoding. For the finalreconstruction, we use the spatiotemporal bilinear operation to interpolate themissing frames and then expand the feature maps to the frequency domain forresidual mapping. Besides data-driven losses, the proposed physics-informedloss guarantees gradient consistency and momentum changes. Their combinationcontributes to the overall 24% improvements in RMSE. To train the proposedmodel, we propose a large-scale coastal simulation dataset and use it for modeloptimization and evaluation. Our method shows superior super-resolution qualityand fast computation compared to the state-of-the-art methods.</description><author>Zhi-Song Liu, Markus Buttner, Vadym Aizinger, Andreas Rupp</author><pubDate>Thu, 29 Aug 2024 14:16:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16553v1</guid></item></channel></rss>