<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 25 Feb 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>PALO: A Polyglot Large Multimodal Model for 5B People</title><link>http://arxiv.org/abs/2402.14818v1</link><description>In pursuit of more inclusive Vision-Language Models (VLMs), this studyintroduces a Large Multilingual Multimodal Model called \textsc{Palo}.\textsc{Palo} offers visual reasoning capabilities in 10 major languages,including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian,Urdu, and Japanese, that span a total of $\sim$5B people (65\% of the worldpopulation). Our approach involves a semi-automated translation approach toadapt the multimodal instruction dataset from English to the target languagesusing a fine-tuned Large Language Model, thereby ensuring high linguisticfidelity while allowing scalability due to minimal manual effort. Theincorporation of diverse instruction sets helps us boost overall performanceacross multiple languages especially those that are underrepresented likeHindi, Arabic, Bengali, and Urdu. The resulting models are trained across threescales (1.7B, 7B and 13B parameters) to show the generalization and scalabilitywhere we observe substantial improvements compared to strong baselines. We alsopropose the first multilingual multimodal benchmark for the forthcomingapproaches to evaluate their vision-language reasoning capabilities acrosslanguages. Code: https://github.com/mbzuai-oryx/PALO.</description><author>Muhammad Maaz, Hanoona Rasheed, Abdelrahman Shaker, Salman Khan, Hisham Cholakal, Rao M. Anwer, Tim Baldwin, Michael Felsberg, Fahad S. Khan</author><pubDate>Thu, 22 Feb 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14818v1</guid></item><item><title>Cameras as Rays: Pose Estimation via Ray Diffusion</title><link>http://arxiv.org/abs/2402.14817v1</link><description>Estimating camera poses is a fundamental task for 3D reconstruction andremains challenging given sparse views (&lt;10). In contrast to existingapproaches that pursue top-down prediction of global parametrizations of cameraextrinsics, we propose a distributed representation of camera pose that treatsa camera as a bundle of rays. This representation allows for a tight couplingwith spatial image features improving pose precision. We observe that thisrepresentation is naturally suited for set-level level transformers and developa regression-based approach that maps image patches to corresponding rays. Tocapture the inherent uncertainties in sparse-view pose inference, we adapt thisapproach to learn a denoising diffusion model which allows us to sampleplausible modes while improving performance. Our proposed methods, bothregression- and diffusion-based, demonstrate state-of-the-art performance oncamera pose estimation on CO3D while generalizing to unseen object categoriesand in-the-wild captures.</description><author>Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, Shubham Tulsiani</author><pubDate>Thu, 22 Feb 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14817v1</guid></item><item><title>Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging</title><link>http://arxiv.org/abs/2402.14815v1</link><description>Advances in artificial intelligence (AI) have achieved expert-levelperformance in medical imaging applications. Notably, self-supervisedvision-language foundation models can detect a broad spectrum of pathologieswithout relying on explicit training annotations. However, it is crucial toensure that these AI models do not mirror or amplify human biases, therebydisadvantaging historically marginalized groups such as females or Blackpatients. The manifestation of such biases could systematically delay essentialmedical care for certain patient subgroups. In this study, we investigate thealgorithmic fairness of state-of-the-art vision-language foundation models inchest X-ray diagnosis across five globally-sourced datasets. Our findingsreveal that compared to board-certified radiologists, these foundation modelsconsistently underdiagnose marginalized groups, with even higher rates seen inintersectional subgroups, such as Black female patients. Such demographicbiases present over a wide range of pathologies and demographic attributes.Further analysis of the model embedding uncovers its significant encoding ofdemographic information. Deploying AI systems with these biases in medicalimaging can intensify pre-existing care disparities, posing potentialchallenges to equitable healthcare access and raising ethical questions abouttheir clinical application.</description><author>Yuzhe Yang, Yujia Liu, Xin Liu, Avanti Gulhane, Domenico Mastrodicasa, Wei Wu, Edward J Wang, Dushyant W Sahani, Shwetak Patel</author><pubDate>Thu, 22 Feb 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14815v1</guid></item><item><title>WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition</title><link>http://arxiv.org/abs/2402.14812v1</link><description>Weakly supervised visual recognition using inexact supervision is a criticalyet challenging learning problem. It significantly reduces human labeling costsand traditionally relies on multi-instance learning and pseudo-labeling. Thispaper introduces WeakSAM and solves the weakly-supervised object detection(WSOD) and segmentation by utilizing the pre-learned world knowledge containedin a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAMaddresses two critical limitations in traditional WSOD retraining, i.e., pseudoground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGTgeneration and Region of Interest (RoI) drop regularization. It also addressesthe SAM's problems of requiring prompts and category unawareness for automaticobject detection and segmentation. Our results indicate that WeakSAMsignificantly surpasses previous state-of-the-art methods in WSOD and WSISbenchmarks with large margins, i.e. average improvements of 7.4% and 8.5%,respectively. The code is available at \url{https://github.com/hustvl/WeakSAM}.</description><author>Lianghui Zhu, Junwei Zhou, Yan Liu, Xin Hao, Wenyu Liu, Xinggang Wang</author><pubDate>Thu, 22 Feb 2024 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14812v1</guid></item><item><title>Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking</title><link>http://arxiv.org/abs/2402.14811v1</link><description>Fine-tuning on generalized tasks such as instruction following, codegeneration, and mathematics has been shown to enhance language models'performance on a range of tasks. Nevertheless, explanations of how suchfine-tuning influences the internal computations in these models remainelusive. We study how fine-tuning affects the internal mechanisms implementedin language models. As a case study, we explore the property of entitytracking, a crucial facet of language comprehension, where models fine-tuned onmathematics have substantial performance gains. We identify the mechanism thatenables entity tracking and show that (i) in both the original model and itsfine-tuned versions primarily the same circuit implements entity tracking. Infact, the entity tracking circuit of the original model on the fine-tunedversions performs better than the full original model. (ii) The circuits of allthe models implement roughly the same functionality: Entity tracking isperformed by tracking the position of the correct entity in both the originalmodel and its fine-tuned versions. (iii) Performance boost in the fine-tunedmodels is primarily attributed to its improved ability to handle the augmentedpositional information. To uncover these findings, we employ: Patch Patching,DCM, which automatically detects model components responsible for specificsemantics, and CMAP, a new approach for patching activations across models toreveal improved mechanisms. Our findings suggest that fine-tuning enhances,rather than fundamentally alters, the mechanistic operation of the model.</description><author>Nikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, David Bau</author><pubDate>Thu, 22 Feb 2024 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14811v1</guid></item><item><title>GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion</title><link>http://arxiv.org/abs/2402.14810v1</link><description>In this work, we tackle the challenging problem of denoising hand-objectinteractions (HOI). Given an erroneous interaction sequence, the objective isto refine the incorrect hand trajectory to remove interaction artifacts for aperceptually realistic sequence. This challenge involves intricate interactionnoise, including unnatural hand poses and incorrect hand-object relations,alongside the necessity for robust generalization to new interactions anddiverse noise patterns. We tackle those challenges through a novel approach,GeneOH Diffusion, incorporating two key designs: an innovative contact-centricHOI representation named GeneOH and a new domain-generalizable denoisingscheme. The contact-centric representation GeneOH informatively parameterizesthe HOI process, facilitating enhanced generalization across various HOIscenarios. The new denoising scheme consists of a canonical denoising modeltrained to project noisy data samples from a whitened noise space to a cleandata manifold and a "denoising via diffusion" strategy which can handle inputtrajectories with various noise patterns by first diffusing them to align withthe whitened noise space and cleaning via the canonical denoiser. Extensiveexperiments on four benchmarks with significant domain variations demonstratethe superior effectiveness of our method. GeneOH Diffusion also shows promisefor various downstream applications. Project website:https://meowuu7.github.io/GeneOH-Diffusion/.</description><author>Xueyi Liu, Li Yi</author><pubDate>Thu, 22 Feb 2024 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14810v1</guid></item><item><title>CriticBench: Benchmarking LLMs for Critique-Correct Reasoning</title><link>http://arxiv.org/abs/2402.14809v1</link><description>The ability of Large Language Models (LLMs) to critique and refine theirreasoning is crucial for their application in evaluation, feedback provision,and self-improvement. This paper introduces CriticBench, a comprehensivebenchmark designed to assess LLMs' abilities to critique and rectify theirreasoning across a variety of tasks. CriticBench encompasses five reasoningdomains: mathematical, commonsense, symbolic, coding, and algorithmic. Itcompiles 15 datasets and incorporates responses from three LLM families.Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs ingeneration, critique, and correction reasoning, i.e., GQC reasoning. Ourfindings reveal: (1) a linear relationship in GQC capabilities, withcritique-focused training markedly enhancing performance; (2) a task-dependentvariation in correction effectiveness, with logic-oriented tasks being moreamenable to correction; (3) GQC knowledge inconsistencies that decrease asmodel size increases; and (4) an intriguing inter-model critiquing dynamic,where stronger models are better at critiquing weaker ones, while weaker modelscan surprisingly surpass stronger ones in their self-critique. We hope theseinsights into the nuanced critique-correct reasoning of LLMs will fosterfurther research in LLM critique and self-improvement.</description><author>Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang</author><pubDate>Thu, 22 Feb 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14809v1</guid></item><item><title>RelayAttention for Efficient Large Language Model Serving with Long System Prompts</title><link>http://arxiv.org/abs/2402.14808v1</link><description>Practical large language model (LLM) services may involve a long systemprompt, which specifies the instructions, examples, and knowledge documents ofthe task and is reused across numerous requests. However, the long systemprompt causes throughput/latency bottlenecks as the cost of generating the nexttoken grows w.r.t. the sequence length. This paper aims to improve theefficiency of LLM services that involve long system prompts. Our keyobservation is that handling these system prompts requires heavily redundantmemory accesses in existing causal attention computation algorithms.Specifically, for batched requests, the cached hidden states (i.e., key-valuepairs) of system prompts are transferred from off-chip DRAM to on-chip SRAMmultiple times, each corresponding to an individual request. To eliminate sucha redundancy, we propose RelayAttention, an attention algorithm that allowsreading these hidden states from DRAM exactly once for a batch of input tokens.RelayAttention is a free lunch: it maintains the generation quality whilerequiring no model retraining, as it is based on a mathematical reformulationof causal attention.</description><author>Lei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W. H. Lau</author><pubDate>Thu, 22 Feb 2024 18:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14808v1</guid></item><item><title>A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health</title><link>http://arxiv.org/abs/2402.14807v1</link><description>Efforts to reduce maternal mortality rate, a key UN Sustainable Developmenttarget (SDG Target 3.1), rely largely on preventative care programs to spreadcritical health information to high-risk populations. These programs face twoimportant challenges: efficiently allocating limited health resources to largebeneficiary populations, and adapting to evolving policy priorities. Whileprior works in restless multi-armed bandit (RMAB) demonstrated success inpublic health allocation tasks, they lack flexibility to adapt to evolvingpolicy priorities. Concurrently, Large Language Models (LLMs) have emerged asadept, automated planners in various domains, including robotic control andnavigation. In this paper, we propose DLM: a Decision Language Model for RMABs.To enable dynamic fine-tuning of RMAB policies for challenging public healthsettings using human-language commands, we propose using LLMs as automatedplanners to (1) interpret human policy preference prompts, (2) propose codereward functions for a multi-agent RL environment for RMABs, and (3) iterate onthe generated reward using feedback from RMAB simulations to effectively adaptpolicy outcomes. In collaboration with ARMMAN, an India-based public healthorganization promoting preventative care for pregnant mothers, we conduct asimulation study, showing DLM can dynamically shape policy outcomes using onlyhuman language commands as input.</description><author>Nikhil Behari, Edwin Zhang, Yunfan Zhao, Aparna Taneja, Dheeraj Nagaraj, Milind Tambe</author><pubDate>Thu, 22 Feb 2024 18:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14807v1</guid></item><item><title>Difference Learning for Air Quality Forecasting Transport Emulation</title><link>http://arxiv.org/abs/2402.14806v1</link><description>Human health is negatively impacted by poor air quality including increasedrisk for respiratory and cardiovascular disease. Due to a recent increase inextreme air quality events, both globally and locally in the United States,finer resolution air quality forecasting guidance is needed to effectivelyadapt to these events. The National Oceanic and Atmospheric Administrationprovides air quality forecasting guidance for the Continental United States.Their air quality forecasting model is based on a 15 km spatial resolution;however, the goal is to reach a three km spatial resolution. This is currentlynot feasible due in part to prohibitive computational requirements for modelingthe transport of chemical species. In this work, we describe a deep learningtransport emulator that is able to reduce computations while maintaining skillcomparable with the existing numerical model. We show how this method maintainsskill in the presence of extreme air quality events, making it a potentialcandidate for operational use. We also explore evaluating how well this modelmaintains the physical properties of the modeled transport for a given set ofspecies.</description><author>Reed River Chen, Christopher Ribaudo, Jennifer Sleeman, Chace Ashcraft, Collin Kofroth, Marisa Hughes, Ivanka Stajner, Kevin Viner, Kai Wang</author><pubDate>Thu, 22 Feb 2024 18:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14806v1</guid></item><item><title>AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer</title><link>http://arxiv.org/abs/2312.05928v3</link><description>Neural style transfer (NST) has evolved significantly in recent years. Yet,despite its rapid progress and advancement, existing NST methods eitherstruggle to transfer aesthetic information from a style effectively or sufferfrom high computational costs and inefficiencies in feature disentanglement dueto using pre-trained models. This work proposes a lightweight but effectivemodel, AesFA -- Aesthetic Feature-Aware NST. The primary idea is to decomposethe image via its frequencies to better disentangle aesthetic styles from thereference image while training the entire model in an end-to-end manner toexclude pre-trained models at inference completely. To improve the network'sability to extract more distinct representations and further enhance thestylization quality, this work introduces a new aesthetic feature: contrastiveloss. Extensive experiments and ablations show the approach not onlyoutperforms recent NST methods in terms of stylization quality, but it alsoachieves faster inference. Codes are available athttps://github.com/Sooyyoungg/AesFA.</description><author>Joonwoo Kwon, Sooyoung Kim, Yuewei Lin, Shinjae Yoo, Jiook Cha</author><pubDate>Thu, 22 Feb 2024 18:57:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05928v3</guid></item><item><title>Identifying Multiple Personalities in Large Language Models with External Evaluation</title><link>http://arxiv.org/abs/2402.14805v1</link><description>As Large Language Models (LLMs) are integrated with human daily applicationsrapidly, many societal and ethical concerns are raised regarding the behaviorof LLMs. One of the ways to comprehend LLMs' behavior is to analyze theirpersonalities. Many recent studies quantify LLMs' personalities usingself-assessment tests that are created for humans. Yet many critiques questionthe applicability and reliability of these self-assessment tests when appliedto LLMs. In this paper, we investigate LLM personalities using an alternatepersonality measurement method, which we refer to as the external evaluationmethod, where instead of prompting LLMs with multiple-choice questions in theLikert scale, we evaluate LLMs' personalities by analyzing their responsestoward open-ended situational questions using an external machine learningmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictorthat outperforms the state-of-the-art models as the tool to analyze LLMs'responses. Then, we prompt the LLMs with situational questions and ask them togenerate Twitter posts and comments, respectively, in order to assess theirpersonalities when playing two different roles. Using the external personalityevaluation method, we identify that the obtained personality types for LLMs aresignificantly different when generating posts versus comments, whereas humansshow a consistent personality profile in these two different situations. Thisshows that LLMs can exhibit different personalities based on differentscenarios, thus highlighting a fundamental difference between personality inLLMs and humans. With our work, we call for a re-evaluation of personalitydefinition and measurement in LLMs.</description><author>Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur</author><pubDate>Thu, 22 Feb 2024 18:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14805v1</guid></item><item><title>Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset</title><link>http://arxiv.org/abs/2402.14804v1</link><description>Recent advancements in Large Multimodal Models (LMMs) have shown promisingresults in mathematical reasoning within visual contexts, with modelsapproaching human-level performance on existing benchmarks such as MathVista.However, we observe significant limitations in the diversity of questions andbreadth of subjects covered by these benchmarks. To address this issue, wepresent the MATH-Vision (MATH-V) dataset, a meticulously curated collection of3,040 high-quality mathematical problems with visual contexts sourced from realmath competitions. Spanning 16 distinct mathematical disciplines and gradedacross 5 levels of difficulty, our dataset provides a comprehensive and diverseset of challenges for evaluating the mathematical reasoning abilities of LMMs.Through extensive experimentation, we unveil a notable performance gap betweencurrent LMMs and human performance on MATH-V, underscoring the imperative forfurther advancements in LMMs. Moreover, our detailed categorization allows fora thorough error analysis of LMMs, offering valuable insights to guide futureresearch and development. The project is available athttps://mathvision-cuhk.github.io</description><author>Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, Hongsheng Li</author><pubDate>Thu, 22 Feb 2024 18:56:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14804v1</guid></item><item><title>Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach</title><link>http://arxiv.org/abs/2402.14802v1</link><description>In the past years, Graph Neural Networks (GNNs) have become the `de facto'standard in various deep learning domains, thanks to their flexibility inmodeling real-world phenomena represented as graphs. However, themessage-passing mechanism of GNNs faces challenges in learnability andexpressivity, hindering high performance on heterophilic graphs, where adjacentnodes frequently have different labels. Most existing solutions addressingthese challenges are primarily confined to specific benchmarks focused on nodeclassification tasks. This narrow focus restricts the potential impact thatlink prediction under heterophily could offer in several applications,including recommender systems. For example, in social networks, two users maybe connected for some latent reason, making it challenging to predict suchconnections in advance. Physics-Inspired GNNs such as GRAFF provided asignificant contribution to enhance node classification performance underheterophily, thanks to the adoption of physics biases in the message-passing.Drawing inspiration from these findings, we advocate that the methodologyemployed by GRAFF can improve link prediction performance as well. To furtherexplore this hypothesis, we introduce GRAFF-LP, an extension of GRAFF to linkprediction. We evaluate its efficacy within a recent collection of heterophilicgraphs, establishing a new benchmark for link prediction under heterophily. Ourapproach surpasses previous methods, in most of the datasets, showcasing astrong flexibility in different contexts, and achieving relative AUROCimprovements of up to 26.7%.</description><author>Andrea Giuseppe Di Francesco, Francesco Caso, Maria Sofia Bucarelli, Fabrizio Silvestri</author><pubDate>Thu, 22 Feb 2024 18:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14802v1</guid></item><item><title>Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models</title><link>http://arxiv.org/abs/2402.14800v1</link><description>A pivotal advancement in the progress of large language models (LLMs) is theemergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs,MoE LLMs can achieve higher performance with fewer parameters, but it is stillhard to deploy them due to their immense parameter sizes. Different fromprevious weight pruning methods that rely on specifically designed hardware,this paper mainly aims to enhance the deployment efficiency of MoE LLMs byintroducing plug-and-play expert-level sparsification techniques. Specifically,we propose, for the first time to our best knowledge, post-training approachesfor task-agnostic and task-specific expert pruning and skipping of MoE LLMs,tailored to improve deployment efficiency while maintaining model performanceacross a wide range of tasks. Extensive experiments show that our proposedmethods can simultaneously reduce model sizes and increase the inference speed,while maintaining satisfactory performance. Data and code will be available athttps://github.com/Lucky-Lance/Expert_Sparsity.</description><author>Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng Li</author><pubDate>Thu, 22 Feb 2024 18:56:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14800v1</guid></item><item><title>Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic</title><link>http://arxiv.org/abs/2402.14798v1</link><description>Contemporary language models enable new opportunities for structuredreasoning with text, such as the construction and evaluation of intuitive,proof-like textual entailment trees without relying on brittle formal logic.However, progress in this direction has been hampered by a long-standing lackof a clear protocol for determining what valid compositional entailment is.This absence causes noisy datasets and limited performance gains by modernneuro-symbolic engines. To address these problems, we formulate a consistentand theoretically grounded approach to annotating decompositional entailmentdatasets, and evaluate its impact on LLM-based textual inference. We find thatour resulting dataset, RDTE (Recognizing Decompositional Textual Entailment),has a substantially higher internal consistency (+9%) than priordecompositional entailment datasets, suggesting that RDTE is a significant stepforward in the long-standing problem of forming a clear protocol for discerningentailment. We also find that training an RDTE-oriented entailment classifiervia knowledge distillation and employing it in a modern neuro-symbolicreasoning engine significantly improves results (both accuracy and proofquality) over other entailment classifier baselines, illustrating the practicalbenefit of this advance for textual inference.</description><author>Nathaniel Weir, Kate Sanders, Orion Weller, Shreya Sharma, Dongwei Jiang, Zhengping Zhang, Bhavana Dalvi Mishra, Oyvind Tafjord, Peter Jansen, Peter Clark, Benjamin Van Durme</author><pubDate>Thu, 22 Feb 2024 18:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14798v1</guid></item><item><title>Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis</title><link>http://arxiv.org/abs/2402.14797v1</link><description>Contemporary models for generating images show remarkable quality andversatility. Swayed by these advantages, the research community repurposes themto generate videos. Since video content is highly redundant, we argue thatnaively bringing advances of image models to the video generation domainreduces motion fidelity, visual quality and impairs scalability. In this work,we build Snap Video, a video-first model that systematically addresses thesechallenges. To do that, we first extend the EDM framework to take into accountspatially and temporally redundant pixels and naturally support videogeneration. Second, we show that a U-Net - a workhorse behind image generation- scales poorly when generating videos, requiring significant computationaloverhead. Hence, we propose a new transformer-based architecture that trains3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows usto efficiently train a text-to-video model with billions of parameters for thefirst time, reach state-of-the-art results on a number of benchmarks, andgenerate videos with substantially higher quality, temporal consistency, andmotion complexity. The user studies showed that our model was favored by alarge margin over the most recent methods. See our website athttps://snap-research.github.io/snapvideo/.</description><author>Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, Sergey Tulyakov</author><pubDate>Thu, 22 Feb 2024 18:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14797v1</guid></item><item><title>CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation</title><link>http://arxiv.org/abs/2402.14795v1</link><description>We introduce CyberDemo, a novel approach to robotic imitation learning thatleverages simulated human demonstrations for real-world tasks. By incorporatingextensive data augmentation in a simulated environment, CyberDemo outperformstraditional in-domain real-world demonstrations when transferred to the realworld, handling diverse physical and visual conditions. Regardless of itsaffordability and convenience in data collection, CyberDemo outperformsbaseline methods in terms of success rates across various tasks and exhibitsgeneralizability with previously unseen objects. For example, it can rotatenovel tetra-valve and penta-valve, despite human demonstrations only involvingtri-valves. Our research demonstrates the significant potential of simulatedhuman demonstrations for real-world dexterous manipulation tasks. More detailscan be found at https://cyber-demo.github.io</description><author>Jun Wang, Yuzhe Qin, Kaiming Kuang, Yigit Korkmaz, Akhilan Gurumoorthy, Hao Su, Xiaolong Wang</author><pubDate>Thu, 22 Feb 2024 18:54:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14795v1</guid></item><item><title>Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution</title><link>http://arxiv.org/abs/2210.00131v4</link><description>Modern language modeling tasks are often underspecified: for a given tokenprediction, many words may satisfy the user's intent of producing naturallanguage at inference time, however only one word will minimize the task's lossfunction at training time. We introduce a simple causal mechanism to describethe role underspecification plays in the generation of spurious correlations.Despite its simplicity, our causal model directly informs the development oftwo lightweight black-box evaluation methods, that we apply to gendered pronounresolution tasks on a wide range of LLMs to 1) aid in the detection ofinference-time task underspecification by exploiting 2) previously unreportedgender vs. time and gender vs. location spurious correlations on LLMs with arange of A) sizes: from BERT-base to GPT-4 Turbo Preview, B) pre-trainingobjectives: from masked &amp; autoregressive language modeling to a mixture ofthese objectives, and C) training stages: from pre-training only toreinforcement learning from human feedback (RLHF). Code and open-source demosavailable at https://github.com/2dot71mily/uspec.</description><author>Emily McMilin</author><pubDate>Thu, 22 Feb 2024 18:52:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.00131v4</guid></item><item><title>Consolidating Attention Features for Multi-view Image Editing</title><link>http://arxiv.org/abs/2402.14792v1</link><description>Large-scale text-to-image models enable a wide range of image editingtechniques, using text prompts or even spatial controls. However, applyingthese editing methods to multi-view images depicting a single scene leads to3D-inconsistent results. In this work, we focus on spatial control-basedgeometric manipulations and introduce a method to consolidate the editingprocess across various views. We build on two insights: (1) maintainingconsistent features throughout the generative process helps attain consistencyin multi-view editing, and (2) the queries in self-attention layerssignificantly influence the image structure. Hence, we propose to improve thegeometric consistency of the edited images by enforcing the consistency of thequeries. To do so, we introduce QNeRF, a neural radiance field trained on theinternal query features of the edited images. Once trained, QNeRF can render3D-consistent queries, which are then softly injected back into theself-attention layers during generation, greatly improving multi-viewconsistency. We refine the process through a progressive, iterative method thatbetter consolidates queries across the diffusion timesteps. We compare ourmethod to a range of existing techniques and demonstrate that it can achievebetter multi-view consistency and higher fidelity to the input scene. Theseadvantages allow us to train NeRFs with fewer visual artifacts, that are betteraligned with the target geometry.</description><author>Or Patashnik, Rinon Gal, Daniel Cohen-Or, Jun-Yan Zhu, Fernando De la Torre</author><pubDate>Thu, 22 Feb 2024 18:50:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14792v1</guid></item><item><title>Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning</title><link>http://arxiv.org/abs/2402.14789v1</link><description>Self-supervised learning excels in learning representations from largeamounts of unlabeled data, demonstrating success across multiple datamodalities. Yet, extending self-supervised learning to new modalities isnon-trivial because the specifics of existing methods are tailored to eachdomain, such as domain-specific augmentations which reflect the invariances inthe target task. While masked modeling is promising as a domain-agnosticframework for self-supervised learning because it does not rely on inputaugmentations, its mask sampling procedure remains domain-specific. We presentSelf-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modelingmethod. SMA trains an attention based model using a masked modeling objective,by learning masks to sample without any domain-specific assumptions. Weevaluate SMA on three self-supervised learning benchmarks in protein biology,chemical property prediction, and particle physics. We find SMA is capable oflearning representations without domain-specific knowledge and achievesstate-of-the-art performance on these three benchmarks.</description><author>Johnathan Xie, Yoonho Lee, Annie S. Chen, Chelsea Finn</author><pubDate>Thu, 22 Feb 2024 18:46:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14789v1</guid></item><item><title>ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs</title><link>http://arxiv.org/abs/2402.11753v2</link><description>Safety is critical to the usage of large language models (LLMs). Multipletechniques such as data filtering and supervised fine-tuning have beendeveloped to strengthen LLM safety. However, currently known techniques presumethat corpora used for safety alignment of LLMs are solely interpreted bysemantics. This assumption, however, does not hold in real-world applications,which leads to severe vulnerabilities in LLMs. For example, users of forumsoften use ASCII art, a form of text-based art, to convey image information. Inthis paper, we propose a novel ASCII art-based jailbreak attack and introduce acomprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate thecapabilities of LLMs in recognizing prompts that cannot be solely interpretedby semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, andLlama2) struggle to recognize prompts provided in the form of ASCII art. Basedon this observation, we develop the jailbreak attack ArtPrompt, which leveragesthe poor performance of LLMs in recognizing ASCII art to bypass safety measuresand elicit undesired behaviors from LLMs. ArtPrompt only requires black-boxaccess to the victim LLMs, making it a practical attack. We evaluate ArtPrompton five SOTA LLMs, and show that ArtPrompt can effectively and efficientlyinduce undesired behaviors from all five LLMs.</description><author>Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran</author><pubDate>Thu, 22 Feb 2024 18:40:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11753v2</guid></item><item><title>Rao-Blackwellising Bayesian Causal Inference</title><link>http://arxiv.org/abs/2402.14781v1</link><description>Bayesian causal inference, i.e., inferring a posterior over causal models forthe use in downstream causal reasoning tasks, poses a hard computationalinference problem that is little explored in literature. In this work, wecombine techniques from order-based MCMC structure learning with recentadvances in gradient-based graph learning into an effective Bayesian causalinference framework. Specifically, we decompose the problem of inferring thecausal structure into (i) inferring a topological order over variables and (ii)inferring the parent sets for each variable. When limiting the number ofparents per variable, we can exactly marginalise over the parent sets inpolynomial time. We further use Gaussian processes to model the unknown causalmechanisms, which also allows their exact marginalisation. This introduces aRao-Blackwellization scheme, where all components are eliminated from themodel, except for the causal order, for which we learn a distribution viagradient-based optimisation. The combination of Rao-Blackwellization with oursequential inference procedure for causal orders yields state-of-the-art onlinear and non-linear additive noise benchmarks with scale-free and Erdos-Renyigraph structures.</description><author>Christian Toth, Christian Knoll, Franz Pernkopf, Robert Peharz</author><pubDate>Thu, 22 Feb 2024 18:39:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14781v1</guid></item><item><title>Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models</title><link>http://arxiv.org/abs/2402.14780v1</link><description>Image customization has been extensively studied in text-to-image (T2I)diffusion models, leading to impressive outcomes and applications. With theemergence of text-to-video (T2V) diffusion models, its temporal counterpart,motion customization, has not yet been well investigated. To address thechallenge of one-shot motion customization, we propose Customize-A-Video thatmodels the motion from a single reference video and adapting it to new subjectsand scenes with both spatial and temporal varieties. It leverages low-rankadaptation (LoRA) on temporal attention layers to tailor the pre-trained T2Vdiffusion model for specific motion modeling from the reference videos. Todisentangle the spatial and temporal information during the training pipeline,we introduce a novel concept of appearance absorbers that detach the originalappearance from the single reference video prior to motion learning. Ourproposed method can be easily extended to various downstream tasks, includingcustom video generation and editing, video appearance customization, andmultiple motion combination, in a plug-and-play fashion. Our project page canbe found at https://anonymous-314.github.io.</description><author>Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, Abhinav Shrivastava</author><pubDate>Thu, 22 Feb 2024 18:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14780v1</guid></item><item><title>Transformers as Support Vector Machines</title><link>http://arxiv.org/abs/2308.16898v3</link><description>Since its inception in "Attention Is All You Need", transformer architecturehas led to revolutionary advancements in NLP. The attention layer within thetransformer admits a sequence of input tokens $X$ and makes them interactthrough pairwise similarities computed as softmax$(XQK^\top X^\top)$, where$(K,Q)$ are the trainable key-query parameters. In this work, we establish aformal equivalence between the optimization geometry of self-attention and ahard-margin SVM problem that separates optimal input tokens from non-optimaltokens using linear constraints on the outer-products of token pairs. Thisformalism allows us to characterize the implicit bias of 1-layer transformersoptimized with gradient descent: (1) Optimizing the attention layer withvanishing regularization, parameterized by $(K,Q)$, converges in direction toan SVM solution minimizing the nuclear norm of the combined parameter$W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius normobjective. We characterize this convergence, highlighting that it can occurtoward locally-optimal directions rather than global ones. (2) Complementingthis, we prove the local/global directional convergence of gradient descentunder suitable geometric conditions. Importantly, we show thatover-parameterization catalyzes global convergence by ensuring the feasibilityof the SVM problem and by guaranteeing a benign optimization landscape devoidof stationary points. (3) While our theory applies primarily to linearprediction heads, we propose a more general SVM equivalence that predicts theimplicit bias with nonlinear heads. Our findings are applicable to arbitrarydatasets and their validity is verified via experiments. We also introduceseveral open problems and research directions. We believe these findingsinspire the interpretation of transformers as a hierarchy of SVMs thatseparates and selects optimal tokens.</description><author>Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, Samet Oymak</author><pubDate>Thu, 22 Feb 2024 18:38:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16898v3</guid></item><item><title>Zero-shot cross-lingual transfer in instruction tuning of large language model</title><link>http://arxiv.org/abs/2402.14778v1</link><description>Instruction tuning (IT) is widely used to teach pretrained large languagemodels (LLMs) to follow arbitrary instructions, but is under-studied inmultilingual settings. In this work, we conduct a systematic study of zero-shotcross-lingual transfer in IT, when an LLM is instruction-tuned on English-onlydata and then tested on user prompts in other languages. We investigate theinfluence of model configuration choices and devise a multi-facet evaluationstrategy for multilingual instruction following. We find that cross-lingualtransfer does happen successfully in IT even if all stages of model trainingare English-centric, but only if multiliguality is taken into account inhyperparameter tuning and with large enough IT data. English-trained LLMs arecapable of generating correct-language, comprehensive and helpful responses inthe other languages, but suffer from low factuality and may occasionally havefluency errors.</description><author>Nadezhda Chirkova, Vassilina Nikoulina</author><pubDate>Thu, 22 Feb 2024 18:37:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14778v1</guid></item><item><title>Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent Factor Models</title><link>http://arxiv.org/abs/2402.14777v1</link><description>We consider the task of causal imputation, where we aim to predict theoutcomes of some set of actions across a wide range of possible contexts. As arunning example, we consider predicting how different drugs affect cells fromdifferent cell types. We study the index-only setting, where the actions andcontexts are categorical variables with a finite number of possible values.Even in this simple setting, a practical challenge arises, since often only asmall subset of possible action-context pairs have been studied. Thus, modelsmust extrapolate to novel action-context pairs, which can be framed as a formof matrix completion with rows indexed by actions, columns indexed by contexts,and matrix entries corresponding to outcomes. We introduce a novel SCM-basedmodel class, where the outcome is expressed as a counterfactual, actions areexpressed as interventions on an instrumental variable, and contexts aredefined based on the initial state of the system. We show that, under alinearity assumption, this setup induces a latent factor model over the matrixof outcomes, with an additional fixed effect term. To perform causal predictionbased on this model class, we introduce simple extension to the SyntheticInterventions estimator (Agarwal et al., 2020). We evaluate several matrixcompletion approaches on the PRISM drug repurposing dataset, showing that ourmethod outperforms all other considered matrix completion approaches.</description><author>Alvaro Ribot, Chandler Squires, Caroline Uhler</author><pubDate>Thu, 22 Feb 2024 18:37:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14777v1</guid></item><item><title>2D Matryoshka Sentence Embeddings</title><link>http://arxiv.org/abs/2402.14776v1</link><description>Common approaches rely on fixed-length embedding vectors from language modelsas sentence embeddings for downstream tasks such as semantic textual similarity(STS). Such methods are limited in their flexibility due to unknowncomputational constraints and budgets across various applications. MatryoshkaRepresentation Learning (MRL) (Kusupati et al., 2022) encodes information atfiner granularities, i.e., with lower embedding dimensions, to adaptivelyaccommodate ad hoc tasks. Similar accuracy can be achieved with a smallerembedding size, leading to speedups in downstream tasks. Despite its improvedefficiency, MRL still requires traversing all Transformer layers beforeobtaining the embedding, which remains the dominant factor in time and memoryconsumption. This prompts consideration of whether the fixed number ofTransformer layers affects representation quality and whether usingintermediate layers for sentence representation is feasible. In this paper, weintroduce a novel sentence embedding model called Two-dimensional MatryoshkaSentence Embedding (2DMSE). It supports elastic settings for both embeddingsizes and Transformer layers, offering greater flexibility and efficiency thanMRL. We conduct extensive experiments on STS tasks and downstream applications.The experimental results demonstrate the effectiveness of our proposed model indynamically supporting different embedding sizes and Transformer layers,allowing it to be highly adaptable to various scenarios.</description><author>Xianming Li, Zongxi Li, Jing Li, Haoran Xie, Qing Li</author><pubDate>Thu, 22 Feb 2024 18:35:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14776v1</guid></item><item><title>EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries</title><link>http://arxiv.org/abs/2402.13372v2</link><description>While Large Language Models (LLMs) excel at the Winograd Schema Challenge(WSC), a coreference resolution task testing common-sense reasoning throughpronoun disambiguation, they struggle with instances that feature minoralterations or rewording. To address this, we introduce EvoGrad, an open-sourceplatform that harnesses a human-in-the-loop approach to create a dynamicdataset tailored to such altered WSC instances. Leveraging ChatGPT'scapabilities, we expand our task instances from 182 to 3,691, setting a newbenchmark for diverse common-sense reasoning datasets. Additionally, weintroduce the error depth metric, assessing model stability in dynamic tasks.Our results emphasize the challenge posed by EvoGrad: Even the best performingLLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2,a stark contrast to human performance of 92. 8% accuracy without perturbationerrors. This highlights ongoing model limitations and the value of dynamicdatasets in uncovering them.</description><author>Jing Han Sun, Ali Emami</author><pubDate>Thu, 22 Feb 2024 18:29:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13372v2</guid></item><item><title>DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models</title><link>http://arxiv.org/abs/2402.14767v1</link><description>We present DualFocus, a novel framework for integrating macro and microperspectives within multi-modal large language models (MLLMs) to enhancevision-language task performance. Current MLLMs typically singularly focus oninputs at a predefined resolution, resulting in deficiencies in detailedquestions involving local regions. We introduced a DualFocus mechanism wherethe model concentrates on the image from a macro perspective, responses to thequestion, and identifies suitable sub-regions to zoom in for subsequent microperspective analysis. Via the integration of answers from both macro and microperspectives, the model is adept at addressing tasks that encompass global,detailed, and combined considerations. To endows the DualFocus mechanism inMLLMs, we curated a tailored dataset derived from the Visual Genome (VG) andadapted it to align with the training regimen of DualFocus. Through comparativestudies across different model sizes and benchmarks, we demonstrate DualFocus'ssuperiority in balancing detailed examination with holistic insight,significantly reducing hallucination instances in MLLMs and improving theirperformance in various vision-language tasks.</description><author>Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, Jiaqi Wang</author><pubDate>Thu, 22 Feb 2024 18:26:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14767v1</guid></item><item><title>MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues</title><link>http://arxiv.org/abs/2402.14762v1</link><description>The advent of Large Language Models (LLMs) has drastically enhanced dialoguesystems. However, comprehensively evaluating the dialogue abilities of LLMsremains a challenge. Previous benchmarks have primarily focused on single-turndialogues or provided coarse-grained and incomplete assessments of multi-turndialogues, overlooking the complexity and fine-grained nuances of real-lifedialogues. To address this issue, we introduce MT-Bench-101, specificallydesigned to evaluate the fine-grained abilities of LLMs in multi-turndialogues. By conducting a detailed analysis of real multi-turn dialogue data,we construct a three-tier hierarchical ability taxonomy comprising 4208 turnsacross 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21popular LLMs based on MT-Bench-101, conducting comprehensive analyses from bothability and task perspectives and observing differing trends in LLMsperformance across dialogue turns within various tasks. Further analysisindicates that neither utilizing common alignment techniques nor chat-specificdesigns has led to obvious enhancements in the multi-turn abilities of LLMs.Extensive case studies suggest that our designed tasks accurately assess thecorresponding multi-turn abilities.</description><author>Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, Wanli Ouyang</author><pubDate>Thu, 22 Feb 2024 18:21:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14762v1</guid></item><item><title>Generalizing Reward Modeling for Out-of-Distribution Preference Learning</title><link>http://arxiv.org/abs/2402.14760v1</link><description>Preference learning (PL) with large language models (LLMs) aims to align theLLMs' generations with human preferences. Previous work on reinforcementlearning from human feedback (RLHF) has demonstrated promising results inin-distribution PL. However, due to the difficulty of obtaining human feedback,discretely training reward models for every encountered distribution ischallenging. Thus, out-of-distribution (OOD) PL is practically useful forenhancing the generalization ability of LLMs with limited preference feedback.This work addresses OOD PL by optimizing a general reward model through ameta-learning approach. During meta-training, a bilevel optimization algorithmis utilized to learn a reward model capable of guiding policy learning to alignwith human preferences across various distributions. When encountering a testdistribution, the meta-test procedure conducts regularized policy optimizationusing the learned reward model for PL. We theoretically demonstrate theconvergence rate of the bilevel optimization algorithm under reasonableassumptions. Additionally, we conduct experiments on two text generation tasksacross 20 held-out domains and outperform a variety of strong baselines acrossvarious evaluation metrics.</description><author>Chen Jia</author><pubDate>Thu, 22 Feb 2024 18:20:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14760v1</guid></item><item><title>Generalising realisability in statistical learning theory under epistemic uncertainty</title><link>http://arxiv.org/abs/2402.14759v1</link><description>The purpose of this paper is to look into how central notions in statisticallearning theory, such as realisability, generalise under the assumption thattrain and test distribution are issued from the same credal set, i.e., a convexset of probability distributions. This can be considered as a first steptowards a more general treatment of statistical learning under epistemicuncertainty.</description><author>Fabio Cuzzolin</author><pubDate>Thu, 22 Feb 2024 18:20:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14759v1</guid></item><item><title>Batch and match: black-box variational inference with a score-based divergence</title><link>http://arxiv.org/abs/2402.14758v1</link><description>Most leading implementations of black-box variational inference (BBVI) arebased on optimizing a stochastic evidence lower bound (ELBO). But suchapproaches to BBVI often converge slowly due to the high variance of theirgradient estimates. In this work, we propose batch and match (BaM), analternative approach to BBVI based on a score-based divergence. Notably, thisscore-based divergence can be optimized by a closed-form proximal update forGaussian variational families with full covariance matrices. We analyze theconvergence of BaM when the target distribution is Gaussian, and we prove thatin the limit of infinite batch size the variational parameter updates convergeexponentially quickly to the target mean and covariance. We also evaluate theperformance of BaM on Gaussian and non-Gaussian target distributions that arisefrom posterior inference in hierarchical and deep generative models. In theseexperiments, we find that BaM typically converges in fewer (and sometimessignificantly fewer) gradient evaluations than leading implementations of BBVIbased on ELBO maximization.</description><author>Diana Cai, Chirag Modi, Loucas Pillaud-Vivien, Charles C. Margossian, Robert M. Gower, David M. Blei, Lawrence K. Saul</author><pubDate>Thu, 22 Feb 2024 18:20:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14758v1</guid></item><item><title>SHM-Traffic: DRL and Transfer learning based UAV Control for Structural Health Monitoring of Bridges with Traffic</title><link>http://arxiv.org/abs/2402.14757v1</link><description>This work focuses on using advanced techniques for structural healthmonitoring (SHM) for bridges with Traffic. We propose an approach using deepreinforcement learning (DRL)-based control for Unmanned Aerial Vehicle (UAV).Our approach conducts a concrete bridge deck survey while traffic is ongoingand detects cracks. The UAV performs the crack detection, and the location ofcracks is initially unknown. We use two edge detection techniques. First, weuse canny edge detection for crack detection. We also use a ConvolutionalNeural Network (CNN) for crack detection and compare it with canny edgedetection. Transfer learning is applied using CNN with pre-trained weightsobtained from a crack image dataset. This enables the model to adapt andimprove its performance in identifying and localizing cracks. Proximal PolicyOptimization (PPO) is applied for UAV control and bridge surveys. Theexperimentation across various scenarios is performed to evaluate theperformance of the proposed methodology. Key metrics such as task completiontime and reward convergence are observed to gauge the effectiveness of theapproach. We observe that the Canny edge detector offers up to 40\% lower taskcompletion time, while the CNN excels in up to 12\% better damage detection and1.8 times better rewards.</description><author>Divija Swetha Gadiraju, Saeed Eftekhar Azam, Deepak Khazanchi</author><pubDate>Thu, 22 Feb 2024 18:19:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14757v1</guid></item><item><title>Prompting a Pretrained Transformer Can Be a Universal Approximator</title><link>http://arxiv.org/abs/2402.14753v1</link><description>Despite the widespread adoption of prompting, prompt tuning and prefix-tuningof transformer models, our theoretical understanding of these fine-tuningmethods remains limited. A key question is whether one can arbitrarily modifythe behavior of pretrained model by prompting or prefix-tuning it. Formally,whether prompting and prefix-tuning a pretrained model can universallyapproximate sequence-to-sequence functions. This paper answers in theaffirmative and demonstrates that much smaller pretrained models thanpreviously thought can be universal approximators when prefixed. In fact, theattention mechanism is uniquely suited for universal approximation withprefix-tuning a single attention head being sufficient to approximate anycontinuous function. Moreover, any sequence-to-sequence function can beapproximated by prefixing a transformer with depth linear in the sequencelength. Beyond these density-type results, we also offer Jackson-type bounds onthe length of the prefix needed to approximate a function to a desiredprecision.</description><author>Aleksandar Petrov, Philip H. S. Torr, Adel Bibi</author><pubDate>Thu, 22 Feb 2024 18:12:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14753v1</guid></item><item><title>Scaling Efficient LLMs</title><link>http://arxiv.org/abs/2402.14746v1</link><description>Trained LLMs are typically sparse in that most of the parameters are zero,raising questions on efficiency. In response, we inquire into efficient LLMs,i.e. those with the fewest parameters that achieve the desired accuracy on atraining corpus. Specifically, we compare theoretical and empirical estimatesfor training loss at current scale to obtain upper and lower bounds on thenumber of unique sequences in a natural training corpus as a function of itssize. Our result implies (1) to double the number of skills represented in atraining corpus, the corpus must scale roughly between three and five fold (2)for efficient LLMs, the number of parameters $N$ and the size $D$ of a naturaltraining corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters ofan LLM is smaller than the number of unique sequences in the training corpus,scaling up can uncover emergent skills.</description><author>B. N. Kausik</author><pubDate>Thu, 22 Feb 2024 18:06:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14746v1</guid></item><item><title>Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation</title><link>http://arxiv.org/abs/2402.14744v1</link><description>This paper introduces a novel approach using Large Language Models (LLMs)integrated into an agent framework for flexible and efficient personal mobilitygeneration. LLMs overcome the limitations of previous models by efficientlyprocessing semantic data and offering versatility in modeling various tasks.Our approach addresses the critical need to align LLMs with real-world urbanmobility data, focusing on three research questions: aligning LLMs with richactivity data, developing reliable activity generation strategies, andexploring LLM applications in urban mobility. The key technical contribution isa novel LLM agent framework that accounts for individual activity patterns andmotivations, including a self-consistency approach to align LLMs withreal-world activity data and a retrieval-augmented strategy for interpretableactivity generation. In experimental studies, comprehensive validation isperformed using real-world data. This research marks the pioneering work ofdesigning an LLM agent framework for activity generation based on real-worldhuman activity data, offering a promising tool for urban mobility analysis.</description><author>Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke Shibasaki, Chuan Xiao</author><pubDate>Thu, 22 Feb 2024 18:03:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14744v1</guid></item><item><title>Dependency Annotation of Ottoman Turkish with Multilingual BERT</title><link>http://arxiv.org/abs/2402.14743v1</link><description>This study introduces a pretrained large language model-based annotationmethodology for the first dependency treebank in Ottoman Turkish. Ourexperimental results show that, iteratively, i) pseudo-annotating data using amultilingual BERT-based parsing model, ii) manually correcting thepseudo-annotations, and iii) fine-tuning the parsing model with the correctedannotations, we speed up and simplify the challenging dependency annotationprocess. The resulting treebank, that will be a part of the UniversalDependencies (UD) project, will facilitate automated analysis of OttomanTurkish documents, unlocking the linguistic richness embedded in thishistorical heritage.</description><author>Şaziye Betül Özateş, Tarık Emre Tıraş, Efe Eren Genç, Esma Fatıma Bilgin Taşdemir</author><pubDate>Thu, 22 Feb 2024 17:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14743v1</guid></item><item><title>Zero-Shot Pediatric Tuberculosis Detection in Chest X-Rays using Self-Supervised Learning</title><link>http://arxiv.org/abs/2402.14741v1</link><description>Tuberculosis (TB) remains a significant global health challenge, withpediatric cases posing a major concern. The World Health Organization (WHO)advocates for chest X-rays (CXRs) for TB screening. However, visualinterpretation by radiologists can be subjective, time-consuming and prone toerror, especially in pediatric TB. Artificial intelligence (AI)-drivencomputer-aided detection (CAD) tools, especially those utilizing deep learning,show promise in enhancing lung disease detection. However, challenges includedata scarcity and lack of generalizability. In this context, we propose a novelself-supervised paradigm leveraging Vision Transformers (ViT) for improved TBdetection in CXR, enabling zero-shot pediatric TB detection. We demonstrateimprovements in TB detection performance ($\sim$12.7% and $\sim$13.4% topAUC/AUPR gains in adults and children, respectively) when conductingself-supervised pre-training when compared to fully-supervised (i.e., nonpre-trained) ViT models, achieving top performances of 0.959 AUC and 0.962 AUPRin adult TB detection, and 0.697 AUC and 0.607 AUPR in zero-shot pediatric TBdetection. As a result, this work demonstrates that self-supervised learning onadult CXRs effectively extends to challenging downstream tasks such aspediatric TB detection, where data are scarce.</description><author>Daniel Capellán-Martín, Abhijeet Parida, Juan J. Gómez-Valverde, Ramon Sanchez-Jacob, Pooneh Roshanitabrizi, Marius G. Linguraru, María J. Ledesma-Carbayo, Syed M. Anwar</author><pubDate>Thu, 22 Feb 2024 17:55:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14741v1</guid></item><item><title>Uncertainty Quantification of Spatiotemporal Travel Demand with Probabilistic Graph Neural Networks</title><link>http://arxiv.org/abs/2303.04040v2</link><description>Recent studies have significantly improved the prediction accuracy of traveldemand using graph neural networks. However, these studies largely ignoreduncertainty that inevitably exists in travel demand prediction. To fill thisgap, this study proposes a framework of probabilistic graph neural networks(Prob-GNN) to quantify the spatiotemporal uncertainty of travel demand. ThisProb-GNN framework is substantiated by deterministic and probabilisticassumptions, and empirically applied to the task of predicting the transit andridesharing demand in Chicago. We found that the probabilistic assumptions(e.g. distribution tail, support) have a greater impact on uncertaintyprediction than the deterministic ones (e.g. deep modules, depth). Among thefamily of Prob-GNNs, the GNNs with truncated Gaussian and Laplace distributionsachieve the highest performance in transit and ridesharing data. Even undersignificant domain shifts, Prob-GNNs can predict the ridership uncertainty in astable manner, when the models are trained on pre-COVID data and tested acrossmultiple periods during and after the COVID-19 pandemic. Prob-GNNs also revealthe spatiotemporal pattern of uncertainty, which is concentrated on theafternoon peak hours and the areas with large travel volumes. Overall, ourfindings highlight the importance of incorporating randomness into deeplearning for spatiotemporal ridership prediction. Future research shouldcontinue to investigate versatile probabilistic assumptions to capturebehavioral randomness, and further develop methods to quantify uncertainty tobuild resilient cities.</description><author>Qingyi Wang, Shenhao Wang, Dingyi Zhuang, Haris Koutsopoulos, Jinhua Zhao</author><pubDate>Thu, 22 Feb 2024 17:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04040v2</guid></item><item><title>Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs</title><link>http://arxiv.org/abs/2402.14740v1</link><description>AI alignment in the shape of Reinforcement Learning from Human Feedback(RLHF) is increasingly treated as a crucial ingredient for high performancelarge language models. \textsc{Proximal Policy Optimization} (PPO) has beenpositioned by recent literature as the canonical method for the RL part ofRLHF. However, it involves both high computational cost and sensitivehyperparameter tuning. We posit that most of the motivational principles thatled to the development of PPO are less of a practical concern in RLHF andadvocate for a less computationally expensive method that preserves and evenincreases performance. We revisit the \textit{formulation} of alignment fromhuman preferences in the context of RL. Keeping simplicity as a guidingprinciple, we show that many components of PPO are unnecessary in an RLHFcontext and that far simpler REINFORCE-style optimization variants outperformboth PPO and newly proposed "RL-free" methods such as DPO and RAFT. Our worksuggests that careful adaptation to LLMs alignment characteristics enablesbenefiting from online RL optimization at low cost.</description><author>Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Ahmet Üstün, Sara Hooker</author><pubDate>Thu, 22 Feb 2024 17:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14740v1</guid></item><item><title>Dissenting Explanations: Leveraging Disagreement to Reduce Model Overreliance</title><link>http://arxiv.org/abs/2307.07636v2</link><description>While explainability is a desirable characteristic of increasingly complexblack-box models, modern explanation methods have been shown to be inconsistentand contradictory. The semantics of explanations is not always fully understood- to what extent do explanations "explain" a decision and to what extent dothey merely advocate for a decision? Can we help humans gain insights fromexplanations accompanying correct predictions and not over-rely on incorrectpredictions advocated for by explanations? With this perspective in mind, weintroduce the notion of dissenting explanations: conflicting predictions withaccompanying explanations. We first explore the advantage of dissentingexplanations in the setting of model multiplicity, where multiple models withsimilar performance may have different predictions. In such cases, providingdissenting explanations could be done by invoking the explanations ofdisagreeing models. Through a pilot study, we demonstrate that dissentingexplanations reduce overreliance on model predictions, without reducing overallaccuracy. Motivated by the utility of dissenting explanations we present bothglobal and local methods for their generation.</description><author>Omer Reingold, Judy Hanwen Shen, Aditi Talati</author><pubDate>Thu, 22 Feb 2024 17:47:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07636v2</guid></item><item><title>How Transformers Learn Causal Structure with Gradient Descent</title><link>http://arxiv.org/abs/2402.14735v1</link><description>The incredible success of transformers on sequence modeling tasks can belargely attributed to the self-attention mechanism, which allows information tobe transferred between different parts of a sequence. Self-attention allowstransformers to encode causal structure which makes them particularly suitablefor sequence modeling. However, the process by which transformers learn suchcausal structure via gradient-based training algorithms remains poorlyunderstood. To better understand this process, we introduce an in-contextlearning task that requires learning latent causal structure. We prove thatgradient descent on a simplified two-layer transformer learns to solve thistask by encoding the latent causal graph in the first attention layer. The keyinsight of our proof is that the gradient of the attention matrix encodes themutual information between tokens. As a consequence of the data processinginequality, the largest entries of this gradient correspond to edges in thelatent causal graph. As a special case, when the sequences are generated fromin-context Markov chains, we prove that transformers learn an induction head(Olsson et al., 2022). We confirm our theoretical findings by showing thattransformers trained on our in-context learning task are able to recover a widevariety of causal structures.</description><author>Eshaan Nichani, Alex Damian, Jason D. Lee</author><pubDate>Thu, 22 Feb 2024 17:47:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14735v1</guid></item><item><title>Deep hybrid model with satellite imagery: how to combine demand modeling and computer vision for behavior analysis?</title><link>http://arxiv.org/abs/2303.04204v2</link><description>Classical demand modeling analyzes travel behavior using only low-dimensionalnumeric data (i.e. sociodemographics and travel attributes) but nothigh-dimensional urban imagery. However, travel behavior depends on the factorsrepresented by both numeric data and urban imagery, thus necessitating asynergetic framework to combine them. This study creates a theoreticalframework of deep hybrid models with a crossing structure consisting of amixing operator and a behavioral predictor, thus integrating the numeric andimagery data into a latent space. Empirically, this framework is applied toanalyze travel mode choice using the MyDailyTravel Survey from Chicago as thenumeric inputs and the satellite images as the imagery inputs. We found thatdeep hybrid models outperform both the traditional demand models and the recentdeep learning in predicting the aggregate and disaggregate travel behavior withour supervision-as-mixing design. The latent space in deep hybrid models can beinterpreted, because it reveals meaningful spatial and social patterns. Thedeep hybrid models can also generate new urban images that do not exist inreality and interpret them with economic theory, such as computing substitutionpatterns and social welfare changes. Overall, the deep hybrid modelsdemonstrate the complementarity between the low-dimensional numeric andhigh-dimensional imagery data and between the traditional demand modeling andrecent deep learning. It generalizes the latent classes and variables inclassical hybrid demand models to a latent space, and leverages thecomputational power of deep learning for imagery while retaining the economicinterpretability on the microeconomics foundation.</description><author>Qingyi Wang, Shenhao Wang, Yunhan Zheng, Hongzhou Lin, Xiaohu Zhang, Jinhua Zhao, Joan Walker</author><pubDate>Thu, 22 Feb 2024 17:46:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04204v2</guid></item><item><title>Clifford-Steerable Convolutional Neural Networks</title><link>http://arxiv.org/abs/2402.14730v1</link><description>We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), anovel class of $\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivectorfields on pseudo-Euclidean spaces $\mathbb{R}^{p,q}$. They cover, for instance,$\mathrm{E}(3)$-equivariance on $\mathbb{R}^3$ and Poincar\'e-equivariance onMinkowski spacetime $\mathbb{R}^{1,3}$. Our approach is based on an implicitparametrization of $\mathrm{O}(p,q)$-steerable kernels via Clifford groupequivariant neural networks. We significantly and consistently outperformbaseline methods on fluid dynamics as well as relativistic electrodynamicsforecasting tasks.</description><author>Maksim Zhdanov, David Ruhe, Maurice Weiler, Ana Lucic, Johannes Brandstetter, Patrick Forré</author><pubDate>Thu, 22 Feb 2024 17:42:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14730v1</guid></item><item><title>Physics-informed deep-learning applications to experimental fluid mechanics</title><link>http://arxiv.org/abs/2203.15402v2</link><description>High-resolution reconstruction of flow-field data from low-resolution andnoisy measurements is of interest due to the prevalence of such problems inexperimental fluid mechanics, where the measurement data are in general sparse,incomplete and noisy. Deep-learning approaches have been shown suitable forsuch super-resolution tasks. However, a high number of high-resolution examplesis needed, which may not be available for many cases. Moreover, the obtainedpredictions may lack in complying with the physical principles, e.g. mass andmomentum conservation. Physics-informed deep learning provides frameworks forintegrating data and physical laws for learning. In this study, we applyphysics-informed neural networks (PINNs) for super-resolution of flow-fielddata both in time and space from a limited set of noisy measurements withouthaving any high-resolution reference data. Our objective is to obtain acontinuous solution of the problem, providing a physically-consistentprediction at any point in the solution domain. We demonstrate theapplicability of PINNs for the super-resolution of flow-field data in time andspace through three canonical cases: Burgers' equation, two-dimensional vortexshedding behind a circular cylinder and the minimal turbulent channel flow. Therobustness of the models is also investigated by adding synthetic Gaussiannoise. Furthermore, we show the capabilities of PINNs to improve the resolutionand reduce the noise in a real experimental dataset consisting ofhot-wire-anemometry measurements. Our results show the adequate capabilities ofPINNs in the context of data augmentation for experiments in fluid mechanics.</description><author>Hamidreza Eivazi, Yuning Wang, Ricardo Vinuesa</author><pubDate>Thu, 22 Feb 2024 17:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.15402v2</guid></item><item><title>The European Commitment to Human-Centered Technology: The Integral Role of HCI in the EU AI Act's Success</title><link>http://arxiv.org/abs/2402.14728v1</link><description>The evolution of AI is set to profoundly reshape the future. The EuropeanUnion, recognizing this impending prominence, has enacted the AI Act,regulating market access for AI-based systems. A salient feature of the Act isto guard democratic and humanistic values by focusing regulation ontransparency, explainability, and the human ability to understand and controlAI systems. Hereby, the EU AI Act does not merely specify technologicalrequirements for AI systems. The EU issues a democratic call for human-centeredAI systems and, in turn, an interdisciplinary research agenda forhuman-centered innovation in AI development. Without robust methods to assessAI systems and their effect on individuals and society, the EU AI Act may leadto repeating the mistakes of the General Data Protection Regulation of the EUand to rushed, chaotic, ad-hoc, and ambiguous implementation, causing moreconfusion than lending guidance. Moreover, determined research activities inHuman-AI interaction will be pivotal for both regulatory compliance and theadvancement of AI in a manner that is both ethical and effective. Such anapproach will ensure that AI development aligns with human values and needs,fostering a technology landscape that is innovative, responsible, and anintegral part of our society.</description><author>André Calero Valdez, Moreen Heine, Thomas Franke, Nicole Jochems, Hans-Christian Jetter, Tim Schrills</author><pubDate>Thu, 22 Feb 2024 17:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14728v1</guid></item><item><title>Incorporating Expert Rules into Neural Networks in the Framework of Concept-Based Learning</title><link>http://arxiv.org/abs/2402.14726v1</link><description>A problem of incorporating the expert rules into machine learning models forextending the concept-based learning is formulated in the paper. It is proposedhow to combine logical rules and neural networks predicting the conceptprobabilities. The first idea behind the combination is to form constraints fora joint probability distribution over all combinations of concept values tosatisfy the expert rules. The second idea is to represent a feasible set ofprobability distributions in the form of a convex polytope and to use itsvertices or faces. We provide several approaches for solving the stated problemand for training neural networks which guarantee that the output probabilitiesof concepts would not violate the expert rules. The solution of the problem canbe viewed as a way for combining the inductive and deductive learning. Expertrules are used in a broader sense when any logical function that connectsconcepts and class labels or just concepts with each other can be regarded as arule. This feature significantly expands the class of the proposed results.Numerical examples illustrate the approaches. The code of proposed algorithmsis publicly available.</description><author>Andrei V. Konstantinov, Lev V. Utkin</author><pubDate>Thu, 22 Feb 2024 17:33:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14726v1</guid></item><item><title>TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview</title><link>http://arxiv.org/abs/2401.01330v2</link><description>Conversational Information Seeking has evolved rapidly in the last few yearswith the development of Large Language Models providing the basis forinterpreting and responding in a naturalistic manner to user requests. iKATemphasizes the creation and research of conversational search agents that adaptresponses based on the user's prior interactions and present context. Thismeans that the same question might yield varied answers, contingent on theuser's profile and preferences. The challenge lies in enabling ConversationalSearch Agents (CSA) to incorporate personalized context to effectively guideusers through the relevant information to them. iKAT's first year attractedseven teams and a total of 24 runs. Most of the runs leveraged Large LanguageModels (LLMs) in their pipelines, with a few focusing on agenerate-then-retrieve approach.</description><author>Mohammad Aliannejadi, Zahra Abbasiantaeb, Shubham Chatterjee, Jeffery Dalton, Leif Azzopardi</author><pubDate>Thu, 22 Feb 2024 17:29:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01330v2</guid></item><item><title>Measuring Moral Inconsistencies in Large Language Models</title><link>http://arxiv.org/abs/2402.01719v2</link><description>A Large Language Model (LLM) is considered consistent if semanticallyequivalent prompts produce semantically equivalent responses. Despite recentadvancements showcasing the impressive capabilities of LLMs in conversationalsystems, we show that even state-of-the-art LLMs are highly inconsistent intheir generations, questioning their reliability. Prior research has tried tomeasure this with task-specific accuracy. However, this approach is unsuitablefor moral scenarios, such as the trolley problem, with no "correct" answer. Toaddress this issue, we propose a novel information-theoretic measure calledSemantic Graph Entropy (SGE) to measure the consistency of an LLM in moralscenarios. We leverage "Rules of Thumb" (RoTs) to explain a model'sdecision-making strategies and further enhance our metric. Compared to existingconsistency metrics, SGE correlates better with human judgments across fiveLLMs. In the future, we aim to investigate the root causes of LLMinconsistencies and propose improvements.</description><author>Vamshi Krishna Bonagiri, Sreeram Vennam, Manas Gaur, Ponnurangam Kumaraguru</author><pubDate>Thu, 22 Feb 2024 17:25:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01719v2</guid></item><item><title>A Transformer Model for Boundary Detection in Continuous Sign Language</title><link>http://arxiv.org/abs/2402.14720v1</link><description>Sign Language Recognition (SLR) has garnered significant attention fromresearchers in recent years, particularly the intricate domain of ContinuousSign Language Recognition (CSLR), which presents heightened complexity comparedto Isolated Sign Language Recognition (ISLR). One of the prominent challengesin CSLR pertains to accurately detecting the boundaries of isolated signswithin a continuous video stream. Additionally, the reliance on handcraftedfeatures in existing models poses a challenge to achieving optimal accuracy. Tosurmount these challenges, we propose a novel approach utilizing aTransformer-based model. Unlike traditional models, our approach focuses onenhancing accuracy while eliminating the need for handcrafted features. TheTransformer model is employed for both ISLR and CSLR. The training processinvolves using isolated sign videos, where hand keypoint features extractedfrom the input video are enriched using the Transformer model. Subsequently,these enriched features are forwarded to the final classification layer. Thetrained model, coupled with a post-processing method, is then applied to detectisolated sign boundaries within continuous sign videos. The evaluation of ourmodel is conducted on two distinct datasets, including both continuous signsand their corresponding isolated signs, demonstrates promising results.</description><author>Razieh Rastgoo, Kourosh Kiani, Sergio Escalera</author><pubDate>Thu, 22 Feb 2024 17:25:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14720v1</guid></item><item><title>Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models</title><link>http://arxiv.org/abs/2402.14714v1</link><description>This report introduces \texttt{EEVE-Korean-v1.0}, a Korean adaptation oflarge language models that exhibit remarkable capabilities across English andKorean text understanding. Building on recent highly capable butEnglish-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English textsare inefficiently processed with English-centric tokenizers, we present anefficient and effective vocabulary expansion (EEVE) method, which encompassesparameter freezing and subword initialization. In contrast to previous effortsthat believe new embeddings require trillions of training tokens, we show thatour method can significantly boost non-English proficiency within just 2billion tokens. Surpassing most instruction-tuned LLMs on the Open Ko-LLMLeaderboard, as of January 2024, our model \texttt{EEVE-Korean-10.8B-v1.0}ranks as the leading Korean pre-trained model in the open-source community,according to Hugging Face's leaderboard. We open-source our models onHuggingface to empower the open research community in various languages.</description><author>Seungduk Kim, Seungtaek Choi, Myeongho Jeong</author><pubDate>Thu, 22 Feb 2024 17:12:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14714v1</guid></item><item><title>IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus</title><link>http://arxiv.org/abs/2402.14710v1</link><description>Large Language Models (LLMs) demonstrate remarkable potential across variousdomains; however, they exhibit a significant performance gap in InformationExtraction (IE). Note that high-quality instruction data is the vital key forenhancing the specific capabilities of LLMs, while current IE datasets tend tobe small in scale, fragmented, and lack standardized schema. To this end, weintroduce IEPile, a comprehensive bilingual (English and Chinese) IEinstruction corpus, which contains approximately 0.32B tokens. We constructIEPile by collecting and cleaning 33 existing IE datasets, and introduceschema-based instruction generation to unearth a large-scale corpus.Experimental results on LLaMA and Baichuan demonstrate that using IEPile canenhance the performance of LLMs for IE, especially the zero-shotgeneralization. We open-source the resource and pre-trained models, hoping toprovide valuable support to the NLP community.</description><author>Honghao Gui, Hongbin Ye, Lin Yuan, Ningyu Zhang, Mengshu Sun, Lei Liang, Huajun Chen</author><pubDate>Thu, 22 Feb 2024 17:11:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14710v1</guid></item><item><title>CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks</title><link>http://arxiv.org/abs/2402.14708v1</link><description>Credit card fraud poses a significant threat to the economy. While GraphNeural Network (GNN)-based fraud detection methods perform well, they oftenoverlook the causal effect of a node's local structure on predictions. Thispaper introduces a novel method for credit card fraud detection, the\textbf{\underline{Ca}}usal \textbf{\underline{T}}emporal\textbf{\underline{G}}raph \textbf{\underline{N}}eural \textbf{N}etwork(CaT-GNN), which leverages causal invariant learning to reveal inherentcorrelations within transaction data. By decomposing the problem into discoveryand intervention phases, CaT-GNN identifies causal nodes within the transactiongraph and applies a causal mixup strategy to enhance the model's robustness andinterpretability. CaT-GNN consists of two key components: Causal-Inspector andCausal-Intervener. The Causal-Inspector utilizes attention weights in thetemporal attention mechanism to identify causal and environment nodes withoutintroducing additional parameters. Subsequently, the Causal-Intervener performsa causal mixup enhancement on environment nodes based on the set of nodes.Evaluated on three datasets, including a private financial dataset and twopublic datasets, CaT-GNN demonstrates superior performance over existingstate-of-the-art methods. Our findings highlight the potential of integratingcausal reasoning with graph neural networks to improve fraud detectioncapabilities in financial transactions.</description><author>Yifan Duan, Guibin Zhang, Shilong Wang, Xiaojiang Peng, Wang Ziqi, Junyuan Mao, Hao Wu, Xinke Jiang, Kun Wang</author><pubDate>Thu, 22 Feb 2024 17:08:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14708v1</guid></item><item><title>Interpreting Shared Circuits for Ordered Sequence Prediction in a Large Language Model</title><link>http://arxiv.org/abs/2311.04131v3</link><description>While transformer models exhibit strong capabilities on linguistic tasks,their complex architectures make them difficult to interpret. Recent work hasaimed to reverse engineer transformer models into human-readablerepresentations called circuits that implement algorithmic functions. We extendthis research by analyzing and comparing circuits for similar sequencecontinuation tasks, which include increasing sequences of digits, number words,and months. Through the application of circuit analysis techniques, we identifykey sub-circuits responsible for detecting sequence members and for predictingthe next member in a sequence. Our analysis reveals that semantically relatedsequences rely on shared circuit subgraphs with analogous roles. Overall,documenting shared computational structures enables better prediction of modelbehaviors, identification of errors, and safer editing procedures. Thismechanistic understanding of transformers is a critical step towards buildingmore robust, aligned, and interpretable language models.</description><author>Michael Lan, Fazl Barez</author><pubDate>Thu, 22 Feb 2024 17:07:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04131v3</guid></item><item><title>Two-stage Cytopathological Image Synthesis for Augmenting Cervical Abnormality Screening</title><link>http://arxiv.org/abs/2402.14707v1</link><description>Automatic thin-prep cytologic test (TCT) screening can assist pathologists infinding cervical abnormality towards accurate and efficient cervical cancerdiagnosis. Current automatic TCT screening systems mostly involve abnormalcervical cell detection, which generally requires large-scale and diversetraining data with high-quality annotations to achieve promising performance.Pathological image synthesis is naturally raised to minimize the efforts indata collection and annotation. However, it is challenging to generaterealistic large-size cytopathological images while simultaneously synthesizingvisually plausible appearances for small-size abnormal cervical cells. In thispaper, we propose a two-stage image synthesis framework to create syntheticdata for augmenting cervical abnormality screening. In the first Global ImageGeneration stage, a Normal Image Generator is designed to generatecytopathological images full of normal cervical cells. In the second Local CellEditing stage, normal cells are randomly selected from the generated images andthen are converted to different types of abnormal cells using the proposedAbnormal Cell Synthesizer. Both Normal Image Generator and Abnormal CellSynthesizer are built upon the pre-trained Stable Diffusion viaparameter-efficient fine-tuning methods for customizing cytopathological imagecontents and extending spatial layout controllability, respectively. Ourexperiments demonstrate the synthetic image quality, diversity, andcontrollability of the proposed synthesis framework, and validate its dataaugmentation effectiveness in enhancing the performance of abnormal cervicalcell detection.</description><author>Zhenrong Shen, Manman Fei, Xin Wang, Jiangdong Cai, Sheng Wang, Lichi Zhang, Qian Wang</author><pubDate>Thu, 22 Feb 2024 17:06:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14707v1</guid></item><item><title>An LLM-Enhanced Adversarial Editing System for Lexical Simplification</title><link>http://arxiv.org/abs/2402.14704v1</link><description>Lexical Simplification (LS) aims to simplify text at the lexical level.Existing methods rely heavily on annotated data, making it challenging to applyin low-resource scenarios. In this paper, we propose a novel LS method withoutparallel corpora. This method employs an Adversarial Editing System withguidance from a confusion loss and an invariance loss to predict lexical editsin the original sentences. Meanwhile, we introduce an innovative LLM-enhancedloss to enable the distillation of knowledge from Large Language Models (LLMs)into a small-size LS system. From that, complex words within sentences aremasked and a Difficulty-aware Filling module is crafted to replace maskedpositions with simpler words. At last, extensive experimental results andanalyses on three benchmark LS datasets demonstrate the effectiveness of ourproposed method.</description><author>Keren Tan, Kangyang Luo, Yunshi Lan, Zheng Yuan, Jinlong Shu</author><pubDate>Thu, 22 Feb 2024 17:04:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14704v1</guid></item><item><title>Mitigating Gender Bias in Face Recognition Using the von Mises-Fisher Mixture Model</title><link>http://arxiv.org/abs/2210.13664v3</link><description>In spite of the high performance and reliability of deep learning algorithmsin a wide range of everyday applications, many investigations tend to show thata lot of models exhibit biases, discriminating against specific subgroups ofthe population (e.g. gender, ethnicity). This urges the practitioner to developfair systems with a uniform/comparable performance across sensitive groups. Inthis work, we investigate the gender bias of deep Face Recognition networks. Inorder to measure this bias, we introduce two new metrics, $\mathrm{BFAR}$ and$\mathrm{BFRR}$, that better reflect the inherent deployment needs of FaceRecognition systems. Motivated by geometric considerations, we mitigate genderbias through a new post-processing methodology which transforms the deepembeddings of a pre-trained model to give more representation power todiscriminated subgroups. It consists in training a shallow neural network byminimizing a Fair von Mises-Fisher loss whose hyperparameters account for theintra-class variance of each gender. Interestingly, we empirically observe thatthese hyperparameters are correlated with our fairness metrics. In fact,extensive numerical experiments on a variety of datasets show that a carefulselection significantly reduces gender bias. The code used for the experimentscan be found at https://github.com/JRConti/EthicalModule_vMF.</description><author>Jean-Rémy Conti, Nathan Noiry, Vincent Despiegel, Stéphane Gentric, Stéphan Clémençon</author><pubDate>Thu, 22 Feb 2024 17:01:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.13664v3</guid></item><item><title>On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation</title><link>http://arxiv.org/abs/2402.14703v1</link><description>We study off-policy evaluation (OPE) in partially observable environmentswith complex observations, with the goal of developing estimators whoseguarantee avoids exponential dependence on the horizon. While such estimatorsexist for MDPs and POMDPs can be converted to history-based MDPs, theirestimation errors depend on the state-density ratio for MDPs which becomeshistory ratios after conversion, an exponential object. Recently, Uehara et al.(2022) proposed future-dependent value functions as a promising framework toaddress this issue, where the guarantee for memoryless policies depends on thedensity ratio over the latent state space. However, it also depends on theboundedness of the future-dependent value function and other relatedquantities, which we show could be exponential-in-length and thus erasing theadvantage of the method. In this paper, we discover novel coverage assumptionstailored to the structure of POMDPs, such as outcome coverage and beliefcoverage. These assumptions not only enable polynomial bounds on theaforementioned quantities, but also lead to the discovery of new algorithmswith complementary properties.</description><author>Yuheng Zhang, Nan Jiang</author><pubDate>Thu, 22 Feb 2024 17:00:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14703v1</guid></item><item><title>PixT3: Pixel-based Table To Text generation</title><link>http://arxiv.org/abs/2311.09808v2</link><description>Table-to-text generation involves generating appropriate textual descriptionsgiven structured tabular data. It has attracted increasing attention in recentyears thanks to the popularity of neural network models and the availability oflarge-scale datasets. A common feature across existing methods is theirtreatment of the input as a string, i.e., by employing linearization techniquesthat do not always preserve information in the table, are verbose, and lackspace efficiency. We propose to rethink data-to-text generation as a visualrecognition task, removing the need for rendering the input in a string format.We present PixT3, a multimodal table-to-text model that overcomes thechallenges of linearization and input size limitations encountered by existingmodels. PixT3 is trained with a new self-supervised learning objective toreinforce table structure awareness and is applicable to open-ended andcontrolled generation settings. Experiments on the ToTTo and Logic2Textbenchmarks show that PixT3 is competitive and, in some settings, superior togenerators that operate solely on text.</description><author>Iñigo Alonso, Eneko Agirre, Mirella Lapata</author><pubDate>Thu, 22 Feb 2024 16:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09808v2</guid></item><item><title>InfFeed: Influence Functions as a Feedback to Improve the Performance of Subjective Tasks</title><link>http://arxiv.org/abs/2402.14702v1</link><description>Recently, influence functions present an apparatus for achievingexplainability for deep neural models by quantifying the perturbation ofindividual train instances that might impact a test prediction. Our objectivesin this paper are twofold. First we incorporate influence functions as afeedback into the model to improve its performance. Second, in a datasetextension exercise, using influence functions to automatically identify datapoints that have been initially `silver' annotated by some existing method andneed to be cross-checked (and corrected) by annotators to improve the modelperformance. To meet these objectives, in this paper, we introduce InfFeed,which uses influence functions to compute the influential instances for atarget instance. Toward the first objective, we adjust the label of the targetinstance based on its influencer(s) label. In doing this, InfFeed outperformsthe state-of-the-art baselines (including LLMs) by a maximum macro F1-scoremargin of almost 4% for hate speech classification, 3.5% for stanceclassification, and 3% for irony and 2% for sarcasm detection. Toward thesecond objective we show that manually re-annotating only those silverannotated data points in the extension set that have a negative influence canimmensely improve the model performance bringing it very close to the scenariowhere all the data points in the extension set have gold labels. This allowsfor huge reduction of the number of data points that need to be manuallyannotated since out of the silver annotated extension dataset, the influencefunction scheme picks up ~1/1000 points that need manual correction.</description><author>Somnath Banerjee, Maulindu Sarkar, Punyajoy Saha, Binny Mathew, Animesh Mukherjee</author><pubDate>Thu, 22 Feb 2024 16:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14702v1</guid></item><item><title>COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling</title><link>http://arxiv.org/abs/2402.14701v1</link><description>The therapeutic working alliance is a critical factor in predicting thesuccess of psychotherapy treatment. Traditionally, working alliance assessmentrelies on questionnaires completed by both therapists and patients. In thispaper, we present COMPASS, a novel framework to directly infer the therapeuticworking alliance from the natural language used in psychotherapy sessions. Ourapproach utilizes advanced large language models to analyze transcripts ofpsychotherapy sessions and compare them with distributed representations ofstatements in the working alliance inventory. Analyzing a dataset of over 950sessions covering diverse psychiatric conditions, we demonstrate theeffectiveness of our method in microscopically mapping patient-therapistalignment trajectories and providing interpretability for clinical psychiatryand in identifying emerging patterns related to the condition being treated. Byemploying various neural topic modeling techniques in combination withgenerative language prompting, we analyze the topical characteristics ofdifferent psychiatric conditions and incorporate temporal modeling to capturethe evolution of topics at a turn-level resolution. This combined frameworkenhances the understanding of therapeutic interactions, enabling timelyfeedback for therapists regarding conversation quality and providinginterpretable insights to improve the effectiveness of psychotherapy.</description><author>Baihan Lin, Djallel Bouneffouf, Yulia Landa, Rachel Jespersen, Cheryl Corcoran, Guillermo Cecchi</author><pubDate>Thu, 22 Feb 2024 16:56:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14701v1</guid></item><item><title>Unveiling Linguistic Regions in Large Language Models</title><link>http://arxiv.org/abs/2402.14700v1</link><description>Large Language Models (LLMs) have demonstrated considerable cross-lingualalignment and generalization ability. Current research primarily focuses onimproving LLMs' cross-lingual generalization capabilities. However, there isstill a lack of research on the intrinsic mechanisms of how LLMs achievecross-lingual alignment. From the perspective of region partitioning, thispaper conducts several investigations on the linguistic competence of LLMs. Wediscover a core region in LLMs that corresponds to linguistic competence,accounting for approximately 1% of the total model parameters. Removing thiscore region by setting parameters to zero results in a significant performancedecrease across 30 different languages. Furthermore, this core region exhibitssignificant dimensional dependency, perturbations to even a single parameter onspecific dimensions leading to a loss of linguistic competence. Moreover, wediscover that distinct regions exist for different monolingual families, anddisruption to these specific regions substantially reduces the LLMs'proficiency in those corresponding languages. Our research also indicates thatfreezing the core linguistic region during further pre-training can mitigatethe issue of catastrophic forgetting (CF), a common occurrence observed duringfurther pre-training of LLMs. Overall, exploring the LLMs' functional regionsprovides insights into the foundation of their intelligence.</description><author>Zhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, Xuanjing Huang</author><pubDate>Thu, 22 Feb 2024 16:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14700v1</guid></item><item><title>Big data analytics to classify earthwork-related locations: A Chengdu study</title><link>http://arxiv.org/abs/2402.14698v1</link><description>Air pollution has significantly intensified, leading to severe healthconsequences worldwide. Earthwork-related locations (ERLs) constitutesignificant sources of urban dust pollution. The effective management of ERLshas long posed challenges for governmental and environmental agencies,primarily due to their classification under different regulatory authorities,information barriers, delays in data updating, and a lack of dust suppressionmeasures for various sources of dust pollution. To address these challenges, weclassified urban dust pollution sources using dump truck trajectory, urbanpoint of interest (POI), and land cover data. We compared several predictionmodels and investigated the relationship between features and dust pollutionsources using real data. The results demonstrate that high-accuracyclassification can be achieved with a limited number of features. This methodwas successfully implemented in the system called Alpha MAPS in Chengdu toprovide decision support for urban pollution control.</description><author>Lei Yu, Ke Han</author><pubDate>Thu, 22 Feb 2024 16:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14698v1</guid></item><item><title>QIS : Interactive Segmentation via Quasi-Conformal Mappings</title><link>http://arxiv.org/abs/2402.14695v1</link><description>Image segmentation plays a crucial role in extracting important objects ofinterest from images, enabling various applications. While existing methodshave shown success in segmenting clean images, they often struggle to produceaccurate segmentation results when dealing with degraded images, such as thosecontaining noise or occlusions. To address this challenge, interactivesegmentation has emerged as a promising approach, allowing users to providemeaningful input to guide the segmentation process. However, an importantproblem in interactive segmentation lies in determining how to incorporateminimal yet meaningful user guidance into the segmentation model. In thispaper, we propose the quasi-conformal interactive segmentation (QIS) model,which incorporates user input in the form of positive and negative clicks.Users mark a few pixels belonging to the object region as positive clicks,indicating that the segmentation model should include a region around theseclicks. Conversely, negative clicks are provided on pixels belonging to thebackground, instructing the model to exclude the region near these clicks fromthe segmentation mask. Additionally, the segmentation mask is obtained bydeforming a template mask with the same topology as the object of interestusing an orientation-preserving quasiconformal mapping. This approach helps toavoid topological errors in the segmentation results. We provide a thoroughanalysis of the proposed model, including theoretical support for the abilityof QIS to include or exclude regions of interest or disinterest based on theuser's indication. To evaluate the performance of QIS, we conduct experimentson synthesized images, medical images, natural images and noisy natural images.The results demonstrate the efficacy of our proposed method.</description><author>Han Zhang, Daoping Zhang, Lok Ming Lui</author><pubDate>Thu, 22 Feb 2024 16:49:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14695v1</guid></item><item><title>Mobiprox: Supporting Dynamic Approximate Computing on Mobiles</title><link>http://arxiv.org/abs/2303.11291v2</link><description>Runtime-tunable context-dependent network compression would make mobile deeplearning (DL) adaptable to often varying resource availability, input"difficulty", or user needs. The existing compression techniques significantlyreduce the memory, processing, and energy tax of DL, yet, the resulting modelstend to be permanently impaired, sacrificing the inference power for reducedresource usage. The existing tunable compression approaches, on the other hand,require expensive re-training, do not support arbitrary strategies for adaptingthe compression and do not provide mobile-ready implementations. In this paper we present Mobiprox, a framework enabling mobile DL withflexible precision. Mobiprox implements tunable approximations of tensoroperations and enables runtime-adaptable approximation of individual networklayers. A profiler and a tuner included with Mobiprox identify the mostpromising neural network approximation configurations leading to the desiredinference quality with the minimal use of resources. Furthermore, we developcontrol strategies that depending on contextual factors, such as the input datadifficulty, dynamically adjust the approximation levels across a mobile DLmodel's layers. We implement Mobiprox in Android OS and through experiments indiverse mobile domains, including human activity recognition and spoken keyworddetection, demonstrate that it can save up to 15% system-wide energy with aminimal impact on the inference accuracy.</description><author>Matevž Fabjančič, Octavian Machidon, Hashim Sharif, Yifan Zhao, Saša Misailović, Veljko Pejović</author><pubDate>Thu, 22 Feb 2024 16:48:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11291v2</guid></item><item><title>A Quick Introduction to Quantum Machine Learning for Non-Practitioners</title><link>http://arxiv.org/abs/2402.14694v1</link><description>This paper provides an introduction to quantum machine learning, exploringthe potential benefits of using quantum computing principles and algorithmsthat may improve upon classical machine learning approaches. Quantum computingutilizes particles governed by quantum mechanics for computational purposes,leveraging properties like superposition and entanglement for informationrepresentation and manipulation. Quantum machine learning applies theseprinciples to enhance classical machine learning models, potentially reducingnetwork size and training time on quantum hardware. The paper covers basicquantum mechanics principles, including superposition, phase space, andentanglement, and introduces the concept of quantum gates that exploit theseproperties. It also reviews classical deep learning concepts, such asartificial neural networks, gradient descent, and backpropagation, beforedelving into trainable quantum circuits as neural networks. An example problemdemonstrates the potential advantages of quantum neural networks, and theappendices provide detailed derivations. The paper aims to help researchers newto quantum mechanics and machine learning develop their expertise moreefficiently.</description><author>Ethan N. Evans, Dominic Byrne, Matthew G. Cook</author><pubDate>Thu, 22 Feb 2024 16:48:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14694v1</guid></item><item><title>PeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a Diffusion Probabilistic Model</title><link>http://arxiv.org/abs/2402.14692v1</link><description>This paper presents a neural vocoder based on a denoising diffusionprobabilistic model (DDPM) incorporating explicit periodic signals as auxiliaryconditioning signals. Recently, DDPM-based neural vocoders have gainedprominence as non-autoregressive models that can generate high-qualitywaveforms. The neural vocoders based on DDPM have the advantage of trainingwith a simple time-domain loss. In practical applications, such as singingvoice synthesis, there is a demand for neural vocoders to generatehigh-fidelity speech waveforms with flexible pitch control. However,conventional DDPM-based neural vocoders struggle to generate speech waveformsunder such conditions. Our proposed model aims to accurately capture theperiodic structure of speech waveforms by incorporating explicit periodicsignals. Experimental results show that our model improves sound quality andprovides better pitch control than conventional DDPM-based neural vocoders.</description><author>Yukiya Hono, Kei Hashimoto, Yoshihiko Nankaku, Keiichi Tokuda</author><pubDate>Thu, 22 Feb 2024 16:47:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14692v1</guid></item><item><title>UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models</title><link>http://arxiv.org/abs/2402.14690v1</link><description>Large language models (LLMs) may generate text that lacks consistency withhuman knowledge, leading to factual inaccuracies or \textit{hallucination}.Existing research for evaluating the factuality of LLMs involves extractingfact claims using an LLM and verifying them against a predefined fact source.However, these evaluation metrics are task-specific, and not scalable, and thesubstitutability of fact sources in different tasks is under-explored. Toaddress these challenges, we categorize four available fact sources:human-written evidence, reference documents, search engine results, and LLMknowledge, along with five text generation tasks containing six representativedatasets. Then, we propose \texttt{UFO}, an LLM-based unified and flexibleevaluation framework to verify facts against plug-and-play fact sources. Weimplement five evaluation scenarios based on this framework. Experimentalresults show that for most QA tasks, human-written evidence and referencedocuments are crucial, and they can substitute for each other inretrieval-augmented QA tasks. In news fact generation tasks, search engineresults and LLM knowledge are essential. Our dataset and code are available at\url{https://github.com/WaldenRUC/UFO}.</description><author>Zhaoheng Huang, Zhicheng Dou, Yutao Zhu, Ji-rong Wen</author><pubDate>Thu, 22 Feb 2024 16:45:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14690v1</guid></item><item><title>Q-Probe: A Lightweight Approach to Reward Maximization for Language Models</title><link>http://arxiv.org/abs/2402.14688v1</link><description>We present an approach called Q-probing to adapt a pre-trained language modelto maximize a task-specific reward function. At a high level, Q-probing sitsbetween heavier approaches such as finetuning and lighter approaches such asfew shot prompting, but can also be combined with either. The idea is to learna simple linear function on a model's embedding space that can be used toreweight candidate completions. We theoretically show that this samplingprocedure is equivalent to a KL-constrained maximization of the Q-probe as thenumber of samples increases. To train the Q-probes we consider either rewardmodeling or a class of novel direct policy learning objectives based onimportance weighted policy gradients. With this technique, we see gains indomains with ground-truth rewards (code generation) as well as implicit rewardsdefined by preference data, even outperforming finetuning in data-limitedregimes. Moreover, a Q-probe can be trained on top of an API since it onlyassumes access to sampling and embeddings. Code:https://github.com/likenneth/q_probe .</description><author>Kenneth Li, Samy Jelassi, Hugh Zhang, Sham Kakade, Martin Wattenberg, David Brandfonbrener</author><pubDate>Thu, 22 Feb 2024 16:43:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14688v1</guid></item><item><title>Adaptive time series forecasting with markovian variance switching</title><link>http://arxiv.org/abs/2402.14684v1</link><description>Adaptive time series forecasting is essential for prediction under regimechanges. Several classical methods assume linear Gaussian state space model(LGSSM) with variances constant in time. However, there are many real-worldprocesses that cannot be captured by such models. We consider a state-spacemodel with Markov switching variances. Such dynamical systems are usuallyintractable because of their computational complexity increasing exponentiallywith time; Variational Bayes (VB) techniques have been applied to this problem.In this paper, we propose a new way of estimating variances based on onlinelearning theory; we adapt expert aggregation methods to learn the variancesover time. We apply the proposed method to synthetic data and to the problem ofelectricity load forecasting. We show that this method is robust tomisspecification and outperforms traditional expert aggregation.</description><author>Baptiste Abélès, Joseph de Vilmarest, Olivier Wintemberger</author><pubDate>Thu, 22 Feb 2024 16:40:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14684v1</guid></item><item><title>Visual Hallucinations of Multi-modal Large Language Models</title><link>http://arxiv.org/abs/2402.14683v1</link><description>Visual hallucination (VH) means that a multi-modal LLM (MLLM) imaginesincorrect details about an image in visual question answering. Existing studiesfind VH instances only in existing image datasets, which results in biasedunderstanding of MLLMs' performance under VH due to limited diversity of suchVH instances. In this work, we propose a tool called VHTest to generate adiverse set of VH instances. Specifically, VHTest finds some initial VHinstances in existing image datasets (e.g., COCO), generates a text descriptionfor each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) togenerate VH images based on the text descriptions. We collect a benchmarkdataset with 1,200 VH instances in 8 VH modes using VHTest. We find thatexisting MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for alarge fraction of the instances in our benchmark. Moreover, we find thatfine-tuning an MLLM using our benchmark dataset reduces its likelihood tohallucinate without sacrificing its performance on other benchmarks. Ourbenchmarks are publicly available: https://github.com/wenhuang2000/VHTest.</description><author>Wen Huang, Hongbin Liu, Minxin Guo, Neil Zhenqiang Gong</author><pubDate>Thu, 22 Feb 2024 16:40:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14683v1</guid></item><item><title>Analytical Verification of Performance of Deep Neural Network Based Time-Synchronized Distribution System State Estimation</title><link>http://arxiv.org/abs/2311.06973v4</link><description>Recently, we demonstrated success of a time-synchronized state estimatorusing deep neural networks (DNNs) for real-time unobservable distributionsystems. In this letter, we provide analytical bounds on the performance ofthat state estimator as a function of perturbations in the input measurements.It has already been shown that evaluating performance based on only the testdataset might not effectively indicate a trained DNN's ability to handle inputperturbations. As such, we analytically verify robustness and trustworthinessof DNNs to input perturbations by treating them as mixed-integer linearprogramming (MILP) problems. The ability of batch normalization in addressingthe scalability limitations of the MILP formulation is also highlighted. Theframework is validated by performing time-synchronized distribution systemstate estimation for a modified IEEE 34-node system and a real-world largedistribution system, both of which are incompletely observed by micro-phasormeasurement units.</description><author>Behrouz Azimian, Shiva Moshtagh, Anamitra Pal, Shanshan Ma</author><pubDate>Thu, 22 Feb 2024 16:33:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06973v4</guid></item><item><title>Is Cognition and Action Consistent or Not: Investigating Large Language Model's Personality</title><link>http://arxiv.org/abs/2402.14679v1</link><description>In this study, we investigate the reliability of Large Language Models (LLMs)in professing human-like personality traits through responses to personalityquestionnaires. Our goal is to evaluate the consistency between LLMs' professedpersonality inclinations and their actual "behavior", examining the extent towhich these models can emulate human-like personality patterns. Through acomprehensive analysis of LLM outputs against established human benchmarks, weseek to understand the cognition-action divergence in LLMs and proposehypotheses for the observed results based on psychological theories andmetrics.</description><author>Yiming Ai, Zhiwei He, Ziyin Zhang, Wenhong Zhu, Hongkun Hao, Kai Yu, Lingjun Chen, Rui Wang</author><pubDate>Thu, 22 Feb 2024 16:32:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14679v1</guid></item><item><title>Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering</title><link>http://arxiv.org/abs/2309.02233v2</link><description>Large-scale language models (LLMs) like ChatGPT have demonstrated impressiveabilities in generating responses based on human instructions. However, theiruse in the medical field can be challenging due to their lack of specific,in-depth knowledge. In this study, we present a system called LLMs Augmentedwith Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs inspecialized domains. LLM-AMT integrates authoritative medical textbooks intothe LLMs' framework using plug-and-play modules. These modules include a QueryAugmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,they incorporate authoritative medical knowledge. Additionally, an LLM Readeraids in contextual understanding. Our experimental results on three medical QAtasks demonstrate that LLMAMT significantly improves response quality, withaccuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as thebase model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained ona massive amount of medical corpus by 2-3%. We found that despite being 100xsmaller in size, medical textbooks as a retrieval corpus is proven to be a moreeffective knowledge database than Wikipedia in the medical domain, boostingperformance by 7.8%-13.7%.</description><author>Yubo Wang, Xueguang Ma, Wenhu Chen</author><pubDate>Thu, 22 Feb 2024 16:32:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02233v2</guid></item><item><title>Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound</title><link>http://arxiv.org/abs/2202.05560v2</link><description>Current PAC-Bayes generalisation bounds are restricted to scalar metrics ofperformance, such as the loss or error rate. However, one ideally wants moreinformation-rich certificates that control the entire distribution of possibleoutcomes, such as the distribution of the test loss in regression, or theprobabilities of different mis classifications. We provide the first PAC-Bayesbound capable of providing such rich information by bounding theKullback-Leibler divergence between the empirical and true probabilities of aset of M error types, which can either be discretized loss values forregression, or the elements of the confusion matrix (or a partition thereof)for classification. We transform our bound into a differentiable trainingobjective. Our bound is especially useful in cases where the severity ofdifferent mis-classifications may change over time; existing PAC-Bayes boundscan only bound a particular pre-decided weighting of the error types. Incontrast our bound implicitly controls all uncountably many weightingssimultaneously.</description><author>Reuben Adams, John Shawe-Taylor, Benjamin Guedj</author><pubDate>Thu, 22 Feb 2024 16:28:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.05560v2</guid></item><item><title>Large Language Models in Cryptocurrency Securities Cases: Can a GPT Model Meaningfully Assist Lawyers?</title><link>http://arxiv.org/abs/2308.06032v4</link><description>Large Language Models (LLMs) could be a useful tool for lawyers. However,empirical research on their effectiveness in conducting legal tasks is scant.We study securities cases involving cryptocurrencies as one of numerouscontexts where AI could support the legal process, studying GPT-3.5's legalreasoning and ChatGPT's legal drafting capabilities. We examine whether a)GPT-3.5 can accurately determine which laws are potentially being violated froma fact pattern, and b) whether there is a difference in juror decision-makingbased on complaints written by a lawyer compared to ChatGPT. We feed factpatterns from real-life cases to GPT-3.5 and evaluate its ability to determinecorrect potential violations from the scenario and exclude spurious violations.Second, we had mock jurors assess complaints written by ChatGPT and lawyers.GPT-3.5's legal reasoning skills proved weak, though we expect improvement infuture models, particularly given the violations it suggested tended to becorrect (it merely missed additional, correct violations). ChatGPT performedbetter at legal drafting, and jurors' decisions were not statisticallysignificantly associated with the author of the document upon which they basedtheir decisions. Because GPT-3.5 cannot satisfactorily conduct legal reasoningtasks, it would be unlikely to be able to help lawyers in a meaningful way atthis stage. However, ChatGPT's drafting skills (though, perhaps, still inferiorto lawyers) could assist lawyers in providing legal services. Our research isthe first to systematically study an LLM's legal drafting and reasoningcapabilities in litigation, as well as in securities law andcryptocurrency-related misconduct.</description><author>Arianna Trozze, Toby Davies, Bennett Kleinberg</author><pubDate>Thu, 22 Feb 2024 16:21:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06032v4</guid></item><item><title>Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments</title><link>http://arxiv.org/abs/2402.14672v1</link><description>The applications of large language models (LLMs) have expanded well beyondthe confines of text processing, signaling a new era where LLMs are envisionedas generalist language agents capable of operating within complex real-worldenvironments. These environments are often highly expansive, making itimpossible for the LLM to process them within its short-term memory. Motivatedby recent research on extending the capabilities of LLMs with tools, this paperinvestigates the intriguing potential of tools to augment LLMs in handling suchcomplexity. To this end, we design customized tools to aid in the proactiveexploration within these massive environments. Such tools can serve as amiddleware layer shielding the LLM from environmental complexity. In tworepresentative complex environments -- knowledge bases (KBs) and databases --we demonstrate the significant potential of augmenting language agents withtools in complex environments. Notably, equipped with these tools, GPT-4achieves 2.8X the performance of the best baseline in tasks requiring access todatabase content and 2.2X in KB tasks. Our findings illuminate the path foradvancing language agents in complex real-world applications.</description><author>Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong, Jie Tang, Jayanth Srinivasa, Hugo Latapie, Yu Su</author><pubDate>Thu, 22 Feb 2024 16:18:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14672v1</guid></item><item><title>Quadruplet Loss For Improving the Robustness to Face Morphing Attacks</title><link>http://arxiv.org/abs/2402.14665v1</link><description>Recent advancements in deep learning have revolutionized technology andsecurity measures, necessitating robust identification methods. Biometricapproaches, leveraging personalized characteristics, offer a promisingsolution. However, Face Recognition Systems are vulnerable to sophisticatedattacks, notably face morphing techniques, enabling the creation of fraudulentdocuments. In this study, we introduce a novel quadruplet loss function forincreasing the robustness of face recognition systems against morphing attacks.Our approach involves specific sampling of face image quadruplets, combinedwith face morphs, for network training. Experimental results demonstrate theefficiency of our strategy in improving the robustness of face recognitionnetworks against morphing attacks.</description><author>Iurii Medvedev, Nuno Gonçalves</author><pubDate>Thu, 22 Feb 2024 16:10:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14665v1</guid></item><item><title>Bayesian Off-Policy Evaluation and Learning for Large Action Spaces</title><link>http://arxiv.org/abs/2402.14664v1</link><description>In interactive systems, actions are often correlated, presenting anopportunity for more sample-efficient off-policy evaluation (OPE) and learning(OPL) in large action spaces. We introduce a unified Bayesian framework tocapture these correlations through structured and informative priors. In thisframework, we propose sDM, a generic Bayesian approach designed for OPE andOPL, grounded in both algorithmic and theoretical foundations. Notably, sDMleverages action correlations without compromising computational efficiency.Moreover, inspired by online Bayesian bandits, we introduce Bayesian metricsthat assess the average performance of algorithms across multiple probleminstances, deviating from the conventional worst-case assessments. We analyzesDM in OPE and OPL, highlighting the benefits of leveraging actioncorrelations. Empirical evidence showcases the strong performance of sDM.</description><author>Imad Aouali, Victor-Emmanuel Brunel, David Rohde, Anna Korba</author><pubDate>Thu, 22 Feb 2024 16:09:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14664v1</guid></item><item><title>SparQ Attention: Bandwidth-Efficient LLM Inference</title><link>http://arxiv.org/abs/2312.04985v2</link><description>Generative large language models (LLMs) have opened up numerous novelpossibilities, but due to their significant computational requirements theirubiquitous use remains challenging. Some of the most useful applicationsrequire processing large numbers of samples at a time and using long contexts,both significantly increasing the memory communication load of the models. Weintroduce SparQ Attention, a technique for increasing the inference throughputof LLMs by reducing the memory bandwidth requirements within the attentionblocks through selective fetching of the cached history. Our proposed techniquecan be applied directly to off-the-shelf LLMs during inference, withoutrequiring any modification to the pre-training setup or additional fine-tuning.We show how SparQ Attention can decrease the attention memory bandwidthrequirements up to eight times without any loss in accuracy by evaluating Llama2 and Pythia models on a wide range of downstream tasks.</description><author>Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas Orr</author><pubDate>Thu, 22 Feb 2024 16:07:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04985v2</guid></item><item><title>ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models</title><link>http://arxiv.org/abs/2402.14660v1</link><description>This paper introduces ConceptMath, a bilingual (English and Chinese),fine-grained benchmark that evaluates concept-wise mathematical reasoning ofLarge Language Models (LLMs). Unlike traditional benchmarks that evaluategeneral mathematical reasoning with an average accuracy, ConceptMathsystematically organizes math problems under a hierarchy of math concepts, sothat mathematical reasoning can be evaluated at different granularity withconcept-wise accuracies. Based on our ConcepthMath, we evaluate a broad rangeof LLMs, and we observe existing LLMs, though achieving high average accuracieson traditional benchmarks, exhibit significant performance variations acrossdifferent math concepts and may even fail catastrophically on the most basicones. Besides, we also introduce an efficient fine-tuning strategy to enhancethe weaknesses of existing LLMs. Finally, we hope ConceptMath could guide thedevelopers to understand the fine-grained mathematical abilities of theirmodels and facilitate the growth of foundation models.</description><author>Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, Wanli Ouyang, Wenbo Su, Bo Zheng</author><pubDate>Thu, 22 Feb 2024 16:06:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14660v1</guid></item><item><title>OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement</title><link>http://arxiv.org/abs/2402.14658v1</link><description>The introduction of large language models has significantly advanced codegeneration. However, open-source models often lack the execution capabilitiesand iterative refinement of advanced systems like the GPT-4 Code Interpreter.To address this, we introduce OpenCodeInterpreter, a family of open-source codesystems designed for generating, executing, and iteratively refining code.Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions,OpenCodeInterpreter integrates execution and human feedback for dynamic coderefinement. Our comprehensive evaluation of OpenCodeInterpreter across keybenchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlusreveals its exceptional performance. Notably, OpenCodeInterpreter-33B achievesan accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval andMBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6)with synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gapbetween open-source code generation models and proprietary systems like GPT-4Code Interpreter.</description><author>Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, Xiang Yue</author><pubDate>Thu, 22 Feb 2024 16:06:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14658v1</guid></item><item><title>Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot</title><link>http://arxiv.org/abs/2402.14654v1</link><description>We present Multi-HMR, a strong single-shot model for multi-person 3D humanmesh recovery from a single RGB image. Predictions encompass the whole body,i.e, including hands and facial expressions, using the SMPL-X parametric modeland spatial location in the camera coordinate system. Our model detects peopleby predicting coarse 2D heatmaps of person centers, using features produced bya standard Vision Transformer (ViT) backbone. It then predicts their whole-bodypose, shape and spatial location using a new cross-attention module called theHuman Prediction Head (HPH), with one query per detected center token,attending to the entire set of features. As direct prediction of SMPL-Xparameters yields suboptimal results, we introduce CUFFS; the Close-Up Framesof Full-Body Subjects dataset, containing humans close to the camera withdiverse hand poses. We show that incorporating this dataset into trainingfurther enhances predictions, particularly for hands, enabling us to achievestate-of-the-art performance. Multi-HMR also optionally accounts for cameraintrinsics, if available, by encoding camera ray directions for each imagetoken. This simple design achieves strong performance on whole-body andbody-only benchmarks simultaneously. We train models with various backbonesizes and input resolutions. In particular, using a ViT-S backbone and$448\times448$ input images already yields a fast and competitive model withrespect to state-of-the-art methods, while considering larger models and higherresolutions further improve performance.</description><author>Fabien Baradel, Matthieu Armando, Salma Galaaoui, Romain Brégier, Philippe Weinzaepfel, Grégory Rogez, Thomas Lucas</author><pubDate>Thu, 22 Feb 2024 16:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14654v1</guid></item><item><title>Cleaner Pretraining Corpus Curation with Neural Web Scraping</title><link>http://arxiv.org/abs/2402.14652v1</link><description>The web contains large-scale, diverse, and abundant information to satisfythe information-seeking needs of humans. Through meticulous data collection,preprocessing, and curation, webpages can be used as a fundamental dataresource for language model pretraining. However, when confronted with theprogressively revolutionized and intricate nature of webpages,rule-based/feature-based web scrapers are becoming increasingly inadequate.This paper presents a simple, fast, and effective Neural web Scraper(NeuScraper) to help extract primary and clean text contents from webpages.Experimental results show that NeuScraper surpasses the baseline scrapers byachieving more than a 20% improvement, demonstrating its potential inextracting higher-quality data to facilitate the language model pretraining.All of the code is available at https://github.com/OpenMatch/NeuScraper.</description><author>Zhipeng Xu, Zhenghao Liu, Yukun Yan, Zhiyuan Liu, Chenyan Xiong, Ge Yu</author><pubDate>Thu, 22 Feb 2024 16:04:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14652v1</guid></item><item><title>GaussianPro: 3D Gaussian Splatting with Progressive Propagation</title><link>http://arxiv.org/abs/2402.14650v1</link><description>The advent of 3D Gaussian Splatting (3DGS) has recently brought about arevolution in the field of neural rendering, facilitating high-qualityrenderings at real-time speed. However, 3DGS heavily depends on the initializedpoint cloud produced by Structure-from-Motion (SfM) techniques. When tacklingwith large-scale scenes that unavoidably contain texture-less surfaces, the SfMtechniques always fail to produce enough points in these surfaces and cannotprovide good initialization for 3DGS. As a result, 3DGS suffers from difficultoptimization and low-quality renderings. In this paper, inspired by classicalmulti-view stereo (MVS) techniques, we propose GaussianPro, a novel method thatapplies a progressive propagation strategy to guide the densification of the 3DGaussians. Compared to the simple split and clone strategies used in 3DGS, ourmethod leverages the priors of the existing reconstructed geometries of thescene and patch matching techniques to produce new Gaussians with accuratepositions and orientations. Experiments on both large-scale and small-scalescenes validate the effectiveness of our method, where our method significantlysurpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB interms of PSNR.</description><author>Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin, Yuexin Ma, Wenping Wang, Xuejin Chen</author><pubDate>Thu, 22 Feb 2024 16:00:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14650v1</guid></item><item><title>Generative Invertible Quantum Neural Networks</title><link>http://arxiv.org/abs/2302.12906v3</link><description>Invertible Neural Networks (INN) have become established tools for thesimulation and generation of highly complex data. We propose a quantum-gatealgorithm for a Quantum Invertible Neural Network (QINN) and apply it to theLHC data of jet-associated production of a Z-boson that decays into leptons, astandard candle process for particle collider precision measurements. Wecompare the QINN's performance for different loss functions and trainingscenarios. For this task, we find that a hybrid QINN matches the performance ofa significantly larger purely classical INN in learning and generating complexdata.</description><author>Armand Rousselot, Michael Spannowsky</author><pubDate>Thu, 22 Feb 2024 15:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12906v3</guid></item><item><title>Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off</title><link>http://arxiv.org/abs/2402.14648v1</link><description>Although adversarial training has been the state-of-the-art approach todefend against adversarial examples (AEs), they suffer from arobustness-accuracy trade-off. In this work, we revisit representation-basedinvariance regularization to learn discriminative yet adversarially invariantrepresentations, aiming to mitigate this trade-off. We empirically identify twokey issues hindering invariance regularization: (1) a "gradient conflict"between invariance loss and classification objectives, indicating the existenceof "collapsing solutions," and (2) the mixture distribution problem arisingfrom diverged distributions of clean and adversarial inputs. To address theseissues, we propose Asymmetrically Representation-regularized AdversarialTraining (AR-AT), which incorporates a stop-gradient operation and a pre-dictorin the invariance loss to avoid "collapsing solutions," inspired by a recentnon-contrastive self-supervised learning approach, and a split-BatchNorm (BN)structure to resolve the mixture distribution problem. Our method significantlyimproves the robustness-accuracy trade-off by learning adversarially invariantrepresentations without sacrificing discriminative power. Furthermore, wediscuss the relevance of our findings to knowledge-distillation-based defensemethods, contributing to a deeper understanding of their relative successes.</description><author>Futa Waseda, Isao Echizen</author><pubDate>Thu, 22 Feb 2024 15:53:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14648v1</guid></item><item><title>Learning and Sustaining Shared Normative Systems via Bayesian Rule Induction in Markov Games</title><link>http://arxiv.org/abs/2402.13399v2</link><description>A universal feature of human societies is the adoption of systems of rulesand norms in the service of cooperative ends. How can we build learning agentsthat do the same, so that they may flexibly cooperate with the humaninstitutions they are embedded in? We hypothesize that agents can achieve thisby assuming there exists a shared set of norms that most others comply withwhile pursuing their individual desires, even if they do not know the exactcontent of those norms. By assuming shared norms, a newly introduced agent caninfer the norms of an existing population from observations of compliance andviolation. Furthermore, groups of agents can converge to a shared set of norms,even if they initially diverge in their beliefs about what the norms are. Thisin turn enables the stability of the normative system: since agents canbootstrap common knowledge of the norms, this leads the norms to be widelyadhered to, enabling new entrants to rapidly learn those norms. We formalizethis framework in the context of Markov games and demonstrate its operation ina multi-agent environment via approximately Bayesian rule induction ofobligative and prohibitive norms. Using our approach, agents are able torapidly learn and sustain a variety of cooperative institutions, includingresource management norms and compensation for pro-social labor, promotingcollective welfare while still allowing agents to act in their own interests.</description><author>Ninell Oldenburg, Tan Zhi-Xuan</author><pubDate>Thu, 22 Feb 2024 15:46:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13399v2</guid></item><item><title>CoLoRA: Continuous low-rank adaptation for reduced implicit neural modeling of parameterized partial differential equations</title><link>http://arxiv.org/abs/2402.14646v1</link><description>This work introduces reduced models based on Continuous Low Rank Adaptation(CoLoRA) that pre-train neural networks for a given partial differentialequation and then continuously adapt low-rank weights in time to rapidlypredict the evolution of solution fields at new physics parameters and newinitial conditions. The adaptation can be either purely data-driven or via anequation-driven variational approach that provides Galerkin-optimalapproximations. Because CoLoRA approximates solution fields locally in time,the rank of the weights can be kept small, which means that only few trainingtrajectories are required offline so that CoLoRA is well suited for data-scarceregimes. Predictions with CoLoRA are orders of magnitude faster than withclassical methods and their accuracy and parameter efficiency is highercompared to other neural network approaches.</description><author>Jules Berman, Benjamin Peherstorfer</author><pubDate>Thu, 22 Feb 2024 15:45:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14646v1</guid></item><item><title>Sparse Linear Regression and Lattice Problems</title><link>http://arxiv.org/abs/2402.14645v1</link><description>Sparse linear regression (SLR) is a well-studied problem in statistics whereone is given a design matrix $X\in\mathbb{R}^{m\times n}$ and a response vector$y=X\theta^*+w$ for a $k$-sparse vector $\theta^*$ (that is,$\|\theta^*\|_0\leq k$) and small, arbitrary noise $w$, and the goal is to finda $k$-sparse $\widehat{\theta} \in \mathbb{R}^n$ that minimizes the meansquared prediction error $\frac{1}{m}\|X\widehat{\theta}-X\theta^*\|^2_2$.While $\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzigselector solve SLR when the design matrix is well-conditioned, no generalalgorithm is known, nor is there any formal evidence of hardness in anaverage-case setting with respect to all efficient algorithms. We give evidence of average-case hardness of SLR w.r.t. all efficientalgorithms assuming the worst-case hardness of lattice problems. Specifically,we give an instance-by-instance reduction from a variant of the boundeddistance decoding (BDD) problem on lattices to SLR, where the condition numberof the lattice basis that defines the BDD instance is directly related to therestricted eigenvalue condition of the design matrix, which characterizes someof the classical statistical-computational gaps for sparse linear regression.Also, by appealing to worst-case to average-case reductions from the world oflattices, this shows hardness for a distribution of SLR instances; while thedesign matrices are ill-conditioned, the resulting SLR instances are in theidentifiable regime. Furthermore, for well-conditioned (essentially) isotropic Gaussian designmatrices, where Lasso is known to behave well in the identifiable regime, weshow hardness of outputting any good solution in the unidentifiable regimewhere there are many solutions, assuming the worst-case hardness of standardand well-studied lattice problems.</description><author>Aparna Gupte, Neekon Vafa, Vinod Vaikuntanathan</author><pubDate>Thu, 22 Feb 2024 15:45:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14645v1</guid></item><item><title>Distributed Radiance Fields for Edge Video Compression and Metaverse Integration in Autonomous Driving</title><link>http://arxiv.org/abs/2402.14642v1</link><description>The metaverse is a virtual space that combines physical and digital elements,creating immersive and connected digital worlds. For autonomous mobility, itenables new possibilities with edge computing and digital twins (DTs) thatoffer virtual prototyping, prediction, and more. DTs can be created with 3Dscene reconstruction methods that capture the real world's geometry,appearance, and dynamics. However, sending data for real-time DT updates in themetaverse, such as camera images and videos from connected autonomous vehicles(CAVs) to edge servers, can increase network congestion, costs, and latency,affecting metaverse services. Herein, a new method is proposed based ondistributed radiance fields (RFs), multi-access edge computing (MEC) networkfor video compression and metaverse DT updates. RF-based encoder and decoderare used to create and restore representations of camera images. The method isevaluated on a dataset of camera images from the CARLA simulator. Data savingsof up to 80% were achieved for H.264 I-frame - P-frame pairs by using RFsinstead of I-frames, while maintaining high peak signal-to-noise ratio (PSNR)and structural similarity index measure (SSIM) qualitative metrics for thereconstructed images. Possible uses and challenges for the metaverse andautonomous mobility are also discussed.</description><author>Eugen Šlapak, Matúš Dopiriak, Mohammad Abdullah Al Faruque, Juraj Gazda, Marco Levorato</author><pubDate>Thu, 22 Feb 2024 15:39:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14642v1</guid></item><item><title>Learning Quadruped Locomotion Policies using Logical Rules</title><link>http://arxiv.org/abs/2107.10969v3</link><description>Quadruped animals are capable of exhibiting a diverse range of locomotiongaits. While progress has been made in demonstrating such gaits on robots,current methods rely on motion priors, dynamics models, or other forms ofextensive manual efforts. People can use natural language to describe dancemoves. Could one use a formal language to specify quadruped gaits? To this end,we aim to enable easy gait specification and efficient policy learning.Leveraging Reward Machines~(RMs) for high-level gait specification over footcontacts, our approach is called RM-based Locomotion Learning~(RMLL), andsupports adjusting gait frequency at execution time. Gait specification isenabled through the use of a few logical rules per gait (e.g., alternatebetween moving front feet and back feet) and does not require labor-intensivemotion priors. Experimental results in simulation highlight the diversity oflearned gaits (including two novel gaits), their energy consumption andstability across different terrains, and the superior sample-efficiency whencompared to baselines. We also demonstrate these learned policies with a realquadruped robot. Video and supplementary materials:https://sites.google.com/view/rm-locomotion-learning/home</description><author>David DeFazio, Yohei Hayamizu, Shiqi Zhang</author><pubDate>Thu, 22 Feb 2024 15:36:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.10969v3</guid></item><item><title>Can We Identify Stance Without Target Arguments? A Study for Rumour Stance Classification</title><link>http://arxiv.org/abs/2303.12665v2</link><description>Considering a conversation thread, rumour stance classification aims toidentify the opinion (e.g. agree or disagree) of replies towards a target(rumour story). Although the target is expected to be an essential component intraditional stance classification, we show that rumour stance classificationdatasets contain a considerable amount of real-world data whose stance could benaturally inferred directly from the replies, contributing to the strongperformance of the supervised models without awareness of the target. We findthat current target-aware models underperform in cases where the context of thetarget is crucial. Finally, we propose a simple yet effective framework toenhance reasoning with the targets, achieving state-of-the-art performance ontwo benchmark datasets.</description><author>Yue Li, Carolina Scarton</author><pubDate>Thu, 22 Feb 2024 15:36:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12665v2</guid></item><item><title>Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data</title><link>http://arxiv.org/abs/2402.12424v2</link><description>In this paper, we investigate the effectiveness of various LLMs ininterpreting tabular data through different prompting strategies and dataformats. Our analysis extends across six benchmarks for table-related taskssuch as question-answering and fact-checking. We introduce for the first timethe assessment of LLMs' performance on image-based table representations.Specifically, we compare five text-based and three image-based tablerepresentations, demonstrating the influence of representation and prompting onLLM performance. Our study provides insights into the effective use of LLMs ontable-related tasks.</description><author>Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, Rada Mihalcea</author><pubDate>Thu, 22 Feb 2024 15:34:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12424v2</guid></item><item><title>Towards true discovery of the differential equations</title><link>http://arxiv.org/abs/2308.04901v2</link><description>Differential equation discovery, a machine learning subfield, is used todevelop interpretable models, particularly in nature-related applications. Byexpertly incorporating the general parametric form of the equation of motionand appropriate differential terms, algorithms can autonomously uncoverequations from data. This paper explores the prerequisites and tools forindependent equation discovery without expert input, eliminating the need forequation form assumptions. We focus on addressing the challenge of assessingthe adequacy of discovered equations when the correct equation is unknown, withthe aim of providing insights for reliable equation discovery without priorknowledge of the equation form.</description><author>Alexander Hvatov, Roman Titov</author><pubDate>Thu, 22 Feb 2024 15:30:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04901v2</guid></item><item><title>An Identity-Preserved Framework for Human Motion Transfer</title><link>http://arxiv.org/abs/2204.06862v3</link><description>Human motion transfer (HMT) aims to generate a video clip for the targetsubject by imitating the source subject's motion. Although previous methodshave achieved good results in synthesizing good-quality videos, they lose sightof individualized motion information from the source and target motions, whichis significant for the realism of the motion in the generated video. To addressthis problem, we propose a novel identity-preserved HMT network, termed\textit{IDPres}. This network is a skeleton-based approach that uniquelyincorporates the target's individualized motion and skeleton information toaugment identity representations. This integration significantly enhances therealism of movements in the generated videos. Our method focuses on thefine-grained disentanglement and synthesis of motion. To improve therepresentation learning capability in latent space and facilitate the trainingof \textit{IDPres}, we introduce three training schemes. These schemes enable\textit{IDPres} to concurrently disentangle different representations andaccurately control them, ensuring the synthesis of ideal motions. To evaluatethe proportion of individualized motion information in the generated video, weare the first to introduce a new quantitative metric called Identity Score(\textit{ID-Score}), motivated by the success of gait recognition methods incapturing identity information. Moreover, we collect an identity-motion paireddataset, $Dancer101$, consisting of solo-dance videos of 101 subjects from thepublic domain, providing a benchmark to prompt the development of HMT methods.Extensive experiments demonstrate that the proposed \textit{IDPres} methodsurpasses existing state-of-the-art techniques in terms of reconstructionaccuracy, realistic motion, and identity preservation.</description><author>Jingzhe Ma, Xiaoqing Zhang, Shiqi Yu</author><pubDate>Thu, 22 Feb 2024 15:29:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.06862v3</guid></item><item><title>RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation</title><link>http://arxiv.org/abs/2402.14623v1</link><description>Rapid progress in high-level task planning and code generation for open-worldrobot manipulation has been witnessed in Embodied AI. However, previous studiesput much effort into general common sense reasoning and task planningcapabilities of large-scale language or multi-modal models, relatively littleeffort on ensuring the deployability of generated code on real robots, andother fundamental components of autonomous robot systems including robotperception, motion planning, and control. To bridge this ``ideal-to-real'' gap,this paper presents \textbf{RobotScript}, a platform for 1) a deployable robotmanipulation pipeline powered by code generation; and 2) a code generationbenchmark for robot manipulation tasks in free-form natural language. TheRobotScript platform addresses this gap by emphasizing the unified interfacewith both simulation and real robots, based on abstraction from the RobotOperating System (ROS), ensuring syntax compliance and simulation validationwith Gazebo. We demonstrate the adaptability of our code generation frameworkacross multiple robot embodiments, including the Franka and UR5 robot arms, andmultiple grippers. Additionally, our benchmark assesses reasoning abilities forphysical space and constraints, highlighting the differences between GPT-3.5,GPT-4, and Gemini in handling complex physical interactions. Finally, wepresent a thorough evaluation on the whole system, exploring how each module inthe pipeline: code generation, perception, motion planning, and even objectgeometric properties, impact the overall performance of the system.</description><author>Junting Chen, Yao Mu, Qiaojun Yu, Tianming Wei, Silang Wu, Zhecheng Yuan, Zhixuan Liang, Chao Yang, Kaipeng Zhang, Wenqi Shao, Yu Qiao, Huazhe Xu, Mingyu Ding, Ping Luo</author><pubDate>Thu, 22 Feb 2024 15:12:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14623v1</guid></item><item><title>Knowledge of Pretrained Language Models on Surface Information of Tokens</title><link>http://arxiv.org/abs/2402.09808v2</link><description>Do pretrained language models have knowledge regarding the surfaceinformation of tokens? We examined the surface information stored in word orsubword embeddings acquired by pretrained language models from the perspectivesof token length, substrings, and token constitution. Additionally, we evaluatedthe ability of models to generate knowledge regarding token surfaces. Wefocused on 12 pretrained language models that were mainly trained on Englishand Japanese corpora. Experimental results demonstrate that pretrained languagemodels have knowledge regarding token length and substrings but not tokenconstitution. Additionally, the results imply that there is a bottleneck on thedecoder side in terms of effectively utilizing acquired knowledge.</description><author>Tatsuya Hiraoka, Naoaki Okazaki</author><pubDate>Thu, 22 Feb 2024 15:11:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09808v2</guid></item></channel></rss>