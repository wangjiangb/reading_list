<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 14 Jun 2024 06:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination</title><link>http://arxiv.org/abs/2406.05132v2</link><description>The integration of language and 3D perception is crucial for developingembodied agents and robots that comprehend and interact with the physicalworld. While large language models (LLMs) have demonstrated impressive languageunderstanding and generation capabilities, their adaptation to 3D environments(3D-LLMs) remains in its early stages. A primary challenge is the absence oflarge-scale datasets that provide dense grounding between language and 3Dscenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale datasetcomprising 40,087 household scenes paired with 6.2 million densely-groundedscene-language instructions. Our results show that instruction tuning with3D-GRAND significantly enhances grounding capabilities and reduceshallucinations in 3D-LLMs. As part of our contributions, we propose acomprehensive benchmark 3D-POPE to systematically evaluate hallucination in3D-LLMs, enabling fair comparisons among future models. Our experimentshighlight a scaling effect between dataset size and 3D-LLM performance,emphasizing the critical role of large-scale 3D-text datasets in advancingembodied AI research. Notably, our results demonstrate early signals foreffective sim-to-real transfer, indicating that models trained on largesynthetic data can perform well on real-world 3D scans. Through 3D-GRAND and3D-POPE, we aim to equip the embodied AI community with essential resources andinsights, setting the stage for more reliable and better-grounded 3D-LLMs.Project website: https://3d-grand.github.io</description><author>Jianing Yang, Xuweiyi Chen, Nikhil Madaan, Madhavan Iyengar, Shengyi Qian, David F. Fouhey, Joyce Chai</author><pubDate>Wed, 12 Jun 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05132v2</guid></item><item><title>CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples</title><link>http://arxiv.org/abs/2402.13254v4</link><description>We propose CounterCurate, a framework to comprehensively improve thevisio-linguistic compositional reasoning capability for both contrastive andgenerative multimodal models. In particular, we identify two criticalunder-explored problems: the neglect of the physically grounded reasoning(counting and position understanding) and the potential of using highly capabletext and image generation models for semantic counterfactual fine-tuning. Ourwork pioneers an approach that addresses these gaps. We first spotlight thenear-chance performance of multimodal models like CLIP and LLaVA in physicallygrounded compositional reasoning. We then apply simple data augmentation usinggrounded image generation model GLIGEN to generate fine-tuning data, resultingin significant performance improvements: +33% and +37% for CLIP and LLaVA,respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, weexploit the capabilities of high-performing text generation and imagegeneration models, specifically GPT-4V and DALLE-3, to curate challengingsemantic counterfactuals, thereby further enhancing compositional reasoningcapabilities on benchmarks such as SugarCrepe, where CounterCurate outperformsGPT-4V. To facilitate future research, we release our code, dataset, benchmark,and checkpoints at https://countercurate.github.io.</description><author>Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee</author><pubDate>Wed, 12 Jun 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13254v4</guid></item><item><title>ICE-G: Image Conditional Editing of 3D Gaussian Splats</title><link>http://arxiv.org/abs/2406.08488v1</link><description>Recently many techniques have emerged to create high quality 3D assets andscenes. When it comes to editing of these objects, however, existing approachesare either slow, compromise on quality, or do not provide enough customization.We introduce a novel approach to quickly edit a 3D model from a singlereference view. Our technique first segments the edit image, and then matchessemantically corresponding regions across chosen segmented dataset views usingDINO features. A color or texture change from a particular region of the editimage can then be applied to other views automatically in a semanticallysensible manner. These edited views act as an updated dataset to further trainand re-style the 3D scene. The end-result is therefore an edited 3D model. Ourframework enables a wide variety of editing tasks such as manual local edits,correspondence based style transfer from any example image, and a combinationof different styles from multiple example images. We use Gaussian Splats as ourprimary 3D representation due to their speed and ease of local editing, but ourtechnique works for other methods such as NeRFs as well. We show throughmultiple examples that our method produces higher quality results whileoffering fine-grained control of editing. Project page: ice-gaussian.github.io</description><author>Vishnu Jaganathan, Hannah Hanyun Huang, Muhammad Zubair Irshad, Varun Jampani, Amit Raj, Zsolt Kira</author><pubDate>Wed, 12 Jun 2024 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08488v1</guid></item><item><title>Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models</title><link>http://arxiv.org/abs/2406.08487v1</link><description>Seeing clearly with high resolution is a foundation of Large MultimodalModels (LMMs), which has been proven to be vital for visual perception andreasoning. Existing works usually employ a straightforward resolution upscalingmethod, where the image consists of global and local branches, with the latterbeing the sliced image patches but resized to the same resolution as theformer. This means that higher resolution requires more local patches,resulting in exorbitant computational expenses, and meanwhile, the dominance oflocal image tokens may diminish the global context. In this paper, we dive intothe problems and propose a new framework as well as an elaborate optimizationstrategy. Specifically, we extract contextual information from the global viewusing a mixture of adapters, based on the observation that different adaptersexcel at different tasks. With regard to local patches, learnable queryembeddings are introduced to reduce image tokens, the most important tokensaccounting for the user question will be further selected by a similarity-basedselector. Our empirical results demonstrate a `less is more' pattern, where\textit{utilizing fewer but more informative local image tokens leads toimproved performance}. Besides, a significant challenge lies in the trainingstrategy, as simultaneous end-to-end training of the global mining block andlocal compression block does not yield optimal results. We thus advocate for analternating training way, ensuring balanced learning between global and localaspects. Finally, we also introduce a challenging dataset with highrequirements for image detail, enhancing the training of the local compressionlayer. The proposed method, termed LMM with Sophisticated Tasks, Local imagecompression, and Mixture of global Experts (SliME), achieves leadingperformance across various benchmarks with only 2 million training data.</description><author>Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, Rong Jin</author><pubDate>Wed, 12 Jun 2024 18:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08487v1</guid></item><item><title>On Evaluating Adversarial Robustness of Volumetric Medical Segmentation Models</title><link>http://arxiv.org/abs/2406.08486v1</link><description>Volumetric medical segmentation models have achieved significant success onorgan and tumor-based segmentation tasks in recent years. However, theirvulnerability to adversarial attacks remains largely unexplored, raisingserious concerns regarding the real-world deployment of tools employing suchmodels in the healthcare sector. This underscores the importance ofinvestigating the robustness of existing models. In this context, our work aimsto empirically examine the adversarial robustness across current volumetricsegmentation architectures, encompassing Convolutional, Transformer, andMamba-based models. We extend this investigation across four volumetricsegmentation datasets, evaluating robustness under both white box and black boxadversarial attacks. Overall, we observe that while both pixel andfrequency-based attacks perform reasonably well under white box setting, thelatter performs significantly better under transfer-based black box attacks.Across our experiments, we observe transformer-based models show higherrobustness than convolution-based models with Mamba-based models being the mostvulnerable. Additionally, we show that large-scale training of volumetricsegmentation models improves the model's robustness against adversarialattacks. The code and pretrained models will be made available athttps://github.com/HashmatShadab/Robustness-of-Volumetric-Medical-Segmentation-Models.</description><author>Hashmat Shadab Malik, Numan Saeed, Asif Hanif, Muzammal Naseer, Mohammad Yaqub, Salman Khan, Fahad Shahbaz Khan</author><pubDate>Wed, 12 Jun 2024 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08486v1</guid></item><item><title>Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation</title><link>http://arxiv.org/abs/2406.08482v1</link><description>Diffusion models are the state of the art in text-to-image generation, buttheir perceptual variability remains understudied. In this paper, we examinehow prompts affect image variability in black-box diffusion-based models. Wepropose W1KP, a human-calibrated measure of variability in a set of images,bootstrapped from existing image-pair perceptual distances. Current datasets donot cover recent diffusion models, thus we curate three test sets forevaluation. Our best perceptual distance outperforms nine baselines by up to 18points in accuracy, and our calibration matches graded human judgements 78% ofthe time. Using W1KP, we study prompt reusability and show that Imagen promptscan be reused for 10-50 random seeds before new images become too similar toalready generated images, while Stable Diffusion XL and DALL-E 3 can be reused50-200 times. Lastly, we analyze 56 linguistic features of real prompts,finding that the prompt's length, CLIP embedding norm, concreteness, and wordsenses influence variability most. As far as we are aware, we are the first toanalyze diffusion variability from a visuolinguistic perspective. Our projectpage is at http://w1kp.com</description><author>Raphael Tang, Xinyu Zhang, Lixinyu Xu, Yao Lu, Wenyan Li, Pontus Stenetorp, Jimmy Lin, Ferhan Ture</author><pubDate>Wed, 12 Jun 2024 18:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08482v1</guid></item><item><title>Enhancing End-to-End Autonomous Driving with Latent World Model</title><link>http://arxiv.org/abs/2406.08481v1</link><description>End-to-end autonomous driving has garnered widespread attention. Currentend-to-end approaches largely rely on the supervision from perception taskssuch as detection, tracking, and map segmentation to aid in learning scenerepresentations. However, these methods require extensive annotations,hindering the data scalability. To address this challenge, we propose a novelself-supervised method to enhance end-to-end driving without the need forcostly labels. Specifically, our framework \textbf{LAW} uses a LAtent Worldmodel to predict future latent features based on the predicted ego actions andthe latent feature of the current frame. The predicted latent features aresupervised by the actually observed features in the future. This supervisionjointly optimizes the latent feature learning and action prediction, whichgreatly enhances the driving performance. As a result, our approach achievesstate-of-the-art performance in both open-loop and closed-loop benchmarkswithout costly annotations.</description><author>Yingyan Li, Lue Fan, Jiawei He, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang, Tieniu Tan</author><pubDate>Wed, 12 Jun 2024 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08481v1</guid></item><item><title>Real3D: Scaling Up Large Reconstruction Models with Real-World Images</title><link>http://arxiv.org/abs/2406.08479v1</link><description>The default strategy for training single-view Large Reconstruction Models(LRMs) follows the fully supervised route using large-scale datasets ofsynthetic 3D assets or multi-view captures. Although these resources simplifythe training procedure, they are hard to scale up beyond the existing datasetsand they are not necessarily representative of the real distribution of objectshapes. To address these limitations, in this paper, we introduce Real3D, thefirst LRM system that can be trained using single-view real-world images.Real3D introduces a novel self-training framework that can benefit from boththe existing synthetic data and diverse single-view real images. We propose twounsupervised losses that allow us to supervise LRMs at the pixel- andsemantic-level, even for training examples without ground-truth 3D or novelviews. To further improve performance and scale up the image data, we developan automatic data curation approach to collect high-quality examples fromin-the-wild images. Our experiments show that Real3D consistently outperformsprior work in four diverse evaluation settings that include real and syntheticdata, as well as both in-domain and out-of-domain shapes. Code and model can befound here: https://hwjiang1510.github.io/Real3D/</description><author>Hanwen Jiang, Qixing Huang, Georgios Pavlakos</author><pubDate>Wed, 12 Jun 2024 18:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08479v1</guid></item><item><title>What If We Recaption Billions of Web Images with LLaMA-3?</title><link>http://arxiv.org/abs/2406.08478v1</link><description>Web-crawled image-text pairs are inherently noisy. Prior studies demonstratethat semantically aligning and enriching textual descriptions of these pairscan significantly enhance model training across various vision-language tasks,particularly text-to-image generation. However, large-scale investigations inthis area remain predominantly closed-source. Our paper aims to bridge thiscommunity effort, leveraging the powerful and \textit{open-sourced} LLaMA-3, aGPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune aLLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion imagesfrom the DataComp-1B dataset. Our empirical results confirm that this enhanceddataset, Recap-DataComp-1B, offers substantial benefits in training advancedvision-language models. For discriminative models like CLIP, we observeenhanced zero-shot performance in cross-modal retrieval tasks. For generativemodels like text-to-image Diffusion Transformers, the generated images exhibita significant improvement in alignment with users' text instructions,especially in following complex queries. Our project page ishttps://www.haqtu.me/Recap-Datacomp-1B/</description><author>Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, Cihang Xie</author><pubDate>Wed, 12 Jun 2024 18:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08478v1</guid></item><item><title>RMem: Restricted Memory Banks Improve Video Object Segmentation</title><link>http://arxiv.org/abs/2406.08476v1</link><description>With recent video object segmentation (VOS) benchmarks evolving tochallenging scenarios, we revisit a simple but overlooked strategy: restrictingthe size of memory banks. This diverges from the prevalent practice ofexpanding memory banks to accommodate extensive historical information. Ourspecially designed "memory deciphering" study offers a pivotal insightunderpinning such a strategy: expanding memory banks, while seeminglybeneficial, actually increases the difficulty for VOS modules to decoderelevant features due to the confusion from redundant information. Byrestricting memory banks to a limited number of essential frames, we achieve anotable improvement in VOS accuracy. This process balances the importance andfreshness of frames to maintain an informative memory bank within a boundedcapacity. Additionally, restricted memory banks reduce the training-inferencediscrepancy in memory lengths compared with continuous expansion. This fostersnew opportunities in temporal reasoning and enables us to introduce thepreviously overlooked "temporal positional embedding." Finally, our insightsare embodied in "RMem" ("R" for restricted), a simple yet effective VOSmodification that excels at challenging VOS scenarios and establishes new stateof the art for object state changes (on the VOST dataset) and long videos (onthe Long Videos dataset). Our code and demo are available athttps://restricted-memory.github.io/.</description><author>Junbao Zhou, Ziqi Pang, Yu-Xiong Wang</author><pubDate>Wed, 12 Jun 2024 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08476v1</guid></item><item><title>Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models</title><link>http://arxiv.org/abs/2406.08475v1</link><description>Creating realistic avatars from a single RGB image is an attractive yetchallenging problem. Due to its ill-posed nature, recent works leveragepowerful prior from 2D diffusion models pretrained on large datasets. Although2D diffusion models demonstrate strong generalization capability, they cannotprovide multi-view shape priors with guaranteed 3D consistency. We proposeHuman 3Diffusion: Realistic Avatar Creation via Explicit 3D ConsistentDiffusion. Our key insight is that 2D multi-view diffusion and 3Dreconstruction models provide complementary information for each other, and bycoupling them in a tight manner, we can fully leverage the potential of bothmodels. We introduce a novel image-conditioned generative 3D Gaussian Splatsreconstruction model that leverages the priors from 2D multi-view diffusionmodels, and provides an explicit 3D representation, which further guides the 2Dreverse sampling process to have better 3D consistency. Experiments show thatour proposed framework outperforms state-of-the-art methods and enables thecreation of realistic avatars from a single RGB image, achieving high-fidelityin both geometry and appearance. Extensive ablations also validate the efficacyof our design, (1) multi-view 2D priors conditioning in generative 3Dreconstruction and (2) consistency refinement of sampling trajectory via theexplicit 3D representation. Our code and models will be released onhttps://yuxuan-xue.com/human-3diffusion.</description><author>Yuxuan Xue, Xianghui Xie, Riccardo Marin, Gerard Pons-Moll</author><pubDate>Wed, 12 Jun 2024 18:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08475v1</guid></item><item><title>Real2Code: Reconstruct Articulated Objects via Code Generation</title><link>http://arxiv.org/abs/2406.08474v1</link><description>We present Real2Code, a novel approach to reconstructing articulated objectsvia code generation. Given visual observations of an object, we firstreconstruct its part geometry using an image segmentation model and a shapecompletion model. We then represent the object parts with oriented boundingboxes, which are input to a fine-tuned large language model (LLM) to predictjoint articulation as code. By leveraging pre-trained vision and languagemodels, our approach scales elegantly with the number of articulated parts, andgeneralizes from synthetic training data to real world objects in unstructuredenvironments. Experimental results demonstrate that Real2Code significantlyoutperforms previous state-of-the-art in reconstruction accuracy, and is thefirst approach to extrapolate beyond objects' structural complexity in thetraining set, and reconstructs objects with up to 10 articulated parts. Whenincorporated with a stereo reconstruction model, Real2Code also generalizes toreal world objects from a handful of multi-view RGB images, without the needfor depth or camera information.</description><author>Zhao Mandi, Yijia Weng, Dominik Bauer, Shuran Song</author><pubDate>Wed, 12 Jun 2024 18:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08474v1</guid></item><item><title>Strategies for Pretraining Neural Operators</title><link>http://arxiv.org/abs/2406.08473v1</link><description>Pretraining for partial differential equation (PDE) modeling has recentlyshown promise in scaling neural operators across datasets to improvegeneralizability and performance. Despite these advances, our understanding ofhow pretraining affects neural operators is still limited; studies generallypropose tailored architectures and datasets that make it challenging to compareor examine different pretraining frameworks. To address this, we comparevarious pretraining methods without optimizing architecture choices tocharacterize pretraining dynamics on different models and datasets as well asto understand its scaling and generalization behavior. We find that pretrainingis highly dependent on model and dataset choices, but in general transferlearning or physics-based pretraining strategies work best. In addition,pretraining performance can be further improved by using data augmentations.Lastly, pretraining is additionally beneficial when fine-tuning in scarce dataregimes or when generalizing to downstream data similar to the pretrainingdistribution. Through providing insights into pretraining neural operators forphysics prediction, we hope to motivate future work in developing andevaluating pretraining methods for PDEs.</description><author>Anthony Zhou, Cooper Lorsung, AmirPouya Hemmasian, Amir Barati Farimani</author><pubDate>Wed, 12 Jun 2024 18:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08473v1</guid></item><item><title>RILe: Reinforced Imitation Learning</title><link>http://arxiv.org/abs/2406.08472v1</link><description>Reinforcement Learning has achieved significant success in generating complexbehavior but often requires extensive reward function engineering. Adversarialvariants of Imitation Learning and Inverse Reinforcement Learning offer analternative by learning policies from expert demonstrations via adiscriminator. Employing discriminators increases their data- and computationalefficiency over the standard approaches; however, results in sensitivity toimperfections in expert data. We propose RILe, a teacher-student system thatachieves both robustness to imperfect data and efficiency. In RILe, the studentlearns an action policy while the teacher dynamically adjusts a reward functionbased on the student's performance and its alignment with expertdemonstrations. By tailoring the reward function to both performance of thestudent and expert similarity, our system reduces dependence on thediscriminator and, hence, increases robustness against data imperfections.Experiments show that RILe outperforms existing methods by 2x in settings withlimited or noisy expert data.</description><author>Mert Albaba, Sammy Christen, Christoph Gebhardt, Thomas Langarek, Michael J. Black, Otmar Hilliges</author><pubDate>Wed, 12 Jun 2024 18:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08472v1</guid></item><item><title>Surprise! Using Physiological Stress for Allostatic Regulation Under the Active Inference Framework [Pre-Print]</title><link>http://arxiv.org/abs/2406.08471v1</link><description>Allostasis proposes that long-term viability of a living system is achievedthrough anticipatory adjustments of its physiology and behaviour: emphasisingphysiological and affective stress as an adaptive state of adaptation thatminimizes long-term prediction errors. More recently, the active inferenceframework (AIF) has also sought to explain action and long-term adaptationthrough the minimization of future errors (free energy), through the learningof statistical contingencies of the world, offering a formalism for allostaticregulation. We suggest that framing prediction errors through the lens ofbiological hormonal dynamics proposed by allostasis offers a way to integratethese two models together in a biologically-plausible manner. In this paper, wedescribe our initial work in developing a model that grounds prediction errors(surprisal) into the secretion of a physiological stress hormone (cortisol)acting as an adaptive, allostatic mediator on a homeostatically-controlledphysiology. We evaluate this using a computational model in simulations usingan active inference agent endowed with an artificial physiology, regulatedthrough homeostatic and allostatic control in a stochastic environment. Ourresults find that allostatic functions of cortisol (stress), secreted as afunction of prediction errors, provide adaptive advantages to the agent'slong-term physiological regulation. We argue that the coupling ofinformation-theoretic prediction errors to low-level, biological hormonaldynamics of stress can provide a computationally efficient model to long-termregulation for embodied intelligent systems.</description><author>Imran Khan, Robert Lowe</author><pubDate>Wed, 12 Jun 2024 18:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08471v1</guid></item><item><title>A flexible empirical Bayes approach to multiple linear regression and connections with penalized regression</title><link>http://arxiv.org/abs/2208.10910v3</link><description>We introduce a new empirical Bayes approach for large-scale multiple linearregression. Our approach combines two key ideas: (i) the use of flexible"adaptive shrinkage" priors, which approximate the nonparametric family ofscale mixture of normal distributions by a finite mixture of normaldistributions; and (ii) the use of variational approximations to efficientlyestimate prior hyperparameters and compute approximate posteriors. Combiningthese two ideas results in fast and flexible methods, with computational speedcomparable to fast penalized regression methods such as the Lasso, and withcompetitive prediction accuracy across a wide range of scenarios. Further, weprovide new results that establish conceptual connections between our empiricalBayes methods and penalized methods. Specifically, we show that the posteriormean from our method solves a penalized regression problem, with the form ofthe penalty function being learned from the data by directly solving anoptimization problem (rather than being tuned by cross-validation). Our methodsare implemented in an R package, mr.ash.alpha, available fromhttps://github.com/stephenslab/mr.ash.alpha.</description><author>Youngseok Kim, Wei Wang, Peter Carbonetto, Matthew Stephens</author><pubDate>Wed, 12 Jun 2024 18:54:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.10910v3</guid></item><item><title>PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences</title><link>http://arxiv.org/abs/2406.08469v1</link><description>Large foundation models pretrained on raw web-scale data are not readilydeployable without additional step of extensive alignment to human preferences.Such alignment is typically done by collecting large amounts of pairwisecomparisons from humans ("Do you prefer output A or B?") and learning a rewardmodel or a policy with the Bradley-Terry-Luce (BTL) model as a proxy for ahuman's underlying implicit preferences. These methods generally suffer fromassuming a universal preference shared by all humans, which lacks theflexibility of adapting to plurality of opinions and preferences. In this work,we propose PAL, a framework to model human preference complementary to existingpretraining strategies, which incorporates plurality from the ground up. Wepropose using the ideal point model as a lens to view alignment usingpreference comparisons. Together with our novel reformulation and using mixturemodeling, our framework captures the plurality of population preferences whilesimultaneously learning a common preference latent space across differentpreferences, which can few-shot generalize to new, unseen users. Our approachenables us to use the penultimate-layer representation of large foundationmodels and simple MLP layers to learn reward functions that are on-par with theexisting large state-of-the-art reward models, thereby enhancing efficiency ofreward modeling significantly. We show that PAL achieves competitive rewardmodel accuracy compared to strong baselines on 1) Language models with Summarydataset ; 2) Image Generative models with Pick-a-Pic dataset ; 3) A newsemisynthetic heterogeneous dataset generated using Anthropic Personas.Finally, our experiments also highlight the shortcoming of current preferencedatasets that are created using rigid rubrics which wash away heterogeneity,and call for more nuanced data collection approaches.</description><author>Daiwei Chen, Yi Chen, Aniket Rege, Ramya Korlakai Vinayak</author><pubDate>Wed, 12 Jun 2024 18:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08469v1</guid></item><item><title>DafnyBench: A Benchmark for Formal Software Verification</title><link>http://arxiv.org/abs/2406.08467v1</link><description>We introduce DafnyBench, the largest benchmark of its kind for training andevaluating machine learning systems for formal software verification. We testthe ability of LLMs such as GPT-4 and Claude 3 to auto-generate enough hintsfor the Dafny formal verification engine to successfully verify over 750programs with about 53,000 lines of code. The best model and prompting schemeachieved 68% success rate, and we quantify how this rate improves when retryingwith error message feedback and how it deteriorates with the amount of requiredcode and hints. We hope that DafnyBench will enable rapid improvements fromthis baseline as LLMs and verification techniques grow in quality.</description><author>Chloe Loughridge, Qinyi Sun, Seth Ahrenbach, Federico Cassano, Chuyue Sun, Ying Sheng, Anish Mudide, Md Rakib Hossain Misu, Nada Amin, Max Tegmark</author><pubDate>Wed, 12 Jun 2024 18:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08467v1</guid></item><item><title>Scaling Laws in Linear Regression: Compute, Parameters, and Data</title><link>http://arxiv.org/abs/2406.08466v1</link><description>Empirically, large-scale deep learning models often satisfy a neural scalinglaw: the test error of the trained model improves polynomially as the modelsize and data size grow. However, conventional wisdom suggests the test errorconsists of approximation, bias, and variance errors, where the variance errorincreases with model size. This disagrees with the general form of neuralscaling laws, which predict that increasing model size monotonically improvesperformance. We study the theory of scaling laws in an infinite dimensional linearregression setup. Specifically, we consider a model with $M$ parameters as alinear function of sketched covariates. The model is trained by one-passstochastic gradient descent (SGD) using $N$ data. Assuming the optimalparameter satisfies a Gaussian prior and the data covariance matrix has apower-law spectrum of degree $a&gt;1$, we show that the reducible part of the testerror is $\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, whichincreases with $M$, is dominated by the other errors due to the implicitregularization of SGD, thus disappearing from the bound. Our theory isconsistent with the empirical neural scaling laws and verified by numericalsimulation.</description><author>Licong Lin, Jingfeng Wu, Sham M. Kakade, Peter L. Bartlett, Jason D. Lee</author><pubDate>Wed, 12 Jun 2024 18:53:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08466v1</guid></item><item><title>Nonconvex Federated Learning on Compact Smooth Submanifolds With Heterogeneous Data</title><link>http://arxiv.org/abs/2406.08465v1</link><description>Many machine learning tasks, such as principal component analysis andlow-rank matrix completion, give rise to manifold optimization problems.Although there is a large body of work studying the design and analysis ofalgorithms for manifold optimization in the centralized setting, there arecurrently very few works addressing the federated setting. In this paper, weconsider nonconvex federated learning over a compact smooth submanifold in thesetting of heterogeneous client data. We propose an algorithm that leveragesstochastic Riemannian gradients and a manifold projection operator to improvecomputational efficiency, uses local updates to improve communicationefficiency, and avoids client drift. Theoretically, we show that our proposedalgorithm converges sub-linearly to a neighborhood of a first-order optimalsolution by using a novel analysis that jointly exploits the manifold structureand properties of the loss functions. Numerical experiments demonstrate thatour algorithm has significantly smaller computational and communicationoverhead than existing methods.</description><author>Jiaojiao Zhang, Jiang Hu, Anthony Man-Cho So, Mikael Johansson</author><pubDate>Wed, 12 Jun 2024 18:53:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08465v1</guid></item><item><title>Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing</title><link>http://arxiv.org/abs/2406.08464v1</link><description>High-quality instruction data is critical for aligning large language models(LLMs). Although some models, such as Llama-3-Instruct, have open weights,their alignment data remain private, which hinders the democratization of AI.High human labor costs and a limited, predefined scope for prompting preventexisting open-source data creation methods from scaling effectively,potentially limiting the diversity and quality of public alignment datasets. Isit possible to synthesize high-quality instruction data at scale by extractingit directly from an aligned LLM? We present a self-synthesis method forgenerating large-scale alignment data named Magpie. Our key observation is thataligned LLMs like Llama-3-Instruct can generate a user query when we input onlythe left-side templates up to the position reserved for user messages, thanksto their auto-regressive nature. We use this method to prompt Llama-3-Instructand generate 4 million instructions along with their corresponding responses.We perform a comprehensive analysis of the extracted data and select 300Khigh-quality instances. To compare Magpie data with other public instructiondatasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate theperformance of the fine-tuned models. Our results indicate that in some tasks,models fine-tuned with Magpie perform comparably to the officialLlama-3-8B-Instruct, despite the latter being enhanced with 10 million datapoints through supervised fine-tuning (SFT) and subsequent feedback learning.We also show that using Magpie solely for SFT can surpass the performance ofprevious public datasets utilized for both SFT and preference optimization,such as direct preference optimization with UltraFeedback. This advantage isevident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.</description><author>Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, Bill Yuchen Lin</author><pubDate>Wed, 12 Jun 2024 18:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08464v1</guid></item><item><title>Self-supervised Learning of Neural Implicit Feature Fields for Camera Pose Refinement</title><link>http://arxiv.org/abs/2406.08463v1</link><description>Visual localization techniques rely upon some underlying scene representationto localize against. These representations can be explicit such as 3D SFM mapor implicit, such as a neural network that learns to encode the scene. Theformer requires sparse feature extractors and matchers to build the scenerepresentation. The latter might lack geometric grounding not capturing the 3Dstructure of the scene well enough. This paper proposes to jointly learn thescene representation along with a 3D dense feature field and a 2D featureextractor whose outputs are embedded in the same metric space. Through acontrastive framework we align this volumetric field with the image-basedextractor and regularize the latter with a ranking loss from learned surfaceinformation. We learn the underlying geometry of the scene with an implicitfield through volumetric rendering and design our feature field to leverageintermediate geometric information encoded in the implicit field. The resultingfeatures are discriminative and robust to viewpoint change while maintainingrich encoded information. Visual localization is then achieved by aligning theimage-based features and the rendered volumetric features. We show theeffectiveness of our approach on real-world scenes, demonstrating that ourapproach outperforms prior and concurrent work on leveraging implicit scenerepresentations for localization.</description><author>Maxime Pietrantoni, Gabriela Csurka, Martin Humenberger, Torsten Sattler</author><pubDate>Wed, 12 Jun 2024 18:51:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08463v1</guid></item><item><title>ConceptHash: Interpretable Fine-Grained Hashing via Concept Discovery</title><link>http://arxiv.org/abs/2406.08457v1</link><description>Existing fine-grained hashing methods typically lack code interpretability asthey compute hash code bits holistically using both global and local features.To address this limitation, we propose ConceptHash, a novel method thatachieves sub-code level interpretability. In ConceptHash, each sub-codecorresponds to a human-understandable concept, such as an object part, andthese concepts are automatically discovered without human annotations.Specifically, we leverage a Vision Transformer architecture and introduceconcept tokens as visual prompts, along with image patch tokens as modelinputs. Each concept is then mapped to a specific sub-code at the model output,providing natural sub-code interpretability. To capture subtle visualdifferences among highly similar sub-categories (e.g., bird species), weincorporate language guidance to ensure that the learned hash codes aredistinguishable within fine-grained object classes while maintaining semanticalignment. This approach allows us to develop hash codes that exhibitsimilarity within families of species while remaining distinct from species inother families. Extensive experiments on four fine-grained image retrievalbenchmarks demonstrate that ConceptHash outperforms previous methods by asignificant margin, offering unique sub-code interpretability as an additionalbenefit. Code at: https://github.com/kamwoh/concepthash.</description><author>Kam Woh Ng, Xiatian Zhu, Yi-Zhe Song, Tao Xiang</author><pubDate>Wed, 12 Jun 2024 18:49:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08457v1</guid></item><item><title>Knowledge-Induced Medicine Prescribing Network for Medication Recommendation</title><link>http://arxiv.org/abs/2310.14552v2</link><description>Extensive adoption of electronic health records (EHRs) offers opportunitiesfor their use in various downstream clinical analyses. To accomplish thispurpose, enriching an EHR cohort with external knowledge (e.g., standardizedmedical ontology and wealthy semantics) could help us reveal more comprehensiveinsights via a spectrum of informative relations among medical codes.Nevertheless, harnessing those beneficial interconnections was scarcelyexercised, especially in the medication recommendation task. This studyproposes a novel Knowledge-Induced Medicine Prescribing Network (KindMed) torecommend medicines by inducing knowledge from myriad medical-related externalsources upon the EHR cohort and rendering interconnected medical codes asmedical knowledge graphs (KGs). On top of relation-aware graph representationlearning to obtain an adequate embedding over such KGs, we leveragehierarchical sequence learning to discover and fuse temporal dynamics ofclinical (i.e., diagnosis and procedures) and medicine streams across patients'historical admissions to foster personalized recommendations. Eventually, weemploy attentive prescribing that accounts for three essential patientrepresentations, i.e., a summary of joint historical medical records, clinicalprogression, and the current clinical state of patients. We validated theeffectiveness of our KindMed on the augmented real-world EHR cohorts, achievingimproved recommendation performances against a handful of graph-drivenbaselines.</description><author>Ahmad Wisnu Mulyadi, Heung-Il Suk</author><pubDate>Wed, 12 Jun 2024 18:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14552v2</guid></item><item><title>GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices</title><link>http://arxiv.org/abs/2406.08451v1</link><description>Smartphone users often navigate across multiple applications (apps) tocomplete tasks such as sharing content between social media platforms.Autonomous Graphical User Interface (GUI) navigation agents can enhance userexperience in communication, entertainment, and productivity by streamliningworkflows and reducing manual intervention. However, prior GUI agents oftentrained with datasets comprising simple tasks that can be completed within asingle app, leading to poor performance in cross-app navigation. To addressthis problem, we introduce GUI Odyssey, a comprehensive dataset for trainingand evaluating cross-app navigation agents. GUI Odyssey consists of 7,735episodes from 6 mobile devices, spanning 6 types of cross-app tasks, 201 apps,and 1.4K app combos. Leveraging GUI Odyssey, we developed OdysseyAgent, amultimodal cross-app navigation agent by fine-tuning the Qwen-VL model with ahistory resampling module. Extensive experiments demonstrate OdysseyAgent'ssuperior accuracy compared to existing models. For instance, OdysseyAgentsurpasses fine-tuned Qwen-VL and zero-shot GPT-4V by 1.44\% and 55.49\%in-domain accuracy, and 2.29\% and 48.14\% out-of-domain accuracy on average.The dataset and code will be released in\url{https://github.com/OpenGVLab/GUI-Odyssey}.</description><author>Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, Ping Luo</author><pubDate>Wed, 12 Jun 2024 18:44:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08451v1</guid></item><item><title>The Impact of Initialization on LoRA Finetuning Dynamics</title><link>http://arxiv.org/abs/2406.08447v1</link><description>In this paper, we study the role of initialization in Low Rank Adaptation(LoRA) as originally introduced in Hu et al. (2021). Essentially, to start fromthe pretrained model as initialization for finetuning, one can eitherinitialize B to zero and A to random (default initialization in PEFT package),or vice-versa. In both cases, the product BA is equal to zero atinitialization, which makes finetuning starts from the pretrained model. Thesetwo initialization schemes are seemingly similar. They should in-principleyield the same performance and share the same optimal learning rate. Wedemonstrate that this is an incorrect intuition and that the first scheme(initializing B to zero and A to random) on average yields better performancecompared to the other scheme. Our theoretical analysis shows that the reasonbehind this might be that the first initialization allows the use of largerlearning rates (without causing output instability) compared to the secondinitialization, resulting in more efficient learning of the first scheme. Wevalidate our results with extensive experiments on LLMs.</description><author>Soufiane Hayou, Nikhil Ghosh, Bin Yu</author><pubDate>Wed, 12 Jun 2024 18:38:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08447v1</guid></item><item><title>SVSNet+: Enhancing Speaker Voice Similarity Assessment Models with Representations from Speech Foundation Models</title><link>http://arxiv.org/abs/2406.08445v1</link><description>Representations from pre-trained speech foundation models (SFMs) have shownimpressive performance in many downstream tasks. However, the potentialbenefits of incorporating pre-trained SFM representations into speaker voicesimilarity assessment have not been thoroughly investigated. In this paper, wepropose SVSNet+, a model that integrates pre-trained SFM representations toimprove performance in assessing speaker voice similarity. Experimental resultson the Voice Conversion Challenge 2018 and 2020 datasets show that SVSNet+incorporating WavLM representations shows significant improvements compared tobaseline models. In addition, while fine-tuning WavLM with a small dataset ofthe downstream task does not improve performance, using the same dataset tolearn a weighted-sum representation of WavLM can substantially improveperformance. Furthermore, when WavLM is replaced by other SFMs, SVSNet+ stilloutperforms the baseline models and exhibits strong generalization ability.</description><author>Chun Yin, Tai-Shih Chi, Yu Tsao, Hsin-Min Wang</author><pubDate>Wed, 12 Jun 2024 18:37:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08445v1</guid></item><item><title>OLMES: A Standard for Language Model Evaluations</title><link>http://arxiv.org/abs/2406.08446v1</link><description>Progress in AI is often demonstrated by new models claiming improvedperformance on tasks measuring model capabilities. Evaluating language modelsin particular is challenging, as small changes to how a model is evaluated on atask can lead to large changes in measured performance. There is no commonstandard setup, so different models are evaluated on the same tasks indifferent ways, leading to claims about which models perform best not beingreproducible. We propose OLMES, a completely documented, practical, openstandard for reproducible LLM evaluations. In developing this standard, weidentify and review the varying factors in evaluation practices adopted by thecommunity - such as details of prompt formatting, choice of in-contextexamples, probability normalizations, and task formulation. In particular,OLMES supports meaningful comparisons between smaller base models that requirethe unnatural "cloze" formulation of multiple-choice questions against largermodels that can utilize the original formulation. OLMES includeswell-considered recommendations guided by results from existing literature aswell as new experiments investigating open questions.</description><author>Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, Hannaneh Hajishirzi</author><pubDate>Wed, 12 Jun 2024 18:37:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08446v1</guid></item><item><title>PixMamba: Leveraging State Space Models in a Dual-Level Architecture for Underwater Image Enhancement</title><link>http://arxiv.org/abs/2406.08444v1</link><description>Underwater Image Enhancement (UIE) is critical for marine research andexploration but hindered by complex color distortions and severe blurring.Recent deep learning-based methods have achieved remarkable results, yet thesemethods struggle with high computational costs and insufficient globalmodeling, resulting in locally under- or over- adjusted regions. We presentPixMamba, a novel architecture, designed to overcome these challenges byleveraging State Space Models (SSMs) for efficient global dependency modeling.Unlike convolutional neural networks (CNNs) with limited receptive fields andtransformer networks with high computational costs, PixMamba efficientlycaptures global contextual information while maintaining computationalefficiency. Our dual-level strategy features the patch-level Efficient MambaNet (EMNet) for reconstructing enhanced image feature and the pixel-levelPixMamba Net (PixNet) to ensure fine-grained feature capturing and globalconsistency of enhanced image that were previously difficult to obtain.PixMamba achieves state-of-the-art performance across various underwater imagedatasets and delivers visually superior results. Code is available at:https://github.com/weitunglin/pixmamba.</description><author>Wei-Tung Lin, Yong-Xiang Lin, Jyun-Wei Chen, Kai-Lung Hua</author><pubDate>Wed, 12 Jun 2024 18:34:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08444v1</guid></item><item><title>Transformation-Dependent Adversarial Attacks</title><link>http://arxiv.org/abs/2406.08443v1</link><description>We introduce transformation-dependent adversarial attacks, a new class ofthreats where a single additive perturbation can trigger diverse, controllablemis-predictions by systematically transforming the input (e.g., scaling,blurring, compression). Unlike traditional attacks with static effects, ourperturbations embed metamorphic properties to enable different adversarialattacks as a function of the transformation parameters. We demonstrate thetransformation-dependent vulnerability across models (e.g., convolutionalnetworks and vision transformers) and vision tasks (e.g., image classificationand object detection). Our proposed geometric and photometric transformationsenable a range of targeted errors from one crafted input (e.g., higher than 90%attack success rate for classifiers). We analyze effects of model architectureand type/variety of transformations on attack effectiveness. This work forces aparadigm shift by redefining adversarial inputs as dynamic, controllablethreats. We highlight the need for robust defenses against such multifaceted,chameleon-like perturbations that current techniques are ill-prepared for.</description><author>Yaoteng Tan, Zikui Cai, M. Salman Asif</author><pubDate>Wed, 12 Jun 2024 18:31:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08443v1</guid></item><item><title>Higher-Order Newton Methods with Polynomial Work per Iteration</title><link>http://arxiv.org/abs/2311.06374v2</link><description>We present generalizations of Newton's method that incorporate derivatives ofan arbitrary order $d$ but maintain a polynomial dependence on dimension intheir cost per iteration. At each step, our $d^{\text{th}}$-order method usessemidefinite programming to construct and minimize a sum of squares-convexapproximation to the $d^{\text{th}}$-order Taylor expansion of the function wewish to minimize. We prove that our $d^{\text{th}}$-order method has localconvergence of order $d$. This results in lower oracle complexity compared tothe classical Newton method. We show on numerical examples that basins ofattraction around local minima can get larger as $d$ increases. Underadditional assumptions, we present a modified algorithm, again with polynomialcost per iteration, which is globally convergent and has local convergence oforder $d$.</description><author>Amir Ali Ahmadi, Abraar Chaudhry, Jeffrey Zhang</author><pubDate>Wed, 12 Jun 2024 18:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06374v2</guid></item><item><title>Adaptive Swarm Mesh Refinement using Deep Reinforcement Learning with Local Rewards</title><link>http://arxiv.org/abs/2406.08440v1</link><description>Simulating physical systems is essential in engineering, but analyticalsolutions are limited to straightforward problems. Consequently, numericalmethods like the Finite Element Method (FEM) are widely used. However, the FEMbecomes computationally expensive as problem complexity and accuracy demandsincrease. Adaptive Mesh Refinement (AMR) improves the FEM by dynamicallyallocating mesh elements on the domain, balancing computational speed andaccuracy. Classical AMR depends on heuristics or expensive error estimators,limiting its use in complex simulations. While learning-based AMR methods arepromising, they currently only scale to simple problems. In this work, weformulate AMR as a system of collaborating, homogeneous agents that iterativelysplit into multiple new agents. This agent-wise perspective enables a spatialreward formulation focused on reducing the maximum mesh element error. Ourapproach, Adaptive Swarm Mesh Refinement (ASMR), offers efficient, stableoptimization and generates highly adaptive meshes at user-defined resolutionduring inference. Extensive experiments, including volumetric meshes andNeumann boundary conditions, demonstrate that ASMR exceeds heuristic approachesand learned baselines, matching the performance of expensive error-based oracleAMR strategies. ASMR additionally generalizes to different domains duringinference, and produces meshes that simulate up to 2 orders of magnitude fasterthan uniform refinements in more demanding settings.</description><author>Niklas Freymuth, Philipp Dahlinger, Tobias Würth, Simon Reisch, Luise Kärger, Gerhard Neumann</author><pubDate>Wed, 12 Jun 2024 18:26:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08440v1</guid></item><item><title>Coherent Optical Modems for Full-Wavefield Lidar</title><link>http://arxiv.org/abs/2406.08439v1</link><description>The advent of the digital age has driven the development of coherent opticalmodems -- devices that modulate the amplitude and phase of light in multiplepolarization states. These modems transmit data through fiber optic cables thatare thousands of kilometers in length at data rates exceeding one terabit persecond. This remarkable technology is made possible through near-THz-rateprogrammable control and sensing of the full optical wavefield. While coherentoptical modems form the backbone of telecommunications networks around theworld, their extraordinary capabilities also provide unique opportunities forimaging. Here, we introduce full-wavefield lidar: a new imaging modality thatrepurposes off-the-shelf coherent optical modems to simultaneously measuredistance, axial velocity, and polarization. We demonstrate this modality bycombining a 74 GHz-bandwidth coherent optical modem with free-space couplingoptics and scanning mirrors. We develop a time-resolved image formation modelfor this system and formulate a maximum-likelihood reconstruction algorithm torecover depth, velocity, and polarization information at each scene point fromthe modem's raw transmitted and received symbols. Compared to existing lidars,full-wavefield lidar promises improved mm-scale ranging accuracy from brief,microsecond exposure times, reliable velocimetry, and robustness to intererencefrom ambient light or other lidar signals.</description><author>Parsa Mirdehghan, Brandon Buscaino, Maxx Wu, Doug Charlton, Mohammad E. Mousa-Pasandi, Kiriakos N. Kutulakos, David B. Lindell</author><pubDate>Wed, 12 Jun 2024 18:24:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08439v1</guid></item><item><title>Deep Latent Variable Modeling of Physiological Signals</title><link>http://arxiv.org/abs/2405.19277v3</link><description>A deep latent variable model is a powerful method for capturing complexdistributions. These models assume that underlying structures, but unobserved,are present within the data. In this dissertation, we explore high-dimensionalproblems related to physiological monitoring using latent variable models.First, we present a novel deep state-space model to generate electricalwaveforms of the heart using optically obtained signals as inputs. This canbring about clinical diagnoses of heart disease via simple assessment throughwearable devices. Second, we present a brain signal modeling scheme thatcombines the strengths of probabilistic graphical models and deep adversariallearning. The structured representations can provide interpretability andencode inductive biases to reduce the data complexity of neural oscillations.The efficacy of the learned representations is further studied in epilepsyseizure detection formulated as an unsupervised learning problem. Third, wepropose a framework for the joint modeling of physiological measures andbehavior. Existing methods to combine multiple sources of brain data providedare limited. Direct analysis of the relationship between different types ofphysiological measures usually does not involve behavioral data. Our method canidentify the unique and shared contributions of brain regions to behavior andcan be used to discover new functions of brain regions. The success of theseinnovative computational methods would allow the translation of biomarkerfindings across species and provide insight into neurocognitive analysis innumerous biological studies and clinical diagnoses, as well as emergingconsumer applications.</description><author>Khuong Vo</author><pubDate>Wed, 12 Jun 2024 18:24:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19277v3</guid></item><item><title>TasTe: Teaching Large Language Models to Translate through Self-Reflection</title><link>http://arxiv.org/abs/2406.08434v1</link><description>Large language models (LLMs) have exhibited remarkable performance in variousnatural language processing tasks. Techniques like instruction tuning haveeffectively enhanced the proficiency of LLMs in the downstream task of machinetranslation. However, the existing approaches fail to yield satisfactorytranslation outputs that match the quality of supervised neural machinetranslation (NMT) systems. One plausible explanation for this discrepancy isthat the straightforward prompts employed in these methodologies are unable tofully exploit the acquired instruction-following capabilities. To this end, wepropose the TasTe framework, which stands for translating throughself-reflection. The self-reflection process includes two stages of inference.In the first stage, LLMs are instructed to generate preliminary translationsand conduct self-assessments on these translations simultaneously. In thesecond stage, LLMs are tasked to refine these preliminary translationsaccording to the evaluation results. The evaluation results in four languagedirections on the WMT22 benchmark reveal the effectiveness of our approachcompared to existing methods. Our work presents a promising approach to unleashthe potential of LLMs and enhance their capabilities in MT. The codes anddatasets are open-sourced at https://github.com/YutongWang1216/ReflectionLLMMT.</description><author>Yutong Wang, Jiali Zeng, Xuebo Liu, Fandong Meng, Jie Zhou, Min Zhang</author><pubDate>Wed, 12 Jun 2024 18:21:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08434v1</guid></item><item><title>LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene Graphs with Weak Supervision</title><link>http://arxiv.org/abs/2304.07647v4</link><description>We propose LASER, a neuro-symbolic approach to learn semantic videorepresentations that capture rich spatial and temporal properties in video databy leveraging high-level logic specifications. In particular, we formulate theproblem in terms of alignment between raw videos and spatio-temporal logicspecifications. The alignment algorithm leverages a differentiable symbolicreasoner and a combination of contrastive, temporal, and semantics losses. Iteffectively and efficiently trains low-level perception models to extract afine-grained video representation in the form of a spatio-temporal scene graphthat conforms to the desired high-level specification. To practically reducethe manual effort of obtaining ground truth labels, we derive logicspecifications from captions by employing a large language model with a genericprompting template. In doing so, we explore a novel methodology that weaklysupervises the learning of spatio-temporal scene graphs with widely accessiblevideo-caption data. We evaluate our method on three datasets with rich spatialand temporal specifications: 20BN-Something-Something, MUGEN, and OpenPVSG. Wedemonstrate that our method learns better fine-grained video semantics thanexisting baselines.</description><author>Jiani Huang, Ziyang Li, Mayur Naik, Ser-Nam Lim</author><pubDate>Wed, 12 Jun 2024 18:16:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07647v4</guid></item><item><title>Diffusion Soup: Model Merging for Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2406.08431v1</link><description>We present Diffusion Soup, a compartmentalization method for Text-to-ImageGeneration that averages the weights of diffusion models trained on shardeddata. By construction, our approach enables training-free continual learningand unlearning with no additional memory or inference costs, since modelscorresponding to data shards can be added or removed by re-averaging. We showthat Diffusion Soup samples from a point in weight space that approximates thegeometric mean of the distributions of constituent datasets, which offersanti-memorization guarantees and enables zero-shot style mixing. Empirically,Diffusion Soup outperforms a paragon model trained on the union of all datashards and achieves a 30% improvement in Image Reward (.34 $\to$ .44) on domainsharded data, and a 59% improvement in IR (.37 $\to$ .59) on aesthetic data. Inboth cases, souping also prevails in TIFA score (respectively, 85.5 $\to$ 86.5and 85.6 $\to$ 86.8). We demonstrate robust unlearning -- removing anyindividual domain shard only lowers performance by 1% in IR (.45 $\to$ .44) --and validate our theoretical insights on anti-memorization using real data.Finally, we showcase Diffusion Soup's ability to blend the distinct styles ofmodels finetuned on different shards, resulting in the zero-shot generation ofhybrid styles.</description><author>Benjamin Biggs, Arjun Seshadri, Yang Zou, Achin Jain, Aditya Golatkar, Yusheng Xie, Alessandro Achille, Ashwin Swaminathan, Stefano Soatto</author><pubDate>Wed, 12 Jun 2024 18:16:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08431v1</guid></item><item><title>Dynamic Retrieval Augmented Generation of Ontologies using Artificial Intelligence (DRAGON-AI)</title><link>http://arxiv.org/abs/2312.10904v2</link><description>Background: Ontologies are fundamental components of informaticsinfrastructure in domains such as biomedical, environmental, and food sciences,representing consensus knowledge in an accurate and computable form. However,their construction and maintenance demand substantial resources and necessitatesubstantial collaboration between domain experts, curators, and ontologyexperts. We present Dynamic Retrieval Augmented Generation of Ontologies usingAI (DRAGON-AI), an ontology generation method employing Large Language Models(LLMs) and Retrieval Augmented Generation (RAG). DRAGON-AI can generate textualand logical ontology components, drawing from existing knowledge in multipleontologies and unstructured text sources. Results: We assessed performance of DRAGON-AI on de novo term constructionacross ten diverse ontologies, making use of extensive manual evaluation ofresults. Our method has high precision for relationship generation, but hasslightly lower precision than from logic-based reasoning. Our method is alsoable to generate definitions deemed acceptable by expert evaluators, but thesescored worse than human-authored definitions. Notably, evaluators with thehighest level of confidence in a domain were better able to discern flaws inAI-generated definitions. We also demonstrated the ability of DRAGON-AI toincorporate natural language instructions in the form of GitHub issues. Conclusions: These findings suggest DRAGON-AI's potential to substantiallyaid the manual ontology construction process. However, our results alsounderscore the importance of having expert curators and ontology editors drivethe ontology generation process.</description><author>Sabrina Toro, Anna V Anagnostopoulos, Sue Bello, Kai Blumberg, Rhiannon Cameron, Leigh Carmody, Alexander D Diehl, Damion Dooley, William Duncan, Petra Fey, Pascale Gaudet, Nomi L Harris, Marcin Joachimiak, Leila Kiani, Tiago Lubiana, Monica C Munoz-Torres, Shawn O'Neil, David Osumi-Sutherland, Aleix Puig, Justin P Reese, Leonore Reiser, Sofia Robb, Troy Ruemping, James Seager, Eric Sid, Ray Stefancsik, Magalie Weber, Valerie Wood, Melissa A Haendel, Christopher J Mungall</author><pubDate>Wed, 12 Jun 2024 18:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10904v2</guid></item><item><title>Improving Noise Robustness through Abstractions and its Impact on Machine Learning</title><link>http://arxiv.org/abs/2406.08428v1</link><description>Noise is a fundamental problem in learning theory with huge effects in theapplication of Machine Learning (ML) methods, due to real world data tendencyto be noisy. Additionally, introduction of malicious noise can make ML methodsfail critically, as is the case with adversarial attacks. Thus, finding anddeveloping alternatives to improve robustness to noise is a fundamental problemin ML. In this paper, we propose a method to deal with noise: mitigating itseffect through the use of data abstractions. The goal is to reduce the effectof noise over the model's performance through the loss of information producedby the abstraction. However, this information loss comes with a cost: it canresult in an accuracy reduction due to the missing information. First, weexplored multiple methodologies to create abstractions, using the trainingdataset, for the specific case of numerical data and binary classificationtasks. We also tested how these abstractions can affect robustness to noisewith several experiments that explore the robustness of an Artificial NeuralNetwork to noise when trained using raw data \emph{vs} when trained usingabstracted data. The results clearly show that using abstractions is a viableapproach for developing noise robust ML methods.</description><author>Alfredo Ibias, Karol Capala, Varun Ravi Varma, Anna Drozdz, Jose Sousa</author><pubDate>Wed, 12 Jun 2024 18:14:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08428v1</guid></item><item><title>Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL</title><link>http://arxiv.org/abs/2406.08426v1</link><description>Generating accurate SQL according to natural language questions (text-to-SQL)is a long-standing problem since it is challenging in user questionunderstanding, database schema comprehension, and SQL generation. Conventionaltext-to-SQL systems include human engineering and deep neural networks.Subsequently, pre-trained language models (PLMs) have been developed andutilized for text-to-SQL tasks, achieving promising performance. As moderndatabases become more complex and corresponding user questions morechallenging, PLMs with limited comprehension capabilities can lead to incorrectSQL generation. This necessitates more sophisticated and tailored optimizationmethods, which, in turn, restricts the applications of PLM-based systems. Mostrecently, large language models (LLMs) have demonstrated significant abilitiesin natural language understanding as the model scale remains increasing.Therefore, integrating the LLM-based implementation can bring uniqueopportunities, challenges, and solutions to text-to-SQL research. In thissurvey, we present a comprehensive review of LLM-based text-to-SQL.Specifically, we propose a brief overview of the current challenges and theevolutionary process of text-to-SQL. Then, we provide a detailed introductionto the datasets and metrics designed to evaluate text-to-SQL systems. Afterthat, we present a systematic analysis of recent advances in LLM-basedtext-to-SQL. Finally, we discuss the remaining challenges in this field andpropose expectations for future directions.</description><author>Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran Huang, Xiao Huang</author><pubDate>Wed, 12 Jun 2024 18:13:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08426v1</guid></item><item><title>FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion</title><link>http://arxiv.org/abs/2406.06858v2</link><description>Large deep learning models have demonstrated strong ability to solve manytasks across a wide range of applications. Those large models typically requiretraining and inference to be distributed. Tensor parallelism is a commontechnique partitioning computation of an operation or layer across devices toovercome the memory capacity limitation of a single processor, and/or toaccelerate computation to meet a certain latency requirement. However, thiskind of parallelism introduces additional communication that might contribute asignificant portion of overall runtime. Thus limits scalability of thistechnique within a group of devices with high speed interconnects, such as GPUswith NVLinks in a node. This paper proposes a novel method, Flux, tosignificantly hide communication latencies with dependent computations forGPUs. Flux over-decomposes communication and computation operations into muchfiner-grained operations and further fuses them into a larger kernel toeffectively hide communication without compromising kernel efficiency. Flux canpotentially overlap up to 96% of communication given a fused kernel. Overall,it can achieve up to 1.24x speedups for training over Megatron-LM on a clusterof 128 GPUs with various GPU generations and interconnects, and up to 1.66x and1.30x speedups for prefill and decoding inference over vLLM on a cluster with 8GPUs with various GPU generations and interconnects.</description><author>Liwen Chang, Wenlei Bao, Qi Hou, Chengquan Jiang, Ningxin Zheng, Yinmin Zhong, Xuanrun Zhang, Zuquan Song, Ziheng Jiang, Haibin Lin, Xin Jin, Xin Liu</author><pubDate>Wed, 12 Jun 2024 18:12:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06858v2</guid></item><item><title>AWGUNET: Attention-Aided Wavelet Guided U-Net for Nuclei Segmentation in Histopathology Images</title><link>http://arxiv.org/abs/2406.08425v1</link><description>Accurate nuclei segmentation in histopathological images is crucial forcancer diagnosis. Automating this process offers valuable support to clinicalexperts, as manual annotation is time-consuming and prone to human errors.However, automating nuclei segmentation presents challenges due to uncertaincell boundaries, intricate staining, and diverse structures. In this paper, wepresent a segmentation approach that combines the U-Net architecture with aDenseNet-121 backbone, harnessing the strengths of both to capturecomprehensive contextual and spatial information. Our model introduces theWavelet-guided channel attention module to enhance cell boundary delineation,along with a learnable weighted global attention module for channel-specificattention. The decoder module, composed of an upsample block and convolutionblock, further refines segmentation in handling staining patterns. Theexperimental results conducted on two publicly accessible histopathologydatasets, namely Monuseg and TNBC, underscore the superiority of our proposedmodel, demonstrating its potential to advance histopathological image analysisand cancer diagnosis. The code is made available at:https://github.com/AyushRoy2001/AWGUNET.</description><author>Ayush Roy, Payel Pramanik, Dmitrii Kaplun, Sergei Antonov, Ram Sarkar</author><pubDate>Wed, 12 Jun 2024 18:10:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08425v1</guid></item><item><title>State Soup: In-Context Skill Learning, Retrieval and Mixing</title><link>http://arxiv.org/abs/2406.08423v1</link><description>A new breed of gated-linear recurrent neural networks has reachedstate-of-the-art performance on a range of sequence modeling problems. Suchmodels naturally handle long sequences efficiently, as the cost of processing anew input is independent of sequence length. Here, we explore another advantageof these stateful sequence models, inspired by the success of model mergingthrough parameter interpolation. Building on parallels between fine-tuning andin-context learning, we investigate whether we can treat internal states astask vectors that can be stored, retrieved, and then linearly combined,exploiting the linearity of recurrence. We study this form of fast modelmerging on Mamba-2.8b, a pretrained recurrent model, and present preliminaryevidence that simple linear state interpolation methods suffice to improvenext-token perplexity as well as downstream in-context learning taskperformance.</description><author>Maciej Pióro, Maciej Wołczyk, Razvan Pascanu, Johannes von Oswald, João Sacramento</author><pubDate>Wed, 12 Jun 2024 18:06:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08423v1</guid></item><item><title>PRIBOOT: A New Data-Driven Expert for Improved Driving Simulations</title><link>http://arxiv.org/abs/2406.08421v1</link><description>The development of Autonomous Driving (AD) systems in simulated environmentslike CARLA is crucial for advancing real-world automotive technologies. Todrive innovation, CARLA introduced Leaderboard 2.0, significantly morechallenging than its predecessor. However, current AD methods have struggled toachieve satisfactory outcomes due to a lack of sufficient ground truth data.Human driving logs provided by CARLA are insufficient, and previouslysuccessful expert agents like Autopilot and Roach, used for collectingdatasets, have seen reduced effectiveness under these more demandingconditions. To overcome these data limitations, we introduce PRIBOOT, an expertagent that leverages limited human logs with privileged information. We havedeveloped a novel BEV representation specifically tailored to meet the demandsof this new benchmark and processed it as an RGB image to facilitate theapplication of transfer learning techniques, instead of using a set of masks.Additionally, we propose the Infraction Rate Score (IRS), a new evaluationmetric designed to provide a more balanced assessment of driving performanceover extended routes. PRIBOOT is the first model to achieve a Route Completion(RC) of 75% in Leaderboard 2.0, along with a Driving Score (DS) and IRS of 20%and 45%, respectively. With PRIBOOT, researchers can now generate extensivedatasets, potentially solving the data availability issues that have hinderedprogress in this benchmark.</description><author>Daniel Coelho, Miguel Oliveira, Vitor Santos, Antonio M. Lopez</author><pubDate>Wed, 12 Jun 2024 18:05:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08421v1</guid></item><item><title>OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text</title><link>http://arxiv.org/abs/2406.08418v1</link><description>Image-text interleaved data, consisting of multiple images and texts arrangedin a natural document format, aligns with the presentation paradigm of internetdata and closely resembles human reading habits. Recent studies have shown thatsuch data aids multimodal in-context learning and maintains the capabilities oflarge language models during multimodal fine-tuning. However, the limited scaleand diversity of current image-text interleaved data restrict the developmentof multimodal large language models. In this paper, we introduce OmniCorpus, a10 billion-scale image-text interleaved dataset. Using an efficient dataengine, we filter and extract large-scale high-quality documents, which contain8.6 billion images and 1,696 billion text tokens. Compared to counterparts(e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales whilemaintaining good data quality; 2) features more diverse sources, including bothEnglish and non-English websites as well as video-centric websites; 3) is moreflexible, easily degradable from an image-text interleaved format to pure textcorpus and image-text pairs. Through comprehensive analysis and experiments, wevalidate the quality, usability, and effectiveness of the proposed dataset. Wehope this could provide a solid data foundation for future multimodal modelresearch. Code and data are released athttps://github.com/OpenGVLab/OmniCorpus.</description><author>Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, Jiashuo Yu, Hao Tian, Jiasheng Zhou, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Pei Chu, Yi Wang, Min Dou, Changyao Tian, Xizhou Zhu, Lewei Lu, Yushi Chen, Junjun He, Tong Lu, Yali Wang, Limin Wang, Dahua Lin, Yu Qiao, Botian Shi, Conghui He, Jifeng Dai</author><pubDate>Wed, 12 Jun 2024 18:01:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08418v1</guid></item><item><title>Discovering Preference Optimization Algorithms with and for Large Language Models</title><link>http://arxiv.org/abs/2406.08414v1</link><description>Offline preference optimization is a key method for enhancing and controllingthe quality of Large Language Model (LLM) outputs. Typically, preferenceoptimization is approached as an offline supervised learning task usingmanually-crafted convex loss functions. While these methods are based ontheoretical insights, they are inherently constrained by human creativity, sothe large search space of possible loss functions remains under explored. Weaddress this by performing LLM-driven objective discovery to automaticallydiscover new state-of-the-art preference optimization algorithms without(expert) human intervention. Specifically, we iteratively prompt an LLM topropose and implement new preference optimization loss functions based onpreviously-evaluated performance metrics. This process leads to the discoveryof previously-unknown and performant preference optimization algorithms. Thebest performing of these we call Discovered Preference Optimization (DiscoPOP),a novel algorithm that adaptively blends logistic and exponential losses.Experiments demonstrate the state-of-the-art performance of DiscoPOP and itssuccessful transfer to held-out tasks.</description><author>Chris Lu, Samuel Holt, Claudio Fanconi, Alex J. Chan, Jakob Foerster, Mihaela van der Schaar, Robert Tjarko Lange</author><pubDate>Wed, 12 Jun 2024 17:58:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08414v1</guid></item><item><title>Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference</title><link>http://arxiv.org/abs/2406.08413v1</link><description>Large language models (LLMs) have recently transformed natural languageprocessing, enabling machines to generate human-like text and engage inmeaningful conversations. This development necessitates speed, efficiency, andaccessibility in LLM inference as the computational and memory requirements ofthese systems grow exponentially. Meanwhile, advancements in computing andmemory capabilities are lagging behind, exacerbated by the discontinuation ofMoore's law. With LLMs exceeding the capacity of single GPUs, they requirecomplex, expert-level configurations for parallel processing. Memory accessesbecome significantly more expensive than computation, posing a challenge forefficient scaling, known as the memory wall. Here, compute-in-memory (CIM)technologies offer a promising solution for accelerating AI inference bydirectly performing analog computations in memory, potentially reducing latencyand power consumption. By closely integrating memory and compute elements, CIMeliminates the von Neumann bottleneck, reducing data movement and improvingenergy efficiency. This survey paper provides an overview and analysis oftransformer-based models, reviewing various CIM architectures and exploring howthey can address the imminent challenges of modern AI computing systems. Wediscuss transformer-related operators and their hardware acceleration schemesand highlight challenges, trends, and insights in corresponding CIM designs.</description><author>Christopher Wolters, Xiaoxuan Yang, Ulf Schlichtmann, Toyotaro Suzumura</author><pubDate>Wed, 12 Jun 2024 17:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08413v1</guid></item><item><title>Tailoring Generative AI Chatbots for Multiethnic Communities in Disaster Preparedness Communication: Extending the CASA Paradigm</title><link>http://arxiv.org/abs/2406.08411v1</link><description>This study is among the first to develop different prototypes of generativeAI (GenAI) chatbots powered by GPT 4 to communicate hurricane preparednessinformation to diverse residents. Drawing from the Computers Are Social Actors(CASA) paradigm and the literature on disaster vulnerability and culturaltailoring, this study conducted a between-subjects experiment with 441 Black,Hispanic, and Caucasian residents of Florida. A computational analysis of chatlogs (N = 7,848) shows that anthropomorphism and personalization are keycommunication topics in GenAI chatbot-user interactions. SEM results (N = 441)suggest that GenAI chatbots varying in tone formality and cultural tailoringsignificantly predict bot perceptions and, subsequently, hurricane preparednessoutcomes. These results highlight the potential of using GenAI chatbots toimprove diverse communities' disaster preparedness.</description><author>Xinyan Zhao, Yuan Sun, Wenlin Liu, Chau-Wai Wong</author><pubDate>Wed, 12 Jun 2024 17:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08411v1</guid></item><item><title>MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos</title><link>http://arxiv.org/abs/2406.08407v1</link><description>Multimodal Language Language Models (MLLMs) demonstrate the emergingabilities of "world models" -- interpreting and reasoning about complexreal-world dynamics. To assess these abilities, we posit videos are the idealmedium, as they encapsulate rich representations of real-world dynamics andcausalities. To this end, we introduce MMWorld, a new benchmark formulti-discipline, multi-faceted multimodal video understanding. MMWorlddistinguishes itself from previous video understanding benchmarks with twounique advantages: (1) multi-discipline, covering various disciplines thatoften require domain expertise for comprehensive understanding; (2)multi-faceted reasoning, including explanation, counterfactual thinking, futureprediction, etc. MMWorld consists of a human-annotated dataset to evaluateMLLMs with questions about the whole videos and a synthetic dataset to analyzeMLLMs within a single modality of perception. Together, MMWorld encompasses1,910 videos across seven broad disciplines and 69 subdisciplines, completewith 6,627 question-answer pairs and associated captions. The evaluationincludes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld(e.g., GPT-4V performs the best with only 52.3\% accuracy), showing large roomfor improvement. Further ablation studies reveal other interesting findingssuch as models' different skill sets from humans. We hope MMWorld can serve asan essential step towards world model evaluation in videos.</description><author>Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Kevin Lin, William Yang Wang, Lijuan Wang, Xin Eric Wang</author><pubDate>Wed, 12 Jun 2024 17:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08407v1</guid></item><item><title>Harder or Different? Understanding Generalization of Audio Deepfake Detection</title><link>http://arxiv.org/abs/2406.03512v3</link><description>Recent research has highlighted a key issue in speech deepfake detection:models trained on one set of deepfakes perform poorly on others. The questionarises: is this due to the continuously improving quality of Text-to-Speech(TTS) models, i.e., are newer DeepFakes just 'harder' to detect? Or, is itbecause deepfakes generated with one model are fundamentally different to thosegenerated using another model? We answer this question by decomposing theperformance gap between in-domain and out-of-domain test data into 'hardness'and 'difference' components. Experiments performed using ASVspoof databasesindicate that the hardness component is practically negligible, with theperformance gap being attributed primarily to the difference component. Thishas direct implications for real-world deepfake detection, highlighting thatmerely increasing model capacity, the currently-dominant research trend, maynot effectively address the generalization challenge.</description><author>Nicolas M. Müller, Nicholas Evans, Hemlata Tak, Philip Sperl, Konstantin Böttinger</author><pubDate>Wed, 12 Jun 2024 17:54:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03512v3</guid></item><item><title>RRLS : Robust Reinforcement Learning Suite</title><link>http://arxiv.org/abs/2406.08406v1</link><description>Robust reinforcement learning is the problem of learning control policiesthat provide optimal worst-case performance against a span of adversarialenvironments. It is a crucial ingredient for deploying algorithms in real-worldscenarios with prevalent environmental uncertainties and has been along-standing object of attention in the community, without a standardized setof benchmarks. This contribution endeavors to fill this gap. We introduce theRobust Reinforcement Learning Suite (RRLS), a benchmark suite based on Mujocoenvironments. RRLS provides six continuous control tasks with two types ofuncertainty sets for training and evaluation. Our benchmark aims to standardizerobust reinforcement learning tasks, facilitating reproducible and comparableexperiments, in particular those from recent state-of-the-art contributions,for which we demonstrate the use of RRLS. It is also designed to be easilyexpandable to new environments. The source code is available at\href{https://github.com/SuReLI/RRLS}{https://github.com/SuReLI/RRLS}.</description><author>Adil Zouitine, David Bertoin, Pierre Clavier, Matthieu Geist, Emmanuel Rachelson</author><pubDate>Wed, 12 Jun 2024 17:53:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08406v1</guid></item><item><title>Batch and match: black-box variational inference with a score-based divergence</title><link>http://arxiv.org/abs/2402.14758v2</link><description>Most leading implementations of black-box variational inference (BBVI) arebased on optimizing a stochastic evidence lower bound (ELBO). But suchapproaches to BBVI often converge slowly due to the high variance of theirgradient estimates and their sensitivity to hyperparameters. In this work, wepropose batch and match (BaM), an alternative approach to BBVI based on ascore-based divergence. Notably, this score-based divergence can be optimizedby a closed-form proximal update for Gaussian variational families with fullcovariance matrices. We analyze the convergence of BaM when the targetdistribution is Gaussian, and we prove that in the limit of infinite batch sizethe variational parameter updates converge exponentially quickly to the targetmean and covariance. We also evaluate the performance of BaM on Gaussian andnon-Gaussian target distributions that arise from posterior inference inhierarchical and deep generative models. In these experiments, we find that BaMtypically converges in fewer (and sometimes significantly fewer) gradientevaluations than leading implementations of BBVI based on ELBO maximization.</description><author>Diana Cai, Chirag Modi, Loucas Pillaud-Vivien, Charles C. Margossian, Robert M. Gower, David M. Blei, Lawrence K. Saul</author><pubDate>Wed, 12 Jun 2024 17:53:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14758v2</guid></item><item><title>Scaling Value Iteration Networks to 5000 Layers for Extreme Long-Term Planning</title><link>http://arxiv.org/abs/2406.08404v1</link><description>The Value Iteration Network (VIN) is an end-to-end differentiablearchitecture that performs value iteration on a latent MDP for planning inreinforcement learning (RL). However, VINs struggle to scale to long-term andlarge-scale planning tasks, such as navigating a $100\times 100$ maze -- a taskwhich typically requires thousands of planning steps to solve. We observe thatthis deficiency is due to two issues: the representation capacity of the latentMDP and the planning module's depth. We address these by augmenting the latentMDP with a dynamic transition kernel, dramatically improving itsrepresentational capacity, and, to mitigate the vanishing gradient problem,introducing an "adaptive highway loss" that constructs skip connections toimprove gradient flow. We evaluate our method on both 2D maze navigationenvironments and the ViZDoom 3D navigation benchmark. We find that our newmethod, named Dynamic Transition VIN (DT-VIN), easily scales to 5000 layers andcasually solves challenging versions of the above tasks. Altogether, we believethat DT-VIN represents a concrete step forward in performing long-termlarge-scale planning in RL environments.</description><author>Yuhui Wang, Qingyuan Wu, Weida Li, Dylan R. Ashley, Francesco Faccio, Chao Huang, Jürgen Schmidhuber</author><pubDate>Wed, 12 Jun 2024 17:52:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08404v1</guid></item><item><title>Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models</title><link>http://arxiv.org/abs/2406.08402v1</link><description>Large audio-language models (LALMs) enhance traditional large language modelsby integrating audio perception capabilities, allowing them to tackleaudio-related tasks. Previous research has primarily focused on assessing theperformance of LALMs across various tasks, yet overlooking their reliability,particularly concerning issues like object hallucination. In our study, weintroduce methods to assess the extent of object hallucination of publiclyavailable LALMs. Our findings reveal that LALMs are comparable to specializedaudio captioning models in their understanding of audio content, but struggleto answer discriminative questions, specifically those requiring theidentification of the presence of particular object sounds within an audioclip. This limitation highlights a critical weakness in current LALMs: theirinadequate understanding of discriminative queries. Moreover, we explore thepotential of prompt engineering to enhance LALMs' performance on discriminativequestions.</description><author>Chun-Yi Kuan, Wei-Ping Huang, Hung-yi Lee</author><pubDate>Wed, 12 Jun 2024 17:51:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08402v1</guid></item><item><title>Nyström Kernel Stein Discrepancy</title><link>http://arxiv.org/abs/2406.08401v1</link><description>Kernel methods underpin many of the most successful approaches in datascience and statistics, and they allow representing probability measures aselements of a reproducing kernel Hilbert space without loss of information.Recently, the kernel Stein discrepancy (KSD), which combines Stein's methodwith kernel techniques, gained considerable attention. Through the Steinoperator, KSD allows the construction of powerful goodness-of-fit tests whereit is sufficient to know the target distribution up to a multiplicativeconstant. However, the typical U- and V-statistic-based KSD estimators sufferfrom a quadratic runtime complexity, which hinders their application inlarge-scale settings. In this work, we propose a Nystr\"om-based KSDacceleration -- with runtime $\mathcal O\!\left(mn+m^3\right)$ for $n$ samplesand $m\ll n$ Nystr\"om points -- , show its $\sqrt{n}$-consistency under thenull with a classical sub-Gaussian assumption, and demonstrate itsapplicability for goodness-of-fit testing on a suite of benchmarks.</description><author>Florian Kalinke, Zoltan Szabo, Bharath K. Sriperumbudur</author><pubDate>Wed, 12 Jun 2024 17:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08401v1</guid></item><item><title>Differentiable Cost-Parameterized Monge Map Estimators</title><link>http://arxiv.org/abs/2406.08399v1</link><description>Within the field of optimal transport (OT), the choice of ground cost iscrucial to ensuring that the optimality of a transport map corresponds tousefulness in real-world applications. It is therefore desirable to use knowninformation to tailor cost functions and hence learn OT maps which are adaptedto the problem at hand. By considering a class of neural ground costs whoseMonge maps have a known form, we construct a differentiable Monge map estimatorwhich can be optimized to be consistent with known information about an OT map.In doing so, we simultaneously learn both an OT map estimator and acorresponding adapted cost function. Through suitable choices of loss function,our method provides a general approach for incorporating prior informationabout the Monge map itself when learning adapted OT maps and cost functions.</description><author>Samuel Howard, George Deligiannidis, Patrick Rebeschini, James Thornton</author><pubDate>Wed, 12 Jun 2024 17:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08399v1</guid></item><item><title>cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers</title><link>http://arxiv.org/abs/2406.08398v1</link><description>An emerging area of research in situated and multimodal interactiveconversations (SIMMC) includes interactions in scientific papers. Sincescientific papers are primarily composed of text, equations, figures, andtables, SIMMC methods must be developed specifically for each component tosupport the depth of inquiry and interactions required by research scientists.This work introduces Conversational Papers (cPAPERS), a dataset ofconversational question-answer pairs from reviews of academic papers groundedin these paper components and their associated references from scientificdocuments available on arXiv. We present a data collection strategy to collectthese question-answer pairs from OpenReview and associate them with contextualinformation from LaTeX source files. Additionally, we present a series ofbaseline approaches utilizing Large Language Models (LLMs) in both zero-shotand fine-tuned configurations to address the cPAPERS dataset.</description><author>Anirudh Sundar, Jin Xu, William Gay, Christopher Richardson, Larry Heck</author><pubDate>Wed, 12 Jun 2024 17:46:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08398v1</guid></item><item><title>Neural Blind Source Separation and Diarization for Distant Speech Recognition</title><link>http://arxiv.org/abs/2406.08396v1</link><description>This paper presents a neural method for distant speech recognition (DSR) thatjointly separates and diarizes speech mixtures without supervision by isolatedsignals. A standard separation method for multi-talker DSR is a statisticalmultichannel method called guided source separation (GSS). While GSS does notrequire signal-level supervision, it relies on speaker diarization results tohandle unknown numbers of active speakers. To overcome this limitation, weintroduce and train a neural inference model in a weakly-supervised manner,employing the objective function of a statistical separation method. Thistraining requires only multichannel mixtures and their temporal annotations ofspeaker activities. In contrast to GSS, the trained model can jointly separateand diarize speech mixtures without any auxiliary information. The experimentswith the AMI corpus show that our method outperforms GSS with oraclediarization results regarding word error rates. The code is available online.</description><author>Yoshiaki Bando, Tomohiko Nakamura, Shinji Watanabe</author><pubDate>Wed, 12 Jun 2024 17:45:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08396v1</guid></item><item><title>Time-Constrained Robust MDPs</title><link>http://arxiv.org/abs/2406.08395v1</link><description>Robust reinforcement learning is essential for deploying reinforcementlearning algorithms in real-world scenarios where environmental uncertaintypredominates. Traditional robust reinforcement learning often depends onrectangularity assumptions, where adverse probability measures of outcomestates are assumed to be independent across different states and actions. Thisassumption, rarely fulfilled in practice, leads to overly conservativepolicies. To address this problem, we introduce a new time-constrained robustMDP (TC-RMDP) formulation that considers multifactorial, correlated, andtime-dependent disturbances, thus more accurately reflecting real-worlddynamics. This formulation goes beyond the conventional rectangularityparadigm, offering new perspectives and expanding the analytical framework forrobust RL. We propose three distinct algorithms, each using varying levels ofenvironmental information, and evaluate them extensively on continuous controlbenchmarks. Our results demonstrate that these algorithms yield an efficienttradeoff between performance and robustness, outperforming traditional deeprobust RL methods in time-constrained environments while preserving robustnessin classical benchmarks. This study revisits the prevailing assumptions inrobust RL and opens new avenues for developing more practical and realistic RLapplications.</description><author>Adil Zouitine, David Bertoin, Pierre Clavier, Matthieu Geist, Emmanuel Rachelson</author><pubDate>Wed, 12 Jun 2024 17:45:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08395v1</guid></item><item><title>VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks</title><link>http://arxiv.org/abs/2406.08394v1</link><description>We present VisionLLM v2, an end-to-end generalist multimodal large model(MLLM) that unifies visual perception, understanding, and generation within asingle framework. Unlike traditional MLLMs limited to text output, VisionLLM v2significantly broadens its application scope. It excels not only inconventional visual question answering (VQA) but also in open-ended,cross-domain vision tasks such as object localization, pose estimation, andimage generation and editing. To this end, we propose a new informationtransmission mechanism termed "super link", as a medium to connect MLLM withtask-specific decoders. It not only allows flexible transmission of taskinformation and gradient feedback between the MLLM and multiple downstreamdecoders but also effectively resolves training conflicts in multi-taskingscenarios. In addition, to support the diverse range of tasks, we carefullycollected and combed training data from hundreds of public vision andvision-language tasks. In this way, our model can be joint-trained end-to-endon hundreds of vision language tasks and generalize to these tasks using a setof shared parameters through different user prompts, achieving performancecomparable to task-specific models. We believe VisionLLM v2 will offer a newperspective on the generalization of MLLMs.</description><author>Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai, Zhaoyang Liu, Wenhai Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Ping Luo, Yu Qiao, Jifeng Dai</author><pubDate>Wed, 12 Jun 2024 17:44:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08394v1</guid></item><item><title>FontStudio: Shape-Adaptive Diffusion Model for Coherent and Consistent Font Effect Generation</title><link>http://arxiv.org/abs/2406.08392v1</link><description>Recently, the application of modern diffusion-based text-to-image generationmodels for creating artistic fonts, traditionally the domain of professionaldesigners, has garnered significant interest. Diverging from the majority ofexisting studies that concentrate on generating artistic typography, ourresearch aims to tackle a novel and more demanding challenge: the generation oftext effects for multilingual fonts. This task essentially requires generatingcoherent and consistent visual content within the confines of a font-shapedcanvas, as opposed to a traditional rectangular canvas. To address this task,we introduce a novel shape-adaptive diffusion model capable of interpreting thegiven shape and strategically planning pixel distributions within the irregularcanvas. To achieve this, we curate a high-quality shape-adaptive image-textdataset and incorporate the segmentation mask as a visual condition to steerthe image generation process within the irregular-canvas. This approach enablesthe traditionally rectangle canvas-based diffusion model to produce the desiredconcepts in accordance with the provided geometric shapes. Second, to maintainconsistency across multiple letters, we also present a training-free,shape-adaptive effect transfer method for transferring textures from agenerated reference letter to others. The key insights are building a fonteffect noise prior and propagating the font effect information in aconcatenated latent space. The efficacy of our FontStudio system is confirmedthrough user preference studies, which show a marked preference (78% win-rateson aesthetics) for our system even when compared to the latest unrivaledcommercial product, Adobe Firefly.</description><author>Xinzhi Mu, Li Chen, Bohan Chen, Shuyang Gu, Jianmin Bao, Dong Chen, Ji Li, Yuhui Yuan</author><pubDate>Wed, 12 Jun 2024 17:43:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08392v1</guid></item><item><title>Descriptive Image Quality Assessment in the Wild</title><link>http://arxiv.org/abs/2405.18842v2</link><description>With the rapid advancement of Vision Language Models (VLMs), VLM-based ImageQuality Assessment (IQA) seeks to describe image quality linguistically toalign with human expression and capture the multifaceted nature of IQA tasks.However, current methods are still far from practical usage. First, prior worksfocus narrowly on specific sub-tasks or settings, which do not align withdiverse real-world applications. Second, their performance is sub-optimal dueto limitations in dataset coverage, scale, and quality. To overcome thesechallenges, we introduce Depicted image Quality Assessment in the Wild(DepictQA-Wild). Our method includes a multi-functional IQA task paradigm thatencompasses both assessment and comparison tasks, brief and detailed responses,full-reference and non-reference scenarios. We introduce aground-truth-informed dataset construction approach to enhance data quality,and scale up the dataset to 495K under the brief-detail joint framework.Consequently, we construct a comprehensive, large-scale, and high-qualitydataset, named DQ-495K. We also retain image resolution during training tobetter handle resolution-related quality issues, and estimate a confidencescore that is helpful to filter out low-quality responses. Experimental resultsdemonstrate that DepictQA-Wild significantly outperforms traditionalscore-based methods, prior VLM-based IQA models, and proprietary GPT-4V indistortion identification, instant rating, and reasoning tasks. Our advantagesare further confirmed by real-world applications including assessing theweb-downloaded images and ranking model-processed images. Datasets and codeswill be released in https://depictqa.github.io/depictqa-wild/.</description><author>Zhiyuan You, Jinjin Gu, Zheyuan Li, Xin Cai, Kaiwen Zhu, Chao Dong, Tianfan Xue</author><pubDate>Wed, 12 Jun 2024 17:42:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18842v2</guid></item><item><title>Large Language Models Must Be Taught to Know What They Don't Know</title><link>http://arxiv.org/abs/2406.08391v1</link><description>When using large language models (LLMs) in high-stakes applications, we needto know when we can trust their predictions. Some works argue that promptinghigh-performance LLMs is sufficient to produce calibrated uncertainties, whileothers introduce sampling methods that can be prohibitively expensive. In thiswork, we first argue that prompting on its own is insufficient to achieve goodcalibration and then show that fine-tuning on a small dataset of correct andincorrect answers can create an uncertainty estimate with good generalizationand small computational overhead. We show that a thousand graded examples aresufficient to outperform baseline methods and that training through thefeatures of a model is necessary for good performance and tractable for largeopen-source models when using LoRA. We also investigate the mechanisms thatenable reliable LLM uncertainty estimation, finding that many models can beused as general-purpose uncertainty estimators, applicable not just to theirown uncertainties but also the uncertainty of other models. Lastly, we showthat uncertainty estimates inform human use of LLMs in human-AI collaborativesettings through a user study.</description><author>Sanyam Kapoor, Nate Gruver, Manley Roberts, Katherine Collins, Arka Pal, Umang Bhatt, Adrian Weller, Samuel Dooley, Micah Goldblum, Andrew Gordon Wilson</author><pubDate>Wed, 12 Jun 2024 17:41:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08391v1</guid></item><item><title>PPG-to-ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling</title><link>http://arxiv.org/abs/2309.15375v4</link><description>Photoplethysmography (PPG) is a cost-effective and non-invasive techniquethat utilizes optical methods to measure cardiac physiology. PPG has becomeincreasingly popular in health monitoring and is used in various commercial andclinical wearable devices. Compared to electrocardiography (ECG), PPG does notprovide substantial clinical diagnostic value, despite the strong correlationbetween the two. Here, we propose a subject-independent attention-based deepstate-space model (ADSSM) to translate PPG signals to corresponding ECGwaveforms. The model is not only robust to noise but also data-efficient byincorporating probabilistic prior knowledge. To evaluate our approach, 55subjects' data from the MIMIC-III database were used in their original form,and then modified with noise, mimicking real-world scenarios. Our approach wasproven effective as evidenced by the PR-AUC of 0.986 achieved when inputtingthe translated ECG signals into an existing atrial fibrillation (AFib)detector. ADSSM enables the integration of ECG's extensive knowledge base andPPG's continuous measurement for early diagnosis of cardiovascular disease.</description><author>Khuong Vo, Mostafa El-Khamy, Yoojin Choi</author><pubDate>Wed, 12 Jun 2024 17:41:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15375v4</guid></item><item><title>Diff-A-Riff: Musical Accompaniment Co-creation via Latent Diffusion Models</title><link>http://arxiv.org/abs/2406.08384v1</link><description>Recent advancements in deep generative models present new opportunities formusic production but also pose challenges, such as high computational demandsand limited audio quality. Moreover, current systems frequently rely solely ontext input and typically focus on producing complete musical pieces, which isincompatible with existing workflows in music production. To address theseissues, we introduce "Diff-A-Riff," a Latent Diffusion Model designed togenerate high-quality instrumental accompaniments adaptable to any musicalcontext. This model offers control through either audio references, textprompts, or both, and produces 48kHz pseudo-stereo audio while significantlyreducing inference time and memory usage. We demonstrate the model'scapabilities through objective metrics and subjective listening tests, withextensive examples available on the accompanying website:sonycslparis.github.io/diffariff-companion/</description><author>Javier Nistal, Marco Pasini, Cyran Aouameur, Maarten Grachten, Stefan Lattner</author><pubDate>Wed, 12 Jun 2024 17:34:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08384v1</guid></item><item><title>LaneCPP: Continuous 3D Lane Detection using Physical Priors</title><link>http://arxiv.org/abs/2406.08381v1</link><description>Monocular 3D lane detection has become a fundamental problem in the contextof autonomous driving, which comprises the tasks of finding the road surfaceand locating lane markings. One major challenge lies in a flexible but robustline representation capable of modeling complex lane structures, while stillavoiding unpredictable behavior. While previous methods rely on fullydata-driven approaches, we instead introduce a novel approach LaneCPP that usesa continuous 3D lane detection model leveraging physical prior knowledge aboutthe lane structure and road geometry. While our sophisticated lane model iscapable of modeling complex road structures, it also shows robust behaviorsince physical constraints are incorporated by means of a regularization schemethat can be analytically applied to our parametric representation. Moreover, weincorporate prior knowledge about the road geometry into the 3D feature spaceby modeling geometry-aware spatial features, guiding the network to learn aninternal road surface representation. In our experiments, we show the benefitsof our contributions and prove the meaningfulness of using priors to make 3Dlane detection more robust. The results show that LaneCPP achievesstate-of-the-art performance in terms of F-Score and geometric errors.</description><author>Maximilian Pittner, Joel Janai, Alexandru P. Condurache</author><pubDate>Wed, 12 Jun 2024 17:31:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08381v1</guid></item><item><title>Towards Unsupervised Speech Recognition Without Pronunciation Models</title><link>http://arxiv.org/abs/2406.08380v1</link><description>Recent advancements in supervised automatic speech recognition (ASR) haveachieved remarkable performance, largely due to the growing availability oflarge transcribed speech corpora. However, most languages lack sufficientpaired speech and text data to effectively train these systems. In thisarticle, we tackle the challenge of developing ASR systems without pairedspeech and text corpora by proposing the removal of reliance on a phonemelexicon. We explore a new research direction: word-level unsupervised ASR.Using a curated speech corpus containing only high-frequency English words, oursystem achieves a word error rate of nearly 20% without parallel transcripts ororacle word boundaries. Furthermore, we experimentally demonstrate that anunsupervised speech recognizer can emerge from joint speech-to-speech andtext-to-text masked token-infilling. This innovative model surpasses theperformance of previous unsupervised ASR models trained with directdistribution matching.</description><author>Junrui Ni, Liming Wang, Yang Zhang, Kaizhi Qian, Heting Gao, Mark Hasegawa-Johnson, Chang D. Yoo</author><pubDate>Wed, 12 Jun 2024 17:30:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08380v1</guid></item><item><title>Eyes Wide Unshut: Unsupervised Mistake Detection in Egocentric Video by Detecting Unpredictable Gaze</title><link>http://arxiv.org/abs/2406.08379v1</link><description>In this paper, we address the challenge of unsupervised mistake detection inegocentric video through the analysis of gaze signals, a critical component foradvancing user assistance in smart glasses. Traditional supervised methods,reliant on manually labeled mistakes, suffer from domain-dependence andscalability issues. This research introduces an unsupervised method fordetecting mistakes in videos of human activities, overcoming the challenges ofdomain-specific requirements and the necessity for annotated data. By analyzingunusual gaze patterns that signal user disorientation during tasks, we proposea gaze completion model that forecasts eye gaze trajectories from incompleteinputs. The difference between the anticipated and observed gaze paths acts asan indicator for identifying errors. Our method is validated on the EPIC-Tentdataset, showing its superiority compared to current one-class supervised andunsupervised techniques.</description><author>Michele Mazzamuto, Antonino Furnari, Giovanni Maria Farinella</author><pubDate>Wed, 12 Jun 2024 17:29:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08379v1</guid></item><item><title>DDR: Exploiting Deep Degradation Response as Flexible Image Descriptor</title><link>http://arxiv.org/abs/2406.08377v1</link><description>Image deep features extracted by pre-trained networks are known to containrich and informative representations. In this paper, we present DeepDegradation Response (DDR), a method to quantify changes in image deep featuresunder varying degradation conditions. Specifically, our approach facilitatesflexible and adaptive degradation, enabling the controlled synthesis of imagedegradation through text-driven prompts. Extensive evaluations demonstrate theversatility of DDR as an image descriptor, with strong correlations observedwith key image attributes such as complexity, colorfulness, sharpness, andoverall quality. Moreover, we demonstrate the efficacy of DDR across a spectrumof applications. It excels as a blind image quality assessment metric,outperforming existing methodologies across multiple datasets. Additionally,DDR serves as an effective unsupervised learning objective in image restorationtasks, yielding notable advancements in image deblurring and single-imagesuper-resolution. Our code will be made available.</description><author>Juncheng Wu, Zhangkai Ni, Hanli Wang, Wenhan Yang, Yuyin Zhou, Shiqi Wang</author><pubDate>Wed, 12 Jun 2024 17:26:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08377v1</guid></item><item><title>2.5D Multi-view Averaging Diffusion Model for 3D Medical Image Translation: Application to Low-count PET Reconstruction with CT-less Attenuation Correction</title><link>http://arxiv.org/abs/2406.08374v1</link><description>Positron Emission Tomography (PET) is an important clinical imaging tool butinevitably introduces radiation hazards to patients and healthcare providers.Reducing the tracer injection dose and eliminating the CT acquisition forattenuation correction can reduce the overall radiation dose, but often resultsin PET with high noise and bias. Thus, it is desirable to develop 3D methods totranslate the non-attenuation-corrected low-dose PET (NAC-LDPET) intoattenuation-corrected standard-dose PET (AC-SDPET). Recently, diffusion modelshave emerged as a new state-of-the-art deep learning method for image-to-imagetranslation, better than traditional CNN-based methods. However, due to thehigh computation cost and memory burden, it is largely limited to 2Dapplications. To address these challenges, we developed a novel 2.5D Multi-viewAveraging Diffusion Model (MADM) for 3D image-to-image translation withapplication on NAC-LDPET to AC-SDPET translation. Specifically, MADM employsseparate diffusion models for axial, coronal, and sagittal views, whose outputsare averaged in each sampling step to ensure the 3D generation quality frommultiple views. To accelerate the 3D sampling process, we also proposed astrategy to use the CNN-based 3D generation as a prior for the diffusion model.Our experimental results on human patient studies suggested that MADM cangenerate high-quality 3D translation images, outperforming previous CNN-basedand Diffusion-based baseline methods.</description><author>Tianqi Chen, Jun Hou, Yinchi Zhou, Huidong Xie, Xiongchao Chen, Qiong Liu, Xueqi Guo, Menghua Xia, James S. Duncan, Chi Liu, Bo Zhou</author><pubDate>Wed, 12 Jun 2024 17:22:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08374v1</guid></item><item><title>Deep Learning Based Joint Multi-User MISO Power Allocation and Beamforming Design</title><link>http://arxiv.org/abs/2406.08373v1</link><description>The evolution of fifth generation (5G) wireless communication networks hasled to an increased need for wireless resource management solutions thatprovide higher data rates, wide coverage, low latency, and power efficiency.Yet, many of existing traditional approaches remain non-practical due tocomputational limitations, and unrealistic presumptions of static networkconditions and algorithm initialization dependencies. This creates an importantgap between theoretical analysis and real-time processing of algorithms. Tobridge this gap, deep learning based techniques offer promising solutions withtheir representational capabilities for universal function approximation. Wepropose a novel unsupervised deep learning based joint power allocation andbeamforming design for multi-user multiple-input single-output (MU-MISO)system. The objective is to enhance the spectral efficiency by maximizing thesum-rate with the proposed joint design framework, NNBF-P while also offeringcomputationally efficient solution in contrast to conventional approaches. Weconduct experiments for diverse settings to compare the performance of NNBF-Pwith zero-forcing beamforming (ZFBF), minimum mean square error (MMSE)beamforming, and NNBF, which is also our deep learning based beamforming designwithout joint power allocation scheme. Experiment results demonstrate thesuperiority of NNBF-P compared to ZFBF, and MMSE while NNBF can have lowerperformances than MMSE and ZFBF in some experiment settings. It can alsodemonstrate the effectiveness of joint design framework with respect to NNBF.</description><author>Cemil Vahapoglu, Timothy J. O'Shea, Tamoghna Roy, Sennur Ulukus</author><pubDate>Wed, 12 Jun 2024 17:21:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08373v1</guid></item><item><title>APSeg: Auto-Prompt Network for Cross-Domain Few-Shot Semantic Segmentatio</title><link>http://arxiv.org/abs/2406.08372v1</link><description>Few-shot semantic segmentation (FSS) endeavors to segment unseen classes withonly a few labeled samples. Current FSS methods are commonly built on theassumption that their training and application scenarios share similar domains,and their performances degrade significantly while applied to a distinctdomain. To this end, we propose to leverage the cutting-edge foundation model,the Segment Anything Model (SAM), for generalization enhancement. The SAMhowever performs unsatisfactorily on domains that are distinct from itstraining data, which primarily comprise natural scene images, and it does notsupport automatic segmentation of specific semantics due to its interactiveprompting mechanism. In our work, we introduce APSeg, a novel auto-promptnetwork for cross-domain few-shot semantic segmentation (CD-FSS), which isdesigned to be auto-prompted for guiding cross-domain segmentation.Specifically, we propose a Dual Prototype Anchor Transformation (DPAT) modulethat fuses pseudo query prototypes extracted based on cycle-consistency withsupport prototypes, allowing features to be transformed into a more stabledomain-agnostic space. Additionally, a Meta Prompt Generator (MPG) module isintroduced to automatically generate prompt embeddings, eliminating the needfor manual visual prompts. We build an efficient model which can be applieddirectly to target domains without fine-tuning. Extensive experiments on fourcross-domain datasets show that our model outperforms the state-of-the-artCD-FSS method by 5.24% and 3.10% in average accuracy on 1-shot and 5-shotsettings, respectively.</description><author>Weizhao He, Yang Zhang, Wei Zhuo, Linlin Shen, Jiaqi Yang, Songhe Deng, Liang Sun</author><pubDate>Wed, 12 Jun 2024 17:20:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08372v1</guid></item><item><title>Rankability-enhanced Revenue Uplift Modeling Framework for Online Marketing</title><link>http://arxiv.org/abs/2405.15301v2</link><description>Uplift modeling has been widely employed in online marketing by predictingthe response difference between the treatment and control groups, so as toidentify the sensitive individuals toward interventions like coupons ordiscounts. Compared with traditional \textit{conversion uplift modeling},\textit{revenue uplift modeling} exhibits higher potential due to its directconnection with the corporate income. However, previous works can hardly handlethe continuous long-tail response distribution in revenue uplift modeling.Moreover, they have neglected to optimize the uplift ranking among differentindividuals, which is actually the core of uplift modeling. To address suchissues, in this paper, we first utilize the zero-inflated lognormal (ZILN) lossto regress the responses and customize the corresponding modeling network,which can be adapted to different existing uplift models. Then, we study theranking-related uplift modeling error from the theoretical perspective andpropose two tighter error bounds as the additional loss terms to theconventional response regression loss. Finally, we directly model the upliftranking error for the entire population with a listwise uplift ranking loss.The experiment results on offline public and industrial datasets validate theeffectiveness of our method for revenue uplift modeling. Furthermore, weconduct large-scale experiments on a prominent online fintech marketingplatform, Tencent FiT, which further demonstrates the superiority of our methodin real-world applications.</description><author>Bowei He, Yunpeng Weng, Xing Tang, Ziqiang Cui, Zexu Sun, Liang Chen, Xiuqiang He, Chen Ma</author><pubDate>Wed, 12 Jun 2024 17:14:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15301v2</guid></item><item><title>Neural Thermodynamic Integration: Free Energies from Energy-based Diffusion Models</title><link>http://arxiv.org/abs/2406.02313v2</link><description>Thermodynamic integration (TI) offers a rigorous method for estimatingfree-energy differences by integrating over a sequence of interpolatingconformational ensembles. However, TI calculations are computationallyexpensive and typically limited to coupling a small number of degrees offreedom due to the need to sample numerous intermediate ensembles withsufficient conformational-space overlap. In this work, we propose to perform TIalong an alchemical pathway represented by a trainable neural network, which weterm Neural TI. Critically, we parametrize a time-dependent Hamiltonianinterpolating between the interacting and non-interacting systems, and optimizeits gradient using a denoising-diffusion objective. The ability of theresulting energy-based diffusion model to sample all intermediate ensemblesallows us to perform TI from a single reference calculation. We apply ourmethod to Lennard-Jones fluids, where we report accurate calculations of theexcess chemical potential, demonstrating that Neural TI is capable of couplinghundreds of degrees of freedom at once.</description><author>Bálint Máté, François Fleuret, Tristan Bereau</author><pubDate>Wed, 12 Jun 2024 17:08:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02313v2</guid></item><item><title>Multiple-Choice Questions are Efficient and Robust LLM Evaluators</title><link>http://arxiv.org/abs/2405.11966v3</link><description>We present GSM-MC and MATH-MC, two multiple-choice (MC) datasets constructedby collecting answers and incorrect predictions on GSM8K and MATH from 60open-source models. Through extensive experiments, we show that LLMs'performance on the MC versions of these two popular benchmarks is stronglycorrelated with their performance on the original versions and is quite robustto distractor choices and option orders, while the evaluation time is reducedby a factor of up to 30. Following a similar procedure, we introduce PythonIO,a new program output prediction MC dataset constructed from two other popularLLM evaluation benchmarks, HumanEval and MBPP. Our data and code are availableat https://github.com/Geralt-Targaryen/MC-Evaluation.</description><author>Ziyin Zhang, Lizhen Xu, Zhaokun Jiang, Hongkun Hao, Rui Wang</author><pubDate>Wed, 12 Jun 2024 17:05:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11966v3</guid></item><item><title>Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis</title><link>http://arxiv.org/abs/2402.09151v2</link><description>In the current environment, psychological issues are prevalent andwidespread, with social media serving as a key outlet for individuals to sharetheir feelings. This results in the generation of vast quantities of datadaily, where negative emotions have the potential to precipitate crisissituations. There is a recognized need for models capable of efficientanalysis. While pre-trained language models have demonstrated theireffectiveness broadly, there's a noticeable gap in pre-trained models tailoredfor specialized domains like psychology. To address this, we have collected ahuge dataset from Chinese social media platforms and enriched it with publiclyavailable datasets to create a comprehensive database encompassing 3.36 milliontext entries. To enhance the model's applicability to psychological textanalysis, we integrated psychological lexicons into the pre-training maskingmechanism. Building on an existing Chinese language model, we performedadaptive training to develop a model specialized for the psychological domain.We evaluated our model's performance across six public datasets, where itdemonstrated improvements compared to eight other models. Additionally, in thequalitative comparison experiment, our model provided psychologically relevantpredictions given the masked sentences. Due to concerns regarding data privacy,the dataset will not be made publicly available. However, we have made thepre-trained models and codes publicly accessible to the community via:https://github.com/zwzzzQAQ/Chinese-MentalBERT.</description><author>Wei Zhai, Hongzhi Qi, Qing Zhao, Jianqiang Li, Ziqi Wang, Han Wang, Bing Xiang Yang, Guanghui Fu</author><pubDate>Wed, 12 Jun 2024 17:03:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09151v2</guid></item><item><title>An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration</title><link>http://arxiv.org/abs/2402.04978v2</link><description>While Large Language Models (LLMs) demonstrate exceptional performance in amultitude of Natural Language Processing (NLP) tasks, they encounter challengesin practical applications, including issues with hallucinations, inadequateknowledge updating, and limited transparency in the reasoning process. Toovercome these limitations, this study innovatively proposes a collaborativetraining-free reasoning scheme involving tight cooperation between KnowledgeGraph (KG) and LLMs. This scheme first involves using LLMs to iterativelyexplore KG, selectively retrieving a task-relevant knowledge subgraph tosupport reasoning. The LLMs are then guided to further combine inherentimplicit knowledge to reason on the subgraph while explicitly elucidating thereasoning process. Through such a cooperative approach, our scheme achievesmore reliable knowledge-based reasoning and facilitates the tracing of thereasoning results. Experimental results show that our scheme significantlyprogressed across multiple datasets, notably achieving over a 10% improvementon the QALD10 dataset compared to the best baseline and the fine-tunedstate-of-the-art (SOTA) work. Building on this success, this study hopes tooffer a valuable reference for future research in the fusion of KG and LLMs,thereby enhancing LLMs' proficiency in solving complex issues.</description><author>Yihao Li, Ru Zhang, Jianyi Liu</author><pubDate>Wed, 12 Jun 2024 17:03:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04978v2</guid></item><item><title>From a Social Cognitive Perspective: Context-aware Visual Social Relationship Recognition</title><link>http://arxiv.org/abs/2406.08358v1</link><description>People's social relationships are often manifested through theirsurroundings, with certain objects or interactions acting as symbols forspecific relationships, e.g., wedding rings, roses, hugs, or holding hands.This brings unique challenges to recognizing social relationships, requiringunderstanding and capturing the essence of these contexts from visualappearances. However, current methods of social relationship understanding relyon the basic classification paradigm of detected persons and objects, whichfails to understand the comprehensive context and often overlooks decisivesocial factors, especially subtle visual cues. To highlight the social-awarecontext and intricate details, we propose a novel approach that recognizes\textbf{Con}textual \textbf{So}cial \textbf{R}elationships (\textbf{ConSoR})from a social cognitive perspective. Specifically, to incorporate social-awaresemantics, we build a lightweight adapter upon the frozen CLIP to learn socialconcepts via our novel multi-modal side adapter tuning mechanism. Further, weconstruct social-aware descriptive language prompts (e.g., scene, activity,objects, emotions) with social relationships for each image, and then compelConSoR to concentrate more intensively on the decisive visual social factorsvia visual-linguistic contrasting. Impressively, ConSoR outperforms previousmethods with a 12.2\% gain on the People-in-Social-Context (PISC) dataset and a9.8\% increase on the People-in-Photo-Album (PIPA) benchmark. Furthermore, weobserve that ConSoR excels at finding critical visual evidence to reveal socialrelationships.</description><author>Shiwei Wu, Chao Zhang, Joya Chen, Tong Xu, Likang Wu, Yao Hu, Enhong Chen</author><pubDate>Wed, 12 Jun 2024 17:02:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08358v1</guid></item><item><title>On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top-$n$ Recommendation</title><link>http://arxiv.org/abs/2307.15053v3</link><description>Approaches to recommendation are typically evaluated in one of two ways: (1)via a (simulated) online experiment, often seen as the gold standard, or (2)via some offline evaluation procedure, where the goal is to approximate theoutcome of an online experiment. Several offline evaluation metrics have beenadopted in the literature, inspired by ranking metrics prevalent in the fieldof Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is onesuch metric that has seen widespread adoption in empirical studies, and higher(n)DCG values have been used to present new methods as the state-of-the-art intop-$n$ recommendation for many years. Our work takes a critical look at this approach, and investigates when we canexpect such metrics to approximate the gold standard outcome of an onlineexperiment. We formally present the assumptions that are necessary to considerDCG an unbiased estimator of online reward and provide a derivation for thismetric from first principles, highlighting where we deviate from itstraditional uses in IR. Importantly, we show that normalising the metricrenders it inconsistent, in that even when DCG is unbiased, ranking competingmethods by their normalised DCG can invert their relative order. Through acorrelation analysis between off- and on-line experiments conducted on alarge-scale recommendation platform, we show that our unbiased DCG estimatesstrongly correlate with online reward, even when some of the metric's inherentassumptions are violated. This statement no longer holds for its normalisedvariant, suggesting that nDCG's practical utility may be limited.</description><author>Olivier Jeunen, Ivan Potapov, Aleksei Ustimenko</author><pubDate>Wed, 12 Jun 2024 17:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15053v3</guid></item><item><title>DocSynthv2: A Practical Autoregressive Modeling for Document Generation</title><link>http://arxiv.org/abs/2406.08354v1</link><description>While the generation of document layouts has been extensively explored,comprehensive document generation encompassing both layout and content presentsa more complex challenge. This paper delves into this advanced domain,proposing a novel approach called DocSynthv2 through the development of asimple yet effective autoregressive structured model. Our model, distinct inits integration of both layout and textual cues, marks a step beyond existinglayout-generation approaches. By focusing on the relationship between thestructural elements and the textual content within documents, we aim togenerate cohesive and contextually relevant documents without any reliance onvisual components. Through experimental studies on our curated benchmark forthe new task, we demonstrate the ability of our model combining layout andtextual information in enhancing the generation quality and relevance ofdocuments, opening new pathways for research in document creation and automateddesign. Our findings emphasize the effectiveness of autoregressive models inhandling complex document generation tasks.</description><author>Sanket Biswas, Rajiv Jain, Vlad I. Morariu, Jiuxiang Gu, Puneet Mathur, Curtis Wigington, Tong Sun, Josep Lladós</author><pubDate>Wed, 12 Jun 2024 17:00:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08354v1</guid></item><item><title>Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques</title><link>http://arxiv.org/abs/2406.08353v1</link><description>Text data is commonly utilized as a primary input to enhance Speech EmotionRecognition (SER) performance and reliability. However, the reliance onhuman-transcribed text in most studies impedes the development of practical SERsystems, creating a gap between in-lab research and real-world scenarios whereAutomatic Speech Recognition (ASR) serves as the text source. Hence, this studybenchmarks SER performance using ASR transcripts with varying Word Error Rates(WERs) on well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Ourevaluation includes text-only and bimodal SER with diverse fusion techniques,aiming for a comprehensive analysis that uncovers novel findings and challengesfaced by current SER research. Additionally, we propose a unified ASRerror-robust framework integrating ASR error correction and modality-gatedfusion, achieving lower WER and higher SER results compared to thebest-performing ASR transcript. This research is expected to provide insightsinto SER with ASR assistance, especially for real-world applications.</description><author>Yuanchao Li, Peter Bell, Catherine Lai</author><pubDate>Wed, 12 Jun 2024 16:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08353v1</guid></item><item><title>Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions</title><link>http://arxiv.org/abs/2405.20267v3</link><description>As LLMs evolve on a daily basis, there is an urgent need for a trustworthyevaluation method that can provide robust evaluation results in a timelyfashion. Currently, as static benchmarks are prone to contamination concerns,users tend to trust human voting platforms, such as Chatbot Arena. However,human annotations require extensive manual efforts. To provide an automatic,robust, and trustworthy evaluation framework, we innovatively propose theAuto-Arena of LLMs, which automates the entire evaluation process with LLMagents. Firstly, an examiner LLM devises queries. Then, a pair of candidateLLMs engage in a multi-round peer-battle around the query, during which theLLM's true performance gaps become visible. Finally, a committee of LLM judgescollectively discuss and determine the winner, which alleviates bias andpromotes fairness. In our extensive experiment on the 17 newest LLMs,Auto-Arena shows the highest correlation with human preferences, providing apromising alternative to human evaluation platforms.</description><author>Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Deli Zhao, Lidong Bing</author><pubDate>Wed, 12 Jun 2024 16:53:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20267v3</guid></item><item><title>What Drives Online Popularity: Author, Content or Sharers? Estimating Spread Dynamics with Bayesian Mixture Hawkes</title><link>http://arxiv.org/abs/2406.03390v2</link><description>The spread of content on social media is shaped by intertwining factors onthree levels: the source, the content itself, and the pathways of contentspread. At the lowest level, the popularity of the sharing user determines itseventual reach. However, higher-level factors such as the nature of the onlineitem and the credibility of its source also play crucial roles in determininghow widely and rapidly the online item spreads. In this work, we propose theBayesian Mixture Hawkes (BMH) model to jointly learn the influence of source,content and spread. We formulate the BMH model as a hierarchical mixture modelof separable Hawkes processes, accommodating different classes of Hawkesdynamics and the influence of feature sets on these classes. We test the BMHmodel on two learning tasks, cold-start popularity prediction and temporalprofile generalization performance, applying to two real-world retweet cascadedatasets referencing articles from controversial and traditional mediapublishers. The BMH model outperforms the state-of-the-art models andpredictive baselines on both datasets and utilizes cascade- and item-levelinformation better than the alternatives. Lastly, we perform a counter-factualanalysis where we apply the trained publisher-level BMH models to a set ofarticle headlines and show that effectiveness of headline writing style(neutral, clickbait, inflammatory) varies across publishers. The BMH modelunveils differences in style effectiveness between controversial and reputablepublishers, where we find clickbait to be notably more effective for reputablepublishers as opposed to controversial ones, which links to the latter'soveruse of clickbait.</description><author>Pio Calderon, Marian-Andrei Rizoiu</author><pubDate>Wed, 12 Jun 2024 16:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03390v2</guid></item><item><title>Blind Image Deblurring using FFT-ReLU with Deep Learning Pipeline Integration</title><link>http://arxiv.org/abs/2406.08344v1</link><description>Blind image deblurring is the process of deriving a sharp image and a blurkernel from a blurred image. Blurry images are typically modeled as theconvolution of a sharp image with a blur kernel, necessitating the estimationof the unknown blur kernel to perform blind image deblurring effectively.Existing approaches primarily focus on domain-specific features of images, suchas salient edges, dark channels, and light streaks. These features serve asprobabilistic priors to enhance the estimation of the blur kernel. For improvedgenerality, we propose a novel prior (ReLU sparsity prior) that estimates blurkernel effectively across all distributions of images (natural, facial, text,low-light, saturated etc). Our approach demonstrates superior efficiency, withinference times up to three times faster, while maintaining high accuracy inPSNR, SSIM, and error ratio metrics. We also observe noticeable improvement inthe performance of the state-of-the-art architectures (in terms ofaforementioned metrics) in deep learning based approaches when our method isused as a post-processing unit.</description><author>Abdul Mohaimen Al Radi, Prothito Shovon Majumder, Syed Mumtahin Mahmud, Mahdi Mohd Hossain Noki, Md. Haider Ali, Md. Mosaddek Khan</author><pubDate>Wed, 12 Jun 2024 16:51:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08344v1</guid></item><item><title>Continuous-Time Digital Twin with Analogue Memristive Neural Ordinary Differential Equation Solver</title><link>http://arxiv.org/abs/2406.08343v1</link><description>Digital twins, the cornerstone of Industry 4.0, replicate real-world entitiesthrough computer models, revolutionising fields such as manufacturingmanagement and industrial automation. Recent advances in machine learningprovide data-driven methods for developing digital twins using discrete-timedata and finite-depth models on digital computers. However, this approach failsto capture the underlying continuous dynamics and struggles with modellingcomplex system behaviour. Additionally, the architecture of digital computers,with separate storage and processing units, necessitates frequent datatransfers and Analogue-Digital (A/D) conversion, thereby significantlyincreasing both time and energy costs. Here, we introduce a memristive neuralordinary differential equation (ODE) solver for digital twins, which is capableof capturing continuous-time dynamics and facilitates the modelling of complexsystems using an infinite-depth model. By integrating storage and computationwithin analogue memristor arrays, we circumvent the von Neumann bottleneck,thus enhancing both speed and energy efficiency. We experimentally validate ourapproach by developing a digital twin of the HP memristor, which accuratelyextrapolates its nonlinear dynamics, achieving a 4.2-fold projected speedup anda 41.4-fold projected decrease in energy consumption compared tostate-of-the-art digital hardware, while maintaining an acceptable errormargin. Additionally, we demonstrate scalability through experimentallygrounded simulations of Lorenz96 dynamics, exhibiting projected performanceimprovements of 12.6-fold in speed and 189.7-fold in energy efficiency relativeto traditional digital approaches. By harnessing the capabilities of fullyanalogue computing, our breakthrough accelerates the development of digitaltwins, offering an efficient and rapid solution to meet the demands of Industry4.0.</description><author>Hegan Chen, Jichang Yang, Jia Chen, Songqi Wang, Shaocong Wang, Dingchen Wang, Xinyu Tian, Yifei Yu, Xi Chen, Yinan Lin, Yangu He, Xiaoshan Wu, Yi Li, Xinyuan Zhang, Ning Lin, Meng Xu, Yi Li, Xumeng Zhang, Zhongrui Wang, Han Wang, Dashan Shang, Qi Liu, Kwang-Ting Cheng, Ming Liu</author><pubDate>Wed, 12 Jun 2024 16:50:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08343v1</guid></item><item><title>Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review</title><link>http://arxiv.org/abs/2403.15274v2</link><description>The year 2023 marked a significant surge in the exploration of applying largelanguage model (LLM) chatbots, notably ChatGPT, across various disciplines. Wesurveyed the applications of ChatGPT in bioinformatics and biomedicalinformatics throughout the year, covering omics, genetics, biomedical textmining, drug discovery, biomedical image understanding, bioinformaticsprogramming, and bioinformatics education. Our survey delineates the currentstrengths and limitations of this chatbot in bioinformatics and offers insightsinto potential avenues for future developments.</description><author>Jinge Wang, Zien Cheng, Qiuming Yao, Li Liu, Dong Xu, Gangqing Hu</author><pubDate>Wed, 12 Jun 2024 16:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15274v2</guid></item><item><title>WMAdapter: Adding WaterMark Control to Latent Diffusion Models</title><link>http://arxiv.org/abs/2406.08337v1</link><description>Watermarking is crucial for protecting the copyright of AI-generated images.We propose WMAdapter, a diffusion model watermark plugin that takesuser-specified watermark information and allows for seamless watermarkimprinting during the diffusion generation process. WMAdapter is efficient androbust, with a strong emphasis on high generation quality. To achieve this, wemake two key designs: (1) We develop a contextual adapter structure that islightweight and enables effective knowledge transfer from heavily pretrainedpost-hoc watermarking models. (2) We introduce an extra finetuning step anddesign a hybrid finetuning strategy to further improve image quality andeliminate tiny artifacts. Empirical results demonstrate that WMAdapter offersstrong flexibility, exceptional image generation quality and competitivewatermark robustness.</description><author>Hai Ci, Yiren Song, Pei Yang, Jinheng Xie, Mike Zheng Shou</author><pubDate>Wed, 12 Jun 2024 16:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08337v1</guid></item><item><title>CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction</title><link>http://arxiv.org/abs/2406.08336v1</link><description>Dysarthric speech reconstruction (DSR) aims to transform dysarthric speechinto normal speech. It still suffers from low speaker similarity and poorprosody naturalness. In this paper, we propose a multi-modal DSR model byleveraging neural codec language modeling to improve the reconstructionresults, especially for the speaker similarity and prosody naturalness. Ourproposed model consists of: (i) a multi-modal content encoder to extract robustphoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) aspeaker codec encoder to extract and normalize the speaker-aware codecs fromthe dysarthric speech, in order to provide original timbre and normal prosody;(iii) a codec language model based speech decoder to reconstruct the speechbased on the extracted phoneme embeddings and normalized codecs. Evaluations onthe commonly used UASpeech corpus show that our proposed model can achievesignificant improvements in terms of speaker similarity and prosodynaturalness.</description><author>Xueyuan Chen, Dongchao Yang, Dingdong Wang, Xixin Wu, Zhiyong Wu, Helen Meng</author><pubDate>Wed, 12 Jun 2024 16:42:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08336v1</guid></item><item><title>A Survey of Pipeline Tools for Data Engineering</title><link>http://arxiv.org/abs/2406.08335v1</link><description>Currently, a variety of pipeline tools are available for use in dataengineering. Data scientists can use these tools to resolve data wranglingissues associated with data and accomplish some data engineering tasks fromdata ingestion through data preparation to utilization as input for machinelearning (ML). Some of these tools have essential built-in components or can becombined with other tools to perform desired data engineering operations. Whilesome tools are wholly or partly commercial, several open-source tools areavailable to perform expert-level data engineering tasks. This survey examinesthe broad categories and examples of pipeline tools based on their design anddata engineering intentions. These categories are Extract TransformLoad/Extract Load Transform (ETL/ELT), pipelines for Data Integration,Ingestion, and Transformation, Data Pipeline Orchestration and WorkflowManagement, and Machine Learning Pipelines. The survey also provides a broadoutline of the utilization with examples within these broad groups and finally,a discussion is presented with case studies indicating the usage of pipelinetools for data engineering. The studies present some first-user applicationexperiences with sample data, some complexities of the applied pipeline, and asummary note of approaches to using these tools to prepare data for machinelearning.</description><author>Anthony Mbata, Yaji Sripada, Mingjun Zhong</author><pubDate>Wed, 12 Jun 2024 16:41:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08335v1</guid></item><item><title>ProTrain: Efficient LLM Training via Memory-Aware Techniques</title><link>http://arxiv.org/abs/2406.08334v1</link><description>It is extremely memory-hungry to train Large Language Models (LLM). To solvethis problem, existing work exploits the combination of CPU and GPU for thetraining process, such as ZeRO-Offload. Such a technique largely democratizesbillion-scale model training, making it possible to train with few consumergraphics cards. However, based on our observation, existing frameworks oftenprovide coarse-grained memory management and require experienced experts inconfiguration tuning, leading to suboptimal hardware utilization andperformance. This paper proposes ProTrain, a novel training system thatintelligently balances memory usage and performance by coordinating memory,computation, and IO. ProTrain achieves adaptive memory management throughChunk-Based Model State Management and Block-Wise Activation Management, guidedby a Memory-Aware Runtime Profiler without user intervention. ProTrain does notchange the training algorithm and thus does not compromise accuracy.Experiments show that ProTrain improves training throughput by 1.43$\times$ to2.71$\times$ compared to the SOTA training systems.</description><author>Hanmei Yang, Jin Zhou, Yao Fu, Xiaoqun Wang, Ramine Roane, Hui Guan, Tongping Liu</author><pubDate>Wed, 12 Jun 2024 16:40:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08334v1</guid></item><item><title>UDON: Universal Dynamic Online distillatioN for generic image representations</title><link>http://arxiv.org/abs/2406.08332v1</link><description>Universal image representations are critical in enabling real-worldfine-grained and instance-level recognition applications, where objects andentities from any domain must be identified at large scale. Despite recentadvances, existing methods fail to capture important domain-specific knowledge,while also ignoring differences in data distribution across different domains.This leads to a large performance gap between efficient universal solutions andexpensive approaches utilising a collection of specialist models, one for eachdomain. In this work, we make significant strides towards closing this gap, byintroducing a new learning technique, dubbed UDON (Universal Dynamic OnlineDistillatioN). UDON employs multi-teacher distillation, where each teacher isspecialized in one domain, to transfer detailed domain-specific knowledge intothe student universal embedding. UDON's distillation approach is not onlyeffective, but also very efficient, by sharing most model parameters betweenthe student and all teachers, where all models are jointly trained in an onlinemanner. UDON also comprises a sampling technique which adapts the trainingprocess to dynamically allocate batches to domains which are learned slower andrequire more frequent processing. This boosts significantly the learning ofcomplex domains which are characterised by a large number of classes andlong-tail distributions. With comprehensive experiments, we validate eachcomponent of UDON, and showcase significant improvements over the state of theart in the recent UnED benchmark. Code: https://github.com/nikosips/UDON .</description><author>Nikolaos-Antonios Ypsilantis, Kaifeng Chen, André Araujo, Ondřej Chum</author><pubDate>Wed, 12 Jun 2024 16:36:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08332v1</guid></item><item><title>LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry</title><link>http://arxiv.org/abs/2401.01887v2</link><description>Visual odometry estimates the motion of a moving camera based on visualinput. Existing methods, mostly focusing on two-view point tracking, oftenignore the rich temporal context in the image sequence, thereby overlooking theglobal motion patterns and providing no assessment of the full trajectoryreliability. These shortcomings hinder performance in scenarios with occlusion,dynamic objects, and low-texture areas. To address these challenges, we presentthe Long-term Effective Any Point Tracking (LEAP) module. LEAP innovativelycombines visual, inter-track, and temporal cues with mindfully selected anchorsfor dynamic track estimation. Moreover, LEAP's temporal probabilisticformulation integrates distribution updates into a learnable iterativerefinement module to reason about point-wise uncertainty. Based on thesetraits, we develop LEAP-VO, a robust visual odometry system adept at handlingocclusions and dynamic scenes. Our mindful integration showcases a novelpractice by employing long-term point tracking as the front-end. Extensiveexperiments demonstrate that the proposed pipeline significantly outperformsexisting baselines across various visual odometry benchmarks.</description><author>Weirong Chen, Le Chen, Rui Wang, Marc Pollefeys</author><pubDate>Wed, 12 Jun 2024 16:36:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01887v2</guid></item><item><title>Genetic Column Generation for Computing Lower Bounds for Adversarial Classification</title><link>http://arxiv.org/abs/2406.08331v1</link><description>Recent theoretical results on adversarial multi-class classification showed asimilarity to the multi-marginal formulation of Wasserstein-barycenter inoptimal transport. Unfortunately, both problems suffer from the curse ofdimension, making it hard to exploit the nice linear program structure of theproblems for numerical calculations. We investigate how ideas from GeneticColumn Generation for multi-marginal optimal transport can be used to overcomethe curse of dimension in computing the minimal adversarial risk in multi-classclassification.</description><author>Maximilian Penka</author><pubDate>Wed, 12 Jun 2024 16:34:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08331v1</guid></item><item><title>It's all about PR -- Smart Benchmarking AI Accelerators using Performance Representatives</title><link>http://arxiv.org/abs/2406.08330v1</link><description>Statistical models are widely used to estimate the performance of commercialoff-the-shelf (COTS) AI hardware accelerators. However, training of statisticalperformance models often requires vast amounts of data, leading to asignificant time investment and can be difficult in case of limited hardwareavailability. To alleviate this problem, we propose a novel performancemodeling methodology that significantly reduces the number of training sampleswhile maintaining good accuracy. Our approach leverages knowledge of the targethardware architecture and initial parameter sweeps to identify a set ofPerformance Representatives (PR) for deep neural network (DNN) layers. ThesePRs are then used for benchmarking, building a statistical performance model,and making estimations. This targeted approach drastically reduces the numberof training samples needed, opposed to random sampling, to achieve a betterestimation accuracy. We achieve a Mean Absolute Percentage Error (MAPE) of aslow as 0.02% for single-layer estimations and 0.68% for whole DNN estimationswith less than 10000 training samples. The results demonstrate the superiorityof our method for single-layer estimations compared to models trained withrandomly sampled datasets of the same size.</description><author>Alexander Louis-Ferdinand Jung, Jannik Steinmetz, Jonathan Gietz, Konstantin Lübeck, Oliver Bringmann</author><pubDate>Wed, 12 Jun 2024 16:34:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08330v1</guid></item><item><title>Predictive Linear Online Tracking for Unknown Targets</title><link>http://arxiv.org/abs/2402.10036v2</link><description>In this paper, we study the problem of online tracking in linear controlsystems, where the objective is to follow a moving target. Unlike classicaltracking control, the target is unknown, non-stationary, and its state isrevealed sequentially, thus, fitting the framework of online non-stochasticcontrol. We consider the case of quadratic costs and propose a new algorithm,called predictive linear online tracking (PLOT). The algorithm uses recursiveleast squares with exponential forgetting to learn a time-varying dynamic modelof the target. The learned model is used in the optimal policy under theframework of receding horizon control. We show the dynamic regret of PLOTscales with $\mathcal{O}(\sqrt{TV_T})$, where $V_T$ is the total variation ofthe target dynamics and $T$ is the time horizon. Unlike prior work, ourtheoretical results hold for non-stationary targets. We implement PLOT on areal quadrotor and provide open-source software, thus, showcasing one of thefirst successful applications of online control methods on real hardware.</description><author>Anastasios Tsiamis, Aren Karapetyan, Yueshan Li, Efe C. Balta, John Lygeros</author><pubDate>Wed, 12 Jun 2024 16:27:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10036v2</guid></item><item><title>Diffusion models for Gaussian distributions: Exact solutions and Wasserstein errors</title><link>http://arxiv.org/abs/2405.14250v3</link><description>Diffusion or score-based models recently showed high performance in imagegeneration. They rely on a forward and a backward stochastic differentialequations (SDE). The sampling of a data distribution is achieved by solvingnumerically the backward SDE or its associated flow ODE. Studying theconvergence of these models necessitates to control four different types oferror: the initialization error, the truncation error, the discretization andthe score approximation. In this paper, we study theoretically the behavior ofdiffusion models and their numerical implementation when the data distributionis Gaussian. In this restricted framework where the score function is a linearoperator, we can derive the analytical solutions of the forward and backwardSDEs as well as the associated flow ODE. This provides exact expressions forvarious Wasserstein errors which enable us to compare the influence of eacherror type for any sampling scheme, thus allowing to monitor convergencedirectly in the data space instead of relying on Inception features. Ourexperiments show that the recommended numerical schemes from the diffusionmodels literature are also the best sampling schemes for Gaussiandistributions.</description><author>Emile Pierret, Bruno Galerne</author><pubDate>Wed, 12 Jun 2024 16:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14250v3</guid></item><item><title>LaMOT: Language-Guided Multi-Object Tracking</title><link>http://arxiv.org/abs/2406.08324v1</link><description>Vision-Language MOT is a crucial tracking problem and has drawn increasingattention recently. It aims to track objects based on human language commands,replacing the traditional use of templates or pre-set information from trainingsets in conventional tracking tasks. Despite various efforts, a key challengelies in the lack of a clear understanding of why language is used for tracking,which hinders further development in this field. In this paper, we address thischallenge by introducing Language-Guided MOT, a unified task framework, alongwith a corresponding large-scale benchmark, termed LaMOT, which encompassesdiverse scenarios and language descriptions. Specially, LaMOT comprises 1,660sequences from 4 different datasets and aims to unify various Vision-LanguageMOT tasks while providing a standardized evaluation platform. To ensurehigh-quality annotations, we manually assign appropriate descriptive texts toeach target in every video and conduct careful inspection and correction. Tothe best of our knowledge, LaMOT is the first benchmark dedicated toLanguage-Guided MOT. Additionally, we propose a simple yet effective tracker,termed LaMOTer. By establishing a unified task framework, providing challengingbenchmarks, and offering insights for future algorithm design and evaluation,we expect to contribute to the advancement of research in Vision-Language MOT.We will release the data at https://github.com/Nathan-Li123/LaMOT.</description><author>Yunhao Li, Xiaoqiong Liu, Luke Liu, Heng Fan, Libo Zhang</author><pubDate>Wed, 12 Jun 2024 16:24:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08324v1</guid></item><item><title>Illustrating the benefits of efficient creation and adaption of behavior models in intelligent Digital Twins over the machine life cycle</title><link>http://arxiv.org/abs/2406.08323v1</link><description>The concept of the Digital Twin, which in the context of this paper is thevirtual representation of a production system or its components, can be used asa "digital playground" to master the increasing complexity of these assets.Central subcomponents of the Digital Twin are behavior models that can providebenefits over the entire machine life cycle. However, the creation, adaptionand use of behavior models throughout the machine life cycle is verytime-consuming, which is why approaches to improve the cost-benefit ratio areneeded. Furthermore, there is a lack of specific use cases that illustrate theapplication and added benefit of behavior models over the machine life cycle,which is why the universal application of behavior models in industry is stilllacking compared to research. This paper first presents the fundamentals,challenges and related work on Digital Twins and behavior models in the contextof the machine life cycle. Then, concepts for low-effort creation and automaticadaption of Digital Twins are presented, with a focus on behavior models.Finally, the aforementioned gap between research and industry is addressed bydemonstrating various realized use cases over the machine life cycle, in whichthe advantages as well as the application of behavior models in the differentlife phases are shown.</description><author>Daniel Dittler, Valentin Stegmaier, Nasser Jazdi, Michael Weyrich</author><pubDate>Wed, 12 Jun 2024 16:23:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08323v1</guid></item><item><title>MMIL: A novel algorithm for disease associated cell type discovery</title><link>http://arxiv.org/abs/2406.08322v1</link><description>Single-cell datasets often lack individual cell labels, making it challengingto identify cells associated with disease. To address this, we introduceMixture Modeling for Multiple Instance Learning (MMIL), an expectationmaximization method that enables the training and calibration of cell-levelclassifiers using patient-level labels. Our approach can be used to train e.g.lasso logistic regression models, gradient boosted trees, and neural networks.When applied to clinically-annotated, primary patient samples in Acute MyeloidLeukemia (AML) and Acute Lymphoblastic Leukemia (ALL), our method accuratelyidentifies cancer cells, generalizes across tissues and treatment timepoints,and selects biologically relevant features. In addition, MMIL is capable ofincorporating cell labels into model training when they are known, providing apowerful framework for leveraging both labeled and unlabeled datasimultaneously. Mixture Modeling for MIL offers a novel approach for cellclassification, with significant potential to advance disease understanding andmanagement, especially in scenarios with unknown gold-standard labels and highdimensionality.</description><author>Erin Craig, Timothy Keyes, Jolanda Sarno, Maxim Zaslavsky, Garry Nolan, Kara Davis, Trevor Hastie, Robert Tibshirani</author><pubDate>Wed, 12 Jun 2024 16:22:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08322v1</guid></item><item><title>Deep learning from strongly mixing observations: Sparse-penalized regularization and minimax optimality</title><link>http://arxiv.org/abs/2406.08321v1</link><description>The explicit regularization and optimality of deep neural networks estimatorsfrom independent data have made considerable progress recently. The study ofsuch properties on dependent data is still a challenge. In this paper, we carryout deep learning from strongly mixing observations, and deal with the squaredand a broad class of loss functions. We consider sparse-penalizedregularization for deep neural network predictor. For a general framework thatincludes, regression estimation, classification, time seriesprediction,$\cdots$, oracle inequality for the expected excess risk isestablished and a bound on the class of H\"older smooth functions is provided.For nonparametric regression from strong mixing data and sub-exponentiallyerror, we provide an oracle inequality for the $L_2$ error and investigate anupper bound of this error on a class of H\"older composition functions. For thespecific case of nonparametric autoregression with Gaussian and Laplace errors,a lower bound of the $L_2$ error on this H\"older composition class isestablished. Up to logarithmic factor, this bound matches its upper bound; so,the deep neural network estimator attains the minimax optimal rate.</description><author>William Kengne, Modou Wade</author><pubDate>Wed, 12 Jun 2024 16:21:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08321v1</guid></item></channel></rss>