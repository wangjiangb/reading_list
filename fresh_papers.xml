<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 21 Feb 2024 14:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey</title><link>http://arxiv.org/abs/2402.13255v1</link><description>Over the past two decades, research in the field of Simultaneous Localizationand Mapping (SLAM) has undergone a significant evolution, highlighting itscritical role in enabling autonomous exploration of unknown environments. Thisevolution ranges from hand-crafted methods, through the era of deep learning,to more recent developments focused on Neural Radiance Fields (NeRFs) and 3DGaussian Splatting (3DGS) representations. Recognizing the growing body ofresearch and the absence of a comprehensive survey on the topic, this paperaims to provide the first comprehensive overview of SLAM progress through thelens of the latest advancements in radiance fields. It sheds light on thebackground, evolutionary path, inherent strengths and limitations, and servesas a fundamental reference to highlight the dynamic progress and specificchallenges.</description><author>Fabio Tosi, Youmin Zhang, Ziren Gong, Erik Sandstr√∂m, Stefano Mattoccia, Martin R. Oswald, Matteo Poggi</author><pubDate>Tue, 20 Feb 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13255v1</guid></item><item><title>CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples</title><link>http://arxiv.org/abs/2402.13254v1</link><description>We propose CounterCurate, a framework to comprehensively improve thevisio-linguistic compositional reasoning capability for both contrastive andgenerative multimodal models. In particular, we identify two under-exploredcritical problems: the neglect of the physically grounded reasoning (countingand position understanding) and the potential of using highly capable text andimage generation models for semantic counterfactual fine-tuning. Our workpioneers an approach that addresses these gaps. We first spotlight thenear-chance performance of multimodal models like CLIP and LLaVA in physicallygrounded compositional reasoning. We then apply simple data augmentation usinga grounded image generation model, GLIGEN, to generate finetuning data,resulting in significant performance improvements: +33% and +37% for CLIP andLLaVA, respectively, on our newly curated Flickr30k-Positions benchmark.Moreover, we exploit the capabilities of high-performing text generation andimage generation models, specifically GPT-4V and DALLE-3, to curate challengingsemantic counterfactuals, thereby further enhancing compositional reasoningcapabilities on benchmarks such as SugarCrepe, where CounterCurate outperformsGPT-4V.</description><author>Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee</author><pubDate>Tue, 20 Feb 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13254v1</guid></item><item><title>BiMediX: Bilingual Medical Mixture of Experts LLM</title><link>http://arxiv.org/abs/2402.13253v1</link><description>In this paper, we introduce BiMediX, the first bilingual medical mixture ofexperts LLM designed for seamless interaction in both English and Arabic. Ourmodel facilitates a wide range of medical interactions in English and Arabic,including multi-turn chats to inquire about additional details such as patientsymptoms and medical history, multiple-choice question answering, andopen-ended question answering. We propose a semi-automated English-to-Arabictranslation pipeline with human refinement to ensure high-quality translations.We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs.Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingualinstruction set covering 1.3 Million diverse medical interactions, resulting inover 632 million healthcare specialized tokens for instruction tuning. OurBiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats andmaintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-artMed42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively,computed across multiple medical evaluation benchmarks in English, whileoperating at 8-times faster inference. Moreover, our BiMediX outperforms thegeneric Arabic-English bilingual LLM, Jais-30B, by average absolute gains of10% on our Arabic medical benchmark and 15% on bilingual evaluations acrossmultiple datasets. Our project page with source code and trained model isavailable at https://github.com/mbzuai-oryx/BiMediX .</description><author>Sara Pieri, Sahal Shaji Mullappilly, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan, Timothy Baldwin, Hisham Cholakkal</author><pubDate>Tue, 20 Feb 2024 18:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13253v1</guid></item><item><title>Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields</title><link>http://arxiv.org/abs/2402.13252v1</link><description>In this paper, we propose an algorithm that allows joint refinement of camerapose and scene geometry represented by decomposed low-rank tensor, using only2D images as supervision. First, we conduct a pilot study based on a 1D signaland relate our findings to 3D scenarios, where the naive joint poseoptimization on voxel-based NeRFs can easily lead to sub-optimal solutions.Moreover, based on the analysis of the frequency spectrum, we propose to applyconvolutional Gaussian filters on 2D and 3D radiance fields for acoarse-to-fine training schedule that enables joint camera pose optimization.Leveraging the decomposition property in decomposed low-rank tensor, our methodachieves an equivalent effect to brute-force 3D convolution with only incurringlittle computational overhead. To further improve the robustness and stabilityof joint optimization, we also propose techniques of smoothed 2D supervision,randomly scaled kernel parameters, and edge-guided loss mask. Extensivequantitative and qualitative evaluations demonstrate that our proposedframework achieves superior performance in novel view synthesis as well asrapid convergence for optimization.</description><author>Bo-Yu Cheng, Wei-Chen Chiu, Yu-Lun Liu</author><pubDate>Tue, 20 Feb 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13252v1</guid></item><item><title>FlashTex: Fast Relightable Mesh Texturing with LightControlNet</title><link>http://arxiv.org/abs/2402.13251v1</link><description>Manually creating textures for 3D meshes is time-consuming, even for expertvisual content creators. We propose a fast approach for automatically texturingan input 3D mesh based on a user-provided text prompt. Importantly, ourapproach disentangles lighting from surface material/reflectance in theresulting texture so that the mesh can be properly relit and rendered in anylighting environment. We introduce LightControlNet, a new text-to-image modelbased on the ControlNet architecture, which allows the specification of thedesired lighting as a conditioning image to the model. Our text-to-texturepipeline then constructs the texture in two stages. The first stage produces asparse set of visually consistent reference views of the mesh usingLightControlNet. The second stage applies a texture optimization based on ScoreDistillation Sampling (SDS) that works with LightControlNet to increase thetexture quality while disentangling surface material from lighting. Ourpipeline is significantly faster than previous text-to-texture methods, whileproducing high-quality and relightable textures.</description><author>Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan Zhu, Tinghui Zhou, Maneesh Agrawala</author><pubDate>Tue, 20 Feb 2024 18:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13251v1</guid></item><item><title>Video ReCap: Recursive Captioning of Hour-Long Videos</title><link>http://arxiv.org/abs/2402.13250v1</link><description>Most video captioning models are designed to process short video clips of fewseconds and output text describing low-level visual concepts (e.g., objects,scenes, atomic actions). However, most real-world videos last for minutes orhours and have a complex hierarchical structure spanning different temporalgranularities. We propose Video ReCap, a recursive video captioning model thatcan process video inputs of dramatically different lengths (from 1 second to 2hours) and output video captions at multiple hierarchy levels. The recursivevideo-language architecture exploits the synergy between different videohierarchies and can process hour-long videos efficiently. We utilize acurriculum learning training scheme to learn the hierarchical structure ofvideos, starting from clip-level captions describing atomic actions, thenfocusing on segment-level descriptions, and concluding with generatingsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset byaugmenting Ego4D with 8,267 manually collected long-range video summaries. Ourrecursive model can flexibly generate captions at different hierarchy levelswhile also being useful for other complex video understanding tasks, such asVideoQA on EgoSchema. Data, code, and models are available at:https://sites.google.com/view/vidrecap</description><author>Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius</author><pubDate>Tue, 20 Feb 2024 18:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13250v1</guid></item><item><title>TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization</title><link>http://arxiv.org/abs/2402.13249v1</link><description>Single document news summarization has seen substantial progress onfaithfulness in recent years, driven by research on the evaluation of factualconsistency, or hallucinations. We ask whether these advances carry over toother text summarization domains. We propose a new evaluation benchmark ontopic-focused dialogue summarization, generated by LLMs of varying sizes. Weprovide binary sentence-level human annotations of the factual consistency ofthese summaries along with detailed explanations of factually inconsistentsentences. Our analysis shows that existing LLMs hallucinate significantamounts of factual errors in the dialogue domain, regardless of the model'ssize. On the other hand, when LLMs, including GPT-4, serve as binary factualevaluators, they perform poorly and can be outperformed by prevailingstate-of-the-art specialized factuality evaluation metrics. Finally, weconducted an analysis of hallucination types with a curated error taxonomy. Wefind that there are diverse errors and error distributions in model-generatedsummaries and that non-LLM based metrics can capture all error types betterthan LLM-based evaluators.</description><author>Liyan Tang, Igor Shalyminov, Amy Wing-mei Wong, Jon Burnsky, Jake W. Vincent, Yu'an Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su, Lijia Sun, Yi Zhang, Saab Mansour, Kathleen McKeown</author><pubDate>Tue, 20 Feb 2024 18:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13249v1</guid></item><item><title>Learning in Mean Field Games: A Survey</title><link>http://arxiv.org/abs/2205.12944v3</link><description>Non-cooperative and cooperative games with a very large number of playershave many applications but remain generally intractable when the number ofplayers increases. Introduced by Lasry and Lions, and Huang, Caines andMalham\'e, Mean Field Games (MFGs) rely on a mean-field approximation to allowthe number of players to grow to infinity. Traditional methods for solvingthese games generally rely on solving partial or stochastic differentialequations with a full knowledge of the model. Recently, Reinforcement Learning(RL) has appeared promising to solve complex problems at scale. The combinationof RL and MFGs is promising to solve games at a very large scale both in termsof population size and environment complexity. In this survey, we review thequickly growing recent literature on RL methods to learn equilibria and socialoptima in MFGs. We first identify the most common settings (static, stationary,and evolutive) of MFGs. We then present a general framework for classicaliterative methods (based on best-response computation or policy evaluation) tosolve MFGs in an exact way. Building on these algorithms and the connectionwith Markov Decision Processes, we explain how RL can be used to learn MFGsolutions in a model-free way. Last, we present numerical illustrations on abenchmark problem, and conclude with some perspectives.</description><author>Mathieu Lauri√®re, Sarah Perrin, Julien P√©rolat, Sertan Girgin, Paul Muller, Romuald √âlie, Matthieu Geist, Olivier Pietquin</author><pubDate>Tue, 20 Feb 2024 18:56:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.12944v3</guid></item><item><title>Textless Low-Resource Speech-to-Speech Translation With Unit Language Models</title><link>http://arxiv.org/abs/2305.15405v2</link><description>Existing speech-to-speech translation models fall into two camps: textlessmodels trained with hundreds of hours of parallel speech data or unsupervisedmodels that leverage text as an intermediate step. Both approaches limitbuilding speech-to-speech translation models for a wide range of languages, asthey exclude languages that are primarily spoken and language pairs that lacklarge-scale parallel speech data. We present a new framework for trainingtextless low-resource speech-to-speech translation (S2ST) systems that onlyneed dozens of hours of parallel speech data. We reformulate S2ST as aunit-to-unit seq2seq translation task, and start by pretraining a model onlarge-scale monolingual speech data. Then, we finetune it with a small amountof parallel speech data ($20-60$ hours). Lastly, we improve model performancethrough an unsupervised backtranslation objective. We train and evaluate ourmodels for English-to-German, German-to-English and Marathi-to-Englishtranslation on three different domains (European Parliament, Common Voice, andAll India Radio) with single-speaker synthesized speech data. Evaluated usingthe ASR-BLEU metric, our models achieve reasonable performance on all threedomains, with some being within 1-2 points of our supervised topline.</description><author>Anuj Diwan, Anirudh Srinivasan, David Harwath, Eunsol Choi</author><pubDate>Tue, 20 Feb 2024 18:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15405v2</guid></item><item><title>VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning</title><link>http://arxiv.org/abs/2402.13243v1</link><description>Learning a human-like driving policy from large-scale driving demonstrationsis promising, but the uncertainty and non-deterministic nature of planning makeit challenging. In this work, to cope with the uncertainty problem, we proposeVADv2, an end-to-end driving model based on probabilistic planning. VADv2 takesmulti-view image sequences as input in a streaming manner, transforms sensordata into environmental token embeddings, outputs the probabilisticdistribution of action, and samples one action to control the vehicle. Onlywith camera sensors, VADv2 achieves state-of-the-art closed-loop performance onthe CARLA Town05 benchmark, significantly outperforming all existing methods.It runs stably in a fully end-to-end manner, even without the rule-basedwrapper. Closed-loop demos are presented at https://hgao-cv.github.io/VADv2.</description><author>Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, Xinggang Wang</author><pubDate>Tue, 20 Feb 2024 18:55:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13243v1</guid></item><item><title>AnoMalNet: Outlier Detection based Malaria Cell Image Classification Method Leveraging Deep Autoencoder</title><link>http://arxiv.org/abs/2303.05789v2</link><description>Class imbalance is a pervasive issue in the field of disease classificationfrom medical images. It is necessary to balance out the class distributionwhile training a model for decent results. However, in the case of rare medicaldiseases, images from affected patients are much harder to come by compared toimages from non-affected patients, resulting in unwanted class imbalance.Various processes of tackling class imbalance issues have been explored so far,each having its fair share of drawbacks. In this research, we propose anoutlier detection based binary medical image classification technique which canhandle even the most extreme case of class imbalance. We have utilized adataset of malaria parasitized and uninfected cells. An autoencoder modeltitled AnoMalNet is trained with only the uninfected cell images at thebeginning and then used to classify both the affected and non-affected cellimages by thresholding a loss value. We have achieved an accuracy, precision,recall, and F1 score of 98.49%, 97.07%, 100%, and 98.52% respectively,performing better than large deep learning models and other published works. Asour proposed approach can provide competitive results without needing thedisease-positive samples during training, it should prove to be useful inbinary disease classification on imbalanced datasets.</description><author>Aminul Huq, Md Tanzim Reza, Shahriar Hossain, Shakib Mahmud Dipto</author><pubDate>Tue, 20 Feb 2024 18:54:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05789v2</guid></item><item><title>Federated Causal Discovery from Heterogeneous Data</title><link>http://arxiv.org/abs/2402.13241v1</link><description>Conventional causal discovery methods rely on centralized data, which isinconsistent with the decentralized nature of data in many real-worldsituations. This discrepancy has motivated the development of federated causaldiscovery (FCD) approaches. However, existing FCD methods may be limited bytheir potentially restrictive assumptions of identifiable functional causalmodels or homogeneous data distributions, narrowing their applicability indiverse scenarios. In this paper, we propose a novel FCD method attempting toaccommodate arbitrary causal models and heterogeneous data. We first utilize asurrogate variable corresponding to the client index to account for the dataheterogeneity across different clients. We then develop a federated conditionalindependence test (FCIT) for causal skeleton discovery and establish afederated independent change principle (FICP) to determine causal directions.These approaches involve constructing summary statistics as a proxy of the rawdata to protect data privacy. Owing to the nonparametric properties, FCIT andFICP make no assumption about particular functional forms, thereby facilitatingthe handling of arbitrary causal models. We conduct extensive experiments onsynthetic and real datasets to show the efficacy of our method. The code isavailable at \url{https://github.com/lokali/FedCDH.git}.</description><author>Loka Li, Ignavier Ng, Gongxu Luo, Biwei Huang, Guangyi Chen, Tongliang Liu, Bin Gu, Kun Zhang</author><pubDate>Tue, 20 Feb 2024 18:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13241v1</guid></item><item><title>Seed-Guided Fine-Grained Entity Typing in Science and Engineering Domains</title><link>http://arxiv.org/abs/2401.13129v2</link><description>Accurately typing entity mentions from text segments is a fundamental taskfor various natural language processing applications. Many previous approachesrely on massive human-annotated data to perform entity typing. Nevertheless,collecting such data in highly specialized science and engineering domains(e.g., software engineering and security) can be time-consuming and costly,without mentioning the domain gaps between training and inference data if themodel needs to be applied to confidential datasets. In this paper, we study thetask of seed-guided fine-grained entity typing in science and engineeringdomains, which takes the name and a few seed entities for each entity type asthe only supervision and aims to classify new entity mentions into both seenand unseen types (i.e., those without seed entities). To solve this problem, wepropose SEType which first enriches the weak supervision by finding moreentities for each seen type from an unlabeled corpus using the contextualizedrepresentations of pre-trained language models. It then matches the enrichedentities to unlabeled text to get pseudo-labeled samples and trains a textualentailment model that can make inferences for both seen and unseen types.Extensive experiments on two datasets covering four domains demonstrate theeffectiveness of SEType in comparison with various baselines.</description><author>Yu Zhang, Yunyi Zhang, Yanzhen Shen, Yu Deng, Lucian Popa, Larisa Shwartz, ChengXiang Zhai, Jiawei Han</author><pubDate>Tue, 20 Feb 2024 18:50:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13129v2</guid></item><item><title>Unlocking Insights: Semantic Search in Jupyter Notebooks</title><link>http://arxiv.org/abs/2402.13234v1</link><description>Semantic search, a process aimed at delivering highly relevant search resultsby comprehending the searcher's intent and the contextual meaning of termswithin a searchable dataspace, plays a pivotal role in information retrieval.In this paper, we investigate the application of large language models toenhance semantic search capabilities, specifically tailored for the domain ofJupyter Notebooks. Our objective is to retrieve generated outputs, such asfigures or tables, associated functions and methods, and other pertinentinformation. We demonstrate a semantic search framework that achieves a comprehensivesemantic understanding of the entire notebook's contents, enabling it toeffectively handle various types of user queries. Key components of thisframework include: 1). A data preprocessor is designed to handle diverse types of cells withinJupyter Notebooks, encompassing both markdown and code cells. 2). An innovativemethodology is devised to address token size limitations that arise withcode-type cells. We implement a finer-grained approach to data input,transitioning from the cell level to the function level, effectively resolvingthese issues.</description><author>Lan Li, Jinpeng Lv</author><pubDate>Tue, 20 Feb 2024 18:49:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13234v1</guid></item><item><title>SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification</title><link>http://arxiv.org/abs/2402.13233v1</link><description>Many real-world applications of the Internet of Things (IoT) employ machinelearning (ML) algorithms to analyze time series information collected byinterconnected sensors. However, distribution shift, a fundamental challenge indata-driven ML, arises when a model is deployed on a data distributiondifferent from the training data and can substantially degrade modelperformance. Additionally, increasingly sophisticated deep neural networks(DNNs) are required to capture intricate spatial and temporal dependencies inmulti-sensor time series data, often exceeding the capabilities of today's edgedevices. In this paper, we propose SMORE, a novel resource-efficient domainadaptation (DA) algorithm for multi-sensor time series classification,leveraging the efficient and parallel operations of hyperdimensional computing.SMORE dynamically customizes test-time models with explicit consideration ofthe domain context of each sample to mitigate the negative impacts of domainshifts. Our evaluation on a variety of multi-sensor time series classificationtasks shows that SMORE achieves on average 1.98% higher accuracy thanstate-of-the-art (SOTA) DNN-based DA algorithms with 18.81x faster training and4.63x faster inference.</description><author>Junyao Wang, Mohammad Abdullah Al Faruque</author><pubDate>Tue, 20 Feb 2024 18:48:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13233v1</guid></item><item><title>A Touch, Vision, and Language Dataset for Multimodal Alignment</title><link>http://arxiv.org/abs/2402.13232v1</link><description>Touch is an important sensing modality for humans, but it has not yet beenincorporated into a multimodal generative language model. This is partially dueto the difficulty of obtaining natural language labels for tactile data and thecomplexity of aligning tactile readings with both visual observations andlanguage descriptions. As a step towards bridging that gap, this workintroduces a new dataset of 44K in-the-wild vision-touch pairs, with Englishlanguage labels annotated by humans (10%) and textual pseudo-labels from GPT-4V(90%). We use this dataset to train a vision-language-aligned tactile encoderfor open-vocabulary classification and a touch-vision-language (TVL) model fortext generation using the trained encoder. Results suggest that byincorporating touch, the TVL model improves (+29% classification accuracy)touch-vision-language alignment over existing models trained on any pair ofthose modalities. Although only a small fraction of the dataset ishuman-labeled, the TVL model demonstrates improved visual-tactile understandingover GPT-4V (+12%) and open-source vision-language models (+32%) on a newtouch-vision understanding benchmark. Code and data:https://tactile-vlm.github.io.</description><author>Letian Fu, Gaurav Datta, Huang Huang, William Chung-Ho Panitch, Jaimyn Drake, Joseph Ortiz, Mustafa Mukadam, Mike Lambeta, Roberto Calandra, Ken Goldberg</author><pubDate>Tue, 20 Feb 2024 18:47:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13232v1</guid></item><item><title>Investigating Cultural Alignment of Large Language Models</title><link>http://arxiv.org/abs/2402.13231v1</link><description>The intricate relationship between language and culture has long been asubject of exploration within the realm of linguistic anthropology. LargeLanguage Models (LLMs), promoted as repositories of collective human knowledge,raise a pivotal question: do these models genuinely encapsulate the diverseknowledge adopted by different cultures? Our study reveals that these modelsdemonstrate greater cultural alignment along two dimensions -- firstly, whenprompted with the dominant language of a specific culture, and secondly, whenpretrained with a refined mixture of languages employed by that culture. Wequantify cultural alignment by simulating sociological surveys, comparing modelresponses to those of actual survey participants as references. Specifically,we replicate a survey conducted in various regions of Egypt and the UnitedStates through prompting LLMs with different pretraining data mixtures in bothArabic and English with the personas of the real respondents and the surveyquestions. Further analysis reveals that misalignment becomes more pronouncedfor underrepresented personas and for culturally sensitive topics, such asthose probing social values. Finally, we introduce Anthropological Prompting, anovel method leveraging anthropological reasoning to enhance culturalalignment. Our study emphasizes the necessity for a more balanced multilingualpretraining dataset to better represent the diversity of human experience andthe plurality of different cultures with many implications on the topic ofcross-lingual transfer.</description><author>Badr AlKhamissi, Muhammad ElNokrashy, Mai AlKhamissi, Mona Diab</author><pubDate>Tue, 20 Feb 2024 18:47:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13231v1</guid></item><item><title>Unsupervised Optimisation of GNNs for Node Clustering</title><link>http://arxiv.org/abs/2402.07845v2</link><description>Graph Neural Networks (GNNs) can be trained to detect communities within agraph by learning from the duality of feature and connectivity information.Currently, the common approach for optimisation of GNNs is to use comparisonsto ground-truth for hyperparameter tuning and model selection. In this work, weshow that nodes can be clustered into communities with GNNs by solelyoptimising for modularity, without any comparison to ground-truth. Althoughmodularity is a graph partitioning quality metric, we show that this can beused to optimise GNNs that also encode features without a drop in performance.We take it a step further and also study whether the unsupervised metricperformance can predict ground-truth performance. To investigate why modularitycan be used to optimise GNNs, we design synthetic experiments that show thelimitations of this approach. The synthetic graphs are created to highlightcurrent capabilities in distinct, random and zero information space partitionsin attributed graphs. We conclude that modularity can be used forhyperparameter optimisation and model selection on real-world datasets as wellas being a suitable proxy for predicting ground-truth performance, however,GNNs fail to balance the information duality when the spaces containconflicting signals.</description><author>William Leeney, Ryan McConville</author><pubDate>Tue, 20 Feb 2024 18:46:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07845v2</guid></item><item><title>Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities</title><link>http://arxiv.org/abs/2312.15006v2</link><description>This study critically evaluates the efficacy of prompting methods inenhancing the mathematical reasoning capability of large language models(LLMs). The investigation uses three prescriptive prompting methods - simple,persona, and conversational prompting - known for their effectiveness inenhancing the linguistic tasks of LLMs. We conduct this analysis on OpenAI'sLLM chatbot, ChatGPT-3.5, on extensive problem sets from the MATH, GSM8K, andMMLU datasets, encompassing a broad spectrum of mathematical challenges. Agrading script adapted to each dataset is used to determine the effectivenessof these prompting interventions in enhancing the model's mathematical analysispower. Contrary to expectations, our empirical analysis reveals that none ofthe investigated methods consistently improves over ChatGPT-3.5's baselineperformance, with some causing significant degradation. Our findings suggestthat prompting strategies do not necessarily generalize to new domains, in thisstudy failing to enhance mathematical performance.</description><author>Yuhao Chen, Chloe Wong, Hanwen Yang, Juan Aguenza, Sai Bhujangari, Benthan Vu, Xun Lei, Amisha Prasad, Manny Fluss, Eric Phuong, Minghao Liu, Raja Kumar, Vanshika Vats, James Davis</author><pubDate>Tue, 20 Feb 2024 18:44:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15006v2</guid></item><item><title>Acquiring Weak Annotations for Tumor Localization in Temporal and Volumetric Data</title><link>http://arxiv.org/abs/2310.15098v2</link><description>Creating large-scale and well-annotated datasets to train AI algorithms iscrucial for automated tumor detection and localization. However, with limitedresources, it is challenging to determine the best type of annotations whenannotating massive amounts of unlabeled data. To address this issue, we focuson polyps in colonoscopy videos and pancreatic tumors in abdominal CT scans;both applications require significant effort and time for pixel-wise annotationdue to the high dimensional nature of the data, involving either temporary orspatial dimensions. In this paper, we develop a new annotation strategy, termedDrag&amp;Drop, which simplifies the annotation process to drag and drop. Thisannotation strategy is more efficient, particularly for temporal and volumetricimaging, than other types of weak annotations, such as per-pixel, boundingboxes, scribbles, ellipses, and points. Furthermore, to exploit our Drag&amp;Dropannotations, we develop a novel weakly supervised learning method based on thewatershed algorithm. Experimental results show that our method achieves betterdetection and localization performance than alternative weak annotations and,more importantly, achieves similar performance to that trained on detailedper-pixel annotations. Interestingly, we find that, with limited resources,allocating weak annotations from a diverse patient population can foster modelsmore robust to unseen images than allocating per-pixel annotations for a smallset of images. In summary, this research proposes an efficient annotationstrategy for tumor detection and localization that is less accurate thanper-pixel annotations but useful for creating large-scale datasets forscreening tumors in various medical modalities.</description><author>Yu-Cheng Chou, Bowen Li, Deng-Ping Fan, Alan Yuille, Zongwei Zhou</author><pubDate>Tue, 20 Feb 2024 18:43:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15098v2</guid></item><item><title>Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive</title><link>http://arxiv.org/abs/2402.13228v1</link><description>Direct Preference Optimisation (DPO) is effective at significantly improvingthe performance of large language models (LLMs) on downstream tasks such asreasoning, summarisation, and alignment. Using pairs of preferred anddispreferred data, DPO models the \textit{relative} probability of picking oneresponse over another. In this work, first we show theoretically that thestandard DPO loss can lead to a \textit{reduction} of the model's likelihood ofthe preferred examples, as long as the relative probability between thepreferred and dispreferred classes increases. We then show empirically thatthis phenomenon occurs when fine-tuning LLMs on common datasets, especiallydatasets in which the edit distance between pairs of completions is low. Usingthese insights, we design DPO-Positive (DPOP), a new loss function and trainingprocedure which avoids this failure mode. Surprisingly, we also find that DPOPsignificantly outperforms DPO across a wide variety of datasets and downstreamtasks, including datasets with high edit distances between completions. Byfine-tuning with DPOP, we create and release Smaug-34B and Smaug-72B, whichachieve state-of-the-art open-source performance. Notably, Smaug-72B is nearly2\% better than any other open-source model on the HuggingFace Open LLMLeaderboard and becomes the first open-source LLM to surpass an averageaccuracy of 80\%.</description><author>Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, Colin White</author><pubDate>Tue, 20 Feb 2024 18:42:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13228v1</guid></item><item><title>NeRF Solves Undersampled MRI Reconstruction</title><link>http://arxiv.org/abs/2402.13226v1</link><description>This article presents a novel undersampled magnetic resonance imaging (MRI)technique that leverages the concept of Neural Radiance Field (NeRF). Withradial undersampling, the corresponding imaging problem can be reformulatedinto an image modeling task from sparse-view rendered data; therefore, a highdimensional MR image is obtainable from undersampled $k$-space data by takingadvantage of implicit neural representation. A multi-layer perceptron, which isdesigned to output an image intensity from a spatial coordinate, learns the MRphysics-driven rendering relation between given measurement data and desiredimage. Effective undersampling strategies for high-quality neuralrepresentation are investigated. The proposed method serves two benefits: (i)The learning is based fully on single undersampled $k$-space data, not a bunchof measured data and target image sets. It can be used potentially fordiagnostic MR imaging, such as fetal MRI, where data acquisition is relativelyrare or limited against diversity of clinical images while undersampledreconstruction is highly demanded. (ii) A reconstructed MR image is ascan-specific representation highly adaptive to the given $k$-spacemeasurement. Numerous experiments validate the feasibility and capability ofthe proposed approach.</description><author>Tae Jun Jang, Chang Min Hyun</author><pubDate>Tue, 20 Feb 2024 18:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13226v1</guid></item><item><title>Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective</title><link>http://arxiv.org/abs/2402.10184v3</link><description>There is a trilemma in reinforcement learning from human feedback (RLHF): theincompatibility between highly diverse contexts, low labeling cost, andreliable alignment performance. Here we aim to mitigate such incompatibilitythrough the design of dataset information structures during reward modeling,and meanwhile propose new, generalizable methods of analysis that have widerapplications, including potentially shedding light on goal misgeneralization.Specifically, we first reexamine the RLHF process and propose a theoreticalframework portraying it as an autoencoding process over text distributions. Ourframework formalizes the RLHF objective of ensuring distributional consistencybetween human preference and large language model (LLM) behavior. Based on thisframework, we introduce a new method to model generalization in the rewardmodeling stage of RLHF, the induced Bayesian network (IBN). Drawing from randomgraph theory and causal analysis, it enables empirically grounded derivation ofgeneralization error bounds, a key improvement over classical methods ofgeneralization analysis. An insight from our analysis is the superiority of thetree-based information structure in reward modeling, compared to chain-basedbaselines in conventional RLHF methods. We derive that in complex contexts withlimited data, the tree-based reward model (RM) induces up to $\Theta(\logn/\log\log n)$ times less variance than chain-based RM where $n$ is the datasetsize. As validation, we demonstrate that on three NLP tasks, the tree-based RMachieves 65% win rate on average against chain-based baselines. Looking ahead,we hope to extend the IBN analysis to help understand the phenomenon of goalmisgeneralization.</description><author>Tianyi Qiu, Fanzhi Zeng, Jiaming Ji, Dong Yan, Kaile Wang, Jiayi Zhou, Han Yang, Josef Dai, Xuehai Pan, Yaodong Yang</author><pubDate>Tue, 20 Feb 2024 18:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10184v3</guid></item><item><title>AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning</title><link>http://arxiv.org/abs/2402.13225v1</link><description>Clinical calculators play a vital role in healthcare by offering accurateevidence-based predictions for various purposes such as prognosis.Nevertheless, their widespread utilization is frequently hindered by usabilitychallenges, poor dissemination, and restricted functionality. Augmenting largelanguage models with extensive collections of clinical calculators presents anopportunity to overcome these obstacles and improve workflow efficiency, butthe scalability of the manual curation process poses a significant challenge.In response, we introduce AgentMD, a novel language agent capable of curatingand applying clinical calculators across various clinical contexts. Using thepublished literature, AgentMD has automatically curated a collection of 2,164diverse clinical calculators with executable functions and structureddocumentation, collectively named RiskCalcs. Manual evaluations show thatRiskCalcs tools achieve an accuracy of over 80% on three quality metrics. Atinference time, AgentMD can automatically select and apply the relevantRiskCalcs tools given any patient description. On the newly established RiskQAbenchmark, AgentMD significantly outperforms chain-of-thought prompting withGPT-4 (87.7% vs. 40.9% in accuracy). Additionally, we also applied AgentMD toreal-world clinical notes for analyzing both population-level and risk-levelpatient characteristics. In summary, our study illustrates the utility oflanguage agents augmented with clinical calculators for healthcare analyticsand patient care.</description><author>Qiao Jin, Zhizheng Wang, Yifan Yang, Qingqing Zhu, Donald Wright, Thomas Huang, W John Wilbur, Zhe He, Andrew Taylor, Qingyu Chen, Zhiyong Lu</author><pubDate>Tue, 20 Feb 2024 18:37:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13225v1</guid></item><item><title>Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming</title><link>http://arxiv.org/abs/2402.13224v1</link><description>This paper introduces an Electric Vehicle Charging Station (EVCS) model thatincorporates real-world constraints, such as slot power limitations, contractthreshold overruns penalties, or early disconnections of electric vehicles(EVs). We propose a formulation of the problem of EVCS control underuncertainty, and implement two Multi-Stage Stochastic Programming approachesthat leverage user-provided information, namely, Model Predictive Control andTwo-Stage Stochastic Programming. The model addresses uncertainties in chargingsession start and end times, as well as in energy demand. A user's behaviormodel based on a sojourn-time-dependent stochastic process enhances costreduction while maintaining customer satisfaction. The benefits of the twoproposed methods are showcased against two baselines over a 22-day simulationusing a real-world dataset. The two-stage approach proves robust against earlydisconnections, considering a more significant number of uncertainty scenariosfor optimization. The algorithm prioritizing user satisfaction over electricitycost achieves a 20% and 36% improvement in two user satisfaction metricscompared to an industry-standard baseline. Additionally, the algorithm strikingthe best balance between cost and user satisfaction exhibits a mere 3% relativecost increase compared to the theoretically optimal baseline - for which thenonanticipativity constraint is relaxed - while attaining 94% and 84% of theuser satisfaction performance in the two used satisfaction metrics.</description><author>Alban Puech, Tristan Rigaut, William Templier, Maud Tournoud</author><pubDate>Tue, 20 Feb 2024 18:37:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13224v1</guid></item><item><title>RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian</title><link>http://arxiv.org/abs/2402.13222v1</link><description>Recently, large language models (LLMs) have become increasingly powerful andhave become capable of solving a plethora of tasks through proper instructionsin natural language. However, the vast majority of testing suites assume thatthe instructions are written in English, the de facto prompting language. Codeintelligence and problem solving still remain a difficult task, even for themost advanced LLMs. Currently, there are no datasets to measure thegeneralization power for code-generation models in a language other thanEnglish. In this work, we present RoCode, a competitive programming dataset,consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ andPython and comprehensive testing suites for each problem. The purpose of RoCodeis to provide a benchmark for evaluating the code intelligence of languagemodels trained on Romanian / multilingual text as well as a fine-tuning set forpretrained Romanian models. Through our results and review of related works, weargue for the need to develop code models for languages other than English.</description><author>Adrian Cosma, Bogdan Iordache, Paolo Rosso</author><pubDate>Tue, 20 Feb 2024 18:32:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13222v1</guid></item><item><title>CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning</title><link>http://arxiv.org/abs/2402.13221v1</link><description>Advances in graph machine learning (ML) have been driven by applications inchemistry as graphs have remained the most expressive representations ofmolecules. While early graph ML methods focused primarily on small organicmolecules, recently, the scope of graph ML has expanded to include inorganicmaterials. Modelling the periodicity and symmetry of inorganic crystallinematerials poses unique challenges, which existing graph ML methods are unableto address. Moving to inorganic nanomaterials increases complexity as the scaleof number of nodes within each graph can be broad ($10$ to $10^5$). The bulk ofexisting graph ML focuses on characterising molecules and materials bypredicting target properties with graphs as input. However, the most excitingapplications of graph ML will be in their generative capabilities, which iscurrently not at par with other domains such as images or text. We invite the graph ML community to address these open challenges bypresenting two new chemically-informed large-scale inorganic (CHILI)nanomaterials datasets: A medium-scale dataset (with overall &gt;6M nodes, &gt;49Medges) of mono-metallic oxide nanomaterials generated from 12 selected crystaltypes (CHILI-3K) and a large-scale dataset (with overall &gt;183M nodes, &gt;1.2Bedges) of nanomaterials generated from experimentally determined crystalstructures (CHILI-100K). We define 11 property prediction tasks and 6 structureprediction tasks, which are of special interest for nanomaterial research. Webenchmark the performance of a wide array of baseline methods and use thesebenchmarking results to highlight areas which need future work. To the best ofour knowledge, CHILI-3K and CHILI-100K are the first open-source nanomaterialdatasets of this scale -- both on the individual graph level and of the datasetas a whole -- and the only nanomaterials datasets with high structural andelemental diversity.</description><author>Ulrik Friis-Jensen, Frederik L. Johansen, Andy S. Anker, Erik B. Dam, Kirsten M. √ò. Jensen, Raghavendra Selvan</author><pubDate>Tue, 20 Feb 2024 18:32:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13221v1</guid></item><item><title>Analyzing Operator States and the Impact of AI-Enhanced Decision Support in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning Framework for Intervention Strategies</title><link>http://arxiv.org/abs/2402.13219v1</link><description>In complex industrial and chemical process control rooms, effectivedecision-making is crucial for safety and effi- ciency. The experiments in thispaper evaluate the impact and applications of an AI-based decision supportsystem integrated into an improved human-machine interface, using dynamicinflu- ence diagrams, a hidden Markov model, and deep reinforcement learning.The enhanced support system aims to reduce operator workload, improvesituational awareness, and provide different intervention strategies to theoperator adapted to the current state of both the system and human performance.Such a system can be particularly useful in cases of information overload whenmany alarms and inputs are presented all within the same time window, or forjunior operators during training. A comprehensive cross-data analysis wasconducted, involving 47 participants and a diverse range of data sources suchas smartwatch metrics, eye- tracking data, process logs, and responses fromquestionnaires. The results indicate interesting insights regarding the effec-tiveness of the approach in aiding decision-making, decreasing perceivedworkload, and increasing situational awareness for the scenarios considered.Additionally, the results provide valuable insights to compare differencesbetween styles of information gathering when using the system by individualparticipants. These findings are particularly relevant when predicting theoverall performance of the individual participant and their capacity tosuccessfully handle a plant upset and the alarms connected to it using processand human-machine interaction logs in real-time. These predictions enable thedevelopment of more effective intervention strategies.</description><author>Ammar N. Abbas, Chidera W. Amazu, Joseph Mietkiewicz, Houda Briwa, Andres Alonzo Perez, Gabriele Baldissone, Micaela Demichela, Georgios G. Chasparis, John D. Kelleher, Maria Chiara Leva</author><pubDate>Tue, 20 Feb 2024 18:31:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13219v1</guid></item><item><title>How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts</title><link>http://arxiv.org/abs/2402.13220v1</link><description>The remarkable advancements in Multimodal Large Language Models (MLLMs) havenot rendered them immune to challenges, particularly in the context of handlingdeceptive information in prompts, thus producing hallucinated responses undersuch conditions. To quantitatively assess this vulnerability, we presentMAD-Bench, a carefully curated benchmark that contains 850 test samples dividedinto 6 categories, such as non-existent objects, count of objects, spatialrelationship, and visual confusion. We provide a comprehensive analysis ofpopular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such asLLaVA-1.5 and CogVLM. Empirically, we observe significant performance gapsbetween GPT-4V and other models; and previous robust instruction-tuned models,such as LRV-Instruction and LLaVA-RLHF, are not effective on this newbenchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy ofany other model in our experiments ranges from 5% to 35%. We further propose aremedy that adds an additional paragraph to the deceptive prompts to encouragemodels to think twice before answering the question. Surprisingly, this simplemethod can even double the accuracy; however, the absolute numbers are stilltoo low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmarkto stimulate further research to enhance models' resilience against deceptiveprompts.</description><author>Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan</author><pubDate>Tue, 20 Feb 2024 18:31:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13220v1</guid></item><item><title>VideoPrism: A Foundational Visual Encoder for Video Understanding</title><link>http://arxiv.org/abs/2402.13217v1</link><description>We introduce VideoPrism, a general-purpose video encoder that tackles diversevideo understanding tasks with a single frozen model. We pretrain VideoPrism ona heterogeneous corpus containing 36M high-quality video-caption pairs and 582Mvideo clips with noisy parallel text (e.g., ASR transcripts). The pretrainingapproach improves upon masked autoencoding by global-local distillation ofsemantic video embeddings and a token shuffling scheme, enabling VideoPrism tofocus primarily on the video modality while leveraging the invaluable textassociated with videos. We extensively test VideoPrism on four broad groups ofvideo understanding tasks, from web video question answering to CV for science,achieving state-of-the-art performance on 30 out of 33 video understandingbenchmarks.</description><author>Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, Boqing Gong</author><pubDate>Tue, 20 Feb 2024 18:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13217v1</guid></item><item><title>Privacy Issues in Large Language Models: A Survey</title><link>http://arxiv.org/abs/2312.06717v3</link><description>This is the first survey of the active area of AI research that focuses onprivacy issues in Large Language Models (LLMs). Specifically, we focus on workthat red-teams models to highlight privacy risks, attempts to build privacyinto the training or inference process, enables efficient data deletion fromtrained models to comply with existing privacy regulations, and tries tomitigate copyright issues. Our focus is on summarizing technical research thatdevelops algorithms, proves theorems, and runs empirical evaluations. Whilethere is an extensive body of legal and policy work addressing these challengesfrom a different angle, that is not the focus of our survey. Nevertheless,these works, along with recent legal developments do inform how these technicalproblems are formalized, and so we discuss them briefly in Section 1. While wehave made our best effort to include all the relevant work, due to the fastmoving nature of this research we may have missed some recent work. If we havemissed some of your work please contact us, as we will attempt to keep thissurvey relatively up to date. We are maintaining a repository with the list ofpapers covered in this survey and any relevant code that was publicly availableat https://github.com/safr-ml-lab/survey-llm.</description><author>Seth Neel, Peter Chang</author><pubDate>Tue, 20 Feb 2024 18:26:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06717v3</guid></item><item><title>GraphKD: Exploring Knowledge Distillation Towards Document Object Detection with Structured Graph Creation</title><link>http://arxiv.org/abs/2402.11401v2</link><description>Object detection in documents is a key step to automate the structuralelements identification process in a digital or scanned document throughunderstanding the hierarchical structure and relationships between differentelements. Large and complex models, while achieving high accuracy, can becomputationally expensive and memory-intensive, making them impractical fordeployment on resource constrained devices. Knowledge distillation allows us tocreate small and more efficient models that retain much of the performance oftheir larger counterparts. Here we present a graph-based knowledge distillationframework to correctly identify and localize the document objects in a documentimage. Here, we design a structured graph with nodes containing proposal-levelfeatures and edges representing the relationship between the different proposalregions. Also, to reduce text bias an adaptive node sampling strategy isdesigned to prune the weight distribution and put more weightage on non-textnodes. We encode the complete graph as a knowledge representation and transferit from the teacher to the student through the proposed distillation loss byeffectively capturing both local and global information concurrently. Extensiveexperimentation on competitive benchmarks demonstrates that the proposedframework outperforms the current state-of-the-art approaches. The code will beavailable at: https://github.com/ayanban011/GraphKD.</description><author>Ayan Banerjee, Sanket Biswas, Josep Llad√≥s, Umapada Pal</author><pubDate>Tue, 20 Feb 2024 18:25:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11401v2</guid></item><item><title>Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&amp;A</title><link>http://arxiv.org/abs/2402.13213v1</link><description>Although large language models (LLMs) perform impressively on many tasks,overconfidence remains a problem. We hypothesized that on multiple-choice Q&amp;Atasks, wrong answers would be associated with smaller maximum softmaxprobabilities (MSPs) compared to correct answers. We comprehensively evaluatethis hypothesis on ten open-source LLMs and five datasets, and find strongevidence for our hypothesis among models which perform well on the original Q&amp;Atask. For the six LLMs with the best Q&amp;A performance, the AUROC derived fromthe MSP was better than random chance with p &lt; 10^{-4} in 59/60 instances.Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveragingthese findings, we propose a multiple-choice Q&amp;A task with an option to abstainand show that performance can be improved by selectively abstaining based onthe MSP of the initial model response. We also run the same experiments withpre-softmax logits instead of softmax probabilities and find similar (but notidentical) results.</description><author>Benjamin Plaut, Khanh Nguyen, Tu Trinh</author><pubDate>Tue, 20 Feb 2024 18:24:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13213v1</guid></item><item><title>Soft Self-Consistency Improves Language Model Agents</title><link>http://arxiv.org/abs/2402.13212v1</link><description>Generations from large language models (LLMs) can be improved by sampling andscoring multiple solutions to select a final answer. Current "sample andselect" methods such as self-consistency (SC) rely on majority voting to scoreanswers. However, when tasks have many distinct and valid answers, selection byvoting requires a large number of samples. This makes SC prohibitivelyexpensive for interactive tasks that involve generating multiple actions(answers) sequentially. After establishing that majority voting fails toprovide consistent gains on such tasks, we demonstrate how to increase successrates by softening the scoring criterion. We introduce Soft Self-Consistency(Soft-SC), which replaces SC's discontinuous scoring with a continuous scorecomputed from model likelihoods, allowing for selection even when actions aresparsely distributed. Soft-SC improves both performance and efficiency onlong-horizon interactive tasks, requiring half as many samples as SC forcomparable or better performance. For a fixed number of samples, Soft-SC leadsto a 1.3% increase over SC in absolute success rate on writing bash programs, a6.6% increase on online shopping (WebShop), and a 4.7% increase for aninteractive household game (ALFWorld). Finally, we show that Soft-SC can beapplied to both open-source and black-box models.</description><author>Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal</author><pubDate>Tue, 20 Feb 2024 18:22:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13212v1</guid></item><item><title>Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation</title><link>http://arxiv.org/abs/2402.13211v1</link><description>Emotional Support Conversation (ESC) is a task aimed at alleviatingindividuals' emotional distress through daily conversation. Given its inherentcomplexity and non-intuitive nature, ESConv dataset incorporates supportstrategies to facilitate the generation of appropriate responses. Recently,despite the remarkable conversational ability of large language models (LLMs),previous studies have suggested that they often struggle with providing usefulemotional support. Hence, this work initially analyzes the results of LLMs onESConv, revealing challenges in selecting the correct strategy and a notablepreference for a specific strategy. Motivated by these, we explore the impactof the inherent preference in LLMs on providing emotional support, andconsequently, we observe that exhibiting high preference for specificstrategies hinders effective emotional support, aggravating its robustness inpredicting the appropriate strategy. Moreover, we conduct a methodologicalstudy to offer insights into the necessary approaches for LLMs to serve asproficient emotional supporters. Our findings emphasize that (1) low preferencefor specific strategies hinders the progress of emotional support, (2) externalassistance helps reduce preference bias, and (3) LLMs alone cannot become goodemotional supporters. These insights suggest promising avenues for futureresearch to enhance the emotional intelligence of LLMs.</description><author>Dongjin Kang, Sunghwan Kim, Taeyoon Kwon, Seungjun Moon, Hyunsouk Cho, Youngjae Yu, Dongha Lee, Jinyoung Yeo</author><pubDate>Tue, 20 Feb 2024 18:21:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13211v1</guid></item><item><title>Bayesian Reward Models for LLM Alignment</title><link>http://arxiv.org/abs/2402.13210v1</link><description>To ensure that large language model (LLM) responses are helpful andnon-toxic, we usually fine-tune a reward model on human preference data. Wethen select policy responses with high rewards (best-of-n sampling) or furtheroptimize the policy to produce responses with high rewards (reinforcementlearning from human feedback). However, this process is vulnerable to rewardoveroptimization or hacking, in which the responses selected have high rewardsdue to errors in the reward model rather than a genuine preference. This isespecially problematic as the prompt or response diverges from the trainingdata. It should be possible to mitigate these issues by training a Bayesianreward model, which signals higher uncertainty further from the training datadistribution. Therefore, we trained Bayesian reward models using Laplace-LoRA(Yang et al., 2024) and found that the resulting uncertainty estimates cansuccessfully mitigate reward overoptimization in best-of-n sampling.</description><author>Adam X. Yang, Maxime Robeyns, Thomas Coste, Jun Wang, Haitham Bou-Ammar, Laurence Aitchison</author><pubDate>Tue, 20 Feb 2024 18:20:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13210v1</guid></item><item><title>How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena</title><link>http://arxiv.org/abs/2402.13208v1</link><description>The attention mechanism, a cornerstone of state-of-the-art neural models,faces computational hurdles in processing long sequences due to its quadraticcomplexity. Consequently, research efforts in the last few years focused onfinding more efficient alternatives. Among them, Hyena (Poli et al., 2023)stands out for achieving competitive results in both language modeling andimage classification, while offering sub-quadratic memory and computationalcomplexity. Building on these promising results, we propose ConfHyena, aConformer whose encoder self-attentions are replaced with an adaptation ofHyena for speech processing, where the long input sequences cause highcomputational costs. Through experiments in automatic speech recognition (forEnglish) and translation (from English into 8 target languages), we show thatour best ConfHyena model significantly reduces the training time by 27%, at thecost of minimal quality degradation (~1%), which, in most cases, is notstatistically significant.</description><author>Marco Gaido, Sara Papi, Matteo Negri, Luisa Bentivogli</author><pubDate>Tue, 20 Feb 2024 18:19:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13208v1</guid></item><item><title>Touring sampling with pushforward maps</title><link>http://arxiv.org/abs/2311.13845v2</link><description>The number of sampling methods could be daunting for a practitioner lookingto cast powerful machine learning methods to their specific problem. This papertakes a theoretical stance to review and organize many sampling approaches inthe ``generative modeling'' setting, where one wants to generate new data thatare similar to some training examples. By revealing links between existingmethods, it might prove useful to overcome some of the current challenges insampling with diffusion models, such as long inference time due to diffusionsimulation, or the lack of diversity in generated samples.</description><author>Vivien Cabannes, Charles Arnal</author><pubDate>Tue, 20 Feb 2024 18:17:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13845v2</guid></item><item><title>SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural Architecture Search</title><link>http://arxiv.org/abs/2402.13204v1</link><description>Recent advancements in Artificial Intelligence (AI), driven by NeuralNetworks (NN), demand innovative neural architecture designs, particularlywithin the constrained environments of Internet of Things (IoT) systems, tobalance performance and efficiency. HW-aware Neural Architecture Search(HW-aware NAS) emerges as an attractive strategy to automate the design of NNusing multi-objective optimization approaches, such as evolutionary algorithms.However, the intricate relationship between NN design parameters and HW-awareNAS optimization objectives remains an underexplored research area, overlookingopportunities to effectively leverage this knowledge to guide the searchprocess accordingly. Furthermore, the large amount of evaluation data producedduring the search holds untapped potential for refining the optimizationstrategy and improving the approximation of the Pareto front. Addressing theseissues, we propose SONATA, a self-adaptive evolutionary algorithm for HW-awareNAS. Our method leverages adaptive evolutionary operators guided by the learnedimportance of NN design parameters. Specifically, through tree-based surrogatemodels and a Reinforcement Learning agent, we aspire to gather knowledge on'How' and 'When' to evolve NN architectures. Comprehensive evaluations acrossvarious NAS search spaces and hardware devices on the ImageNet-1k dataset haveshown the merit of SONATA with up to 0.25% improvement in accuracy and up to2.42x gains in latency and energy. Our SONATA has seen up to sim$93.6% Paretodominance over the native NSGA-II, further stipulating the importance ofself-adaptive evolution operators in HW-aware NAS.</description><author>Halima Bouzidi, Smail Niar, Hamza Ouarnoughi, El-Ghazali Talbi</author><pubDate>Tue, 20 Feb 2024 18:15:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13204v1</guid></item><item><title>Tiny Reinforcement Learning for Quadruped Locomotion using Decision Transformers</title><link>http://arxiv.org/abs/2402.13201v1</link><description>Resource-constrained robotic platforms are particularly useful for tasks thatrequire low-cost hardware alternatives due to the risk of losing the robot,like in search-and-rescue applications, or the need for a large number ofdevices, like in swarm robotics. For this reason, it is crucial to findmechanisms for adapting reinforcement learning techniques to the constraintsimposed by lower computational power and smaller memory capacities of theseultra low-cost robotic platforms. We try to address this need by proposing amethod for making imitation learning deployable onto resource-constrainedrobotic platforms. Here we cast the imitation learning problem as a conditionalsequence modeling task and we train a decision transformer using expertdemonstrations augmented with a custom reward. Then, we compress the resultinggenerative model using software optimization schemes, including quantizationand pruning. We test our method in simulation using Isaac Gym, a realisticphysics simulation environment designed for reinforcement learning. Weempirically demonstrate that our method achieves natural looking gaits forBittle, a resource-constrained quadruped robot. We also run multiplesimulations to show the effects of pruning and quantization on the performanceof the model. Our results show that quantization (down to 4 bits) and pruningreduce model size by around 30\% while maintaining a competitive reward, makingthe model deployable in a resource-constrained system.</description><author>Orhan Eren Akg√ºn, N√©stor Cuevas, Matheus Farias, Daniel Garces</author><pubDate>Tue, 20 Feb 2024 18:10:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13201v1</guid></item><item><title>Practical Kernel Tests of Conditional Independence</title><link>http://arxiv.org/abs/2402.13196v1</link><description>We describe a data-efficient, kernel-based approach to statistical testing ofconditional independence. A major challenge of conditional independencetesting, absent in tests of unconditional independence, is to obtain thecorrect test level (the specified upper bound on the rate of false positives),while still attaining competitive test power. Excess false positives arise dueto bias in the test statistic, which is obtained using nonparametric kernelridge regression. We propose three methods for bias control to correct the testlevel, based on data splitting, auxiliary data, and (where possible) simplerfunction classes. We show these combined strategies are effective both forsynthetic and real-world data.</description><author>Roman Pogodin, Antonin Schrab, Yazhe Li, Danica J. Sutherland, Arthur Gretton</author><pubDate>Tue, 20 Feb 2024 18:07:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13196v1</guid></item><item><title>Design and Flight Demonstration of a Quadrotor for Urban Mapping and Target Tracking Research</title><link>http://arxiv.org/abs/2402.13195v1</link><description>This paper describes the hardware design and flight demonstration of a smallquadrotor with imaging sensors for urban mapping, hazard avoidance, and targettracking research. The vehicle is equipped with five cameras, including twopairs of fisheye stereo cameras that enable a nearly omnidirectional view and atwo-axis gimbaled camera. An onboard NVIDIA Jetson Orin Nano computer runningthe Robot Operating System software is used for data collection. An autonomoustracking behavior was implemented to coordinate the motion of the quadrotor andgimbaled camera to track a moving GPS coordinate. The data collection systemwas demonstrated through a flight test that tracked a moving GPS-tagged vehiclethrough a series of roads and parking lots. A map of the environment wasreconstructed from the collected images using the Direct Sparse Odometry (DSO)algorithm. The performance of the quadrotor was also characterized by acousticnoise, communication range, battery voltage in hover, and maximum speed tests.</description><author>Collin Hague, Nick Kakavitsas, Jincheng Zhang, Chris Beam, Andrew Willis, Artur Wolek</author><pubDate>Tue, 20 Feb 2024 18:06:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13195v1</guid></item><item><title>Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice</title><link>http://arxiv.org/abs/2402.10870v2</link><description>Adaptive experimental design (AED) methods are increasingly being used inindustry as a tool to boost testing throughput or reduce experimentation costrelative to traditional A/B/N testing methods. However, the behavior andguarantees of such methods are not well-understood beyond idealized stationarysettings. This paper shares lessons learned regarding the challenges of naivelyusing AED systems in industrial settings where non-stationarity is prevalent,while also providing perspectives on the proper objectives and systemspecifications in such settings. We developed an AED framework forcounterfactual inference based on these experiences, and tested it in acommercial environment.</description><author>Tanner Fiez, Houssam Nassif, Arick Chen, Sergio Gamez, Lalit Jain</author><pubDate>Tue, 20 Feb 2024 18:04:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10870v2</guid></item><item><title>Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model</title><link>http://arxiv.org/abs/2402.10204v2</link><description>Reconstructing sky models from dirty radio images for accurate sourcelocalization and flux estimation is crucial for studying galaxy evolution athigh redshift, especially in deep fields using instruments like the AtacamaLarge Millimetre Array (ALMA). With new projects like the Square KilometreArray (SKA), there's a growing need for better source extraction methods.Current techniques, such as CLEAN and PyBDSF, often fail to detect faintsources, highlighting the need for more accurate methods. This study proposesusing stochastic neural networks to rebuild sky models directly from dirtyimages. This method can pinpoint radio sources and measure their fluxes withrelated uncertainties, marking a potential improvement in radio sourcecharacterization. We tested this approach on 10164 images simulated with theCASA tool simalma, based on ALMA's Cycle 5.3 antenna setup. We appliedconditional Denoising Diffusion Probabilistic Models (DDPMs) for sky modelsreconstruction, then used Photutils to determine source coordinates and fluxes,assessing the model's performance across different water vapor levels. Ourmethod showed excellence in source localization, achieving more than 90%completeness at a signal-to-noise ratio (SNR) as low as 2. It also surpassedPyBDSF in flux estimation, accurately identifying fluxes for 96% of sources inthe test set, a significant improvement over CLEAN+ PyBDSF's 57%. ConditionalDDPMs is a powerful tool for image-to-image translation, yielding accurate androbust characterisation of radio sources, and outperforming existingmethodologies. While this study underscores its significant potential forapplications in radio astronomy, we also acknowledge certain limitations thataccompany its usage, suggesting directions for further refinement and research.</description><author>Mariia Drozdova, Vitaliy Kinakh, Omkar Bait, Olga Taran, Erica Lastufka, Miroslava Dessauges-Zavadsky, Taras Holotyak, Daniel Schaerer, Slava Voloshynovskiy</author><pubDate>Tue, 20 Feb 2024 18:00:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10204v2</guid></item><item><title>Question Calibration and Multi-Hop Modeling for Temporal Question Answering</title><link>http://arxiv.org/abs/2402.13188v1</link><description>Many models that leverage knowledge graphs (KGs) have recently demonstratedremarkable success in question answering (QA) tasks. In the real world, manyfacts contained in KGs are time-constrained thus temporal KGQA has receivedincreasing attention. Despite the fruitful efforts of previous models intemporal KGQA, they still have several limitations. (I) They adopt pre-trainedlanguage models (PLMs) to obtain question representations, while PLMs tend tofocus on entity information and ignore entity transfer caused by temporalconstraints, and finally fail to learn specific temporal representations ofentities. (II) They neither emphasize the graph structure between entities norexplicitly model the multi-hop relationship in the graph, which will make itdifficult to solve complex multi-hop question answering. To alleviate thisproblem, we propose a novel Question Calibration and Multi-Hop Modeling(QC-MHM) approach. Specifically, We first calibrate the question representationby fusing the question and the time-constrained concepts in KG. Then, weconstruct the GNN layer to complete multi-hop message passing. Finally, thequestion representation is combined with the embedding output by the GNN togenerate the final prediction. Empirical results verify that the proposed modelachieves better performance than the state-of-the-art models in the benchmarkdataset. Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestionsdataset's complex questions are absolutely improved by 5.1% and 1.2% comparedto the best-performing baseline. Moreover, QC-MHM can generate interpretableand trustworthy predictions.</description><author>Chao Xue, Di Liang, Pengfei Wang, Jing Zhang</author><pubDate>Tue, 20 Feb 2024 17:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13188v1</guid></item><item><title>Simplicial Convolutional Filters</title><link>http://arxiv.org/abs/2201.11720v3</link><description>We study linear filters for processing signals supported on abstracttopological spaces modeled as simplicial complexes, which may be interpreted asgeneralizations of graphs that account for nodes, edges, triangular faces etc.To process such signals, we develop simplicial convolutional filters defined asmatrix polynomials of the lower and upper Hodge Laplacians. First, we study theproperties of these filters and show that they are linear and shift-invariant,as well as permutation and orientation equivariant. These filters can also beimplemented in a distributed fashion with a low computational complexity, asthey involve only (multiple rounds of) simplicial shifting between upper andlower adjacent simplices. Second, focusing on edge-flows, we study thefrequency responses of these filters and examine how we can use theHodge-decomposition to delineate gradient, curl and harmonic frequencies. Wediscuss how these frequencies correspond to the lower- and the upper-adjacentcouplings and the kernel of the Hodge Laplacian, respectively, and can be tunedindependently by our filter designs. Third, we study different procedures fordesigning simplicial convolutional filters and discuss their relativeadvantages. Finally, we corroborate our simplicial filters in severalapplications: to extract different frequency components of a simplicial signal,to denoise edge flows, and to analyze financial markets and traffic networks.</description><author>Maosheng Yang, Elvin Isufi, Michael T. Schaub, Geert Leus</author><pubDate>Tue, 20 Feb 2024 17:53:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.11720v3</guid></item><item><title>Testing Calibration in Subquadratic Time</title><link>http://arxiv.org/abs/2402.13187v1</link><description>In the recent literature on machine learning and decision making, calibrationhas emerged as a desirable and widely-studied statistical property of theoutputs of binary prediction models. However, the algorithmic aspects ofmeasuring model calibration have remained relatively less well-explored.Motivated by [BGHN23], which proposed a rigorous framework for measuringdistances to calibration, we initiate the algorithmic study of calibrationthrough the lens of property testing. We define the problem of calibrationtesting from samples where given $n$ draws from a distribution $\mathcal{D}$ on(predictions, binary outcomes), our goal is to distinguish between the casewhere $\mathcal{D}$ is perfectly calibrated, and the case where $\mathcal{D}$is $\varepsilon$-far from calibration. We design an algorithm based on approximate linear programming, which solvescalibration testing information-theoretically optimally (up to constantfactors) in time $O(n^{1.5} \log(n))$. This improves upon state-of-the-artblack-box linear program solvers requiring $\Omega(n^\omega)$ time, where$\omega &gt; 2$ is the exponent of matrix multiplication. We also developalgorithms for tolerant variants of our testing problem, and give samplecomplexity lower bounds for alternative calibration distances to the oneconsidered in this work. Finally, we present preliminary experiments showingthat the testing problem we define faithfully captures standard notions ofcalibration, and that our algorithms scale to accommodate moderate samplesizes.</description><author>Lunjia Hu, Kevin Tian, Chutong Yang</author><pubDate>Tue, 20 Feb 2024 17:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13187v1</guid></item><item><title>UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing</title><link>http://arxiv.org/abs/2402.13185v1</link><description>Recent advances in text-guided video editing have showcased promising resultsin appearance editing (e.g., stylization). However, video motion editing in thetemporal dimension (e.g., from eating to waving), which distinguishes videoediting from image editing, is underexplored. In this work, we present UniEdit,a tuning-free framework that supports both video motion and appearance editingby harnessing the power of a pre-trained text-to-video generator within aninversion-then-generation framework. To realize motion editing while preservingsource video content, based on the insights that temporal and spatialself-attention layers encode inter-frame and intra-frame dependencyrespectively, we introduce auxiliary motion-reference and reconstructionbranches to produce text-guided motion and source features respectively. Theobtained features are then injected into the main editing path via temporal andspatial self-attention layers. Extensive experiments demonstrate that UniEditcovers video motion editing and various appearance editing scenarios, andsurpasses the state-of-the-art methods. Our code will be publicly available.</description><author>Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, Jiang Bian</author><pubDate>Tue, 20 Feb 2024 17:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13185v1</guid></item><item><title>What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents</title><link>http://arxiv.org/abs/2402.13184v1</link><description>In this study, we introduce "CosmoAgent," an innovative artificialintelligence framework utilizing Large Language Models (LLMs) to simulatecomplex interactions between human and extraterrestrial civilizations, with aspecial emphasis on Stephen Hawking's cautionary advice about not sending radiosignals haphazardly into the universe. The goal is to assess the feasibility ofpeaceful coexistence while considering potential risks that could threatenwell-intentioned civilizations. Employing mathematical models and statetransition matrices, our approach quantitatively evaluates the developmenttrajectories of civilizations, offering insights into future decision-making atcritical points of growth and saturation. Furthermore, the paper acknowledgesthe vast diversity in potential living conditions across the universe, whichcould foster unique cosmologies, ethical codes, and worldviews among variouscivilizations. Recognizing the Earth-centric bias inherent in current LLMdesigns, we propose the novel concept of using LLMs with diverse ethicalparadigms and simulating interactions between entities with distinct moralprinciples. This innovative research provides a new way to understand complexinter-civilizational dynamics, expanding our perspective while pioneering novelstrategies for conflict resolution, crucial for preventing interstellarconflicts. We have also released the code and datasets to enable furtheracademic investigation into this interesting area of research. The code isavailable at https://github.com/agiresearch/AlienAgent.</description><author>Mingyu Jin, Beichen Wang, Zhaoqian Xue, Suiyuan Zhu, Wenyue Hua, Hua Tang, Kai Mei, Mengnan Du, Yongfeng Zhang</author><pubDate>Tue, 20 Feb 2024 17:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13184v1</guid></item><item><title>Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness</title><link>http://arxiv.org/abs/2402.13182v1</link><description>We consider distributed kernel bandits where $N$ agents aim tocollaboratively maximize an unknown reward function that lies in a reproducingkernel Hilbert space. Each agent sequentially queries the function to obtainnoisy observations at the query points. Agents can share information through acentral server, with the objective of minimizing regret that is accumulatingover time $T$ and aggregating over agents. We develop the first algorithm thatachieves the optimal regret order (as defined by centralized learning) with acommunication cost that is sublinear in both $N$ and $T$. The key features ofthe proposed algorithm are the uniform exploration at the local agents andshared randomness with the central server. Working together with the sparseapproximation of the GP model, these two key components make it possible topreserve the learning rate of the centralized setting at a diminishing rate ofcommunication.</description><author>Nikola Pavlovic, Sudeep Salgia, Qing Zhao</author><pubDate>Tue, 20 Feb 2024 17:49:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13182v1</guid></item><item><title>DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models</title><link>http://arxiv.org/abs/2402.13181v1</link><description>We propose DINOBot, a novel imitation learning framework for robotmanipulation, which leverages the image-level and pixel-level capabilities offeatures extracted from Vision Transformers trained with DINO. When interactingwith a novel object, DINOBot first uses these features to retrieve the mostvisually similar object experienced during human demonstrations, and then usesthis object to align its end-effector with the novel object to enable effectiveinteraction. Through a series of real-world experiments on everyday tasks, weshow that exploiting both the image-level and pixel-level properties of visionfoundation models enables unprecedented learning efficiency and generalisation.Videos and code are available at https://www.robot-learning.uk/dinobot.</description><author>Norman Di Palo, Edward Johns</author><pubDate>Tue, 20 Feb 2024 17:48:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13181v1</guid></item><item><title>Benchmarking Retrieval-Augmented Generation for Medicine</title><link>http://arxiv.org/abs/2402.13178v1</link><description>While large language models (LLMs) have achieved state-of-the-art performanceon a wide range of medical question answering (QA) tasks, they still facechallenges with hallucinations and outdated knowledge. Retrieval-augmentedgeneration (RAG) is a promising solution and has been widely adopted. However,a RAG system can involve multiple flexible components, and there is a lack ofbest practices regarding the optimal RAG setting for various medical purposes.To systematically evaluate such systems, we propose the Medical InformationRetrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kindbenchmark including 7,663 questions from five medical QA datasets. UsingMIRAGE, we conducted large-scale experiments with over 1.8 trillion prompttokens on 41 combinations of different corpora, retrievers, and backbone LLMsthrough the MedRAG toolkit introduced in this work. Overall, MedRAG improvesthe accuracy of six different LLMs by up to 18% over chain-of-thoughtprompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Ourresults show that the combination of various medical corpora and retrieversachieves the best performance. In addition, we discovered a log-linear scalingproperty and the "lost-in-the-middle" effects in medical RAG. We believe ourcomprehensive evaluations can serve as practical guidelines for implementingRAG systems for medicine.</description><author>Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang</author><pubDate>Tue, 20 Feb 2024 17:44:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13178v1</guid></item><item><title>Can Fairness be Automated? Guidelines and Opportunities for Fairness-aware AutoML</title><link>http://arxiv.org/abs/2303.08485v2</link><description>The field of automated machine learning (AutoML) introduces techniques thatautomate parts of the development of machine learning (ML) systems,accelerating the process and reducing barriers for novices. However, decisionsderived from ML models can reproduce, amplify, or even introduce unfairness inour societies, causing harm to (groups of) individuals. In response,researchers have started to propose AutoML systems that jointly optimizefairness and predictive performance to mitigate fairness-related harm. However,fairness is a complex and inherently interdisciplinary subject, and solelyposing it as an optimization problem can have adverse side effects. With thiswork, we aim to raise awareness among developers of AutoML systems about suchlimitations of fairness-aware AutoML, while also calling attention to thepotential of AutoML as a tool for fairness research. We present a comprehensiveoverview of different ways in which fairness-related harm can arise and theensuing implications for the design of fairness-aware AutoML. We conclude thatwhile fairness cannot be automated, fairness-aware AutoML can play an importantrole in the toolbox of ML practitioners. We highlight several open technicalchallenges for future work in this direction. Additionally, we advocate for thecreation of more user-centered assistive systems designed to tackle challengesencountered in fairness work</description><author>Hilde Weerts, Florian Pfisterer, Matthias Feurer, Katharina Eggensperger, Edward Bergman, Noor Awad, Joaquin Vanschoren, Mykola Pechenizkiy, Bernd Bischl, Frank Hutter</author><pubDate>Tue, 20 Feb 2024 17:36:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08485v2</guid></item><item><title>Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees</title><link>http://arxiv.org/abs/2308.09842v2</link><description>Identifying safe areas is a key point to guarantee trust for systems that arebased on Deep Neural Networks (DNNs). To this end, we introduce theAllDNN-Verification problem: given a safety property and a DNN, enumerate theset of all the regions of the property input domain which are safe, i.e., wherethe property does hold. Due to the #P-hardness of the problem, we propose anefficient approximation method called epsilon-ProVe. Our approach exploits acontrollable underestimation of the output reachable sets obtained viastatistical prediction of tolerance limits, and can provide a tight (withprovable probabilistic guarantees) lower estimate of the safe areas. Ourempirical evaluation on different standard benchmarks shows the scalability andeffectiveness of our method, offering valuable insights for this new type ofverification of DNNs.</description><author>Luca Marzari, Davide Corsi, Enrico Marchesini, Alessandro Farinelli, Ferdinando Cicalese</author><pubDate>Tue, 20 Feb 2024 17:35:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09842v2</guid></item><item><title>3D Kinematics Estimation from Video with a Biomechanical Model and Synthetic Training Data</title><link>http://arxiv.org/abs/2402.13172v1</link><description>Accurate 3D kinematics estimation of human body is crucial in variousapplications for human health and mobility, such as rehabilitation, injuryprevention, and diagnosis, as it helps to understand the biomechanical loadingexperienced during movement. Conventional marker-based motion capture isexpensive in terms of financial investment, time, and the expertise required.Moreover, due to the scarcity of datasets with accurate annotations, existingmarkerless motion capture methods suffer from challenges including unreliable2D keypoint detection, limited anatomic accuracy, and low generalizationcapability. In this work, we propose a novel biomechanics-aware network thatdirectly outputs 3D kinematics from two input views with consideration ofbiomechanical prior and spatio-temporal information. To train the model, wecreate synthetic dataset ODAH with accurate kinematics annotations generated byaligning the body mesh from the SMPL-X model and a full-body OpenSim skeletalmodel. Our extensive experiments demonstrate that the proposed approach, onlytrained on synthetic data, outperforms previous state-of-the-art methods whenevaluated across multiple datasets, revealing a promising direction forenhancing video-based human motion capture.</description><author>Zhi-Yi Lin, Bofan Lyu, Judith Cueto Fernandez, Eline van der Kruk, Ajay Seth, Xucong Zhang</author><pubDate>Tue, 20 Feb 2024 17:33:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13172v1</guid></item><item><title>Structure of activity in multiregion recurrent neural networks</title><link>http://arxiv.org/abs/2402.12188v2</link><description>Neural circuits are composed of multiple regions, each with rich dynamics andengaging in communication with other regions. The combination of local,within-region dynamics and global, network-level dynamics is thought to providecomputational flexibility. However, the nature of such multiregion dynamics andthe underlying synaptic connectivity patterns remain poorly understood. Here,we study the dynamics of recurrent neural networks with multiple interconnectedregions. Within each region, neurons have a combination of random andstructured recurrent connections. Motivated by experimental evidence ofcommunication subspaces between cortical areas, these networks have low-rankconnectivity between regions, enabling selective routing of activity. Thesenetworks exhibit two interacting forms of dynamics: high-dimensionalfluctuations within regions and low-dimensional signal transmission betweenregions. To characterize this interaction, we develop a dynamical mean-fieldtheory to analyze such networks in the limit where each region containsinfinitely many neurons, with cross-region currents as key order parameters.Regions can act as both generators and transmitters of activity, roles that weshow are in conflict. Specifically, taming the complexity of activity within aregion is necessary for it to route signals to and from other regions. Unlikeprevious models of routing in neural circuits, which suppressed the activitiesof neuronal groups to control signal flow, routing in our model is achieved byexciting different high-dimensional activity patterns through a combination ofconnectivity structure and nonlinear recurrent dynamics. This theory providesinsight into the interpretation of both multiregion neural data and trainedneural networks.</description><author>David G. Clark, Manuel Beiran</author><pubDate>Tue, 20 Feb 2024 17:32:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12188v2</guid></item><item><title>A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia</title><link>http://arxiv.org/abs/2312.02073v2</link><description>Large language models (LLMs) have an impressive ability to draw on novelinformation supplied in their context. Yet the mechanisms underlying thiscontextual grounding remain unknown, especially in situations where contextualinformation contradicts factual knowledge stored in the parameters, which LLMsalso excel at recalling. Favoring the contextual information is critical forretrieval-augmented generation methods, which enrich the context withup-to-date information, hoping that grounding can rectify outdated or noisystored knowledge. We present a novel method to study grounding abilities usingFakepedia, a dataset of counterfactual texts constructed to clash with amodel's internal parametric knowledge. We benchmark various LLMs with Fakepediaand then we conduct a causal mediation analysis, based on our Masked GroupedCausal Tracing (MGCT), on LLM components when answering Fakepedia queries.Within this analysis, we identify distinct computational patterns betweengrounded and ungrounded responses. We finally demonstrate that distinguishinggrounded from ungrounded responses is achievable through computational analysisalone. Our results, together with existing findings about factual recallmechanisms, provide a coherent narrative of how grounding and factual recallmechanisms interact within LLMs.</description><author>Giovanni Monea, Maxime Peyrard, Martin Josifoski, Vishrav Chaudhary, Jason Eisner, Emre Kƒ±cƒ±man, Hamid Palangi, Barun Patra, Robert West</author><pubDate>Tue, 20 Feb 2024 17:27:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02073v2</guid></item><item><title>LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores</title><link>http://arxiv.org/abs/2311.09766v3</link><description>Automatic evaluation of generated textual content presents an ongoingchallenge within the field of NLP. Given the impressive capabilities of modernlanguage models (LMs) across diverse NLP tasks, there is a growing trend toemploy these models in creating innovative evaluation metrics for automatedassessment of generation tasks. This paper investigates a pivotal question: Dolanguage model-driven evaluation metrics inherently exhibit bias favoring textsgenerated by the same underlying language model? Specifically, we assesswhether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, andGPTScore) demonstrate a favorable bias toward their respective underlying LMsin the context of summarization tasks. Our findings unveil a latent bias,particularly pronounced when such evaluation metrics are used in anreference-free manner without leveraging gold summaries. These resultsunderscore that assessments provided by generative evaluation models can beinfluenced by factors beyond the inherent text quality, highlighting thenecessity of developing more dependable evaluation protocols in the future.</description><author>Yiqi Liu, Nafise Sadat Moosavi, Chenghua Lin</author><pubDate>Tue, 20 Feb 2024 17:21:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09766v3</guid></item><item><title>Clifford Group Equivariant Simplicial Message Passing Networks</title><link>http://arxiv.org/abs/2402.10011v2</link><description>We introduce Clifford Group Equivariant Simplicial Message Passing Networks,a method for steerable E(n)-equivariant message passing on simplicialcomplexes. Our method integrates the expressivity of Clifford group-equivariantlayers with simplicial message passing, which is topologically more intricatethan regular graph message passing. Clifford algebras include higher-orderobjects such as bivectors and trivectors, which express geometric features(e.g., areas, volumes) derived from vectors. Using this knowledge, we representsimplex features through geometric products of their vertices. To achieveefficient simplicial message passing, we share the parameters of the messagenetwork across different dimensions. Additionally, we restrict the finalmessage to an aggregation of the incoming messages from different dimensions,leading to what we term shared simplicial message passing. Experimental resultsshow that our method is able to outperform both equivariant and simplicialgraph neural networks on a variety of geometric tasks.</description><author>Cong Liu, David Ruhe, Floor Eijkelboom, Patrick Forr√©</author><pubDate>Tue, 20 Feb 2024 17:12:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10011v2</guid></item><item><title>AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies</title><link>http://arxiv.org/abs/2402.13152v1</link><description>More than 7,000 known languages are spoken around the world. However, due tothe lack of annotated resources, only a small fraction of them are currentlycovered by speech technologies. Albeit self-supervised speech representations,recent massive speech corpora collections, as well as the organization ofchallenges, have alleviated this inequality, most studies are mainlybenchmarked on English. This situation is aggravated when tasks involving bothacoustic and visual speech modalities are addressed. In order to promoteresearch on low-resource languages for audio-visual speech technologies, wepresent AnnoTheia, a semi-automatic annotation toolkit that detects when aperson speaks on the scene and the corresponding transcription. In addition, toshow the complete process of preparing AnnoTheia for a language of interest, wealso describe the adaptation of a pre-trained model for active speakerdetection to Spanish, using a database not initially conceived for this type oftask. The AnnoTheia toolkit, tutorials, and pre-trained models are available onGitHub.</description><author>Jos√©-M. Acosta-Triana, David Gimeno-G√≥mez, Carlos-D. Mart√≠nez-Hinarejos</author><pubDate>Tue, 20 Feb 2024 17:07:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13152v1</guid></item><item><title>Defending Jailbreak Prompts via In-Context Adversarial Game</title><link>http://arxiv.org/abs/2402.13148v1</link><description>Large Language Models (LLMs) demonstrate remarkable capabilities acrossdiverse applications. However, concerns regarding their security, particularlythe vulnerability to jailbreak attacks, persist. Drawing inspiration fromadversarial training in deep learning and LLM agent learning processes, weintroduce the In-Context Adversarial Game (ICAG) for defending againstjailbreaks without the need for fine-tuning. ICAG leverages agent learning toconduct an adversarial game, aiming to dynamically extend knowledge to defendagainst jailbreaks. Unlike traditional methods that rely on static datasets,ICAG employs an iterative process to enhance both the defense and attackagents. This continuous improvement process strengthens defenses against newlygenerated jailbreak prompts. Our empirical studies affirm ICAG's efficacy,where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak successrates across various attack scenarios. Moreover, ICAG demonstrates remarkabletransferability to other LLMs, indicating its potential as a versatile defensemechanism.</description><author>Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, Xiangliang Zhang</author><pubDate>Tue, 20 Feb 2024 17:04:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13148v1</guid></item><item><title>SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations</title><link>http://arxiv.org/abs/2402.13147v1</link><description>We consider offline imitation learning (IL), which aims to mimic the expert'sbehavior from its demonstration without further interaction with theenvironment. One of the main challenges in offline IL is dealing with thelimited support of expert demonstrations that cover only a small fraction ofthe state-action spaces. In this work, we consider offline IL, where expertdemonstrations are limited but complemented by a larger set of sub-optimaldemonstrations of lower expertise levels. Most of the existing offline ILmethods developed for this setting are based on behavior cloning ordistribution matching, where the aim is to match the occupancy distribution ofthe imitation policy with that of the expert policy. Such an approach oftensuffers from over-fitting, as expert demonstrations are limited to accuratelyrepresent any occupancy distribution. On the other hand, since sub-optimal setsare much larger, there is a high chance that the imitation policy is trainedtowards sub-optimal policies. In this paper, to address these issues, wepropose a new approach based on inverse soft-Q learning, where a regularizationterm is added to the training objective, with the aim of aligning the learnedrewards with a pre-assigned reward function that allocates higher weights tostate-action pairs from expert demonstrations, and lower weights to those fromlower expertise levels. On standard benchmarks, our inverse soft-Q learningsignificantly outperforms other offline IL baselines by a large margin.</description><author>Huy Hoang, Tien Mai, Pradeep Varakantham</author><pubDate>Tue, 20 Feb 2024 17:02:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13147v1</guid></item><item><title>OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog</title><link>http://arxiv.org/abs/2402.13146v1</link><description>We present the Object Language Video Transformer (OLViT) - a novel model forvideo dialog operating over a multi-modal attention-based dialog state tracker.Existing video dialog models struggle with questions requiring both spatial andtemporal localization within videos, long-term temporal reasoning, and accurateobject tracking across multiple dialog turns. OLViT addresses these challengesby maintaining a global dialog state based on the output of an Object StateTracker (OST) and a Language State Tracker (LST): while the OST attends to themost important objects within the video, the LST keeps track of the mostimportant linguistic co-references to previous dialog turns. In stark contrastto previous works, our approach is generic by nature and is therefore capableof learning continuous multi-modal dialog state representations of the mostrelevant objects and rounds. As a result, they can be seamlessly integratedinto Large Language Models (LLMs) and offer high flexibility in dealing withdifferent datasets and tasks. Evaluations on the challenging DVD (responseclassification) and SIMMC 2.1 (response generation) datasets show that OLViTachieves new state-of-the-art performance across both datasets.</description><author>Adnen Abdessaied, Manuel von Hochmeister, Andreas Bulling</author><pubDate>Tue, 20 Feb 2024 17:00:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13146v1</guid></item><item><title>CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for Boosting Metaphor Generation</title><link>http://arxiv.org/abs/2402.13145v1</link><description>Metaphor is a prominent linguistic device in human language and literature,as they add color, imagery, and emphasis to enhance effective communication.This paper introduces a large-scale high quality annotated Chinese MetaphorCorpus, which comprises around 28K sentences drawn from a diverse range ofChinese literary sources, such as poems, prose, song lyrics, etc. To ensure theaccuracy and consistency of our annotations, we introduce a comprehensive setof guidelines. These guidelines address the facets of metaphor annotation,including identifying tenors, vehicles, and grounds to handling thecomplexities of similes, personifications, juxtapositions, and hyperboles.Breaking tradition, our approach to metaphor generation emphasizes grounds andtheir distinct features rather than the conventional combination of tenors andvehicles. By integrating "ground" as a CoT (Chain of Thoughts) input, we areable to generate metaphors that resonate more with real-world intuition. Wetest generative models such as Belle, Baichuan, and Chinese-alpaca-33B usingour annotated corpus. These models are able to generate creative and fluentmetaphor sentences more frequently induced by selected samples from ourdataset, demonstrating the value of our corpus for Chinese metaphor research.The code is available in thehttps://anonymous.4open.science/r/Chinese_Metaphor_Explanation-63F2.</description><author>Yujie Shao, Xinrong Yao, Xingwei Qu, Chenghua Lin, Shi Wang, Stephen W. Huang, Ge Zhang, Jie Fu</author><pubDate>Tue, 20 Feb 2024 17:00:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13145v1</guid></item><item><title>Humans or LLMs as the Judge? A Study on Judgement Biases</title><link>http://arxiv.org/abs/2402.10669v2</link><description>Adopting human and large language models (LLM) as judges (\textit{a.k.a}human- and LLM-as-a-judge) for evaluating the performance of existing LLMs hasrecently gained attention. Nonetheless, this approach concurrently introducespotential biases from human and LLM judges, questioning the reliability of theevaluation results. In this paper, we propose a novel framework forinvestigating 5 types of biases for LLM and human judges. We curate a datasetwith 142 samples referring to the revised Bloom's Taxonomy and conductthousands of human and LLM evaluations. Results show that human and LLM judgesare vulnerable to perturbations to various degrees, and that even the mostcutting-edge judges possess considerable biases. We further exploit theirweakness and conduct attacks on LLM judges. We hope that our work can notifythe community of the vulnerability of human- and LLM-as-a-judge againstperturbations, as well as the urgency of developing robust evaluation systems.</description><author>Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang</author><pubDate>Tue, 20 Feb 2024 17:00:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10669v2</guid></item><item><title>Neural Network Diffusion</title><link>http://arxiv.org/abs/2402.13144v1</link><description>Diffusion models have achieved remarkable success in image and videogeneration. In this work, we demonstrate that diffusion models can also\textit{generate high-performing neural network parameters}. Our approach issimple, utilizing an autoencoder and a standard latent diffusion model. Theautoencoder extracts latent representations of a subset of the trained networkparameters. A diffusion model is then trained to synthesize these latentparameter representations from random noise. It then generates newrepresentations that are passed through the autoencoder's decoder, whoseoutputs are ready to use as new subsets of network parameters. Across variousarchitectures and datasets, our diffusion process consistently generates modelsof comparable or improved performance over trained networks, with minimaladditional cost. Notably, we empirically find that the generated models performdifferently with the trained networks. Our results encourage more explorationon the versatile use of diffusion models.</description><author>Kai Wang, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, Yang You</author><pubDate>Tue, 20 Feb 2024 16:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13144v1</guid></item><item><title>The Hidden Space of Transformer Language Adapters</title><link>http://arxiv.org/abs/2402.13137v1</link><description>We analyze the operation of transformer language adapters, which are smallmodules trained on top of a frozen language model to adapt its predictions tonew target languages. We show that adapted predictions mostly evolve in thesource language the model was trained on, while the target language becomespronounced only in the very last layers of the model. Moreover, the adaptationprocess is gradual and distributed across layers, where it is possible to skipsmall groups of adapters without decreasing adaptation performance. Last, weshow that adapters operate on top of the model's frozen representation spacewhile largely preserving its structure, rather than on an 'isolated' subspace.Our findings provide a deeper view into the adaptation process of languagemodels to new languages, showcasing the constraints imposed on it by theunderlying model and introduces practical implications to enhance itsefficiency.</description><author>Jesujoba O. Alabi, Marius Mosbach, Matan Eyal, Dietrich Klakow, Mor Geva</author><pubDate>Tue, 20 Feb 2024 16:53:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13137v1</guid></item><item><title>Unified Hallucination Detection for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2402.03190v3</link><description>Despite significant strides in multimodal tasks, Multimodal Large LanguageModels (MLLMs) are plagued by the critical issue of hallucination. The reliabledetection of such hallucinations in MLLMs has, therefore, become a vital aspectof model evaluation and the safeguarding of practical application deployment.Prior research in this domain has been constrained by a narrow focus onsingular tasks, an inadequate range of hallucination categories addressed, anda lack of detailed granularity. In response to these challenges, our workexpands the investigative horizons of hallucination detection. We present anovel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitatethe evaluation of advancements in hallucination detection methods.Additionally, we unveil a novel unified multimodal hallucination detectionframework, UNIHD, which leverages a suite of auxiliary tools to validate theoccurrence of hallucinations robustly. We demonstrate the effectiveness ofUNIHD through meticulous evaluation and comprehensive analysis. We also providestrategic insights on the application of specific tools for addressing variouscategories of hallucinations.</description><author>Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen</author><pubDate>Tue, 20 Feb 2024 16:47:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03190v3</guid></item><item><title>exploreCOSMOS: Interactive Exploration of Conditional Statistical Shape Models in the Web-Browser</title><link>http://arxiv.org/abs/2402.13131v1</link><description>Statistical Shape Models of faces and various body parts are heavily used inmedical image analysis, computer vision and visualization. Whilst the field iswell explored with many existing tools, all of them aim at experts, whichlimits their applicability. We demonstrate the first tool that enables theconvenient exploration of statistical shape models in the browser, with thecapability to manipulate the faces in a targeted manner. This manipulation isperformed via a posterior model given partial observations. We release our codeand application on GitHub https://github.com/maximilian-hahn/exploreCOSMOS</description><author>Maximilian Hahn, Bernhard Egger</author><pubDate>Tue, 20 Feb 2024 16:44:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13131v1</guid></item><item><title>Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity</title><link>http://arxiv.org/abs/2402.13130v1</link><description>While BERT produces high-quality sentence embeddings, its pre-trainingcomputational cost is a significant drawback. In contrast, ELECTRA delivers acost-effective pre-training objective and downstream task performanceimprovements, but not as performant sentence embeddings. The community tacitlystopped utilizing ELECTRA's sentence embeddings for semantic textual similarity(STS). We notice a significant drop in performance when using the ELECTRAdiscriminator's last layer in comparison to earlier layers. We explore thisdrop and devise a way to repair ELECTRA's embeddings, proposing a noveltruncated model fine-tuning (TMFT) method. TMFT improves the Spearmancorrelation coefficient by over 8 points while increasing parameter efficiencyon the STS benchmark dataset. We extend our analysis to various model sizes andlanguages. Further, we discover the surprising efficacy of ELECTRA's generatormodel, which performs on par with BERT, using significantly fewer parametersand a substantially smaller embedding size. Finally, we observe further boostsby combining TMFT with a word similarity task or domain adaptive pre-training.</description><author>Ivan Rep, David Dukiƒá, Jan ≈†najder</author><pubDate>Tue, 20 Feb 2024 16:43:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13130v1</guid></item><item><title>VGMShield: Mitigating Misuse of Video Generative Models</title><link>http://arxiv.org/abs/2402.13126v1</link><description>With the rapid advancement in video generation, people can convenientlyutilize video generation models to create videos tailored to their specificdesires. Nevertheless, there are also growing concerns about their potentialmisuse in creating and disseminating false information. In this work, we introduce VGMShield: a set of three straightforward butpioneering mitigations through the lifecycle of fake video generation. We startfrom \textit{fake video detection} trying to understand whether there isuniqueness in generated videos and whether we can differentiate them from realvideos; then, we investigate the \textit{tracing} problem, which maps a fakevideo back to a model that generates it. Towards these, we propose to leveragepre-trained models that focus on {\it spatial-temporal dynamics} as thebackbone to identify inconsistencies in videos. Through experiments on sevenstate-of-the-art open-source models, we demonstrate that current models stillcannot perfectly handle spatial-temporal relationships, and thus, we canaccomplish detection and tracing with nearly perfect accuracy. Furthermore, anticipating future generative model improvements, we propose a{\it prevention} method that adds invisible perturbations to images to make thegenerated videos look unreal. Together with fake video detection and tracing,our multi-faceted set of solutions can effectively mitigate misuse of videogenerative models.</description><author>Yan Pang, Yang Zhang, Tianhao Wang</author><pubDate>Tue, 20 Feb 2024 16:39:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13126v1</guid></item><item><title>TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning</title><link>http://arxiv.org/abs/2402.13125v1</link><description>Recently, numerous new benchmarks have been established to evaluate theperformance of large language models (LLMs) via either computing a holisticscore or employing another LLM as a judge. However, these approaches sufferfrom data leakage due to the open access of the benchmark and inflexibleevaluation process. To address this issue, we introduce $\textbf{TreeEval}$, abenchmark-free evaluation method for LLMs that let a high-performance LLM hostan irreproducible evaluation session and essentially avoids the data leakage.Moreover, this LLM performs as an examiner to raise up a series of questionsunder a topic with a tree planing strategy, which considers the currentevaluation status to decide the next question generation and ensures thecompleteness and efficiency of the evaluation process. We evaluate $6$ modelsof different parameter sizes, including $7$B, $13$B, and $33$B, and ultimatelyachieved the highest correlation coefficient with AlpacaEval2.0 using onlyaround $45$ questions. We also conduct more analysis to show the robustness andreliability of TreeEval. Our code can be accessed via the providedhttps://github.com/Ashura5/TreeEval.</description><author>Xiang Li, Yunshi Lan, Chao Yang</author><pubDate>Tue, 20 Feb 2024 16:38:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13125v1</guid></item><item><title>Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model</title><link>http://arxiv.org/abs/2402.13122v1</link><description>Many practical applications require training of semantic segmentation modelson unlabelled datasets and their execution on low-resource hardware.Distillation from a trained source model may represent a solution for the firstbut does not account for the different distribution of the training data.Unsupervised domain adaptation (UDA) techniques claim to solve the domainshift, but in most cases assume the availability of the source data or anaccessible white-box source model, which in practical applications are oftenunavailable for commercial and/or safety reasons. In this paper, we investigatea more challenging setting in which a lightweight model has to be trained on atarget unlabelled dataset for semantic segmentation, under the assumption thatwe have access only to black-box source model predictions. Our method, namedCoRTe, consists of (i) a pseudo-labelling function that extracts reliableknowledge from the black-box source model using its relative confidence, (ii) apseudo label refinement method to retain and enhance the novel informationlearned by the student model on the target data, and (iii) a consistenttraining of the model using the extracted pseudo labels. We benchmark CoRTe ontwo synthetic-to-real settings, demonstrating remarkable results when usingblack-box models to transfer knowledge on lightweight models for a target datadistribution.</description><author>Claudia Cuttano, Antonio Tavera, Fabio Cermelli, Giuseppe Averta, Barbara Caputo</author><pubDate>Tue, 20 Feb 2024 16:35:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13122v1</guid></item><item><title>Going Further: Flatness at the Rescue of Early Stopping for Adversarial Example Transferability</title><link>http://arxiv.org/abs/2304.02688v2</link><description>Transferability is the property of adversarial examples to be misclassifiedby other models than the surrogate model for which they were crafted. Previousresearch has shown that early stopping the training of the surrogate modelsubstantially increases transferability. A common hypothesis to explain this isthat deep neural networks (DNNs) first learn robust features, which are moregeneric, thus a better surrogate. Then, at later epochs, DNNs learn non-robustfeatures, which are more brittle, hence worst surrogate. First, we tend torefute this hypothesis, using transferability as a proxy for representationsimilarity. We then establish links between transferability and the explorationof the loss landscape in parameter space, focusing on sharpness, which isaffected by early stopping. This leads us to evaluate surrogate models trainedwith seven minimizers that minimize both loss value and loss sharpness. Amongthem, SAM consistently outperforms early stopping by up to 28.8 percentagepoints. We discover that the strong SAM regularization from large flatneighborhoods tightly links to transferability. Finally, the bestsharpness-aware minimizers prove competitive with other training methods andcomplement existing transferability techniques.</description><author>Martin Gubri, Maxime Cordy, Yves Le Traon</author><pubDate>Tue, 20 Feb 2024 16:29:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02688v2</guid></item><item><title>INTERVENOR: Prompting the Coding Ability of Large Language Models with the Interactive Chain of Repair</title><link>http://arxiv.org/abs/2311.09868v4</link><description>This paper introduces INTERVENOR (INTERactiVE chaiN Of Repair), a systemdesigned to emulate the interactive code repair processes observed in humans,encompassing both code diagnosis and code repair. INTERVENOR prompts LargeLanguage Models (LLMs) to play distinct roles during the code repair process,functioning as both a Code Learner and a Code Teacher. Specifically, the CodeLearner is tasked with adhering to instructions to generate or repair code,while the Code Teacher is responsible for crafting a Chain-of-Repair (CoR) toserve as guidance for the Code Learner. During generating the CoR, the CodeLearner needs to check the generated codes from Code Learner and reassess howto address code bugs based on error feedback received from compilers.Experimental results demonstrate that INTERVENOR surpasses baseline models,exhibiting improvements of approximately 18% and 4.3% over GPT-3.5 in codegeneration and code translation tasks, respectively. Our further analyses showthat CoR is effective to illuminate the reasons behind bugs and outlinesolution plans in natural language. With the feedback of code compilers,INTERVENOR can accurately identify syntax errors and assertion errors andprovide precise instructions to repair codes. All data and codes are availableat https://github.com/NEUIR/INTERVENOR</description><author>Hanbin Wang, Zhenghao Liu, Shuo Wang, Ganqu Cui, Ning Ding, Zhiyuan Liu, Ge Yu</author><pubDate>Tue, 20 Feb 2024 16:26:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09868v4</guid></item><item><title>FLAIM: AIM-based Synthetic Data Generation in the Federated Setting</title><link>http://arxiv.org/abs/2310.03447v2</link><description>Preserving individual privacy while enabling collaborative data sharing iscrucial for organizations. Synthetic data generation is one solution, producingartificial data that mirrors the statistical properties of private data. Whilenumerous techniques have been devised under differential privacy, theypredominantly assume data is centralized. However, data is often distributedacross multiple clients in a federated manner. In this work, we initiate thestudy of federated synthetic tabular data generation. Building upon a SOTAcentral method known as AIM, we present DistAIM and FLAIM. We first show thatit is straightforward to distribute AIM, extending a recent approach based onsecure multi-party computation which necessitates additional overhead, makingit less suited to federated scenarios. We then demonstrate that naivelyfederating AIM can lead to substantial degradation in utility under thepresence of heterogeneity. To mitigate both issues, we propose an augmentedFLAIM approach that maintains a private proxy of heterogeneity. We simulate ourmethods across a range of benchmark datasets under different degrees ofheterogeneity and show we can improve utility while reducing overhead.</description><author>Samuel Maddock, Graham Cormode, Carsten Maple</author><pubDate>Tue, 20 Feb 2024 16:18:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03447v2</guid></item><item><title>A multimodal dynamical variational autoencoder for audiovisual speech representation learning</title><link>http://arxiv.org/abs/2305.03582v3</link><description>In this paper, we present a multimodal and dynamical VAE (MDVAE) applied tounsupervised audio-visual speech representation learning. The latent space isstructured to dissociate the latent dynamical factors that are shared betweenthe modalities from those that are specific to each modality. A static latentvariable is also introduced to encode the information that is constant overtime within an audiovisual speech sequence. The model is trained in anunsupervised manner on an audiovisual emotional speech dataset, in two stages.In the first stage, a vector quantized VAE (VQ-VAE) is learned independentlyfor each modality, without temporal modeling. The second stage consists inlearning the MDVAE model on the intermediate representation of the VQ-VAEsbefore quantization. The disentanglement between static versus dynamical andmodality-specific versus modality-common information occurs during this secondtraining stage. Extensive experiments are conducted to investigate howaudiovisual speech latent factors are encoded in the latent space of MDVAE.These experiments include manipulating audiovisual speech, audiovisual facialimage denoising, and audiovisual speech emotion recognition. The results showthat MDVAE effectively combines the audio and visual information in its latentspace. They also show that the learned static representation of audiovisualspeech can be used for emotion recognition with few labeled data, and withbetter accuracy compared with unimodal baselines and a state-of-the-artsupervised model based on an audiovisual transformer architecture.</description><author>Samir Sadok, Simon Leglaive, Laurent Girin, Xavier Alameda-Pineda, Renaud S√©guier</author><pubDate>Tue, 20 Feb 2024 16:18:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03582v3</guid></item><item><title>A Survey on Knowledge Distillation of Large Language Models</title><link>http://arxiv.org/abs/2402.13116v1</link><description>This survey presents an in-depth exploration of knowledge distillation (KD)techniques within the realm of Large Language Models (LLMs), spotlighting thepivotal role of KD in transferring sophisticated capabilities from proprietarygiants such as GPT-4 to accessible, open-source models like LLaMA and Mistral.Amidst the evolving AI landscape, this work elucidates the critical disparitiesbetween proprietary and open-source LLMs, demonstrating how KD serves as anessential conduit for imbuing the latter with the former's advancedfunctionalities and nuanced understandings. Our survey is meticulouslystructured around three foundational pillars: algorithm, skill, andverticalization -- providing a comprehensive examination of KD mechanisms, theenhancement of specific cognitive abilities, and their practical implicationsacross diverse fields. Crucially, the survey navigates the intricate interplaybetween data augmentation (DA) and KD, illustrating how DA emerges as apowerful paradigm within the KD framework to bolster LLMs' performance. Byleveraging DA to generate context-rich, skill-specific training data, KDtranscends traditional boundaries, enabling open-source models to approximatethe contextual adeptness, ethical alignment, and deep semantic insightscharacteristic of their proprietary counterparts. This work aims to provide aninsightful guide for researchers and practitioners, offering a detailedoverview of current methodologies in knowledge distillation and proposingfuture research directions. By bridging the gap between proprietary andopen-source LLMs, this survey underscores the potential for more accessible,efficient, and sustainable AI solutions, fostering a more inclusive andequitable landscape in AI advancements. An associated Github repository isavailable at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.</description><author>Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou</author><pubDate>Tue, 20 Feb 2024 16:17:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13116v1</guid></item><item><title>BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer Nodes</title><link>http://arxiv.org/abs/2402.13114v1</link><description>Class imbalance in graph-structured data, where minor classes aresignificantly underrepresented, poses a critical challenge for Graph NeuralNetworks (GNNs). To address this challenge, existing studies generally generatenew minority nodes and edges connecting new nodes to the original graph to makeclasses balanced. However, they do not solve the problem that majority classesstill propagate information to minority nodes by edges in the original graphwhich introduces bias towards majority classes. To address this, we introduceBuffGraph, which inserts buffer nodes into the graph, modulating the impact ofmajority classes to improve minor class representation. Our extensiveexperiments across diverse real-world datasets empirically demonstrate thatBuffGraph outperforms existing baseline methods in class-imbalanced nodeclassification in both natural settings and imbalanced settings. Code isavailable at https://anonymous.4open.science/r/BuffGraph-730A.</description><author>Qian Wang, Zemin Liu, Zhen Zhang, Bingsheng He</author><pubDate>Tue, 20 Feb 2024 16:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13114v1</guid></item><item><title>SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping</title><link>http://arxiv.org/abs/2310.01201v2</link><description>Tensor decomposition has recently been gaining attention in the machinelearning community for the analysis of individual traces, such as ElectronicHealth Records (EHR). However, this task becomes significantly more difficultwhen the data follows complex temporal patterns. This paper introduces thenotion of a temporal phenotype as an arrangement of features over time and itproposes SWoTTeD (Sliding Window for Temporal Tensor Decomposition), a novelmethod to discover hidden temporal patterns. SWoTTeD integrates severalconstraints and regularizations to enhance the interpretability of theextracted phenotypes. We validate our proposal using both synthetic andreal-world datasets, and we present an original usecase using data from theGreater Paris University Hospital. The results show that SWoTTeD achieves atleast as accurate reconstruction as recent state-of-the-art tensordecomposition models, and extracts temporal phenotypes that are meaningful forclinicians.</description><author>Hana Sebia, Thomas Guyet, Etienne Audureau</author><pubDate>Tue, 20 Feb 2024 16:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01201v2</guid></item><item><title>When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality</title><link>http://arxiv.org/abs/2402.13113v1</link><description>Incremental models that process sentences one token at a time will sometimesencounter points where more than one interpretation is possible. Causal modelsare forced to output one interpretation and continue, whereas models that canrevise may edit their previous output as the ambiguity is resolved. In thiswork, we look at how restart-incremental Transformers build and update internalstates, in an effort to shed light on what processes cause revisions not viablein autoregressive models. We propose an interpretable way to analyse theincremental states, showing that their sequential structure encodes informationon the garden path effect and its resolution. Our method brings insights onvarious bidirectional encoders for contextualised meaning representation anddependency parsing, contributing to show their advantage over causal modelswhen it comes to revisions.</description><author>Brielen Madureira, Patrick Kahardipraja, David Schlangen</author><pubDate>Tue, 20 Feb 2024 16:09:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13113v1</guid></item><item><title>Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in RL</title><link>http://arxiv.org/abs/2305.17342v3</link><description>Most existing works focus on direct perturbations to the victim'sstate/action or the underlying transition dynamics to demonstrate thevulnerability of reinforcement learning agents to adversarial attacks. However,such direct manipulations may not be always realizable. In this paper, weconsider a multi-agent setting where a well-trained victim agent $\nu$ isexploited by an attacker controlling another agent $\alpha$ with an\textit{adversarial policy}. Previous models do not account for the possibilitythat the attacker may only have partial control over $\alpha$ or that theattack may produce easily detectable "abnormal" behaviors. Furthermore, thereis a lack of provably efficient defenses against these adversarial policies. Toaddress these limitations, we introduce a generalized attack framework that hasthe flexibility to model to what extent the adversary is able to control theagent, and allows the attacker to regulate the state distribution shift andproduce stealthier adversarial policies. Moreover, we offer a provablyefficient defense with polynomial convergence to the most robust victim policythrough adversarial training with timescale separation. This stands in sharpcontrast to supervised learning, where adversarial training typically providesonly \textit{empirical} defenses. Using the Robosumo competition experiments,we show that our generalized attack formulation results in much stealthieradversarial policies when maintaining the same winning rate as baselines.Additionally, our adversarial training approach yields stable learning dynamicsand less exploitable victim policies.</description><author>Xiangyu Liu, Souradip Chakraborty, Yanchao Sun, Furong Huang</author><pubDate>Tue, 20 Feb 2024 16:05:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17342v3</guid></item><item><title>SemiReward: A General Reward Model for Semi-supervised Learning</title><link>http://arxiv.org/abs/2310.03013v2</link><description>Semi-supervised learning (SSL) has witnessed great progress with variousimprovements in the self-training framework with pseudo labeling. The mainchallenge is how to distinguish high-quality pseudo labels against theconfirmation bias. However, existing pseudo-label selection strategies arelimited to pre-defined schemes or complex hand-crafted policies speciallydesigned for classification, failing to achieve high-quality labels, fastconvergence, and task versatility simultaneously. To these ends, we propose aSemi-supervised Reward framework (SemiReward) that predicts reward scores toevaluate and filter out high-quality pseudo labels, which is pluggable tomainstream SSL methods in wide task types and scenarios. To mitigateconfirmation bias, SemiReward is trained online in two stages with a generatormodel and subsampling strategy. With classification and regression tasks on 13standard SSL benchmarks across three modalities, extensive experiments verifythat SemiReward achieves significant performance gains and faster convergencespeeds upon Pseudo Label, FlexMatch, and Free/SoftMatch. Code and models areavailable at https://github.com/Westlake-AI/SemiReward.</description><author>Siyuan Li, Weiyang Jin, Zedong Wang, Fang Wu, Zicheng Liu, Cheng Tan, Stan Z. Li</author><pubDate>Tue, 20 Feb 2024 16:02:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03013v2</guid></item><item><title>CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models</title><link>http://arxiv.org/abs/2402.13109v1</link><description>The advancement of large language models (LLMs) has enhanced the ability togeneralize across a wide range of unseen natural language processing (NLP)tasks through instruction-following. Yet, their effectiveness often diminishesin low-resource languages like Chinese, exacerbated by biased evaluations fromdata leakage, casting doubt on their true generalizability to new linguisticterritories. In response, we introduce the Chinese Instruction-FollowingBenchmark (CIF-Bench), designed to evaluate the zero-shot generalizability ofLLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000input-output pairs, developed by native speakers to test complex reasoning andChinese cultural nuances across 20 categories. To mitigate evaluation bias, werelease only half of the dataset publicly, with the remainder kept private, andintroduce diversified instructions to minimize score variance, totaling 45,000data instances. Our evaluation of 28 selected LLMs reveals a noticeableperformance gap, with the best model scoring only 52.9%, highlighting thelimitations of LLMs in less familiar language and task contexts. This work aimsto uncover the current limitations of LLMs in handling Chinese tasks, pushingtowards the development of more culturally informed and linguistically diversemodels with the released data and benchmark(https://yizhilll.github.io/CIF-Bench/).</description><author>Yizhi LI, Ge Zhang, Xingwei Qu, Jiali Li, Zhaoqun Li, Zekun Wang, Hao Li, Ruibin Yuan, Yinghao Ma, Kai Zhang, Wangchunshu Zhou, Yiming Liang, Lei Zhang, Lei Ma, Jiajun Zhang, Zuowen Li, Stephen W. Huang, Chenghua Lin, Wenhu Chen, Jie Fu</author><pubDate>Tue, 20 Feb 2024 16:02:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13109v1</guid></item><item><title>On the Stability of Gradient Descent for Large Learning Rate</title><link>http://arxiv.org/abs/2402.13108v1</link><description>There currently is a significant interest in understanding the Edge ofStability (EoS) phenomenon, which has been observed in neural networkstraining, characterized by a non-monotonic decrease of the loss function overepochs, while the sharpness of the loss (spectral norm of the Hessian)progressively approaches and stabilizes around 2/(learning rate). Reasons forthe existence of EoS when training using gradient descent have recently beenproposed -- a lack of flat minima near the gradient descent trajectory togetherwith the presence of compact forward-invariant sets. In this paper, we showthat linear neural networks optimized under a quadratic loss function satisfythe first assumption and also a necessary condition for the second assumption.More precisely, we prove that the gradient descent map is non-singular, the setof global minimizers of the loss function forms a smooth manifold, and thestable minima form a bounded subset in parameter space. Additionally, we provethat if the step-size is too big, then the set of initializations from whichgradient descent converges to a critical point has measure zero.</description><author>Alexandru CrƒÉciun, Debarghya Ghoshdastidar</author><pubDate>Tue, 20 Feb 2024 16:01:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13108v1</guid></item><item><title>On Generalization Bounds for Deep Compound Gaussian Neural Networks</title><link>http://arxiv.org/abs/2402.13106v1</link><description>Algorithm unfolding or unrolling is the technique of constructing a deepneural network (DNN) from an iterative algorithm. Unrolled DNNs often providebetter interpretability and superior empirical performance over standard DNNsin signal estimation tasks. An important theoretical question, which has onlyrecently received attention, is the development of generalization error boundsfor unrolled DNNs. These bounds deliver theoretical and practical insights intothe performance of a DNN on empirical datasets that are distinct from, butsampled from, the probability density generating the DNN training data. In thispaper, we develop novel generalization error bounds for a class of unrolledDNNs that are informed by a compound Gaussian prior. These compound Gaussiannetworks have been shown to outperform comparative standard and unfolded deepneural networks in compressive sensing and tomographic imaging problems. Thegeneralization error bound is formulated by bounding the Rademacher complexityof the class of compound Gaussian network estimates with Dudley's integral.Under realistic conditions, we show that, at worst, the generalization errorscales $\mathcal{O}(n\sqrt{\ln(n)})$ in the signal dimension and$\mathcal{O}(($Network Size$)^{3/2})$ in network size.</description><author>Carter Lyons, Raghu G. Raj, Margaret Cheney</author><pubDate>Tue, 20 Feb 2024 16:01:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13106v1</guid></item><item><title>Multivariate Functional Linear Discriminant Analysis for the Classification of Short Time Series with Missing Data</title><link>http://arxiv.org/abs/2402.13103v1</link><description>Functional linear discriminant analysis (FLDA) is a powerful tool thatextends LDA-mediated multiclass classification and dimension reduction tounivariate time-series functions. However, in the age of large multivariate andincomplete data, statistical dependencies between features must be estimated ina computationally tractable way, while also dealing with missing data. There isa need for a computationally tractable approach that considers the statisticaldependencies between features and can handle missing values. We here develop amultivariate version of FLDA (MUDRA) to tackle this issue and describe anefficient expectation/conditional-maximization (ECM) algorithm to infer itsparameters. We assess its predictive power on the "Articulary Word Recognition"data set and show its improvement over the state-of-the-art, especially in thecase of missing data. MUDRA allows interpretable classification of data setswith large proportions of missing data, which will be particularly useful formedical or psychological data sets.</description><author>Rahul Bordoloi, Cl√©mence R√©da, Orell Trautmann, Saptarshi Bej, Olaf Wolkenhauer</author><pubDate>Tue, 20 Feb 2024 15:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13103v1</guid></item><item><title>A Microstructure-based Graph Neural Network for Accelerating Multiscale Simulations</title><link>http://arxiv.org/abs/2402.13101v1</link><description>Simulating the mechanical response of advanced materials can be done moreaccurately using concurrent multiscale models than with single-scalesimulations. However, the computational costs stand in the way of the practicalapplication of this approach. The costs originate from microscale FiniteElement (FE) models that must be solved at every macroscopic integration point.A plethora of surrogate modeling strategies attempt to alleviate this cost bylearning to predict macroscopic stresses from macroscopic strains, completelyreplacing the microscale models. In this work, we introduce an alternativesurrogate modeling strategy that allows for keeping the multiscale nature ofthe problem, allowing it to be used interchangeably with an FE solver for anytime step. Our surrogate provides all microscopic quantities, which are thenhomogenized to obtain macroscopic quantities of interest. We achieve this foran elasto-plastic material by predicting full-field microscopic strains using agraph neural network (GNN) while retaining the microscopic constitutivematerial model to obtain the stresses. This hybrid data-physics graph-basedapproach avoids the high dimensionality originating from predicting full-fieldresponses while allowing non-locality to arise. By training the GNN on avariety of meshes, it learns to generalize to unseen meshes, allowing a singlemodel to be used for a range of microstructures. The embedded microscopicconstitutive model in the GNN implicitly tracks history-dependent variables andleads to improved accuracy. We demonstrate for several challenging scenariosthat the surrogate can predict complex macroscopic stress-strain paths. As thecomputation time of our method scales favorably with the number of elements inthe microstructure compared to the FE method, our method can significantlyaccelerate FE2 simulations.</description><author>J. Storm, I. B. C. M. Rocha, F. P. van der Meer</author><pubDate>Tue, 20 Feb 2024 15:54:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13101v1</guid></item><item><title>ELAD: Explanation-Guided Large Language Models Active Distillation</title><link>http://arxiv.org/abs/2402.13098v1</link><description>The deployment and application of Large Language Models (LLMs) is hindered bytheir memory inefficiency, computational demands, and the high costs of APIinferences. Traditional distillation methods, which transfer the capabilitiesof LLMs to smaller models, often fail to determine whether the knowledge hasbeen sufficiently transferred, potentially resulting in high costs orincomplete distillation. In this paper, we propose an Explanation-Guided LLMsActive Distillation (ELAD) framework that employs an active learning strategyto optimize the balance between annotation costs and model performance. Toimprove efficient sample selection, we introduce an explanation-guided sampleselection method that identifies samples challenging its reasoning byexploiting uncertainties in explanation steps. Additionally, we present acustomized LLM-annotated explanation revision technique where the teacher modeldetects and corrects flaws in the student model's reasoning. Our experimentsacross various reasoning datasets demonstrate that our framework significantlyenhances the efficiency of LLM knowledge distillation.</description><author>Yifei Zhang, Bo Pan, Chen Ling, Yuntong Hu, Liang Zhao</author><pubDate>Tue, 20 Feb 2024 15:47:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13098v1</guid></item><item><title>Exact Hard Monotonic Attention for Character-Level Transduction</title><link>http://arxiv.org/abs/1905.06319v3</link><description>Many common character-level, string-to string transduction tasks, e.g.,grapheme-tophoneme conversion and morphological inflection, consist almostexclusively of monotonic transductions. However, neural sequence-to sequencemodels that use non-monotonic soft attention often outperform popular monotonicmodels. In this work, we ask the following question: Is monotonicity really ahelpful inductive bias for these tasks? We develop a hard attentionsequence-to-sequence model that enforces strict monotonicity and learns alatent alignment jointly while learning to transduce. With the help of dynamicprogramming, we are able to compute the exact marginalization over allmonotonic alignments. Our models achieve state-of-the-art performance onmorphological inflection. Furthermore, we find strong performance on two othercharacter-level transduction tasks. Code is available athttps://github.com/shijie-wu/neural-transducer.</description><author>Shijie Wu, Ryan Cotterell</author><pubDate>Tue, 20 Feb 2024 15:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1905.06319v3</guid></item><item><title>Procedural Fairness Through Decoupling Objectionable Data Generating Components</title><link>http://arxiv.org/abs/2311.14688v2</link><description>We reveal and address the frequently overlooked yet important issue ofdisguised procedural unfairness, namely, the potentially inadvertentalterations on the behavior of neutral (i.e., not problematic) aspects of datagenerating process, and/or the lack of procedural assurance of the greatestbenefit of the least advantaged individuals. Inspired by John Rawls's advocacyfor pure procedural justice, we view automated decision-making as a microcosmof social institutions, and consider how the data generating process itself cansatisfy the requirements of procedural fairness. We propose a framework thatdecouples the objectionable data generating components from the neutral ones byutilizing reference points and the associated value instantiation rule. Ourfindings highlight the necessity of preventing disguised procedural unfairness,drawing attention not only to the objectionable data generating components thatwe aim to mitigate, but also more importantly, to the neutral components thatwe intend to keep unaffected.</description><author>Zeyu Tang, Jialu Wang, Yang Liu, Peter Spirtes, Kun Zhang</author><pubDate>Tue, 20 Feb 2024 15:40:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14688v2</guid></item><item><title>Digital Comprehensibility Assessment of Simplified Texts among Persons with Intellectual Disabilities</title><link>http://arxiv.org/abs/2402.13094v1</link><description>Text simplification refers to the process of increasing the comprehensibilityof texts. Automatic text simplification models are most commonly evaluated byexperts or crowdworkers instead of the primary target groups of simplifiedtexts, such as persons with intellectual disabilities. We conducted anevaluation study of text comprehensibility including participants with andwithout intellectual disabilities reading unsimplified, automatically andmanually simplified German texts on a tablet computer. We explored fourdifferent approaches to measuring comprehensibility: multiple-choicecomprehension questions, perceived difficulty ratings, response time, andreading speed. The results revealed significant variations in thesemeasurements, depending on the reader group and whether the text had undergoneautomatic or manual simplification. For the target group of persons withintellectual disabilities, comprehension questions emerged as the most reliablemeasure, while analyzing reading speed provided valuable insights intoparticipants' reading behavior.</description><author>Andreas S√§uberli, Franz Holzknecht, Patrick Haller, Silvana Deilen, Laura Schiffl, Silvia Hansen-Schirra, Sarah Ebling</author><pubDate>Tue, 20 Feb 2024 15:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13094v1</guid></item><item><title>Event-level Knowledge Editing</title><link>http://arxiv.org/abs/2402.13093v1</link><description>Knowledge editing aims at updating knowledge of large language models (LLMs)to prevent them from becoming outdated. Existing work edits LLMs at the levelof factual knowledge triplets. However, natural knowledge updates in the realworld come from the occurrences of new events rather than direct changes infactual triplets. In this paper, we propose a new task setting: event-levelknowledge editing, which directly edits new events into LLMs and improves overconventional triplet-level editing on (1) Efficiency. A single event edit leadsto updates in multiple entailed knowledge triplets. (2) Completeness. Beyondupdating factual knowledge, event-level editing also requires considering theevent influences and updating LLMs' knowledge about future trends. We constructa high-quality event-level editing benchmark ELKEN, consisting of 1,515 eventedits, 6,449 questions about factual knowledge, and 10,150 questions aboutfuture tendencies. We systematically evaluate the performance of variousknowledge editing methods and LLMs on this benchmark. We find that ELKEN posessignificant challenges to existing knowledge editing approaches. Our codes anddataset are publicly released to facilitate further research.</description><author>Hao Peng, Xiaozhi Wang, Chunyang Li, Kaisheng Zeng, Jiangshan Duo, Yixin Cao, Lei Hou, Juanzi Li</author><pubDate>Tue, 20 Feb 2024 15:36:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13093v1</guid></item><item><title>Generative Sliced MMD Flows with Riesz Kernels</title><link>http://arxiv.org/abs/2305.11463v4</link><description>Maximum mean discrepancy (MMD) flows suffer from high computational costs inlarge scale computations. In this paper, we show that MMD flows with Rieszkernels $K(x,y) = - \|x-y\|^r$, $r \in (0,2)$ have exceptional properties whichallow their efficient computation. We prove that the MMD of Riesz kernels,which is also known as energy distance, coincides with the MMD of their slicedversion. As a consequence, the computation of gradients of MMDs can beperformed in the one-dimensional setting. Here, for $r=1$, a simple sortingalgorithm can be applied to reduce the complexity from $O(MN+N^2)$ to$O((M+N)\log(M+N))$ for two measures with $M$ and $N$ support points. Asanother interesting follow-up result, the MMD of compactly supported measurescan be estimated from above and below by the Wasserstein-1 distance. For theimplementations we approximate the gradient of the sliced MMD by using only afinite number $P$ of slices. We show that the resulting error has complexity$O(\sqrt{d/P})$, where $d$ is the data dimension. These results enable us totrain generative models by approximating MMD gradient flows by neural networkseven for image applications. We demonstrate the efficiency of our model byimage generation on MNIST, FashionMNIST and CIFAR10.</description><author>Johannes Hertrich, Christian Wald, Fabian Altekr√ºger, Paul Hagemann</author><pubDate>Tue, 20 Feb 2024 15:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11463v4</guid></item><item><title>LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2310.05668v3</link><description>Most of current anomaly detection models assume that the normal patternremains same all the time. However, the normal patterns of Web services changedramatically and frequently. The model trained on old-distribution data isoutdated after such changes. Retraining the whole model every time isexpensive. Besides, at the beginning of normal pattern changes, there is notenough observation data from the new distribution. Retraining a large neuralnetwork model with limited data is vulnerable to overfitting. Thus, we proposea Light and Anti-overfitting Retraining Approach (LARA) for deep variationalauto-encoder based time series anomaly detection methods (VAEs). This work aimsto make three novel contributions: 1) the retraining process is formulated as aconvex problem and can converge at a fast rate as well as prevent overfitting;2) designing a ruminate block, which leverages the historical data without theneed to store them; 3) mathematically proving that when fine-tuning the latentvector and reconstructed data, the linear formations can achieve the leastadjusting errors between the ground truths and the fine-tuned ones. Moreover, we have performed many experiments to verify that retraining LARAwith even 43 time slots of data from new distribution can result in itscompetitive F1 Score in comparison with the state-of-the-art anomaly detectionmodels trained with sufficient data. Besides, we verify its light overhead.</description><author>Feiyi Chen, Zhen Qin, Yingying Zhang, Shuiguang Deng, Yi Xiao, Guansong Pang, Qingsong Wen</author><pubDate>Tue, 20 Feb 2024 15:33:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05668v3</guid></item><item><title>Towards an empirical understanding of MoE design choices</title><link>http://arxiv.org/abs/2402.13089v1</link><description>In this study, we systematically evaluate the impact of common design choicesin Mixture of Experts (MoEs) on validation performance, uncovering distinctinfluences at token and sequence levels. We also present empirical evidenceshowing comparable performance between a learned router and a frozen, randomlyinitialized router, suggesting that learned routing may not be essential. Ourstudy further reveals that Sequence-level routing can result in topic-specificweak expert specialization, in contrast to syntax specialization observed withToken-level routing.</description><author>Dongyang Fan, Bettina Messmer, Martin Jaggi</author><pubDate>Tue, 20 Feb 2024 15:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13089v1</guid></item><item><title>Stable Nonconvex-Nonconcave Training via Linear Interpolation</title><link>http://arxiv.org/abs/2310.13459v3</link><description>This paper presents a theoretical analysis of linear interpolation as aprincipled method for stabilizing (large-scale) neural network training. Weargue that instabilities in the optimization process are often caused by thenonmonotonicity of the loss landscape and show how linear interpolation canhelp by leveraging the theory of nonexpansive operators. We construct a newoptimization scheme called relaxed approximate proximal point (RAPP), which isthe first explicit method without anchoring to achieve last iterate convergencerates for $\rho$-comonotone problems while only requiring $\rho &gt;-\tfrac{1}{2L}$. The construction extends to constrained and regularizedsettings. By replacing the inner optimizer in RAPP we rediscover the family ofLookahead algorithms for which we establish convergence in cohypomonotoneproblems even when the base optimizer is taken to be gradient descent ascent.The range of cohypomonotone problems in which Lookahead converges is furtherexpanded by exploiting that Lookahead inherits the properties of the baseoptimizer. We corroborate the results with experiments on generativeadversarial networks which demonstrates the benefits of the linearinterpolation present in both RAPP and Lookahead.</description><author>Thomas Pethick, Wanyun Xie, Volkan Cevher</author><pubDate>Tue, 20 Feb 2024 15:31:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13459v3</guid></item><item><title>Slot-VLM: SlowFast Slots for Video-Language Modeling</title><link>http://arxiv.org/abs/2402.13088v1</link><description>Video-Language Models (VLMs), powered by the advancements in Large LanguageModels (LLMs), are charting new frontiers in video understanding. A pivotalchallenge is the development of an efficient method to encapsulate videocontent into a set of representative tokens to align with LLMs. In this work,we introduce Slot-VLM, a novel framework designed to generate semanticallydecomposed video tokens, in terms of object-wise and event-wise visualrepresentations, to facilitate LLM inference. Particularly, we design aSlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the densevideo tokens from the CLIP vision encoder to a set of representative slots. Inorder to take into account both the spatial object details and the variedtemporal dynamics, SF-Slots is built with a dual-branch structure. TheSlow-Slots branch focuses on extracting object-centric slots from features athigh spatial resolution but low (slow) frame sample rate, emphasizing detailedobject information. Conversely, Fast-Slots branch is engineered to learnevent-centric slots from high temporal sample rate but low spatial resolutionfeatures. These complementary slots are combined to form the vision context,serving as the input to the LLM for efficient question answering. Ourexperimental results demonstrate the effectiveness of our Slot-VLM, whichachieves the state-of-the-art performance on video question-answering.</description><author>Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, Yan Lu</author><pubDate>Tue, 20 Feb 2024 15:30:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13088v1</guid></item><item><title>How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning</title><link>http://arxiv.org/abs/2402.13087v1</link><description>We study the problem of guaranteeing Differential Privacy (DP) inhyper-parameter tuning, a crucial process in machine learning involving theselection of the best run from several. Unlike many private algorithms,including the prevalent DP-SGD, the privacy implications of tuning remaininsufficiently understood. Recent works propose a generic private solution forthe tuning process, yet a fundamental question still persists: is the currentprivacy bound for this solution tight? This paper contributes both positive and negative answers to this question.Initially, we provide studies affirming the current privacy analysis is indeedtight in a general sense. However, when we specifically study thehyper-parameter tuning problem, such tightness no longer holds. This is firstdemonstrated by applying privacy audit on the tuning process. Our findingsunderscore a substantial gap between the current theoretical privacy bound andthe empirical bound derived even under the strongest audit setup. The gap found is not a fluke. Our subsequent study provides an improvedprivacy result for private hyper-parameter tuning due to its distinctproperties. Our privacy results are also more generalizable compared to prioranalyses that are only easily applicable in specific setups.</description><author>Zihang Xiang, Chenglong Wang, Di Wang</author><pubDate>Tue, 20 Feb 2024 15:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13087v1</guid></item><item><title>Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models</title><link>http://arxiv.org/abs/2310.06692v3</link><description>Large language models (LLMs) have unveiled remarkable reasoning capabilitiesby exploiting chain-of-thought (CoT) prompting, which generates intermediatereasoning chains to serve as the rationale for deriving the answer. However,current CoT methods either simply employ general prompts such as Let's thinkstep by step, or heavily rely on pre-defined task-specific demonstrations toattain preferable performances, thereby engendering an inescapable gap betweenperformance and generalization. To bridge this gap, we propose GeM-CoT, aGeneralizable CoT prompting mechanism in Mixed-task scenarios where the type ofinput questions is unknown. GeM-CoT first categorizes the question type andsubsequently samples or constructs demonstrations from the corresponding datapool in an automatic pattern. With this technical design, GeM-CoTsimultaneously enjoys superior generalization capabilities and remarkableperformances on 10 public reasoning tasks and 23 BBH tasks.</description><author>Anni Zou, Zhuosheng Zhang, Hai Zhao, Xiangru Tang</author><pubDate>Tue, 20 Feb 2024 15:27:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06692v3</guid></item></channel></rss>