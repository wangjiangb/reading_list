<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 08 Aug 2023 06:01:22 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>3D Motion Magnification: Visualizing Subtle Motions with Time Varying Radiance Fields</title><link>http://arxiv.org/abs/2308.03757v1</link><description>Motion magnification helps us visualize subtle, imperceptible motion.However, prior methods only work for 2D videos captured with a fixed camera. Wepresent a 3D motion magnification method that can magnify subtle motions fromscenes captured by a moving camera, while supporting novel view rendering. Werepresent the scene with time-varying radiance fields and leverage the Eulerianprinciple for motion magnification to extract and amplify the variation of theembedding of a fixed point over time. We study and validate our proposedprinciple for 3D motion magnification using both implicit and tri-plane-basedradiance fields as our underlying 3D scene representation. We evaluate theeffectiveness of our method on both synthetic and real-world scenes capturedunder various camera setups.</description><author>Brandon Y. Feng, Hadi Alzayer, Michael Rubinstein, William T. Freeman, Jia-Bin Huang</author><pubDate>Mon, 07 Aug 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03757v1</guid></item><item><title>CUTS: A Fully Unsupervised Framework for Medical Image Segmentation</title><link>http://arxiv.org/abs/2209.11359v4</link><description>In this work we introduce CUTS (Contrastive and Unsupervised Training forSegmentation), a fully unsupervised deep learning framework for medical imagesegmentation to better utilize the vast majority of imaging data that is notlabeled or annotated. We utilize self-supervision from pixels and their localneighborhoods in the images themselves. Our unsupervised approach optimizes atraining objective that leverages concepts from contrastive learning andautoencoding. Our framework segments medical images with a novel two-stageapproach without relying on any labeled data at any stage. The first stageinvolves the creation of a "pixel-centered patch" that embeds every pixel alongwith its surrounding patch, using a vector representation in a high-dimensionallatent embedding space. The second stage utilizes diffusion condensation, amulti-scale topological data analysis approach, to dynamically coarse-grainthese embedding vectors at all levels of granularity. The final outcome is aseries of coarse-to-fine segmentations that highlight image structures atvarious scales. In this work, we show successful multi-scale segmentation onnatural images, retinal fundus images, and brain MRI images. Our frameworkdelineates structures and patterns at different scales which, in the cases ofmedical images, may carry distinct information relevant to clinicalinterpretation. Quantitatively, our framework demonstrates improvements rangingfrom 10% to 200% on dice coefficient and Hausdorff distance compared toexisting unsupervised methods across three medical image datasets. As we tacklethe problem of segmenting medical images at multiple meaningful granularitieswithout relying on any label, we hope to demonstrate the possibility tocircumvent tedious and repetitive manual annotations in future practice.</description><author>Chen Liu, Matthew Amodio, Liangbo L. Shen, Feng Gao, Arman Avesta, Sanjay Aneja, Jay C. Wang, Lucian V. Del Priore, Smita Krishnaswamy</author><pubDate>Mon, 07 Aug 2023 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.11359v4</guid></item><item><title>FSD V2: Improving Fully Sparse 3D Object Detection with Virtual Voxels</title><link>http://arxiv.org/abs/2308.03755v1</link><description>LiDAR-based fully sparse architecture has garnered increasing attention.FSDv1 stands out as a representative work, achieving impressive efficacy andefficiency, albeit with intricate structures and handcrafted designs. In thispaper, we present FSDv2, an evolution that aims to simplify the previous FSDv1while eliminating the inductive bias introduced by its handcraftedinstance-level representation, thus promoting better general applicability. Tothis end, we introduce the concept of \textbf{virtual voxels}, which takes overthe clustering-based instance segmentation in FSDv1. Virtual voxels not onlyaddress the notorious issue of the Center Feature Missing problem in fullysparse detectors but also endow the framework with a more elegant andstreamlined approach. Consequently, we develop a suite of components tocomplement the virtual voxel concept, including a virtual voxel encoder, avirtual voxel mixer, and a virtual voxel assignment strategy. Through empiricalvalidation, we demonstrate that the virtual voxel mechanism is functionallysimilar to the handcrafted clustering in FSDv1 while being more general. Weconduct experiments on three large-scale datasets: Waymo Open Dataset,Argoverse 2 dataset, and nuScenes dataset. Our results showcasestate-of-the-art performance on all three datasets, highlighting thesuperiority of FSDv2 in long-range scenarios and its general applicability toachieve competitive performance across diverse scenarios. Moreover, we providecomprehensive experimental analysis to elucidate the workings of FSDv2. Tofoster reproducibility and further research, we have open-sourced FSDv2 athttps://github.com/tusen-ai/SST.</description><author>Lue Fan, Feng Wang, Naiyan Wang, Zhaoxiang Zhang</author><pubDate>Mon, 07 Aug 2023 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03755v1</guid></item><item><title>Quantum algorithms applied to satellite mission planning for Earth observation</title><link>http://arxiv.org/abs/2302.07181v2</link><description>Earth imaging satellites are a crucial part of our everyday lives that enableglobal tracking of industrial activities. Use cases span many applications,from weather forecasting to digital maps, carbon footprint tracking, andvegetation monitoring. However, there are limitations; satellites are difficultto manufacture, expensive to maintain, and tricky to launch into orbit.Therefore, satellites must be employed efficiently. This poses a challengeknown as the satellite mission planning problem, which could be computationallyprohibitive to solve on large scales. However, close-to-optimal algorithms,such as greedy reinforcement learning and optimization algorithms, can oftenprovide satisfactory resolutions. This paper introduces a set of quantumalgorithms to solve the mission planning problem and demonstrate an advantageover the classical algorithms implemented thus far. The problem is formulatedas maximizing the number of high-priority tasks completed on real datasetscontaining thousands of tasks and multiple satellites. This work demonstratesthat through solution-chaining and clustering, optimization and machinelearning algorithms offer the greatest potential for optimal solutions. Thispaper notably illustrates that a hybridized quantum-enhanced reinforcementlearning agent can achieve a completion percentage of 98.5% over high-prioritytasks, significantly improving over the baseline greedy methods with acompletion rate of 75.8%. The results presented in this work pave the way toquantum-enabled solutions in the space industry and, more generally, futuremission planning problems across industries.</description><author>Serge Rainjonneau, Igor Tokarev, Sergei Iudin, Saaketh Rayaprolu, Karan Pinto, Daria Lemtiuzhnikova, Miras Koblan, Egor Barashov, Mo Kordzanganeh, Markus Pflitsch, Alexey Melnikov</author><pubDate>Mon, 07 Aug 2023 18:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07181v2</guid></item><item><title>LLEDA -- Lifelong Self-Supervised Domain Adaptation</title><link>http://arxiv.org/abs/2211.09027v3</link><description>Humans and animals have the ability to continuously learn new informationover their lifetime without losing previously acquired knowledge. However,artificial neural networks struggle with this due to new informationconflicting with old knowledge, resulting in catastrophic forgetting. Thecomplementary learning systems (CLS) theory suggests that the interplay betweenhippocampus and neocortex systems enables long-term and efficient learning inthe mammalian brain, with memory replay facilitating the interaction betweenthese two systems to reduce forgetting. The proposed Lifelong Self-SupervisedDomain Adaptation (LLEDA) framework draws inspiration from the CLS theory andmimics the interaction between two networks: a DA network inspired by thehippocampus that quickly adjusts to changes in data distribution and an SSLnetwork inspired by the neocortex that gradually learns domain-agnostic generalrepresentations. LLEDA's latent replay technique facilitates communicationbetween these two networks by reactivating and replaying the past memory latentrepresentations to stabilise long-term generalisation and retention withoutinterfering with the previously learned information. Extensive experimentsdemonstrate that the proposed method outperforms several other methodsresulting in a long-term adaptation while being less prone to catastrophicforgetting when transferred to new domains.</description><author>Mamatha Thota, Dewei Yi, Georgios Leontidis</author><pubDate>Mon, 07 Aug 2023 18:56:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09027v3</guid></item><item><title>Mask Frozen-DETR: High Quality Instance Segmentation with One GPU</title><link>http://arxiv.org/abs/2308.03747v1</link><description>In this paper, we aim to study how to build a strong instance segmenter withminimal training time and GPUs, as opposed to the majority of currentapproaches that pursue more accurate instance segmenter by building moreadvanced frameworks at the cost of longer training time and higher GPUrequirements. To achieve this, we introduce a simple and general framework,termed Mask Frozen-DETR, which can convert any existing DETR-based objectdetection model into a powerful instance segmentation model. Our method onlyrequires training an additional lightweight mask network that predicts instancemasks within the bounding boxes given by a frozen DETR-based object detector.Remarkably, our method outperforms the state-of-the-art instance segmentationmethod Mask DINO in terms of performance on the COCO test-dev split (55.3% vs.54.7%) while being over 10X times faster to train. Furthermore, all of ourexperiments can be trained using only one Tesla V100 GPU with 16 GB of memory,demonstrating the significant efficiency of our proposed framework.</description><author>Zhanhao Liang, Yuhui Yuan</author><pubDate>Mon, 07 Aug 2023 18:53:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03747v1</guid></item><item><title>OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models</title><link>http://arxiv.org/abs/2308.01390v2</link><description>We introduce OpenFlamingo, a family of autoregressive vision-language modelsranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to producean open-source replication of DeepMind's Flamingo models. On sevenvision-language datasets, OpenFlamingo models average between 80 - 89% ofcorresponding Flamingo performance. This technical report describes our models,training data, hyperparameters, and evaluation suite. We share our models andcode at https://github.com/mlfoundations/open_flamingo.</description><author>Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt</author><pubDate>Mon, 07 Aug 2023 18:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01390v2</guid></item><item><title>The Copycat Perceptron: Smashing Barriers Through Collective Learning</title><link>http://arxiv.org/abs/2308.03743v1</link><description>We characterize the equilibrium properties of a model of $y$ coupled binaryperceptrons in the teacher-student scenario, subject to a suitable learningrule, with an explicit ferromagnetic coupling proportional to the Hammingdistance between the students' weights. In contrast to recent works, we analyzea more general setting in which a thermal noise is present that affects thegeneralization performance of each student. Specifically, in the presence of anonzero temperature, which assigns nonzero probability to configurations thatmisclassify samples with respect to the teacher's prescription, we find thatthe coupling of replicas leads to a shift of the phase diagram to smallervalues of $\alpha$: This suggests that the free energy landscape gets smootheraround the solution with good generalization (i.e., the teacher) at a fixedfraction of reviewed examples, which allows local update algorithms such asSimulated Annealing to reach the solution before the dynamics gets frozen.Finally, from a learning perspective, these results suggest that more students(in this case, with the same amount of data) are able to learn the same rulewhen coupled together with a smaller amount of data.</description><author>Giovanni Catania, Aurélien Decelle, Beatriz Seoane</author><pubDate>Mon, 07 Aug 2023 18:51:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03743v1</guid></item><item><title>Reasoning with Language Model Prompting: A Survey</title><link>http://arxiv.org/abs/2212.09597v6</link><description>Reasoning, as an essential ability for complex problem-solving, can provideback-end support for various real-world applications, such as medicaldiagnosis, negotiation, etc. This paper provides a comprehensive survey ofcutting-edge research on reasoning with language model prompting. We introduceresearch works with comparisons and summaries and provide systematic resourcesto help beginners. We also discuss the potential reasons for emerging suchreasoning abilities and highlight future research directions. Resources areavailable at https://github.com/zjunlp/Prompt4ReasoningPapers (updatedperiodically).</description><author>Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen</author><pubDate>Mon, 07 Aug 2023 18:50:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09597v6</guid></item><item><title>What about translation? New coding system for content analysis on the perception of literary translation around the political transformation in 1989 in Hungary as a classification problem on an unbalanced dataset</title><link>http://arxiv.org/abs/2308.03742v1</link><description>To track trends in the perception of literary translation around thepolitical transformation in 1989 in Hungary, a coding system was developed onthe paragraphs of the 1980-1999 issues of the literary journal Alf\"old. Thispaper describes how we trained BERT models to carry over the coding system tothe 1980-1999 issues of the literary journal Nagyvil\'ag. We use extensivehyperparameter tuning, loss functions robust to label unbalance, 10-foldcross-validation for precise evaluations and a model ensemble for prediction,manual validation on the predict set, a new calibration method to betterpredict label counts for sections of the Nagyvil\'ag corpus, and to study therelations between labels, we construct label relation networks.</description><author>Dalma Galambos, Pál Zsámboki</author><pubDate>Mon, 07 Aug 2023 18:46:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03742v1</guid></item><item><title>Selective Explanations: Leveraging Human Input to Align Explainable AI</title><link>http://arxiv.org/abs/2301.09656v3</link><description>While a vast collection of explainable AI (XAI) algorithms have beendeveloped in recent years, they are often criticized for significant gaps withhow humans produce and consume explanations. As a result, current XAItechniques are often found to be hard to use and lack effectiveness. In thiswork, we attempt to close these gaps by making AI explanations selective -- afundamental property of human explanations -- by selectively presenting asubset from a large set of model reasons based on what aligns with therecipient's preferences. We propose a general framework for generatingselective explanations by leveraging human input on a small sample. Thisframework opens up a rich design space that accounts for different selectivitygoals, types of input, and more. As a showcase, we use a decision-support taskto explore selective explanations based on what the decision-maker wouldconsider relevant to the decision task. We conducted two experimental studiesto examine three out of a broader possible set of paradigms based on ourproposed framework: in Study 1, we ask the participants to provide their owninput to generate selective explanations, with either open-ended orcritique-based input. In Study 2, we show participants selective explanationsbased on input from a panel of similar users (annotators). Our experimentsdemonstrate the promise of selective explanations in reducing over-reliance onAI and improving decision outcomes and subjective perceptions of the AI, butalso paint a nuanced picture that attributes some of these positive effects tothe opportunity to provide one's own input to augment AI explanations. Overall,our work proposes a novel XAI framework inspired by human communicationbehaviors and demonstrates its potentials to encourage future work to betteralign AI explanations with human production and consumption of explanations.</description><author>Vivian Lai, Yiming Zhang, Chacha Chen, Q. Vera Liao, Chenhao Tan</author><pubDate>Mon, 07 Aug 2023 18:40:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.09656v3</guid></item><item><title>A Cost Analysis of Generative Language Models and Influence Operations</title><link>http://arxiv.org/abs/2308.03740v1</link><description>Despite speculation that recent large language models (LLMs) are likely to beused maliciously to improve the quality or scale of influence operations,uncertainty persists regarding the economic value that LLMs offerpropagandists. This research constructs a model of costs facing propagandistsfor content generation at scale and analyzes (1) the potential savings thatLLMs could offer propagandists, (2) the potential deterrent effect ofmonitoring controls on API-accessible LLMs, and (3) the optimal strategy forpropagandists choosing between multiple private and/or open source LLMs whenconducting influence operations. Primary results suggest that LLMs need onlyproduce usable outputs with relatively low reliability (roughly 25%) to offercost savings to propagandists, that the potential reduction in contentgeneration costs can be quite high (up to 70% for a highly reliable model), andthat monitoring capabilities have sharply limited cost imposition effects whenalternative open source models are available. In addition, these resultssuggest that nation-states -- even those conducting many large-scale influenceoperations per year -- are unlikely to benefit economically from trainingcustom LLMs specifically for use in influence operations.</description><author>Micah Musser</author><pubDate>Mon, 07 Aug 2023 18:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03740v1</guid></item><item><title>Randomized algorithms for precise measurement of differentially-private, personalized recommendations</title><link>http://arxiv.org/abs/2308.03735v1</link><description>Personalized recommendations form an important part of today's internetecosystem, helping artists and creators to reach interested users, and helpingusers to discover new and engaging content. However, many users today areskeptical of platforms that personalize recommendations, in part due tohistorically careless treatment of personal data and data privacy. Now,businesses that rely on personalized recommendations are entering a newparadigm, where many of their systems must be overhauled to be privacy-first.In this article, we propose an algorithm for personalized recommendations thatfacilitates both precise and differentially-private measurement. We consideradvertising as an example application, and conduct offline experiments toquantify how the proposed privacy-preserving algorithm affects key metricsrelated to user experience, advertiser value, and platform revenue compared tothe extremes of both (private) non-personalized and non-private, personalizedimplementations.</description><author>Allegra Laro, Yanqing Chen, Hao He, Babak Aghazadeh</author><pubDate>Mon, 07 Aug 2023 18:34:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03735v1</guid></item><item><title>SurvBeX: An explanation method of the machine learning survival models based on the Beran estimator</title><link>http://arxiv.org/abs/2308.03730v1</link><description>An explanation method called SurvBeX is proposed to interpret predictions ofthe machine learning survival black-box models. The main idea behind the methodis to use the modified Beran estimator as the surrogate explanation model.Coefficients, incorporated into Beran estimator, can be regarded as values ofthe feature impacts on the black-box model prediction. Following the well-knownLIME method, many points are generated in a local area around an example ofinterest. For every generated example, the survival function of the black-boxmodel is computed, and the survival function of the surrogate model (the Beranestimator) is constructed as a function of the explanation coefficients. Inorder to find the explanation coefficients, it is proposed to minimize the meandistance between the survival functions of the black-box model and the Beranestimator produced by the generated examples. Many numerical experiments withsynthetic and real survival data demonstrate the SurvBeX efficiency and comparethe method with the well-known method SurvLIME. The method is also comparedwith the method SurvSHAP. The code implementing SurvBeX is available at:https://github.com/DanilaEremenko/SurvBeX</description><author>Lev V. Utkin, Danila Y. Eremenko, Andrei V. Konstantinov</author><pubDate>Mon, 07 Aug 2023 18:18:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03730v1</guid></item><item><title>Tiny LVLM-eHub: Early Multimodal Experiments with Bard</title><link>http://arxiv.org/abs/2308.03729v1</link><description>Recent advancements in Large Vision-Language Models (LVLMs) have demonstratedsignificant progress in tackling complex multimodal tasks. Among thesecutting-edge developments, Google's Bard stands out for its remarkablemultimodal capabilities, promoting comprehensive comprehension and reasoningacross various domains. This work presents an early and holistic evaluation ofLVLMs' multimodal abilities, with a particular focus on Bard, by proposing alightweight variant of LVLM-eHub, named Tiny LVLM-eHub. In comparison to thevanilla version, Tiny LVLM-eHub possesses several appealing properties.Firstly, it provides a systematic assessment of six categories of multimodalcapabilities, including visual perception, visual knowledge acquisition, visualreasoning, visual commonsense, object hallucination, and embodied intelligence,through quantitative evaluation of $42$ standard text-related visualbenchmarks. Secondly, it conducts an in-depth analysis of LVLMs' predictionsusing the ChatGPT Ensemble Evaluation (CEE), which leads to a robust andaccurate evaluation and exhibits improved alignment with human evaluationcompared to the word matching approach. Thirdly, it comprises a mere $2.1$Kimage-text pairs, facilitating ease of use for practitioners to evaluate theirown offline LVLMs. Through extensive experimental analysis, this studydemonstrates that Bard outperforms previous LVLMs in most multimodalcapabilities except object hallucination, to which Bard is still susceptible.Tiny LVLM-eHub serves as a baseline evaluation for various LVLMs and encouragesinnovative strategies aimed at advancing multimodal techniques. Our project ispublicly available at \url{https://github.com/OpenGVLab/Multi-Modality-Arena}.</description><author>Wenqi Shao, Yutao Hu, Peng Gao, Meng Lei, Kaipeng Zhang, Fanqing Meng, Peng Xu, Siyuan Huang, Hongsheng Li, Yu Qiao, Ping Luo</author><pubDate>Mon, 07 Aug 2023 18:17:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03729v1</guid></item><item><title>AdaptiveSAM: Towards Efficient Tuning of SAM for Surgical Scene Segmentation</title><link>http://arxiv.org/abs/2308.03726v1</link><description>Segmentation is a fundamental problem in surgical scene analysis usingartificial intelligence. However, the inherent data scarcity in this domainmakes it challenging to adapt traditional segmentation techniques for thistask. To tackle this issue, current research employs pretrained models andfinetunes them on the given data. Even so, these require training deep networkswith millions of parameters every time new data becomes available. A recentlypublished foundation model, Segment-Anything (SAM), generalizes well to a largevariety of natural images, hence tackling this challenge to a reasonableextent. However, SAM does not generalize well to the medical domain as iswithout utilizing a large amount of compute resources for fine-tuning and usingtask-specific prompts. Moreover, these prompts are in the form ofbounding-boxes or foreground/background points that need to be annotatedexplicitly for every image, making this solution increasingly tedious withhigher data size. In this work, we propose AdaptiveSAM - an adaptivemodification of SAM that can adjust to new datasets quickly and efficiently,while enabling text-prompted segmentation. For finetuning AdaptiveSAM, wepropose an approach called bias-tuning that requires a significantly smallernumber of trainable parameters than SAM (less than 2\%). At the same time,AdaptiveSAM requires negligible expert intervention since it uses free-formtext as prompt and can segment the object of interest with just the label nameas prompt. Our experiments show that AdaptiveSAM outperforms currentstate-of-the-art methods on various medical imaging datasets including surgery,ultrasound and X-ray. Code is available athttps://github.com/JayParanjape/biastuning</description><author>Jay N. Paranjape, Nithin Gopalakrishnan Nair, Shameema Sikder, S. Swaroop Vedula, Vishal M. Patel</author><pubDate>Mon, 07 Aug 2023 18:12:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03726v1</guid></item><item><title>Sliced Optimal Partial Transport</title><link>http://arxiv.org/abs/2212.08049v9</link><description>Optimal transport (OT) has become exceedingly popular in machine learning,data science, and computer vision. The core assumption in the OT problem is theequal total amount of mass in source and target measures, which limits itsapplication. Optimal Partial Transport (OPT) is a recently proposed solution tothis limitation. Similar to the OT problem, the computation of OPT relies onsolving a linear programming problem (often in high dimensions), which canbecome computationally prohibitive. In this paper, we propose an efficientalgorithm for calculating the OPT problem between two non-negative measures inone dimension. Next, following the idea of sliced OT distances, we utilizeslicing to define the sliced OPT distance. Finally, we demonstrate thecomputational and accuracy benefits of the sliced OPT-based method in variousnumerical experiments. In particular, we show an application of our proposedSliced-OPT in noisy point cloud registration.</description><author>Yikun Bai, Berhnard Schmitzer, Mathew Thorpe, Soheil Kolouri</author><pubDate>Mon, 07 Aug 2023 18:09:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08049v9</guid></item><item><title>A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe</title><link>http://arxiv.org/abs/2307.14361v2</link><description>This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, andGloVe to classify gene mutations using Kaggle's Personalized Medicine:Redefining Cancer Treatment dataset. The results were compared againstwell-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, andtheir LSTM ensembles. Our model outperformed all other models in terms ofaccuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, italso needed less training time, resulting in a perfect combination ofperformance and efficiency. This study demonstrates the utility of ensemblemodels for difficult tasks such as gene mutation classification.</description><author>Sanad Aburass, Osama Dorgham, Jamil Al Shaqsi</author><pubDate>Mon, 07 Aug 2023 18:09:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14361v2</guid></item><item><title>Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation</title><link>http://arxiv.org/abs/2308.03725v1</link><description>Temporal Sentence Grounding in Videos (TSGV) aims to detect the eventtimestamps described by the natural language query from untrimmed videos. Thispaper discusses the challenge of achieving efficient computation in TSGV modelswhile maintaining high performance. Most existing approaches exquisitely designcomplex architectures to improve accuracy with extra layers and loss, sufferingfrom inefficiency and heaviness. Although some works have noticed that, theyonly make an issue of feature fusion layers, which can hardly enjoy thehighspeed merit in the whole clunky network. To tackle this problem, we proposea novel efficient multi-teacher model (EMTM) based on knowledge distillation totransfer diverse knowledge from both heterogeneous and isomorphic networks.Specifically, We first unify different outputs of the heterogeneous models intoone single form. Next, a Knowledge Aggregation Unit (KAU) is built to acquirehigh-quality integrated soft labels from multiple teachers. After that, the KAUmodule leverages the multi-scale video and global query information toadaptively determine the weights of different teachers. A Shared Encoderstrategy is then proposed to solve the problem that the student shallow layershardly benefit from teachers, in which an isomorphic teacher is collaborativelytrained with the student to align their hidden states. Extensive experimentalresults on three popular TSGV benchmarks demonstrate that our method is botheffective and efficient without bells and whistles.</description><author>Renjie Liang, Yiming Yang, Hui Lu, Li Li</author><pubDate>Mon, 07 Aug 2023 18:07:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03725v1</guid></item><item><title>LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs</title><link>http://arxiv.org/abs/2308.01157v2</link><description>We show that large language models (LLMs) are remarkably good at working withinterpretable models that decompose complex outcomes into univariategraph-represented components. By adopting a hierarchical approach to reasoning,LLMs can provide comprehensive model-level summaries without ever requiring theentire model to fit in context. This approach enables LLMs to apply theirextensive background knowledge to automate common tasks in data science such asdetecting anomalies that contradict prior knowledge, describing potentialreasons for the anomalies, and suggesting repairs that would remove theanomalies. We use multiple examples in healthcare to demonstrate the utility ofthese new capabilities of LLMs, with particular emphasis on GeneralizedAdditive Models (GAMs). Finally, we present the package $\texttt{TalkToEBM}$ asan open-source LLM-GAM interface.</description><author>Benjamin J. Lengerich, Sebastian Bordt, Harsha Nori, Mark E. Nunnally, Yin Aphinyanaphongs, Manolis Kellis, Rich Caruana</author><pubDate>Mon, 07 Aug 2023 18:06:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01157v2</guid></item><item><title>Predicting Software Performance with Divide-and-Learn</title><link>http://arxiv.org/abs/2306.06651v2</link><description>Predicting the performance of highly configurable software systems is thefoundation for performance testing and quality assurance. To that end, recentwork has been relying on machine/deep learning to model software performance.However, a crucial yet unaddressed challenge is how to cater for the sparsityinherited from the configuration landscape: the influence of configurationoptions (features) and the distribution of data samples are highly sparse. In this paper, we propose an approach based on the concept of'divide-and-learn', dubbed $DaL$. The basic idea is that, to handle samplesparsity, we divide the samples from the configuration landscape into distantdivisions, for each of which we build a regularized Deep Neural Network as thelocal model to deal with the feature sparsity. A newly given configurationwould then be assigned to the right model of division for the final prediction. Experiment results from eight real-world systems and five sets of trainingdata reveal that, compared with the state-of-the-art approaches, $DaL$ performsno worse than the best counterpart on 33 out of 40 cases (within which 26 casesare significantly better) with up to $1.94\times$ improvement on accuracy;requires fewer samples to reach the same/better accuracy; and producingacceptable training overhead. Practically, $DaL$ also considerably improvesdifferent global models when using them as the underlying local models, whichfurther strengthens its flexibility. To promote open science, all the data,code, and supplementary figures of this work can be accessed at our repository:https://github.com/ideas-labo/DaL.</description><author>Jingzhi Gong, Tao Chen</author><pubDate>Mon, 07 Aug 2023 17:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06651v2</guid></item><item><title>Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation</title><link>http://arxiv.org/abs/2308.03723v1</link><description>Clinically deployed segmentation models are known to fail on data outside oftheir training distribution. As these models perform well on most cases, it isimperative to detect out-of-distribution (OOD) images at inference to protectagainst automation bias. This work applies the Mahalanobis distance post hoc tothe bottleneck features of a Swin UNETR model that segments the liver onT1-weighted magnetic resonance imaging. By reducing the dimensions of thebottleneck features with principal component analysis, OOD images were detectedwith high performance and minimal computational load.</description><author>McKell Woodland, Nihil Patel, Mais Al Taie, Joshua P. Yung, Tucker J. Netherton, Ankit B. Patel, Kristy K. Brock</author><pubDate>Mon, 07 Aug 2023 17:58:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03723v1</guid></item><item><title>SEM-GAT: Explainable Semantic Pose Estimation using Learned Graph Attention</title><link>http://arxiv.org/abs/2308.03718v1</link><description>This paper proposes a GNN-based method for exploiting semantics and localgeometry to guide the identification of reliable pointcloud registrationcandidates. Semantic and morphological features of the environment serve as keyreference points for registration, enabling accurate lidar-based poseestimation. Our novel lightweight static graph structure informs ourattention-based keypoint node aggregation GNN network by identifying semanticinstance-based relationships, acting as inductive bias to significantly reducethe computational burden of pointcloud registration. By connecting candidatenodes and exploiting cross-graph attention, we identify confidence scores forall potential registration correspondences, estimating the displacement betweenpointcloud scans. Our pipeline enables introspective analysis of the model'sperformance by correlating it with the individual contributions of localstructures in the environment, providing valuable insights into the system'sbehaviour. We test our method on the KITTI odometry dataset, achievingcompetitive accuracy compared to benchmark methods and a higher tracksmoothness while relying on significantly fewer network parameters.</description><author>Efimia Panagiotaki, Daniele De Martini, Georgi Pramatarov, Matthew Gadd, Lars Kunze</author><pubDate>Mon, 07 Aug 2023 17:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03718v1</guid></item><item><title>Automated Real Time Delineation of Supraclavicular Brachial Plexus in Neck Ultrasonography Videos: A Deep Learning Approach</title><link>http://arxiv.org/abs/2308.03717v1</link><description>Peripheral nerve blocks are crucial to treatment of post-surgical pain andare associated with reduction in perioperative opioid use and hospital stay.Accurate interpretation of sono-anatomy is critical for the success ofultrasound (US) guided peripheral nerve blocks and can be challenging to thenew operators. This prospective study enrolled 227 subjects who weresystematically scanned for supraclavicular and interscalene brachial plexus invarious settings using three different US machines to create a dataset of 227unique videos. In total, 41,000 video frames were annotated by experiencedanaesthesiologists using partial automation with object tracking and activecontour algorithms. Four baseline neural network models were trained on thedataset and their performance was evaluated for object detection andsegmentation tasks. Generalizability of the best suited model was then testedon the datasets constructed from separate US scanners with and withoutfine-tuning. The results demonstrate that deep learning models can be leveragedfor real time segmentation of supraclavicular brachial plexus in neckultrasonography videos with high accuracy and reliability. Model was alsotested for its ability to differentiate between supraclavicular and adjoininginterscalene brachial plexus. The entire dataset has been released publicly forfurther study by the research community.</description><author>Abhay Tyagi, Abhishek Tyagi, Manpreet Kaur, Jayanthi Sivaswami, Richa Aggarwal, Kapil Dev Soni, Anjan Trikha</author><pubDate>Mon, 07 Aug 2023 17:40:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03717v1</guid></item><item><title>Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection</title><link>http://arxiv.org/abs/2308.02029v2</link><description>Thalassemia is a heritable blood disorder which is the outcome of a geneticdefect causing lack of production of hemoglobin polypeptide chains. However,there is less understanding of the precise frequency as well as sharing inthese areas. Knowing about the frequency of thalassemia occurrence anddependable mutations is thus a significant step in preventing, controlling, andtreatment planning. Here, Political Tangent Search Optimizer based TransferLearning (PTSO_TL) is introduced for thalassemia detection. Initially, inputdata obtained from a particular dataset is normalized in the data normalizationstage. Quantile normalization is utilized in the data normalization stage, andthe data are then passed to the feature fusion phase, in which WeightedEuclidean Distance with Deep Maxout Network (DMN) is utilized. Thereafter, dataaugmentation is performed using the oversampling method to increase datadimensionality. Lastly, thalassemia detection is carried out by TL, wherein aconvolutional neural network (CNN) is utilized with hyperparameters from atrained model such as Xception. TL is tuned by PTSO, and the training algorithmPTSO is presented by merging of Political Optimizer (PO) and Tangent SearchAlgorithm (TSA). Furthermore, PTSO_TL obtained maximal precision, recall, andf-measure values of about 94.3%, 96.1%, and 95.2%, respectively.</description><author>Hemn Barzan Abdalla, Awder Ahmed, Guoquan Li, Nasser Mustafa, Abdur Rashid Sangi</author><pubDate>Mon, 07 Aug 2023 17:36:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02029v2</guid></item><item><title>Communication-Efficient Framework for Distributed Image Semantic Wireless Transmission</title><link>http://arxiv.org/abs/2308.03713v1</link><description>Multi-node communication, which refers to the interaction among multipledevices, has attracted lots of attention in many Internet-of-Things (IoT)scenarios. However, its huge amounts of data flows and inflexibility for taskextension have triggered the urgent requirement of communication-efficientdistributed data transmission frameworks. In this paper, inspired by the greatsuperiorities on bandwidth reduction and task adaptation of semanticcommunications, we propose a federated learning-based semantic communication(FLSC) framework for multi-task distributed image transmission with IoTdevices. Federated learning enables the design of independent semanticcommunication link of each user while further improves the semantic extractionand task performance through global aggregation. Each link in FLSC is composedof a hierarchical vision transformer (HVT)-based extractor and a task-adaptivetranslator for coarse-to-fine semantic extraction and meaning translationaccording to specific tasks. In order to extend the FLSC into more realisticconditions, we design a channel state information-based multiple-inputmultiple-output transmission module to combat channel fading and noise.Simulation results show that the coarse semantic information can deal with arange of image-level tasks. Moreover, especially in low signal-to-noise ratioand channel bandwidth ratio regimes, FLSC evidently outperforms the traditionalscheme, e.g. about 10 peak signal-to-noise ratio gain in the 3 dB channelcondition.</description><author>Bingyan Xie, Yongpeng Wu, Yuxuan Shi, Derrick Wing Kwan Ng, Wenjun Zhang</author><pubDate>Mon, 07 Aug 2023 17:32:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03713v1</guid></item><item><title>Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience</title><link>http://arxiv.org/abs/2308.03712v1</link><description>This paper asks whether current self-supervised learning methods, ifsufficiently scaled up, would be able to reach human-level visual objectrecognition capabilities with the same type and amount of visual experiencehumans learn from. Previous work on this question only considered the scalingof data size. Here, we consider the simultaneous scaling of data size, modelsize, and image resolution. We perform a scaling experiment with visiontransformers up to 633M parameters in size (ViT-H/14) trained with up to 5Khours of human-like video data (long, continuous, mostly egocentric videos)with image resolutions of up to 476x476 pixels. The efficiency of maskedautoencoders (MAEs) as a self-supervised learning algorithm makes it possibleto run this scaling experiment on an unassuming academic budget. We find thatit is feasible to reach human-level object recognition capacity at sub-humanscales of model size, data size, and image size, if these factors are scaled upsimultaneously. To give a concrete example, we estimate that a 2.5B parameterViT model trained with 20K hours (2.3 years) of human-like video data with aspatial resolution of 952x952 pixels should be able to reach human-levelaccuracy on ImageNet. Human-level competence is thus achievable for afundamental perceptual capability from human-like perceptual experience(human-like in both amount and type) with extremely generic learning algorithmsand architectures and without any substantive inductive biases.</description><author>A. Emin Orhan</author><pubDate>Mon, 07 Aug 2023 17:31:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03712v1</guid></item><item><title>Prototype Learning for Out-of-Distribution Polyp Segmentation</title><link>http://arxiv.org/abs/2308.03709v1</link><description>Existing polyp segmentation models from colonoscopy images often fail toprovide reliable segmentation results on datasets from different centers,limiting their applicability. Our objective in this study is to create a robustand well-generalized segmentation model named PrototypeLab that can assist inpolyp segmentation. To achieve this, we incorporate various lighting modes suchas White light imaging (WLI), Blue light imaging (BLI), Linked color imaging(LCI), and Flexible spectral imaging color enhancement (FICE) into our newsegmentation model, that learns to create prototypes for each class of objectpresent in the images. These prototypes represent the characteristic featuresof the objects, such as their shape, texture, color. Our model is designed toperform effectively on out-of-distribution (OOD) datasets from multiplecenters. We first generate a coarse mask that is used to learn prototypes forthe main object class, which are then employed to generate the finalsegmentation mask. By using prototypes to represent the main class, ourapproach handles the variability present in the medical images and generalizewell to new data since prototype capture the underlying distribution of thedata. PrototypeLab offers a promising solution with a dice coefficient of$\geq$ 90\% and mIoU $\geq$ 85\% with a near real-time processing speed forpolyp segmentation. It achieved superior performance on OOD datasets comparedto 16 state-of-the-art image segmentation architectures, potentially improvingclinical outcomes. Codes are available athttps://github.com/xxxxx/PrototypeLab.</description><author>Nikhil Kumar Tomar, Debesh Jha, Ulas Bagci</author><pubDate>Mon, 07 Aug 2023 17:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03709v1</guid></item><item><title>DeRisk: An Effective Deep Learning Framework for Credit Risk Prediction over Real-World Financial Data</title><link>http://arxiv.org/abs/2308.03704v1</link><description>Despite the tremendous advances achieved over the past years by deep learningtechniques, the latest risk prediction models for industrial applications stillrely on highly handtuned stage-wised statistical learning tools, such asgradient boosting and random forest methods. Different from images orlanguages, real-world financial data are high-dimensional, sparse, noisy andextremely imbalanced, which makes deep neural network models particularlychallenging to train and fragile in practice. In this work, we propose DeRisk,an effective deep learning risk prediction framework for credit risk predictionon real-world financial data. DeRisk is the first deep risk prediction modelthat outperforms statistical learning approaches deployed in our company'sproduction system. We also perform extensive ablation studies on our method topresent the most critical factors for the empirical success of DeRisk.</description><author>Yancheng Liang, Jiajie Zhang, Hui Li, Xiaochen Liu, Yi Hu, Yong Wu, Jinyao Zhang, Yongyan Liu, Yi Wu</author><pubDate>Mon, 07 Aug 2023 17:22:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03704v1</guid></item><item><title>Video-based Person Re-identification with Long Short-Term Representation Learning</title><link>http://arxiv.org/abs/2308.03703v1</link><description>Video-based person Re-Identification (V-ReID) aims to retrieve specificpersons from raw videos captured by non-overlapped cameras. As a fundamentaltask, it spreads many multimedia and computer vision applications. However, dueto the variations of persons and scenes, there are still many obstacles thatmust be overcome for high performance. In this work, we notice that both thelong-term and short-term information of persons are important for robust videorepresentations. Thus, we propose a novel deep learning framework named LongShort-Term Representation Learning (LSTRL) for effective V-ReID. Morespecifically, to extract long-term representations, we propose aMulti-granularity Appearance Extractor (MAE), in which four granularityappearances are effectively captured across multiple frames. Meanwhile, toextract short-term representations, we propose a Bi-direction Motion Estimator(BME), in which reciprocal motion information is efficiently extracted fromconsecutive frames. The MAE and BME are plug-and-play and can be easilyinserted into existing networks for efficient feature learning. As a result,they significantly improve the feature representation ability for V-ReID.Extensive experiments on three widely used benchmarks show that our proposedapproach can deliver better performances than most state-of-the-arts.</description><author>Xuehu Liu, Pingping Zhang, Huchuan Lu</author><pubDate>Mon, 07 Aug 2023 17:22:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03703v1</guid></item><item><title>Screen-based 3D Subjective Experiment Software</title><link>http://arxiv.org/abs/2308.03698v1</link><description>Recently, widespread 3D graphics (e.g., point clouds and meshes) have drawnconsiderable efforts from academia and industry to assess their perceptualquality by conducting subjective experiments. However, lacking a handy softwarefor 3D subjective experiments complicates the construction of 3D graphicsquality assessment datasets, thus hindering the prosperity of relevant fields.In this paper, we develop a powerful platform with which users can flexiblydesign their 3D subjective methodologies and build high-quality datasets,easing a broad spectrum of 3D graphics subjective quality study. To accuratelyillustrate the perceptual quality differences of 3D stimuli, our software cansimultaneously render the source stimulus and impaired stimulus and allows bothstimuli to respond synchronously to viewer interactions. Compared with amateur3D visualization tool-based or image/video rendering-based schemes, ourapproach embodies typical 3D applications while minimizing cognitive overloadduring subjective experiments. We organized a subjective experiment involving40 participants to verify the validity of the proposed software. Experimentalanalyses demonstrate that subjective tests on our software can producereasonable subjective quality scores of 3D models. All resources in this papercan be found at https://openi.pcl.ac.cn/OpenDatasets/3DQA.</description><author>Songlin Fan, Wei Gao</author><pubDate>Mon, 07 Aug 2023 17:14:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03698v1</guid></item><item><title>Deep Deterministic Policy Gradient for End-to-End Communication Systems without Prior Channel Knowledge</title><link>http://arxiv.org/abs/2305.07448v2</link><description>End-to-End (E2E) learning-based concept has been recently introduced tojointly optimize both the transmitter and the receiver in wirelesscommunication systems. Unfortunately, this E2E learning architecture requires aprior differentiable channel model to jointly train the deep neural networks(DNNs) at the transceivers, which is hardly obtained in practice. This paperaims to solve this issue by developing a deep deterministic policy gradient(DDPG)-based framework. In particular, the proposed solution uses the lossvalue of the receiver DNN as the reward to train the transmitter DNN. Thesimulation results then show that our proposed solution can jointly train thetransmitter and the receiver without requiring the prior channel model. Inaddition, we demonstrate that the proposed DDPG-based solution can achievebetter detection performance compared to the state-of-the-art solutions.</description><author>Bolun Zhang, Nguyen Van Huynh</author><pubDate>Mon, 07 Aug 2023 17:10:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07448v2</guid></item><item><title>Safe Multimodal Communication in Human-Robot Collaboration</title><link>http://arxiv.org/abs/2308.03690v1</link><description>The new industrial settings are characterized by the presence of human androbots that work in close proximity, cooperating in performing the requiredjob. Such a collaboration, however, requires to pay attention to many aspects.Firstly, it is crucial to enable a communication between this two actors thatis natural and efficient. Secondly, the robot behavior must always be compliantwith the safety regulations, ensuring always a safe collaboration. In thispaper, we propose a framework that enables multi-channel communication betweenhumans and robots by leveraging multimodal fusion of voice and gesture commandswhile always respecting safety regulations. The framework is validated througha comparative experiment, demonstrating that, thanks to multimodalcommunication, the robot can extract valuable information for performing therequired task and additionally, with the safety layer, the robot can scale itsspeed to ensure the operator's safety.</description><author>Davide Ferrari, Andrea Pupa, Alberto Signoretti, Cristian Secchi</author><pubDate>Mon, 07 Aug 2023 17:08:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03690v1</guid></item><item><title>AgentBench: Evaluating LLMs as Agents</title><link>http://arxiv.org/abs/2308.03688v1</link><description>Large Language Models (LLMs) are becoming increasingly smart and autonomous,targeting real-world pragmatic missions beyond traditional NLP tasks. As aresult, there has been an urgent need to evaluate LLMs as agents on challengingtasks in interactive environments. We present AgentBench, a multi-dimensionalevolving benchmark that currently consists of 8 distinct environments to assessLLM-as-Agent's reasoning and decision-making abilities in a multi-turnopen-ended generation setting. Our extensive test over 25 LLMs (including APIsand open-sourced models) shows that, while top commercial LLMs present a strongability of acting as agents in complex environments, there is a significantdisparity in performance between them and open-sourced competitors. It alsoserves as a component of an ongoing project with wider coverage and deeperconsideration towards systematic LLM evaluation. Datasets, environments, and anintegrated evaluation package for AgentBench are released athttps://github.com/THUDM/AgentBench</description><author>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang</author><pubDate>Mon, 07 Aug 2023 17:08:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03688v1</guid></item><item><title>Unsupervised machine-learning shock-capturing technique for high-order solvers</title><link>http://arxiv.org/abs/2308.00086v2</link><description>We present a novel unsupervised machine learning shock capturing algorithmbased on Gaussian Mixture Models (GMMs). The proposed GMM sensor demonstratesremarkable accuracy in detecting shocks and is robust across diverse test caseswithout the need for parameter tuning. We compare the GMM-based sensor withstate-of-the-art alternatives. All methods are integrated into a high-ordercompressible discontinuous Galerkin solver where artificial viscosity can bemodulated to capture shocks. Supersonic test cases, including high Reynoldsnumbers, showcase the sensor's performance, demonstrating the sameeffectiveness as fine-tuned state-of-the-art sensors. %The nodal DG aproachallows for potential applications in sub-cell flux-differencing formulations,supersonic feature detection, and mesh refinement. The adaptive nature andability to function without extensive training datasets make this GMM-basedsensor suitable for complex geometries and varied flow configurations. Ourstudy reveals the potential of unsupervised machine learning methods,exemplified by the GMM sensor, to improve the robustness and efficiency ofadvanced CFD codes.</description><author>Andrés Mateo-Gabín, Kenza Tlales, Eusebio Valero, Esteban Ferrer, Gonzalo Rubio</author><pubDate>Mon, 07 Aug 2023 17:04:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00086v2</guid></item><item><title>Almost-sure convergence of iterates and multipliers in stochastic sequential quadratic optimization</title><link>http://arxiv.org/abs/2308.03687v1</link><description>Stochastic sequential quadratic optimization (SQP) methods for solvingcontinuous optimization problems with nonlinear equality constraints haveattracted attention recently, such as for solving large-scale data-fittingproblems subject to nonconvex constraints. However, for a recently proposedsubclass of such methods that is built on the popular stochastic-gradientmethodology from the unconstrained setting, convergence guarantees have beenlimited to the asymptotic convergence of the expected value of a stationaritymeasure to zero. This is in contrast to the unconstrained setting in whichalmost-sure convergence guarantees (of the gradient of the objective to zero)can be proved for stochastic-gradient-based methods. In this paper, newalmost-sure convergence guarantees for the primal iterates, Lagrangemultipliers, and stationarity measures generated by a stochastic SQP algorithmin this subclass of methods are proved. It is shown that the error in theLagrange multipliers can be bounded by the distance of the primal iterate to aprimal stationary point plus the error in the latest stochastic gradientestimate. It is further shown that, subject to certain assumptions, this lattererror can be made to vanish by employing a running average of the Lagrangemultipliers that are computed during the run of the algorithm. The results ofnumerical experiments are provided to demonstrate the proved theoreticalguarantees.</description><author>Frank E. Curtis, Xin Jiang, Qi Wang</author><pubDate>Mon, 07 Aug 2023 17:03:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03687v1</guid></item><item><title>Linear Convergence Bounds for Diffusion Models via Stochastic Localization</title><link>http://arxiv.org/abs/2308.03686v1</link><description>Diffusion models are a powerful method for generating approximate samplesfrom high-dimensional data distributions. Several recent results have providedpolynomial bounds on the convergence rate of such models, assuming$L^2$-accurate score estimators. However, up until now the best known suchbounds were either superlinear in the data dimension or required strongsmoothness assumptions. We provide the first convergence bounds which arelinear in the data dimension (up to logarithmic factors) assuming only finitesecond moments of the data distribution. We show that diffusion models requireat most $\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$ steps toapproximate an arbitrary data distribution on $\mathbb{R}^d$ corrupted withGaussian noise of variance $\delta$ to within $\varepsilon^2$ inKullback--Leibler divergence. Our proof builds on the Girsanov-based methods ofprevious works. We introduce a refined treatment of the error arising from thediscretization of the reverse SDE, which is based on tools from stochasticlocalization.</description><author>Joe Benton, Valentin De Bortoli, Arnaud Doucet, George Deligiannidis</author><pubDate>Mon, 07 Aug 2023 17:01:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03686v1</guid></item><item><title>Learning Concise and Descriptive Attributes for Visual Recognition</title><link>http://arxiv.org/abs/2308.03685v1</link><description>Recent advances in foundation models present new opportunities forinterpretable visual recognition -- one can first query Large Language Models(LLMs) to obtain a set of attributes that describe each class, then applyvision-language models to classify images via these attributes. Pioneering workshows that querying thousands of attributes can achieve performance competitivewith image features. However, our further investigation on 8 datasets revealsthat LLM-generated attributes in a large quantity perform almost the same asrandom words. This surprising finding suggests that significant noise may bepresent in these attributes. We hypothesize that there exist subsets ofattributes that can maintain the classification performance with much smallersizes, and propose a novel learning-to-search method to discover those concisesets of attributes. As a result, on the CUB dataset, our method achievesperformance close to that of massive LLM-generated attributes (e.g., 10kattributes for CUB), yet using only 32 attributes in total to distinguish 200bird species. Furthermore, our new paradigm demonstrates several additionalbenefits: higher interpretability and interactivity for humans, and the abilityto summarize knowledge for a recognition task.</description><author>An Yan, Yu Wang, Yiwu Zhong, Chengyu Dong, Zexue He, Yujie Lu, William Wang, Jingbo Shang, Julian McAuley</author><pubDate>Mon, 07 Aug 2023 17:00:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03685v1</guid></item><item><title>Distributed Dynamic Programming and an O.D.E. Framework of Distributed TD-Learning for Networked Multi-Agent Markov Decision Processes</title><link>http://arxiv.org/abs/2307.16706v2</link><description>The primary objective of this paper is to investigate distributed dynamicprogramming (DP) and distributed temporal difference (TD) learning algorithmsfor networked multi-agent Markov decision problems (MAMDPs). In our study, weadopt a distributed multi-agent framework where individual agents have accessonly to their own rewards, lacking insights into the rewards of other agents.Additionally, each agent has the ability to share its parameters withneighboring agents through a communication network, represented by a graph. Ourcontributions can be summarized in two key points: 1) We introduce a noveldistributed DP, inspired by the averaging consensus method in thecontinuous-time domain. The convergence of this DP is assessed through controltheory perspectives. 2) Building upon the aforementioned DP, we devise a newdistributed TD-learning algorithm and prove its convergence. A standout featureof our proposed distributed DP is its incorporation of two independent dynamicsystems, each with a distinct role. This characteristic sets the stage for anovel distributed TD-learning strategy, the convergence of which can bedirectly established using the Borkar-Meyn theorem.</description><author>Donghwan Lee</author><pubDate>Mon, 07 Aug 2023 16:52:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16706v2</guid></item><item><title>SemOpenAlex: The Scientific Landscape in 26 Billion RDF Triples</title><link>http://arxiv.org/abs/2308.03671v1</link><description>We present SemOpenAlex, an extensive RDF knowledge graph that contains over26 billion triples about scientific publications and their associated entities,such as authors, institutions, journals, and concepts. SemOpenAlex is licensedunder CC0, providing free and open access to the data. We offer the datathrough multiple channels, including RDF dump files, a SPARQL endpoint, and asa data source in the Linked Open Data cloud, complete with resolvable URIs andlinks to other data sources. Moreover, we provide embeddings for knowledgegraph entities using high-performance computing. SemOpenAlex enables a broadrange of use-case scenarios, such as exploratory semantic search via ourwebsite, large-scale scientific impact quantification, and other forms ofscholarly big data analytics within and across scientific disciplines.Additionally, it enables academic recommender systems, such as recommendingcollaborators, publications, and venues, including explainability capabilities.Finally, SemOpenAlex can serve for RDF query optimization benchmarks, creatingscholarly knowledge-guided language models, and as a hub for semanticscientific publishing.</description><author>Michael Färber, David Lamprecht, Johan Krause, Linn Aung, Peter Haase</author><pubDate>Mon, 07 Aug 2023 16:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03671v1</guid></item><item><title>Improving FHB Screening in Wheat Breeding Using an Efficient Transformer Model</title><link>http://arxiv.org/abs/2308.03670v1</link><description>Fusarium head blight is a devastating disease that causes significanteconomic losses annually on small grains. Efficiency, accuracy, and timelydetection of FHB in the resistance screening are critical for wheat and barleybreeding programs. In recent years, various image processing techniques havebeen developed using supervised machine learning algorithms for the earlydetection of FHB. The state-of-the-art convolutional neural network-basedmethods, such as U-Net, employ a series of encoding blocks to create a localrepresentation and a series of decoding blocks to capture the semanticrelations. However, these methods are not often capable of long-range modelingdependencies inside the input data, and their ability to model multi-scaleobjects with significant variations in texture and shape is limited. Visiontransformers as alternative architectures with innate global self-attentionmechanisms for sequence-to-sequence prediction, due to insufficient low-leveldetails, may also limit localization capabilities. To overcome theselimitations, a new Context Bridge is proposed to integrate the localrepresentation capability of the U-Net network in the transformer model. Inaddition, the standard attention mechanism of the original transformer isreplaced with Efficient Self-attention, which is less complicated than otherstate-of-the-art methods. To train the proposed network, 12,000 wheat imagesfrom an FHB-inoculated wheat field at the SDSU research farm in Volga, SD, werecaptured. In addition to healthy and unhealthy plants, these images encompassvarious stages of the disease. A team of expert pathologists annotated theimages for training and evaluating the developed model. As a result, theeffectiveness of the transformer-based method for FHB-disease detection,through extensive experiments across typical tasks for plant imagesegmentation, is demonstrated.</description><author>Babak Azad, Ahmed Abdalla, Kwanghee Won, Ali Mirzakhani Nafchi</author><pubDate>Mon, 07 Aug 2023 16:44:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03670v1</guid></item><item><title>Diffusion Model in Causal Inference with Unmeasured Confounders</title><link>http://arxiv.org/abs/2308.03669v1</link><description>We study how to extend the use of the diffusion model to answer the causalquestion from the observational data under the existence of unmeasuredconfounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) tocapture the causal intervention, a Diffusion-based Causal Model (DCM) wasproposed incorporating the diffusion model to answer the causal questions moreaccurately, assuming that all of the confounders are observed. However,unmeasured confounders in practice exist, which hinders DCM from beingapplicable. To alleviate this limitation of DCM, we propose an extended modelcalled Backdoor Criterion based DCM (BDCM), whose idea is rooted in theBackdoor criterion to find the variables in DAG to be included in the decodingprocess of the diffusion model so that we can extend DCM to the case withunmeasured confounders. Synthetic data experiment demonstrates that ourproposed model captures the counterfactual distribution more precisely than DCMunder the unmeasured confounders.</description><author>Tatsuhiro Shimizu</author><pubDate>Mon, 07 Aug 2023 16:40:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03669v1</guid></item><item><title>Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling</title><link>http://arxiv.org/abs/2304.05365v6</link><description>There is a growing interest in using reinforcement learning (RL) topersonalize sequences of treatments in digital health to support users inadopting healthier behaviors. Such sequential decision-making problems involvedecisions about when to treat and how to treat based on the user's context(e.g., prior activity level, location, etc.). Online RL is a promisingdata-driven approach for this problem as it learns based on each user'shistorical responses and uses that knowledge to personalize these decisions.However, to decide whether the RL algorithm should be included in an``optimized'' intervention for real-world deployment, we must assess the dataevidence indicating that the RL algorithm is actually personalizing thetreatments to its users. Due to the stochasticity in the RL algorithm, one mayget a false impression that it is learning in certain states and using thislearning to provide specific treatments. We use a working definition ofpersonalization and introduce a resampling-based methodology for investigatingwhether the personalization exhibited by the RL algorithm is an artifact of theRL algorithm stochasticity. We illustrate our methodology with a case study byanalyzing the data from a physical activity clinical trial called HeartSteps,which included the use of an online RL algorithm. We demonstrate how ourapproach enhances data-driven truth-in-advertising of algorithm personalizationboth across all users as well as within specific users in the study.</description><author>Susobhan Ghosh, Raphael Kim, Prasidh Chhabria, Raaz Dwivedi, Predrag Klasnja, Peng Liao, Kelly Zhang, Susan Murphy</author><pubDate>Mon, 07 Aug 2023 16:39:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05365v6</guid></item><item><title>Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness</title><link>http://arxiv.org/abs/2308.03666v1</link><description>As researchers strive to narrow the gap between machine intelligence andhuman through the development of artificial intelligence technologies, it isimperative that we recognize the critical importance of trustworthiness inopen-world, which has become ubiquitous in all aspects of daily life foreveryone. However, several challenges may create a crisis of trust in currentartificial intelligence systems that need to be bridged: 1) Insufficientexplanation of predictive results; 2) Inadequate generalization for learningmodels; 3) Poor adaptability to uncertain environments. Consequently, weexplore a neural program to bridge trustworthiness and open-world learning,extending from single-modal to multi-modal scenarios for readers. 1) To enhancedesign-level interpretability, we first customize trustworthy networks withspecific physical meanings; 2) We then design environmental well-beingtask-interfaces via flexible learning regularizers for improving thegeneralization of trustworthy learning; 3) We propose to increase therobustness of trustworthy learning by integrating open-world recognition losseswith agent mechanisms. Eventually, we enhance various trustworthy propertiesthrough the establishment of design-level explainability, environmentalwell-being task-interfaces and open-world recognition programs. These designedopen-world protocols are applicable across a wide range of surroundings, underopen-world multimedia recognition scenarios with significant performanceimprovements observed.</description><author>Shide Du, Zihan Fang, Shiyang Lan, Yanchao Tan, Manuel Günther, Shiping Wang, Wenzhong Guo</author><pubDate>Mon, 07 Aug 2023 16:35:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03666v1</guid></item><item><title>QDax: A Library for Quality-Diversity and Population-based Algorithms with Hardware Acceleration</title><link>http://arxiv.org/abs/2308.03665v1</link><description>QDax is an open-source library with a streamlined and modular API forQuality-Diversity (QD) optimization algorithms in Jax. The library serves as aversatile tool for optimization purposes, ranging from black-box optimizationto continuous control. QDax offers implementations of popular QD,Neuroevolution, and Reinforcement Learning (RL) algorithms, supported byvarious examples. All the implementations can be just-in-time compiled withJax, facilitating efficient execution across multiple accelerators, includingGPUs and TPUs. These implementations effectively demonstrate the framework'sflexibility and user-friendliness, easing experimentation for researchpurposes. Furthermore, the library is thoroughly documented and tested with95\% coverage.</description><author>Felix Chalumeau, Bryan Lim, Raphael Boige, Maxime Allard, Luca Grillotti, Manon Flageat, Valentin Macé, Arthur Flajolet, Thomas Pierrot, Antoine Cully</author><pubDate>Mon, 07 Aug 2023 16:29:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03665v1</guid></item><item><title>Two-stage Early Prediction Framework of Remaining Useful Life for Lithium-ion Batteries</title><link>http://arxiv.org/abs/2308.03664v1</link><description>Early prediction of remaining useful life (RUL) is crucial for effectivebattery management across various industries, ranging from household appliancesto large-scale applications. Accurate RUL prediction improves the reliabilityand maintainability of battery technology. However, existing methods havelimitations, including assumptions of data from the same sensors ordistribution, foreknowledge of the end of life (EOL), and neglect to determinethe first prediction cycle (FPC) to identify the start of the unhealthy stage.This paper proposes a novel method for RUL prediction of Lithium-ion batteries.The proposed framework comprises two stages: determining the FPC using a neuralnetwork-based model to divide the degradation data into distinct health statesand predicting the degradation pattern after the FPC to estimate the remaininguseful life as a percentage. Experimental results demonstrate that the proposedmethod outperforms conventional approaches in terms of RUL prediction.Furthermore, the proposed method shows promise for real-world scenarios,providing improved accuracy and applicability for battery management.</description><author>Dhruv Mittal, Hymalai Bello, Bo Zhou, Mayank Shekhar Jha, Sungho Suh, Paul Lukowicz</author><pubDate>Mon, 07 Aug 2023 16:28:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03664v1</guid></item><item><title>Matrix Completion in Almost-Verification Time</title><link>http://arxiv.org/abs/2308.03661v1</link><description>We give a new framework for solving the fundamental problem of low-rankmatrix completion, i.e., approximating a rank-$r$ matrix $\mathbf{M} \in\mathbb{R}^{m \times n}$ (where $m \ge n$) from random observations. First, weprovide an algorithm which completes $\mathbf{M}$ on $99\%$ of rows and columnsunder no further assumptions on $\mathbf{M}$ from $\approx mr$ samples andusing $\approx mr^2$ time. Then, assuming the row and column spans of$\mathbf{M}$ satisfy additional regularity properties, we show how to boostthis partial completion guarantee to a full matrix completion algorithm byaggregating solutions to regression problems involving the observations. In the well-studied setting where $\mathbf{M}$ has incoherent row and columnspans, our algorithms complete $\mathbf{M}$ to high precision from$mr^{2+o(1)}$ observations in $mr^{3 + o(1)}$ time (omitting logarithmicfactors in problem parameters), improving upon the prior state-of-the-art[JN15] which used $\approx mr^5$ samples and $\approx mr^7$ time. Under anassumption on the row and column spans of $\mathbf{M}$ we introduce (which issatisfied by random subspaces with high probability), our sample complexityimproves to an almost information-theoretically optimal $mr^{1 + o(1)}$, andour runtime improves to $mr^{2 + o(1)}$. Our runtimes have the appealingproperty of matching the best known runtime to verify that a rank-$r$decomposition $\mathbf{U}\mathbf{V}^\top$ agrees with the sampled observations.We also provide robust variants of our algorithms that, given randomobservations from $\mathbf{M} + \mathbf{N}$ with $\|\mathbf{N}\|_{F} \le\Delta$, complete $\mathbf{M}$ to Frobenius norm distance $\approxr^{1.5}\Delta$ in the same runtimes as the noiseless setting. Prior noisymatrix completion algorithms [CP10] only guaranteed a distance of $\approx\sqrt{n}\Delta$.</description><author>Jonathan A. Kelner, Jerry Li, Allen Liu, Aaron Sidford, Kevin Tian</author><pubDate>Mon, 07 Aug 2023 16:24:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03661v1</guid></item><item><title>Fusing VHR Post-disaster Aerial Imagery and LiDAR Data for Roof Classification in the Caribbean using CNNs</title><link>http://arxiv.org/abs/2307.16177v2</link><description>Accurate and up-to-date information on building characteristics is essentialfor vulnerability assessment; however, the high costs and long timeframesassociated with conducting traditional field surveys can be an obstacle toobtaining critical exposure datasets needed for disaster risk management. Inthis work, we leverage deep learning techniques for the automatedclassification of roof characteristics from very high-resolution orthophotosand airborne LiDAR data obtained in Dominica following Hurricane Maria in 2017.We demonstrate that the fusion of multimodal earth observation data performsbetter than using any single data source alone. Using our proposed methods, weachieve F1 scores of 0.93 and 0.92 for roof type and roof materialclassification, respectively. This work is intended to help governments producemore timely building information to improve resilience and disaster response inthe Caribbean.</description><author>Isabelle Tingzon, Nuala Margaret Cowan, Pierre Chrzanowski</author><pubDate>Mon, 07 Aug 2023 16:22:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16177v2</guid></item><item><title>Detecting Spells in Fantasy Literature with a Transformer Based Artificial Intelligence</title><link>http://arxiv.org/abs/2308.03660v1</link><description>Transformer architectures and models have made significant progress inlanguage-based tasks. In this area, is BERT one of the most widely used andfreely available transformer architecture. In our work, we use BERT forcontext-based phrase recognition of magic spells in the Harry Potter novelseries. Spells are a common part of active magic in fantasy novels. Typically,spells are used in a specific context to achieve a supernatural effect. Aseries of investigations were conducted to see if a Transformer architecturecould recognize such phrases based on their context in the Harry Potter saga.For our studies a pre-trained BERT model was used and fine-tuned utilisingdifferent datasets and training methods to identify the searched context. Byconsidering different approaches for sequence classification as well as tokenclassification, it is shown that the context of spells can be recognised.According to our investigations, the examined sequence length for fine-tuningand validation of the model plays a significant role in context recognition.Based on this, we have investigated whether spells have overarching propertiesthat allow a transfer of the neural network models to other fantasy universesas well. The application of our model showed promising results and is worth tobe deepened in subsequent studies.</description><author>Marcel Moravek, Alexander Zender, Andreas Müller</author><pubDate>Mon, 07 Aug 2023 16:20:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03660v1</guid></item><item><title>Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench</title><link>http://arxiv.org/abs/2308.03656v1</link><description>Recently, the community has witnessed the advancement of Large LanguageModels (LLMs), which have shown remarkable performance on various downstreamtasks. Led by powerful models like ChatGPT and Claude, LLMs are revolutionizinghow users engage with software, assuming more than mere tools but intelligentassistants. Consequently, evaluating LLMs' anthropomorphic capabilities becomesincreasingly important in contemporary discourse. Utilizing the emotionappraisal theory from psychology, we propose to evaluate the empathy ability ofLLMs, i.e., how their feelings change when presented with specific situations.After a careful and comprehensive survey, we collect a dataset containing over400 situations that have proven effective in eliciting the eight emotionscentral to our study. Categorizing the situations into 36 factors, we conduct ahuman evaluation involving more than 1,200 subjects worldwide. With the humanevaluation results as references, our evaluation includes five LLMs, coveringboth commercial and open-source models, including variations in model sizes,featuring the latest iterations, such as GPT-4 and LLaMA 2. A conclusion can bedrawn from the results that, despite several misalignments, LLMs can generallyrespond appropriately to certain situations. Nevertheless, they fall short inalignment with the emotional behaviors of human beings and cannot establishconnections between similar situations. Our collected dataset of situations,the human evaluation results, and the code of our testing framework, dubbedEmotionBench, is made publicly in https://github.com/CUHK-ARISE/EmotionBench.We aspire to contribute to the advancement of LLMs regarding better alignmentwith the emotional behaviors of human beings, thereby enhancing their utilityand applicability as intelligent assistants.</description><author>Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu</author><pubDate>Mon, 07 Aug 2023 16:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03656v1</guid></item><item><title>Gradient Sparsification for Efficient Wireless Federated Learning with Differential Privacy</title><link>http://arxiv.org/abs/2304.04164v2</link><description>Federated learning (FL) enables distributed clients to collaboratively traina machine learning model without sharing raw data with each other. However, itsuffers the leakage of private information from uploading models. In addition,as the model size grows, the training latency increases due to limitedtransmission bandwidth and the model performance degrades while usingdifferential privacy (DP) protection. In this paper, we propose a gradientsparsification empowered FL framework over wireless channels, in order toimprove training efficiency without sacrificing convergence performance.Specifically, we first design a random sparsification algorithm to retain afraction of the gradient elements in each client's local training, therebymitigating the performance degradation induced by DP and and reducing thenumber of transmission parameters over wireless channels. Then, we analyze theconvergence bound of the proposed algorithm, by modeling a non-convex FLproblem. Next, we formulate a time-sequential stochastic optimization problemfor minimizing the developed convergence bound, under the constraints oftransmit power, the average transmitting delay, as well as the client's DPrequirement. Utilizing the Lyapunov drift-plus-penalty framework, we develop ananalytical solution to the optimization problem. Extensive experiments havebeen implemented on three real life datasets to demonstrate the effectivenessof our proposed algorithm. We show that our proposed algorithms can fullyexploit the interworking between communication and computation to outperformthe baselines, i.e., random scheduling, round robin and delay-minimizationalgorithms.</description><author>Kang Wei, Jun Li, Chuan Ma, Ming Ding, Feng Shu, Haitao Zhao, Wen Chen, Hongbo Zhu</author><pubDate>Mon, 07 Aug 2023 16:11:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04164v2</guid></item><item><title>FFF: Fragments-Guided Flexible Fitting for Building Complete Protein Structures</title><link>http://arxiv.org/abs/2308.03654v1</link><description>Cryo-electron microscopy (cryo-EM) is a technique for reconstructing the3-dimensional (3D) structure of biomolecules (especially large proteincomplexes and molecular assemblies). As the resolution increases to thenear-atomic scale, building protein structures de novo from cryo-EM mapsbecomes possible. Recently, recognition-based de novo building methods haveshown the potential to streamline this process. However, it cannot build acomplete structure due to the low signal-to-noise ratio (SNR) problem. At thesame time, AlphaFold has led to a great breakthrough in predicting proteinstructures. This has inspired us to combine fragment recognition and structureprediction methods to build a complete structure. In this paper, we propose anew method named FFF that bridges protein structure prediction and proteinstructure recognition with flexible fitting. First, a multi-level recognitionnetwork is used to capture various structural features from the input 3Dcryo-EM map. Next, protein structural fragments are generated using pseudopeptide vectors and a protein sequence alignment method based on theseextracted features. Finally, a complete structural model is constructed usingthe predicted protein fragments via flexible fitting. Based on our benchmarktests, FFF outperforms the baseline methods for building complete proteinstructures.</description><author>Weijie Chen, Xinyan Wang, Yuhang Wang</author><pubDate>Mon, 07 Aug 2023 16:10:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03654v1</guid></item><item><title>WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-guided Procedures</title><link>http://arxiv.org/abs/2308.03652v1</link><description>Accurate catheter tracking is crucial during minimally invasive endovascularprocedures (MIEP), and electromagnetic (EM) tracking is a widely usedtechnology that serves this purpose. However, registration between preoperativeimages and the EM tracking system is often challenging. Existing registrationmethods typically require manual interactions, which can be time-consuming,increase the risk of errors and change the procedural workflow. Althoughseveral registration methods are available for catheter tracking, such asmarker-based and path-based approaches, their limitations can impact theaccuracy of the resulting tracking solution, consequently, the outcome of themedical procedure. This paper introduces a novel automated catheter registration method forEM-guided MIEP. The method utilizes 3D signal temporal analysis, such asDynamic Time Warping (DTW) algorithms, to improve registration accuracy andreliability compared to existing methods. DTW can accurately warp and matchEM-tracked paths to the vessel's centerline, making it particularly suitablefor registration. The introduced registration method is evaluated for accuracyin a vascular phantom using a marker-based registration as the ground truth.The results indicate that the DTW method yields accurate and reliableregistration outcomes, with a mean error of $2.22$mm. The introducedregistration method presents several advantages over state-of-the-art methods,such as high registration accuracy, no initialization required, and increasedautomation.</description><author>Ardit Ramadani, Peter Ewert, Heribert Schunkert, Nassir Navab</author><pubDate>Mon, 07 Aug 2023 16:07:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03652v1</guid></item><item><title>Generative Forests</title><link>http://arxiv.org/abs/2308.03648v1</link><description>Tabular data represents one of the most prevalent form of data. When it comesto data generation, many approaches would learn a density for the datageneration process, but would not necessarily end up with a sampler, even lessso being exact with respect to the underlying density. A second issue is onmodels: while complex modeling based on neural nets thrives in image or textgeneration (etc.), less is known for powerful generative models on tabulardata. A third problem is the visible chasm on tabular data between trainingalgorithms for supervised learning with remarkable properties (e.g. boosting),and a comparative lack of guarantees when it comes to data generation. In thispaper, we tackle the three problems, introducing new tree-based generativemodels convenient for density modeling and tabular data generation that improveon modeling capabilities of recent proposals, and a training algorithm whichsimplifies the training setting of previous approaches and displaysboosting-compliant convergence. This algorithm has the convenient property torely on a supervised training scheme that can be implemented by a few tweaks tothe most popular induction scheme for decision tree induction with two classes.Experiments are provided on missing data imputation and comparing generateddata to real data, displaying the quality of the results obtained by ourapproach, in particular against state of the art.</description><author>Richard Nock, Mathieu Guillame-Bert</author><pubDate>Mon, 07 Aug 2023 15:58:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03648v1</guid></item><item><title>Getting pwn'd by AI: Penetration Testing with Large Language Models</title><link>http://arxiv.org/abs/2308.00121v2</link><description>The field of software security testing, more specifically penetrationtesting, is an activity that requires high levels of expertise and involvesmany manual testing and analysis steps. This paper explores the potential usageof large-language models, such as GPT3.5, to augment penetration testers withAI sparring partners. We explore the feasibility of supplementing penetrationtesters with AI models for two distinct use cases: high-level task planning forsecurity testing assignments and low-level vulnerability hunting within avulnerable virtual machine. For the latter, we implemented a closed-feedbackloop between LLM-generated low-level actions with a vulnerable virtual machine(connected through SSH) and allowed the LLM to analyze the machine state forvulnerabilities and suggest concrete attack vectors which were automaticallyexecuted within the virtual machine. We discuss promising initial results,detail avenues for improvement, and close deliberating on the ethics ofproviding AI-based sparring partners.</description><author>Andreas Happe, Jürgen Cito</author><pubDate>Mon, 07 Aug 2023 15:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00121v2</guid></item><item><title>KITLM: Domain-Specific Knowledge InTegration into Language Models for Question Answering</title><link>http://arxiv.org/abs/2308.03638v1</link><description>Large language models (LLMs) have demonstrated remarkable performance in awide range of natural language tasks. However, as these models continue to growin size, they face significant challenges in terms of computational costs.Additionally, LLMs often lack efficient domain-specific understanding, which isparticularly crucial in specialized fields such as aviation and healthcare. Toboost the domain-specific understanding, we propose, KITLM, a novel knowledgebase integration approach into language model through relevant informationinfusion. By integrating pertinent knowledge, not only the performance of thelanguage model is greatly enhanced, but the model size requirement is alsosignificantly reduced while achieving comparable performance. Our proposedknowledge-infused model surpasses the performance of both GPT-3.5-turbo and thestate-of-the-art knowledge infusion method, SKILL, achieving over 1.5 timesimprovement in exact match scores on the MetaQA. KITLM showed a similarperformance boost in the aviation domain with AeroQA. The drastic performanceimprovement of KITLM over the existing methods can be attributed to theinfusion of relevant knowledge while mitigating noise. In addition, we releasetwo curated datasets to accelerate knowledge infusion research in specializedfields: a) AeroQA, a new benchmark dataset designed for multi-hopquestion-answering within the aviation domain, and b) Aviation Corpus, adataset constructed from unstructured text extracted from the NationalTransportation Safety Board reports. Our research contributes to advancing thefield of domain-specific language understanding and showcases the potential ofknowledge infusion techniques in improving the performance of language modelson question-answering.</description><author>Ankush Agarwal, Sakharam Gawade, Amar Prakash Azad, Pushpak Bhattacharyya</author><pubDate>Mon, 07 Aug 2023 15:42:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03638v1</guid></item><item><title>Implementing Immune Repertoire Models Using Weighted Finite State Machines</title><link>http://arxiv.org/abs/2308.03637v1</link><description>The adaptive immune system's T and B cells can be viewed as large populationsof simple, diverse classifiers. Artificial immune systems (AIS)$\unicode{x2013}$ algorithmic models of T or B cell repertoires$\unicode{x2013}$ are used in both computational biology and natural computingto investigate how the immune system adapts to its changing environments.However, researchers have struggled to build such systems at scale. Forstring-based AISs, finite state machines (FSMs) can store cell repertoires incompressed representations that are orders of magnitude smaller than explicitlystored receptor sets. This strategy allows AISs with billions of receptors tobe generated in a matter of seconds. However, to date, these FSM-based AISshave been unable to deal with multiplicity in input data. Here, we show howweighted FSMs can be used to represent cell repertoires and model immunologicalprocesses like negative and positive selection, while also taking into accountthe multiplicity of input data. We use our method to build simpleimmune-inspired classifier systems that solve various toy problems in anomalydetection, showing how weights can be crucial for both performance androbustness to parameters. Our approach can potentially be extended to increasethe scale of other population-based machine learning algorithms such aslearning classifier systems.</description><author>Gijs Schröder, Inge MN Wortel, Johannes Textor</author><pubDate>Mon, 07 Aug 2023 15:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03637v1</guid></item><item><title>FedOBD: Opportunistic Block Dropout for Efficiently Training Large-scale Neural Networks through Federated Learning</title><link>http://arxiv.org/abs/2208.05174v5</link><description>Large-scale neural networks possess considerable expressive power. They arewell-suited for complex learning tasks in industrial applications. However,large-scale models pose significant challenges for training under the currentFederated Learning (FL) paradigm. Existing approaches for efficient FL trainingoften leverage model parameter dropout. However, manipulating individual modelparameters is not only inefficient in meaningfully reducing the communicationoverhead when training large-scale FL models, but may also be detrimental tothe scaling efforts and model performance as shown by recent research. Toaddress these issues, we propose the Federated Opportunistic Block Dropout(FedOBD) approach. The key novelty is that it decomposes large-scale modelsinto semantic blocks so that FL participants can opportunistically uploadquantized blocks, which are deemed to be significant towards training themodel, to the FL server for aggregation. Extensive experiments evaluatingFedOBD against four state-of-the-art approaches based on multiple real-worlddatasets show that it reduces the overall communication overhead by more than88% compared to the best performing baseline approach, while achieving thehighest test accuracy. To the best of our knowledge, FedOBD is the firstapproach to perform dropout on FL models at the block level rather than at theindividual parameter level.</description><author>Yuanyuan Chen, Zichen Chen, Pengcheng Wu, Han Yu</author><pubDate>Mon, 07 Aug 2023 15:37:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.05174v5</guid></item><item><title>Segmentation Framework for Heat Loss Identification in Thermal Images: Empowering Scottish Retrofitting and Thermographic Survey Companies</title><link>http://arxiv.org/abs/2308.03631v1</link><description>Retrofitting and thermographic survey (TS) companies in Scotland collaboratewith social housing providers to tackle fuel poverty. They employ ground-levelinfrared (IR) camera-based-TSs (GIRTSs) for collecting thermal images toidenti-fy the heat loss sources resulting from poor insulation. However, thisidentifica-tion process is labor-intensive and time-consuming, necessitatingextensive data processing. To automate this, an AI-driven approach isnecessary. Therefore, this study proposes a deep learning (DL)-basedsegmentation framework using the Mask Region Proposal Convolutional NeuralNetwork (Mask RCNN) to validate its applicability to these thermal images. Theobjective of the framework is to au-tomatically identify, and crop heat losssources caused by weak insulation, while also eliminating obstructive objectspresent in those images. By doing so, it min-imizes labor-intensive tasks andprovides an automated, consistent, and reliable solution. To validate theproposed framework, approximately 2500 thermal imag-es were collected incollaboration with industrial TS partner. Then, 1800 repre-sentative imageswere carefully selected with the assistance of experts and anno-tated tohighlight the target objects (TO) to form the final dataset. Subsequently, atransfer learning strategy was employed to train the dataset, progressivelyaug-menting the training data volume and fine-tuning the pre-trained baselineMask RCNN. As a result, the final fine-tuned model achieved a mean averageprecision (mAP) score of 77.2% for segmenting the TO, demonstrating thesignificant po-tential of proposed framework in accurately quantifying energyloss in Scottish homes.</description><author>Md Junayed Hasan, Eyad Elyan, Yijun Yan, Jinchang Ren, Md Mostafa Kamal Sarker</author><pubDate>Mon, 07 Aug 2023 15:36:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03631v1</guid></item><item><title>MedMine: Examining Pre-trained Language Models on Medication Mining</title><link>http://arxiv.org/abs/2308.03629v1</link><description>Automatic medication mining from clinical and biomedical text has become apopular topic due to its real impact on healthcare applications and the recentdevelopment of powerful language models (LMs). However, fully-automaticextraction models still face obstacles to be overcome such that they can bedeployed directly into clinical practice for better impacts. Such obstaclesinclude their imbalanced performances on different entity types and clinicalevents. In this work, we examine current state-of-the-art pre-trained languagemodels (PLMs) on such tasks, via fine-tuning including the monolingual modelMed7 and multilingual large language model (LLM) XLM-RoBERTa. We compare theiradvantages and drawbacks using historical medication mining shared task datasets from n2c2-2018 challenges. We report the findings we get from thesefine-tuning experiments such that they can facilitate future research onaddressing them, for instance, how to combine their outputs, merge such models,or improve their overall accuracy by ensemble learning and data augmentation.MedMine is part of the M3 Initiative \url{https://github.com/HECTA-UoM/M3}</description><author>Haifa Alrdahi, Lifeng Han, Hendrik Šuvalov, Goran Nenadic</author><pubDate>Mon, 07 Aug 2023 15:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03629v1</guid></item><item><title>MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation</title><link>http://arxiv.org/abs/2308.03624v1</link><description>In this paper, we present a novel method for mobile manipulators to performmultiple contact-rich manipulation tasks. While learning-based methods have thepotential to generate actions in an end-to-end manner, they often suffer frominsufficient action accuracy and robustness against noise. On the other hand,classical control-based methods can enhance system robustness, but at the costof extensive parameter tuning. To address these challenges, we presentMOMA-Force, a visual-force imitation method that seamlessly combinesrepresentation learning for perception, imitation learning for complex motiongeneration, and admittance whole-body control for system robustness andcontrollability. MOMA-Force enables a mobile manipulator to learn multiplecomplex contact-rich tasks with high success rates and small contact forces. Ina real household setting, our method outperforms baseline methods in terms oftask success rates. Moreover, our method achieves smaller contact forces andsmaller force variances compared to baseline methods without force imitation.Overall, we offer a promising approach for efficient and robust mobilemanipulation in the real world. Videos and more details can be found on\url{https://visual-force-imitation.github.io}</description><author>Taozheng Yang, Ya Jing, Hongtao Wu, Jiafeng Xu, Kuankuan Sima, Guangzeng Chen, Qie Sima, Tao Kong</author><pubDate>Mon, 07 Aug 2023 15:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03624v1</guid></item><item><title>Democratising AI: Multiple Meanings, Goals, and Methods</title><link>http://arxiv.org/abs/2303.12642v3</link><description>Numerous parties are calling for the democratisation of AI, but the phrase isused to refer to a variety of goals, the pursuit of which sometimes conflict.This paper identifies four kinds of AI democratisation that are commonlydiscussed: (1) the democratisation of AI use, (2) the democratisation of AIdevelopment, (3) the democratisation of AI profits, and (4) the democratisationof AI governance. Numerous goals and methods of achieving each form ofdemocratisation are discussed. The main takeaway from this paper is that AIdemocratisation is a multifarious and sometimes conflicting concept that shouldnot be conflated with improving AI accessibility. If we want to move beyondambiguous commitments to democratising AI, to productive discussions ofconcrete policies and trade-offs, then we need to recognise the principal roleof the democratisation of AI governance in navigating tradeoffs and risksacross decisions around use, development, and profits.</description><author>Elizabeth Seger, Aviv Ovadya, Ben Garfinkel, Divya Siddarth, Allan Dafoe</author><pubDate>Mon, 07 Aug 2023 15:29:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12642v3</guid></item><item><title>Exploring Visual Pre-training for Robot Manipulation: Datasets, Models and Methods</title><link>http://arxiv.org/abs/2308.03620v1</link><description>Visual pre-training with large-scale real-world data has made great progressin recent years, showing great potential in robot learning with pixelobservations. However, the recipes of visual pre-training for robotmanipulation tasks are yet to be built. In this paper, we thoroughlyinvestigate the effects of visual pre-training strategies on robot manipulationtasks from three fundamental perspectives: pre-training datasets, modelarchitectures and training methods. Several significant experimental findingsare provided that are beneficial for robot learning. Further, we propose avisual pre-training scheme for robot manipulation termed Vi-PRoM, whichcombines self-supervised learning and supervised learning. Concretely, theformer employs contrastive learning to acquire underlying patterns fromlarge-scale unlabeled data, while the latter aims learning visual semantics andtemporal dynamics. Extensive experiments on robot manipulations in varioussimulation environments and the real robot demonstrate the superiority of theproposed scheme. Videos and more details can be found on\url{https://explore-pretrain-robot.github.io}.</description><author>Ya Jing, Xuelin Zhu, Xingbin Liu, Qie Sima, Taozheng Yang, Yunhai Feng, Tao Kong</author><pubDate>Mon, 07 Aug 2023 15:24:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03620v1</guid></item><item><title>Adaptive Semi-Supervised Segmentation of Brain Vessels with Ambiguous Labels</title><link>http://arxiv.org/abs/2308.03613v1</link><description>Accurate segmentation of brain vessels is crucial for cerebrovascular diseasediagnosis and treatment. However, existing methods face challenges in capturingsmall vessels and handling datasets that are partially or ambiguouslyannotated. In this paper, we propose an adaptive semi-supervised approach toaddress these challenges. Our approach incorporates innovative techniquesincluding progressive semi-supervised learning, adaptative training strategy,and boundary enhancement. Experimental results on 3DRA datasets demonstrate thesuperiority of our method in terms of mesh-based segmentation metrics. Byleveraging the partially and ambiguously labeled data, which only annotates themain vessels, our method achieves impressive segmentation performance onmislabeled fine vessels, showcasing its potential for clinical applications.</description><author>Fengming Lin, Yan Xia, Nishant Ravikumar, Qiongyao Liu, Michael MacRaild, Alejandro F Frangi</author><pubDate>Mon, 07 Aug 2023 15:16:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03613v1</guid></item><item><title>AvatarVerse: High-quality &amp; Stable 3D Avatar Creation from Text and Pose</title><link>http://arxiv.org/abs/2308.03610v1</link><description>Creating expressive, diverse and high-quality 3D avatars from highlycustomized text descriptions and pose guidance is a challenging task, due tothe intricacy of modeling and texturing in 3D that ensure details and variousstyles (realistic, fictional, etc). We present AvatarVerse, a stable pipelinefor generating expressive high-quality 3D avatars from nothing but textdescriptions and pose guidance. In specific, we introduce a 2D diffusion modelconditioned on DensePose signal to establish 3D pose control of avatars through2D images, which enhances view consistency from partially observed scenarios.It addresses the infamous Janus Problem and significantly stablizes thegeneration process. Moreover, we propose a progressive high-resolution 3Dsynthesis strategy, which obtains substantial improvement over the quality ofthe created 3D avatars. To this end, the proposed AvatarVerse pipeline achieveszero-shot 3D modeling of 3D avatars that are not only more expressive, but alsoin higher quality and fidelity than previous works. Rigorous qualitativeevaluations and user studies showcase AvatarVerse's superiority in synthesizinghigh-fidelity 3D avatars, leading to a new standard in high-quality and stable3D avatar creation. Our project page is: https://avatarverse3d.github.io</description><author>Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu Wang, Li Chen, Chao Long, Feida Zhu, Kang Du, Min Zheng</author><pubDate>Mon, 07 Aug 2023 15:09:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03610v1</guid></item><item><title>Recurrent Self-Supervised Video Denoising with Denser Receptive Field</title><link>http://arxiv.org/abs/2308.03608v1</link><description>Self-supervised video denoising has seen decent progress through the use ofblind spot networks. However, under their blind spot constraints, previousself-supervised video denoising methods suffer from significant informationloss and texture destruction in either the whole reference frame or neighborframes, due to their inadequate consideration of the receptive field. Moreover,the limited number of available neighbor frames in previous methods leads tothe discarding of distant temporal information. Nonetheless, simply adoptingexisting recurrent frameworks does not work, since they easily break theconstraints on the receptive field imposed by self-supervision. In this paper,we propose RDRF for self-supervised video denoising, which not only fullyexploits both the reference and neighbor frames with a denser receptive field,but also better leverages the temporal information from both local and distantneighbor features. First, towards a comprehensive utilization of informationfrom both reference and neighbor frames, RDRF realizes a denser receptive fieldby taking more neighbor pixels along the spatial and temporal dimensions.Second, it features a self-supervised recurrent video denoising framework,which concurrently integrates distant and near-neighbor temporal features. Thisenables long-term bidirectional information aggregation, while mitigating erroraccumulation in the plain recurrent framework. Our method exhibits superiorperformance on both synthetic and real video denoising datasets. Codes will beavailable at https://github.com/Wang-XIaoDingdd/RDRF.</description><author>Zichun Wang, Yulun Zhang, Debing Zhang, Ying Fu</author><pubDate>Mon, 07 Aug 2023 15:09:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03608v1</guid></item><item><title>POAR: Towards Open Vocabulary Pedestrian Attribute Recognition</title><link>http://arxiv.org/abs/2303.14643v2</link><description>Pedestrian attribute recognition (PAR) aims to predict the attributes of atarget pedestrian in a surveillance system. Existing methods address the PARproblem by training a multi-label classifier with predefined attribute classes.However, it is impossible to exhaust all pedestrian attributes in the realworld. To tackle this problem, we develop a novel pedestrian open-attributerecognition (POAR) framework. Our key idea is to formulate the POAR problem asan image-text search problem. We design a Transformer-based image encoder witha masking strategy. A set of attribute tokens are introduced to focus onspecific pedestrian parts (e.g., head, upper body, lower body, feet, etc.) andencode corresponding attributes into visual embeddings. Each attribute categoryis described as a natural language sentence and encoded by the text encoder.Then, we compute the similarity between the visual and text embeddings ofattributes to find the best attribute descriptions for the input images.Different from existing methods that learn a specific classifier for eachattribute category, we model the pedestrian at a part-level and explore thesearching method to handle the unseen attributes. Finally, a many-to-manycontrastive (MTMC) loss with masked tokens is proposed to train the networksince a pedestrian image can comprise multiple attributes. Extensiveexperiments have been conducted on benchmark PAR datasets with anopen-attribute setting. The results verified the effectiveness of the proposedPOAR method, which can form a strong baseline for the POAR task. Our code isavailable at \url{https://github.com/IvyYZ/POAR}.</description><author>Yue Zhang, Suchen Wang, Shichao Kan, Zhenyu Weng, Yigang Cen, Yap-peng Tan</author><pubDate>Mon, 07 Aug 2023 15:08:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14643v2</guid></item><item><title>Negative Lexical Constraints in Neural Machine Translation</title><link>http://arxiv.org/abs/2308.03601v1</link><description>This paper explores negative lexical constraining in English to Czech neuralmachine translation. Negative lexical constraining is used to prohibit certainwords or expressions in the translation produced by the neural translationmodel. We compared various methods based on modifying either the decodingprocess or the training data. The comparison was performed on two tasks:paraphrasing and feedback-based translation refinement. We also studied towhich extent these methods "evade" the constraints presented to the model(usually in the dictionary form) by generating a different surface form of agiven constraint.We propose a way to mitigate the issue through training withstemmed negative constraints to counter the model's ability to induce a varietyof the surface forms of a word that can result in bypassing the constraint. Wedemonstrate that our method improves the constraining, although the problemstill persists in many cases.</description><author>Josef Jon, Dušan Variš, Michal Novák, João Paulo Aires, Ondřej Bojar</author><pubDate>Mon, 07 Aug 2023 15:04:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03601v1</guid></item><item><title>Why We Don't Have AGI Yet</title><link>http://arxiv.org/abs/2308.03598v1</link><description>The original vision of AI was re-articulated in 2002 via the term 'ArtificialGeneral Intelligence' or AGI. This vision is to build 'Thinking Machines' -computer systems that can learn, reason, and solve problems similar to the wayhumans do. This is in stark contrast to the 'Narrow AI' approach practiced byalmost everyone in the field over the many decades. While several large-scaleefforts have nominally been working on AGI (most notably DeepMind), the fieldof pure focused AGI development has not been well funded or promoted. This issurprising given the fantastic value that true AGI can bestow on humanity. Inaddition to the dearth of effort in this field, there are also severaltheoretical and methodical missteps that are hampering progress. We highlightwhy purely statistical approaches are unlikely to lead to AGI, and identifyseveral crucial cognitive abilities required to achieve human-like adaptabilityand autonomous learning. We conclude with a survey of socio-technical factorsthat have undoubtedly slowed progress towards AGI.</description><author>Peter Voss, Mladjan Jovanovic</author><pubDate>Mon, 07 Aug 2023 14:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03598v1</guid></item><item><title>FeatEnHancer: Enhancing Hierarchical Features for Object Detection and Beyond Under Low-Light Vision</title><link>http://arxiv.org/abs/2308.03594v1</link><description>Extracting useful visual cues for the downstream tasks is especiallychallenging under low-light vision. Prior works create enhanced representationsby either correlating visual quality with machine perception or designingillumination-degrading transformation methods that require pre-training onsynthetic datasets. We argue that optimizing enhanced image representationpertaining to the loss of the downstream task can result in more expressiverepresentations. Therefore, in this work, we propose a novel module,FeatEnHancer, that hierarchically combines multiscale features usingmultiheaded attention guided by task-related loss function to create suitablerepresentations. Furthermore, our intra-scale enhancement improves the qualityof features extracted at each scale or level, as well as combines features fromdifferent scales in a way that reflects their relative importance for the taskat hand. FeatEnHancer is a general-purpose plug-and-play module and can beincorporated into any low-light vision pipeline. We show with extensiveexperimentation that the enhanced representation produced with FeatEnHancersignificantly and consistently improves results in several low-light visiontasks, including dark object detection (+5.7 mAP on ExDark), face detection(+1.5 mAPon DARK FACE), nighttime semantic segmentation (+5.1 mIoU on ACDC ),and video object detection (+1.8 mAP on DarkVision), highlighting theeffectiveness of enhancing hierarchical features under low-light vision.</description><author>Khurram Azeem Hashmi, Goutham Kallempudi, Didier Stricker, Muhammamd Zeshan Afzal</author><pubDate>Mon, 07 Aug 2023 14:52:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03594v1</guid></item><item><title>Feature Importance versus Feature Influence and What It Signifies for Explainable AI</title><link>http://arxiv.org/abs/2308.03589v1</link><description>When used in the context of decision theory, feature importance expresses howmuch changing the value of a feature can change the model outcome (or theutility of the outcome), compared to other features. Feature importance shouldnot be confused with the feature influence used by most state-of-the-artpost-hoc Explainable AI methods. Contrary to feature importance, featureinfluence is measured against a reference level or baseline. The ContextualImportance and Utility (CIU) method provides a unified definition of global andlocal feature importance that is applicable also for post-hoc explanations,where the value utility concept provides instance-level assessment of howfavorable or not a feature value is for the outcome. The paper shows how CIUcan be applied to both global and local explainability, assesses the fidelityand stability of different methods, and shows how explanations that usecontextual importance and contextual utility can provide more expressive andflexible explanations than when using influence only.</description><author>Kary Främling</author><pubDate>Mon, 07 Aug 2023 14:46:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03589v1</guid></item><item><title>SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe</title><link>http://arxiv.org/abs/2308.03586v1</link><description>Digital soil mapping (DSM) is an advanced approach that integratesstatistical modeling and cutting-edge technologies, including machine learning(ML) methods, to accurately depict soil properties and their spatialdistribution. Soil organic carbon (SOC) is a crucial soil attribute providingvaluable insights into soil health, nutrient cycling, greenhouse gas emissions,and overall ecosystem productivity. This study highlights the significance ofspatial-temporal deep learning (DL) techniques within the DSM framework. Anovel architecture is proposed, incorporating spatial information using a baseconvolutional neural network (CNN) model and spatial attention mechanism, alongwith climate temporal information using a long short-term memory (LSTM)network, for SOC prediction across Europe. The model utilizes a comprehensiveset of environmental features, including Landsat-8 images, topography, remotesensing indices, and climate time series, as input features. Resultsdemonstrate that the proposed framework outperforms conventional ML approacheslike random forest commonly used in DSM, yielding lower root mean square error(RMSE). This model is a robust tool for predicting SOC and could be applied toother soil properties, thereby contributing to the advancement of DSMtechniques and facilitating land management and decision-making processes basedon accurate information.</description><author>Nafiseh Kakhani, Moien Rangzan, Ali Jamali, Sara Attarchi, Seyed Kazem Alavipanah, Thomas Scholten</author><pubDate>Mon, 07 Aug 2023 14:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03586v1</guid></item><item><title>WIKITIDE: A Wikipedia-Based Timestamped Definition Pairs Dataset</title><link>http://arxiv.org/abs/2308.03582v1</link><description>A fundamental challenge in the current NLP context, dominated by languagemodels, comes from the inflexibility of current architectures to 'learn' newinformation. While model-centric solutions like continual learning orparameter-efficient fine tuning are available, the question still remains ofhow to reliably identify changes in language or in the world. In this paper, wepropose WikiTiDe, a dataset derived from pairs of timestamped definitionsextracted from Wikipedia. We argue that such resource can be helpful foraccelerating diachronic NLP, specifically, for training models able to scanknowledge resources for core updates concerning a concept, an event, or a namedentity. Our proposed end-to-end method is fully automatic, and leverages abootstrapping algorithm for gradually creating a high-quality dataset. Ourresults suggest that bootstrapping the seed version of WikiTiDe leads to betterfine-tuned models. We also leverage fine-tuned models in a number of downstreamtasks, showing promising results with respect to competitive baselines.</description><author>Hsuvas Borkakoty, Luis Espinosa-Anke</author><pubDate>Mon, 07 Aug 2023 14:38:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03582v1</guid></item><item><title>Towards Controllable Natural Language Inference through Lexical Inference Types</title><link>http://arxiv.org/abs/2308.03581v1</link><description>Explainable natural language inference aims to provide a mechanism to produceexplanatory (abductive) inference chains which ground claims to theirsupporting premises. A recent corpus called EntailmentBank strives to advancethis task by explaining the answer to a question using an entailment tree\cite{dalvi2021explaining}. They employ the T5 model to directly generate thetree, which can explain how the answer is inferred. However, it lacks theability to explain and control the generation of intermediate steps, which iscrucial for the multi-hop inference process. % One recent corpus,EntailmentBank, aims to push this task forward by explaining an answer to aquestion according to an entailment tree \cite{dalvi2021explaining}. Theyemploy T5 to generate the tree directly, which can explain how the answer isinferred but cannot explain how the intermediate is generated, which isessential to the multi-hop inference process. In this work, we focus onproposing a controlled natural language inference architecture formulti-premise explanatory inference. To improve control and enable explanatoryanalysis over the generation, we define lexical inference types based onAbstract Meaning Representation (AMR) graph and modify the architecture of T5to learn a latent sentence representation (T5 bottleneck) conditioned on saidtype information. We also deliver a dataset of approximately 5000 annotatedexplanatory inference steps, with well-grounded lexical-symbolic operations.Experimental results indicate that the inference typing induced at the T5bottleneck can help T5 to generate a conclusion under explicit control.</description><author>Yingji Zhang, Danilo S. Carvalho, Ian Pratt-Hartmann, Andre Freitas</author><pubDate>Mon, 07 Aug 2023 14:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03581v1</guid></item><item><title>Revealing the Underlying Patterns: Investigating Dataset Similarity, Performance, and Generalization</title><link>http://arxiv.org/abs/2308.03580v1</link><description>Supervised deep learning models require significant amount of labelled datato achieve an acceptable performance on a specific task. However, when testedon unseen data, the models may not perform well. Therefore, the models need tobe trained with additional and varying labelled data to improve thegeneralization. In this work, our goal is to understand the models, theirperformance and generalization. We establish image-image, dataset-dataset, andimage-dataset distances to gain insights into the model's behavior. Ourproposed distance metric when combined with model performance can help inselecting an appropriate model/architecture from a pool of candidatearchitectures. We have shown that the generalization of these models can beimproved by only adding a small number of unseen images (say 1, 3 or 7) intothe training set. Our proposed approach reduces training and annotation costswhile providing an estimate of model performance on unseen data in dynamicenvironments.</description><author>Akshit Achara, Ram Krishna Pandey</author><pubDate>Mon, 07 Aug 2023 14:35:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03580v1</guid></item><item><title>Generalized Early Stopping in Evolutionary Direct Policy Search</title><link>http://arxiv.org/abs/2308.03574v1</link><description>Lengthy evaluation times are common in many optimization problems such asdirect policy search tasks, especially when they involve conducting evaluationsin the physical world, e.g. in robotics applications. Often, when evaluating asolution over a fixed time period, it becomes clear that the objective valuewill not increase with additional computation time (for example, when atwo-wheeled robot continuously spins on the spot). In such cases, it makessense to stop the evaluation early to save computation time. However, mostapproaches to stop the evaluation are problem-specific and need to bespecifically designed for the task at hand. Therefore, we propose an earlystopping method for direct policy search. The proposed method only looks at theobjective value at each time step and requires no problem-specific knowledge. We test the introduced stopping criterion in five direct policy searchenvironments drawn from games, robotics, and classic control domains, and showthat it can save up to 75% of the computation time. We also compare it withproblem-specific stopping criteria and demonstrate that it performs comparablywhile being more generally applicable.</description><author>Etor Arza, Leni K. Le Goff, Emma Hart</author><pubDate>Mon, 07 Aug 2023 14:25:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03574v1</guid></item><item><title>When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection</title><link>http://arxiv.org/abs/2308.03573v1</link><description>Federated Learning (FL) is a technique that allows multiple participants tocollaboratively train a Deep Neural Network (DNN) without the need ofcentralizing their data. Among other advantages, it comes withprivacy-preserving properties making it attractive for application in sensitivecontexts, such as health care or the military. Although the data are notexplicitly exchanged, the training procedure requires sharing information aboutparticipants' models. This makes the individual models vulnerable to theft orunauthorized distribution by malicious actors. To address the issue ofownership rights protection in the context of Machine Learning (ML), DNNWatermarking methods have been developed during the last five years. Mostexisting works have focused on watermarking in a centralized manner, but only afew methods have been designed for FL and its unique constraints. In thispaper, we provide an overview of recent advancements in Federated Learningwatermarking, shedding light on the new challenges and opportunities that arisein this field.</description><author>Mohammed Lansari, Reda Bellafqira, Katarzyna Kapusta, Vincent Thouvenot, Olivier Bettan, Gouenou Coatrieux</author><pubDate>Mon, 07 Aug 2023 14:24:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03573v1</guid></item><item><title>Provably Efficient Learning in Partially Observable Contextual Bandit</title><link>http://arxiv.org/abs/2308.03572v1</link><description>In this paper, we investigate transfer learning in partially observablecontextual bandits, where agents have limited knowledge from other agents andpartial information about hidden confounders. We first convert the problem toidentifying or partially identifying causal effects between actions and rewardsthrough optimization problems. To solve these optimization problems, wediscretize the original functional constraints of unknown distributions intolinear constraints, and sample compatible causal models via sequentiallysolving linear programmings to obtain causal bounds with the consideration ofestimation error. Our sampling algorithms provide desirable convergence resultsfor suitable sampling distributions. We then show how causal bounds can beapplied to improving classical bandit algorithms and affect the regrets withrespect to the size of action sets and function spaces. Notably, in the taskwith function approximation which allows us to handle general contextdistributions, our method improves the order dependence on function space sizecompared with previous literatures. We formally prove that our causallyenhanced algorithms outperform classical bandit algorithms and achieve ordersof magnitude faster convergence rates. Finally, we perform simulations thatdemonstrate the efficiency of our strategy compared to the currentstate-of-the-art methods. This research has the potential to enhance theperformance of contextual bandit agents in real-world applications where datais scarce and costly to obtain.</description><author>Xueping Gong, Jiheng Zhang</author><pubDate>Mon, 07 Aug 2023 14:24:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03572v1</guid></item><item><title>CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains</title><link>http://arxiv.org/abs/2206.08083v4</link><description>Unsupervised Domain Adaptation demonstrates great potential to mitigatedomain shifts by transferring models from labeled source domains to unlabeledtarget domains. While Unsupervised Domain Adaptation has been applied to a widevariety of complex vision tasks, only few works focus on lane detection forautonomous driving. This can be attributed to the lack of publicly availabledatasets. To facilitate research in these directions, we propose CARLANE, a3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANEencompasses the single-target datasets MoLane and TuLane and the multi-targetdataset MuLane. These datasets are built from three different domains, whichcover diverse scenes and contain a total of 163K unique images, 118K of whichare annotated. In addition we evaluate and report systematic baselines,including our own method, which builds upon Prototypical Cross-domainSelf-supervised Learning. We find that false positive and false negative ratesof the evaluated domain adaptation methods are high compared to those of fullysupervised baselines. This affirms the need for benchmarks such as CARLANE tofurther strengthen research in Unsupervised Domain Adaptation for lanedetection. CARLANE, all evaluated models and the corresponding implementationsare publicly available at https://carlanebenchmark.github.io.</description><author>Julian Gebele, Bonifaz Stuhr, Johann Haselberger</author><pubDate>Mon, 07 Aug 2023 14:24:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.08083v4</guid></item><item><title>In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning</title><link>http://arxiv.org/abs/2307.12375v2</link><description>The performance of Large Language Models (LLMs) on downstream tasks oftenimproves significantly when including examples of the input-label relationshipin the context. However, there is currently no consensus about how thisin-context learning (ICL) ability of LLMs works: for example, while Xie et al.(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b)argue ICL does not even learn label relationships from in-context examples. Inthis paper, we study (1) how labels of in-context examples affect predictions,(2) how label relationships learned during pre-training interact withinput-label examples provided in-context, and (3) how ICL aggregates labelinformation across in-context examples. Our findings suggests LLMs usuallyincorporate information from in-context labels, but that pre-training andin-context label relationships are treated differently, and that the model doesnot consider all in-context information equally. Our results give insights intounderstanding and aligning LLM behavior.</description><author>Jannik Kossen, Tom Rainforth, Yarin Gal</author><pubDate>Mon, 07 Aug 2023 14:22:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12375v2</guid></item><item><title>Partial identification of kernel based two sample tests with mismeasured data</title><link>http://arxiv.org/abs/2308.03570v1</link><description>Nonparametric two-sample tests such as the Maximum Mean Discrepancy (MMD) areoften used to detect differences between two distributions in machine learningapplications. However, the majority of existing literature assumes thaterror-free samples from the two distributions of interest are available.Werelax this assumption and study the estimation of the MMD under$\epsilon$-contamination, where a possibly non-random $\epsilon$ proportion ofone distribution is erroneously grouped with the other. We show that under$\epsilon$-contamination, the typical estimate of the MMD is unreliable.Instead, we study partial identification of the MMD, and characterize sharpupper and lower bounds that contain the true, unknown MMD. We propose a methodto estimate these bounds, and show that it gives estimates that converge to thesharpest possible bounds on the MMD as sample size increases, with aconvergence rate that is faster than alternative approaches. Using threedatasets, we empirically validate that our approach is superior to thealternatives: it gives tight bounds with a low false coverage rate.</description><author>Ron Nafshi, Maggie Makar</author><pubDate>Mon, 07 Aug 2023 14:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03570v1</guid></item><item><title>Topological Interpretations of GPT-3</title><link>http://arxiv.org/abs/2308.03565v1</link><description>This is an experiential study of investigating a consistent method forderiving the correlation between sentence vector and semantic meaning of asentence. We first used three state-of-the-art word/sentence embedding methodsincluding GPT-3, Word2Vec, and Sentence-BERT, to embed plain text sentencestrings into high dimensional spaces. Then we compute the pairwise distancebetween any possible combination of two sentence vectors in an embedding spaceand map them into a matrix. Based on each distance matrix, we compute thecorrelation of distances of a sentence vector with respect to the othersentence vectors in an embedding space. Then we compute the correlation of eachpair of the distance matrices. We observed correlations of the same sentence indifferent embedding spaces and correlations of different sentences in the sameembedding space. These observations are consistent with our hypothesis and takeus to the next stage.</description><author>Tianyi Sun, Bradley Nelson</author><pubDate>Mon, 07 Aug 2023 14:16:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03565v1</guid></item><item><title>Mondrian: Prompt Abstraction Attack Against Large Language Models for Cheaper API Pricing</title><link>http://arxiv.org/abs/2308.03558v1</link><description>The Machine Learning as a Service (MLaaS) market is rapidly expanding andbecoming more mature. For example, OpenAI's ChatGPT is an advanced largelanguage model (LLM) that generates responses for various queries withassociated fees. Although these models can deliver satisfactory performance,they are far from perfect. Researchers have long studied the vulnerabilitiesand limitations of LLMs, such as adversarial attacks and model toxicity.Inevitably, commercial ML models are also not exempt from such issues, whichcan be problematic as MLaaS continues to grow. In this paper, we discover a newattack strategy against LLM APIs, namely the prompt abstraction attack.Specifically, we propose Mondrian, a simple and straightforward method thatabstracts sentences, which can lower the cost of using LLM APIs. In thisapproach, the adversary first creates a pseudo API (with a lower establishedprice) to serve as the proxy of the target API (with a higher establishedprice). Next, the pseudo API leverages Mondrian to modify the user query,obtain the abstracted response from the target API, and forward it back to theend user. Our results show that Mondrian successfully reduces user queries'token length ranging from 13% to 23% across various tasks, including textclassification, generation, and question answering. Meanwhile, these abstractedqueries do not significantly affect the utility of task-specific and generallanguage models like ChatGPT. Mondrian also reduces instruction prompts' tokenlength by at least 11% without compromising output quality. As a result, theprompt abstraction attack enables the adversary to profit without bearing thecost of API development and deployment.</description><author>Wai Man Si, Michael Backes, Yang Zhang</author><pubDate>Mon, 07 Aug 2023 14:10:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03558v1</guid></item><item><title>Realistic Data Enrichment for Robust Image Segmentation in Histopathology</title><link>http://arxiv.org/abs/2304.09534v2</link><description>Poor performance of quantitative analysis in histopathological Whole SlideImages (WSI) has been a significant obstacle in clinical practice. Annotatinglarge-scale WSIs manually is a demanding and time-consuming task, unlikely toyield the expected results when used for fully supervised learning systems.Rarely observed disease patterns and large differences in object scales aredifficult to model through conventional patient intake. Prior methods eitherfall back to direct disease classification, which only requires learning a fewfactors per image, or report on average image segmentation performance, whichis highly biased towards majority observations. Geometric image augmentation iscommonly used to improve robustness for average case predictions and to enrichlimited datasets. So far no method provided sampling of a realistic posteriordistribution to improve stability, e.g. for the segmentation of imbalancedobjects within images. Therefore, we propose a new approach, based on diffusionmodels, which can enrich an imbalanced dataset with plausible examples fromunderrepresented groups by conditioning on segmentation maps. Our method cansimply expand limited clinical datasets making them suitable to train machinelearning pipelines, and provides an interpretable and human-controllable way ofgenerating histopathology images that are indistinguishable from real ones tohuman experts. We validate our findings on two datasets, one from the publicdomain and one from a Kidney Transplant study.</description><author>Sarah Cechnicka, James Ball, Hadrien Reynaud, Callum Arthurs, Candice Roufosse, Bernhard Kainz</author><pubDate>Mon, 07 Aug 2023 14:00:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09534v2</guid></item><item><title>FairGrad: Fairness Aware Gradient Descent</title><link>http://arxiv.org/abs/2206.10923v2</link><description>We address the problem of group fairness in classification, where theobjective is to learn models that do not unjustly discriminate againstsubgroups of the population. Most existing approaches are limited to simplebinary tasks or involve difficult to implement training mechanisms whichreduces their practical applicability. In this paper, we propose FairGrad, amethod to enforce fairness based on a re-weighting scheme that iterativelylearns group specific weights based on whether they are advantaged or not.FairGrad is easy to implement, accommodates various standard fairnessdefinitions, and comes with minimal overhead. Furthermore, we show that it iscompetitive with standard baselines over various datasets including ones usedin natural language processing and computer vision. FairGrad is available as a PyPI package at -https://pypi.org/project/fairgrad</description><author>Gaurav Maheshwari, Michaël Perrot</author><pubDate>Mon, 07 Aug 2023 13:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.10923v2</guid></item><item><title>Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue</title><link>http://arxiv.org/abs/2308.03549v1</link><description>Recent advances in Large Language Models (LLMs) have achieved remarkablebreakthroughs in understanding and responding to user intents. However, theirperformance lag behind general use cases in some expertise domains, such asChinese medicine. Existing efforts to incorporate Chinese medicine into LLMsrely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialoguedata. These models lack the ability for doctor-like proactive inquiry andmulti-turn comprehension and cannot always align responses with safety andprofessionalism experts. In this work, we introduce Zhongjing, the firstChinese medical LLaMA-based LLM that implements an entire training pipelinefrom pre-training to reinforcement learning with human feedback (RLHF).Additionally, we introduce a Chinese multi-turn medical dialogue dataset of70,000 authentic doctor-patient dialogues, CMtMedQA, which significantlyenhances the model's capability for complex dialogue and proactive inquiryinitiation. We define a refined annotation rule and evaluation criteria giventhe biomedical domain's unique characteristics. Results show that our modeloutperforms baselines in various capacities and matches the performance ofChatGPT in a few abilities, despite having 50x training data with previous bestmodel and 100x parameters with ChatGPT. RLHF further improves the model'sinstruction-following ability and safety. We also release our code, datasetsand model for further research.</description><author>Songhua Yang, Hanjia Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, Hongying Zan</author><pubDate>Mon, 07 Aug 2023 13:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03549v1</guid></item><item><title>A Transfer Learning Framework for Proactive Ramp Metering Performance Assessment</title><link>http://arxiv.org/abs/2308.03542v1</link><description>Transportation agencies need to assess ramp metering performance whendeploying or expanding a ramp metering system. The evaluation of a rampmetering strategy is primarily centered around examining its impact on freewaytraffic mobility. One way these effects can be explored is by comparing trafficstates, such as the speed before and after the ramp metering strategy has beenaltered. Predicting freeway traffic states for the after scenarios followingthe implementation of a new ramp metering control strategy could offer valuableinsights into the potential effectiveness of the target strategy. However, theuse of machine learning methods in predicting the freeway traffic state for theafter scenarios and evaluating the effectiveness of transportation policies ortraffic control strategies such as ramp metering is somewhat limited in thecurrent literature. To bridge the research gap, this study presents a frameworkfor predicting freeway traffic parameters (speed, occupancy, and flow rate) forthe after situations when a new ramp metering control strategy is implemented.By learning the association between the spatial-temporal features of trafficstates in before and after situations for known freeway segments, the proposedframework can transfer this learning to predict the traffic parameters for newfreeway segments. The proposed framework is built upon a transfer learningmodel. Experimental results show that the proposed framework is feasible foruse as an alternative for predicting freeway traffic parameters to proactivelyevaluate ramp metering performance.</description><author>Xiaobo Ma, Adrian Cottam, Mohammad Razaur Rahman Shaon, Yao-Jan Wu</author><pubDate>Mon, 07 Aug 2023 13:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03542v1</guid></item><item><title>On-ramp and Off-ramp Traffic Flows Estimation Based on A Data-driven Transfer Learning Framework</title><link>http://arxiv.org/abs/2308.03538v1</link><description>To develop the most appropriate control strategy and monitor, maintain, andevaluate the traffic performance of the freeway weaving areas, state and localDepartments of Transportation need to have access to traffic flows at each pairof on-ramp and off-ramp. However, ramp flows are not always readily availableto transportation agencies and little effort has been made to estimate thesemissing flows in locations where no physical sensors are installed. To bridgethis research gap, a data-driven framework is proposed that can accuratelyestimate the missing ramp flows by solely using data collected from loopdetectors on freeway mainlines. The proposed framework employs a transferlearning model. The transfer learning model relaxes the assumption that theunderlying data distributions of the source and target domains must be thesame. Therefore, the proposed framework can guarantee high-accuracy estimationof on-ramp and off-ramp flows on freeways with different traffic patterns,distributions, and characteristics. Based on the experimental results, the flowestimation mean absolute errors range between 23.90 veh/h to 40.85 veh/h foron-ramps, and 31.58 veh/h to 45.31 veh/h for off-ramps; the flow estimationroot mean square errors range between 34.55 veh/h to 57.77 veh/h for on-ramps,and 41.75 veh/h to 58.80 veh/h for off-ramps. Further, the comparison analysisshows that the proposed framework outperforms other conventional machinelearning models. The estimated ramp flows based on the proposed method can helptransportation agencies to enhance the operations of their ramp controlstrategies for locations where physical sensors are not installed.</description><author>Xiaobo Ma, Abolfazl Karimpour, Yao-Jan Wu</author><pubDate>Mon, 07 Aug 2023 13:36:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03538v1</guid></item><item><title>Machine Unlearning of Features and Labels</title><link>http://arxiv.org/abs/2108.11577v4</link><description>Removing information from a machine learning model is a non-trivial task thatrequires to partially revert the training process. This task is unavoidablewhen sensitive data, such as credit card numbers or passwords, accidentallyenter the model and need to be removed afterwards. Recently, different conceptsfor machine unlearning have been proposed to address this problem. While theseapproaches are effective in removing individual data points, they do not scaleto scenarios where larger groups of features and labels need to be reverted. Inthis paper, we propose the first method for unlearning features and labels. Ourapproach builds on the concept of influence functions and realizes unlearningthrough closed-form updates of model parameters. It enables to adapt theinfluence of training data on a learning model retrospectively, therebycorrecting data leaks and privacy issues. For learning models with stronglyconvex loss functions, our method provides certified unlearning withtheoretical guarantees. For models with non-convex losses, we empirically showthat unlearning features and labels is effective and significantly faster thanother strategies.</description><author>Alexander Warnecke, Lukas Pirch, Christian Wressnegger, Konrad Rieck</author><pubDate>Mon, 07 Aug 2023 13:33:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.11577v4</guid></item><item><title>Measuring Variety, Balance, and Disparity: An Analysis of Media Coverage of the 2021 German Federal Election</title><link>http://arxiv.org/abs/2308.03531v1</link><description>Determining and measuring diversity in news articles is important for anumber of reasons, including preventing filter bubbles and fueling publicdiscourse, especially before elections. So far, the identification and analysisof diversity have been illuminated in a variety of ways, such as measuring theoverlap of words or topics between news articles related to US elections.However, the question of how diversity in news articles can be measuredholistically, i.e., with respect to (1) variety, (2) balance, and (3)disparity, considering individuals, parties, and topics, has not beenaddressed. In this paper, we present a framework for determining diversity innews articles according to these dimensions. Furthermore, we create and providea dataset of Google Top Stories, encompassing more than 26,000 unique headlinesfrom more than 900 news outlets collected within two weeks before and after the2021 German federal election. While we observe high diversity for more generalsearch terms (e.g., "election"), a range of search terms ("education,""Europe," "climate protection," "government") resulted in news articles withhigh diversity in two out of three dimensions. This reflects a more subjective,dedicated discussion on rather future-oriented topics.</description><author>Michael Färber, Jannik Schwade, Adam Jatowt</author><pubDate>Mon, 07 Aug 2023 13:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03531v1</guid></item><item><title>Deep Feature Learning for Wireless Spectrum Data</title><link>http://arxiv.org/abs/2308.03530v1</link><description>In recent years, the traditional feature engineering process for trainingmachine learning models is being automated by the feature extraction layersintegrated in deep learning architectures. In wireless networks, many studieswere conducted in automatic learning of feature representations fordomain-related challenges. However, most of the existing works assume somesupervision along the learning process by using labels to optimize the model.In this paper, we investigate an approach to learning feature representationsfor wireless transmission clustering in a completely unsupervised manner, i.e.requiring no labels in the process. We propose a model based on convolutionalneural networks that automatically learns a reduced dimensionalityrepresentation of the input data with 99.3% less components compared to abaseline principal component analysis (PCA). We show that the automaticrepresentation learning is able to extract fine-grained clusters containing theshapes of the wireless transmission bursts, while the baseline enables onlygeneral separability of the data based on the background noise.</description><author>Ljupcho Milosheski, Gregor Cerar, Blaž Bertalanič, Carolina Fortuna, Mihael Mohorčič</author><pubDate>Mon, 07 Aug 2023 13:27:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03530v1</guid></item><item><title>Feature Decoupling-Recycling Network for Fast Interactive Segmentation</title><link>http://arxiv.org/abs/2308.03529v1</link><description>Recent interactive segmentation methods iteratively take source image, userguidance and previously predicted mask as the input without considering theinvariant nature of the source image. As a result, extracting features from thesource image is repeated in each interaction, resulting in substantialcomputational redundancy. In this work, we propose the FeatureDecoupling-Recycling Network (FDRN), which decouples the modeling componentsbased on their intrinsic discrepancies and then recycles components for eachuser interaction. Thus, the efficiency of the whole interactive process can besignificantly improved. To be specific, we apply the Decoupling-Recyclingstrategy from three perspectives to address three types of discrepancies,respectively. First, our model decouples the learning of source image semanticsfrom the encoding of user guidance to process two types of input domainsseparately. Second, FDRN decouples high-level and low-level features fromstratified semantic representations to enhance feature learning. Third, duringthe encoding of user guidance, current user guidance is decoupled fromhistorical guidance to highlight the effect of current user guidance. Weconduct extensive experiments on 6 datasets from different domains andmodalities, which demonstrate the following merits of our model: 1) superiorefficiency than other methods, particularly advantageous in challengingscenarios requiring long-term interactions (up to 4.25x faster), whileachieving favorable segmentation performance; 2) strong applicability tovarious methods serving as a universal enhancement technique; 3) wellcross-task generalizability, e.g., to medical image segmentation, androbustness against misleading user guidance.</description><author>Huimin Zeng, Weinong Wang, Xin Tao, Zhiwei Xiong, Yu-Wing Tai, Wenjie Pei</author><pubDate>Mon, 07 Aug 2023 13:26:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03529v1</guid></item><item><title>Exploring ChatGPT's Empathic Abilities</title><link>http://arxiv.org/abs/2308.03527v1</link><description>Empathy is often understood as the ability to share and understand anotherindividual's state of mind or emotion. With the increasing use of chatbots invarious domains, e.g., children seeking help with homework, individuals lookingfor medical advice, and people using the chatbot as a daily source of everydaycompanionship, the importance of empathy in human-computer interaction hasbecome more apparent. Therefore, our study investigates the extent to whichChatGPT based on GPT-3.5 can exhibit empathetic responses and emotionalexpressions. We analyzed the following three aspects: (1) understanding andexpressing emotions, (2) parallel emotional response, and (3) empathicpersonality. Thus, we not only evaluate ChatGPT on various empathy aspects andcompare it with human behavior but also show a possible way to analyze theempathy of chatbots in general. Our results show, that in 91.7% of the cases,ChatGPT was able to correctly identify emotions and produces appropriateanswers. In conversations, ChatGPT reacted with a parallel emotion in 70.7% ofcases. The empathic capabilities of ChatGPT were evaluated using a set of fivequestionnaires covering different aspects of empathy. Even though the resultsindicate that the empathic abilities of ChatGPT are still below the average ofhealthy humans, the scores are better than those of people who have beendiagnosed with Asperger syndrome / high-functioning autism.</description><author>Kristina Schaaff, Caroline Reinig, Tim Schlippe</author><pubDate>Mon, 07 Aug 2023 13:23:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03527v1</guid></item><item><title>AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2308.03526v1</link><description>StarCraft II is one of the most challenging simulated reinforcement learningenvironments; it is partially observable, stochastic, multi-agent, andmastering StarCraft II requires strategic planning over long time horizons withreal-time low-level execution. It also has an active professional competitivescene. StarCraft II is uniquely suited for advancing offline RL algorithms,both because of its challenging nature and because Blizzard has released amassive dataset of millions of StarCraft II games played by human players. Thispaper leverages that and establishes a benchmark, called AlphaStar Unplugged,introducing unprecedented challenges for offline reinforcement learning. Wedefine a dataset (a subset of Blizzard's release), tools standardizing an APIfor machine learning methods, and an evaluation protocol. We also presentbaseline agents, including behavior cloning, offline variants of actor-criticand MuZero. We improve the state of the art of agents using only offline data,and we achieve 90% win rate against previously published AlphaStar behaviorcloning agent.</description><author>Michaël Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Richard Powell, Konrad Żołna, Julian Schrittwieser, David Choi, Petko Georgiev, Daniel Toyama, Aja Huang, Roman Ring, Igor Babuschkin, Timo Ewalds, Mahyar Bordbar, Sarah Henderson, Sergio Gómez Colmenarejo, Aäron van den Oord, Wojciech Marian Czarnecki, Nando de Freitas, Oriol Vinyals</author><pubDate>Mon, 07 Aug 2023 13:21:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03526v1</guid></item><item><title>Vocab-Expander: A System for Creating Domain-Specific Vocabularies Based on Word Embeddings</title><link>http://arxiv.org/abs/2308.03519v1</link><description>In this paper, we propose Vocab-Expander at https://vocab-expander.com, anonline tool that enables end-users (e.g., technology scouts) to create andexpand a vocabulary of their domain of interest. It utilizes an ensemble ofstate-of-the-art word embedding techniques based on web text and ConceptNet, acommon-sense knowledge base, to suggest related terms for already given terms.The system has an easy-to-use interface that allows users to quickly confirm orreject term suggestions. Vocab-Expander offers a variety of potential usecases, such as improving concept-based information retrieval in technology andinnovation management, enhancing communication and collaboration withinorganizations or interdisciplinary projects, and creating vocabularies forspecific courses in education.</description><author>Michael Färber, Nicholas Popovic</author><pubDate>Mon, 07 Aug 2023 13:13:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03519v1</guid></item><item><title>IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer</title><link>http://arxiv.org/abs/2307.14863v2</link><description>Advanced image tampering techniques are increasingly challenging thetrustworthiness of multimedia, leading to the development of Image ManipulationLocalization (IML). But what makes a good IML model? The answer lies in the wayto capture artifacts. Exploiting artifacts requires the model to extractnon-semantic discrepancies between manipulated and authentic regions,necessitating explicit comparisons between the two areas. With theself-attention mechanism, naturally, the Transformer should be a bettercandidate to capture artifacts. However, due to limited datasets, there iscurrently no pure ViT-based approach for IML to serve as a benchmark, and CNNsdominate the entire task. Nevertheless, CNNs suffer from weak long-range andnon-semantic modeling. To bridge this gap, based on the fact that artifacts aresensitive to image resolution, amplified under multi-scale features, andmassive at the manipulation border, we formulate the answer to the formerquestion as building a ViT with high-resolution capacity, multi-scale featureextraction capability, and manipulation edge supervision that could convergewith a small amount of data. We term this simple but effective ViT paradigmIML-ViT, which has significant potential to become a new benchmark for IML.Extensive experiments on five benchmark datasets verified our model outperformsthe state-of-the-art manipulation localization methods.Code and models areavailable at \url{https://github.com/SunnyHaze/IML-ViT}.</description><author>Xiaochen Ma, Bo Du, Zhuohang Jiang, Ahmed Y. Al Hammadi, Jizhe Zhou</author><pubDate>Mon, 07 Aug 2023 13:13:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14863v2</guid></item><item><title>Keyword Spotting Simplified: A Segmentation-Free Approach using Character Counting and CTC re-scoring</title><link>http://arxiv.org/abs/2308.03515v1</link><description>Recent advances in segmentation-free keyword spotting treat this problemw.r.t. an object detection paradigm and borrow from state-of-the-art detectionsystems to simultaneously propose a word bounding box proposal mechanism andcompute a corresponding representation. Contrary to the norm of such methodsthat rely on complex and large DNN models, we propose a novel segmentation-freesystem that efficiently scans a document image to find rectangular areas thatinclude the query information. The underlying model is simple and compact,predicting character occurrences over rectangular areas through an implicitlylearned scale map, trained on word-level annotated images. The proposeddocument scanning is then performed using this character counting in acost-effective manner via integral images and binary search. Finally, theretrieval similarity by character counting is refined by a pyramidalrepresentation and a CTC-based re-scoring algorithm, fully utilizing thetrained CNN model. Experimental validation on two widely-used datasets showsthat our method achieves state-of-the-art results outperforming the morecomplex alternatives, despite the simplicity of the underlying model.</description><author>George Retsinas, Giorgos Sfikas, Christophoros Nikou</author><pubDate>Mon, 07 Aug 2023 13:11:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03515v1</guid></item><item><title>Worker Activity Recognition in Manufacturing Line Using Near-body Electric Field</title><link>http://arxiv.org/abs/2308.03514v1</link><description>Manufacturing industries strive to improve production efficiency and productquality by deploying advanced sensing and control systems. Wearable sensors areemerging as a promising solution for achieving this goal, as they can providecontinuous and unobtrusive monitoring of workers' activities in themanufacturing line. This paper presents a novel wearable sensing prototype thatcombines IMU and body capacitance sensing modules to recognize workeractivities in the manufacturing line. To handle these multimodal sensor data,we propose and compare early, and late sensor data fusion approaches formulti-channel time-series convolutional neural networks and deep convolutionalLSTM. We evaluate the proposed hardware and neural network model by collectingand annotating sensor data using the proposed sensing prototype and AppleWatches in the testbed of the manufacturing line. Experimental resultsdemonstrate that our proposed methods achieve superior performance compared tothe baseline methods, indicating the potential of the proposed approach forreal-world applications in manufacturing industries. Furthermore, the proposedsensing prototype with a body capacitive sensor and feature fusion methodimproves by 6.35%, yielding a 9.38% higher macro F1 score than the proposedsensing prototype without a body capacitive sensor and Apple Watch data,respectively.</description><author>Sungho Suh, Vitor Fortes Rey, Sizhen Bian, Yu-Chi Huang, Jože M. Rožanec, Hooman Tavakoli Ghinani, Bo Zhou, Paul Lukowicz</author><pubDate>Mon, 07 Aug 2023 13:10:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03514v1</guid></item><item><title>TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT</title><link>http://arxiv.org/abs/2307.08674v3</link><description>Tables are prevalent in real-world databases, requiring significant time andeffort for humans to analyze and manipulate. The advancements in large languagemodels (LLMs) have made it possible to interact with tables using naturallanguage input, bringing this capability closer to reality. In this paper, wepresent TableGPT, a unified fine-tuned framework that enables LLMs tounderstand and operate on tables using external functional commands. Itintroduces the capability to seamlessly interact with tables, enabling a widerange of functionalities such as question answering, data manipulation (e.g.,insert, delete, query, and modify operations), data visualization, analysisreport generation, and automated prediction. TableGPT aims to provideconvenience and accessibility to users by empowering them to effortlesslyleverage tabular data. At the core of TableGPT lies the novel concept of globaltabular representations, which empowers LLMs to gain a comprehensiveunderstanding of the entire table beyond meta-information. By jointly trainingLLMs on both table and text modalities, TableGPT achieves a deep understandingof tabular data and the ability to perform complex operations on tables throughchain-of-command instructions. Importantly, TableGPT offers the advantage ofbeing a self-contained system rather than relying on external API interfaces.Moreover, it supports efficient data process flow, query rejection (whenappropriate) and private deployment, enabling faster domain data fine-tuningand ensuring data privacy, which enhances the framework's adaptability tospecific use cases.</description><author>Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang Li, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou, Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye, Yali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, Junbo Zhao</author><pubDate>Mon, 07 Aug 2023 13:08:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08674v3</guid></item><item><title>On the Within-Group Fairness of Screening Classifiers</title><link>http://arxiv.org/abs/2302.00025v2</link><description>Screening classifiers are increasingly used to identify qualified candidatesin a variety of selection processes. In this context, it has been recentlyshown that, if a classifier is calibrated, one can identify the smallest set ofcandidates which contains, in expectation, a desired number of qualifiedcandidates using a threshold decision rule. This lends support to focusing oncalibration as the only requirement for screening classifiers. In this paper,we argue that screening policies that use calibrated classifiers may sufferfrom an understudied type of within-group unfairness -- they may unfairly treatqualified members within demographic groups of interest. Further, we argue thatthis type of unfairness can be avoided if classifiers satisfy within-groupmonotonicity, a natural monotonicity property within each of the groups. Then,we introduce an efficient post-processing algorithm based on dynamicprogramming to minimally modify a given calibrated classifier so that itsprobability estimates satisfy within-group monotonicity. We validate ouralgorithm using US Census survey data and show that within-group monotonicitycan be often achieved at a small cost in terms of prediction granularity andshortlist size.</description><author>Nastaran Okati, Stratis Tsirtsis, Manuel Gomez Rodriguez</author><pubDate>Mon, 07 Aug 2023 13:06:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00025v2</guid></item></channel></rss>