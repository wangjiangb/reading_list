<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 16 Jul 2024 13:00:20 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion</title><link>http://arxiv.org/abs/2407.10973v1</link><description>Can we generate a control policy for an agent using just one demonstration ofdesired behaviors as a prompt, as effortlessly as creating an image from atextual description? In this paper, we present Make-An-Agent, a novel policyparameter generator that leverages the power of conditional diffusion modelsfor behavior-to-policy generation. Guided by behavior embeddings that encodetrajectory information, our policy generator synthesizes latent parameterrepresentations, which can then be decoded into policy networks. Trained onpolicy network checkpoints and their corresponding trajectories, our generationmodel demonstrates remarkable versatility and scalability on multiple tasks andhas a strong generalization ability on unseen tasks to output well-performedpolicies with only few-shot demonstrations as inputs. We showcase its efficacyand efficiency on various domains and tasks, including varying objectives,behaviors, and even across different robot manipulators. Beyond simulation, wedirectly deploy policies generated by Make-An-Agent onto real-world robots onlocomotion tasks.</description><author>Yongyuan Liang, Tingqiang Xu, Kaizhe Hu, Guangqi Jiang, Furong Huang, Huazhe Xu</author><pubDate>Mon, 15 Jul 2024 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10973v1</guid></item><item><title>VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation</title><link>http://arxiv.org/abs/2407.10972v1</link><description>In the realm of vision models, the primary mode of representation is usingpixels to rasterize the visual world. Yet this is not always the best or uniqueway to represent visual content, especially for designers and artists whodepict the world using geometry primitives such as polygons. Vector graphics(VG), on the other hand, offer a textual representation of visual content,which can be more concise and powerful for content like cartoons or sketches.Recent studies have shown promising results on processing vector graphics withcapable Large Language Models (LLMs). However, such works focus solely onqualitative results, understanding, or a specific type of vector graphics. Wepropose VGBench, a comprehensive benchmark for LLMs on handling vector graphicsthrough diverse aspects, including (a) both visual understanding andgeneration, (b) evaluation of various vector graphics formats, (c) diversequestion types, (d) wide range of prompting techniques, (e) under multipleLLMs. Evaluating on our collected 4279 understanding and 5845 generationsamples, we find that LLMs show strong capability on both aspects whileexhibiting less desirable performance on low-level formats (SVG). Both data andevaluation pipeline will be open-sourced at https://vgbench.github.io.</description><author>Bocheng Zou, Mu Cai, Jianrui Zhang, Yong Jae Lee</author><pubDate>Mon, 15 Jul 2024 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10972v1</guid></item><item><title>Walking the Values in Bayesian Inverse Reinforcement Learning</title><link>http://arxiv.org/abs/2407.10971v1</link><description>The goal of Bayesian inverse reinforcement learning (IRL) is recovering aposterior distribution over reward functions using a set of demonstrations froman expert optimizing for a reward unknown to the learner. The resultingposterior over rewards can then be used to synthesize an apprentice policy thatperforms well on the same or a similar task. A key challenge in Bayesian IRL isbridging the computational gap between the hypothesis space of possible rewardsand the likelihood, often defined in terms of Q values: vanilla Bayesian IRLneeds to solve the costly forward planning problem - going from rewards to theQ values - at every step of the algorithm, which may need to be done thousandsof times. We propose to solve this by a simple change: instead of focusing onprimarily sampling in the space of rewards, we can focus on primarily workingin the space of Q-values, since the computation required to go from Q-values toreward is radically cheaper. Furthermore, this reversion of the computationmakes it easy to compute the gradient allowing efficient sampling usingHamiltonian Monte Carlo. We propose ValueWalk - a new Markov chain Monte Carlomethod based on this insight - and illustrate its advantages on several tasks.</description><author>Ondrej Bajgar, Alessandro Abate, Konstantinos Gatsis, Michael A. Osborne</author><pubDate>Mon, 15 Jul 2024 17:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10971v1</guid></item><item><title>Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</title><link>http://arxiv.org/abs/2407.10969v1</link><description>We introduce, Q-Sparse, a simple yet effective approach to trainingsparsely-activated large language models (LLMs). Q-Sparse enables full sparsityof activations in LLMs which can bring significant efficiency gains ininference. This is achieved by applying top-K sparsification to the activationsand the straight-through-estimator to the training. The key results from thiswork are, (1) Q-Sparse can achieve results comparable to those of baseline LLMswhile being much more efficient at inference time; (2) We present aninference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse iseffective in different settings, including training-from-scratch,continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works forboth full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, thesynergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides thecornerstone and a clear path to revolutionize the efficiency, including costand energy consumption, of future LLMs.</description><author>Hongyu Wang, Shuming Ma, Ruiping Wang, Furu Wei</author><pubDate>Mon, 15 Jul 2024 17:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10969v1</guid></item><item><title>BECAUSE: Bilinear Causal Representation for Generalizable Offline Model-based Reinforcement Learning</title><link>http://arxiv.org/abs/2407.10967v1</link><description>Offline model-based reinforcement learning (MBRL) enhances data efficiency byutilizing pre-collected datasets to learn models and policies, especially inscenarios where exploration is costly or infeasible. Nevertheless, itsperformance often suffers from the objective mismatch between model and policylearning, resulting in inferior performance despite accurate model predictions.This paper first identifies the primary source of this mismatch comes from theunderlying confounders present in offline data for MBRL. Subsequently, weintroduce \textbf{B}ilin\textbf{E}ar \textbf{CAUS}alr\textbf{E}presentation~(BECAUSE), an algorithm to capture causalrepresentation for both states and actions to reduce the influence of thedistribution shift, thus mitigating the objective mismatch problem.Comprehensive evaluations on 18 tasks that vary in data quality and environmentcontext demonstrate the superior performance of BECAUSE over existing offlineRL algorithms. We show the generalizability and robustness of BECAUSE underfewer samples or larger numbers of confounders. Additionally, we offertheoretical analysis of BECAUSE to prove its error bound and sample efficiencywhen integrating causal representation into offline MBRL.</description><author>Haohong Lin, Wenhao Ding, Jian Chen, Laixi Shi, Jiacheng Zhu, Bo Li, Ding Zhao</author><pubDate>Mon, 15 Jul 2024 17:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10967v1</guid></item><item><title>No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations</title><link>http://arxiv.org/abs/2407.10964v1</link><description>This paper introduces FUNGI, Features from UNsupervised GradIents, a methodto enhance the features of vision encoders by leveraging self-supervisedgradients. Our method is simple: given any pretrained model, we first computegradients from various self-supervised objectives for each input. These areprojected to a lower dimension and then concatenated with the model'sembedding. The resulting features are evaluated on k-nearest neighborclassification over 11 datasets from vision, 5 from natural languageprocessing, and 2 from audio. Across backbones spanning various sizes andpretraining strategies, FUNGI features provide consistent performanceimprovements over the embeddings. We also show that using FUNGI features canbenefit linear classification and image retrieval, and that they significantlyimprove the retrieval-based in-context scene understanding abilities ofpretrained models, for example improving upon DINO by +17% for semanticsegmentation - without any training.</description><author>Walter Simoncini, Spyros Gidaris, Andrei Bursuc, Yuki M. Asano</author><pubDate>Mon, 15 Jul 2024 17:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10964v1</guid></item><item><title>Multi-Attention Integrated Deep Learning Frameworks for Enhanced Breast Cancer Segmentation and Identification</title><link>http://arxiv.org/abs/2407.02844v3</link><description>Breast cancer poses a profound threat to lives globally, claiming numerouslives each year. Therefore, timely detection is crucial for early interventionand improved chances of survival. Accurately diagnosing and classifying breasttumors using ultrasound images is a persistent challenge in medicine, demandingcutting-edge solutions for improved treatment strategies. This researchintroduces multiattention-enhanced deep learning (DL) frameworks designed forthe classification and segmentation of breast cancer tumors from ultrasoundimages. A spatial channel attention mechanism is proposed for segmenting tumorsfrom ultrasound images, utilizing a novel LinkNet DL framework with anInceptionResNet backbone. Following this, the paper proposes a deepconvolutional neural network with an integrated multi-attention framework(DCNNIMAF) to classify the segmented tumor as benign, malignant, or normal.From experimental results, it is observed that the segmentation model hasrecorded an accuracy of 98.1%, with a minimal loss of 0.6%. It has alsoachieved high Intersection over Union (IoU) and Dice Coefficient scores of96.9% and 97.2%, respectively. Similarly, the classification model has attainedan accuracy of 99.2%, with a low loss of 0.31%. Furthermore, the classificationframework has achieved outstanding F1-Score, precision, and recall values of99.1%, 99.3%, and 99.1%, respectively. By offering a robust framework for earlydetection and accurate classification of breast cancer, this proposed worksignificantly advances the field of medical image analysis, potentiallyimproving diagnostic precision and patient outcomes.</description><author>Pandiyaraju V, Shravan Venkatraman, Pavan Kumar S, Santhosh Malarvannan, Kannan A</author><pubDate>Mon, 15 Jul 2024 17:55:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02844v3</guid></item><item><title>Fast Matrix Multiplications for Lookup Table-Quantized LLMs</title><link>http://arxiv.org/abs/2407.10960v1</link><description>The deployment of large language models (LLMs) is often constrained by memorybandwidth, where the primary bottleneck is the cost of transferring modelparameters from the GPU's global memory to its registers. When coupled withcustom kernels that fuse the dequantization and matmul operations, weight-onlyquantization can thus enable faster inference by reducing the amount of memorymovement. However, developing high-performance kernels for weight-quantizedLLMs presents substantial challenges, especially when the weights arecompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,lookup table (LUT) quantization. This paper describes FLUTE, a flexible lookuptable engine for LUT-quantized LLMs, which uses offline restructuring of thequantized weight matrix to minimize bit manipulations associated withunpacking, and vectorization and duplication of the lookup table to mitigateshared memory bandwidth constraints. At batch sizes &lt; 32 and quantization groupsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x fasterthan existing GEMM kernels. As an application of FLUTE, we explore a simpleextension to lookup table-based NormalFloat quantization and apply it toquantize LLaMA3 to various configurations, obtaining competitive quantizationperformance against strong baselines while obtaining an end-to-end throughputincrease of 1.5 to 2 times.</description><author>Han Guo, William Brandon, Radostin Cholakov, Jonathan Ragan-Kelley, Eric P. Xing, Yoon Kim</author><pubDate>Mon, 15 Jul 2024 17:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10960v1</guid></item><item><title>A unified theory and statistical learning approach for traffic conflict detection</title><link>http://arxiv.org/abs/2407.10959v1</link><description>This study proposes a unified theory and statistical learning approach fortraffic conflict detection, addressing the long-existing call for a consistentand comprehensive methodology to evaluate the collision risk emerged in roaduser interactions. The proposed theory assumes a context-dependentprobabilistic collision risk and frames conflict detection as estimating therisk by statistical learning from observed proximities and contextualvariables. Three primary tasks are integrated: representing interaction contextfrom selected observables, inferring proximity distributions in differentcontexts, and applying extreme value theory to relate conflict intensity withconflict probability. As a result, this methodology is adaptable to variousroad users and interaction scenarios, enhancing its applicability without theneed for pre-labelled conflict data. Demonstration experiments are executedusing real-world trajectory data, with the unified metric trained onlane-changing interactions on German highways and applied to near-crash eventsfrom the 100-Car Naturalistic Driving Study in the U.S. The experimentsdemonstrate the methodology's ability to provide effective collision warnings,generalise across different datasets and traffic environments, cover a broadrange of conflicts, and deliver a long-tailed distribution of conflictintensity. This study contributes to traffic safety by offering a consistentand explainable methodology for conflict detection applicable across variousscenarios. Its societal implications include enhanced safety evaluations oftraffic infrastructures, more effective collision warning systems forautonomous and driving assistance systems, and a deeper understanding of roaduser behaviour in different traffic conditions, contributing to a potentialreduction in accident rates and improving overall traffic safety.</description><author>Yiru Jiao, Simeon C. Calvert, Sander van Cranenburgh, Hans van Lint</author><pubDate>Mon, 15 Jul 2024 17:55:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10959v1</guid></item><item><title>InVi: Object Insertion In Videos Using Off-the-Shelf Diffusion Models</title><link>http://arxiv.org/abs/2407.10958v1</link><description>We introduce InVi, an approach for inserting or replacing objects withinvideos (referred to as inpainting) using off-the-shelf, text-to-image latentdiffusion models. InVi targets controlled manipulation of objects and blendingthem seamlessly into a background video unlike existing video editing methodsthat focus on comprehensive re-styling or entire scene alterations. To achievethis goal, we tackle two key challenges. Firstly, for high quality control andblending, we employ a two-step process involving inpainting and matching. Thisprocess begins with inserting the object into a single frame using aControlNet-based inpainting diffusion model, and then generating subsequentframes conditioned on features from an inpainted frame as an anchor to minimizethe domain gap between the background and the object. Secondly, to ensuretemporal coherence, we replace the diffusion model's self-attention layers withextended-attention layers. The anchor frame features serve as the keys andvalues for these layers, enhancing consistency across frames. Our approachremoves the need for video-specific fine-tuning, presenting an efficient andadaptable solution. Experimental results demonstrate that InVi achievesrealistic object insertion with consistent blending and coherence acrossframes, outperforming existing methods.</description><author>Nirat Saini, Navaneeth Bodla, Ashish Shrivastava, Avinash Ravichandran, Xiao Zhang, Abhinav Shrivastava, Bharat Singh</author><pubDate>Mon, 15 Jul 2024 17:55:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10958v1</guid></item><item><title>Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes</title><link>http://arxiv.org/abs/2407.10957v1</link><description>Traditional reference segmentation tasks have predominantly focused on silentvisual scenes, neglecting the integral role of multimodal perception andinteraction in human experiences. In this work, we introduce a novel taskcalled Reference Audio-Visual Segmentation (Ref-AVS), which seeks to segmentobjects within the visual domain based on expressions containing multimodalcues. Such expressions are articulated in natural language forms but areenriched with multimodal cues, including audio and visual descriptions. Tofacilitate this research, we construct the first Ref-AVS benchmark, whichprovides pixel-level annotations for objects described in correspondingmultimodal-cue expressions. To tackle the Ref-AVS task, we propose a new methodthat adequately utilizes multimodal cues to offer precise segmentationguidance. Finally, we conduct quantitative and qualitative experiments on threetest subsets to compare our approach with existing methods from related tasks.The results demonstrate the effectiveness of our method, highlighting itscapability to precisely segment objects using multimodal-cue expressions.Dataset is available at\href{https://gewu-lab.github.io/Ref-AVS}{https://gewu-lab.github.io/Ref-AVS}.</description><author>Yaoting Wang, Peiwen Sun, Dongzhan Zhou, Guangyao Li, Honggang Zhang, Di Hu</author><pubDate>Mon, 15 Jul 2024 17:54:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10957v1</guid></item><item><title>Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?</title><link>http://arxiv.org/abs/2407.10956v1</link><description>Data science and engineering workflows often span multiple stages, fromwarehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. Asvision language models (VLMs) advance in multimodal understanding and codegeneration, VLM-based agents could potentially automate these workflows bygenerating SQL queries, Python code, and GUI operations. This automation canimprove the productivity of experts while democratizing access to large-scaledata analysis. In this paper, we introduce Spider2-V, the first multimodalagent benchmark focusing on professional data science and engineeringworkflows, featuring 494 real-world tasks in authentic computer environmentsand incorporating 20 enterprise-level professional applications. These tasks,derived from real-world use cases, evaluate the ability of a multimodal agentto perform data-related tasks by writing code and managing the GUI inenterprise data software systems. To balance realistic simulation withevaluation simplicity, we devote significant effort to developing automaticconfigurations for task setup and carefully crafting evaluation metrics foreach task. Furthermore, we supplement multimodal agents with comprehensivedocuments of these enterprise data software systems. Our empirical evaluationreveals that existing state-of-the-art LLM/VLM-based agents do not reliablyautomate full data workflows (14.0% success). Even with step-by-step guidance,these agents still underperform in tasks that require fine-grained,knowledge-intensive GUI actions (16.2%) and involve remote cloud-hostedworkspaces (10.6%). We hope that Spider2-V paves the way for autonomousmultimodal agents to transform the automation of data science and engineeringworkflow. Our code and data are available at https://spider2-v.github.io.</description><author>Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongshen Xu, Danyang Zhang, Sida Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor Zhong, Lu Chen, Kai Yu, Tao Yu</author><pubDate>Mon, 15 Jul 2024 17:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10956v1</guid></item><item><title>Enhancing Stochastic Optimization for Statistical Efficiency Using ROOT-SGD with Diminishing Stepsize</title><link>http://arxiv.org/abs/2407.10955v1</link><description>In this paper, we revisit \textsf{ROOT-SGD}, an innovative method forstochastic optimization to bridge the gap between stochastic optimization andstatistical efficiency. The proposed method enhances the performance andreliability of \textsf{ROOT-SGD} by integrating a carefully designed\emph{diminishing stepsize strategy}. This approach addresses key challenges inoptimization, providing robust theoretical guarantees and practical benefits.Our analysis demonstrates that \textsf{ROOT-SGD} with diminishing achievesoptimal convergence rates while maintaining computational efficiency. Bydynamically adjusting the learning rate, \textsf{ROOT-SGD} ensures improvedstability and precision throughout the optimization process. The findings ofthis study offer valuable insights for developing advanced optimizationalgorithms that are both efficient and statistically robust.</description><author>Tong Zhang, Chris Junchi Li</author><pubDate>Mon, 15 Jul 2024 17:54:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10955v1</guid></item><item><title>A Unified Differentiable Boolean Operator with Fuzzy Logic</title><link>http://arxiv.org/abs/2407.10954v1</link><description>This paper presents a unified differentiable boolean operator for implicitsolid shape modeling using Constructive Solid Geometry (CSG). Traditional CSGrelies on min, max operators to perform boolean operations on implicit shapes.But because these boolean operators are discontinuous and discrete in thechoice of operations, this makes optimization over the CSG representationchallenging. Drawing inspiration from fuzzy logic, we present a unified booleanoperator that outputs a continuous function and is differentiable with respectto operator types. This enables optimization of both the primitives and theboolean operations employed in CSG with continuous optimization techniques,such as gradient descent. We further demonstrate that such a continuous booleanoperator allows modeling of both sharp mechanical objects and smooth organicshapes with the same framework. Our proposed boolean operator opens up newpossibilities for future research toward fully continuous CSG optimization.</description><author>Hsueh-Ti Derek Liu, Maneesh Agrawala, Cem Yuksel, Tim Omernick, Vinith Misra, Stefano Corazza, Morgan McGuire, Victor Zordan</author><pubDate>Mon, 15 Jul 2024 17:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10954v1</guid></item><item><title>MMM: Multilingual Mutual Reinforcement Effect Mix Datasets &amp; Test with Open-domain Information Extraction Large Language Models</title><link>http://arxiv.org/abs/2407.10953v1</link><description>The Mutual Reinforcement Effect (MRE) represents a promising avenue ininformation extraction and multitasking research. Nevertheless, itsapplicability has been constrained due to the exclusive availability of MRE mixdatasets in Japanese, thereby limiting comprehensive exploration by the globalresearch community. To address this limitation, we introduce a Multilingual MREmix dataset (MMM) that encompasses 21 sub-datasets in English, Japanese, andChinese. In this paper, we also propose a method for dataset translationassisted by Large Language Models (LLMs), which significantly reduces themanual annotation time required for dataset construction by leveraging LLMs totranslate the original Japanese datasets. Additionally, we have enriched thedataset by incorporating open-domain Named Entity Recognition (NER) andsentence classification tasks. Utilizing this expanded dataset, we developed aunified input-output framework to train an Open-domain Information ExtractionLarge Language Model (OIELLM). The OIELLM model demonstrates the capability toeffectively process novel MMM datasets, exhibiting significant improvements inperformance.</description><author>Chengguang Gan, Qingyu Yin, Xinyang He, Hanjun Wei, Yunhao Liang, Younghun Lim, Shijian Wang, Hexiang Huang, Qinghao Zhang, Shiwen Ni, Tatsunori Mori</author><pubDate>Mon, 15 Jul 2024 17:50:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10953v1</guid></item><item><title>Representing Rule-based Chatbots with Transformers</title><link>http://arxiv.org/abs/2407.10949v1</link><description>Transformer-based chatbots can conduct fluent, natural-soundingconversations, but we have limited understanding of the mechanisms underlyingtheir behavior. Prior work has taken a bottom-up approach to understandingTransformers by constructing Transformers for various synthetic and formallanguage tasks, such as regular expressions and Dyck languages. However, it isnot obvious how to extend this approach to understand more naturalisticconversational agents. In this work, we take a step in this direction byconstructing a Transformer that implements the ELIZA program, a classic,rule-based chatbot. ELIZA illustrates some of the distinctive challenges of theconversational setting, including both local pattern matching and long-termdialog state tracking. We build on constructions from prior work -- inparticular, for simulating finite-state automata -- showing how simplerconstructions can be composed and extended to give rise to more sophisticatedbehavior. Next, we train Transformers on a dataset of synthetically generatedELIZA conversations and investigate the mechanisms the models learn. Ouranalysis illustrates the kinds of mechanisms these models tend to prefer -- forexample, models favor an induction head mechanism over a more precise, positionbased copying mechanism; and using intermediate generations to simulaterecurrent data structures, like ELIZA's memory mechanisms. Overall, by drawingan explicit connection between neural chatbots and interpretable, symbolicmechanisms, our results offer a new setting for mechanistic analysis ofconversational agents.</description><author>Dan Friedman, Abhishek Panigrahi, Danqi Chen</author><pubDate>Mon, 15 Jul 2024 17:45:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10949v1</guid></item><item><title>Can Textual Semantics Mitigate Sounding Object Segmentation Preference?</title><link>http://arxiv.org/abs/2407.10947v1</link><description>The Audio-Visual Segmentation (AVS) task aims to segment sounding objects inthe visual space using audio cues. However, in this work, it is recognized thatprevious AVS methods show a heavy reliance on detrimental segmentationpreferences related to audible objects, rather than precise audio guidance. Weargue that the primary reason is that audio lacks robust semantics compared tovision, especially in multi-source sounding scenes, resulting in weak audioguidance over the visual space. Motivated by the the fact that text modality iswell explored and contains rich abstract semantics, we propose leveraging textcues from the visual scene to enhance audio guidance with the semanticsinherent in text. Our approach begins by obtaining scene descriptions throughan off-the-shelf image captioner and prompting a frozen large language model todeduce potential sounding objects as text cues. Subsequently, we introduce anovel semantics-driven audio modeling module with a dynamic mask to integrateaudio features with text cues, leading to representative sounding objectfeatures. These features not only encompass audio cues but also possess vividsemantics, providing clearer guidance in the visual space. Experimental resultson AVS benchmarks validate that our method exhibits enhanced sensitivity toaudio when aided by text cues, achieving highly competitive performance on allthree subsets. Project page:\href{https://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preference}{https://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preference}</description><author>Yaoting Wang, Peiwen Sun, Yuanchao Li, Honggang Zhang, Di Hu</author><pubDate>Mon, 15 Jul 2024 17:45:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10947v1</guid></item><item><title>Learning from Naturally Occurring Feedback</title><link>http://arxiv.org/abs/2407.10944v1</link><description>Human feedback data is a critical component in developing language models.However, collecting this feedback is costly and ultimately not scalable. Wepropose a scalable method for extracting feedback that users naturally includewhen interacting with chat models, and leveraging it for model training. We arefurther motivated by previous work that showed there are also qualitativeadvantages to using naturalistic (rather than auto-generated) feedback, such asless hallucinations and biases. We manually annotated conversation data toconfirm the presence of naturally occurring feedback in a standard corpus,finding that as much as 30% of the chats include explicit feedback. We applyour method to over 1M conversations to obtain hundreds of thousands of feedbacksamples. Training with the extracted feedback shows significant performanceimprovements over baseline models, demonstrating the efficacy of our approachin enhancing model alignment to human preferences.</description><author>Shachar Don-Yehiya, Leshem Choshen, Omri Abend</author><pubDate>Mon, 15 Jul 2024 17:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10944v1</guid></item><item><title>GRUtopia: Dream General Robots in a City at Scale</title><link>http://arxiv.org/abs/2407.10943v1</link><description>Recent works have been exploring the scaling laws in the field of EmbodiedAI. Given the prohibitive costs of collecting real-world data, we believe theSimulation-to-Real (Sim2Real) paradigm is a crucial step for scaling thelearning of embodied models. This paper introduces project GRUtopia, the firstsimulated interactive 3D society designed for various robots. It featuresseveral advancements: (a) The scene dataset, GRScenes, includes 100kinteractive, finely annotated scenes, which can be freely combined intocity-scale environments. In contrast to previous works mainly focusing on home,GRScenes covers 89 diverse scene categories, bridging the gap ofservice-oriented environments where general robots would be initially deployed.(b) GRResidents, a Large Language Model (LLM) driven Non-Player Character (NPC)system that is responsible for social interaction, task generation, and taskassignment, thus simulating social scenarios for embodied AI applications. (c)The benchmark, GRBench, supports various robots but focuses on legged robots asprimary agents and poses moderately challenging tasks involving ObjectLoco-Navigation, Social Loco-Navigation, and Loco-Manipulation. We hope thatthis work can alleviate the scarcity of high-quality data in this field andprovide a more comprehensive assessment of Embodied AI research. The project isavailable at https://github.com/OpenRobotLab/GRUtopia.</description><author>Hanqing Wang, Jiahe Chen, Wensi Huang, Qingwei Ben, Tai Wang, Boyu Mi, Tao Huang, Siheng Zhao, Yilun Chen, Sizhe Yang, Peizhou Cao, Wenye Yu, Zichao Ye, Jialun Li, Junfeng Long, Zirui Wang, Huiling Wang, Ying Zhao, Zhongying Tu, Yu Qiao, Dahua Lin, Jiangmiao Pang</author><pubDate>Mon, 15 Jul 2024 17:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10943v1</guid></item><item><title>SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant</title><link>http://arxiv.org/abs/2403.11299v2</link><description>Recent advances in vision-language models have shown notable generalizationin broad tasks through visual instruction tuning. However, bridging the gapbetween the pre-trained vision encoder and the large language models (LLMs)becomes the whole network's bottleneck. To improve cross-modality alignment,existing works usually consider more visual instruction data covering a broaderrange of vision tasks to fine-tune the model for question-answering, which,however, is costly to obtain and has not thoroughly explored the richcontextual information contained in images. This paper first attempts toharness the overlooked context within visual instruction data, training themodel to self-supervised "learning" how to ask high-quality questions. In thisway, we introduce a novel framework named SQ-LLaVA: Self-Questioning for LargeVision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexibleand meaningful image-related questions while analyzing the visual clue andprior language knowledge, signifying an advanced level of generalized visualunderstanding. Moreover, fine-tuning SQ-LLaVA on higher-quality instructiondata shows a performance improvement compared with traditionalvisual-instruction tuning methods. This improvement highlights the efficacy ofself-questioning techniques in achieving a deeper and more nuancedcomprehension of visual content across various contexts.</description><author>Guohao Sun, Can Qin, Jiamian Wang, Zeyuan Chen, Ran Xu, Zhiqiang Tao</author><pubDate>Mon, 15 Jul 2024 17:37:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11299v2</guid></item><item><title>IDOL: Unified Dual-Modal Latent Diffusion for Human-Centric Joint Video-Depth Generation</title><link>http://arxiv.org/abs/2407.10937v1</link><description>Significant advances have been made in human-centric video generation, yetthe joint video-depth generation problem remains underexplored. Most existingmonocular depth estimation methods may not generalize well to synthesizedimages or videos, and multi-view-based methods have difficulty controlling thehuman appearance and motion. In this work, we present IDOL (unIfied Dual-mOdalLatent diffusion) for high-quality human-centric joint video-depth generation.Our IDOL consists of two novel designs. First, to enable dual-modal generationand maximize the information exchange between video and depth generation, wepropose a unified dual-modal U-Net, a parameter-sharing framework for jointvideo and depth denoising, wherein a modality label guides the denoisingtarget, and cross-modal attention enables the mutual information flow. Second,to ensure a precise video-depth spatial alignment, we propose a motionconsistency loss that enforces consistency between the video and depth featuremotion fields, leading to harmonized outputs. Additionally, a cross-attentionmap consistency loss is applied to align the cross-attention map of the videodenoising with that of the depth denoising, further facilitating spatialalignment. Extensive experiments on the TikTok and NTU120 datasets show oursuperior performance, significantly surpassing existing methods in terms ofvideo FVD and depth accuracy.</description><author>Yuanhao Zhai, Kevin Lin, Linjie Li, Chung-Ching Lin, Jianfeng Wang, Zhengyuan Yang, David Doermann, Junsong Yuan, Zicheng Liu, Lijuan Wang</author><pubDate>Mon, 15 Jul 2024 17:36:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10937v1</guid></item><item><title>STARS: Self-supervised Tuning for 3D Action Recognition in Skeleton Sequences</title><link>http://arxiv.org/abs/2407.10935v1</link><description>Self-supervised pretraining methods with masked prediction demonstrateremarkable within-dataset performance in skeleton-based action recognition.However, we show that, unlike contrastive learning approaches, they do notproduce well-separated clusters. Additionally, these methods struggle withgeneralization in few-shot settings. To address these issues, we proposeSelf-supervised Tuning for 3D Action Recognition in Skeleton sequences (STARS).Specifically, STARS first uses a masked prediction stage using anencoder-decoder architecture. It then employs nearest-neighbor contrastivelearning to partially tune the weights of the encoder, enhancing the formationof semantic clusters for different actions. By tuning the encoder for a fewepochs, and without using hand-crafted data augmentations, STARS achievesstate-of-the-art self-supervised results in various benchmarks, includingNTU-60, NTU-120, and PKU-MMD. In addition, STARS exhibits significantly betterresults than masked prediction models in few-shot settings, where the model hasnot seen the actions throughout pretraining. Project page:https://soroushmehraban.github.io/stars/</description><author>Soroush Mehraban, Mohammad Javad Rajabi, Babak Taati</author><pubDate>Mon, 15 Jul 2024 17:35:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10935v1</guid></item><item><title>DiarizationLM: Speaker Diarization Post-Processing with Large Language Models</title><link>http://arxiv.org/abs/2401.03506v7</link><description>In this paper, we introduce DiarizationLM, a framework to leverage largelanguage models (LLM) to post-process the outputs from a speaker diarizationsystem. Various goals can be achieved with the proposed framework, such asimproving the readability of the diarized transcript, or reducing the worddiarization error rate (WDER). In this framework, the outputs of the automaticspeech recognition (ASR) and speaker diarization systems are represented as acompact textual format, which is included in the prompt to an optionallyfinetuned LLM. The outputs of the LLM can be used as the refined diarizationresults with the desired enhancement. As a post-processing step, this frameworkcan be easily applied to any off-the-shelf ASR and speaker diarization systemswithout retraining existing components. Our experiments show that a finetunedPaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephoneconversation dataset, and rel. 44.9% on the Callhome English dataset.</description><author>Quan Wang, Yiling Huang, Guanlong Zhao, Evan Clark, Wei Xia, Hank Liao</author><pubDate>Mon, 15 Jul 2024 17:32:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.03506v7</guid></item><item><title>Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together</title><link>http://arxiv.org/abs/2407.10930v1</link><description>Natural Language Processing (NLP) systems are increasingly taking the form ofmulti-stage pipelines involving multiple distinct language models (LMs) andprompting strategies. Here we address the question of how to fine-tune suchsystems to improve their performance. We cast this as a problem of optimizingthe underlying LM weights and the prompting strategies together, and consider achallenging but highly realistic scenario in which we have no gold labels forany intermediate stages in the pipeline. To address this challenge, we evaluateapproximate optimization strategies in which we bootstrap training labels forall pipeline stages and use these to optimize the pipeline's prompts andfine-tune its weights alternatingly. In experiments with multi-hop QA,mathematical reasoning, and feature-based classification, we find that simpleapproaches for optimizing the prompts and weights together outperform directlyoptimizing weights alone and prompts alone by up to 65% and 5%, respectively,on average across LMs and tasks. We will release our new optimizers in DSPy athttp://dspy.ai</description><author>Dilara Soylu, Christopher Potts, Omar Khattab</author><pubDate>Mon, 15 Jul 2024 17:30:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10930v1</guid></item><item><title>In-Loop Filtering via Trained Look-Up Tables</title><link>http://arxiv.org/abs/2407.10926v1</link><description>In-loop filtering (ILF) is a key technology for removing the artifacts inimage/video coding standards. Recently, neural network-based in-loop filteringmethods achieve remarkable coding gains beyond the capability of advanced videocoding standards, which becomes a powerful coding tool candidate for futurevideo coding standards. However, the utilization of deep neural networks bringsheavy time and computational complexity, and high demands of high-performancehardware, which is challenging to apply to the general uses of coding scene. Toaddress this limitation, inspired by explorations in image restoration, wepropose an efficient and practical in-loop filtering scheme by adopting theLook-up Table (LUT). We train the DNN of in-loop filtering within a fixedfiltering reference range, and cache the output values of the DNN into a LUTvia traversing all possible inputs. At testing time in the coding process, thefiltered pixel is generated by locating input pixels (to-be-filtered pixel withreference pixels) and interpolating cached filtered pixel values. To furtherenable the large filtering reference range with the limited storage cost ofLUT, we introduce the enhanced indexing mechanism in the filtering process, andclipping/finetuning mechanism in the training. The proposed method isimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.Experimental results show that the ultrafast, very fast, and fast mode of theproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%BD-rate reduction, under the all intra (AI) and random access (RA)configurations. Especially, our method has friendly time and computationalcomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,and only 164-1148 KB storage cost for a single model. Our solution may shedlight on the journey of practical neural network-based coding tool evolution.</description><author>Zhuoyuan Li, Jiacheng Li, Yao Li, Li Li, Dong Liu, Feng Wu</author><pubDate>Mon, 15 Jul 2024 17:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10926v1</guid></item><item><title>OPa-Ma: Text Guided Mamba for 360-degree Image Out-painting</title><link>http://arxiv.org/abs/2407.10923v1</link><description>In this paper, we tackle the recently popular topic of generating 360-degreeimages given the conventional narrow field of view (NFoV) images that could betaken from a single camera or cellphone. This task aims to predict thereasonable and consistent surroundings from the NFoV images. Existing methodsfor feature extraction and fusion, often built with transformer-basedarchitectures, incur substantial memory usage and computational expense. Theyalso have limitations in maintaining visual continuity across the entire360-degree images, which could cause inconsistent texture and style generation.To solve the aforementioned issues, we propose a novel text-guided out-paintingframework equipped with a State-Space Model called Mamba to utilize itslong-sequence modelling and spatial continuity. Furthermore, incorporatingtextual information is an effective strategy for guiding image generation,enriching the process with detailed context and increasing diversity.Efficiently extracting textual features and integrating them with imageattributes presents a significant challenge for 360-degree image out-painting.To address this, we develop two modules, Visual-textual Consistency Refiner(VCR) and Global-local Mamba Adapter (GMA). VCR enhances contextual richness byfusing the modified text features with the image features, while GMA providesadaptive state-selective conditions by capturing the information flow fromglobal to local representations. Our proposed method achieves state-of-the-artperformance with extensive experiments on two broadly used 360-degree imagedatasets, including indoor and outdoor settings.</description><author>Penglei Gao, Kai Yao, Tiandi Ye, Steven Wang, Yuan Yao, Xiaofeng Wang</author><pubDate>Mon, 15 Jul 2024 17:23:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10923v1</guid></item><item><title>A Dual-Attention Aware Deep Convolutional Neural Network for Early Alzheimer's Detection</title><link>http://arxiv.org/abs/2407.10921v1</link><description>Alzheimer's disease (AD) represents the primary form of neurodegeneration,impacting millions of individuals each year and causing progressive cognitivedecline. Accurately diagnosing and classifying AD using neuroimaging datapresents ongoing challenges in medicine, necessitating advanced interventionsthat will enhance treatment measures. In this research, we introduce a dualattention enhanced deep learning (DL) framework for classifying AD fromneuroimaging data. Combined spatial and self-attention mechanisms play a vitalrole in emphasizing focus on neurofibrillary tangles and amyloid plaques fromthe MRI images, which are difficult to discern with regular imaging techniques.Results demonstrate that our model yielded remarkable performance in comparisonto existing state of the art (SOTA) convolutional neural networks (CNNs), withan accuracy of 99.1%. Moreover, it recorded remarkable metrics, with anF1-Score of 99.31%, a precision of 99.24%, and a recall of 99.5%. These resultshighlight the promise of cutting edge DL methods in medical diagnostics,contributing to highly reliable and more efficient healthcare solutions.</description><author>Pandiyaraju V, Shravan Venkatraman, Abeshek A, Aravintakshan S A, Pavan Kumar S, Kannan A</author><pubDate>Mon, 15 Jul 2024 17:22:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10921v1</guid></item><item><title>Benchmarking Vision Language Models for Cultural Understanding</title><link>http://arxiv.org/abs/2407.10920v1</link><description>Foundation models and vision-language pre-training have notably advancedVision Language Models (VLMs), enabling multimodal processing of visual andlinguistic data. However, their performance has been typically assessed ongeneral scene understanding - recognizing objects, attributes, and actions -rather than cultural comprehension. This study introduces CulturalVQA, a visualquestion-answering benchmark aimed at assessing VLM's geo-diverse culturalunderstanding. We curate a collection of 2,378 image-question pairs with 1-5answers per question representing cultures from 11 countries across 5continents. The questions probe understanding of various facets of culture suchas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs onCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level ofcultural understanding across regions, with strong cultural understandingcapabilities for North America while significantly lower performance forAfrica. We observe disparity in their performance across cultural facets too,with clothing, rituals, and traditions seeing higher performances than food anddrink. These disparities help us identify areas where VLMs lack culturalunderstanding and demonstrate the potential of CulturalVQA as a comprehensiveevaluation set for gauging VLM progress in understanding diverse cultures.</description><author>Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd van Steenkiste, Lisa Anne Hendricks, Karolina Stańczak, Aishwarya Agrawal</author><pubDate>Mon, 15 Jul 2024 17:21:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10920v1</guid></item><item><title>PartImageNet++ Dataset: Scaling up Part-based Models for Robust Recognition</title><link>http://arxiv.org/abs/2407.10918v1</link><description>Deep learning-based object recognition systems can be easily fooled byvarious adversarial perturbations. One reason for the weak robustness may bethat they do not have part-based inductive bias like the human recognitionprocess. Motivated by this, several part-based recognition models have beenproposed to improve the adversarial robustness of recognition. However, due tothe lack of part annotations, the effectiveness of these methods is onlyvalidated on small-scale nonstandard datasets. In this work, we propose PIN++,short for PartImageNet++, a dataset providing high-quality part segmentationannotations for all categories of ImageNet-1K (IN-1K). With these annotations,we build part-based methods directly on the standard IN-1K dataset for robustrecognition. Different from previous two-stage part-based models, we propose aMulti-scale Part-supervised Model (MPM), to learn a robust representation withpart annotations. Experiments show that MPM yielded better adversarialrobustness on the large-scale IN-1K over strong baselines across various attacksettings. Furthermore, MPM achieved improved robustness on common corruptionsand several out-of-distribution datasets. The dataset, together with theseresults, enables and encourages researchers to explore the potential ofpart-based models in more real applications.</description><author>Xiao Li, Yining Liu, Na Dong, Sitian Qin, Xiaolin Hu</author><pubDate>Mon, 15 Jul 2024 17:19:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10918v1</guid></item><item><title>Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation</title><link>http://arxiv.org/abs/2401.05675v2</link><description>Recent works have demonstrated that using reinforcement learning (RL) withmultiple quality rewards can improve the quality of generated images intext-to-image (T2I) generation. However, manually adjusting reward weightsposes challenges and may cause over-optimization in certain metrics. To solvethis, we propose Parrot, which addresses the issue through multi-objectiveoptimization and introduces an effective multi-reward optimization strategy toapproximate Pareto optimal. Utilizing batch-wise Pareto optimal selection,Parrot automatically identifies the optimal trade-off among different rewards.We use the novel multi-reward optimization algorithm to jointly optimize theT2I model and a prompt expansion network, resulting in significant improvementof image quality and also allow to control the trade-off of different rewardsusing a reward related prompt during inference. Furthermore, we introduceoriginal prompt-centered guidance at inference time, ensuring fidelity to userinput after prompt expansion. Extensive experiments and a user study validatethe superiority of Parrot over several baselines across various qualitycriteria, including aesthetics, human preference, text-image alignment, andimage sentiment.</description><author>Seung Hyun Lee, Yinxiao Li, Junjie Ke, Innfarn Yoo, Han Zhang, Jiahui Yu, Qifei Wang, Fei Deng, Glenn Entis, Junfeng He, Gang Li, Sangpil Kim, Irfan Essa, Feng Yang</author><pubDate>Mon, 15 Jul 2024 17:19:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05675v2</guid></item><item><title>When Heterophily Meets Heterogeneity: New Graph Benchmarks and Effective Methods</title><link>http://arxiv.org/abs/2407.10916v1</link><description>Many real-world graphs frequently present challenges for graph learning dueto the presence of both heterophily and heterogeneity. However, existingbenchmarks for graph learning often focus on heterogeneous graphs withhomophily or homogeneous graphs with heterophily, leaving a gap inunderstanding how methods perform on graphs that are both heterogeneous andheterophilic. To bridge this gap, we introduce H2GB, a novel graph benchmarkthat brings together the complexities of both the heterophily and heterogeneityproperties of graphs. Our benchmark encompasses 9 diverse real-world datasetsacross 5 domains, 28 baseline model implementations, and 26 benchmark results.In addition, we present a modular graph transformer framework UnifiedGT and anew model variant, H2G-former, that excels at this challenging benchmark. Byintegrating masked label embeddings, cross-type heterogeneous attention, andtype-specific FFNs, H2G-former effectively tackles graph heterophily andheterogeneity. Extensive experiments across 26 baselines on H2GB revealinadequacies of current models on heterogeneous heterophilic graph learning,and demonstrate the superiority of our H2G-former over existing solutions. Boththe benchmark and the framework are available on GitHub(https://github.com/junhongmit/H2GB) and PyPI (https://pypi.org/project/H2GB),and documentation can be found at https://junhongmit.github.io/H2GB/.</description><author>Junhong Lin, Xiaojie Guo, Shuaicheng Zhang, Dawei Zhou, Yada Zhu, Julian Shun</author><pubDate>Mon, 15 Jul 2024 17:18:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10916v1</guid></item><item><title>GOEmbed: Gradient Origin Embeddings for Representation Agnostic 3D Feature Learning</title><link>http://arxiv.org/abs/2312.08744v2</link><description>Encoding information from 2D views of an object into a 3D representation iscrucial for generalized 3D feature extraction. Such features can then enable 3Dreconstruction, 3D generation, and other applications. We propose GOEmbed(Gradient Origin Embeddings) that encodes input 2D images into any 3Drepresentation, without requiring a pre-trained image feature extractor; unliketypical prior approaches in which input images are either encoded using 2Dfeatures extracted from large pre-trained models, or customized features aredesigned to handle different 3D representations; or worse, encoders may not yetbe available for specialized 3D neural representations such as MLPs andhash-grids. We extensively evaluate our proposed GOEmbed under differentexperimental settings on the OmniObject3D benchmark. First, we evaluate howwell the mechanism compares against prior encoding mechanisms on multiple 3Drepresentations using an illustrative experiment called Plenoptic-Encoding.Second, the efficacy of the GOEmbed mechanism is further demonstrated byachieving a new SOTA FID of 22.12 on the OmniObject3D generation task using acombination of GOEmbed and DFM (Diffusion with Forward Models), which we callGOEmbedFusion. Finally, we evaluate how the GOEmbed mechanism bolsterssparse-view 3D reconstruction pipelines.</description><author>Animesh Karnewar, Roman Shapovalov, Tom Monnier, Andrea Vedaldi, Niloy J. Mitra, David Novotny</author><pubDate>Mon, 15 Jul 2024 17:15:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08744v2</guid></item><item><title>DataDream: Few-shot Guided Dataset Generation</title><link>http://arxiv.org/abs/2407.10910v1</link><description>While text-to-image diffusion models have been shown to achievestate-of-the-art results in image synthesis, they have yet to prove theireffectiveness in downstream applications. Previous work has proposed togenerate data for image classifier training given limited real data access.However, these methods struggle to generate in-distribution images or depictfine-grained features, thereby hindering the generalization of classificationmodels trained on synthetic datasets. We propose DataDream, a framework forsynthesizing classification datasets that more faithfully represents the realdata distribution when guided by few-shot examples of the target classes.DataDream fine-tunes LoRA weights for the image generation model on the fewreal images before generating the training data using the adapted model. Wethen fine-tune LoRA weights for CLIP using the synthetic data to improvedownstream image classification over previous approaches on a large variety ofdatasets. We demonstrate the efficacy of DataDream through extensiveexperiments, surpassing state-of-the-art classification accuracy with few-shotdata across 7 out of 10 datasets, while being competitive on the other 3.Additionally, we provide insights into the impact of various factors, such asthe number of real-shot and generated images as well as the fine-tuning computeon model performance. The code is available athttps://github.com/ExplainableML/DataDream.</description><author>Jae Myung Kim, Jessica Bader, Stephan Alaniz, Cordelia Schmid, Zeynep Akata</author><pubDate>Mon, 15 Jul 2024 17:10:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10910v1</guid></item><item><title>Dissecting Deep RL with High Update Ratios: Combatting Value Divergence</title><link>http://arxiv.org/abs/2403.05996v2</link><description>We show that deep reinforcement learning algorithms can retain their abilityto learn without resetting network parameters in settings where the number ofgradient updates greatly exceeds the number of environment samples bycombatting value function divergence. Under large update-to-data ratios, arecent study by Nikishin et al. (2022) suggested the emergence of a primacybias, in which agents overfit early interactions and downplay later experience,impairing their ability to learn. In this work, we investigate the phenomenaleading to the primacy bias. We inspect the early stages of training that wereconjectured to cause the failure to learn and find that one fundamentalchallenge is a long-standing acquaintance: value function divergence.Overinflated Q-values are found not only on out-of-distribution but alsoin-distribution data and can be linked to overestimation on unseen actionprediction propelled by optimizer momentum. We employ a simple unit-ballnormalization that enables learning under large update ratios, show itsefficacy on the widely used dm_control suite, and obtain strong performance onthe challenging dog tasks, competitive with model-based approaches. Our resultsquestion, in parts, the prior explanation for sub-optimal learning due tooverfitting early data.</description><author>Marcel Hussing, Claas Voelcker, Igor Gilitschenski, Amir-massoud Farahmand, Eric Eaton</author><pubDate>Mon, 15 Jul 2024 17:08:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05996v2</guid></item><item><title>Human-in-the-Loop Visual Re-ID for Population Size Estimation</title><link>http://arxiv.org/abs/2312.05287v2</link><description>Computer vision-based re-identification (Re-ID) systems are increasinglybeing deployed for estimating population size in large image collections.However, the estimated size can be significantly inaccurate when the task ischallenging or when deployed on data from new distributions. We propose ahuman-in-the-loop approach for estimating population size driven by a pairwisesimilarity derived from an off-the-shelf Re-ID system. Our approach, based onnested importance sampling, selects pairs of images for human vetting driven bythe pairwise similarity, and produces asymptotically unbiased population sizeestimates with associated confidence intervals. We perform experiments onvarious animal Re-ID datasets and demonstrate that our method outperformsstrong baselines and active clustering approaches. In many cases, we are ableto reduce the error rates of the estimated size from around 80% using CV aloneto less than 20% by vetting a fraction (often less than 0.002%) of the totalpairs. The cost of vetting reduces with the increase in accuracy and provides apractical approach for population size estimation within a desired tolerancewhen deploying Re-ID systems.</description><author>Gustavo Perez, Daniel Sheldon, Grant Van Horn, Subhransu Maji</author><pubDate>Mon, 15 Jul 2024 17:05:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05287v2</guid></item><item><title>OmniGS: Fast Radiance Field Reconstruction using Omnidirectional Gaussian Splatting</title><link>http://arxiv.org/abs/2404.03202v3</link><description>Photorealistic reconstruction relying on 3D Gaussian Splatting has shownpromising potential in various domains. However, the current 3D GaussianSplatting system only supports radiance field reconstruction using undistortedperspective images. In this paper, we present OmniGS, a novel omnidirectionalGaussian splatting system, to take advantage of omnidirectional images for fastradiance field reconstruction. Specifically, we conduct a theoretical analysisof spherical camera model derivatives in 3D Gaussian Splatting. According tothe derivatives, we then implement a new GPU-accelerated omnidirectionalrasterizer that directly splats 3D Gaussians onto the equirectangular screenspace for omnidirectional image rendering. We realize differentiableoptimization of the omnidirectional radiance field without the requirement ofcube-map rectification or tangent-plane approximation. Extensive experimentsconducted in egocentric and roaming scenarios demonstrate that our methodachieves state-of-the-art reconstruction quality and high rendering speed usingomnidirectional images. The code will be publicly available.</description><author>Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng</author><pubDate>Mon, 15 Jul 2024 16:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03202v3</guid></item><item><title>SHINE: Saliency-aware HIerarchical NEgative Ranking for Compositional Temporal Grounding</title><link>http://arxiv.org/abs/2407.05118v2</link><description>Temporal grounding, also known as video moment retrieval, aims at locatingvideo segments corresponding to a given query sentence. The compositionalnature of natural language enables the localization beyond predefined events,posing a certain challenge to the compositional generalizability of existingmethods. Recent studies establish the correspondence between videos and queriesthrough a decompose-reconstruct manner to achieve compositional generalization.However, they only consider dominant primitives and build negative queriesthrough random sampling and recombination, resulting in semanticallyimplausible negatives that hinder the models from learning rationalcompositions. In addition, recent DETR-based methods still underperform incompositional temporal grounding, showing irrational saliency responses whengiven negative queries that have subtle differences from positive queries. Toaddress these limitations, we first propose a large language model-drivenmethod for negative query construction, utilizing GPT-3.5-Turbo to generatesemantically plausible hard negative queries. Subsequently, we introduce acoarse-to-fine saliency ranking strategy, which encourages the model to learnthe multi-granularity semantic relationships between videos and hierarchicalnegative queries to boost compositional generalization. Extensive experimentson two challenging benchmarks validate the effectiveness and generalizabilityof our proposed method. Our code is available athttps://github.com/zxccade/SHINE.</description><author>Zixu Cheng, Yujiang Pu, Shaogang Gong, Parisa Kordjamshidi, Yu Kong</author><pubDate>Mon, 15 Jul 2024 16:53:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05118v2</guid></item><item><title>Interpreting Hand gestures using Object Detection and Digits Classification</title><link>http://arxiv.org/abs/2407.10902v1</link><description>Hand gestures have evolved into a natural and intuitive means of engagingwith technology. The objective of this research is to develop a robust systemthat can accurately recognize and classify hand gestures representing numbers.The proposed approach involves collecting a dataset of hand gesture images,preprocessing and enhancing the images, extracting relevant features, andtraining a machine learning model. The advancement of computer visiontechnology and object detection techniques, in conjunction with OpenCV'scapability to analyze and comprehend hand gestures, presents a chance totransform the identification of numerical digits and its potentialapplications. The advancement of computer vision technology and objectidentification technologies, along with OpenCV's capacity to analyze andinterpret hand gestures, has the potential to revolutionize human interaction,boosting people's access to information, education, and employmentopportunities. Keywords: Computer Vision, Machine learning, Deep Learning,Neural Networks</description><author>Sangeetha K, Balaji VS, Kamalesh P, Anirudh Ganapathy PS</author><pubDate>Mon, 15 Jul 2024 16:53:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10902v1</guid></item><item><title>Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis</title><link>http://arxiv.org/abs/2407.10899v1</link><description>Effective educational measurement relies heavily on the curation ofwell-designed item pools (i.e., possessing the right psychometric properties).However, item calibration is time-consuming and costly, requiring a sufficientnumber of respondents for the response process. We explore using six differentLLMs (GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini-Pro, and Cohere Command R Plus)and various combinations of them using sampling methods to produce responseswith psychometric properties similar to human answers. Results show that someLLMs have comparable or higher proficiency in College Algebra than collegestudents. No single LLM mimics human respondents due to narrow proficiencydistributions, but an ensemble of LLMs can better resemble college students'ability distribution. The item parameters calibrated by LLM-Respondents havehigh correlations (e.g. &gt; 0.8 for GPT-3.5) compared to their human calibratedcounterparts, and closely resemble the parameters of the human subset (e.g.0.02 Spearman correlation difference). Several augmentation strategies areevaluated for their relative performance, with resampling methods proving mosteffective, enhancing the Spearman correlation from 0.89 (human only) to 0.93(augmented human).</description><author>Yunting Liu, Shreya Bhandari, Zachary A. Pardos</author><pubDate>Mon, 15 Jul 2024 16:49:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10899v1</guid></item><item><title>Generalizable Physics-Informed Learning for Stochastic Safety-Critical Systems</title><link>http://arxiv.org/abs/2407.08868v2</link><description>Accurate estimate of long-term risk is critical for safe decision-making, butsampling from rare risk events and long-term trajectories can be prohibitivelycostly. Risk gradient can be used in many first-order techniques for learningand control methods, but gradient estimate is difficult to obtain using MonteCarlo (MC) methods because the infinitesimal divisor may significantly amplifysampling noise. Motivated by this gap, we propose an efficient method toevaluate long-term risk probabilities and their gradients using short-termsamples without sufficient risk events. We first derive that four types oflong-term risk probability are solutions of certain partial differentialequations (PDEs). Then, we propose a physics-informed learning technique thatintegrates data and physics information (aforementioned PDEs). The physicsinformation helps propagate information beyond available data and obtainprovable generalization beyond available data, which in turn enables long-termrisk to be estimated using short-term samples of safe events. Finally, wedemonstrate in simulation that the proposed technique has improved sampleefficiency, generalizes well to unseen regions, and adapts to changing systemparameters.</description><author>Zhuoyuan Wang, Albert Chern, Yorie Nakahira</author><pubDate>Mon, 15 Jul 2024 16:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08868v2</guid></item><item><title>Optical Diffusion Models for Image Generation</title><link>http://arxiv.org/abs/2407.10897v1</link><description>Diffusion models generate new samples by progressively decreasing the noisefrom the initially provided random distribution. This inference proceduregenerally utilizes a trained neural network numerous times to obtain the finaloutput, creating significant latency and energy consumption on digitalelectronic hardware such as GPUs. In this study, we demonstrate that thepropagation of a light beam through a semi-transparent medium can be programmedto implement a denoising diffusion model on image samples. This frameworkprojects noisy image patterns through passive diffractive optical layers, whichcollectively only transmit the predicted noise term in the image. The opticaltransparent layers, which are trained with an online training approach,backpropagating the error to the analytical model of the system, are passiveand kept the same across different steps of denoising. Hence this methodenables high-speed image generation with minimal power consumption, benefitingfrom the bandwidth and energy efficiency of optical information processing.</description><author>Ilker Oguz, Niyazi Ulas Dinc, Mustafa Yildirim, Junjie Ke, Innfarn Yoo, Qifei Wang, Feng Yang, Christophe Moser, Demetri Psaltis</author><pubDate>Mon, 15 Jul 2024 16:46:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10897v1</guid></item><item><title>Explore as a Storm, Exploit as a Raindrop: On the Benefit of Fine-Tuning Kernel Schedulers with Coordinate Descent</title><link>http://arxiv.org/abs/2406.20037v2</link><description>Machine-learning models consist of kernels, which are algorithms applyingoperations on tensors -- data indexed by a linear combination of naturalnumbers. Examples of kernels include convolutions, transpositions, andvectorial products. There are many ways to implement a kernel. Theseimplementations form the kernel's optimization space. Kernel scheduling is theproblem of finding the best implementation, given an objective function --typically execution speed. Kernel optimizers such as Ansor, Halide, and AutoTVMsolve this problem via search heuristics, which combine two phases: explorationand exploitation. The first step evaluates many different kernel optimizationspaces. The latter tries to improve the best implementations by investigating akernel within the same space. For example, Ansor combines kernel generationthrough sketches for exploration and leverages an evolutionary algorithm toexploit the best sketches. In this work, we demonstrate the potential to reduceAnsor's search time while enhancing kernel quality by incorporating DropletSearch, an AutoTVM algorithm, into Ansor's exploration phase. The approachinvolves limiting the number of samples explored by Ansor, selecting the best,and exploiting it with a coordinate descent algorithm. By applying thisapproach to the first 300 kernels that Ansor generates, we usually obtainbetter kernels in less time than if we let Ansor analyze 10,000 kernels. Thisresult has been replicated in 20 well-known deep-learning models (AlexNet,ResNet, VGG, DenseNet, etc.) running on four architectures: an AMD Ryzen 7(x86), an NVIDIA A100 tensor core, an NVIDIA RTX 3080 GPU, and an ARM A64FX. Apatch with this combined approach was approved in Ansor in February 2024. Asevidence of the generality of this search methodology, a similar patch,achieving equally good results, was submitted to TVM's MetaSchedule in June2024.</description><author>Michael Canesche, Gaurav Verma, Fernando Magno Quintao Pereira</author><pubDate>Mon, 15 Jul 2024 16:42:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.20037v2</guid></item><item><title>AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering</title><link>http://arxiv.org/abs/2311.14906v2</link><description>We propose a novel and challenging benchmark, AutoEval-Video, tocomprehensively evaluate large vision-language models in open-ended videoquestion answering. The comprehensiveness of AutoEval-Video is demonstrated intwo aspects: 1) AutoEval-Video constructs open-ended video-questions across 9skill dimensions, addressing capabilities of perception, comprehension, andgeneration. 2) AutoEval-Video contains newly collected videos that cover over40 distinct themes. To efficiently evaluate responses to the open-endedquestions, we employ an LLM-based evaluation approach, but instead of merelyproviding a reference answer, we annotate unique evaluation rules for everysingle instance (video-question pair). To maximize the robustness of theserules, we develop a novel adversarial annotation mechanism. By usinginstance-specific rules as prompt, GPT-4, as an automatic evaluator, canachieve a stable evaluation accuracy of around 97.0%, comparable to the 94.9% -97.5% accuracy of a human evaluator. Furthermore, we assess the performance ofeight large vision-language models on AutoEval-Video. Among them, GPT-4V(ision)significantly outperforms other models, achieving an accuracy of 32.2%.However, there is still substantial room for improvement compared to humanaccuracy of 72.8%. By conducting an extensive case study, we uncover severaldrawbacks of GPT-4V, such as limited temporal and dynamic comprehension, andoverly general responses. Code is available athttps://github.com/Xiuyuan-Chen/AutoEval-Video.</description><author>Xiuyuan Chen, Yuan Lin, Yuchen Zhang, Weiran Huang</author><pubDate>Mon, 15 Jul 2024 16:42:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14906v2</guid></item><item><title>Leveraging Multimodal CycleGAN for the Generation of Anatomically Accurate Synthetic CT Scans from MRIs</title><link>http://arxiv.org/abs/2407.10888v1</link><description>In many clinical settings, the use of both Computed Tomography (CT) andMagnetic Resonance (MRI) is necessary to pursue a thorough understanding of thepatient's anatomy and to plan a suitable therapeutical strategy; this is oftenthe case in MRI-based radiotherapy, where CT is always necessary to prepare thedose delivery, as it provides the essential information about the radiationabsorption properties of the tissues. Sometimes, MRI is preferred to contourthe target volumes. However, this approach is often not the most efficient, asit is more expensive, time-consuming and, most importantly, stressful for thepatients. To overcome this issue, in this work, we analyse the capabilities ofdifferent configurations of Deep Learning models to generate synthetic CT scansfrom MRI, leveraging the power of Generative Adversarial Networks (GANs) and,in particular, the CycleGAN architecture, capable of working in an unsupervisedmanner and without paired images, which were not available. Several CycleGANmodels were trained unsupervised to generate CT scans from different MRImodalities with and without contrast agents. To overcome the problem of nothaving a ground truth, distribution-based metrics were used to assess themodel's performance quantitatively, together with a qualitative evaluationwhere physicians were asked to differentiate between real and synthetic imagesto understand how realistic the generated images were. The results show how,depending on the input modalities, the models can have very differentperformances; however, models with the best quantitative results, according tothe distribution-based metrics used, can generate very difficult images todistinguish from the real ones, even for physicians, demonstrating theapproach's potential.</description><author>Leonardo Crespi, Samuele Camnasio, Damiano Dei, Nicola Lambri, Pietro Mancosu, Marta Scorsetti, Daniele Loiacono</author><pubDate>Mon, 15 Jul 2024 16:38:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10888v1</guid></item><item><title>Hey, That's My Model! Introducing Chain &amp; Hash, An LLM Fingerprinting Technique</title><link>http://arxiv.org/abs/2407.10887v1</link><description>Amid growing concerns over the ease of theft and misuse of Large LanguageModels (LLMs), the need for fingerprinting models has increased.Fingerprinting, in this context, means that the model owner can link a givenmodel to their original version, thereby identifying if their model is beingmisused or has been completely stolen. In this paper, we first define a setfive properties a successful fingerprint should satisfy; namely, thefingerprint should be Transparent, Efficient, Persistent, Robust, andUnforgeable. Next, we propose Chain &amp; Hash, a new, simple fingerprintingapproach that implements a fingerprint with a cryptographic flavor, achievingall these properties. Chain &amp; Hash involves generating a set of questions (thefingerprints) along with a set of potential answers. These elements are hashedtogether using a secure hashing technique to select the value for eachquestion, hence providing an unforgeability property-preventing adversariesfrom claiming false ownership. We evaluate the Chain &amp; Hash technique onmultiple models and demonstrate its robustness against benign transformations,such as fine-tuning on different datasets, and adversarial attempts to erasethe fingerprint. Finally, our experiments demonstrate the efficiency ofimplementing Chain &amp; Hash and its utility, where fingerprinted models achievealmost the same performance as non-fingerprinted ones across differentbenchmarks.</description><author>Mark Russinovich, Ahmed Salem</author><pubDate>Mon, 15 Jul 2024 16:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10887v1</guid></item><item><title>SLIP: Securing LLMs IP Using Weights Decomposition</title><link>http://arxiv.org/abs/2407.10886v1</link><description>Large language models (LLMs) have recently seen widespread adoption, in bothacademia and industry. As these models grow, they become valuable intellectualproperty (IP), reflecting enormous investments by their owners. Moreover, thehigh cost of cloud-based deployment has driven interest towards deployment toedge devices, yet this risks exposing valuable parameters to theft andunauthorized use. Current methods to protect models' IP on the edge havelimitations in terms of practicality, loss in accuracy, or suitability torequirements. In this paper, we introduce a novel hybrid inference algorithm,named SLIP, designed to protect edge-deployed models from theft. SLIP is thefirst hybrid protocol that is both practical for real-world applications andprovably secure, while having zero accuracy degradation and minimal impact onlatency. It involves partitioning the model between two computing resources,one secure but expensive, and another cost-effective but vulnerable. This isachieved through matrix decomposition, ensuring that the secure resourceretains a maximally sensitive portion of the model's IP while performing aminimal amount of computations, and vice versa for the vulnerable resource.Importantly, the protocol includes security guarantees that prevent attackersfrom exploiting the partition to infer the secured information. Finally, wepresent experimental results that show the robustness and effectiveness of ourmethod, positioning it as a compelling solution for protecting LLMs.</description><author>Yehonathan Refael, Adam Hakim, Lev Greenberg, Tal Aviv, Satya Lokam, Ben Fishman, Shachar Seidman</author><pubDate>Mon, 15 Jul 2024 16:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10886v1</guid></item><item><title>DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning</title><link>http://arxiv.org/abs/2310.12128v2</link><description>Text-to-image (T2I) generation has seen significant growth over the past fewyears. Despite this, there has been little work on generating diagrams with T2Imodels. A diagram is a symbolic/schematic representation that explainsinformation using structurally rich and spatially complex visualizations (e.g.,a dense combination of related objects, text labels, directional arrows/lines,etc.). Existing state-of-the-art T2I models often fail at diagram generationbecause they lack fine-grained object layout control when many objects aredensely connected via complex relations such as arrows/lines, and also oftenfail to render comprehensible text labels. To address this gap, we presentDiagrammerGPT, a novel two-stage text-to-diagram generation frameworkleveraging the layout guidance capabilities of LLMs to generate more accuratediagrams. In the first stage, we use LLMs to generate and iteratively refine'diagram plans' (in a planner-auditor feedback loop). In the second stage, weuse a diagram generator, DiagramGLIGEN, and a text label rendering module togenerate diagrams (with clear text labels) following the diagram plans. Tobenchmark the text-to-diagram generation task, we introduce AI2D-Caption, adensely annotated diagram dataset built on top of the AI2D dataset. We showthat our DiagrammerGPT framework produces more accurate diagrams, outperformingexisting T2I models. We also provide comprehensive analysis, includingopen-domain diagram generation, multi-platform vector graphic diagramgeneration, human-in-the-loop editing, and multimodal planner/auditor LLMs.</description><author>Abhay Zala, Han Lin, Jaemin Cho, Mohit Bansal</author><pubDate>Mon, 15 Jul 2024 16:32:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12128v2</guid></item><item><title>Deep Causal Learning to Explain and Quantify The Geo-Tension's Impact on Natural Gas Market</title><link>http://arxiv.org/abs/2407.10878v1</link><description>Natural gas demand is a crucial factor for predicting natural gas prices andthus has a direct influence on the power system. However, existing methods facechallenges in assessing the impact of shocks, such as the outbreak of theRussian-Ukrainian war. In this context, we apply deep neural network-basedGranger causality to identify important drivers of natural gas demand.Furthermore, the resulting dependencies are used to construct a counterfactualcase without the outbreak of the war, providing a quantifiable estimate of theoverall effect of the shock on various German energy sectors. The code anddataset are available at https://github.com/bonaldli/CausalEnergy.</description><author>Philipp Kai Peter, Yulin Li, Ziyue Li, Wolfgang Ketter</author><pubDate>Mon, 15 Jul 2024 16:28:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10878v1</guid></item><item><title>RepVF: A Unified Vector Fields Representation for Multi-task 3D Perception</title><link>http://arxiv.org/abs/2407.10876v1</link><description>Concurrent processing of multiple autonomous driving 3D perception taskswithin the same spatiotemporal scene poses a significant challenge, inparticular due to the computational inefficiencies and feature competitionbetween tasks when using traditional multi-task learning approaches. This paperaddresses these issues by proposing a novel unified representation, RepVF,which harmonizes the representation of various perception tasks such as 3Dobject detection and 3D lane detection within a single framework. RepVFcharacterizes the structure of different targets in the scene through a vectorfield, enabling a single-head, multi-task learning model that significantlyreduces computational redundancy and feature competition. Building upon RepVF,we introduce RFTR, a network designed to exploit the inherent connectionsbetween different tasks by utilizing a hierarchical structure of queries thatimplicitly model the relationships both between and within tasks. This approacheliminates the need for task-specific heads and parameters, fundamentallyreducing the conflicts inherent in traditional multi-task learning paradigms.We validate our approach by combining labels from the OpenLane dataset with theWaymo Open dataset. Our work presents a significant advancement in theefficiency and effectiveness of multi-task perception in autonomous driving,offering a new perspective on handling multiple 3D perception taskssynchronously and in parallel. The code will be available at:https://github.com/jbji/RepVF</description><author>Chunliang Li, Wencheng Han, Junbo Yin, Sanyuan Zhao, Jianbing Shen</author><pubDate>Mon, 15 Jul 2024 16:25:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10876v1</guid></item><item><title>Random Channel Ablation for Robust Hand Gesture Classification with Multimodal Biosignals</title><link>http://arxiv.org/abs/2407.10874v1</link><description>Biosignal-based hand gesture classification is an important component ofeffective human-machine interaction. For multimodal biosignal sensing, themodalities often face data loss due to missing channels in the data which canadversely affect the gesture classification performance. To make theclassifiers robust to missing channels in the data, this paper proposes usingRandom Channel Ablation (RChA) during the training process. Ultrasound andforce myography (FMG) data were acquired from the forearm for 12 hand gesturesover 2 subjects. The resulting multimodal data had 16 total channels, 8 foreach modality. The proposed method was applied to convolutional neural networkarchitecture, and compared with baseline, imputation, and oracle methods. Using5-fold cross-validation for the two subjects, on average, 12.2% and 24.5%improvement was observed for gesture classification with up to 4 and 8 missingchannels respectively compared to the baseline. Notably, the proposed method isalso robust to an increase in the number of missing channels compared to othermethods. These results show the efficacy of using random channel ablation toimprove classifier robustness for multimodal and multi-channel biosignal-basedhand gesture classification.</description><author>Keshav Bimbraw, Jing Liu, Ye Wang, Toshiaki Koike-Akino</author><pubDate>Mon, 15 Jul 2024 16:23:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10874v1</guid></item><item><title>Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models</title><link>http://arxiv.org/abs/2407.10873v1</link><description>Automated heuristic design (AHD) has gained considerable attention for itspotential to automate the development of effective heuristics. The recentadvent of large language models (LLMs) has paved a new avenue for AHD, withinitial efforts focusing on framing AHD as an evolutionary program search (EPS)problem. However, inconsistent benchmark settings, inadequate baselines, and alack of detailed component analysis have left the necessity of integrating LLMswith search strategies and the true progress achieved by existing LLM-based EPSmethods to be inadequately justified. This work seeks to fulfill these researchqueries by conducting a large-scale benchmark comprising four LLM-based EPSmethods and four AHD problems across nine LLMs and five independent runs. Ourextensive experiments yield meaningful insights, providing empirical groundingfor the importance of evolutionary search in LLM-based AHD approaches, whilealso contributing to the advancement of future EPS algorithmic development. Tofoster accessibility and reproducibility, we have fully open-sourced ourbenchmark and corresponding results.</description><author>Rui Zhang, Fei Liu, Xi Lin, Zhenkun Wang, Zhichao Lu, Qingfu Zhang</author><pubDate>Mon, 15 Jul 2024 16:21:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10873v1</guid></item><item><title>Learning Distributions on Manifolds with Free-form Flows</title><link>http://arxiv.org/abs/2312.09852v2</link><description>We propose Manifold Free-Form Flows (M-FFF), a simple new generative modelfor data on manifolds. The existing approaches to learning a distribution onarbitrary manifolds are expensive at inference time, since sampling requiressolving a differential equation. Our method overcomes this limitation bysampling in a single function evaluation. The key innovation is to optimize aneural network via maximum likelihood on the manifold, possible by adapting thefree-form flow framework to Riemannian manifolds. M-FFF is straightforwardlyadapted to any manifold with a known projection. It consistently matches oroutperforms previous single-step methods specialized to specific manifolds, andis competitive with multi-step methods with typically two orders of magnitudefaster inference speed. We make our code public athttps://github.com/vislearn/FFF.</description><author>Peter Sorrenson, Felix Draxler, Armand Rousselot, Sander Hummerich, Ullrich Köthe</author><pubDate>Mon, 15 Jul 2024 16:19:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09852v2</guid></item><item><title>GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM</title><link>http://arxiv.org/abs/2407.10870v1</link><description>Large vision-language models (LVLMs), such as the Generative Pre-trainedTransformer 4-omni (GPT-4o), are emerging multi-modal foundation models whichhave great potential as powerful artificial-intelligence (AI) assistance toolsfor a myriad of applications, including healthcare, industrial, and academicsectors. Although such foundation models perform well in a wide range ofgeneral tasks, their capability without fine-tuning is often limited inspecialized tasks. However, full fine-tuning of large foundation models ischallenging due to enormous computation/memory/dataset requirements. We showthat GPT-4o can decode hand gestures from forearm ultrasound data even with nofine-tuning, and improves with few-shot, in-context learning.</description><author>Keshav Bimbraw, Ye Wang, Jing Liu, Toshiaki Koike-Akino</author><pubDate>Mon, 15 Jul 2024 16:18:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10870v1</guid></item><item><title>Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game</title><link>http://arxiv.org/abs/2406.11012v5</link><description>The New York Times Connections game has emerged as a popular and challengingpursuit for word puzzle enthusiasts. We collect 200 Connections games toevaluate the performance of state-of-the-art large language models (LLMs)against expert and novice human players. Our results show that even thebest-performing LLM, GPT-4o, which has otherwise shown impressive reasoningabilities on a wide variety of benchmarks, can only fully solve 8% of thegames. Compared to GPT-4o, novice and expert players perform better, withexpert human players significantly outperforming GPT-4o. To deepen ourunderstanding we create a taxonomy of the knowledge types required tosuccessfully categorize words in the Connections game, revealing that LLMsstruggle with associative, encyclopedic, and linguistic knowledge. Our findingsestablish the New York Times Connections game as a challenging benchmark forevaluating abstract reasoning capabilities in humans and AI systems.</description><author>Prisha Samadarshi, Mariam Mustafa, Anushka Kulkarni, Raven Rothkopf, Tuhin Chakrabarty, Smaranda Muresan</author><pubDate>Mon, 15 Jul 2024 16:17:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11012v5</guid></item><item><title>Deep Learning on Object-centric 3D Neural Fields</title><link>http://arxiv.org/abs/2312.13277v2</link><description>In recent years, Neural Fields (NFs) have emerged as an effective tool forencoding diverse continuous signals such as images, videos, audio, and 3Dshapes. When applied to 3D data, NFs offer a solution to the fragmentation andlimitations associated with prevalent discrete representations. However, giventhat NFs are essentially neural networks, it remains unclear whether and howthey can be seamlessly integrated into deep learning pipelines for solvingdownstream tasks. This paper addresses this research problem and introducesnf2vec, a framework capable of generating a compact latent representation foran input NF in a single inference pass. We demonstrate that nf2vec effectivelyembeds 3D objects represented by the input NFs and showcase how the resultingembeddings can be employed in deep learning pipelines to successfully addressvarious tasks, all while processing exclusively NFs. We test this framework onseveral NFs used to represent 3D surfaces, such as unsigned/signed distance andoccupancy fields. Moreover, we demonstrate the effectiveness of our approachwith more complex NFs that encompass both geometry and appearance of 3D objectssuch as neural radiance fields.</description><author>Pierluigi Zama Ramirez, Luca De Luigi, Daniele Sirocchi, Adriano Cardace, Riccardo Spezialetti, Francesco Ballerini, Samuele Salti, Luigi Di Stefano</author><pubDate>Mon, 15 Jul 2024 16:13:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13277v2</guid></item><item><title>Provable Robustness of (Graph) Neural Networks Against Data Poisoning and Backdoor Attacks</title><link>http://arxiv.org/abs/2407.10867v1</link><description>Generalization of machine learning models can be severely compromised by datapoisoning, where adversarial changes are applied to the training data, as wellas backdoor attacks that additionally manipulate the test data. Thesevulnerabilities have led to interest in certifying (i.e., proving) that suchchanges up to a certain magnitude do not affect test predictions. We, for thefirst time, certify Graph Neural Networks (GNNs) against poisoning and backdoorattacks targeting the node features of a given graph. Our certificates arewhite-box and based upon $(i)$ the neural tangent kernel, which characterizesthe training dynamics of sufficiently wide networks; and $(ii)$ a novelreformulation of the bilevel optimization problem describing poisoning as amixed-integer linear program. Consequently, we leverage our framework toprovide fundamental insights into the role of graph structure and itsconnectivity on the worst-case robustness behavior of convolution-based andPageRank-based GNNs. We note that our framework is more general and constitutesthe first approach to derive white-box poisoning certificates for NNs, whichcan be of independent interest beyond graph-related tasks.</description><author>Lukas Gosch, Mahalakshmi Sabanayagam, Debarghya Ghoshdastidar, Stephan Günnemann</author><pubDate>Mon, 15 Jul 2024 16:12:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10867v1</guid></item><item><title>R3D-AD: Reconstruction via Diffusion for 3D Anomaly Detection</title><link>http://arxiv.org/abs/2407.10862v1</link><description>3D anomaly detection plays a crucial role in monitoring parts for localizedinherent defects in precision manufacturing. Embedding-based andreconstruction-based approaches are among the most popular and successfulmethods. However, there are two major challenges to the practical applicationof the current approaches: 1) the embedded models suffer the prohibitivecomputational and storage due to the memory bank structure; 2) thereconstructive models based on the MAE mechanism fail to detect anomalies inthe unmasked regions. In this paper, we propose R3D-AD, reconstructinganomalous point clouds by diffusion model for precise 3D anomaly detection. Ourapproach capitalizes on the data distribution conversion of the diffusionprocess to entirely obscure the input's anomalous geometry. It step-wiselylearns a strict point-level displacement behavior, which methodically correctsthe aberrant points. To increase the generalization of the model, we furtherpresent a novel 3D anomaly simulation strategy named Patch-Gen to generaterealistic and diverse defect shapes, which narrows the domain gap betweentraining and testing. Our R3D-AD ensures a uniform spatial transformation,which allows straightforwardly generating anomaly results by distancecomparison. Extensive experiments show that our R3D-AD outperforms previousstate-of-the-art methods, achieving 73.4% Image-level AUROC on the Real3D-ADdataset and 74.9% Image-level AUROC on the Anomaly-ShapeNet dataset with anexceptional efficiency.</description><author>Zheyuan Zhou, Le Wang, Naiyu Fang, Zili Wang, Lemiao Qiu, Shuyou Zhang</author><pubDate>Mon, 15 Jul 2024 16:10:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10862v1</guid></item><item><title>Human-Centric Transformer for Domain Adaptive Action Recognition</title><link>http://arxiv.org/abs/2407.10860v1</link><description>We study the domain adaptation task for action recognition, namely domainadaptive action recognition, which aims to effectively transfer actionrecognition power from a label-sufficient source domain to a label-free targetdomain. Since actions are performed by humans, it is crucial to exploit humancues in videos when recognizing actions across domains. However, existingmethods are prone to losing human cues but prefer to exploit the correlationbetween non-human contexts and associated actions for recognition, and thecontexts of interest agnostic to actions would reduce recognition performancein the target domain. To overcome this problem, we focus on uncoveringhuman-centric action cues for domain adaptive action recognition, and ourconception is to investigate two aspects of human-centric action cues, namelyhuman cues and human-context interaction cues. Accordingly, our proposedHuman-Centric Transformer (HCTransformer) develops a decoupled human-centriclearning paradigm to explicitly concentrate on human-centric action cues indomain-variant video feature learning. Our HCTransformer first conductshuman-aware temporal modeling by a human encoder, aiming to avoid a loss ofhuman cues during domain-invariant video feature learning. Then, by aTransformer-like architecture, HCTransformer exploits domain-invariant andaction-correlated contexts by a context encoder, and further modelsdomain-invariant interaction between humans and action-correlated contexts. Weconduct extensive experiments on three benchmarks, namely UCF-HMDB,Kinetics-NecDrone and EPIC-Kitchens-UDA, and the state-of-the-art performancedemonstrates the effectiveness of our proposed HCTransformer.</description><author>Kun-Yu Lin, Jiaming Zhou, Wei-Shi Zheng</author><pubDate>Mon, 15 Jul 2024 16:10:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10860v1</guid></item><item><title>Physics-Inspired Generative Models in Medical Imaging: A Review</title><link>http://arxiv.org/abs/2407.10856v1</link><description>Physics-inspired generative models, in particular diffusion and Poisson flowmodels, enhance Bayesian methods and promise great utilities in medicalimaging. This review examines the transformative role of such generativemethods. First, a variety of physics-inspired generative models, includingDenoising Diffusion Probabilistic Models (DDPM), Score-based Diffusion Models,and Poisson Flow Generative Models (PFGM and PFGM++), are revisited, with anemphasis on their accuracy, robustness as well as acceleration. Then, majorapplications of physics-inspired generative models in medical imaging arepresented, comprising image reconstruction, image generation, and imageanalysis. Finally, future research directions are brainstormed, includingunification of physics-inspired generative models, integration withvision-language models (VLMs),and potential novel applications of generativemodels. Since the development of generative methods has been rapid, this reviewwill hopefully give peers and learners a timely snapshot of this new family ofphysics-driven generative models and help capitalize their enormous potentialfor medical imaging.</description><author>Dennis Hein, Afshin Bozorgpour, Dorit Merhof, Ge Wang</author><pubDate>Mon, 15 Jul 2024 16:08:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10856v1</guid></item><item><title>Weighted Grouped Query Attention in Transformers</title><link>http://arxiv.org/abs/2407.10855v1</link><description>The attention mechanism forms the foundational blocks for transformerlanguage models. Recent approaches show that scaling the model achieveshuman-level performance. However, with increasing demands for scaling andconstraints on hardware memory, the inference costs of these models remainhigh. To reduce the inference time, Multi-Query Attention (MQA) andGrouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieetal., 2023) respectively. In this paper, we propose a variation of Grouped-QueryAttention, termed Weighted Grouped-Query Attention (WGQA). We introduced newlearnable parameters for each key and value head in the T5 decoder attentionblocks, enabling the model to take a weighted average during finetuning. Ourmodel achieves an average of 0.53% improvement over GQA, and the performanceconverges to traditional Multi-head attention (MHA) with no additional overheadduring inference. We evaluated the introduction of these parameters andsubsequent finetuning informs the model about the grouping mechanism duringtraining, thereby enhancing performance. Additionally, we demonstrate thescaling laws in our analysis by comparing the results between T5-small andT5-base architecture.</description><author>Sai Sena Chinnakonduru, Astarag Mohapatra</author><pubDate>Mon, 15 Jul 2024 16:07:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10855v1</guid></item><item><title>Principal Component Flow Map Learning of PDEs from Incomplete, Limited, and Noisy Data</title><link>http://arxiv.org/abs/2407.10854v1</link><description>We present a computational technique for modeling the evolution of dynamicalsystems in a reduced basis, with a focus on the challenging problem of modelingpartially-observed partial differential equations (PDEs) on high-dimensionalnon-uniform grids. We address limitations of previous work on data-driven flowmap learning in the sense that we focus on noisy and limited data to movetoward data collection scenarios in real-world applications. Leveraging recentwork on modeling PDEs in modal and nodal spaces, we present a neural networkstructure that is suitable for PDE modeling with noisy and limited dataavailable only on a subset of the state variables or computational domain. Inparticular, spatial grid-point measurements are reduced using a learned lineartransformation, after which the dynamics are learned in this reduced basisbefore being transformed back out to the nodal space. This approach yields adrastically reduced parameterization of the neural network compared withprevious flow map models for nodal space learning. This primarily allows forsmaller training data sets, but also enables reduced training times.</description><author>Victor Churchill</author><pubDate>Mon, 15 Jul 2024 16:06:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10854v1</guid></item><item><title>An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases</title><link>http://arxiv.org/abs/2407.10853v1</link><description>Large language models (LLMs) can exhibit bias in a variety of ways. Suchbiases can create or exacerbate unfair outcomes for certain groups within aprotected attribute, including, but not limited to sex, race, sexualorientation, or age. This paper aims to provide a technical guide forpractitioners to assess bias and fairness risks in LLM use cases. The maincontribution of this work is a decision framework that allows practitioners todetermine which metrics to use for a specific LLM use case. To achieve this,this study categorizes LLM bias and fairness risks, maps those risks to ataxonomy of LLM use cases, and then formally defines various metrics to assesseach type of risk. As part of this work, several new bias and fairness metricsare introduced, including innovative counterfactual metrics as well as metricsbased on stereotype classifiers. Instead of focusing solely on the modelitself, the sensitivity of both prompt-risk and model-risk are taken intoaccount by defining evaluations at the level of an LLM use case, characterizedby a model and a population of prompts. Furthermore, because all of theevaluation metrics are calculated solely using the LLM output, the proposedframework is highly practical and easily actionable for practitioners.</description><author>Dylan Bouchard</author><pubDate>Mon, 15 Jul 2024 16:04:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10853v1</guid></item><item><title>Learning to Generate Answers with Citations via Factual Consistency Models</title><link>http://arxiv.org/abs/2406.13124v2</link><description>Large Language Models (LLMs) frequently hallucinate, impeding theirreliability in mission-critical situations. One approach to address this issueis to provide citations to relevant sources alongside generated content,enhancing the verifiability of generations. However, citing passages accuratelyin answers remains a substantial challenge. This paper proposes aweakly-supervised fine-tuning method leveraging factual consistency models(FCMs). Our approach alternates between generating texts with citations andsupervised fine-tuning with FCM-filtered citation data. Focused learning isintegrated into the objective, directing the fine-tuning process to emphasisethe factual unit tokens, as measured by an FCM. Results on the ALCE few-shotcitation benchmark with various instruction-tuned LLMs demonstrate superiorperformance compared to in-context learning, vanilla supervised fine-tuning,and state-of-the-art methods, with an average improvement of $34.1$, $15.5$,and $10.5$ citation F$_1$ points, respectively. Moreover, in a domain transfersetting we show that the obtained citation generation ability robustlytransfers to unseen datasets. Notably, our citation improvements contribute tothe lowest factual error rate across baselines.</description><author>Rami Aly, Zhiqiang Tang, Samson Tan, George Karypis</author><pubDate>Mon, 15 Jul 2024 16:04:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13124v2</guid></item><item><title>Rotationally Invariant Latent Distances for Uncertainty Estimation of Relaxed Energy Predictions by Graph Neural Network Potentials</title><link>http://arxiv.org/abs/2407.10844v1</link><description>Graph neural networks (GNNs) have been shown to be astonishingly capablemodels for molecular property prediction, particularly as surrogates forexpensive density functional theory calculations of relaxed energy for novelmaterial discovery. However, one limitation of GNNs in this context is the lackof useful uncertainty prediction methods, as this is critical to the materialdiscovery pipeline. In this work, we show that uncertainty quantification forrelaxed energy calculations is more complex than uncertainty quantification forother kinds of molecular property prediction, due to the effect that structureoptimizations have on the error distribution. We propose that distribution-freetechniques are more useful tools for assessing calibration, recalibrating, anddeveloping uncertainty prediction methods for GNNs performing relaxed energycalculations. We also develop a relaxed energy task for evaluating uncertaintymethods for equivariant GNNs, based on distribution-free recalibration andusing the Open Catalyst Project dataset. We benchmark a set of popularuncertainty prediction methods on this task, and show that latent distancemethods, with our novel improvements, are the most well-calibrated andeconomical approach for relaxed energy calculations. Finally, we demonstratethat our latent space distance method produces results which align with ourexpectations on a clustering example, and on specific equation of state andadsorbate coverage examples from outside the training dataset.</description><author>Joseph Musielewicz, Janice Lan, Matt Uyttendaele, John R. Kitchin</author><pubDate>Mon, 15 Jul 2024 15:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10844v1</guid></item><item><title>Offline Reinforcement Learning with Imputed Rewards</title><link>http://arxiv.org/abs/2407.10839v1</link><description>Offline Reinforcement Learning (ORL) offers a robust solution to trainingagents in applications where interactions with the environment must be strictlylimited due to cost, safety, or lack of accurate simulation environments.Despite its potential to facilitate deployment of artificial agents in the realworld, Offline Reinforcement Learning typically requires very manydemonstrations annotated with ground-truth rewards. Consequently,state-of-the-art ORL algorithms can be difficult or impossible to apply indata-scarce scenarios. In this paper we propose a simple but effective RewardModel that can estimate the reward signal from a very limited sample ofenvironment transitions annotated with rewards. Once the reward signal ismodeled, we use the Reward Model to impute rewards for a large sample ofreward-free transitions, thus enabling the application of ORL techniques. Wedemonstrate the potential of our approach on several D4RL continuous locomotiontasks. Our results show that, using only 1\% of reward-labeled transitions fromthe original datasets, our learned reward model is able to impute rewards forthe remaining 99\% of the transitions, from which performant agents can belearned using Offline Reinforcement Learning.</description><author>Carlo Romeo, Andrew D. Bagdanov</author><pubDate>Mon, 15 Jul 2024 15:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10839v1</guid></item><item><title>RoboGolf: Mastering Real-World Minigolf with a Reflective Multi-Modality Vision-Language Model</title><link>http://arxiv.org/abs/2406.10157v3</link><description>Minigolf, a game with countless court layouts, and complex ball motion,constitutes a compelling real-world testbed for the study of embodiedintelligence. As it not only challenges spatial and kinodynamic reasoning butalso requires reflective and corrective capacities to address erroneouslydesigned courses. We introduce RoboGolf, a VLM-based framework that perceivesdual-camera visual inputs with nested VLM-empowered closed-loop control andreflective equilibrium loop. Extensive experiments demonstrate theeffectiveness of RoboGolf on challenging minigolf courts including those thatare impossible to finish.</description><author>Hantao Zhou, Tianying Ji, Lukas Sommerhalder, Michael Goerner, Norman Hendrich, Jianwei Zhang, Fuchun Sun, Huazhe Xu</author><pubDate>Mon, 15 Jul 2024 15:51:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10157v3</guid></item><item><title>Data-Guided Physics-Informed Neural Networks for Solving Inverse Problems in Partial Differential Equations</title><link>http://arxiv.org/abs/2407.10836v1</link><description>Physics-informed neural networks (PINNs) represent a significant advancementin scientific machine learning by integrating fundamental physical laws intotheir architecture through loss functions. PINNs have been successfully appliedto solve various forward and inverse problems in partial differential equations(PDEs). However, a notable challenge can emerge during the early trainingstages when solving inverse problems. Specifically, data losses remain highwhile PDE residual losses are minimized rapidly, thereby exacerbating theimbalance between loss terms and impeding the overall efficiency of PINNs. Toaddress this challenge, this study proposes a novel framework termeddata-guided physics-informed neural networks (DG-PINNs). The DG-PINNs frameworkis structured into two distinct phases: a pre-training phase and a fine-tuningphase. In the pre-training phase, a loss function with only the data loss isminimized in a neural network. In the fine-tuning phase, a composite lossfunction, which consists of the data loss, PDE residual loss, and, ifavailable, initial and boundary condition losses, is minimized in the sameneural network. Notably, the pre-training phase ensures that the data loss isalready at a low value before the fine-tuning phase commences. This approachenables the fine-tuning phase to converge to a minimal composite loss functionwith fewer iterations compared to existing PINNs. To validate theeffectiveness, noise-robustness, and efficiency of DG-PINNs, extensivenumerical investigations are conducted on inverse problems related to severalclassical PDEs, including the heat equation, wave equation, Euler--Bernoullibeam equation, and Navier--Stokes equation. The numerical results demonstratethat DG-PINNs can accurately solve these inverse problems and exhibitrobustness against noise in training data.</description><author>Wei Zhou, Y. F. Xu</author><pubDate>Mon, 15 Jul 2024 15:47:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10836v1</guid></item><item><title>Constrained 6-DoF Grasp Generation on Complex Shapes for Improved Dual-Arm Manipulation</title><link>http://arxiv.org/abs/2404.04643v2</link><description>Efficiently generating grasp poses tailored to specific regions of an objectis vital for various robotic manipulation tasks, especially in a dual-armsetup. This scenario presents a significant challenge due to the complexgeometries involved, requiring a deep understanding of the local geometry togenerate grasps efficiently on the specified constrained regions. Existingmethods only explore settings involving table-top/small objects and requireaugmented datasets to train, limiting their performance on complex objects. Wepropose CGDF: Constrained Grasp Diffusion Fields, a diffusion-based graspgenerative model that generalizes to objects with arbitrary geometries, as wellas generates dense grasps on the target regions. CGDF uses a part-guideddiffusion approach that enables it to get high sample efficiency in constrainedgrasping without explicitly training on massive constraint-augmented datasets.We provide qualitative and quantitative comparisons using analytical metricsand in simulation, in both unconstrained and constrained settings to show thatour method can generalize to generate stable grasps on complex objects,especially useful for dual-arm manipulation settings, while existing methodsstruggle to do so.</description><author>Gaurav Singh, Sanket Kalwar, Md Faizal Karim, Bipasha Sen, Nagamanikandan Govindan, Srinath Sridhar, K Madhava Krishna</author><pubDate>Mon, 15 Jul 2024 15:45:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04643v2</guid></item><item><title>Exploration in Knowledge Transfer Utilizing Reinforcement Learning</title><link>http://arxiv.org/abs/2407.10835v1</link><description>The contribution focuses on the problem of exploration within the task ofknowledge transfer. Knowledge transfer refers to the useful application of theknowledge gained while learning the source task in the target task. Theintended benefit of knowledge transfer is to speed up the learning process ofthe target task. The article aims to compare several exploration methods usedwithin a deep transfer learning algorithm, particularly Deep Target Transfer$Q$-learning. The methods used are $\epsilon$-greedy, Boltzmann, and upperconfidence bound exploration. The aforementioned transfer learning algorithmsand exploration methods were tested on the virtual drone problem. The resultshave shown that the upper confidence bound algorithm performs the best out ofthese options. Its sustainability to other applications is to be checked.</description><author>Adam Jedlička, Tatiana Valentine Guy</author><pubDate>Mon, 15 Jul 2024 15:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10835v1</guid></item><item><title>MetaLLM: A High-performant and Cost-efficient Dynamic Framework for Wrapping LLMs</title><link>http://arxiv.org/abs/2407.10834v1</link><description>The rapid progress in machine learning (ML) has brought forth many largelanguage models (LLMs) that excel in various tasks and areas. These LLMs comewith different abilities and costs in terms of computation or pricing. Sincethe demand for each query can vary, e.g., because of the queried domain or itscomplexity, defaulting to one LLM in an application is not usually the bestchoice, whether it is the biggest, priciest, or even the one with the bestaverage test performance. Consequently, picking the right LLM that is bothaccurate and cost-effective for an application remains a challenge. In thispaper, we introduce MetaLLM, a framework that dynamically and intelligentlyroutes each query to the optimal LLM (among several available LLMs) forclassification tasks, achieving significantly improved accuracy andcost-effectiveness. By framing the selection problem as a multi-armed bandit,MetaLLM balances prediction accuracy and cost efficiency under uncertainty. Ourexperiments, conducted on popular LLM platforms such as OpenAI's GPT models,Amazon's Titan, Anthropic's Claude, and Meta's LLaMa, showcase MetaLLM'sefficacy in real-world scenarios, laying the groundwork for future extensionsbeyond classification tasks.</description><author>Quang H. Nguyen, Duy C. Hoang, Juliette Decugis, Saurav Manchanda, Nitesh V. Chawla, Khoa D. Doan</author><pubDate>Mon, 15 Jul 2024 15:45:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10834v1</guid></item><item><title>MoE-DiffIR: Task-customized Diffusion Priors for Universal Compressed Image Restoration</title><link>http://arxiv.org/abs/2407.10833v1</link><description>We present MoE-DiffIR, an innovative universal compressed image restoration(CIR) method with task-customized diffusion priors. This intends to handle twopivotal challenges in the existing CIR methods: (i) lacking adaptability anduniversality for different image codecs, e.g., JPEG and WebP; (ii) poor texturegeneration capability, particularly at low bitrates. Specifically, ourMoE-DiffIR develops the powerful mixture-of-experts (MoE) prompt module, wheresome basic prompts cooperate to excavate the task-customized diffusion priorsfrom Stable Diffusion (SD) for each compression task. Moreover, thedegradation-aware routing mechanism is proposed to enable the flexibleassignment of basic prompts. To activate and reuse the cross-modalitygeneration prior of SD, we design the visual-to-text adapter for MoE-DiffIR,which aims to adapt the embedding of low-quality images from the visual domainto the textual domain as the textual guidance for SD, enabling more consistentand reasonable texture generation. We also construct one comprehensivebenchmark dataset for universal CIR, covering 21 types of degradations from 7popular traditional and learned codecs. Extensive experiments on universal CIRhave demonstrated the excellent robustness and texture restoration capabilityof our proposed MoE-DiffIR. The project can be found athttps://renyulin-f.github.io/MoE-DiffIR.github.io/.</description><author>Yulin Ren, Xin Li, Bingchen Li, Xingrui Wang, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen</author><pubDate>Mon, 15 Jul 2024 15:43:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10833v1</guid></item><item><title>Temporal Event Stereo via Joint Learning with Stereoscopic Flow</title><link>http://arxiv.org/abs/2407.10831v1</link><description>Event cameras are dynamic vision sensors inspired by the biological retina,characterized by their high dynamic range, high temporal resolution, and lowpower consumption. These features make them capable of perceiving 3Denvironments even in extreme conditions. Event data is continuous across thetime dimension, which allows a detailed description of each pixel's movements.To fully utilize the temporally dense and continuous nature of event cameras,we propose a novel temporal event stereo, a framework that continuously usesinformation from previous time steps. This is accomplished through thesimultaneous training of an event stereo matching network alongsidestereoscopic flow, a new concept that captures all pixel movements from stereocameras. Since obtaining ground truth for optical flow during training ischallenging, we propose a method that uses only disparity maps to train thestereoscopic flow. The performance of event-based stereo matching is enhancedby temporally aggregating information using the flows. We have achievedstate-of-the-art performance on the MVSEC and the DSEC datasets. The method iscomputationally efficient, as it stacks previous information in a cascadingmanner. The code is available athttps://github.com/mickeykang16/TemporalEventStereo.</description><author>Hoonhee Cho, Jae-Young Kang, Kuk-Jin Yoon</author><pubDate>Mon, 15 Jul 2024 15:43:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10831v1</guid></item><item><title>Explicit Modelling of Theory of Mind for Belief Prediction in Nonverbal Social Interactions</title><link>http://arxiv.org/abs/2407.06762v2</link><description>We propose MToMnet - a Theory of Mind (ToM) neural network for predictingbeliefs and their dynamics during human social interactions from multimodalinput. ToM is key for effective nonverbal human communication andcollaboration, yet, existing methods for belief modelling have not includedexplicit ToM modelling or have typically been limited to one or two modalities.MToMnet encodes contextual cues (scene videos and object locations) andintegrates them with person-specific cues (human gaze and body language) in aseparate MindNet for each person. Inspired by prior research on socialcognition and computational ToM, we propose three different MToMnet variants:two involving fusion of latent representations and one involving re-ranking ofclassification scores. We evaluate our approach on two challenging real-worlddatasets, one focusing on belief prediction, while the other examining beliefdynamics prediction. Our results demonstrate that MToMnet surpasses existingmethods by a large margin while at the same time requiring a significantlysmaller number of parameters. Taken together, our method opens up a highlypromising direction for future work on artificial intelligent systems that canrobustly predict human beliefs from their non-verbal behaviour and, as such,more effectively collaborate with humans.</description><author>Matteo Bortoletto, Constantin Ruhdorfer, Lei Shi, Andreas Bulling</author><pubDate>Mon, 15 Jul 2024 15:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06762v2</guid></item><item><title>BiasScanner: Automatic Detection and Classification of News Bias to Strengthen Democracy</title><link>http://arxiv.org/abs/2407.10829v1</link><description>The increasing consumption of news online in the 21st century coincided withincreased publication of disinformation, biased reporting, hate speech andother unwanted Web content. We describe BiasScanner, an application that aimsto strengthen democracy by supporting news consumers with scrutinizing newsarticles they are reading online. BiasScanner contains a server-sidepre-trained large language model to identify biased sentences of news articlesand a front-end Web browser plug-in. At the time of writing, BiasScanner canidentify and classify more than two dozen types of media bias at the sentencelevel, making it the most fine-grained model and only deployed application(automatic system in use) of its kind. It was implemented in a light-weight andprivacy-respecting manner, and in addition to highlighting likely biasedsentence it also provides explanations for each classification decision as wellas a summary analysis for each news article. While prior research has addressednews bias detection, we are not aware of any work that resulted in a deployedbrowser plug-in (c.f. also biasscanner.org for a Web demo).</description><author>Tim Menzner, Jochen L. Leidner</author><pubDate>Mon, 15 Jul 2024 15:42:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10829v1</guid></item><item><title>Towards Enhanced Classification of Abnormal Lung sound in Multi-breath: A Light Weight Multi-label and Multi-head Attention Classification Method</title><link>http://arxiv.org/abs/2407.10828v1</link><description>This study aims to develop an auxiliary diagnostic system for classifyingabnormal lung respiratory sounds, enhancing the accuracy of automatic abnormalbreath sound classification through an innovative multi-label learning approachand multi-head attention mechanism. Addressing the issue of class imbalance andlack of diversity in existing respiratory sound datasets, our study employs alightweight and highly accurate model, using a two-dimensional label set torepresent multiple respiratory sound characteristics. Our method achieved a59.2% ICBHI score in the four-category task on the ICBHI2017 dataset,demonstrating its advantages in terms of lightweight and high accuracy. Thisstudy not only improves the accuracy of automatic diagnosis of lung respiratorysound abnormalities but also opens new possibilities for clinical applications.</description><author>Yi-Wei Chua, Yun-Chien Cheng</author><pubDate>Mon, 15 Jul 2024 15:40:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10828v1</guid></item><item><title>LLM Circuit Analyses Are Consistent Across Training and Scale</title><link>http://arxiv.org/abs/2407.10827v1</link><description>Most currently deployed large language models (LLMs) undergo continuoustraining or additional finetuning. By contrast, most research into LLMs'internal mechanisms focuses on models at one snapshot in time (the end ofpre-training), raising the question of whether their results generalize toreal-world settings. Existing studies of mechanisms over time focus onencoder-only or toy models, which differ significantly from most deployedmodels. In this study, we track how model mechanisms, operationalized ascircuits, emerge and evolve across 300 billion tokens of training indecoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters.We find that task abilities and the functional components that support thememerge consistently at similar token counts across scale. Moreover, althoughsuch components may be implemented by different attention heads over time, theoverarching algorithm that they implement remains. Surprisingly, both thesealgorithms and the types of components involved therein can replicate acrossmodel scale. These results suggest that circuit analyses conducted on smallmodels at the end of pre-training can provide insights that still apply afteradditional pre-training and over model scale.</description><author>Curt Tigges, Michael Hanna, Qinan Yu, Stella Biderman</author><pubDate>Mon, 15 Jul 2024 15:38:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10827v1</guid></item><item><title>Wicked Oddities: Selectively Poisoning for Effective Clean-Label Backdoor Attacks</title><link>http://arxiv.org/abs/2407.10825v1</link><description>Deep neural networks are vulnerable to backdoor attacks, a type ofadversarial attack that poisons the training data to manipulate the behavior ofmodels trained on such data. Clean-label attacks are a more stealthy form ofbackdoor attacks that can perform the attack without changing the labels ofpoisoned data. Early works on clean-label attacks added triggers to a randomsubset of the training set, ignoring the fact that samples contribute unequallyto the attack's success. This results in high poisoning rates and low attacksuccess rates. To alleviate the problem, several supervised learning-basedsample selection strategies have been proposed. However, these methods assumeaccess to the entire labeled training set and require training, which isexpensive and may not always be practical. This work studies a new and morepractical (but also more challenging) threat model where the attacker onlyprovides data for the target class (e.g., in face recognition systems) and hasno knowledge of the victim model or any other classes in the training set. Westudy different strategies for selectively poisoning a small set of trainingsamples in the target class to boost the attack success rate in this setting.Our threat model poses a serious threat in training machine learning modelswith third-party datasets, since the attack can be performed effectively withlimited information. Experiments on benchmark datasets illustrate theeffectiveness of our strategies in improving clean-label backdoor attacks.</description><author>Quang H. Nguyen, Nguyen Ngoc-Hieu, The-Anh Ta, Thanh Nguyen-Tang, Hoang Thanh-Tung, Khoa D. Doan</author><pubDate>Mon, 15 Jul 2024 15:38:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10825v1</guid></item><item><title>A Conservative Approach for Few-Shot Transfer in Off-Dynamics Reinforcement Learning</title><link>http://arxiv.org/abs/2312.15474v3</link><description>Off-dynamics Reinforcement Learning (ODRL) seeks to transfer a policy from asource environment to a target environment characterized by distinct yetsimilar dynamics. In this context, traditional RL agents depend excessively onthe dynamics of the source environment, resulting in the discovery of policiesthat excel in this environment but fail to provide reasonable performance inthe target one. In the few-shot framework, a limited number of transitions fromthe target environment are introduced to facilitate a more effective transfer.Addressing this challenge, we propose an innovative approach inspired by recentadvancements in Imitation Learning and conservative RL algorithms. The proposedmethod introduces a penalty to regulate the trajectories generated by thesource-trained policy. We evaluate our method across various environmentsrepresenting diverse off-dynamics conditions, where access to the targetenvironment is extremely limited. These experiments include high-dimensionalsystems relevant to real-world applications. Across most tested scenarios, ourproposed method demonstrates performance improvements compared to existingbaselines.</description><author>Paul Daoudi, Christophe Prieur, Bogdan Robu, Merwan Barlier, Ludovic Dos Santos</author><pubDate>Mon, 15 Jul 2024 15:36:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15474v3</guid></item><item><title>Enabling MCTS Explainability for Sequential Planning Through Computation Tree Logic</title><link>http://arxiv.org/abs/2407.10820v1</link><description>Monte Carlo tree search (MCTS) is one of the most capable online searchalgorithms for sequential planning tasks, with significant applications inareas such as resource allocation and transit planning. Despite its strongperformance in real-world deployment, the inherent complexity of MCTS makes itchallenging to understand for users without technical background. This paperconsiders the use of MCTS in transportation routing services, where thealgorithm is integrated to develop optimized route plans. These plans arerequired to meet a range of constraints and requirements simultaneously,further complicating the task of explaining the algorithm's operation inreal-world contexts. To address this critical research gap, we introduce anovel computation tree logic-based explainer for MCTS. Our framework begins bytaking user-defined requirements and translating them into rigorous logicspecifications through the use of language templates. Then, our explainerincorporates a logic verification and quantitative evaluation module thatvalidates the states and actions traversed by the MCTS algorithm. The outcomesof this analysis are then rendered into human-readable descriptive text using asecond set of language templates. The user satisfaction of our approach wasassessed through a survey with 82 participants. The results indicated that ourexplanatory approach significantly outperforms other baselines in userpreference.</description><author>Ziyan An, Hendrik Baier, Abhishek Dubey, Ayan Mukhopadhyay, Meiyi Ma</author><pubDate>Mon, 15 Jul 2024 15:35:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10820v1</guid></item><item><title>Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation</title><link>http://arxiv.org/abs/2407.10817v1</link><description>As large language models (LLMs) advance, it becomes more challenging toreliably evaluate their output due to the high costs of human evaluation. Tomake progress towards better LLM autoraters, we introduce FLAMe, a family ofFoundational Large Autorater Models. FLAMe is trained on our large and diversecollection of 100+ quality assessment tasks comprising 5M+ human judgments,curated and standardized using publicly released human evaluations fromprevious research. FLAMe significantly improves generalization to a widevariety of held-out tasks, outperforming LLMs trained on proprietary data likeGPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as apowerful starting point for further downstream fine-tuning, using rewardmodeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, ourFLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generativemodel trained exclusively on permissively licensed data, outperforming bothGPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a morecomputationally efficient approach using a novel tail-patch fine-tuningstrategy to optimize our FLAMe multitask mixture for reward modeling evaluation(FLAMe-Opt-RM), offering competitive RewardBench performance while requiringapproximately 25x less training datapoints. Overall, our FLAMe variantsoutperform all popular proprietary LLM-as-a-Judge models we consider across 8out of 12 autorater evaluation benchmarks, encompassing 53 quality assessmenttasks, including RewardBench and LLM-AggreFact. Finally, our analysis revealsthat FLAMe is significantly less biased than these LLM-as-a-Judge models on theCoBBLEr autorater bias benchmark, while effectively identifying high-qualityresponses for code generation.</description><author>Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, Yun-Hsuan Sung</author><pubDate>Mon, 15 Jul 2024 15:33:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10817v1</guid></item><item><title>Category Adaptation Meets Projected Distillation in Generalized Continual Category Discovery</title><link>http://arxiv.org/abs/2308.12112v3</link><description>Generalized Continual Category Discovery (GCCD) tackles learning fromsequentially arriving, partially labeled datasets while uncovering newcategories. Traditional methods depend on feature distillation to preventforgetting the old knowledge. However, this strategy restricts the model'sability to adapt and effectively distinguish new categories. To address this,we introduce a novel technique integrating a learnable projector with featuredistillation, thus enhancing model adaptability without sacrificing pastknowledge. The resulting distribution shift of the previously learnedcategories is mitigated with the auxiliary category adaptation network. Wedemonstrate that while each component offers modest benefits individually,their combination - dubbed CAMP (Category Adaptation Meets Projecteddistillation) - significantly improves the balance between learning newinformation and retaining old. CAMP exhibits superior performance acrossseveral GCCD and Class Incremental Learning scenarios. The code is available athttps://github.com/grypesc/CAMP.</description><author>Grzegorz Rypeść, Daniel Marczak, Sebastian Cygert, Tomasz Trzciński, Bartłomiej Twardowski</author><pubDate>Mon, 15 Jul 2024 15:33:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12112v3</guid></item><item><title>Pathology-knowledge Enhanced Multi-instance Prompt Learning for Few-shot Whole Slide Image Classification</title><link>http://arxiv.org/abs/2407.10814v1</link><description>Current multi-instance learning algorithms for pathology image analysis oftenrequire a substantial number of Whole Slide Images for effective training butexhibit suboptimal performance in scenarios with limited learning data. Inclinical settings, restricted access to pathology slides is inevitable due topatient privacy concerns and the prevalence of rare or emerging diseases. Theemergence of the Few-shot Weakly Supervised WSI Classification accommodates thesignificant challenge of the limited slide data and sparse slide-level labelsfor diagnosis. Prompt learning based on the pre-trained models (\eg, CLIP)appears to be a promising scheme for this setting; however, current research inthis area is limited, and existing algorithms often focus solely on patch-levelprompts or confine themselves to language prompts. This paper proposes amulti-instance prompt learning framework enhanced with pathology knowledge,\ie, integrating visual and textual prior knowledge into prompts at both patchand slide levels. The training process employs a combination of static andlearnable prompts, effectively guiding the activation of pre-trained models andfurther facilitating the diagnosis of key pathology patterns. LightweightMessenger (self-attention) and Summary (attention-pooling) layers areintroduced to model relationships between patches and slides within the samepatient data. Additionally, alignment-wise contrastive losses ensure thefeature-level alignment between visual and textual learnable prompts for bothpatches and slides. Our method demonstrates superior performance in threechallenging clinical tasks, significantly outperforming comparative few-shotmethods.</description><author>Linhao Qu, Dingkang Yang, Dan Huang, Qinhao Guo, Rongkui Luo, Shaoting Zhang, Xiaosong Wang</author><pubDate>Mon, 15 Jul 2024 15:31:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10814v1</guid></item><item><title>Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark</title><link>http://arxiv.org/abs/2402.13654v2</link><description>This paper presents a learning-based control strategy for non-linear throttlevalves with an asymmetric hysteresis, leading to a near-optimal controllerwithout requiring any prior knowledge about the environment. We start with acarefully tuned Proportional Integrator (PI) controller and exploit the recentadvances in Reinforcement Learning (RL) with Guides to improve the closed-loopbehavior by learning from the additional interactions with the valve. We testthe proposed control method in various scenarios on three different valves, allhighlighting the benefits of combining both PI and RL frameworks to improvecontrol performance in non-linear stochastic systems. In all the experimentaltest cases, the resulting agent has a better sample efficiency than traditionalRL agents and outperforms the PI controller.</description><author>Paul Daoudi, Bojan Mavkov, Bogdan Robu, Christophe Prieur, Emmanuel Witrant, Merwan Barlier, Ludovic Dos Santos</author><pubDate>Mon, 15 Jul 2024 15:27:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13654v2</guid></item><item><title>GuideLight: "Industrial Solution" Guidance for More Practical Traffic Signal Control Agents</title><link>http://arxiv.org/abs/2407.10811v1</link><description>Currently, traffic signal control (TSC) methods based on reinforcementlearning (RL) have proven superior to traditional methods. However, most RLmethods face difficulties when applied in the real world due to three factors:input, output, and the cycle-flow relation. The industry's observable input ismuch more limited than simulation-based RL methods. For real-world solutions,only flow can be reliably collected, whereas common RL methods need more. Forthe output action, most RL methods focus on acyclic control, which real-worldsignal controllers do not support. Most importantly, industry standards requirea consistent cycle-flow relationship: non-decreasing and different responsestrategies for low, medium, and high-level flows, which is ignored by the RLmethods. To narrow the gap between RL methods and industry standards, weinnovatively propose to use industry solutions to guide the RL agent.Specifically, we design behavior cloning and curriculum learning to guide theagent to mimic and meet industry requirements and, at the same time, leveragethe power of exploration and exploitation in RL for better performance. Wetheoretically prove that such guidance can largely decrease the samplecomplexity to polynomials in the horizon when searching for an optimal policy.Our rigid experiments show that our method has good cycle-flow relation andsuperior performance.</description><author>Haoyuan Jiang, Xuantang Xiong, Ziyue Li, Hangyu Mao, Guanghu Sui, Jingqing Ruan, Yuheng Cheng, Hua Wei, Wolfgang Ketter, Rui Zhao</author><pubDate>Mon, 15 Jul 2024 15:26:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10811v1</guid></item><item><title>FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect Knowledge Queries</title><link>http://arxiv.org/abs/2407.10810v1</link><description>Intelligence is key to advancing integrated circuit (IC) fabrication. Recentbreakthroughs in Large Multimodal Models (LMMs) have unlocked unparalleledabilities in understanding images and text, fostering intelligent fabrication.Leveraging the power of LMMs, we introduce FabGPT, a customized IC fabricationlarge multimodal model for wafer defect knowledge query. FabGPT manifestsexpertise in conducting defect detection in Scanning Electron Microscope (SEM)images, performing root cause analysis, and providing expert question-answering(Q&amp;A) on fabrication processes. FabGPT matches enhanced multimodal features toautomatically detect minute defects under complex wafer backgrounds and reducethe subjectivity of manual threshold settings. Besides, the proposed modulationmodule and interactive corpus training strategy embed wafer defect knowledgeinto the pre-trained model, effectively balancing Q&amp;A queries related to defectknowledge and original knowledge and mitigating the modality bias issues.Experiments on in-house fab data (SEM-WaD) show that our FabGPT achievessignificant performance improvement in wafer defect detection and knowledgequerying.</description><author>Yuqi Jiang, Xudong Lu, Qian Jin, Qi Sun, Hanming Wu, Cheng Zhuo</author><pubDate>Mon, 15 Jul 2024 15:25:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10810v1</guid></item><item><title>Employing Sentence Space Embedding for Classification of Data Stream from Fake News Domain</title><link>http://arxiv.org/abs/2407.10807v1</link><description>Tabular data is considered the last unconquered castle of deep learning, yetthe task of data stream classification is stated to be an equally important anddemanding research area. Due to the temporal constraints, it is assumed thatdeep learning methods are not the optimal solution for application in thisfield. However, excluding the entire -- and prevalent -- group of methods seemsrather rash given the progress that has been made in recent years in itsdevelopment. For this reason, the following paper is the first to present anapproach to natural language data stream classification using the sentencespace method, which allows for encoding text into the form of a discretedigital signal. This allows the use of convolutional deep networks dedicated toimage classification to solve the task of recognizing fake news based on textdata. Based on the real-life Fakeddit dataset, the proposed approach wascompared with state-of-the-art algorithms for data stream classification basedon generalization ability and time complexity.</description><author>Paweł Zyblewski, Jakub Klikowski, Weronika Borek-Marciniec, Paweł Ksieniewicz</author><pubDate>Mon, 15 Jul 2024 15:23:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10807v1</guid></item><item><title>ChEX: Interactive Localization and Region Description in Chest X-rays</title><link>http://arxiv.org/abs/2404.15770v2</link><description>Report generation models offer fine-grained textual interpretations ofmedical images like chest X-rays, yet they often lack interactivity (i.e. theability to steer the generation process through user queries) and localizedinterpretability (i.e. visually grounding their predictions), which we deemessential for future adoption in clinical practice. While there have beenefforts to tackle these issues, they are either limited in their interactivityby not supporting textual queries or fail to also offer localizedinterpretability. Therefore, we propose a novel multitask architecture andtraining paradigm integrating textual prompts and bounding boxes for diverseaspects like anatomical regions and pathologies. We call this approach theChest X-Ray Explainer (ChEX). Evaluations across a heterogeneous set of 9 chestX-ray tasks, including localized image interpretation and report generation,showcase its competitiveness with SOTA models while additional analysisdemonstrates ChEX's interactive capabilities. Code:https://github.com/philip-mueller/chex</description><author>Philip Müller, Georgios Kaissis, Daniel Rueckert</author><pubDate>Mon, 15 Jul 2024 15:22:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15770v2</guid></item><item><title>Enhancing Robustness to Noise Corruption for Point Cloud Model via Spatial Sorting and Set-Mixing Aggregation Module</title><link>http://arxiv.org/abs/2407.10806v1</link><description>Current models for point cloud recognition demonstrate promising performanceon synthetic datasets. However, real-world point cloud data inevitably containsnoise, impacting model robustness. While recent efforts focus on enhancingrobustness through various strategies, there still remains a gap incomprehensive analyzes from the standpoint of network architecture design.Unlike traditional methods that rely on generic techniques, our approachoptimizes model robustness to noise corruption through network architecturedesign. Inspired by the token-mixing technique applied in 2D images, we proposeSet-Mixer, a noise-robust aggregation module which facilitates communicationamong all points to extract geometric shape information and mitigating theinfluence of individual noise points. A sorting strategy is designed to enableour module to be invariant to point permutation, which also tackles theunordered structure of point cloud and introduces consistent relative spatialinformation. Experiments conducted on ModelNet40-C indicate that Set-Mixersignificantly enhances the model performance on noisy point clouds,underscoring its potential to advance real-world applicability in 3Drecognition and perception tasks.</description><author>Dingxin Zhang, Jianhui Yu, Tengfei Xue, Chaoyi Zhang, Dongnan Liu, Weidong Cai</author><pubDate>Mon, 15 Jul 2024 15:21:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10806v1</guid></item><item><title>Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval</title><link>http://arxiv.org/abs/2407.10805v1</link><description>Retrieval-augmented generation (RAG) has significantly advanced largelanguage models (LLMs) by enabling dynamic information retrieval to mitigateknowledge gaps and hallucinations in generated content. However, these systemsoften falter with complex reasoning and consistency across diverse queries. Inthis work, we present Think-on-Graph 2.0, an enhanced RAG framework that alignsquestions with the knowledge graph and uses it as a navigational tool, whichdeepens and refines the RAG paradigm for information collection andintegration. The KG-guided navigation fosters deep and long-range associationsto uphold logical consistency and optimize the scope of retrieval for precisionand interoperability. In conjunction, factual consistency can be better ensuredthrough semantic similarity guided by precise directives. ToG${2.0}$ not onlyimproves the accuracy and reliability of LLMs' responses but also demonstratesthe potential of hybrid structured knowledge systems to significantly advanceLLM reasoning, aligning it closer to human-like performance. We conductedextensive experiments on four public datasets to demonstrate the advantages ofour method compared to the baseline.</description><author>Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo</author><pubDate>Mon, 15 Jul 2024 15:20:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10805v1</guid></item><item><title>Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment</title><link>http://arxiv.org/abs/2407.10804v1</link><description>Adapting general large language models (LLMs) to specialized domains presentsgreat challenges due to varied data distributions. This adaptation typicallyrequires continual pre-training on massive domain-specific corpora tofacilitate knowledge memorization, followed by training to apply this knowledgefollowing human instructions and preferences. However, this method may resultin inefficient knowledge memorization due to a lack of awareness of knowledgeutilization and imposes substantial demands on LLMs to simultaneously learnknowledge utilization and format alignment with limited training samples. Tofacilitate the domain adaptation of LLM, we revise this process and propose anew domain adaptation framework including domain knowledge learning and generalformat alignment, called Mix-CPT. Specifically, we first conduct a knowledgemixture continual pre-training that concurrently focuses on knowledgememorization and utilization, allowing for mutual reinforcement. To avoidcatastrophic forgetting during the continual pre-training process, we furtherincorporate a logit swap self-distillation constraint. Subsequently, leveragingthe knowledge and capabilities acquired during continual pre-training, weefficiently perform instruction tuning and alignment with a few generaltraining samples to achieve format alignment. Extensive experiments demonstratethat our proposed Mix-CPT framework can simultaneously improve the task-solvingcapabilities of LLMs on the target and general domains compared to thetraditional adaptation methods.</description><author>Jinhao Jiang, Junyi Li, Wayne Xin Zhao, Yang Song, Tao Zhang, Ji-Rong Wen</author><pubDate>Mon, 15 Jul 2024 15:20:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10804v1</guid></item><item><title>Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head Capture</title><link>http://arxiv.org/abs/2406.00440v3</link><description>4D head capture aims to generate dynamic topological meshes and correspondingtexture maps from videos, which is widely utilized in movies and games for itsability to simulate facial muscle movements and recover dynamic textures inpore-squeezing. The industry often adopts the method involving multi-viewstereo and non-rigid alignment. However, this approach is prone to errors andheavily reliant on time-consuming manual processing by artists. To simplifythis process, we propose Topo4D, a novel framework for automatic geometry andtexture generation, which optimizes densely aligned 4D heads and 8K texturemaps directly from calibrated multi-view time-series images. Specifically, wefirst represent the time-series faces as a set of dynamic 3D Gaussians withfixed topology in which the Gaussian centers are bound to the mesh vertices.Afterward, we perform alternative geometry and texture optimizationframe-by-frame for high-quality geometry and texture learning while maintainingtemporal topology stability. Finally, we can extract dynamic facial meshes inregular wiring arrangement and high-fidelity textures with pore-level detailsfrom the learned Gaussians. Extensive experiments show that our method achievessuperior results than the current SOTA face reconstruction methods both in thequality of meshes and textures. Project page:https://xuanchenli.github.io/Topo4D/.</description><author>Xuanchen Li, Yuhao Cheng, Xingyu Ren, Haozhe Jia, Di Xu, Wenhan Zhu, Yichao Yan</author><pubDate>Mon, 15 Jul 2024 15:20:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00440v3</guid></item><item><title>DINO Pre-training for Vision-based End-to-end Autonomous Driving</title><link>http://arxiv.org/abs/2407.10803v1</link><description>In this article, we focus on the pre-training of visual autonomous drivingagents in the context of imitation learning. Current methods often rely on aclassification-based pre-training, which we hypothesise to be holding back fromextending capabilities of implicit image understanding. We propose pre-trainingthe visual encoder of a driving agent using the self-distillation with nolabels (DINO) method, which relies on a self-supervised learning paradigm.% andis trained on an unrelated task. Our experiments in CARLA environment inaccordance with the Leaderboard benchmark reveal that the proposed pre-trainingis more efficient than classification-based pre-training, and is on par withthe recently proposed pre-training based on visual place recognition (VPRPre).</description><author>Shubham Juneja, Povilas Daniušis, Virginijus Marcinkevičius</author><pubDate>Mon, 15 Jul 2024 15:18:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10803v1</guid></item><item><title>Motion-prior Contrast Maximization for Dense Continuous-Time Motion Estimation</title><link>http://arxiv.org/abs/2407.10802v1</link><description>Current optical flow and point-tracking methods rely heavily on syntheticdatasets. Event cameras are novel vision sensors with advantages in challengingvisual conditions, but state-of-the-art frame-based methods cannot be easilyadapted to event data due to the limitations of current event simulators. Weintroduce a novel self-supervised loss combining the Contrast Maximizationframework with a non-linear motion prior in the form of pixel-leveltrajectories and propose an efficient solution to solve the high-dimensionalassignment problem between non-linear trajectories and events. Theireffectiveness is demonstrated in two scenarios: In dense continuous-time motionestimation, our method improves the zero-shot performance of a syntheticallytrained model on the real-world dataset EVIMO2 by 29%. In optical flowestimation, our method elevates a simple UNet to achieve state-of-the-artperformance among self-supervised methods on the DSEC optical flow benchmark.Our code is available at https://github.com/tub-rip/MotionPriorCMax.</description><author>Friedhelm Hamann, Ziyun Wang, Ioannis Asmanis, Kenneth Chaney, Guillermo Gallego, Kostas Daniilidis</author><pubDate>Mon, 15 Jul 2024 15:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10802v1</guid></item><item><title>Mammographic Breast Positioning Assessment via Deep Learning</title><link>http://arxiv.org/abs/2407.10796v1</link><description>Breast cancer remains a leading cause of cancer-related deaths among womenworldwide, with mammography screening as the most effective method for theearly detection. Ensuring proper positioning in mammography is critical, aspoor positioning can lead to diagnostic errors, increased patient stress, andhigher costs due to recalls. Despite advancements in deep learning (DL) forbreast cancer diagnostics, limited focus has been given to evaluatingmammography positioning. This paper introduces a novel DL methodology toquantitatively assess mammogram positioning quality, specifically inmediolateral oblique (MLO) views using attention and coordinate convolutionmodules. Our method identifies key anatomical landmarks, such as the nipple andpectoralis muscle, and automatically draws a posterior nipple line (PNL),offering robust and inherently explainable alternative to well-knownclassification and regression-based approaches. We compare the performance ofproposed methodology with various regression and classification-based models.The CoordAtt UNet model achieved the highest accuracy of 88.63% $\pm$ 2.84 andspecificity of 90.25% $\pm$ 4.04, along with a noteworthy sensitivity of 86.04%$\pm$ 3.41. In landmark detection, the same model also recorded the lowest meanerrors in key anatomical points and the smallest angular error of 2.42 degrees.Our results indicate that models incorporating attention mechanisms andCoordConv module increase the accuracy in classifying breast positioningquality and detecting anatomical landmarks. Furthermore, we make the labels andsource codes available to the community to initiate an open research area formammography, accessible at https://github.com/tanyelai/deep-breast-positioning.</description><author>Toygar Tanyel, Nurper Denizoglu, Mustafa Ege Seker, Deniz Alis, Esma Cerekci, Ercan Karaarslan, Erkin Aribal, Ilkay Oksuz</author><pubDate>Mon, 15 Jul 2024 15:14:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10796v1</guid></item><item><title>Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping</title><link>http://arxiv.org/abs/2407.10795v1</link><description>Decoding by contrasting layers (DoLa), is designed to improve the generationquality of large language models (LLMs) by contrasting the predictionprobabilities between an early exit output (amateur logits) and the finaloutput (expert logits). However, we find that this approach does not work wellon non-English tasks. Inspired by previous interpretability work on languagetransition during the model's forward pass, we discover that this issue arisesfrom a language mismatch between early exit output and final output. In thiswork, we propose an improved contrastive decoding algorithm that is effectivefor diverse languages beyond English. To obtain more helpful amateur logits, wedevise two strategies to skip a set of bottom, language-agnostic layers basedon our preliminary analysis. Experimental results on multilingual reasoningbenchmarks demonstrate that our proposed method outperforms previouscontrastive decoding baselines and substantially improves LLM'schain-of-thought reasoning accuracy across 11 languages. The project will beavailable at: https://github.com/NJUNLP/SkipLayerCD.</description><author>Wenhao Zhu, Sizhe Liu, Shujian Huang, Shuaijie She, Chris Wendler, Jiajun Chen</author><pubDate>Mon, 15 Jul 2024 15:14:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10795v1</guid></item><item><title>Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education</title><link>http://arxiv.org/abs/2407.10794v1</link><description>Knowledge graphs (KGs) are crucial in the field of artificial intelligenceand are widely applied in downstream tasks, such as enhancing QuestionAnswering (QA) systems. The construction of KGs typically requires significanteffort from domain experts. Recently, Large Language Models (LLMs) have beenused for knowledge graph construction (KGC), however, most existing approachesfocus on a local perspective, extracting knowledge triplets from individualsentences or documents. In this work, we introduce Graphusion, a zero-shot KGCframework from free text. The core fusion module provides a global view oftriplets, incorporating entity merging, conflict resolution, and novel tripletdiscovery. We showcase how Graphusion could be applied to the natural languageprocessing (NLP) domain and validate it in the educational scenario.Specifically, we introduce TutorQA, a new expert-verified benchmark for graphreasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Ourevaluation demonstrates that Graphusion surpasses supervised baselines by up to10% in accuracy on link prediction. Additionally, it achieves average scores of2.92 and 2.37 out of 3 in human evaluations for concept entity extraction andrelation recognition, respectively.</description><author>Rui Yang, Boming Yang, Sixun Ouyang, Tianwei She, Aosong Feng, Yuang Jiang, Freddy Lecue, Jinghui Lu, Irene Li</author><pubDate>Mon, 15 Jul 2024 15:13:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10794v1</guid></item><item><title>A Neurosymbolic Approach to Adaptive Feature Extraction in SLAM</title><link>http://arxiv.org/abs/2407.06889v2</link><description>Autonomous robots, autonomous vehicles, and humans wearing mixed-realityheadsets require accurate and reliable tracking services for safety-criticalapplications in dynamically changing real-world environments. However, theexisting tracking approaches, such as Simultaneous Localization and Mapping(SLAM), do not adapt well to environmental changes and boundary conditionsdespite extensive manual tuning. On the other hand, while deep learning-basedapproaches can better adapt to environmental changes, they typically demandsubstantial data for training and often lack flexibility in adapting to newdomains. To solve this problem, we propose leveraging the neurosymbolic programsynthesis approach to construct adaptable SLAM pipelines that integrate thedomain knowledge from traditional SLAM approaches while leveraging data tolearn complex relationships. While the approach can synthesize end-to-end SLAMpipelines, we focus on synthesizing the feature extraction module. We firstdevise a domain-specific language (DSL) that can encapsulate domain knowledgeon the important attributes for feature extraction and the real-worldperformance of various feature extractors. Our neurosymbolic architecture thenundertakes adaptive feature extraction, optimizing parameters via learningwhile employing symbolic reasoning to select the most suitable featureextractor. Our evaluations demonstrate that our approach, neurosymbolic FeatureEXtraction (nFEX), yields higher-quality features. It also reduces the poseerror observed for the state-of-the-art baseline feature extractors ORB andSIFT by up to 90% and up to 66%, respectively, thereby enhancing the system'sefficiency and adaptability to novel environments.</description><author>Yasra Chandio, Momin A. Khan, Khotso Selialia, Luis Garcia, Joseph DeGol, Fatima M. Anwar</author><pubDate>Mon, 15 Jul 2024 15:12:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06889v2</guid></item><item><title>GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework</title><link>http://arxiv.org/abs/2407.10793v1</link><description>Methods to evaluate Large Language Model (LLM) responses and detectinconsistencies, also known as hallucinations, with respect to the providedknowledge, are becoming increasingly important for LLM applications. Currentmetrics fall short in their ability to provide explainable decisions,systematically check all pieces of information in the response, and are oftentoo computationally expensive to be used in practice. We present GraphEval: ahallucination evaluation framework based on representing information inKnowledge Graph (KG) structures. Our method identifies the specific triples inthe KG that are prone to hallucinations and hence provides more insight intowhere in the response a hallucination has occurred, if at all, than previousmethods. Furthermore, using our approach in conjunction with state-of-the-artnatural language inference (NLI) models leads to an improvement in balancedaccuracy on various hallucination benchmarks, compared to using the raw NLImodels. Lastly, we explore the use of GraphEval for hallucination correction byleveraging the structure of the KG, a method we name GraphCorrect, anddemonstrate that the majority of hallucinations can indeed be rectified.</description><author>Hannah Sansford, Nicholas Richardson, Hermina Petric Maretic, Juba Nait Saada</author><pubDate>Mon, 15 Jul 2024 15:11:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10793v1</guid></item><item><title>Interpretability analysis on a pathology foundation model reveals biologically relevant embeddings across modalities</title><link>http://arxiv.org/abs/2407.10785v1</link><description>Mechanistic interpretability has been explored in detail for large languagemodels (LLMs). For the first time, we provide a preliminary investigation withsimilar interpretability methods for medical imaging. Specifically, we analyzethe features from a ViT-Small encoder obtained from a pathology FoundationModel via application to two datasets: one dataset of pathology images, and onedataset of pathology images paired with spatial transcriptomics. We discover aninterpretable representation of cell and tissue morphology, along with geneexpression within the model embedding space. Our work paves the way for furtherexploration around interpretable feature dimensions and their utility formedical and clinical applications.</description><author>Nhat Le, Ciyue Shen, Chintan Shah, Blake Martin, Daniel Shenker, Harshith Padigela, Jennifer Hipp, Sean Grullon, John Abel, Harsha Vardhan Pokkalla, Dinkar Juyal</author><pubDate>Mon, 15 Jul 2024 15:03:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10785v1</guid></item><item><title>AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty Calibrator and Label Distribution Handler</title><link>http://arxiv.org/abs/2407.10784v1</link><description>In real-world applications, tabular data often suffer from distributionshifts due to their widespread and abundant nature, leading to erroneouspredictions of pre-trained machine learning models. However, addressing suchdistribution shifts in the tabular domain has been relatively underexplored dueto unique challenges such as varying attributes and dataset sizes, as well asthe limited representation learning capabilities of deep learning models fortabular data. Particularly, with the recent promising paradigm of test-timeadaptation (TTA), where we adapt the off-the-shelf model to the unlabeledtarget domain during the inference phase without accessing the source domain,we observe that directly adopting commonly used TTA methods from other domainsoften leads to model collapse. We systematically explore challenges in tabulardata test-time adaptation, including skewed entropy, complex latent spacedecision boundaries, confidence calibration issues with both overconfident andunder-confident, and model bias towards source label distributions along withclass imbalances. Based on these insights, we introduce AdapTable, a noveltabular test-time adaptation method that directly modifies output probabilitiesby estimating target label distributions and adjusting initial probabilitiesbased on calibrated uncertainty. Extensive experiments on both naturaldistribution shifts and synthetic corruptions demonstrate the adaptationefficacy of the proposed method.</description><author>Changhun Kim, Taewon Kim, Seungyeon Woo, June Yong Yang, Eunho Yang</author><pubDate>Mon, 15 Jul 2024 15:02:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10784v1</guid></item></channel></rss>