<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 13 Jan 2025 13:00:21 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Multi-subject Open-set Personalization in Video Generation</title><link>http://arxiv.org/abs/2501.06187v1</link><description>Video personalization methods allow us to synthesize videos with specificconcepts such as people, pets, and places. However, existing methods oftenfocus on limited domains, require time-consuming optimization per subject, orsupport only a single subject. We present Video Alchemist $-$ a video modelwith built-in multi-subject, open-set personalization capabilities for bothforeground objects and background, eliminating the need for time-consumingtest-time optimization. Our model is built on a new Diffusion Transformermodule that fuses each conditional reference image and its correspondingsubject-level text prompt with cross-attention layers. Developing such a largemodel presents two main challenges: dataset and evaluation. First, as paireddatasets of reference images and videos are extremely hard to collect, wesample selected video frames as reference images and synthesize a clip of thetarget video. However, while models can easily denoise training videos givenreference frames, they fail to generalize to new contexts. To mitigate thisissue, we design a new automatic data construction pipeline with extensiveimage augmentations. Second, evaluating open-set video personalization is achallenge in itself. To address this, we introduce a personalization benchmarkthat focuses on accurate subject fidelity and supports diverse personalizationscenarios. Finally, our extensive experiments show that our methodsignificantly outperforms existing personalization methods in both quantitativeand qualitative evaluations.</description><author>Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, Sergey Tulyakov</author><pubDate>Fri, 10 Jan 2025 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06187v1</guid></item><item><title>LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs</title><link>http://arxiv.org/abs/2501.06186v1</link><description>Reasoning is a fundamental capability for solving complex multi-stepproblems, particularly in visual contexts where sequential step-wiseunderstanding is essential. Existing approaches lack a comprehensive frameworkfor evaluating visual reasoning and do not emphasize step-wise problem-solving.To this end, we propose a comprehensive framework for advancing step-by-stepvisual reasoning in large language models (LMMs) through three keycontributions. First, we introduce a visual reasoning benchmark specificallydesigned to evaluate multi-step reasoning tasks. The benchmark presents adiverse set of challenges with eight different categories ranging from complexvisual perception to scientific reasoning with over 4k reasoning steps intotal, enabling robust evaluation of LLMs' abilities to perform accurate andinterpretable visual reasoning across multiple steps. Second, we propose anovel metric that assesses visual reasoning quality at the granularity ofindividual steps, emphasizing both correctness and logical coherence. Theproposed metric offers deeper insights into reasoning performance compared totraditional end-task accuracy metrics. Third, we present a new multimodalvisual reasoning model, named LlamaV-o1, trained using a multi-step curriculumlearning approach, where tasks are progressively organized to facilitateincremental skill acquisition and problem-solving. The proposed LlamaV-o1 isdesigned for multi-step reasoning and learns step-by-step through a structuredtraining paradigm. Extensive experiments show that our LlamaV-o1 outperformsexisting open-source models and performs favorably against close-sourceproprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves anaverage score of 67.3 with an absolute gain of 3.8\% across six benchmarkswhile being 5 times faster during inference scaling. Our benchmark, model, andcode are publicly available.</description><author>Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, Hisham Cholakkal, Ivan Laptev, Mubarak Shah, Fahad Shahbaz Khan, Salman Khan</author><pubDate>Fri, 10 Jan 2025 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06186v1</guid></item><item><title>PEACE: Empowering Geologic Map Holistic Understanding with MLLMs</title><link>http://arxiv.org/abs/2501.06184v1</link><description>Geologic map, as a fundamental diagram in geology science, provides criticalinsights into the structure and composition of Earth's subsurface and surface.These maps are indispensable in various fields, including disaster detection,resource exploration, and civil engineering. Despite their significance,current Multimodal Large Language Models (MLLMs) often fall short in geologicmap understanding. This gap is primarily due to the challenging nature ofcartographic generalization, which involves handling high-resolution map,managing multiple associated components, and requiring domain-specificknowledge. To quantify this gap, we construct GeoMap-Bench, the first-everbenchmark for evaluating MLLMs in geologic map understanding, which assessesthe full-scale abilities in extracting, referring, grounding, reasoning, andanalyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agentdesigned for geologic map understanding, which features three modules:Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),and Prompt-enhanced Question Answering (PEQA). Inspired by theinterdisciplinary collaboration among human scientists, an AI expert group actsas consultants, utilizing a diverse tool pool to comprehensively analyzequestions. Through comprehensive experiments, GeoMap-Agent achieves an overallscore of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,paves the way for advanced AI applications in geology, enhancing the efficiencyand accuracy of geological investigations.</description><author>Yangyu Huang, Tianyi Gao, Haoran Xu, Qihao Zhao, Yang Song, Zhipeng Gui, Tengchao Lv, Hao Chen, Lei Cui, Scarlett Li, Furu Wei</author><pubDate>Fri, 10 Jan 2025 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06184v1</guid></item><item><title>Decentralized Diffusion Models</title><link>http://arxiv.org/abs/2501.05450v2</link><description>Large-scale AI model training divides work across thousands of GPUs, thensynchronizes gradients across them at each step. This incurs a significantnetwork burden that only centralized, monolithic clusters can support, drivingup infrastructure costs and straining power systems. We propose DecentralizedDiffusion Models, a scalable framework for distributing diffusion modeltraining across independent clusters or datacenters by eliminating thedependence on a centralized, high-bandwidth networking fabric. Our methodtrains a set of expert diffusion models over partitions of the dataset, each infull isolation from one another. At inference time, the experts ensemblethrough a lightweight router. We show that the ensemble collectively optimizesthe same objective as a single model trained over the whole dataset. This meanswe can divide the training burden among a number of "compute islands," loweringinfrastructure costs and improving resilience to localized GPU failures.Decentralized diffusion models empower researchers to take advantage ofsmaller, more cost-effective and more readily available compute like on-demandGPU nodes rather than central integrated systems. We conduct extensiveexperiments on ImageNet and LAION Aesthetics, showing that decentralizeddiffusion models FLOP-for-FLOP outperform standard diffusion models. We finallyscale our approach to 24 billion parameters, demonstrating that high-qualitydiffusion models can now be trained with just eight individual GPU nodes inless than a week.</description><author>David McAllister, Matthew Tancik, Jiaming Song, Angjoo Kanazawa</author><pubDate>Fri, 10 Jan 2025 18:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05450v2</guid></item><item><title>VideoAuteur: Towards Long Narrative Video Generation</title><link>http://arxiv.org/abs/2501.06173v1</link><description>Recent video generation models have shown promising results in producinghigh-quality video clips lasting several seconds. However, these models facechallenges in generating long sequences that convey clear and informativeevents, limiting their ability to support coherent narrations. In this paper,we present a large-scale cooking video dataset designed to advance long-formnarrative generation in the cooking domain. We validate the quality of ourproposed dataset in terms of visual fidelity and textual caption accuracy usingstate-of-the-art Vision-Language Models (VLMs) and video generation models,respectively. We further introduce a Long Narrative Video Director to enhanceboth visual and semantic coherence in generated videos and emphasize the roleof aligning visual embeddings to achieve improved overall video quality. Ourmethod demonstrates substantial improvements in generating visually detailedand semantically aligned keyframes, supported by finetuning techniques thatintegrate text and image embeddings within the video generation process.Project page: https://videoauteur.github.io/</description><author>Junfei Xiao, Feng Cheng, Lu Qi, Liangke Gui, Jiepeng Cen, Zhibei Ma, Alan Yuille, Lu Jiang</author><pubDate>Fri, 10 Jan 2025 18:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06173v1</guid></item><item><title>Machine Learning Force-Field Approach for Itinerant Electron Magnets</title><link>http://arxiv.org/abs/2501.06171v1</link><description>We review the recent development of machine-learning (ML) force-fieldframeworks for Landau-Lifshitz-Gilbert (LLG) dynamics simulations of itinerantelectron magnets, focusing on the general theory and implementations ofsymmetry-invariant representations of spin configurations. The crucialproperties that such magnetic descriptors must satisfy are differentiabilitywith respect to spin rotations and invariance to both lattice point-groupsymmetry and internal spin rotation symmetry. We propose an efficientimplementation based on the concept of reference irreducible representations,modified from the group-theoretical power-spectrum and bispectrum methods. TheML framework is demonstrated using the s-d models, which are widely applied inspintronics research. We show that LLG simulations based on local fieldspredicted by the trained ML models successfully reproduce representativenon-collinear spin structures, including 120$^\circ$, tetrahedral, and skyrmioncrystal orders of the triangular-lattice s-d models. Large-scale thermal quenchsimulations enabled by ML models further reveal intriguing freezing dynamicsand glassy stripe states consisting of skyrmions and bi-merons. Our workhighlights the utility of ML force-field approach to dynamical modeling ofcomplex spin orders in itinerant electron magnets.</description><author>Sheng Zhang, Yunhao Fan, Kotaro Shimizu, Gia-Wei Chern</author><pubDate>Fri, 10 Jan 2025 18:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06171v1</guid></item><item><title>Meta-Learning for Physically-Constrained Neural System Identification</title><link>http://arxiv.org/abs/2501.06167v1</link><description>We present a gradient-based meta-learning framework for rapid adaptation ofneural state-space models (NSSMs) for black-box system identification. Whenapplicable, we also incorporate domain-specific physical constraints to improvethe accuracy of the NSSM. The major benefit of our approach is that instead ofrelying solely on data from a single target system, our framework utilizes datafrom a diverse set of source systems, enabling learning from limited targetdata, as well as with few online training iterations. Through benchmarkexamples, we demonstrate the potential of our approach, study the effect offine-tuning subnetworks rather than full fine-tuning, and report real-worldcase studies to illustrate the practical application and generalizability ofthe approach to practical problems with physical-constraints. Specifically, weshow that the meta-learned models result in improved downstream performance inmodel-based state estimation in indoor localization and energy systems.</description><author>Ankush Chakrabarty, Gordon Wichern, Vedang M. Deshpande, Abraham P. Vinod, Karl Berntorp, Christopher R. Laughman</author><pubDate>Fri, 10 Jan 2025 18:46:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06167v1</guid></item><item><title>S2-Attention: Hardware-Aware Context Sharding Among Attention Heads</title><link>http://arxiv.org/abs/2407.17678v6</link><description>Sparse attention, which selectively attends to a subset of tokens in thecontext was supposed to be efficient. However, its theoretical reduction inFLOPs has rarely translated into wall-clock speed-up over its dense attentioncounterparts due to the lack of hardware-aware optimizations likeFlashAttention. Meanwhile, it remains unclear whether sparse attention canmaintain the model's quality at a scale of today's large language models (LLMs)and how. This paper presents Sparsely-Sharded(S2) Attention, a Triton librarythat provides kernel optimization for sparse attention customizable at bothper-head and per-context-range levels. S2-Attention enables the exploration ofnovel and high-performance sparse attention techniques, which we demonstratethrough extensive ablations across a wide range of sparse attention designs atvarious model scales. From these insights, we present several basic guidelinesto design sparse attention that can achieve not only practical efficiencyimprovements, but also strong downstream performance. To achieve highparallelization and optimized memory IO, sparse attention should shard thecontext heterogeneously across attention heads, where each head attends to adifferent subset of tokens while collectively covering the full context.Meanwhile, we find hybrid architectures combining sparse and dense attentionparticularly beneficial in practice. S2-Attention achieves wall-clock speedupof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline withstrong downstream performance on-par with full attention and perfect retrievalperformance at a 128k context length. At inference, for 7B models, our model,with the help of our S2-Attention kernel, achieves 4.5x speed-up compared todense counterparts. S2-Attention is released with easy-to-customize APIs fordirect usage in Megatron and vLLM.</description><author>Xihui Lin, Yunan Zhang, Suyu Ge, Liliang Ren, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song</author><pubDate>Fri, 10 Jan 2025 18:45:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17678v6</guid></item><item><title>Model Alignment Search</title><link>http://arxiv.org/abs/2501.06164v1</link><description>When can we say that two neural systems are the same? The answer to thisquestion is goal-dependent, and it is often addressed through correlativemethods such as Representational Similarity Analysis (RSA) and Centered KernelAlignment (CKA). What do we miss when we forgo causal explorations, and how canwe target specific types of similarity? In this work, we introduce ModelAlignment Search (MAS), a method for causally exploring distributedrepresentational similarity. The method learns invertible lineartransformations that align a subspace between two distributed networks'representations where causal information can be freely interchanged. We firstshow that the method can be used to transfer specific causal variables, such asthe number of items in a counting task, between networks with differenttraining seeds. We then explore open questions in number cognition by comparingdifferent types of numeric representations in models trained on structurallydifferent numeric tasks. We then explore differences between MAS vs preexistingcausal similarity methods, showing MAS to be more resistant to unwantedexchanges. Lastly, we introduce a counterfactual latent auxiliary loss functionthat helps shape causally relevant alignments even in cases where we do nothave causal access to one of the two models for training.</description><author>Satchel Grant</author><pubDate>Fri, 10 Jan 2025 18:39:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06164v1</guid></item><item><title>Efficient Transition State Searches by Freezing String Method with Graph Neural Network Potentials</title><link>http://arxiv.org/abs/2501.06159v1</link><description>Transition states are a critical bottleneck in chemical transformations.Significant efforts have been made to develop algorithms that efficientlylocate transition states on potential energy surfaces. However, thecomputational cost of ab-initio potential energy surface evaluation limits thesize of chemical systems that can routinely studied. In this work, we developand fine-tune a graph neural network potential energy function suitable fordescribing organic chemical reactions and use it to rapidly identify transitionstate guess structures. We successfully refine guess structures and locate atransition state in each test system considered and reduce the average numberof ab-initio calculations by 47% though use of the graph neural networkpotential energy function. Our results show that modern machine learning modelshave reached levels of reliability whereby they can be used to accelerateroutine computational chemistry tasks.</description><author>Jonah Marks, Joseph Gomes</author><pubDate>Fri, 10 Jan 2025 18:32:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06159v1</guid></item><item><title>Beyond Item Dissimilarities: Diversifying by Intent in Recommender Systems</title><link>http://arxiv.org/abs/2405.12327v3</link><description>It has become increasingly clear that recommender systems that overly focuson short-term engagement prevents users from exploring diverse interests,ultimately hurting long-term user experience. To tackle this challenge,numerous diversification algorithms have been proposed. These algorithmstypically rely on measures of item similarity, aiming to maximize thedissimilarity across items in the final set of recommendations. However, inthis work, we demonstrate the benefits of going beyond item-level similaritiesby utilizing higher-level user understanding--specifically, user intents thatpersist across multiple interactions--in diversification. Our approach ismotivated by the observation that user behaviors on online platforms arelargely driven by their underlying intents. Therefore, recommendations shouldensure that diverse user intents are accurately represented. While intent hasprimarily been studied in the context of search, it is less clear how toincorporate real-time dynamic intent predictions into recommender systems. Toaddress this gap, we develop a probabilistic intent-based whole-pagediversification framework for the final stage of a recommender system. Startingwith a prior belief of user intents, the proposed framework sequentiallyselects items for each position based on these beliefs and subsequently updatesposterior beliefs about the intents. This approach ensures that different userintents are represented on a page, towards optimizing long-term userexperience. We experiment with the intent diversification framework on YouTube,the world's largest video recommendation platform, serving billions of usersdaily. Live experiments on a diverse set of intents show that the proposedframework increases Daily Active Users (DAU) and overall user enjoyment,validating its effectiveness in facilitating long-term planning.</description><author>Yuyan Wang, Cheenar Banerjee, Samer Chucri, Fabio Soldo, Sriraj Badam, Ed H. Chi, Minmin Chen</author><pubDate>Fri, 10 Jan 2025 18:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12327v3</guid></item><item><title>GenMol: A Drug Discovery Generalist with Discrete Diffusion</title><link>http://arxiv.org/abs/2501.06158v1</link><description>Drug discovery is a complex process that involves multiple scenarios andstages, such as fragment-constrained molecule generation, hit generation andlead optimization. However, existing molecular generative models can onlytackle one or two of these scenarios and lack the flexibility to addressvarious aspects of the drug discovery pipeline. In this paper, we presentGeneralist Molecular generative model (GenMol), a versatile framework thataddresses these limitations by applying discrete diffusion to the SequentialAttachment-based Fragment Embedding (SAFE) molecular representation. GenMolgenerates SAFE sequences through non-autoregressive bidirectional paralleldecoding, thereby allowing utilization of a molecular context that does notrely on the specific token ordering and enhanced computational efficiency.Moreover, under the discrete diffusion framework, we introduce fragmentremasking, a strategy that optimizes molecules by replacing fragments withmasked tokens and regenerating them, enabling effective exploration of chemicalspace. GenMol significantly outperforms the previous GPT-based model trained onSAFE representations in de novo generation and fragment-constrained generation,and achieves state-of-the-art performance in goal-directed hit generation andlead optimization. These experimental results demonstrate that GenMol cantackle a wide range of drug discovery tasks, providing a unified and versatileapproach for molecular design.</description><author>Seul Lee, Karsten Kreis, Srimukh Prasad Veccham, Meng Liu, Danny Reidenbach, Yuxing Peng, Saee Paliwal, Weili Nie, Arash Vahdat</author><pubDate>Fri, 10 Jan 2025 18:30:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06158v1</guid></item><item><title>PySpatial: A High-Speed Whole Slide Image Pathomics Toolkit</title><link>http://arxiv.org/abs/2501.06151v1</link><description>Whole Slide Image (WSI) analysis plays a crucial role in modern digitalpathology, enabling large-scale feature extraction from tissue samples.However, traditional feature extraction pipelines based on tools likeCellProfiler often involve lengthy workflows, requiring WSI segmentation intopatches, feature extraction at the patch level, and subsequent mapping back tothe original WSI. To address these challenges, we present PySpatial, ahigh-speed pathomics toolkit specifically designed for WSI-level analysis.PySpatial streamlines the conventional pipeline by directly operating oncomputational regions of interest, reducing redundant processing steps.Utilizing rtree-based spatial indexing and matrix-based computation, PySpatialefficiently maps and processes computational regions, significantlyaccelerating feature extraction while maintaining high accuracy. Ourexperiments on two datasets-Perivascular Epithelioid Cell (PEC) and data fromthe Kidney Precision Medicine Project (KPMP)-demonstrate substantialperformance improvements. For smaller and sparse objects in PEC datasets,PySpatial achieves nearly a 10-fold speedup compared to standard CellProfilerpipelines. For larger objects, such as glomeruli and arteries in KPMP datasets,PySpatial achieves a 2-fold speedup. These results highlight PySpatial'spotential to handle large-scale WSI analysis with enhanced efficiency andaccuracy, paving the way for broader applications in digital pathology.</description><author>Yuechen Yang, Yu Wang, Tianyuan Yao, Ruining Deng, Mengmeng Yin, Shilin Zhao, Haichun Yang, Yuankai Huo</author><pubDate>Fri, 10 Jan 2025 18:24:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06151v1</guid></item><item><title>From discrete-time policies to continuous-time diffusion samplers: Asymptotic equivalences and faster training</title><link>http://arxiv.org/abs/2501.06148v1</link><description>We study the problem of training neural stochastic differential equations, ordiffusion models, to sample from a Boltzmann distribution without access totarget samples. Existing methods for training such models enforce time-reversalof the generative and noising processes, using either differentiable simulationor off-policy reinforcement learning (RL). We prove equivalences betweenfamilies of objectives in the limit of infinitesimal discretization steps,linking entropic RL methods (GFlowNets) with continuous-time objects (partialdifferential equations and path space measures). We further show that anappropriate choice of coarse time discretization during training allows greatlyimproved sample efficiency and the use of time-local objectives, achievingcompetitive performance on standard sampling benchmarks with reducedcomputational cost.</description><author>Julius Berner, Lorenz Richter, Marcin Sendera, Jarrid Rector-Brooks, Nikolay Malkin</author><pubDate>Fri, 10 Jan 2025 18:18:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06148v1</guid></item><item><title>Guess What I Think: Streamlined EEG-to-Image Generation with Latent Diffusion Models</title><link>http://arxiv.org/abs/2410.02780v2</link><description>Generating images from brain waves is gaining increasing attention due to itspotential to advance brain-computer interface (BCI) systems by understandinghow brain signals encode visual cues. Most of the literature has focused onfMRI-to-Image tasks as fMRI is characterized by high spatial resolution.However, fMRI is an expensive neuroimaging modality and does not allow forreal-time BCI. On the other hand, electroencephalography (EEG) is a low-cost,non-invasive, and portable neuroimaging technique, making it an attractiveoption for future real-time applications. Nevertheless, EEG presents inherentchallenges due to its low spatial resolution and susceptibility to noise andartifacts, which makes generating images from EEG more difficult. In thispaper, we address these problems with a streamlined framework based on theControlNet adapter for conditioning a latent diffusion model (LDM) through EEGsignals. We conduct experiments and ablation studies on popular benchmarks todemonstrate that the proposed method beats other state-of-the-art models.Unlike these methods, which often require extensive preprocessing, pretraining,different losses, and captioning models, our approach is efficient andstraightforward, requiring only minimal preprocessing and a few components. Thecode is available at https://github.com/LuigiSigillo/GWIT.</description><author>Eleonora Lopez, Luigi Sigillo, Federica Colonnese, Massimo Panella, Danilo Comminiello</author><pubDate>Fri, 10 Jan 2025 18:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02780v2</guid></item><item><title>xLSTM-SENet: xLSTM for Single-Channel Speech Enhancement</title><link>http://arxiv.org/abs/2501.06146v1</link><description>While attention-based architectures, such as Conformers, excel in speechenhancement, they face challenges such as scalability with respect to inputsequence length. In contrast, the recently proposed Extended Long Short-TermMemory (xLSTM) architecture offers linear scalability. However, xLSTM-basedmodels remain unexplored for speech enhancement. This paper introducesxLSTM-SENet, the first xLSTM-based single-channel speech enhancement system. Acomparative analysis reveals that xLSTM-and notably, even LSTM-can match oroutperform state-of-the-art Mamba- and Conformer-based systems across variousmodel sizes in speech enhancement on the VoiceBank+Demand dataset. Throughablation studies, we identify key architectural design choices such asexponential gating and bidirectionality contributing to its effectiveness. Ourbest xLSTM-based model, xLSTM-SENet2, outperforms state-of-the-art Mamba- andConformer-based systems on the Voicebank+DEMAND dataset.</description><author>Nikolai Lund Kühne, Jan Østergaard, Jesper Jensen, Zheng-Hua Tan</author><pubDate>Fri, 10 Jan 2025 18:10:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06146v1</guid></item><item><title>Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories</title><link>http://arxiv.org/abs/2501.06143v1</link><description>We investigate the multilingual and multimodal performance of a largelanguage model-based artificial intelligence (AI) system, GPT-4o, on a diverseset of physics concept inventories spanning multiple languages and subjectareas. The inventories taken from the PhysPort website cover the classicalphysics topics of mechanics, electromagnetism, optics, and thermodynamics aswell as relativity, quantum mechanics, astronomy, mathematics, and laboratoryskills. Unlike previous text-only studies, we uploaded the inventories asimages mirroring what a student would see on paper, assessing the system'smultimodal functionality. The AI is prompted in English and autonomouslychooses the language of its response - either remaining in the nominal languageof the test, switching entirely to English, or mixing languages - revealingadaptive behavior dependent on linguistic complexity and data availability. Ourresults indicate some variation in performance across subject areas, withlaboratory skills standing out as the area of poorest performance. Furthermore,the AI's performance on questions that require visual interpretation of imagesis worse than on purely text-based questions. Questions that are difficult forthe AI tend to be that way invariably of the inventory language. We also findlarge variations in performance across languages, with some appearing tobenefit substantially from language switching, a phenomenon similar tocode-switching ofhuman speakers. Overall, comparing the obtained AI results tothe existing literature, we find that the AI system outperforms averageundergraduate students post-instruction in all subject areas but laboratoryskills.</description><author>Gerd Kortemeyer, Marina Babayeva, Giulia Polverini, Bor Gregorcic, Ralf Widenhorn</author><pubDate>Fri, 10 Jan 2025 18:08:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06143v1</guid></item><item><title>Emergent Symbol-like Number Variables in Artificial Neural Networks</title><link>http://arxiv.org/abs/2501.06141v1</link><description>What types of numeric representations emerge in Neural Networks (NNs)? Towhat degree do NNs induce abstract, mutable, slot-like numeric variables, andin what situations do these representations emerge? How do theserepresentations change over learning, and how can we understand the neuralimplementations in ways that are unified across different NNs? In this work, weapproach these questions by first training sequence based neural systems usingNext Token Prediction (NTP) objectives on numeric tasks. We then seek tounderstand the neural solutions through the lens of causal abstractions orsymbolic algorithms. We use a combination of causal interventions andvisualization methods to find that artificial neural models do indeed developanalogs of interchangeable, mutable, latent number variables purely from theNTP objective. We then ask how variations on the tasks and model architecturesaffect the models' learned solutions to find that these symbol-like numericrepresentations do not form for every variant of the task, and transformerssolve the problem in a notably different way than their recurrent counterparts.We then show how the symbol-like variables change over the course of trainingto find a strong correlation between the models' task performance and thealignment of their symbol-like representations. Lastly, we show that in allcases, some degree of gradience exists in these neural symbols, highlightingthe difficulty of finding simple, interpretable symbolic stories of how neuralnetworks perform numeric tasks. Taken together, our results are consistent withthe view that neural networks can approximate interpretable symbolic programsof number cognition, but the particular program they approximate and the extentto which they approximate it can vary widely, depending on the networkarchitecture, training data, extent of training, and network size.</description><author>Satchel Grant, Noah D. Goodman, James L. McClelland</author><pubDate>Fri, 10 Jan 2025 18:03:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06141v1</guid></item><item><title>Two Stage Segmentation of Cervical Tumors using PocketNet</title><link>http://arxiv.org/abs/2409.11456v2</link><description>Cervical cancer remains the fourth most common malignancy amongst womenworldwide.1 Concurrent chemoradiotherapy (CRT) serves as the mainstaydefinitive treatment regimen for locally advanced cervical cancers and includesexternal beam radiation followed by brachytherapy.2 Integral to radiotherapytreatment planning is the routine contouring of both the target tumor at thelevel of the cervix, associated gynecologic anatomy and the adjacent organs atrisk (OARs). However, manual contouring of these structures is both time andlabor intensive and associated with known interobserver variability that canimpact treatment outcomes. While multiple tools have been developed toautomatically segment OARs and the high-risk clinical tumor volume (HR-CTV)using computed tomography (CT) images,3,4,5,6 the development of deeplearning-based tumor segmentation tools using routine T2-weighted (T2w)magnetic resonance imaging (MRI) addresses an unmet clinical need to improvethe routine contouring of both anatomical structures and cervical cancers,thereby increasing quality and consistency of radiotherapy planning. This workapplied a novel deep-learning model (PocketNet) to segment the cervix, vagina,uterus, and tumor(s) on T2w MRI. The performance of the PocketNet architecturewas evaluated, when trained on data via 5-fold cross validation. PocketNetachieved a mean Dice-Sorensen similarity coefficient (DSC) exceeding 70% fortumor segmentation and 80% for organ segmentation. These results suggest thatPocketNet is robust to variations in contrast protocols, providing reliablesegmentation of the regions of interest.</description><author>Awj Twam, Megan Jacobsen, Rachel Glenn, Peng Wei, Jia Sun, Ann Klopp, Aradhana M. Venkatesan, David Fuentes</author><pubDate>Fri, 10 Jan 2025 17:54:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11456v2</guid></item><item><title>MS-Temba : Multi-Scale Temporal Mamba for Efficient Temporal Action Detection</title><link>http://arxiv.org/abs/2501.06138v1</link><description>Action detection in real-world scenarios is particularly challenging due todensely distributed actions in hour-long untrimmed videos. It requires modelingboth short- and long-term temporal relationships while handling significantintra-class temporal variations. Previous state-of-the-art (SOTA)Transformer-based architectures, though effective, are impractical forreal-world deployment due to their high parameter count, GPU memory usage, andlimited throughput, making them unsuitable for very long videos. In this work,we innovatively adapt the Mamba architecture for action detection and proposeMulti-scale Temporal Mamba (MS-Temba), comprising two key components: TemporalMamba (Temba) Blocks and the Temporal Mamba Fuser. Temba Blocks include theTemporal Local Module (TLM) for short-range temporal modeling and the DilatedTemporal SSM (DTS) for long-range dependencies. By introducing dilations, anovel concept for Mamba, TLM and DTS capture local and global features atmultiple scales. The Temba Fuser aggregates these scale-specific features usingMamba to learn comprehensive multi-scale representations of untrimmed videos.MS-Temba is validated on three public datasets, outperforming SOTA methods onlong videos and matching prior methods on short videos while using onlyone-eighth of the parameters.</description><author>Arkaprava Sinha, Monish Soundar Raj, Pu Wang, Ahmed Helmy, Srijan Das</author><pubDate>Fri, 10 Jan 2025 17:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06138v1</guid></item><item><title>Learning a Consensus Sub-Network with Polarization Regularization and One Pass Training</title><link>http://arxiv.org/abs/2302.10798v5</link><description>The subject of green AI has been gaining attention within the deep learningcommunity given the recent trend of ever larger and more complex neural networkmodels. Existing solutions for reducing the computational load of training atinference time usually involve pruning the network parameters. Pruning schemesoften create extra overhead either by iterative training and fine-tuning forstatic pruning or repeated computation of a dynamic pruning graph. We propose anew parameter pruning strategy for learning a lighter-weight sub-network thatminimizes the energy cost while maintaining comparable performance to the fullyparameterised network on given downstream tasks. Our proposed pruning scheme isgreen-oriented, as it only requires a one-off training to discover the optimalstatic sub-networks by dynamic pruning methods. The pruning scheme consists ofa binary gating module and a polarizing loss function to uncover sub-networkswith user-defined sparsity. Our method enables pruning and trainingsimultaneously, which saves energy in both the training and inference phasesand avoids extra computational overhead from gating modules at inference time.Our results on CIFAR-10, CIFAR-100, and Tiny Imagenet suggest that our schemecan remove 50% of connections in deep networks with &lt;1% reduction inclassification accuracy. Compared to other related pruning methods, our methoddemonstrates a lower drop in accuracy for equivalent reductions incomputational cost.</description><author>Xiaoying Zhi, Varun Babbar, Rundong Liu, Pheobe Sun, Fran Silavong, Ruibo Shi, Sean Moran</author><pubDate>Fri, 10 Jan 2025 13:43:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10798v5</guid></item><item><title>Effective faking of verbal deception detection with target-aligned adversarial attacks</title><link>http://arxiv.org/abs/2501.05962v1</link><description>Background: Deception detection through analysing language is a promisingavenue using both human judgments and automated machine learning judgments. Forboth forms of credibility assessment, automated adversarial attacks thatrewrite deceptive statements to appear truthful pose a serious threat. Methods:We used a dataset of 243 truthful and 262 fabricated autobiographical storiesin a deception detection task for humans and machine learning models. A largelanguage model was tasked to rewrite deceptive statements so that they appeartruthful. In Study 1, humans who made a deception judgment or used thedetailedness heuristic and two machine learning models (a fine-tuned languagemodel and a simple n-gram model) judged original or adversarial modificationsof deceptive statements. In Study 2, we manipulated the target alignment of themodifications, i.e. tailoring the attack to whether the statements would beassessed by humans or computer models. Results: When adversarial modificationswere aligned with their target, human (d=-0.07 and d=-0.04) and machinejudgments (51% accuracy) dropped to the chance level. When the attack was notaligned with the target, both human heuristics judgments (d=0.30 and d=0.36)and machine learning predictions (63-78%) were significantly better thanchance. Conclusions: Easily accessible language models can effectively helpanyone fake deception detection efforts both by humans and machine learningmodels. Robustness against adversarial modifications for humans and machinesdepends on that target alignment. We close with suggestions on advancingdeception research with adversarial attack designs.</description><author>Bennett Kleinberg, Riccardo Loconte, Bruno Verschuere</author><pubDate>Fri, 10 Jan 2025 13:42:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05962v1</guid></item><item><title>Swin-X2S: Reconstructing 3D Shape from 2D Biplanar X-ray with Swin Transformers</title><link>http://arxiv.org/abs/2501.05961v1</link><description>The conversion from 2D X-ray to 3D shape holds significant potential forimproving diagnostic efficiency and safety. However, existing reconstructionmethods often rely on hand-crafted features, manual intervention, and priorknowledge, resulting in unstable shape errors and additional processing costs.In this paper, we introduce Swin-X2S, an end-to-end deep learning method fordirectly reconstructing 3D segmentation and labeling from 2D biplanarorthogonal X-ray images. Swin-X2S employs an encoder-decoder architecture: theencoder leverages 2D Swin Transformer for X-ray information extraction, whilethe decoder employs 3D convolution with cross-attention to integrate structuralfeatures from orthogonal views. A dimension-expanding module is introduced tobridge the encoder and decoder, ensuring a smooth conversion from 2D pixels to3D voxels. We evaluate proposed method through extensive qualitative andquantitative experiments across nine publicly available datasets covering fouranatomies (femur, hip, spine, and rib), with a total of 54 categories.Significant improvements over previous methods have been observed not only inthe segmentation and labeling metrics but also in the clinically relevantparameters that are of primary concern in practical applications, whichdemonstrates the promise of Swin-X2S to provide an effective option foranatomical shape reconstruction in clinical scenarios. Code implementation isavailable at: \url{https://github.com/liukuan5625/Swin-X2S}.</description><author>Kuan Liu, Zongyuan Ying, Jie Jin, Dongyan Li, Ping Huang, Wenjian Wu, Zhe Chen, Jin Qi, Yong Lu, Lianfu Deng, Bo Chen</author><pubDate>Fri, 10 Jan 2025 13:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05961v1</guid></item><item><title>On Large Language Models in Mission-Critical IT Governance: Are We Ready Yet?</title><link>http://arxiv.org/abs/2412.11698v2</link><description>Context. The security of critical infrastructure has been a pressing concernsince the advent of computers and has become even more critical in today's eraof cyber warfare. Protecting mission-critical systems (MCSs), essential fornational security, requires swift and robust governance, yet recent eventsreveal the increasing difficulty of meeting these challenges. Aim. Building onprior research showcasing the potential of Generative AI (GAI), such as LargeLanguage Models, in enhancing risk analysis, we aim to explore practitioners'views on integrating GAI into the governance of IT MCSs. Our goal is to provideactionable insights and recommendations for stakeholders, includingresearchers, practitioners, and policymakers. Method. We designed a survey tocollect practical experiences, concerns, and expectations of practitioners whodevelop and implement security solutions in the context of MCSs. Conclusionsand Future Works. Our findings highlight that the safe use of LLMs in MCSgovernance requires interdisciplinary collaboration. Researchers should focuson designing regulation-oriented models and focus on accountability;practitioners emphasize data protection and transparency, while policymakersmust establish a unified AI framework with global benchmarks to ensure ethicaland secure LLMs-based MCS governance.</description><author>Matteo Esposito, Francesco Palagiano, Valentina Lenarduzzi, Davide Taibi</author><pubDate>Fri, 10 Jan 2025 13:35:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11698v2</guid></item><item><title>Scalable Vision Language Model Training via High Quality Data Curation</title><link>http://arxiv.org/abs/2501.05952v1</link><description>In this paper, we introduce SAIL-VL (ScAlable Vision Language Model TraIningvia High QuaLity Data Curation), an open-source vision language model (VLM) ofstate-of-the-art (SOTA) performance with 2B parameters. We introduce three keyimprovements that contribute to SAIL-VL's leading performance: (1) Scalablehigh-quality visual understanding data construction: We implement a visualunderstanding data construction pipeline, which enables hundred-million-scalehigh-quality recaption data annotation. Equipped with this pipeline, we curateSAIL-Caption, a large-scale caption dataset with large quantity and the highestdata quality compared with opensource caption datasets. (2) ScalablePretraining with High-Quality Visual Understanding Data: We scale SAIL-VL'spretraining budget up to 131B tokens and show that even a 2B VLM benefits fromscaled up training data sizes, exhibiting expected data size scaling laws invisual understanding and instruction following performance. (3) Scalable SFTvia quantity and quality scaling: We introduce general guidance for instructiondata curation to scale up instruction data continuously, allowing us toconstruct a large SFT dataset with the highest quality. To further improveSAIL-VL's performance, we propose quality scaling, a multi-stage trainingrecipe with curriculum learning, to improve model performance scaling curvesw.r.t. data sizes from logarithmic to be near-linear. SAIL-VL obtains thehighest average score in 19 commonly used benchmarks in our evaluation andachieves top1 performance among VLMs of comparable sizes on OpenCompass(https://rank.opencompass.org.cn/leaderboard-multimodal). We release ourSAIL-VL-2B model at HuggingFace(https://huggingface.co/BytedanceDouyinContent/SAIL-VL-2B).</description><author>Hongyuan Dong, Zijian Kang, Weijie Yin, Xiao Liang, Chao Feng, Jiao Ran</author><pubDate>Fri, 10 Jan 2025 13:27:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05952v1</guid></item><item><title>Strip R-CNN: Large Strip Convolution for Remote Sensing Object Detection</title><link>http://arxiv.org/abs/2501.03775v3</link><description>While witnessed with rapid development, remote sensing object detectionremains challenging for detecting high aspect ratio objects. This paper showsthat large strip convolutions are good feature representation learners forremote sensing object detection and can detect objects of various aspect ratioswell. Based on large strip convolutions, we build a new network architecturecalled Strip R-CNN, which is simple, efficient, and powerful. Unlike recentremote sensing object detectors that leverage large-kernel convolutions withsquare shapes, our Strip R-CNN takes advantage of sequential orthogonal largestrip convolutions to capture spatial information. In addition, we enhance thelocalization capability of remote-sensing object detectors by decoupling thedetection heads and equipping the localization head with strip convolutions tobetter localize the target objects. Extensive experiments on severalbenchmarks, e.g., DOTA, FAIR1M, HRSC2016, and DIOR, show that our Strip R-CNNcan largely improve previous works. Notably, our 30M model achieves 82.75% mAPon DOTA-v1.0, setting a new state-of-the-art record.Code is available athttps://github.com/YXB-NKU/Strip-R-CNN.</description><author>Xinbin Yuan, Zhaohui Zheng, Yuxuan Li, Xialei Liu, Li Liu, Xiang Li, Qibin Hou, Ming-Ming Cheng</author><pubDate>Fri, 10 Jan 2025 13:25:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.03775v3</guid></item><item><title>Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary Empirical Study</title><link>http://arxiv.org/abs/2406.11629v5</link><description>Utilizing Large Language Models (LLMs) as evaluators to assess theperformance of LLMs has garnered attention. However, this kind of evaluationapproach is affected by potential biases within LLMs, raising concerns aboutthe accuracy and reliability of the evaluation results of LLMs. To address thisproblem, we propose and study two many-shot In-Context Learning (ICL) prompttemplates to help LLM evaluators mitigate potential biases: Many-Shot withReference (MSwR) and Many-Shot without Reference (MSoR). Specifically, theformer utilizes in-context examples with model-generated evaluation rationalesas references, while the latter does not include these references. Using theseprompt designs, we investigate the impact of increasing the number ofin-context examples on the consistency and quality of the evaluation results.Experimental results show that advanced LLMs, such as GPT-4o, perform better inthe many-shot regime than in the zero-shot and few-shot regimes. Furthermore,when using GPT-4o as an evaluator in the many-shot regime, adopting MSwR as theprompt template performs better than MSoR.</description><author>Mingyang Song, Mao Zheng, Xuan Luo, Yue Pan</author><pubDate>Fri, 10 Jan 2025 13:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11629v5</guid></item><item><title>Universal-2-TF: Robust All-Neural Text Formatting for ASR</title><link>http://arxiv.org/abs/2501.05948v1</link><description>This paper introduces an all-neural text formatting (TF) model designed forcommercial automatic speech recognition (ASR) systems, encompassing punctuationrestoration (PR), truecasing, and inverse text normalization (ITN). Unliketraditional rule-based or hybrid approaches, this method leverages a two-stageneural architecture comprising a multi-objective token classifier and asequence-to-sequence (seq2seq) model. This design minimizes computational costsand reduces hallucinations while ensuring flexibility and robustness acrossdiverse linguistic entities and text domains. Developed as part of theUniversal-2 ASR system, the proposed method demonstrates superior performancein TF accuracy, computational efficiency, and perceptual quality, as validatedthrough comprehensive evaluations using both objective and subjective methods.This work underscores the importance of holistic TF models in enhancing ASRusability in practical settings.</description><author>Yash Khare, Taufiquzzaman Peyash, Andrea Vanzo, Takuya Yoshioka</author><pubDate>Fri, 10 Jan 2025 13:21:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05948v1</guid></item><item><title>Reusable specimen-level inference in computational pathology</title><link>http://arxiv.org/abs/2501.05945v1</link><description>Foundation models for computational pathology have shown great promise forspecimen-level tasks and are increasingly accessible to researchers. However,specimen-level models built on these foundation models remain largelyunavailable, hindering their broader utility and impact. To address this gap,we developed SpinPath, a toolkit designed to democratize specimen-level deeplearning by providing a zoo of pretrained specimen-level models, a Python-basedinference engine, and a JavaScript-based inference platform. We demonstrate theutility of SpinPath in metastasis detection tasks across nine foundationmodels. SpinPath may foster reproducibility, simplify experimentation, andaccelerate the adoption of specimen-level deep learning in computationalpathology research.</description><author>Jakub R. Kaczmarzyk, Rishul Sharma, Peter K. Koo, Joel H. Saltz</author><pubDate>Fri, 10 Jan 2025 13:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05945v1</guid></item><item><title>Dolphin: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback</title><link>http://arxiv.org/abs/2501.03916v2</link><description>The scientific research paradigm is undergoing a profound transformationowing to the development of Artificial Intelligence (AI). Recent worksdemonstrate that various AI-assisted research methods can largely improveresearch efficiency by improving data analysis, accelerating computation, andfostering novel idea generation. To further move towards the ultimate goal(i.e., automatic scientific research), in this paper, we propose Dolphin, thefirst closed-loop open-ended auto-research framework to further build theentire process of human scientific research. Dolphin can generate researchideas, perform experiments, and get feedback from experimental results togenerate higher-quality ideas. More specifically, Dolphin first generates novelideas based on relevant papers which are ranked by the topic and taskattributes. Then, the codes are automatically generated and debugged with theexception-traceback-guided local code structure. Finally, Dolphin automaticallyanalyzes the results of each idea and feeds the results back to the next roundof idea generation. Experiments are conducted on the benchmark datasets ofdifferent topics and results show that Dolphin can generate novel ideascontinuously and complete the experiment in a loop. We highlight that Dolphincan automatically propose methods that are comparable to the state-of-the-artin some tasks such as 2D image classification and 3D point classification.</description><author>Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen, Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, Bowen Zhou</author><pubDate>Fri, 10 Jan 2025 13:14:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.03916v2</guid></item><item><title>Soft regression trees: a model variant and a decomposition training algorithm</title><link>http://arxiv.org/abs/2501.05942v1</link><description>Decision trees are widely used for classification and regression tasks in avariety of application fields due to their interpretability and good accuracy.During the past decade, growing attention has been devoted to globallyoptimized decision trees with deterministic or soft splitting rules at branchnodes, which are trained by optimizing the error function over all the treeparameters. In this work, we propose a new variant of soft multivariateregression trees (SRTs) where, for every input vector, the prediction isdefined as the linear regression associated to a single leaf node, namely, theleaf node obtained by routing the input vector from the root along the brancheswith higher probability. SRTs exhibit the conditional computational property,i.e., each prediction depends on a small number of nodes (parameters), and ournonlinear optimization formulation for training them is amenable todecomposition. After showing a universal approximation result for SRTs, wepresent a decomposition training algorithm including a clustering-basedinitialization procedure and a heuristic for reassigning the input vectorsalong the tree. Under mild assumptions, we establish asymptotic convergenceguarantees. Experiments on 15 wellknown datasets indicate that our SRTs anddecomposition algorithm yield higher accuracy and robustness compared withtraditional soft regression trees trained using the nonlinear optimizationformulation of Blanquero et al., and a significant reduction in training timesas well as a slightly better average accuracy compared with the mixed-integeroptimization approach of Bertsimas and Dunn. We also report a comparison withthe Random Forest ensemble method.</description><author>Antonio Consolo, Edoardo Amaldi, Andrea Manno</author><pubDate>Fri, 10 Jan 2025 13:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05942v1</guid></item><item><title>Class Distance Weighted Cross Entropy Loss for Classification of Disease Severity</title><link>http://arxiv.org/abs/2412.01246v2</link><description>Assessing disease severity involving ordinal classes, where each classrepresents increasing levels of severity, benefit from loss functions thataccount for this ordinal structure. Traditional categorical loss functions,like Cross-Entropy (CE), often perform suboptimally in these scenarios. Toaddress this, we propose a novel loss function, Class Distance WeightedCross-Entropy (CDW-CE), which penalizes misclassifications more harshly whenclasses are farther apart. We evaluated CDW-CE on the Labeled Images forUlcerative Colitis (LIMUC) dataset using various deep architectures. Itsperformance was compared against several categorical and ordinal lossfunctions. To analyze the quality of latent representations, we usedt-distributed stochastic neighbor embedding (t-SNE) visualizations andquantified their clustering with the Silhouette Score. We also compared ClassActivation Maps (CAM) generated by models trained with CDW-CE and CE loss,incorporating domain expert feedback to evaluate alignment with expertknowledge. Our results show that CDW-CE consistently improves performance inordinal image classification tasks. It achieves higher Silhouette Scores,indicating better differentiation of class representations, and its CAMvisualizations demonstrate a stronger focus on clinically significant regions,as confirmed by domain experts.</description><author>Gorkem Polat, Ümit Mert Çağlar, Alptekin Temizel</author><pubDate>Fri, 10 Jan 2025 13:02:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.01246v2</guid></item><item><title>LitSumm: Large language models for literature summarisation of non-coding RNAs</title><link>http://arxiv.org/abs/2311.03056v4</link><description>Curation of literature in life sciences is a growing challenge. The continuedincrease in the rate of publication, coupled with the relatively fixed numberof curators worldwide presents a major challenge to developers of biomedicalknowledgebases. Very few knowledgebases have resources to scale to the wholerelevant literature and all have to prioritise their efforts. In this work, we take a first step to alleviating the lack of curator time inRNA science by generating summaries of literature for non-coding RNAs usinglarge language models (LLMs). We demonstrate that high-quality, factuallyaccurate summaries with accurate references can be automatically generated fromthe literature using a commercial LLM and a chain of prompts and checks. Manualassessment was carried out for a subset of summaries, with the majority beingrated extremely high quality. We apply our tool to a selection of over 4,600 ncRNAs and make the generatedsummaries available via the RNAcentral resource. We conclude that automatedliterature summarization is feasible with the current generation of LLMs,provided careful prompting and automated checking are applied.</description><author>Andrew Green, Carlos Ribas, Nancy Ontiveros-Palacios, Sam Griffiths-Jones, Anton I. Petrov, Alex Bateman, Blake Sweeney</author><pubDate>Fri, 10 Jan 2025 13:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03056v4</guid></item><item><title>A Multimodal Dataset for Enhancing Industrial Task Monitoring and Engagement Prediction</title><link>http://arxiv.org/abs/2501.05936v1</link><description>Detecting and interpreting operator actions, engagement, and objectinteractions in dynamic industrial workflows remains a significant challenge inhuman-robot collaboration research, especially within complex, real-worldenvironments. Traditional unimodal methods often fall short of capturing theintricacies of these unstructured industrial settings. To address this gap, wepresent a novel Multimodal Industrial Activity Monitoring (MIAM) dataset thatcaptures realistic assembly and disassembly tasks, facilitating the evaluationof key meta-tasks such as action localization, object interaction, andengagement prediction. The dataset comprises multi-view RGB, depth, andInertial Measurement Unit (IMU) data collected from 22 sessions, amounting to290 minutes of untrimmed video, annotated in detail for task performance andoperator behavior. Its distinctiveness lies in the integration of multiple datamodalities and its emphasis on real-world, untrimmed industrial workflows-keyfor advancing research in human-robot collaboration and operator monitoring.Additionally, we propose a multimodal network that fuses RGB frames, IMU data,and skeleton sequences to predict engagement levels during industrial tasks.Our approach improves the accuracy of recognizing engagement states, providinga robust solution for monitoring operator performance in dynamic industrialenvironments. The dataset and code can be accessed fromhttps://github.com/navalkishoremehta95/MIAM/.</description><author>Naval Kishore Mehta, Arvind, Himanshu Kumar, Abeer Banerjee, Sumeet Saurav, Sanjay Singh</author><pubDate>Fri, 10 Jan 2025 12:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05936v1</guid></item><item><title>CloudTrack: Scalable UAV Tracking with Cloud Semantics</title><link>http://arxiv.org/abs/2409.16111v2</link><description>Nowadays, unmanned aerial vehicles (UAVs) are commonly used in search andrescue scenarios to gather information in the search area. The automaticidentification of the person searched for in aerial footage could increase theautonomy of such systems, reduce the search time, and thus increase the missedperson's chances of survival. In this paper, we present a novel approach toperform semantically conditioned open vocabulary object tracking that isspecifically designed to cope with the limitations of UAV hardware. Ourapproach has several advantages. It can run with verbal descriptions of themissing person, e.g., the color of the shirt, it does not require dedicatedtraining to execute the mission and can efficiently track a potentially movingperson. Our experimental results demonstrate the versatility and efficacy ofour approach.</description><author>Yannik Blei, Michael Krawez, Nisarga Nilavadi, Tanja Katharina Kaiser, Wolfram Burgard</author><pubDate>Fri, 10 Jan 2025 12:56:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16111v2</guid></item><item><title>Encoded Spatial Attribute in Multi-Tier Federated Learning</title><link>http://arxiv.org/abs/2501.05934v1</link><description>This research presents an Encoded Spatial Multi-Tier Federated Learningapproach for a comprehensive evaluation of aggregated models for geospatialdata. In the client tier, encoding spatial information is introduced to betterpredict the target outcome. The research aims to assess the performance ofthese models across diverse datasets and spatial attributes, highlightingvariations in predictive accuracy. Using evaluation metrics such as accuracy,our research reveals insights into the complexities of spatial granularity andthe challenges of capturing underlying patterns in the data. We extended thescope of federated learning (FL) by having multi-tier along with thefunctionality of encoding spatial attributes. Our N-tier FL approach usedencoded spatial data to aggregate in different tiers. We obtained multiplemodels that predicted the different granularities of spatial data. Our findingsunderscore the need for further research to improve predictive accuracy andmodel generalization, with potential avenues including incorporating additionalfeatures, refining model architectures, and exploring alternative modelingapproaches. Our experiments have several tiers representing different levels ofspatial aspects. We obtained accuracy of 75.62% and 89.52% for the global modelwithout having to train the model using the data constituted with thedesignated tier. The research also highlights the importance of the proposedapproach in real-time applications.</description><author>Asfia Kawnine, Francis Palma, Seyed Alireza Rahimi Azghadi, Hung Cao</author><pubDate>Fri, 10 Jan 2025 12:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05934v1</guid></item><item><title>Weakly Supervised Segmentation of Hyper-Reflective Foci with Compact Convolutional Transformers and SAM2</title><link>http://arxiv.org/abs/2501.05933v1</link><description>Weakly supervised segmentation has the potential to greatly reduce theannotation effort for training segmentation models for small structures such ashyper-reflective foci (HRF) in optical coherence tomography (OCT). However,most weakly supervised methods either involve a strong downsampling of inputimages, or only achieve localization at a coarse resolution, both of which areunsatisfactory for small structures. We propose a novel framework thatincreases the spatial resolution of a traditional attention-based MultipleInstance Learning (MIL) approach by using Layer-wise Relevance Propagation(LRP) to prompt the Segment Anything Model (SAM~2), and increases recall withiterative inference. Moreover, we demonstrate that replacing MIL with a CompactConvolutional Transformer (CCT), which adds a positional encoding, and permitsan exchange of information between different regions of the OCT image, leads toa further and substantial increase in segmentation accuracy.</description><author>Olivier Morelle, Justus Bisten, Maximilian W. M. Wintergerst, Robert P. Finger, Thomas Schultz</author><pubDate>Fri, 10 Jan 2025 12:56:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05933v1</guid></item><item><title>DiffuSETS: 12-lead ECG Generation Conditioned on Clinical Text Reports and Patient-Specific Information</title><link>http://arxiv.org/abs/2501.05932v1</link><description>Heart disease remains a significant threat to human health. As a non-invasivediagnostic tool, the electrocardiogram (ECG) is one of the most widely usedmethods for cardiac screening. However, the scarcity of high-quality ECG data,driven by privacy concerns and limited medical resources, creates a pressingneed for effective ECG signal generation. Existing approaches for generatingECG signals typically rely on small training datasets, lack comprehensiveevaluation frameworks, and overlook potential applications beyond dataaugmentation. To address these challenges, we propose DiffuSETS, a novelframework capable of generating ECG signals with high semantic alignment andfidelity. DiffuSETS accepts various modalities of clinical text reports andpatient-specific information as inputs, enabling the creation of clinicallymeaningful ECG signals. Additionally, to address the lack of standardizedevaluation in ECG generation, we introduce a comprehensive benchmarkingmethodology to assess the effectiveness of generative models in this domain.Our model achieve excellent results in tests, proving its superiority in thetask of ECG generation. Furthermore, we showcase its potential to mitigate datascarcity while exploring novel applications in cardiology education and medicalknowledge discovery, highlighting the broader impact of our work.</description><author>Yongfan Lai, Jiabo Chen, Deyun Zhang, Yue Wang, Shijia Geng, Hongyan Li, Shenda Hong</author><pubDate>Fri, 10 Jan 2025 12:55:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05932v1</guid></item><item><title>Random Sparse Lifts: Construction, Analysis and Convergence of finite sparse networks</title><link>http://arxiv.org/abs/2501.05930v1</link><description>We present a framework to define a large class of neural networks for which,by construction, training by gradient flow provably reaches arbitrarily lowloss when the number of parameters grows. Distinct from the fixed-space globaloptimality of non-convex optimization, this new form of convergence, and thetechniques introduced to prove such convergence, pave the way for a usable deeplearning convergence theory in the near future, without overparameterizationassumptions relating the number of parameters and training samples. We definethese architectures from a simple computation graph and a mechanism to lift it,thus increasing the number of parameters, generalizing the idea of increasingthe widths of multi-layer perceptrons. We show that architectures similar tomost common deep learning models are present in this class, obtained bysparsifying the weight tensors of usual architectures at initialization.Leveraging tools of algebraic topology and random graph theory, we use thecomputation graph's geometry to propagate properties guaranteeing convergenceto any precision for these large sparse models.</description><author>David A. R. Robin, Kevin Scaman, Marc Lelarge</author><pubDate>Fri, 10 Jan 2025 12:52:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05930v1</guid></item><item><title>Towards Backdoor Stealthiness in Model Parameter Space</title><link>http://arxiv.org/abs/2501.05928v1</link><description>Recent research on backdoor stealthiness focuses mainly on indistinguishabletriggers in input space and inseparable backdoor representations in featurespace, aiming to circumvent backdoor defenses that examine these respectivespaces. However, existing backdoor attacks are typically designed to resist aspecific type of backdoor defense without considering the diverse range ofdefense mechanisms. Based on this observation, we pose a natural question: Arecurrent backdoor attacks truly a real-world threat when facing diversepractical defenses? To answer this question, we examine 12 common backdoor attacks that focus oninput-space or feature-space stealthiness and 17 diverse representativedefenses. Surprisingly, we reveal a critical blind spot: Backdoor attacksdesigned to be stealthy in input and feature spaces can be mitigated byexamining backdoored models in parameter space. To investigate the underlyingcauses behind this common vulnerability, we study the characteristics ofbackdoor attacks in the parameter space. Notably, we find that input- andfeature-space attacks introduce prominent backdoor-related neurons in parameterspace, which are not thoroughly considered by current backdoor attacks. Takingcomprehensive stealthiness into account, we propose a novel supply-chain attackcalled Grond. Grond limits the parameter changes by a simple yet effectivemodule, Adversarial Backdoor Injection (ABI), which adaptively increases theparameter-space stealthiness during the backdoor injection. Extensiveexperiments demonstrate that Grond outperforms all 12 backdoor attacks againststate-of-the-art (including adaptive) defenses on CIFAR-10, GTSRB, and a subsetof ImageNet. In addition, we show that ABI consistently improves theeffectiveness of common backdoor attacks.</description><author>Xiaoyun Xu, Zhuoran Liu, Stefanos Koffas, Stjepan Picek</author><pubDate>Fri, 10 Jan 2025 12:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05928v1</guid></item><item><title>LLMs Reproduce Stereotypes of Sexual and Gender Minorities</title><link>http://arxiv.org/abs/2501.05926v1</link><description>A large body of research has found substantial gender bias in NLP systems.Most of this research takes a binary, essentialist view of gender: limiting itsvariation to the categories _men_ and _women_, conflating gender with sex, andignoring different sexual identities. But gender and sexuality exist on aspectrum, so in this paper we study the biases of large language models (LLMs)towards sexual and gender minorities beyond binary categories. Grounding ourstudy in a widely used psychological framework -- the Stereotype Content Model-- we demonstrate that English-language survey questions about socialperceptions elicit more negative stereotypes of sexual and gender minoritiesfrom LLMs, just as they do from humans. We then extend this framework to a morerealistic use case: text generation. Our analysis shows that LLMs generatestereotyped representations of sexual and gender minorities in this setting,raising concerns about their capacity to amplify representational harms increative writing, a widely promoted use case.</description><author>Ruby Ostrow, Adam Lopez</author><pubDate>Fri, 10 Jan 2025 12:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05926v1</guid></item><item><title>Navigating Tomorrow: Reliably Assessing Large Language Models Performance on Future Event Prediction</title><link>http://arxiv.org/abs/2501.05925v1</link><description>Predicting future events is an important activity with applications acrossmultiple fields and domains. For example, the capacity to foresee stock markettrends, natural disasters, business developments, or political events canfacilitate early preventive measures and uncover new opportunities. Multiplediverse computational methods for attempting future predictions, includingpredictive analysis, time series forecasting, and simulations have beenproposed. This study evaluates the performance of several large language models(LLMs) in supporting future prediction tasks, an under-explored domain. Weassess the models across three scenarios: Affirmative vs. Likelihoodquestioning, Reasoning, and Counterfactual analysis. For this, we create adataset1 by finding and categorizing news articles based on entity type and itspopularity. We gather news articles before and after the LLMs training cutoffdate in order to thoroughly test and compare model performance. Our researchhighlights LLMs potential and limitations in predictive modeling, providing afoundation for future improvements.</description><author>Petraq Nako, Adam Jatowt</author><pubDate>Fri, 10 Jan 2025 12:44:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05925v1</guid></item><item><title>Neural Differential Appearance Equations</title><link>http://arxiv.org/abs/2410.07128v2</link><description>We propose a method to reproduce dynamic appearance textures withspace-stationary but time-varying visual statistics. While most previous workdecomposes dynamic textures into static appearance and motion, we focus ondynamic appearance that results not from motion but variations of fundamentalproperties, such as rusting, decaying, melting, and weathering. To this end, weadopt the neural ordinary differential equation (ODE) to learn the underlyingdynamics of appearance from a target exemplar. We simulate the ODE in twophases. At the "warm-up" phase, the ODE diffuses a random noise to an initialstate. We then constrain the further evolution of this ODE to replicate theevolution of visual feature statistics in the exemplar during the generationphase. The particular innovation of this work is the neural ODE achieving bothdenoising and evolution for dynamics synthesis, with a proposed temporaltraining scheme. We study both relightable (BRDF) and non-relightable (RGB)appearance models. For both we introduce new pilot datasets, allowing, for thefirst time, to study such phenomena: For RGB we provide 22 dynamic texturesacquired from free online sources; For BRDFs, we further acquire a dataset of21 flash-lit videos of time-varying materials, enabled by a simple-to-constructsetup. Our experiments show that our method consistently yields realistic andcoherent results, whereas prior works falter under pronounced temporalappearance variations. A user study confirms our approach is preferred toprevious work for such exemplars.</description><author>Chen Liu, Tobias Ritschel</author><pubDate>Fri, 10 Jan 2025 12:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07128v2</guid></item><item><title>Fast unsupervised ground metric learning with tree-Wasserstein distance</title><link>http://arxiv.org/abs/2411.07432v2</link><description>The performance of unsupervised methods such as clustering depends on thechoice of distance metric between features, or ground metric. Commonly, groundmetrics are decided with heuristics or learned via supervised algorithms.However, since many interesting datasets are unlabelled, unsupervised groundmetric learning approaches have been introduced. One promising option employsWasserstein singular vectors (WSVs), which emerge when computing optimaltransport distances between features and samples simultaneously. WSVs areeffective, but can be prohibitively computationally expensive in someapplications: $\mathcal{O}(n^2m^2(n \log(n) + m \log(m))$ for $n$ samples and$m$ features. In this work, we propose to augment the WSV method by embeddingsamples and features on trees, on which we compute the tree-Wassersteindistance (TWD). We demonstrate theoretically and empirically that the algorithmconverges to a better approximation of the standard WSV approach than the bestknown alternatives, and does so with $\mathcal{O}(n^3+m^3+mn)$ complexity. Inaddition, we prove that the initial tree structure can be chosen flexibly,since tree geometry does not constrain the richness of the approximation up tothe number of edge weights. This proof suggests a fast and recursive algorithmfor computing the tree parameter basis set, which we find crucial to realisingthe efficiency gains at scale. Finally, we employ the tree-WSV algorithm toseveral single-cell RNA sequencing genomics datasets, demonstrating itsscalability and utility for unsupervised cell-type clustering problems. Theseresults poise unsupervised ground metric learning with TWD as a low-rankapproximation of WSV with the potential for widespread application.</description><author>Kira M. Düsterwald, Samo Hromadka, Makoto Yamada</author><pubDate>Fri, 10 Jan 2025 12:33:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07432v2</guid></item><item><title>Chimera: Improving Generalist Model with Domain-Specific Experts</title><link>http://arxiv.org/abs/2412.05983v2</link><description>Recent advancements in Large Multi-modal Models (LMMs) underscore theimportance of scaling by increasing image-text paired data, achievingimpressive performance on general tasks. Despite their effectiveness in broadapplications, generalist models are primarily trained on web-scale datasetsdominated by natural images, resulting in the sacrifice of specializedcapabilities for domain-specific tasks that require extensive domain priorknowledge. Moreover, directly integrating expert models tailored for specificdomains is challenging due to the representational gap and imbalancedoptimization between the generalist model and experts. To address thesechallenges, we introduce Chimera, a scalable and low-cost multi-modal pipelinedesigned to boost the ability of existing LMMs with domain-specific experts.Specifically, we design a progressive training strategy to integrate featuresfrom expert models into the input of a generalist LMM. To address theimbalanced optimization caused by the well-aligned general visual encoder, weintroduce a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism.This results in a versatile model that excels across the chart, table, math,and document domains, achieving state-of-the-art performance on multi-modalreasoning and visual content extraction tasks, both of which are challengingtasks for assessing existing LMMs.</description><author>Tianshuo Peng, Mingsheng Li, Hongbin Zhou, Renqiu Xia, Renrui Zhang, Lei Bai, Song Mao, Bin Wang, Conghui He, Aojun Zhou, Botian Shi, Tao Chen, Bo Zhang, Xiangyu Yue</author><pubDate>Fri, 10 Jan 2025 12:28:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05983v2</guid></item><item><title>The New Anticipatory Governance Culture for Innovation: Regulatory Foresight, Regulatory Experimentation and Regulatory Learning</title><link>http://arxiv.org/abs/2501.05921v1</link><description>With the rapid pace of technological innovation, traditional methods ofpolicy formation and legislating are becoming conspicuously anachronistic. Theneed for regulatory choices to be made to counter the deadening effect ofregulatory lag is more important to developing markets and fostering growththan achieving one off regulatory perfection. This article advances scholarshipon innovation policy and the regulation of technological innovation in theEuropean Union. It does so by considering what building an agile yet robustanticipatory governance regulatory culture involves. It systematicallyexcavates a variety of tools and elements that are being put into use ininventive ways and argues that these need to be more cohesively andsystemically integrated into the regulatory toolbox. Approaches covered includestrategic foresight, the critical embrace of iterative policy development andregulatory learning in the face of uncertainty and the embrace of bottom upapproaches to cocreation of policy such as Policy Labs and the testing andregulatory learning through pilot regulation and experimentation. The growinguse of regulatory sandboxes as an EU policy tool to boost innovation andnavigate regulatory complexity as seen in the EU AI Act is also probed</description><author>Deirdre Ahern</author><pubDate>Fri, 10 Jan 2025 12:26:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05921v1</guid></item><item><title>GeoX: Geometric Problem Solving Through Unified Formalized Vision-Language Pre-training</title><link>http://arxiv.org/abs/2412.11863v2</link><description>Despite their proficiency in general tasks, Multi-modal Large Language Models(MLLMs) struggle with automatic Geometry Problem Solving (GPS), which demandsunderstanding diagrams, interpreting symbols, and performing complex reasoning.This limitation arises from their pre-training on natural images and texts,along with the lack of automated verification in the problem-solving process.Besides, current geometric specialists are limited by their task-specificdesigns, making them less effective for broader geometric problems. To thisend, we present GeoX, a multi-modal large model focusing on geometricunderstanding and reasoning tasks. Given the significant differences betweengeometric diagram-symbol and natural image-text, we introduce unimodalpre-training to develop a diagram encoder and symbol decoder, enhancing theunderstanding of geometric images and corpora. Furthermore, we introducegeometry-language alignment, an effective pre-training paradigm that bridgesthe modality gap between unimodal geometric experts. We propose aGenerator-And-Sampler Transformer (GS-Former) to generate discriminativequeries and eliminate uninformative representations from unevenly distributedgeometric signals. Finally, GeoX benefits from visual instruction tuning,empowering it to take geometric images and questions as input and generateverifiable solutions. Experiments show that GeoX outperforms both generalistsand geometric specialists on publicly recognized benchmarks, such as GeoQA,UniGeo, Geometry3K, and PGPS9k.</description><author>Renqiu Xia, Mingsheng Li, Hancheng Ye, Wenjie Wu, Hongbin Zhou, Jiakang Yuan, Tianshuo Peng, Xinyu Cai, Xiangchao Yan, Bin Wang, Conghui He, Botian Shi, Tao Chen, Junchi Yan, Bo Zhang</author><pubDate>Fri, 10 Jan 2025 12:22:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11863v2</guid></item><item><title>Backdoor Attacks against No-Reference Image Quality Assessment Models via a Scalable Trigger</title><link>http://arxiv.org/abs/2412.07277v2</link><description>No-Reference Image Quality Assessment (NR-IQA), responsible for assessing thequality of a single input image without using any reference, plays a criticalrole in evaluating and optimizing computer vision systems, e.g., low-lightenhancement. Recent research indicates that NR-IQA models are susceptible toadversarial attacks, which can significantly alter predicted scores withvisually imperceptible perturbations. Despite revealing vulnerabilities, theseattack methods have limitations, including high computational demands,untargeted manipulation, limited practical utility in white-box scenarios, andreduced effectiveness in black-box scenarios. To address these challenges, weshift our focus to another significant threat and present a novelpoisoning-based backdoor attack against NR-IQA (BAIQA), allowing the attackerto manipulate the IQA model's output to any desired target value by simplyadjusting a scaling coefficient $\alpha$ for the trigger. We propose to injectthe trigger in the discrete cosine transform (DCT) domain to improve the localinvariance of the trigger for countering trigger diminishment in NR-IQA modelsdue to widely adopted data augmentations. Furthermore, the universaladversarial perturbations (UAP) in the DCT space are designed as the trigger,to increase IQA model susceptibility to manipulation and improve attackeffectiveness. In addition to the heuristic method for poison-label BAIQA(P-BAIQA), we explore the design of clean-label BAIQA (C-BAIQA), focusing on$\alpha$ sampling and image data refinement, driven by theoretical insights wereveal. Extensive experiments on diverse datasets and various NR-IQA modelsdemonstrate the effectiveness of our attacks. Code can be found athttps://github.com/yuyi-sd/BAIQA.</description><author>Yi Yu, Song Xia, Xun Lin, Wenhan Yang, Shijian Lu, Yap-peng Tan, Alex Kot</author><pubDate>Fri, 10 Jan 2025 12:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07277v2</guid></item><item><title>Flexible inference in heterogeneous and attributed multilayer networks</title><link>http://arxiv.org/abs/2405.20918v2</link><description>Networked datasets can be enriched by different types of information aboutindividual nodes or edges. However, most existing methods for analyzing suchdatasets struggle to handle the complexity of heterogeneous data, oftenrequiring substantial model-specific analysis. In this paper, we develop aprobabilistic generative model to perform inference in multilayer networks witharbitrary types of information. Our approach employs a Bayesian frameworkcombined with the Laplace matching technique to ease interpretation of inferredparameters. Furthermore, the algorithmic implementation relies on automaticdifferentiation, avoiding the need for explicit derivations. This makes ourmodel scalable and flexible to adapt to any combination of input data. Wedemonstrate the effectiveness of our method in detecting overlapping communitystructures and performing various prediction tasks on heterogeneous multilayerdata, where nodes and edges have different types of attributes. Additionally,we showcase its ability to unveil a variety of patterns in a social supportnetwork among villagers in rural India by effectively utilizing all inputinformation in a meaningful way.</description><author>Martina Contisciani, Marius Hobbhahn, Eleanor A. Power, Philipp Hennig, Caterina De Bacco</author><pubDate>Fri, 10 Jan 2025 12:15:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20918v2</guid></item><item><title>Q-MAML: Quantum Model-Agnostic Meta-Learning for Variational Quantum Algorithms</title><link>http://arxiv.org/abs/2501.05906v1</link><description>In the Noisy Intermediate-Scale Quantum (NISQ) era, using variational quantumalgorithms (VQAs) to solve optimization problems has become a key application.However, these algorithms face significant challenges, such as choosing aneffective initial set of parameters and the limited quantum processing timethat restricts the number of optimization iterations. In this study, weintroduce a new framework for optimizing parameterized quantum circuits (PQCs)that employs a classical optimizer, inspired by Model-Agnostic Meta-Learning(MAML) technique. This approach aim to achieve better parameter initializationthat ensures fast convergence. Our framework features a classical neuralnetwork, called Learner}, which interacts with a PQC using the output ofLearner as an initial parameter. During the pre-training phase, Learner istrained with a meta-objective based on the quantum circuit cost function. Inthe adaptation phase, the framework requires only a few PQC updates to convergeto a more accurate value, while the learner remains unchanged. This method ishighly adaptable and is effectively extended to various Hamiltonianoptimization problems. We validate our approach through experiments, includingdistribution function mapping and optimization of the Heisenberg XYZHamiltonian. The result implies that the Learner successfully estimates initialparameters that generalize across the problem space, enabling fast adaptation.</description><author>Junyong Lee, JeiHee Cho, Shiho Kim</author><pubDate>Fri, 10 Jan 2025 12:07:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05906v1</guid></item><item><title>PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity Surface Reconstruction</title><link>http://arxiv.org/abs/2406.06521v2</link><description>Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention dueto its high-quality rendering, and ultra-fast training and rendering speed.However, due to the unstructured and irregular nature of Gaussian point clouds,it is difficult to guarantee geometric reconstruction accuracy and multi-viewconsistency simply by relying on image reconstruction loss. Although manystudies on surface reconstruction based on 3DGS have emerged recently, thequality of their meshes is generally unsatisfactory. To address this problem,we propose a fast planar-based Gaussian splatting reconstruction representation(PGSR) to achieve high-fidelity surface reconstruction while ensuringhigh-quality rendering. Specifically, we first introduce an unbiased depthrendering method, which directly renders the distance from the camera origin tothe Gaussian plane and the corresponding normal map based on the Gaussiandistribution of the point cloud, and divides the two to obtain the unbiaseddepth. We then introduce single-view geometric, multi-view photometric, andgeometric regularization to preserve global geometric accuracy. We also proposea camera exposure compensation model to cope with scenes with largeillumination variations. Experiments on indoor and outdoor scenes show that ourmethod achieves fast training and rendering while maintaining high-fidelityrendering and geometric reconstruction, outperforming 3DGS-based and NeRF-basedmethods.</description><author>Danpeng Chen, Hai Li, Weicai Ye, Yifan Wang, Weijian Xie, Shangjin Zhai, Nan Wang, Haomin Liu, Hujun Bao, Guofeng Zhang</author><pubDate>Fri, 10 Jan 2025 12:05:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06521v2</guid></item><item><title>VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling</title><link>http://arxiv.org/abs/2501.00574v2</link><description>Long-context modeling is a critical capability for multimodal large languagemodels (MLLMs), enabling them to process long-form contents with implicitmemorization. Despite its advances, handling extremely long videos remainschallenging due to the difficulty in maintaining crucial features over extendedsequences. This paper introduces a Hierarchical visual token Compression (HiCo)method designed for high-fidelity representation and a practical contextmodeling system VideoChat-Flash tailored for multimodal long-sequenceprocessing. HiCo capitalizes on the redundancy of visual information in longvideos to compress long video context from the clip-level to the video-level,reducing the compute significantly while preserving essential details.VideoChat-Flash features a multi-stage short-to-long learning scheme, a richdataset of real-world long videos named LongVid, and an upgraded"Needle-In-A-video-Haystack" (NIAH) for evaluating context capacities. Inextensive experiments, VideoChat-Flash shows the leading performance on bothmainstream long and short video benchmarks at the 2B and 7B model scale. Itfirstly gets 99.1% accuracy over 10,000 frames in NIAH among open-sourcemodels.</description><author>Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, Limin Wang</author><pubDate>Fri, 10 Jan 2025 12:00:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.00574v2</guid></item><item><title>Binary Event-Driven Spiking Transformer</title><link>http://arxiv.org/abs/2501.05904v1</link><description>Transformer-based Spiking Neural Networks (SNNs) introduce a novelevent-driven self-attention paradigm that combines the high performance ofTransformers with the energy efficiency of SNNs. However, the larger model sizeand increased computational demands of the Transformer structure limit theirpracticality in resource-constrained scenarios. In this paper, we integratebinarization techniques into Transformer-based SNNs and propose the BinaryEvent-Driven Spiking Transformer, i.e. BESTformer. The proposed BESTformer cansignificantly reduce storage and computational demands by representing weightsand attention maps with a mere 1-bit. However, BESTformer suffers from a severeperformance drop from its full-precision counterpart due to the limitedrepresentation capability of binarization. To address this issue, we propose aCoupled Information Enhancement (CIE) method, which consists of a reversibleframework and information enhancement distillation. By maximizing the mutualinformation between the binary model and its full-precision counterpart, theCIE method effectively mitigates the performance degradation of the BESTformer.Extensive experiments on static and neuromorphic datasets demonstrate that ourmethod achieves superior performance to other binary SNNs, showcasing itspotential as a compact yet high-performance model for resource-limited edgedevices.</description><author>Honglin Cao, Zijian Zhou, Wenjie Wei, Ammar Belatreche, Yu Liang, Dehao Zhang, Malu Zhang, Yang Yang, Haizhou Li</author><pubDate>Fri, 10 Jan 2025 12:00:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05904v1</guid></item><item><title>Discovery of sustainable energy materials via the machine-learned material space</title><link>http://arxiv.org/abs/2501.05903v1</link><description>Does a machine learning model actually gain an understanding of the materialspace? We answer this question in the affirmative on the example of theOptiMate model, a graph attention network trained to predict the opticalproperties of semiconductors and insulators. By applying the UMAPdimensionality reduction technique to its latent embeddings, we demonstratethat the model captures a nuanced and interpretable representation of thematerials space, reflecting chemical and physical principles, without anyuser-induced bias. This enables clustering of almost 10,000 materials based onoptical properties and chemical similarities. Beyond this understanding, wedemonstrate how the learned material space can be used to identify moresustainable alternatives to critical materials in energy-related technologies,such as photovoltaics. These findings demonstrate the dual utility of machinelearning models in materials science: Accurately predicting material propertieswhile providing insights into the underlying materials space. The approachdemonstrates the broader potential of leveraging learned materials spaces forthe discovery and design of materials for diverse applications, and is easilyapplicable to any state-of-the-art machine learning model.</description><author>Malte Grunert, Max Großmann, Erich Runge</author><pubDate>Fri, 10 Jan 2025 12:00:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05903v1</guid></item><item><title>Valley2: Exploring Multimodal Models with Scalable Vision-Language Design</title><link>http://arxiv.org/abs/2501.05901v1</link><description>Recently, vision-language models have made remarkable progress, demonstratingoutstanding capabilities in various tasks such as image captioning and videounderstanding. We introduce Valley2, a novel multimodal large language modeldesigned to enhance performance across all domains and extend the boundaries ofpractical applications in e-commerce and short video scenarios. Notably,Valley2 achieves state-of-the-art (SOTA) performance on e-commerce benchmarks,surpassing open-source models of similar size by a large margin (79.66 vs.72.76). Additionally, Valley2 ranks second on the OpenCompass leaderboard amongmodels with fewer than 10B parameters, with an impressive average score of67.4. The code and model weights are open-sourced athttps://github.com/bytedance/Valley.</description><author>Ziheng Wu, Zhenghao Chen, Ruipu Luo, Can Zhang, Yuan Gao, Zhentao He, Xian Wang, Haoran Lin, Minghui Qiu</author><pubDate>Fri, 10 Jan 2025 11:53:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05901v1</guid></item><item><title>OmniCount: Multi-label Object Counting with Semantic-Geometric Priors</title><link>http://arxiv.org/abs/2403.05435v6</link><description>Object counting is pivotal for understanding the composition of scenes.Previously, this task was dominated by class-specific methods, which havegradually evolved into more adaptable class-agnostic strategies. However, thesestrategies come with their own set of limitations, such as the need for manualexemplar input and multiple passes for multiple categories, resulting insignificant inefficiencies. This paper introduces a more practical approachenabling simultaneous counting of multiple object categories using anopen-vocabulary framework. Our solution, OmniCount, stands out by usingsemantic and geometric insights (priors) from pre-trained models to countmultiple categories of objects as specified by users, all without additionaltraining. OmniCount distinguishes itself by generating precise object masks andleveraging varied interactive prompts via the Segment Anything Model forefficient counting. To evaluate OmniCount, we created the OmniCount-191benchmark, a first-of-its-kind dataset with multi-label object counts,including points, bounding boxes, and VQA annotations. Our comprehensiveevaluation in OmniCount-191, alongside other leading benchmarks, demonstratesOmniCount's exceptional performance, significantly outpacing existingsolutions. The project webpage is available athttps://mondalanindya.github.io/OmniCount.</description><author>Anindya Mondal, Sauradip Nag, Xiatian Zhu, Anjan Dutta</author><pubDate>Fri, 10 Jan 2025 11:50:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05435v6</guid></item><item><title>Learning In-Distribution Representations for Anomaly Detection</title><link>http://arxiv.org/abs/2501.05130v2</link><description>Anomaly detection involves identifying data patterns that deviate from theanticipated norm. Traditional methods struggle in high-dimensional spaces dueto the curse of dimensionality. In recent years, self-supervised learning,particularly through contrastive objectives, has driven advances in anomalydetection. However, vanilla contrastive learning struggles to align with theunique demands of anomaly detection, as it lacks a pretext task tailored to thehomogeneous nature of In-Distribution (ID) data and the diversity ofOut-of-Distribution (OOD) anomalies. Methods that attempt to address thesechallenges, such as introducing hard negatives through synthetic outliers,Outlier Exposure (OE), and supervised objectives, often rely on pretext tasksthat fail to balance compact clustering of ID samples with sufficientseparation from OOD data. In this work, we propose Focused In-distributionRepresentation Modeling (FIRM), a contrastive learning objective specificallydesigned for anomaly detection. Unlike existing approaches, FIRM incorporatessynthetic outliers into its pretext task in a way that actively shapes therepresentation space, promoting compact clustering of ID samples whileenforcing strong separation from outliers. This formulation addresses thechallenges of class collision, enhancing both the compactness of IDrepresentations and the discriminative power of the learned feature space. Weshow that FIRM surpasses other contrastive methods in standard benchmarks,significantly enhancing anomaly detection compared to both traditional andsupervised contrastive learning objectives. Our ablation studies confirm thatFIRM consistently improves the quality of representations and shows robustnessacross a range of scoring methods. The code is available at:https://github.com/willtl/firm.</description><author>Willian T. Lunardi, Abdulrahman Banabila, Dania Herzalla, Martin Andreoni</author><pubDate>Fri, 10 Jan 2025 11:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05130v2</guid></item><item><title>Text2Playlist: Generating Personalized Playlists from Text on Deezer</title><link>http://arxiv.org/abs/2501.05894v1</link><description>The streaming service Deezer heavily relies on the search to help usersnavigate through its extensive music catalog. Nonetheless, it is primarilydesigned to find specific items and does not lead directly to a smoothlistening experience. We present Text2Playlist, a stand-alone tool thataddresses these limitations. Text2Playlist leverages generative AI, musicinformation retrieval and recommendation systems to generate query-specific andpersonalized playlists, successfully deployed at scale.</description><author>Mathieu Delcluze, Antoine Khoury, Clémence Vast, Valerio Arnaudo, Léa Briand, Walid Bendada, Thomas Bouabça</author><pubDate>Fri, 10 Jan 2025 11:46:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05894v1</guid></item><item><title>Beyond Flat Text: Dual Self-inherited Guidance for Visual Text Generation</title><link>http://arxiv.org/abs/2501.05892v1</link><description>In real-world images, slanted or curved texts, especially those on cans,banners, or badges, appear as frequently, if not more so, than flat texts dueto artistic design or layout constraints. While high-quality visual textgeneration has become available with the advanced generative capabilities ofdiffusion models, these models often produce distorted text and inharmonioustext background when given slanted or curved text layouts due to training datalimitation. In this paper, we introduce a new training-free framework, STGen,which accurately generates visual texts in challenging scenarios (\eg, slantedor curved text layouts) while harmonizing them with the text background. Ourframework decomposes the visual text generation process into two branches: (i)\textbf{Semantic Rectification Branch}, which leverages the ability ingenerating flat but accurate visual texts of the model to guide the generationof challenging scenarios. The generated latent of flat text is abundant inaccurate semantic information related both to the text itself and itsbackground. By incorporating this, we rectify the semantic information of thetexts and harmonize the integration of the text with its background in complexlayouts. (ii) \textbf{Structure Injection Branch}, which reinforces the visualtext structure during inference. We incorporate the latent information of theglyph image, rich in glyph structure, as a new condition to further strengthenthe text structure. To enhance image harmony, we also apply an effectivecombination method to merge the priors, providing a solid foundation forgeneration. Extensive experiments across a variety of visual text layoutsdemonstrate that our framework achieves superior accuracy and outstandingquality.</description><author>Minxing Luo, Zixun Xia, Liaojun Chen, Zhenhang Li, Weichao Zeng, Jianye Wang, Wentao Cheng, Yaxing Wang, Yu Zhou, Jian Yang</author><pubDate>Fri, 10 Jan 2025 11:44:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05892v1</guid></item><item><title>Affordably Fine-tuned LLMs Provide Better Answers to Course-specific MCQs</title><link>http://arxiv.org/abs/2501.05891v1</link><description>In education, the capability of generating human-like text of Large LanguageModels (LLMs) inspired work on how they can increase the efficiency of learningand teaching. We study the affordability of these models for educators andstudents by investigating how LLMs answer multiple-choice questions (MCQs) withrespect to hardware constraints and refinement techniques. We explore thisspace by using generic pre-trained LLMs (the 7B, 13B, and 70B variants ofLLaMA-2) to answer 162 undergraduate-level MCQs from a course on ProgrammingLanguages (PL) -- the MCQ dataset is a contribution of this work, which we makepublicly available. Specifically, we dissect how different factors, such asusing readily-available material -- (parts of) the course's textbook -- forfine-tuning and quantisation (to decrease resource usage) can change theaccuracy of the responses. The main takeaway is that smaller textbook-basedfine-tuned models outperform generic larger ones (whose pre-training requiresconspicuous resources), making the usage of LLMs for answering MCQs resource-and material-wise affordable.</description><author>Bianca Raimondi, Saverio Giallorenzo, Maurizio Gabbrielli</author><pubDate>Fri, 10 Jan 2025 11:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05891v1</guid></item><item><title>EDNet: Edge-Optimized Small Target Detection in UAV Imagery -- Faster Context Attention, Better Feature Fusion, and Hardware Acceleration</title><link>http://arxiv.org/abs/2501.05885v1</link><description>Detecting small targets in drone imagery is challenging due to lowresolution, complex backgrounds, and dynamic scenes. We propose EDNet, a noveledge-target detection framework built on an enhanced YOLOv10 architecture,optimized for real-time applications without post-processing. EDNetincorporates an XSmall detection head and a Cross Concat strategy to improvefeature fusion and multi-scale context awareness for detecting tiny targets indiverse environments. Our unique C2f-FCA block employs Faster Context Attentionto enhance feature extraction while reducing computational complexity. The WIoUloss function is employed for improved bounding box regression. With sevenmodel sizes ranging from Tiny to XL, EDNet accommodates various deploymentenvironments, enabling local real-time inference and ensuring data privacy.Notably, EDNet achieves up to a 5.6% gain in mAP@50 with significantly fewerparameters. On an iPhone 12, EDNet variants operate at speeds ranging from 16to 55 FPS, providing a scalable and efficient solution for edge-based objectdetection in challenging drone imagery. The source code and pre-trained modelsare available at: https://github.com/zsniko/EDNet.</description><author>Zhifan Song, Yuan Zhang, Abd Al Rahman M. Abu Ebayyeh</author><pubDate>Fri, 10 Jan 2025 11:37:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05885v1</guid></item><item><title>Gender Bias in Text-to-Video Generation Models: A case study of Sora</title><link>http://arxiv.org/abs/2501.01987v2</link><description>The advent of text-to-video generation models has revolutionized contentcreation as it produces high-quality videos from textual prompts. However,concerns regarding inherent biases in such models have prompted scrutiny,particularly regarding gender representation. Our study investigates thepresence of gender bias in OpenAI's Sora, a state-of-the-art text-to-videogeneration model. We uncover significant evidence of bias by analyzing thegenerated videos from a diverse set of gender-neutral and stereotypicalprompts. The results indicate that Sora disproportionately associates specificgenders with stereotypical behaviors and professions, which reflects societalprejudices embedded in its training data.</description><author>Mohammad Nadeem, Shahab Saquib Sohail, Erik Cambria, Björn W. Schuller, Amir Hussain</author><pubDate>Fri, 10 Jan 2025 11:36:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01987v2</guid></item><item><title>Text-to-Edit: Controllable End-to-End Video Ad Creation via Multimodal LLMs</title><link>http://arxiv.org/abs/2501.05884v1</link><description>The exponential growth of short-video content has ignited a surge in thenecessity for efficient, automated solutions to video editing, with challengesarising from the need to understand videos and tailor the editing according touser requirements. Addressing this need, we propose an innovative end-to-endfoundational framework, ultimately actualizing precise control over the finalvideo content editing. Leveraging the flexibility and generalizability ofMultimodal Large Language Models (MLLMs), we defined clear input-outputmappings for efficient video creation. To bolster the model's capability inprocessing and comprehending video content, we introduce a strategiccombination of a denser frame rate and a slow-fast processing technique,significantly enhancing the extraction and understanding of both temporal andspatial video information. Furthermore, we introduce a text-to-edit mechanismthat allows users to achieve desired video outcomes through textual input,thereby enhancing the quality and controllability of the edited videos. Throughcomprehensive experimentation, our method has not only showcased significanteffectiveness within advertising datasets, but also yields universallyapplicable conclusions on public datasets.</description><author>Dabing Cheng, Haosen Zhan, Xingchen Zhao, Guisheng Liu, Zemin Li, Jinghui Xie, Zhao Song, Weiguo Feng, Bingyue Peng</author><pubDate>Fri, 10 Jan 2025 11:35:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05884v1</guid></item><item><title>Solving nonograms using Neural Networks</title><link>http://arxiv.org/abs/2501.05882v1</link><description>Nonograms are logic puzzles in which cells in a grid must be colored or leftblank according to the numbers that are located in its headers. In this study,we analyze different techniques to solve this type of logical problem using anHeuristic Algorithm, Genetic Algorithm, and Heuristic Algorithm with NeuralNetwork. Furthermore, we generate a public dataset to train the neuralnetworks. We published this dataset and the code of the algorithms. Combinationof the heuristic algorithm with a neural network obtained the best results.From state of the art review, no previous works used neural network to solvenonograms, nor combined a network with other algorithms to accelerate theresolution process.</description><author>José María Buades Rubio, Antoni Jaume-i-Capó, David López González, Gabriel Moyà Alcover</author><pubDate>Fri, 10 Jan 2025 11:34:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05882v1</guid></item><item><title>TakuNet: an Energy-Efficient CNN for Real-Time Inference on Embedded UAV systems in Emergency Response Scenarios</title><link>http://arxiv.org/abs/2501.05880v1</link><description>Designing efficient neural networks for embedded devices is a criticalchallenge, particularly in applications requiring real-time performance, suchas aerial imaging with drones and UAVs for emergency responses. In this work,we introduce TakuNet, a novel light-weight architecture which employstechniques such as depth-wise convolutions and an early downsampling stem toreduce computational complexity while maintaining high accuracy. It leveragesdense connections for fast convergence during training and uses 16-bitfloating-point precision for optimization on embedded hardware accelerators.Experimental evaluation on two public datasets shows that TakuNet achievesnear-state-of-the-art accuracy in classifying aerial images of emergencysituations, despite its minimal parameter count. Real-world tests on embeddeddevices, namely Jetson Orin Nano and Raspberry Pi, confirm TakuNet'sefficiency, achieving more than 650 fps on the 15W Jetson board, making itsuitable for real-time AI processing on resource-constrained platforms andadvancing the applicability of drones in emergency scenarios. The code andimplementation details are publicly released.</description><author>Daniel Rossi, Guido Borghi, Roberto Vezzani</author><pubDate>Fri, 10 Jan 2025 11:32:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05880v1</guid></item><item><title>Paraphrase Types Elicit Prompt Engineering Capabilities</title><link>http://arxiv.org/abs/2406.19898v4</link><description>Much of the success of modern language models depends on finding a suitableprompt to instruct the model. Until now, it has been largely unknown howvariations in the linguistic expression of prompts affect these models. Thisstudy systematically and empirically evaluates which linguistic featuresinfluence models through paraphrase types, i.e., different linguistic changesat particular positions. We measure behavioral changes for five models across120 tasks and six families of paraphrases (i.e., morphology, syntax, lexicon,lexico-syntax, discourse, and others). We also control for other promptengineering factors (e.g., prompt length, lexical diversity, and proximity totraining data). Our results show a potential for language models to improvetasks when their prompts are adapted in specific paraphrase types (e.g., 6.7%median gain in Mixtral 8x7B; 5.5% in LLaMA 3 8B). In particular, changes inmorphology and lexicon, i.e., the vocabulary used, showed promise in improvingprompts. These findings contribute to developing more robust language modelscapable of handling variability in linguistic expression.</description><author>Jan Philip Wahle, Terry Ruas, Yang Xu, Bela Gipp</author><pubDate>Fri, 10 Jan 2025 11:17:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19898v4</guid></item><item><title>VideoRAG: Retrieval-Augmented Generation over Video Corpus</title><link>http://arxiv.org/abs/2501.05874v1</link><description>Retrieval-Augmented Generation (RAG) is a powerful strategy to address theissue of generating factually incorrect outputs in foundation models byretrieving external knowledge relevant to queries and incorporating it intotheir generation process. However, existing RAG approaches have primarilyfocused on textual information, with some recent advancements beginning toconsider images, and they largely overlook videos, a rich source of multimodalknowledge capable of representing events, processes, and contextual detailsmore effectively than any other modality. While a few recent studies explorethe integration of videos in the response generation process, they eitherpredefine query-associated videos without retrieving them according to queries,or convert videos into the textual descriptions without harnessing theirmultimodal richness. To tackle these, we introduce VideoRAG, a novel frameworkthat not only dynamically retrieves relevant videos based on their relevancewith queries but also utilizes both visual and textual information of videos inthe output generation. Further, to operationalize this, our method revolvesaround the recent advance of Large Video Language Models (LVLMs), which enablethe direct processing of video content to represent it for retrieval andseamless integration of the retrieved videos jointly with queries. Weexperimentally validate the effectiveness of VideoRAG, showcasing that it issuperior to relevant baselines.</description><author>Soyeong Jeong, Kangsan Kim, Jinheon Baek, Sung Ju Hwang</author><pubDate>Fri, 10 Jan 2025 11:17:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05874v1</guid></item><item><title>Collaborative Content Moderation in the Fediverse</title><link>http://arxiv.org/abs/2501.05871v1</link><description>The Fediverse, a group of interconnected servers providing a variety ofinteroperable services (e.g. micro-blogging in Mastodon) has gained rapidpopularity. This sudden growth, partly driven by Elon Musk's acquisition ofTwitter, has created challenges for administrators though. This paper focuseson one particular challenge: content moderation, e.g. the need to remove spamor hate speech. While centralized platforms like Facebook and Twitter rely onautomated tools for moderation, their dependence on massive labeled datasetsand specialized infrastructure renders them impractical for decentralized,low-resource settings like the Fediverse. In this work, we design and evaluateFedMod, a collaborative content moderation system based on federated learning.Our system enables servers to exchange parameters of partially trained localcontent moderation models with similar servers, creating a federated modelshared among collaborating servers. FedMod demonstrates robust performance onthree different content moderation tasks: harmful content detection, botcontent detection, and content warning assignment, achieving average per-servermacro-F1 scores of 0.71, 0.73, and 0.58, respectively.</description><author>Haris Bin Zia, Aravindh Raman, Ignacio Castro, Gareth Tyson</author><pubDate>Fri, 10 Jan 2025 11:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05871v1</guid></item><item><title>A Neighbor-based Approach to Pitch Ownership Models in Soccer</title><link>http://arxiv.org/abs/2501.05870v1</link><description>Pitch ownership models allow many types of analysis in soccer and providevaluable assistance to tactical analysts in understanding the game's dynamics.The novelty they provide over event-based analysis is that tracking dataincorporates context that event-based data does not possess, like playerpositioning. This paper proposes a novel approach to building pitch ownershipmodels in soccer games using the K-Nearest Neighbors (KNN) algorithm. Ourapproach provides a fast inference mechanism that can model differentapproaches to pitch control using the same algorithm. Despite its flexibility,it uses only three hyperparameters to tune the model, facilitating the tuningprocess for different player skill levels. The flexibility of the approachallows for the emulation of different methods available in the literature byadjusting a small number of parameters, including adjusting for differentlevels of uncertainty. In summary, the proposed model provides a new and moreflexible strategy for building pitch ownership models, extending beyond justreplicating existing algorithms, and can provide valuable insights for tacticalanalysts and open up new avenues for future research. We thoroughly visualizeseveral examples demonstrating the presented models' strengths and weaknesses.The code is available at github.com/nvsclub/KNNPitchControl.</description><author>Tiago Mendes-Neves, Luís Meireles, João Mendes-Moreira</author><pubDate>Fri, 10 Jan 2025 11:11:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05870v1</guid></item><item><title>Neural Network Verification is a Programming Language Challenge</title><link>http://arxiv.org/abs/2501.05867v1</link><description>Neural network verification is a new and rapidly developing field ofresearch. So far, the main priority has been establishing efficientverification algorithms and tools, while proper support from the programminglanguage perspective has been considered secondary or unimportant. Yet, thereis mounting evidence that insights from the programming language community maymake a difference in the future development of this domain. In this paper, weformulate neural network verification challenges as programming languagechallenges and suggest possible future solutions.</description><author>Lucas C. Cordeiro, Matthew L. Daggitt, Julien Girard-Satabin, Omri Isac, Taylor T. Johnson, Guy Katz, Ekaterina Komendantskaya, Augustin Lemesle, Edoardo Manino, Artjoms Šinkarovs, Haoze Wu</author><pubDate>Fri, 10 Jan 2025 11:08:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05867v1</guid></item><item><title>Language-Inspired Relation Transfer for Few-shot Class-Incremental Learning</title><link>http://arxiv.org/abs/2501.05862v1</link><description>Depicting novel classes with language descriptions by observing few-shotsamples is inherent in human-learning systems. This lifelong learningcapability helps to distinguish new knowledge from old ones through theincrease of open-world learning, namely Few-Shot Class-Incremental Learning(FSCIL). Existing works to solve this problem mainly rely on the careful tuningof visual encoders, which shows an evident trade-off between the base knowledgeand incremental ones. Motivated by human learning systems, we propose a newLanguage-inspired Relation Transfer (LRT) paradigm to understand objects byjoint visual clues and text depictions, composed of two major steps. We firsttransfer the pretrained text knowledge to the visual domains by proposing agraph relation transformation module and then fuse the visual and languageembedding by a text-vision prototypical fusion module. Second, to mitigate thedomain gap caused by visual finetuning, we propose context prompt learning forfast domain alignment and imagined contrastive learning to alleviate theinsufficient text data during alignment. With collaborative learning of domainalignments and text-image transfer, our proposed LRT outperforms thestate-of-the-art models by over $13\%$ and $7\%$ on the final session ofmini-ImageNet and CIFAR-100 FSCIL benchmarks.</description><author>Yifan Zhao, Jia Li, Zeyin Song, Yonghong Tian</author><pubDate>Fri, 10 Jan 2025 10:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05862v1</guid></item><item><title>ConSim: Measuring Concept-Based Explanations' Effectiveness with Automated Simulatability</title><link>http://arxiv.org/abs/2501.05855v1</link><description>Concept-based explanations work by mapping complex model computations tohuman-understandable concepts. Evaluating such explanations is very difficult,as it includes not only the quality of the induced space of possible conceptsbut also how effectively the chosen concepts are communicated to users.Existing evaluation metrics often focus solely on the former, neglecting thelatter. We introduce an evaluation framework for measuring concept explanationsvia automated simulatability: a simulator's ability to predict the explainedmodel's outputs based on the provided explanations. This approach accounts forboth the concept space and its interpretation in an end-to-end evaluation.Human studies for simulatability are notoriously difficult to enact,particularly at the scale of a wide, comprehensive empirical evaluation (whichis the subject of this work). We propose using large language models (LLMs) assimulators to approximate the evaluation and report various analyses to makesuch approximations reliable. Our method allows for scalable and consistentevaluation across various models and datasets. We report a comprehensiveempirical evaluation using this framework and show that LLMs provide consistentrankings of explanation methods. Code available athttps://github.com/AnonymousConSim/ConSim</description><author>Antonin Poché, Alon Jacovi, Agustin Martin Picard, Victor Boutin, Fanny Jourdan</author><pubDate>Fri, 10 Jan 2025 10:53:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05855v1</guid></item><item><title>A Portable Solution for Simultaneous Human Movement and Mobile EEG Acquisition: Readiness Potentials for Basketball Free-throw Shooting</title><link>http://arxiv.org/abs/2501.05378v2</link><description>Advances in wireless electroencephalography (EEG) technology promise torecord brain-electrical activity in everyday situations. To better understandthe relationship between brain activity and natural behavior, it is necessaryto monitor human movement patterns. Here, we present a pocketable setupconsisting of two smartphones to simultaneously capture human posture and EEGsignals. We asked 26 basketball players to shoot 120 free throws each. First,we investigated whether our setup allows us to capture the readiness potential(RP) that precedes voluntary actions. Second, we investigated whether the RPdiffers between successful and unsuccessful free-throw attempts. The resultsconfirmed the presence of the RP, but the amplitude of the RP was not relatedto shooting success. However, offline analysis of real-time human pose signalsderived from a smartphone camera revealed pose differences between successfuland unsuccessful shots for some individuals. We conclude that a highlyportable, low-cost and lightweight acquisition setup, consisting of twosmartphones and a head-mounted wireless EEG amplifier, is sufficient to monitorcomplex human movement patterns and associated brain dynamics outside thelaboratory.</description><author>Miguel Contreras-Altamirano, Melanie Klapprott, Nadine Jacobsen, Paul Maanen, Julius Welzel, Stefan Debener</author><pubDate>Fri, 10 Jan 2025 10:51:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05378v2</guid></item><item><title>MRI Patterns of the Hippocampus and Amygdala for Predicting Stages of Alzheimer's Progression: A Minimal Feature Machine Learning Framework</title><link>http://arxiv.org/abs/2501.05852v1</link><description>Alzheimer's disease (AD) progresses through distinct stages, from early mildcognitive impairment (EMCI) to late mild cognitive impairment (LMCI) andeventually to AD. Accurate identification of these stages, especiallydistinguishing LMCI from EMCI, is crucial for developing pre-dementiatreatments but remains challenging due to subtle and overlapping imagingfeatures. This study proposes a minimal-feature machine learning framework thatleverages structural MRI data, focusing on the hippocampus and amygdala asregions of interest. The framework addresses the curse of dimensionalitythrough feature selection, utilizes region-specific voxel information, andimplements innovative data organization to enhance classification performanceby reducing noise. The methodology integrates dimensionality reductiontechniques such as PCA and t-SNE with state-of-the-art classifiers, achievingthe highest accuracy of 88.46%. This framework demonstrates the potential forefficient and accurate staging of AD progression while providing valuableinsights for clinical applications.</description><author>Aswini Kumar Patra, Soraisham Elizabeth Devi, Tejashwini Gajurel</author><pubDate>Fri, 10 Jan 2025 10:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05852v1</guid></item><item><title>MC-VTON: Minimal Control Virtual Try-On Diffusion Transformer</title><link>http://arxiv.org/abs/2501.03630v2</link><description>Virtual try-on methods based on diffusion models achieve realistic try-oneffects. They use an extra reference network or an additional image encoder toprocess multiple conditional image inputs, which adds complexity pre-processingand additional computational costs. Besides, they require more than 25inference steps, bringing longer inference time. In this work, with thedevelopment of diffusion transformer (DiT), we rethink the necessity ofadditional reference network or image encoder and introduce MC-VTON, whichleverages DiT's intrinsic backbone to seamlessly integrate minimal conditionaltry-on inputs. Compared to existing methods, the superiority of MC-VTON isdemonstrated in four aspects: (1) Superior detail fidelity. Our DiT-basedMC-VTON exhibits superior fidelity in preserving fine-grained details. (2)Simplified network and inputs. We remove any extra reference network or imageencoder. We also remove unnecessary conditions like the long prompt, poseestimation, human parsing, and depth map. We require only the masked personimage and the garment image. (3) Parameter-efficient training. To process thetry-on task, we fine-tune the FLUX.1-dev with only 39.7M additional parameters(0.33% of the backbone parameters). (4) Less inference steps. We applydistillation diffusion on MC-VTON and only need 8 steps to generate a realistictry-on image, with only 86.8M additional parameters (0.72% of the backboneparameters). Experiments show that MC-VTON achieves superior qualitative andquantitative results with fewer condition inputs, trainable parameters, andinference steps than baseline methods.</description><author>Junsheng Luan, Guangyuan Li, Lei Zhao, Wei Xing</author><pubDate>Fri, 10 Jan 2025 10:45:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.03630v2</guid></item><item><title>Identity-aware Feature Decoupling Learning for Clothing-change Person Re-identification</title><link>http://arxiv.org/abs/2501.05851v1</link><description>Clothing-change person re-identification (CC Re-ID) has attracted increasingattention in recent years due to its application prospect. Most existing worksstruggle to adequately extract the ID-related information from the original RGBimages. In this paper, we propose an Identity-aware Feature Decoupling (IFD)learning framework to mine identity-related features. Particularly, IFDexploits a dual stream architecture that consists of a main stream and anattention stream. The attention stream takes the clothing-masked images asinputs and derives the identity attention weights for effectively transferringthe spatial knowledge to the main stream and highlighting the regions withabundant identity-related information. To eliminate the semantic gap betweenthe inputs of two streams, we propose a clothing bias diminishing modulespecific to the main stream to regularize the features of clothing-relevantregions. Extensive experimental results demonstrate that our frameworkoutperforms other baseline models on several widely-used CC Re-ID datasets.</description><author>Haoxuan Xu, Bo Li, Guanglin Niu</author><pubDate>Fri, 10 Jan 2025 10:45:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05851v1</guid></item><item><title>VLM-driven Behavior Tree for Context-aware Task Planning</title><link>http://arxiv.org/abs/2501.03968v2</link><description>The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)has recently gained attention in the robotics community, yet remains in itsearly stages of development. In this paper, we propose a novel framework thatleverages Vision-Language Models (VLMs) to interactively generate and edit BTsthat address visual conditions, enabling context-aware robot operations invisually complex environments. A key feature of our approach lies in theconditional control through self-prompted visual conditions. Specifically, theVLM generates BTs with visual condition nodes, where conditions are expressedas free-form text. Another VLM process integrates the text into its prompt andevaluates the conditions against real-world images during robot execution. Wevalidated our framework in a real-world cafe scenario, demonstrating both itsfeasibility and limitations.</description><author>Naoki Wake, Atsushi Kanehira, Jun Takamatsu, Kazuhiro Sasabuchi, Katsushi Ikeuchi</author><pubDate>Fri, 10 Jan 2025 10:38:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.03968v2</guid></item><item><title>Long Story Short: Story-level Video Understanding from 20K Short Films</title><link>http://arxiv.org/abs/2406.10221v2</link><description>Recent developments in vision-language models have significantly advancedvideo understanding. Existing datasets and tasks, however, have notablelimitations. Most datasets are confined to short videos with limited events andnarrow narratives. For example, datasets with instructional and egocentricvideos often depict activities of one person in a single scene. Althoughexisting movie datasets offer richer content, they are often limited toshort-term tasks, lack publicly available videos, and frequently encounter dataleakage issues given the use of subtitles and other information aboutcommercial movies during LLM pretraining. To address the above limitations, wepropose Short-Films 20K (SF20K), the largest publicly available movie dataset.SF20K is composed of 20,143 amateur films and offers long-term video tasks inthe form of multiple-choice and open-ended question answering. Our extensiveanalysis of SF20K reveals minimal data leakage, emphasizes the need forlong-term reasoning, and demonstrates the strong performance of recent VLMs.Finally, we show that instruction tuning on the SF20K-Train set substantiallyimproves model performance, paving the way for future progress in long-termvideo understanding.</description><author>Ridouane Ghermi, Xi Wang, Vicky Kalogeiton, Ivan Laptev</author><pubDate>Fri, 10 Jan 2025 10:36:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10221v2</guid></item><item><title>Imprecise Markov Semigroups and their Ergodicity</title><link>http://arxiv.org/abs/2405.00081v3</link><description>We introduce the concept of an imprecise Markov semigroup $\mathbf{Q}$. It isa tool that allows to represent ambiguity around both the initial and thetransition probabilities of a Markov process via a compact collection of Markovsemigroups, each associated with a (possibly different) Markov process. We usetechniques from set theory, topology, geometry, and probability to study theergodic behavior of $\mathbf{Q}$. We show that, if the initial distribution ofthe Markov processes associated with the elements of $\mathbf{Q}$ is known andinvariant, under some conditions that also involve the geometry of the statespace, eventually the ambiguity around their transition probability fades. Wecall this property ergodicity of the imprecise Markov semigroup, and we relateit to the classical notion of ergodicity. We prove ergodicity both when thestate space is Euclidean or a Riemannian manifold, and when it is an arbitrarymeasurable space. The importance of our findings for the fields of machinelearning and computer vision is also discussed.</description><author>Michele Caprio</author><pubDate>Fri, 10 Jan 2025 10:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00081v3</guid></item><item><title>Annealing Machine-assisted Learning of Graph Neural Network for Combinatorial Optimization</title><link>http://arxiv.org/abs/2501.05845v1</link><description>While Annealing Machines (AM) have shown increasing capabilities in solvingcomplex combinatorial problems, positioning themselves as a more immediatealternative to the expected advances of future fully quantum solutions, thereare still scaling limitations. In parallel, Graph Neural Networks (GNN) havebeen recently adapted to solve combinatorial problems, showing competitiveresults and potentially high scalability due to their distributed nature. Wepropose a merging approach that aims at retaining both the accuracy exhibitedby AMs and the representational flexibility and scalability of GNNs. Our modelconsiders a compression step, followed by a supervised interaction wherepartial solutions obtained from the AM are used to guide local GNNs from wherenode feature representations are obtained and combined to initialize anadditional GNN-based solver that handles the original graph's target problem.Intuitively, the AM can solve the combinatorial problem indirectly by infusingits knowledge into the GNN. Experiments on canonical optimization problems showthat the idea is feasible, effectively allowing the AM to solve size problemsbeyond its original limits.</description><author>Pablo Loyola, Kento Hasegawa, Andres Hoyos-Idobro, Kazuo Ono, Toyotaro Suzumura, Yu Hirate, Masanao Yamaoka</author><pubDate>Fri, 10 Jan 2025 10:36:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05845v1</guid></item><item><title>"Cause" is Mechanistic Narrative within Scientific Domains: An Ordinary Language Philosophical Critique of "Causal Machine Learning"</title><link>http://arxiv.org/abs/2501.05844v1</link><description>Causal Learning has emerged as a major theme of AI in recent years, promisingto use special techniques to reveal the true nature of cause and effect in anumber of important domains. We consider the Epistemology of learning andrecognizing true cause and effect phenomena. Through thought exercises on thecustomary use of the word ''cause'', especially in scientific domains, weinvestigate what, in practice, constitutes a valid causal claim. We recognizethe word's uses across scientific domains in disparate form but consistentfunction within the scientific paradigm. We highlight fundamental distinctionsof practice that can be performed in the natural and social sciences, highlightthe importance of many systems of interest being open and irreducible andidentify the important notion of Hermeneutic knowledge for social scienceinquiry. We posit that the distinct properties require that definitive causalclaims can only come through an agglomeration of consistent evidence acrossmultiple domains and levels of abstraction, such as empirical, physiological,biochemical, etc. We present Cognitive Science as an exemplarymulti-disciplinary field providing omnipresent opportunity for such a ResearchProgram, and highlight the main general modes of practice of scientific inquirythat can adequately merge, rather than place as incorrigibly conflictual,multi-domain multi-abstraction scientific practices and language games.</description><author>Vyacheslav Kungurtsev, Leonardo Christov Moore, Gustav Sir, Martin Krutsky</author><pubDate>Fri, 10 Jan 2025 10:36:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05844v1</guid></item><item><title>Orthogonal projection-based regularization for efficient model augmentation</title><link>http://arxiv.org/abs/2501.05842v1</link><description>Deep-learning-based nonlinear system identification has shown the ability toproduce reliable and highly accurate models in practice. However, theseblack-box models lack physical interpretability, and often a considerable partof the learning effort is spent on capturing already expected/known behaviordue to first-principles-based understanding of some aspects of the system. Apotential solution is to integrate prior physical knowledge directly into themodel structure, combining the strengths of physics-based modeling anddeep-learning-based identification. The most common approach is to use anadditive model augmentation structure, where the physics-based and themachine-learning (ML) components are connected in parallel. However, suchmodels are overparametrized, training them is challenging, potentially causingthe physics-based part to lose interpretability. To overcome this challenge,this paper proposes an orthogonal projection-based regularization technique toenhance parameter learning, convergence, and even model accuracy inlearning-based augmentation of nonlinear baseline models.</description><author>Bendegúz M. Györök, Jan H. Hoekstra, Johan Kon, Tamás Péni, Maarten Schoukens, Roland Tóth</author><pubDate>Fri, 10 Jan 2025 10:33:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05842v1</guid></item><item><title>Geometry of Linear Neural Networks: Equivariance and Invariance under Permutation Groups</title><link>http://arxiv.org/abs/2309.13736v3</link><description>The set of functions parameterized by a linear fully-connected neural networkis a determinantal variety. We investigate the subvariety of functions that areequivariant or invariant under the action of a permutation group. Examples ofsuch group actions are translations or $90^\circ$ rotations on images. Wedescribe such equivariant or invariant subvarieties as direct products ofdeterminantal varieties, from which we deduce their dimension, degree,Euclidean distance degree, and their singularities. We fully characterizeinvariance for arbitrary permutation groups, and equivariance for cyclicgroups. We draw conclusions for the parameterization and the design ofequivariant and invariant linear networks in terms of sparsity andweight-sharing properties. We prove that all invariant linear functions can beparameterized by a single linear autoencoder with a weight-sharing propertyimposed by the cycle decomposition of the considered permutation. The space ofrank-bounded equivariant functions has several irreducible components, so itcan not be parameterized by a single network-but each irreducible componentcan. Finally, we show that minimizing the squared-error loss on our invariantor equivariant networks reduces to minimizing the Euclidean distance fromdeterminantal varieties via the Eckart-Young theorem.</description><author>Kathlén Kohn, Anna-Laura Sattelberger, Vahid Shahverdi</author><pubDate>Fri, 10 Jan 2025 10:31:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13736v3</guid></item><item><title>Empowering Aggregators with Practical Data-Driven Tools: Harnessing Aggregated and Disaggregated Flexibility for Demand Response</title><link>http://arxiv.org/abs/2401.10726v4</link><description>This study explores the interaction between aggregators and buildingoccupants in activating flexibility through Demand Response (DR) programs, witha focus on reinforcing the resilience of the energy system considering theuncertainties presented by Renewable Energy Sources (RES). Firstly, itintroduces a methodology of optimizing aggregated flexibility provisionstrategies in environments with limited data, utilizing Discrete FourierTransformation (DFT) and clustering techniques to identify building occupants'activity patterns. Secondly, the study assesses the disaggregated flexibilityprovision of Heating Ventilation and Air Conditioning (HVAC) systems during DRevents, employing machine learning and optimization techniques for precise,device-level analysis. The first approach offers a non-intrusive pathway foraggregators to provide flexibility services in environments of a single smartmeter for the whole building's consumption, while the second approach maximizesthe amount of flexibility in the case of dedicated metering devices to the HVACsystems by carefully considering building occupants' thermal comfort profiles.Through the application of data-driven techniques and encompassing case studiesfrom both industrial and residential buildings, this paper not only unveilspivotal opportunities for aggregators in the balancing and emerging flexibilitymarkets but also successfully develops and demonstrates end-to-end practicaltools for aggregators.</description><author>Costas Mylonas, Donata Boric, Leila Luttenberger Maric, Alexandros Tsitsanis, Eleftheria Petrianou, Magda Foti</author><pubDate>Fri, 10 Jan 2025 10:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10726v4</guid></item><item><title>Poetry in Pixels: Prompt Tuning for Poem Image Generation via Diffusion Models</title><link>http://arxiv.org/abs/2501.05839v1</link><description>The task of text-to-image generation has encountered significant challengeswhen applied to literary works, especially poetry. Poems are a distinct form ofliterature, with meanings that frequently transcend beyond the literal words.To address this shortcoming, we propose a PoemToPixel framework designed togenerate images that visually represent the inherent meanings of poems. Ourapproach incorporates the concept of prompt tuning in our image generationframework to ensure that the resulting images closely align with the poeticcontent. In addition, we propose the PoeKey algorithm, which extracts three keyelements in the form of emotions, visual elements, and themes from poems toform instructions which are subsequently provided to a diffusion model forgenerating corresponding images. Furthermore, to expand the diversity of thepoetry dataset across different genres and ages, we introduce MiniPo, a novelmultimodal dataset comprising 1001 children's poems and images. Leveraging thisdataset alongside PoemSum, we conducted both quantitative and qualitativeevaluations of image generation using our PoemToPixel framework. This paperdemonstrates the effectiveness of our approach and offers a fresh perspectiveon generating images from literary sources.</description><author>Sofia Jamil, Bollampalli Areen Reddy, Raghvendra Kumar, Sriparna Saha, K J Joseph, Koustava Goswami</author><pubDate>Fri, 10 Jan 2025 10:26:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05839v1</guid></item><item><title>Benchmarking Uncertainty Quantification Methods for Large Language Models with LM-Polygraph</title><link>http://arxiv.org/abs/2406.15627v3</link><description>The rapid proliferation of large language models (LLMs) has stimulatedresearchers to seek effective and efficient approaches to deal with LLMhallucinations and low-quality outputs. Uncertainty quantification (UQ) is akey element of machine learning applications in dealing with such challenges.However, research to date on UQ for LLMs has been fragmented in terms oftechniques and evaluation methodologies. In this work, we address this issue byintroducing a novel benchmark that implements a collection of state-of-the-artUQ baselines and offers an environment for controllable and consistentevaluation of novel UQ techniques over various text generation tasks. Ourbenchmark also supports the assessment of confidence normalization methods interms of their ability to provide interpretable scores. Using our benchmark, weconduct a large-scale empirical investigation of UQ and normalizationtechniques across eleven tasks, identifying the most effective approaches.Code: https://github.com/IINemo/lm-polygraph Benchmark:https://huggingface.co/LM-Polygraph</description><author>Roman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev, Lyudmila Rvanova, Akim Tsvigun, Daniil Vasilev, Rui Xing, Abdelrahman Boda Sadallah, Kirill Grishchenkov, Sergey Petrakov, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov, Artem Shelmanov</author><pubDate>Fri, 10 Jan 2025 10:24:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.15627v3</guid></item><item><title>dlordinal: a Python package for deep ordinal classification</title><link>http://arxiv.org/abs/2407.17163v3</link><description>dlordinal is a new Python library that unifies many recent deep ordinalclassification methodologies available in the literature. Developed usingPyTorch as underlying framework, it implements the top performingstate-of-the-art deep learning techniques for ordinal classification problems.Ordinal approaches are designed to leverage the ordering information present inthe target variable. Specifically, it includes loss functions, various outputlayers, dropout techniques, soft labelling methodologies, and otherclassification strategies, all of which are appropriately designed toincorporate the ordinal information. Furthermore, as the performance metrics toassess novel proposals in ordinal classification depend on the distance betweentarget and predicted classes in the ordinal scale, suitable ordinal evaluationmetrics are also included. dlordinal is distributed under the BSD-3-Clauselicense and is available at https://github.com/ayrna/dlordinal.</description><author>Francisco Bérchez-Moreno, Víctor M. Vargas, Rafael Ayllón-Gavilán, David Guijo-Rubio, César Hervás-Martínez, Juan C. Fernández, Pedro A. Gutiérrez</author><pubDate>Fri, 10 Jan 2025 10:17:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17163v3</guid></item><item><title>Fine-tuning is Not Fine: Mitigating Backdoor Attacks in GNNs with Limited Clean Data</title><link>http://arxiv.org/abs/2501.05835v1</link><description>Graph Neural Networks (GNNs) have achieved remarkable performance throughtheir message-passing mechanism. However, recent studies have highlighted thevulnerability of GNNs to backdoor attacks, which can lead the model tomisclassify graphs with attached triggers as the target class. Theeffectiveness of recent promising defense techniques, such as fine-tuning ordistillation, is heavily contingent on having comprehensive knowledge of thesufficient training dataset. Empirical studies have shown that fine-tuningmethods require a clean dataset of 20% to reduce attack accuracy to below 25%,while distillation methods require a clean dataset of 15%. However, obtainingsuch a large amount of clean data is commonly impractical. In this paper, we propose a practical backdoor mitigation framework, denotedas GRAPHNAD, which can capture high-quality intermediate-layer representationsin GNNs to enhance the distillation process with limited clean data. To achievethis, we address the following key questions: How to identify the appropriateattention representations in graphs for distillation? How to enhancedistillation with limited data? By adopting the graph attention transfermethod, GRAPHNAD can effectively align the intermediate-layer attentionrepresentations of the backdoored model with that of the teacher model, forcingthe backdoor neurons to transform into benign ones. Besides, we extract therelation maps from intermediate-layer transformation and enforce the relationmaps of the backdoored model to be consistent with that of the teacher model,thereby ensuring model accuracy while further reducing the influence ofbackdoors. Extensive experimental results show that by fine-tuning a teachermodel with only 3% of the clean data, GRAPHNAD can reduce the attack successrate to below 5%.</description><author>Jiale Zhang, Bosen Rao, Chengcheng Zhu, Xiaobing Sun, Qingming Li, Haibo Hu, Xiapu Luo, Qingqing Ye, Shouling Ji</author><pubDate>Fri, 10 Jan 2025 10:16:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05835v1</guid></item><item><title>Fractional Concepts in Neural Networks: Enhancing Activation Functions</title><link>http://arxiv.org/abs/2310.11875v2</link><description>Designing effective neural networks requires tuning architectural elements.This study integrates fractional calculus into neural networks by introducingfractional order derivatives (FDO) as tunable parameters in activationfunctions, allowing diverse activation functions by adjusting the FDO. Weevaluate these fractional activation functions on various datasets and networkarchitectures, comparing their performance with traditional and new activationfunctions. Our experiments assess their impact on accuracy, time complexity,computational overhead, and memory usage. Results suggest fractional activationfunctions, particularly fractional Sigmoid, offer benefits in some scenarios.Challenges related to consistency and efficiency remain. Practical implicationsand limitations are discussed.</description><author>Zahra Alijani, Vojtech Molek</author><pubDate>Fri, 10 Jan 2025 10:15:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11875v2</guid></item><item><title>MoColl: Agent-Based Specific and General Model Collaboration for Image Captioning</title><link>http://arxiv.org/abs/2501.01834v2</link><description>Image captioning is a critical task at the intersection of computer visionand natural language processing, with wide-ranging applications across variousdomains. For complex tasks such as diagnostic report generation, deep learningmodels require not only domain-specific image-caption datasets but also theincorporation of relevant general knowledge to provide contextual accuracy.Existing approaches exhibit inherent limitations: specialized models excel incapturing domain-specific details but lack generalization, whilevision-language models (VLMs) built on large language models (LLMs) leveragegeneral knowledge but struggle with domain-specific adaptation. To addressthese limitations, this paper proposes a novel agent-enhanced modelcollaboration framework, which we call MoColl, designed to effectivelyintegrate domain-specific and general knowledge. Specifically, our approach isto decompose complex image captioning tasks into a series of interconnectedquestion-answer subtasks. A trainable visual question answering (VQA) model isemployed as a specialized tool to focus on domain-specific visual analysis,answering task-specific questions based on image content. Concurrently, anLLM-based agent with general knowledge formulates these questions andsynthesizes the resulting question-answer pairs into coherent captions. Beyondits role in leveraging the VQA model, the agent further guides its training toenhance its domain-specific capabilities. Experimental results on radiologyreport generation validate the effectiveness of the proposed framework,demonstrating significant improvements in the quality of generated reports.</description><author>Pu Yang, Bin Dong</author><pubDate>Fri, 10 Jan 2025 10:08:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01834v2</guid></item><item><title>Towards a Multimodal Large Language Model with Pixel-Level Insight for Biomedicine</title><link>http://arxiv.org/abs/2412.09278v2</link><description>In recent years, Multimodal Large Language Models (MLLM) have achievednotable advancements, demonstrating the feasibility of developing anintelligent biomedical assistant. However, current biomedical MLLMspredominantly focus on image-level understanding and restrict interactions totextual commands, thus limiting their capability boundaries and the flexibilityof usage. In this paper, we introduce a novel end-to-end multimodal largelanguage model for the biomedical domain, named MedPLIB, which possessespixel-level understanding. Excitingly, it supports visual question answering(VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-formshapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE)multi-stage training strategy, which divides MoE into separate training phasesfor a visual-language expert model and a pixel-grounding expert model, followedby fine-tuning using MoE. This strategy effectively coordinates multitasklearning while maintaining the computational cost at inference equivalent tothat of a single expert model. To advance the research of biomedical MLLMs, weintroduce the Medical Complex Vision Question Answering Dataset (MeCoVQA),which comprises an array of 8 modalities for complex medical imaging questionanswering and image region understanding. Experimental results indicate thatMedPLIB has achieved state-of-the-art outcomes across multiple medical visuallanguage tasks. More importantly, in zero-shot evaluations for the pixelgrounding task, MedPLIB leads the best small and large models by margins of19.7 and 15.6 respectively on the mDice metric. The codes, data, and modelcheckpoints will be made publicly available athttps://github.com/ShawnHuang497/MedPLIB.</description><author>Xiaoshuang Huang, Lingdong Shen, Jia Liu, Fangxin Shang, Hongxiang Li, Haifeng Huang, Yehui Yang</author><pubDate>Fri, 10 Jan 2025 10:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09278v2</guid></item><item><title>UltraRay: Full-Path Ray Tracing for Enhancing Realism in Ultrasound Simulation</title><link>http://arxiv.org/abs/2501.05828v1</link><description>Traditional ultrasound simulators solve the wave equation to model pressuredistribution fields, achieving high accuracy but requiring significantcomputational time and resources. To address this, ray tracing approaches havebeen introduced, modeling wave propagation as rays interacting with boundariesand scatterers. However, existing models simplify ray propagation, generatingechoes at interaction points without considering return paths to the sensor.This can result in unrealistic artifacts and necessitates careful scene tuningfor plausible results. We propose a novel ultrasound simulation pipeline thatutilizes a ray tracing algorithm to generate echo data, tracing each ray fromthe transducer through the scene and back to the sensor. To replicate advancedultrasound imaging, we introduce a ray emission scheme optimized for plane waveimaging, incorporating delay and steering capabilities. Furthermore, weintegrate a standard signal processing pipeline to simulate end-to-endultrasound image formation. We showcase the efficacy of the proposed pipelineby modeling synthetic scenes featuring highly reflective objects, such asbones. In doing so, our proposed approach, UltraRay, not only enhances theoverall visual quality but also improves the realism of the simulated images byaccurately capturing secondary reflections and reducing unnatural artifacts. Bybuilding on top of a differentiable framework, the proposed pipeline lays thegroundwork for a fast and differentiable ultrasound simulation tool necessaryfor gradient-based optimization, enabling advanced ultrasound beamformingstrategies, neural network integration, and accurate inverse scenereconstruction.</description><author>Felix Duelmer, Mohammad Farid Azampour, Nassir Navab</author><pubDate>Fri, 10 Jan 2025 10:07:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05828v1</guid></item><item><title>AI-Driven Diabetic Retinopathy Screening: Multicentric Validation of AIDRSS in India</title><link>http://arxiv.org/abs/2501.05826v1</link><description>Purpose: Diabetic retinopathy (DR) is a major cause of vision loss,particularly in India, where access to retina specialists is limited in ruralareas. This study aims to evaluate the Artificial Intelligence-based DiabeticRetinopathy Screening System (AIDRSS) for DR detection and prevalenceassessment, addressing the growing need for scalable, automated screeningsolutions in resource-limited settings. Approach: A multicentric, cross-sectional study was conducted in Kolkata,India, involving 5,029 participants and 10,058 macula-centric retinal fundusimages. The AIDRSS employed a deep learning algorithm with 50 million trainableparameters, integrated with Contrast Limited Adaptive Histogram Equalization(CLAHE) preprocessing for enhanced image quality. DR was graded using theInternational Clinical Diabetic Retinopathy (ICDR) Scale, categorizing diseaseinto five stages (DR0 to DR4). Statistical metrics including sensitivity,specificity, and prevalence rates were evaluated against expert retinaspecialist assessments. Results: The prevalence of DR in the general population was 13.7%, rising to38.2% among individuals with elevated random blood glucose levels. The AIDRSSachieved an overall sensitivity of 92%, specificity of 88%, and 100%sensitivity for detecting referable DR (DR3 and DR4). These results demonstratethe system's robust performance in accurately identifying and grading DR in adiverse population. Conclusions: AIDRSS provides a reliable, scalable solution for early DRdetection in resource-constrained environments. Its integration of advanced AItechniques ensures high diagnostic accuracy, with potential to significantlyreduce the burden of diabetes-related vision loss in underserved regions.</description><author>Amit Kr Dey, Pradeep Walia, Girish Somvanshi, Abrar Ali, Sagarnil Das, Pallabi Paul, Minakhi Ghosh</author><pubDate>Fri, 10 Jan 2025 10:03:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05826v1</guid></item><item><title>PersonaHOI: Effortlessly Improving Personalized Face with Human-Object Interaction Generation</title><link>http://arxiv.org/abs/2501.05823v1</link><description>We introduce PersonaHOI, a training- and tuning-free framework that fuses ageneral StableDiffusion model with a personalized face diffusion (PFD) model togenerate identity-consistent human-object interaction (HOI) images. Whileexisting PFD models have advanced significantly, they often overemphasizefacial features at the expense of full-body coherence, PersonaHOI introduces anadditional StableDiffusion (SD) branch guided by HOI-oriented text inputs. Byincorporating cross-attention constraints in the PFD branch and spatial mergingat both latent and residual levels, PersonaHOI preserves personalized facialdetails while ensuring interactive non-facial regions. Experiments, validatedby a novel interaction alignment metric, demonstrate the superior realism andscalability of PersonaHOI, establishing a new standard for practicalpersonalized face with HOI generation. Our code will be available athttps://github.com/JoyHuYY1412/PersonaHOI</description><author>Xinting Hu, Haoran Wang, Jan Eric Lenssen, Bernt Schiele</author><pubDate>Fri, 10 Jan 2025 10:01:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05823v1</guid></item><item><title>HazeCLIP: Towards Language Guided Real-World Image Dehazing</title><link>http://arxiv.org/abs/2407.13719v2</link><description>Existing methods have achieved remarkable performance in image dehazing,particularly on synthetic datasets. However, they often struggle withreal-world hazy images due to domain shift, limiting their practicalapplicability. This paper introduces HazeCLIP, a language-guided adaptationframework designed to enhance the real-world performance of pre-traineddehazing networks. Inspired by the Contrastive Language-Image Pre-training(CLIP) model's ability to distinguish between hazy and clean images, weleverage it to evaluate dehazing results. Combined with a region-specificdehazing technique and tailored prompt sets, the CLIP model accuratelyidentifies hazy areas, providing a high-quality, human-like prior that guidesthe fine-tuning process of pre-trained networks. Extensive experimentsdemonstrate that HazeCLIP achieves state-of-the-art performance in real-wordimage dehazing, evaluated through both visual quality and image qualityassessment metrics. Codes are available at https://github.com/Troivyn/HazeCLIP.</description><author>Ruiyi Wang, Wenhao Li, Xiaohong Liu, Chunyi Li, Zicheng Zhang, Xiongkuo Min, Guangtao Zhai</author><pubDate>Fri, 10 Jan 2025 10:00:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13719v2</guid></item><item><title>Diffusion Models for Smarter UAVs: Decision-Making and Modeling</title><link>http://arxiv.org/abs/2501.05819v1</link><description>Unmanned Aerial Vehicles (UAVs) are increasingly adopted in moderncommunication networks. However, challenges in decision-making and digitalmodeling continue to impede their rapid advancement. Reinforcement Learning(RL) algorithms face limitations such as low sample efficiency and limited dataversatility, further magnified in UAV communication scenarios. Moreover,Digital Twin (DT) modeling introduces substantial decision-making and datamanagement complexities. RL models, often integrated into DT frameworks,require extensive training data to achieve accurate predictions. In contrast totraditional approaches that focus on class boundaries, Diffusion Models (DMs),a new class of generative AI, learn the underlying probability distributionfrom the training data and can generate trustworthy new patterns based on thislearned distribution. This paper explores the integration of DMs with RL and DTto effectively address these challenges. By combining the data generationcapabilities of DMs with the decision-making framework of RL and the modelingaccuracy of DT, the integration improves the adaptability and real-timeperformance of UAV communication. Moreover, the study shows how DMs canalleviate data scarcity, improve policy networks, and optimize dynamicmodeling, providing a robust solution for complex UAV communication scenarios.</description><author>Yousef Emami, Hao Zhou, Luis Almeida, Kai Li</author><pubDate>Fri, 10 Jan 2025 09:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05819v1</guid></item><item><title>Threshold Neuron: A Brain-inspired Artificial Neuron for Efficient On-device Inference</title><link>http://arxiv.org/abs/2412.13902v2</link><description>Enhancing the computational efficiency of on-device Deep Neural Networks(DNNs) remains a significant challengein mobile and edge computing. As we aimto execute increasingly complex tasks with constrained computational resources,much of the research has focused on compressing neural network structures andoptimizing systems. Although many studies have focused on compressing neuralnetwork structures and parameters or optimizing underlying systems, there hasbeen limited attention on optimizing the fundamental building blocks of neuralnetworks: the neurons. In this study, we deliberate on a simple but importantresearch question: Can we design artificial neurons that offer greaterefficiency than the traditional neuron paradigm? Inspired by the thresholdmechanisms and the excitation-inhibition balance observed in biologicalneurons, we propose a novel artificial neuron model, Threshold Neurons. UsingThreshold Neurons, we can construct neural networks similar to those withtraditional artificial neurons, while significantly reducing hardwareimplementation complexity. Our extensive experiments validate the effectivenessof neural networks utilizing Threshold Neurons, achieving substantial powersavings of 7.51x to 8.19x and area savings of 3.89x to 4.33x at the kernellevel, with minimal loss in precision. Furthermore, FPGA-based implementationsof these networks demonstrate 2.52x power savings and 1.75x speed enhancementsat the system level. The source code will be made available upon publication.</description><author>Zihao Zheng, Yuanchun Li, Jiayu Chen, Peng Zhou, Xiang Chen, Yunxin Liu</author><pubDate>Fri, 10 Jan 2025 09:55:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13902v2</guid></item><item><title>SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety</title><link>http://arxiv.org/abs/2404.05399v2</link><description>The last two years have seen a rapid growth in concerns around the safety oflarge language models (LLMs). Researchers and practitioners have met theseconcerns by creating an abundance of datasets for evaluating and improving LLMsafety. However, much of this work has happened in parallel, and with verydifferent goals in mind, ranging from the mitigation of near-term risks aroundbias and toxic content generation to the assessment of longer-term catastrophicrisk potential. This makes it difficult for researchers and practitioners tofind the most relevant datasets for their use case, and to identify gaps indataset coverage that future work may fill. To remedy these issues, we conducta first systematic review of open datasets for evaluating and improving LLMsafety. We review 144 datasets, which we identified through an iterative andcommunity-driven process over the course of several months. We highlightpatterns and trends, such as a trend towards fully synthetic datasets, as wellas gaps in dataset coverage, such as a clear lack of non-English andnaturalistic datasets. We also examine how LLM safety datasets are used inpractice -- in LLM release publications and popular LLM benchmarks -- findingthat current evaluation practices are highly idiosyncratic and make use of onlya small fraction of available datasets. Our contributions are based onSafetyPrompts.com, a living catalogue of open datasets for LLM safety, which weplan to update continuously as the field of LLM safety develops.</description><author>Paul Röttger, Fabio Pernisi, Bertie Vidgen, Dirk Hovy</author><pubDate>Fri, 10 Jan 2025 09:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05399v2</guid></item><item><title>Programmatic Reinforcement Learning: Navigating Gridworlds</title><link>http://arxiv.org/abs/2402.11650v2</link><description>The field of reinforcement learning (RL) is concerned with algorithms forlearning optimal policies in unknown stochastic environments. Programmatic RLstudies representations of policies as programs, meaning involving higher orderconstructs such as control loops. Despite attracting a lot of attention at theintersection of the machine learning and formal methods communities, verylittle is known on the theoretical front about programmatic RL: what are goodclasses of programmatic policies? How large are optimal programmatic policies?How can we learn them? The goal of this paper is to give first answers to thesequestions, initiating a theoretical study of programmatic RL. Considering aclass of gridworld environments, we define a class of programmatic policies.Our main contributions are to place upper bounds on the size of optimalprogrammatic policies, and to construct an algorithm for synthesizing them.These theoretical findings are complemented by a prototype implementation ofthe algorithm.</description><author>Guruprerana Shabadi, Nathanaël Fijalkow, Théo Matricon</author><pubDate>Fri, 10 Jan 2025 09:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11650v2</guid></item><item><title>Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial Expressions Using Static Expression Data</title><link>http://arxiv.org/abs/2409.06154v2</link><description>Dynamic facial expression recognition (DFER) infers emotions from thetemporal evolution of expressions, unlike static facial expression recognition(SFER), which relies solely on a single snapshot. This temporal analysisprovides richer information and promises greater recognition capability.However, current DFER methods often exhibit unsatisfied performance largely dueto fewer training samples compared to SFER. Given the inherent correlationbetween static and dynamic expressions, we hypothesize that leveraging theabundant SFER data can enhance DFER. To this end, we propose Static-for-Dynamic(S4D), a unified dual-modal learning framework that integrates SFER data as acomplementary resource for DFER. Specifically, S4D employs dual-modalself-supervised pre-training on facial images and videos using a shared VisionTransformer (ViT) encoder-decoder architecture, yielding improvedspatiotemporal representations. The pre-trained encoder is then fine-tuned onstatic and dynamic expression datasets in a multi-task learning setup tofacilitate emotional information interaction. Unfortunately, vanilla multi-tasklearning in our study results in negative transfer. To address this, we proposean innovative Mixture of Adapter Experts (MoAE) module that facilitatestask-specific knowledge acquisition while effectively extracting sharedknowledge from both static and dynamic expression data. Extensive experimentsdemonstrate that S4D achieves a deeper understanding of DFER, setting newstate-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, withweighted average recall (WAR) of 53.65\%, 58.44\%, and 76.68\%, respectively.Additionally, a systematic correlation analysis between SFER and DFER tasks ispresented, which further elucidates the potential benefits of leveraging SFER.</description><author>Yin Chen, Jia Li, Yu Zhang, Zhenzhen Hu, Shiguang Shan, Meng Wang, Richang Hong</author><pubDate>Fri, 10 Jan 2025 09:44:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06154v2</guid></item></channel></rss>