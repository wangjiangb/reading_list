<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 25 Jul 2023 06:00:49 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Parallel $Q$-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation</title><link>http://arxiv.org/abs/2307.12983v1</link><description>Reinforcement learning is time-consuming for complex tasks due to the needfor large amounts of training data. Recent advances in GPU-based simulation,such as Isaac Gym, have sped up data collection thousands of times on acommodity GPU. Most prior works used on-policy methods like PPO due to theirsimplicity and ease of scaling. Off-policy methods are more data efficient butchallenging to scale, resulting in a longer wall-clock training time. Thispaper presents a Parallel $Q$-Learning (PQL) scheme that outperforms PPO inwall-clock time while maintaining superior sample efficiency of off-policylearning. PQL achieves this by parallelizing data collection, policy learning,and value learning. Different from prior works on distributed off-policylearning, such as Apex, our scheme is designed specifically for massivelyparallel GPU-based simulation and optimized to work on a single workstation. Inexperiments, we demonstrate that $Q$-learning can be scaled to \textit{tens ofthousands of parallel environments} and investigate important factors affectinglearning speed. The code is available at https://github.com/Improbable-AI/pql.</description><author>Zechu Li, Tao Chen, Zhang-Wei Hong, Anurag Ajay, Pulkit Agrawal</author><pubDate>Mon, 24 Jul 2023 18:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12983v1</guid></item><item><title>Consistent model selection in the spiked Wigner model via AIC-type criteria</title><link>http://arxiv.org/abs/2307.12982v1</link><description>Consider the spiked Wigner model \[ X = \sum_{i = 1}^k \lambda_i u_i u_i^\top+ \sigma G, \] where $G$ is an $N \times N$ GOE random matrix, and theeigenvalues $\lambda_i$ are all spiked, i.e. above the Baik-Ben Arous-P\'ech\'e(BBP) threshold $\sigma$. We consider AIC-type model selection criteria of theform \[ -2 \, (\text{maximised log-likelihood}) + \gamma \, (\text{number ofparameters}) \] for estimating the number $k$ of spikes. For $\gamma &gt; 2$, theabove criterion is strongly consistent provided $\lambda_k &gt; \lambda_{\gamma}$,where $\lambda_{\gamma}$ is a threshold strictly above the BBP threshold,whereas for $\gamma &lt; 2$, it almost surely overestimates $k$. Although AIC(which corresponds to $\gamma = 2$) is not strongly consistent, we show thattaking $\gamma = 2 + \delta_N$, where $\delta_N \to 0$ and $\delta_N \ggN^{-2/3}$, results in a weakly consistent estimator of $k$. We also show that acertain soft minimiser of AIC is strongly consistent.</description><author>Soumendu Sundar Mukherjee</author><pubDate>Mon, 24 Jul 2023 18:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12982v1</guid></item><item><title>3D-LLM: Injecting the 3D World into Large Language Models</title><link>http://arxiv.org/abs/2307.12981v1</link><description>Large language models (LLMs) and Vision-Language Models (VLMs) have beenproven to excel at multiple tasks, such as commonsense reasoning. Powerful asthese models can be, they are not grounded in the 3D physical world, whichinvolves richer concepts such as spatial relationships, affordances, physics,layout, and so on. In this work, we propose to inject the 3D world into largelanguage models and introduce a whole new family of 3D-LLMs. Specifically,3D-LLMs can take 3D point clouds and their features as input and perform adiverse set of 3D-related tasks, including captioning, dense captioning, 3Dquestion answering, task decomposition, 3D grounding, 3D-assisted dialog,navigation, and so on. Using three types of prompting mechanisms that wedesign, we are able to collect over 300k 3D-language data covering these tasks.To efficiently train 3D-LLMs, we first utilize a 3D feature extractor thatobtains 3D features from rendered multi- view images. Then, we use 2D VLMs asour backbones to train our 3D-LLMs. By introducing a 3D localization mechanism,3D-LLMs can better capture 3D spatial information. Experiments on ScanQA showthat our model outperforms state-of-the-art baselines by a large margin (e.g.,the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore,experiments on our held-in datasets for 3D captioning, task composition, and3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitativeexamples also show that our model could perform more tasks beyond the scope ofexisting LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.</description><author>Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan</author><pubDate>Mon, 24 Jul 2023 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12981v1</guid></item><item><title>Exphormer: Sparse Transformers for Graphs</title><link>http://arxiv.org/abs/2303.06147v2</link><description>Graph transformers have emerged as a promising architecture for a variety ofgraph learning and representation tasks. Despite their successes, though, itremains challenging to scale graph transformers to large graphs whilemaintaining accuracy competitive with message-passing networks. In this paper,we introduce Exphormer, a framework for building powerful and scalable graphtransformers. Exphormer consists of a sparse attention mechanism based on twomechanisms: virtual global nodes and expander graphs, whose mathematicalcharacteristics, such as spectral expansion, pseduorandomness, and sparsity,yield graph transformers with complexity only linear in the size of the graph,while allowing us to prove desirable theoretical properties of the resultingtransformer models. We show that incorporating Exphormer into therecently-proposed GraphGPS framework produces models with competitive empiricalresults on a wide variety of graph datasets, including state-of-the-art resultson three datasets. We also show that Exphormer can scale to datasets on largergraphs than shown in previous graph transformer architectures. Code can befound at \url{https://github.com/hamed1375/Exphormer}.</description><author>Hamed Shirzad, Ameya Velingker, Balaji Venkatachalam, Danica J. Sutherland, Ali Kemal Sinop</author><pubDate>Mon, 24 Jul 2023 18:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06147v2</guid></item><item><title>Segmenting Known Objects and Unseen Unknowns without Prior Knowledge</title><link>http://arxiv.org/abs/2209.05407v3</link><description>Panoptic segmentation methods assign a known class to each pixel given ininput. Even for state-of-the-art approaches, this inevitably enforces decisionsthat systematically lead to wrong predictions for objects outside the trainingcategories. However, robustness against out-of-distribution samples and cornercases is crucial in safety-critical settings to avoid dangerous consequences.Since real-world datasets cannot contain enough data points to adequatelysample the long tail of the underlying distribution, models must be able todeal with unseen and unknown scenarios as well. Previous methods targeted thisby re-identifying already-seen unlabeled objects. In this work, we propose thenecessary step to extend segmentation with a new setting which we term holisticsegmentation. Holistic segmentation aims to identify and separate objects ofunseen unknown categories into instances, without any prior knowledge aboutthem, while performing panoptic segmentation of known classes. We tackle thisnew problem with U3HS, which finds unknowns as highly uncertain regions andclusters their corresponding instance-aware embeddings into individual objects.By doing so, for the first time in panoptic segmentation with unknown objects,our U3HS is trained without unknown categories, reducing assumptions andleaving the settings as unconstrained as in real-life scenarios. Extensiveexperiments on public data from MS COCO, Cityscapes, and Lost&amp;Found demonstratethe effectiveness of U3HS for this new, challenging, and assumptions-freesetting called holistic segmentation.</description><author>Stefano Gasperini, Alvaro Marcos-Ramiro, Michael Schmidt, Nassir Navab, Benjamin Busam, Federico Tombari</author><pubDate>Mon, 24 Jul 2023 18:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.05407v3</guid></item><item><title>A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models</title><link>http://arxiv.org/abs/2307.12980v1</link><description>Prompt engineering is a technique that involves augmenting a largepre-trained model with task-specific hints, known as prompts, to adapt themodel to new tasks. Prompts can be created manually as natural languageinstructions or generated automatically as either natural language instructionsor vector representations. Prompt engineering enables the ability to performpredictions based solely on prompts without updating model parameters, and theeasier application of large pre-trained models in real-world tasks. In pastyears, Prompt engineering has been well-studied in natural language processing.Recently, it has also been intensively studied in vision-language modeling.However, there is currently a lack of a systematic overview of promptengineering on pre-trained vision-language models. This paper aims to provide acomprehensive survey of cutting-edge research in prompt engineering on threetypes of vision-language models: multimodal-to-text generation models (e.g.Flamingo), image-text matching models (e.g. CLIP), and text-to-image generationmodels (e.g. Stable Diffusion). For each type of model, a brief model summary,prompting methods, prompting-based applications, and the correspondingresponsibility and integrity issues are summarized and discussed. Furthermore,the commonalities and differences between prompting on vision-language models,language models, and vision models are also discussed. The challenges, futuredirections, and research opportunities are summarized to foster future researchon this topic.</description><author>Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, Philip Torr</author><pubDate>Mon, 24 Jul 2023 18:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12980v1</guid></item><item><title>An Isometric Stochastic Optimizer</title><link>http://arxiv.org/abs/2307.12979v1</link><description>The Adam optimizer is the standard choice in deep learning applications. Ipropose a simple explanation of Adam's success: it makes each parameter's stepsize independent of the norms of the other parameters. Based on this principleI derive Iso, a new optimizer which makes the norm of a parameter's updateinvariant to the application of any linear transformation to its inputs andoutputs. I develop a variant of Iso called IsoAdam that allows optimalhyperparameters to be transferred from Adam, and demonstrate that IsoAdamobtains a speedup over Adam when training a small Transformer.</description><author>Jacob Jackson</author><pubDate>Mon, 24 Jul 2023 18:56:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12979v1</guid></item><item><title>Evaluating the Ripple Effects of Knowledge Editing in Language Models</title><link>http://arxiv.org/abs/2307.12976v1</link><description>Modern language models capture a large body of factual knowledge. However,some facts can be incorrectly induced or become obsolete over time, resultingin factually incorrect generations. This has led to the development of variousediting methods that allow updating facts encoded by the model. Evaluation ofthese methods has primarily focused on testing whether an individual fact hasbeen successfully injected, and if similar predictions for other subjects havenot changed. Here we argue that such evaluation is limited, since injecting onefact (e.g. ``Jack Depp is the son of Johnny Depp'') introduces a ``rippleeffect'' in the form of additional facts that the model needs to update(e.g.``Jack Depp is the sibling of Lily-Rose Depp''). To address this issue, wepropose a novel set of evaluation criteria that consider the implications of anedit on related facts. Using these criteria, we then construct \ripple{}, adiagnostic benchmark of 5K factual edits, capturing a variety of types ofripple effects. We evaluate prominent editing methods on \ripple{}, showingthat current methods fail to introduce consistent changes in the model'sknowledge. In addition, we find that a simple in-context editing baselineobtains the best scores on our benchmark, suggesting a promising researchdirection for model editing.</description><author>Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, Mor Geva</author><pubDate>Mon, 24 Jul 2023 18:52:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12976v1</guid></item><item><title>Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems</title><link>http://arxiv.org/abs/2307.12975v1</link><description>A crucial task in decision-making problems is reward engineering. It iscommon in practice that no obvious choice of reward function exists. Thus, apopular approach is to introduce human feedback during training and leveragesuch feedback to learn a reward function. Among all policy learning methodsthat use human feedback, preference-based methods have demonstrated substantialsuccess in recent empirical applications such as InstructGPT. In this work, wedevelop a theory that provably shows the benefits of preference-based methodsin offline contextual bandits. In particular, we improve the modeling andsuboptimality analysis for running policy learning methods on human-scoredsamples directly. Then, we compare it with the suboptimality guarantees ofpreference-based methods and show that preference-based methods enjoy lowersuboptimality.</description><author>Xiang Ji, Huazheng Wang, Minshuo Chen, Tuo Zhao, Mengdi Wang</author><pubDate>Mon, 24 Jul 2023 18:50:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12975v1</guid></item><item><title>Leveraging Label Variation in Large Language Models for Zero-Shot Text Classification</title><link>http://arxiv.org/abs/2307.12973v1</link><description>The zero-shot learning capabilities of large language models (LLMs) make themideal for text classification without annotation or supervised training. Manystudies have shown impressive results across multiple tasks. While tasks, data,and results differ widely, their similarities to human annotation can aid us intackling new tasks with minimal expenses. We evaluate using 5 state-of-the-artLLMs as "annotators" on 5 different tasks (age, gender, topic, sentimentprediction, and hate speech detection), across 4 languages: English, French,German, and Spanish. No single model excels at all tasks, across languages, oracross all labels within a task. However, aggregation techniques designed forhuman annotators perform substantially better than any one individual model.Overall, though, LLMs do not rival even simple supervised models, so they donot (yet) replace the need for human annotation. We also discuss the tradeoffsbetween speed, accuracy, cost, and bias when it comes to aggregated modellabeling versus human annotation.</description><author>Flor Miriam Plaza-del-Arco, Debora Nozza, Dirk Hovy</author><pubDate>Mon, 24 Jul 2023 18:49:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12973v1</guid></item><item><title>DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting</title><link>http://arxiv.org/abs/2307.12972v1</link><description>In this paper, we propose a new operator, called 3D DeFormable Attention(DFA3D), for 2D-to-3D feature lifting, which transforms multi-view 2D imagefeatures into a unified 3D space for 3D object detection. Existing featurelifting approaches, such as Lift-Splat-based and 2D attention-based, either useestimated depth to get pseudo LiDAR features and then splat them to a 3D space,which is a one-pass operation without feature refinement, or ignore depth andlift features by 2D attention mechanisms, which achieve finer semantics whilesuffering from a depth ambiguity problem. In contrast, our DFA3D-based methodfirst leverages the estimated depth to expand each view's 2D feature map to 3Dand then utilizes DFA3D to aggregate features from the expanded 3D featuremaps. With the help of DFA3D, the depth ambiguity problem can be effectivelyalleviated from the root, and the lifted features can be progressively refinedlayer by layer, thanks to the Transformer-like architecture. In addition, wepropose a mathematically equivalent implementation of DFA3D which cansignificantly improve its memory efficiency and computational speed. Weintegrate DFA3D into several methods that use 2D attention-based featurelifting with only a few modifications in code and evaluate on the nuScenesdataset. The experiment results show a consistent improvement of +1.41\% mAP onaverage, and up to +15.1\% mAP improvement when high-quality depth informationis available, demonstrating the superiority, applicability, and huge potentialof DFA3D. The code is available athttps://github.com/IDEA-Research/3D-deformable-attention.git.</description><author>Hongyang Li, Hao Zhang, Zhaoyang Zeng, Shilong Liu, Feng Li, Tianhe Ren, Lei Zhang</author><pubDate>Mon, 24 Jul 2023 18:49:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12972v1</guid></item><item><title>Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques</title><link>http://arxiv.org/abs/2307.12971v1</link><description>This article intends to systematically identify and comparatively analyzestate-of-the-art supply chain (SC) forecasting strategies and technologies. Anovel framework has been proposed incorporating Big Data Analytics in SCManagement (problem identification, data sources, exploratory data analysis,machine-learning model training, hyperparameter tuning, performance evaluation,and optimization), forecasting effects on human-workforce, inventory, andoverall SC. Initially, the need to collect data according to SC strategy andhow to collect them has been discussed. The article discusses the need fordifferent types of forecasting according to the period or SC objective. The SCKPIs and the error-measurement systems have been recommended to optimize thetop-performing model. The adverse effects of phantom inventory on forecastingand the dependence of managerial decisions on the SC KPIs for determining modelperformance parameters and improving operations management, transparency, andplanning efficiency have been illustrated. The cyclic connection within theframework introduces preprocessing optimization based on the post-process KPIs,optimizing the overall control process (inventory management, workforcedetermination, cost, production and capacity planning). The contribution ofthis research lies in the standard SC process framework proposal, recommendedforecasting data analysis, forecasting effects on SC performance, machinelearning algorithms optimization followed, and in shedding light on futureresearch.</description><author>Md Abrar Jahin, Md Sakib Hossain Shovon, Jungpil Shin, Istiyaque Ahmed Ridoy, Yoichi Tomioka, M. F. Mridha</author><pubDate>Mon, 24 Jul 2023 18:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12971v1</guid></item><item><title>Volcanic ash delimitation using Artificial Intelligence based on Pix2Pix</title><link>http://arxiv.org/abs/2307.12970v1</link><description>Volcanic eruptions emit ash that can be harmful to human health and causedamage to infrastructure, economic activities and the environment. Thedelimitation of ash clouds allows to know their behavior and dispersion, whichhelps in the prevention and mitigation of this phenomenon. Traditional methodstake advantage of specialized software programs to process the bands orchannels that compose the satellite images. However, their use is limited toexperts and demands a lot of time and significant computational resources. Inrecent years, Artificial Intelligence has been a milestone in the computationaltreatment of complex problems in different areas. In particular, Deep Learningtechniques allow automatic, fast and accurate processing of digital images. Thepresent work proposes the use of the Pix2Pix model, a type of generativeadversarial network that, once trained, learns the mapping of input images tooutput images. The architecture of such a network consisting of a generator anda discriminator provides the versatility needed to produce black and white ashcloud images from multispectral satellite images. The evaluation of the model,based on loss and accuracy plots, a confusion matrix, and visual inspection,indicates a satisfactory solution for accurate ash cloud delineation,applicable in any area of the world and becomes a useful tool in riskmanagement.</description><author>Christian Carrillo, Gissela Torres, Christian Mejia-Escobar</author><pubDate>Mon, 24 Jul 2023 18:49:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12970v1</guid></item><item><title>A Connection between One-Step Regularization and Critic Regularization in Reinforcement Learning</title><link>http://arxiv.org/abs/2307.12968v1</link><description>As with any machine learning problem with limited data, effective offline RLalgorithms require careful regularization to avoid overfitting. One-stepmethods perform regularization by doing just a single step of policyimprovement, while critic regularization methods do many steps of policyimprovement with a regularized objective. These methods appear distinct.One-step methods, such as advantage-weighted regression and conditionalbehavioral cloning, truncate policy iteration after just one step. This ``earlystopping'' makes one-step RL simple and stable, but can limit its asymptoticperformance. Critic regularization typically requires more compute but hasappealing lower-bound guarantees. In this paper, we draw a close connectionbetween these methods: applying a multi-step critic regularization method witha regularization coefficient of 1 yields the same policy as one-step RL. Whilepractical implementations violate our assumptions and critic regularization istypically applied with smaller regularization coefficients, our experimentsnevertheless show that our analysis makes accurate, testable predictions aboutpractical offline RL methods (CQL and one-step RL) with commonly-usedhyperparameters. Our results that every problem can be solved with a singlestep of policy improvement, but rather that one-step RL might be competitivewith critic regularization on RL problems that demand strong regularization.</description><author>Benjamin Eysenbach, Matthieu Geist, Sergey Levine, Ruslan Salakhutdinov</author><pubDate>Mon, 24 Jul 2023 18:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12968v1</guid></item><item><title>Learning Dense Correspondences between Photos and Sketches</title><link>http://arxiv.org/abs/2307.12967v1</link><description>Humans effortlessly grasp the connection between sketches and real-worldobjects, even when these sketches are far from realistic. Moreover, humansketch understanding goes beyond categorization -- critically, it also entailsunderstanding how individual elements within a sketch correspond to parts ofthe physical world it represents. What are the computational ingredients neededto support this ability? Towards answering this question, we make twocontributions: first, we introduce a new sketch-photo correspondence benchmark,$\textit{PSC6k}$, containing 150K annotations of 6250 sketch-photo pairs across125 object categories, augmenting the existing Sketchy dataset withfine-grained correspondence metadata. Second, we propose a self-supervisedmethod for learning dense correspondences between sketch-photo pairs, buildingupon recent advances in correspondence learning for pairs of photos. Our modeluses a spatial transformer network to estimate the warp flow between latentrepresentations of a sketch and photo extracted by a contrastive learning-basedConvNet backbone. We found that this approach outperformed several strongbaselines and produced predictions that were quantitatively consistent withother warp-based methods. However, our benchmark also revealed systematicdifferences between predictions of the suite of models we tested and those ofhumans. Taken together, our work suggests a promising path towards developingartificial systems that achieve more human-like understanding of visual imagesat different levels of abstraction. Project page:https://photo-sketch-correspondence.github.io</description><author>Xuanchen Lu, Xiaolong Wang, Judith E Fan</author><pubDate>Mon, 24 Jul 2023 18:45:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12967v1</guid></item><item><title>Aligning Large Language Models with Human: A Survey</title><link>http://arxiv.org/abs/2307.12966v1</link><description>Large Language Models (LLMs) trained on extensive textual corpora haveemerged as leading solutions for a broad array of Natural Language Processing(NLP) tasks. Despite their notable performance, these models are prone tocertain limitations such as misunderstanding human instructions, generatingpotentially biased content, or factually incorrect (hallucinated) information.Hence, aligning LLMs with human expectations has become an active area ofinterest within the research community. This survey presents a comprehensiveoverview of these alignment technologies, including the following aspects. (1)Data collection: the methods for effectively collecting high-qualityinstructions for LLM alignment, including the use of NLP benchmarks, humanannotations, and leveraging strong LLMs. (2) Training methodologies: a detailedreview of the prevailing training methods employed for LLM alignment. Ourexploration encompasses Supervised Fine-tuning, both Online and Offline humanpreference training, along with parameter-efficient training mechanisms. (3)Model Evaluation: the methods for evaluating the effectiveness of thesehuman-aligned LLMs, presenting a multifaceted approach towards theirassessment. In conclusion, we collate and distill our findings, shedding lighton several promising future research avenues in the field. This survey,therefore, serves as a valuable resource for anyone invested in understandingand advancing the alignment of LLMs to better suit human-oriented tasks andexpectations. An associated GitHub link collecting the latest papers isavailable at https://github.com/GaryYufei/AlignLLMHumanSurvey.</description><author>Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu</author><pubDate>Mon, 24 Jul 2023 18:44:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12966v1</guid></item><item><title>Audio-Enhanced Text-to-Video Retrieval using Text-Conditioned Feature Alignment</title><link>http://arxiv.org/abs/2307.12964v1</link><description>Text-to-video retrieval systems have recently made significant progress byutilizing pre-trained models trained on large-scale image-text pairs. However,most of the latest methods primarily focus on the video modality whiledisregarding the audio signal for this task. Nevertheless, a recent advancementby ECLIPSE has improved long-range text-to-video retrieval by developing anaudiovisual video representation. Nonetheless, the objective of thetext-to-video retrieval task is to capture the complementary audio and videoinformation that is pertinent to the text query rather than simply achievingbetter audio and video alignment. To address this issue, we introduce TEFAL, aTExt-conditioned Feature ALignment method that produces both audio and videorepresentations conditioned on the text query. Instead of using only anaudiovisual attention block, which could suppress the audio informationrelevant to the text query, our approach employs two independent cross-modalattention blocks that enable the text to attend to the audio and videorepresentations separately. Our proposed method's efficacy is demonstrated onfour benchmark datasets that include audio: MSR-VTT, LSMDC, VATEX, andCharades, and achieves better than state-of-the-art performance consistentlyacross the four datasets. This is attributed to the additionaltext-query-conditioned audio representation and the complementary informationit adds to the text-query-conditioned video representation.</description><author>Sarah Ibrahimi, Xiaohang Sun, Pichao Wang, Amanmeet Garg, Ashutosh Sanan, Mohamed Omar</author><pubDate>Mon, 24 Jul 2023 18:43:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12964v1</guid></item><item><title>How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding</title><link>http://arxiv.org/abs/2303.04245v2</link><description>While the successes of transformers across many domains are indisputable,accurate understanding of the learning mechanics is still largely lacking.Their capabilities have been probed on benchmarks which include a variety ofstructured and reasoning tasks -- but mathematical understanding is laggingsubstantially behind. Recent lines of work have begun studying representationalaspects of this question: that is, the size/depth/complexity of attention-basednetworks to perform certain tasks. However, there is no guarantee the learningdynamics will converge to the constructions proposed. In our paper, we providefine-grained mechanistic understanding of how transformers learn "semanticstructure", understood as capturing co-occurrence structure of words.Precisely, we show, through a combination of mathematical analysis andexperiments on Wikipedia data and synthetic data modeled by Latent DirichletAllocation (LDA), that the embedding layer and the self-attention layer encodethe topical structure. In the former case, this manifests as higher averageinner product of embeddings between same-topic words. In the latter, itmanifests as higher average pairwise attention between same-topic words. Themathematical results involve several assumptions to make the analysistractable, which we verify on data, and might be of independent interest aswell.</description><author>Yuchen Li, Yuanzhi Li, Andrej Risteski</author><pubDate>Mon, 24 Jul 2023 18:29:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04245v2</guid></item><item><title>RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment</title><link>http://arxiv.org/abs/2307.12950v1</link><description>We propose Reinforcement Learning from Contrast Distillation (RLCD), a methodfor aligning language models to follow natural language principles withoutusing human feedback. RLCD trains a preference model using simulated preferencepairs that contain both a high-quality and low-quality example, generated usingcontrasting positive and negative prompts. The preference model is then used toimprove a base unaligned language model via reinforcement learning.Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and contextdistillation (Huang et al., 2022) baselines across three diverse alignmenttasks--harmlessness, helpfulness, and story outline generation--and on both 7Band 30B model scales for preference data simulation.</description><author>Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, Yuandong Tian</author><pubDate>Mon, 24 Jul 2023 18:23:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12950v1</guid></item><item><title>Boosting Punctuation Restoration with Data Generation and Reinforcement Learning</title><link>http://arxiv.org/abs/2307.12949v1</link><description>Punctuation restoration is an important task in automatic speech recognition(ASR) which aim to restore the syntactic structure of generated ASR texts toimprove readability. While punctuated texts are abundant from writtendocuments, the discrepancy between written punctuated texts and ASR textslimits the usability of written texts in training punctuation restorationsystems for ASR texts. This paper proposes a reinforcement learning method toexploit in-topic written texts and recent advances in large pre-trainedgenerative language models to bridge this gap. The experiments show that ourmethod achieves state-of-the-art performance on the ASR test set on twobenchmark datasets for punctuation restoration.</description><author>Viet Dac Lai, Abel Salinas, Hao Tan, Trung Bui, Quan Tran, Seunghyun Yoon, Hanieh Deilamsalehy, Franck Dernoncourt, Thien Huu Nguyen</author><pubDate>Mon, 24 Jul 2023 18:22:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12949v1</guid></item><item><title>Efficiently Sampling the PSD Cone with the Metric Dikin Walk</title><link>http://arxiv.org/abs/2307.12943v1</link><description>Semi-definite programs represent a frontier of efficient computation. Whilethere has been much progress on semi-definite optimization, with moderate-sizedinstances currently solvable in practice by the interior-point method, thebasic problem of sampling semi-definite solutions remains a formidablechallenge. The direct application of known polynomial-time algorithms forsampling general convex bodies to semi-definite sampling leads to aprohibitively high running time. In addition, known general methods require anexpensive rounding phase as pre-processing. Here we analyze the Dikin walk, byfirst adapting it to general metrics, then devising suitable metrics for thePSD cone with affine constraints. The resulting mixing time and per-stepcomplexity are considerably smaller, and by an appropriate choice of themetric, the dependence on the number of constraints can be madepolylogarithmic. We introduce a refined notion of self-concordant matrixfunctions and give rules for combining different metrics. Along the way, wefurther develop the theory of interior-point methods for sampling.</description><author>Yunbum Kook, Santosh S. Vempala</author><pubDate>Mon, 24 Jul 2023 18:15:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12943v1</guid></item><item><title>On Privileged and Convergent Bases in Neural Network Representations</title><link>http://arxiv.org/abs/2307.12941v1</link><description>In this study, we investigate whether the representations learned by neuralnetworks possess a privileged and convergent basis. Specifically, we examinethe significance of feature directions represented by individual neurons.First, we establish that arbitrary rotations of neural representations cannotbe inverted (unlike linear networks), indicating that they do not exhibitcomplete rotational invariance. Subsequently, we explore the possibility ofmultiple bases achieving identical performance. To do this, we compare thebases of networks trained with the same parameters but with varying randominitializations. Our study reveals two findings: (1) Even in wide networks suchas WideResNets, neural networks do not converge to a unique basis; (2) Basiscorrelation increases significantly when a few early layers of the network arefrozen identically. Furthermore, we analyze Linear Mode Connectivity, which has been studied as ameasure of basis correlation. Our findings give evidence that while Linear ModeConnectivity improves with increased network width, this improvement is not dueto an increase in basis correlation.</description><author>Davis Brown, Nikhil Vyas, Yamini Bansal</author><pubDate>Mon, 24 Jul 2023 18:11:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12941v1</guid></item><item><title>Unsupervised Learning of Invariance Transformations</title><link>http://arxiv.org/abs/2307.12937v1</link><description>The need for large amounts of training data in modern machine learning is oneof the biggest challenges of the field. Compared to the brain, currentartificial algorithms are much less capable of learning invariancetransformations and employing them to extrapolate knowledge from small samplesets. It has recently been proposed that the brain might encode perceptualinvariances as approximate graph symmetries in the network of synapticconnections. Such symmetries may arise naturally through a biologicallyplausible process of unsupervised Hebbian learning. In the present paper, weillustrate this proposal on numerical examples, showing that invariancetransformations can indeed be recovered from the structure of recurrentsynaptic connections which form within a layer of feature detector neurons viaa simple Hebbian learning rule. In order to numerically recover the invariancetransformations from the resulting recurrent network, we develop a generalalgorithmic framework for finding approximate graph automorphisms. We discusshow this framework can be used to find approximate automorphisms in weightedgraphs in general.</description><author>Aleksandar Vučković, Benedikt Stock, Alexander V. Hopp, Mathias Winkel, Helmut Linde</author><pubDate>Mon, 24 Jul 2023 18:03:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12937v1</guid></item><item><title>Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study</title><link>http://arxiv.org/abs/2307.08572v3</link><description>Coping with distributional shifts is an important part of transfer learningmethods in order to perform well in real-life tasks. However, most of theexisting approaches in this area either focus on an ideal scenario in which thedata does not contain noises or employ a complicated training paradigm or modeldesign to deal with distributional shifts. In this paper, we revisit therobustness of the minimum error entropy (MEE) criterion, a widely usedobjective in statistical signal processing to deal with non-Gaussian noises,and investigate its feasibility and usefulness in real-life transfer learningregression tasks, where distributional shifts are common. Specifically, we putforward a new theoretical result showing the robustness of MEE againstcovariate shift. We also show that by simply replacing the mean squared error(MSE) loss with the MEE on basic transfer learning algorithms such asfine-tuning and linear probing, we can achieve competitive performance withrespect to state-of-the-art transfer learning algorithms. We justify ourarguments on both synthetic data and 5 real-world time-series data.</description><author>Luis Pedro Silvestrin, Shujian Yu, Mark Hoogendoorn</author><pubDate>Mon, 24 Jul 2023 18:01:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08572v3</guid></item><item><title>Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection</title><link>http://arxiv.org/abs/2307.12935v1</link><description>Classic approaches to content moderation typically apply a rule-basedheuristic approach to flag content. While rules are easily customizable andintuitive for humans to interpret, they are inherently fragile and lack theflexibility or robustness needed to moderate the vast amount of undesirablecontent found online today. Recent advances in deep learning have demonstratedthe promise of using highly effective deep neural models to overcome thesechallenges. However, despite the improved performance, these data-driven modelslack transparency and explainability, often leading to mistrust from everydayusers and a lack of adoption by many platforms. In this paper, we present RuleBy Example (RBE): a novel exemplar-based contrastive learning approach forlearning from logical rules for the task of textual content moderation. RBE iscapable of providing rule-grounded predictions, allowing for more explainableand customizable predictions compared to typical deep learning-basedapproaches. We demonstrate that our approach is capable of learning rich ruleembedding representations using only a few data examples. Experimental resultson 3 popular hate speech classification datasets show that RBE is able tooutperform state-of-the-art deep learning classifiers as well as the use ofrules in both supervised and unsupervised settings while providing explainablemodel predictions via rule-grounding.</description><author>Christopher Clarke, Matthew Hall, Gaurav Mittal, Ye Yu, Sandra Sajeev, Jason Mars, Mei Chen</author><pubDate>Mon, 24 Jul 2023 17:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12935v1</guid></item><item><title>Theoretically Guaranteed Policy Improvement Distilled from Model-Based Planning</title><link>http://arxiv.org/abs/2307.12933v1</link><description>Model-based reinforcement learning (RL) has demonstrated remarkable successeson a range of continuous control tasks due to its high sample efficiency. Tosave the computation cost of conducting planning online, recent practices tendto distill optimized action sequences into an RL policy during the trainingphase. Although the distillation can incorporate both the foresight of planningand the exploration ability of RL policies, the theoretical understanding ofthese methods is yet unclear. In this paper, we extend the policy improvementstep of Soft Actor-Critic (SAC) by developing an approach to distill frommodel-based planning to the policy. We then demonstrate that such an approachof policy improvement has a theoretical guarantee of monotonic improvement andconvergence to the maximum value defined in SAC. We discuss effective designchoices and implement our theory as a practical algorithm -- Model-basedPlanning Distilled to Policy (MPDP) -- that updates the policy jointly overmultiple future time steps. Extensive experiments show that MPDP achievesbetter sample efficiency and asymptotic performance than both model-free andmodel-based planning algorithms on six continuous control benchmark tasks inMuJoCo.</description><author>Chuming Li, Ruonan Jia, Jie Liu, Yinmin Zhang, Yazhe Niu, Yaodong Yang, Yu Liu, Wanli Ouyang</author><pubDate>Mon, 24 Jul 2023 17:52:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12933v1</guid></item><item><title>Contextual Bandits and Imitation Learning via Preference-Based Active Queries</title><link>http://arxiv.org/abs/2307.12926v1</link><description>We consider the problem of contextual bandits and imitation learning, wherethe learner lacks direct knowledge of the executed action's reward. Instead,the learner can actively query an expert at each round to compare two actionsand receive noisy preference feedback. The learner's objective is two-fold: tominimize the regret associated with the executed actions, while simultaneously,minimizing the number of comparison queries made to the expert. In this paper,we assume that the learner has access to a function class that can representthe expert's preference model under appropriate link functions, and provide analgorithm that leverages an online regression oracle with respect to thisfunction class for choosing its actions and deciding when to query. For thecontextual bandit setting, our algorithm achieves a regret bound that combinesthe best of both worlds, scaling as $O(\min\{\sqrt{T}, d/\Delta\})$, where $T$represents the number of interactions, $d$ represents the eluder dimension ofthe function class, and $\Delta$ represents the minimum preference of theoptimal action over any suboptimal action under all contexts. Our algorithmdoes not require the knowledge of $\Delta$, and the obtained regret bound iscomparable to what can be achieved in the standard contextual bandits settingwhere the learner observes reward signals at each round. Additionally, ouralgorithm makes only $O(\min\{T, d^2/\Delta^2\})$ queries to the expert. Wethen extend our algorithm to the imitation learning setting, where the learningagent engages with an unknown environment in episodes of length $H$ each, andprovide similar guarantees for regret and query complexity. Interestingly, ouralgorithm for imitation learning can even learn to outperform the underlyingexpert, when it is suboptimal, highlighting a practical benefit ofpreference-based feedback in imitation learning.</description><author>Ayush Sekhari, Karthik Sridharan, Wen Sun, Runzhe Wu</author><pubDate>Mon, 24 Jul 2023 17:36:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12926v1</guid></item><item><title>Hierarchical Skeleton Meta-Prototype Contrastive Learning with Hard Skeleton Mining for Unsupervised Person Re-Identification</title><link>http://arxiv.org/abs/2307.12917v1</link><description>With rapid advancements in depth sensors and deep learning, skeleton-basedperson re-identification (re-ID) models have recently achieved remarkableprogress with many advantages. Most existing solutions learn single-levelskeleton features from body joints with the assumption of equal skeletonimportance, while they typically lack the ability to exploit more informativeskeleton features from various levels such as limb level with more global bodypatterns. The label dependency of these methods also limits their flexibilityin learning more general skeleton representations. This paper proposes ageneric unsupervised Hierarchical skeleton Meta-Prototype Contrastive learning(Hi-MPC) approach with Hard Skeleton Mining (HSM) for person re-ID withunlabeled 3D skeletons. Firstly, we construct hierarchical representations ofskeletons to model coarse-to-fine body and motion features from the levels ofbody joints, components, and limbs. Then a hierarchical meta-prototypecontrastive learning model is proposed to cluster and contrast the most typicalskeleton features ("prototypes") from different-level skeletons. By convertingoriginal prototypes into meta-prototypes with multiple homogeneoustransformations, we induce the model to learn the inherent consistency ofprototypes to capture more effective skeleton features for person re-ID.Furthermore, we devise a hard skeleton mining mechanism to adaptively infer theinformative importance of each skeleton, so as to focus on harder skeletons tolearn more discriminative skeleton representations. Extensive evaluations onfive datasets demonstrate that our approach outperforms a wide variety ofstate-of-the-art skeleton-based methods. We further show the generalapplicability of our method to cross-view person re-ID and RGB-based scenarioswith estimated skeletons.</description><author>Haocong Rao, Cyril Leung, Chunyan Miao</author><pubDate>Mon, 24 Jul 2023 17:18:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12917v1</guid></item><item><title>Consensus-based Participatory Budgeting for Legitimacy: Decision Support via Multi-agent Reinforcement Learning</title><link>http://arxiv.org/abs/2307.12915v1</link><description>The legitimacy of bottom-up democratic processes for the distribution ofpublic funds by policy-makers is challenging and complex. Participatorybudgeting is such a process, where voting outcomes may not always be fair orinclusive. Deliberation for which project ideas to put for voting and choosefor implementation lack systematization and do not scale. This paper addressesthese grand challenges by introducing a novel and legitimate iterativeconsensus-based participatory budgeting process. Consensus is designed to be aresult of decision support via an innovative multi-agent reinforcement learningapproach. Voters are assisted to interact with each other to make viablecompromises. Extensive experimental evaluation with real-world participatorybudgeting data from Poland reveal striking findings: Consensus is reachable,efficient and robust. Compromise is required, which is though comparable to theone of existing voting aggregation methods that promote fairness and inclusionwithout though attaining consensus.</description><author>Srijoni Majumdar, Evangelos Pournaras</author><pubDate>Mon, 24 Jul 2023 17:16:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12915v1</guid></item><item><title>Towards a Visual-Language Foundation Model for Computational Pathology</title><link>http://arxiv.org/abs/2307.12914v1</link><description>The accelerated adoption of digital pathology and advances in deep learninghave enabled the development of powerful models for various pathology tasksacross a diverse array of diseases and patient cohorts. However, model trainingis often difficult due to label scarcity in the medical domain and the model'susage is limited by the specific task and disease for which it is trained.Additionally, most models in histopathology leverage only image data, a starkcontrast to how humans teach each other and reason about histopathologicentities. We introduce CONtrastive learning from Captions for Histopathology(CONCH), a visual-language foundation model developed using diverse sources ofhistopathology images, biomedical text, and notably over 1.17 millionimage-caption pairs via task-agnostic pretraining. Evaluated on a suite of 13diverse benchmarks, CONCH can be transferred to a wide range of downstreamtasks involving either or both histopathology images and text, achievingstate-of-the-art performance on histology image classification, segmentation,captioning, text-to-image and image-to-text retrieval. CONCH represents asubstantial leap over concurrent visual-language pretrained systems forhistopathology, with the potential to directly facilitate a wide array ofmachine learning-based workflows requiring minimal or no further supervisedfine-tuning.</description><author>Ming Y. Lu, Bowen Chen, Drew F. K. Williamson, Richard J. Chen, Ivy Liang, Tong Ding, Guillaume Jaume, Igor Odintsov, Andrew Zhang, Long Phi Le, Georg Gerber, Anil V Parwani, Faisal Mahmood</author><pubDate>Mon, 24 Jul 2023 17:13:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12914v1</guid></item><item><title>Dyn-E: Local Appearance Editing of Dynamic Neural Radiance Fields</title><link>http://arxiv.org/abs/2307.12909v1</link><description>Recently, the editing of neural radiance fields (NeRFs) has gainedconsiderable attention, but most prior works focus on static scenes whileresearch on the appearance editing of dynamic scenes is relatively lacking. Inthis paper, we propose a novel framework to edit the local appearance ofdynamic NeRFs by manipulating pixels in a single frame of training video.Specifically, to locally edit the appearance of dynamic NeRFs while preservingunedited regions, we introduce a local surface representation of the editedregion, which can be inserted into and rendered along with the original NeRFand warped to arbitrary other frames through a learned invertible motionrepresentation network. By employing our method, users without professionalexpertise can easily add desired content to the appearance of a dynamic scene.We extensively evaluate our approach on various scenes and show that ourapproach achieves spatially and temporally consistent editing results. Notably,our approach is versatile and applicable to different variants of dynamic NeRFrepresentations.</description><author>Shangzhan Zhang, Sida Peng, Yinji ShenTu, Qing Shuai, Tianrun Chen, Kaicheng Yu, Hujun Bao, Xiaowei Zhou</author><pubDate>Mon, 24 Jul 2023 17:08:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12909v1</guid></item><item><title>GridMM: Grid Memory Map for Vision-and-Language Navigation</title><link>http://arxiv.org/abs/2307.12907v1</link><description>Vision-and-language navigation (VLN) enables the agent to navigate to aremote location following the natural language instruction in 3D environments.To represent the previously visited environment, most approaches for VLNimplement memory using recurrent states, topological maps, or top-down semanticmaps. In contrast to these approaches, we build the top-down egocentric anddynamically growing Grid Memory Map (i.e., GridMM) to structure the visitedenvironment. From a global perspective, historical observations are projectedinto a unified grid map in a top-down view, which can better represent thespatial relations of the environment. From a local perspective, we furtherpropose an instruction relevance aggregation method to capture fine-grainedvisual clues in each grid region. Extensive experiments are conducted on boththe REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CEdataset in the continuous environments, showing the superiority of our proposedmethod.</description><author>Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Shuqiang Jiang</author><pubDate>Mon, 24 Jul 2023 17:02:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12907v1</guid></item><item><title>An Approximation Theory for Metric Space-Valued Functions With A View Towards Deep Learning</title><link>http://arxiv.org/abs/2304.12231v2</link><description>Motivated by the developing mathematics of deep learning, we build universalfunctions approximators of continuous maps between arbitrary Polish metricspaces $\mathcal{X}$ and $\mathcal{Y}$ using elementary functions betweenEuclidean spaces as building blocks. Earlier results assume that the targetspace $\mathcal{Y}$ is a topological vector space. We overcome this limitationby ``randomization'': our approximators output discrete probability measuresover $\mathcal{Y}$. When $\mathcal{X}$ and $\mathcal{Y}$ are Polish withoutadditional structure, we prove very general qualitative guarantees; when theyhave suitable combinatorial structure, we prove quantitative guarantees forH\"{o}lder-like maps, including maps between finite graphs, solution operatorsto rough differential equations between certain Carnot groups, and continuousnon-linear operators between Banach spaces arising in inverse problems. Inparticular, we show that the required number of Dirac measures is determined bythe combinatorial structure of $\mathcal{X}$ and $\mathcal{Y}$. For barycentric$\mathcal{Y}$, including Banach spaces, $\mathbb{R}$-trees, Hadamard manifolds,or Wasserstein spaces on Polish metric spaces, our approximators reduce to$\mathcal{Y}$-valued functions. When the Euclidean approximators are neuralnetworks, our constructions generalize transformer networks, providing a newprobabilistic viewpoint of geometric deep learning.</description><author>Anastasis Kratsios, Chong Liu, Matti Lassas, Maarten V. de Hoop, Ivan Dokmanić</author><pubDate>Mon, 24 Jul 2023 17:00:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12231v2</guid></item><item><title>QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network</title><link>http://arxiv.org/abs/2307.12906v1</link><description>Supply chain management relies on accurate backorder prediction foroptimizing inventory control, reducing costs, and enhancing customersatisfaction. However, traditional machine-learning models struggle withlarge-scale datasets and complex relationships, hindering real-world datacollection. This research introduces a novel methodological framework forsupply chain backorder prediction, addressing the challenge of handling largedatasets. Our proposed model, QAmplifyNet, employs quantum-inspired techniqueswithin a quantum-classical neural network to predict backorders effectively onshort and imbalanced datasets. Experimental evaluations on a benchmark datasetdemonstrate QAmplifyNet's superiority over classical models, quantum ensembles,quantum neural networks, and deep reinforcement learning. Its proficiency inhandling short, imbalanced datasets makes it an ideal solution for supply chainmanagement. To enhance model interpretability, we use Explainable ArtificialIntelligence techniques. Practical implications include improved inventorycontrol, reduced backorders, and enhanced operational efficiency. QAmplifyNetseamlessly integrates into real-world supply chain management systems, enablingproactive decision-making and efficient resource allocation. Future workinvolves exploring additional quantum-inspired techniques, expanding thedataset, and investigating other supply chain applications. This researchunlocks the potential of quantum computing in supply chain optimization andpaves the way for further exploration of quantum-inspired machine learningmodels in supply chain management. Our framework and QAmplifyNet model offer abreakthrough approach to supply chain backorder prediction, providing superiorperformance and opening new avenues for leveraging quantum-inspired techniquesin supply chain management.</description><author>Md Abrar Jahin, Md Sakib Hossain Shovon, Md. Saiful Islam, Jungpil Shin, M. F. Mridha, Yuichi Okuyama</author><pubDate>Mon, 24 Jul 2023 16:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12906v1</guid></item><item><title>Universal Approximation Theorem and error bounds for quantum neural networks and quantum reservoirs</title><link>http://arxiv.org/abs/2307.12904v1</link><description>Universal approximation theorems are the foundations of classical neuralnetworks, providing theoretical guarantees that the latter are able toapproximate maps of interest. Recent results have shown that this can also beachieved in a quantum setting, whereby classical functions can be approximatedby parameterised quantum circuits. We provide here precise error bounds forspecific classes of functions and extend these results to the interesting newsetup of randomised quantum circuits, mimicking classical reservoir neuralnetworks. Our results show in particular that a quantum neural network with$\mathcal{O}(\varepsilon^{-2})$ weights and $\mathcal{O} (\lceil\log_2(\varepsilon^{-1}) \rceil)$ qubits suffices to achieve accuracy$\varepsilon&gt;0$ when approximating functions with integrable Fourier transform.</description><author>Lukas Gonon, Antoine Jacquier</author><pubDate>Mon, 24 Jul 2023 16:52:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12904v1</guid></item><item><title>Towards Bridging the FL Performance-Explainability Trade-Off: A Trustworthy 6G RAN Slicing Use-Case</title><link>http://arxiv.org/abs/2307.12903v1</link><description>In the context of sixth-generation (6G) networks, where diverse networkslices coexist, the adoption of AI-driven zero-touch management andorchestration (MANO) becomes crucial. However, ensuring the trustworthiness ofAI black-boxes in real deployments is challenging. Explainable AI (XAI) toolscan play a vital role in establishing transparency among the stakeholders inthe slicing ecosystem. But there is a trade-off between AI performance andexplainability, posing a dilemma for trustworthy 6G network slicing because thestakeholders require both highly performing AI models for efficient resourceallocation and explainable decision-making to ensure fairness, accountability,and compliance. To balance this trade off and inspired by the closed loopautomation and XAI methodologies, this paper presents a novelexplanation-guided in-hoc federated learning (FL) approach where a constrainedresource allocation model and an explainer exchange -- in a closed loop (CL)fashion -- soft attributions of the features as well as inference predictionsto achieve a transparent 6G network slicing resource management in a RAN-Edgesetup under non-independent identically distributed (non-IID) datasets. Inparticular, we quantitatively validate the faithfulness of the explanations viathe so-called attribution-based confidence metric that is included as aconstraint to guide the overall training process in the run-time FLoptimization task. In this respect, Integrated-Gradient (IG) as well as Input$\times$ Gradient and SHAP are used to generate the attributions for ourproposed in-hoc scheme, wherefore simulation results under different methodsconfirm its success in tackling the performance-explainability trade-off andits superiority over the unconstrained Integrated-Gradient post-hoc FLbaseline.</description><author>Swastika Roy, Hatim Chergui, Christos Verikoukis</author><pubDate>Mon, 24 Jul 2023 16:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12903v1</guid></item><item><title>Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data</title><link>http://arxiv.org/abs/2206.02909v2</link><description>Advances in deep learning for human activity recognition have been relativelylimited due to the lack of large labelled datasets. In this study, we leverageself-supervised learning techniques on the UK-Biobank activity trackerdataset--the largest of its kind to date--containing more than 700,000person-days of unlabelled wearable sensor data. Our resulting activityrecognition model consistently outperformed strong baselines across sevenbenchmark datasets, with an F1 relative improvement of 2.5%-100% (median18.4%), the largest improvements occurring in the smaller datasets. In contrastto previous studies, our results generalise across external datasets, devices,and environments. Our open-source model will help researchers and developers tobuild customisable and generalisable activity classifiers with highperformance.</description><author>Hang Yuan, Shing Chan, Andrew P. Creagh, Catherine Tong, David A. Clifton, Aiden Doherty</author><pubDate>Mon, 24 Jul 2023 16:47:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.02909v2</guid></item><item><title>Automotive Object Detection via Learning Sparse Events by Temporal Dynamics of Spiking Neurons</title><link>http://arxiv.org/abs/2307.12900v1</link><description>Event-based sensors, with their high temporal resolution (1us) and dynamicalrange (120dB), have the potential to be deployed in high-speed platforms suchas vehicles and drones. However, the highly sparse and fluctuating nature ofevents poses challenges for conventional object detection techniques based onArtificial Neural Networks (ANNs). In contrast, Spiking Neural Networks (SNNs)are well-suited for representing event-based data due to their inherenttemporal dynamics. In particular, we demonstrate that the membrane potentialdynamics can modulate network activity upon fluctuating events and strengthenfeatures of sparse input. In addition, the spike-triggered adaptive thresholdcan stabilize training which further improves network performance. Based onthis, we develop an efficient spiking feature pyramid network for event-basedobject detection. Our proposed SNN outperforms previous SNNs and sophisticatedANNs with attention mechanisms, achieving a mean average precision (map50) of47.7% on the Gen1 benchmark dataset. This result significantly surpasses theprevious best SNN by 9.7% and demonstrates the potential of SNNs forevent-based vision. Our model has a concise architecture while maintaining highaccuracy and much lower computation cost as a result of sparse computation. Ourcode will be publicly available.</description><author>Hu Zhang, Luziwei Leng, Kaiwei Che, Qian Liu, Jie Cheng, Qinghai Guo, Jiangxing Liao, Ran Cheng</author><pubDate>Mon, 24 Jul 2023 16:47:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12900v1</guid></item><item><title>As Time Goes By: Adding a Temporal Dimension Towards Resolving Delegations in Liquid Democracy</title><link>http://arxiv.org/abs/2307.12898v1</link><description>In recent years, the study of various models and questions related to LiquidDemocracy has been of growing interest among the community of ComputationalSocial Choice. A concern that has been raised, is that current academicliterature focuses solely on static inputs, concealing a key characteristic ofLiquid Democracy: the right for a voter to change her mind as time goes by,regarding her options of whether to vote herself or delegate her vote to otherparticipants, till the final voting deadline. In real life, a period ofextended deliberation preceding the election-day motivates voters to adapttheir behaviour over time, either based on observations of the remainingelectorate or on information acquired for the topic at hand. By adding atemporal dimension to Liquid Democracy, such adaptations can increase thenumber of possible delegation paths and reduce the loss of votes due todelegation cycles or delegating paths towards abstaining agents, ultimatelyenhancing participation. Our work takes a first step to integrate a timehorizon into decision-making problems in Liquid Democracy systems. Ourapproach, via a computational complexity analysis, exploits concepts and toolsfrom temporal graph theory which turn out to be convenient for our framework.</description><author>Evangelos Markakis, Georgios Papasotiropoulos</author><pubDate>Mon, 24 Jul 2023 16:46:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12898v1</guid></item><item><title>Anytime Model Selection in Linear Bandits</title><link>http://arxiv.org/abs/2307.12897v1</link><description>Model selection in the context of bandit optimization is a challengingproblem, as it requires balancing exploration and exploitation not only foraction selection, but also for model selection. One natural approach is to relyon online learning algorithms that treat different models as experts. Existingmethods, however, scale poorly ($\text{poly}M$) with the number of models $M$in terms of their regret. Our key insight is that, for model selection inlinear bandits, we can emulate full-information feedback to the online learnerwith a favorable bias-variance trade-off. This allows us to develop ALEXP,which has an exponentially improved ($\log M$) dependence on $M$ for itsregret. ALEXP has anytime guarantees on its regret, and neither requiresknowledge of the horizon $n$, nor relies on an initial purely exploratorystage. Our approach utilizes a novel time-uniform analysis of the Lasso,establishing a new connection between online learning and high-dimensionalstatistics.</description><author>Parnian Kassraie, Aldo Pacchiano, Nicolas Emmenegger, Andreas Krause</author><pubDate>Mon, 24 Jul 2023 16:44:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12897v1</guid></item><item><title>Corrections of Zipf's and Heaps' Laws Derived from Hapax Rate Models</title><link>http://arxiv.org/abs/2307.12896v1</link><description>The article introduces corrections to Zipf's and Heaps' laws based onsystematic models of the hapax rate. The derivation rests on two assumptions:The first one is the standard urn model which predicts that marginal frequencydistributions for shorter texts look as if word tokens were sampled blindlyfrom a given longer text. The second assumption posits that the rate of hapaxesis a simple function of the text size. Four such functions are discussed: theconstant model, the Davis model, the linear model, and the logistic model. Itis shown that the logistic model yields the best fit.</description><author>Łukasz Dębowski</author><pubDate>Mon, 24 Jul 2023 16:44:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12896v1</guid></item><item><title>A Statistical View of Column Subset Selection</title><link>http://arxiv.org/abs/2307.12892v1</link><description>We consider the problem of selecting a small subset of representativevariables from a large dataset. In the computer science literature, thisdimensionality reduction problem is typically formalized as Column SubsetSelection (CSS). Meanwhile, the typical statistical formalization is to find aninformation-maximizing set of Principal Variables. This paper shows that thesetwo approaches are equivalent, and moreover, both can be viewed as maximumlikelihood estimation within a certain semi-parametric model. Using theseconnections, we show how to efficiently (1) perform CSS using only summarystatistics from the original dataset; (2) perform CSS in the presence ofmissing and/or censored data; and (3) select the subset size for CSS in ahypothesis testing framework.</description><author>Anav Sood, Trevor Hastie</author><pubDate>Mon, 24 Jul 2023 16:42:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12892v1</guid></item><item><title>PubMed and Beyond: Recent Advances and Best Practices in Biomedical Literature Search</title><link>http://arxiv.org/abs/2307.09683v2</link><description>Biomedical research yields a wealth of information, much of which is onlyaccessible through the literature. Consequently, literature search is anessential tool for building on prior knowledge in clinical and biomedicalresearch. Although recent improvements in artificial intelligence have expandedfunctionality beyond keyword-based search, these advances may be unfamiliar toclinicians and researchers. In response, we present a survey of literaturesearch tools tailored to both general and specific information needs inbiomedicine, with the objective of helping readers efficiently fulfill theirinformation needs. We first examine the widely used PubMed search engine,discussing recent improvements and continued challenges. We then describeliterature search tools catering to five specific information needs: 1.Identifying high-quality clinical research for evidence-based medicine. 2.Retrieving gene-related information for precision medicine and genomics. 3.Searching by meaning, including natural language questions. 4. Locating relatedarticles with literature recommendation. 5. Mining literature to discoverassociations between concepts such as diseases and genetic variants.Additionally, we cover practical considerations and best practices for choosingand using these tools. Finally, we provide a perspective on the future ofliterature search engines, considering recent breakthroughs in large languagemodels such as ChatGPT. In summary, our survey provides a comprehensive view ofbiomedical literature search functionalities with 36 publicly available tools.</description><author>Qiao Jin, Robert Leaman, Zhiyong Lu</author><pubDate>Mon, 24 Jul 2023 16:41:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09683v2</guid></item><item><title>DRL Enabled Coverage and Capacity Optimization in STAR-RIS Assisted Networks</title><link>http://arxiv.org/abs/2209.00511v2</link><description>Simultaneously transmitting and reflecting reconfigurable intelligentsurfaces (STAR-RISs) is a promising passive device that contributes to afull-space coverage via transmitting and reflecting the incident signalsimultaneously. As a new paradigm in wireless communications, how to analyzethe coverage and capacity performance of STAR-RISs becomes essential butchallenging. To solve the coverage and capacity optimization (CCO) problem inSTAR-RIS assisted networks, a multi-objective proximal policy optimization(MO-PPO) algorithm is proposed to handle long-term benefits than conventionaloptimization algorithms. To strike a balance between each objective, the MO-PPOalgorithm provides a set of optimal solutions to form a Pareto front (PF),where any solution on the PF is regarded as an optimal result. Moreover, inorder to improve the performance of the MO-PPO algorithm, two updatestrategies, i.e., action-value-based update strategy (AVUS) and lossfunction-based update strategy (LFUS), are investigated. For the AVUS, theimproved point is to integrate the action values of both coverage and capacityand then update the loss function. For the LFUS, the improved point is only toassign dynamic weights for both loss functions of coverage and capacity, whilethe weights are calculated by a min-norm solver at every update. The numericalresults demonstrated that the investigated update strategies outperform thefixed weights MO optimization algorithms in different cases, which includes adifferent number of sample grids, the number of STAR-RISs, the number ofelements in the STAR-RISs, and the size of STAR-RISs. Additionally, theSTAR-RIS assisted networks achieve better performance than conventionalwireless networks without STAR-RISs. Moreover, with the same bandwidth,millimeter wave is able to provide higher capacity than sub-6 GHz, but at acost of smaller coverage.</description><author>Xinyu Gao, Wenqiang Yi, Yuanwei Liu, Jianhua Zhang, Ping Zhang</author><pubDate>Mon, 24 Jul 2023 16:38:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.00511v2</guid></item><item><title>Classification of US Supreme Court Cases using BERT-Based Techniques</title><link>http://arxiv.org/abs/2304.08649v3</link><description>Models based on bidirectional encoder representations from transformers(BERT) produce state of the art (SOTA) results on many natural languageprocessing (NLP) tasks such as named entity recognition (NER), part-of-speech(POS) tagging etc. An interesting phenomenon occurs when classifying longdocuments such as those from the US supreme court where BERT-based models canbe considered difficult to use on a first-pass or out-of-the-box basis. In thispaper, we experiment with several BERT-based classification techniques for USsupreme court decisions or supreme court database (SCDB) and compare them withthe previous SOTA results. We then compare our results specifically with SOTAmodels for long documents. We compare our results for two classification tasks:(1) a broad classification task with 15 categories and (2) a fine-grainedclassification task with 279 categories. Our best result produces an accuracyof 80\% on the 15 broad categories and 60\% on the fine-grained 279 categorieswhich marks an improvement of 8\% and 28\% respectively from previouslyreported SOTA results.</description><author>Shubham Vatsal, Adam Meyers, John E. Ortega</author><pubDate>Mon, 24 Jul 2023 16:33:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08649v3</guid></item><item><title>Learning Optimal Prescriptive Trees from Observational Data</title><link>http://arxiv.org/abs/2108.13628v2</link><description>We consider the problem of learning an optimal prescriptive tree (i.e., aninterpretable treatment assignment policy in the form of a binary tree) ofmoderate depth, from observational data. This problem arises in numeroussocially important domains such as public health and personalized medicine,where interpretable and data-driven interventions are sought based on datagathered in deployment -- through passive collection of data -- rather thanfrom randomized trials. We propose a method for learning optimal prescriptivetrees using mixed-integer optimization (MIO) technology. We show that undermild conditions our method is asymptotically exact in the sense that itconverges to an optimal out-of-sample treatment assignment policy as the numberof historical data samples tends to infinity. Contrary to existing literature,our approach: 1) does not require data to be randomized, 2) does not imposestringent assumptions on the learned trees, and 3) has the ability to modeldomain specific constraints. Through extensive computational experiments, wedemonstrate that our asymptotic guarantees translate to significant performanceimprovements in finite samples, as well as showcase our uniquely flexiblemodeling power by incorporating budget and fairness constraints.</description><author>Nathanael Jo, Sina Aghaei, Andrés Gómez, Phebe Vayanos</author><pubDate>Mon, 24 Jul 2023 16:31:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.13628v2</guid></item><item><title>Approximate blocked Gibbs sampling for Bayesian neural networks</title><link>http://arxiv.org/abs/2208.11389v3</link><description>In this work, minibatch MCMC sampling for feedforward neural networks is mademore feasible. To this end, it is proposed to sample subgroups of parametersvia a blocked Gibbs sampling scheme. By partitioning the parameter space,sampling is possible irrespective of layer width. It is also possible toalleviate vanishing acceptance rates for increasing depth by reducing theproposal variance in deeper layers. Increasing the length of a non-convergentchain increases the predictive accuracy in classification tasks, so avoidingvanishing acceptance rates and consequently enabling longer chain runs havepractical benefits. Moreover, non-convergent chain realizations aid in thequantification of predictive uncertainty. An open problem is how to performminibatch MCMC sampling for feedforward neural networks in the presence ofaugmented data.</description><author>Theodore Papamarkou</author><pubDate>Mon, 24 Jul 2023 16:28:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.11389v3</guid></item><item><title>Generalizing similarity in noisy setups: the DIBS phenomenon</title><link>http://arxiv.org/abs/2201.12803v3</link><description>This work uncovers an interplay among data density, noise, and thegeneralization ability in similarity learning. We consider Siamese NeuralNetworks (SNNs), which are the basic form of contrastive learning, and exploretwo types of noise that can impact SNNs, Pair Label Noise (PLN) and SingleLabel Noise (SLN). Our investigation reveals that SNNs exhibit double descentbehaviour regardless of the training setup and that it is further exacerbatedby noise. We demonstrate that the density of data pairs is crucial forgeneralization. When SNNs are trained on sparse datasets with the same amountof PLN or SLN, they exhibit comparable generalization properties. However, whenusing dense datasets, PLN cases generalize worse than SLN ones in theoverparametrized region, leading to a phenomenon we call Density-Induced Breakof Similarity (DIBS). In this regime, PLN similarity violation becomesmacroscopical, corrupting the dataset to the point where complete interpolationcannot be achieved, regardless of the number of model parameters. Our analysisalso delves into the correspondence between online optimization and offlinegeneralization in similarity learning. The results show that this equivalencefails in the presence of label noise in all the scenarios considered.</description><author>Nayara Fonseca, Veronica Guidetti</author><pubDate>Mon, 24 Jul 2023 16:27:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.12803v3</guid></item><item><title>(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs</title><link>http://arxiv.org/abs/2307.10490v3</link><description>We demonstrate how images and sounds can be used for indirect prompt andinstruction injection in multi-modal LLMs. An attacker generates an adversarialperturbation corresponding to the prompt and blends it into an image or audiorecording. When the user asks the (unmodified, benign) model about theperturbed image or audio, the perturbation steers the model to output theattacker-chosen text and/or make the subsequent dialog follow the attacker'sinstruction. We illustrate this attack with several proof-of-concept examplestargeting LLaVa and PandaGPT.</description><author>Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, Vitaly Shmatikov</author><pubDate>Mon, 24 Jul 2023 16:24:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10490v3</guid></item><item><title>Transformer Training Strategies for Forecasting Multiple Load Time Series</title><link>http://arxiv.org/abs/2306.10891v2</link><description>In the smart grid of the future, accurate load forecasts on the level ofindividual clients can help to balance supply and demand locally and to preventgrid outages. While the number of monitored clients will increase with theongoing smart meter rollout, the amount of data per client will always belimited. We evaluate whether a Transformer load forecasting model benefits froma transfer learning strategy, where a global univariate model is trained on theload time series from multiple clients. In experiments with two datasetscontaining load time series from several hundred clients, we find that theglobal training strategy is superior to the multivariate and local trainingstrategies used in related work. On average, the global training strategyresults in 21.8% and 12.8% lower forecasting errors than the two otherstrategies, measured across forecasting horizons from one day to one month intothe future. A comparison to linear models, multi-layer perceptrons and LSTMsshows that Transformers are effective for load forecasting when they aretrained with the global training strategy.</description><author>Matthias Hertel, Maximilian Beichter, Benedikt Heidrich, Oliver Neumann, Benjamin Schäfer, Ralf Mikut, Veit Hagenmeyer</author><pubDate>Mon, 24 Jul 2023 16:16:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10891v2</guid></item><item><title>Data-free Black-box Attack based on Diffusion Model</title><link>http://arxiv.org/abs/2307.12872v1</link><description>Since the training data for the target model in a data-free black-box attackis not available, most recent schemes utilize GANs to generate data fortraining substitute model. However, these GANs-based schemes suffer from lowtraining efficiency as the generator needs to be retrained for each targetmodel during the substitute training process, as well as low generationquality. To overcome these limitations, we consider utilizing the diffusionmodel to generate data, and propose a data-free black-box attack scheme basedon diffusion model to improve the efficiency and accuracy of substitutetraining. Despite the data generated by the diffusion model exhibits highquality, it presents diverse domain distributions and contains many samplesthat do not meet the discriminative criteria of the target model. To furtherfacilitate the diffusion model to generate data suitable for the target model,we propose a Latent Code Augmentation (LCA) method to guide the diffusion modelin generating data. With the guidance of LCA, the data generated by thediffusion model not only meets the discriminative criteria of the target modelbut also exhibits high diversity. By utilizing this data, it is possible totrain substitute model that closely resemble the target model more efficiently.Extensive experiments demonstrate that our LCA achieves higher attack successrates and requires fewer query budgets compared to GANs-based schemes fordifferent target models.</description><author>Mingwen Shao, Lingzhuang Meng, Yuanjian Qiao, Lixu Zhang, Wangmeng Zuo</author><pubDate>Mon, 24 Jul 2023 16:10:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12872v1</guid></item><item><title>Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry</title><link>http://arxiv.org/abs/2307.12868v1</link><description>Despite the success of diffusion models (DMs), we still lack a thoroughunderstanding of their latent space. To understand the latent space$\mathbf{x}_t \in \mathcal{X}$, we analyze them from a geometrical perspective.Specifically, we utilize the pullback metric to find the local latent basis in$\mathcal{X}$ and their corresponding local tangent basis in $\mathcal{H}$, theintermediate feature maps of DMs. The discovered latent basis enablesunsupervised image editing capability through latent space traversal. Weinvestigate the discovered structure from two perspectives. First, we examinehow geometric structure evolves over diffusion timesteps. Through analysis, weshow that 1) the model focuses on low-frequency components early in thegenerative process and attunes to high-frequency details later; 2) At earlytimesteps, different samples share similar tangent spaces; and 3) The simplerdatasets that DMs trained on, the more consistent the tangent space for eachtimestep. Second, we investigate how the geometric structure changes based ontext conditioning in Stable Diffusion. The results show that 1) similar promptsyield comparable tangent spaces; and 2) the model depends less on textconditions in later timesteps. To the best of our knowledge, this paper is thefirst to present image editing through $\mathbf{x}$-space traversal and providethorough analyses of the latent structure of DMs.</description><author>Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, Youngjung Uh</author><pubDate>Mon, 24 Jul 2023 16:06:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12868v1</guid></item><item><title>Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories</title><link>http://arxiv.org/abs/2306.09224v2</link><description>We propose Encyclopedic-VQA, a large scale visual question answering (VQA)dataset featuring visual questions about detailed properties of fine-grainedcategories and instances. It contains 221k unique question+answer pairs eachmatched with (up to) 5 images, resulting in a total of 1M VQA samples.Moreover, our dataset comes with a controlled knowledge base derived fromWikipedia, marking the evidence to support each answer. Empirically, we showthat our dataset poses a hard challenge for large vision+language models asthey perform poorly on our dataset: PaLI [14] is state-of-the-art on OK-VQA[37], yet it only achieves 13.0% accuracy on our dataset. Moreover, weexperimentally show that progress on answering our encyclopedic questions canbe achieved by augmenting large models with a mechanism that retrieves relevantinformation from the knowledge base. An oracle experiment with perfectretrieval achieves 87.0% accuracy on the single-hop portion of our dataset, andan automatic retrieval-augmented prototype yields 48.8%. We believe that ourdataset enables future research on retrieval-augmented vision+language models.It is available athttps://github.com/google-research/google-research/tree/master/encyclopedic_vqa .</description><author>Thomas Mensink, Jasper Uijlings, Lluis Castrejon, Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha, André Araujo, Vittorio Ferrari</author><pubDate>Mon, 24 Jul 2023 16:05:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09224v2</guid></item><item><title>Stochastic Step-wise Feature Selection for Exponential Random Graph Models (ERGMs)</title><link>http://arxiv.org/abs/2307.12862v1</link><description>Statistical analysis of social networks provides valuable insights intocomplex network interactions across various scientific disciplines. However,accurate modeling of networks remains challenging due to the heavycomputational burden and the need to account for observed network dependencies.Exponential Random Graph Models (ERGMs) have emerged as a promising techniqueused in social network modeling to capture network dependencies byincorporating endogenous variables. Nevertheless, using ERGMs poses multiplechallenges, including the occurrence of ERGM degeneracy, which generatesunrealistic and meaningless network structures. To address these challenges andenhance the modeling of collaboration networks, we propose and test a novelapproach that focuses on endogenous variable selection within ERGMs. Our methodaims to overcome the computational burden and improve the accommodation ofobserved network dependencies, thereby facilitating more accurate andmeaningful interpretations of network phenomena in various scientific fields.We conduct empirical testing and rigorous analysis to contribute to theadvancement of statistical techniques and offer practical insights for networkanalysis.</description><author>Helal El-Zaatari, Fei Yu, Michael R Kosorok</author><pubDate>Mon, 24 Jul 2023 16:02:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12862v1</guid></item><item><title>Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data</title><link>http://arxiv.org/abs/2307.12858v1</link><description>Intracerebral hemorrhage (ICH) is the second most common and deadliest formof stroke. Despite medical advances, predicting treat ment outcomes for ICHremains a challenge. This paper proposes a novel prognostic model that utilizesboth imaging and tabular data to predict treatment outcome for ICH. Our modelis trained on observational data collected from non-randomized controlledtrials, providing reliable predictions of treatment success. Specifically, wepropose to employ a variational autoencoder model to generate a low-dimensionalprognostic score, which can effectively address the selection bias resultingfrom the non-randomized controlled trials. Importantly, we develop avariational distributions combination module that combines the information fromimaging data, non-imaging clinical data, and treatment assignment to accuratelygenerate the prognostic score. We conducted extensive experiments on areal-world clinical dataset of intracerebral hemorrhage. Our proposed methoddemonstrates a substantial improvement in treatment outcome prediction comparedto existing state-of-the-art approaches. Code is available athttps://github.com/med-air/TOP-GPM</description><author>Wenao Ma, Cheng Chen, Jill Abrigo, Calvin Hoi-Kwan Mak, Yuqi Gong, Nga Yan Chan, Chu Han, Zaiyi Liu, Qi Dou</author><pubDate>Mon, 24 Jul 2023 15:57:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12858v1</guid></item><item><title>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis</title><link>http://arxiv.org/abs/2307.12856v1</link><description>Pre-trained large language models (LLMs) have recently achieved bettergeneralization and sample efficiency in autonomous web navigation. However, theperformance on real-world websites has still suffered from (1) open domainness,(2) limited context length, and (3) lack of inductive bias on HTML. Weintroduce WebAgent, an LLM-driven agent that can complete the tasks on realwebsites following natural language instructions. WebAgent plans ahead bydecomposing instructions into canonical sub-instructions, summarizes long HTMLdocuments into task-relevant snippets, and acts on websites via generatedPython programs from those. We design WebAgent with Flan-U-PaLM, for groundedcode generation, and HTML-T5, new pre-trained LLMs for long HTML documentsusing local and global attention mechanisms and a mixture of long-spandenoising objectives, for planning and summarization. We empiricallydemonstrate that our recipe improves the success on a real website by over 50%,and that HTML-T5 is the best model to solve HTML-based tasks; achieving 14.9%higher success rate than prior SoTA on the MiniWoB web navigation benchmark andbetter accuracy on offline task planning evaluation.</description><author>Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust</author><pubDate>Mon, 24 Jul 2023 15:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12856v1</guid></item><item><title>Multiscale Video Pretraining for Long-Term Activity Forecasting</title><link>http://arxiv.org/abs/2307.12854v1</link><description>Long-term activity forecasting is an especially challenging research problembecause it requires understanding the temporal relationships between observedactions, as well as the variability and complexity of human activities. Despiterelying on strong supervision via expensive human annotations, state-of-the-artforecasting approaches often generalize poorly to unseen data. To alleviatethis issue, we propose Multiscale Video Pretraining (MVP), a novelself-supervised pretraining approach that learns robust representations forforecasting by learning to predict contextualized representations of futurevideo clips over multiple timescales. MVP is based on our observation thatactions in videos have a multiscale nature, where atomic actions typicallyoccur at a short timescale and more complex actions may span longer timescales.We compare MVP to state-of-the-art self-supervised video learning approaches ondownstream long-term forecasting tasks including long-term action anticipationand video summary prediction. Our comprehensive experiments across the Ego4Dand Epic-Kitchens-55/100 datasets demonstrate that MVP out-performsstate-of-the-art methods by significant margins. Notably, MVP obtains arelative performance gain of over 20% accuracy in video summary forecastingover existing methods.</description><author>Reuben Tan, Matthias De Lange, Michael Iuzzolino, Bryan A. Plummer, Kate Saenko, Karl Ridgeway, Lorenzo Torresani</author><pubDate>Mon, 24 Jul 2023 15:55:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12854v1</guid></item><item><title>BoxSnake: Polygonal Instance Segmentation with Box Supervision</title><link>http://arxiv.org/abs/2303.11630v3</link><description>Box-supervised instance segmentation has gained much attention as it requiresonly simple box annotations instead of costly mask or polygon annotations.However, existing box-supervised instance segmentation models mainly focus onmask-based frameworks. We propose a new end-to-end training technique, termedBoxSnake, to achieve effective polygonal instance segmentation using only boxannotations for the first time. Our method consists of two loss functions: (1)a point-based unary loss that constrains the bounding box of predicted polygonsto achieve coarse-grained segmentation; and (2) a distance-aware pairwise lossthat encourages the predicted polygons to fit the object boundaries. Comparedwith the mask-based weakly-supervised methods, BoxSnake further reduces theperformance gap between the predicted segmentation and the bounding box, andshows significant superiority on the Cityscapes dataset. The code has beenavailable publicly.</description><author>Rui Yang, Lin Song, Yixiao Ge, Xiu Li</author><pubDate>Mon, 24 Jul 2023 15:53:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11630v3</guid></item><item><title>Spatiotemporal Modeling Encounters 3D Medical Image Analysis: Slice-Shift UNet with Multi-View Fusion</title><link>http://arxiv.org/abs/2307.12853v1</link><description>As a fundamental part of computational healthcare, Computer Tomography (CT)and Magnetic Resonance Imaging (MRI) provide volumetric data, making thedevelopment of algorithms for 3D image analysis a necessity. Despite beingcomputationally cheap, 2D Convolutional Neural Networks can only extractspatial information. In contrast, 3D CNNs can extract three-dimensionalfeatures, but they have higher computational costs and latency, which is alimitation for clinical practice that requires fast and efficient models.Inspired by the field of video action recognition we propose a new 2D-basedmodel dubbed Slice SHift UNet (SSH-UNet) which encodes three-dimensionalfeatures at 2D CNN's complexity. More precisely multi-view features arecollaboratively learned by performing 2D convolutions along the threeorthogonal planes of a volume and imposing a weights-sharing mechanism. Thethird dimension, which is neglected by the 2D convolution, is reincorporated byshifting a portion of the feature maps along the slices' axis. Theeffectiveness of our approach is validated in Multi-Modality AbdominalMulti-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the CranialVault (BTCV) datasets, showing that SSH-UNet is more efficient while on par inperformance with state-of-the-art architectures.</description><author>C. I. Ugwu, S. Casarin, O. Lanz</author><pubDate>Mon, 24 Jul 2023 15:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12853v1</guid></item><item><title>Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization</title><link>http://arxiv.org/abs/2307.12851v1</link><description>This paper studies the problem of training a two-layer ReLU network forbinary classification using gradient flow with small initialization. Weconsider a training dataset with well-separated input vectors: Any pair ofinput data with the same label are positively correlated, and any pair withdifferent labels are negatively correlated. Our analysis shows that, during theearly phase of training, neurons in the first layer try to align with eitherthe positive data or the negative data, depending on its corresponding weighton the second layer. A careful analysis of the neurons' directional dynamicsallows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound onthe time it takes for all neurons to achieve good alignment with the inputdata, where $n$ is the number of data points and $\mu$ measures how well thedata are separated. After the early alignment phase, the loss converges to zeroat a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layeris approximately low-rank. Numerical experiments on the MNIST datasetillustrate our theoretical findings.</description><author>Hancheng Min, René Vidal, Enrique Mallada</author><pubDate>Mon, 24 Jul 2023 15:51:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12851v1</guid></item><item><title>Multi-View Vertebra Localization and Identification from CT Images</title><link>http://arxiv.org/abs/2307.12845v1</link><description>Accurately localizing and identifying vertebrae from CT images is crucial forvarious clinical applications. However, most existing efforts are performed on3D with cropping patch operation, suffering from the large computation costsand limited global information. In this paper, we propose a multi-view vertebralocalization and identification from CT images, converting the 3D problem intoa 2D localization and identification task on different views. Without thelimitation of the 3D cropped patch, our method can learn the multi-view globalinformation naturally. Moreover, to better capture the anatomical structureinformation from different view perspectives, a multi-view contrastive learningstrategy is developed to pre-train the backbone. Additionally, we furtherpropose a Sequence Loss to maintain the sequential structure embedded along thevertebrae. Evaluation results demonstrate that, with only two 2D networks, ourmethod can localize and identify vertebrae in CT images accurately, andoutperforms the state-of-the-art methods consistently. Our code is available athttps://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images.</description><author>Han Wu, Jiadong Zhang, Yu Fang, Zhentao Liu, Nizhuan Wang, Zhiming Cui, Dinggang Shen</author><pubDate>Mon, 24 Jul 2023 15:43:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12845v1</guid></item><item><title>Coupling a Recurrent Neural Network to SPAD TCSPC Systems for Real-time Fluorescence Lifetime Imaging</title><link>http://arxiv.org/abs/2306.15599v2</link><description>Fluorescence lifetime imaging (FLI) has been receiving increased attention inrecent years as a powerful diagnostic technique in biological and medicalresearch. However, existing FLI systems often suffer from a tradeoff betweenprocessing speed, accuracy, and robustness. In this paper, we propose a robustapproach that enables fast FLI with no degradation of accuracy. The approach isbased on a SPAD TCSPC system coupled to a recurrent neural network (RNN) thataccurately estimates the fluorescence lifetime directly from raw timestampswithout building histograms, thereby drastically reducing transfer data volumesand hardware resource utilization, thus enabling FLI acquisition at video rate.We train two variants of the RNN on a synthetic dataset and compare the resultsto those obtained using center-of-mass method (CMM) and least squares fitting(LS fitting). Results demonstrate that two RNN variants, gated recurrent unit(GRU) and long short-term memory (LSTM), are comparable to CMM and LS fittingin terms of accuracy, while outperforming them in background noise by a largemargin. To explore the ultimate limits of the approach, we derived theCramer-Rao lower bound of the measurement, showing that RNN yields lifetimeestimations with near-optimal precision. Moreover, our FLI model, which ispurely trained on synthetic datasets, works well with never-seen-before,real-world data. To demonstrate real-time operation, we have built a FLImicroscope based on Piccolo, a 32x32 SPAD sensor developed in our lab. Fourquantized GRU cores, capable of processing up to 4 million photons per second,are deployed on a Xilinx Kintex-7 FPGA. Powered by the GRU, the FLI setup canretrieve real-time fluorescence lifetime images at up to 10 frames per second.The proposed FLI system is promising and ideally suited for biomedicalapplications.</description><author>Yang Lin, Paul Mos, Andrei Ardelean, Claudio Bruschini, Edoardo Charbon</author><pubDate>Mon, 24 Jul 2023 15:41:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15599v2</guid></item><item><title>Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials</title><link>http://arxiv.org/abs/2307.12840v1</link><description>We study the problem of PAC learning a linear combination of $k$ ReLUactivations under the standard Gaussian distribution on $\mathbb{R}^d$ withrespect to the square loss. Our main result is an efficient algorithm for thislearning task with sample and computational complexity $(dk/\epsilon)^{O(k)}$,where $\epsilon&gt;0$ is the target accuracy. Prior work had given an algorithmfor this problem with complexity $(dk/\epsilon)^{h(k)}$, where the function$h(k)$ scales super-polynomially in $k$. Interestingly, the complexity of ouralgorithm is near-optimal within the class of Correlational Statistical Queryalgorithms. At a high-level, our algorithm uses tensor decomposition toidentify a subspace such that all the $O(k)$-order moments are small in theorthogonal directions. Its analysis makes essential use of the theory of Schurpolynomials to show that the higher-moment error tensors are small given thatthe lower-order ones are.</description><author>Ilias Diakonikolas, Daniel M. Kane</author><pubDate>Mon, 24 Jul 2023 15:37:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12840v1</guid></item><item><title>Towards Saner Deep Image Registration</title><link>http://arxiv.org/abs/2307.09696v2</link><description>With recent advances in computing hardware and surges of deep-learningarchitectures, learning-based deep image registration methods have surpassedtheir traditional counterparts, in terms of metric performance and inferencetime. However, these methods focus on improving performance measurements suchas Dice, resulting in less attention given to model behaviors that are equallydesirable for registrations, especially for medical imaging. This paperinvestigates these behaviors for popular learning-based deep registrationsunder a sanity-checking microscope. We find that most existing registrationssuffer from low inverse consistency and nondiscrimination of identical pairsdue to overly optimized image similarities. To rectify these behaviors, wepropose a novel regularization-based sanity-enforcer method that imposes twosanity checks on the deep model to reduce its inverse consistency errors andincrease its discriminative power simultaneously. Moreover, we derive a set oftheoretical guarantees for our sanity-checked image registration method, withexperimental results supporting our theoretical findings and theireffectiveness in increasing the sanity of models without sacrificing anyperformance. Our code and models are available athttps://github.com/tuffr5/Saner-deep-registration.</description><author>Bin Duan, Ming Zhong, Yan Yan</author><pubDate>Mon, 24 Jul 2023 15:36:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09696v2</guid></item><item><title>EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge: Mixed Sequences Prediction</title><link>http://arxiv.org/abs/2307.12837v1</link><description>This report presents the technical details of our approach for theEPIC-Kitchens-100 Unsupervised Domain Adaptation (UDA) Challenge in ActionRecognition. Our approach is based on the idea that the order in which actionsare performed is similar between the source and target domains. Based on this,we generate a modified sequence by randomly combining actions from the sourceand target domains. As only unlabelled target data are available under the UDAsetting, we use a standard pseudo-labeling strategy for extracting actionlabels for the target. We then ask the network to predict the resulting actionsequence. This allows to integrate information from both domains duringtraining and to achieve better transfer results on target. Additionally, tobetter incorporate sequence information, we use a language model to filterunlikely sequences. Lastly, we employed a co-occurrence matrix to eliminateunseen combinations of verbs and nouns. Our submission, labeled as 'sshayan',can be found on the leaderboard, where it currently holds the 2nd position for'verb' and the 4th position for both 'noun' and 'action'.</description><author>Amirshayan Nasirimajd, Simone Alberto Peirone, Chiara Plizzari, Barbara Caputo</author><pubDate>Mon, 24 Jul 2023 15:35:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12837v1</guid></item><item><title>Joint Dropout: Improving Generalizability in Low-Resource Neural Machine Translation through Phrase Pair Variables</title><link>http://arxiv.org/abs/2307.12835v1</link><description>Despite the tremendous success of Neural Machine Translation (NMT), itsperformance on low-resource language pairs still remains subpar, partly due tothe limited ability to handle previously unseen inputs, i.e., generalization.In this paper, we propose a method called Joint Dropout, that addresses thechallenge of low-resource neural machine translation by substituting phraseswith variables, resulting in significant enhancement of compositionality, whichis a key aspect of generalization. We observe a substantial improvement intranslation quality for language pairs with minimal resources, as seen in BLEUand Direct Assessment scores. Furthermore, we conduct an error analysis, andfind Joint Dropout to also enhance generalizability of low-resource NMT interms of robustness and adaptability across different domains</description><author>Ali Araabi, Vlad Niculae, Christof Monz</author><pubDate>Mon, 24 Jul 2023 15:33:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12835v1</guid></item><item><title>Automated patent extraction powers generative modeling in focused chemical spaces</title><link>http://arxiv.org/abs/2303.08272v3</link><description>Deep generative models have emerged as an exciting avenue for inversemolecular design, with progress coming from the interplay between trainingalgorithms and molecular representations. One of the key challenges in theirapplicability to materials science and chemistry has been the lack of access tosizeable training datasets with property labels. Published patents contain thefirst disclosure of new materials prior to their publication in journals, andare a vast source of scientific knowledge that has remained relatively untappedin the field of data-driven molecular design. Because patents are filed seekingto protect specific uses, molecules in patents can be considered to be weaklylabeled into application classes. Furthermore, patents published by the USPatent and Trademark Office (USPTO) are downloadable and have machine-readabletext and molecular structures. In this work, we train domain-specificgenerative models using patent data sources by developing an automated pipelineto go from USPTO patent digital files to the generation of novel candidateswith minimal human intervention. We test the approach on two in-class extracteddatasets, one in organic electronics and another in tyrosine kinase inhibitors.We then evaluate the ability of generative models trained on these in-classdatasets on two categories of tasks (distribution learning and propertyoptimization), identify strengths and limitations, and suggest possibleexplanations and remedies that could be used to overcome these in practice.</description><author>Akshay Subramanian, Kevin P. Greenman, Alexis Gervaix, Tzuhsiung Yang, Rafael Gómez-Bombarelli</author><pubDate>Mon, 24 Jul 2023 15:28:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08272v3</guid></item><item><title>End-to-End Deep Transfer Learning for Calibration-free Motor Imagery Brain Computer Interfaces</title><link>http://arxiv.org/abs/2307.12827v1</link><description>A major issue in Motor Imagery Brain-Computer Interfaces (MI-BCIs) is theirpoor classification accuracy and the large amount of data that is required forsubject-specific calibration. This makes BCIs less accessible to general usersin out-of-the-lab applications. This study employed deep transfer learning fordevelopment of calibration-free subject-independent MI-BCI classifiers. Unlikeearlier works that applied signal preprocessing and feature engineering stepsin transfer learning, this study adopted an end-to-end deep learning approachon raw EEG signals. Three deep learning models (MIN2Net, EEGNet andDeepConvNet) were trained and compared using an openly available dataset. Thedataset contained EEG signals from 55 subjects who conducted a left- vs.right-hand motor imagery task. To evaluate the performance of each model, aleave-one-subject-out cross validation was used. The results of the modelsdiffered significantly. MIN2Net was not able to differentiate right- vs.left-hand motor imagery of new users, with a median accuracy of 51.7%. Theother two models performed better, with median accuracies of 62.5% for EEGNetand 59.2% for DeepConvNet. These accuracies do not reach the required thresholdof 70% needed for significant control, however, they are similar to theaccuracies of these models when tested on other datasets without transferlearning.</description><author>Maryam Alimardani, Steven Kocken, Nikki Leeuwis</author><pubDate>Mon, 24 Jul 2023 15:24:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12827v1</guid></item><item><title>Learning when to observe: A frugal reinforcement learning framework for a high-cost world</title><link>http://arxiv.org/abs/2307.02620v2</link><description>Reinforcement learning (RL) has been shown to learn sophisticated controlpolicies for complex tasks including games, robotics, heating and coolingsystems and text generation. The action-perception cycle in RL, however,generally assumes that a measurement of the state of the environment isavailable at each time step without a cost. In applications such as materialsdesign, deep-sea and planetary robot exploration and medicine, however, therecan be a high cost associated with measuring, or even approximating, the stateof the environment. In this paper, we survey the recently growing literaturethat adopts the perspective that an RL agent might not need, or even want, acostly measurement at each time step. Within this context, we propose the DeepDynamic Multi-Step Observationless Agent (DMSOA), contrast it with theliterature and empirically evaluate it on OpenAI gym and Atari Pongenvironments. Our results, show that DMSOA learns a better policy with fewerdecision steps and measurements than the considered alternative from theliterature. The corresponding code is available at:\url{https://github.com/cbellinger27/Learning-when-to-observe-in-RL</description><author>Colin Bellinger, Mark Crowley, Isaac Tamblyn</author><pubDate>Mon, 24 Jul 2023 15:21:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02620v2</guid></item><item><title>User Tampering in Reinforcement Learning Recommender Systems</title><link>http://arxiv.org/abs/2109.04083v3</link><description>In this paper, we introduce new formal methods and provide empirical evidenceto highlight a unique safety concern prevalent in reinforcement learning(RL)-based recommendation algorithms -- 'user tampering.' User tampering is asituation where an RL-based recommender system may manipulate a media user'sopinions through its suggestions as part of a policy to maximize long-term userengagement. We use formal techniques from causal modeling to critically analyzeprevailing solutions proposed in the literature for implementing scalableRL-based recommendation systems, and we observe that these methods do notadequately prevent user tampering. Moreover, we evaluate existing mitigationstrategies for reward tampering issues, and show that these methods areinsufficient in addressing the distinct phenomenon of user tampering within thecontext of recommendations. We further reinforce our findings with a simulationstudy of an RL-based recommendation system focused on the dissemination ofpolitical content. Our study shows that a Q-learning algorithm consistentlylearns to exploit its opportunities to polarize simulated users with its earlyrecommendations in order to have more consistent success with subsequentrecommendations that align with this induced polarization. Our findingsemphasize the necessity for developing safer RL-based recommendation systemsand suggest that achieving such safety would require a fundamental shift in thedesign away from the approaches we have seen in the recent literature.</description><author>Charles Evans, Atoosa Kasirzadeh</author><pubDate>Mon, 24 Jul 2023 15:19:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.04083v3</guid></item><item><title>Learning Provably Robust Estimators for Inverse Problems via Jittering</title><link>http://arxiv.org/abs/2307.12822v1</link><description>Deep neural networks provide excellent performance for inverse problems suchas denoising. However, neural networks can be sensitive to adversarial orworst-case perturbations. This raises the question of whether such networks canbe trained efficiently to be worst-case robust. In this paper, we investigatewhether jittering, a simple regularization technique that adds isotropicGaussian noise during training, is effective for learning worst-case robustestimators for inverse problems. While well studied for prediction inclassification tasks, the effectiveness of jittering for inverse problems hasnot been systematically investigated. In this paper, we present a novelanalytical characterization of the optimal $\ell_2$-worst-case robust estimatorfor linear denoising and show that jittering yields optimal robust denoisers.Furthermore, we examine jittering empirically via training deep neural networks(U-nets) for natural image denoising, deconvolution, and accelerated magneticresonance imaging (MRI). The results show that jittering significantly enhancesthe worst-case robustness, but can be suboptimal for inverse problems beyonddenoising. Moreover, our results imply that training on real data which oftencontains slight noise is somewhat robustness enhancing.</description><author>Anselm Krainovic, Mahdi Soltanolkotabi, Reinhard Heckel</author><pubDate>Mon, 24 Jul 2023 15:19:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12822v1</guid></item><item><title>CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks</title><link>http://arxiv.org/abs/2307.02813v2</link><description>Dynamic graph data mining has gained popularity in recent years due to therich information contained in dynamic graphs and their widespread use in thereal world. Despite the advances in dynamic graph neural networks (DGNNs), therich information and diverse downstream tasks have posed significantdifficulties for the practical application of DGNNs in industrial scenarios. Tothis end, in this paper, we propose to address them by pre-training and presentthe Contrastive Pre-Training Method for Dynamic Graph Neural Networks (CPDG).CPDG tackles the challenges of pre-training for DGNNs, including generalizationcapability and long-short term modeling capability, through a flexiblestructural-temporal subgraph sampler along with structural-temporal contrastivepre-training schemes. Extensive experiments conducted on both large-scaleresearch and industrial dynamic graph datasets show that CPDG outperformsexisting methods in dynamic graph pre-training for various downstream tasksunder three transfer settings.</description><author>Yuanchen Bei, Hao Xu, Sheng Zhou, Huixuan Chi, Haishuai Wang, Mengdi Zhang, Zhao Li, Jiajun Bu</author><pubDate>Mon, 24 Jul 2023 15:17:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02813v2</guid></item><item><title>Exposing the Troublemakers in Described Object Detection</title><link>http://arxiv.org/abs/2307.12813v1</link><description>Detecting objects based on language descriptions is a popular task thatincludes Open-Vocabulary object Detection (OVD) and Referring ExpressionComprehension (REC). In this paper, we advance them to a more practical settingcalled Described Object Detection (DOD) by expanding category names to flexiblelanguage expressions for OVD and overcoming the limitation of REC to onlygrounding the pre-existing object. We establish the research foundation for DODtasks by constructing a Description Detection Dataset ($D^3$), featuringflexible language expressions and annotating all described objects withoutomission. By evaluating previous SOTA methods on $D^3$, we find sometroublemakers that fail current REC, OVD, and bi-functional methods. RECmethods struggle with confidence scores, rejecting negative instances, andmulti-target scenarios, while OVD methods face constraints with long andcomplex descriptions. Recent bi-functional methods also do not work well on DODdue to their separated training procedures and inference strategies for REC andOVD tasks. Building upon the aforementioned findings, we propose a baselinethat largely improves REC methods by reconstructing the training data andintroducing a binary classification sub-task, outperforming existing methods.Data and code is available at https://github.com/shikras/d-cube.</description><author>Chi Xie, Zhao Zhang, Yixuan Wu, Feng Zhu, Rui Zhao, Shuang Liang</author><pubDate>Mon, 24 Jul 2023 15:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12813v1</guid></item><item><title>Compound Attention and Neighbor Matching Network for Multi-contrast MRI Super-resolution</title><link>http://arxiv.org/abs/2307.02148v2</link><description>Multi-contrast magnetic resonance imaging (MRI) reflects information abouthuman tissue from different perspectives and has many clinical applications. Byutilizing the complementary information among different modalities,multi-contrast super-resolution (SR) of MRI can achieve better results thansingle-image super-resolution. However, existing methods of multi-contrast MRISR have the following shortcomings that may limit their performance: First,existing methods either simply concatenate the reference and degraded featuresor exploit global feature-matching between them, which are unsuitable formulti-contrast MRI SR. Second, although many recent methods employ transformersto capture long-range dependencies in the spatial dimension, they neglect thatself-attention in the channel dimension is also important for low-level visiontasks. To address these shortcomings, we proposed a novel network architecturewith compound-attention and neighbor matching (CANM-Net) for multi-contrast MRISR: The compound self-attention mechanism effectively captures the dependenciesin both spatial and channel dimension; the neighborhood-based feature-matchingmodules are exploited to match degraded features and adjacent referencefeatures and then fuse them to obtain the high-quality images. We conductexperiments of SR tasks on the IXI, fastMRI, and real-world scanning datasets.The CANM-Net outperforms state-of-the-art approaches in both retrospective andprospective experiments. Moreover, the robustness study in our work shows thatthe CANM-Net still achieves good performance when the reference and degradedimages are imperfectly registered, proving good potential in clinicalapplications.</description><author>Wenxuan Chen, Sirui Wu, Shuai Wang, Zhongsen Li, Jia Yang, Huifeng Yao, Xiaomeng Li, Xiaolei Song</author><pubDate>Mon, 24 Jul 2023 14:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02148v2</guid></item><item><title>Guidance in Radiology Report Summarization: An Empirical Evaluation and Error Analysis</title><link>http://arxiv.org/abs/2307.12803v1</link><description>Automatically summarizing radiology reports into a concise impression canreduce the manual burden of clinicians and improve the consistency ofreporting. Previous work aimed to enhance content selection and factualitythrough guided abstractive summarization. However, two key issues persist.First, current methods heavily rely on domain-specific resources to extract theguidance signal, limiting their transferability to domains and languages wherethose resources are unavailable. Second, while automatic metrics like ROUGEshow progress, we lack a good understanding of the errors and failure modes inthis task. To bridge these gaps, we first propose a domain-agnostic guidancesignal in form of variable-length extractive summaries. Our empirical resultson two English benchmarks demonstrate that this guidance signal improves uponunguided summarization while being competitive with domain-specific methods.Additionally, we run an expert evaluation of four systems according to ataxonomy of 11 fine-grained errors. We find that the most pressing differencesbetween automatic summaries and those of radiologists relate to contentselection including omissions (up to 52%) and additions (up to 57%). Wehypothesize that latent reporting factors and corpus-level inconsistencies maylimit models to reliably learn content selection from the available data,presenting promising directions for future work.</description><author>Jan Trienes, Paul Youssef, Jörg Schlötterer, Christin Seifert</author><pubDate>Mon, 24 Jul 2023 14:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12803v1</guid></item><item><title>Improving Cross-Modal Retrieval with Set of Diverse Embeddings</title><link>http://arxiv.org/abs/2211.16761v3</link><description>Cross-modal retrieval across image and text modalities is a challenging taskdue to its inherent ambiguity: An image often exhibits various situations, anda caption can be coupled with diverse images. Set-based embedding has beenstudied as a solution to this problem. It seeks to encode a sample into a setof different embedding vectors that capture different semantics of the sample.In this paper, we present a novel set-based embedding method, which is distinctfrom previous work in two aspects. First, we present a new similarity functioncalled smooth-Chamfer similarity, which is designed to alleviate the sideeffects of existing similarity functions for set-based embedding. Second, wepropose a novel set prediction module to produce a set of embedding vectorsthat effectively captures diverse semantics of input by the slot attentionmechanism. Our method is evaluated on the COCO and Flickr30K datasets acrossdifferent visual backbones, where it outperforms existing methods includingones that demand substantially larger computation at inference.</description><author>Dongwon Kim, Namyup Kim, Suha Kwak</author><pubDate>Mon, 24 Jul 2023 14:53:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16761v3</guid></item><item><title>RRAML: Reinforced Retrieval Augmented Machine Learning</title><link>http://arxiv.org/abs/2307.12798v1</link><description>The emergence of large language models (LLMs) has revolutionized machinelearning and related fields, showcasing remarkable abilities in comprehending,generating, and manipulating human language. However, their conventional usagethrough API-based text prompt submissions imposes certain limitations in termsof context constraints and external source availability. To address thesechallenges, we propose a novel framework called Reinforced Retrieval AugmentedMachine Learning (RRAML). RRAML integrates the reasoning capabilities of LLMswith supporting information retrieved by a purpose-built retriever from a vastuser-provided database. By leveraging recent advancements in reinforcementlearning, our method effectively addresses several critical challenges.Firstly, it circumvents the need for accessing LLM gradients. Secondly, ourmethod alleviates the burden of retraining LLMs for specific tasks, as it isoften impractical or impossible due to restricted access to the model and thecomputational intensity involved. Additionally we seamlessly link theretriever's task with the reasoner, mitigating hallucinations and reducingirrelevant, and potentially damaging retrieved documents. We believe that theresearch agenda outlined in this paper has the potential to profoundly impactthe field of AI, democratizing access to and utilization of LLMs for a widerange of entities.</description><author>Andrea Bacciu, Florin Cocunasu, Federico Siciliano, Fabrizio Silvestri, Nicola Tonellotto, Giovanni Trappolini</author><pubDate>Mon, 24 Jul 2023 14:51:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12798v1</guid></item><item><title>Causal Fair Machine Learning via Rank-Preserving Interventional Distributions</title><link>http://arxiv.org/abs/2307.12797v1</link><description>A decision can be defined as fair if equal individuals are treated equallyand unequals unequally. Adopting this definition, the task of designing machinelearning models that mitigate unfairness in automated decision-making systemsmust include causal thinking when introducing protected attributes. Following arecent proposal, we define individuals as being normatively equal if they areequal in a fictitious, normatively desired (FiND) world, where the protectedattribute has no (direct or indirect) causal effect on the target. We proposerank-preserving interventional distributions to define an estimand of this FiNDworld and a warping method for estimation. Evaluation criteria for both themethod and resulting model are presented and validated through simulations andempirical data. With this, we show that our warping approach effectivelyidentifies the most discriminated individuals and mitigates unfairness.</description><author>Ludwig Bothmann, Susanne Dandl, Michael Schomaker</author><pubDate>Mon, 24 Jul 2023 14:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12797v1</guid></item><item><title>Learning Temporally Extended Skills in Continuous Domains as Symbolic Actions for Planning</title><link>http://arxiv.org/abs/2207.05018v3</link><description>Problems which require both long-horizon planning and continuous controlcapabilities pose significant challenges to existing reinforcement learningagents. In this paper we introduce a novel hierarchical reinforcement learningagent which links temporally extended skills for continuous control with aforward model in a symbolic discrete abstraction of the environment's state forplanning. We term our agent SEADS for Symbolic Effect-Aware Diverse Skills. Weformulate an objective and corresponding algorithm which leads to unsupervisedlearning of a diverse set of skills through intrinsic motivation given a knownstate abstraction. The skills are jointly learned with the symbolic forwardmodel which captures the effect of skill execution in the state abstraction.After training, we can leverage the skills as symbolic actions using theforward model for long-horizon planning and subsequently execute the plan usingthe learned continuous-action control skills. The proposed algorithm learnsskills and forward models that can be used to solve complex tasks which requireboth continuous control and long-horizon planning capabilities with highsuccess rate. It compares favorably with other flat and hierarchicalreinforcement learning baseline agents and is successfully demonstrated with areal robot.</description><author>Jan Achterhold, Markus Krimmel, Joerg Stueckler</author><pubDate>Mon, 24 Jul 2023 14:46:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.05018v3</guid></item><item><title>Compact &amp; Capable: Harnessing Graph Neural Networks and Edge Convolution for Medical Image Classification</title><link>http://arxiv.org/abs/2307.12790v1</link><description>Graph-based neural network models are gaining traction in the field ofrepresentation learning due to their ability to uncover latent topologicalrelationships between entities that are otherwise challenging to identify.These models have been employed across a diverse range of domains, encompassingdrug discovery, protein interactions, semantic segmentation, and fluid dynamicsresearch. In this study, we investigate the potential of Graph Neural Networks(GNNs) for medical image classification. We introduce a novel model thatcombines GNNs and edge convolution, leveraging the interconnectedness of RGBchannel feature values to strongly represent connections between crucial graphnodes. Our proposed model not only performs on par with state-of-the-art DeepNeural Networks (DNNs) but does so with 1000 times fewer parameters, resultingin reduced training time and data requirements. We compare our GraphConvolutional Neural Network (GCNN) to pre-trained DNNs for classifyingMedMNIST dataset classes, revealing promising prospects for GNNs in medicalimage analysis. Our results also encourage further exploration of advancedgraph-based models such as Graph Attention Networks (GAT) and GraphAuto-Encoders in the medical imaging domain. The proposed model yields morereliable, interpretable, and accurate outcomes for tasks like semanticsegmentation and image classification compared to simpler GCNNs</description><author>Aryan Singh, Pepijn Van de Ven, Ciarán Eising, Patrick Denny</author><pubDate>Mon, 24 Jul 2023 14:39:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12790v1</guid></item><item><title>AdaBest: Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation</title><link>http://arxiv.org/abs/2204.13170v4</link><description>In Federated Learning (FL), a number of clients or devices collaborate totrain a model without sharing their data. Models are optimized locally at eachclient and further communicated to a central hub for aggregation. While FL isan appealing decentralized training paradigm, heterogeneity among data fromdifferent clients can cause the local optimization to drift away from theglobal objective. In order to estimate and therefore remove this drift,variance reduction techniques have been incorporated into FL optimizationrecently. However, these approaches inaccurately estimate the clients' driftand ultimately fail to remove it properly. In this work, we propose an adaptivealgorithm that accurately estimates drift across clients. In comparison toprevious works, our approach necessitates less storage and communicationbandwidth, as well as lower compute costs. Additionally, our proposedmethodology induces stability by constraining the norm of estimates for clientdrift, making it more practical for large scale FL. Experimental findingsdemonstrate that the proposed algorithm converges significantly faster andachieves higher accuracy than the baselines across various FL benchmarks.</description><author>Farshid Varno, Marzie Saghayi, Laya Rafiee Sevyeri, Sharut Gupta, Stan Matwin, Mohammad Havaei</author><pubDate>Mon, 24 Jul 2023 14:35:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.13170v4</guid></item><item><title>Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine</title><link>http://arxiv.org/abs/2307.12788v1</link><description>The 2022 Russian invasion of Ukraine was accompanied by a large-scale,pro-Russian propaganda campaign on social media. However, the strategy behindthe dissemination of propaganda has remained unclear, particularly how theonline discourse was strategically shaped by the propagandists' community.Here, we analyze the strategy of the Twitter community using an inversereinforcement learning (IRL) approach. Specifically, IRL allows us to modelonline behavior as a Markov decision process, where the goal is to infer theunderlying reward structure that guides propagandists when interacting withusers with a supporting or opposing stance toward the invasion. Thereby, we aimto understand empirically whether and how between-user interactions arestrategically used to promote the proliferation of Russian propaganda. Forthis, we leverage a large-scale dataset with 349,455 posts with pro-Russianpropaganda from 132,131 users. We show that bots and humans follow a differentstrategy: bots respond predominantly to pro-invasion messages, suggesting thatthey seek to drive virality; while messages indicating opposition primarilyelicit responses from humans, suggesting that they tend to engage in criticaldiscussions. To the best of our knowledge, this is the first study analyzingthe strategy behind propaganda from the 2022 Russian invasion of Ukrainethrough the lens of IRL.</description><author>Dominique Geissler, Stefan Feuerriegel</author><pubDate>Mon, 24 Jul 2023 14:35:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12788v1</guid></item><item><title>Deployment of Image Analysis Algorithms under Prevalence Shifts</title><link>http://arxiv.org/abs/2303.12540v2</link><description>Domain gaps are among the most relevant roadblocks in the clinicaltranslation of machine learning (ML)-based solutions for medical imageanalysis. While current research focuses on new training paradigms and networkarchitectures, little attention is given to the specific effect of prevalenceshifts on an algorithm deployed in practice. Such discrepancies between classfrequencies in the data used for a method's development/validation and that inits deployment environment(s) are of great importance, for example in thecontext of artificial intelligence (AI) democratization, as disease prevalencesmay vary widely across time and location. Our contribution is twofold. First,we empirically demonstrate the potentially severe consequences of missingprevalence handling by analyzing (i) the extent of miscalibration, (ii) thedeviation of the decision threshold from the optimum, and (iii) the ability ofvalidation metrics to reflect neural network performance on the deploymentpopulation as a function of the discrepancy between development and deploymentprevalence. Second, we propose a workflow for prevalence-aware imageclassification that uses estimated deployment prevalences to adjust a trainedclassifier to a new environment, without requiring additional annotateddeployment data. Comprehensive experiments based on a diverse set of 30 medicalclassification tasks showcase the benefit of the proposed workflow ingenerating better classifier decisions and more reliable performance estimatescompared to current practice.</description><author>Patrick Godau, Piotr Kalinowski, Evangelia Christodoulou, Annika Reinke, Minu Tizabi, Luciana Ferrer, Paul Jäger, Lena Maier-Hein</author><pubDate>Mon, 24 Jul 2023 14:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12540v2</guid></item><item><title>Is attention all you need in medical image analysis? A review</title><link>http://arxiv.org/abs/2307.12775v1</link><description>Medical imaging is a key component in clinical diagnosis, treatment planningand clinical trial design, accounting for almost 90% of all healthcare data.CNNs achieved performance gains in medical image analysis (MIA) over the lastyears. CNNs can efficiently model local pixel interactions and be trained onsmall-scale MI data. The main disadvantage of typical CNN models is that theyignore global pixel relationships within images, which limits theirgeneralisation ability to understand out-of-distribution data with different'global' information. The recent progress of Artificial Intelligence gave riseto Transformers, which can learn global relationships from data. However, fullTransformer models need to be trained on large-scale data and involvetremendous computational complexity. Attention and Transformer compartments(Transf/Attention) which can well maintain properties for modelling globalrelationships, have been proposed as lighter alternatives of full Transformers.Recently, there is an increasing trend to co-pollinate complementarylocal-global properties from CNN and Transf/Attention architectures, which ledto a new era of hybrid models. The past years have witnessed substantial growthin hybrid CNN-Transf/Attention models across diverse MIA problems. In thissystematic review, we survey existing hybrid CNN-Transf/Attention models,review and unravel key architectural designs, analyse breakthroughs, andevaluate current and future opportunities as well as challenges. We alsointroduced a comprehensive analysis framework on generalisation opportunitiesof scientific and clinical impact, based on which new data-driven domaingeneralisation and adaptation methods can be stimulated.</description><author>Giorgos Papanastasiou, Nikolaos Dikaios, Jiahao Huang, Chengjia Wang, Guang Yang</author><pubDate>Mon, 24 Jul 2023 14:24:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12775v1</guid></item><item><title>Fast Full-frame Video Stabilization with Iterative Optimization</title><link>http://arxiv.org/abs/2307.12774v1</link><description>Video stabilization refers to the problem of transforming a shaky video intoa visually pleasing one. The question of how to strike a good trade-off betweenvisual quality and computational speed has remained one of the open challengesin video stabilization. Inspired by the analogy between wobbly frames andjigsaw puzzles, we propose an iterative optimization-based learning approachusing synthetic datasets for video stabilization, which consists of twointeracting submodules: motion trajectory smoothing and full-frame outpainting.First, we develop a two-level (coarse-to-fine) stabilizing algorithm based onthe probabilistic flow field. The confidence map associated with the estimatedoptical flow is exploited to guide the search for shared regions throughbackpropagation. Second, we take a divide-and-conquer approach and propose anovel multiframe fusion strategy to render full-frame stabilized views. Animportant new insight brought about by our iterative optimization approach isthat the target video can be interpreted as the fixed point of nonlinearmapping for video stabilization. We formulate video stabilization as a problemof minimizing the amount of jerkiness in motion trajectories, which guaranteesconvergence with the help of fixed-point theory. Extensive experimental resultsare reported to demonstrate the superiority of the proposed approach in termsof computational speed and visual quality. The code will be available onGitHub.</description><author>Weiyue Zhao, Xin Li, Zhan Peng, Xianrui Luo, Xinyi Ye, Hao Lu, Zhiguo Cao</author><pubDate>Mon, 24 Jul 2023 14:24:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12774v1</guid></item><item><title>XTQA: Span-Level Explanations of the Textbook Question Answering</title><link>http://arxiv.org/abs/2011.12662v4</link><description>Textbook Question Answering (TQA) is a task that one should answer adiagram/non-diagram question given a large multi-modal context consisting ofabundant essays and diagrams. We argue that the explainability of this taskshould place students as a key aspect to be considered. To address this issue,we devise a novel architecture towards span-level eXplanations of the TQA(XTQA) based on our proposed coarse-to-fine grained algorithm, which canprovide not only the answers but also the span-level evidences to choose themfor students. This algorithm first coarsely chooses top $M$ paragraphs relevantto questions using the TF-IDF method, and then chooses top $K$ evidence spansfinely from all candidate spans within these paragraphs by computing theinformation gain of each span to questions. Experimental results shows thatXTQA significantly improves the state-of-the-art performance compared withbaselines. The source code is available athttps://github.com/keep-smile-001/opentqa</description><author>Jie Ma, Qi Chai, Jun Liu, Qingyu Yin, Pinghui Wang, Qinghua Zheng</author><pubDate>Mon, 24 Jul 2023 14:22:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.12662v4</guid></item><item><title>Detecting disturbances in network-coupled dynamical systems with machine learning</title><link>http://arxiv.org/abs/2307.12771v1</link><description>Identifying disturbances in network-coupled dynamical systems withoutknowledge of the disturbances or underlying dynamics is a problem with a widerange of applications. For example, one might want to know which nodes in thenetwork are being disturbed and identify the type of disturbance. Here wepresent a model-free method based on machine learning to identify such unknowndisturbances based only on prior observations of the system when forced by aknown training function. We find that this method is able to identify thelocations and properties of many different types of unknown disturbances usinga variety of known forcing functions. We illustrate our results both withlinear and nonlinear disturbances using food web and neuronal activity models.Finally, we discuss how to scale our method to large networks.</description><author>Per Sebastian Skardal, Juan G. Restrepo</author><pubDate>Mon, 24 Jul 2023 14:19:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12771v1</guid></item><item><title>Rényi Divergence Deep Mutual Learning</title><link>http://arxiv.org/abs/2209.05732v6</link><description>This paper revisits Deep Mutual Learning (DML), a simple yet effectivecomputing paradigm. We propose using R\'{e}nyi divergence instead of the KLdivergence, which is more flexible and tunable, to improve vanilla DML. Thismodification is able to consistently improve performance over vanilla DML withlimited additional complexity. The convergence properties of the proposedparadigm are analyzed theoretically, and Stochastic Gradient Descent with aconstant learning rate is shown to converge with $\mathcal{O}(1)$-bias in theworst case scenario for nonconvex optimization tasks. That is, learning willreach nearby local optima but continue searching within a bounded scope, whichmay help mitigate overfitting. Finally, our extensive empirical resultsdemonstrate the advantage of combining DML and R\'{e}nyi divergence, leading tofurther improvement in model generalization.</description><author>Weipeng Huang, Junjie Tao, Changbo Deng, Ming Fan, Wenqiang Wan, Qi Xiong, Guangyuan Piao</author><pubDate>Mon, 24 Jul 2023 14:15:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.05732v6</guid></item><item><title>LiDAR Meta Depth Completion</title><link>http://arxiv.org/abs/2307.12761v1</link><description>Depth estimation is one of the essential tasks to be addressed when creatingmobile autonomous systems. While monocular depth estimation methods haveimproved in recent times, depth completion provides more accurate and reliabledepth maps by additionally using sparse depth information from other sensorssuch as LiDAR. However, current methods are specifically trained for a singleLiDAR sensor. As the scanning pattern differs between sensors, every new sensorwould require re-training a specialized depth completion model, which iscomputationally inefficient and not flexible. Therefore, we propose todynamically adapt the depth completion model to the used sensor type enablingLiDAR adaptive depth completion. Specifically, we propose a meta depthcompletion network that uses data patterns derived from the data to learn atask network to alter weights of the main depth completion network to solve agiven depth completion task effectively. The method demonstrates a strongcapability to work on multiple LiDAR scanning patterns and can also generalizeto scanning patterns that are unseen during training. While using a singlemodel, our method yields significantly better results than a non-adaptivebaseline trained on different LiDAR patterns. It outperforms LiDAR-specificexpert models for very sparse cases. These advantages allow flexible deploymentof a single depth completion model on different sensors, which could also provevaluable to process the input of nascent LiDAR technology with adaptive insteadof fixed scanning patterns.</description><author>Wolfgang Boettcher, Lukas Hoyer, Ozan Unal, Dengxin Dai</author><pubDate>Mon, 24 Jul 2023 14:05:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12761v1</guid></item><item><title>Deep Learning-based Anonymization of Chest Radiographs: A Utility-preserving Measure for Patient Privacy</title><link>http://arxiv.org/abs/2209.11531v2</link><description>Robust and reliable anonymization of chest radiographs constitutes anessential step before publishing large datasets of such for research purposes.The conventional anonymization process is carried out by obscuring personalinformation in the images with black boxes and removing or replacingmeta-information. However, such simple measures retain biometric information inthe chest radiographs, allowing patients to be re-identified by a linkageattack. Therefore, there is an urgent need to obfuscate the biometricinformation appearing in the images. We propose the first deep learning-basedapproach (PriCheXy-Net) to targetedly anonymize chest radiographs whilemaintaining data utility for diagnostic and machine learning purposes. Ourmodel architecture is a composition of three independent neural networks that,when collectively used, allow for learning a deformation field that is able toimpede patient re-identification. Quantitative results on the ChestX-ray14dataset show a reduction of patient re-identification from 81.8% to 57.7% (AUC)after re-training with little impact on the abnormality classificationperformance. This indicates the ability to preserve underlying abnormalitypatterns while increasing patient privacy. Lastly, we compare our proposedanonymization approach with two other obfuscation-based methods (Privacy-Net,DP-Pix) and demonstrate the superiority of our method towards resolving theprivacy-utility trade-off for chest radiographs.</description><author>Kai Packhäuser, Sebastian Gündel, Florian Thamm, Felix Denzinger, Andreas Maier</author><pubDate>Mon, 24 Jul 2023 14:04:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.11531v2</guid></item><item><title>Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN</title><link>http://arxiv.org/abs/2307.12759v1</link><description>Call Centers have huge amount of audio data which can be used for achievingvaluable business insights and transcription of phone calls is manually tedioustask. An effective Automated Speech Recognition system can accuratelytranscribe these calls for easy search through call history for specificcontext and content allowing automatic call monitoring, improving QoS throughkeyword search and sentiment analysis. ASR for Call Center requires morerobustness as telephonic environment are generally noisy. Moreover, there aremany low-resourced languages that are on verge of extinction which can bepreserved with help of Automatic Speech Recognition Technology. Urdu is the$10^{th}$ most widely spoken language in the world, with 231,295,440 worldwidestill remains a resource constrained language in ASR. Regional call-centerconversations operate in local language, with a mix of English numbers andtechnical terms generally causing a "code-switching" problem. Hence, this paperdescribes an implementation framework of a resource efficient Automatic SpeechRecognition/ Speech to Text System in a noisy call-center environment usingChain Hybrid HMM and CNN-TDNN for Code-Switched Urdu Language. Using HybridHMM-DNN approach allowed us to utilize the advantages of Neural Network withless labelled data. Adding CNN with TDNN has shown to work better in noisyenvironment due to CNN's additional frequency dimension which captures extrainformation from noisy speech, thus improving accuracy. We collected data fromvarious open sources and labelled some of the unlabelled data after analysingits general context and content from Urdu language as well as from commonlyused words from other languages, primarily English and were able to achieve WERof 5.2% with noisy as well as clean environment in isolated words or numbers aswell as in continuous spontaneous speech.</description><author>Muhammad Danyal Khan, Raheem Ali, Arshad Aziz</author><pubDate>Mon, 24 Jul 2023 14:04:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12759v1</guid></item><item><title>Generalizable Embeddings with Cross-batch Metric Learning</title><link>http://arxiv.org/abs/2307.07620v2</link><description>Global average pooling (GAP) is a popular component in deep metric learning(DML) for aggregating features. Its effectiveness is often attributed totreating each feature vector as a distinct semantic entity and GAP as acombination of them. Albeit substantiated, such an explanation's algorithmicimplications to learn generalizable entities to represent unseen classes, acrucial DML goal, remain unclear. To address this, we formulate GAP as a convexcombination of learnable prototypes. We then show that the prototype learningcan be expressed as a recursive process fitting a linear predictor to a batchof samples. Building on that perspective, we consider two batches of disjointclasses at each iteration and regularize the learning by expressing the samplesof a batch with the prototypes that are fitted to the other batch. We validateour approach on 4 popular DML benchmarks.</description><author>Yeti Z. Gurbuz, A. Aydin Alatan</author><pubDate>Mon, 24 Jul 2023 14:03:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07620v2</guid></item><item><title>Explainable AI with counterfactual paths</title><link>http://arxiv.org/abs/2307.07764v2</link><description>Explainable AI (XAI) is an increasingly important area of research in machinelearning, which in principle aims to make black-box models transparent andinterpretable. In this paper, we propose a novel approach to XAI that usescounterfactual paths generated by conditional permutations. Our method providescounterfactual explanations by identifying alternative paths that could haveled to different outcomes. The proposed method is particularly suitable forgenerating explanations based on counterfactual paths in knowledge graphs. Byexamining hypothetical changes to the input data in the knowledge graph, we cansystematically validate the behaviour of the model and examine the features orcombination of features that are most important to the model's predictions. Ourapproach provides a more intuitive and interpretable explanation for themodel's behaviour than traditional feature weighting methods and can helpidentify and mitigate biases in the model.</description><author>Bastian Pfeifer, Mateusz Krzyzinski, Hubert Baniecki, Anna Saranti, Andreas Holzinger, Przemyslaw Biecek</author><pubDate>Mon, 24 Jul 2023 14:00:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07764v2</guid></item><item><title>Shuffled Multi-Channel Sparse Signal Recovery</title><link>http://arxiv.org/abs/2212.07368v3</link><description>Mismatches between samples and their respective channel or target commonlyarise in several real-world applications. For instance, whole-brain calciumimaging of freely moving organisms, multiple-target tracking or multi-personcontactless vital sign monitoring may be severely affected by mismatchedsample-channel assignments. To systematically address this fundamental problem,we pose it as a signal reconstruction problem where we have lostcorrespondences between the samples and their respective channels. Assumingthat we have a sensing matrix for the underlying signals, we show that theproblem is equivalent to a structured unlabeled sensing problem, and establishsufficient conditions for unique recovery. To the best of our knowledge, asampling result for the reconstruction of shuffled multi-channel signals hasnot been considered in the literature and existing methods for unlabeledsensing cannot be directly applied. We extend our results to the case where thesignals admit a sparse representation in an overcomplete dictionary (i.e., thesensing matrix is not precisely known), and derive sufficient conditions forthe reconstruction of shuffled sparse signals. We propose a robustreconstruction method that combines sparse signal recovery with robust linearregression for the two-channel case. The performance and robustness of theproposed approach is illustrated in an application related to whole-braincalcium imaging. The proposed methodology can be generalized to sparse signalrepresentations other than the ones considered in this work to be applied in avariety of real-world problems with imprecise measurement or channelassignment.</description><author>Taulant Koka, Manolis C. Tsakiris, Michael Muma, Benjamín Béjar Haro</author><pubDate>Mon, 24 Jul 2023 13:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07368v3</guid></item><item><title>Nonparametric Linear Feature Learning in Regression Through Regularisation</title><link>http://arxiv.org/abs/2307.12754v1</link><description>Representation learning plays a crucial role in automated feature selection,particularly in the context of high-dimensional data, where non-parametricmethods often struggle. In this study, we focus on supervised learningscenarios where the pertinent information resides within a lower-dimensionallinear subspace of the data, namely the multi-index model. If this subspacewere known, it would greatly enhance prediction, computation, andinterpretation. To address this challenge, we propose a novel method for linearfeature learning with non-parametric prediction, which simultaneously estimatesthe prediction function and the linear subspace. Our approach employs empiricalrisk minimisation, augmented with a penalty on function derivatives, ensuringversatility. Leveraging the orthogonality and rotation invariance properties ofHermite polynomials, we introduce our estimator, named RegFeaL. By utilisingalternative minimisation, we iteratively rotate the data to improve alignmentwith leading directions and accurately estimate the relevant dimension inpractical settings. We establish that our method yields a consistent estimatorof the prediction function with explicit rates. Additionally, we provideempirical results demonstrating the performance of RegFeaL in variousexperiments.</description><author>Bertille Follain, Umut Simsekli, Francis Bach</author><pubDate>Mon, 24 Jul 2023 13:52:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12754v1</guid></item><item><title>ICF-SRSR: Invertible scale-Conditional Function for Self-Supervised Real-world Single Image Super-Resolution</title><link>http://arxiv.org/abs/2307.12751v1</link><description>Single image super-resolution (SISR) is a challenging ill-posed problem thataims to up-sample a given low-resolution (LR) image to a high-resolution (HR)counterpart. Due to the difficulty in obtaining real LR-HR training pairs,recent approaches are trained on simulated LR images degraded by simplifieddown-sampling operators, e.g., bicubic. Such an approach can be problematic inpractice because of the large gap between the synthesized and real-world LRimages. To alleviate the issue, we propose a novel Invertible scale-ConditionalFunction (ICF), which can scale an input image and then restore the originalinput with different scale conditions. By leveraging the proposed ICF, weconstruct a novel self-supervised SISR framework (ICF-SRSR) to handle thereal-world SR task without using any paired/unpaired training data.Furthermore, our ICF-SRSR can generate realistic and feasible LR-HR pairs,which can make existing supervised SISR networks more robust. Extensiveexperiments demonstrate the effectiveness of the proposed method in handlingSISR in a fully self-supervised manner. Our ICF-SRSR demonstrates superiorperformance compared to the existing methods trained on synthetic paired imagesin real-world scenarios and exhibits comparable performance compared tostate-of-the-art supervised/unsupervised methods on public benchmark datasets.</description><author>Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, Kyoung Mu Lee</author><pubDate>Mon, 24 Jul 2023 13:42:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12751v1</guid></item><item><title>Concept-based explainability for an EEG transformer model</title><link>http://arxiv.org/abs/2307.12745v1</link><description>Deep learning models are complex due to their size, structure, and inherentrandomness in training procedures. Additional complexity arises from theselection of datasets and inductive biases. Addressing these challenges forexplainability, Kim et al. (2018) introduced Concept Activation Vectors (CAVs),which aim to understand deep models' internal states in terms of human-alignedconcepts. These concepts correspond to directions in latent space, identifiedusing linear discriminants. Although this method was first applied to imageclassification, it was later adapted to other domains, including naturallanguage processing. In this work, we attempt to apply the method toelectroencephalogram (EEG) data for explainability in Kostas et al.'s BENDR(2021), a large-scale transformer model. A crucial part of this endeavorinvolves defining the explanatory concepts and selecting relevant datasets toground concepts in the latent space. Our focus is on two mechanisms for EEGconcept formation: the use of externally labeled EEG datasets, and theapplication of anatomically defined concepts. The former approach is astraightforward generalization of methods used in image classification, whilethe latter is novel and specific to EEG. We present evidence that bothapproaches to concept formation yield valuable insights into therepresentations learned by deep EEG models.</description><author>Anders Gjølbye Madsen, William Theodor Lehn-Schiøler, Áshildur Jónsdóttir, Bergdís Arnardóttir, Lars Kai Hansen</author><pubDate>Mon, 24 Jul 2023 13:36:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12745v1</guid></item><item><title>Reducing Training Time in Cross-Silo Federated Learning using Multigraph Topology</title><link>http://arxiv.org/abs/2207.09657v3</link><description>Federated learning is an active research topic since it enables severalparticipants to jointly train a model without sharing local data. Currently,cross-silo federated learning is a popular training setting that utilizes a fewhundred reliable data silos with high-speed access links to training a model.While this approach has been widely applied in real-world scenarios, designinga robust topology to reduce the training time remains an open problem. In thispaper, we present a new multigraph topology for cross-silo federated learning.We first construct the multigraph using the overlay graph. We then parse thismultigraph into different simple graphs with isolated nodes. The existence ofisolated nodes allows us to perform model aggregation without waiting for othernodes, hence effectively reducing the training time. Intensive experiments onthree public datasets show that our proposed method significantly reduces thetraining time compared with recent state-of-the-art topologies whilemaintaining the accuracy of the learned model. Our code can be found athttps://github.com/aioz-ai/MultigraphFL</description><author>Tuong Do, Binh X. Nguyen, Vuong Pham, Toan Tran, Erman Tjiputra, Quang Tran, Anh Nguyen</author><pubDate>Mon, 24 Jul 2023 13:35:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.09657v3</guid></item><item><title>BiofilmScanner: A Computational Intelligence Approach to Obtain Bacterial Cell Morphological Attributes from Biofilm Image</title><link>http://arxiv.org/abs/2302.09629v2</link><description>Desulfovibrio alaskensis G20 (DA-G20) is utilized as a model forsulfate-reducing bacteria (SRB) that are associated with corrosion issuescaused by microorganisms. SRB-based biofilms are thought to be responsible forthe billion-dollar-per-year bio-corrosion of metal infrastructure.Understanding the extraction of the bacterial cells' shape and size propertiesin the SRB-biofilm at different growth stages will assist with the design ofanti-corrosion techniques. However, numerous issues affect current approaches,including time-consuming geometric property extraction, low efficiency, andhigh error rates. This paper proposes BiofilScanner, a Yolact-based deeplearning method integrated with invariant moments to address these problems.Our approach efficiently detects and segments bacterial cells in an SRB imagewhile simultaneously invariant moments measure the geometric characteristics ofthe segmented cells with low errors. The numerical experiments of the proposedmethod demonstrate that the BiofilmScanner is 2.1x and 6.8x faster than ourearlier Mask-RCNN and DLv3+ methods for detecting, segmenting, and measuringthe geometric properties of the cell. Furthermore, the BiofilmScanner achievedan F1-score of 85.28% while Mask-RCNN and DLv3+ obtained F1-scores of 77.67%and 75.18%, respectively.</description><author>Md Hafizur Rahman, Md Ali Azam, Md Abir Hossen, Shankarachary Ragi, Venkataramana Gadhamshetty</author><pubDate>Mon, 24 Jul 2023 13:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09629v2</guid></item><item><title>Defining data science: a new field of inquiry</title><link>http://arxiv.org/abs/2306.16177v3</link><description>Data science is not a science. It is a research paradigm. Its power, scope,and scale will surpass science, our most powerful research paradigm, to enableknowledge discovery and change our world. We have yet to understand and defineit, vital to realizing its potential and managing its risks. Modern datascience is in its infancy. Emerging slowly since 1962 and rapidly since 2000,it is a fundamentally new field of inquiry, one of the most active, powerful,and rapidly evolving 21st century innovations. Due to its value, power, andapplicability, it is emerging in over 40 disciplines, hundreds of researchareas, and thousands of applications. Millions of data science publicationscontain myriad definitions of data science and data science problem solving.Due to its infancy, many definitions are independent, application specific,mutually incomplete, redundant, or inconsistent, hence so is data science. Thisresearch addresses this data science multiple definitions challenge byproposing the development of coherent, unified definition based on a datascience reference framework using a data science journal for the data sciencecommunity to achieve such a definition. This paper provides candidatedefinitions for essential data science artifacts that are required to discusssuch a definition. They are based on the classical research paradigm conceptconsisting of a philosophy of data science, the data science problem solvingparadigm, and the six component data science reference framework (axiology,ontology, epistemology, methodology, methods, technology) that is a frequentlycalled for unifying framework with which to define, unify, and evolve datascience. It presents challenges for defining data science, solution approaches,i.e., means for defining data science, and their requirements and benefits asthe basis of a comprehensive solution.</description><author>Michael L Brodie</author><pubDate>Mon, 24 Jul 2023 13:32:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16177v3</guid></item></channel></rss>