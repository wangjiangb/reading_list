<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 03 Oct 2023 14:00:11 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Construction numbers: How to build a graph?</title><link>http://arxiv.org/abs/2302.13186v3</link><description>Counting the number of linear extensions of a partial order was considered byStanley about 50 years ago. For the partial order on the vertices and edges ofa graph determined by inclusion, we call such linear extensions {\itconstruction sequences} for the graph as each edge follows both of itsendpoints. The number of such sequences for paths, cycles, stars, double-stars,and complete graphs is found. For paths, we agree with Stanley (the Tangentnumbers) and get formulas for the other classes. Structure and applications arealso studied.</description><author>Paul C. Kainen</author><pubDate>Mon, 02 Oct 2023 18:51:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13186v3</guid></item><item><title>Vision-Language Dataset Distillation</title><link>http://arxiv.org/abs/2308.07545v2</link><description>Dataset distillation methods promise to reduce large-scale datasets down tosignificantly smaller sets of (potentially synthetic) training examples, whichpreserve sufficient information for training a new model from scratch. So far,dataset distillation methods have been developed for image classification.However, with the rise in capabilities of vision-language models (VLMs), andespecially given the scale of datasets necessary to train these models, thetime is ripe to expand dataset distillation methods beyond imageclassification. In this work, we take the first steps towards this goal byexpanding the idea of trajectory matching to create a distillation method forvision-language datasets. A key challenge is that vision-language datasets donot have a set of discrete classes. To overcome this, our proposedvision-language dataset distillation method jointly distills the image-textpairs in a contrastive formulation. Since there are no existing baselines, wecompare our approach to three coreset selection methods (strategic subsamplingof the training dataset), which we adapt to the vision-language setting. Wedemonstrate significant improvements on the challenging Flickr30K and COCOretrieval benchmarks: for example, on Flickr30K, the best coreset selectionmethod selecting 1000 image-text pairs for training achieves only 5.6%image-to-text retrieval accuracy (i.e., recall@1); in contrast, our datasetdistillation approach almost doubles that to 9.9% with just 100 (an order ofmagnitude fewer) training pairs.</description><author>Xindi Wu, Byron Zhang, Zhiwei Deng, Olga Russakovsky</author><pubDate>Mon, 02 Oct 2023 18:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07545v2</guid></item><item><title>ICML 2023 Topological Deep Learning Challenge : Design and Results</title><link>http://arxiv.org/abs/2309.15188v2</link><description>This paper presents the computational challenge on topological deep learningthat was hosted within the ICML 2023 Workshop on Topology and Geometry inMachine Learning. The competition asked participants to provide open-sourceimplementations of topological neural networks from the literature bycontributing to the python packages TopoNetX (data processing) and TopoModelX(deep learning). The challenge attracted twenty-eight qualifying submissions inits two-month duration. This paper describes the design of the challenge andsummarizes its main findings.</description><author>Mathilde Papillon, Mustafa Hajij, Florian Frantzen, Josef Hoppe, Helen Jenne, Johan Mathe, Audun Myers, Theodore Papamarkou, Michael T. Schaub, Ghada Zamzmi, Tolga Birdal, Tamal Dey, Tim Doster, Tegan Emerson, Gurusankar Gopalakrishnan, Devendra Govil, Vincent Grande, Aldo Guzmán-Sáenz, Henry Kvinge, Neal Livesay, Jan Meisner, Soham Mukherjee, Shreyas N. Samaga, Karthikeyan Natesan Ramamurthy, Maneel Reddy Karri, Paul Rosen, Sophia Sanborn, Michael Scholkemper, Robin Walters, Jens Agerberg, Georg Bökman, Sadrodin Barikbin, Claudio Battiloro, Gleb Bazhenov, Guillermo Bernardez, Aiden Brent, Sergio Escalera, Simone Fiorellino, Dmitrii Gavrilev, Mohammed Hassanin, Paul Häusner, Odin Hoff Gardaa, Abdelwahed Khamis, Manuel Lecha, German Magai, Tatiana Malygina, Pavlo Melnyk, Rubén Ballester, Ka</author><pubDate>Mon, 02 Oct 2023 18:48:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15188v2</guid></item><item><title>A Counterfactual Fair Model for Longitudinal Electronic Health Records via Deconfounder</title><link>http://arxiv.org/abs/2308.11819v3</link><description>The fairness issue of clinical data modeling, especially on Electronic HealthRecords (EHRs), is of utmost importance due to EHR's complex latent structureand potential selection bias. It is frequently necessary to mitigate healthdisparity while keeping the model's overall accuracy in practice. However,traditional methods often encounter the trade-off between accuracy andfairness, as they fail to capture the underlying factors beyond observed data.To tackle this challenge, we propose a novel model called Fair LongitudinalMedical Deconfounder (FLMD) that aims to achieve both fairness and accuracy inlongitudinal Electronic Health Records (EHR) modeling. Drawing inspiration fromthe deconfounder theory, FLMD employs a two-stage training process. In thefirst stage, FLMD captures unobserved confounders for each encounter, whicheffectively represents underlying medical factors beyond observed EHR, such aspatient genotypes and lifestyle habits. This unobserved confounder is crucialfor addressing the accuracy/fairness dilemma. In the second stage, FLMDcombines the learned latent representation with other relevant features to makepredictions. By incorporating appropriate fairness criteria, such ascounterfactual fairness, FLMD ensures that it maintains high predictionaccuracy while simultaneously minimizing health disparities. We conductedcomprehensive experiments on two real-world EHR datasets to demonstrate theeffectiveness of FLMD. Apart from the comparison of baseline methods and FLMDvariants in terms of fairness and accuracy, we assessed the performance of allmodels on disturbed/imbalanced and synthetic datasets to showcase thesuperiority of FLMD across different settings and provide valuable insightsinto its capabilities.</description><author>Zheng Liu, Xiaohan Li, Philip Yu</author><pubDate>Mon, 02 Oct 2023 18:46:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11819v3</guid></item><item><title>Efficient Deep Learning of Robust, Adaptive Policies using Tube MPC-Guided Data Augmentation</title><link>http://arxiv.org/abs/2303.15688v2</link><description>The deployment of agile autonomous systems in challenging, unstructuredenvironments requires adaptation capabilities and robustness to uncertainties.Existing robust and adaptive controllers, such as those based on modelpredictive control (MPC), can achieve impressive performance at the cost ofheavy online onboard computations. Strategies that efficiently learn robust andonboard-deployable policies from MPC have emerged, but they still lackfundamental adaptation capabilities. In this work, we extend an existingefficient Imitation Learning (IL) algorithm for robust policy learning from MPCwith the ability to learn policies that adapt to challenging model/environmentuncertainties. The key idea of our approach consists in modifying the ILprocedure by conditioning the policy on a learned lower-dimensionalmodel/environment representation that can be efficiently estimated online. Wetailor our approach to the task of learning an adaptive position and attitudecontrol policy to track trajectories under challenging disturbances on amultirotor. Evaluations in simulation show that a high-quality adaptive policycan be obtained in about $1.3$ hours. We additionally empirically demonstraterapid adaptation to in- and out-of-training-distribution uncertainties,achieving a $6.1$ cm average position error under wind disturbances thatcorrespond to about $50\%$ of the weight of the robot, and that are $36\%$larger than the maximum wind seen during training.</description><author>Tong Zhao, Andrea Tagliabue, Jonathan P. How</author><pubDate>Mon, 02 Oct 2023 18:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15688v2</guid></item><item><title>Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics</title><link>http://arxiv.org/abs/2309.06687v2</link><description>Although Deep Reinforcement Learning (DRL) has achieved notable success innumerous robotic applications, designing a high-performing reward functionremains a challenging task that often requires substantial manual input.Recently, Large Language Models (LLMs) have been extensively adopted to addresstasks demanding in-depth common-sense knowledge, such as reasoning andplanning. Recognizing that reward function design is also inherently linked tosuch knowledge, LLM offers a promising potential in this context. Motivated bythis, we propose in this work a novel LLM framework with a self-refinementmechanism for automated reward function design. The framework commences withthe LLM formulating an initial reward function based on natural languageinputs. Then, the performance of the reward function is assessed, and theresults are presented back to the LLM for guiding its self-refinement process.We examine the performance of our proposed framework through a variety ofcontinuous robotic control tasks across three diverse robotic systems. Theresults indicate that our LLM-designed reward functions are able to rival oreven surpass manually designed reward functions, highlighting the efficacy andapplicability of our approach.</description><author>Jiayang Song, Zhehua Zhou, Jiawei Liu, Chunrong Fang, Zhan Shu, Lei Ma</author><pubDate>Mon, 02 Oct 2023 18:20:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06687v2</guid></item><item><title>ChemCrow: Augmenting large-language models with chemistry tools</title><link>http://arxiv.org/abs/2304.05376v5</link><description>Over the last decades, excellent computational chemistry tools have beendeveloped. Integrating them into a single platform with enhanced accessibilitycould help reaching their full potential by overcoming steep learning curves.Recently, large-language models (LLMs) have shown strong performance in tasksacross domains, but struggle with chemistry-related problems. Moreover, thesemodels lack access to external knowledge sources, limiting their usefulness inscientific applications. In this study, we introduce ChemCrow, an LLM chemistryagent designed to accomplish tasks across organic synthesis, drug discovery,and materials design. By integrating 18 expert-designed tools, ChemCrowaugments the LLM performance in chemistry, and new capabilities emerge. Ouragent autonomously planned and executed the syntheses of an insect repellent,three organocatalysts, and guided the discovery of a novel chromophore. Ourevaluation, including both LLM and expert assessments, demonstrates ChemCrow'seffectiveness in automating a diverse set of chemical tasks. Surprisingly, wefind that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4completions and Chemcrow's performance. Our work not only aids expert chemistsand lowers barriers for non-experts, but also fosters scientific advancement bybridging the gap between experimental and computational chemistry.</description><author>Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller</author><pubDate>Mon, 02 Oct 2023 18:03:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05376v5</guid></item><item><title>Learning a Generic Value-Selection Heuristic Inside a Constraint Programming Solver</title><link>http://arxiv.org/abs/2301.01913v3</link><description>Constraint programming is known for being an efficient approach for solvingcombinatorial problems. Important design choices in a solver are the branchingheuristics, which are designed to lead the search to the best solutions in aminimum amount of time. However, developing these heuristics is atime-consuming process that requires problem-specific expertise. Thisobservation has motivated many efforts to use machine learning to automaticallylearn efficient heuristics without expert intervention. To the best of ourknowledge, it is still an open research question. Although several genericvariable-selection heuristics are available in the literature, the options fora generic value-selection heuristic are more scarce. In this paper, we proposeto tackle this issue by introducing a generic learning procedure that can beused to obtain a value-selection heuristic inside a constraint programmingsolver. This has been achieved thanks to the combination of a deep Q-learningalgorithm, a tailored reward signal, and a heterogeneous graph neural networkarchitecture. Experiments on graph coloring, maximum independent set, andmaximum cut problems show that our framework is able to find better solutionsclose to optimality without requiring a large amounts of backtracks while beinggeneric.</description><author>Tom Marty, Tristan François, Pierre Tessier, Louis Gauthier, Louis-Martin Rousseau, Quentin Cappart</author><pubDate>Mon, 02 Oct 2023 17:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.01913v3</guid></item><item><title>An Unsupervised Method for Estimating Class Separability of Datasets with Application to LLMs Fine-Tuning</title><link>http://arxiv.org/abs/2305.15016v2</link><description>This paper proposes an unsupervised method that leverages topologicalcharacteristics of data manifolds to estimate class separability of the datawithout requiring labels. Experiments conducted in this paper on severaldatasets demonstrate a clear correlation and consistency between the classseparability estimated by the proposed method with supervised metrics likeFisher Discriminant Ratio~(FDR) and cross-validation of a classifier, whichboth require labels. This can enable implementing learning paradigms aimed atlearning from both labeled and unlabeled data, like semi-supervised andtransductive learning. This would be particularly useful when we have limitedlabeled data and a relatively large unlabeled dataset that can be used toenhance the learning process. The proposed method is implemented for languagemodel fine-tuning with automated stopping criterion by monitoring classseparability of the embedding-space manifold in an unsupervised setting. Theproposed methodology has been first validated on synthetic data, where theresults show a clear consistency between class separability estimated by theproposed method and class separability computed by FDR. The method has beenalso implemented on both public and internal data. The results show that theproposed method can effectively aid -- without the need for labels -- adecision on when to stop or continue the fine-tuning of a language model andwhich fine-tuning iteration is expected to achieve a maximum classificationperformance through quantification of the class separability of the embeddingmanifold.</description><author>Najah Ghalyan, Kostis Gourgoulias, Yash Satsangi, Sean Moran, Maxime Labonne, Joseph Sabelja</author><pubDate>Mon, 02 Oct 2023 17:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15016v2</guid></item><item><title>Sample-efficient Model-based Reinforcement Learning for Quantum Control</title><link>http://arxiv.org/abs/2304.09718v2</link><description>We propose a model-based reinforcement learning (RL) approach for noisytime-dependent gate optimization with improved sample complexity overmodel-free RL. Sample complexity is the number of controller interactions withthe physical system. Leveraging an inductive bias, inspired by recent advancesin neural ordinary differential equations (ODEs), we use an auto-differentiableODE parametrised by a learnable Hamiltonian ansatz to represent the modelapproximating the environment whose time-dependent part, including the control,is fully known. Control alongside Hamiltonian learning of continuoustime-independent parameters is addressed through interactions with the system.We demonstrate an order of magnitude advantage in the sample complexity of ourmethod over standard model-free RL in preparing some standard unitary gateswith closed and open system dynamics, in realistic numerical experimentsincorporating single shot measurements, arbitrary Hilbert space truncations anduncertainty in Hamiltonian parameters. Also, the learned Hamiltonian can beleveraged by existing control methods like GRAPE for further gradient-basedoptimization with the controllers found by RL as initializations. Our algorithmthat we apply on nitrogen vacancy (NV) centers and transmons in this paper iswell suited for controlling partially characterised one and two qubit systems.</description><author>Irtaza Khalid, Carrie A. Weidner, Edmond A. Jonckheere, Sophie G. Shermer, Frank C. Langbein</author><pubDate>Mon, 02 Oct 2023 17:50:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09718v2</guid></item><item><title>MUBen: Benchmarking the Uncertainty of Molecular Representation Models</title><link>http://arxiv.org/abs/2306.10060v2</link><description>Large molecular representation models pre-trained on massive unlabeled datahave shown great success in predicting molecular properties. However, thesemodels may tend to overfit the fine-tuning data, resulting in over-confidentpredictions on test data that fall outside of the training distribution. Toaddress this issue, uncertainty quantification (UQ) methods can be used toimprove the models' calibration of predictions. Although many UQ approachesexist, not all of them lead to improved performance. While some studies haveincluded UQ to improve molecular pre-trained models, the process of selectingsuitable backbone and UQ methods for reliable molecular uncertainty estimationremains underexplored. To address this gap, we present MUBen, which evaluatesdifferent UQ methods for state-of-the-art backbone molecular representationmodels to investigate their capabilities. By fine-tuning various backbonesusing different molecular descriptors as inputs with UQ methods from differentcategories, we critically assess the influence of architectural decisions andtraining strategies. Our study offers insights for selecting UQ for backbonemodels, which can facilitate research on uncertainty-critical applications infields such as materials science and drug discovery.</description><author>Yinghao Li, Lingkai Kong, Yuanqi Du, Yue Yu, Yuchen Zhuang, Wenhao Mu, Chao Zhang</author><pubDate>Mon, 02 Oct 2023 17:44:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10060v2</guid></item><item><title>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</title><link>http://arxiv.org/abs/2304.10592v2</link><description>The recent GPT-4 has demonstrated extraordinary multi-modal abilities, suchas directly generating websites from handwritten text and identifying humorouselements within images. These features are rarely observed in previousvision-language models. However, the technical details behind GPT-4 continue toremain undisclosed. We believe that the enhanced multi-modal generationcapabilities of GPT-4 stem from the utilization of sophisticated large languagemodels (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns afrozen visual encoder with a frozen advanced LLM, Vicuna, using one projectionlayer. Our work, for the first time, uncovers that properly aligning the visualfeatures with an advanced large language model can possess numerous advancedmulti-modal abilities demonstrated by GPT-4, such as detailed image descriptiongeneration and website creation from hand-drawn drafts. Furthermore, we alsoobserve other emerging capabilities in MiniGPT-4, including writing stories andpoems inspired by given images, teaching users how to cook based on foodphotos, and so on. In our experiment, we found that the model trained on shortimage caption pairs could produce unnatural language outputs (e.g., repetitionand fragmentation). To address this problem, we curate a detailed imagedescription dataset in the second stage to finetune the model, whichconsequently improves the model's generation reliability and overall usability.Our code, pre-trained model, and collected dataset are available athttps://minigpt-4.github.io/.</description><author>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny</author><pubDate>Mon, 02 Oct 2023 17:38:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10592v2</guid></item><item><title>GeoVLN: Learning Geometry-Enhanced Visual Representation with Slot Attention for Vision-and-Language Navigation</title><link>http://arxiv.org/abs/2305.17102v2</link><description>Most existing works solving Room-to-Room VLN problem only utilize RGB imagesand do not consider local context around candidate views, which lack sufficientvisual cues about surrounding environment. Moreover, natural language containscomplex semantic information thus its correlations with visual inputs are hardto model merely with cross attention. In this paper, we propose GeoVLN, whichlearns Geometry-enhanced visual representation based on slot attention forrobust Visual-and-Language Navigation. The RGB images are compensated with thecorresponding depth maps and normal maps predicted by Omnidata as visualinputs. Technically, we introduce a two-stage module that combine local slotattention and CLIP model to produce geometry-enhanced representation from suchinput. We employ V&amp;L BERT to learn a cross-modal representation thatincorporate both language and vision informations. Additionally, a novelmultiway attention module is designed, encouraging different phrases of inputinstruction to exploit the most related features from visual input. Extensiveexperiments demonstrate the effectiveness of our newly designed modules andshow the compelling performance of the proposed method.</description><author>Jingyang Huo, Qiang Sun, Boyan Jiang, Haitao Lin, Yanwei Fu</author><pubDate>Mon, 02 Oct 2023 17:23:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17102v2</guid></item><item><title>Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals</title><link>http://arxiv.org/abs/2308.14945v3</link><description>We consider the problem of sampling from a distribution governed by apotential function. This work proposes an explicit score based MCMC method thatis deterministic, resulting in a deterministic evolution for particles ratherthan a stochastic differential equation evolution. The score term is given inclosed form by a regularized Wasserstein proximal, using a kernel convolutionthat is approximated by sampling. We demonstrate fast convergence on variousproblems and show improved dimensional dependence of mixing time bounds for thecase of Gaussian distributions compared to the unadjusted Langevin algorithm(ULA) and the Metropolis-adjusted Langevin algorithm (MALA). We additionallyderive closed form expressions for the distributions at each iterate forquadratic potential functions, characterizing the variance reduction. Empiricalresults demonstrate that the particles behave in an organized manner, lying onlevel set contours of the potential. Moreover, the posterior mean estimator ofthe proposed method is shown to be closer to the maximum a-posteriori estimatorcompared to ULA and MALA in the context of Bayesian logistic regression.Additional examples demonstrate competitive performance for Bayesian neuralnetwork training.</description><author>Hong Ye Tan, Stanley Osher, Wuchen Li</author><pubDate>Mon, 02 Oct 2023 17:08:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14945v3</guid></item><item><title>DADO -- Low-Cost Query Strategies for Deep Active Design Optimization</title><link>http://arxiv.org/abs/2307.04536v2</link><description>In this experience report, we apply deep active learning to the field ofdesign optimization to reduce the number of computationally expensive numericalsimulations. We are interested in optimizing the design of structuralcomponents, where the shape is described by a set of parameters. If we canpredict the performance based on these parameters and consider only thepromising candidates for simulation, there is an enormous potential for savingcomputing power. We present two selection strategies for self-optimization toreduce the computational cost in multi-objective design optimization problems.Our proposed methodology provides an intuitive approach that is easy to apply,offers significant improvements over random sampling, and circumvents the needfor uncertainty estimation. We evaluate our strategies on a large dataset fromthe domain of fluid dynamics and introduce two new evaluation metrics todetermine the model's performance. Findings from our evaluation highlights theeffectiveness of our selection strategies in accelerating design optimization.We believe that the introduced method is easily transferable to otherself-optimization problems.</description><author>Jens Decke, Christian Gruhl, Lukas Rauch, Bernhard Sick</author><pubDate>Mon, 02 Oct 2023 16:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04536v2</guid></item><item><title>Understanding the limitations of self-supervised learning for tabular anomaly detection</title><link>http://arxiv.org/abs/2309.08374v2</link><description>While self-supervised learning has improved anomaly detection in computervision and natural language processing, it is unclear whether tabular data canbenefit from it. This paper explores the limitations of self-supervision fortabular anomaly detection. We conduct several experiments spanning variouspretext tasks on 26 benchmark datasets to understand why this is the case. Ourresults confirm representations derived from self-supervision do not improvetabular anomaly detection performance compared to using the raw representationsof the data. We show this is due to neural networks introducing irrelevantfeatures, which reduces the effectiveness of anomaly detectors. However, wedemonstrate that using a subspace of the neural network's representation canrecover performance.</description><author>Kimberly T. Mai, Toby Davies, Lewis D. Griffin</author><pubDate>Mon, 02 Oct 2023 16:55:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08374v2</guid></item><item><title>Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators</title><link>http://arxiv.org/abs/2308.13498v2</link><description>In machine learning, the ability to assess uncertainty in model predictionsis crucial for decision-making, safety-critical applications, and modelgeneralizability. This work introduces a novel approach for epistemicuncertainty estimation for ensemble models using pairwise-distance estimators(PaiDEs). These estimators utilize the pairwise-distance between modelcomponents to establish bounds on entropy, which are then used as estimates forinformation-based criterion. Unlike recent deep learning methods for epistemicuncertainty estimation, which rely on sample-based Monte Carlo estimators,PaiDEs are able to estimate epistemic uncertainty up to 100 times faster, overa larger input space (up to 100 times) and perform more accurately in higherdimensions. To validate our approach, we conducted a series of experimentscommonly used to evaluate epistemic uncertainty estimation: 1D sinusoidal data,$\textit{Pendulum-v0}$, $\textit{Hopper-v2}$, $\textit{Ant-v2}$ and$\textit{Humanoid-v2}$. For each experimental setting, an Active Learningframework was applied to demonstrate the advantages of PaiDEs for epistemicuncertainty estimation.</description><author>Lucas Berry, David Meger</author><pubDate>Mon, 02 Oct 2023 16:46:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13498v2</guid></item><item><title>Exact Diffusion Inversion via Bi-directional Integration Approximation</title><link>http://arxiv.org/abs/2307.10829v4</link><description>Recently, various methods have been proposed to address the inconsistencyissue of DDIM inversion to enable image editing, such as EDICT [36] andNull-text inversion [22]. However, the above methods introduce considerablecomputational overhead. In this paper, we propose a new technique, named\emph{bi-directional integration approximation} (BDIA), to perform exactdiffusion inversion with neglible computational overhead. Suppose we would liketo estimate the next diffusion state $\boldsymbol{z}_{i-1}$ at timestep $t_i$with the historical information $(i,\boldsymbol{z}_i)$ and$(i+1,\boldsymbol{z}_{i+1})$. We first obtain the estimated Gaussian noise$\hat{\boldsymbol{\epsilon}}(\boldsymbol{z}_i,i)$, and then apply the DDIMupdate procedure twice for approximating the ODE integration over the nexttime-slot $[t_i, t_{i-1}]$ in the forward manner and the previous time-slot$[t_i, t_{t+1}]$ in the backward manner. The DDIM step for the previoustime-slot is used to refine the integration approximation made earlier whencomputing $\boldsymbol{z}_i$. A nice property of BDIA-DDIM is that the updateexpression for $\boldsymbol{z}_{i-1}$ is a linear combination of$(\boldsymbol{z}_{i+1}, \boldsymbol{z}_i,\hat{\boldsymbol{\epsilon}}(\boldsymbol{z}_i,i))$. This allows for exactbackward computation of $\boldsymbol{z}_{i+1}$ given $(\boldsymbol{z}_i,\boldsymbol{z}_{i-1})$, thus leading to exact diffusion inversion. It isdemonstrated with experiments that (round-trip) BDIA-DDIM is particularlyeffective for image editing. Our experiments further show that BDIA-DDIMproduces markedly better image sampling qualities than DDIM for text-to-imagegeneration. BDIA can also be applied to improve the performance of other ODE solvers inaddition to DDIM. In our work, it is found that applying BDIA to the EDMsampling procedure produces new SOTA performance over CIFAR10.</description><author>Guoqiang Zhang, J. P. Lewis, W. Bastiaan Kleijn</author><pubDate>Mon, 02 Oct 2023 16:39:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10829v4</guid></item><item><title>Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate Optimization Problems</title><link>http://arxiv.org/abs/2309.07936v2</link><description>In this paper, we introduce a new heuristics for global optimization inscenarios where extensive evaluations of the cost function are expensive,inaccessible, or even prohibitive. The method, which we callLandscape-Sketch-and-Step (LSS), combines Machine Learning, StochasticOptimization, and Reinforcement Learning techniques, relying on historicalinformation from previously sampled points to make judicious choices ofparameter values where the cost function should be evaluated at. Unlikeoptimization by Replica Exchange Monte Carlo methods, the number of evaluationsof the cost function required in this approach is comparable to that used bySimulated Annealing, quality that is especially important in contexts likehigh-throughput computing or high-performance computing tasks, whereevaluations are either computationally expensive or take a long time to beperformed. The method also differs from standard Surrogate Optimizationtechniques, for it does not construct a surrogate model that aims atapproximating or reconstructing the objective function. We illustrate ourmethod by applying it to low dimensional optimization problems (dimensions 1,2, 4, and 8) that mimick known difficulties of minimization on rugged energylandscapes often seen in Condensed Matter Physics, where cost functions arerugged and plagued with local minima. When compared to classical SimulatedAnnealing, the LSS shows an effective acceleration of the optimization process.</description><author>Rafael Monteiro, Kartik Sau</author><pubDate>Mon, 02 Oct 2023 16:37:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07936v2</guid></item><item><title>Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder</title><link>http://arxiv.org/abs/2303.15564v2</link><description>Deep neural networks are vulnerable to backdoor attacks, where an adversarymaliciously manipulates the model behavior through overlaying images withspecial triggers. Existing backdoor defense methods often require accessing afew validation data and model parameters, which are impractical in manyreal-world applications, e.g., when the model is provided as a cloud service.In this paper, we address the practical task of blind backdoor defense at testtime, in particular for black-box models. The true label of every test imageneeds to be recovered on the fly from a suspicious model regardless of imagebenignity. We focus on test-time image purification methods that incapacitatepossible triggers while keeping semantic contents intact. Due to diversetrigger patterns and sizes, the heuristic trigger search in image space can beunscalable. We circumvent such barrier by leveraging the strong reconstructionpower of generative models, and propose a framework of Blind Defense withMasked AutoEncoder (BDMAE). It detects possible triggers in the token spaceusing image structural similarity and label consistency between the test imageand MAE restorations. The detection results are then refined by consideringtrigger topology. Finally, we fuse MAE restorations adaptively into a purifiedimage for making prediction. Our approach is blind to the model architectures,trigger patterns and image benignity. Extensive experiments under differentbackdoor settings validate its effectiveness and generalizability. Code isavailable at https://github.com/tsun/BDMAE.</description><author>Tao Sun, Lu Pang, Chao Chen, Haibin Ling</author><pubDate>Mon, 02 Oct 2023 16:33:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15564v2</guid></item><item><title>Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models</title><link>http://arxiv.org/abs/2306.08018v3</link><description>Large Language Models (LLMs), with their remarkable task-handlingcapabilities and innovative outputs, have catalyzed significant advancementsacross a spectrum of fields. However, their proficiency within specializeddomains such as biomolecular studies remains limited. To address thischallenge, we introduce Mol-Instructions, a comprehensive instruction datasetdesigned for the biomolecular domain. Mol-Instructions encompasses three keycomponents: molecule-oriented instructions, protein-oriented instructions, andbiomolecular text instructions. Each component aims to improve theunderstanding and prediction capabilities of LLMs concerning biomolecularfeatures and behaviors. Through extensive instruction tuning experiments onLLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing largemodels' performance in the intricate realm of biomolecular studies, thusfostering progress in the biomolecular research community. Mol-Instructions ispublicly available for ongoing research and will undergo regular updates toenhance its applicability.</description><author>Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Huajun Chen</author><pubDate>Mon, 02 Oct 2023 16:27:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08018v3</guid></item><item><title>Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite</title><link>http://arxiv.org/abs/2309.08448v2</link><description>The evaluation of large language models is an essential task in the field oflanguage understanding and generation. As language models continue to advance,the need for effective benchmarks to assess their performance has becomeimperative. In the context of Traditional Chinese, there is a scarcity ofcomprehensive and diverse benchmarks to evaluate the capabilities of languagemodels, despite the existence of certain benchmarks such as DRCD, TTQA, CMDQA,and FGC dataset. To address this gap, we propose a novel set of benchmarks thatleverage existing English datasets and are tailored to evaluate language modelsin Traditional Chinese. These benchmarks encompass a wide range of tasks,including contextual question-answering, summarization, classification, andtable understanding. The proposed benchmarks offer a comprehensive evaluationframework, enabling the assessment of language models' capabilities acrossdifferent tasks. In this paper, we evaluate the performance of GPT-3.5,Taiwan-LLaMa-v1.0, and Model 7-C, our proprietary model, on these benchmarks.The evaluation results highlight that our model, Model 7-C, achievesperformance comparable to GPT-3.5 with respect to a part of the evaluatedcapabilities. In an effort to advance the evaluation of language models inTraditional Chinese and stimulate further research in this field, we haveopen-sourced our benchmark and opened the model for trial.</description><author>Chan-Jan Hsu, Chang-Le Liu, Feng-Ting Liao, Po-Chun Hsu, Yi-Chang Chen, Da-shan Shiu</author><pubDate>Mon, 02 Oct 2023 16:22:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08448v2</guid></item><item><title>Domain-Agnostic Molecular Generation with Self-feedback</title><link>http://arxiv.org/abs/2301.11259v5</link><description>The generation of molecules with desired properties has gained tremendouspopularity, revolutionizing the way scientists design molecular structures andproviding valuable support for chemical and drug design. However, despite thepotential of language models in molecule generation, they face numerouschallenges such as the generation of syntactically or chemically flawedmolecules, narrow domain focus, and limitations in creating diverse anddirectionally feasible molecules due to a dearth of annotated data or externalmolecular databases. To tackle these challenges, we introduce MolGen, apre-trained molecular language model tailored specifically for moleculegeneration. Through the reconstruction of over 100 million molecular SELFIES,MolGen internalizes profound structural and grammatical insights. This isfurther enhanced by domain-agnostic molecular prefix tuning, fostering robustknowledge transfer across diverse domains. Importantly, our self-feedbackparadigm steers the model away from ``molecular hallucinations'', ensuringalignment between the model's estimated probabilities and real-world chemicalpreferences. Extensive experiments on well-known benchmarks underscore MolGen'soptimization capabilities in properties such as penalized logP, QED, andmolecular docking. Additional analyses affirm its proficiency in accuratelycapturing molecule distributions, discerning intricate structural patterns, andefficiently exploring the chemical space. Code is available athttps://github.com/zjunlp/MolGen.</description><author>Yin Fang, Ningyu Zhang, Zhuo Chen, Lingbing Guo, Xiaohui Fan, Huajun Chen</author><pubDate>Mon, 02 Oct 2023 16:17:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11259v5</guid></item><item><title>IFAN: An Explainability-Focused Interaction Framework for Humans and NLP Models</title><link>http://arxiv.org/abs/2303.03124v2</link><description>Interpretability and human oversight are fundamental pillars of deployingcomplex NLP models into real-world applications. However, applyingexplainability and human-in-the-loop methods requires technical proficiency.Despite existing toolkits for model understanding and analysis, options tointegrate human feedback are still limited. We propose IFAN, a framework forreal-time explanation-based interaction with NLP models. Through IFAN'sinterface, users can provide feedback to selected model explanations, which isthen integrated through adapter layers to align the model with human rationale.We show the system to be effective in debiasing a hate speech classifier withminimal impact on performance. IFAN also offers a visual admin system and APIto manage models (and datasets) as well as control access rights. A demo islive at https://ifan.ml.</description><author>Edoardo Mosca, Daryna Dementieva, Tohid Ebrahim Ajdari, Maximilian Kummeth, Kirill Gringauz, Yutong Zhou, Georg Groh</author><pubDate>Mon, 02 Oct 2023 16:15:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.03124v2</guid></item><item><title>Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs</title><link>http://arxiv.org/abs/2305.14279v3</link><description>Large language models (LLMs) have achieved widespread success on a variety ofin-context few-shot tasks, but this success is typically evaluated viacorrectness rather than consistency. We argue that self-consistency is animportant criteria for valid multi-step reasoning in tasks where the solutionis composed of the answers to multiple sub-steps. We propose two types ofself-consistency that are particularly important for multi-step reasoning --hypothetical consistency (a model's ability to predict what its output would bein a hypothetical other context) and compositional consistency (consistency ofa model's final outputs when intermediate sub-steps are replaced with themodel's outputs for those steps). We demonstrate that multiple variants of theGPT-3/-4 models exhibit poor consistency rates across both types of consistencyon a variety of tasks.</description><author>Angelica Chen, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen Zhao, Samuel R. Bowman, Kyunghyun Cho</author><pubDate>Mon, 02 Oct 2023 16:09:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14279v3</guid></item><item><title>Tuning Pre-trained Model via Moment Probing</title><link>http://arxiv.org/abs/2307.11342v3</link><description>Recently, efficient fine-tuning of large-scale pre-trained models hasattracted increasing research interests, where linear probing (LP) as afundamental module is involved in exploiting the final representations fortask-dependent classification. However, most of the existing methods focus onhow to effectively introduce a few of learnable parameters, and little workpays attention to the commonly used LP module. In this paper, we propose anovel Moment Probing (MP) method to further explore the potential of LP.Distinguished from LP which builds a linear classification head based on themean of final features (e.g., word tokens for ViT) or classification tokens,our MP performs a linear classifier on feature distribution, which provides thestronger representation ability by exploiting richer statistical informationinherent in features. Specifically, we represent feature distribution by itscharacteristic function, which is efficiently approximated by using first- andsecond-order moments of features. Furthermore, we propose a multi-headconvolutional cross-covariance (MHC$^3$) to compute second-order moments in anefficient and effective manner. By considering that MP could affect featurelearning, we introduce a partially shared module to learn two recalibratingparameters (PSRP) for backbones based on MP, namely MP$_{+}$. Extensiveexperiments on ten benchmarks using various models show that our MPsignificantly outperforms LP and is competitive with counterparts at lesstraining cost, while our MP$_{+}$ achieves state-of-the-art performance.</description><author>Mingze Gao, Qilong Wang, Zhenyi Lin, Pengfei Zhu, Qinghua Hu, Jingbo Zhou</author><pubDate>Mon, 02 Oct 2023 15:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11342v3</guid></item><item><title>Joint Audio and Speech Understanding</title><link>http://arxiv.org/abs/2309.14405v2</link><description>Humans are surrounded by audio signals that include both speech andnon-speech sounds. The recognition and understanding of speech and non-speechaudio events, along with a profound comprehension of the relationship betweenthem, constitute fundamental cognitive capabilities. For the first time, webuild a machine learning model, called LTU-AS, that has a conceptually similaruniversal audio perception and advanced reasoning ability. Specifically, byintegrating Whisper as a perception module and LLaMA as a reasoning module,LTU-AS can simultaneously recognize and jointly understand spoken text, speechparalinguistics, and non-speech audio events - almost everything perceivablefrom audio signals.</description><author>Yuan Gong, Alexander H. Liu, Hongyin Luo, Leonid Karlinsky, James Glass</author><pubDate>Mon, 02 Oct 2023 15:53:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14405v2</guid></item><item><title>How Two-Layer Neural Networks Learn, One (Giant) Step at a Time</title><link>http://arxiv.org/abs/2305.18270v2</link><description>We investigate theoretically how the features of a two-layer neural networkadapt to the structure of the target function through a few large batchgradient descent steps, leading to improvement in the approximation capacitywith respect to the initialization. We compare the influence of batch size andthat of multiple (but finitely many) steps. For a single gradient step, a batchof size $n = \mathcal{O}(d)$ is both necessary and sufficient to align with thetarget function, although only a single direction can be learned. In contrast,$n = \mathcal{O}(d^2)$ is essential for neurons to specialize to multiplerelevant directions of the target with a single gradient step. Even in thiscase, we show there might exist ``hard'' directions requiring $n =\mathcal{O}(d^\ell)$ samples to be learned, where $\ell$ is known as the leapindex of the target. The picture drastically improves over multiple gradientsteps: we show that a batch-size of $n = \mathcal{O}(d)$ is indeed enough tolearn multiple target directions satisfying a staircase property, where moreand more directions can be learned over time. Finally, we discuss how thesedirections allows to drastically improve the approximation capacity andgeneralization error over the initialization, illustrating a separation ofscale between the random features/lazy regime, and the feature learning regime.Our technical analysis leverages a combination of techniques related toconcentration, projection-based conditioning, and Gaussian equivalence which webelieve are of independent interest. By pinning down the conditions necessaryfor specialization and learning, our results highlight the interaction betweenbatch size and number of iterations, and lead to a hierarchical depiction wherelearning performance exhibits a stairway to accuracy over time and batch size,shedding new light on how neural networks adapt to features of the data.</description><author>Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, Ludovic Stephan</author><pubDate>Mon, 02 Oct 2023 15:51:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18270v2</guid></item><item><title>Factify 2: A Multimodal Fake News and Satire News Dataset</title><link>http://arxiv.org/abs/2304.03897v2</link><description>The internet gives the world an open platform to express their views andshare their stories. While this is very valuable, it makes fake news one of oursociety's most pressing problems. Manual fact checking process is timeconsuming, which makes it challenging to disprove misleading assertions beforethey cause significant harm. This is he driving interest in automatic fact orclaim verification. Some of the existing datasets aim to support development ofautomating fact-checking techniques, however, most of them are text based.Multi-modal fact verification has received relatively scant attention. In thispaper, we provide a multi-modal fact-checking dataset called FACTIFY 2,improving Factify 1 by using new data sources and adding satire articles.Factify 2 has 50,000 new data instances. Similar to FACTIFY 1.0, we have threebroad categories - support, no-evidence, and refute, with sub-categories basedon the entailment of visual and textual data. We also provide a BERT and VisonTransformer based baseline, which achieves 65% F1 score in the test set. Thebaseline codes and the dataset will be made available athttps://github.com/surya1701/Factify-2.0.</description><author>S Suryavardan, Shreyash Mishra, Parth Patwa, Megha Chakraborty, Anku Rani, Aishwarya Reganti, Aman Chadha, Amitava Das, Amit Sheth, Manoj Chinnakotla, Asif Ekbal, Srijan Kumar</author><pubDate>Mon, 02 Oct 2023 15:48:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03897v2</guid></item><item><title>MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning</title><link>http://arxiv.org/abs/2309.07915v2</link><description>Since the resurgence of deep learning, vision-language models (VLMs) enhancedby large language models (LLMs) have grown exponentially in popularity.However, while LLMs can utilize extensive background knowledge and taskinformation with in-context learning, most VLMs still struggle withunderstanding complex multi-modal prompts with multiple images, making VLMsless effective in downstream vision-language tasks. In this paper, we addressthe limitation above by 1) introducing MMICL, a new approach to allow the VLMto deal with multi-modal inputs efficiently; 2) proposing a novel contextscheme to augment the in-context learning ability of the VLM; 3) constructingthe Multi-modal In-Context Learning (MIC) dataset, designed to enhance theVLM's ability to understand complex multi-modal prompts. Our experimentsconfirm that MMICL achieves new state-of-the-art zero-shot performance on awide range of general vision-language tasks, especially for complex benchmarks,including MME and MMBench. Our analysis demonstrates that MMICL effectivelytackles the challenge of complex multi-modal prompt understanding and emergesthe impressive ICL ability. Furthermore, we observe that MMICL successfullyalleviates language bias in VLMs, a common issue for VLMs that often leads tohallucination when faced with extensive textual context.</description><author>Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang</author><pubDate>Mon, 02 Oct 2023 15:46:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07915v2</guid></item><item><title>Physics-informed neural networks with unknown measurement noise</title><link>http://arxiv.org/abs/2211.15498v3</link><description>Physics-informed neural networks (PINNs) constitute a flexible approach toboth finding solutions and identifying parameters of partial differentialequations. Most works on the topic assume noiseless data, or data contaminatedby weak Gaussian noise. We show that the standard PINN framework breaks down incase of non-Gaussian noise. We give a way of resolving this fundamental issueand we propose to jointly train an energy-based model (EBM) to learn thecorrect noise distribution. We illustrate the improved performance of ourapproach using multiple examples.</description><author>Philipp Pilar, Niklas Wahlström</author><pubDate>Mon, 02 Oct 2023 15:44:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15498v3</guid></item><item><title>Data Filtering Networks</title><link>http://arxiv.org/abs/2309.17425v2</link><description>Large training sets have become a cornerstone of machine learning and are thefoundation for recent advances in language modeling and multimodal learning.While data curation for pre-training is often still ad-hoc, one common paradigmis to first collect a massive pool of data from the Web and then filter thiscandidate pool down to an actual training set via various heuristics. In thiswork, we study the problem of learning a data filtering network (DFN) for thissecond step of filtering a large uncurated dataset. Our key finding is that thequality of a network for filtering is distinct from its performance ondownstream tasks: for instance, a model that performs well on ImageNet canyield worse training sets than a model with low ImageNet accuracy that istrained on a small amount of high-quality data. Based on our insights, weconstruct new data filtering networks that induce state-of-the-art image-textdatasets. Specifically, our best performing dataset DFN-5B enables us to trainstate-of-the-art models for their compute budgets: among other improvements ona variety of tasks, a ViT-H trained on our dataset achieves 83.0% zero-shottransfer accuracy on ImageNet, out-performing models trained on other datasetssuch as LAION-2B, DataComp-1B, or OpenAI's WIT. In order to facilitate furtherresearch in dataset design, we also release a new 2 billion example datasetDFN-2B and show that high performance data filtering networks can be trainedfrom scratch using only publicly available data.</description><author>Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, Vaishaal Shankar</author><pubDate>Mon, 02 Oct 2023 15:40:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17425v2</guid></item><item><title>AI Driven Near Real-time Locational Marginal Pricing Method: A Feasibility and Robustness Study</title><link>http://arxiv.org/abs/2306.10080v2</link><description>Accurate price predictions are essential for market participants in order tooptimize their operational schedules and bidding strategies, especially in thecurrent context where electricity prices become more volatile and lesspredictable using classical approaches. The Locational Marginal Pricing (LMP)pricing mechanism is used in many modern power markets, where the traditionalapproach utilizes optimal power flow (OPF) solvers. However, for largeelectricity grids this process becomes prohibitively time-consuming andcomputationally intensive. Machine learning (ML) based predictions couldprovide an efficient tool for LMP prediction, especially in energy markets withintermittent sources like renewable energy. This study evaluates theperformance of popular machine learning and deep learning models in predictingLMP on multiple electricity grids. The accuracy and robustness of these modelsin predicting LMP is assessed considering multiple scenarios. The results showthat ML models can predict LMP 4-5 orders of magnitude faster than traditionalOPF solvers with 5-6\% error rate, highlighting the potential of ML models inLMP prediction for large-scale power models with the assistance of hardwareinfrastructure like multi-core CPUs and GPUs in modern HPC clusters.</description><author>Naga Venkata Sai Jitin Jami, Juraj Kardoš, Olaf Schenk, Harald Köstler</author><pubDate>Mon, 02 Oct 2023 15:39:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10080v2</guid></item><item><title>Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning</title><link>http://arxiv.org/abs/2305.14062v2</link><description>Photoplethysmography (PPG) refers to the measurement of variations in bloodvolume using light and is a feature of most wearable devices. The PPG signalsprovide insight into the body's circulatory system and can be employed toextract various bio-features, such as heart rate and vascular ageing. Althoughseveral algorithms have been proposed for this purpose, many exhibitlimitations, including heavy reliance on human calibration, high signal qualityrequirements, and a lack of generalisation. In this paper, we introduce a PPGsignal processing framework that integrates graph theory and computer visionalgorithms, to provide an analysis framework which is amplitude-independent andinvariant to affine transformations. It also requires minimal preprocessing,fuses information through RGB channels and exhibits robust generalisationacross tasks and datasets. The proposed VGTL-net achieves state-of-the-artperformance in the prediction of vascular ageing and demonstrates robustestimation of continuous blood pressure waveforms.</description><author>Yuyang Miao, Harry J. Davies, Danilo P. Mandic</author><pubDate>Mon, 02 Oct 2023 15:37:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14062v2</guid></item><item><title>The Isotonic Mechanism for Exponential Family Estimation</title><link>http://arxiv.org/abs/2304.11160v3</link><description>In 2023, the International Conference on Machine Learning (ICML) requiredauthors with multiple submissions to rank their submissions based on perceivedquality. In this paper, we aim to employ these author-specified rankings toenhance peer review in machine learning and artificial intelligence conferencesby extending the Isotonic Mechanism to exponential family distributions. Thismechanism generates adjusted scores that closely align with the original scoreswhile adhering to author-specified rankings. Despite its applicability to abroad spectrum of exponential family distributions, implementing this mechanismdoes not require knowledge of the specific distribution form. We demonstratethat an author is incentivized to provide accurate rankings when her utilitytakes the form of a convex additive function of the adjusted review scores. Fora certain subclass of exponential family distributions, we prove that theauthor reports truthfully only if the question involves only pairwisecomparisons between her submissions, thus indicating the optimality of rankingin truthful information elicitation. Moreover, we show that the adjusted scoresimprove dramatically the estimation accuracy compared to the original scoresand achieve nearly minimax optimality when the ground-truth scores have boundedtotal variation. We conclude the paper by presenting experiments conducted onthe ICML 2023 ranking data, which show significant estimation gain using theIsotonic Mechanism.</description><author>Yuling Yan, Weijie J. Su, Jianqing Fan</author><pubDate>Mon, 02 Oct 2023 15:33:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11160v3</guid></item><item><title>Memotion 3: Dataset on Sentiment and Emotion Analysis of Codemixed Hindi-English Memes</title><link>http://arxiv.org/abs/2303.09892v3</link><description>Memes are the new-age conveyance mechanism for humor on social media sites.Memes often include an image and some text. Memes can be used to promotedisinformation or hatred, thus it is crucial to investigate in details. Weintroduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike otherprevalent datasets in the domain, including prior iterations of Memotion,Memotion 3 introduces Hindi-English Codemixed memes while prior works in thearea were limited to only the English memes. We describe the Memotion task, thedata collection and the dataset creation methodologies. We also provide abaseline for the task. The baseline code and dataset will be made available athttps://github.com/Shreyashm16/Memotion-3.0</description><author>Shreyash Mishra, S Suryavardan, Parth Patwa, Megha Chakraborty, Anku Rani, Aishwarya Reganti, Aman Chadha, Amitava Das, Amit Sheth, Manoj Chinnakotla, Asif Ekbal, Srijan Kumar</author><pubDate>Mon, 02 Oct 2023 15:28:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09892v3</guid></item><item><title>EvoPrompting: Language Models for Code-Level Neural Architecture Search</title><link>http://arxiv.org/abs/2302.14838v2</link><description>Given the recent impressive accomplishments of language models (LMs) for codegeneration, we explore the use of LMs as adaptive mutation and crossoveroperators for an evolutionary neural architecture search (NAS) algorithm. WhileNAS still proves too difficult a task for LMs to succeed at solely throughprompting, we find that the combination of evolutionary prompt engineering withsoft prompt-tuning, a method we term EvoPrompting, consistently finds diverseand high performing models. We first demonstrate that EvoPrompting is effectiveon the computationally efficient MNIST-1D dataset, where EvoPrompting producesconvolutional architecture variants that outperform both those designed byhuman experts and naive few-shot prompting in terms of accuracy and model size.We then apply our method to searching for graph neural networks on the CLRSAlgorithmic Reasoning Benchmark, where EvoPrompting is able to design novelarchitectures that outperform current state-of-the-art models on 21 out of 30algorithmic reasoning tasks while maintaining similar model size. EvoPromptingis successful at designing accurate and efficient neural network architecturesacross a variety of machine learning tasks, while also being general enough foreasy adaptation to other tasks beyond neural network design.</description><author>Angelica Chen, David M. Dohan, David R. So</author><pubDate>Mon, 02 Oct 2023 15:26:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14838v2</guid></item><item><title>On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters</title><link>http://arxiv.org/abs/2309.17053v2</link><description>Seminal research in the field of graph neural networks (GNNs) has revealed adirect correspondence between the expressive capabilities of GNNs and the$k$-dimensional Weisfeiler-Leman ($k$WL) test, a widely-recognized method forverifying graph isomorphism. This connection has reignited interest incomprehending the specific graph properties effectively distinguishable by the$k$WL test. A central focus of research in this field revolves arounddetermining the least dimensionality $k$, for which $k$WL can discern graphswith different number of occurrences of a pattern graph $P$. We refer to such aleast $k$ as the WL-dimension of this pattern counting problem. This inquirytraditionally delves into two distinct counting problems related to patterns:subgraph counting and induced subgraph counting. Intriguingly, despite theirinitial appearance as separate challenges with seemingly divergent approaches,both of these problems are interconnected components of a more comprehensiveproblem: "graph motif parameters". In this paper, we provide a precisecharacterization of the WL-dimension of labeled graph motif parameters. Asspecific instances of this result, we obtain characterizations of theWL-dimension of the subgraph counting and induced subgraph counting problem forevery labeled pattern $P$. We additionally demonstrate that in cases where the$k$WL test distinguishes between graphs with varying occurrences of a pattern$P$, the exact number of occurrences of $P$ can be computed uniformly usingonly local information of the last layer of a corresponding GNN. We finallydelve into the challenge of recognizing the WL-dimension of various graphparameters. We give a polynomial time algorithm for determining theWL-dimension of the subgraph counting problem for given pattern $P$, answeringan open question from previous work.</description><author>Matthias Lanzinger, Pablo Barceló</author><pubDate>Mon, 02 Oct 2023 15:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17053v2</guid></item><item><title>SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem</title><link>http://arxiv.org/abs/2309.15111v2</link><description>In this work, we consider the optimization process of minibatch stochasticgradient descent (SGD) on a 2-layer neural network with data separated by aquadratic ground truth function. We prove that with data drawn from the$d$-dimensional Boolean hypercube labeled by the quadratic ``XOR'' function $y= -x_ix_j$, it is possible to train to a population error $o(1)$ with $d\:\text{polylog}(d)$ samples. Our result considers simultaneously training bothlayers of the two-layer-neural network with ReLU activations via standardminibatch SGD on the logistic loss. To our knowledge, this work is the first togive a sample complexity of $\tilde{O}(d)$ for efficiently learning the XORfunction on isotropic data on a standard neural network with standard training.Our main technique is showing that the network evolves in two phases: a$\textit{signal-finding}$ phase where the network is small and many of theneurons evolve independently to find features, and a $\textit{signal-heavy}$phase, where SGD maintains and balances the features. We leverage thesimultaneous training of the layers to show that it is sufficient for only asmall fraction of the neurons to learn features, since those neurons will beamplified by the simultaneous growth of their second layer weights.</description><author>Margalit Glasgow</author><pubDate>Mon, 02 Oct 2023 15:21:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15111v2</guid></item><item><title>Combining Monte Carlo and Tensor-network Methods for Partial Differential Equations via Sketching</title><link>http://arxiv.org/abs/2305.17884v4</link><description>In this paper, we propose a general framework for solving high-dimensionalpartial differential equations with tensor networks. Our approach uses aMonte-Carlo simulations to update the solution and re-estimates the newsolution from samples as a tensor-network using a recently proposed tensortrain sketching technique. We showcase the versatility and flexibility of ourapproach by applying it to two specific scenarios: simulating the Fokker-Planckequation through Langevin dynamics and quantum imaginary time evolution viaauxiliary-field quantum Monte Carlo. We also provide convergence guarantees andnumerical experiments to demonstrate the efficacy of the proposed method.</description><author>Yian Chen, Yuehaw Khoo</author><pubDate>Mon, 02 Oct 2023 15:08:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17884v4</guid></item><item><title>Backorder Prediction in Inventory Management: Classification Techniques and Cost Considerations</title><link>http://arxiv.org/abs/2309.13837v2</link><description>This article introduces an advanced analytical approach for predictingbackorders in inventory management. Backorder refers to an order that cannot beimmediately fulfilled due to stock depletion. Multiple classificationtechniques, including Balanced Bagging Classifiers, Fuzzy Logic, VariationalAutoencoder - Generative Adversarial Networks, and Multi-layer Perceptronclassifiers, are assessed in this work using performance evaluation metricssuch as ROC-AUC and PR-AUC. Moreover, this work incorporates a profit functionand misclassification costs, considering the financial implications and costsassociated with inventory management and backorder handling. The resultsdemonstrate the effectiveness of the predictive model in enhancing inventorysystem service levels, which leads to customer satisfaction and overallorganizational performance. Considering interpretability is a significantaspect of using AI in commercial applications, permutation importance isapplied to the selected model to determine the importance of features. Thisresearch contributes to the advancement of predictive analytics and offersvaluable insights for future investigations in backorder forecasting andinventory control optimization for decision-making.</description><author>Sarit Maitra, Sukanya Kundu</author><pubDate>Mon, 02 Oct 2023 14:55:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13837v2</guid></item><item><title>Confidence-Aware and Self-Supervised Image Anomaly Localisation</title><link>http://arxiv.org/abs/2303.13227v2</link><description>Universal anomaly detection still remains a challenging problem in machinelearning and medical image analysis. It is possible to learn an expecteddistribution from a single class of normative samples, e.g., through epistemicuncertainty estimates, auto-encoding models, or from synthetic anomalies in aself-supervised way. The performance of self-supervised anomaly detectionapproaches is still inferior compared to methods that use examples from knownunknown classes to shape the decision boundary. However, outlier exposuremethods often do not identify unknown unknowns. Here we discuss an improvedself-supervised single-class training strategy that supports the approximationof probabilistic inference with loosen feature locality constraints. We showthat up-scaling of gradients with histogram-equalised images is beneficial forrecently proposed self-supervision tasks. Our method is integrated into severalout-of-distribution (OOD) detection models and we show evidence that our methodoutperforms the state-of-the-art on various benchmark datasets.</description><author>Johanna P. Müller, Matthew Baugh, Jeremy Tan, Mischa Dombrowski, Bernhard Kainz</author><pubDate>Mon, 02 Oct 2023 14:36:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13227v2</guid></item><item><title>Energy-guided Entropic Neural Optimal Transport</title><link>http://arxiv.org/abs/2304.06094v3</link><description>Energy-based models (EBMs) are known in the Machine Learning community fordecades. Since the seminal works devoted to EBMs dating back to the noughties,there have been a lot of efficient methods which solve the generative modellingproblem by means of energy potentials (unnormalized likelihood functions). Incontrast, the realm of Optimal Transport (OT) and, in particular, neural OTsolvers is much less explored and limited by few recent works (excludingWGAN-based approaches which utilize OT as a loss function and do not model OTmaps themselves). In our work, we bridge the gap between EBMs andEntropy-regularized OT. We present a novel methodology which allows utilizingthe recent developments and technical improvements of the former in order toenrich the latter. From the theoretical perspective, we prove generalizationbounds for our technique. In practice, we validate its applicability in toy 2Dand image domains. To showcase the scalability, we empower our method with apre-trained StyleGAN and apply it to high-res AFHQ $512\times 512$ unpaired I2Itranslation. For simplicity, we choose simple short- and long-run EBMs as abackbone of our Energy-guided Entropic OT approach, leaving the application ofmore sophisticated EBMs for future research. Our code is publicly available.</description><author>Petr Mokrov, Alexander Korotin, Alexander Kolesov, Nikita Gushchin, Evgeny Burnaev</author><pubDate>Mon, 02 Oct 2023 14:34:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06094v3</guid></item><item><title>Total-Recon: Deformable Scene Reconstruction for Embodied View Synthesis</title><link>http://arxiv.org/abs/2304.12317v2</link><description>We explore the task of embodied view synthesis from monocular videos ofdeformable scenes. Given a minute-long RGBD video of people interacting withtheir pets, we render the scene from novel camera trajectories derived from thein-scene motion of actors: (1) egocentric cameras that simulate the point ofview of a target actor and (2) 3rd-person cameras that follow the actor.Building such a system requires reconstructing the root-body and articulatedmotion of every actor, as well as a scene representation that supportsfree-viewpoint synthesis. Longer videos are more likely to capture the scenefrom diverse viewpoints (which helps reconstruction) but are also more likelyto contain larger motions (which complicates reconstruction). To address thesechallenges, we present Total-Recon, the first method to photorealisticallyreconstruct deformable scenes from long monocular RGBD videos. Crucially, toscale to long videos, our method hierarchically decomposes the scene into thebackground and objects, whose motion is decomposed into carefully initializedroot-body motion and local articulations. To quantify such "in-the-wild"reconstruction and view synthesis, we collect ground-truth data from aspecialized stereo RGBD capture rig for 11 challenging videos, significantlyoutperforming prior methods. Our code, model, and data can be found athttps://andrewsonga.github.io/totalrecon .</description><author>Chonghyuk Song, Gengshan Yang, Kangle Deng, Jun-Yan Zhu, Deva Ramanan</author><pubDate>Mon, 02 Oct 2023 14:07:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12317v2</guid></item><item><title>Passive learning of active causal strategies in agents and language models</title><link>http://arxiv.org/abs/2305.16183v2</link><description>What can be learned about causality and experimentation from passive data?This question is salient given recent successes of passively-trained languagemodels in interactive domains such as tool use. Passive learning is inherentlylimited. However, we show that purely passive learning can in fact allow anagent to learn generalizable strategies for determining and using causalstructures, as long as the agent can intervene at test time. We formallyillustrate that learning a strategy of first experimenting, then seeking goals,can allow generalization from passive learning in principle. We then showempirically that agents trained via imitation on expert data can indeedgeneralize at test time to infer and use causal links which are never presentin the training data; these agents can also generalize experimentationstrategies to novel variable sets never observed in training. We then show thatstrategies for causal intervention and exploitation can be generalized frompassive data even in a more complex environment with high-dimensionalobservations, with the support of natural language explanations. Explanationscan even allow passive learners to generalize out-of-distribution fromperfectly-confounded training data. Finally, we show that language models,trained only on passive next-word prediction, can generalize causalintervention strategies from a few-shot prompt containing examples ofexperimentation, together with explanations and reasoning. These resultshighlight the surprising power of passive learning of active causal strategies,and may help to understand the behaviors and capabilities of language models.</description><author>Andrew Kyle Lampinen, Stephanie C Y Chan, Ishita Dasgupta, Andrew J Nam, Jane X Wang</author><pubDate>Mon, 02 Oct 2023 14:04:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16183v2</guid></item><item><title>Language-based Action Concept Spaces Improve Video Self-Supervised Learning</title><link>http://arxiv.org/abs/2307.10922v2</link><description>Recent contrastive language image pre-training has led to learning highlytransferable and robust image representations. However, adapting these modelsto video domains with minimal supervision remains an open problem. We explore asimple step in that direction, using language tied self-supervised learning toadapt an image CLIP model to the video domain. A backbone modified for temporalmodeling is trained under self-distillation settings with train objectivesoperating in an action concept space. Feature vectors of various actionconcepts extracted from a language encoder using relevant textual promptsconstruct this space. We introduce two train objectives, concept distillationand concept alignment, that retain generality of original representations whileenforcing relations between actions and their attributes. Our approach improveszero-shot and linear probing performance on three action recognitionbenchmarks.</description><author>Kanchana Ranasinghe, Michael Ryoo</author><pubDate>Mon, 02 Oct 2023 13:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10922v2</guid></item><item><title>Learning Decentralized Flocking Controllers with Spatio-Temporal Graph Neural Network</title><link>http://arxiv.org/abs/2309.17437v2</link><description>Recently a line of researches has delved the use of graph neural networks(GNNs) for decentralized control in swarm robotics. However, it has beenobserved that relying solely on the states of immediate neighbors isinsufficient to imitate a centralized control policy. To address thislimitation, prior studies proposed incorporating $L$-hop delayed states intothe computation. While this approach shows promise, it can lead to a lack ofconsensus among distant flock members and the formation of small clusters,consequently resulting in the failure of cohesive flocking behaviors. Instead,our approach leverages spatiotemporal GNN, named STGNN that encompasses bothspatial and temporal expansions. The spatial expansion collects delayed statesfrom distant neighbors, while the temporal expansion incorporates previousstates from immediate neighbors. The broader and more comprehensive informationgathered from both expansions results in more effective and accuratepredictions. We develop an expert algorithm for controlling a swarm of robotsand employ imitation learning to train our decentralized STGNN model based onthe expert algorithm. We simulate the proposed STGNN approach in varioussettings, demonstrating its decentralized capacity to emulate the global expertalgorithm. Further, we implemented our approach to achieve cohesive flocking,leader following and obstacle avoidance by a group of Crazyflie drones. Theperformance of STGNN underscores its potential as an effective and reliableapproach for achieving cohesive flocking, leader following and obstacleavoidance tasks.</description><author>Siji Chen, Yanshen Sun, Peihan Li, Lifeng Zhou, Chang-Tien Lu</author><pubDate>Mon, 02 Oct 2023 13:41:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17437v2</guid></item><item><title>FreMIM: Fourier Transform Meets Masked Image Modeling for Medical Image Segmentation</title><link>http://arxiv.org/abs/2304.10864v2</link><description>The research community has witnessed the powerful potential ofself-supervised Masked Image Modeling (MIM), which enables the models capableof learning visual representation from unlabeled data.In this paper, toincorporate both the crucial global structural information and local detailsfor dense prediction tasks, we alter the perspective to the frequency domainand present a new MIM-based framework named FreMIM for self-supervisedpre-training to better accomplish medical image segmentation task. Based on theobservations that the detailed structural information mainly lies in thehigh-frequency components and the high-level semantics are abundant in thelow-frequency counterparts, we further incorporate multi-stage supervision toguide the representation learning during the pre-training phase. Extensiveexperiments on three benchmark datasets show the superior advantage of ourFreMIM over previous state-of-the-art MIM methods. Compared with variousbaselines trained from scratch, our FreMIM could consistently bringconsiderable improvements to model performance. The code will be made publiclyavailable.</description><author>Wenxuan Wang, Jing Wang, Chen Chen, Jianbo Jiao, Lichao Sun, Yuanxiu Cai, Shanshan Song, Jiangyun Li</author><pubDate>Mon, 02 Oct 2023 13:40:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10864v2</guid></item><item><title>mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs</title><link>http://arxiv.org/abs/2307.06930v2</link><description>Modular vision-language models (Vision-LLMs) align pretrained image encoderswith frozen large language models (LLMs), representing a computationally muchmore efficient alternative to end-to-end training of large vision-languagemodels from scratch, which is prohibitively expensive for most researchers andpractitioners. Vision-LLMs instead post-hoc condition LLMs to `understand' theoutput of an image encoder. With the abundance of readily availablehigh-quality English image-text data as well as monolingual English LLMs, theresearch focus has been on English-only Vision-LLMs. Multilingualvision-language models are still predominantly obtained via expensiveend-to-end pretraining, resulting in comparatively smaller models, trained onlimited multilingual image data supplemented with text-only multilingualcorpora. In this work, we present mBLIP, the first multilingual Vision-LLM,which we obtain in a computationally efficient manner -- on consumer hardwareand using only a few million training examples -- by leveraging a pretrainedmultilingual LLM. To this end, we \textit{re-align} an image encoder previouslytuned to an English LLM to a new, multilingual LLM -- for this, we leveragemultilingual data from a mix of vision-and-language tasks, which we obtain bymachine-translating high-quality English data to 95 languages. On the IGLUEbenchmark, mBLIP yields results competitive with state-of-the-art models.Moreover, in image captioning on XM3600, mBLIP (zero-shot) even outperformsPaLI-X (a model with 55B parameters). Compared to these very large multilingualvision-language models trained from scratch, we obtain mBLIP by training ordersof magnitude fewer parameters on magnitudes less data. We release our model andcode at \url{https://github.com/gregor-ge/mBLIP}.</description><author>Gregor Geigle, Abhay Jain, Radu Timofte, Goran Glavaš</author><pubDate>Mon, 02 Oct 2023 12:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06930v2</guid></item><item><title>Precise localization of corneal reflections in eye images using deep learning trained on synthetic data</title><link>http://arxiv.org/abs/2304.05673v2</link><description>We present a deep learning method for accurately localizing the center of asingle corneal reflection (CR) in an eye image. Unlike previous approaches, weuse a convolutional neural network (CNN) that was trained solely usingsimulated data. Using only simulated data has the benefit of completelysidestepping the time-consuming process of manual annotation that is requiredfor supervised training on real eye images. To systematically evaluate theaccuracy of our method, we first tested it on images with simulated CRs placedon different backgrounds and embedded in varying levels of noise. Second, wetested the method on high-quality videos captured from real eyes. Our methodoutperformed state-of-the-art algorithmic methods on real eye images with a 35%reduction in terms of spatial precision, and performed on par withstate-of-the-art on simulated images in terms of spatial accuracy.We concludethat our method provides a precise method for CR center localization andprovides a solution to the data availability problem which is one of theimportant common roadblocks in the development of deep learning models for gazeestimation. Due to the superior CR center localization and ease of application,our method has the potential to improve the accuracy and precision of CR-basedeye trackers</description><author>Sean Anthony Byrne, Marcus Nyström, Virmarie Maquiling, Enkelejda Kasneci, Diederick C. Niehorster</author><pubDate>Mon, 02 Oct 2023 12:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05673v2</guid></item><item><title>Scaling MLPs: A Tale of Inductive Bias</title><link>http://arxiv.org/abs/2306.13575v2</link><description>In this work we revisit the most fundamental building block in deep learning,the multi-layer perceptron (MLP), and study the limits of its performance onvision tasks. Empirical insights into MLPs are important for multiple reasons.(1) Given the recent narrative "less inductive bias is better", popularized dueto transformers eclipsing convolutional models, it is natural to explore thelimits of this hypothesis. To that end, MLPs offer an ideal test bed, as theylack any vision-specific inductive bias. (2) MLPs have almost exclusively beenthe main protagonist in the deep learning theory literature due to theirmathematical simplicity, serving as a proxy to explain empirical phenomenaobserved for more complex architectures. Surprisingly, experimental datapointsfor MLPs are very difficult to find in the literature, especially when coupledwith large pre-training protocols. This discrepancy between practice and theoryis worrying: Do MLPs reflect the empirical advances exhibited by practicalmodels? Or do theorists need to rethink the role of MLPs as a proxy? We provideinsights into both these aspects. We show that the performance of MLPsdrastically improves with scale (94% on CIFAR10, 81% on CIFAR100, 58% onImageNet ReaL), highlighting that lack of inductive bias can indeed becompensated. We observe that MLPs mimic the behaviour of their moderncounterparts faithfully, with some components in the learning setting howeverexhibiting stronger or unexpected behaviours. Due to their inherentcomputational efficiency, large pre-training experiments become more accessiblefor academic researchers. All of our experiments were run on a single GPU.</description><author>Gregor Bachmann, Sotiris Anagnostidis, Thomas Hofmann</author><pubDate>Mon, 02 Oct 2023 12:40:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13575v2</guid></item><item><title>Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities</title><link>http://arxiv.org/abs/2303.12706v4</link><description>One of the challenges of studying common neurological disorders is diseaseheterogeneity including differences in causes, neuroimaging characteristics,comorbidities, or genetic variation. Normative modelling has become a popularmethod for studying such cohorts where the 'normal' behaviour of aphysiological system is modelled and can be used at subject level to detectdeviations relating to disease pathology. For many heterogeneous diseases, weexpect to observe abnormalities across a range of neuroimaging and biologicalvariables. However, thus far, normative models have largely been developed forstudying a single imaging modality. We aim to develop a multi-modal normativemodelling framework where abnormality is aggregated across variables ofmultiple modalities and is better able to detect deviations than uni-modalbaselines. We propose two multi-modal VAE normative models to detect subjectlevel deviations across T1 and DTI data. Our proposed models were better ableto detect diseased individuals, capture disease severity, and correlate withpatient cognition than baseline approaches. We also propose a multivariatelatent deviation metric, measuring deviations from the joint latent space,which outperformed feature-based metrics.</description><author>Ana Lawry Aguila, James Chapman, Andre Altmann</author><pubDate>Mon, 02 Oct 2023 12:04:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12706v4</guid></item><item><title>A PAC-Bayes oracle inequality for sparse neural networks</title><link>http://arxiv.org/abs/2204.12392v2</link><description>We study the Gibbs posterior distribution for sparse deep neural nets in anonparametric regression setting. The posterior can be accessed viaMetropolis-adjusted Langevin algorithms. Using a mixture over uniform priors onsparse sets of network weights, we prove an oracle inequality which shows thatthe method adapts to the unknown regularity and hierarchical structure of theregression function. The estimator achieves the minimax-optimal rate ofconvergence (up to a logarithmic factor).</description><author>Maximilian F. Steffen, Mathias Trabs</author><pubDate>Mon, 02 Oct 2023 11:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.12392v2</guid></item><item><title>MVDream: Multi-view Diffusion for 3D Generation</title><link>http://arxiv.org/abs/2308.16512v2</link><description>We introduce MVDream, a multi-view diffusion model that is able to generateconsistent multi-view images from a given text prompt. Learning from both 2Dand 3D data, a multi-view diffusion model can achieve the generalizability of2D diffusion models and the consistency of 3D renderings. We demonstrate thatsuch a multi-view prior can serve as a generalizable 3D prior that is agnosticto 3D representations. It can be applied to 3D generation via ScoreDistillation Sampling, significantly enhancing the consistency and stability ofexisting 2D-lifting methods. It can also learn new concepts from a few 2Dexamples, akin to DreamBooth, but for 3D generation.</description><author>Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, Xiao Yang</author><pubDate>Mon, 02 Oct 2023 11:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16512v2</guid></item><item><title>Improving Facade Parsing with Vision Transformers and Line Integration</title><link>http://arxiv.org/abs/2309.15523v3</link><description>Facade parsing stands as a pivotal computer vision task with far-reachingapplications in areas like architecture, urban planning, and energy efficiency.Despite the recent success of deep learning-based methods in yieldingimpressive results on certain open-source datasets, their viability forreal-world applications remains uncertain. Real-world scenarios areconsiderably more intricate, demanding greater computational efficiency.Existing datasets often fall short in representing these settings, and previousmethods frequently rely on extra models to enhance accuracy, which requiresmuch computation cost. In this paper, we introduce Comprehensive Facade Parsing(CFP), a dataset meticulously designed to encompass the intricacies ofreal-world facade parsing tasks. Comprising a total of 602 high-resolutionstreet-view images, this dataset captures a diverse array of challengingscenarios, including sloping angles and densely clustered buildings, withpainstakingly curated annotations for each image. We introduce a new pipelineknown as Revision-based Transformer Facade Parsing (RTFP). This marks thepioneering utilization of Vision Transformers (ViT) in facade parsing, and ourexperimental results definitively substantiate its merit. We also design LineAcquisition, Filtering, and Revision (LAFR), an efficient yet accurate revisionalgorithm that can improve the segment result solely from simple line detectionusing prior knowledge of the facade. In ECP 2011, RueMonge 2014, and our CFP,we evaluate the superiority of our method. The dataset and code are availableat https://github.com/wbw520/RTFP.</description><author>Bowen Wang, Jiaxing Zhang, Ran Zhang, Yunqin Li, Liangzhi Li, Yuta Nakashima</author><pubDate>Mon, 02 Oct 2023 11:39:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15523v3</guid></item><item><title>Lyra: Orchestrating Dual Correction in Automated Theorem Proving</title><link>http://arxiv.org/abs/2309.15806v2</link><description>Large Language Models (LLMs) present an intriguing avenue for exploration inthe field of formal theorem proving. Nevertheless, their full potential,particularly concerning the mitigation of hallucinations and refinement throughprover error messages, remains an area that has yet to be thoroughlyinvestigated. To enhance the effectiveness of LLMs in the field, we introducethe Lyra, a new framework that employs two distinct correction mechanisms: ToolCorrection (TC) and Conjecture Correction (CC). To implement Tool Correction inthe post-processing of formal proofs, we leverage prior knowledge to utilizepredefined prover tools (e.g., Sledgehammer) for guiding the replacement ofincorrect tools. Tool Correction significantly contributes to mitigatinghallucinations, thereby improving the overall accuracy of the proof. Inaddition, we introduce Conjecture Correction, an error feedback mechanismdesigned to interact with prover to refine formal proof conjectures with provererror messages. Compared to the previous refinement framework, the proposedConjecture Correction refines generation with instruction but does not collectpaired (generation, error &amp; refinement) prompts. Our method has achievedstate-of-the-art (SOTA) performance on both miniF2F validation (48.0% -&gt; 55.3%)and test (45.5% -&gt; 51.2%). We also present 3 IMO problems solved by Lyra. Webelieve Tool Correction (post-process for hallucination mitigation) andConjecture Correction (subgoal adjustment from interaction with environment)could provide a promising avenue for future research in this field.</description><author>Chuanyang Zheng, Haiming Wang, Enze Xie, Zhengying Liu, Jiankai Sun, Huajian Xin, Jianhao Shen, Zhenguo Li, Yu Li</author><pubDate>Mon, 02 Oct 2023 11:32:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15806v2</guid></item><item><title>On the Trade-off Between Efficiency and Precision of Neural Abstraction</title><link>http://arxiv.org/abs/2307.15546v2</link><description>Neural abstractions have been recently introduced as formal approximations ofcomplex, nonlinear dynamical models. They comprise a neural ODE and a certifiedupper bound on the error between the abstract neural network and the concretedynamical model. So far neural abstractions have exclusively been obtained asneural networks consisting entirely of $ReLU$ activation functions, resultingin neural ODE models that have piecewise affine dynamics, and which can beequivalently interpreted as linear hybrid automata. In this work, we observethat the utility of an abstraction depends on its use: some scenarios mightrequire coarse abstractions that are easier to analyse, whereas others mightrequire more complex, refined abstractions. We therefore consider neuralabstractions of alternative shapes, namely either piecewise constant ornonlinear non-polynomial (specifically, obtained via sigmoidal activations). Weemploy formal inductive synthesis procedures to generate neural abstractionsthat result in dynamical models with these semantics. Empirically, wedemonstrate the trade-off that these different neural abstraction templateshave vis-a-vis their precision and synthesis time, as well as the time requiredfor their safety verification (done via reachability computation). We improveexisting synthesis techniques to enable abstraction of higher-dimensionalmodels, and additionally discuss the abstraction of complex neural ODEs toimprove the efficiency of reachability analysis for these models.</description><author>Alec Edwards, Mirco Giacobbe, Alessandro Abate</author><pubDate>Mon, 02 Oct 2023 10:56:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15546v2</guid></item><item><title>Compressor-Based Classification for Atrial Fibrillation Detection</title><link>http://arxiv.org/abs/2308.13328v2</link><description>Atrial fibrillation (AF) is one of the most common arrhythmias withchallenging public health implications. Therefore, automatic detection of AFepisodes on ECG is one of the essential tasks in biomedical engineering. Inthis paper, we applied the recently introduced method of compressor-based textclassification with gzip algorithm for AF detection (binary classificationbetween heart rhythms). We investigated the normalized compression distanceapplied to RR-interval and $\Delta$RR-interval sequences ($\Delta$RR-intervalis the difference between subsequent RR-intervals). Here, the configuration ofthe k-nearest neighbour classifier, an optimal window length, and the choice ofdata types for compression were analyzed. We achieved good classificationresults while learning on the full MIT-BIH Atrial Fibrillation database, closeto the best specialized AF detection algorithms (avg. sensitivity = 97.1\%,avg. specificity = 91.7\%, best sensitivity of 99.8\%, best specificity of97.6\% with fivefold cross-validation). In addition, we evaluated theclassification performance under the few-shot learning setting. Our resultssuggest that gzip compression-based classification, originally proposed fortexts, is suitable for biomedical data and quantized continuous stochasticsequences in general.</description><author>Nikita Markov, Konstantin Ushenin, Yakov Bozhko, Olga Solovyova</author><pubDate>Mon, 02 Oct 2023 10:56:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13328v2</guid></item><item><title>L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models</title><link>http://arxiv.org/abs/2309.17446v2</link><description>Recently, large language models (LLMs), especially those that are pretrainedon code, have demonstrated strong capabilities in generating programs fromnatural language inputs in a few-shot or even zero-shot manner. Despitepromising results, there is a notable lack of a comprehensive evaluation ofthese models language-to-code generation capabilities. Existing studies oftenfocus on specific tasks, model architectures, or learning paradigms, leading toa fragmented understanding of the overall landscape. In this work, we presentL2CEval, a systematic evaluation of the language-to-code generationcapabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing,math reasoning and Python programming, analyzing the factors that potentiallyaffect their performance, such as model size, pretraining data, instructiontuning, and different prompting methods. In addition to assessing modelperformance, we measure confidence calibration for the models and conduct humanevaluations of the output programs. This enables us to identify and analyze thetypical failure modes across various tasks and models. L2CEval offers acomprehensive understanding of the capabilities and limitations of LLMs inlanguage-to-code generation. We also release the evaluation framework and allmodel outputs, hoping to lay the groundwork for further future research in thisdomain.</description><author>Ansong Ni, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, Ye Liu, Semih Yavuz, Caiming Xiong, Shafiq Joty, Yingbo Zhou, Dragomir Radev, Arman Cohan</author><pubDate>Mon, 02 Oct 2023 10:54:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17446v2</guid></item><item><title>UniQuadric: A SLAM Backend for Unknown Rigid Object 3D Tracking and Light-Weight Modeling</title><link>http://arxiv.org/abs/2309.17036v2</link><description>Tracking and modeling unknown rigid objects in the environment play a crucialrole in autonomous unmanned systems and virtual-real interactive applications.However, many existing Simultaneous Localization, Mapping and Moving ObjectTracking (SLAMMOT) methods focus solely on estimating specific object poses andlack estimation of object scales and are unable to effectively track unknownobjects. In this paper, we propose a novel SLAM backend that unifies ego-motiontracking, rigid object motion tracking, and modeling within a jointoptimization framework. In the perception part, we designed a pixel-levelasynchronous object tracker (AOT) based on the Segment Anything Model (SAM) andDeAOT, enabling the tracker to effectively track target unknown objects guidedby various predefined tasks and prompts. In the modeling part, we present anovel object-centric quadric parameterization to unify both static and dynamicobject initialization and optimization. Subsequently, in the part of objectstate estimation, we propose a tightly coupled optimization model for objectpose and scale estimation, incorporating hybrids constraints into a novel dualsliding window optimization framework for joint estimation. To our knowledge,we are the first to tightly couple object pose tracking with light-weightmodeling of dynamic and static objects using quadric. We conduct qualitativeand quantitative experiments on simulation datasets and real-world datasets,demonstrating the state-of-the-art robustness and accuracy in motion estimationand modeling. Our system showcases the potential application of objectperception in complex dynamic scenes.</description><author>Linghao Yang, Yanmin Wu, Yu Deng, Rui Tian, Xinggang Hu, Tiefeng Ma</author><pubDate>Mon, 02 Oct 2023 10:50:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17036v2</guid></item><item><title>Disentangling the Link Between Image Statistics and Human Perception</title><link>http://arxiv.org/abs/2303.09874v2</link><description>In the 1950s, Barlow and Attneave hypothesised a link between biologicalvision and information maximisation. Following Shannon, information was definedusing the probability of natural images. A number of physiological andpsychophysical phenomena have been derived ever since from principles likeinfo-max, efficient coding, or optimal denoising. However, it remains unclearhow this link is expressed in mathematical terms from image probability. First,classical derivations were subjected to strong assumptions on the probabilitymodels and on the behaviour of the sensors. Moreover, the direct evaluation ofthe hypothesis was limited by the inability of the classical image models todeliver accurate estimates of the probability. In this work we directlyevaluate image probabilities using an advanced generative model for naturalimages, and we analyse how probability-related factors can be combined topredict human perception via sensitivity of state-of-the-art subjective imagequality metrics. We use information theory and regression analysis to find acombination of just two probability-related factors that achieves 0.8correlation with subjective metrics. This probability-based sensitivity ispsychophysically validated by reproducing the basic trends of the ContrastSensitivity Function, its suprathreshold variation, and trends of the Weber-lawand masking.</description><author>Alexander Hepburn, Valero Laparra, Raúl Santos-Rodriguez, Jesús Malo</author><pubDate>Mon, 02 Oct 2023 10:40:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09874v2</guid></item><item><title>Short-length SSVEP data extension by a novel generative adversarial networks based framework</title><link>http://arxiv.org/abs/2301.05599v5</link><description>Steady-state visual evoked potentials (SSVEPs) based brain-computer interface(BCI) has received considerable attention due to its high information transferrate (ITR) and available quantity of targets. However, the performance offrequency identification methods heavily hinges on the amount of usercalibration data and data length, which hinders the deployment in real-worldapplications. Recently, generative adversarial networks (GANs)-based datageneration methods have been widely adopted to create syntheticelectroencephalography (EEG) data, holds promise to address these issues. Inthis paper, we proposed a GAN-based end-to-end signal transformation networkfor Time-window length Extension, termed as TEGAN. TEGAN transformsshort-length SSVEP signals into long-length artificial SSVEP signals. Byincorporating a novel U-Net generator architecture and an auxiliary classifierinto the network architecture, the TEGAN could produce conditioned features inthe synthetic data. Additionally, we introduced a two-stage training strategyand the LeCam-divergence regularization term to regularize the training processof GAN during the network implementation. The proposed TEGAN was evaluated ontwo public SSVEP datasets (a 4-class dataset and a 12-class dataset). With theassistance of TEGAN, the performance of traditional frequency recognitionmethods and deep learning-based methods have been significantly improved underlimited calibration data. And the classification performance gap of variousfrequency recognition methods has been narrowed. This study substantiates thefeasibility of the proposed method to extend the data length for short-timeSSVEP signals for developing a high-performance BCI system. The proposedGAN-based methods have the great potential of shortening the calibration timeand cutting down the budget for various real-world BCI-based applications.</description><author>Yudong Pan, Ning Li, Yangsong Zhang, Peng Xu, Dezhong Yao</author><pubDate>Mon, 02 Oct 2023 10:26:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.05599v5</guid></item><item><title>Supervision Adaptation Balancing In-distribution Generalization and Out-of-distribution Detection</title><link>http://arxiv.org/abs/2206.09380v2</link><description>The discrepancy between in-distribution (ID) and out-of-distribution (OOD)samples can lead to \textit{distributional vulnerability} in deep neuralnetworks, which can subsequently lead to high-confidence predictions for OODsamples. This is mainly due to the absence of OOD samples during training,which fails to constrain the network properly. To tackle this issue, severalstate-of-the-art methods include adding extra OOD samples to training andassign them with manually-defined labels. However, this practice can introduceunreliable labeling, negatively affecting ID classification. The distributionalvulnerability presents a critical challenge for non-IID deep learning, whichaims for OOD-tolerant ID classification by balancing ID generalization and OODdetection. In this paper, we introduce a novel \textit{supervision adaptation}approach to generate adaptive supervision information for OOD samples, makingthem more compatible with ID samples. Firstly, we measure the dependencybetween ID samples and their labels using mutual information, revealing thatthe supervision information can be represented in terms of negativeprobabilities across all classes. Secondly, we investigate data correlationsbetween ID and OOD samples by solving a series of binary regression problems,with the goal of refining the supervision information for more distinctlyseparable ID classes. Our extensive experiments on four advanced networkarchitectures, two ID datasets, and eleven diversified OOD datasets demonstratethe efficacy of our supervision adaptation approach in improving both IDclassification and OOD detection capabilities.</description><author>Zhilin Zhao, Longbing Cao, Kun-Yu Lin</author><pubDate>Mon, 02 Oct 2023 10:00:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.09380v2</guid></item><item><title>Token-Level Serialized Output Training for Joint Streaming ASR and ST Leveraging Textual Alignments</title><link>http://arxiv.org/abs/2307.03354v2</link><description>In real-world applications, users often require both translations andtranscriptions of speech to enhance their comprehension, particularly instreaming scenarios where incremental generation is necessary. This paperintroduces a streaming Transformer-Transducer that jointly generates automaticspeech recognition (ASR) and speech translation (ST) outputs using a singledecoder. To produce ASR and ST content effectively with minimal latency, wepropose a joint token-level serialized output training method that interleavessource and target words by leveraging an off-the-shelf textual aligner.Experiments in monolingual (it-en) and multilingual (\{de,es,it\}-en) settingsdemonstrate that our approach achieves the best quality-latency balance. Withan average ASR latency of 1s and ST latency of 1.3s, our model shows nodegradation or even improves output quality compared to separate ASR and STmodels, yielding an average improvement of 1.1 WER and 0.4 BLEU in themultilingual case.</description><author>Sara Papi, Peidong Wang, Junkun Chen, Jian Xue, Jinyu Li, Yashesh Gaur</author><pubDate>Mon, 02 Oct 2023 09:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03354v2</guid></item><item><title>GLISp-r: A preference-based optimization algorithm with convergence guarantees</title><link>http://arxiv.org/abs/2202.01125v2</link><description>Preference-based optimization algorithms are iterative procedures that seekthe optimal calibration of a decision vector based only on comparisons betweencouples of different tunings. At each iteration, a human decision-makerexpresses a preference between two calibrations (samples), highlighting whichone, if any, is better than the other. The optimization procedure must use theobserved preferences to find the tuning of the decision vector that is mostpreferred by the decision-maker, while also minimizing the number ofcomparisons. In this work, we formulate the preference-based optimizationproblem from a utility theory perspective. Then, we propose GLISp-r, anextension of a recent preference-based optimization procedure called GLISp. Thelatter uses a Radial Basis Function surrogate to describe the tastes of thedecision-maker. Iteratively, GLISp proposes new samples to compare with thebest calibration available by trading off exploitation of the surrogate modeland exploration of the decision space. In GLISp-r, we propose a differentcriterion to use when looking for new candidate samples that is inspired byMSRS, a popular procedure in the black-box optimization framework. Compared toGLISp, GLISp-r is less likely to get stuck on local optima of thepreference-based optimization problem. We motivate this claim theoretically,with a proof of global convergence, and empirically, by comparing theperformances of GLISp and GLISp-r on several benchmark optimization problems.</description><author>Davide Previtali, Mirko Mazzoleni, Antonio Ferramosca, Fabio Previdi</author><pubDate>Mon, 02 Oct 2023 09:39:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.01125v2</guid></item><item><title>Adversarial Likelihood Estimation With One-Way Flows</title><link>http://arxiv.org/abs/2307.09882v3</link><description>Generative Adversarial Networks (GANs) can produce high-quality samples, butdo not provide an estimate of the probability density around the samples.However, it has been noted that maximizing the log-likelihood within anenergy-based setting can lead to an adversarial framework where thediscriminator provides unnormalized density (often called energy). We furtherdevelop this perspective, incorporate importance sampling, and show that 1)Wasserstein GAN performs a biased estimate of the partition function, and wepropose instead to use an unbiased estimator; and 2) when optimizing forlikelihood, one must maximize generator entropy. This is hypothesized toprovide a better mode coverage. Different from previous works, we explicitlycompute the density of the generated samples. This is the key enabler todesigning an unbiased estimator of the partition function and computation ofthe generator entropy term. The generator density is obtained via a new type offlow network, called one-way flow network, that is less constrained in terms ofarchitecture, as it does not require a tractable inverse function. Ourexperimental results show that our method converges faster, produces comparablesample quality to GANs with similar architecture, successfully avoidsover-fitting to commonly used datasets and produces smooth low-dimensionallatent representations of the training data.</description><author>Omri Ben-Dov, Pravir Singh Gupta, Victoria Abrevaya, Michael J. Black, Partha Ghosh</author><pubDate>Mon, 02 Oct 2023 09:35:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09882v3</guid></item><item><title>Modality-Independent Teachers Meet Weakly-Supervised Audio-Visual Event Parser</title><link>http://arxiv.org/abs/2305.17343v2</link><description>Audio-visual learning has been a major pillar of multi-modal machinelearning, where the community mostly focused on its modality-aligned setting,i.e., the audio and visual modality are both assumed to signal the predictiontarget. With the Look, Listen, and Parse dataset (LLP), we investigate theunder-explored unaligned setting, where the goal is to recognize audio andvisual events in a video with only weak labels observed. Such weak video-levellabels only tell what events happen without knowing the modality they areperceived (audio, visual, or both). To enhance learning in this challengingsetting, we incorporate large-scale contrastively pre-trained models as themodality teachers. A simple, effective, and generic method, termed Visual-AudioLabel Elaboration (VALOR), is innovated to harvest modality labels for thetraining events. Empirical studies show that the harvested labels significantlyimprove an attentional baseline by 8.0 in average F-score (Type@AV).Surprisingly, we found that modality-independent teachers outperform theirmodality-fused counterparts since they are noise-proof from the otherpotentially unaligned modality. Moreover, our best model achieves the newstate-of-the-art on all metrics of LLP by a substantial margin (+5.4 F-scorefor Type@AV). VALOR is further generalized to Audio-Visual Event Localizationand achieves the new state-of-the-art as well. Code is available at:https://github.com/Franklin905/VALOR.</description><author>Yung-Hsuan Lai, Yen-Chun Chen, Yu-Chiang Frank Wang</author><pubDate>Mon, 02 Oct 2023 09:34:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17343v2</guid></item><item><title>Subject-driven Text-to-Image Generation via Apprenticeship Learning</title><link>http://arxiv.org/abs/2304.00186v5</link><description>Recent text-to-image generation models like DreamBooth have made remarkableprogress in generating highly customized images of a target subject, byfine-tuning an ``expert model'' for a given subject from a few examples.However, this process is expensive, since a new expert model must be learnedfor each subject. In this paper, we present SuTI, a Subject-drivenText-to-Image generator that replaces subject-specific fine tuning within-context learning. Given a few demonstrations of a new subject, SuTI caninstantly generate novel renditions of the subject in different scenes, withoutany subject-specific optimization. SuTI is powered by apprenticeship learning,where a single apprentice model is learned from data generated by a massivenumber of subject-specific expert models. Specifically, we mine millions ofimage clusters from the Internet, each centered around a specific visualsubject. We adopt these clusters to train a massive number of expert models,each specializing in a different subject. The apprentice model SuTI then learnsto imitate the behavior of these fine-tuned experts. SuTI can generatehigh-quality and customized subject-specific images 20x faster thanoptimization-based SoTA methods. On the challenging DreamBench andDreamBench-v2, our human evaluation shows that SuTI significantly outperformsexisting models like InstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt,Re-Imagen and DreamBooth, especially on the subject and text alignment aspects.</description><author>Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, William W. Cohen</author><pubDate>Mon, 02 Oct 2023 09:08:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.00186v5</guid></item><item><title>Benchmarking Collaborative Learning Methods Cost-Effectiveness for Prostate Segmentation</title><link>http://arxiv.org/abs/2309.17097v2</link><description>Healthcare data is often split into medium/small-sized collections acrossmultiple hospitals and access to it is encumbered by privacy regulations. Thisbrings difficulties to use them for the development of machine learning anddeep learning models, which are known to be data-hungry. One way to overcomethis limitation is to use collaborative learning (CL) methods, which allowhospitals to work collaboratively to solve a task, without the need toexplicitly share local data. In this paper, we address a prostate segmentation problem from MRI in acollaborative scenario by comparing two different approaches: federatedlearning (FL) and consensus-based methods (CBM). To the best of our knowledge, this is the first work in which CBM, such aslabel fusion techniques, are used to solve a problem of collaborative learning.In this setting, CBM combine predictions from locally trained models to obtaina federated strong learner with ideally improved robustness and predictivevariance properties. Our experiments show that, in the considered practical scenario, CBMs provideequal or better results than FL, while being highly cost-effective. Our resultsdemonstrate that the consensus paradigm may represent a valid alternative to FLfor typical training tasks in medical imaging.</description><author>Lucia Innocenti, Michela Antonelli, Francesco Cremonesi, Kenaan Sarhan, Alejandro Granados, Vicky Goh, Sebastien Ourselin, Marco Lorenzi</author><pubDate>Mon, 02 Oct 2023 08:45:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17097v2</guid></item><item><title>HUST bearing: a practical dataset for ball bearing fault diagnosis</title><link>http://arxiv.org/abs/2302.12533v2</link><description>In this work, we introduce a practical dataset named HUST bearing, thatprovides a large set of vibration data on different ball bearings. This datasetcontains 90 raw vibration data of 6 types of defects (inner crack, outer crack,ball crack, and their 2-combinations) on 5 types of bearing at 3 workingconditions with the sample rate of 51,200 samples per second. We establishedthe envelope analysis and order tracking analysis on the introduced dataset toallow an initial evaluation of the data. A number of classical machine learningclassification methods are used to identify bearing faults of the dataset usingfeatures in different domains. The typical advanced unsupervised transferlearning algorithms also perform to observe the transferability of knowledgeamong parts of the dataset. The experimental results of examined methods on thedataset gain divergent accuracy up to 100% on classification task and 60-80% onunsupervised transfer learning task.</description><author>Nguyen Duc Thuan, Hoang Si Hong</author><pubDate>Mon, 02 Oct 2023 08:38:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12533v2</guid></item><item><title>PDIWS: Thermal Imaging Dataset for Person Detection in Intrusion Warning Systems</title><link>http://arxiv.org/abs/2302.13293v2</link><description>In this paper, we present a synthetic thermal imaging dataset for PersonDetection in Intrusion Warning Systems (PDIWS). The dataset consists of atraining set with 2000 images and a test set with 500 images. Each image issynthesized by compounding a subject (intruder) with a background using themodified Poisson image editing method. There are a total of 50 differentbackgrounds and nearly 1000 subjects divided into five classes according tofive human poses: creeping, crawling, stooping, climbing and other. Thepresence of the intruder will be confirmed if the first four poses aredetected. Advanced object detection algorithms have been implemented with thisdataset and give relatively satisfactory results, with the highest mAP valuesof 95.5% and 90.9% for IoU of 0.5 and 0.75 respectively. The dataset is freelypublished online for research purposes athttps://github.com/thuan-researcher/Intruder-Thermal-Dataset.</description><author>Nguyen Duc Thuan, Le Hai Anh, Hoang Si Hong</author><pubDate>Mon, 02 Oct 2023 08:37:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13293v2</guid></item><item><title>Demystifying CLIP Data</title><link>http://arxiv.org/abs/2309.16671v3</link><description>Contrastive Language-Image Pre-training (CLIP) is an approach that hasadvanced research and applications in computer vision, fueling modernrecognition systems and generative models. We believe that the main ingredientto the success of CLIP is its data and not the model architecture orpre-training objective. However, CLIP only provides very limited informationabout its data and how it has been collected, leading to works that aim toreproduce CLIP's data by filtering with its model parameters. In this work, weintend to reveal CLIP's data curation approach and in our pursuit of making itopen to the community introduce Metadata-Curated Language-Image Pre-training(MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP'sconcepts) and yields a balanced subset over the metadata distribution. Ourexperimental study rigorously isolates the model and training settings,concentrating solely on data. MetaCLIP applied to CommonCrawl with 400Mimage-text data pairs outperforms CLIP's data on multiple standard benchmarks.In zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy,surpassing CLIP's 68.3% on ViT-B models. Scaling to 1B data, while maintainingthe same training budget, attains 72.4%. Our observations hold across variousmodel sizes, exemplified by ViT-H achieving 80.5%, without anybells-and-whistles. Curation code and training data distribution on metadata ismade available at https://github.com/facebookresearch/MetaCLIP.</description><author>Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, Christoph Feichtenhofer</author><pubDate>Mon, 02 Oct 2023 08:12:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16671v3</guid></item><item><title>On decoder-only architecture for speech-to-text and large language model integration</title><link>http://arxiv.org/abs/2307.03917v3</link><description>Large language models (LLMs) have achieved remarkable success in the field ofnatural language processing, enabling better human-computer interaction usingnatural language. However, the seamless integration of speech signals into LLMshas not been explored well. The "decoder-only" architecture has also not beenwell studied for speech processing tasks. In this research, we introduceSpeech-LLaMA, a novel approach that effectively incorporates acousticinformation into text-based large language models. Our method leveragesConnectionist Temporal Classification and a simple audio encoder to map thecompressed acoustic features to the continuous semantic space of the LLM. Inaddition, we further probe the decoder-only architecture for speech-to-texttasks by training a smaller scale randomly initialized speech-LLaMA model fromspeech-text paired data alone. We conduct experiments on multilingualspeech-to-text translation tasks and demonstrate a significant improvement overstrong baselines, highlighting the potential advantages of decoder-only modelsfor speech-to-text conversion.</description><author>Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, Yu Wu</author><pubDate>Mon, 02 Oct 2023 07:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03917v3</guid></item><item><title>LDPC codes: comparing cluster graphs to factor graphs</title><link>http://arxiv.org/abs/2204.06350v2</link><description>We present a comparison study between a cluster and factor graphrepresentation of LDPC codes. In probabilistic graphical models, cluster graphsretain useful dependence between random variables during inference, which areadvantageous in terms of computational cost, convergence speed, and accuracy ofmarginal probabilities. This study investigates these benefits in the contextof LDPC codes and shows that a cluster graph representation outperforms thetraditional factor graph representation.</description><author>J du Toit, J du Preez, R Wolhuter</author><pubDate>Mon, 02 Oct 2023 07:23:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.06350v2</guid></item><item><title>LDPC codes: tracking non-stationary channel noise using sequential variational Bayesian estimates</title><link>http://arxiv.org/abs/2204.07037v2</link><description>We present a sequential Bayesian learning method for tracking non-stationarysignal-to-noise ratios in LDPC codes using probabilistic graphical models. Werepresent the LDPC code as a cluster graph using a general purpose clustergraph construction algorithm called the layered trees running intersectionproperty (LTRIP) algorithm. The channel noise estimator is a global Gammacluster, which we extend to allow for Bayesian tracking of non-stationary noisevariation. We evaluate our proposed model on real-world 5G drive test data. Ourresults show that our model is capable of tracking non-stationary channelnoise, which outperforms an LDPC code with a fixed knowledge of the actualaverage channel noise.</description><author>J du Toit, J du Preez, R Wolhuter</author><pubDate>Mon, 02 Oct 2023 07:17:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.07037v2</guid></item><item><title>Textbooks Are All You Need</title><link>http://arxiv.org/abs/2306.11644v2</link><description>We introduce phi-1, a new large language model for code, with significantlysmaller size than competing models: phi-1 is a Transformer-based model with1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbookquality" data from the web (6B tokens) and synthetically generated textbooksand exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attainspass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displayssurprising emergent properties compared to phi-1-base, our model before ourfinetuning stage on a dataset of coding exercises, and phi-1-small, a smallermodel with 350M parameters trained with the same pipeline as phi-1 that stillachieves 45% on HumanEval.</description><author>Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, Yuanzhi Li</author><pubDate>Mon, 02 Oct 2023 07:12:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11644v2</guid></item><item><title>Slingshot Perturbation to Learning in Monotone Games</title><link>http://arxiv.org/abs/2305.16610v2</link><description>This paper addresses the problem of learning Nash equilibria in {\it monotonegames} where the gradient of the payoff functions is monotone in the strategyprofile space, potentially containing additive noise. The optimistic family oflearning algorithms, exemplified by optimistic Follow-the-Regularized-Leaderand optimistic Mirror Descent, successfully achieves last-iterate convergencein scenarios devoid of noise, leading the dynamics to a Nash equilibrium. Arecent emerging trend underscores the promise of the perturbation approach,where payoff functions are perturbed based on the distance from an anchoring,or {\it slingshot}, strategy. In response, we first establish a unifiedframework for learning equilibria in monotone games, accommodating both fulland noisy feedback. Second, we construct the convergence rates toward anapproximated equilibrium, irrespective of noise presence. Thirdly, we introducea twist by updating the slingshot strategy, anchoring the current strategy atfinite intervals. This innovation empowers us to identify the exact Nashequilibrium of the underlying game with guaranteed rates. The proposedframework is all-encompassing, integrating existing payoff-perturbedalgorithms. Finally, empirical demonstrations affirm that our algorithms,grounded in this framework, exhibit significantly accelerated convergence.</description><author>Kenshi Abe, Kaito Ariu, Mitsuki Sakamoto, Atsushi Iwasaki</author><pubDate>Mon, 02 Oct 2023 07:11:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16610v2</guid></item><item><title>PlaceNav: Topological Navigation through Place Recognition</title><link>http://arxiv.org/abs/2309.17260v2</link><description>Recent results suggest that splitting topological navigation intorobot-independent and robot-specific components improves navigation performanceby enabling the robot-independent part to be trained with data collected bydifferent robot types. However, the navigation methods are still limited by thescarcity of suitable training data and suffer from poor computational scaling.In this work, we present~\methodname, subdividing the robot-independent partinto navigation-specific and generic computer vision components. We utilizevisual place recognition for the subgoal selection of the topologicalnavigation pipeline. This makes subgoal selection more efficient and enablesleveraging large-scale datasets from non-robotics sources, increasing trainingdata availability. Bayes filtering, enabled by place recognition, furtherimproves navigation performance by increasing the temporal consistency ofsubgoals. Our experimental results verify the design and the new model obtainsa 76% higher success rate in indoor and 23% higher in outdoor navigation taskswith higher computational efficiency.</description><author>Lauri Suomela, Jussi Kalliola, Harry Edelman, Joni-Kristian Kämäräinen</author><pubDate>Mon, 02 Oct 2023 07:05:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17260v2</guid></item><item><title>PAGAR: Taming Reward Misalignment in Inverse Reinforcement Learning-Based Imitation Learning with Protagonist Antagonist Guided Adversarial Reward</title><link>http://arxiv.org/abs/2306.01731v2</link><description>Many imitation learning (IL) algorithms employ inverse reinforcement learning(IRL) to infer the underlying reward function that an expert is implicitlyoptimizing for, based on their demonstrated behaviors. However, a misalignmentbetween the inferred reward and the true task objective can result in taskfailures. In this paper, we introduce Protagonist Antagonist Guided AdversarialReward (PAGAR), a semi-supervised reward design paradigm to tackle this rewardmisalignment problem in IRL-based IL. We identify the conditions on thecandidate reward functions under which PAGAR can guarantee to induce a policythat succeeds in the underlying task. Furthermore, we present a practicalon-and-off policy approach to implement PAGAR in IRL-based IL. Experimentalresults show that our algorithm outperforms competitive baselines on complex ILtasks and zero-shot IL tasks in transfer environments with limiteddemonstrations.</description><author>Weichao Zhou, Wenchao Li</author><pubDate>Mon, 02 Oct 2023 06:47:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01731v2</guid></item><item><title>Reverse Diffusion Monte Carlo</title><link>http://arxiv.org/abs/2307.02037v2</link><description>The efficacy of modern generative models is commonly contingent upon theprecision of score estimation along the diffusion path, with a focus ondiffusion models and their ability to generate high-quality data samples. Thisstudy delves into the application of reverse diffusion to Monte Carlo sampling.It is shown that score estimation can be transformed into a mean estimationproblem via the decomposition of the transition kernel. By estimating the meanof the posterior distribution, we derive a novel Monte Carlo sampling algorithmfrom the reverse diffusion process, which is distinct from traditional MarkovChain Monte Carlo (MCMC) methods. We calculate the error requirements andsample size for the posterior distribution, and use the result to derive analgorithm that can approximate the target distribution to any desired accuracy.Additionally, by estimating the log-Sobolev constant of the posteriordistribution, we show under suitable conditions the problem of sampling fromthe posterior can be easier than direct sampling from the target distributionusing traditional MCMC techniques. For Gaussian mixture models, we demonstratethat the new algorithm achieves significant improvement over the traditionalLangevin-style MCMC sampling methods both theoretically and practically. Ouralgorithm offers a new perspective and solution beyond classical MCMCalgorithms for challenging complex distributions.</description><author>Xunpeng Huang, Hanze Dong, Yifan Hao, Yian Ma, Tong Zhang</author><pubDate>Mon, 02 Oct 2023 06:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02037v2</guid></item><item><title>Task Graph offloading via Deep Reinforcement Learning in Mobile Edge Computing</title><link>http://arxiv.org/abs/2309.10569v2</link><description>Various mobile applications that comprise dependent tasks are gainingwidespread popularity and are increasingly complex. These applications oftenhave low-latency requirements, resulting in a significant surge in demand forcomputing resources. With the emergence of mobile edge computing (MEC), itbecomes the most significant issue to offload the application tasks ontosmall-scale devices deployed at the edge of the mobile network for obtaining ahigh-quality user experience. However, since the environment of MEC is dynamic,most existing works focusing on task graph offloading, which rely heavily onexpert knowledge or accurate analytical models, fail to fully adapt to suchenvironmental changes, resulting in the reduction of user experience. Thispaper investigates the task graph offloading in MEC, considering thetime-varying computation capabilities of edge computing devices. To adapt toenvironmental changes, we model the task graph scheduling for computationoffloading as a Markov Decision Process (MDP). Then, we design a deepreinforcement learning algorithm (SATA-DRL) to learn the task schedulingstrategy from the interaction with the environment, to improve user experience.Extensive simulations validate that SATA-DRL is superior to existing strategiesin terms of reducing average makespan and deadline violation.</description><author>Jiagang Liu, Yun Mi, Xinyu Zhang</author><pubDate>Mon, 02 Oct 2023 06:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10569v2</guid></item><item><title>LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud</title><link>http://arxiv.org/abs/2309.17157v2</link><description>In the current user-server interaction paradigm of prompted generation withlarge language models (LLM) on cloud, the server fully controls the generationprocess, which leaves zero options for users who want to keep the generatedtext to themselves. We propose LatticeGen, a cooperative framework in which theserver still handles most of the computation while the user controls thesampling operation. The key idea is that the true generated sequence is mixedwith noise tokens by the user and hidden in a noised lattice. Consideringpotential attacks from a hypothetically malicious server and how the user candefend against it, we propose the repeated beam-search attack and the mixingnoise scheme. In our experiments we apply LatticeGen to protect both prompt andgeneration. It is shown that while the noised lattice degrades generationquality, LatticeGen successfully protects the true generation to a remarkabledegree under strong attacks (more than 50% of the semantic remains hidden asmeasured by BERTScore).</description><author>Mengke Zhang, Tianxing He, Tianle Wang, Lu Mi, Fatemehsadat Mireshghallah, Binyi Chen, Hao Wang, Yulia Tsvetkov</author><pubDate>Mon, 02 Oct 2023 06:44:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17157v2</guid></item><item><title>MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems</title><link>http://arxiv.org/abs/2307.11394v2</link><description>MeetEval is an open-source toolkit to evaluate all kinds of meetingtranscription systems. It provides a unified interface for the computation ofcommonly used Word Error Rates (WERs), specifically cpWER, ORC WER and MIMO WERalong other WER definitions. We extend the cpWER computation by a temporalconstraint to ensure that only words are identified as correct when thetemporal alignment is plausible. This leads to a better quality of the matchingof the hypothesis string to the reference string that more closely resemblesthe actual transcription quality, and a system is penalized if it provides poortime annotations. Since word-level timing information is often not available,we present a way to approximate exact word-level timings from segment-leveltimings (e.g., a sentence) and show that the approximation leads to a similarWER as a matching with exact word-level annotations. At the same time, the timeconstraint leads to a speedup of the matching algorithm, which outweighs theadditional overhead caused by processing the time stamps.</description><author>Thilo von Neumann, Christoph Boeddeker, Marc Delcroix, Reinhold Haeb-Umbach</author><pubDate>Mon, 02 Oct 2023 06:25:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11394v2</guid></item><item><title>Hierarchical Relationships: A New Perspective to Enhance Scene Graph Generation</title><link>http://arxiv.org/abs/2303.06842v3</link><description>This paper presents a finding that leveraging the hierarchical structuresamong labels for relationships and objects can substantially improve theperformance of scene graph generation systems. The focus of this work is tocreate an informative hierarchical structure that can divide object andrelationship categories into disjoint super-categories in a systematic way.Specifically, we introduce a Bayesian prediction head to jointly predict thesuper-category of relationships between a pair of object instances, as well asthe detailed relationship within that super-category simultaneously,facilitating more informative predictions. The resulting model exhibits thecapability to produce a more extensive set of predicates beyond the datasetannotations, and to tackle the prevalent issue of low annotation quality. Whileour paper presents preliminary findings, experiments on the Visual Genomedataset show its strong performance, particularly in predicate classificationsand zero-shot settings, that demonstrates the promise of our approach.</description><author>Bowen Jiang, Camillo J. Taylor</author><pubDate>Mon, 02 Oct 2023 06:05:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06842v3</guid></item><item><title>Diffeomorphic Deformation via Sliced Wasserstein Distance Optimization for Cortical Surface Reconstruction</title><link>http://arxiv.org/abs/2305.17555v2</link><description>Mesh deformation is a core task for 3D mesh reconstruction, but defining anefficient discrepancy between predicted and target meshes remains an openproblem. A prevalent approach in current deep learning is the set-basedapproach which measures the discrepancy between two surfaces by comparing tworandomly sampled point-clouds from the two meshes with Chamfer pseudo-distance.Nevertheless, the set-based approach still has limitations such as lacking atheoretical guarantee for choosing the number of points in sampledpoint-clouds, and the pseudo-metricity and the quadratic complexity of theChamfer divergence. To address these issues, we propose a novel metric forlearning mesh deformation. The metric is defined by sliced Wasserstein distanceon meshes represented as probability measures that generalize the set-basedapproach. By leveraging probability measure space, we gain flexibility inencoding meshes using diverse forms of probability measures, such ascontinuous, empirical, and discrete measures via \textit{varifold}representation. After having encoded probability measures, we can comparemeshes by using the sliced Wasserstein distance which is an effective optimaltransport distance with linear computational complexity and can provide a faststatistical rate for approximating the surface of meshes. Furthermore, weemploy a neural ordinary differential equation (ODE) to deform the inputsurface into the target shape by modeling the trajectories of the points on thesurface. Our experiments on cortical surface reconstruction demonstrate thatour approach surpasses other competing methods in multiple datasets andmetrics.</description><author>Tung Le, Khai Nguyen, Shanlin Sun, Kun Han, Nhat Ho, Xiaohui Xie</author><pubDate>Mon, 02 Oct 2023 05:47:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17555v2</guid></item><item><title>RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought</title><link>http://arxiv.org/abs/2305.11499v2</link><description>Large language Models (LLMs) have achieved promising performance onarithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT)prompting. However, LLMs face challenges in maintaining factual consistencyduring reasoning, exhibiting tendencies to condition overlooking, questionmisinterpretation, and condition hallucination over given problems. Existingmethods use coarse-grained feedback (e.g., whether the answer is correct) toimprove factual consistency. In this work, we propose RCoT (ReversingChain-of-Thought), a novel method to improve LLMs' reasoning abilities byautomatically detecting and rectifying factual inconsistency in LLMs, generatedsolutions. To detect factual inconsistency, RCoT first asks LLMs to reconstructthe problem based on generated solutions. Then fine-grained comparisons betweenthe original problem and the reconstructed problem expose the factualinconsistency in the original solutions. To rectify the solution, RCoTformulates detected factual inconsistency into fine-grained feedback to guideLLMs in revising solutions. Experimental results demonstrate improvements ofRCoT over standard CoT, Self-Consistency and Self-Refine across sevenarithmetic datasets. Moreover, we find that manually written fine-grainedfeedback can dramatically improve LLMs' reasoning abilities (e.g., ChatGPTreaches 94.6% accuracy on GSM8K), encouraging the community to further explorethe fine-grained feedback generation methods.</description><author>Tianci Xue, Ziqi Wang, Zhenhailong Wang, Chi Han, Pengfei Yu, Heng Ji</author><pubDate>Mon, 02 Oct 2023 04:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11499v2</guid></item><item><title>Memorization Through the Lens of Curvature of Loss Function Around Samples</title><link>http://arxiv.org/abs/2307.05831v2</link><description>Deep neural networks are over-parameterized and easily overfit the datasetsthey train on. In the extreme case, it has been shown that these networks canmemorize a training set with fully randomized labels. We propose using thecurvature of loss function around each training sample, averaged over trainingepochs, as a measure of memorization of the sample. We use this metric to studythe generalization versus memorization properties of different samples inpopular image datasets and show that it captures memorization statistics well,both qualitatively and quantitatively. We first show that the high curvaturesamples visually correspond to long-tailed, mislabeled, or conflicting samples,those that are most likely to be memorized. This analysis helps us find, to thebest of our knowledge, a novel failure mode on the CIFAR100 and ImageNetdatasets: that of duplicated images with differing labels. Quantitatively, wecorroborate the validity of our scores via two methods. First, we validate ourscores against an independent and comprehensively calculated baseline, byshowing high cosine similarity with the memorization scores released by Feldmanand Zhang (2020). Second, we inject corrupted samples which are memorized bythe network, and show that these are learned with high curvature. To this end,we synthetically mislabel a random subset of the dataset. We overfit a networkto it and show that sorting by curvature yields high AUROC values foridentifying the corrupted samples. An added advantage of our method is that itis scalable, as it requires training only a single network as opposed to thethousands trained by the baseline, while capturing the aforementioned failuremode that the baseline fails to identify.</description><author>Isha Garg, Deepak Ravikumar, Kaushik Roy</author><pubDate>Mon, 02 Oct 2023 04:50:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05831v2</guid></item><item><title>Co-learning Planning and Control Policies Constrained by Differentiable Logic Specifications</title><link>http://arxiv.org/abs/2303.01346v3</link><description>Synthesizing planning and control policies in robotics is a fundamental task,further complicated by factors such as complex logic specifications andhigh-dimensional robot dynamics. This paper presents a novel reinforcementlearning approach to solving high-dimensional robot navigation tasks withcomplex logic specifications by co-learning planning and control policies.Notably, this approach significantly reduces the sample complexity in training,allowing us to train high-quality policies with much fewer samples compared toexisting reinforcement learning algorithms. In addition, our methodologystreamlines complex specification extraction from map images and enables theefficient generation of long-horizon robot motion paths across different maplayouts. Moreover, our approach also demonstrates capabilities forhigh-dimensional control and avoiding suboptimal policies via policy alignment.The efficacy of our approach is demonstrated through experiments involvingsimulated high-dimensional quadruped robot dynamics and a real-worlddifferential drive robot (TurtleBot3) under different types of taskspecifications.</description><author>Zikang Xiong, Daniel Lawson, Joe Eappen, Ahmed H. Qureshi, Suresh Jagannathan</author><pubDate>Mon, 02 Oct 2023 04:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01346v3</guid></item><item><title>Exploring Sparse Visual Prompt for Domain Adaptive Dense Prediction</title><link>http://arxiv.org/abs/2303.09792v2</link><description>The visual prompts have provided an efficient manner in addressing visualcross-domain problems. In previous works, Visual Domain Prompt (VDP) firstintroduces domain prompts to tackle the classification Test-Time Adaptation(TTA) problem by warping image-level prompts on the input and fine-tuningprompts for each target domain. However, since the image-level prompts mask outcontinuous spatial details in the prompt-allocated region, it will suffer frominaccurate contextual information and limited domain knowledge extraction,particularly when dealing with dense prediction TTA problems. To overcome thesechallenges, we propose a novel Sparse Visual Domain Prompts (SVDP) approach,which holds minimal trainable parameters (e.g., 0.1\%) in the image-levelprompt and reserves more spatial information of the input. To better apply SVDPin extracting domain-specific knowledge, we introduce the Domain PromptPlacement (DPP) method to adaptively allocates trainable parameters of SVDP onthe pixels with large distribution shifts. Furthermore, recognizing that eachtarget domain sample exhibits a unique domain shift, we design Domain PromptUpdating (DPU) strategy to optimize prompt parameters differently for eachsample, facilitating efficient adaptation to the target domain. Extensiveexperiments were conducted on widely-used TTA and continual TTA benchmarks, andour proposed method achieves state-of-the-art performance in both semanticsegmentation and depth estimation tasks.</description><author>Senqiao Yang, Jiarui Wu, Jiaming Liu, Xiaoqi Li, Qizhe Zhang, Mingjie Pan, Yulu Gan, Zehui Chen, Shanghang Zhang</author><pubDate>Mon, 02 Oct 2023 04:41:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09792v2</guid></item><item><title>Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models</title><link>http://arxiv.org/abs/2308.16463v2</link><description>Large language models exhibit enhanced zero-shot performance on various taskswhen fine-tuned with instruction-following data. Multimodalinstruction-following models extend these capabilities by integrating both textand images. However, existing models such as MiniGPT-4 face challenges inmaintaining dialogue coherence in scenarios involving multiple images. Aprimary reason is the lack of a specialized dataset for this criticalapplication. To bridge these gaps, we present SparklesChat, a multimodalinstruction-following model for open-ended dialogues across multiple images. Tosupport the training, we introduce SparklesDialogue, the firstmachine-generated dialogue dataset tailored for word-level interleavedmulti-image and text interactions. Furthermore, we construct SparklesEval, aGPT-assisted benchmark for quantitatively assessing a model's conversationalcompetence across multiple images and dialogue turns. Our experiments validatethe effectiveness of SparklesChat in understanding and reasoning acrossmultiple images and dialogue turns. Specifically, SparklesChat outperformedMiniGPT-4 on established vision-and-language benchmarks, including the BISONbinary image selection task and the NLVR2 visual reasoning task. Moreover,SparklesChat scored 8.56 out of 10 on SparklesEval, substantially exceedingMiniGPT-4's score of 3.91 and nearing GPT-4's score of 9.26. Qualitativeevaluations further demonstrate SparklesChat's generality in handlingreal-world applications. All resources are available athttps://github.com/HYPJUDY/Sparkles.</description><author>Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, Yutong Lu</author><pubDate>Mon, 02 Oct 2023 04:31:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16463v2</guid></item><item><title>Trustworthy Optimization: A Novel Approach to Counter Numerical Instability in 16-bit Neural Network Training</title><link>http://arxiv.org/abs/2307.16189v4</link><description>In this research, we address critical trustworthiness concerns related to thenumerical instability observed in 16-bit computations of machine learningmodels. Such instability, particularly when employing popular optimizationalgorithms like RMSProp and Adam, often leads to unreliable training of deepneural networks. This not only disrupts the learning process but also posessignificant challenges in deploying dependable models in real-worldapplications. Our investigation identifies the epsilon hyperparameter as theprimary source of this instability. A nuanced exploration reveals that subtleadjustments to epsilon within 16-bit computations can enhance the reliabilityof RMSProp and Adam, enabling more trustworthy training of 16-bit neuralnetworks. We propose a novel, dependable approach that leverages updates fromthe Adam optimizer to bolster the stability of the learning process. Ourcontributions provide deeper insights into optimization challenges inlow-precision computations and offer solutions to ensure the trustworthinessand stability of deep neural network training, paving the way for theirdependable use in various applications.</description><author>Juyoung Yun</author><pubDate>Mon, 02 Oct 2023 03:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16189v4</guid></item><item><title>Bootstrap Diffusion Model Curve Estimation for High Resolution Low-Light Image Enhancement</title><link>http://arxiv.org/abs/2309.14709v3</link><description>Learning-based methods have attracted a lot of research attention and led tosignificant improvements in low-light image enhancement. However, most of themstill suffer from two main problems: expensive computational cost in highresolution images and unsatisfactory performance in simultaneous enhancementand denoising. To address these problems, we propose BDCE, a bootstrapdiffusion model that exploits the learning of the distribution of the curveparameters instead of the normal-light image itself. Specifically, we adopt thecurve estimation method to handle the high-resolution images, where the curveparameters are estimated by our bootstrap diffusion model. In addition, adenoise module is applied in each iteration of curve adjustment to denoise theintermediate enhanced result of each iteration. We evaluate BDCE on commonlyused benchmark datasets, and extensive experiments show that it achievesstate-of-the-art qualitative and quantitative performance.</description><author>Jiancheng Huang, Yifan Liu, Shifeng Chen</author><pubDate>Mon, 02 Oct 2023 03:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14709v3</guid></item><item><title>RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2307.00997v2</link><description>The Segment Anything Model (SAM) has gained significant attention for itsimpressive performance in image segmentation. However, it lacks proficiency inreferring video object segmentation (RVOS) due to the need for preciseuser-interactive prompts and a limited understanding of different modalities,such as language and vision. This paper presents the RefSAM model, whichexplores the potential of SAM for RVOS by incorporating multi-view informationfrom diverse modalities and successive frames at different timestamps in anonline manner. Our proposed approach adapts the original SAM model to enhancecross-modality learning by employing a lightweight Cross-Modal MLP thatprojects the text embedding of the referring expression into sparse and denseembeddings, serving as user-interactive prompts. Additionally, we haveintroduced the hierarchical dense attention module to fuse hierarchical visualsemantic information with sparse embeddings in order to obtain fine-graineddense embeddings, and an implicit tracking module to generate a track token andprovide historical information for the mask decoder. Furthermore, we employ aparameter-efficient tuning strategy to effectively align and fuse the languageand vision features. Through comprehensive ablation studies, we demonstrate thepractical and effective design choices of our model. Extensive experimentsconducted on Ref-Youtu-VOS, Ref-DAVIS17, and three referring image segmentationdatasets validate the superiority and effectiveness of our RefSAM model overexisting methods. The code and models will be made publicly at\href{https://github.com/LancasterLi/RefSAM}{github.com/LancasterLi/RefSAM}.</description><author>Yonglin Li, Jing Zhang, Xiao Teng, Long Lan</author><pubDate>Mon, 02 Oct 2023 03:32:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00997v2</guid></item><item><title>Performance-guaranteed regularization in maximum likelihood method: Gauge symmetry in Kullback -- Leibler divergence</title><link>http://arxiv.org/abs/2303.16721v2</link><description>The maximum likelihood method is the best-known method for estimating theprobabilities behind the data. However, the conventional method obtains theprobability model closest to the empirical distribution, resulting inoverfitting. Then regularization methods prevent the model from beingexcessively close to the wrong probability, but little is known systematicallyabout their performance. The idea of regularization is similar toerror-correcting codes, which obtain optimal decoding by mixing suboptimalsolutions with an incorrectly received code. The optimal decoding inerror-correcting codes is achieved based on gauge symmetry. We propose atheoretically guaranteed regularization in the maximum likelihood method byfocusing on a gauge symmetry in Kullback -- Leibler divergence. In ourapproach, we obtain the optimal model without the need to search forhyperparameters frequently appearing in regularization.</description><author>Akihisa Ichiki</author><pubDate>Mon, 02 Oct 2023 03:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16721v2</guid></item><item><title>Deep Learning Meets Adaptive Filtering: A Stein's Unbiased Risk Estimator Approach</title><link>http://arxiv.org/abs/2307.16708v2</link><description>This paper revisits two prominent adaptive filtering algorithms through thelens of algorithm unrolling, namely recursive least squares (RLS) andequivariant adaptive source separation (EASI), in the context of sourceestimation and separation. Building upon the unrolling methodology, weintroduce novel task-based deep learning frameworks, denoted as Deep RLS andDeep EASI. These architectures transform the iterations of the originalalgorithms into layers of a deep neural network, thereby enabling efficientsource signal estimation by taking advantage of a training process. To furtherenhance performance, we propose training these deep unrolled networks utilizinga loss function grounded on a Stein's unbiased risk estimator (SURE). Ourempirical evaluations demonstrate the efficacy of this SURE-based approach forenhanced source signal estimation.</description><author>Zahra Esmaeilbeig, Mojtaba Soltanalian</author><pubDate>Mon, 02 Oct 2023 03:22:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16708v2</guid></item><item><title>LibCity: A Unified Library Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction</title><link>http://arxiv.org/abs/2304.14343v6</link><description>As deep learning technology advances and more urban spatial-temporal dataaccumulates, an increasing number of deep learning models are being proposed tosolve urban spatial-temporal prediction problems. However, there arelimitations in the existing field, including open-source data being in variousformats and difficult to use, few papers making their code and data openlyavailable, and open-source models often using different frameworks andplatforms, making comparisons challenging. A standardized framework is urgentlyneeded to implement and evaluate these methods. To address these issues, wepropose LibCity, an open-source library that offers researchers a credibleexperimental tool and a convenient development framework. In this library, wehave reproduced 65 spatial-temporal prediction models and collected 55spatial-temporal datasets, allowing researchers to conduct comprehensiveexperiments conveniently. By enabling fair model comparisons, designing aunified data storage format, and simplifying the process of developing newmodels, LibCity is poised to make significant contributions to thespatial-temporal prediction field.</description><author>Jiawei Jiang, Chengkai Han, Wenjun Jiang, Wayne Xin Zhao, Jingyuan Wang</author><pubDate>Mon, 02 Oct 2023 03:19:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14343v6</guid></item><item><title>Beyond Demographic Parity: Redefining Equal Treatment</title><link>http://arxiv.org/abs/2303.08040v3</link><description>Liberalism-oriented political philosophy reasons that all individuals shouldbe treated equally independently of their protected characteristics. Relatedwork in machine learning has translated the concept of \emph{equal treatment}into terms of \emph{equal outcome} and measured it as \emph{demographic parity}(also called \emph{statistical parity}). Our analysis reveals that the twoconcepts of equal outcome and equal treatment diverge; therefore, demographicparity does not faithfully represent the notion of \emph{equal treatment}. Wepropose a new formalization for equal treatment by (i) considering theinfluence of feature values on predictions, such as computed by Shapley valuesdecomposing predictions across its features, (ii) defining distributions ofexplanations, and (iii) comparing explanation distributions between populationswith different protected characteristics. We show the theoretical properties ofour notion of equal treatment and devise a classifier two-sample test based onthe AUC of an equal treatment inspector. We study our formalization of equaltreatment on synthetic and natural data. We release \texttt{explanationspace},an open-source Python package with methods and tutorials.</description><author>Carlos Mougan, Laura State, Antonio Ferrara, Salvatore Ruggieri, Steffen Staab</author><pubDate>Mon, 02 Oct 2023 03:06:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08040v3</guid></item><item><title>LLM-grounded Video Diffusion Models</title><link>http://arxiv.org/abs/2309.17444v2</link><description>Text-conditioned diffusion models have emerged as a promising tool for neuralvideo generation. However, current models still struggle with intricatespatiotemporal prompts and often generate restricted or incorrect motion (e.g.,even lacking the ability to be prompted for objects moving from left to right).To address these limitations, we introduce LLM-grounded Video Diffusion (LVD).Instead of directly generating videos from the text inputs, LVD first leveragesa large language model (LLM) to generate dynamic scene layouts based on thetext inputs and subsequently uses the generated layouts to guide a diffusionmodel for video generation. We show that LLMs are able to understand complexspatiotemporal dynamics from text alone and generate layouts that align closelywith both the prompts and the object motion patterns typically observed in thereal world. We then propose to guide video diffusion models with these layoutsby adjusting the attention maps. Our approach is training-free and can beintegrated into any video diffusion model that admits classifier guidance. Ourresults demonstrate that LVD significantly outperforms its base video diffusionmodel and several strong baseline methods in faithfully generating videos withthe desired attributes and motion patterns.</description><author>Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, Boyi Li</author><pubDate>Mon, 02 Oct 2023 02:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17444v2</guid></item><item><title>Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization Approach</title><link>http://arxiv.org/abs/2109.12701v3</link><description>We study the Sparse Plus Low-Rank decomposition problem (SLR), which is theproblem of decomposing a corrupted data matrix into a sparse matrix ofperturbations plus a low-rank matrix containing the ground truth. SLR is afundamental problem in Operations Research and Machine Learning which arises invarious applications, including data compression, latent semantic indexing,collaborative filtering, and medical imaging. We introduce a novel formulationfor SLR that directly models its underlying discreteness. For this formulation,we develop an alternating minimization heuristic that computes high-qualitysolutions and a novel semidefinite relaxation that provides meaningful boundsfor the solutions returned by our heuristic. We also develop a custombranch-and-bound algorithm that leverages our heuristic and convex relaxationsto solve small instances of SLR to certifiable (near) optimality. Given aninput $n$-by-$n$ matrix, our heuristic scales to solve instances where$n=10000$ in minutes, our relaxation scales to instances where $n=200$ inhours, and our branch-and-bound algorithm scales to instances where $n=25$ inminutes. Our numerical results demonstrate that our approach outperformsexisting state-of-the-art approaches in terms of rank, sparsity, andmean-square error while maintaining a comparable runtime.</description><author>Dimitris Bertsimas, Ryan Cory-Wright, Nicholas A. G. Johnson</author><pubDate>Mon, 02 Oct 2023 02:38:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.12701v3</guid></item><item><title>Decoupled Self-supervised Learning for Non-Homophilous Graphs</title><link>http://arxiv.org/abs/2206.03601v3</link><description>This paper studies the problem of conducting self-supervised learning fornode representation learning on graphs. Most existing self-supervised learningmethods assume the graph is homophilous, where linked nodes often belong to thesame class or have similar features. However, such assumptions of homophily donot always hold in real-world graphs. We address this problem by developing adecoupled self-supervised learning (DSSL) framework for graph neural networks.DSSL imitates a generative process of nodes and links from latent variablemodeling of the semantic structure, which decouples different underlyingsemantics between different neighborhoods into the self-supervised learningprocess. Our DSSL framework is agnostic to the encoders and does not needprefabricated augmentations, thus is flexible to different graphs. Toeffectively optimize the framework, we derive the evidence lower bound of theself-supervised objective and develop a scalable training algorithm withvariational inference. We provide a theoretical analysis to justify that DSSLenjoys the better downstream performance. Extensive experiments on varioustypes of graph benchmarks demonstrate that our proposed framework can achievebetter performance compared with competitive baselines.</description><author>Teng Xiao, Zhengyu Chen, Zhimeng Guo, Zeyang Zhuang, Suhang Wang</author><pubDate>Mon, 02 Oct 2023 02:33:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.03601v3</guid></item></channel></rss>