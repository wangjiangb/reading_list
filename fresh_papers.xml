<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 18 Dec 2024 13:00:08 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ExBody2: Advanced Expressive Humanoid Whole-Body Control</title><link>http://arxiv.org/abs/2412.13196v1</link><description>This paper enables real-world humanoid robots to maintain stability whileperforming expressive motions like humans do. We propose ExBody2, a generalizedwhole-body tracking framework that can take any reference motion inputs andcontrol the humanoid to mimic the motion. The model is trained in simulationwith Reinforcement Learning and then transferred to the real world. Itdecouples keypoint tracking with velocity control, and effectively leverages aprivileged teacher policy to distill precise mimic skills into the targetstudent policy, which enables high-fidelity replication of dynamic movementssuch as running, crouching, dancing, and other challenging motions. We presenta comprehensive qualitative and quantitative analysis of crucial design factorsin the paper. We conduct our experiments on two humanoid platforms anddemonstrate the superiority of our approach against state-of-the-arts,providing practical guidelines to pursue the extreme of whole-body control forhumanoid robots.</description><author>Mazeyu Ji, Xuanbin Peng, Fangchen Liu, Jialong Li, Ge Yang, Xuxin Cheng, Xiaolong Wang</author><pubDate>Tue, 17 Dec 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13196v1</guid></item><item><title>CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2412.13195v1</link><description>Text-to-image diffusion models excel at generating photorealistic images, butcommonly struggle to render accurate spatial relationships described in textprompts. We identify two core issues underlying this common failure: 1) theambiguous nature of spatial-related data in existing datasets, and 2) theinability of current text encoders to accurately interpret the spatialsemantics of input descriptions. We address these issues with CoMPaSS, aversatile training framework that enhances spatial understanding of any T2Idiffusion model. CoMPaSS solves the ambiguity of spatial-related data with theSpatial Constraints-Oriented Pairing (SCOP) data engine, which curatesspatially-accurate training data through a set of principled spatialconstraints. To better exploit the curated high-quality spatial priors, CoMPaSSfurther introduces a Token ENcoding ORdering (TENOR) module to allow betterexploitation of high-quality spatial priors, effectively compensating for theshortcoming of text encoders. Extensive experiments on four popular open-weightT2I diffusion models covering both UNet- and MMDiT-based architecturesdemonstrate the effectiveness of CoMPaSS by setting new state-of-the-arts withsubstantial relative gains across well-known benchmarks on spatialrelationships generation, including VISOR (+98%), T2I-CompBench Spatial (+67%),and GenEval Position (+131%). Code will be available athttps://github.com/blurgyy/CoMPaSS.</description><author>Gaoyang Zhang, Bingtao Fu, Qingnan Fan, Qi Zhang, Runxing Liu, Hong Gu, Huaqi Zhang, Xinguo Liu</author><pubDate>Tue, 17 Dec 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13195v1</guid></item><item><title>Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation Model Internet Agents</title><link>http://arxiv.org/abs/2412.13194v1</link><description>The vision of a broadly capable and goal-directed agent, such as anInternet-browsing agent in the digital world and a household humanoid in thephysical world, has rapidly advanced, thanks to the generalization capabilityof foundation models. Such a generalist agent needs to have a large and diverseskill repertoire, such as finding directions between two travel locations andbuying specific items from the Internet. If each skill needs to be specifiedmanually through a fixed set of human-annotated instructions, the agent's skillrepertoire will necessarily be limited due to the quantity and diversity ofhuman-annotated instructions. In this work, we address this challenge byproposing Proposer-Agent-Evaluator, an effective learning system that enablesfoundation model agents to autonomously discover and practice skills in thewild. At the heart of PAE is a context-aware task proposer that autonomouslyproposes tasks for the agent to practice with context information of theenvironment such as user demos or even just the name of the website itself forInternet-browsing agents. Then, the agent policy attempts those tasks withthoughts and actual grounded operations in the real world with resultingtrajectories evaluated by an autonomous VLM-based success evaluator. Thesuccess evaluation serves as the reward signal for the agent to refine itspolicies through RL. We validate PAE on challenging vision-based webnavigation, using both real-world and self-hosted websites from WebVoyager andWebArena.To the best of our knowledge, this work represents the first effectivelearning system to apply autonomous task proposal with RL for agents thatgeneralizes real-world human-annotated benchmarks with SOTA performances. Ouropen-source checkpoints and code can be found in https://yanqval.github.io/PAE/</description><author>Yifei Zhou, Qianlan Yang, Kaixiang Lin, Min Bai, Xiong Zhou, Yu-Xiong Wang, Sergey Levine, Erran Li</author><pubDate>Tue, 17 Dec 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13194v1</guid></item><item><title>GaussTR: Foundation Model-Aligned Gaussian Transformer for Self-Supervised 3D Spatial Understanding</title><link>http://arxiv.org/abs/2412.13193v1</link><description>3D Semantic Occupancy Prediction is fundamental for spatial understanding asit provides a comprehensive semantic cognition of surrounding environments.However, prevalent approaches primarily rely on extensive labeled data andcomputationally intensive voxel-based modeling, restricting the scalability andgeneralizability of 3D representation learning. In this paper, we introduceGaussTR, a novel Gaussian Transformer that leverages alignment with foundationmodels to advance self-supervised 3D spatial understanding. GaussTR adopts aTransformer architecture to predict sparse sets of 3D Gaussians that representscenes in a feed-forward manner. Through aligning rendered Gaussian featureswith diverse knowledge from pre-trained foundation models, GaussTR facilitatesthe learning of versatile 3D representations and enables open-vocabularyoccupancy prediction without explicit annotations. Empirical evaluations on theOcc3D-nuScenes dataset showcase GaussTR's state-of-the-art zero-shotperformance, achieving 11.70 mIoU while reducing training duration byapproximately 50%. These experimental results highlight the significantpotential of GaussTR for scalable and holistic 3D spatial understanding, withpromising implications for autonomous driving and embodied agents. Code isavailable at https://github.com/hustvl/GaussTR.</description><author>Haoyi Jiang, Liu Liu, Tianheng Cheng, Xinjie Wang, Tianwei Lin, Zhizhong Su, Wenyu Liu, Xinggang Wang</author><pubDate>Tue, 17 Dec 2024 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13193v1</guid></item><item><title>MotionBridge: Dynamic Video Inbetweening with Flexible Controls</title><link>http://arxiv.org/abs/2412.13190v1</link><description>By generating plausible and smooth transitions between two image frames,video inbetweening is an essential tool for video editing and long videosynthesis. Traditional works lack the capability to generate complex largemotions. While recent video generation techniques are powerful in creatinghigh-quality results, they often lack fine control over the details ofintermediate frames, which can lead to results that do not align with thecreative mind. We introduce MotionBridge, a unified video inbetweeningframework that allows flexible controls, including trajectory strokes,keyframes, masks, guide pixels, and text. However, learning such multi-modalcontrols in a unified framework is a challenging task. We thus design twogenerators to extract the control signal faithfully and encode feature throughdual-branch embedders to resolve ambiguities. We further introduce a curriculumtraining strategy to smoothly learn various controls. Extensive qualitative andquantitative experiments have demonstrated that such multi-modal controlsenable a more dynamic, customizable, and contextually accurate visualnarrative.</description><author>Maham Tanveer, Yang Zhou, Simon Niklaus, Ali Mahdavi Amiri, Hao Zhang, Krishna Kumar Singh, Nanxuan Zhao</author><pubDate>Tue, 17 Dec 2024 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13190v1</guid></item><item><title>InstantSplat: Sparse-view SfM-free Gaussian Splatting in Seconds</title><link>http://arxiv.org/abs/2403.20309v4</link><description>While neural 3D reconstruction has advanced substantially, it typicallyrequires densely captured multi-view data with carefully initialized poses(e.g., using COLMAP). However, this requirement limits its broaderapplicability, as Structure-from-Motion (SfM) is often unreliable insparse-view scenarios where feature matches are limited, resulting incumulative errors. In this paper, we introduce InstantSplat, a novel andlightning-fast neural reconstruction system that builds accurate 3Drepresentations from as few as 2-3 images. InstantSplat adopts aself-supervised framework that bridges the gap between 2D images and 3Drepresentations using Gaussian Bundle Adjustment (GauBA) and can be optimizedin an end-to-end manner. InstantSplat integrates dense stereo priors andco-visibility relationships between frames to initialize pixel-aligned geometryby progressively expanding the scene avoiding redundancy. Gaussian BundleAdjustment is used to adapt both the scene representation and camera parametersquickly by minimizing gradient-based photometric error. Overall, InstantSplatachieves large-scale 3D reconstruction in mere seconds by reducing the requirednumber of input views. It achieves an acceleration of over 20 times inreconstruction, improves visual quality (SSIM) from 0.3755 to 0.7624 thanCOLMAP with 3D-GS, and is compatible with multiple 3D representations (3D-GS,2D-GS, and Mip-Splatting).</description><author>Zhiwen Fan, Kairun Wen, Wenyan Cong, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang</author><pubDate>Tue, 17 Dec 2024 18:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20309v4</guid></item><item><title>StreetCrafter: Street View Synthesis with Controllable Video Diffusion Models</title><link>http://arxiv.org/abs/2412.13188v1</link><description>This paper aims to tackle the problem of photorealistic view synthesis fromvehicle sensor data. Recent advancements in neural scene representation haveachieved notable success in rendering high-quality autonomous driving scenes,but the performance significantly degrades as the viewpoint deviates from thetraining trajectory. To mitigate this problem, we introduce StreetCrafter, anovel controllable video diffusion model that utilizes LiDAR point cloudrenderings as pixel-level conditions, which fully exploits the generative priorfor novel view synthesis, while preserving precise camera control. Moreover,the utilization of pixel-level LiDAR conditions allows us to make accuratepixel-level edits to target scenes. In addition, the generative prior ofStreetCrafter can be effectively incorporated into dynamic scenerepresentations to achieve real-time rendering. Experiments on Waymo OpenDataset and PandaSet demonstrate that our model enables flexible control overviewpoint changes, enlarging the view synthesis regions for satisfyingrendering, which outperforms existing methods.</description><author>Yunzhi Yan, Zhen Xu, Haotong Lin, Haian Jin, Haoyu Guo, Yida Wang, Kun Zhan, Xianpeng Lang, Hujun Bao, Xiaowei Zhou, Sida Peng</author><pubDate>Tue, 17 Dec 2024 18:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13188v1</guid></item><item><title>HandsOnVLM: Vision-Language Models for Hand-Object Interaction Prediction</title><link>http://arxiv.org/abs/2412.13187v1</link><description>How can we predict future interaction trajectories of human hands in a scenegiven high-level colloquial task specifications in the form of naturallanguage? In this paper, we extend the classic hand trajectory prediction taskto two tasks involving explicit or implicit language queries. Our proposedtasks require extensive understanding of human daily activities and reasoningabilities about what should be happening next given cues from the currentscene. We also develop new benchmarks to evaluate the proposed two tasks,Vanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). Weenable solving these tasks by integrating high-level world knowledge andreasoning capabilities of Vision-Language Models (VLMs) with theauto-regressive nature of low-level ego-centric hand trajectories. Our model,HandsOnVLM is a novel VLM that can generate textual responses and producefuture hand trajectories through natural-language conversations. Ourexperiments show that HandsOnVLM outperforms existing task-specific methods andother VLM baselines on proposed tasks, and demonstrates its ability toeffectively utilize world knowledge for reasoning about low-level human handtrajectories based on the provided context. Our website contains code anddetailed video results \url{https://www.chenbao.tech/handsonvlm/}</description><author>Chen Bao, Jiarui Xu, Xiaolong Wang, Abhinav Gupta, Homanga Bharadhwaj</author><pubDate>Tue, 17 Dec 2024 18:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13187v1</guid></item><item><title>Move-in-2D: 2D-Conditioned Human Motion Generation</title><link>http://arxiv.org/abs/2412.13185v1</link><description>Generating realistic human videos remains a challenging task, with the mosteffective methods currently relying on a human motion sequence as a controlsignal. Existing approaches often use existing motion extracted from othervideos, which restricts applications to specific motion types and global scenematching. We propose Move-in-2D, a novel approach to generate human motionsequences conditioned on a scene image, allowing for diverse motion that adaptsto different scenes. Our approach utilizes a diffusion model that accepts botha scene image and text prompt as inputs, producing a motion sequence tailoredto the scene. To train this model, we collect a large-scale video datasetfeaturing single-human activities, annotating each video with the correspondinghuman motion as the target output. Experiments demonstrate that our methodeffectively predicts human motion that aligns with the scene image afterprojection. Furthermore, we show that the generated motion sequence improveshuman motion quality in video synthesis tasks.</description><author>Hsin-Ping Huang, Yang Zhou, Jui-Hsien Wang, Difan Liu, Feng Liu, Ming-Hsuan Yang, Zhan Xu</author><pubDate>Tue, 17 Dec 2024 18:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13185v1</guid></item><item><title>Tilted Quantile Gradient Updates for Quantile-Constrained Reinforcement Learning</title><link>http://arxiv.org/abs/2412.13184v1</link><description>Safe reinforcement learning (RL) is a popular and versatile paradigm to learnreward-maximizing policies with safety guarantees. Previous works tend toexpress the safety constraints in an expectation form due to the ease ofimplementation, but this turns out to be ineffective in maintaining safetyconstraints with high probability. To this end, we move to thequantile-constrained RL that enables a higher level of safety without anyexpectation-form approximations. We directly estimate the quantile gradientsthrough sampling and provide the theoretical proofs of convergence. Then atilted update strategy for quantile gradients is implemented to compensate theasymmetric distributional density, with a direct benefit of return performance.Experiments demonstrate that the proposed model fully meets safety requirements(quantile constraints) while outperforming the state-of-the-art benchmarks withhigher return.</description><author>Chenglin Li, Guangchun Ruan, Hua Geng</author><pubDate>Tue, 17 Dec 2024 18:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13184v1</guid></item><item><title>Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures</title><link>http://arxiv.org/abs/2412.13183v1</link><description>Real-time free-view human rendering from sparse-view RGB inputs is achallenging task due to the sensor scarcity and the tight time budget. Toensure efficiency, recent methods leverage 2D CNNs operating in texture spaceto learn rendering primitives. However, they either jointly learn geometry andappearance, or completely ignore sparse image information for geometryestimation, significantly harming visual quality and robustness to unseen bodyposes. To address these issues, we present Double Unprojected Textures, whichat the core disentangles coarse geometric deformation estimation fromappearance synthesis, enabling robust and photorealistic 4K rendering inreal-time. Specifically, we first introduce a novel image-conditioned templatedeformation network, which estimates the coarse deformation of the humantemplate from a first unprojected texture. This updated geometry is then usedto apply a second and more accurate texture unprojection. The resulting texturemap has fewer artifacts and better alignment with input views, which benefitsour learning of finer-level geometry and appearance represented by Gaussiansplats. We validate the effectiveness and efficiency of the proposed method inquantitative and qualitative experiments, which significantly surpasses otherstate-of-the-art methods.</description><author>Guoxing Sun, Rishabh Dabral, Heming Zhu, Pascal Fua, Christian Theobalt, Marc Habermann</author><pubDate>Tue, 17 Dec 2024 18:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13183v1</guid></item><item><title>Feather the Throttle: Revisiting Visual Token Pruning for Vision-Language Model Acceleration</title><link>http://arxiv.org/abs/2412.13180v1</link><description>Recent works on accelerating Vision-Language Models show that strongperformance can be maintained across a variety of vision-language tasks despitehighly compressing visual information. In this work, we examine the popularacceleration approach of early pruning of visual tokens inside the languagemodel and find that its strong performance across many tasks is not due to anexceptional ability to compress visual information, but rather the benchmarks'limited ability to assess fine-grained visual capabilities. Namely, wedemonstrate a core issue with the acceleration approach where most tokenstowards the top of the image are pruned away. Yet, this issue is only reflectedin performance for a small subset of tasks such as localization. For the otherevaluated tasks, strong performance is maintained with the flawed pruningstrategy. Noting the limited visual capabilities of the studied accelerationtechnique, we propose FEATHER (Fast and Effective Acceleration wiTH EnsemblecRiteria), a straightforward approach that (1) resolves the identified issuewith early-layer pruning, (2) incorporates uniform sampling to ensure coverageacross all image regions, and (3) applies pruning in two stages to allow thecriteria to become more effective at a later layer while still achievingsignificant speedup through early-layer pruning. With comparable computationalsavings, we find that FEATHER has more than $5\times$ performance improvementon the vision-centric localization benchmarks compared to the originalacceleration approach.</description><author>Mark Endo, Xiaohan Wang, Serena Yeung-Levy</author><pubDate>Tue, 17 Dec 2024 18:56:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13180v1</guid></item><item><title>SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents</title><link>http://arxiv.org/abs/2412.13178v1</link><description>With the integration of large language models (LLMs), embodied agents havestrong capabilities to execute complicated instructions in natural language,paving a way for the potential deployment of embodied robots. However, aforeseeable issue is that those embodied agents can also flawlessly executesome hazardous tasks, potentially causing damages in real world. To study thisissue, we present SafeAgentBench -- a new benchmark for safety-aware taskplanning of embodied LLM agents. SafeAgentBench includes: (1) a new datasetwith 750 tasks, covering 10 potential hazards and 3 task types; (2)SafeAgentEnv, a universal embodied environment with a low-level controller,supporting multi-agent execution with 17 high-level actions for 8state-of-the-art baselines; and (3) reliable evaluation methods from bothexecution and semantic perspectives. Experimental results show that thebest-performing baseline gets 69% success rate for safe tasks, but only 5%rejection rate for hazardous tasks, indicating significant safety risks. Moredetails and codes are available athttps://github.com/shengyin1224/SafeAgentBench.</description><author>Sheng Yin, Xianghe Pang, Yuanzhuo Ding, Menglan Chen, Yutong Bi, Yichen Xiong, Wenhao Huang, Zhen Xiang, Jing Shao, Siheng Chen</author><pubDate>Tue, 17 Dec 2024 18:55:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13178v1</guid></item><item><title>DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback</title><link>http://arxiv.org/abs/2410.06215v2</link><description>The process of creating training data to teach models is currently driven byhumans, who manually analyze model weaknesses and plan how to create data thatimproves a student model. Approaches using LLMs as annotators reduce humaneffort, but still require humans to interpret feedback from evaluations andcontrol the LLM to produce data the student needs. Automating thislabor-intensive process by creating autonomous data generation agents - orteachers - is desirable, but requires environments that can simulate thefeedback-driven, iterative, closed loop of data creation. To enable rapid,scalable testing for such agents and their modules, we introduce DataEnvGym, atestbed of teacher environments for data generation agents. DataEnvGym framesdata generation as a sequential decision-making task, involving an agentconsisting of a data generation policy (which generates a plan for creatingtraining data) and a data generation engine (which transforms the plan intodata), inside an environment that provides student feedback. The agent's goalis to improve student performance. Students are iteratively trained andevaluated on generated data, and their feedback (in the form of errors or weakskills) is reported to the agent after each iteration. DataEnvGym includesmultiple teacher environment instantiations across 3 levels of structure in thestate representation and action space. More structured environments are basedon inferred skills and offer more interpretability and curriculum control. Wesupport 4 domains (math, code, VQA, and tool-use) and test multiple studentsand teachers. Example agents in our teaching environments can iterativelyimprove students across tasks and settings. Moreover, we show that environmentsteach different skill levels and test variants of key modules, pointing tofuture work in improving data generation agents, engines, and feedbackmechanisms.</description><author>Zaid Khan, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal</author><pubDate>Tue, 17 Dec 2024 18:54:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06215v2</guid></item><item><title>NFL-BA: Improving Endoscopic SLAM with Near-Field Light Bundle Adjustment</title><link>http://arxiv.org/abs/2412.13176v1</link><description>Simultaneous Localization And Mapping (SLAM) from a monocular endoscopy videocan enable autonomous navigation, guidance to unsurveyed regions, and 3Dvisualizations, which can significantly improve endoscopy experience forsurgeons and patient outcomes. Existing dense SLAM algorithms often assumedistant and static lighting and textured surfaces, and alternate betweenoptimizing scene geometry and camera parameters by minimizing a photometricrendering loss, often called Photometric Bundle Adjustment. However, endoscopicenvironments exhibit dynamic near-field lighting due to the co-located lightand camera moving extremely close to the surface, textureless surfaces, andstrong specular reflections due to mucus layers. When not considered, thesenear-field lighting effects can cause significant performance reductions forexisting SLAM algorithms from indoor/outdoor scenes when applied to endoscopyvideos. To mitigate this problem, we introduce a new Near-Field Lighting BundleAdjustment Loss $(L_{NFL-BA})$ that can also be alternatingly optimized, alongwith the Photometric Bundle Adjustment loss, such that the captured images'intensity variations match the relative distance and orientation between thesurface and the co-located light and camera. We derive a general NFL-BA lossfunction for 3D Gaussian surface representations and demonstrate that adding$L_{NFL-BA}$ can significantly improve the tracking and mapping performance oftwo state-of-the-art 3DGS-SLAM systems, MonoGS (35% improvement in tracking,48% improvement in mapping with predicted depth maps) and EndoGSLAM (22%improvement in tracking, marginal improvement in mapping with predicteddepths), on the C3VD endoscopy dataset for colons. The project page isavailable at https://asdunnbe.github.io/NFL-BA/</description><author>Andrea Dunn Beltran, Daniel Rho, Marc Niethammer, Roni Sengupta</author><pubDate>Tue, 17 Dec 2024 18:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13176v1</guid></item><item><title>DnDScore: Decontextualization and Decomposition for Factuality Verification in Long-Form Text Generation</title><link>http://arxiv.org/abs/2412.13175v1</link><description>The decompose-then-verify strategy for verification of Large Language Model(LLM) generations decomposes claims that are then independently verified.Decontextualization augments text (claims) to ensure it can be verified outsideof the original context, enabling reliable verification. While decompositionand decontextualization have been explored independently, their interactions ina complete system have not been investigated. Their conflicting purposes cancreate tensions: decomposition isolates atomic facts while decontextualizationinserts relevant information. Furthermore, a decontextualized subclaim presentsa challenge to the verification step: what part of the augmented text should beverified as it now contains multiple atomic facts? We conduct an evaluation ofdifferent decomposition, decontextualization, and verification strategies andfind that the choice of strategy matters in the resulting factuality scores.Additionally, we introduce DnDScore, a decontextualization aware verificationmethod which validates subclaims in the context of contextual information.</description><author>Miriam Wanner, Benjamin Van Durme, Mark Dredze</author><pubDate>Tue, 17 Dec 2024 18:54:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13175v1</guid></item><item><title>ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark Detection</title><link>http://arxiv.org/abs/2412.13174v1</link><description>Although facial landmark detection (FLD) has gained significant progress,existing FLD methods still suffer from performance drops on partiallynon-visible faces, such as faces with occlusions or under extreme lightingconditions or poses. To address this issue, we introduce ORFormer, a noveltransformer-based method that can detect non-visible regions and recover theirmissing features from visible parts. Specifically, ORFormer associates eachimage patch token with one additional learnable token called the messengertoken. The messenger token aggregates features from all but its patch. Thisway, the consensus between a patch and other patches can be assessed byreferring to the similarity between its regular and messenger embeddings,enabling non-visible region identification. Our method then recovers occludedpatches with features aggregated by the messenger tokens. Leveraging therecovered features, ORFormer compiles high-quality heatmaps for the downstreamFLD task. Extensive experiments show that our method generates heatmapsresilient to partial occlusions. By integrating the resultant heatmaps intoexisting FLD methods, our method performs favorably against the state of thearts on challenging datasets such as WFLW and COFW.</description><author>Jui-Che Chiang, Hou-Ning Hu, Bo-Syuan Hou, Chia-Yu Tseng, Yu-Lun Liu, Min-Hung Chen, Yen-Yu Lin</author><pubDate>Tue, 17 Dec 2024 18:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13174v1</guid></item><item><title>Locate n' Rotate: Two-stage Openable Part Detection with Foundation Model Priors</title><link>http://arxiv.org/abs/2412.13173v1</link><description>Detecting the openable parts of articulated objects is crucial for downstreamapplications in intelligent robotics, such as pulling a drawer. This task posesa multitasking challenge due to the necessity of understanding objectcategories and motion. Most existing methods are either category-specific ortrained on specific datasets, lacking generalization to unseen environments andobjects. In this paper, we propose a Transformer-based Openable Part Detection(OPD) framework named Multi-feature Openable Part Detection (MOPD) thatincorporates perceptual grouping and geometric priors, outperforming previousmethods in performance. In the first stage of the framework, we introduce aperceptual grouping feature model that provides perceptual grouping featurepriors for openable part detection, enhancing detection results through across-attention mechanism. In the second stage, a geometric understandingfeature model offers geometric feature priors for predicting motion parameters.Compared to existing methods, our proposed approach shows better performance inboth detection and motion parameter prediction. Codes and models are publiclyavailable at https://github.com/lisiqi-zju/MOPD</description><author>Siqi Li, Xiaoxue Chen, Haoyu Cheng, Guyue Zhou, Hao Zhao, Guanzhong Tian</author><pubDate>Tue, 17 Dec 2024 18:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13173v1</guid></item><item><title>Compressed Chain of Thought: Efficient Reasoning Through Dense Representations</title><link>http://arxiv.org/abs/2412.13171v1</link><description>Chain-of-thought (CoT) decoding enables language models to improve reasoningperformance at the cost of high generation latency in decoding. Recentproposals have explored variants of contemplation tokens, a term we introducethat refers to special tokens used during inference to allow for extracomputation. Prior work has considered fixed-length sequences drawn from adiscrete set of embeddings as contemplation tokens. Here we propose CompressedChain-of-Thought (CCoT), a framework to generate contentful and continuouscontemplation tokens of variable sequence length. The generated contemplationtokens are compressed representations of explicit reasoning chains, and ourmethod can be applied to off-the-shelf decoder language models. Throughexperiments, we illustrate how CCoT enables additional reasoning over densecontentful representations to achieve corresponding improvements in accuracy.Moreover, the reasoning improvements can be adaptively modified on demand bycontrolling the number of contemplation tokens generated.</description><author>Jeffrey Cheng, Benjamin Van Durme</author><pubDate>Tue, 17 Dec 2024 18:50:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13171v1</guid></item><item><title>Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study</title><link>http://arxiv.org/abs/2412.13169v1</link><description>In recent research, large language models (LLMs) have been increasingly usedto investigate public opinions. This study investigates the algorithmicfidelity of LLMs, i.e., the ability to replicate the socio-cultural context andnuanced opinions of human participants. Using open-ended survey data from theGerman Longitudinal Election Studies (GLES), we prompt different LLMs togenerate synthetic public opinions reflective of German subpopulations byincorporating demographic features into the persona prompts. Our results showthat Llama performs better than other LLMs at representing subpopulations,particularly when there is lower opinion diversity within those groups. Ourfindings further reveal that the LLM performs better for supporters ofleft-leaning parties like The Greens and The Left compared to other parties,and matches the least with the right-party AfD. Additionally, the inclusion orexclusion of specific variables in the prompts can significantly impact themodels' predictions. These findings underscore the importance of aligning LLMsto more effectively model diverse public opinions while minimizing politicalbiases and enhancing robustness in representativeness.</description><author>Bolei Ma, Berk Yoztyurk, Anna-Carolina Haensch, Xinpeng Wang, Markus Herklotz, Frauke Kreuter, Barbara Plank, Matthias Assenmacher</author><pubDate>Tue, 17 Dec 2024 18:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13169v1</guid></item><item><title>Causal Diffusion Transformers for Generative Modeling</title><link>http://arxiv.org/abs/2412.12095v2</link><description>We introduce Causal Diffusion as the autoregressive (AR) counterpart ofDiffusion models. It is a next-token(s) forecasting framework that is friendlyto both discrete and continuous modalities and compatible with existingnext-token prediction models like LLaMA and GPT. While recent works attempt tocombine diffusion with AR models, we show that introducing sequentialfactorization to a diffusion model can substantially improve its performanceand enables a smooth transition between AR and diffusion generation modes.Hence, we propose CausalFusion - a decoder-only transformer thatdual-factorizes data across sequential tokens and diffusion noise levels,leading to state-of-the-art results on the ImageNet generation benchmark whilealso enjoying the AR advantage of generating an arbitrary number of tokens forin-context reasoning. We further demonstrate CausalFusion's multimodalcapabilities through a joint image generation and captioning model, andshowcase CausalFusion's ability for zero-shot in-context image manipulations.We hope that this work could provide the community with a fresh perspective ontraining multimodal models over discrete and continuous data.</description><author>Chaorui Deng, Deyao Zhu, Kunchang Li, Shi Guang, Haoqi Fan</author><pubDate>Tue, 17 Dec 2024 18:45:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12095v2</guid></item><item><title>Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial Dynamics in the Wild</title><link>http://arxiv.org/abs/2412.13168v1</link><description>In-the-wild Dynamic facial expression recognition (DFER) encounters asignificant challenge in recognizing emotion-related expressions, which areoften temporally and spatially diluted by emotion-irrelevant expressions andglobal context respectively. Most of the prior DFER methods model tightlycoupled spatiotemporal representations which may incorporate weakly relevantfeatures, leading to information redundancy and emotion-irrelevant contextbias. Several DFER methods have highlighted the significance of dynamicinformation, but utilize explicit manners to extract dynamic features withoverly strong prior knowledge. In this paper, we propose a novel ImplicitFacial Dynamics Disentanglement framework (IFDD). Through expanding waveletlifting scheme to fully learnable framework, IFDD disentangles emotion-relateddynamic information from emotion-irrelevant global context in an implicitmanner, i.e., without exploit operations and external guidance. Thedisentanglement process of IFDD contains two stages, i.e., Inter-frameStatic-dynamic Splitting Module (ISSM) for rough disentanglement estimation andLifting-based Aggregation-Disentanglement Module (LADM) for further refinement.Specifically, ISSM explores inter-frame correlation to generate content-awaresplitting indexes on-the-fly. We preliminarily utilize these indexes to splitframe features into two groups, one with greater global similarity, and theother with more unique dynamic features. Subsequently, LADM first aggregatesthese two groups of features to obtain fine-grained global context features byan updater, and then disentangles emotion-related facial dynamic features fromthe global context by a predictor. Extensive experiments on in-the-wilddatasets have demonstrated that IFDD outperforms prior supervised DFER methodswith higher recognition accuracy and comparable efficiency.</description><author>Xingjian Wang, Li Chai</author><pubDate>Tue, 17 Dec 2024 18:45:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13168v1</guid></item><item><title>Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization</title><link>http://arxiv.org/abs/2412.04619v2</link><description>Language models (LMs), like other neural networks, often favor shortcutheuristics based on surface-level patterns. Although LMs behave like n-grammodels early in training, they must eventually learn hierarchical syntacticrepresentations to correctly apply grammatical rules out-of-distribution (OOD).In this work, we use case studies of English grammar to explore how complex,diverse training data drives models to generalize OOD. We construct a frameworkthat unifies our understanding of random variation with training dynamics, ruleselection with memorization, and data diversity with complexity. We show thatthese factors are nuanced, and that intermediate levels of diversity andcomplexity lead to inconsistent behavior across random seeds and to unstabletraining dynamics. Our findings emphasize the critical role of training data inshaping generalization patterns and illuminate how competing model strategieslead to inconsistent generalization outcomes across random seeds. Code isavailable at https://github.com/sunnytqin/concept_comp.git.</description><author>Tian Qin, Naomi Saphra, David Alvarez-Melis</author><pubDate>Tue, 17 Dec 2024 18:42:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04619v2</guid></item><item><title>BanglishRev: A Large-Scale Bangla-English and Code-mixed Dataset of Product Reviews in E-Commerce</title><link>http://arxiv.org/abs/2412.13161v1</link><description>This work presents the BanglishRev Dataset, the largest e-commerce productreview dataset to date for reviews written in Bengali, English, a mixture ofboth and Banglish, Bengali words written with English alphabets. The datasetcomprises of 1.74 million written reviews from 3.2 million ratings informationcollected from a total of 128k products being sold in online e-commerceplatforms targeting the Bengali population. It includes an extensive array ofrelated metadata for each of the reviews including the rating given by thereviewer, date the review was posted and date of purchase, number of likes,dislikes, response from the seller, images associated with the review etc. Withsentiment analysis being the most prominent usage of review datasets,experimentation with a binary sentiment analysis model with the review ratingserving as an indicator of positive or negative sentiment was conducted toevaluate the effectiveness of the large amount of data presented in BanglishRevfor sentiment analysis tasks. A BanglishBERT model is trained on the data fromBanglishRev with reviews being considered labeled positive if the rating isgreater than 3 and negative if the rating is less than or equal to 3. The modelis evaluated by being testing against a previously published manually annotateddataset for e-commerce reviews written in a mixture of Bangla, English andBanglish. The experimental model achieved an exceptional accuracy of 94\% andF1 score of 0.94, demonstrating the dataset's efficacy for sentiment analysis.Some of the intriguing patterns and observations seen within the dataset andfuture research directions where the dataset can be utilized is also discussedand explored. The dataset can be accessed throughhttps://huggingface.co/datasets/BanglishRev/bangla-english-and-code-mixed-ecommerce-review-dataset.</description><author>Mohammad Nazmush Shamael, Sabila Nawshin, Swakkhar Shatabda, Salekul Islam</author><pubDate>Tue, 17 Dec 2024 18:39:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13161v1</guid></item><item><title>Estimating Body and Hand Motion in an Ego-sensed World</title><link>http://arxiv.org/abs/2410.03665v3</link><description>We present EgoAllo, a system for human motion estimation from a head-mounteddevice. Using only egocentric SLAM poses and images, EgoAllo guides samplingfrom a conditional diffusion model to estimate 3D body pose, height, and handparameters that capture a device wearer's actions in the allocentric coordinateframe of the scene. To achieve this, our key insight is in representation: wepropose spatial and temporal invariance criteria for improving modelperformance, from which we derive a head motion conditioning parameterizationthat improves estimation by up to 18%. We also show how the bodies estimated byour system can improve hand estimation: the resulting kinematic and temporalconstraints can reduce world-frame errors in single-frame estimates by 40%.Project page: https://egoallo.github.io/</description><author>Brent Yi, Vickie Ye, Maya Zheng, Yunqi Li, Lea Müller, Georgios Pavlakos, Yi Ma, Jitendra Malik, Angjoo Kanazawa</author><pubDate>Tue, 17 Dec 2024 18:39:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03665v3</guid></item><item><title>QEDCartographer: Automating Formal Verification Using Reward-Free Reinforcement Learning</title><link>http://arxiv.org/abs/2408.09237v7</link><description>Formal verification is a promising method for producing reliable software,but the difficulty of manually writing verification proofs severely limits itsutility in practice. Recent methods have automated some proof synthesis byguiding a search through the proof space using a theorem prover. Unfortunately,the theorem prover provides only the crudest estimate of progress, resulting ineffectively undirected search. To address this problem, we createQEDCartographer, an automated proof-synthesis tool that combines supervised andreinforcement learning to more effectively explore the proof space.QEDCartographer incorporates the proofs' branching structure, enablingreward-free search and overcoming the sparse reward problem inherent to formalverification. We evaluate QEDCartographer using the CoqGym benchmark of 68.5Ktheorems from 124 open-source Coq projects. QEDCartographer fully automaticallyproves 21.4% of the test-set theorems. Previous search-based proof-synthesistools Tok, Tac, ASTactic, Passport, and Proverbot9001, which rely only onsupervised learning, prove 9.6%, 9.8%, 10.9%, 12.5%, and 19.8%, respectively.Diva, which combines 62 tools, proves 19.2%. Comparing to the most effectiveprior tool, Proverbot9001, QEDCartographer produces 34% shorter proofs 29%faster, on average over the theorems both tools prove. Together,QEDCartographer and non-learning-based CoqHammer prove 30.3% of the theorems,while CoqHammer alone proves 26.6%. Our work demonstrates that reinforcementlearning is a fruitful research direction for improving proof-synthesis tools'search mechanisms.</description><author>Alex Sanchez-Stern, Abhishek Varghese, Zhanna Kaufman, Dylan Zhang, Talia Ringer, Yuriy Brun</author><pubDate>Tue, 17 Dec 2024 18:37:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.09237v7</guid></item><item><title>A Conformal Approach to Feature-based Newsvendor under Model Misspecification</title><link>http://arxiv.org/abs/2412.13159v1</link><description>In many data-driven decision-making problems, performance guarantees oftendepend heavily on the correctness of model assumptions, which may frequentlyfail in practice. We address this issue in the context of a feature-basednewsvendor problem, where demand is influenced by observed features such asdemographics and seasonality. To mitigate the impact of model misspecification,we propose a model-free and distribution-free framework inspired by conformalprediction. Our approach consists of two phases: a training phase, which canutilize any type of prediction method, and a calibration phase thatconformalizes the model bias. To enhance predictive performance, we explore thebalance between data quality and quantity, recognizing the inherent trade-off:more selective training data improves quality but reduces quantity.Importantly, we provide statistical guarantees for the conformalized criticalquantile, independent of the correctness of the underlying model. Moreover, wequantify the confidence interval of the critical quantile, with its widthdecreasing as data quality and quantity improve. We validate our frameworkusing both simulated data and a real-world dataset from the Capital Bikeshareprogram in Washington, D.C. Across these experiments, our proposed methodconsistently outperforms benchmark algorithms, reducing newsvendor loss by upto 40% on the simulated data and 25% on the real-world dataset.</description><author>Junyu Cao</author><pubDate>Tue, 17 Dec 2024 18:34:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13159v1</guid></item><item><title>On Model Extrapolation in Marginal Shapley Values</title><link>http://arxiv.org/abs/2412.13158v1</link><description>As the use of complex machine learning models continues to grow, so does theneed for reliable explainability methods. One of the most popular methods formodel explainability is based on Shapley values. There are two most commonlyused approaches to calculating Shapley values which produce different resultswhen features are correlated, conditional and marginal. In our previous work,it was demonstrated that the conditional approach is fundamentally flawed dueto implicit assumptions of causality. However, it is a well-known fact thatmarginal approach to calculating Shapley values leads to model extrapolationwhere it might not be well defined. In this paper we explore the impacts ofmodel extrapolation on Shapley values in the case of a simple linear splinemodel. Furthermore, we propose an approach which while using marginal averagingavoids model extrapolation and with addition of causal information replicatescausal Shapley values. Finally, we demonstrate our method on the real dataexample.</description><author>Ilya Rozenfeld</author><pubDate>Tue, 17 Dec 2024 18:33:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13158v1</guid></item><item><title>Learning Visuotactile Estimation and Control for Non-prehensile Manipulation under Occlusions</title><link>http://arxiv.org/abs/2412.13157v1</link><description>Manipulation without grasping, known as non-prehensile manipulation, isessential for dexterous robots in contact-rich environments, but presents manychallenges relating with underactuation, hybrid-dynamics, and frictionaluncertainty. Additionally, object occlusions in a scenario of contactuncertainty and where the motion of the object evolves independently from therobot becomes a critical problem, which previous literature fails to address.We present a method for learning visuotactile state estimators anduncertainty-aware control policies for non-prehensile manipulation underocclusions, by leveraging diverse interaction data from privileged policiestrained in simulation. We formulate the estimator within a Bayesian deeplearning framework, to model its uncertainty, and then train uncertainty-awarecontrol policies by incorporating the pre-learned estimator into thereinforcement learning (RL) loop, both of which lead to significantly improvedestimator and policy performance. Therefore, unlike prior non-prehensileresearch that relies on complex external perception set-ups, our methodsuccessfully handles occlusions after sim-to-real transfer to robotic hardwarewith a simple onboard camera. See our video: https://youtu.be/hW-C8i_HWgs.</description><author>Juan Del Aguila Ferrandis, João Moura, Sethu Vijayakumar</author><pubDate>Tue, 17 Dec 2024 18:33:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13157v1</guid></item><item><title>S2S2: Semantic Stacking for Robust Semantic Segmentation in Medical Imaging</title><link>http://arxiv.org/abs/2412.13156v1</link><description>Robustness and generalizability in medical image segmentation are oftenhindered by scarcity and limited diversity of training data, which stands incontrast to the variability encountered during inference. While conventionalstrategies -- such as domain-specific augmentation, specialized architectures,and tailored training procedures -- can alleviate these issues, they depend onthe availability and reliability of domain knowledge. When such knowledge isunavailable, misleading, or improperly applied, performance may deteriorate. Inresponse, we introduce a novel, domain-agnostic, add-on, and data-drivenstrategy inspired by image stacking in image denoising. Termed ``semanticstacking,'' our method estimates a denoised semantic representation thatcomplements the conventional segmentation loss during training. This methoddoes not depend on domain-specific assumptions, making it broadly applicableacross diverse image modalities, model architectures, and augmentationtechniques. Through extensive experiments, we validate the superiority of ourapproach in improving segmentation performance under diverse conditions. Codeis available at https://github.com/ymp5078/Semantic-Stacking.</description><author>Yimu Pan, Sitao Zhang, Alison D. Gernand, Jeffery A. Goldstein, James Z. Wang</author><pubDate>Tue, 17 Dec 2024 18:30:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13156v1</guid></item><item><title>F-Bench: Rethinking Human Preference Evaluation Metrics for Benchmarking Face Generation, Customization, and Restoration</title><link>http://arxiv.org/abs/2412.13155v1</link><description>Artificial intelligence generative models exhibit remarkable capabilities incontent creation, particularly in face image generation, customization, andrestoration. However, current AI-generated faces (AIGFs) often fall short ofhuman preferences due to unique distortions, unrealistic details, andunexpected identity shifts, underscoring the need for a comprehensive qualityevaluation framework for AIGFs. To address this need, we introduce FaceQ, alarge-scale, comprehensive database of AI-generated Face images withfine-grained Quality annotations reflecting human preferences. The FaceQdatabase comprises 12,255 images generated by 29 models across three tasks: (1)face generation, (2) face customization, and (3) face restoration. It includes32,742 mean opinion scores (MOSs) from 180 annotators, assessed across multipledimensions: quality, authenticity, identity (ID) fidelity, and text-imagecorrespondence. Using the FaceQ database, we establish F-Bench, a benchmark forcomparing and evaluating face generation, customization, and restorationmodels, highlighting strengths and weaknesses across various prompts andevaluation dimensions. Additionally, we assess the performance of existingimage quality assessment (IQA), face quality assessment (FQA), AI-generatedcontent image quality assessment (AIGCIQA), and preference evaluation metrics,manifesting that these standard metrics are relatively ineffective inevaluating authenticity, ID fidelity, and text-image correspondence. The FaceQdatabase will be publicly available upon publication.</description><author>Lu Liu, Huiyu Duan, Qiang Hu, Liu Yang, Chunlei Cai, Tianxiao Ye, Huayu Liu, Xiaoyun Zhang, Guangtao Zhai</author><pubDate>Tue, 17 Dec 2024 18:28:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13155v1</guid></item><item><title>KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning</title><link>http://arxiv.org/abs/2409.12865v2</link><description>Knowledge graph reasoning plays a vital role in various applications and hasgarnered considerable attention. Recently, path-based methods have achievedimpressive performance. However, they may face limitations stemming fromconstraints in message-passing neural networks, such as missing paths andinformation over-squashing. In this paper, we revisit the application oftransformers for knowledge graph reasoning to address the constraints faced bypath-based methods and propose a novel method KnowFormer. KnowFormer utilizes atransformer architecture to perform reasoning on knowledge graphs from themessage-passing perspective, rather than reasoning by textual information likeprevious pretrained language model based methods. Specifically, we define theattention computation based on the query prototype of knowledge graphreasoning, facilitating convenient construction and efficient optimization. Toincorporate structural information into the self-attention mechanism, weintroduce structure-aware modules to calculate query, key, and valuerespectively. Additionally, we present an efficient attention computationmethod for better scalability. Experimental results demonstrate the superiorperformance of KnowFormer compared to prominent baseline methods on bothtransductive and inductive benchmarks.</description><author>Junnan Liu, Qianren Mao, Weifeng Jiang, Jianxin Li</author><pubDate>Tue, 17 Dec 2024 18:27:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12865v2</guid></item><item><title>Continuous Patient Monitoring with AI: Real-Time Analysis of Video in Hospital Care Settings</title><link>http://arxiv.org/abs/2412.13152v1</link><description>This study introduces an AI-driven platform for continuous and passivepatient monitoring in hospital settings, developed by LookDeep Health.Leveraging advanced computer vision, the platform provides real-time insightsinto patient behavior and interactions through video analysis, securely storinginference results in the cloud for retrospective evaluation. The dataset,compiled in collaboration with 11 hospital partners, encompasses over 300high-risk fall patients and over 1,000 days of inference, enabling applicationssuch as fall detection and safety monitoring for vulnerable patientpopulations. To foster innovation and reproducibility, an anonymized subset ofthis dataset is publicly available. The AI system detects key components inhospital rooms, including individual presence and role, furniture location,motion magnitude, and boundary crossings. Performance evaluation demonstratesstrong accuracy in object detection (macro F1-score = 0.92) and patient-roleclassification (F1-score = 0.98), as well as reliable trend analysis for the"patient alone" metric (mean logistic regression accuracy = 0.82 \pm 0.15).These capabilities enable automated detection of patient isolation, wandering,or unsupervised movement-key indicators for fall risk and other adverse events.This work establishes benchmarks for validating AI-driven patient monitoringsystems, highlighting the platform's potential to enhance patient safety andcare by providing continuous, data-driven insights into patient behavior andinteractions.</description><author>Paolo Gabriel, Peter Rehani, Tyler Troy, Tiffany Wyatt, Michael Choma, Narinder Singh</author><pubDate>Tue, 17 Dec 2024 18:23:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13152v1</guid></item><item><title>SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training With Significant Memory Reduction</title><link>http://arxiv.org/abs/2412.13148v1</link><description>Adaptive optimizers such as Adam (Kingma &amp; Ba, 2015) have been central to thesuccess of large language models. However, they maintain additional movingaverage states throughout training, which results in memory requirementsseveral times greater than the model. This overhead imposes constraints onscalability and computational efficiency. On the other hand, while stochasticgradient descent (SGD) is optimal in terms of memory efficiency, theircapability in LLM training is limited (Zhao et al., 2024b). To address this dilemma, we show that pre-processing SGD is sufficient toreach Adam-level performance on LLMs. Specifically, we propose to preprocessthe instantaneous stochastic gradients with two simple operators:$\mathtt{GradNorm}$ and $\mathtt{GradWhitening}$. $\mathtt{GradNorm}$stabilizes gradient distributions, and $\mathtt{GradWhitening}$ counteracts thelocal curvature of the loss landscape, respectively. This results in SWAN (SGDwith Whitening And Normalization), a stochastic optimizer that eliminates theneed to store any accumulative state variables. Empirically, SWAN has the samememory footprint as SGD, achieving $\approx 50\%$ reduction on total end-to-endmemory compared to Adam. In language modeling tasks, SWAN demonstrates the sameor even a substantial improvement over Adam. Specifically, when pre-trainingthe LLaMa model with 350M and 1.3B parameters, SWAN achieves a 2x speedup byreaching the same evaluation perplexity in less than half tokens seen.</description><author>Chao Ma, Wenbo Gong, Meyer Scetbon, Edward Meeds</author><pubDate>Tue, 17 Dec 2024 18:13:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13148v1</guid></item><item><title>Are Your LLMs Capable of Stable Reasoning?</title><link>http://arxiv.org/abs/2412.13147v1</link><description>The rapid advancement of Large Language Models (LLMs) has demonstratedremarkable progress in complex reasoning tasks. However, a significantdiscrepancy persists between benchmark performances and real-worldapplications. We identify this gap as primarily stemming from currentevaluation protocols and metrics, which inadequately capture the full spectrumof LLM capabilities, particularly in complex reasoning tasks where bothaccuracy and consistency are crucial. This work makes two key contributions.First, we introduce G-Pass@k, a novel evaluation metric that provides acontinuous assessment of model performance across multiple sampling attempts,quantifying both the model's peak performance potential and its stability.Second, we present LiveMathBench, a dynamic benchmark comprising challenging,contemporary mathematical problems designed to minimize data leakage risksduring evaluation. Through extensive experiments using G-Pass@k onstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insightsinto both their maximum capabilities and operational consistency. Our findingsreveal substantial room for improvement in LLMs' "realistic" reasoningcapabilities, highlighting the need for more robust evaluation methods. Thebenchmark and detailed results are available at:https://github.com/open-compass/GPassK.</description><author>Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, Kai Chen</author><pubDate>Tue, 17 Dec 2024 18:12:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13147v1</guid></item><item><title>Syntactic Transfer to Kyrgyz Using the Treebank Translation Method</title><link>http://arxiv.org/abs/2412.13146v1</link><description>The Kyrgyz language, as a low-resource language, requires significant effortto create high-quality syntactic corpora. This study proposes an approach tosimplify the development process of a syntactic corpus for Kyrgyz. We present atool for transferring syntactic annotations from Turkish to Kyrgyz based on atreebank translation method. The effectiveness of the proposed tool wasevaluated using the TueCL treebank. The results demonstrate that this approachachieves higher syntactic annotation accuracy compared to a monolingual modeltrained on the Kyrgyz KTMU treebank. Additionally, the study introduces amethod for assessing the complexity of manual annotation for the resultingsyntactic trees, contributing to further optimization of the annotationprocess.</description><author>Anton Alekseev, Alina Tillabaeva, Gulnara Dzh. Kabaeva, Sergey I. Nikolenko</author><pubDate>Tue, 17 Dec 2024 18:12:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13146v1</guid></item><item><title>Agnosticism About Artificial Consciousness</title><link>http://arxiv.org/abs/2412.13145v1</link><description>Could an AI have conscious experiences? Any answer to this question shouldconform to Evidentialism - that is, it should be based not on intuition, dogmaor speculation but on solid scientific evidence. I argue that such evidence ishard to come by and that the only justifiable stance on the prospects ofartificial consciousness is agnosticism. In the current debate, the maindivision is between biological views that are sceptical of artificialconsciousness and functional views that are sympathetic to it. I argue thatboth camps make the same mistake of over-estimating what the evidence tells us.Scientific insights into consciousness have been achieved through the study ofconscious organisms. Although this has enabled cautious assessments ofconsciousness in various creatures, extending this to AI faces seriousobstacles. AI thus presents consciousness researchers with a dilemma: eitherreach a verdict on artificial consciousness but violate Evidentialism; orrespect Evidentialism but offer no verdict on the prospects of artificialconsciousness. The dominant trend in the literature has been to take the firstoption while purporting to follow the scientific evidence. I argue that if wetruly follow the evidence, we must take the second option and adoptagnosticism.</description><author>Tom McClelland</author><pubDate>Tue, 17 Dec 2024 18:11:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13145v1</guid></item><item><title>Label Errors in the Tobacco3482 Dataset</title><link>http://arxiv.org/abs/2412.13140v1</link><description>Tobacco3482 is a widely used document classification benchmark dataset.However, our manual inspection of the entire dataset uncovers widespreadontological issues, especially large amounts of annotation label problems inthe dataset. We establish data label guidelines and find that 11.7% of thedataset is improperly annotated and should either have an unknown label or acorrected label, and 16.7% of samples in the dataset have multiple validlabels. We then analyze the mistakes of a top-performing model and find that35% of the model's mistakes can be directly attributed to these label issues,highlighting the inherent problems with using a noisily labeled dataset as abenchmark. Supplementary material, including dataset annotations and code, isavailable at https://github.com/gordon-lim/tobacco3482-mistakes/.</description><author>Gordon Lim, Stefan Larson, Kevin Leach</author><pubDate>Tue, 17 Dec 2024 18:06:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13140v1</guid></item><item><title>Reinforcement Learning Enhanced LLMs: A Survey</title><link>http://arxiv.org/abs/2412.10400v2</link><description>This paper surveys research in the rapidly growing field of enhancing largelanguage models (LLMs) with reinforcement learning (RL), a technique thatenables LLMs to improve their performance by receiving feedback in the form ofrewards based on the quality of their outputs, allowing them to generate moreaccurate, coherent, and contextually appropriate responses. In this work, wemake a systematic review of the most up-to-date state of knowledge onRL-enhanced LLMs, attempting to consolidate and analyze the rapidly growingresearch in this field, helping researchers understand the current challengesand advancements. Specifically, we (1) detail the basics of RL; (2) introducepopular RL-enhanced LLMs; (3) review researches on two widely-used rewardmodel-based RL techniques: Reinforcement Learning from Human Feedback (RLHF)and Reinforcement Learning from AI Feedback (RLAIF); and (4) explore DirectPreference Optimization (DPO), a set of methods that bypass the reward model todirectly use human preference data for aligning LLM outputs with humanexpectations. We will also point out current challenges and deficiencies ofexisting methods and suggest some avenues for further improvements. Projectpage of this work can be found at:\url{https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey}.</description><author>Shuhe Wang, Shengyu Zhang, Jie Zhang, Runyi Hu, Xiaoya Li, Tianwei Zhang, Jiwei Li, Fei Wu, Guoyin Wang, Eduard Hovy</author><pubDate>Tue, 17 Dec 2024 18:05:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10400v2</guid></item><item><title>Unlocking the Potential of Digital Pathology: Novel Baselines for Compression</title><link>http://arxiv.org/abs/2412.13137v1</link><description>Digital pathology offers a groundbreaking opportunity to transform clinicalpractice in histopathological image analysis, yet faces a significant hurdle:the substantial file sizes of pathological Whole Slide Images (WSI). Whilecurrent digital pathology solutions rely on lossy JPEG compression to addressthis issue, lossy compression can introduce color and texture disparities,potentially impacting clinical decision-making. While prior research addressesperceptual image quality and downstream performance independently of eachother, we jointly evaluate compression schemes for perceptual and downstreamtask quality on four different datasets. In addition, we collect an initiallyuncompressed dataset for an unbiased perceptual evaluation of compressionschemes. Our results show that deep learning models fine-tuned for perceptualquality outperform conventional compression schemes like JPEG-XL or WebP forfurther compression of WSI. However, they exhibit a significant bias towardsthe compression artifacts present in the training data and struggle togeneralize across various compression schemes. We introduce a novel evaluationmetric based on feature similarity between original files and compressed filesthat aligns very well with the actual downstream performance on the compressedWSI. Our metric allows for a general and standardized evaluation of lossycompression schemes and mitigates the requirement to independently assessdifferent downstream tasks. Our study provides novel insights for theassessment of lossy compression schemes for WSI and encourages a unifiedevaluation of lossy compression schemes to accelerate the clinical uptake ofdigital pathology.</description><author>Maximilian Fischer, Peter Neher, Peter Schüffler, Sebastian Ziegler, Shuhan Xiao, Robin Peretzke, David Clunie, Constantin Ulrich, Michael Baumgartner, Alexander Muckenhuber, Silvia Dias Almeida, Michael Götz, Jens Kleesiek, Marco Nolden, Rickmer Braren, Klaus Maier-Hein</author><pubDate>Tue, 17 Dec 2024 18:04:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13137v1</guid></item><item><title>Practicable Black-box Evasion Attacks on Link Prediction in Dynamic Graphs -- A Graph Sequential Embedding Method</title><link>http://arxiv.org/abs/2412.13134v1</link><description>Link prediction in dynamic graphs (LPDG) has been widely applied toreal-world applications such as website recommendation, traffic flowprediction, organizational studies, etc. These models are usually kept localand secure, with only the interactive interface restrictively available to thepublic. Thus, the problem of the black-box evasion attack on the LPDG model,where model interactions and data perturbations are restricted, seems to beessential and meaningful in practice. In this paper, we propose the firstpracticable black-box evasion attack method that achieves effective attacksagainst the target LPDG model, within a limited amount of interactions andperturbations. To perform effective attacks under limited perturbations, wedevelop a graph sequential embedding model to find the desired state embeddingof the dynamic graph sequences, under a deep reinforcement learning framework.To overcome the scarcity of interactions, we design a multi-environmenttraining pipeline and train our agent for multiple instances, by sharing anaggregate interaction buffer. Finally, we evaluate our attack against threeadvanced LPDG models on three real-world graph datasets of different scales andcompare its performance with related methods under the interaction andperturbation constraints. Experimental results show that our attack is botheffective and practicable.</description><author>Jiate Li, Meng Pang, Binghui Wang</author><pubDate>Tue, 17 Dec 2024 17:53:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13134v1</guid></item><item><title>Previous Knowledge Utilization In Online Anytime Belief Space Planning</title><link>http://arxiv.org/abs/2412.13128v1</link><description>Online planning under uncertainty remains a critical challenge in roboticsand autonomous systems. While tree search techniques are commonly employed toconstruct partial future trajectories within computational constraints, mostexisting methods discard information from previous planning sessionsconsidering continuous spaces. This study presents a novel, computationallyefficient approach that leverages historical planning data in currentdecision-making processes. We provide theoretical foundations for ourinformation reuse strategy and introduce an algorithm based on Monte Carlo TreeSearch (MCTS) that implements this approach. Experimental results demonstratethat our method significantly reduces computation time while maintaining highperformance levels. Our findings suggest that integrating historical planninginformation can substantially improve the efficiency of online decision-makingin uncertain environments, paving the way for more responsive and adaptiveautonomous systems.</description><author>Michael Novitsky, Moran Barenboim, Vadim Indelman</author><pubDate>Tue, 17 Dec 2024 17:45:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13128v1</guid></item><item><title>A Knowledge-enhanced Pathology Vision-language Foundation Model for Cancer Diagnosis</title><link>http://arxiv.org/abs/2412.13126v1</link><description>Deep learning has enabled the development of highly robust foundation modelsfor various pathological tasks across diverse diseases and patient cohorts.Among these models, vision-language pre-training, which leverages large-scalepaired data to align pathology image and text embedding spaces, and provides anovel zero-shot paradigm for downstream tasks. However, existing models havebeen primarily data-driven and lack the incorporation of domain-specificknowledge, which limits their performance in cancer diagnosis, especially forrare tumor subtypes. To address this limitation, we establish aKnowledge-enhanced Pathology (KEEP) foundation model that harnesses diseaseknowledge to facilitate vision-language pre-training. Specifically, we firstconstruct a disease knowledge graph (KG) that covers 11,454 human diseases with139,143 disease attributes, including synonyms, definitions, and hypernymrelations. We then systematically reorganize the millions of publicly availablenoisy pathology image-text pairs, into 143K well-structured semantic groupslinked through the hierarchical relations of the disease KG. To derive morenuanced image and text representations, we propose a novel knowledge-enhancedvision-language pre-training approach that integrates disease knowledge intothe alignment within hierarchical semantic groups instead of unstructuredimage-text pairs. Validated on 18 diverse benchmarks with more than 14,000whole slide images (WSIs), KEEP achieves state-of-the-art performance inzero-shot cancer diagnostic tasks. Notably, for cancer detection, KEEPdemonstrates an average sensitivity of 89.8% at a specificity of 95.0% across 7cancer types. For cancer subtyping, KEEP achieves a median balanced accuracy of0.456 in subtyping 30 rare brain cancers, indicating strong generalizabilityfor diagnosing rare tumors.</description><author>Xiao Zhou, Luoyi Sun, Dexuan He, Wenbin Guan, Ruifen Wang, Lifeng Wang, Xin Sun, Kun Sun, Ya Zhang, Yanfeng Wang, Weidi Xie</author><pubDate>Tue, 17 Dec 2024 17:45:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13126v1</guid></item><item><title>Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models</title><link>http://arxiv.org/abs/2409.13474v3</link><description>Machine unlearning aims to efficiently eliminate the influence of specifictraining data, known as the forget set, from the model. However, existingunlearning methods for Large Language Models (LLMs) face a critical challenge:they rely solely on negative feedback to suppress responses related to theforget set, which often results in nonsensical or inconsistent outputs,diminishing model utility and posing potential privacy risks. To address thislimitation, we propose a novel approach called Alternate PreferenceOptimization (AltPO), which combines negative feedback with in-domain positivefeedback on the forget set. Additionally, we introduce new evaluation metricsto assess the quality of responses related to the forget set. Extensiveexperiments show that our approach not only enables effective unlearning butalso avoids undesirable model behaviors while maintaining overall modelperformance. Our implementation can be found athttps://github.com/molereddy/Alternate-Preference-Optimization.</description><author>Anmol Mekala, Vineeth Dorna, Shreya Dubey, Abhishek Lalwani, David Koleczek, Mukund Rungta, Sadid Hasan, Elita Lobo</author><pubDate>Tue, 17 Dec 2024 17:45:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13474v3</guid></item><item><title>Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling</title><link>http://arxiv.org/abs/2412.05271v3</link><description>We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)series that builds upon InternVL 2.0, maintaining its core model architecturewhile introducing significant enhancements in training and testing strategiesas well as data quality. In this work, we delve into the relationship betweenmodel scaling and performance, systematically exploring the performance trendsin vision encoders, language models, dataset sizes, and test-timeconfigurations. Through extensive evaluations on a wide range of benchmarks,including multi-discipline reasoning, document understanding, multi-image /video understanding, real-world comprehension, multimodal hallucinationdetection, visual grounding, multilingual capabilities, and pure languageprocessing, InternVL 2.5 exhibits competitive performance, rivaling leadingcommercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model isthe first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasingstrong potential for test-time scaling. We hope this model contributes to theopen-source community by setting new standards for developing and applyingmultimodal AI systems. HuggingFace demo seehttps://huggingface.co/spaces/OpenGVLab/InternVL</description><author>Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, Wenhai Wang</author><pubDate>Tue, 17 Dec 2024 17:44:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05271v3</guid></item><item><title>RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation</title><link>http://arxiv.org/abs/2409.16383v4</link><description>Riddle-solving requires advanced reasoning skills, pushing LLMs to engage inabstract thinking and creative problem-solving, often revealing limitations intheir cognitive abilities. In this paper, we examine the riddle-solvingcapabilities of LLMs using a multiple-choice format, exploring how differentprompting techniques impact performance on riddles that demand diversereasoning skills. To enhance results, we introduce RISCORE (RIddle Solving withCOntext REcontruciton) a novel fully automated prompting method that generatesand utilizes contextually reconstructed sentence-based puzzles in conjunctionwith the original examples to create few-shot exemplars. Our experimentsdemonstrate that RISCORE significantly improves the performance of languagemodels in both vertical and lateral thinking tasks, surpassing traditionalexemplar selection strategies across a variety of few-shot settings.</description><author>Ioannis Panagiotopoulos, Giorgos Filandrianos, Maria Lymperaiou, Giorgos Stamou</author><pubDate>Tue, 17 Dec 2024 17:42:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16383v4</guid></item><item><title>Equity in the Use of ChatGPT for the Classroom: A Comparison of the Accuracy and Precision of ChatGPT 3.5 vs. ChatGPT4 with Respect to Statistics and Data Science Exams</title><link>http://arxiv.org/abs/2412.13116v1</link><description>A college education historically has been seen as method of moving upwardwith regards to income brackets and social status. Indeed, many collegesrecognize this connection and seek to enroll talented low income students.While these students might have their education, books, room, and board paid;there are other items that they might be expected to use that are not part ofmost college scholarship packages. One of those items that has recentlysurfaced is access to generative AI platforms. The most popular of theseplatforms is ChatGPT, and it has a paid version (ChatGPT4) and a free version(ChatGPT3.5). We seek to explore differences in the free and paid versions inthe context of homework questions and data analyses as might be seen in atypical introductory statistics course. We determine the extent to whichstudents who cannot afford newer and faster versions of generative AI programswould be disadvantaged in terms of writing such projects and learning thesemethods.</description><author>Monnie McGee, Bivin Sadler</author><pubDate>Tue, 17 Dec 2024 17:38:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13116v1</guid></item><item><title>Motion-2-to-3: Leveraging 2D Motion Data to Boost 3D Motion Generation</title><link>http://arxiv.org/abs/2412.13111v1</link><description>Text-driven human motion synthesis is capturing significant attention for itsability to effortlessly generate intricate movements from abstract text cues,showcasing its potential for revolutionizing motion design not only in filmnarratives but also in virtual reality experiences and computer gamedevelopment. Existing methods often rely on 3D motion capture data, whichrequire special setups resulting in higher costs for data acquisition,ultimately limiting the diversity and scope of human motion. In contrast, 2Dhuman videos offer a vast and accessible source of motion data, covering awider range of styles and activities. In this paper, we explore leveraging 2Dhuman motion extracted from videos as an alternative data source to improvetext-driven 3D motion generation. Our approach introduces a novel frameworkthat disentangles local joint motion from global movements, enabling efficientlearning of local motion priors from 2D data. We first train a single-view 2Dlocal motion generator on a large dataset of text-motion pairs. To enhance thismodel to synthesize 3D motion, we fine-tune the generator with 3D data,transforming it into a multi-view generator that predicts view-consistent localjoint motion and root dynamics. Experiments on the HumanML3D dataset and noveltext prompts demonstrate that our method efficiently utilizes 2D data,supporting realistic 3D human motion generation and broadening the range ofmotion types it supports. Our code will be made publicly available athttps://zju3dv.github.io/Motion-2-to-3/.</description><author>Huaijin Pi, Ruoxi Guo, Zehong Shen, Qing Shuai, Zechen Hu, Zhumei Wang, Yajiao Dong, Ruizhen Hu, Taku Komura, Sida Peng, Xiaowei Zhou</author><pubDate>Tue, 17 Dec 2024 17:34:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13111v1</guid></item><item><title>Improving Explainability of Sentence-level Metrics via Edit-level Attribution for Grammatical Error Correction</title><link>http://arxiv.org/abs/2412.13110v1</link><description>Various evaluation metrics have been proposed for Grammatical ErrorCorrection (GEC), but many, particularly reference-free metrics, lackexplainability. This lack of explainability hinders researchers from analyzingthe strengths and weaknesses of GEC models and limits the ability to providedetailed feedback for users. To address this issue, we propose attributingsentence-level scores to individual edits, providing insight into how specificcorrections contribute to the overall performance. For the attribution method,we use Shapley values, from cooperative game theory, to compute thecontribution of each edit. Experiments with existing sentence-level metricsdemonstrate high consistency across different edit granularities and showapproximately 70\% alignment with human evaluations. In addition, we analyzebiases in the metrics based on the attribution results, revealing trends suchas the tendency to ignore orthographic edits. Our implementation is availableat \url{https://github.com/naist-nlp/gec-attribute}.</description><author>Takumi Goto, Justin Vasselli, Taro Watanabe</author><pubDate>Tue, 17 Dec 2024 17:31:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13110v1</guid></item><item><title>Anytime Multi-Agent Path Finding with an Adaptive Delay-Based Heuristic</title><link>http://arxiv.org/abs/2408.02960v2</link><description>Anytime multi-agent path finding (MAPF) is a promising approach to scalablepath optimization in multi-agent systems. MAPF-LNS, based on Large NeighborhoodSearch (LNS), is the current state-of-the-art approach where a fast initialsolution is iteratively optimized by destroying and repairing selected paths ofthe solution. Current MAPF-LNS variants commonly use an adaptive selectionmechanism to choose among multiple destroy heuristics. However, to determinepromising destroy heuristics, MAPF-LNS requires a considerable amount ofexploration time. As common destroy heuristics are non-adaptive, anyperformance bottleneck caused by these heuristics cannot be overcome viaadaptive heuristic selection alone, thus limiting the overall effectiveness ofMAPF-LNS in terms of solution cost. In this paper, we propose AdaptiveDelay-based Destroy-and-Repair Enhanced with Success-based Self-Learning(ADDRESS) as a single-destroy-heuristic variant of MAPF-LNS. ADDRESS appliesrestricted Thompson Sampling to the top-K set of the most delayed agents toselect a seed agent for adaptive LNS neighborhood generation. We evaluateADDRESS in multiple maps from the MAPF benchmark set and demonstrate costimprovements by at least 50% in large-scale scenarios with up to a thousandagents, compared with the original MAPF-LNS and other state-of-the-art methods.</description><author>Thomy Phan, Benran Zhang, Shao-Hung Chan, Sven Koenig</author><pubDate>Tue, 17 Dec 2024 17:29:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02960v2</guid></item><item><title>Stably unactivated neurons in ReLU neural networks</title><link>http://arxiv.org/abs/2412.06829v2</link><description>The choice of architecture of a neural network influences which functionswill be realizable by that neural network and, as a result, studying theexpressiveness of a chosen architecture has received much attention. In ReLUneural networks, the presence of stably unactivated neurons can reduce thenetwork's expressiveness. In this work, we investigate the probability of aneuron in the second hidden layer of such neural networks being stablyunactivated when the weights and biases are initialized from symmetricprobability distributions. For networks with input dimension $n_0$, we provethat if the first hidden layer has $n_0+1$ neurons then this probability isexactly $\frac{2^{n_0}+1}{4^{n_0+1}}$, and if the first hidden layer has $n_1$neurons, $n_1 \le n_0$, then the probability is $\frac{1}{2^{n_1+1}}$. Finally,for the case when the first hidden layer has more neurons than $n_0+1$, aconjecture is proposed along with the rationale. Computational evidence ispresented to support the conjecture.</description><author>Natalie Brownlowe, Christopher R. Cornwell, Ethan Montes, Gabriel Quijano, Grace Stulman, Na Zhang</author><pubDate>Tue, 17 Dec 2024 17:28:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06829v2</guid></item><item><title>Active Reinforcement Learning Strategies for Offline Policy Improvement</title><link>http://arxiv.org/abs/2412.13106v1</link><description>Learning agents that excel at sequential decision-making tasks mustcontinuously resolve the problem of exploration and exploitation for optimallearning. However, such interactions with the environment online might beprohibitively expensive and may involve some constraints, such as a limitedbudget for agent-environment interactions and restricted exploration in certainregions of the state space. Examples include selecting candidates for medicaltrials and training agents in complex navigation environments. This problemnecessitates the study of active reinforcement learning strategies that collectminimal additional experience trajectories by reusing existing offline datapreviously collected by some unknown behavior policy. In this work, we proposea representation-aware uncertainty-based active trajectory collection methodthat intelligently decides interaction strategies that consider thedistribution of the existing offline data. With extensive experimentation, wedemonstrate that our proposed method reduces additional online interaction withthe environment by up to 75% over competitive baselines across variouscontinuous control environments.</description><author>Ambedkar Dukkipati, Ranga Shaarad Ayyagari, Bodhisattwa Dasgupta, Parag Dutta, Prabhas Reddy Onteru</author><pubDate>Tue, 17 Dec 2024 17:22:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13106v1</guid></item><item><title>Systematic Biases in LLM Simulations of Debates</title><link>http://arxiv.org/abs/2402.04049v3</link><description>The emergence of Large Language Models (LLMs), has opened excitingpossibilities for constructing computational simulations designed to replicatehuman behavior accurately. Current research suggests that LLM-based agentsbecome increasingly human-like in their performance, sparking interest in usingthese AI agents as substitutes for human participants in behavioral studies.However, LLMs are complex statistical learners without straightforwarddeductive rules, making them prone to unexpected behaviors. Hence, it iscrucial to study and pinpoint the key behavioral distinctions between humansand LLM-based agents. In this study, we highlight the limitations of LLMs insimulating human interactions, particularly focusing on LLMs' ability tosimulate political debates on topics that are important aspects of people'sday-to-day lives and decision-making processes. Our findings indicate atendency for LLM agents to conform to the model's inherent social biasesdespite being directed to debate from certain political perspectives. Thistendency results in behavioral patterns that seem to deviate fromwell-established social dynamics among humans. We reinforce these observationsusing an automatic self-fine-tuning method, which enables us to manipulate thebiases within the LLM and demonstrate that agents subsequently align with thealtered biases. These results underscore the need for further research todevelop methods that help agents overcome these biases, a critical step towardcreating more realistic simulations.</description><author>Amir Taubenfeld, Yaniv Dover, Roi Reichart, Ariel Goldstein</author><pubDate>Tue, 17 Dec 2024 17:17:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04049v3</guid></item><item><title>AI PERSONA: Towards Life-long Personalization of LLMs</title><link>http://arxiv.org/abs/2412.13103v1</link><description>In this work, we introduce the task of life-long personalization of largelanguage models. While recent mainstream efforts in the LLM community mainlyfocus on scaling data and compute for improved capabilities of LLMs, we arguethat it is also very important to enable LLM systems, or language agents, tocontinuously adapt to the diverse and ever-changing profiles of every distinctuser and provide up-to-date personalized assistance. We provide a clear taskformulation and introduce a simple, general, effective, and scalable frameworkfor life-long personalization of LLM systems and language agents. To facilitatefuture research on LLM personalization, we also introduce methods to synthesizerealistic benchmarks and robust evaluation metrics. We will release all codesand data for building and benchmarking life-long personalized LLM systems.</description><author>Tiannan Wang, Meiling Tao, Ruoyu Fang, Huilin Wang, Shuai Wang, Yuchen Eleanor Jiang, Wangchunshu Zhou</author><pubDate>Tue, 17 Dec 2024 17:17:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13103v1</guid></item><item><title>AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark</title><link>http://arxiv.org/abs/2412.13102v1</link><description>Evaluation plays a crucial role in the advancement of information retrieval(IR) models. However, current benchmarks, which are based on predefined domainsand human-labeled data, face limitations in addressing evaluation needs foremerging domains both cost-effectively and efficiently. To address thischallenge, we propose the Automated Heterogeneous Information RetrievalBenchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)Automated. The testing data in AIR-Bench is automatically generated by largelanguage models (LLMs) without human intervention. 2) Heterogeneous. Thetesting data in AIR-Bench is generated with respect to diverse tasks, domainsand languages. 3) Dynamic. The domains and languages covered by AIR-Bench areconstantly augmented to provide an increasingly comprehensive evaluationbenchmark for community developers. We develop a reliable and robust datageneration pipeline to automatically create diverse and high-quality evaluationdatasets based on real-world corpora. Our findings demonstrate that thegenerated testing data in AIR-Bench aligns well with human-labeled testingdata, making AIR-Bench a dependable benchmark for evaluating IR models. Theresources in AIR-Bench are publicly available athttps://github.com/AIR-Bench/AIR-Bench.</description><author>Jianlyu Chen, Nan Wang, Chaofan Li, Bo Wang, Shitao Xiao, Han Xiao, Hao Liao, Defu Lian, Zheng Liu</author><pubDate>Tue, 17 Dec 2024 17:15:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13102v1</guid></item><item><title>TKAN: Temporal Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2405.07344v3</link><description>Recurrent Neural Networks (RNNs) have revolutionized many areas of machinelearning, particularly in natural language and data sequence processing. LongShort-Term Memory (LSTM) has demonstrated its ability to capture long-termdependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks(KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposeda new neural networks architecture inspired by KAN and the LSTM, the TemporalKolomogorov-Arnold Networks (TKANs). TKANs combined the strenght of bothnetworks, it is composed of Recurring Kolmogorov-Arnold Networks (RKANs) Layersembedding memory management. This innovation enables us to perform multi-steptime series forecasting with enhanced accuracy and efficiency. By addressingthe limitations of traditional models in handling complex sequential patterns,the TKAN architecture offers significant potential for advancements in fieldsrequiring more than one step ahead forecasting.</description><author>Remi Genet, Hugo Inzirillo</author><pubDate>Tue, 17 Dec 2024 17:13:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07344v3</guid></item><item><title>Accuracy Limits as a Barrier to Biometric System Security</title><link>http://arxiv.org/abs/2412.13099v1</link><description>Biometric systems are widely used for identity verification andidentification, including authentication (i.e., one-to-one matching to verify aclaimed identity) and identification (i.e., one-to-many matching to find asubject in a database). The matching process relies on measuring similaritiesor dissimilarities between a fresh biometric template and enrolled templates.The False Match Rate FMR is a key metric for assessing the accuracy andreliability of such systems. This paper analyzes biometric systems based ontheir FMR, with two main contributions. First, we explore untargeted attacks,where an adversary aims to impersonate any user within a database. We determinethe number of trials required for an attacker to successfully impersonate auser and derive the critical population size (i.e., the maximum number of usersin the database) required to maintain a given level of security. Furthermore,we compute the critical FMR value needed to ensure resistance againstuntargeted attacks as the database size increases. Second, we revisit thebiometric birthday problem to evaluate the approximate and exact probabilitiesthat two users in a database collide (i.e., can impersonate each other). Basedon this analysis, we derive both the approximate critical population size andthe critical FMR value needed to bound the likelihood of such collisionsoccurring with a given probability. These thresholds offer insights fordesigning systems that mitigate the risk of impersonation and collisions,particularly in large-scale biometric databases. Our findings indicate thatcurrent biometric systems fail to deliver sufficient accuracy to achieve anadequate security level against untargeted attacks, even in small-scaledatabases. Moreover, state-of-the-art systems face significant challenges inaddressing the biometric birthday problem, especially as database sizes grow.</description><author>Axel Durbet, Paul-Marie Grollemund, Pascal Lafourcade, Kevin Thiry-Atighehchi</author><pubDate>Tue, 17 Dec 2024 17:10:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13099v1</guid></item><item><title>Uchaguzi-2022: A Dataset of Citizen Reports on the 2022 Kenyan Election</title><link>http://arxiv.org/abs/2412.13098v1</link><description>Online reporting platforms have enabled citizens around the world tocollectively share their opinions and report in real time on events impactingtheir local communities. Systematically organizing (e.g., categorizing byattributes) and geotagging large amounts of crowdsourced information is crucialto ensuring that accurate and meaningful insights can be drawn from this dataand used by policy makers to bring about positive change. These tasks, however,typically require extensive manual annotation efforts. In this paper we presentUchaguzi-2022, a dataset of 14k categorized and geotagged citizen reportsrelated to the 2022 Kenyan General Election containing mentions ofelection-related issues such as official misconduct, vote count irregularities,and acts of violence. We use this dataset to investigate whether languagemodels can assist in scalably categorizing and geotagging reports, thushighlighting its potential application in the AI for Social Good space.</description><author>Roberto Mondini, Neema Kotonya, Robert L. Logan IV, Elizabeth M Olson, Angela Oduor Lungati, Daniel Duke Odongo, Tim Ombasa, Hemank Lamba, Aoife Cahill, Joel R. Tetreault, Alejandro Jaimes</author><pubDate>Tue, 17 Dec 2024 17:08:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13098v1</guid></item><item><title>Incremental Online Learning of Randomized Neural Network with Forward Regularization</title><link>http://arxiv.org/abs/2412.13096v1</link><description>Online learning of deep neural networks suffers from challenges such ashysteretic non-incremental updating, increasing memory usage, pastretrospective retraining, and catastrophic forgetting. To alleviate thesedrawbacks and achieve progressive immediate decision-making, we propose a novelIncremental Online Learning (IOL) process of Randomized Neural Networks(Randomized NN), a framework facilitating continuous improvements to RandomizedNN performance in restrictive online scenarios. Within the framework, wefurther introduce IOL with ridge regularization (-R) and IOL with forwardregularization (-F). -R generates stepwise incremental updates withoutretrospective retraining and avoids catastrophic forgetting. Moreover, wesubstituted -R with -F as it enhanced precognition learning ability usingsemi-supervision and realized better online regrets to offline global expertscompared to -R during IOL. The algorithms of IOL for Randomized NN with -R/-Fon non-stationary batch stream were derived respectively, featuring recursiveweight updates and variable learning rates. Additionally, we conducted adetailed analysis and theoretically derived relative cumulative regret boundsof the Randomized NN learners with -R/-F in IOL under adversarial assumptionsusing a novel methodology and presented several corollaries, from which weobserved the superiority on online learning acceleration and regret bounds ofemploying -F in IOL. Finally, our proposed methods were rigorously examinedacross regression and classification tasks on diverse datasets, whichdistinctly validated the efficacy of IOL frameworks of Randomized NN and theadvantages of forward regularization.</description><author>Junda Wang, Minghui Hu, Ning Li, Abdulaziz Al-Ali, Ponnuthurai Nagaratnam Suganthan</author><pubDate>Tue, 17 Dec 2024 17:06:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13096v1</guid></item><item><title>Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur automatischen Bewertung von Hausaufgaben</title><link>http://arxiv.org/abs/2412.06651v4</link><description>[Study in German language.] This study examines the AI-powered grading tool"AI Grading Assistant" by the German company Fobizz, designed to supportteachers in evaluating and providing feedback on student assignments. Againstthe societal backdrop of an overburdened education system and risingexpectations for artificial intelligence as a solution to these challenges, theinvestigation evaluates the tool's functional suitability through two testseries. The results reveal significant shortcomings: The tool's numericalgrades and qualitative feedback are often random and do not improve even whenits suggestions are incorporated. The highest ratings are achievable only withtexts generated by ChatGPT. False claims and nonsensical submissions frequentlygo undetected, while the implementation of some grading criteria is unreliableand opaque. Since these deficiencies stem from the inherent limitations oflarge language models (LLMs), fundamental improvements to this or similar toolsare not immediately foreseeable. The study critiques the broader trend ofadopting AI as a quick fix for systemic problems in education, concluding thatFobizz's marketing of the tool as an objective and time-saving solution ismisleading and irresponsible. Finally, the study calls for systematicevaluation and subject-specific pedagogical scrutiny of the use of AI tools ineducational contexts.</description><author>Rainer Muehlhoff, Marte Henningsen</author><pubDate>Tue, 17 Dec 2024 17:06:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06651v4</guid></item><item><title>Agentic AI-Driven Technical Troubleshooting for Enterprise Systems: A Novel Weighted Retrieval-Augmented Generation Paradigm</title><link>http://arxiv.org/abs/2412.12006v2</link><description>Technical troubleshooting in enterprise environments often involvesnavigating diverse, heterogeneous data sources to resolve complex issueseffectively. This paper presents a novel agentic AI solution built on aWeighted Retrieval-Augmented Generation (RAG) Framework tailored for enterprisetechnical troubleshooting. By dynamically weighting retrieval sources such asproduct manuals, internal knowledge bases, FAQs, and troubleshooting guidesbased on query context, the framework prioritizes the most relevant data. Forinstance, it gives precedence to product manuals for SKU-specific queries whileincorporating general FAQs for broader issues. The system employs FAISS forefficient dense vector search, coupled with a dynamic aggregation mechanism toseamlessly integrate results from multiple sources. A Llama-basedself-evaluator ensures the contextual accuracy and confidence of the generatedresponses before delivering them. This iterative cycle of retrieval andvalidation enhances precision, diversity, and reliability in responsegeneration. Preliminary evaluations on large enterprise datasets demonstratethe framework's efficacy in improving troubleshooting accuracy, reducingresolution times, and adapting to varied technical challenges. Future researchaims to enhance the framework by integrating advanced conversational AIcapabilities, enabling more interactive and intuitive troubleshootingexperiences. Efforts will also focus on refining the dynamic weightingmechanism through reinforcement learning to further optimize the relevance andprecision of retrieved information. By incorporating these advancements, theproposed framework is poised to evolve into a comprehensive, autonomous AIsolution, redefining technical service workflows across enterprise settings.</description><author>Rajat Khanda</author><pubDate>Tue, 17 Dec 2024 17:02:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12006v2</guid></item><item><title>Reservoir Computing for Fast, Simplified Reinforcement Learning on Memory Tasks</title><link>http://arxiv.org/abs/2412.13093v1</link><description>Tasks in which rewards depend upon past information not available in thecurrent observation set can only be solved by agents that are equipped withshort-term memory. Usual choices for memory modules include trainable recurrenthidden layers, often with gated memory. Reservoir computing presents analternative, in which a recurrent layer is not trained, but rather has a set offixed, sparse recurrent weights. The weights are scaled to produce stabledynamical behavior such that the reservoir state contains a high-dimensional,nonlinear impulse response function of the inputs. An output decoder networkcan then be used to map the compressive history represented by the reservoir'sstate to any outputs, including agent actions or predictions. In this study, wefind that reservoir computing greatly simplifies and speeds up reinforcementlearning on memory tasks by (1) eliminating the need for backpropagation ofgradients through time, (2) presenting all recent history simultaneously to thedownstream network, and (3) performing many useful and generic nonlinearcomputations upstream from the trained modules. In particular, these findingsoffer significant benefit to meta-learning that depends primarily on efficientand highly general memory systems.</description><author>Kevin McKee</author><pubDate>Tue, 17 Dec 2024 17:02:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13093v1</guid></item><item><title>LMUnit: Fine-grained Evaluation with Natural Language Unit Tests</title><link>http://arxiv.org/abs/2412.13091v1</link><description>As language models become integral to critical workflows, assessing theirbehavior remains a fundamental challenge -- human evaluation is costly andnoisy, while automated metrics provide only coarse, difficult-to-interpretsignals. We introduce natural language unit tests, a paradigm that decomposesresponse quality into explicit, testable criteria, along with a unified scoringmodel, LMUnit, which combines multi-objective training across preferences,direct ratings, and natural language rationales. Through controlled humanstudies, we show this paradigm significantly improves inter-annotator agreementand enables more effective LLM development workflows. LMUnit achievesstate-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) andcompetitive results on RewardBench. These results validate both our proposedparadigm and scoring model, suggesting a promising path forward for languagemodel evaluation and development.</description><author>Jon Saad-Falcon, Rajan Vivek, William Berrios, Nandita Shankar Naik, Matija Franklin, Bertie Vidgen, Amanpreet Singh, Douwe Kiela, Shikib Mehri</author><pubDate>Tue, 17 Dec 2024 17:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13091v1</guid></item><item><title>Prompt Augmentation for Self-supervised Text-guided Image Manipulation</title><link>http://arxiv.org/abs/2412.13081v1</link><description>Text-guided image editing finds applications in various creative andpractical fields. While recent studies in image generation have advanced thefield, they often struggle with the dual challenges of coherent imagetransformation and context preservation. In response, our work introducesprompt augmentation, a method amplifying a single input prompt into severaltarget prompts, strengthening textual context and enabling localised imageediting. Specifically, we use the augmented prompts to delineate the intendedmanipulation area. We propose a Contrastive Loss tailored to driving effectiveimage editing by displacing edited areas and drawing preserved regions closer.Acknowledging the continuous nature of image manipulations, we further refineour approach by incorporating the similarity concept, creating a SoftContrastive Loss. The new losses are incorporated to the diffusion model,demonstrating improved or competitive image editing results on public datasetsand generated images over state-of-the-art approaches.</description><author>Rumeysa Bodur, Binod Bhattarai, Tae-Kyun Kim</author><pubDate>Tue, 17 Dec 2024 16:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13081v1</guid></item><item><title>PersonaMark: Personalized LLM watermarking for model protection and user attribution</title><link>http://arxiv.org/abs/2409.09739v2</link><description>The rapid advancement of customized Large Language Models (LLMs) offersconsiderable convenience. However, it also intensifies concerns regarding theprotection of copyright/confidential information. With the extensive adoptionof private LLMs, safeguarding model copyright and ensuring data privacy havebecome critical. Text watermarking has emerged as a viable solution fordetecting AI-generated content and protecting models. However, existing methodsfall short in providing individualized watermarks for each user, a criticalfeature for enhancing accountability and traceability. In this paper, weintroduce PersonaMark, a novel personalized text watermarking scheme designedto protect LLMs' copyrights and bolster accountability. PersonaMark leveragessentence structure as a subtle carrier of watermark information and optimizesthe generation process to maintain the natural output of the model. Byemploying a personalized hashing function, unique watermarks are embedded foreach user, enabling high-quality text generation without compromising themodel's performance. This approach is both time-efficient and scalable, capableof handling large numbers of users through a multi-user hashing mechanism. Tothe best of our knowledge, this is a pioneer study to explore personalizedwatermarking in LLMs. We conduct extensive evaluations across four LLMs,analyzing various metrics such as perplexity, sentiment, alignment, andreadability. The results validate that PersonaMark preserves text quality,ensures unbiased watermark insertion, and offers robust watermark detectioncapabilities, all while maintaining the model's behavior with minimaldisruption.</description><author>Yuehan Zhang, Peizhuo Lv, Yinpeng Liu, Yongqiang Ma, Wei Lu, Xiaofeng Wang, Xiaozhong Liu, Jiawei Liu</author><pubDate>Tue, 17 Dec 2024 16:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09739v2</guid></item><item><title>Identifying Bias in Deep Neural Networks Using Image Transforms</title><link>http://arxiv.org/abs/2412.13079v1</link><description>CNNs have become one of the most commonly used computational tool in the pasttwo decades. One of the primary downsides of CNNs is that they work as a``black box", where the user cannot necessarily know how the image data areanalyzed, and therefore needs to rely on empirical evaluation to test theefficacy of a trained CNN. This can lead to hidden biases that affect theperformance evaluation of neural networks, but are difficult to identify. Herewe discuss examples of such hidden biases in common and widely used benchmarkdatasets, and propose techniques for identifying dataset biases that can affectthe standard performance evaluation metrics. One effective approach to identifydataset bias is to perform image classification by using merely blankbackground parts of the original images. However, in some situations a blankbackground in the images is not available, making it more difficult to separateforeground or contextual information from the bias. To overcome this, wepropose a method to identify dataset bias without the need to crop backgroundinformation from the images. That method is based on applying several imagetransforms to the original images, including Fourier transform, wavelettransforms, median filter, and their combinations. These transforms wereapplied to recover background bias information that CNNs use to classifyimages. This transformations affect the contextual visual information in adifferent manner than it affects the systemic background bias. Therefore, themethod can distinguish between contextual information and the bias, and alerton the presence of background bias even without the need to separate sub-imagesparts from the blank background of the original images. Code used in theexperiments is publicly available.</description><author>Sai Teja Erukude, Akhil Joshi, Lior Shamir</author><pubDate>Tue, 17 Dec 2024 16:51:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13079v1</guid></item><item><title>Speak &amp; Improve Challenge 2025: Tasks and Baseline Systems</title><link>http://arxiv.org/abs/2412.11985v2</link><description>This paper presents the "Speak &amp; Improve Challenge 2025: Spoken LanguageAssessment and Feedback" -- a challenge associated with the ISCA SLaTE 2025Workshop. The goal of the challenge is to advance research on spoken languageassessment and feedback, with tasks associated with both the underlyingtechnology and language learning feedback. Linked with the challenge, the Speak&amp; Improve (S&amp;I) Corpus 2025 is being pre-released, a dataset of L2 learnerEnglish data with holistic scores and language error annotation, collected fromopen (spontaneous) speaking tests on the Speak &amp; Improve learning platform. Thecorpus consists of approximately 315 hours of audio data from second languageEnglish learners with holistic scores, and a 55-hour subset with manualtranscriptions and error labels. The Challenge has four shared tasks: AutomaticSpeech Recognition (ASR), Spoken Language Assessment (SLA), Spoken GrammaticalError Correction (SGEC), and Spoken Grammatical Error Correction Feedback(SGECF). Each of these tasks has a closed track where a predetermined set ofmodels and data sources are allowed to be used, and an open track where anypublic resource may be used. Challenge participants may do one or more of thetasks. This paper describes the challenge, the S&amp;I Corpus 2025, and thebaseline systems released for the Challenge.</description><author>Mengjie Qian, Kate Knill, Stefano Banno, Siyuan Tang, Penny Karanasou, Mark J. F. Gales, Diane Nicholls</author><pubDate>Tue, 17 Dec 2024 16:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11985v2</guid></item><item><title>BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities</title><link>http://arxiv.org/abs/2410.14672v2</link><description>We introduce BiGR, a novel conditional image generation model using compactbinary latent codes for generative training, focusing on enhancing bothgeneration and representation capabilities. BiGR is the first conditionalgenerative model that unifies generation and discrimination within the sameframework. BiGR features a binary tokenizer, a masked modeling mechanism, and abinary transcoder for binary code prediction. Additionally, we introduce anovel entropy-ordered sampling method to enable efficient image generation.Extensive experiments validate BiGR's superior performance in generationquality, as measured by FID-50k, and representation capabilities, as evidencedby linear-probe accuracy. Moreover, BiGR showcases zero-shot generalizationacross various vision tasks, enabling applications such as image inpainting,outpainting, editing, interpolation, and enrichment, without the need forstructural modifications. Our findings suggest that BiGR unifies generative anddiscriminative tasks effectively, paving the way for further advancements inthe field. We further enable BiGR to perform text-to-image generation,showcasing its potential for broader applications.</description><author>Shaozhe Hao, Xuantong Liu, Xianbiao Qi, Shihao Zhao, Bojia Zi, Rong Xiao, Kai Han, Kwan-Yee K. Wong</author><pubDate>Tue, 17 Dec 2024 16:47:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14672v2</guid></item><item><title>Walk Wisely on Graph: Knowledge Graph Reasoning with Dual Agents via Efficient Guidance-Exploration</title><link>http://arxiv.org/abs/2408.01880v3</link><description>Recent years, multi-hop reasoning has been widely studied for knowledge graph(KG) reasoning due to its efficacy and interpretability. However, previousmulti-hop reasoning approaches are subject to two primary shortcomings. First,agents struggle to learn effective and robust policies at the early phase dueto sparse rewards. Second, these approaches often falter on specific datasetslike sparse knowledge graphs, where agents are required to traverse lengthyreasoning paths. To address these problems, we propose a multi-hop reasoningmodel with dual agents based on hierarchical reinforcement learning (HRL),which is named FULORA. FULORA tackles the above reasoning challenges byeFficient GUidance-ExpLORAtion between dual agents. The high-level agent walkson the simplified knowledge graph to provide stage-wise hints for the low-levelagent walking on the original knowledge graph. In this framework, the low-levelagent optimizes a value function that balances two objectives: (1) maximizingreturn, and (2) integrating efficient guidance from the high-level agent.Experiments conducted on three real-word knowledge graph datasets demonstratethat FULORA outperforms RL-based baselines, especially in the case oflong-distance reasoning.</description><author>Zijian Wang, Bin Wang, Haifeng Jing, Huayu Li, Hongbo Dou</author><pubDate>Tue, 17 Dec 2024 16:47:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01880v3</guid></item><item><title>Dual Interpretation of Machine Learning Forecasts</title><link>http://arxiv.org/abs/2412.13076v1</link><description>Machine learning predictions are typically interpreted as the sum ofcontributions of predictors. Yet, each out-of-sample prediction can also beexpressed as a linear combination of in-sample values of the predictedvariable, with weights corresponding to pairwise proximity scores betweencurrent and past economic events. While this dual route leads nowhere in somecontexts (e.g., large cross-sectional datasets), it provides sparserinterpretations in settings with many regressors and little training data-likemacroeconomic forecasting. In this case, the sequence of contributions can bevisualized as a time series, allowing analysts to explain predictions asquantifiable combinations of historical analogies. Moreover, the weights can beviewed as those of a data portfolio, inspiring new diagnostic measures such asforecast concentration, short position, and turnover. We show how weights canbe retrieved seamlessly for (kernel) ridge regression, random forest, boostedtrees, and neural networks. Then, we apply these tools to analyze post-pandemicforecasts of inflation, GDP growth, and recession probabilities. In all cases,the approach opens the black box from a new angle and demonstrates how machinelearning models leverage history partly repeating itself.</description><author>Philippe Goulet Coulombe, Maximilian Goebel, Karin Klieber</author><pubDate>Tue, 17 Dec 2024 16:44:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13076v1</guid></item><item><title>Rethinking the Alignment of Psychotherapy Dialogue Generation with Motivational Interviewing Strategies</title><link>http://arxiv.org/abs/2408.06527v2</link><description>Recent advancements in large language models (LLMs) have shown promise ingenerating psychotherapeutic dialogues, particularly in the context ofmotivational interviewing (MI). However, the inherent lack of transparency inLLM outputs presents significant challenges given the sensitive nature ofpsychotherapy. Applying MI strategies, a set of MI skills, to generate morecontrollable therapeutic-adherent conversations with explainability provides apossible solution. In this work, we explore the alignment of LLMs with MIstrategies by first prompting the LLMs to predict the appropriate strategies asreasoning and then utilizing these strategies to guide the subsequent dialoguegeneration. We seek to investigate whether such alignment leads to morecontrollable and explainable generations. Multiple experiments includingautomatic and human evaluations are conducted to validate the effectiveness ofMI strategies in aligning psychotherapy dialogue generation. Our findingsdemonstrate the potential of LLMs in producing strategically aligned dialoguesand suggest directions for practical applications in psychotherapeuticsettings.</description><author>Xin Sun, Xiao Tang, Abdallah El Ali, Zhuying Li, Pengjie Ren, Jan de Wit, Jiahuan Pei, Jos A. Bosch</author><pubDate>Tue, 17 Dec 2024 16:44:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06527v2</guid></item><item><title>Predicting Change, Not States: An Alternate Framework for Neural PDE Surrogates</title><link>http://arxiv.org/abs/2412.13074v1</link><description>Neural surrogates for partial differential equations (PDEs) have becomepopular due to their potential to quickly simulate physics. With a fewexceptions, neural surrogates generally treat the forward evolution oftime-dependent PDEs as a black box by directly predicting the next state. Whilethis is a natural and easy framework for applying neural surrogates, it can bean over-simplified and rigid framework for predicting physics. In this work, wepropose an alternative framework in which neural solvers predict the temporalderivative and an ODE integrator forwards the solution in time, which haslittle overhead and is broadly applicable across model architectures and PDEs.We find that by simply changing the training target and introducing numericalintegration during inference, neural surrogates can gain accuracy andstability. Predicting temporal derivatives also allows models to not beconstrained to a specific temporal discretization, allowing for flexibletime-stepping during inference or training on higher-resolution PDE data.Lastly, we investigate why this new framework can be beneficial and in whatsituations does it work well.</description><author>Anthony Zhou, Amir Barati Farimani</author><pubDate>Tue, 17 Dec 2024 16:41:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13074v1</guid></item><item><title>LossVal: Efficient Data Valuation for Neural Networks</title><link>http://arxiv.org/abs/2412.04158v2</link><description>Assessing the importance of individual training samples is a key challenge inmachine learning. Traditional approaches retrain models with and withoutspecific samples, which is computationally expensive and ignores dependenciesbetween data points. We introduce LossVal, an efficient data valuation methodthat computes importance scores during neural network training by embedding aself-weighting mechanism into loss functions like cross-entropy and meansquared error. LossVal reduces computational costs, making it suitable forlarge datasets and practical applications. Experiments on classification andregression tasks across multiple datasets show that LossVal effectivelyidentifies noisy samples and is able to distinguish helpful from harmfulsamples. We examine the gradient calculation of LossVal to highlight itsadvantages. The source code is available at:https://github.com/twibiral/LossVal</description><author>Tim Wibiral, Mohamed Karim Belaid, Maximilian Rabus, Ansgar Scherp</author><pubDate>Tue, 17 Dec 2024 16:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04158v2</guid></item><item><title>Speak &amp; Improve Corpus 2025: an L2 English Speech Corpus for Language Assessment and Feedback</title><link>http://arxiv.org/abs/2412.11986v2</link><description>We introduce the Speak &amp; Improve Corpus 2025, a dataset of L2 learner Englishdata with holistic scores and language error annotation, collected from open(spontaneous) speaking tests on the Speak &amp; Improve learning platform. The aimof the corpus release is to address a major challenge to developing L2 spokenlanguage processing systems, the lack of publicly available data withhigh-quality annotations. It is being made available for non-commercial use onthe ELiT website. In designing this corpus we have sought to make it cover awide-range of speaker attributes, from their L1 to their speaking ability, aswell as providing manual annotations. This enables a range of language-learningtasks to be examined, such as assessing speaking proficiency or providingfeedback on grammatical errors in a learner's speech. Additionally the datasupports research into the underlying technology required for these tasksincluding automatic speech recognition (ASR) of low resource L2 learnerEnglish, disfluency detection or spoken grammatical error correction (GEC). Thecorpus consists of around 315 hours of L2 English learners audio with holisticscores, and a subset of audio annotated with transcriptions and error labels.</description><author>Kate Knill, Diane Nicholls, Mark J. F. Gales, Mengjie Qian, Pawel Stroinski</author><pubDate>Tue, 17 Dec 2024 16:40:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11986v2</guid></item><item><title>CLASP: Contrastive Language-Speech Pretraining for Multilingual Multimodal Information Retrieval</title><link>http://arxiv.org/abs/2412.13071v1</link><description>This study introduces CLASP (Contrastive Language-Speech Pretraining), amultilingual, multimodal representation tailored for audio-text informationretrieval. CLASP leverages the synergy between spoken content and textual data.During training, we utilize our newly introduced speech-text dataset, whichencompasses 15 diverse categories ranging from fiction to religion. CLASP'saudio component integrates audio spectrograms with a pre-trainedself-supervised speech model, while its language encoding counterpart employs asentence encoder pre-trained on over 100 languages. This unified lightweightmodel bridges the gap between various modalities and languages, enhancing itseffectiveness in handling and retrieving multilingual and multimodal data. Ourevaluations across multiple languages demonstrate that CLASP establishes newbenchmarks in HITS@1, MRR, and meanR metrics, outperforming traditionalASR-based retrieval approaches in specific scenarios.</description><author>Mohammad Mahdi Abootorabi, Ehsaneddin Asgari</author><pubDate>Tue, 17 Dec 2024 16:38:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13071v1</guid></item><item><title>On the Readiness of Scientific Data for a Fair and Transparent Use in Machine Learning</title><link>http://arxiv.org/abs/2401.10304v2</link><description>To ensure the fairness and trustworthiness of machine learning (ML) systems,recent legislative initiatives and relevant research in the ML community havepointed out the need to document the data used to train ML models. Besides,data-sharing practices in many scientific domains have evolved in recent yearsfor reproducibility purposes. In this sense, academic institutions' adoption ofthese practices has encouraged researchers to publish their data and technicaldocumentation in peer-reviewed publications such as data papers. In this study,we analyze how this broader scientific data documentation meets the needs ofthe ML community and regulatory bodies for its use in ML technologies. Weexamine a sample of 4041 data papers of different domains, assessing theircompleteness, coverage of the requested dimensions, and trends in recent years.We focus on the most and least documented dimensions and compare the resultswith those of an ML-focused venue (NeurIPS D&amp;B track) publishing papersdescribing datasets. As a result, we propose a set of recommendation guidelinesfor data creators and scientific data publishers to increase their data'spreparedness for its transparent and fairer use in ML technologies.</description><author>Joan Giner-Miguelez, Abel Gómez, Jordi Cabot</author><pubDate>Tue, 17 Dec 2024 16:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10304v2</guid></item><item><title>Learning of Patch-Based Smooth-Plus-Sparse Models for Image Reconstruction</title><link>http://arxiv.org/abs/2412.13070v1</link><description>We aim at the solution of inverse problems in imaging, by combining apenalized sparse representation of image patches with an unconstrained smoothone. This allows for a straightforward interpretation of the reconstruction. Weformulate the optimization as a bilevel problem. The inner problem deploysclassical algorithms while the outer problem optimizes the dictionary and theregularizer parameters through supervised learning. The process is carried outvia implicit differentiation and gradient-based optimization. We evaluate ourmethod for denoising, super-resolution, and compressed-sensingmagnetic-resonance imaging. We compare it to other classical models as well asdeep-learning-based methods and show that it always outperforms the former andalso the latter in some instances.</description><author>Stanislas Ducotterd, Sebastian Neumayer, Michael Unser</author><pubDate>Tue, 17 Dec 2024 16:34:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13070v1</guid></item><item><title>MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning</title><link>http://arxiv.org/abs/2409.12059v3</link><description>Large Language Model can reasonably understand and generate human expressionsbut may lack of thorough thinking and reasoning mechanisms. Recently there havebeen several studies which enhance the thinking ability of language models butmost of them are not data-driven or training-based. In this paper, we aremotivated by the cognitive mechanism in the natural world, and design a novelmodel architecture called TaS which allows it to first consider the thoughtsand then express the response based upon the query. We design several pipelinesto annotate or generate the thought contents from prompt-response samples, thenadd language heads in a middle layer which behaves as the thinking layer. Wetrain the language model by the thoughts-augmented data and successfully letthe thinking layer automatically generate reasonable thoughts and finallyoutput more reasonable responses. Both qualitative examples and quantitativeresults validate the effectiveness and performance of TaS. Our code isavailable at https://anonymous.4open.science/r/TadE.</description><author>Ningyuan Xi, Xiaoyu Wang, Yetao Wu, Teng Chen, Qingqing Gu, Yue Zhao, Jinxian Qu, Zhonglin Jiang, Yong Chen, Luo Ji</author><pubDate>Tue, 17 Dec 2024 16:30:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12059v3</guid></item><item><title>Smartphone-based Iris Recognition through High-Quality Visible Spectrum Iris Capture</title><link>http://arxiv.org/abs/2412.13063v1</link><description>Iris recognition is widely acknowledged for its exceptional accuracy inbiometric authentication, traditionally relying on near-infrared (NIR) imaging.Recently, visible spectrum (VIS) imaging via accessible smartphone cameras hasbeen explored for biometric capture. However, a thorough study of irisrecognition using smartphone-captured 'High-Quality' VIS images andcross-spectral matching with previously enrolled NIR images has not beenconducted. The primary challenge lies in capturing high-quality biometrics, aknown limitation of smartphone cameras. This study introduces a novel Androidapplication designed to consistently capture high-quality VIS iris imagesthrough automated focus and zoom adjustments. The application integrates aYOLOv3-tiny model for precise eye and iris detection and a lightweightGhost-Attention U-Net (G-ATTU-Net) for segmentation, while adhering to ISO/IEC29794-6 standards for image quality. The approach was validated usingsmartphone-captured VIS and NIR iris images from 47 subjects, achieving a TrueAcceptance Rate (TAR) of 96.57% for VIS images and 97.95% for NIR images, withconsistent performance across various capture distances and iris colors. Thisrobust solution is expected to significantly advance the field of irisbiometrics, with important implications for enhancing smartphone security.</description><author>Naveenkumar G Venkataswamy, Yu Liu, Surendra Singh, Soumyabrata Dey, Stephanie Schuckers, Masudul H Imtiaz</author><pubDate>Tue, 17 Dec 2024 16:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13063v1</guid></item><item><title>Sparse Signature Coefficient Recovery via Kernels</title><link>http://arxiv.org/abs/2412.08579v2</link><description>Central to rough path theory is the signature transform of a path, aninfinite series of tensors given by the iterated integrals of the underlyingpath. The signature poses an effective way to capture sequentially orderedinformation, thanks both to its rich analytic and algebraic properties as wellas its universality when used as a basis to approximate functions on pathspace. Whilst a truncated version of the signature can be efficiently computedusing Chen's identity, there is a lack of efficient methods for computing asparse collection of iterated integrals contained in high levels of thesignature. We address this problem by leveraging signature kernels, defined asthe inner product of two signatures, and computable efficiently by means ofPDE-based methods. By forming a filter in signature space with which to takekernels, one can effectively isolate specific groups of signature coefficientsand, in particular, a singular coefficient at any depth of the transform. Weshow that such a filter can be expressed as a linear combination of suitablesignature transforms and demonstrate empirically the effectiveness of ourapproach. To conclude, we give an example use case for sparse collections ofsignature coefficients based on the construction of N-step Euler schemes forsparse CDEs.</description><author>Daniil Shmelev, Cristopher Salvi</author><pubDate>Tue, 17 Dec 2024 16:27:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08579v2</guid></item><item><title>On Distilling the Displacement Knowledge for Few-Shot Class-Incremental Learning</title><link>http://arxiv.org/abs/2412.11017v2</link><description>Few-shot Class-Incremental Learning (FSCIL) addresses the challenges ofevolving data distributions and the difficulty of data acquisition inreal-world scenarios. To counteract the catastrophic forgetting typicallyencountered in FSCIL, knowledge distillation is employed as a way to maintainthe knowledge from learned data distribution. Recognizing the limitations ofgenerating discriminative feature representations in a few-shot context, ourapproach incorporates structural information between samples into knowledgedistillation. This structural information serves as a remedy for the lowquality of features. Diverging from traditional structured distillation methodsthat compute sample similarity, we introduce the Displacement KnowledgeDistillation (DKD) method. DKD utilizes displacement rather than similaritybetween samples, incorporating both distance and angular information tosignificantly enhance the information density retained through knowledgedistillation. Observing performance disparities in feature distribution betweenbase and novel classes, we propose the Dual Distillation Network (DDNet). Thisnetwork applies traditional knowledge distillation to base classes and DKD tonovel classes, challenging the conventional integration of novel classes withbase classes. Additionally, we implement an instance-aware sample selectorduring inference to dynamically adjust dual branch weights, thereby leveragingthe complementary strengths of each approach. Extensive testing on threebenchmarks demonstrates that DDNet achieves state-of-the-art results. Moreover,through rigorous experimentation and comparison, we establish the robustnessand general applicability of our proposed DKD method.</description><author>Pengfei Fang, Yongchun Qin, Hui Xue</author><pubDate>Tue, 17 Dec 2024 16:27:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11017v2</guid></item><item><title>VidTok: A Versatile and Open-Source Video Tokenizer</title><link>http://arxiv.org/abs/2412.13061v1</link><description>Encoding video content into compact latent tokens has become a fundamentalstep in video generation and understanding, driven by the need to address theinherent redundancy in pixel-level representations. Consequently, there is agrowing demand for high-performance, open-source video tokenizers asvideo-centric research gains prominence. We introduce VidTok, a versatile videotokenizer that delivers state-of-the-art performance in both continuous anddiscrete tokenizations. VidTok incorporates several key advancements overexisting approaches: 1) model architecture such as convolutional layers andup/downsampling modules; 2) to address the training instability and codebookcollapse commonly associated with conventional Vector Quantization (VQ), weintegrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3)improved training strategies, including a two-stage training process and theuse of reduced frame rates. By integrating these advancements, VidTok achievessubstantial improvements over existing methods, demonstrating superiorperformance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD,under standardized evaluation settings.</description><author>Anni Tang, Tianyu He, Junliang Guo, Xinle Cheng, Li Song, Jiang Bian</author><pubDate>Tue, 17 Dec 2024 16:27:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13061v1</guid></item><item><title>3D MedDiffusion: A 3D Medical Diffusion Model for Controllable and High-quality Medical Image Generation</title><link>http://arxiv.org/abs/2412.13059v1</link><description>The generation of medical images presents significant challenges due to theirhigh-resolution and three-dimensional nature. Existing methods often yieldsuboptimal performance in generating high-quality 3D medical images, and thereis currently no universal generative framework for medical imaging. In thispaper, we introduce the 3D Medical Diffusion (3D MedDiffusion) model forcontrollable, high-quality 3D medical image generation. 3D MedDiffusionincorporates a novel, highly efficient Patch-Volume Autoencoder that compressesmedical images into latent space through patch-wise encoding and recovers backinto image space through volume-wise decoding. Additionally, we design a newnoise estimator to capture both local details and global structure informationduring diffusion denoising process. 3D MedDiffusion can generate fine-detailed,high-resolution images (up to 512x512x512) and effectively adapt to variousdownstream tasks as it is trained on large-scale datasets covering CT and MRImodalities and different anatomical regions (from head to leg). Experimentalresults demonstrate that 3D MedDiffusion surpasses state-of-the-art methods ingenerative quality and exhibits strong generalizability across tasks such assparse-view CT reconstruction, fast MRI reconstruction, and data augmentation.</description><author>Haoshen Wang, Zhentao Liu, Kaicong Sun, Xiaodong Wang, Dinggang Shen, Zhiming Cui</author><pubDate>Tue, 17 Dec 2024 16:25:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13059v1</guid></item><item><title>An Ad-hoc graph node vector embedding algorithm for general knowledge graphs using Kinetica-Graph</title><link>http://arxiv.org/abs/2407.15906v2</link><description>This paper discusses how to generate general graph node embeddings fromknowledge graph representations. The embedded space is composed of a number ofsub-features to mimic both local affinity and remote structural relevance.These sub-feature dimensions are defined by several indicators that wespeculate to catch nodal similarities, such as hop-based topological patterns,the number of overlapping labels, the transitional probabilities (markov-chainprobabilities), and the cluster indices computed by our recursive spectralbisection (RSB) algorithm. These measures are flattened over the onedimensional vector space into their respective sub-component ranges such thatthe entire set of vector similarity functions could be used for finding similarnodes. The error is defined by the sum of pairwise square differences across arandomly selected sample of graph nodes between the assumed embeddings and theground truth estimates as our novel loss function. The ground truth isestimated to be a combination of pairwise Jaccard similarity and the number ofoverlapping labels. Finally, we demonstrate a multi-variate stochastic gradientdescent (SGD) algorithm to compute the weighing factors among sub-vector spacesto minimize the average error using a random sampling logic.</description><author>B. Kaan Karamete, Eli Glaser</author><pubDate>Tue, 17 Dec 2024 16:25:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15906v2</guid></item><item><title>CondiMen: Conditional Multi-Person Mesh Recovery</title><link>http://arxiv.org/abs/2412.13058v1</link><description>Multi-person human mesh recovery (HMR) consists in detecting all individualsin a given input image, and predicting the body shape, pose, and 3D locationfor each detected person. The dominant approaches to this task rely on neuralnetworks trained to output a single prediction for each detected individual. Incontrast, we propose CondiMen, a method that outputs a joint parametricdistribution over likely poses, body shapes, intrinsics and distances to thecamera, using a Bayesian network. This approach offers several advantages.First, a probability distribution can handle some inherent ambiguities of thistask -- such as the uncertainty between a person's size and their distance tothe camera, or simply the loss of information when projecting 3D data onto the2D image plane. Second, the output distribution can be combined with additionalinformation to produce better predictions, by using e.g. known camera or bodyshape parameters, or by exploiting multi-view observations. Third, one canefficiently extract the most likely predictions from the output distribution,making our proposed approach suitable for real-time applications. Empiricallywe find that our model i) achieves performance on par with or better than thestate-of-the-art, ii) captures uncertainties and correlations inherent in poseestimation and iii) can exploit additional information at test time, such asmulti-view consistency or body shape priors. CondiMen spices up the modeling ofambiguity, using just the right ingredients on hand.</description><author>Brégier Romain, Baradel Fabien, Lucas Thomas, Galaaoui Salma, Armando Matthieu, Weinzaepfel Philippe, Rogez Grégory</author><pubDate>Tue, 17 Dec 2024 16:22:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13058v1</guid></item><item><title>SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree</title><link>http://arxiv.org/abs/2410.16268v2</link><description>The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundationmodel for object segmentation in both images and videos, paving the way forvarious downstream video applications. The crucial design of SAM 2 for videosegmentation is its memory module, which prompts object-aware memories fromprevious frames for current frame prediction. However, its greedy-selectionmemory design suffers from the "error accumulation" problem, where an erroredor missed mask will cascade and influence the segmentation of the subsequentframes, which limits the performance of SAM 2 toward complex long-term videos.To this end, we introduce SAM2Long, an improved training-free video objectsegmentation strategy, which considers the segmentation uncertainty within eachframe and chooses the video-level optimal results from multiple segmentationpathways in a constrained tree search manner. In practice, we maintain a fixednumber of segmentation pathways throughout the video. For each frame, multiplemasks are proposed based on the existing pathways, creating various candidatebranches. We then select the same fixed number of branches with highercumulative scores as the new pathways for the next frame. After processing thefinal frame, the pathway with the highest cumulative score is chosen as thefinal segmentation result. Benefiting from its heuristic search design,SAM2Long is robust toward occlusions and object reappearances, and caneffectively segment and track objects for complex long-term videos. Notably,SAM2Long achieves an average improvement of 3.0 points across all 24head-to-head comparisons, with gains of up to 5.3 points in J&amp;F on long-termvideo object segmentation benchmarks such as SA-V and LVOS. The code isreleased at https://github.com/Mark12Ding/SAM2Long.</description><author>Shuangrui Ding, Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Yuwei Guo, Dahua Lin, Jiaqi Wang</author><pubDate>Tue, 17 Dec 2024 16:22:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16268v2</guid></item><item><title>FunEditor: Achieving Complex Image Edits via Function Aggregation with Diffusion Models</title><link>http://arxiv.org/abs/2408.08495v2</link><description>Diffusion models have demonstrated outstanding performance in generativetasks, making them ideal candidates for image editing. Recent studies highlighttheir ability to apply desired edits effectively by following textualinstructions, yet with two key challenges remaining. First, these modelsstruggle to apply multiple edits simultaneously, resulting in computationalinefficiencies due to their reliance on sequential processing. Second, relyingon textual prompts to determine the editing region can lead to unintendedalterations to the image. We introduce FunEditor, an efficient diffusion modeldesigned to learn atomic editing functions and perform complex edits byaggregating simpler functions. This approach enables complex editing tasks,such as object movement, by aggregating multiple functions and applying themsimultaneously to specific areas. Our experiments demonstrate that FunEditorsignificantly outperforms recent inference-time optimization methods andfine-tuned models, either quantitatively across various metrics or throughvisual comparisons or both, on complex tasks like object movement and objectpasting. In the meantime, with only 4 steps of inference, FunEditor achieves5-24x inference speedups over existing popular methods. The code is availableat: mhmdsmdi.github.io/funeditor/.</description><author>Mohammadreza Samadi, Fred X. Han, Mohammad Salameh, Hao Wu, Fengyu Sun, Chunhua Zhou, Di Niu</author><pubDate>Tue, 17 Dec 2024 16:21:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08495v2</guid></item><item><title>On the Hardness of Training Deep Neural Networks Discretely</title><link>http://arxiv.org/abs/2412.13057v1</link><description>We study neural network training (NNT): optimizing a neural network'sparameters to minimize the training loss over a given dataset. NNT has beenstudied extensively under theoretic lenses, mainly on two-layer networks withlinear or ReLU activation functions where the parameters can take any realvalue (here referred to as continuous NNT (C-NNT)). However, less is knownabout deeper neural networks, which exhibit substantially stronger capabilitiesin practice. In addition, the complexity of the discrete variant of the problem(D-NNT in short), in which the parameters are taken from a given finite set ofoptions, has remained less explored despite its theoretical and practicalsignificance. In this work, we show that the hardness of NNT is dramatically affected bythe network depth. Specifically, we show that, under standard complexityassumptions, D-NNT is not in the complexity class NP even for instances withfixed dimensions and dataset size, having a deep architecture. This separatesD-NNT from any NP-complete problem. Furthermore, using a polynomial reductionwe show that the above result also holds for C-NNT, albeit with more structuredinstances. We complement these results with a comprehensive list of NP-hardnesslower bounds for D-NNT on two-layer networks, showing that fixing the number ofdimensions, the dataset size, or the number of neurons in the hidden layerleaves the problem challenging. Finally, we obtain a pseudo-polynomialalgorithm for D-NNT on a two-layer network with a fixed dataset size.</description><author>Ilan Doron-Arad</author><pubDate>Tue, 17 Dec 2024 16:20:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13057v1</guid></item><item><title>SMOSE: Sparse Mixture of Shallow Experts for Interpretable Reinforcement Learning in Continuous Control Tasks</title><link>http://arxiv.org/abs/2412.13053v1</link><description>Continuous control tasks often involve high-dimensional, dynamic, andnon-linear environments. State-of-the-art performance in these tasks isachieved through complex closed-box policies that are effective, but sufferfrom an inherent opacity. Interpretable policies, while generallyunderperforming compared to their closed-box counterparts, advantageouslyfacilitate transparent decision-making within automated systems. Hence, theirusage is often essential for diagnosing and mitigating errors, supportingethical and legal accountability, and fostering trust among stakeholders. Inthis paper, we propose SMOSE, a novel method to train sparsely activatedinterpretable controllers, based on a top-1 Mixture-of-Experts architecture.SMOSE combines a set of interpretable decisionmakers, trained to be experts indifferent basic skills, and an interpretable router that assigns tasks amongthe experts. The training is carried out via state-of-the-art ReinforcementLearning algorithms, exploiting load-balancing techniques to ensure fair expertusage. We then distill decision trees from the weights of the router,significantly improving the ease of interpretation. We evaluate SMOSE on sixbenchmark environments from MuJoCo: our method outperforms recent interpretablebaselines and narrows the gap with noninterpretable state-of-the-art algorithms</description><author>Mátyás Vincze, Laura Ferrarotti, Leonardo Lucio Custode, Bruno Lepri, Giovanni Iacca</author><pubDate>Tue, 17 Dec 2024 16:15:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13053v1</guid></item><item><title>Modality-Inconsistent Continual Learning of Multimodal Large Language Models</title><link>http://arxiv.org/abs/2412.13050v1</link><description>In this paper, we introduce Modality-Inconsistent Continual Learning (MICL),a new continual learning scenario for Multimodal Large Language Models (MLLMs)that involves tasks with inconsistent modalities (image, audio, or video) andvarying task types (captioning or question-answering). Unlike existingvision-only or modality-incremental settings, MICL combines modality and tasktype shifts, both of which drive catastrophic forgetting. To address thesechallenges, we propose MoInCL, which employs a Pseudo Targets Generation Moduleto mitigate forgetting caused by task type shifts in previously seenmodalities. It also incorporates Instruction-based Knowledge Distillation topreserve the model's ability to handle previously learned modalities when newones are introduced. We benchmark MICL using a total of six tasks and conductexperiments to validate the effectiveness of our proposed MoInCL. Theexperimental results highlight the superiority of MoInCL, showing significantimprovements over representative and state-of-the-art continual learningbaselines.</description><author>Weiguo Pian, Shijian Deng, Shentong Mo, Yunhui Guo, Yapeng Tian</author><pubDate>Tue, 17 Dec 2024 16:13:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13050v1</guid></item><item><title>TIMESAFE: Timing Interruption Monitoring and Security Assessment for Fronthaul Environments</title><link>http://arxiv.org/abs/2412.13049v1</link><description>5G and beyond cellular systems embrace the disaggregation of Radio AccessNetwork (RAN) components, exemplified by the evolution of the fronthual (FH)connection between cellular baseband and radio unit equipment. Crucially,synchronization over the FH is pivotal for reliable 5G services. In recentyears, there has been a push to move these links to an Ethernet-based packetnetwork topology, leveraging existing standards and ongoing research forTime-Sensitive Networking (TSN). However, TSN standards, such as Precision TimeProtocol (PTP), focus on performance with little to no concern for security.This increases the exposure of the open FH to security risks. Attacks targetingsynchronization mechanisms pose significant threats, potentially disrupting 5Gnetworks and impairing connectivity. In this paper, we demonstrate the impact of successful spoofing and replayattacks against PTP synchronization. We show how a spoofing attack is able tocause a production-ready O-RAN and 5G-compliant private cellular base stationto catastrophically fail within 2 seconds of the attack, necessitating manualintervention to restore full network operations. To counter this, we design aMachine Learning (ML)-based monitoring solution capable of detecting variousmalicious attacks with over 97.5% accuracy.</description><author>Joshua Groen, Simone Di Valerio, Imtiaz Karim, Davide Villa, Yiewi Zhang, Leonardo Bonati, Michele Polese, Salvatore D'Oro, Tommaso Melodia, Elisa Bertino, Francesca Cuomo, Kaushik Chowdhury</author><pubDate>Tue, 17 Dec 2024 16:13:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13049v1</guid></item><item><title>SVGBuilder: Component-Based Colored SVG Generation with Text-Guided Autoregressive Transformers</title><link>http://arxiv.org/abs/2412.10488v2</link><description>Scalable Vector Graphics (SVG) are essential XML-based formats for versatilegraphics, offering resolution independence and scalability. Unlike rasterimages, SVGs use geometric shapes and support interactivity, animation, andmanipulation via CSS and JavaScript. Current SVG generation methods facechallenges related to high computational costs and complexity. In contrast,human designers use component-based tools for efficient SVG creation. Inspiredby this, SVGBuilder introduces a component-based, autoregressive model forgenerating high-quality colored SVGs from textual input. It significantlyreduces computational overhead and improves efficiency compared to traditionalmethods. Our model generates SVGs up to 604 times faster thanoptimization-based approaches. To address the limitations of existing SVGdatasets and support our research, we introduce ColorSVG-100K, the firstlarge-scale dataset of colored SVGs, comprising 100,000 graphics. This datasetfills the gap in color information for SVG generation models and enhancesdiversity in model training. Evaluation against state-of-the-art modelsdemonstrates SVGBuilder's superior performance in practical applications,highlighting its efficiency and quality in generating complex SVG graphics.</description><author>Zehao Chen, Rong Pan</author><pubDate>Tue, 17 Dec 2024 16:13:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10488v2</guid></item><item><title>EOGS: Gaussian Splatting for Earth Observation</title><link>http://arxiv.org/abs/2412.13047v1</link><description>Recently, Gaussian splatting has emerged as a strong alternative to NeRF,demonstrating impressive 3D modeling capabilities while requiring only afraction of the training and rendering time. In this paper, we show how thestandard Gaussian splatting framework can be adapted for remote sensing,retaining its high efficiency. This enables us to achieve state-of-the-artperformance in just a few minutes, compared to the day-long optimizationrequired by the best-performing NeRF-based Earth observation methods. Theproposed framework incorporates remote-sensing improvements from EO-NeRF, suchas radiometric correction and shadow modeling, while introducing novelcomponents, including sparsity, view consistency, and opacity regularizations.</description><author>Luca Savant Aira, Gabriele Facciolo, Thibaud Ehret</author><pubDate>Tue, 17 Dec 2024 16:11:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13047v1</guid></item><item><title>Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment</title><link>http://arxiv.org/abs/2403.02738v3</link><description>Despite the notable advancements of existing prompting methods, such asIn-Context Learning and Chain-of-Thought for Large Language Models (LLMs), theystill face challenges related to various biases. Traditional debiasing methodsprimarily focus on the model training stage, including approaches based on dataaugmentation and reweighting, yet they struggle with the complex biasesinherent in LLMs. To address such limitations, the causal relationship behindthe prompting methods is uncovered using a structural causal model, and a novelcausal prompting method based on front-door adjustment is proposed toeffectively mitigate LLMs biases. In specific, causal intervention is achievedby designing the prompts without accessing the parameters and logits of LLMs.The chain-of-thought generated by LLM is employed as the mediator variable andthe causal effect between input prompts and output answers is calculatedthrough front-door adjustment to mitigate model biases. Moreover, to accuratelyrepresent the chain-of-thoughts and estimate the causal effects, contrastivelearning is used to fine-tune the encoder of chain-of-thought by aligning itsspace with that of the LLM. Experimental results show that the proposed causalprompting approach achieves excellent performance across seven natural languageprocessing datasets on both open-source and closed-source LLMs.</description><author>Congzhi Zhang, Linhai Zhang, Jialong Wu, Yulan He, Deyu Zhou</author><pubDate>Tue, 17 Dec 2024 16:10:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02738v3</guid></item><item><title>Harnessing Event Sensory Data for Error Pattern Prediction in Vehicles: A Language Model Approach</title><link>http://arxiv.org/abs/2412.13041v1</link><description>In this paper, we draw an analogy between processing natural languages andprocessing multivariate event streams from vehicles in order to predict$\textit{when}$ and $\textit{what}$ error pattern is most likely to occur inthe future for a given car. Our approach leverages the temporal dynamics andcontextual relationships of our event data from a fleet of cars. Event data iscomposed of discrete values of error codes as well as continuous values such astime and mileage. Modelled by two causal Transformers, we can anticipatevehicle failures and malfunctions before they happen. Thus, we introduce$\textit{CarFormer}$, a Transformer model trained via a new self-supervisedlearning strategy, and $\textit{EPredictor}$, an autoregressive Transformerdecoder model capable of predicting $\textit{when}$ and $\textit{what}$ errorpattern will most likely occur after some error code apparition. Despite thechallenges of high cardinality of event types, their unbalanced frequency ofappearance and limited labelled data, our experimental results demonstrate theexcellent predictive ability of our novel model. Specifically, with sequencesof $160$ error codes on average, our model is able with only half of the errorcodes to achieve $80\%$ F1 score for predicting $\textit{what}$ error patternwill occur and achieves an average absolute error of $58.4 \pm 13.2$h$\textit{when}$ forecasting the time of occurrence, thus enabling confidentpredictive maintenance and enhancing vehicle safety.</description><author>Hugo Math, Rainer Lienhart, Robin Schön</author><pubDate>Tue, 17 Dec 2024 16:05:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13041v1</guid></item><item><title>CNNSum: Exploring Long-Context Summarization with Large Language Models in Chinese Novels</title><link>http://arxiv.org/abs/2412.02819v4</link><description>Large Language Models (LLMs) have been well-researched in variouslong-context tasks. However, the scarcity of high-quality long-contextsummarization datasets has hindered further advancements in this area. Toaddress this, we introduce CNNSum, a multi-scale long-context summarizationbenchmark based on Chinese novels, featuring human-driven annotations, whichcomprises four subsets totaling 695 samples, with lengths ranging from 16k to128k. We evaluate numerous LLMs and conduct detailed case analyses.Furthermore, we conduct extensive fine-tuning experiments to explore andimprove long-context summarization. In our study: (1) Advanced LLMs like GPT-4omay still generate subjective commentary, leading to vague summaries. (2)Currently, long-context summarization mainly relies on memory ability affordedby longer context lengths. The advantages of Large LLMs are hard to utilize,thus small LLMs are the most cost-effective. (3) Different prompt templatespaired with various version models may cause large performance gaps. In furtherfine-tuning, these can be mitigated, and the Base version models performbetter. (4) LLMs with RoPE-base scaled exhibit strong extrapolation potential;using short-context data can significantly improve long-context summarizationperformance. However, further applying other interpolation methods requirescareful selection. (5) CNNSum provides more reliable and insightful evaluationresults than other benchmarks. We release CNNSum to advance future research inthis field. https://github.com/CxsGhost/CNNSum</description><author>Lingxiao Wei, He Yan, Xiangju Lu, Junmin Zhu, Jun Wang, Wei Zhang</author><pubDate>Tue, 17 Dec 2024 16:03:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02819v4</guid></item><item><title>Open-Set Heterogeneous Domain Adaptation: Theoretical Analysis and Algorithm</title><link>http://arxiv.org/abs/2412.13036v1</link><description>Domain adaptation (DA) tackles the issue of distribution shift by learning amodel from a source domain that generalizes to a target domain. However, mostexisting DA methods are designed for scenarios where the source and targetdomain data lie within the same feature space, which limits their applicabilityin real-world situations. Recently, heterogeneous DA (HeDA) methods have beenintroduced to address the challenges posed by heterogeneous feature spacebetween source and target domains. Despite their successes, current HeDAtechniques fall short when there is a mismatch in both feature and labelspaces. To address this, this paper explores a new DA scenario called open-setHeDA (OSHeDA). In OSHeDA, the model must not only handle heterogeneity infeature space but also identify samples belonging to novel classes. To tacklethis challenge, we first develop a novel theoretical framework that constructslearning bounds for prediction error on target domain. Guided by thisframework, we propose a new DA method called Representation Learning for OSHeDA(RL-OSHeDA). This method is designed to simultaneously transfer knowledgebetween heterogeneous data sources and identify novel classes. Experimentsacross text, image, and clinical data demonstrate the effectiveness of ouralgorithm. Model implementation is available at\url{https://github.com/pth1993/OSHeDA}.</description><author>Thai-Hoang Pham, Yuanlong Wang, Changchang Yin, Xueru Zhang, Ping Zhang</author><pubDate>Tue, 17 Dec 2024 15:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13036v1</guid></item><item><title>Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2310.20587v5</link><description>Offline reinforcement learning (RL) aims to find a near-optimal policy usingpre-collected datasets. In real-world scenarios, data collection could becostly and risky; therefore, offline RL becomes particularly challenging whenthe in-domain data is limited. Given recent advances in Large Language Models(LLMs) and their few-shot learning prowess, this paper introduces$\textbf{La}$nguage Models for $\textbf{Mo}$tion Control ($\textbf{LaMo}$), ageneral framework based on Decision Transformers to effectively use pre-trainedLanguage Models (LMs) for offline RL. Our framework highlights four crucialcomponents: (1) Initializing Decision Transformers with sequentiallypre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast tofull-weight fine-tuning, to combine the pre-trained knowledge from LMs andin-domain knowledge effectively, (3) using the non-linear MLP transformationinstead of linear projections, to generate embeddings, and (4) integrating anauxiliary language prediction loss during fine-tuning to stabilize the LMs andretain their original abilities on languages. Empirical results indicate$\textbf{LaMo}$ achieves excellent performance in sparse-reward tasks andcloses the gap between value-based offline RL methods and decision transformersin dense-reward tasks. In particular, our method demonstrates superiorperformance in scenarios with limited data samples.</description><author>Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S. Du, Huazhe Xu</author><pubDate>Tue, 17 Dec 2024 15:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.20587v5</guid></item><item><title>WHAT-IF: Exploring Branching Narratives by Meta-Prompting Large Language Models</title><link>http://arxiv.org/abs/2412.10582v2</link><description>WHAT-IF -- Writing a Hero's Alternate Timeline through Interactive Fiction --is a system that uses zero-shot meta-prompting to create branching narrativesfrom a prewritten story. Played as an interactive fiction (IF) game, WHAT-IFlets the player choose between decisions that the large language model (LLM)GPT-4 generates as possible branches in the story. Starting with an existinglinear plot as input, a branch is created at each key decision taken by themain character. By meta-prompting the LLM to consider the major plot pointsfrom the story, the system produces coherent and well-structured alternatestorylines. WHAT-IF stores the branching plot tree in a graph which helps it toboth keep track of the story for prompting and maintain the structure for thefinal IF system. A video demo of our system can be found here:https://youtu.be/8vBqjqtupcc.</description><author>Runsheng "Anson" Huang, Lara J. Martin, Chris Callison-Burch</author><pubDate>Tue, 17 Dec 2024 15:56:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10582v2</guid></item><item><title>Benchmarking Embedding Aggregation Methods in Computational Pathology: A Clinical Data Perspective</title><link>http://arxiv.org/abs/2407.07841v2</link><description>Recent advances in artificial intelligence (AI), in particularself-supervised learning of foundation models (FMs), are revolutionizingmedical imaging and computational pathology (CPath). A constant challenge inthe analysis of digital Whole Slide Images (WSIs) is the problem of aggregatingtens of thousands of tile-level image embeddings to a slide-levelrepresentation. Due to the prevalent use of datasets created for genomicresearch, such as TCGA, for method development, the performance of thesetechniques on diagnostic slides from clinical practice has been inadequatelyexplored. This study conducts a thorough benchmarking analysis of tenslide-level aggregation techniques across nine clinically relevant tasks,including diagnostic assessment, biomarker classification, and outcomeprediction. The results yield following key insights: (1) Embeddings derivedfrom domain-specific (histological images) FMs outperform those from genericImageNet-based models across aggregation methods. (2) Spatial-aware aggregatorsenhance the performance significantly when using ImageNet pre-trained modelsbut not when using FMs. (3) No single model excels in all tasks andspatially-aware models do not show general superiority as it would be expected.These findings underscore the need for more adaptable and universallyapplicable aggregation techniques, guiding future research towards tools thatbetter meet the evolving needs of clinical-AI in pathology. The code used inthis work is available at\url{https://github.com/fuchs-lab-public/CPath_SABenchmark}.</description><author>Shengjia Chen, Gabriele Campanella, Abdulkadir Elmas, Aryeh Stock, Jennifer Zeng, Alexandros D. Polydorides, Adam J. Schoenfeld, Kuan-lin Huang, Jane Houldsworth, Chad Vanderbilt, Thomas J. Fuchs</author><pubDate>Tue, 17 Dec 2024 15:54:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07841v2</guid></item></channel></rss>