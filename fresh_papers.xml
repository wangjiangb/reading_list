<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 28 Jul 2023 06:00:17 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation</title><link>http://arxiv.org/abs/2307.15063v1</link><description>The goal of Online Domain Adaptation for semantic segmentation is to handleunforeseeable domain changes that occur during deployment, like sudden weatherevents. However, the high computational costs associated with brute-forceadaptation make this paradigm unfeasible for real-world applications. In thispaper we propose HAMLET, a Hardware-Aware Modular Least Expensive Trainingframework for real-time domain adaptation. Our approach includes ahardware-aware back-propagation orchestration agent (HAMT) and a dedicateddomain-shift detector that enables active control over when and how the modelis adapted (LT). Thanks to these advancements, our approach is capable ofperforming semantic segmentation while simultaneously adapting at more than29FPS on a single consumer-grade GPU. Our framework's encouraging accuracy andspeed trade-off is demonstrated on OnDA and SHIFT benchmarks throughexperimental results.</description><author>Marc Botet Colomer, Pier Luigi Dovesi, Theodoros Panagiotakopoulos, Joao Frederico Carvalho, Linus Härenstam-Nielsen, Hossein Azizpour, Hedvig Kjellström, Daniel Cremers, Matteo Poggi</author><pubDate>Thu, 27 Jul 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15063v1</guid></item><item><title>Self-Supervised Visual Acoustic Matching</title><link>http://arxiv.org/abs/2307.15064v1</link><description>Acoustic matching aims to re-synthesize an audio clip to sound as if it wererecorded in a target acoustic environment. Existing methods assume access topaired training data, where the audio is observed in both source and targetenvironments, but this limits the diversity of training data or requires theuse of simulated data or heuristics to create paired samples. We propose aself-supervised approach to visual acoustic matching where training samplesinclude only the target scene image and audio -- without acousticallymismatched source audio for reference. Our approach jointly learns todisentangle room acoustics and re-synthesize audio into the target environment,via a conditional GAN framework and a novel metric that quantifies the level ofresidual acoustic information in the de-biased audio. Training with eitherin-the-wild web data or simulated data, we demonstrate it outperforms thestate-of-the-art on multiple challenging datasets and a wide variety ofreal-world audio and environments.</description><author>Arjun Somayazulu, Changan Chen, Kristen Grauman</author><pubDate>Thu, 27 Jul 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15064v1</guid></item><item><title>The RoboDepth Challenge: Methods and Advancements Towards Robust Depth Estimation</title><link>http://arxiv.org/abs/2307.15061v1</link><description>Accurate depth estimation under out-of-distribution (OoD) scenarios, such asadverse weather conditions, sensor failure, and noise contamination, isdesirable for safety-critical applications. Existing depth estimation systems,however, suffer inevitably from real-world corruptions and perturbations andare struggled to provide reliable depth predictions under such cases. In thispaper, we summarize the winning solutions from the RoboDepth Challenge -- anacademic competition designed to facilitate and advance robust OoD depthestimation. This challenge was developed based on the newly established KITTI-Cand NYUDepth2-C benchmarks. We hosted two stand-alone tracks, with an emphasison robust self-supervised and robust fully-supervised depth estimation,respectively. Out of more than two hundred participants, nine unique andtop-performing solutions have appeared, with novel designs ranging from thefollowing aspects: spatial- and frequency-domain augmentations, masked imagemodeling, image restoration and super-resolution, adversarial training,diffusion-based noise suppression, vision-language pre-training, learned modelensembling, and hierarchical feature enhancement. Extensive experimentalanalyses along with insightful observations are drawn to better understand therationale behind each design. We hope this challenge could lay a solidfoundation for future research on robust and reliable depth estimation andbeyond. The datasets, competition toolkit, workshop recordings, and source codefrom the winning teams are publicly available on the challenge website.</description><author>Lingdong Kong, Yaru Niu, Shaoyuan Xie, Hanjiang Hu, Lai Xing Ng, Benoit R. Cottereau, Ding Zhao, Liangjun Zhang, Hesheng Wang, Wei Tsang Ooi, Ruijie Zhu, Ziyang Song, Li Liu, Tianzhu Zhang, Jun Yu, Mohan Jing, Pengwei Li, Xiaohua Qi, Cheng Jin, Yingfeng Chen, Jie Hou, Jie Zhang, Zhen Kan, Qiang Ling, Liang Peng, Minglei Li, Di Xu, Changpeng Yang, Yuanqi Yao, Gang Wu, Jian Kuai, Xianming Liu, Junjun Jiang, Jiamian Huang, Baojun Li, Jiale Chen, Shuang Zhang, Sun Ao, Zhenyu Li, Runze Chen, Haiyong Luo, Fang Zhao, Jingze Yu</author><pubDate>Thu, 27 Jul 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15061v1</guid></item><item><title>MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous Driving</title><link>http://arxiv.org/abs/2307.15058v1</link><description>Nowadays, autonomous cars can drive smoothly in ordinary cases, and it iswidely recognized that realistic sensor simulation will play a critical role insolving remaining corner cases by simulating them. To this end, we propose anautonomous driving simulator based upon neural radiance fields (NeRFs).Compared with existing works, ours has three notable features: (1)Instance-aware. Our simulator models the foreground instances and backgroundenvironments separately with independent networks so that the static (e.g.,size and appearance) and dynamic (e.g., trajectory) properties of instances canbe controlled separately. (2) Modular. Our simulator allows flexible switchingbetween different modern NeRF-related backbones, sampling strategies, inputmodalities, etc. We expect this modular design to boost academic progress andindustrial deployment of NeRF-based autonomous driving simulation. (3)Realistic. Our simulator set new state-of-the-art photo-realism results giventhe best module selection. Our simulator will be open-sourced while most of ourcounterparts are not. Project page: https://open-air-sun.github.io/mars/.</description><author>Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng Chen, Hongmin Xiao, Chao Hou, Haozhe Lou, Yuantao Chen, Runyi Yang, Yuxin Huang, Xiaoyu Ye, Zike Yan, Yongliang Shi, Yiyi Liao, Hao Zhao</author><pubDate>Thu, 27 Jul 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15058v1</guid></item><item><title>PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking</title><link>http://arxiv.org/abs/2307.15055v1</link><description>We introduce PointOdyssey, a large-scale synthetic dataset, and datageneration framework, for the training and evaluation of long-term fine-grainedtracking algorithms. Our goal is to advance the state-of-the-art by placingemphasis on long videos with naturalistic motion. Toward the goal ofnaturalism, we animate deformable characters using real-world motion capturedata, we build 3D scenes to match the motion capture environments, and werender camera viewpoints using trajectories mined via structure-from-motion onreal videos. We create combinatorial diversity by randomizing characterappearance, motion profiles, materials, lighting, 3D assets, and atmosphericeffects. Our dataset currently includes 104 videos, averaging 2,000 frameslong, with orders of magnitude more correspondence annotations than prior work.We show that existing methods can be trained from scratch in our dataset andoutperform the published variants. Finally, we introduce modifications to thePIPs point tracking method, greatly widening its temporal receptive field,which improves its performance on PointOdyssey as well as on two real-worldbenchmarks. Our data and code are publicly available at:https://pointodyssey.com</description><author>Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wetzstein, Leonidas J. Guibas</author><pubDate>Thu, 27 Jul 2023 18:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15055v1</guid></item><item><title>A Geometric Notion of Causal Probing</title><link>http://arxiv.org/abs/2307.15054v1</link><description>Large language models rely on real-valued representations of text to maketheir predictions. These representations contain information learned from thedata that the model has trained on, including knowledge of linguisticproperties and forms of demographic bias, e.g., based on gender. A growing bodyof work has considered information about concepts such as these usingorthogonal projections onto subspaces of the representation space. Wecontribute to this body of work by proposing a formal definition of intrinsicinformation in a subspace of a language model's representation space. Wepropose a counterfactual approach that avoids the failure mode of spuriouscorrelations (Kumar et al., 2022) by treating components in the subspace andits orthogonal complement independently. We show that our counterfactual notionof information in a subspace is optimizing by an causal concept subspace.Furthermore, this intervention allows us to attempt concept controlledgeneration by manipulating the value of the conceptual component of arepresentation. Empirically, we find that R-LACE (Ravfogel et al., 2022)returns a one-dimensional subspace containing roughly half of total conceptinformation under our framework. Our causal controlled intervention shows that,for at least one model, the subspace returned by R-LACE can be used tomanipulate the concept value of the generated word with precision.</description><author>Clément Guerner, Anej Svete, Tianyu Liu, Alexander Warstadt, Ryan Cotterell</author><pubDate>Thu, 27 Jul 2023 18:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15054v1</guid></item><item><title>On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation</title><link>http://arxiv.org/abs/2307.15053v1</link><description>Approaches to recommendation are typically evaluated in one of two ways: (1)via a (simulated) online experiment, often seen as the gold standard, or (2)via some offline evaluation procedure, where the goal is to approximate theoutcome of an online experiment. Several offline evaluation metrics have beenadopted in the literature, inspired by ranking metrics prevalent in the fieldof Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is onesuch metric that has seen widespread adoption in empirical studies, and higher(n)DCG values have been used to present new methods as the state-of-the-art intop-$n$ recommendation for many years. Our work takes a critical look at this approach, and investigates when we canexpect such metrics to approximate the gold standard outcome of an onlineexperiment. We formally present the assumptions that are necessary to considerDCG an unbiased estimator of online reward and provide a derivation for thismetric from first principles, highlighting where we deviate from itstraditional uses in IR. Importantly, we show that normalising the metricrenders it inconsistent, in that even when DCG is unbiased, ranking competingmethods by their normalised DCG can invert their relative order. Through acorrelation analysis between off- and on-line experiments conducted on alarge-scale recommendation platform, we show that our unbiased DCG estimatesstrongly correlate with online reward, even when some of the metric's inherentassumptions are violated. This statement no longer holds for its normalisedvariant, suggesting that nDCG's practical utility may be limited.</description><author>Olivier Jeunen, Ivan Potapov, Aleksei Ustimenko</author><pubDate>Thu, 27 Jul 2023 18:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15053v1</guid></item><item><title>Learning Depth Estimation for Transparent and Mirror Surfaces</title><link>http://arxiv.org/abs/2307.15052v1</link><description>Inferring the depth of transparent or mirror (ToM) surfaces represents a hardchallenge for either sensors, algorithms, or deep networks. We propose a simplepipeline for learning to estimate depth properly for such surfaces with neuralnetworks, without requiring any ground-truth annotation. We unveil how toobtain reliable pseudo labels by in-painting ToM objects in images andprocessing them with a monocular depth estimation model. These labels can beused to fine-tune existing monocular or stereo networks, to let them learn howto deal with ToM surfaces. Experimental results on the Booster dataset show thedramatic improvements enabled by our remarkably simple proposal.</description><author>Alex Costanzino, Pierluigi Zama Ramirez, Matteo Poggi, Fabio Tosi, Stefano Mattoccia, Luigi Di Stefano</author><pubDate>Thu, 27 Jul 2023 18:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15052v1</guid></item><item><title>Matching Patients to Clinical Trials with Large Language Models</title><link>http://arxiv.org/abs/2307.15051v1</link><description>Clinical trials are vital in advancing drug development and evidence-basedmedicine, but their success is often hindered by challenges in patientrecruitment. In this work, we investigate the potential of large languagemodels (LLMs) to assist individual patients and referral physicians inidentifying suitable clinical trials from an extensive selection. Specifically,we introduce TrialGPT, a novel architecture employing LLMs to predictcriterion-level eligibility with detailed explanations, which are thenaggregated for ranking and excluding candidate clinical trials based onfree-text patient notes. We evaluate TrialGPT on three publicly availablecohorts of 184 patients and 18,238 annotated clinical trials. The experimentalresults demonstrate several key findings: First, TrialGPT achieves highcriterion-level prediction accuracy with faithful explanations. Second, theaggregated trial-level TrialGPT scores are highly correlated with experteligibility annotations. Third, these scores prove effective in rankingclinical trials and exclude ineligible candidates. Our error analysis suggeststhat current LLMs still make some mistakes due to limited medical knowledge anddomain-specific context understanding. Nonetheless, we believe the explanatorycapabilities of LLMs are highly valuable. Future research is warranted on howsuch AI assistants can be integrated into the routine trial matching workflowin real-world settings to improve its efficiency.</description><author>Qiao Jin, Zifeng Wang, Charalampos S. Floudas, Jimeng Sun, Zhiyong Lu</author><pubDate>Thu, 27 Jul 2023 18:56:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15051v1</guid></item><item><title>Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models</title><link>http://arxiv.org/abs/2307.15049v1</link><description>Prompt tuning and adapter tuning have shown great potential in transferringpre-trained vision-language models (VLMs) to various downstream tasks. In thiswork, we design a new type of tuning method, termed as regularized mask tuning,which masks the network parameters through a learnable selection. Inspired byneural pathways, we argue that the knowledge required by a downstream taskalready exists in the pre-trained weights but just gets concealed in theupstream pre-training stage. To bring the useful knowledge back into light, wefirst identify a set of parameters that are important to a given downstreamtask, then attach a binary mask to each parameter, and finally optimize thesemasks on the downstream data with the parameters frozen. When updating themask, we introduce a novel gradient dropout strategy to regularize theparameter selection, in order to prevent the model from forgetting oldknowledge and overfitting the downstream data. Experimental results on 11datasets demonstrate the consistent superiority of our method over previousalternatives. It is noteworthy that we manage to deliver 18.73% performanceimprovement compared to the zero-shot CLIP via masking an average of only 2.56%parameters. Furthermore, our method is synergistic with most existingparameter-efficient tuning methods and can boost the performance on top ofthem. Project page can be found here (https://wuw2019.github.io/RMT/).</description><author>Kecheng Zheng, Wei Wu, Ruili Feng, Kai Zhu, Jiawei Liu, Deli Zhao, Zheng-Jun Zha, Wei Chen, Yujun Shen</author><pubDate>Thu, 27 Jul 2023 18:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15049v1</guid></item><item><title>Mining Clues from Incomplete Utterance: A Query-enhanced Network for Incomplete Utterance Rewriting</title><link>http://arxiv.org/abs/2307.00866v2</link><description>Incomplete utterance rewriting has recently raised wide attention. However,previous works do not consider the semantic structural information betweenincomplete utterance and rewritten utterance or model the semantic structureimplicitly and insufficiently. To address this problem, we propose aQUEry-Enhanced Network (QUEEN). Firstly, our proposed query template explicitlybrings guided semantic structural knowledge between the incomplete utteranceand the rewritten utterance making model perceive where to refer back to orrecover omitted tokens. Then, we adopt a fast and effective edit operationscoring network to model the relation between two tokens. Benefiting fromproposed query template and the well-designed edit operation scoring network,QUEEN achieves state-of-the-art performance on several public datasets.</description><author>Shuzheng Si, Shuang Zeng, Baobao Chang</author><pubDate>Thu, 27 Jul 2023 18:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00866v2</guid></item><item><title>A Transformer-based Approach for Arabic Offline Handwritten Text Recognition</title><link>http://arxiv.org/abs/2307.15045v1</link><description>Handwriting recognition is a challenging and critical problem in the fieldsof pattern recognition and machine learning, with applications spanning a widerange of domains. In this paper, we focus on the specific issue of recognizingoffline Arabic handwritten text. Existing approaches typically utilize acombination of convolutional neural networks for image feature extraction andrecurrent neural networks for temporal modeling, with connectionist temporalclassification used for text generation. However, these methods suffer from alack of parallelization due to the sequential nature of recurrent neuralnetworks. Furthermore, these models cannot account for linguistic rules,necessitating the use of an external language model in the post-processingstage to boost accuracy. To overcome these issues, we introduce two alternativearchitectures, namely the Transformer Transducer and the standardsequence-to-sequence Transformer, and compare their performance in terms ofaccuracy and speed. Our approach can model language dependencies and reliesonly on the attention mechanism, thereby making it more parallelizable and lesscomplex. We employ pre-trained Transformers for both image understanding andlanguage modeling. Our evaluation on the Arabic KHATT dataset demonstrates thatour proposed method outperforms the current state-of-the-art approaches forrecognizing offline Arabic handwritten text.</description><author>Saleh Momeni, Bagher BabaAli</author><pubDate>Thu, 27 Jul 2023 18:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15045v1</guid></item><item><title>Universal and Transferable Adversarial Attacks on Aligned Language Models</title><link>http://arxiv.org/abs/2307.15043v1</link><description>Because "out-of-the-box" large language models are capable of generating agreat deal of objectionable content, recent work has focused on aligning thesemodels in an attempt to prevent undesirable generation. While there has beensome success at circumventing these measures -- so-called "jailbreaks" againstLLMs -- these attacks have required significant human ingenuity and are brittlein practice. In this paper, we propose a simple and effective attack methodthat causes aligned language models to generate objectionable behaviors.Specifically, our approach finds a suffix that, when attached to a wide rangeof queries for an LLM to produce objectionable content, aims to maximize theprobability that the model produces an affirmative response (rather thanrefusing to answer). However, instead of relying on manual engineering, ourapproach automatically produces these adversarial suffixes by a combination ofgreedy and gradient-based search techniques, and also improves over pastautomatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approachare quite transferable, including to black-box, publicly released LLMs.Specifically, we train an adversarial attack suffix on multiple prompts (i.e.,queries asking for many different types of objectionable content), as well asmultiple models (in our case, Vicuna-7B and 13B). When doing so, the resultingattack suffix is able to induce objectionable content in the public interfacesto ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,Pythia, Falcon, and others. In total, this work significantly advances thestate-of-the-art in adversarial attacks against aligned language models,raising important questions about how such systems can be prevented fromproducing objectionable information. Code is available atgithub.com/llm-attacks/llm-attacks.</description><author>Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson</author><pubDate>Thu, 27 Jul 2023 18:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15043v1</guid></item><item><title>TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis</title><link>http://arxiv.org/abs/2307.15042v1</link><description>The gradual nature of a diffusion process that synthesizes samples in smallincrements constitutes a key ingredient of Denoising Diffusion ProbabilisticModels (DDPM), which have presented unprecedented quality in image synthesisand been recently explored in the motion domain. In this work, we propose toadapt the gradual diffusion concept (operating along a diffusion time-axis)into the temporal-axis of the motion sequence. Our key idea is to extend theDDPM framework to support temporally varying denoising, thereby entangling thetwo axes. Using our special formulation, we iteratively denoise a motion bufferthat contains a set of increasingly-noised poses, which auto-regressivelyproduces an arbitrarily long stream of frames. With a stationary diffusiontime-axis, in each diffusion step we increment only the temporal-axis of themotion such that the framework produces a new, clean frame which is removedfrom the beginning of the buffer, followed by a newly drawn noise vector thatis appended to it. This new mechanism paves the way towards a new framework forlong-term motion synthesis with applications to character animation and otherdomains.</description><author>Zihan Zhang, Richard Liu, Kfir Aberman, Rana Hanocka</author><pubDate>Thu, 27 Jul 2023 18:48:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15042v1</guid></item><item><title>Algorithmic Gaussianization through Sketching: Converting Data into Sub-gaussian Random Designs</title><link>http://arxiv.org/abs/2206.10291v2</link><description>Algorithmic Gaussianization is a phenomenon that can arise when usingrandomized sketching or sampling methods to produce smaller representations oflarge datasets: For certain tasks, these sketched representations have beenobserved to exhibit many robust performance characteristics that are known tooccur when a data sample comes from a sub-gaussian random design, which is apowerful statistical model of data distributions. However, this phenomenon hasonly been studied for specific tasks and metrics, or by relying oncomputationally expensive methods. We address this by providing an algorithmicframework for gaussianizing data distributions via averaging, proving that itis possible to efficiently construct data sketches that are nearlyindistinguishable (in terms of total variation distance) from sub-gaussianrandom designs. In particular, relying on a recently introduced sketchingtechnique called Leverage Score Sparsified (LESS) embeddings, we show that onecan construct an $n\times d$ sketch of an $N\times d$ matrix $A$, where $n\llN$, that is nearly indistinguishable from a sub-gaussian design, in time$O(\text{nnz}(A)\log N + nd^2)$, where $\text{nnz}(A)$ is the number ofnon-zero entries in $A$. As a consequence, strong statistical guarantees andprecise asymptotics available for the estimators produced from sub-gaussiandesigns (e.g., for least squares and Lasso regression, covariance estimation,low-rank approximation, etc.) can be straightforwardly adapted to our sketchingframework. We illustrate this with a new approximation guarantee for sketchedleast squares, among other examples.</description><author>Michał Dereziński</author><pubDate>Thu, 27 Jul 2023 18:48:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.10291v2</guid></item><item><title>A Sparse Quantized Hopfield Network for Online-Continual Memory</title><link>http://arxiv.org/abs/2307.15040v1</link><description>An important difference between brains and deep neural networks is the waythey learn. Nervous systems learn online where a stream of noisy data pointsare presented in a non-independent, identically distributed (non-i.i.d.) way.Further, synaptic plasticity in the brain depends only on information local tosynapses. Deep networks, on the other hand, typically use non-local learningalgorithms and are trained in an offline, non-noisy, i.i.d. setting.Understanding how neural networks learn under the same constraints as the brainis an open problem for neuroscience and neuromorphic computing. A standardapproach to this problem has yet to be established. In this paper, we proposethat discrete graphical models that learn via an online maximum a posteriorilearning algorithm could provide such an approach. We implement this kind ofmodel in a novel neural network called the Sparse Quantized Hopfield Network(SQHN). We show that SQHNs outperform state-of-the-art neural networks onassociative memory tasks, outperform these models in online, non-i.i.d.settings, learn efficiently with noisy inputs, and are better than baselines ona novel episodic memory task.</description><author>Nick Alonso, Jeff Krichmar</author><pubDate>Thu, 27 Jul 2023 18:46:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15040v1</guid></item><item><title>Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services</title><link>http://arxiv.org/abs/2305.02109v2</link><description>Federated learning (FL) is the most popular distributed machine learningtechnique. However, implementation of FL over modern wireless networks faceskey challenges caused by (i) dynamics of the network conditions and (ii) thecoexistence of multiple FL services/tasks and other network services in thesystem, which are not jointly considered in prior works. Motivated by thesechallenges, we introduce a generic FL paradigm over NextG networks, calleddynamic multi-service FL (DMS-FL). We identify three unexplored designconsiderations in DMS-FL: (i) FL service operator accumulation, (ii) wirelessresource fragmentation, and (iii) signal strength fluctuations. We take thefirst steps towards addressing these design considerations by proposing a noveldistributed ML architecture called elastic virtualized FL (EV-FL). EV-FLunleashes the full potential of Open RAN (O-RAN) systems and introduces anelastic resource provisioning methodology to execute FL services. It furtherconstitutes a multi-time-scale FL management system that introduces threedimensions into existing FL architectures: (i) virtualization, (ii)scalability, and (iii) elasticity. Through investigating EV-FL, we reveal aseries of open research directions for future work. We finally simulate EV-FLto demonstrate its potential in saving wireless resources and increasingfairness among FL services.</description><author>Payam Abdisarabshali, Nicholas Accurso, Filippo Malandra, Weifeng Su, Seyyedali Hosseinalipour</author><pubDate>Thu, 27 Jul 2023 18:44:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02109v2</guid></item><item><title>Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance</title><link>http://arxiv.org/abs/2307.03811v2</link><description>Advanced computational methods are being actively sought for addressing thechallenges associated with discovery and development of new combinatorialmaterial such as formulations. A widely adopted approach involves domaininformed high-throughput screening of individual components that can becombined into a formulation. This manages to accelerate the discovery of newcompounds for a target application but still leave the process of identifyingthe right 'formulation' from the shortlisted chemical space largely alaboratory experiment-driven process. We report a deep learning model,Formulation Graph Convolution Network (F-GCN), that can mapstructure-composition relationship of the individual components to the propertyof liquid formulation as whole. Multiple GCNs are assembled in parallel thatfeaturize formulation constituents domain-intuitively on the fly. The resultingmolecular descriptors are scaled based on respective constituent's molarpercentage in the formulation, followed by formalizing into a combineddescriptor that represents a complete formulation to an external learningarchitecture. The use case of proposed formulation learning model isdemonstrated for battery electrolytes by training and testing it on twoexemplary datasets representing electrolyte formulations vs battery performance-- one dataset is sourced from literature about Li/Cu half-cells, while theother is obtained by lab-experiments related to lithium-iodide full-cellchemistry. The model is shown to predict the performance metrics like CoulombicEfficiency (CE) and specific capacity of new electrolyte formulations withlowest reported errors. The best performing F-GCN model uses moleculardescriptors derived from molecular graphs that are informed with HOMO-LUMO andelectric moment properties of the molecules using a knowledge transfertechnique.</description><author>Vidushi Sharma, Maxwell Giammona, Dmitry Zubarev, Andy Tek, Khanh Nugyuen, Linda Sundberg, Daniele Congiu, Young-Hye La</author><pubDate>Thu, 27 Jul 2023 18:42:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03811v2</guid></item><item><title>Speeding up Fourier Neural Operators via Mixed Precision</title><link>http://arxiv.org/abs/2307.15034v1</link><description>The Fourier neural operator (FNO) is a powerful technique for learningsurrogate maps for partial differential equation (PDE) solution operators. Formany real-world applications, which often require high-resolution data points,training time and memory usage are significant bottlenecks. While there aremixed-precision training techniques for standard neural networks, those workfor real-valued datatypes on finite dimensions and therefore cannot be directlyapplied to FNO, which crucially operates in the (complex-valued) Fourier domainand in function spaces. On the other hand, since the Fourier transform isalready an approximation (due to discretization error), we do not need toperform the operation at full precision. In this work, we (i) profile memoryand runtime for FNO with full and mixed-precision training, (ii) conduct astudy on the numerical stability of mixed-precision training of FNO, and (iii)devise a training routine which substantially decreases training time andmemory usage (up to 34%), with little or no reduction in accuracy, on theNavier-Stokes and Darcy flow equations. Combined with the recently proposedtensorized FNO (Kossaifi et al., 2023), the resulting model has far betterperformance while also being significantly faster than the original FNO.</description><author>Colin White, Renbo Tu, Jean Kossaifi, Gennady Pekhimenko, Kamyar Azizzadenesheli, Anima Anandkumar</author><pubDate>Thu, 27 Jul 2023 18:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15034v1</guid></item><item><title>Diverse Inpainting and Editing with GAN Inversion</title><link>http://arxiv.org/abs/2307.15033v1</link><description>Recent inversion methods have shown that real images can be inverted intoStyleGAN's latent space and numerous edits can be achieved on those imagesthanks to the semantically rich feature representations of well-trained GANmodels. However, extensive research has also shown that image inversion ischallenging due to the trade-off between high-fidelity reconstruction andeditability. In this paper, we tackle an even more difficult task, invertingerased images into GAN's latent space for realistic inpaintings and editings.Furthermore, by augmenting inverted latent codes with different latent samples,we achieve diverse inpaintings. Specifically, we propose to learn an encoderand mixing network to combine encoded features from erased images withStyleGAN's mapped features from random samples. To encourage the mixing networkto utilize both inputs, we train the networks with generated data via a novelset-up. We also utilize higher-rate features to prevent color inconsistenciesbetween the inpainted and unerased parts. We run extensive experiments andcompare our method with state-of-the-art inversion and inpainting methods.Qualitative metrics and visual comparisons show significant improvements.</description><author>Ahmet Burak Yildirim, Hamza Pehlivan, Bahri Batuhan Bilecen, Aysegul Dundar</author><pubDate>Thu, 27 Jul 2023 18:41:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15033v1</guid></item><item><title>Adaptive Segmentation Network for Scene Text Detection</title><link>http://arxiv.org/abs/2307.15029v1</link><description>Inspired by deep convolution segmentation algorithms, scene text detectorsbreak the performance ceiling of datasets steadily. However, these methodsoften encounter threshold selection bottlenecks and have poor performance ontext instances with extreme aspect ratios. In this paper, we propose toautomatically learn the discriminate segmentation threshold, whichdistinguishes text pixels from background pixels for segmentation-based scenetext detectors and then further reduces the time-consuming manual parameteradjustment. Besides, we design a Global-information Enhanced Feature PyramidNetwork (GE-FPN) for capturing text instances with macro size and extremeaspect ratios. Following the GE-FPN, we introduce a cascade optimizationstructure to further refine the text instances. Finally, together with theproposed threshold learning strategy and text detection structure, we design anAdaptive Segmentation Network (ASNet) for scene text detection. Extensiveexperiments are carried out to demonstrate that the proposed ASNet can achievethe state-of-the-art performance on four text detection benchmarks, i.e., ICDAR2015, MSRA-TD500, ICDAR 2017 MLT and CTW1500. The ablation experiments alsoverify the effectiveness of our contributions.</description><author>Guiqin Zhao</author><pubDate>Thu, 27 Jul 2023 18:37:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15029v1</guid></item><item><title>SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark</title><link>http://arxiv.org/abs/2307.15020v1</link><description>Large language models (LLMs) have shown the potential to be integrated intohuman daily lives. Therefore, user preference is the most critical criterionfor assessing LLMs' performance in real-world scenarios. However, existingbenchmarks mainly focus on measuring models' accuracy using multi-choicequestions, which limits the understanding of their capabilities in realapplications. We fill this gap by proposing a comprehensive Chinese benchmarkSuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUEencompasses three sub-tasks: actual users' queries and ratings derived from anLLM battle platform (CArena), open-ended questions with single andmultiple-turn dialogues (OPEN), and closed-ended questions with the same stemsas open-ended single-turn ones (CLOSE). Our study shows that accuracy onclosed-ended questions is insufficient to reflect human preferences achieved onopen-ended ones. At the same time, they can complement each other to predictactual user preferences. We also demonstrate that GPT-4 is a reliable judge toautomatically evaluate human preferences on open-ended questions in a Chinesecontext. Our benchmark will be released at https://www.CLUEbenchmarks.com</description><author>Liang Xu, Anqi Li, Lei Zhu, Hang Xue, Changtai Zhu, Kangkang Zhao, Haonan He, Xuanwei Zhang, Qiyue Kang, Zhenzhong Lan</author><pubDate>Thu, 27 Jul 2023 18:24:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15020v1</guid></item><item><title>Self-Supervised Graph Transformer for Deepfake Detection</title><link>http://arxiv.org/abs/2307.15019v1</link><description>Deepfake detection methods have shown promising results in recognizingforgeries within a given dataset, where training and testing take place on thein-distribution dataset. However, their performance deteriorates significantlywhen presented with unseen samples. As a result, a reliable deepfake detectionsystem must remain impartial to forgery types, appearance, and quality forguaranteed generalizable detection performance. Despite various attempts toenhance cross-dataset generalization, the problem remains challenging,particularly when testing against common post-processing perturbations, such asvideo compression or blur. Hence, this study introduces a deepfake detectionframework, leveraging a self-supervised pre-training model that deliversexceptional generalization ability, withstanding common corruptions andenabling feature explainability. The framework comprises three key components:a feature extractor based on vision Transformer architecture that ispre-trained via self-supervised contrastive learning methodology, a graphconvolution network coupled with a Transformer discriminator, and a graphTransformer relevancy map that provides a better understanding of manipulatedregions and further explains the model's decision. To assess the effectivenessof the proposed framework, several challenging experiments are conducted,including in-data distribution performance, cross-dataset, cross-manipulationgeneralization, and robustness against common post-production perturbations.The results achieved demonstrate the remarkable effectiveness of the proposeddeepfake detection framework, surpassing the current state-of-the-artapproaches.</description><author>Aminollah Khormali, Jiann-Shiun Yuan</author><pubDate>Thu, 27 Jul 2023 18:22:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15019v1</guid></item><item><title>Samplable Anonymous Aggregation for Private Federated Data Analysis</title><link>http://arxiv.org/abs/2307.15017v1</link><description>We revisit the problem of designing scalable protocols for private statisticsand private federated learning when each device holds its private data. Ourfirst contribution is to propose a simple primitive that allows for efficientimplementation of several commonly used algorithms, and allows for privacyaccounting that is close to that in the central setting without requiring thestrong trust assumptions it entails. Second, we propose a system architecturethat implements this primitive and perform a security analysis of the proposedsystem.</description><author>Kunal Talwar, Shan Wang, Audra McMillan, Vojta Jina, Vitaly Feldman, Bailey Basile, Aine Cahill, Yi Sheng Chan, Mike Chatzidakis, Junye Chen, Oliver Chick, Mona Chitnis, Suman Ganta, Yusuf Goren, Filip Granqvist, Kristine Guo, Frederic Jacobs, Omid Javidbakht, Albert Liu, Richard Low, Dan Mascenik, Steve Myers, David Park, Wonhee Park, Gianni Parsa, Tommy Pauly, Christian Priebe, Rehan Rishi, Guy Rothblum, Michael Scaria, Linmao Song, Congzheng Song, Karl Tarbe, Sebastian Vogt, Luke Winstrom, Shundong Zhou</author><pubDate>Thu, 27 Jul 2023 18:19:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15017v1</guid></item><item><title>How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges</title><link>http://arxiv.org/abs/2307.15016v1</link><description>Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT inthe field of conversational AI. Notably, Bard has recently been updated tohandle visual inputs alongside text prompts during conversations. Given Bard'simpressive track record in handling textual inputs, we explore its capabilitiesin understanding and interpreting visual data (images) conditioned by textquestions. This exploration holds the potential to unveil new insights andchallenges for Bard and other forthcoming multi-modal Generative models,especially in addressing complex computer vision problems that demand accuratevisual and language understanding. Specifically, in this study, we focus on 15diverse task scenarios encompassing regular, camouflaged, medical, under-waterand remote sensing data to comprehensively evaluate Bard's performance. Ourprimary finding indicates that Bard still struggles in these vision scenarios,highlighting the significant gap in vision-based understanding that needs to bebridged in future developments. We expect that this empirical study will provevaluable in advancing future models, leading to enhanced capabilities incomprehending and interpreting fine-grained visual data. Our project isreleased on https://github.com/htqin/GoogleBard-VisUnderstand</description><author>Haotong Qin, Ge-Peng Ji, Salman Khan, Deng-Ping Fan, Fahad Shahbaz Khan, Luc Van Gool</author><pubDate>Thu, 27 Jul 2023 18:19:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15016v1</guid></item><item><title>Dynamics of specialization in neural modules under resource constraints</title><link>http://arxiv.org/abs/2106.02626v2</link><description>It has long been believed that the brain is highly modular both in terms ofstructure and function, although recent evidence has led some to question theextent of both types of modularity. We used artificial neural networks to testthe hypothesis that structural modularity is sufficient to guarantee functionalspecialization, and find that in general, this doesn't necessarily hold exceptat extreme levels. We then systematically tested which features of theenvironment and network do lead to the emergence of specialization. We used asimple toy environment, task and network, allowing us precise control, and showthat in this setup, several distinct measures of specialization givequalitatively similar results. We further find that (1) specialization can onlyemerge in environments where features of that environment are meaningfullyseparable, (2) specialization preferentially emerges when the network isstrongly resource-constrained, and (3) these findings are qualitatively similaracross different network architectures, but the quantitative relationshipsdepends on the architecture type. Finally, we show that functionalspecialization varies dynamically across time, and demonstrate that thesedynamics depend on both the timing and bandwidth of information flow in thenetwork. We conclude that a static notion of specialization, based onstructural modularity, is likely too simple a framework for understandingintelligent systems in situations of real-world complexity. We propose thatthoroughly stress testing candidate definitions of functional modularity insimplified scenarios before extending to more complex data, network models andelectrophysiological recordings is likely to be a fruitful approach.</description><author>Gabriel Béna, Dan F. M. Goodman</author><pubDate>Thu, 27 Jul 2023 18:19:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.02626v2</guid></item><item><title>How to Scale Your EMA</title><link>http://arxiv.org/abs/2307.13813v2</link><description>Preserving training dynamics across batch sizes is an important tool forpractical machine learning as it enables the trade-off between batch size andwall-clock time. This trade-off is typically enabled by a scaling rule, forexample, in stochastic gradient descent, one should scale the learning ratelinearly with the batch size. Another important tool for practical machinelearning is the model Exponential Moving Average (EMA), which is a model copythat does not receive gradient information, but instead follows its targetmodel with some momentum. This model EMA can improve the robustness andgeneralization properties of supervised learning, stabilize pseudo-labeling,and provide a learning signal for Self-Supervised Learning (SSL). Prior workshave treated the model EMA separately from optimization, leading to differenttraining dynamics across batch sizes and lower model performance. In this work,we provide a scaling rule for optimization in the presence of model EMAs anddemonstrate its validity across a range of architectures, optimizers, and datamodalities. We also show the rule's validity where the model EMA contributes tothe optimization of the target model, enabling us to train EMA-basedpseudo-labeling and SSL methods at small and large batch sizes. For SSL, weenable training of BYOL up to batch size 24,576 without sacrificingperformance, optimally a 6$\times$ wall-clock time reduction.</description><author>Dan Busbridge, Jason Ramapuram, Pierre Ablin, Tatiana Likhomanenko, Eeshan Gunesh Dhekane, Xavier Suau, Russ Webb</author><pubDate>Thu, 27 Jul 2023 18:17:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13813v2</guid></item><item><title>Harnessing Synthetic Active Particles for Physical Reservoir Computing</title><link>http://arxiv.org/abs/2307.15010v1</link><description>The processing of information is an indispensable property of living systemsrealized by networks of active processes with enormous complexity. They haveinspired many variants of modern machine learning one of them being reservoircomputing, in which stimulating a network of nodes with fading memory enablescomputations and complex predictions. Reservoirs are implemented on computerhardware, but also on unconventional physical substrates such as mechanicaloscillators, spins, or bacteria often summarized as physical reservoircomputing. Here we demonstrate physical reservoir computing with a syntheticactive microparticle system that self-organizes from an active and passivecomponent into inherently noisy nonlinear dynamical units. Theself-organization and dynamical response of the unit is the result of a delayedpropulsion of the microswimmer to a passive target. A reservoir of such unitswith a self-coupling via the delayed response can perform predictive tasksdespite the strong noise resulting from Brownian motion of the microswimmers.To achieve efficient noise suppression, we introduce a special architecturethat uses historical reservoir states for output. Our results pave the way forthe study of information processing in synthetic self-organized active particlesystems.</description><author>Xiangzun Wang, Frank Cichos</author><pubDate>Thu, 27 Jul 2023 18:08:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15010v1</guid></item><item><title>Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability</title><link>http://arxiv.org/abs/2307.15007v1</link><description>With the increased deployment of machine learning models in variousreal-world applications, researchers and practitioners alike have emphasizedthe need for explanations of model behaviour. To this end, two broad strategieshave been outlined in prior literature to explain models. Post hoc explanationmethods explain the behaviour of complex black-box models by highlightingfeatures that are critical to model predictions; however, prior work has shownthat these explanations may not be faithful, and even more concerning is ourinability to verify them. Specifically, it is nontrivial to evaluate if a givenattribution is correct with respect to the underlying model. Inherentlyinterpretable models, on the other hand, circumvent these issues by explicitlyencoding explanations into model architecture, meaning their explanations arenaturally faithful and verifiable, but they often exhibit poor predictiveperformance due to their limited expressive power. In this work, we aim tobridge the gap between the aforementioned strategies by proposing VerifiabilityTuning (VerT), a method that transforms black-box models into models thatnaturally yield faithful and verifiable feature attributions. We begin byintroducing a formal theoretical framework to understand verifiability and showthat attributions produced by standard models cannot be verified. We thenleverage this framework to propose a method to build verifiable models andfeature attributions out of fully trained black-box models. Finally, we performextensive experiments on semi-synthetic and real-world datasets, and show thatVerT produces models that (1) yield explanations that are correct andverifiable and (2) are faithful to the original black-box models they are meantto explain.</description><author>Usha Bhalla, Suraj Srinivas, Himabindu Lakkaraju</author><pubDate>Thu, 27 Jul 2023 18:06:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15007v1</guid></item><item><title>Gzip versus bag-of-words for text classification with KNN</title><link>http://arxiv.org/abs/2307.15002v1</link><description>The effectiveness of compression distance in KNN-based text classification('gzip') has recently garnered lots of attention. In this note, we show thatsimilar or better effectiveness can be achieved with simpler means, and textcompression may not be necessary. Indeed, we find that a simple 'bag-of-words'matching can achieve similar or better accuracy, and is more efficient.</description><author>Juri Opitz</author><pubDate>Thu, 27 Jul 2023 17:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15002v1</guid></item><item><title>RELDEC: Reinforcement Learning-Based Decoding of Moderate Length LDPC Codes</title><link>http://arxiv.org/abs/2112.13934v3</link><description>In this work we propose RELDEC, a novel approach for sequential decoding ofmoderate length low-density parity-check (LDPC) codes. The main idea behindRELDEC is that an optimized decoding policy is subsequently obtained viareinforcement learning based on a Markov decision process (MDP). In contrast toour previous work, where an agent learns to schedule only a single check node(CN) within a group (cluster) of CNs per iteration, in this work we train theagent to schedule all CNs in a cluster, and all clusters in every iteration.That is, in each learning step of RELDEC an agent learns to schedule CNclusters sequentially depending on a reward associated with the outcome ofscheduling a particular cluster. We also modify the state space representationof the MDP, enabling RELDEC to be suitable for larger block length LDPC codesthan those studied in our previous work. Furthermore, to address decoding undervarying channel conditions, we propose agile meta-RELDEC (AM-RELDEC) thatemploys meta-reinforcement learning. The proposed RELDEC scheme significantlyoutperforms standard flooding and random sequential decoding for a variety ofLDPC codes, including codes designed for 5G new radio.</description><author>Salman Habib, Allison Beemer, Joerg Kliewer</author><pubDate>Thu, 27 Jul 2023 17:53:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.13934v3</guid></item><item><title>Likelihood-Free Parameter Estimation with Neural Bayes Estimators</title><link>http://arxiv.org/abs/2208.12942v4</link><description>Neural point estimators are neural networks that map data to parameter pointestimates. They are fast, likelihood free and, due to their amortised nature,amenable to fast bootstrap-based uncertainty quantification. In this paper, weaim to increase the awareness of statisticians to this relatively newinferential tool, and to facilitate its adoption by providing user-friendlyopen-source software. We also give attention to the ubiquitous problem ofmaking inference from replicated data, which we address in the neural settingusing permutation-invariant neural networks. Through extensive simulationstudies we show that these neural point estimators can quickly and optimally(in a Bayes sense) estimate parameters in weakly-identified andhighly-parameterised models with relative ease. We demonstrate theirapplicability through an analysis of extreme sea-surface temperature in the RedSea where, after training, we obtain parameter estimates and bootstrap-basedconfidence intervals from hundreds of spatial fields in a fraction of a second.</description><author>Matthew Sainsbury-Dale, Andrew Zammit-Mangion, Raphaël Huser</author><pubDate>Thu, 27 Jul 2023 17:50:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.12942v4</guid></item><item><title>Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale</title><link>http://arxiv.org/abs/2306.00017v4</link><description>Large language models (LLMs) have achieved a milestone that undenia-blychanged many held beliefs in artificial intelligence (AI). However, thereremains many limitations of these LLMs when it comes to true languageunderstanding, limitations that are a byproduct of the under-lying architectureof deep neural networks. Moreover, and due to their subsymbolic nature,whatever knowledge these models acquire about how language works will always beburied in billions of microfeatures (weights), none of which is meaningful onits own, making such models hopelessly unexplainable. To address theselimitations, we suggest com-bining the strength of symbolic representationswith what we believe to be the key to the success of LLMs, namely a successfulbottom-up re-verse engineering of language at scale. As such we argue for abottom-up reverse engineering of language in a symbolic setting. Hints on whatthis project amounts to have been suggested by several authors, and we discussin some detail here how this project could be accomplished.</description><author>Walid S. Saba</author><pubDate>Thu, 27 Jul 2023 17:47:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00017v4</guid></item><item><title>Scaling TransNormer to 175 Billion Parameters</title><link>http://arxiv.org/abs/2307.14995v1</link><description>We present TransNormerLLM, the first linear attention-based Large LanguageModel (LLM) that outperforms conventional softmax attention-based models interms of both accuracy and efficiency. TransNormerLLM evolves from the previouslinear attention architecture TransNormer by making advanced modifications thatinclude positional embedding, linear attention acceleration, gating mechanism,tensor normalization, inference acceleration and stabilization. Specifically,we use LRPE together with an exponential decay to avoid attention dilutionissues while allowing the model to retain global interactions between tokens.Additionally, we propose Lightning Attention, a cutting-edge technique thataccelerates linear attention by more than twice in runtime and reduces memoryusage by a remarkable four times. To further enhance the performance ofTransNormer, we leverage a gating mechanism to smooth training and a new tensornormalization scheme to accelerate the model, resulting in an impressiveacceleration of over 20%. Furthermore, we have developed a robust inferencealgorithm that ensures numerical stability and consistent inference speed,regardless of the sequence length, showcasing superior efficiency during bothtraining and inference stages. Scalability is at the heart of our model'sdesign, enabling seamless deployment on large-scale clusters and facilitatingexpansion to even more extensive models, all while maintaining outstandingperformance metrics. Rigorous validation of our model design is achievedthrough a series of comprehensive experiments on our self-collected corpus,boasting a size exceeding 6TB and containing over 2 trillion tokens. To ensuredata quality and relevance, we implement a new self-cleaning strategy to filterour collected data. Our pre-trained models will be released to foster communityadvancements in efficient LLMs.</description><author>Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, Yu Qiao, Yiran Zhong</author><pubDate>Thu, 27 Jul 2023 17:45:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14995v1</guid></item><item><title>Thinker: Learning to Plan and Act</title><link>http://arxiv.org/abs/2307.14993v1</link><description>We propose the Thinker algorithm, a novel approach that enables reinforcementlearning agents to autonomously interact with and utilize a learned worldmodel. The Thinker algorithm wraps the environment with a world model andintroduces new actions designed for interacting with the world model. Thesemodel-interaction actions enable agents to perform planning by proposingalternative plans to the world model before selecting a final action to executein the environment. This approach eliminates the need for hand-crafted planningalgorithms by enabling the agent to learn how to plan autonomously and allowsfor easy interpretation of the agent's plan with visualization. We demonstratethe algorithm's effectiveness through experimental results in the game ofSokoban and the Atari 2600 benchmark, where the Thinker algorithm achievesstate-of-the-art performance and competitive results, respectively.Visualizations of agents trained with the Thinker algorithm demonstrate thatthey have learned to plan effectively with the world model to select betteractions. The algorithm's generality opens a new research direction on how aworld model can be used in reinforcement learning and how planning can beseamlessly integrated into an agent's decision-making process.</description><author>Stephen Chung, Ivan Anokhin, David Krueger</author><pubDate>Thu, 27 Jul 2023 17:40:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14993v1</guid></item><item><title>Trace Recovery from Stochastically Known Logs</title><link>http://arxiv.org/abs/2206.12672v2</link><description>In this work we propose an algorithm for trace recovery from stochasticallyknown logs, a setting that is becoming more common with the increasing numberof sensors and predictive models that generate uncertain data. The suggestedapproach calculates the conformance between a process model and astochastically known trace and recovers the best alignment within thisstochastic trace as the true trace. The paper offers an analysis of the impactof various cost models on trace recovery accuracy and makes use of a productmulti-graph to compare alternative trace recovery options. The average accuracyof our approach, evaluated using two publicly available datasets, isimpressive, with an average recovery accuracy score of 90-97%, significantlyimproving a common heuristic that chooses the most likely value for eachuncertain activity. We believe that the effectiveness of the proposed algorithmin recovering correct traces from stochastically known logs may be a powerfulaid for developing credible decision-making tools in uncertain settings.</description><author>Eli Bogdanov, Izack Cohen, Avigdor Gal</author><pubDate>Thu, 27 Jul 2023 17:38:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.12672v2</guid></item><item><title>Multilingual Code Co-Evolution Using Large Language Models</title><link>http://arxiv.org/abs/2307.14991v1</link><description>Many software projects implement APIs and algorithms in multiple programminglanguages. Maintaining such projects is tiresome, as developers have to ensurethat any change (e.g., a bug fix or a new feature) is being propagated, timelyand without errors, to implementations in other programming languages. In theworld of ever-changing software, using rule-based translation tools (i.e.,transpilers) or machine learning models for translating code from one languageto another provides limited value. Translating each time the entire codebasefrom one language to another is not the way developers work. In this paper, wetarget a novel task: translating code changes from one programming language toanother using large language models (LLMs). We design and implement the firstLLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models codechanges as edit sequences and learns to correlate changes across programminglanguages. To evaluate Codeditor, we collect a corpus of 6,613 aligned codechanges from 8 pairs of open-source software projects implementing similarfunctionalities in two programming languages (Java and C#). Results show thatCodeditor outperforms the state-of-the-art approaches by a large margin on allcommonly used automatic metrics. Our work also reveals that Codeditor iscomplementary to the existing generation-based models, and their combinationensures even greater performance.</description><author>Jiyang Zhang, Pengyu Nie, Junyi Jessy Li, Milos Gligoric</author><pubDate>Thu, 27 Jul 2023 17:37:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14991v1</guid></item><item><title>Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs</title><link>http://arxiv.org/abs/2307.14988v1</link><description>Deep learning often faces the challenge of efficiently processing dynamicinputs, such as sensor data or user inputs. For example, an AI writingassistant is required to update its suggestions in real time as a document isedited. Re-running the model each time is expensive, even with compressiontechniques like knowledge distillation, pruning, or quantization. Instead, wetake an incremental computing approach, looking to reuse calculations as theinputs change. However, the dense connectivity of conventional architecturesposes a major obstacle to incremental computation, as even minor input changescascade through the network and restrict information reuse. To address this, weuse vector quantization to discretize intermediate values in the network, whichfilters out noisy and unnecessary modifications to hidden neurons, facilitatingthe reuse of their values. We apply this approach to the transformersarchitecture, creating an efficient incremental inference algorithm withcomplexity proportional to the fraction of the modified inputs. Our experimentswith adapting the OPT-125M pre-trained language model demonstrate comparableaccuracy on document classification while requiring 12.1X (median) feweroperations for processing sequences of atomic edits.</description><author>Or Sharir, Anima Anandkumar</author><pubDate>Thu, 27 Jul 2023 17:30:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14988v1</guid></item><item><title>GP-guided MPPI for Efficient Navigation in Complex Unknown Cluttered Environments</title><link>http://arxiv.org/abs/2307.04019v2</link><description>Robotic navigation in unknown, cluttered environments with limited sensingcapabilities poses significant challenges in robotics. Local trajectoryoptimization methods, such as Model Predictive Path Intergal (MPPI), are apromising solution to this challenge. However, global guidance is required toensure effective navigation, especially when encountering challengingenvironmental conditions or navigating beyond the planning horizon. This studypresents the GP-MPPI, an online learning-based control strategy that integratesMPPI with a local perception model based on Sparse Gaussian Process (SGP). Thekey idea is to leverage the learning capability of SGP to construct a variance(uncertainty) surface, which enables the robot to learn about the navigablespace surrounding it, identify a set of suggested subgoals, and ultimatelyrecommend the optimal subgoal that minimizes a predefined cost function to thelocal MPPI planner. Afterward, MPPI computes the optimal control sequence thatsatisfies the robot and collision avoidance constraints. Such an approacheliminates the necessity of a global map of the environment or an offlinetraining process. We validate the efficiency and robustness of our proposedcontrol strategy through both simulated and real-world experiments of 2Dautonomous navigation tasks in complex unknown environments, demonstrating itssuperiority in guiding the robot safely towards its desired goal while avoidingobstacles and escaping entrapment in local minima. The GPU implementation ofGP-MPPI, including the supplementary video, is available athttps://github.com/IhabMohamed/GP-MPPI.</description><author>Ihab S. Mohamed, Mahmoud Ali, Lantao Liu</author><pubDate>Thu, 27 Jul 2023 17:22:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04019v2</guid></item><item><title>Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation</title><link>http://arxiv.org/abs/2306.04169v2</link><description>Weighted low rank approximation is a fundamental problem in numerical linearalgebra, and it has many applications in machine learning. Given a matrix $M\in \mathbb{R}^{n \times n}$, a weight matrix $W \in \mathbb{R}_{\geq 0}^{n\times n}$, a parameter $k$, the goal is to output two matrices $U, V \in\mathbb{R}^{n \times k}$ such that $\| W \circ (M - U V^\top) \|_F$ isminimized, where $\circ$ denotes the Hadamard product. Such a problem is knownto be NP-hard and even hard to approximate assuming Exponential Time Hypothesis[GG11, RSW16]. Meanwhile, alternating minimization is a good heuristic solutionfor approximating weighted low rank approximation. The work [LLR16] shows that,under mild assumptions, alternating minimization does provide provableguarantees. In this work, we develop an efficient and robust framework foralternating minimization. For weighted low rank approximation, this improvesthe runtime of [LLR16] from $n^2 k^2$ to $n^2k$. At the heart of our workframework is a high-accuracy multiple response regression solver together witha robust analysis of alternating minimization.</description><author>Zhao Song, Mingquan Ye, Junze Yin, Lichen Zhang</author><pubDate>Thu, 27 Jul 2023 17:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04169v2</guid></item><item><title>MapNeRF: Incorporating Map Priors into Neural Radiance Fields for Driving View Simulation</title><link>http://arxiv.org/abs/2307.14981v1</link><description>Simulating camera sensors is a crucial task in autonomous driving. Althoughneural radiance fields are exceptional at synthesizing photorealistic views indriving simulations, they still fail in generating extrapolated views. Thispaper proposes to incorporate map priors into neural radiance fields tosynthesize out-of-trajectory driving views with semantic road consistency. Thekey insight is that map information can be utilized as a prior to guide thetraining of the radiance fields with uncertainty. Specifically, we utilize thecoarse ground surface as uncertain information to supervise the density fieldand warp depth with uncertainty from unknown camera poses to ensure multi-viewconsistency. Experimental results demonstrate that our approach can producesemantic consistency in deviated views for vehicle camera simulation.</description><author>Chenming Wu, Jiadai Sun, Zhelun Shen, Liangjun Zhang</author><pubDate>Thu, 27 Jul 2023 17:19:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14981v1</guid></item><item><title>Causal Lifting and Link Prediction</title><link>http://arxiv.org/abs/2302.01198v2</link><description>Existing causal models for link prediction assume an underlying set ofinherent node factors -- an innate characteristic defined at the node's birth-- that governs the causal evolution of links in the graph. In some causaltasks, however, link formation is path-dependent: The outcome of linkinterventions depends on existing links. Unfortunately, these existing causalmethods are not designed for path-dependent link formation, as the cascadingfunctional dependencies between links (arising from path dependence) are eitherunidentifiable or require an impractical number of control variables. Toovercome this, we develop the first causal model capable of dealing with pathdependencies in link prediction. In this work we introduce the concept ofcausal lifting, an invariance in causal models of independent interest that, ongraphs, allows the identification of causal link prediction queries usinglimited interventional data. Further, we show how structural pairwiseembeddings exhibit lower bias and correctly represent the task's causalstructure, as opposed to existing node embeddings, e.g., graph neural networknode embeddings and matrix factorization. Finally, we validate our theoreticalfindings on three scenarios for causal link prediction tasks: knowledge basecompletion, covariance matrix estimation and consumer-product recommendations.</description><author>Leonardo Cotta, Beatrice Bevilacqua, Nesreen Ahmed, Bruno Ribeiro</author><pubDate>Thu, 27 Jul 2023 17:11:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01198v2</guid></item><item><title>Gaussian Latent Representations for Uncertainty Estimation using Mahalanobis Distance in Deep Classifiers</title><link>http://arxiv.org/abs/2305.13849v2</link><description>Recent works show that the data distribution in a network's latent space isuseful for estimating classification uncertainty and detectingOut-of-distribution (OOD) samples. To obtain a well-regularized latent spacethat is conducive for uncertainty estimation, existing methods bring insignificant changes to model architectures and training procedures. In thispaper, we present a lightweight, fast, and high-performance regularizationmethod for Mahalanobis distance-based uncertainty prediction, and that requiresminimal changes to the network's architecture. To derive Gaussian latentrepresentation favourable for Mahalanobis Distance calculation, we introduce aself-supervised representation learning method that separates in-classrepresentations into multiple Gaussians. Classes with non-Gaussianrepresentations are automatically identified and dynamically clustered intomultiple new classes that are approximately Gaussian. Evaluation on standardOOD benchmarks shows that our method achieves state-of-the-art results on OODdetection with minimal inference time, and is very competitive on predictiveprobability calibration. Finally, we show the applicability of our method to areal-life computer vision use case on microorganism classification.</description><author>Aishwarya Venkataramanan, Assia Benbihi, Martin Laviale, Cedric Pradalier</author><pubDate>Thu, 27 Jul 2023 17:09:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13849v2</guid></item><item><title>Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models</title><link>http://arxiv.org/abs/2307.14971v1</link><description>With the overwhelming trend of mask image modeling led by MAE, generativepre-training has shown a remarkable potential to boost the performance offundamental models in 2D vision. However, in 3D vision, the over-reliance onTransformer-based backbones and the unordered nature of point clouds haverestricted the further development of generative pre-training. In this paper,we propose a novel 3D-to-2D generative pre-training method that is adaptable toany point cloud model. We propose to generate view images from differentinstructed poses via the cross-attention mechanism as the pre-training scheme.Generating view images has more precise supervision than its point cloudcounterpart, thus assisting 3D backbones to have a finer comprehension of thegeometrical structure and stereoscopic relations of the point cloud.Experimental results have proved the superiority of our proposed 3D-to-2Dgenerative pre-training over previous pre-training methods. Our method is alsoeffective in boosting the performance of architecture-oriented approaches,achieving state-of-the-art performance when fine-tuning on ScanObjectNNclassification and ShapeNetPart segmentation tasks. Code is available athttps://github.com/wangzy22/TAP.</description><author>Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, Jiwen Lu</author><pubDate>Thu, 27 Jul 2023 17:07:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14971v1</guid></item><item><title>Learning locally dominant force balances in active particle systems</title><link>http://arxiv.org/abs/2307.14970v1</link><description>We use a combination of unsupervised clustering and sparsity-promotinginference algorithms to learn locally dominant force balances that explainmacroscopic pattern formation in self-organized active particle systems. Theself-organized emergence of macroscopic patterns from microscopic interactionsbetween self-propelled particles can be widely observed nature. Althoughhydrodynamic theories help us better understand the physical basis of thisphenomenon, identifying a sufficient set of local interactions that shape,regulate, and sustain self-organized structures in active particle systemsremains challenging. We investigate a classic hydrodynamic model ofself-propelled particles that produces a wide variety of patterns, like astersand moving density bands. Our data-driven analysis shows that propagating bandsare formed by local alignment interactions driven by density gradients, whilesteady-state asters are shaped by a mechanism of splay-induced negativecompressibility arising from strong particle interactions. Our method alsoreveals analogous physical principles of pattern formation in a system wherethe speed of the particle is influenced by local density. This demonstrates theability of our method to reveal physical commonalities across models. Thephysical mechanisms inferred from the data are in excellent agreement withanalytical scaling arguments and experimental observations.</description><author>Dominik Sturm, Suryanarayana Maddu, Ivo F. Sbalzarini</author><pubDate>Thu, 27 Jul 2023 17:06:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14970v1</guid></item><item><title>From Contextual Data to Newsvendor Decisions: On the Actual Performance of Data-Driven Algorithms</title><link>http://arxiv.org/abs/2302.08424v3</link><description>In this work, we explore a framework for contextual decision-making to studyhow the relevance and quantity of past data affects the performance of adata-driven policy. We analyze a contextual Newsvendor problem in which adecision-maker needs to trade-off between an underage and an overage cost inthe face of uncertain demand. We consider a setting in which past demandsobserved under ``close by'' contexts come from close by distributions andanalyze the performance of data-driven algorithms through a notion ofcontext-dependent worst-case expected regret. We analyze the broad class ofWeighted Empirical Risk Minimization (WERM) policies which weigh past dataaccording to their similarity in the contextual space. This class includesclassical policies such as ERM, k-Nearest Neighbors and kernel-based policies.Our main methodological contribution is to characterize exactly the worst-caseregret of any WERM policy on any given configuration of contexts. To the bestof our knowledge, this provides the first understanding of tight performanceguarantees in any contextual decision-making problem, with past literaturefocusing on upper bounds via concentration inequalities. We instead take anoptimization approach, and isolate a structure in the Newsvendor loss functionthat allows to reduce the infinite-dimensional optimization problem overworst-case distributions to a simple line search. This in turn allows us to unveil fundamental insights that were obfuscated byprevious general-purpose bounds. We characterize actual guaranteed performanceas a function of the contexts, as well as granular insights on the learningcurve of algorithms.</description><author>Omar Besbes, Will Ma, Omar Mouchtaki</author><pubDate>Thu, 27 Jul 2023 16:56:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08424v3</guid></item><item><title>Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification</title><link>http://arxiv.org/abs/2307.14959v1</link><description>In the medical field, federated learning commonly deals with highlyimbalanced datasets, including skin lesions and gastrointestinal images.Existing federated methods under highly imbalanced datasets primarily focus onoptimizing a global model without incorporating the intra-class variations thatcan arise in medical imaging due to different populations, findings, andscanners. In this paper, we study the inter-client intra-class variations withpublicly available self-supervised auxiliary networks. Specifically, we findthat employing a shared auxiliary pre-trained model, like MoCo-V2, locally onevery client yields consistent divergence measurements. Based on thesefindings, we derive a dynamic balanced model aggregation via self-supervisedpriors (MAS) to guide the global model optimization. Fed-MAS can be utilizedwith different local learning methods for effective model aggregation toward ahighly robust and unbiased global model. Our code is available at\url{https://github.com/xmed-lab/Fed-MAS}.</description><author>Marawan Elbatel, Hualiang Wang, Robert Martí, Huazhu Fu, Xiaomeng Li</author><pubDate>Thu, 27 Jul 2023 16:52:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14959v1</guid></item><item><title>Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space</title><link>http://arxiv.org/abs/2307.14953v1</link><description>This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aimsto mitigate data distribution shifts when transferring knowledge from multiplelabeled source domains to an unlabeled target domain. We propose a novel MSDAframework based on dictionary learning and optimal transport. We interpret eachdomain in MSDA as an empirical distribution. As such, we express each domain asa Wasserstein barycenter of dictionary atoms, which are empiricaldistributions. We propose a novel algorithm, DaDiL, for learning viamini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates.Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, basedon the reconstruction of labeled samples in the target domain, and DaDiL-E,based on the ensembling of classifiers learned on atom distributions. Weevaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU,where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% inclassification performance. Finally, we show that interpolations in theWasserstein hull of learned atoms provide data that can generalize to thetarget domain.</description><author>Eduardo Fernandes Montesuma, Fred Ngolè Mboula, Antoine Souloumiac</author><pubDate>Thu, 27 Jul 2023 16:46:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14953v1</guid></item><item><title>Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning</title><link>http://arxiv.org/abs/2307.14952v1</link><description>As the network scale increases, existing fully distributed solutions start tolag behind the real-world challenges such as (1) slow information propagation,(2) network communication failures, and (3) external adversarial attacks. Inthis paper, we focus on hierarchical system architecture and address theproblem of non-Bayesian learning over networks that are vulnerable tocommunication failures and adversarial attacks. On network communication, weconsider packet-dropping link failures. We first propose a hierarchical robust push-sum algorithm that can achieveaverage consensus despite frequent packet-dropping link failures. We provide asparse information fusion rule between the parameter server and arbitrarilyselected network representatives. Then, interleaving the consensus update stepwith a dual averaging update with Kullback-Leibler (KL) divergence as theproximal function, we obtain a packet-dropping fault-tolerant non-Bayesianlearning algorithm with provable convergence guarantees. On external adversarial attacks, we consider Byzantine attacks in which thecompromised agents can send maliciously calibrated messages to others(including both the agents and the parameter server). To avoid the curse ofdimensionality of Byzantine consensus, we solve the non-Bayesian learningproblem via running multiple dynamics, each of which only involves Byzantineconsensus with scalar inputs. To facilitate resilient information propagationacross sub-networks, we use a novel Byzantine-resilient gossiping-type rule atthe parameter server.</description><author>Connor Mclaughlin, Matthew Ding, Denis Edogmus, Lili Su</author><pubDate>Thu, 27 Jul 2023 16:46:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14952v1</guid></item><item><title>A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs</title><link>http://arxiv.org/abs/2307.14940v1</link><description>The continuous dynamics of natural systems has been effectively modelledusing Neural Ordinary Differential Equations (Neural ODEs). However, foraccurate and meaningful predictions, it is crucial that the models follow theunderlying rules or laws that govern these systems. In this work, we propose aself-adaptive penalty algorithm for Neural ODEs to enable modelling ofconstrained natural systems. The proposed self-adaptive penalty function candynamically adjust the penalty parameters. The explicit introduction of priorknowledge helps to increase the interpretability of Neural ODE -based models.We validate the proposed approach by modelling three natural systems with priorknowledge constraints: population growth, chemical reaction evolution, anddamped harmonic oscillator motion. The numerical experiments and a comparisonwith other penalty Neural ODE approaches and \emph{vanilla} Neural ODE,demonstrate the effectiveness of the proposed self-adaptive penalty algorithmfor Neural ODEs in modelling constrained natural systems. Moreover, theself-adaptive penalty approach provides more accurate and robust models withreliable and meaningful predictions.</description><author>C. Coelho, M. Fernanda P. Costa, L. L. Ferrás</author><pubDate>Thu, 27 Jul 2023 16:32:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14940v1</guid></item><item><title>Likely, Light, and Accurate Context-Free Clusters-based Trajectory Prediction</title><link>http://arxiv.org/abs/2307.14788v1</link><description>Autonomous systems in the road transportation network require intelligentmechanisms that cope with uncertainty to foresee the future. In this paper, wepropose a multi-stage probabilistic approach for trajectory forecasting:trajectory transformation to displacement space, clustering of displacementtime series, trajectory proposals, and ranking proposals. We introduce a newdeep feature clustering method, underlying self-conditioned GAN, which copesbetter with distribution shifts than traditional methods. Additionally, wepropose novel distance-based ranking proposals to assign probabilities to thegenerated trajectories that are more efficient yet accurate than an auxiliaryneural network. The overall system surpasses context-free deep generativemodels in human and road agents trajectory data while performing similarly topoint estimators when comparing the most probable trajectory.</description><author>Tiago Rodrigues de Almeida, Oscar Martinez Mozos</author><pubDate>Thu, 27 Jul 2023 12:29:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14788v1</guid></item><item><title>Towards Deeply Unified Depth-aware Panoptic Segmentation with Bi-directional Guidance Learning</title><link>http://arxiv.org/abs/2307.14786v1</link><description>Depth-aware panoptic segmentation is an emerging topic in computer visionwhich combines semantic and geometric understanding for more robust sceneinterpretation. Recent works pursue unified frameworks to tackle this challengebut mostly still treat it as two individual learning tasks, which limits theirpotential for exploring cross-domain information. We propose a deeply unifiedframework for depth-aware panoptic segmentation, which performs jointsegmentation and depth estimation both in a per-segment manner with identicalobject queries. To narrow the gap between the two tasks, we further design ageometric query enhancement method, which is able to integrate scene geometryinto object queries using latent representations. In addition, we propose abi-directional guidance learning approach to facilitate cross-task featurelearning by taking advantage of their mutual relations. Our method sets the newstate of the art for depth-aware panoptic segmentation on both Cityscapes-DVPSand SemKITTI-DVPS datasets. Moreover, our guidance learning approach is shownto deliver performance improvement even under incomplete supervision labels.</description><author>Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Jun-Yan He, Jin-Peng Lan, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Thu, 27 Jul 2023 12:28:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14786v1</guid></item><item><title>Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling Model</title><link>http://arxiv.org/abs/2307.14785v1</link><description>This paper presents a series of approaches aimed at enhancing the performanceof Aspect-Based Sentiment Analysis (ABSA) by utilizing extracted semanticinformation from a Semantic Role Labeling (SRL) model. We propose a novelend-to-end Semantic Role Labeling model that effectively captures most of thestructured semantic information within the Transformer hidden state. We believethat this end-to-end model is well-suited for our newly proposed models thatincorporate semantic information. We evaluate the proposed models in twolanguages, English and Czech, employing ELECTRA-small models. Our combinedmodels improve ABSA performance in both languages. Moreover, we achieved newstate-of-the-art results on the Czech ABSA.</description><author>Pavel Přibáň, Ondřej Pražák</author><pubDate>Thu, 27 Jul 2023 12:28:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14785v1</guid></item><item><title>Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset</title><link>http://arxiv.org/abs/2307.14783v1</link><description>We present a new large-scale emotion-labeled symbolic music datasetconsisting of 12k MIDI songs. To create this dataset, we first trained emotionclassification models on the GoEmotions dataset, achieving state-of-the-artresults with a model half the size of the baseline. We then applied thesemodels to lyrics from two large-scale MIDI datasets. Our dataset covers a widerange of fine-grained emotions, providing a valuable resource to explore theconnection between music and emotions and, especially, to develop models thatcan generate music based on specific emotions. Our code for inference, trainedmodels, and datasets are available online.</description><author>Serkan Sulun, Pedro Oliveira, Paula Viana</author><pubDate>Thu, 27 Jul 2023 12:24:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14783v1</guid></item><item><title>Exploring Effective Priors and Efficient Models for Weakly-Supervised Change Detection</title><link>http://arxiv.org/abs/2307.10853v3</link><description>Weakly-supervised change detection (WSCD) aims to detect pixel-level changeswith only image-level annotations. Owing to its label efficiency, WSCD isdrawing increasing attention recently. However, current WSCD methods oftenencounter the challenge of change missing and fabricating, i.e., theinconsistency between image-level annotations and pixel-level predictions.Specifically, change missing refer to the situation that the WSCD model failsto predict any changed pixels, even though the image-level label indicateschanged, and vice versa for change fabricating. To address this challenge, inthis work, we leverage global-scale and local-scale priors in WSCD and proposetwo components: a Dilated Prior (DP) decoder and a Label Gated (LG) constraint.The DP decoder decodes samples with the changed image-level label, skipssamples with the unchanged label, and replaces them with an all-unchangedpixel-level label. The LG constraint is derived from the correspondence betweenchanged representations and image-level labels, penalizing the model when itmispredicts the change status. Additionally, we develop TransWCD, a simple yetpowerful transformer-based model, showcasing the potential of weakly-supervisedlearning in change detection. By integrating the DP decoder and LG constraintinto TransWCD, we form TransWCD-DL. Our proposed TransWCD and TransWCD-DLachieve significant +6.33% and +9.55% F1 score improvements over thestate-of-the-art methods on the WHU-CD dataset, respectively. Some performancemetrics even exceed several fully-supervised change detection (FSCD)competitors. Code will be available athttps://github.com/zhenghuizhao/TransWCD.</description><author>Zhenghui Zhao, Lixiang Ru, Chen Wu</author><pubDate>Thu, 27 Jul 2023 12:24:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10853v3</guid></item><item><title>Contrastive Knowledge Amalgamation for Unsupervised Image Classification</title><link>http://arxiv.org/abs/2307.14781v1</link><description>Knowledge amalgamation (KA) aims to learn a compact student model to handlethe joint objective from multiple teacher models that are are specialized fortheir own tasks respectively. Current methods focus on coarsely aligningteachers and students in the common representation space, making it difficultfor the student to learn the proper decision boundaries from a set ofheterogeneous teachers. Besides, the KL divergence in previous works onlyminimizes the probability distribution difference between teachers and thestudent, ignoring the intrinsic characteristics of teachers. Therefore, wepropose a novel Contrastive Knowledge Amalgamation (CKA) framework, whichintroduces contrastive losses and an alignment loss to achieve intra-classcohesion and inter-class separation.Contrastive losses intra- and inter- modelsare designed to widen the distance between representations of differentclasses. The alignment loss is introduced to minimize the sample-leveldistribution differences of teacher-student models in the common representationspace.Furthermore, the student learns heterogeneous unsupervised classificationtasks through soft targets efficiently and flexibly in the task-levelamalgamation. Extensive experiments on benchmarks demonstrate thegeneralization capability of CKA in the amalgamation of specific task as wellas multiple tasks. Comprehensive ablation studies provide a further insightinto our CKA.</description><author>Shangde Gao, Yichao Fu, Ke Liu, Yuqiang Han</author><pubDate>Thu, 27 Jul 2023 12:21:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14781v1</guid></item><item><title>Distracting Downpour: Adversarial Weather Attacks for Motion Estimation</title><link>http://arxiv.org/abs/2305.06716v2</link><description>Current adversarial attacks on motion estimation, or optical flow, optimizesmall per-pixel perturbations, which are unlikely to appear in the real world.In contrast, adverse weather conditions constitute a much more realistic threatscenario. Hence, in this work, we present a novel attack on motion estimationthat exploits adversarially optimized particles to mimic weather effects likesnowflakes, rain streaks or fog clouds. At the core of our attack framework isa differentiable particle rendering system that integrates particles (i)consistently over multiple time steps (ii) into the 3D space (iii) with aphoto-realistic appearance. Through optimization, we obtain adversarial weatherthat significantly impacts the motion estimation. Surprisingly, methods thatpreviously showed good robustness towards small per-pixel perturbations areparticularly vulnerable to adversarial weather. At the same time, augmentingthe training with non-optimized weather increases a method's robustness towardsweather effects and improves generalizability at almost no additional cost. Ourcode will be available at https://github.com/cv-stuttgart/DistractingDownpour.</description><author>Jenny Schmalfuss, Lukas Mehl, Andrés Bruhn</author><pubDate>Thu, 27 Jul 2023 12:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06716v2</guid></item><item><title>MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data</title><link>http://arxiv.org/abs/2307.14778v1</link><description>Non-intrusive load monitoring (NILM) identifies the status and powerconsumption of various household appliances by disaggregating the total powerusage signal of an entire house. Efficient and accurate load monitoringfacilitates user profile establishment, intelligent household energymanagement, and peak load shifting. This is beneficial for both the end-usersand utilities by improving the overall efficiency of a power distributionnetwork. Existing approaches mainly focus on developing an individual model foreach appliance. Those approaches typically rely on a large amount ofhousehold-labeled data which is hard to collect. In this paper, we propose amulti-appliance-task framework with a training-efficient sample augmentation(SA) scheme that boosts the disaggregation performance with limited labeleddata. For each appliance, we develop a shared-hierarchical split structure forits regression and classification tasks. In addition, we also propose atwo-dimensional attention mechanism in order to capture spatio-temporalcorrelations among all appliances. With only one-day training data and limitedappliance operation profiles, the proposed SA algorithm can achieve comparabletest performance to the case of training with the full dataset. Finally,simulation results show that our proposed approach features a significantlyimproved performance over many baseline models. The relative errors can bereduced by more than 50\% on average. The codes of this work are available athttps://github.com/jxiong22/MATNilm</description><author>Jing Xiong, Tianqi Hong, Dongbo Zhao, Yu Zhang</author><pubDate>Thu, 27 Jul 2023 12:14:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14778v1</guid></item><item><title>pCTFusion: Point Convolution-Transformer Fusion with Semantic Aware Loss for Outdoor LiDAR Point Cloud Segmentation</title><link>http://arxiv.org/abs/2307.14777v1</link><description>LiDAR-generated point clouds are crucial for perceiving outdoor environments.The segmentation of point clouds is also essential for many applications.Previous research has focused on using self-attention and convolution (localattention) mechanisms individually in semantic segmentation architectures.However, there is limited work on combining the learned representations ofthese attention mechanisms to improve performance. Additionally, existingresearch that combines convolution with self-attention relies on globalattention, which is not practical for processing large point clouds. To addressthese challenges, this study proposes a new architecture, pCTFusion, whichcombines kernel-based convolutions and self-attention mechanisms for betterfeature learning and capturing local and global dependencies in segmentation.The proposed architecture employs two types of self-attention mechanisms, localand global, based on the hierarchical positions of the encoder blocks.Furthermore, the existing loss functions do not consider the semantic andposition-wise importance of the points, resulting in reduced accuracy,particularly at sharp class boundaries. To overcome this, the study models anovel attention-based loss function called Pointwise Geometric Anisotropy(PGA), which assigns weights based on the semantic distribution of points in aneighborhood. The proposed architecture is evaluated on SemanticKITTI outdoordataset and showed a 5-7% improvement in performance compared to thestate-of-the-art architectures. The results are particularly encouraging forminor classes, often misclassified due to class imbalance, lack of space, andneighbor-aware feature encoding. These developed methods can be leveraged forthe segmentation of complex datasets and can drive real-world applications ofLiDAR point cloud.</description><author>Abhishek Kuriyal, Vaibhav Kumar, Bharat Lohani</author><pubDate>Thu, 27 Jul 2023 12:12:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14777v1</guid></item><item><title>Learning Full-Head 3D GANs from a Single-View Portrait Dataset</title><link>http://arxiv.org/abs/2307.14770v1</link><description>33D-aware face generators are commonly trained on 2D real-life face imagedatasets. Nevertheless, existing facial recognition methods often struggle toextract face data captured from various camera angles. Furthermore, in-the-wildimages with diverse body poses introduce a high-dimensional challenge for3D-aware generators, making it difficult to utilize data that contains completeneck and shoulder regions. Consequently, these face image datasets oftencontain only near-frontal face data, which poses challenges for 3D-aware facegenerators to construct \textit{full-head} 3D portraits. To this end, we firstcreate the dataset {$\it{360}^{\circ}$}-\textit{Portrait}-\textit{HQ}(\textit{$\it{360}^{\circ}$PHQ}), which consists of high-quality single-viewreal portraits annotated with a variety of camera parameters {(the yaw anglesspan the entire $360^{\circ}$ range)} and body poses. We then propose\textit{3DPortraitGAN}, the first 3D-aware full-head portrait generator thatlearns a canonical 3D avatar distribution from the body-pose-various\textit{$\it{360}^{\circ}$PHQ} dataset with body pose self-learning. Our modelcan generate view-consistent portrait images from all camera angles(${360}^{\circ}$) with a full-head 3D representation. We incorporate amesh-guided deformation field into volumetric rendering to produce deformedresults to generate portrait images that conform to the body pose distributionof the dataset using our canonical generator. We integrate two pose predictorsinto our framework to predict more accurate body poses to address the issue ofinaccurately estimated body poses in our dataset. Our experiments show that theproposed framework can generate view-consistent, realistic portrait images withcomplete geometry from all camera angles and accurately predict portrait bodypose.</description><author>Yiqian Wu, Hao Xu, Xiangjun Tang, Hongbo Fu, Xiaogang Jin</author><pubDate>Thu, 27 Jul 2023 12:02:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14770v1</guid></item><item><title>Gloss-free Sign Language Translation: Improving from Visual-Language Pretraining</title><link>http://arxiv.org/abs/2307.14768v1</link><description>Sign Language Translation (SLT) is a challenging task due to its cross-domainnature, involving the translation of visual-gestural language to text. Manyprevious methods employ an intermediate representation, i.e., gloss sequences,to facilitate SLT, thus transforming it into a two-stage task of sign languagerecognition (SLR) followed by sign language translation (SLT). However, thescarcity of gloss-annotated sign language data, combined with the informationbottleneck in the mid-level gloss representation, has hindered the furtherdevelopment of the SLT task. To address this challenge, we propose a novelGloss-Free SLT based on Visual-Language Pretraining (GFSLT-VLP), which improvesSLT by inheriting language-oriented prior knowledge from pre-trained models,without any gloss annotation assistance. Our approach involves two stages: (i)integrating Contrastive Language-Image Pre-training (CLIP) with maskedself-supervised learning to create pre-tasks that bridge the semantic gapbetween visual and textual representations and restore masked sentences, and(ii) constructing an end-to-end architecture with an encoder-decoder-likestructure that inherits the parameters of the pre-trained Visual Encoder andText Decoder from the first stage. The seamless combination of these noveldesigns forms a robust sign language representation and significantly improvesgloss-free sign language translation. In particular, we have achievedunprecedented improvements in terms of BLEU-4 score on the PHOENIX14T dataset(&gt;+5) and the CSL-Daily dataset (&gt;+3) compared to state-of-the-art gloss-freeSLT methods. Furthermore, our approach also achieves competitive results on thePHOENIX14T dataset when compared with most of the gloss-based methods. Our codeis available at https://github.com/zhoubenjia/GFSLT-VLP.</description><author>Benjia Zhou, Zhigang Chen, Albert Clapés, Jun Wan, Yanyan Liang, Sergio Escalera, Zhen Lei, Du Zhang</author><pubDate>Thu, 27 Jul 2023 11:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14768v1</guid></item><item><title>GeoUDF: Surface Reconstruction from 3D Point Clouds via Geometry-guided Distance Representation</title><link>http://arxiv.org/abs/2211.16762v4</link><description>We present a learning-based method, namely GeoUDF,to tackle the long-standingand challenging problem of reconstructing a discrete surface from a sparsepoint cloud.To be specific, we propose a geometry-guided learning method forUDF and its gradient estimation that explicitly formulates the unsigneddistance of a query point as the learnable affine averaging of its distances tothe tangent planes of neighboring points on the surface. Besides,we model thelocal geometric structure of the input point clouds by explicitly learning aquadratic polynomial for each point. This not only facilitates upsampling theinput sparse point cloud but also naturally induces unoriented normal, whichfurther augments UDF estimation. Finally, to extract triangle meshes from thepredicted UDF we propose a customized edge-based marching cube module. Weconduct extensive experiments and ablation studies to demonstrate thesignificant advantages of our method over state-of-the-art methods in terms ofreconstruction accuracy, efficiency, and generality. The source code ispublicly available at https://github.com/rsy6318/GeoUDF.</description><author>Siyu Ren, Junhui Hou, Xiaodong Chen, Ying He, Wenping Wang</author><pubDate>Thu, 27 Jul 2023 11:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16762v4</guid></item><item><title>Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items</title><link>http://arxiv.org/abs/2307.13709v2</link><description>Many properties in the real world, such as desirability or strength incompetitive environment, can't be directly observed, which makes them difficultto evaluate. To deal with this challenging problem, prior works have primarilyfocused on estimating those properties of known items, especially the strengthof sports players, only of those who appears in paired comparison dataset. Inthis paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML frameworkto evaluate any properties of unknown items, not necessarily present in thetraining data. Our method seamlessly integrates traditional Bradley-Terry modelwith a neural network structure. We also generalizes this architecture furtherfor asymmetric environment with unfairness, which is much more common in realworld settings. In our experimental analysis, DBTR successfully learned desiredquantification of those properties.</description><author>Satoru Fujii</author><pubDate>Thu, 27 Jul 2023 11:37:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13709v2</guid></item><item><title>Towards Practicable Sequential Shift Detectors</title><link>http://arxiv.org/abs/2307.14758v1</link><description>There is a growing awareness of the harmful effects of distribution shift onthe performance of deployed machine learning models. Consequently, there is agrowing interest in detecting these shifts before associated costs have time toaccumulate. However, desiderata of crucial importance to the practicabledeployment of sequential shift detectors are typically overlooked by existingworks, precluding their widespread adoption. We identify three such desiderata,highlight existing works relevant to their satisfaction, and recommendimpactful directions for future research.</description><author>Oliver Cobb, Arnaud Van Looveren</author><pubDate>Thu, 27 Jul 2023 11:33:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14758v1</guid></item><item><title>Fair Machine Unlearning: Data Removal while Mitigating Disparities</title><link>http://arxiv.org/abs/2307.14754v1</link><description>As public consciousness regarding the collection and use of personalinformation by corporations grows, it is of increasing importance thatconsumers be active participants in the curation of corporate datasets. Inlight of this, data governance frameworks such as the General Data ProtectionRegulation (GDPR) have outlined the right to be forgotten as a key principleallowing individuals to request that their personal data be deleted from thedatabases and models used by organizations. To achieve forgetting in practice,several machine unlearning methods have been proposed to address thecomputational inefficiencies of retraining a model from scratch with eachunlearning request. While efficient online alternatives to retraining, it isunclear how these methods impact other properties critical to real-worldapplications, such as fairness. In this work, we propose the first fair machineunlearning method that can provably and efficiently unlearn data instanceswhile preserving group fairness. We derive theoretical results whichdemonstrate that our method can provably unlearn data instances whilemaintaining fairness objectives. Extensive experimentation with real-worlddatasets highlight the efficacy of our method at unlearning data instanceswhile preserving fairness.</description><author>Alex Oesterling, Jiaqi Ma, Flavio P. Calmon, Hima Lakkaraju</author><pubDate>Thu, 27 Jul 2023 11:26:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14754v1</guid></item><item><title>Pre-Training with Diffusion models for Dental Radiography segmentation</title><link>http://arxiv.org/abs/2307.14066v2</link><description>Medical radiography segmentation, and specifically dental radiography, ishighly limited by the cost of labeling which requires specific expertise andlabor-intensive annotations. In this work, we propose a straightforwardpre-training method for semantic segmentation leveraging Denoising DiffusionProbabilistic Models (DDPM), which have shown impressive results for generativemodeling. Our straightforward approach achieves remarkable performance in termsof label efficiency and does not require architectural modifications betweenpre-training and downstream tasks. We propose to first pre-train a Unet byexploiting the DDPM training objective, and then fine-tune the resulting modelon a segmentation task. Our experimental results on the segmentation of dentalradiographs demonstrate that the proposed method is competitive withstate-of-the-art pre-training methods.</description><author>Jérémy Rousseau, Christian Alaka, Emma Covili, Hippolyte Mayard, Laura Misrachi, Willy Au</author><pubDate>Thu, 27 Jul 2023 11:22:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14066v2</guid></item><item><title>FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks</title><link>http://arxiv.org/abs/2307.14751v1</link><description>We propose FLARE, the first fingerprinting mechanism to verify whether asuspected Deep Reinforcement Learning (DRL) policy is an illegitimate copy ofanother (victim) policy. We first show that it is possible to findnon-transferable, universal adversarial masks, i.e., perturbations, to generateadversarial examples that can successfully transfer from a victim policy to itsmodified versions but not to independently trained policies. FLARE employsthese masks as fingerprints to verify the true ownership of stolen DRL policiesby measuring an action agreement value over states perturbed via such masks.Our empirical evaluations show that FLARE is effective (100% action agreementon stolen copies) and does not falsely accuse independent policies (no falsepositives). FLARE is also robust to model modification attacks and cannot beeasily evaded by more informed adversaries without negatively impacting agentperformance. We also show that not all universal adversarial masks are suitablecandidates for fingerprints due to the inherent characteristics of DRLpolicies. The spatio-temporal dynamics of DRL problems and sequentialdecision-making process make characterizing the decision boundary of DRLpolicies more difficult, as well as searching for universal masks that capturethe geometry of it.</description><author>Buse G. A. Tekgul, N. Asokan</author><pubDate>Thu, 27 Jul 2023 11:19:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14751v1</guid></item><item><title>Empower Your Model with Longer and Better Context Comprehension</title><link>http://arxiv.org/abs/2307.13365v2</link><description>Recently, with the emergence of numerous Large Language Models (LLMs), theimplementation of AI has entered a new era. Irrespective of these models' owncapacity and structure, there is a growing demand for LLMs to possess enhancedcomprehension of longer and more complex contexts with relatively smallersizes. Models often encounter an upper limit when processing sequences ofsentences that extend beyond their comprehension capacity and result inoff-topic or even chaotic responses. While several recent works attempt toaddress this issue in various ways, they rarely focus on "why models are unableto compensate or strengthen their capabilities on their own". In this paper, wethoroughly investigate the nature of information transfer within LLMs andpropose a novel technique called Attention Transition. This technique empowersmodels to achieve longer and better context comprehension with minimaladditional training or impact on generation fluency. Our experiments areconducted on the challenging XSum dataset using LLaMa-7b model with contexttoken length ranging from 800 to 1900. Results demonstrate that we achievesubstantial improvements compared with the original generation resultsevaluated by GPT4.</description><author>Yifei Gao, Lei Wang, Jun Fang, Longhua Hu, Jun Cheng</author><pubDate>Thu, 27 Jul 2023 11:17:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13365v2</guid></item><item><title>Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation</title><link>http://arxiv.org/abs/2307.14750v1</link><description>Training an image captioner without annotated image-sentence pairs has gainedtraction in recent years. Previous approaches can be categorized into twostrategies: crawling sentences from mismatching corpora and aligning them withthe given images as pseudo annotations, or pre-training the captioner usingexternal image-text pairs. However, the aligning setting seems to reach itsperformance limit due to the quality problem of pairs, and pre-trainingrequires significant computational resources. To address these challenges, wepropose a new strategy ``LPM + retrieval-augmented learning" where the priorknowledge from large pre-trained models (LPMs) is leveraged as supervision, anda retrieval process is integrated to further reinforce its effectiveness.Specifically, we introduce Retrieval-augmented Pseudo Sentence Generation(RaPSG), which adopts an efficient approach to retrieve highly relevant shortregion descriptions from the mismatching corpora and use them to generate avariety of pseudo sentences with distinct representations as well as highquality via LPMs. In addition, a fluency filter and a CLIP-guided trainingobjective are further introduced to facilitate model optimization. Experimentalresults demonstrate that our method surpasses the SOTA pre-training model(Flamingo3B) by achieving a CIDEr score of 78.1 (+5.1) while utilizing only0.3% of its trainable parameters (1.3B VS 33M). Importantly, our approacheliminates the need of computationally expensive pre-training processes onexternal datasets (e.g., the requirement of 312M image-text pairs forFlamingo3B). We further show that with a simple extension, the generated pseudosentences can be deployed as weak supervision to boost the 1% semi-supervisedimage caption benchmark up to 93.4 CIDEr score (+8.9) which showcases theversatility and effectiveness of our approach.</description><author>Zhiyuan Li, Dongnan Liu, Heng Wang, Chaoyi Zhang, Weidong Cai</author><pubDate>Thu, 27 Jul 2023 11:16:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14750v1</guid></item><item><title>Semantic Image Completion and Enhancement using GANs</title><link>http://arxiv.org/abs/2307.14748v1</link><description>Semantic inpainting or image completion alludes to the task of inferringarbitrary large missing regions in images based on image semantics. Since theprediction of image pixels requires an indication of high-level context, thismakes it significantly tougher than image completion, which is often moreconcerned with correcting data corruption and removing entire objects from theinput image. On the other hand, image enhancement attempts to eliminateunwanted noise and blur from the image, along with sustaining most of the imagedetails. Efficient image completion and enhancement model should be able torecover the corrupted and masked regions in images and then refine the imagefurther to increase the quality of the output image. Generative AdversarialNetworks (GAN), have turned out to be helpful in picture completion tasks. Inthis chapter, we will discuss the underlying GAN architecture and how they canbe used used for image completion tasks.</description><author>Priyansh Saxena, Raahat Gupta, Akshat Maheshwari, Saumil Maheshwari</author><pubDate>Thu, 27 Jul 2023 11:12:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14748v1</guid></item><item><title>Experimental Study on Reinforcement Learning-based Control of an Acrobot</title><link>http://arxiv.org/abs/2011.09246v2</link><description>We present computational and experimental results on how artificialintelligence (AI) learns to control an Acrobot using reinforcement learning(RL). Thereby the experimental setup is designed as an embedded system, whichis of interest for robotics and energy harvesting applications. Specifically,we study the control of angular velocity of the Acrobot, as well as control ofits total energy, which is the sum of the kinetic and the potential energy. Bythis means the RL algorithm is designed to drive the angular velocity or theenergy of the first pendulum of the Acrobot towards a desired value. With this,libration or full rotation of the unactuated pendulum of the Acrobot isachieved. Moreover, investigations of the Acrobot control are carried out,which lead to insights about the influence of the state space discretization,the episode length, the action space or the mass of the driven pendulum on theRL control. By further numerous simulations and experiments the effects ofparameter variations are evaluated.</description><author>Leo Dostal, Alexej Bespalko, Daniel A. Duecker</author><pubDate>Thu, 27 Jul 2023 11:09:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.09246v2</guid></item><item><title>Fine-Tuned but Zero-Shot 3D Shape Sketch View Similarity and Retrieval</title><link>http://arxiv.org/abs/2306.08541v2</link><description>Recently, encoders like ViT (vision transformer) and ResNet have been trainedon vast datasets and utilized as perceptual metrics for comparing sketches andimages, as well as multi-domain encoders in a zero-shot setting. However, therehas been limited effort to quantify the granularity of these encoders. Our workaddresses this gap by focusing on multi-modal 2D projections of individual 3Dinstances. This task holds crucial implications for retrieval and sketch-basedmodeling. We show that in a zero-shot setting, the more abstract the sketch,the higher the likelihood of incorrect image matches. Even within the samesketch domain, sketches of the same object drawn in different styles, forexample by distinct individuals, might not be accurately matched. One of thekey findings of our research is that meticulous fine-tuning on one class of 3Dshapes can lead to improved performance on other shape classes, reaching orsurpassing the accuracy of supervised methods. We compare and discuss severalfine-tuning strategies. Additionally, we delve deeply into how the scale of anobject in a sketch influences the similarity of features at different networklayers, helping us identify which network layers provide the most accuratematching. Significantly, we discover that ViT and ResNet perform best whendealing with similar object scales. We believe that our work will have asignificant impact on research in the sketch domain, providing insights andguidance on how to adopt large pretrained models as perceptual losses.</description><author>Gianluca Berardi, Yulia Gryaditskaya</author><pubDate>Thu, 27 Jul 2023 11:07:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08541v2</guid></item><item><title>A Confidence-based Partial Label Learning Model for Crowd-Annotated Named Entity Recognition</title><link>http://arxiv.org/abs/2305.12485v2</link><description>Existing models for named entity recognition (NER) are mainly based onlarge-scale labeled datasets, which always obtain using crowdsourcing. However,it is hard to obtain a unified and correct label via majority voting frommultiple annotators for NER due to the large labeling space and complexity ofthis task. To address this problem, we aim to utilize the originalmulti-annotator labels directly. Particularly, we propose a Confidence-basedPartial Label Learning (CPLL) method to integrate the prior confidence (givenby annotators) and posterior confidences (learned by models) forcrowd-annotated NER. This model learns a token- and content-dependentconfidence via an Expectation-Maximization (EM) algorithm by minimizingempirical risk. The true posterior estimator and confidence estimator performiteratively to update the true posterior and confidence respectively. Weconduct extensive experimental results on both real-world and syntheticdatasets, which show that our model can improve performance effectivelycompared with strong baselines.</description><author>Limao Xiong, Jie Zhou, Qunxi Zhu, Xiao Wang, Yuanbin Wu, Qi Zhang, Tao Gui, Xuanjing Huang, Jin Ma, Ying Shan</author><pubDate>Thu, 27 Jul 2023 11:06:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12485v2</guid></item><item><title>Turning Whisper into Real-Time Transcription System</title><link>http://arxiv.org/abs/2307.14743v1</link><description>Whisper is one of the recent state-of-the-art multilingual speech recognitionand translation models, however, it is not designed for real timetranscription. In this paper, we build on top of Whisper and createWhisper-Streaming, an implementation of real-time speech transcription andtranslation of Whisper-like models. Whisper-Streaming uses local agreementpolicy with self-adaptive latency to enable streaming transcription. We showthat Whisper-Streaming achieves high quality and 3.3 seconds latency onunsegmented long-form speech transcription test set, and we demonstrate itsrobustness and practical usability as a component in live transcription serviceat a multilingual conference.</description><author>Dominik Macháček, Raj Dabre, Ondřej Bojar</author><pubDate>Thu, 27 Jul 2023 11:00:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14743v1</guid></item><item><title>The Impact of Partial Occlusion on Pedestrian Detectability</title><link>http://arxiv.org/abs/2205.04812v6</link><description>Robust detection of vulnerable road users is a safety critical requirementfor the deployment of autonomous vehicles in heterogeneous traffic. One of themost complex outstanding challenges is that of partial occlusion where a targetobject is only partially available to the sensor due to obstruction by anotherforeground object. A number of leading pedestrian detection benchmarks provideannotation for partial occlusion, however each benchmark varies greatly intheir definition of the occurrence and severity of occlusion. Recent researchdemonstrates that a high degree of subjectivity is used to classify occlusionlevel in these cases and occlusion is typically categorized into 2 to 3 broadcategories such as partially and heavily occluded. This can lead to inaccurateor inconsistent reporting of pedestrian detection model performance dependingon which benchmark is used. This research introduces a novel, objectivebenchmark for partially occluded pedestrian detection to facilitate theobjective characterization of pedestrian detection models. Characterization iscarried out on seven popular pedestrian detection models for a range ofocclusion levels from 0-99%, in order to demonstrate the efficacy and increasedanalysis capabilities of the proposed characterization method. Resultsdemonstrate that pedestrian detection performance degrades, and the number offalse negative detections increase as pedestrian occlusion level increases. Ofthe seven popular pedestrian detection routines characterized, CenterNet hasthe greatest overall performance, followed by SSDlite. RetinaNet has the lowestoverall detection performance across the range of occlusion levels.</description><author>Shane Gilroy, Darragh Mullins, Edward Jones, Ashkan Parsi, Martin Glavin</author><pubDate>Thu, 27 Jul 2023 10:57:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.04812v6</guid></item><item><title>Semantic-Aware Dual Contrastive Learning for Multi-label Image Classification</title><link>http://arxiv.org/abs/2307.09715v2</link><description>Extracting image semantics effectively and assigning corresponding labels tomultiple objects or attributes for natural images is challenging due to thecomplex scene contents and confusing label dependencies. Recent works havefocused on modeling label relationships with graph and understanding objectregions using class activation maps (CAM). However, these methods ignore thecomplex intra- and inter-category relationships among specific semanticfeatures, and CAM is prone to generate noisy information. To this end, wepropose a novel semantic-aware dual contrastive learning framework thatincorporates sample-to-sample contrastive learning (SSCL) as well asprototype-to-sample contrastive learning (PSCL). Specifically, we leveragesemantic-aware representation learning to extract category-related localdiscriminative features and construct category prototypes. Then based on SSCL,label-level visual representations of the same category are aggregatedtogether, and features belonging to distinct categories are separated.Meanwhile, we construct a novel PSCL module to narrow the distance betweenpositive samples and category prototypes and push negative samples away fromthe corresponding category prototypes. Finally, the discriminative label-levelfeatures related to the image content are accurately captured by the jointtraining of the above three parts. Experiments on five challenging large-scalepublic datasets demonstrate that our proposed method is effective andoutperforms the state-of-the-art methods. Code and supplementary materials arereleased on https://github.com/yu-gi-oh-leilei/SADCL.</description><author>Leilei Ma, Dengdi Sun, Lei Wang, Haifeng Zhao, Bin Luo</author><pubDate>Thu, 27 Jul 2023 10:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09715v2</guid></item><item><title>New Interaction Paradigm for Complex EDA Software Leveraging GPT</title><link>http://arxiv.org/abs/2307.14740v1</link><description>In the rapidly growing field of electronic design automation (EDA),professional software such as KiCad, Cadence , and Altium Designer provideincreasingly extensive design functionalities. However, the intricate commandstructure and high learning curve create a barrier, particularly for noviceprinted circuit board (PCB) designers. This results in difficulties inselecting appropriate functions or plugins for varying design purposes,compounded by the lack of intuitive learning methods beyond traditionaldocumentation, videos, and online forums. To address this challenge, anartificial intelligence (AI) interaction assist plugin for EDA software namedSmartonAl is developed here, also KiCad is taken as the first example.SmartonAI is inspired by the HuggingGPT framework and employs large languagemodels, such as GPT and BERT, to facilitate task planning and execution. Onreceiving a designer request, SmartonAI conducts a task breakdown andefficiently executes relevant subtasks, such as analysis of help documentationparagraphs and execution of different plugins, along with leveraging thebuilt-in schematic and PCB manipulation functions in both SmartonAl itself andsoftware. Our preliminary results demonstrate that SmartonAI can significantlystreamline the PCB design process by simplifying complex commands intointuitive language-based interactions. By harnessing the powerful languagecapabilities of ChatGPT and the rich design functions of KiCad, the plugineffectively bridges the gap between complex EDA software and user-friendlyinteraction. Meanwhile, the new paradigm behind SmartonAI can also extend toother complex software systems, illustrating the immense potential ofAI-assisted user interfaces in advancing digital interactions across variousdomains.</description><author>Boyu Han, Xinyu Wang, Yifan Wang, Junyu Yan, Yidong Tian</author><pubDate>Thu, 27 Jul 2023 10:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14740v1</guid></item><item><title>On Learning the Tail Quantiles of Driving Behavior Distributions via Quantile Regression and Flows</title><link>http://arxiv.org/abs/2305.13106v2</link><description>Towards safe autonomous driving (AD), we consider the problem of learningmodels that accurately capture the diversity and tail quantiles of human driverbehavior probability distributions, in interaction with an AD vehicle. Suchmodels, which predict drivers' continuous actions from their states, areparticularly relevant for closing the gap between AD agent simulations andreality. To this end, we adapt two flexible quantile learning frameworks forthis setting that avoid strong distributional assumptions: (1) quantileregression (based on the titled absolute loss), and (2) autoregressive quantileflows (a version of normalizing flows). Training happens in a behaviorcloning-fashion. We use the highD dataset consisting of driver trajectories onseveral highways. We evaluate our approach in a one-step accelerationprediction task, and in multi-step driver simulation rollouts. We reportquantitative results using the tilted absolute loss as metric, give qualitativeexamples showing that realistic extremal behavior can be learned, and discussthe main insights.</description><author>Jia Yu Tee, Oliver De Candido, Wolfgang Utschick, Philipp Geiger</author><pubDate>Thu, 27 Jul 2023 10:43:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13106v2</guid></item><item><title>Test Time Adaptation for Blind Image Quality Assessment</title><link>http://arxiv.org/abs/2307.14735v1</link><description>While the design of blind image quality assessment (IQA) algorithms hasimproved significantly, the distribution shift between the training and testingscenarios often leads to a poor performance of these methods at inference time.This motivates the study of test time adaptation (TTA) techniques to improvetheir performance at inference time. Existing auxiliary tasks and lossfunctions used for TTA may not be relevant for quality-aware adaptation of thepre-trained model. In this work, we introduce two novel quality-relevantauxiliary tasks at the batch and sample levels to enable TTA for blind IQA. Inparticular, we introduce a group contrastive loss at the batch level and arelative rank loss at the sample level to make the model quality aware andadapt to the target data. Our experiments reveal that even using a small batchof images from the test distribution helps achieve significant improvement inperformance by updating the batch normalization statistics of the source model.</description><author>Subhadeep Roy, Shankhanil Mitra, Soma Biswas, Rajiv Soundararajan</author><pubDate>Thu, 27 Jul 2023 10:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14735v1</guid></item><item><title>A Strategic Framework for Optimal Decisions in Football 1-vs-1 Shot-Taking Situations: An Integrated Approach of Machine Learning, Theory-Based Modeling, and Game Theory</title><link>http://arxiv.org/abs/2307.14732v1</link><description>Complex interactions between two opposing agents frequently occur in domainsof machine learning, game theory, and other application domains. Quantitativelyanalyzing the strategies involved can provide an objective basis fordecision-making. One such critical scenario is shot-taking in football, wheredecisions, such as whether the attacker should shoot or pass the ball andwhether the defender should attempt to block the shot, play a crucial role inthe outcome of the game. However, there are currently no effective data-drivenand/or theory-based approaches to analyzing such situations. To address thisissue, we proposed a novel framework to analyze such scenarios based on gametheory, where we estimate the expected payoff with machine learning (ML)models, and additional features for ML models were extracted with atheory-based shot block model. Conventionally, successes or failures (1 or 0)are used as payoffs, while a success shot (goal) is extremely rare in football.Therefore, we proposed the Expected Probability of Shot On Target (xSOT) metricto evaluate players' actions even if the shot results in no goal; this allowsfor effective differentiation and comparison between different shots and evenenables counterfactual shot situation analysis. In our experiments, we havevalidated the framework by comparing it with baseline and ablated models.Furthermore, we have observed a high correlation between the xSOT and existingmetrics. This alignment of information suggests that xSOT provides valuableinsights. Lastly, as an illustration, we studied optimal strategies in theWorld Cup 2022 and analyzed a shot situation in EURO 2020.</description><author>Calvin C. K. Yeung, Keisuke Fujii</author><pubDate>Thu, 27 Jul 2023 10:42:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14732v1</guid></item><item><title>ThoughtSource: A central hub for large language model reasoning data</title><link>http://arxiv.org/abs/2301.11596v5</link><description>Large language models (LLMs) such as GPT-4 have recently demonstratedimpressive results across a wide range of tasks. LLMs are still limited,however, in that they frequently fail at complex reasoning, their reasoningprocesses are opaque, they are prone to 'hallucinate' facts, and there areconcerns about their underlying biases. Letting models verbalize reasoningsteps as natural language, a technique known as chain-of-thought prompting, hasrecently been proposed as a way to address some of these issues. Here wepresent ThoughtSource, a meta-dataset and software library for chain-of-thought(CoT) reasoning. The goal of ThoughtSource is to improve future artificialintelligence systems by facilitating qualitative understanding of CoTs,enabling empirical evaluations, and providing training data. This first releaseof ThoughtSource integrates seven scientific/medical, three general-domain andfive math word question answering datasets.</description><author>Simon Ott, Konstantin Hebenstreit, Valentin Liévin, Christoffer Egeberg Hother, Milad Moradi, Maximilian Mayrhauser, Robert Praas, Ole Winther, Matthias Samwald</author><pubDate>Thu, 27 Jul 2023 10:37:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11596v5</guid></item><item><title>USTC FLICAR: A Sensors Fusion Dataset of LiDAR-Inertial-Camera for Heavy-duty Autonomous Aerial Work Robots</title><link>http://arxiv.org/abs/2304.01986v2</link><description>In this paper, we present the USTC FLICAR Dataset, which is dedicated to thedevelopment of simultaneous localization and mapping and precise 3Dreconstruction of the workspace for heavy-duty autonomous aerial work robots.In recent years, numerous public datasets have played significant roles in theadvancement of autonomous cars and unmanned aerial vehicles (UAVs). However,these two platforms differ from aerial work robots: UAVs are limited in theirpayload capacity, while cars are restricted to two-dimensional movements. Tofill this gap, we create the "Giraffe" mapping robot based on a bucket truck,which is equipped with a variety of well-calibrated and synchronized sensors:four 3D LiDARs, two stereo cameras, two monocular cameras, Inertial MeasurementUnits (IMUs), and a GNSS/INS system. A laser tracker is used to record themillimeter-level ground truth positions. We also make its ground twin, the"Okapi" mapping robot, to gather data for comparison. The proposed datasetextends the typical autonomous driving sensing suite to aerial scenes,demonstrating the potential of combining autonomous driving perception systemswith bucket trucks to create a versatile autonomous aerial working platform.Moreover, based on the Segment Anything Model (SAM), we produce the SemanticFLICAR dataset, which provides fine-grained semantic segmentation annotationsfor multimodal continuous data in both temporal and spatial dimensions. Thedataset is available for download at: https://ustc-flicar.github.io/.</description><author>Ziming Wang, Yujiang Liu, Yifan Duan, Xingchen Li, Xinran Zhang, Jianmin Ji, Erbao Dong, Yanyong Zhang</author><pubDate>Thu, 27 Jul 2023 10:37:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01986v2</guid></item><item><title>Understanding Silent Failures in Medical Image Classification</title><link>http://arxiv.org/abs/2307.14729v1</link><description>To ensure the reliable use of classification systems in medical applications,it is crucial to prevent silent failures. This can be achieved by eitherdesigning classifiers that are robust enough to avoid failures in the firstplace, or by detecting remaining failures using confidence scoring functions(CSFs). A predominant source of failures in image classification isdistribution shifts between training data and deployment data. To understandthe current state of silent failure prevention in medical imaging, we conductthe first comprehensive analysis comparing various CSFs in four biomedicaltasks and a diverse range of distribution shifts. Based on the result that noneof the benchmarked CSFs can reliably prevent silent failures, we conclude thata deeper understanding of the root causes of failures in the data is required.To facilitate this, we introduce SF-Visuals, an interactive analysis tool thatuses latent space clustering to visualize shifts and failures. On the basis ofvarious examples, we demonstrate how this tool can help researchers gaininsight into the requirements for safe application of classification systems inthe medical domain. The open-source benchmark and tool are at:https://github.com/IML-DKFZ/sf-visuals.</description><author>Till J. Bungert, Levin Kobelke, Paul F. Jaeger</author><pubDate>Thu, 27 Jul 2023 10:35:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14729v1</guid></item><item><title>P2C: Self-Supervised Point Cloud Completion from Single Partial Clouds</title><link>http://arxiv.org/abs/2307.14726v1</link><description>Point cloud completion aims to recover the complete shape based on a partialobservation. Existing methods require either complete point clouds or multiplepartial observations of the same object for learning. In contrast to previousapproaches, we present Partial2Complete (P2C), the first self-supervisedframework that completes point cloud objects using training samples consistingof only a single incomplete point cloud per object. Specifically, our frameworkgroups incomplete point clouds into local patches as input and predicts maskedpatches by learning prior information from different partial objects. We alsopropose Region-Aware Chamfer Distance to regularize shape mismatch withoutlimiting completion capability, and devise the Normal Consistency Constraint toincorporate a local planarity assumption, encouraging the recovered shapesurface to be continuous and complete. In this way, P2C no longer needsmultiple observations or complete point clouds as ground truth. Instead,structural cues are learned from a category-specific dataset to completepartial point clouds of objects. We demonstrate the effectiveness of ourapproach on both synthetic ShapeNet data and real-world ScanNet data, showingthat P2C produces comparable results to methods trained with complete shapes,and outperforms methods learned with multiple partial observations. Code isavailable at https://github.com/CuiRuikai/Partial2Complete.</description><author>Ruikai Cui, Shi Qiu, Saeed Anwar, Jiawei Liu, Chaoyue Xing, Jing Zhang, Nick Barnes</author><pubDate>Thu, 27 Jul 2023 10:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14726v1</guid></item><item><title>vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-level Representations in Medical Images</title><link>http://arxiv.org/abs/2307.14725v1</link><description>This paper introduces vox2vec - a contrastive method for self-supervisedlearning (SSL) of voxel-level representations. vox2vec representations aremodeled by a Feature Pyramid Network (FPN): a voxel representation is aconcatenation of the corresponding feature vectors from different pyramidlevels. The FPN is pre-trained to produce similar representations for the samevoxel in different augmented contexts and distinctive representations fordifferent voxels. This results in unified multi-scale representations thatcapture both global semantics (e.g., body part) and local semantics (e.g.,different small organs or healthy versus tumor tissue). We use vox2vec topre-train a FPN on more than 6500 publicly available computed tomographyimages. We evaluate the pre-trained representations by attaching simple headson top of them and training the resulting models for 22 segmentation tasks. Weshow that vox2vec outperforms existing medical imaging SSL techniques in threeevaluation setups: linear and non-linear probing and end-to-end fine-tuning.Moreover, a non-linear head trained on top of the frozen vox2vecrepresentations achieves competitive performance with the FPN trained fromscratch while having 50 times fewer trainable parameters. The code is availableat https://github.com/mishgon/vox2vec .</description><author>Mikhail Goncharov, Vera Soboleva, Anvar Kurmukov, Maxim Pisov, Mikhail Belyaev</author><pubDate>Thu, 27 Jul 2023 10:30:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14725v1</guid></item><item><title>Group Equivariant Fourier Neural Operators for Partial Differential Equations</title><link>http://arxiv.org/abs/2306.05697v2</link><description>We consider solving partial differential equations (PDEs) with Fourier neuraloperators (FNOs), which operate in the frequency domain. Since the laws ofphysics do not depend on the coordinate system used to describe them, it isdesirable to encode such symmetries in the neural operator architecture forbetter performance and easier learning. While encoding symmetries in thephysical domain using group theory has been studied extensively, how to capturesymmetries in the frequency domain is under-explored. In this work, we extendgroup convolutions to the frequency domain and design Fourier layers that areequivariant to rotations, translations, and reflections by leveraging theequivariance property of the Fourier transform. The resulting $G$-FNOarchitecture generalizes well across input resolutions and performs well insettings with varying levels of symmetry. Our code is publicly available aspart of the AIRS library (https://github.com/divelab/AIRS).</description><author>Jacob Helwig, Xuan Zhang, Cong Fu, Jerry Kurtin, Stephan Wojtowytsch, Shuiwang Ji</author><pubDate>Thu, 27 Jul 2023 10:24:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05697v2</guid></item><item><title>Learning fixed points of recurrent neural networks by reparameterizing the network model</title><link>http://arxiv.org/abs/2307.06732v2</link><description>In computational neuroscience, fixed points of recurrent neural networks arecommonly used to model neural responses to static or slowly changing stimuli.These applications raise the question of how to train the weights in arecurrent neural network to minimize a loss function evaluated on fixed points.A natural approach is to use gradient descent on the Euclidean space ofsynaptic weights. We show that this approach can lead to poor learningperformance due, in part, to singularities that arise in the loss surface. Weuse a reparameterization of the recurrent network model to derive twoalternative learning rules that produces more robust learning dynamics. We showthat these learning rules can be interpreted as steepest descent and gradientdescent, respectively, under a non-Euclidean metric on the space of recurrentweights. Our results question the common, implicit assumption that learning inthe brain should be expected to follow the negative Euclidean gradient ofsynaptic weights.</description><author>Vicky Zhu, Robert Rosenbaum</author><pubDate>Thu, 27 Jul 2023 10:23:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06732v2</guid></item><item><title>EFLNet: Enhancing Feature Learning for Infrared Small Target Detection</title><link>http://arxiv.org/abs/2307.14723v1</link><description>Single-frame infrared small target detection is considered to be achallenging task, due to the extreme imbalance between target and background,bounding box regression is extremely sensitive to infrared small targets, andsmall target information is easy to lose in the high-level semantic layer. Inthis paper, we propose an enhancing feature learning network (EFLNet) based onYOLOv7 framework to solve these problems. First, we notice that there is anextremely imbalance between the target and the background in the infraredimage, which makes the model pay more attention to the background features,resulting in missed detection. To address this problem, we propose a newadaptive threshold focal loss function that adjusts the loss weightautomatically, compelling the model to allocate greater attention to targetfeatures. Second, we introduce the normalized Gaussian Wasserstein distance toalleviate the difficulty of model convergence caused by the extreme sensitivityof the bounding box regression to infrared small targets. Finally, weincorporate a dynamic head mechanism into the network to enable adaptivelearning of the relative importance of each semantic layer. Experimentalresults demonstrate our method can achieve better performance in the detectionperformance of infrared small targets compared to state-of-the-artdeep-learning based methods.</description><author>Bo Yang, Xinyu Zhang, Jiahao Zhu, Jian Zhang, Dongjian Tian, Jun Luo, Mingliang Zhou, Yangjun Pi</author><pubDate>Thu, 27 Jul 2023 10:23:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14723v1</guid></item><item><title>The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions</title><link>http://arxiv.org/abs/2307.14502v1</link><description>Recent work in the field of speech enhancement (SE) has involved the use ofself-supervised speech representations (SSSRs) as feature transformations inloss functions. However, in prior work, very little attention has been paid tothe relationship between the language of the audio used to train theself-supervised representation and that used to train the SE system.Enhancement models trained using a loss function which incorporates aself-supervised representation that shares exactly the language of the noisydata used to train the SE system show better performance than those which donot match exactly. This may lead to enhancement systems which are languagespecific and as such do not generalise well to unseen languages, unlike modelstrained using traditional spectrogram or time domain loss functions. In thiswork, SE models are trained and tested on a number of different languages, withself-supervised representations which themselves are trained using differentlanguage combinations and with differing network structures as loss functionrepresentations. These models are then tested across unseen languages and theirperformances are analysed. It is found that the training language of theself-supervised representation appears to have a minor effect on enhancementperformance, the amount of training data of a particular language, however,greatly affects performance.</description><author>George Close, Thomas Hain, Stefan Goetze</author><pubDate>Thu, 27 Jul 2023 10:20:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14502v1</guid></item><item><title>In-Context Retrieval-Augmented Language Models</title><link>http://arxiv.org/abs/2302.00083v2</link><description>Retrieval-Augmented Language Modeling (RALM) methods, which condition alanguage model (LM) on relevant documents from a grounding corpus duringgeneration, were shown to significantly improve language modeling performance.In addition, they can mitigate the problem of factually inaccurate textgeneration and provide natural source attribution mechanism. Existing RALMapproaches focus on modifying the LM architecture in order to facilitate theincorporation of external information, significantly complicating deployment.This paper considers a simple alternative, which we dub In-Context RALM:leaving the LM architecture unchanged and prepending grounding documents to theinput, without any further training of the LM. We show that In-Context RALMthat builds on off-the-shelf general purpose retrievers provides surprisinglylarge LM gains across model sizes and diverse corpora. We also demonstrate thatthe document retrieval and ranking mechanism can be specialized to the RALMsetting to further boost performance. We conclude that In-Context RALM hasconsiderable potential to increase the prevalence of LM grounding, particularlyin settings where a pretrained LM must be used without modification or even viaAPI access.</description><author>Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham</author><pubDate>Thu, 27 Jul 2023 10:15:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00083v2</guid></item><item><title>GaitMorph: Transforming Gait by Optimally Transporting Discrete Codes</title><link>http://arxiv.org/abs/2307.14713v1</link><description>Gait, the manner of walking, has been proven to be a reliable biometric withuses in surveillance, marketing and security. A promising new direction for thefield is training gait recognition systems without explicit human annotations,through self-supervised learning approaches. Such methods are heavily relianton strong augmentations for the same walking sequence to induce more datavariability and to simulate additional walking variations. Current dataaugmentation schemes are heuristic and cannot provide the necessary datavariation as they are only able to provide simple temporal and spatialdistortions. In this work, we propose GaitMorph, a novel method to modify thewalking variation for an input gait sequence. Our method entails the trainingof a high-compression model for gait skeleton sequences that leveragesunlabelled data to construct a discrete and interpretable latent space, whichpreserves identity-related features. Furthermore, we propose a method based onoptimal transport theory to learn latent transport maps on the discretecodebook that morph gait sequences between variations. We perform extensiveexperiments and show that our method is suitable to synthesize additional viewsfor an input sequence.</description><author>Adrian Cosma, Emilian Radoi</author><pubDate>Thu, 27 Jul 2023 10:09:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14713v1</guid></item><item><title>Evaluating Generative Models for Graph-to-Text Generation</title><link>http://arxiv.org/abs/2307.14712v1</link><description>Large language models (LLMs) have been widely employed for graph-to-textgeneration tasks. However, the process of finetuning LLMs requires significanttraining resources and annotation work. In this paper, we explore thecapability of generative models to generate descriptive text from graph data ina zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on twograph-to-text datasets and compare their performance with that of finetuned LLMmodels such as T5 and BART. Our results demonstrate that generative models arecapable of generating fluent and coherent text, achieving BLEU scores of 10.57and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our erroranalysis reveals that generative models still struggle with understanding thesemantic relations between entities, and they also tend to generate text withhallucinations or irrelevant information. As a part of error analysis, weutilize BERT to detect machine-generated text and achieve high macro-F1 scores.We have made the text generated by generative models publicly available.</description><author>Shuzhou Yuan, Michael Färber</author><pubDate>Thu, 27 Jul 2023 10:03:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14712v1</guid></item><item><title>Pre-training Vision Transformers with Very Limited Synthesized Images</title><link>http://arxiv.org/abs/2307.14710v1</link><description>Formula-driven supervised learning (FDSL) is a pre-training method thatrelies on synthetic images generated from mathematical formulae such asfractals. Prior work on FDSL has shown that pre-training vision transformers onsuch synthetic datasets can yield competitive accuracy on a wide range ofdownstream tasks. These synthetic images are categorized according to theparameters in the mathematical formula that generate them. In the present work,we hypothesize that the process for generating different instances for the samecategory in FDSL, can be viewed as a form of data augmentation. We validatethis hypothesis by replacing the instances with data augmentation, which meanswe only need a single image per category. Our experiments shows that thisone-instance fractal database (OFDB) performs better than the original datasetwhere instances were explicitly generated. We further scale up OFDB to 21,000categories and show that it matches, or even surpasses, the model pre-trainedon ImageNet-21k in ImageNet-1k fine-tuning. The number of images in OFDB is21k, whereas ImageNet-21k has 14M. This opens new possibilities forpre-training vision transformers with much smaller datasets.</description><author>Ryo Nakamura1, Hirokatsu Kataoka, Sora Takashima, Edgar Josafat Martinez Noriega, Rio Yokota, Nakamasa Inoue</author><pubDate>Thu, 27 Jul 2023 09:58:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14710v1</guid></item><item><title>Taxonomy Adaptive Cross-Domain Adaptation in Medical Imaging via Optimization Trajectory Distillation</title><link>http://arxiv.org/abs/2307.14709v1</link><description>The success of automated medical image analysis depends on large-scale andexpert-annotated training sets. Unsupervised domain adaptation (UDA) has beenraised as a promising approach to alleviate the burden of labeled datacollection. However, they generally operate under the closed-set adaptationsetting assuming an identical label set between the source and target domains,which is over-restrictive in clinical practice where new classes commonly existacross datasets due to taxonomic inconsistency. While several methods have beenpresented to tackle both domain shifts and incoherent label sets, none of themtake into account the common characteristics of the two issues and consider thelearning dynamics along network training. In this work, we propose optimizationtrajectory distillation, a unified approach to address the two technicalchallenges from a new perspective. It exploits the low-rank nature of gradientspace and devises a dual-stream distillation algorithm to regularize thelearning dynamics of insufficiently annotated domain and classes with theexternal guidance obtained from reliable sources. Our approach resolves theissue of inadequate navigation along network optimization, which is the majorobstacle in the taxonomy adaptive cross-domain adaptation scenario. We evaluatethe proposed method extensively on several tasks towards various endpoints withclinical and open-world significance. The results demonstrate its effectivenessand improvements over previous methods.</description><author>Jianan Fan, Dongnan Liu, Hang Chang, Heng Huang, Mei Chen, Weidong Cai</author><pubDate>Thu, 27 Jul 2023 09:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14709v1</guid></item><item><title>Predicting Winning Regions in Parity Games via Graph Neural Networks (Extended Abstract)</title><link>http://arxiv.org/abs/2210.09924v2</link><description>Solving parity games is a major building block for numerous applications inreactive program verification and synthesis. While they can be solvedefficiently in practice, no known approach has a polynomial worst-case runtimecomplexity. We present a incomplete polynomial-time approach to determining thewinning regions of parity games via graph neural networks. Our evaluation on 900 randomly generated parity games shows that thisapproach is effective and efficient in practice. It correctly determines thewinning regions of $\sim$60\% of the games in our data set and only incursminor errors in the remaining ones. We believe that this approach can beextended to efficiently solve parity games as well.</description><author>Tobias Hecking, Swathy Muthukrishnan, Alexander Weinert</author><pubDate>Thu, 27 Jul 2023 09:55:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.09924v2</guid></item><item><title>High Dynamic Range Imaging via Visual Attention Modules</title><link>http://arxiv.org/abs/2307.14705v1</link><description>Thanks to High Dynamic Range (HDR) imaging methods, the scope of photographyhas seen profound changes recently. To be more specific, such methods try toreconstruct the lost luminosity of the real world caused by the limitation ofregular cameras from the Low Dynamic Range (LDR) images. Additionally, althoughthe State-Of-The-Art methods in this topic perform well, they mainlyconcentrate on combining different exposures and have less attention toextracting the informative parts of the images. Thus, this paper aims tointroduce a new model capable of incorporating information from the mostvisible areas of each image extracted by a visual attention module (VAM), whichis a result of a segmentation strategy. In particular, the model, based on adeep learning architecture, utilizes the extracted areas to produce the finalHDR image. The results demonstrate that our method outperformed most of theState-Of-The-Art algorithms.</description><author>Ali Reza Omrani, Davide Moroni</author><pubDate>Thu, 27 Jul 2023 09:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14705v1</guid></item><item><title>DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer</title><link>http://arxiv.org/abs/2103.10206v5</link><description>Generating 3D dances from music is an emerged research task that benefits alot of applications in vision and graphics. Previous works treat this task assequence generation, however, it is challenging to render a music-alignedlong-term sequence with high kinematic complexity and coherent movements. Inthis paper, we reformulate it by a two-stage process, ie, a key pose generationand then an in-between parametric motion curve prediction, where the key posesare easier to be synchronized with the music beats and the parametric curvescan be efficiently regressed to render fluent rhythm-aligned movements. Wenamed the proposed method as DanceFormer, which includes two cascadingkinematics-enhanced transformer-guided networks (called DanTrans) that tackleeach stage, respectively. Furthermore, we propose a large-scale musicconditioned 3D dance dataset, called PhantomDance, that is accurately labeledby experienced animators rather than reconstruction or motion capture. Thisdataset also encodes dances as key poses and parametric motion curves apartfrom pose sequences, thus benefiting the training of our DanceFormer. Extensiveexperiments demonstrate that the proposed method, even trained by existingdatasets, can generate fluent, performative, and music-matched 3D dances thatsurpass previous works quantitatively and qualitatively. Moreover, the proposedDanceFormer, together with the PhantomDance dataset(https://github.com/libuyu/PhantomDanceDataset), are seamlessly compatible withindustrial animation software, thus facilitating the adaptation for variousdownstream applications.</description><author>Buyu Li, Yongchi Zhao, Zhelun Shi, Lu Sheng</author><pubDate>Thu, 27 Jul 2023 09:49:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.10206v5</guid></item><item><title>MIM-OOD: Generative Masked Image Modelling for Out-of-Distribution Detection in Medical Images</title><link>http://arxiv.org/abs/2307.14701v1</link><description>Unsupervised Out-of-Distribution (OOD) detection consists in identifyinganomalous regions in images leveraging only models trained on images of healthyanatomy. An established approach is to tokenize images and model thedistribution of tokens with Auto-Regressive (AR) models. AR models are used to1) identify anomalous tokens and 2) in-paint anomalous representations within-distribution tokens. However, AR models are slow at inference time and proneto error accumulation issues which negatively affect OOD detection performance.Our novel method, MIM-OOD, overcomes both speed and error accumulation issuesby replacing the AR model with two task-specific networks: 1) a transformeroptimized to identify anomalous tokens and 2) a transformer optimized toin-paint anomalous tokens using masked image modelling (MIM). Our experimentswith brain MRI anomalies show that MIM-OOD substantially outperforms AR models(DICE 0.458 vs 0.301) while achieving a nearly 25x speedup (9.5s vs 244s).</description><author>Sergio {Naval Marimont}, Vasilis Siomos, Giacomo Tarroni</author><pubDate>Thu, 27 Jul 2023 09:43:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14701v1</guid></item><item><title>Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations</title><link>http://arxiv.org/abs/2307.13423v2</link><description>Self-supervised speech representations (SSSRs) have been successfully appliedto a number of speech-processing tasks, e.g. as feature extractor for speechquality (SQ) prediction, which is, in turn, relevant for assessment andtraining speech enhancement systems for users with normal or impaired hearing.However, exact knowledge of why and how quality-related information is encodedwell in such representations remains poorly understood. In this work,techniques for non-intrusive prediction of SQ ratings are extended to theprediction of intelligibility for hearing-impaired users. It is found thatself-supervised representations are useful as input features to non-intrusiveprediction models, achieving competitive performance to more complex systems. Adetailed analysis of the performance depending on Clarity Prediction Challenge1 listeners and enhancement systems indicates that more data might be needed toallow generalisation to unknown systems and (hearing-impaired) individuals</description><author>George Close, Thomas Hain, Stefan Goetze</author><pubDate>Thu, 27 Jul 2023 09:41:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13423v2</guid></item><item><title>Duet: efficient and scalable hybriD neUral rElation undersTanding</title><link>http://arxiv.org/abs/2307.13494v3</link><description>Learned cardinality estimation methods have achieved high precision comparedto traditional methods. Among learned methods, query-driven approaches face thedata and workload drift problem for a long time. Although both query-driven andhybrid methods are proposed to avoid this problem, even the state-of-art ofthem suffer from high training and estimation costs, limited scalability,instability, and long-tailed distribution problem on high cardinality and highdimensional tables, which seriously affects the practical application oflearned cardinality estimators. In this paper, we prove that most of theseproblems are directly caused by the widely used progressive sampling. We solvethis problem by introducing predicates into the autoregressive model andpropose Duet, a stable, efficient, and scalable hybrid method to estimatecardinality directly without sampling or any non-differentiable process, whichcan not only reduces the inference complexity from $O(n)$ to $O(1)$ compared toNaru and UAE but also achieve higher accuracy on high cardinality and highdimensional tables. Experimental results show that Duet can achieve all thedesign goals above and be much more practical and even has a lower inferencecost on CPU than that of most learned methods on GPU.</description><author>Kaixin Zhang, Hongzhi Wang, Yabin Lu, Ziqi Li, Chang Shu, Yu Yan, Donghua Yang</author><pubDate>Thu, 27 Jul 2023 09:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13494v3</guid></item></channel></rss>