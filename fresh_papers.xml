<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 23 Jan 2025 13:00:08 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Accelerate High-Quality Diffusion Models with Inner Loop Feedback</title><link>http://arxiv.org/abs/2501.13107v1</link><description>We propose Inner Loop Feedback (ILF), a novel approach to acceleratediffusion models' inference. ILF trains a lightweight module to predict futurefeatures in the denoising process by leveraging the outputs from a chosendiffusion backbone block at a given time step. This approach exploits two keyintuitions; (1) the outputs of a given block at adjacent time steps aresimilar, and (2) performing partial computations for a step imposes a lowerburden on the model than skipping the step entirely. Our method is highlyflexible, since we find that the feedback module itself can simply be a blockfrom the diffusion backbone, with all settings copied. Its influence on thediffusion forward can be tempered with a learnable scaling factor from zeroinitialization. We train this module using distillation losses; however, unlikesome prior work where a full diffusion backbone serves as the student, ourmodel freezes the backbone, training only the feedback module. While manyefforts to optimize diffusion models focus on achieving acceptable imagequality in extremely few steps (1-4 steps), our emphasis is on matching bestcase results (typically achieved in 20 steps) while significantly reducingruntime. ILF achieves this balance effectively, demonstrating strongperformance for both class-to-image generation with diffusion transformer (DiT)and text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. Thequality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIPImage Quality Assessment, ImageReward, and qualitative comparisons.</description><author>Matthew Gwilliam, Han Cai, Di Wu, Abhinav Shrivastava, Zhiyu Cheng</author><pubDate>Wed, 22 Jan 2025 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13107v1</guid></item><item><title>VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding</title><link>http://arxiv.org/abs/2501.13106v1</link><description>In this paper, we propose VideoLLaMA3, a more advanced multimodal foundationmodel for image and video understanding. The core design philosophy ofVideoLLaMA3 is vision-centric. The meaning of "vision-centric" is two-fold: thevision-centric training paradigm and vision-centric framework design. The keyinsight of our vision-centric training paradigm is that high-quality image-textdata is crucial for both image and video understanding. Instead of preparingmassive video-text datasets, we focus on constructing large-scale andhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)vision-centric alignment stage, which warms up the vision encoder andprojector; 2) vision-language pretraining stage, which jointly tunes the visionencoder, projector, and LLM with large-scale image-text data covering multipletypes (including scene images, documents, charts) as well as text-only data. 3)multi-task fine-tuning stage, which incorporates image-text SFT data fordownstream tasks and video-text data to establish a foundation for videounderstanding. 4) video-centric fine-tuning, which further improves the model'scapability in video understanding. As for the framework design, to bettercapture fine-grained details in images, the pretrained vision encoder isadapted to encode images of varying sizes into vision tokens with correspondingnumbers, rather than a fixed number of tokens. For video inputs, we reduce thenumber of vision tokens according to their similarity so that therepresentation of videos will be more precise and compact. Benefit fromvision-centric designs, VideoLLaMA3 achieves compelling performances in bothimage and video understanding benchmarks.</description><author>Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, Deli Zhao</author><pubDate>Wed, 22 Jan 2025 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13106v1</guid></item><item><title>Neural Radiance Fields for the Real World: A Survey</title><link>http://arxiv.org/abs/2501.13104v1</link><description>Neural Radiance Fields (NeRFs) have remodeled 3D scene representation sincerelease. NeRFs can effectively reconstruct complex 3D scenes from 2D images,advancing different fields and applications such as scene understanding, 3Dcontent generation, and robotics. Despite significant research progress, athorough review of recent innovations, applications, and challenges is lacking.This survey compiles key theoretical advancements and alternativerepresentations and investigates emerging challenges. It further exploresapplications on reconstruction, highlights NeRFs' impact on computer vision androbotics, and reviews essential datasets and toolkits. By identifying gaps inthe literature, this survey discusses open challenges and offers directions forfuture research.</description><author>Wenhui Xiao, Remi Chierchia, Rodrigo Santa Cruz, Xuesong Li, David Ahmedt-Aristizabal, Olivier Salvado, Clinton Fookes, Leo Lebrat</author><pubDate>Wed, 22 Jan 2025 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13104v1</guid></item><item><title>A Rate-Distortion Framework for Summarization</title><link>http://arxiv.org/abs/2501.13100v1</link><description>This paper introduces an information-theoretic framework for textsummarization. We define the summarizer rate-distortion function and show thatit provides a fundamental lower bound on summarizer performance. We describe aniterative procedure, similar to Blahut-Arimoto algorithm, for computing thisfunction. To handle real-world text datasets, we also propose a practicalmethod that can calculate the summarizer rate-distortion function with limiteddata. Finally, we empirically confirm our theoretical results by comparing thesummarizer rate-distortion function with the performances of differentsummarizers used in practice.</description><author>Enes Arda, Aylin Yener</author><pubDate>Wed, 22 Jan 2025 18:57:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13100v1</guid></item><item><title>Robust Representation Consistency Model via Contrastive Denoising</title><link>http://arxiv.org/abs/2501.13094v1</link><description>Robustness is essential for deep neural networks, especially insecurity-sensitive applications. To this end, randomized smoothing providestheoretical guarantees for certifying robustness against adversarialperturbations. Recently, diffusion models have been successfully employed forrandomized smoothing to purify noise-perturbed samples before makingpredictions with a standard classifier. While these methods excel at smallperturbation radii, they struggle with larger perturbations and incur asignificant computational overhead during inference compared to classicalmethods. To address this, we reformulate the generative modeling task along thediffusion trajectories in pixel space as a discriminative task in the latentspace. Specifically, we use instance discrimination to achieve consistentrepresentations along the trajectories by aligning temporally adjacent points.After fine-tuning based on the learned representations, our model enablesimplicit denoising-then-classification via a single prediction, substantiallyreducing inference costs. We conduct extensive experiments on various datasetsand achieve state-of-the-art performance with minimal computation budget duringinference. For example, our method outperforms the certified accuracy ofdiffusion-based methods on ImageNet across all perturbation radii by 5.3% onaverage, with up to 11.6% at larger radii, while reducing inference costs by85$\times$ on average. Codes are available at:https://github.com/jiachenlei/rRCM.</description><author>Jiachen Lei, Julius Berner, Jiongxiao Wang, Zhongzhu Chen, Zhongjia Ba, Kui Ren, Jun Zhu, Anima Anandkumar</author><pubDate>Wed, 22 Jan 2025 18:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13094v1</guid></item><item><title>Guaranteed Recovery of Unambiguous Clusters</title><link>http://arxiv.org/abs/2501.13093v1</link><description>Clustering is often a challenging problem because of the inherent ambiguityin what the "correct" clustering should be. Even when the number of clusters$K$ is known, this ambiguity often still exists, particularly when there isvariation in density among different clusters, and clusters have multiplerelatively separated regions of high density. In this paper we propose aninformation-theoretic characterization of when a $K$-clustering is ambiguous,and design an algorithm that recovers the clustering whenever it isunambiguous. This characterization formalizes the situation when two highdensity regions within a cluster are separable enough that they look more liketwo distinct clusters than two truly distinct clusters in the clustering. Thealgorithm first identifies $K$ partial clusters (or "seeds") using adensity-based approach, and then adds unclustered points to the initial $K$partial clusters in a greedy manner to form a complete clustering. We implementand test a version of the algorithm that is modified to effectively handleoverlapping clusters, and observe that it requires little parameter selectionand displays improved performance on many datasets compared to widely usedalgorithms for non-convex cluster recovery.</description><author>Kayvon Mazooji, Ilan Shomorony</author><pubDate>Wed, 22 Jan 2025 18:51:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13093v1</guid></item><item><title>Orchid: Image Latent Diffusion for Joint Appearance and Geometry Generation</title><link>http://arxiv.org/abs/2501.13087v1</link><description>Diffusion models are state-of-the-art for image generation. Trained on largedatasets, they capture expressive image priors that have been used for taskslike inpainting, depth, and (surface) normal prediction. However, these modelsare typically trained for one specific task, e.g., a separate model for each ofcolor, depth, and normal prediction. Such models do not leverage the intrinsiccorrelation between appearance and geometry, often leading to inconsistentpredictions. In this paper, we propose using a novel image diffusion prior that jointlyencodes appearance and geometry. We introduce a diffusion model Orchid,comprising a Variational Autoencoder (VAE) to encode color, depth, and surfacenormals to a latent space, and a Latent Diffusion Model (LDM) for generatingthese joint latents. Orchid directly generates photo-realistic color images,relative depth, and surface normals from user-provided text, and can be used tocreate image-aligned partial 3D scenes seamlessly. It can also performimage-conditioned tasks like joint monocular depth and normal prediction and iscompetitive in accuracy to state-of-the-art methods designed for those tasksalone. Lastly, our model learns a joint prior that can be used zero-shot as aregularizer for many inverse problems that entangle appearance and geometry.For example, we demonstrate its effectiveness in color-depth-normal inpainting,showcasing its applicability to problems in 3D generation from sparse views.</description><author>Akshay Krishnan, Xinchen Yan, Vincent Casser, Abhijit Kundu</author><pubDate>Wed, 22 Jan 2025 18:46:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13087v1</guid></item><item><title>Attention-Driven Hierarchical Reinforcement Learning with Particle Filtering for Source Localization in Dynamic Fields</title><link>http://arxiv.org/abs/2501.13084v1</link><description>In many real-world scenarios, such as gas leak detection or environmentalpollutant tracking, solving the Inverse Source Localization andCharacterization problem involves navigating complex, dynamic fields withsparse and noisy observations. Traditional methods face significant challenges,including partial observability, temporal and spatial dynamics,out-of-distribution generalization, and reward sparsity. To address theseissues, we propose a hierarchical framework that integrates Bayesian inferenceand reinforcement learning. The framework leverages an attention-enhancedparticle filtering mechanism for efficient and accurate belief updates, andincorporates two complementary execution strategies: Attention ParticleFiltering Planning and Attention Particle Filtering Reinforcement Learning.These approaches optimize exploration and adaptation under uncertainty.Theoretical analysis proves the convergence of the attention-enhanced particlefilter, while extensive experiments across diverse scenarios validate theframework's superior accuracy, adaptability, and computational efficiency. Ourresults highlight the framework's potential for broad applications in dynamicfield estimation tasks.</description><author>Yiwei Shi, Mengyue Yang, Qi Zhang, Weinan Zhang, Cunjia Liu, Weiru Liu</author><pubDate>Wed, 22 Jan 2025 18:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13084v1</guid></item><item><title>Boosting MCTS with Free Energy Minimization</title><link>http://arxiv.org/abs/2501.13083v1</link><description>Active Inference, grounded in the Free Energy Principle, provides a powerfullens for understanding how agents balance exploration and goal-directedbehavior in uncertain environments. Here, we propose a new planning framework,that integrates Monte Carlo Tree Search (MCTS) with active inference objectivesto systematically reduce epistemic uncertainty while pursuing extrinsicrewards. Our key insight is that MCTS already renowned for its searchefficiency can be naturally extended to incorporate free energy minimization byblending expected rewards with information gain. Concretely, the Cross-EntropyMethod (CEM) is used to optimize action proposals at the root node, while treeexpansions leverage reward modeling alongside intrinsic exploration bonuses.This synergy allows our planner to maintain coherent estimates of value anduncertainty throughout planning, without sacrificing computationaltractability. Empirically, we benchmark our planner on a diverse set ofcontinuous control tasks, where it demonstrates performance gains over bothstandalone CEM and MCTS with random rollouts.</description><author>Mawaba Pascal Dao, Adrian Peter</author><pubDate>Wed, 22 Jan 2025 18:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13083v1</guid></item><item><title>Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment</title><link>http://arxiv.org/abs/2501.13080v1</link><description>Large Language Models (LLMs) have demonstrated powerful capabilities thatrender them valuable in different applications, including conversational AIproducts. It is paramount to ensure the security and reliability of theseproducts by mitigating their vulnerabilities towards malicious userinteractions, which can lead to the exposure of great risks and reputationalrepercussions. In this work, we present a comprehensive study on the efficacyof fine-tuning and aligning Chain-of-Thought (CoT) responses of different LLMsthat serve as input moderation guardrails. We systematically explore varioustuning methods by leveraging a small set of training data to adapt these modelsas proxy defense mechanisms to detect malicious inputs and provide a reasoningfor their verdicts, thereby preventing the exploitation of conversationalagents. We rigorously evaluate the efficacy and robustness of different tuningstrategies to generalize across diverse adversarial and malicious query types.Our experimental results outline the potential of alignment processes tailoredto a varied range of harmful input queries, even with constrained dataresources. These techniques significantly enhance the safety of conversationalAI systems and provide a feasible framework for deploying more secure andtrustworthy AI-driven interactions.</description><author>Melissa Kazemi Rad, Huy Nghiem, Andy Luo, Sahil Wadhwa, Mohammad Sorower, Stephen Rawls</author><pubDate>Wed, 22 Jan 2025 18:40:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13080v1</guid></item><item><title>Evolution and The Knightian Blindspot of Machine Learning</title><link>http://arxiv.org/abs/2501.13075v1</link><description>This paper claims that machine learning (ML) largely overlooks an importantfacet of general intelligence: robustness to a qualitatively unknown future inan open world. Such robustness relates to Knightian uncertainty (KU) ineconomics, i.e. uncertainty that cannot be quantified, which is excluded fromconsideration in ML's key formalisms. This paper aims to identify this blindspot, argue its importance, and catalyze research into addressing it, which webelieve is necessary to create truly robust open-world AI. To help illuminatethe blind spot, we contrast one area of ML, reinforcement learning (RL), withthe process of biological evolution. Despite staggering ongoing progress, RLstill struggles in open-world situations, often failing under unforeseensituations. For example, the idea of zero-shot transferring a self-driving carpolicy trained only in the US to the UK currently seems exceedingly ambitious.In dramatic contrast, biological evolution routinely produces agents thatthrive within an open world, sometimes even to situations that are remarkablyout-of-distribution (e.g. invasive species; or humans, who do undertake suchzero-shot international driving). Interestingly, evolution achieves suchrobustness without explicit theory, formalisms, or mathematical gradients. Weexplore the assumptions underlying RL's typical formalisms, showing how theylimit RL's engagement with the unknown unknowns characteristic of anever-changing complex world. Further, we identify mechanisms through whichevolutionary processes foster robustness to novel and unpredictable challenges,and discuss potential pathways to algorithmically embody them. The conclusionis that the intriguing remaining fragility of ML may result from blind spots inits formalisms, and that significant gains may result from direct confrontationwith the challenge of KU.</description><author>Joel Lehman, Elliot Meyerson, Tarek El-Gaaly, Kenneth O. Stanley, Tarin Ziyaee</author><pubDate>Wed, 22 Jan 2025 18:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13075v1</guid></item><item><title>Autonomy-of-Experts Models</title><link>http://arxiv.org/abs/2501.13074v1</link><description>Mixture-of-Experts (MoE) models mostly use a router to assign tokens tospecific expert modules, activating only partial parameters and oftenoutperforming dense models. We argue that the separation between the router'sdecision-making and the experts' execution is a critical yet overlooked issue,leading to suboptimal expert selection and ineffective learning. To addressthis, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in whichexperts autonomously select themselves to process inputs. AoE is based on theinsight that an expert is aware of its own capacity to effectively process atoken, an awareness reflected in the scale of its internal activations. In AoE,routers are removed; instead, experts pre-compute internal activations forinputs and are ranked based on their activation norms. Only the top-rankingexperts proceed with the forward pass, while the others abort. The overhead ofpre-computing activations is reduced through a low-rank weight factorization.This self-evaluating-then-partner-comparing approach ensures improved expertselection and effective learning. We pre-train language models having 700M upto 4B parameters, demonstrating that AoE outperforms traditional MoE modelswith comparable efficiency.</description><author>Ang Lv, Ruobing Xie, Yining Qian, Songhao Wu, Xingwu Sun, Zhanhui Kang, Di Wang, Rui Yan</author><pubDate>Wed, 22 Jan 2025 18:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13074v1</guid></item><item><title>CHaRNet: Conditioned Heatmap Regression for Robust Dental Landmark Localization</title><link>http://arxiv.org/abs/2501.13073v1</link><description>Identifying anatomical landmarks in 3D dental models is crucial fororthodontic treatment. Manually placing these key points is complex,time-consuming, and requires expert knowledge. While some machine learningmethods have been proposed for automatic tooth landmark detection in 3DIntraoral Scans (IOS), research remains limited, with no fully end-to-endapproaches that avoid teeth segmentation. We propose CHaRNet (Conditioned Heatmap Regression Network), the firstend-to-end deep learning method for tooth landmark detection in 3D IOS. Unliketraditional two-stage methods that segment teeth before detecting landmarks,CHaRNet directly detects landmarks on the input point cloud. It consists offour key modules: (1) a point cloud encoder, (2) a point cloud decoder with aheatmap regression head, (3) a teeth presence classification head, and (4) theinnovative Conditioned Heatmap Regression (CHaR) module. The CHaR modulerefines landmark regression by leveraging teeth presence classification,enabling dynamic adaptation to cases with missing teeth and improving accuracyin complex dental models. We evaluate CHaRNet using five point cloud learning algorithms to validatethe effectiveness of the CHaR module and test it on a clinical dataset of$1,214$ annotated 3D dental models. Both the dataset and code will be publiclyreleased to address the lack of open datasets in orthodontics, promotebenchmarking, and inspire new research. CHaRNet achieves a Mean Euclidean Distance Error (MEDE) of 1.28 mm and a MeanSuccess Ratio (MSR) of 82.40\%, demonstrating robust performance. Notably, itexcels in handling irregular dental geometries, such as models with missingteeth. This end-to-end approach streamlines orthodontic workflows, improves 3DIOS analysis precision, and facilitates efficient computer-assisted treatmentplanning.</description><author>José Rodríguez-Ortega, Siham Tabik</author><pubDate>Wed, 22 Jan 2025 18:35:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13073v1</guid></item><item><title>AdaWM: Adaptive World Model based Planning for Autonomous Driving</title><link>http://arxiv.org/abs/2501.13072v1</link><description>World model based reinforcement learning (RL) has emerged as a promisingapproach for autonomous driving, which learns a latent dynamics model and usesit to train a planning policy. To speed up the learning process, thepretrain-finetune paradigm is often used, where online RL is initialized by apretrained model and a policy learned offline. However, naively performing suchinitialization in RL may result in dramatic performance degradation during theonline interactions in the new task. To tackle this challenge, we first analyzethe performance degradation and identify two primary root causes therein: themismatch of the planning policy and the mismatch of the dynamics model, due todistribution shift. We further analyze the effects of these factors onperformance degradation during finetuning, and our findings reveal that thechoice of finetuning strategies plays a pivotal role in mitigating theseeffects. We then introduce AdaWM, an Adaptive World Model based planningmethod, featuring two key steps: (a) mismatch identification, which quantifiesthe mismatches and informs the finetuning strategy, and (b) alignment-drivenfinetuning, which selectively updates either the policy or the model as neededusing efficient low-rank updates. Extensive experiments on the challengingCARLA driving tasks demonstrate that AdaWM significantly improves thefinetuning process, resulting in more robust and efficient performance inautonomous driving systems.</description><author>Hang Wang, Xin Ye, Feng Tao, Abhirup Mallik, Burhaneddin Yaman, Liu Ren, Junshan Zhang</author><pubDate>Wed, 22 Jan 2025 18:34:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13072v1</guid></item><item><title>Towards impactful challenges: post-challenge paper, benchmarks and other dissemination actions</title><link>http://arxiv.org/abs/2312.06036v4</link><description>The conclusion of an AI challenge is not the end of its lifecycle; ensuring along-lasting impact requires meticulous post-challenge activities. Thelong-lasting impact also needs to be organised. This chapter covers the variousactivities after the challenge is formally finished. This work identifiestarget audiences for post-challenge initiatives and outlines methods forcollecting and organizing challenge outputs. The multiple outputs of thechallenge are listed, along with the means to collect them. The central part ofthe chapter is a template for a typical post-challenge paper, includingpossible graphs and advice on how to turn the challenge into a long-lastingbenchmark.</description><author>Antoine Marot, David Rousseau, Zhen, Xu</author><pubDate>Wed, 22 Jan 2025 18:33:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06036v4</guid></item><item><title>Robust Body Composition Analysis by Generating 3D CT Volumes from Limited 2D Slices</title><link>http://arxiv.org/abs/2501.13071v1</link><description>Body composition analysis provides valuable insights into aging, diseaseprogression, and overall health conditions. Due to concerns of radiationexposure, two-dimensional (2D) single-slice computed tomography (CT) imaginghas been used repeatedly for body composition analysis. However, this approachintroduces significant spatial variability that can impact the accuracy androbustness of the analysis. To mitigate this issue and facilitate bodycomposition analysis, this paper presents a novel method to generate 3D CTvolumes from limited number of 2D slices using a latent diffusion model (LDM).Our approach first maps 2D slices into a latent representation space using avariational autoencoder. An LDM is then trained to capture the 3D context of astack of these latent representations. To accurately interpolateintermediateslices and construct a full 3D volume, we utilize body partregression to determine the spatial location and distance between the acquiredslices. Experiments on both in-house and public 3D abdominal CT datasetsdemonstrate that the proposed method significantly enhances body compositionanalysis compared to traditional 2D-based analysis, with a reduced error ratefrom 23.3% to 15.2%.</description><author>Lianrui Zuo, Xin Yu, Dingjie Su, Kaiwen Xu, Aravind R. Krishnan, Yihao Liu, Shunxing Bao, Fabien Maldonado, Luigi Ferrucci, Bennett A. Landman</author><pubDate>Wed, 22 Jan 2025 18:32:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13071v1</guid></item><item><title>Beyond the Lungs: Extending the Field of View in Chest CT with Latent Diffusion Models</title><link>http://arxiv.org/abs/2501.13068v1</link><description>The interconnection between the human lungs and other organs, such as theliver and kidneys, is crucial for understanding the underlying risks andeffects of lung diseases and improving patient care. However, most researchchest CT imaging is focused solely on the lungs due to considerations of costand radiation dose. This restricted field of view (FOV) in the acquired imagesposes challenges to comprehensive analysis and hinders the ability to gaininsights into the impact of lung diseases on other organs. To address this, wepropose SCOPE (Spatial Coverage Optimization with Prior Encoding), a novelapproach to capture the inter-organ relationships from CT images and extend theFOV of chest CT images. Our approach first trains a variational autoencoder(VAE) to encode 2D axial CT slices individually, then stacks the latentrepresentations of the VAE to form a 3D context for training a latent diffusionmodel. Once trained, our approach extends the FOV of CT images in thez-direction by generating new axial slices in a zero-shot manner. We evaluatedour approach on the National Lung Screening Trial (NLST) dataset, and resultssuggest that it effectively extends the FOV to include the liver and kidneys,which are not completely covered in the original NLST data acquisition.Quantitative results on a held-out whole-body dataset demonstrate that thegenerated slices exhibit high fidelity with acquired data, achieving an SSIM of0.81.</description><author>Lianrui Zuo, Kaiwen Xu, Dingjie Su, Xin Yu, Aravind R. Krishnan, Yihao Liu, Shunxing Bao, Thomas Li, Kim L. Sandler, Fabien Maldonado, Bennett A. Landman</author><pubDate>Wed, 22 Jan 2025 18:28:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13068v1</guid></item><item><title>Cross-D Conv: Cross-Dimensional Transferable Knowledge Base via Fourier Shifting Operation</title><link>http://arxiv.org/abs/2411.02441v3</link><description>In biomedical imaging analysis, the dichotomy between 2D and 3D data presentsa significant challenge. While 3D volumes offer superior real-worldapplicability, they are less available for each modality and not easy to trainin large scale, whereas 2D samples are abundant but less comprehensive. Thispaper introduces \texttt{Cross-D Conv} operation, a novel approach that bridgesthe dimensional gap by learning the phase shifting in the Fourier domain. Ourmethod enables seamless weight transfer between 2D and 3D convolutionoperations, effectively facilitating cross-dimensional learning. The proposedarchitecture leverages the abundance of 2D training data to enhance 3D modelperformance, offering a practical solution to the multimodal data scarcitychallenge in 3D medical model pretraining. Experimental validation on theRadImagenet (2D) and multimodal volumetric sets demonstrates that our approachachieves comparable or superior performance in feature quality assessment. Theenhanced convolution operation presents new opportunities for developingefficient classification and segmentation models in medical imaging. This workrepresents an advancement in cross-dimensional and multimodal medical imageanalysis, offering a robust framework for utilizing 2D priors in 3D modelpretraining while maintaining computational efficiency of 2D training.</description><author>Mehmet Can Yavuz, Yang Yang</author><pubDate>Wed, 22 Jan 2025 18:23:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02441v3</guid></item><item><title>SMART-Vision: Survey of Modern Action Recognition Techniques in Vision</title><link>http://arxiv.org/abs/2501.13066v1</link><description>Human Action Recognition (HAR) is a challenging domain in computer vision,involving recognizing complex patterns by analyzing the spatiotemporal dynamicsof individuals' movements in videos. These patterns arise in sequential data,such as video frames, which are often essential to accurately distinguishactions that would be ambiguous in a single image. HAR has garneredconsiderable interest due to its broad applicability, ranging from robotics andsurveillance systems to sports motion analysis, healthcare, and the burgeoningfield of autonomous vehicles. While several taxonomies have been proposed tocategorize HAR approaches in surveys, they often overlook hybrid methodologiesand fail to demonstrate how different models incorporate various architecturesand modalities. In this comprehensive survey, we present the novel SMART-Visiontaxonomy, which illustrates how innovations in deep learning for HAR complementone another, leading to hybrid approaches beyond traditional categories. Oursurvey provides a clear roadmap from foundational HAR works to currentstate-of-the-art systems, highlighting emerging research directions andaddressing unresolved challenges in discussion sections for architectureswithin the HAR domain. We provide details of the research datasets that variousapproaches used to measure and compare goodness HAR approaches. We also explorethe rapidly emerging field of Open-HAR systems, which challenges HAR systems bypresenting samples from unknown, novel classes during test time.</description><author>Ali K. AlShami, Ryan Rabinowitz, Khang Lam, Yousra Shleibik, Melkamu Mersha, Terrance Boult, Jugal Kalita</author><pubDate>Wed, 22 Jan 2025 18:21:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13066v1</guid></item><item><title>An Efficient Framework for Crediting Data Contributors of Diffusion Models</title><link>http://arxiv.org/abs/2407.03153v2</link><description>As diffusion models are deployed in real-world settings, and theirperformance is driven by training data, appraising the contribution of datacontributors is crucial to creating incentives for sharing quality data and toimplementing policies for data compensation. Depending on the use case, modelperformance corresponds to various global properties of the distributionlearned by a diffusion model (e.g., overall aesthetic quality). Hence, here weaddress the problem of attributing global properties of diffusion models todata contributors. The Shapley value provides a principled approach tovaluation by uniquely satisfying game-theoretic axioms of fairness. However,estimating Shapley values for diffusion models is computationally impracticalbecause it requires retraining on many training data subsets corresponding todifferent contributors and rerunning inference. We introduce a method toefficiently retrain and rerun inference for Shapley value estimation, byleveraging model pruning and fine-tuning. We evaluate the utility of our methodwith three use cases: (i) image quality for a DDPM trained on a CIFAR dataset,(ii) demographic diversity for an LDM trained on CelebA-HQ, and (iii) aestheticquality for a Stable Diffusion model LoRA-finetuned on Post-Impressionistartworks. Our results empirically demonstrate that our framework can identifyimportant data contributors across models' global properties, outperformingexisting attribution methods for diffusion models.</description><author>Chris Lin, Mingyu Lu, Chanwoo Kim, Su-In Lee</author><pubDate>Wed, 22 Jan 2025 18:21:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.03153v2</guid></item><item><title>Towards Affordance-Aware Articulation Synthesis for Rigged Objects</title><link>http://arxiv.org/abs/2501.12393v1</link><description>Rigged objects are commonly used in artist pipelines, as they can flexiblyadapt to different scenes and postures. However, articulating the rigs intorealistic affordance-aware postures (e.g., following the context, respectingthe physics and the personalities of the object) remains time-consuming andheavily relies on human labor from experienced artists. In this paper, wetackle the novel problem and design A3Syn. With a given context, such as theenvironment mesh and a text prompt of the desired posture, A3Syn synthesizesarticulation parameters for arbitrary and open-domain rigged objects obtainedfrom the Internet. The task is incredibly challenging due to the lack oftraining data, and we do not make any topological assumptions about theopen-domain rigs. We propose using 2D inpainting diffusion model and severalcontrol techniques to synthesize in-context affordance information. Then, wedevelop an efficient bone correspondence alignment using a combination ofdifferentiable rendering and semantic correspondence. A3Syn has stableconvergence, completes in minutes, and synthesizes plausible affordance ondifferent combinations of in-the-wild object rigs and scenes.</description><author>Yu-Chu Yu, Chieh Hubert Lin, Hsin-Ying Lee, Chaoyang Wang, Yu-Chiang Frank Wang, Ming-Hsuan Yang</author><pubDate>Tue, 21 Jan 2025 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12393v1</guid></item><item><title>Learning segmentation from point trajectories</title><link>http://arxiv.org/abs/2501.12392v1</link><description>We consider the problem of segmenting objects in videos based on their motionand no other forms of supervision. Prior work has often approached this problemby using the principle of common fate, namely the fact that the motion ofpoints that belong to the same object is strongly correlated. However, mostauthors have only considered instantaneous motion from optical flow. In thiswork, we present a way to train a segmentation network using long-term pointtrajectories as a supervisory signal to complement optical flow. The keydifficulty is that long-term motion, unlike instantaneous motion, is difficultto model -- any parametric approximation is unlikely to capture complex motionpatterns over long periods of time. We instead draw inspiration from subspaceclustering approaches, proposing a loss function that seeks to group thetrajectories into low-rank matrices where the motion of object points can beapproximately explained as a linear combination of other point tracks. Ourmethod outperforms the prior art on motion-based segmentation, which shows theutility of long-term motion and the effectiveness of our formulation.</description><author>Laurynas Karazija, Iro Laina, Christian Rupprecht, Andrea Vedaldi</author><pubDate>Tue, 21 Jan 2025 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12392v1</guid></item><item><title>Physics of Skill Learning</title><link>http://arxiv.org/abs/2501.12391v1</link><description>We aim to understand physics of skill learning, i.e., how skills are learnedin neural networks during training. We start by observing the Domino effect,i.e., skills are learned sequentially, and notably, some skills kick offlearning right after others complete learning, similar to the sequential fallof domino cards. To understand the Domino effect and relevant behaviors ofskill learning, we take physicists' approach of abstraction and simplification.We propose three models with varying complexities -- the Geometry model, theResource model, and the Domino model, trading between reality and simplicity.The Domino effect can be reproduced in the Geometry model, whose resourceinterpretation inspires the Resource model, which can be further simplified tothe Domino model. These models present different levels of abstraction andsimplification; each is useful to study some aspects of skill learning. TheGeometry model provides interesting insights into neural scaling laws andoptimizers; the Resource model sheds light on the learning dynamics ofcompositional tasks; the Domino model reveals the benefits of modularity. Thesemodels are not only conceptually interesting -- e.g., we show how Chinchillascaling laws can emerge from the Geometry model, but also are useful inpractice by inspiring algorithmic development -- e.g., we show how simplealgorithmic changes, motivated by these toy models, can speed up the trainingof deep learning models.</description><author>Ziming Liu, Yizhou Liu, Eric J. Michaud, Jeff Gore, Max Tegmark</author><pubDate>Tue, 21 Jan 2025 18:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12391v1</guid></item><item><title>GPS as a Control Signal for Image Generation</title><link>http://arxiv.org/abs/2501.12390v1</link><description>We show that the GPS tags contained in photo metadata provide a usefulcontrol signal for image generation. We train GPS-to-image models and use themfor tasks that require a fine-grained understanding of how images vary within acity. In particular, we train a diffusion model to generate images conditionedon both GPS and text. The learned model generates images that capture thedistinctive appearance of different neighborhoods, parks, and landmarks. Wealso extract 3D models from 2D GPS-to-image models through score distillationsampling, using GPS conditioning to constrain the appearance of thereconstruction from each viewpoint. Our evaluations suggest that ourGPS-conditioned models successfully learn to generate images that vary based onlocation, and that GPS conditioning improves estimated 3D structure.</description><author>Chao Feng, Ziyang Chen, Aleksander Holynski, Alexei A. Efros, Andrew Owens</author><pubDate>Tue, 21 Jan 2025 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12390v1</guid></item><item><title>Taming Teacher Forcing for Masked Autoregressive Video Generation</title><link>http://arxiv.org/abs/2501.12389v1</link><description>We introduce MAGI, a hybrid video generation framework that combines maskedmodeling for intra-frame generation with causal modeling for next-framegeneration. Our key innovation, Complete Teacher Forcing (CTF), conditionsmasked frames on complete observation frames rather than masked ones (namelyMasked Teacher Forcing, MTF), enabling a smooth transition from token-level(patch-level) to frame-level autoregressive generation. CTF significantlyoutperforms MTF, achieving a +23% improvement in FVD scores on first-frameconditioned video prediction. To address issues like exposure bias, we employtargeted training strategies, setting a new benchmark in autoregressive videogeneration. Experiments show that MAGI can generate long, coherent videosequences exceeding 100 frames, even when trained on as few as 16 frames,highlighting its potential for scalable, high-quality video generation.</description><author>Deyu Zhou, Quan Sun, Yuang Peng, Kun Yan, Runpei Dong, Duomin Wang, Zheng Ge, Nan Duan, Xiangyu Zhang, Lionel M. Ni, Heung-Yeung Shum</author><pubDate>Tue, 21 Jan 2025 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12389v1</guid></item><item><title>Continuous 3D Perception Model with Persistent State</title><link>http://arxiv.org/abs/2501.12387v1</link><description>We present a unified framework capable of solving a broad range of 3D tasks.Our approach features a stateful recurrent model that continuously updates itsstate representation with each new observation. Given a stream of images, thisevolving state can be used to generate metric-scale pointmaps (per-pixel 3Dpoints) for each new input in an online fashion. These pointmaps reside withina common coordinate system, and can be accumulated into a coherent, dense scenereconstruction that updates as new images arrive. Our model, called CUT3R(Continuous Updating Transformer for 3D Reconstruction), captures rich priorsof real-world scenes: not only can it predict accurate pointmaps from imageobservations, but it can also infer unseen regions of the scene by probing atvirtual, unobserved views. Our method is simple yet highly flexible, naturallyaccepting varying lengths of images that may be either video streams orunordered photo collections, containing both static and dynamic content. Weevaluate our method on various 3D/4D tasks and demonstrate competitive orstate-of-the-art performance in each. Project Page: https://cut3r.github.io/</description><author>Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, Angjoo Kanazawa</author><pubDate>Tue, 21 Jan 2025 18:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12387v1</guid></item><item><title>InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling</title><link>http://arxiv.org/abs/2501.12386v1</link><description>This paper aims to improve the performance of video multimodal large languagemodels (MLLM) via long and rich context (LRC) modeling. As a result, we developa new version of InternVideo2.5 with a focus on enhancing the original MLLMs'ability to perceive fine-grained details and capture long-form temporalstructure in videos. Specifically, our approach incorporates dense vision taskannotations into MLLMs using direct preference optimization and developscompact spatiotemporal representations through adaptive hierarchical tokencompression. Experimental results demonstrate this unique design of LRC greatlyimproves the results of video MLLM in mainstream video understanding benchmarks(short &amp; long), enabling the MLLM to memorize significantly longer video inputs(at least 6x longer than the original), and master specialized visioncapabilities like object tracking and segmentation. Our work highlights theimportance of multimodal context richness (length and fineness) in empoweringMLLM's innate abilites (focus and memory), providing new insights for futureresearch on video MLLM. Code and models are available athttps://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5</description><author>Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, Min Dou, Kai Chen, Wenhai Wang, Yu Qiao, Yali Wang, Limin Wang</author><pubDate>Tue, 21 Jan 2025 18:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12386v1</guid></item><item><title>Audio Texture Manipulation by Exemplar-Based Analogy</title><link>http://arxiv.org/abs/2501.12385v1</link><description>Audio texture manipulation involves modifying the perceptual characteristicsof a sound to achieve specific transformations, such as adding, removing, orreplacing auditory elements. In this paper, we propose an exemplar-basedanalogy model for audio texture manipulation. Instead of conditioning ontext-based instructions, our method uses paired speech examples, where one cliprepresents the original sound and another illustrates the desiredtransformation. The model learns to apply the same transformation to new input,allowing for the manipulation of sound textures. We construct a quadrupletdataset representing various editing tasks, and train a latent diffusion modelin a self-supervised manner. We show through quantitative evaluations andperceptual studies that our model outperforms text-conditioned baselines andgeneralizes to real-world, out-of-distribution, and non-speech scenarios.Project page: https://berkeley-speech-group.github.io/audio-texture-analogy/</description><author>Kan Jen Cheng, Tingle Li, Gopala Anumanchipalli</author><pubDate>Tue, 21 Jan 2025 18:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12385v1</guid></item><item><title>CCESAR: Coastline Classification-Extraction From SAR Images Using CNN-U-Net Combination</title><link>http://arxiv.org/abs/2501.12384v1</link><description>In this article, we improve the deep learning solution for coastlineextraction from Synthetic Aperture Radar (SAR) images by proposing a two-stagemodel involving image classification followed by segmentation. We hypothesizethat a single segmentation model usually used for coastline detection isinsufficient to characterize different coastline types. We demonstrate that theneed for a two-stage workflow prevails through different compression levels ofthese images. Our results from experiments using a combination of CNN and U-Netmodels on Sentinel-1 images show that the two-stage workflow, coastlineclassification-extraction from SAR images (CCESAR) outperforms a single U-Netsegmentation model.</description><author>Vidhu Arora, Shreyan Gupta, Ananthakrishna Kudupu, Aditya Priyadarshi, Aswathi Mundayatt, Jaya Sreevalsan-Nair</author><pubDate>Tue, 21 Jan 2025 18:57:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12384v1</guid></item><item><title>DiffDoctor: Diagnosing Image Diffusion Models Before Treating</title><link>http://arxiv.org/abs/2501.12382v1</link><description>In spite of the recent progress, image diffusion models still produceartifacts. A common solution is to refine an established model with a qualityassessment system, which generally rates an image in its entirety. In thiswork, we believe problem-solving starts with identification, yielding therequest that the model should be aware of not just the presence of defects inan image, but their specific locations. Motivated by this, we proposeDiffDoctor, a two-stage pipeline to assist image diffusion models in generatingfewer artifacts. Concretely, the first stage targets developing a robustartifact detector, for which we collect a dataset of over 1M flawed synthesizedimages and set up an efficient human-in-the-loop annotation process,incorporating a carefully designed class-balance strategy. The learned artifactdetector is then involved in the second stage to tune the diffusion modelthrough assigning a per-pixel confidence map for each synthesis. Extensiveexperiments on text-to-image diffusion models demonstrate the effectiveness ofour artifact detector as well as the soundness of our diagnose-then-treatdesign.</description><author>Yiyang Wang, Xi Chen, Xiaogang Xu, Sihui Ji, Yu Liu, Yujun Shen, Hengshuang Zhao</author><pubDate>Tue, 21 Jan 2025 18:56:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12382v1</guid></item><item><title>Parallel Sequence Modeling via Generalized Spatial Propagation Network</title><link>http://arxiv.org/abs/2501.12381v1</link><description>We present the Generalized Spatial Propagation Network (GSPN), a newattention mechanism optimized for vision tasks that inherently captures 2Dspatial structures. Existing attention models, including transformers, linearattention, and state-space models like Mamba, process multi-dimensional data as1D sequences, compromising spatial coherence and efficiency. GSPN overcomesthese limitations by directly operating on spatially coherent image data andforming dense pairwise connections through a line-scan approach. Central toGSPN is the Stability-Context Condition, which ensures stable, context-awarepropagation across 2D sequences and reduces the effective sequence length to$\sqrt{N}$ for a square map with N elements, significantly enhancingcomputational efficiency. With learnable, input-dependent weights and noreliance on positional embeddings, GSPN achieves superior spatial fidelity andstate-of-the-art performance in vision tasks, including ImageNetclassification, class-guided image generation, and text-to-image generation.Notably, GSPN accelerates SD-XL with softmax-attention by over $84\times$ whengenerating 16K images.</description><author>Hongjun Wang, Wonmin Byeon, Jiarui Xu, Jinwei Gu, Ka Chun Cheung, Xiaolong Wang, Kai Han, Jan Kautz, Sifei Liu</author><pubDate>Tue, 21 Jan 2025 18:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12381v1</guid></item><item><title>MMVU: Measuring Expert-Level Multi-Discipline Video Understanding</title><link>http://arxiv.org/abs/2501.12380v1</link><description>We introduce MMVU, a comprehensive expert-level, multi-discipline benchmarkfor evaluating foundation models in video understanding. MMVU includes 3,000expert-annotated questions spanning 27 subjects across four core disciplines:Science, Healthcare, Humanities &amp; Social Sciences, and Engineering. Compared toprior benchmarks, MMVU features three key advancements. First, it challengesmodels to apply domain-specific knowledge and perform expert-level reasoning toanalyze specialized-domain videos, moving beyond the basic visual perceptiontypically assessed in current video benchmarks. Second, each example isannotated by human experts from scratch. We implement strict data qualitycontrols to ensure the high quality of the dataset. Finally, each example isenriched with expert-annotated reasoning rationals and relevant domainknowledge, facilitating in-depth analysis. We conduct an extensive evaluationof 32 frontier multimodal foundation models on MMVU. The latestSystem-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highestperformance among the tested models. However, they still fall short of matchinghuman expertise. Through in-depth error analyses and case studies, we offeractionable insights for future advancements in expert-level,knowledge-intensive video understanding for specialized domains.</description><author>Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, Zhijian Xu, Chengye Wang, Weifeng Pan, Ziyao Shangguan, Xiangru Tang, Zhenwen Liang, Yixin Liu, Chen Zhao, Arman Cohan</author><pubDate>Tue, 21 Jan 2025 18:56:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12380v1</guid></item><item><title>Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</title><link>http://arxiv.org/abs/2501.12375v1</link><description>Depth Anything has achieved remarkable success in monocular depth estimationwith strong generalization ability. However, it suffers from temporalinconsistency in videos, hindering its practical applications. Various methodshave been proposed to alleviate this issue by leveraging video generationmodels or introducing priors from optical flow and camera poses. Nonetheless,these methods are only applicable to short videos (&lt; 10 seconds) and require atrade-off between quality and computational efficiency. We propose Video DepthAnything for high-quality, consistent depth estimation in super-long videos(over several minutes) without sacrificing efficiency. We base our model onDepth Anything V2 and replace its head with an efficient spatial-temporal head.We design a straightforward yet effective temporal consistency loss byconstraining the temporal depth gradient, eliminating the need for additionalgeometric priors. The model is trained on a joint dataset of video depth andunlabeled images, similar to Depth Anything V2. Moreover, a novelkey-frame-based strategy is developed for long video inference. Experimentsshow that our model can be applied to arbitrarily long videos withoutcompromising quality, consistency, or generalization ability. Comprehensiveevaluations on multiple video benchmarks demonstrate that our approach sets anew state-of-the-art in zero-shot video depth estimation. We offer models ofdifferent scales to support a range of scenarios, with our smallest modelcapable of real-time performance at 30 FPS.</description><author>Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, Bingyi Kang</author><pubDate>Tue, 21 Jan 2025 18:53:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12375v1</guid></item><item><title>Expertise elevates AI usage: experimental evidence comparing laypeople and professional artists</title><link>http://arxiv.org/abs/2501.12374v1</link><description>Novel capacities of generative AI to analyze and generate cultural artifactsraise inevitable questions about the nature and value of artistic education andhuman expertise. Has AI already leveled the playing field between professionalartists and laypeople, or do trained artistic expressive capacity, curationskills and experience instead enhance the ability to use these new tools? Inthis pre-registered study, we conduct experimental comparisons between 50active artists and a demographically matched sample of laypeople. We designedtwo tasks to approximate artistic practice for testing their capabilities inboth faithful and creative image creation: replicating a reference image, andmoving as far away as possible from it. We developed a bespoke platform whereparticipants used a modern text-to-image model to complete both tasks. We alsocollected and compared participants' sentiments towards AI. On average, artistsproduced more faithful and creative outputs than their lay counterparts,although only by a small margin. While AI may ease content creation,professional expertise is still valuable - even within the confined space ofgenerative AI itself. Finally, we also explored how well an exemplaryvision-capable large language model (GPT-4o) would complete the same tasks, ifgiven the role of an image generation agent, and found it performed on par incopying but outperformed even artists in the creative task. The very bestresults were still produced by humans in both tasks. These outcomes highlightthe importance of integrating artistic skills with AI training to prepareartists and other visual professionals for a technologically evolvinglandscape. We see a potential in collaborative synergy with generative AI,which could reshape creative industries and education in the arts.</description><author>Thomas F. Eisenmann, Andres Karjus, Mar Canet Sola, Levin Brinkmann, Bramantyo Ibrahim Supriyatno, Iyad Rahwan</author><pubDate>Tue, 21 Jan 2025 18:53:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12374v1</guid></item><item><title>Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL</title><link>http://arxiv.org/abs/2501.12372v1</link><description>Large Language Models (LLMs) have demonstrated impressive capabilities acrossa range of natural language processing tasks. In particular, improvements inreasoning abilities and the expansion of context windows have opened newavenues for leveraging these powerful models. NL2SQL is challenging in that thenatural language question is inherently ambiguous, while the SQL generationrequires a precise understanding of complex data schema and semantics. Oneapproach to this semantic ambiguous problem is to provide more and sufficientcontextual information. In this work, we explore the performance and the latency trade-offs of theextended context window (a.k.a., long context) offered by Google'sstate-of-the-art LLM (\textit{gemini-1.5-pro}). We study the impact of variouscontextual information, including column example values, question and SQL querypairs, user-provided hints, SQL documentation, and schema. To the best of ourknowledge, this is the first work to study how the extended context window andextra contextual information can help NL2SQL generation with respect to bothaccuracy and latency cost. We show that long context LLMs are robust and do notget lost in the extended contextual information. Additionally, our long-contextNL2SQL pipeline based on Google's \textit{gemini-pro-1.5} achieve a strongperformance with 67.41\% on BIRD benchmark (dev) without finetuning andexpensive self-consistency based techniques.</description><author>Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, Fatma Ozcan</author><pubDate>Tue, 21 Jan 2025 18:52:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12372v1</guid></item><item><title>Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models</title><link>http://arxiv.org/abs/2501.12370v1</link><description>Scaling the capacity of language models has consistently proven to be areliable approach for improving performance and unlocking new capabilities.Capacity can be primarily defined by two dimensions: the number of modelparameters and the compute per example. While scaling typically involvesincreasing both, the precise interplay between these factors and their combinedcontribution to overall capacity remains not fully understood. We explore thisrelationship in the context of sparse Mixture-of-Expert models (MoEs), whichallow scaling the number of parameters without proportionally increasing theFLOPs per example. We investigate how varying the sparsity level, i.e., theratio of non-active to total parameters, affects model performance in terms ofboth pretraining and downstream performance. We find that under differentconstraints (e.g. parameter size and total training compute), there is anoptimal level of sparsity that improves both training efficiency and modelperformance. These results provide a better understanding of the impact ofsparsity in scaling laws for MoEs and complement existing works in this area,offering insights for designing more efficient architectures.</description><author>Samira Abnar, Harshay Shah, Dan Busbridge, Alaaeldin Mohamed Elnouby Ali, Josh Susskind, Vimal Thilak</author><pubDate>Tue, 21 Jan 2025 18:51:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12370v1</guid></item><item><title>DARB-Splatting: Generalizing Splatting with Decaying Anisotropic Radial Basis Functions</title><link>http://arxiv.org/abs/2501.12369v1</link><description>Splatting-based 3D reconstruction methods have gained popularity with theadvent of 3D Gaussian Splatting, efficiently synthesizing high-quality novelviews. These methods commonly resort to using exponential family functions,such as the Gaussian function, as reconstruction kernels due to theiranisotropic nature, ease of projection, and differentiability in rasterization.However, the field remains restricted to variations within the exponentialfamily, leaving generalized reconstruction kernels largely underexplored,partly due to the lack of easy integrability in 3D to 2D projections. In thislight, we show that a class of decaying anisotropic radial basis functions(DARBFs), which are non-negative functions of the Mahalanobis distance,supports splatting by approximating the Gaussian function's closed-formintegration advantage. With this fresh perspective, we demonstrate up to 34%faster convergence during training and a 15% reduction in memory consumptionacross various DARB reconstruction kernels, while maintaining comparable PSNR,SSIM, and LPIPS results. We will make the code available.</description><author>Vishagar Arunan, Saeedha Nazar, Hashiru Pramuditha, Vinasirajan Viruthshaan, Sameera Ramasinghe, Simon Lucey, Ranga Rodrigo</author><pubDate>Tue, 21 Jan 2025 18:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12369v1</guid></item><item><title>InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model</title><link>http://arxiv.org/abs/2501.12368v1</link><description>Despite the promising performance of Large Vision Language Models (LVLMs) invisual understanding, they occasionally generate incorrect outputs. Whilereward models (RMs) with reinforcement learning or test-time scaling offer thepotential for improving generation quality, a critical gap remains: publiclyavailable multi-modal RMs for LVLMs are scarce, and the implementation detailsof proprietary models are often unclear. We bridge this gap withInternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effectivemulti-modal reward model that aligns LVLMs with human preferences. To ensurethe robustness and versatility of IXC-2.5-Reward, we set up a high-qualitymulti-modal preference corpus spanning text, image, and video inputs acrossdiverse domains, such as instruction following, general understanding,text-rich documents, mathematical reasoning, and video understanding.IXC-2.5-Reward achieves excellent results on the latest multi-modal rewardmodel benchmark and shows competitive performance on text-only reward modelbenchmarks. We further demonstrate three key applications of IXC-2.5-Reward:(1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Rewardwith Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which showsconsistent improvements in instruction following and multi-modal open-endeddialogue; (2) Selecting the best response from candidate responses fortest-time scaling; and (3) Filtering outlier or noisy samples from existingimage and video instruction tuning training data. To ensure reproducibility andfacilitate further research, we have open-sourced all model weights andtraining recipes at https://github.com/InternLM/InternLM-XComposer</description><author>Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, Jiaqi Wang</author><pubDate>Tue, 21 Jan 2025 18:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12368v1</guid></item><item><title>FoundationStereo: Zero-Shot Stereo Matching</title><link>http://arxiv.org/abs/2501.09898v2</link><description>Tremendous progress has been made in deep stereo matching to excel onbenchmark datasets through per-domain fine-tuning. However, achieving strongzero-shot generalization - a hallmark of foundation models in other computervision tasks - remains challenging for stereo matching. We introduceFoundationStereo, a foundation model for stereo depth estimation designed toachieve strong zero-shot generalization. To this end, we first construct alarge-scale (1M stereo pairs) synthetic training dataset featuring largediversity and high photorealism, followed by an automatic self-curationpipeline to remove ambiguous samples. We then design a number of networkarchitecture components to enhance scalability, including a side-tuning featurebackbone that adapts rich monocular priors from vision foundation models tomitigate the sim-to-real gap, and long-range context reasoning for effectivecost volume filtering. Together, these components lead to strong robustness andaccuracy across domains, establishing a new standard in zero-shot stereo depthestimation. Project page: https://nvlabs.github.io/FoundationStereo/</description><author>Bowen Wen, Matthew Trepte, Joseph Aribido, Jan Kautz, Orazio Gallo, Stan Birchfield</author><pubDate>Tue, 21 Jan 2025 18:46:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.09898v2</guid></item><item><title>Budget-constrained Collaborative Renewable Energy Forecasting Market</title><link>http://arxiv.org/abs/2501.12367v1</link><description>Accurate power forecasting from renewable energy sources (RES) is crucial forintegrating additional RES capacity into the power system and realizingsustainability goals. This work emphasizes the importance of integratingdecentralized spatio-temporal data into forecasting models. However,decentralized data ownership presents a critical obstacle to the success ofsuch spatio-temporal models, and incentive mechanisms to foster data-sharingneed to be considered. The main contributions are a) a comparative analysis ofthe forecasting models, advocating for efficient and interpretable spline LASSOregression models, and b) a bidding mechanism within the data/analytics marketto ensure fair compensation for data providers and enable both buyers andsellers to express their data price requirements. Furthermore, an incentivemechanism for time series forecasting is proposed, effectively incorporatingprice constraints and preventing redundant feature allocation. Results showsignificant accuracy improvements and potential monetary gains for datasellers. For wind power data, an average root mean squared error improvement ofover 10% was achieved by comparing forecasts generated by the proposal withlocally generated ones.</description><author>Carla Goncalves, Ricardo J. Bessa, Tiago Teixeira, Joao Vinagre</author><pubDate>Tue, 21 Jan 2025 18:46:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12367v1</guid></item><item><title>Efficient Algorithm for Sparse Fourier Transform of Generalized q-ary Functions</title><link>http://arxiv.org/abs/2501.12365v1</link><description>Computing the Fourier transform of a $q$-ary function$f:\mathbb{Z}_{q}^n\rightarrow \mathbb{R}$, which maps $q$-ary sequences toreal numbers, is an important problem in mathematics with wide-rangingapplications in biology, signal processing, and machine learning. Previousstudies have shown that, under the sparsity assumption, the Fourier transformcan be computed efficiently using fast and sample-efficient algorithms.However, in many practical settings, the function is defined over a moregeneral space -- the space of generalized $q$-ary sequences $\mathbb{Z}_{q_1}\times \mathbb{Z}_{q_2} \times \cdots \times \mathbb{Z}_{q_n}$ -- where each$\mathbb{Z}_{q_i}$ corresponds to integers modulo $q_i$. A naive approachinvolves setting $q=\max_i{q_i}$ and treating the function as $q$-ary, whichresults in heavy computational overheads. Herein, we develop GFast, analgorithm that computes the $S$-sparse Fourier transform of $f$ with a samplecomplexity of $O(Sn)$, computational complexity of $O(Sn \log N)$, and afailure probability that approaches zero as $N=\prod_{i=1}^n q_i \rightarrow\infty$ with $S = N^\delta$ for some $0 \leq \delta &lt; 1$. In the presence ofnoise, we further demonstrate that a robust version of GFast computes thetransform with a sample complexity of $O(Sn^2)$ and computational complexity of$O(Sn^2 \log N)$ under the same high probability guarantees. Using large-scalesynthetic experiments, we demonstrate that GFast computes the sparse Fouriertransform of generalized $q$-ary functions using $16\times$ fewer samples andrunning $8\times$ faster than existing algorithms. In real-world proteinfitness datasets, GFast explains the predictive interactions of a neuralnetwork with $&gt;25\%$ smaller normalized mean-squared error compared to existingalgorithms.</description><author>Darin Tsui, Kunal Talreja, Amirali Aghazadeh</author><pubDate>Tue, 21 Jan 2025 18:45:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12365v1</guid></item><item><title>The Choice of Normalization Influences Shrinkage in Regularized Regression</title><link>http://arxiv.org/abs/2501.03821v2</link><description>Regularized models are often sensitive to the scales of the features in thedata and it has therefore become standard practice to normalize (center andscale) the features before fitting the model. But there are many different waysto normalize the features and the choice may have dramatic effects on theresulting model. In spite of this, there has so far been no research on thistopic. In this paper, we begin to bridge this knowledge gap by studyingnormalization in the context of lasso, ridge, and elastic net regression. Wefocus on normal and binary features and show that the class balances of binaryfeatures directly influences the regression coefficients and that this effectdepends on the combination of normalization and regularization methods used. Wedemonstrate that this effect can be mitigated by scaling binary features withtheir variance in the case of the lasso and standard deviation in the case ofridge regression, but that this comes at the cost of increased variance. Forthe elastic net, we show that scaling the penalty weights, rather than thefeatures, can achieve the same effect. Finally, we also tackle mixes of binaryand normal features as well as interactions and provide some initial results onhow to normalize features in these cases.</description><author>Johan Larsson, Jonas Wallin</author><pubDate>Tue, 21 Jan 2025 18:42:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.03821v2</guid></item><item><title>Measured Hockey-Stick Divergence and its Applications to Quantum Pufferfish Privacy</title><link>http://arxiv.org/abs/2501.12359v1</link><description>The hockey-stick divergence is a fundamental quantity characterizing severalstatistical privacy frameworks that ensure privacy for classical and quantumdata. In such quantum privacy frameworks, the adversary is allowed to performall possible measurements. However, in practice, there are typicallylimitations to the set of measurements that can be performed. To this end,here, we comprehensively analyze the measured hockey-stick divergence underseveral classes of practically relevant measurement classes. We prove severalof its properties, including data processing and convexity. We show that it isefficiently computable by semi-definite programming for some classes ofmeasurements and can be analytically evaluated for Werner and isotropic states.Notably, we show that the measured hockey-stick divergence characterizesoptimal privacy parameters in the quantum pufferfish privacy framework. Withthis connection and the developed technical tools, we enable methods toquantify and audit privacy for several practically relevant settings. Lastly,we introduce the measured hockey-stick divergence of channels and explore itsapplications in ensuring privacy for channels.</description><author>Theshani Nuradha, Vishal Singh, Mark M. Wilde</author><pubDate>Tue, 21 Jan 2025 18:39:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12359v1</guid></item><item><title>Vision-Language Models for Automated Chest X-ray Interpretation: Leveraging ViT and GPT-2</title><link>http://arxiv.org/abs/2501.12356v1</link><description>Radiology plays a pivotal role in modern medicine due to its non-invasivediagnostic capabilities. However, the manual generation of unstructured medicalreports is time consuming and prone to errors. It creates a significantbottleneck in clinical workflows. Despite advancements in AI-generatedradiology reports, challenges remain in achieving detailed and accurate reportgeneration. In this study we have evaluated different combinations ofmultimodal models that integrate Computer Vision and Natural LanguageProcessing to generate comprehensive radiology reports. We employed apretrained Vision Transformer (ViT-B16) and a SWIN Transformer as the imageencoders. The BART and GPT-2 models serve as the textual decoders. We usedChest X-ray images and reports from the IU-Xray dataset to evaluate theusability of the SWIN Transformer-BART, SWIN Transformer-GPT-2, ViT-B16-BARTand ViT-B16-GPT-2 models for report generation. We aimed at finding the bestcombination among the models. The SWIN-BART model performs as thebest-performing model among the four models achieving remarkable results inalmost all the evaluation metrics like ROUGE, BLEU and BERTScore.</description><author>Md. Rakibul Islam, Md. Zahid Hossain, Mustofa Ahmed, Most. Sharmin Sultana Samu</author><pubDate>Tue, 21 Jan 2025 18:36:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12356v1</guid></item><item><title>Diffusion-aware Censored Gaussian Processes for Demand Modelling</title><link>http://arxiv.org/abs/2501.12354v1</link><description>Inferring the true demand for a product or a service from aggregate data isoften challenging due to the limited available supply, thus resulting inobservations that are censored and correspond to the realized demand, therebynot accounting for the unsatisfied demand. Censored regression models are ableto account for the effect of censoring due to the limited supply, but theydon't consider the effect of substitutions, which may cause the demand forsimilar alternative products or services to increase. This paper proposesDiffusion-aware Censored Demand Models, which combine a Tobit likelihood with agraph diffusion process in order to model the latent process of transfer ofunsatisfied demand between similar products or services. We instantiate thisnew class of models under the framework of GPs and, based on both simulated andreal-world data for modeling sales, bike-sharing demand, and EV chargingdemand, demonstrate its ability to better recover the true demand and producemore accurate out-of-sample predictions.</description><author>Filipe Rodrigues</author><pubDate>Tue, 21 Jan 2025 18:33:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12354v1</guid></item><item><title>Test-time regression: a unifying framework for designing sequence models with associative memory</title><link>http://arxiv.org/abs/2501.12352v1</link><description>Sequences provide a remarkably general way to represent and processinformation. This powerful abstraction has placed sequence modeling at thecenter of modern deep learning applications, inspiring numerous architecturesfrom transformers to recurrent networks. While this fragmented development hasyielded powerful models, it has left us without a unified framework tounderstand their fundamental similarities and explain their effectiveness. Wepresent a unifying framework motivated by an empirical observation: effectivesequence models must be able to perform associative recall. Our key insight isthat memorizing input tokens through an associative memory is equivalent toperforming regression at test-time. This regression-memory correspondenceprovides a framework for deriving sequence models that can perform associativerecall, offering a systematic lens to understand seemingly ad-hoc architecturalchoices. We show numerous recent architectures -- including linear attentionmodels, their gated variants, state-space models, online learners, and softmaxattention -- emerge naturally as specific approaches to test-time regression.Each architecture corresponds to three design choices: the relative importanceof each association, the regressor function class, and the optimizationalgorithm. This connection leads to new understanding: we provide theoreticaljustification for QKNorm in softmax attention, and we motivate higher-ordergeneralizations of softmax attention. Beyond unification, our work unlocksdecades of rich statistical tools that can guide future development of morepowerful yet principled sequence models.</description><author>Ke Alexander Wang, Jiaxin Shi, Emily B. Fox</author><pubDate>Tue, 21 Jan 2025 18:32:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12352v1</guid></item><item><title>CYCle: Choosing Your Collaborators Wisely to Enhance Collaborative Fairness in Decentralized Learning</title><link>http://arxiv.org/abs/2501.12344v1</link><description>Collaborative learning (CL) enables multiple participants to jointly trainmachine learning (ML) models on decentralized data sources without raw datasharing. While the primary goal of CL is to maximize the expected accuracy gainfor each participant, it is also important to ensure that the gains are fairlydistributed. Specifically, no client should be negatively impacted by thecollaboration, and the individual gains must ideally be commensurate with thecontributions. Most existing CL algorithms require central coordination andfocus on the gain maximization objective while ignoring collaborative fairness.In this work, we first show that the existing measure of collaborative fairnessbased on the correlation between accuracy values without and with collaborationhas drawbacks because it does not account for negative collaboration gain. Weargue that maximizing mean collaboration gain (MCG) while simultaneouslyminimizing the collaboration gain spread (CGS) is a fairer alternative. Next,we propose the CYCle protocol that enables individual participants in a privatedecentralized learning (PDL) framework to achieve this objective through anovel reputation scoring method based on gradient alignment between the localcross-entropy and distillation losses. Experiments on the CIFAR-10, CIFAR-100,and Fed-ISIC2019 datasets empirically demonstrate the effectiveness of theCYCle protocol to ensure positive and fair collaboration gain for allparticipants, even in cases where the data distributions of participants arehighly skewed. For the simple mean estimation problem with two participants, wealso theoretically show that CYCle performs better than standard FedAvg,especially when there is large statistical heterogeneity.</description><author>Nurbek Tastan, Samuel Horvath, Karthik Nandakumar</author><pubDate>Tue, 21 Jan 2025 18:22:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12344v1</guid></item><item><title>Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens</title><link>http://arxiv.org/abs/2410.14655v2</link><description>Language models are often trained to maximize the likelihood of the nexttoken given past tokens in the training dataset. However, during inferencetime, they are utilized differently, generating text sequentially andauto-regressively by using previously generated tokens as input to predict thenext one. Marginal differences in predictions at each step can cascade oversuccessive steps, resulting in different distributions from what the modelswere trained for and potentially leading to unpredictable behavior. This paperproposes two simple approaches based on model own generation to address thisdiscrepancy between the training and inference time. Our first approach isBatch-Scheduled Sampling, where, during training, we stochastically choosebetween the ground-truth token from the dataset and the model's own generatedtoken as input to predict the next token. This is done in an offline manner,modifying the context window by interleaving ground-truth tokens with thosegenerated by the model. Our second approach is Reference-Answer-basedCorrection, where we explicitly incorporate a self-correction capability intothe model during training. This enables the model to effectively self-correctthe gaps between the generated sequences and the ground truth data withoutrelying on an external oracle model. By incorporating our proposed strategiesduring training, we have observed an overall improvement in performancecompared to baseline methods, as demonstrated by our extensive experimentsusing summarization, general question-answering, and math question-answeringtasks.</description><author>Zhepeng Cen, Yao Liu, Siliang Zeng, Pratik Chaudhari, Huzefa Rangwala, George Karypis, Rasool Fakoor</author><pubDate>Tue, 21 Jan 2025 18:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14655v2</guid></item><item><title>Treefix: Enabling Execution with a Tree of Prefixes</title><link>http://arxiv.org/abs/2501.12339v1</link><description>The ability to execute code is a prerequisite for various dynamic programanalyses. Learning-guided execution has been proposed as an approach to enablethe execution of arbitrary code snippets by letting a neural model predictlikely values for any missing variables. Although state-of-the-artlearning-guided execution approaches, such as LExecutor, can enable theexecution of a relative high amount of code, they are limited to predicting arestricted set of possible values and do not use any feedback from previousexecutions to execute even more code. This paper presents Treefix, a novellearning-guided execution approach that leverages LLMs to iteratively createcode prefixes that enable the execution of a given code snippet. The approachaddresses the problem in a multi-step fashion, where each step uses feedbackabout the code snippet and its execution to instruct an LLM to improve apreviously generated prefix. This process iteratively creates a tree ofprefixes, a subset of which is returned to the user as prefixes that maximizethe number of executed lines in the code snippet. In our experiments with twodatasets of Python code snippets, Treefix achieves 25% and 7% more coveragerelative to the current state of the art in learning-guided execution, coveringa total of 84% and 82% of all lines in the code snippets.</description><author>Beatriz Souza, Michael Pradel</author><pubDate>Tue, 21 Jan 2025 18:13:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12339v1</guid></item><item><title>FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with XLM-Roberta Sentence Embeddings and Deep Neural Regression</title><link>http://arxiv.org/abs/2501.12336v1</link><description>This paper presents results of our system for CoMeDi Shared Task, focusing onSubtask 2: Disagreement Ranking. Our system leverages sentence embeddingsgenerated by the paraphrase-xlm-r-multilingual-v1 model, combined with a deepneural regression model incorporating batch normalization and dropout forimproved generalization. By predicting the mean of pairwise judgmentdifferences between annotators, our method explicitly targets disagreementranking, diverging from traditional "gold label" aggregation approaches. Weoptimized our system with a customized architecture and training procedure,achieving competitive performance in Spearman correlation against meandisagreement labels. Our results highlight the importance of robust embeddings,effective model architecture, and careful handling of judgment differences forranking disagreement in multilingual contexts. These findings provide insightsinto the use of contextualized representations for ordinal judgment tasks andopen avenues for further refinement of disagreement prediction models.</description><author>Phuoc Duong Huy Chu</author><pubDate>Tue, 21 Jan 2025 18:10:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12336v1</guid></item><item><title>Automatic Labelling with Open-source LLMs using Dynamic Label Schema Integration</title><link>http://arxiv.org/abs/2501.12332v1</link><description>Acquiring labelled training data remains a costly task in real world machinelearning projects to meet quantity and quality requirements. Recently LargeLanguage Models (LLMs), notably GPT-4, have shown great promises in labellingdata with high accuracy. However, privacy and cost concerns prevent theubiquitous use of GPT-4. In this work, we explore effectively leveragingopen-source models for automatic labelling. We identify integrating labelschema as a promising technology but found that naively using the labeldescription for classification leads to poor performance on high cardinalitytasks. To address this, we propose Retrieval Augmented Classification (RAC) forwhich LLM performs inferences for one label at a time using corresponding labelschema; we start with the most related label and iterates until a label ischosen by the LLM. We show that our method, which dynamically integrates labeldescription, leads to performance improvements in labelling tasks. We furthershow that by focusing only on the most promising labels, RAC can trade offbetween label quality and coverage - a property we leverage to automaticallylabel our internal datasets.</description><author>Thomas Walshe, Sae Young Moon, Chunyang Xiao, Yawwani Gunawardana, Fran Silavong</author><pubDate>Tue, 21 Jan 2025 18:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12332v1</guid></item><item><title>$\spadesuit$ SPADE $\spadesuit$ Split Peak Attention DEcomposition</title><link>http://arxiv.org/abs/2411.05852v2</link><description>Demand forecasting faces challenges induced by Peak Events (PEs)corresponding to special periods such as promotions and holidays. Peak eventscreate significant spikes in demand followed by demand ramp down periods.Neural networks like MQCNN and MQT overreact to demand peaks by carrying overthe elevated PE demand into subsequent Post-Peak-Event (PPE) periods, resultingin significantly over-biased forecasts. To tackle this challenge, we introducea neural forecasting model called Split Peak Attention DEcomposition, SPADE.This model reduces the impact of PEs on subsequent forecasts by modelingforecasting as consisting of two separate tasks: one for PEs; and the other forthe rest. Its architecture then uses masked convolution filters and aspecialized Peak Attention module. We show SPADE's performance on a worldwideretail dataset with hundreds of millions of products. Our results reveal anoverall PPE improvement of 4.5%, a 30% improvement for most affected forecastsafter promotions and holidays, and an improvement in PE accuracy by 3.9%,relative to current production models.</description><author>Malcolm Wolff, Kin G. Olivares, Boris Oreshkin, Sunny Ruan, Sitan Yang, Abhinav Katoch, Shankar Ramasubramanian, Youxin Zhang, Michael W. Mahoney, Dmitry Efimov, Vincent Quenneville-Bélair</author><pubDate>Tue, 21 Jan 2025 18:06:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05852v2</guid></item><item><title>Cinepro: Robust Training of Foundation Models for Cancer Detection in Prostate Ultrasound Cineloops</title><link>http://arxiv.org/abs/2501.12331v1</link><description>Prostate cancer (PCa) detection using deep learning (DL) models has shownpotential for enhancing real-time guidance during biopsies. However, prostateultrasound images lack pixel-level cancer annotations, introducing label noise.Current approaches often focus on limited regions of interest (ROIs),disregarding anatomical context necessary for accurate diagnosis. Foundationmodels can overcome this limitation by analyzing entire images to captureglobal spatial relationships; however, they still encounter challenges stemmingfrom the weak labels associated with coarse pathology annotations in ultrasounddata. We introduce Cinepro, a novel framework that strengthens foundationmodels' ability to localize PCa in ultrasound cineloops. Cinepro adapts robusttraining by integrating the proportion of cancer tissue reported by pathologyin a biopsy core into its loss function to address label noise, providing amore nuanced supervision. Additionally, it leverages temporal data acrossmultiple frames to apply robust augmentations, enhancing the model's ability tolearn stable cancer-related features. Cinepro demonstrates superior performanceon a multi-center prostate ultrasound dataset, achieving an AUROC of 77.1% anda balanced accuracy of 83.8%, surpassing current benchmarks. These findingsunderscore Cinepro's promise in advancing foundation models for weakly labeledultrasound data.</description><author>Mohamed Harmanani, Amoon Jamzad, Minh Nguyen Nhat To, Paul F. R. Wilson, Zhuoxin Guo, Fahimeh Fooladgar, Samira Sojoudi, Mahdi Gilany, Silvia Chang, Peter Black, Michael Leveridge, Robert Siemens, Purang Abolmaesumi, Parvin Mousavi</author><pubDate>Tue, 21 Jan 2025 18:05:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12331v1</guid></item><item><title>The Gap Between Principle and Practice of Lossy Image Coding</title><link>http://arxiv.org/abs/2501.12330v1</link><description>Lossy image coding is the art of computing that is principally bounded by theimage's rate-distortion function. This bound, though never accuratelycharacterized, has been approached practically via deep learning technologiesin recent years. Indeed, learned image coding schemes allow direct optimizationof the joint rate-distortion cost, thereby outperforming the handcrafted imagecoding schemes by a large margin. Still, it is observed that there is room forfurther improvement in the rate-distortion performance of learned image coding.In this article, we identify the gap between the ideal rate-distortion functionforecasted by Shannon's information theory and the empirical rate-distortionfunction achieved by the state-of-the-art learned image coding schemes,revealing that the gap is incurred by five different effects: modeling effect,approximation effect, amortization effect, digitization effect, and asymptoticeffect. We design simulations and experiments to quantitively evaluate the lastthree effects, which demonstrates the high potential of future lossy imagecoding technologies.</description><author>Haotian Zhang, Dong Liu</author><pubDate>Tue, 21 Jan 2025 17:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12330v1</guid></item><item><title>Beyond Position: the emergence of wavelet-like properties in Transformers</title><link>http://arxiv.org/abs/2410.18067v3</link><description>This paper studies how transformer models develop robust wavelet-likeproperties that effectively compensate for the theoretical limitations ofRotary Position Embeddings (RoPE), providing insights into how these networksprocess sequential information across different scales. Through theoreticalanalysis and empirical validation across models ranging from 1B to 12Bparameters, we show that attention heads naturally evolve to implementmulti-resolution processing analogous to wavelet transforms. Our analysisestablishes that attention heads consistently organize into complementaryfrequency bands with systematic power distribution patterns, and thesewavelet-like characteristics become more pronounced in larger models. Weprovide mathematical analysis showing how these properties align with optimalsolutions to the fundamental uncertainty principle between positional precisionand frequency resolution. Our findings suggest that the effectiveness of moderntransformer architectures stems significantly from their development of optimalmulti-resolution decompositions that naturally address the theoreticalconstraints of position encoding.</description><author>Valeria Ruscio, Fabrizio Silvestri</author><pubDate>Tue, 21 Jan 2025 17:50:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18067v3</guid></item><item><title>VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model</title><link>http://arxiv.org/abs/2501.12327v1</link><description>We present VARGPT, a novel multimodal large language model (MLLM) thatunifies visual understanding and generation within a single autoregressiveframework. VARGPT employs a next-token prediction paradigm for visualunderstanding and a next-scale prediction paradigm for visual autoregressivegeneration. VARGPT innovatively extends the LLaVA architecture, achievingefficient scale-wise autoregressive visual generation within MLLMs whileseamlessly accommodating mixed-modal input and output within a single modelframework. Our VARGPT undergoes a three-stage unified training process onspecially curated datasets, comprising a pre-training phase and two mixedvisual instruction-tuning phases. The unified training strategy are designed toachieve alignment between visual and textual features, enhance instructionfollowing for both understanding and generation, and improve visual generationquality, respectively. Despite its LLAVA-based architecture for multimodelunderstanding, VARGPT significantly outperforms LLaVA-1.5 across variousvision-centric benchmarks, such as visual question-answering and reasoningtasks. Notably, VARGPT naturally supports capabilities in autoregressive visualgeneration and instruction-to-image synthesis, showcasing its versatility inboth visual understanding and generation tasks. Project page is at:\url{https://vargpt-1.github.io/}</description><author>Xianwei Zhuang, Yuxin Xie, Yufan Deng, Liming Liang, Jinghan Ru, Yuguo Yin, Yuexian Zou</author><pubDate>Tue, 21 Jan 2025 17:50:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12327v1</guid></item><item><title>UI-TARS: Pioneering Automated GUI Interaction with Native Agents</title><link>http://arxiv.org/abs/2501.12326v1</link><description>This paper introduces UI-TARS, a native GUI agent model that solely perceivesthe screenshots as input and performs human-like interactions (e.g., keyboardand mouse operations). Unlike prevailing agent frameworks that depend onheavily wrapped commercial models (e.g., GPT-4o) with expert-crafted promptsand workflows, UI-TARS is an end-to-end model that outperforms thesesophisticated frameworks. Experiments demonstrate its superior performance:UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluatingperception, grounding, and GUI task execution. Notably, in the OSWorldbenchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15steps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld,UI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates severalkey innovations: (1) Enhanced Perception: leveraging a large-scale dataset ofGUI screenshots for context-aware understanding of UI elements and precisecaptioning; (2) Unified Action Modeling, which standardizes actions into aunified space across platforms and achieves precise grounding and interactionthrough large-scale action traces; (3) System-2 Reasoning, which incorporatesdeliberate reasoning into multi-step decision making, involving multiplereasoning patterns such as task decomposition, reflection thinking, milestonerecognition, etc. (4) Iterative Training with Reflective Online Traces, whichaddresses the data bottleneck by automatically collecting, filtering, andreflectively refining new interaction traces on hundreds of virtual machines.Through iterative training and reflection tuning, UI-TARS continuously learnsfrom its mistakes and adapts to unforeseen situations with minimal humanintervention. We also analyze the evolution path of GUI agents to guide thefurther development of this domain.</description><author>Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, Guang Shi</author><pubDate>Tue, 21 Jan 2025 17:48:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12326v1</guid></item><item><title>Let There Be Light: Robust Lensless Imaging Under External Illumination With Deep Learning</title><link>http://arxiv.org/abs/2409.16766v2</link><description>Lensless cameras relax the design constraints of traditional cameras byshifting image formation from analog optics to digital post-processing. Whilenew camera designs and applications can be enabled, lensless imaging is verysensitive to unwanted interference (other sources, noise, etc.). In this work,we address a prevalent noise source that has not been studied for lenslessimaging: external illumination e.g. from ambient and direct lighting. Beingrobust to a variety of lighting conditions would increase the practicality andadoption of lensless imaging. To this end, we propose multiple recoveryapproaches that account for external illumination by incorporating its estimateinto the image recovery process. At the core is a physics-based reconstructionthat combines learnable image recovery and denoisers, all of whose parametersare trained using experimentally gathered data. Compared to standardreconstruction methods, our approach yields significant qualitative andquantitative improvements. We open-source our implementations and a 25K datasetof measurements under multiple lighting conditions.</description><author>Eric Bezzam, Stefan Peters, Martin Vetterli</author><pubDate>Tue, 21 Jan 2025 17:47:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16766v2</guid></item><item><title>Deep Learning Based Segmentation of Blood Vessels from H&amp;E Stained Oesophageal Adenocarcinoma Whole-Slide Images</title><link>http://arxiv.org/abs/2501.12323v1</link><description>Blood vessels (BVs) play a critical role in the Tumor Micro-Environment(TME), potentially influencing cancer progression and treatment response.However, manually quantifying BVs in Hematoxylin and Eosin (H&amp;E) stained imagesis challenging and labor-intensive due to their heterogeneous appearances. Wepropose a novel approach of constructing guiding maps to improve theperformance of state-of-the-art segmentation models for BV segmentation, theguiding maps encourage the models to learn representative features of BVs. Thisis particularly beneficial for computational pathology, where labeled trainingdata is often limited and large models are prone to overfitting. We havequantitative and qualitative results to demonstrate the efficacy of ourapproach in improving segmentation accuracy. In future, we plan to validatethis method to segment BVs across various tissue types and investigate the roleof cellular structures in relation to BVs in the TME.</description><author>Jiaqi Lv, Stefan S Antonowicz, Shan E Ahmed Raza</author><pubDate>Tue, 21 Jan 2025 17:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12323v1</guid></item><item><title>Metric for Evaluating Performance of Reference-Free Demorphing Methods</title><link>http://arxiv.org/abs/2501.12319v1</link><description>A facial morph is an image created by combining two (or more) face imagespertaining to two (or more) distinct identities. Reference-free face demorphinginverts the process and tries to recover the face images constituting a facialmorph without using any other information. However, there is no consensus onthe evaluation metrics to be used to evaluate and compare such demorphingtechniques. In this paper, we first analyze the shortcomings of the demorphingmetrics currently used in the literature. We then propose a new metric calledbiometrically cross-weighted IQA that overcomes these issues and extensivelybenchmark current methods on the proposed metric to show its efficacy.Experiments on three existing demorphing methods and six datasets on twocommonly used face matchers validate the efficacy of our proposed metric.</description><author>Nitish Shukla, Arun Ross</author><pubDate>Tue, 21 Jan 2025 17:38:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12319v1</guid></item><item><title>BlanketGen2-Fit3D: Synthetic Blanket Augmentation Towards Improving Real-World In-Bed Blanket Occluded Human Pose Estimation</title><link>http://arxiv.org/abs/2501.12318v1</link><description>Human Pose Estimation (HPE) from monocular RGB images is crucial for clinicalin-bed skeleton-based action recognition, however, it poses unique challengesfor HPE models due to the frequent presence of blankets occluding the person,while labeled HPE data in this scenario is scarce. To address this we introduceBlanketGen2-Fit3D (BG2-Fit3D), an augmentation of Fit3D dataset that contains1,217,312 frames with synthetic photo-realistic blankets. To generate it weused BlanketGen2, our new and improved version of our BlanketGen pipeline thatsimulates synthetic blankets using ground-truth Skinned Multi-Person Linearmodel (SMPL) meshes and then renders them as transparent images that can belayered on top of the original frames. This dataset was used in combinationwith the original Fit3D to finetune the ViTPose-B HPE model, to evaluatesynthetic blanket augmentation effectiveness. The trained models were furtherevaluated on a real-world blanket occluded in-bed HPE dataset (SLP dataset).Comparing architectures trained on only Fit3D with the ones trained with oursynthetic blanket augmentation the later improved pose estimation performanceon BG2-Fit3D, the synthetic blanket occluded dataset significantly to (0.977Percentage of Correct Keypoints (PCK), 0.149 Normalized Mean Error (NME)) withan absolute 4.4% PCK increase. Furthermore, the test results on SLPdemonstrated the utility of synthetic data augmentation by improvingperformance by an absolute 2.3% PCK, on real-world images with the posesoccluded by real blankets. These results show synthetic blanket augmentationhas the potential to improve in-bed blanket occluded HPE from RGB images. Thedataset as well as the code will be made available to the public.</description><author>Tamás Karácsony, João Carmona, João Paulo Silva Cunha</author><pubDate>Tue, 21 Jan 2025 17:38:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12318v1</guid></item><item><title>Uncertainty Quantification With Noise Injection in Neural Networks: A Bayesian Perspective</title><link>http://arxiv.org/abs/2501.12314v1</link><description>Model uncertainty quantification involves measuring and evaluating theuncertainty linked to a model's predictions, helping assess their reliabilityand confidence. Noise injection is a technique used to enhance the robustnessof neural networks by introducing randomness. In this paper, we establish aconnection between noise injection and uncertainty quantification from aBayesian standpoint. We theoretically demonstrate that injecting noise into theweights of a neural network is equivalent to Bayesian inference on a deepGaussian process. Consequently, we introduce a Monte Carlo Noise Injection(MCNI) method, which involves injecting noise into the parameters duringtraining and performing multiple forward propagations during inference toestimate the uncertainty of the prediction. Through simulation and experimentson regression and classification tasks, our method demonstrates superiorperformance compared to the baseline model.</description><author>Xueqiong Yuan, Jipeng Li, Ercan Engin Kuruoglu</author><pubDate>Tue, 21 Jan 2025 17:28:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12314v1</guid></item><item><title>A Hybrid Supervised and Self-Supervised Graph Neural Network for Edge-Centric Applications</title><link>http://arxiv.org/abs/2501.12309v1</link><description>This paper presents a novel graph-based deep learning model for tasksinvolving relations between two nodes (edge-centric tasks), where the focuslies on predicting relationships and interactions between pairs of nodes ratherthan node properties themselves. This model combines supervised andself-supervised learning, taking into account for the loss function theembeddings learned and patterns with and without ground truth. Additionally itincorporates an attention mechanism that leverages both node and edge features.The architecture, trained end-to-end, comprises two primary components:embedding generation and prediction. First, a graph neural network (GNN)transform raw node features into dense, low-dimensional embeddings,incorporating edge attributes. Then, a feedforward neural model processes thenode embeddings to produce the final output. Experiments demonstrate that ourmodel matches or exceeds existing methods for protein-protein interactionsprediction and Gene Ontology (GO) terms prediction. The model also performseffectively with one-hot encoding for node features, providing a solution forthe previously unsolved problem of predicting similarity between compounds withunknown structures.</description><author>Eugenio Borzone, Leandro Di Persia, Matias Gerard</author><pubDate>Tue, 21 Jan 2025 17:26:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12309v1</guid></item><item><title>A recent evaluation on the performance of LLMs on radiation oncology physics using questions of randomly shuffled options</title><link>http://arxiv.org/abs/2412.10622v3</link><description>Purpose: We present an updated study evaluating the performance of largelanguage models (LLMs) in answering radiation oncology physics questions,focusing on the recently released models. Methods: A set of 100 multiple-choice radiation oncology physics questions,previously created by a well-experienced physicist, was used for this study.The answer options of the questions were randomly shuffled to create "new" examsets. Five LLMs -- OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro,and Claude 3.5 Sonnet -- with the versions released before September 30, 2024,were queried using these new exam sets. To evaluate their deductive reasoningability, the correct answer options in the questions were replaced with "Noneof the above." Then, the explain-first and step-by-step instruction promptswere used to test if this strategy improved their reasoning ability. Theperformance of the LLMs was compared with the answers from medical physicists. Results: All models demonstrated expert-level performance on these questions,with o1-preview even surpassing medical physicists with a majority vote. Whenreplacing the correct answer options with 'None of the above', all modelsexhibited a considerable decline in performance, suggesting room forimprovement. The explain-first and step-by-step instruction prompts helpedenhance the reasoning ability of the LLaMA 3.1 (405B), Gemini 1.5 Pro, andClaude 3.5 Sonnet models. Conclusion: These recently released LLMs demonstrated expert-levelperformance in answering radiation oncology physics questions, exhibiting greatpotential to assist in radiation oncology physics education and training.</description><author>Peilong Wang, Jason Holmes, Zhengliang Liu, Dequan Chen, Tianming Liu, Jiajian Shen, Wei Liu</author><pubDate>Tue, 21 Jan 2025 17:20:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10622v3</guid></item><item><title>LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent Diffusion Models</title><link>http://arxiv.org/abs/2405.14477v2</link><description>Advances in latent diffusion models (LDMs) have revolutionizedhigh-resolution image generation, but the design space of the autoencoder thatis central to these systems remains underexplored. In this paper, we introduceLiteVAE, a new autoencoder design for LDMs, which leverages the 2D discretewavelet transform to enhance scalability and computational efficiency overstandard variational autoencoders (VAEs) with no sacrifice in output quality.We investigate the training methodologies and the decoder architecture ofLiteVAE and propose several enhancements that improve the training dynamics andreconstruction quality. Our base LiteVAE model matches the quality of theestablished VAEs in current LDMs with a six-fold reduction in encoderparameters, leading to faster training and lower GPU memory requirements, whileour larger model outperforms VAEs of comparable complexity across all evaluatedmetrics (rFID, LPIPS, PSNR, and SSIM).</description><author>Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, Romann M. Weber</author><pubDate>Tue, 21 Jan 2025 17:15:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14477v2</guid></item><item><title>LLM-Assisted Knowledge Graph Completion for Curriculum and Domain Modelling in Personalized Higher Education Recommendations</title><link>http://arxiv.org/abs/2501.12300v1</link><description>While learning personalization offers great potential for learners, modernpractices in higher education require a deeper consideration of domain modelsand learning contexts, to develop effective personalization algorithms. Thispaper introduces an innovative approach to higher education curriculummodelling that utilizes large language models (LLMs) for knowledge graph (KG)completion, with the goal of creating personalized learning-pathrecommendations. Our research focuses on modelling university subjects andlinking their topics to corresponding domain models, enabling the integrationof learning modules from different faculties and institutions in the student'slearning path. Central to our approach is a collaborative process, where LLMsassist human experts in extracting high-quality, fine-grained topics fromlecture materials. We develop a domain, curriculum, and user models foruniversity modules and stakeholders. We implement this model to create the KGfrom two study modules: Embedded Systems and Development of Embedded SystemsUsing FPGA. The resulting KG structures the curriculum and links it to thedomain models. We evaluate our approach through qualitative expert feedback andquantitative graph quality metrics. Domain experts validated the relevance andaccuracy of the model, while the graph quality metrics measured the structuralproperties of our KG. Our results show that the LLM-assisted graph completionapproach enhances the ability to connect related courses across disciplines topersonalize the learning experience. Expert feedback also showed highacceptance of the proposed collaborative approach for concept extraction andclassification.</description><author>Hasan Abu-Rasheed, Constance Jumbo, Rashed Al Amin, Christian Weber, Veit Wiese, Roman Obermaisser, Madjid Fathi</author><pubDate>Tue, 21 Jan 2025 17:13:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12300v1</guid></item><item><title>Sublinear Variational Optimization of Gaussian Mixture Models with Millions to Billions of Parameters</title><link>http://arxiv.org/abs/2501.12299v1</link><description>Gaussian Mixture Models (GMMs) range among the most frequently used machinelearning models. However, training large, general GMMs becomes computationallyprohibitive for datasets with many data points $N$ of high-dimensionality $D$.For GMMs with arbitrary covariances, we here derive a highly efficientvariational approximation, which is integrated with mixtures of factoranalyzers (MFAs). For GMMs with $C$ components, our proposed algorithmsignificantly reduces runtime complexity per iteration from$\mathcal{O}(NCD^2)$ to a complexity scaling linearly with $D$ and remainingconstant w.r.t. $C$. Numerical validation of this theoretical complexityreduction then shows the following: the distance evaluations required for theentire GMM optimization process scale sublinearly with $NC$. On large-scalebenchmarks, this sublinearity results in speed-ups of an order-of-magnitudecompared to the state-of-the-art. As a proof of concept, we train GMMs withover 10 billion parameters on about 100 million images, and observe trainingtimes of approximately nine hours on a single state-of-the-art CPU.</description><author>Sebastian Salwig, Till Kahlke, Florian Hirschberger, Dennis Forster, Jörg Lücke</author><pubDate>Tue, 21 Jan 2025 17:11:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12299v1</guid></item><item><title>RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with Retrieval-Augmented Learning</title><link>http://arxiv.org/abs/2501.12296v1</link><description>In the pursuit of robust autonomous driving systems, models trained onreal-world datasets often struggle to adapt to new environments, particularlywhen confronted with corner cases such as extreme weather conditions.Collecting these corner cases in the real world is non-trivial, whichnecessitates the use of simulators for validation. However,the highcomputational cost and the domain gap in data distribution have hindered theseamless transition between real and simulated driving scenarios. To tacklethis challenge, we propose Retrieval-Augmented Learning for Autonomous Driving(RALAD), a novel framework designed to bridge the real-to-sim gap at a lowcost. RALAD features three primary designs, including (1) domain adaptation viaan enhanced Optimal Transport (OT) method that accounts for both individual andgrouped image distances, (2) a simple and unified framework that can be appliedto various models, and (3) efficient fine-tuning techniques that freeze thecomputationally expensive layers while maintaining robustness. Experimentalresults demonstrate that RALAD compensates for the performance degradation insimulated environments while maintaining accuracy in real-world scenariosacross three different models. Taking Cross View as an example, the mIOU andmAP metrics in real-world scenarios remain stable before and after RALADfine-tuning, while in simulated environments,the mIOU and mAP metrics areimproved by 10.30% and 12.29%, respectively. Moreover, the re-training cost ofour approach is reduced by approximately 88.1%. Our code is available athttps://github.com/JiachengZuo/RALAD.git.</description><author>Jiacheng Zuo, Haibo Hu, Zikang Zhou, Yufei Cui, Ziquan Liu, Jianping Wang, Nan Guan, Jin Wang, Chun Jason Xue</author><pubDate>Tue, 21 Jan 2025 17:03:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12296v1</guid></item><item><title>Towards Accurate Unified Anomaly Segmentation</title><link>http://arxiv.org/abs/2501.12295v1</link><description>Unsupervised anomaly detection (UAD) from images strives to model normal datadistributions, creating discriminative representations to distinguish andprecisely localize anomalies. Despite recent advancements in the efficient andunified one-for-all scheme, challenges persist in accurately segmentinganomalies for further monitoring. Moreover, this problem is obscured by thewidely-used AUROC metric under imbalanced UAD settings. This motivates us toemphasize the significance of precise segmentation of anomaly pixels using pAPand DSC as metrics. To address the unsolved segmentation task, we introduce theUnified Anomaly Segmentation (UniAS). UniAS presents a multi-level hybridpipeline that progressively enhances normal information from coarse to fine,incorporating a novel multi-granularity gated CNN (MGG-CNN) into Transformerlayers to explicitly aggregate local details from different granularities.UniAS achieves state-of-the-art anomaly segmentation performance, attaining65.12/59.33 and 40.06/32.50 in pAP/DSC on the MVTec-AD and VisA datasets,respectively, surpassing previous methods significantly. The codes are sharedat https://github.com/Mwxinnn/UniAS.</description><author>Wenxin Ma, Qingsong Yao, Xiang Zhang, Zhelong Huang, Zihang Jiang, S. Kevin Zhou</author><pubDate>Tue, 21 Jan 2025 17:02:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12295v1</guid></item><item><title>Regressor-Guided Image Editing Regulates Emotional Response to Reduce Online Engagement</title><link>http://arxiv.org/abs/2501.12289v1</link><description>Emotions are known to mediate the relationship between users' contentconsumption and their online engagement, with heightened emotional intensityleading to increased engagement. Building on this insight, we propose threeregressor-guided image editing approaches aimed at diminishing the emotionalimpact of images. These include (i) a parameter optimization approach based onglobal image transformations known to influence emotions, (ii) an optimizationapproach targeting the style latent space of a generative adversarial network,and (iii) a diffusion-based approach employing classifier guidance andclassifier-free guidance. Our findings demonstrate that approaches caneffectively alter the emotional properties of images while maintaining highvisual quality. Optimization-based methods primarily adjust low-levelproperties like color hues and brightness, whereas the diffusion-based approachintroduces semantic changes, such as altering appearance or facial expressions.Notably, results from a behavioral study reveal that only the diffusion-basedapproach successfully elicits changes in viewers' emotional responses whilepreserving high perceived image quality. In future work, we will investigatethe impact of these image adaptations on internet user behavior.</description><author>Christoph Gebhardt, Robin Willardt, Seyedmorteza Sadat, Chih-Wei Ning, Andreas Brombach, Jie Song, Otmar Hilliges, Christian Holz</author><pubDate>Tue, 21 Jan 2025 16:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12289v1</guid></item><item><title>Implementation of an Asymmetric Adjusted Activation Function for Class Imbalance Credit Scoring</title><link>http://arxiv.org/abs/2501.12285v1</link><description>Credit scoring is a systematic approach to evaluate a borrower's probabilityof default (PD) on a bank loan. The data associated with such scenarios arecharacteristically imbalanced, complicating binary classification owing to theoften-underestimated cost of misclassification during the classifier's learningprocess. Considering the high imbalance ratio (IR) of these datasets, weintroduce an innovative yet straightforward optimized activation function byincorporating an IR-dependent asymmetric adjusted factor embedded Sigmoidactivation function (ASIG). The embedding of ASIG makes the sensitive margin ofthe Sigmoid function auto-adjustable, depending on the imbalance nature of thedatasets distributed, thereby giving the activation function an asymmetriccharacteristic that prevents the underrepresentation of the minority class(positive samples) during the classifier's learning process. The experimentalresults show that the ASIG-embedded-classifier outperforms traditionalclassifiers on datasets across wide-ranging IRs in the downstreamcredit-scoring task. The algorithm also shows robustness and stability, evenwhen the IR is ultra-high. Therefore, the algorithm provides a competitivealternative in the financial industry, especially in credit scoring, possessingthe ability to effectively process highly imbalanced distribution data.</description><author>Xia Li, Hanghang Zheng, Kunpeng Tao, Mao Mao</author><pubDate>Tue, 21 Jan 2025 16:54:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12285v1</guid></item><item><title>MoGERNN: An Inductive Traffic Predictor for Unobserved Locations in Dynamic Sensing Networks</title><link>http://arxiv.org/abs/2501.12281v1</link><description>Given a partially observed road network, how can we predict the traffic stateof unobserved locations? While deep learning approaches show exceptionalperformance in traffic prediction, most assume sensors at all locations ofinterest, which is impractical due to financial constraints. Furthermore, thesemethods typically require costly retraining when sensor configurations change.We propose MoGERNN, an inductive spatio-temporal graph representation model, toaddress these challenges. Inspired by the Mixture of Experts approach in LargeLanguage Models, we introduce a Mixture of Graph Expert (MoGE) block to modelcomplex spatial dependencies through multiple graph message aggregators and asparse gating network. This block estimates initial states for unobservedlocations, which are then processed by a GRU-based Encoder-Decoder thatintegrates a graph message aggregator to capture spatio-temporal dependenciesand predict future states. Experiments on two real-world datasets show MoGERNNconsistently outperforms baseline methods for both observed and unobservedlocations. MoGERNN can accurately predict congestion evolution even in areaswithout sensors, offering valuable information for traffic management.Moreover, MoGERNN is adaptable to dynamic sensing networks, maintainingcompetitive performance even compared to its retrained counterpart. Tests withdifferent numbers of available sensors confirm its consistent superiority, andablation studies validate the effectiveness of its key modules.</description><author>Qishen Zhou, Yifan Zhang, Michail A. Makridis, Anastasios Kouvelas, Yibing Wang, Simon Hu</author><pubDate>Tue, 21 Jan 2025 16:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12281v1</guid></item><item><title>With Great Backbones Comes Great Adversarial Transferability</title><link>http://arxiv.org/abs/2501.12275v1</link><description>Advances in self-supervised learning (SSL) for machine vision have improvedrepresentation robustness and model performance, giving rise to pre-trainedbackbones like \emph{ResNet} and \emph{ViT} models tuned with SSL methods suchas \emph{SimCLR}. Due to the computational and data demands of pre-training,the utilization of such backbones becomes a strenuous necessity. However,employing these backbones may inherit vulnerabilities to adversarial attacks.While adversarial robustness has been studied under \emph{white-box} and\emph{black-box} settings, the robustness of models tuned on pre-trainedbackbones remains largely unexplored. Additionally, the role of tuningmeta-information in mitigating exploitation risks is unclear. This worksystematically evaluates the adversarial robustness of such models across$20,000$ combinations of tuning meta-information, including fine-tuningtechniques, backbone families, datasets, and attack types. We propose usingproxy models to transfer attacks, simulating varying levels of target knowledgeby fine-tuning these proxies with diverse configurations. Our findings revealthat proxy-based attacks approach the effectiveness of \emph{white-box}methods, even with minimal tuning knowledge. We also introduce a naive"backbone attack," leveraging only the backbone to generate adversarialsamples, which outperforms \emph{black-box} attacks and rivals \emph{white-box}methods, highlighting critical risks in model-sharing practices. Finally, ourablations reveal how increasing tuning meta-information impacts attacktransferability, measuring each meta-information combination.</description><author>Erik Arakelyan, Karen Hambardzumyan, Davit Papikyan, Pasquale Minervini, Albert Gordo, Isabelle Augenstein, Aram H. Markosyan</author><pubDate>Tue, 21 Jan 2025 16:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12275v1</guid></item><item><title>Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement</title><link>http://arxiv.org/abs/2501.12273v1</link><description>The quality of Supervised Fine-Tuning (SFT) data plays a critical role inenhancing the conversational capabilities of Large Language Models (LLMs).However, as LLMs become more advanced, the availability of high-qualityhuman-annotated SFT data has become a significant bottleneck, necessitating agreater reliance on synthetic training data. In this work, we introduce Condor,a novel two-stage synthetic data generation framework that incorporates WorldKnowledge Tree and Self-Reflection Refinement to produce high-quality SFT dataat scale. Our experimental results demonstrate that a base model fine-tuned ononly 20K Condor-generated samples achieves superior performance compared tocounterparts. The additional refinement stage in Condor further enablesiterative self-improvement for LLMs at various scales (up to 72B), validatingthe effectiveness of our approach. Furthermore, our investigation into thescaling for synthetic data in post-training reveals substantial unexploredpotential for performance improvements, opening promising avenues for futureresearch.</description><author>Maosong Cao, Taolin Zhang, Mo Li, Chuyu Zhang, Yunxin Liu, Haodong Duan, Songyang Zhang, Kai Chen</author><pubDate>Tue, 21 Jan 2025 16:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12273v1</guid></item><item><title>Leveraging Explicit Reasoning for Inference Integration in Commonsense-Augmented Dialogue Models</title><link>http://arxiv.org/abs/2406.09138v2</link><description>Open-domain dialogue systems need to grasp social commonsense to understandand respond effectively to human users. Commonsense-augmented dialogue modelshave been proposed that aim to infer commonsense knowledge from dialoguecontexts in order to improve response quality. However, existing approaches tocommonsense-augmented dialogue rely on implicit reasoning to integratecommonsense inferences during response generation. In this study, we explorethe impact of explicit reasoning against implicit reasoning over commonsensefor dialogue response generation. Our findings demonstrate that separatingcommonsense reasoning into explicit steps for generating, selecting, andintegrating commonsense into responses leads to better dialogue interactions,improving naturalness, engagement, specificity, and overall quality. Subsequentanalyses of these findings unveil insights into the effectiveness of varioustypes of commonsense in generating responses and the particular response traitsenhanced through explicit reasoning for commonsense integration. Our workadvances research in open-domain dialogue by achieving a new state-of-the-artin commonsense-augmented response generation.</description><author>Sarah E. Finch, Jinho D. Choi</author><pubDate>Tue, 21 Jan 2025 16:40:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09138v2</guid></item><item><title>Benchmarking Image Perturbations for Testing Automated Driving Assistance Systems</title><link>http://arxiv.org/abs/2501.12269v1</link><description>Advanced Driver Assistance Systems (ADAS) based on deep neural networks(DNNs) are widely used in autonomous vehicles for critical perception taskssuch as object detection, semantic segmentation, and lane recognition. However,these systems are highly sensitive to input variations, such as noise andchanges in lighting, which can compromise their effectiveness and potentiallylead to safety-critical failures. This study offers a comprehensive empirical evaluation of imageperturbations, techniques commonly used to assess the robustness of DNNs, tovalidate and improve the robustness and generalization of ADAS perceptionsystems. We first conducted a systematic review of the literature, identifying38 categories of perturbations. Next, we evaluated their effectiveness inrevealing failures in two different ADAS, both at the component and at thesystem level. Finally, we explored the use of perturbation-based dataaugmentation and continuous learning strategies to improve ADAS adaptation tonew operational design domains. Our results demonstrate that all categories ofimage perturbations successfully expose robustness issues in ADAS and that theuse of dataset augmentation and continuous learning significantly improves ADASperformance in novel, unseen environments.</description><author>Stefano Carlo Lambertenghi, Hannes Leonhard, Andrea Stocco</author><pubDate>Tue, 21 Jan 2025 16:40:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12269v1</guid></item><item><title>SANER: Annotation-free Societal Attribute Neutralizer for Debiasing CLIP</title><link>http://arxiv.org/abs/2408.10202v2</link><description>Large-scale vision-language models, such as CLIP, are known to containsocietal bias regarding protected attributes (e.g., gender, age). This paperaims to address the problems of societal bias in CLIP. Although previousstudies have proposed to debias societal bias through adversarial learning ortest-time projecting, our comprehensive study of these works identifies twocritical limitations: 1) loss of attribute information when it is explicitlydisclosed in the input and 2) use of the attribute annotations during debiasingprocess. To mitigate societal bias in CLIP and overcome these limitationssimultaneously, we introduce a simple-yet-effective debiasing method calledSANER (societal attribute neutralizer) that eliminates attribute informationfrom CLIP text features only of attribute-neutral descriptions. Experimentalresults show that SANER, which does not require attribute annotations andpreserves original information for attribute-specific descriptions,demonstrates superior debiasing ability than the existing methods.Additionally, we observe that SANER does not require retraining CLIP fromscratch with the original dataset. Moreover, the debiased model can be directlyapplied to the text-to-image generation model by simply replacing the textencoder.</description><author>Yusuke Hirota, Min-Hung Chen, Chien-Yi Wang, Yuta Nakashima, Yu-Chiang Frank Wang, Ryo Hachiuma</author><pubDate>Tue, 21 Jan 2025 16:39:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10202v2</guid></item><item><title>VipDiff: Towards Coherent and Diverse Video Inpainting via Training-free Denoising Diffusion Models</title><link>http://arxiv.org/abs/2501.12267v1</link><description>Recent video inpainting methods have achieved encouraging improvements byleveraging optical flow to guide pixel propagation from reference frames eitherin the image space or feature space. However, they would produce severeartifacts in the mask center when the masked area is too large and no pixelcorrespondences can be found for the center. Recently, diffusion models havedemonstrated impressive performance in generating diverse and high-qualityimages, and have been exploited in a number of works for image inpainting.These methods, however, cannot be applied directly to videos to producetemporal-coherent inpainting results. In this paper, we propose a training-freeframework, named VipDiff, for conditioning diffusion model on the reversediffusion process to produce temporal-coherent inpainting results withoutrequiring any training data or fine-tuning the pre-trained diffusion models.VipDiff takes optical flow as guidance to extract valid pixels from referenceframes to serve as constraints in optimizing the randomly sampled Gaussiannoise, and uses the generated results for further pixel propagation andconditional generation. VipDiff also allows for generating diverse videoinpainting results over different sampled noise. Experiments demonstrate thatVipDiff can largely outperform state-of-the-art video inpainting methods interms of both spatial-temporal coherence and fidelity.</description><author>Chaohao Xie, Kai Han, Kwan-Yee K. Wong</author><pubDate>Tue, 21 Jan 2025 16:39:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12267v1</guid></item><item><title>CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification</title><link>http://arxiv.org/abs/2501.12266v1</link><description>The main challenges limiting the adoption of deep learning-based solutions inmedical workflows are the availability of annotated data and the lack ofinterpretability of such systems. Concept Bottleneck Models (CBMs) tackle thelatter by constraining the final disease prediction on a set of predefined andhuman-interpretable concepts. However, the increased interpretability achievedthrough these concept-based explanations implies a higher annotation burden.Moreover, if a new concept needs to be added, the whole system needs to beretrained. Inspired by the remarkable performance shown by LargeVision-Language Models (LVLMs) in few-shot settings, we propose a simple, yeteffective, methodology, CBVLM, which tackles both of the aforementionedchallenges. First, for each concept, we prompt the LVLM to answer if theconcept is present in the input image. Then, we ask the LVLM to classify theimage based on the previous concept predictions. Moreover, in both stages, weincorporate a retrieval module responsible for selecting the best examples forin-context learning. By grounding the final diagnosis on the predictedconcepts, we ensure explainability, and by leveraging the few-shot capabilitiesof LVLMs, we drastically lower the annotation cost. We validate our approachwith extensive experiments across four medical datasets and twelve LVLMs (bothgeneric and medical) and show that CBVLM consistently outperforms CBMs andtask-specific supervised methods without requiring any training and using justa few annotated examples. More information on our project page:https://cristianopatricio.github.io/CBVLM/.</description><author>Cristiano Patrício, Isabel Rio-Torto, Jaime S. Cardoso, Luís F. Teixeira, João C. Neves</author><pubDate>Tue, 21 Jan 2025 16:38:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12266v1</guid></item><item><title>mmCooper: A Multi-agent Multi-stage Communication-efficient and Collaboration-robust Cooperative Perception Framework</title><link>http://arxiv.org/abs/2501.12263v1</link><description>Collaborative perception significantly enhances individual vehicle perceptionperformance through the exchange of sensory information among agents. However,real-world deployment faces challenges due to bandwidth constraints andinevitable calibration errors during information exchange. To address theseissues, we propose mmCooper, a novel multi-agent, multi-stage,communication-efficient, and collaboration-robust cooperative perceptionframework. Our framework leverages a multi-stage collaboration strategy thatdynamically and adaptively balances intermediate- and late-stage information toshare among agents, enhancing perceptual performance while maintainingcommunication efficiency. To support robust collaboration despite potentialmisalignments and calibration errors, our framework captures multi-scalecontextual information for robust fusion in the intermediate stage andcalibrates the received detection results to improve accuracy in the latestage. We validate the effectiveness of mmCooper through extensive experimentson real-world and simulated datasets. The results demonstrate the superiorityof our proposed framework and the effectiveness of each component.</description><author>Bingyi Liu, Jian Teng, Hongfei Xue, Enshu Wang, Chuanhui Zhu, Pu Wang, Libing Wu</author><pubDate>Tue, 21 Jan 2025 16:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12263v1</guid></item><item><title>HAC++: Towards 100X Compression of 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2501.12255v1</link><description>3D Gaussian Splatting (3DGS) has emerged as a promising framework for novelview synthesis, boasting rapid rendering speed with high fidelity. However, thesubstantial Gaussians and their associated attributes necessitate effectivecompression techniques. Nevertheless, the sparse and unorganized nature of thepoint cloud of Gaussians (or anchors in our paper) presents challenges forcompression. To achieve a compact size, we propose HAC++, which leverages therelationships between unorganized anchors and a structured hash grid, utilizingtheir mutual information for context modeling. Additionally, HAC++ capturesintra-anchor contextual relationships to further enhance compressionperformance. To facilitate entropy coding, we utilize Gaussian distributions toprecisely estimate the probability of each quantized attribute, where anadaptive quantization module is proposed to enable high-precision quantizationof these attributes for improved fidelity restoration. Moreover, we incorporatean adaptive masking strategy to eliminate invalid Gaussians and anchors.Overall, HAC++ achieves a remarkable size reduction of over 100X compared tovanilla 3DGS when averaged on all datasets, while simultaneously improvingfidelity. It also delivers more than 20X size reduction compared toScaffold-GS. Our code is available athttps://github.com/YihangChen-ee/HAC-plus.</description><author>Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, Jianfei Cai</author><pubDate>Tue, 21 Jan 2025 16:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12255v1</guid></item><item><title>Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos</title><link>http://arxiv.org/abs/2501.12254v1</link><description>Self-supervised learning holds the promise to learn good representations fromreal-world continuous uncurated data streams. However, most existing works invisual self-supervised learning focus on static images or artificial datastreams. Towards exploring a more realistic learning substrate, we investigatestreaming self-supervised learning from long-form real-world egocentric videostreams. Inspired by the event segmentation mechanism in human perception andmemory, we propose "Memory Storyboard" that groups recent past frames intotemporal segments for more effective summarization of the past visual streamsfor memory replay. To accommodate efficient temporal segmentation, we propose atwo-tier memory hierarchy: the recent past is stored in a short-term memory,and the storyboard temporal segments are then transferred to a long-termmemory. Experiments on real-world egocentric video datasets including SAYCamand KrishnaCam show that contrastive learning objectives on top of storyboardframes result in semantically meaningful representations which outperform thoseproduced by state-of-the-art unsupervised continual learning methods.</description><author>Yanlai Yang, Mengye Ren</author><pubDate>Tue, 21 Jan 2025 16:19:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12254v1</guid></item><item><title>Hire Me or Not? Examining Language Model's Behavior with Occupation Attributes</title><link>http://arxiv.org/abs/2405.06687v3</link><description>With the impressive performance in various downstream tasks, large languagemodels (LLMs) have been widely integrated into production pipelines, likerecruitment and recommendation systems. A known issue of models trained onnatural language data is the presence of human biases, which can impact thefairness of the system. This paper investigates LLMs' behavior with respect togender stereotypes, in the context of occupation decision making. Our frameworkis designed to investigate and quantify the presence of gender stereotypes inLLMs' behavior via multi-round question answering. Inspired by prior works, weconstruct a dataset by leveraging a standard occupation classificationknowledge base released by authoritative agencies. We tested three LLMs(RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all modelsexhibit gender stereotypes analogous to human biases, but with differentpreferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat mayimply the current alignment methods are insufficient for debiasing and couldintroduce new biases contradicting the traditional gender stereotypes.</description><author>Damin Zhang, Yi Zhang, Geetanjali Bihani, Julia Rayz</author><pubDate>Tue, 21 Jan 2025 16:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06687v3</guid></item><item><title>Untrained Perceptual Loss for image denoising of line-like structures in MR images</title><link>http://arxiv.org/abs/2411.05884v2</link><description>In the acquisition of Magnetic Resonance (MR) images shorter scan times leadto higher image noise. Therefore, automatic image denoising using deep learningmethods is of high interest. MR images containing line-like structures such asroots or vessels yield special characteristics as they display connectedstructures and yield sparse information. For this kind of data, it is importantto consider voxel neighborhoods when training a denoising network. In thispaper, we translate the Perceptual Loss to 3D data by comparing feature maps ofuntrained networks in the loss function as done previously for 2D data. Wetested the performance of untrained Perceptual Loss (uPL) on 3D image denoisingof MR images displaying brain vessels (MR angiograms - MRA) and images of plantroots in soil. We investigate the impact of various uPL characteristics such asweight initialization, network depth, kernel size, and pooling operations onthe results. We tested the performance of the uPL loss on four Rician noiselevels using evaluation metrics such as the Structural Similarity Index Metric(SSIM). We observe, that our uPL outperforms conventional loss functions suchas the L1 loss or a loss based on the Structural Similarity Index Metric(SSIM). The uPL network's initialization is not important, while network depthand pooling operations impact denoising performance. E.g. for both datasets anetwork with five convolutional layers led to the best performance while anetwork with more layers led to a performance drop. We also find that small uPLnetworks led to better or comparable results than using large networks such asVGG. We observe superior performance of our loss for both datasets, all noiselevels, and three network architectures. In conclusion, for images containingline-like structures, uPL is an alternative to other loss functions for 3Dimage denoising.</description><author>Elisabeth Pfaehler, Daniel Pflugfelder, Hanno Scharr</author><pubDate>Tue, 21 Jan 2025 16:11:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05884v2</guid></item><item><title>Video Deblurring by Sharpness Prior Detection and Edge Information</title><link>http://arxiv.org/abs/2501.12246v1</link><description>Video deblurring is essential task for autonomous driving, facialrecognition, and security surveillance. Traditional methods directly estimatemotion blur kernels, often introducing artifacts and leading to poor results.Recent approaches utilize the detection of sharp frames within video sequencesto enhance deblurring. However, existing datasets rely on fixed number of sharpframes, which may be too restrictive for some applications and may introduce abias during model training. To address these limitations and enhance domainadaptability, this work first introduces GoPro Random Sharp (GoProRS), a newdataset where the the frequency of sharp frames within the sequence iscustomizable, allowing more diverse training and testing scenarios.Furthermore, it presents a novel video deblurring model, called SPEINet, thatintegrates sharp frame features into blurry frame reconstruction through anattention-based encoder-decoder architecture, a lightweight yet robust sharpframe detection and an edge extraction phase. Extensive experimental resultsdemonstrate that SPEINet outperforms state-of-the-art methods across multipledatasets, achieving an average of +3.2% PSNR improvement over recenttechniques. Given such promising results, we believe that both the proposedmodel and dataset pave the way for future advancements in video deblurringbased on the detection of sharp frames.</description><author>Yang Tian, Fabio Brau, Giulio Rossolini, Giorgio Buttazzo, Hao Meng</author><pubDate>Tue, 21 Jan 2025 16:07:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12246v1</guid></item><item><title>ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability</title><link>http://arxiv.org/abs/2410.11414v2</link><description>Retrieval-Augmented Generation (RAG) models are designed to incorporateexternal knowledge, reducing hallucinations caused by insufficient parametric(internal) knowledge. However, even with accurate and relevant retrievedcontent, RAG models can still produce hallucinations by generating outputs thatconflict with the retrieved information. Detecting such hallucinations requiresdisentangling how Large Language Models (LLMs) utilize external and parametricknowledge. Current detection methods often focus on one of these mechanisms orwithout decoupling their intertwined effects, making accurate detectiondifficult. In this paper, we investigate the internal mechanisms behindhallucinations in RAG scenarios. We discover hallucinations occur when theKnowledge FFNs in LLMs overemphasize parametric knowledge in the residualstream, while Copying Heads fail to effectively retain or integrate externalknowledge from retrieved content. Based on these findings, we propose ReDeEP, anovel method that detects hallucinations by decoupling LLM's utilization ofexternal context and parametric knowledge. Our experiments show that ReDeEPsignificantly improves RAG hallucination detection accuracy. Additionally, weintroduce AARF, which mitigates hallucinations by modulating the contributionsof Knowledge FFNs and Copying Heads.</description><author>Zhongxiang Sun, Xiaoxue Zang, Kai Zheng, Yang Song, Jun Xu, Xiao Zhang, Weijie Yu, Yang Song, Han Li</author><pubDate>Tue, 21 Jan 2025 16:05:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11414v2</guid></item><item><title>Quality Enhancement of Radiographic X-ray Images by Interpretable Mapping</title><link>http://arxiv.org/abs/2501.12245v1</link><description>X-ray imaging is the most widely used medical imaging modality. However, inthe common practice, inconsistency in the initial presentation of X-ray imagesis a common complaint by radiologists. Different patient positions, patienthabitus and scanning protocols can lead to differences in image presentations,e.g., differences in brightness and contrast globally or regionally. Tocompensate for this, additional work will be executed by clinical experts toadjust the images to the desired presentation, which can be time-consuming.Existing deep-learning-based end-to-end solutions can automatically correctimages with promising performances. Nevertheless, these methods are hard to beinterpreted and difficult to be understood by clinical experts. In thismanuscript, a novel interpretable mapping method by deep learning is proposed,which automatically enhances the image brightness and contrast globally andlocally. Meanwhile, because the model is inspired by the workflow of thebrightness and contrast manipulation, it can provide interpretable pixel mapsfor explaining the motivation of image enhancement. The experiment on theclinical datasets show the proposed method can provide consistent brightnessand contrast correction on X-ray images with accuracy of 24.75 dB PSNR and0.8431 SSIM.</description><author>Hongxu Yang, Najib Akram Aboobacker, Xiaomeng Dong, German Gonzalez, Lehel Ferenczi, Gopal Avinash</author><pubDate>Tue, 21 Jan 2025 16:04:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12245v1</guid></item><item><title>Zero-shot Bias Correction: Efficient MR Image Inhomogeneity Reduction Without Any Data</title><link>http://arxiv.org/abs/2501.12244v1</link><description>In recent years, deep neural networks for image inhomogeneity reduction haveshown promising results. However, current methods with (un)supervised solutionsrequire preparing a training dataset, which is expensive and laborious for datacollection. In this work, we demonstrate a novel zero-shot deep neuralnetworks, which requires no data for pre-training and dedicated assumption ofthe bias field. The designed light-weight CNN enables an efficient zero-shotadaptation for bias-corrupted image correction. Our method provides a novelsolution to mitigate the biased corrupted image as iterative homogeneityrefinement, which therefore ensures the considered issue can be solved easierwith stable convergence of zero-shot optimization. Extensive comparison ondifferent datasets show that the proposed method performs better than currentdata-free N4 methods in both efficiency and accuracy.</description><author>Hongxu Yang, Edina Timko, Brice Fernandez</author><pubDate>Tue, 21 Jan 2025 16:04:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12244v1</guid></item><item><title>FOCUS: First Order Concentrated Updating Scheme</title><link>http://arxiv.org/abs/2501.12243v1</link><description>Large language models (LLMs) demonstrate remarkable performance, andimproving their pre-training process appears to be key to enhancing theircapabilities further. Based on the documented success of Adam, learning ratedecay, and weight decay, we hypothesize that the pre-training loss landscapefeatures a narrowing valley structure. Through experiments with synthetic lossfunctions, we discover that when gradient query noise is high relative to thevalley's sharpness, Adam's performance falls behind that of Signum because Adamreduces the effective step size too drastically. This observation led us todevelop FOCUS, an optimizer that enhances Signum by incorporating attractiontoward moving averaged parameters, allowing it to handle noise better whilemaintaining larger step sizes. In training GPT-2, FOCUS proves to be morestable than Signum and faster than Adam. These results suggest that gradientnoise may be an underappreciated limiting factor in LLM training, and FOCUSoffers promising solutions.</description><author>Yizhou Liu, Ziming Liu, Jeff Gore</author><pubDate>Tue, 21 Jan 2025 16:03:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12243v1</guid></item><item><title>Word and Phrase Features in Graph Convolutional Network for Automatic Question Classification</title><link>http://arxiv.org/abs/2409.02481v2</link><description>Effective question classification is crucial for AI-driven educational tools,enabling adaptive learning systems to categorize questions by skill area,difficulty level, and competence. This classification not only supportseducational diagnostics and analytics but also enhances complex tasks likeinformation retrieval and question answering by associating questions withrelevant categories. Traditional methods, often based on word embeddings andconventional classifiers, struggle to capture the nuanced relationships innatural language, leading to suboptimal performance. To address this, wepropose a novel approach leveraging graph convolutional networks, named PhraseQuestion-Graph Convolutional Network (PQ-GCN) to better model the inherentstructure of questions. By representing questions as graphs-where nodes signifywords or phrases and edges denote syntactic or semantic relationships-ourmethod allows the model to learn from the interconnected nature of languagemore effectively. Additionally, we explore the incorporation of phrase-basedfeatures to enhance classification performance on question datasets of variousdomains and characteristics. Our findings demonstrate that the proposed model,augmented with these features, offer a promising solution for more robust andcontext-aware question classification, bridging the gap between graph neuralnetwork research and practical educational applications of AI.</description><author>Junyoung Lee, Ninad Dixit, Kaustav Chakrabarti, S. Supraja</author><pubDate>Tue, 21 Jan 2025 16:03:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02481v2</guid></item><item><title>Investigating Market Strength Prediction with CNNs on Candlestick Chart Images</title><link>http://arxiv.org/abs/2501.12239v1</link><description>This paper investigates predicting market strength solely from candlestickchart images to assist investment decisions. The core research problem isdeveloping an effective computer vision-based model using raw candlestickvisuals without time-series data. We specifically analyze the impact ofincorporating candlestick patterns that were detected by YOLOv8. The studyimplements two approaches: pure CNN on chart images and a Decomposerarchitecture detecting patterns. Experiments utilize diverse financial datasetsspanning stocks, cryptocurrencies, and forex assets. Key findings demonstratecandlestick patterns do not improve model performance over only image data inour research. The significance is illuminating limitations in candlestick imagesignals. Performance peaked at approximately 0.7 accuracy, below more complextime-series models. Outcomes reveal challenges in distilling sufficientpredictive power from visual shapes alone, motivating the incorporation ofother data modalities. This research clarifies how purely image-based modelscan inform trading while confirming patterns add little value over raw charts.Our content is endeavored to be delineated into distinct sections, eachautonomously furnishing a unique contribution while maintaining cohesivelinkage. Note that, the examples discussed herein are not limited to the scope,applicability, or knowledge outlined in the paper.</description><author>Thanh Nam Duong, Trung Kien Hoang, Quoc Khanh Duong, Quoc Dat Dinh, Duc Hoan Le, Huy Tuan Nguyen, Xuan Bach Nguyen, Quy Ban Tran</author><pubDate>Tue, 21 Jan 2025 15:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12239v1</guid></item><item><title>Fast sparse optimization via adaptive shrinkage</title><link>http://arxiv.org/abs/2501.12236v1</link><description>The need for fast sparse optimization is emerging, e.g., to deal withlarge-dimensional data-driven problems and to track time-varying systems. Inthe framework of linear sparse optimization, the iterativeshrinkage-thresholding algorithm is a valuable method to solve Lasso, which isparticularly appreciated for its ease of implementation. Nevertheless, itconverges slowly. In this paper, we develop a proximal method, based onlogarithmic regularization, which turns out to be an iterativeshrinkage-thresholding algorithm with adaptive shrinkage hyperparameter. Thisadaptivity substantially enhances the trajectory of the algorithm, in a waythat yields faster convergence, while keeping the simplicity of the originalmethod. Our contribution is twofold: on the one hand, we derive and analyze theproposed algorithm; on the other hand, we validate its fast convergence vianumerical experiments and we discuss the performance with respect tostate-of-the-art algorithms.</description><author>Vito Cerone, Sophie M. Fosson, Diego Regruto</author><pubDate>Tue, 21 Jan 2025 15:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12236v1</guid></item><item><title>DLEN: Dual Branch of Transformer for Low-Light Image Enhancement in Dual Domains</title><link>http://arxiv.org/abs/2501.12235v1</link><description>Low-light image enhancement (LLE) aims to improve the visual quality ofimages captured in poorly lit conditions, which often suffer from lowbrightness, low contrast, noise, and color distortions. These issues hinder theperformance of computer vision tasks such as object detection, facialrecognition, and autonomous driving.Traditional enhancement techniques, such asmulti-scale fusion and histogram equalization, fail to preserve fine detailsand often struggle with maintaining the natural appearance of enhanced imagesunder complex lighting conditions. Although the Retinex theory provides afoundation for image decomposition, it often amplifies noise, leading tosuboptimal image quality. In this paper, we propose the Dual Light EnhanceNetwork (DLEN), a novel architecture that incorporates two distinct attentionmechanisms, considering both spatial and frequency domains. Our modelintroduces a learnable wavelet transform module in the illumination estimationphase, preserving high- and low-frequency components to enhance edge andtexture details. Additionally, we design a dual-branch structure that leveragesthe power of the Transformer architecture to enhance both the illumination andstructural components of the image.Through extensive experiments, our modeloutperforms state-of-the-art methods on standard benchmarks.Code is availablehere: https://github.com/LaLaLoXX/DLEN</description><author>Junyu Xia, Jiesong Bai, Yihang Dong</author><pubDate>Tue, 21 Jan 2025 15:58:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12235v1</guid></item><item><title>InsTALL: Context-aware Instructional Task Assistance with Multi-modal Large Language Models</title><link>http://arxiv.org/abs/2501.12231v1</link><description>The improved competence of generative models can help building multi-modalvirtual assistants that leverage modalities beyond language. By observinghumans performing multi-step tasks, one can build assistants that havesituational awareness of actions and tasks being performed, enabling them tocater assistance based on this understanding. In this paper, we develop aContext-aware Instructional Task Assistant with Multi-modal Large LanguageModels (InsTALL) that leverages an online visual stream (e.g. a user's screenshare or video recording) and responds in real-time to user queries related tothe task at hand. To enable useful assistance, InsTALL 1) trains a multi-modalmodel on task videos and paired textual data, and 2) automatically extractstask graph from video data and leverages it at training and inference time. Weshow InsTALL achieves state-of-the-art performance across proposed sub-tasksconsidered for multimodal activity understanding -- task recognition (TR),action recognition (AR), next action prediction (AP), and plan prediction (PP)-- and outperforms existing baselines on two novel sub-tasks related toautomatic error identification.</description><author>Pha Nguyen, Sailik Sengupta, Girik Malik, Arshit Gupta, Bonan Min</author><pubDate>Tue, 21 Jan 2025 15:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12231v1</guid></item><item><title>S+t-SNE -- Bringing Dimensionality Reduction to Data Streams</title><link>http://arxiv.org/abs/2403.17643v3</link><description>We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handleinfinite data streams. The core idea behind S+t-SNE is to update the t-SNEembedding incrementally as new data arrives, ensuring scalability andadaptability to handle streaming scenarios. By selecting the most importantpoints at each step, the algorithm ensures scalability while keepinginformative visualisations. By employing a blind method for drift management,the algorithm adjusts the embedding space, which facilitates the visualisationof evolving data dynamics. Our experimental evaluations demonstrate theeffectiveness and efficiency of S+t-SNE, whilst highlighting its ability tocapture patterns in a streaming scenario. We hope our approach offersresearchers and practitioners a real-time tool for understanding andinterpreting high-dimensional data.</description><author>Pedro C. Vieira, João P. Montrezol, João T. Vieira, João Gama</author><pubDate>Tue, 21 Jan 2025 15:52:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17643v3</guid></item><item><title>CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning</title><link>http://arxiv.org/abs/2501.12226v1</link><description>Large Language Models (LLMs) have recently achieved impressive results incomplex reasoning tasks through Chain of Thought (CoT) prompting. However, mostexisting CoT methods rely on using the same prompts, whether manually designedor automatically generated, to handle the entire dataset. Thisone-size-fits-all approach may fail to meet the specific needs arising from thediversities within a single dataset. To solve this problem, we propose theClustered Distance-Weighted Chain of Thought (CDW-CoT) method, whichdynamically constructs prompts tailored to the characteristics of each datainstance by integrating clustering and prompt optimization techniques. Ourmethod employs clustering algorithms to categorize the dataset into distinctgroups, from which a candidate pool of prompts is selected to reflect theinherent diversity within the dataset. For each cluster, CDW-CoT trains theoptimal prompt probability distribution tailored to their specificcharacteristics. Finally, it dynamically constructs a unique prompt probabilitydistribution for each test instance, based on its proximity to cluster centers,from which prompts are selected for reasoning. CDW-CoT consistently outperformstraditional CoT methods across six datasets, including commonsense, symbolic,and mathematical reasoning tasks. Specifically, when compared to manual CoT,CDW-CoT achieves an average accuracy improvement of 25.34% on LLaMA2 (13B) and15.72% on LLaMA3 (8B).</description><author>Yuanheng Fang, Guoqing Chao, Wenqiang Lei, Shaobo Li, Dianhui Chu</author><pubDate>Tue, 21 Jan 2025 15:51:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12226v1</guid></item><item><title>TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space</title><link>http://arxiv.org/abs/2501.12224v1</link><description>We present TokenVerse -- a method for multi-concept personalization,leveraging a pre-trained text-to-image diffusion model. Our framework candisentangle complex visual elements and attributes from as little as a singleimage, while enabling seamless plug-and-play generation of combinations ofconcepts extracted from multiple images. As opposed to existing works,TokenVerse can handle multiple images with multiple concepts each, and supportsa wide-range of concepts, including objects, accessories, materials, pose, andlighting. Our work exploits a DiT-based text-to-image model, in which the inputtext affects the generation through both attention and modulation (shift andscale). We observe that the modulation space is semantic and enables localizedcontrol over complex concepts. Building on this insight, we devise anoptimization-based framework that takes as input an image and a textdescription, and finds for each word a distinct direction in the modulationspace. These directions can then be used to generate new images that combinethe learned concepts in a desired configuration. We demonstrate theeffectiveness of TokenVerse in challenging personalization settings, andshowcase its advantages over existing methods. project's webpage inhttps://token-verse.github.io/</description><author>Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, Tali Dekel</author><pubDate>Tue, 21 Jan 2025 15:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12224v1</guid></item><item><title>Strong phonon-mediated high temperature superconductivity in Li$_2$AuH$_6$ under ambient pressure</title><link>http://arxiv.org/abs/2501.12222v1</link><description>We used our developed AI search engine~(InvDesFlow) to perform extensiveinvestigations regarding ambient stable superconducting hydrides. A cubicstructure Li$_2$AuH$_6$ with Au-H octahedral motifs is identified to be acandidate. After performing thermodynamical analysis, we provide a feasibleroute to experimentally synthesize this material via the known LiAu and LiHcompounds under ambient pressure. The further first-principles calculationssuggest that Li$_2$AuH$_6$ shows a high superconducting transition temperature($T_c$) $\sim$ 140 K under ambient pressure. The H-1$s$ electrons stronglycouple with phonon modes of vibrations of Au-H octahedrons as well asvibrations of Li atoms, where the latter is not taken seriously in otherpreviously similar cases. Hence, different from previous claims of searchingmetallic covalent bonds to find high-$T_c$ superconductors, we emphasize herethe importance of those phonon modes with strong electron-phonon coupling(EPC). And we suggest that one can intercalate atoms into binary or ternaryhydrides to introduce more potential phonon modes with strong EPC, which is aneffective approach to find high-$T_c$ superconductors within multicomponentcompounds.</description><author>Zhenfeng Ouyang, Bo-Wen Yao, Xiao-Qi Han, Peng-Jie Guo, Ze-Feng Gao, Zhong-Yi Lu</author><pubDate>Tue, 21 Jan 2025 15:48:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12222v1</guid></item><item><title>Exploring Temporally-Aware Features for Point Tracking</title><link>http://arxiv.org/abs/2501.12218v1</link><description>Point tracking in videos is a fundamental task with applications in robotics,video editing, and more. While many vision tasks benefit from pre-trainedfeature backbones to improve generalizability, point tracking has primarilyrelied on simpler backbones trained from scratch on synthetic data, which maylimit robustness in real-world scenarios. Additionally, point tracking requirestemporal awareness to ensure coherence across frames, but usingtemporally-aware features is still underexplored. Most current methods oftenemploy a two-stage process: an initial coarse prediction followed by arefinement stage to inject temporal information and correct errors from thecoarse stage. These approach, however, is computationally expensive andpotentially redundant if the feature backbone itself captures sufficienttemporal information. In this work, we introduce Chrono, a feature backbone specifically designedfor point tracking with built-in temporal awareness. Leveraging pre-trainedrepresentations from self-supervised learner DINOv2 and enhanced with atemporal adapter, Chrono effectively captures long-term temporal context,enabling precise prediction even without the refinement stage. Experimentalresults demonstrate that Chrono achieves state-of-the-art performance in arefiner-free setting on the TAP-Vid-DAVIS and TAP-Vid-Kinetics datasets, amongcommon feature backbones used in point tracking as well as DINOv2, withexceptional efficiency. Project page: https://cvlab-kaist.github.io/Chrono/</description><author>Inès Hyeonsu Kim, Seokju Cho, Jiahui Huang, Jung Yi, Joon-Young Lee, Seungryong Kim</author><pubDate>Tue, 21 Jan 2025 15:39:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12218v1</guid></item><item><title>Early Detection and Classification of Breast Cancer Using Deep Learning Techniques</title><link>http://arxiv.org/abs/2501.12217v1</link><description>Breast cancer is one of the deadliest cancers causing about massive number ofpatients to die annually all over the world according to the WHO. It is a kindof cancer that develops when the tissues of the breast grow rapidly andunboundly. This fatality rate can be prevented if the cancer is detected beforeit gets malignant. Using automation for early-age detection of breast cancer,Artificial Intelligence and Machine Learning technologies can be implementedfor the best outcome. In this study, we are using the Breast Cancer ImageClassification dataset collected from the Kaggle depository, which comprises9248 Breast Ultrasound Images and is classified into three categories: Benign,Malignant, and Normal which refers to non-cancerous, cancerous, and normalimages.This research introduces three pretrained model featuring customclassifiers that includes ResNet50, MobileNet, and VGG16, along with a customCNN model utilizing the ReLU activation function.The models ResNet50,MobileNet, VGG16, and a custom CNN recorded accuracies of 98.41%, 97.91%,98.19%, and 92.94% on the dataset, correspondingly, with ResNet50 achieving thehighest accuracy of 98.41%.This model, with its deep and powerful architecture,is particularly successful in detecting aberrant cells as well as cancerous ornon-cancerous tumors. These accuracies show that the Machine Learning methodsare more compatible for the classification and early detection of breastcancer.</description><author>Mst. Mumtahina Labonno, D. M. Asadujjaman, Md. Mahfujur Rahman, Abdullah Tamim, Mst. Jannatul Ferdous, Rafi Muttaki Mahi</author><pubDate>Tue, 21 Jan 2025 15:39:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12217v1</guid></item></channel></rss>