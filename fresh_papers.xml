<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 17 Sep 2024 01:00:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>The unknotting number, hard unknot diagrams, and reinforcement learning</title><link>http://arxiv.org/abs/2409.09032v1</link><description>We have developed a reinforcement learning agent that often finds a minimalsequence of unknotting crossing changes for a knot diagram with up to 200crossings, hence giving an upper bound on the unknotting number. We have usedthis to determine the unknotting number of 57k knots. We took diagrams ofconnected sums of such knots with oppositely signed signatures, where thesummands were overlaid. The agent has found examples where several of thecrossing changes in an unknotting collection of crossings result in hyperbolicknots. Based on this, we have shown that, given knots $K$ and $K'$ that satisfysome mild assumptions, there is a diagram of their connected sum and $u(K) +u(K')$ unknotting crossings such that changing any one of them results in aprime knot. As a by-product, we have obtained a dataset of 2.6 million distincthard unknot diagrams; most of them under 35 crossings. Assuming the additivityof the unknotting number, we have determined the unknotting number of 43 atmost 12-crossing knots for which the unknotting number is unknown.</description><author>Taylor Applebaum, Sam Blackwell, Alex Davies, Thomas Edlich, András Juhász, Marc Lackenby, Nenad Tomašev, Daniel Zheng</author><pubDate>Fri, 13 Sep 2024 17:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09032v1</guid></item><item><title>Agents in Software Engineering: Survey, Landscape, and Vision</title><link>http://arxiv.org/abs/2409.09030v1</link><description>In recent years, Large Language Models (LLMs) have achieved remarkablesuccess and have been widely used in various downstream tasks, especially inthe tasks of the software engineering (SE) field. We find that many studiescombining LLMs with SE have employed the concept of agents either explicitly orimplicitly. However, there is a lack of an in-depth survey to sort out thedevelopment context of existing works, analyze how existing works combine theLLM-based agent technologies to optimize various tasks, and clarify theframework of LLM-based agents in SE. In this paper, we conduct the first surveyof the studies on combining LLM-based agents with SE and present a framework ofLLM-based agents in SE which includes three key modules: perception, memory,and action. We also summarize the current challenges in combining the twofields and propose future opportunities in response to existing challenges. Wemaintain a GitHub repository of the related papers at:https://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.</description><author>Yanxian Huang, Wanjun Zhong, Ensheng Shi, Min Yang, Jiachi Chen, Hui Li, Yuchi Ma, Qianxiang Wang, Zibin Zheng, Yanlin Wang</author><pubDate>Fri, 13 Sep 2024 17:55:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09030v1</guid></item><item><title>Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks</title><link>http://arxiv.org/abs/2409.09026v1</link><description>Music recommender systems frequently utilize network-based models to capturerelationships between music pieces, artists, and users. Although theserelationships provide valuable insights for predictions, new music pieces orartists often face the cold-start problem due to insufficient initialinformation. To address this, one can extract content-based informationdirectly from the music to enhance collaborative-filtering-based methods. Whileprevious approaches have relied on hand-crafted audio features for thispurpose, we explore the use of contrastively pretrained neural audio embeddingmodels, which offer a richer and more nuanced representation of music. Ourexperiments demonstrate that neural embeddings, particularly those generatedwith the Contrastive Language-Audio Pretraining (CLAP) model, present apromising approach to enhancing music recommendation tasks within graph-basedframeworks.</description><author>Florian Grötschla, Luca Strässle, Luca A. Lanzendörfer, Roger Wattenhofer</author><pubDate>Fri, 13 Sep 2024 17:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09026v1</guid></item><item><title>INN-PAR: Invertible Neural Network for PPG to ABP Reconstruction</title><link>http://arxiv.org/abs/2409.09021v1</link><description>Non-invasive and continuous blood pressure (BP) monitoring is essential forthe early prevention of many cardiovascular diseases. Estimating arterial bloodpressure (ABP) from photoplethysmography (PPG) has emerged as a promisingsolution. However, existing deep learning approaches for PPG-to-ABPreconstruction (PAR) encounter certain information loss, impacting theprecision of the reconstructed signal. To overcome this limitation, weintroduce an invertible neural network for PPG to ABP reconstruction (INN-PAR),which employs a series of invertible blocks to jointly learn the mappingbetween PPG and its gradient with the ABP signal and its gradient. INN-PARefficiently captures both forward and inverse mappings simultaneously, therebypreventing information loss. By integrating signal gradients into the learningprocess, INN-PAR enhances the network's ability to capture essentialhigh-frequency details, leading to more accurate signal reconstruction.Moreover, we propose a multi-scale convolution module (MSCM) within theinvertible block, enabling the model to learn features across multiple scaleseffectively. We have experimented on two benchmark datasets, which show thatINN-PAR significantly outperforms the state-of-the-art methods in both waveformreconstruction and BP measurement accuracy.</description><author>Soumitra Kundu, Gargi Panda, Saumik Bhattacharya, Aurobinda Routray, Rajlakshmi Guha</author><pubDate>Fri, 13 Sep 2024 17:48:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09021v1</guid></item><item><title>An Efficient and Streaming Audio Visual Active Speaker Detection System</title><link>http://arxiv.org/abs/2409.09018v1</link><description>This paper delves into the challenging task of Active Speaker Detection(ASD), where the system needs to determine in real-time whether a person isspeaking or not in a series of video frames. While previous works have madesignificant strides in improving network architectures and learning effectiverepresentations for ASD, a critical gap exists in the exploration of real-timesystem deployment. Existing models often suffer from high latency and memoryusage, rendering them impractical for immediate applications. To bridge thisgap, we present two scenarios that address the key challenges posed byreal-time constraints. First, we introduce a method to limit the number offuture context frames utilized by the ASD model. By doing so, we alleviate theneed for processing the entire sequence of future frames before a decision ismade, significantly reducing latency. Second, we propose a more stringentconstraint that limits the total number of past frames the model can accessduring inference. This tackles the persistent memory issues associated withrunning streaming ASD systems. Beyond these theoretical frameworks, we conductextensive experiments to validate our approach. Our results demonstrate thatconstrained transformer models can achieve performance comparable to or evenbetter than state-of-the-art recurrent models, such as uni-directional GRUs,with a significantly reduced number of context frames. Moreover, we shed lighton the temporal memory requirements of ASD systems, revealing that larger pastcontext has a more profound impact on accuracy than future context. Whenprofiling on a CPU we find that our efficient architecture is memory bound bythe amount of past context it can use and that the compute cost is negligibleas compared to the memory cost.</description><author>Arnav Kundu, Yanzi Jin, Mohammad Sekhavat, Max Horton, Danny Tormoen, Devang Naik</author><pubDate>Fri, 13 Sep 2024 17:45:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09018v1</guid></item><item><title>NPGA: Neural Parametric Gaussian Avatars</title><link>http://arxiv.org/abs/2405.19331v2</link><description>The creation of high-fidelity, digital versions of human heads is animportant stepping stone in the process of further integrating virtualcomponents into our everyday lives. Constructing such avatars is a challengingresearch problem, due to a high demand for photo-realism and real-timerendering performance. In this work, we propose Neural Parametric GaussianAvatars (NPGA), a data-driven approach to create high-fidelity, controllableavatars from multi-view video recordings. We build our method around 3DGaussian splatting for its highly efficient rendering and to inherit thetopological flexibility of point clouds. In contrast to previous work, wecondition our avatars' dynamics on the rich expression space of neuralparametric head models (NPHM), instead of mesh-based 3DMMs. To this end, wedistill the backward deformation field of our underlying NPHM into forwarddeformations which are compatible with rasterization-based rendering. Allremaining fine-scale, expression-dependent details are learned from themulti-view videos. For increased representational capacity of our avatars, wepropose per-Gaussian latent features that condition each primitives dynamicbehavior. To regularize this increased dynamic expressivity, we proposeLaplacian terms on the latent features and predicted dynamics. We evaluate ourmethod on the public NeRSemble dataset, demonstrating that NPGA significantlyoutperforms the previous state-of-the-art avatars on the self-reenactment taskby 2.6 PSNR. Furthermore, we demonstrate accurate animation capabilities fromreal-world monocular videos.</description><author>Simon Giebenhain, Tobias Kirschstein, Martin Rünz, Lourdes Agapito, Matthias Nießner</author><pubDate>Fri, 13 Sep 2024 17:41:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19331v2</guid></item><item><title>AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents</title><link>http://arxiv.org/abs/2409.09013v1</link><description>To be safely and successfully deployed, LLMs must simultaneously satisfytruthfulness and utility goals. Yet, often these two goals compete (e.g., an AIagent assisting a used car salesman selling a car with flaws), partly due toambiguous or misleading user instructions. We propose AI-LieDar, a framework tostudy how LLM-based agents navigate scenarios with utility-truthfulnessconflicts in a multi-turn interactive setting. We design a set of realisticscenarios where language agents are instructed to achieve goals that are inconflict with being truthful during a multi-turn conversation with simulatedhuman agents. To evaluate the truthfulness at large scale, we develop atruthfulness detector inspired by psychological literature to assess theagents' responses. Our experiment demonstrates that all models are truthfulless than 50% of the time, although truthfulness and goal achievement (utility)rates vary across models. We further test the steerability of LLMs towardstruthfulness, finding that models follow malicious instructions to deceive, andeven truth-steered models can still lie. These findings reveal the complexnature of truthfulness in LLMs and underscore the importance of furtherresearch to ensure the safe and reliable deployment of LLMs and AI agents.</description><author>Zhe Su, Xuhui Zhou, Sanketh Rangreji, Anubha Kabra, Julia Mendelsohn, Faeze Brahman, Maarten Sap</author><pubDate>Fri, 13 Sep 2024 17:41:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09013v1</guid></item><item><title>VAE Explainer: Supplement Learning Variational Autoencoders with Interactive Visualization</title><link>http://arxiv.org/abs/2409.09011v1</link><description>Variational Autoencoders are widespread in Machine Learning, but aretypically explained with dense math notation or static code examples. Thispaper presents VAE Explainer, an interactive Variational Autoencoder running inthe browser to supplement existing static documentation (e.g., Keras CodeExamples). VAE Explainer adds interactions to the VAE summary with interactivemodel inputs, latent space, and output. VAE Explainer connects the high-levelunderstanding with the implementation: annotated code and a live computationalgraph. The VAE Explainer interactive visualization is live athttps://xnought.github.io/vae-explainer and the code is open source athttps://github.com/xnought/vae-explainer.</description><author>Donald Bertucci, Alex Endert</author><pubDate>Fri, 13 Sep 2024 17:40:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09011v1</guid></item><item><title>Contri(e)ve: Context + Retrieve for Scholarly Question Answering</title><link>http://arxiv.org/abs/2409.09010v1</link><description>Scholarly communication is a rapid growing field containing a wealth ofknowledge. However, due to its unstructured and document format, it ischallenging to extract useful information from them through conventionaldocument retrieval methods. Scholarly knowledge graphs solve this problem, byrepresenting the documents in a semantic network, providing, hidden insights,summaries and ease of accessibility through queries. Naturally, questionanswering for scholarly graphs expands the accessibility to a wider audience.But some of the knowledge in this domain is still presented as unstructuredtext, thus requiring a hybrid solution for question answering systems. In thispaper, we present a two step solution using open source Large LanguageModel(LLM): Llama3.1 for Scholarly-QALD dataset. Firstly, we extract thecontext pertaining to the question from different structured and unstructureddata sources: DBLP, SemOpenAlex knowledge graphs and Wikipedia text. Secondly,we implement prompt engineering to improve the information retrievalperformance of the LLM. Our approach achieved an F1 score of 40% and alsoobserved some anomalous responses from the LLM, that are discussed in the finalpart of the paper.</description><author>Kanchan Shivashankar, Nadine Steinmetz</author><pubDate>Fri, 13 Sep 2024 17:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09010v1</guid></item><item><title>Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach</title><link>http://arxiv.org/abs/2409.09009v1</link><description>Direct speech translation (ST) models often struggle with rare words.Incorrect translation of these words can have severe consequences, impactingtranslation quality and user trust. While rare word translation is inherentlychallenging for neural models due to sparse learning signals, real-worldscenarios often allow access to translations of past recordings on similartopics. To leverage these valuable resources, we propose aretrieval-and-demonstration approach to enhance rare word translation accuracyin direct ST models. First, we adapt existing ST models to incorporateretrieved examples for rare word translation, which allows the model to benefitfrom prepended examples, similar to in-context learning. We then develop across-modal (speech-to-speech, speech-to-text, text-to-text) retriever tolocate suitable examples. We demonstrate that standard ST models can beeffectively adapted to leverage examples for rare word translation, improvingrare word translation accuracy over the baseline by 17.6% with gold examplesand 8.5% with retrieved examples. Moreover, our speech-to-speech retrievalapproach outperforms other modalities and exhibits higher robustness to unseenspeakers. Our code is publicly available(https://github.com/SiqiLii/Retrieve-and-Demonstration-ST).</description><author>Siqi Li, Danni Liu, Jan Niehues</author><pubDate>Fri, 13 Sep 2024 17:38:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09009v1</guid></item><item><title>SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear Complexity</title><link>http://arxiv.org/abs/2409.09007v1</link><description>Learning representations on large graphs is a long-standing challenge due tothe inter-dependence nature. Transformers recently have shown promisingperformance on small graphs thanks to its global attention for capturingall-pair interactions beyond observed structures. Existing approaches tend toinherit the spirit of Transformers in language and vision tasks, and embracecomplicated architectures by stacking deep attention-based propagation layers.In this paper, we attempt to evaluate the necessity of adopting multi-layerattentions in Transformers on graphs, which considerably restricts theefficiency. Specifically, we analyze a generic hybrid propagation layer,comprised of all-pair attention and graph-based propagation, and show thatmulti-layer propagation can be reduced to one-layer propagation, with the samecapability for representation learning. It suggests a new technical path forbuilding powerful and efficient Transformers on graphs, particularly throughsimplifying model architectures without sacrificing expressiveness. Asexemplified by this work, we propose a Simplified Single-layer GraphTransformers (SGFormer), whose main component is a single-layer globalattention that scales linearly w.r.t. graph sizes and requires none of anyapproximation for accommodating all-pair interactions. Empirically, SGFormersuccessfully scales to the web-scale graph ogbn-papers100M, yieldingorders-of-magnitude inference acceleration over peer Transformers onmedium-sized graphs, and demonstrates competitiveness with limited labeleddata.</description><author>Qitian Wu, Kai Yang, Hengrui Zhang, David Wipf, Junchi Yan</author><pubDate>Fri, 13 Sep 2024 17:37:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09007v1</guid></item><item><title>Model-independent variable selection via the rule-based variable priorit</title><link>http://arxiv.org/abs/2409.09003v1</link><description>While achieving high prediction accuracy is a fundamental goal in machinelearning, an equally important task is finding a small number of features withhigh explanatory power. One popular selection technique is permutationimportance, which assesses a variable's impact by measuring the change inprediction error after permuting the variable. However, this can be problematicdue to the need to create artificial data, a problem shared by other methods aswell. Another problem is that variable selection methods can be limited bybeing model-specific. We introduce a new model-independent approach, VariablePriority (VarPro), which works by utilizing rules without the need to generateartificial data or evaluate prediction error. The method is relatively easy touse, requiring only the calculation of sample averages of simple statistics,and can be applied to many data settings, including regression, classification,and survival. We investigate the asymptotic properties of VarPro and show,among other things, that VarPro has a consistent filtering property for noisevariables. Empirical studies using synthetic and real-world data show themethod achieves a balanced performance and compares favorably to manystate-of-the-art procedures currently used for variable selection.</description><author>Min Lu, Hemant Ishwaran</author><pubDate>Fri, 13 Sep 2024 17:32:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09003v1</guid></item><item><title>E2MoCase: A Dataset for Emotional, Event and Moral Observations in News Articles on High-impact Legal Cases</title><link>http://arxiv.org/abs/2409.09001v1</link><description>The way media reports on legal cases can significantly shape public opinion,often embedding subtle biases that influence societal views on justice andmorality. Analyzing these biases requires a holistic approach that captures theemotional tone, moral framing, and specific events within the narratives. Inthis work we introduce E2MoCase, a novel dataset designed to facilitate theintegrated analysis of emotions, moral values, and events within legalnarratives and media coverage. By leveraging advanced models for emotiondetection, moral value identification, and event extraction, E2MoCase offers amulti-dimensional perspective on how legal cases are portrayed in newsarticles.</description><author>Candida M. Greco, Lorenzo Zangari, Davide Picca, Andrea Tagarelli</author><pubDate>Fri, 13 Sep 2024 17:31:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09001v1</guid></item><item><title>Motion Capture Analysis of Verb and Adjective Types in Austrian Sign Language</title><link>http://arxiv.org/abs/2405.05161v2</link><description>Across a number of sign languages, temporal and spatial characteristics ofdominant hand articulation are used to express semantic and grammaticalfeatures. In this study of Austrian Sign Language (\"OsterreichischeGeb\"ardensprache, or \"OGS), motion capture data of four Deaf signers is usedto quantitatively characterize the kinematic parameters of sign production inverbs and adjectives. We investigate (1) the difference in production betweenverbs involving a natural endpoint (telic verbs; e.g. arrive) and verbs lackingan endpoint (atelic verbs; e.g. analyze), and (2) adjective signs inintensified vs. non-intensified (plain) forms. Motion capture data analysisusing linear-mixed effects models (LME) indicates that both the endpointmarking in verbs, as well as marking of intensification in adjectives, areexpressed by movement modulation in \"OGS. While the semantic distinctionbetween verb types (telic/atelic) is marked by higher peak velocity and shorterduration for telic signs compared to atelic ones, the grammatical distinction(intensification) in adjectives is expressed by longer duration for intensifiedcompared to non-intensified adjectives. The observed individual differences ofsigners might be interpreted as personal signing style.</description><author>Julia Krebs, Evie Malaia, Ronnie B. Wilbur, Isabella Fessl, Hans-Peter Wiesinger, Hermann Schwameder, Dietmar Roehm</author><pubDate>Fri, 13 Sep 2024 17:24:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05161v2</guid></item><item><title>Biomimetic Frontend for Differentiable Audio Processing</title><link>http://arxiv.org/abs/2409.08997v1</link><description>While models in audio and speech processing are becoming deeper and moreend-to-end, they as a consequence need expensive training on large data, andare often brittle. We build on a classical model of human hearing and make itdifferentiable, so that we can combine traditional explainable biomimeticsignal processing approaches with deep-learning frameworks. This allows us toarrive at an expressive and explainable model that is easily trained on modestamounts of data. We apply this model to audio processing tasks, includingclassification and enhancement. Results show that our differentiable modelsurpasses black-box approaches in terms of computational efficiency androbustness, even with little training data. We also discuss other potentialapplications.</description><author>Ruolan Leslie Famularo, Dmitry N. Zotkin, Shihab A. Shamma, Ramani Duraiswami</author><pubDate>Fri, 13 Sep 2024 17:23:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08997v1</guid></item><item><title>Policy Optimization finds Nash Equilibrium in Regularized General-Sum LQ Games</title><link>http://arxiv.org/abs/2404.00045v2</link><description>In this paper, we investigate the impact of introducing relative entropyregularization on the Nash Equilibria (NE) of General-Sum $N$-agent games,revealing the fact that the NE of such games conform to linear Gaussianpolicies. Moreover, it delineates sufficient conditions, contingent upon theadequacy of entropy regularization, for the uniqueness of the NE within thegame. As Policy Optimization serves as a foundational approach forReinforcement Learning (RL) techniques aimed at finding the NE, in this work weprove the linear convergence of a policy optimization algorithm which (subjectto the adequacy of entropy regularization) is capable of provably attaining theNE. Furthermore, in scenarios where the entropy regularization provesinsufficient, we present a $\delta$-augmentation technique, which facilitatesthe achievement of an $\epsilon$-NE within the game.</description><author>Muhammad Aneeq uz Zaman, Shubham Aggarwal, Melih Bastopcu, Tamer Başar</author><pubDate>Fri, 13 Sep 2024 16:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00045v2</guid></item><item><title>Clean Label Attacks against SLU Systems</title><link>http://arxiv.org/abs/2409.08985v1</link><description>Poisoning backdoor attacks involve an adversary manipulating the trainingdata to induce certain behaviors in the victim model by inserting a trigger inthe signal at inference time. We adapted clean label backdoor (CLBD)-datapoisoning attacks, which do not modify the training labels, on state-of-the-artspeech recognition models that support/perform a Spoken Language Understandingtask, achieving 99.8% attack success rate by poisoning 10% of the trainingdata. We analyzed how varying the signal-strength of the poison, percent ofsamples poisoned, and choice of trigger impact the attack. We also found thatCLBD attacks are most successful when applied to training samples that areinherently hard for a proxy model. Using this strategy, we achieved an attacksuccess rate of 99.3% by poisoning a meager 1.5% of the training data. Finally,we applied two previously developed defenses against gradient-based attacks,and found that they attain mixed success against poisoning.</description><author>Henry Li Xinyuan, Sonal Joshi, Thomas Thebaud, Jesus Villalba, Najim Dehak, Sanjeev Khudanpur</author><pubDate>Fri, 13 Sep 2024 16:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08985v1</guid></item><item><title>Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data</title><link>http://arxiv.org/abs/2310.03146v3</link><description>Traditional deep learning (DL) models face two key challenges. First, theyassume training samples are independent and identically distributed, anassumption often violated in real-world datasets where samples are grouped byshared measurements (e.g., participants or cells). This leads to performancedegradation, limited generalization, and confounding issues, causing Type 1 andType 2 errors. Second, DL models typically prioritize overall accuracy, oftenoverlooking fairness across underrepresented groups, leading to biased outcomesin critical areas such as loan approvals and healthcare decisions. To addressthese issues, we introduce the Fair Mixed Effects Deep Learning (Fair MEDL)framework. Fair MEDL quantifies cluster-invariant fixed effects (FE) andcluster-specific random effects (RE) through 1) a cluster adversary forlearning invariant FE, 2) a Bayesian neural network for RE, and 3) a mixingfunction combining FE and RE for final predictions. Additionally, weincorporate adversarial debiasing to promote fairness across three key metrics:Equalized Odds, Demographic Parity, and Counterfactual Fairness. Our methodalso identifies and de-weights confounding probes, improving interpretability.Evaluated on three datasets from finance and healthcare, Fair MEDL improvesfairness by up to 73% for age, 47% for race, 83% for sex, and 26% for maritalstatus, while maintaining robust predictive performance. Our implementation ispublicly available on GitHub.</description><author>Son Nguyen, Adam Wang, Albert Montillo</author><pubDate>Fri, 13 Sep 2024 16:55:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03146v3</guid></item><item><title>Predicting Trust In Autonomous Vehicles: Modeling Young Adult Psychosocial Traits, Risk-Benefit Attitudes, And Driving Factors With Machine Learning</title><link>http://arxiv.org/abs/2409.08980v1</link><description>Low trust remains a significant barrier to Autonomous Vehicle (AV) adoption.To design trustworthy AVs, we need to better understand the individual traits,attitudes, and experiences that impact people's trust judgements. We usemachine learning to understand the most important factors that contribute toyoung adult trust based on a comprehensive set of personal factors gathered viasurvey (n = 1457). Factors ranged from psychosocial and cognitive attributes todriving style, experiences, and perceived AV risks and benefits. Using theexplainable AI technique SHAP, we found that perceptions of AV risks andbenefits, attitudes toward feasibility and usability, institutional trust,prior experience, and a person's mental model are the most importantpredictors. Surprisingly, psychosocial and many technology- anddriving-specific factors were not strong predictors. Results highlight theimportance of individual differences for designing trustworthy AVs for diversegroups and lead to key implications for future design and research.</description><author>Robert Kaufman, Emi Lee, Manas Satish Bedmutha, David Kirsh, Nadir Weibel</author><pubDate>Fri, 13 Sep 2024 16:52:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08980v1</guid></item><item><title>Pseudo-Prompt Generating in Pre-trained Vision-Language Models for Multi-Label Medical Image Classification</title><link>http://arxiv.org/abs/2405.06468v3</link><description>The task of medical image recognition is notably complicated by the presenceof varied and multiple pathological indications, presenting a unique challengein multi-label classification with unseen labels. This complexity underlinesthe need for computer-aided diagnosis methods employing multi-label zero-shotlearning. Recent advancements in pre-trained vision-language models (VLMs) haveshowcased notable zero-shot classification abilities on medical images.However, these methods have limitations on leveraging extensive pre-trainedknowledge from broader image datasets, and often depend on manual promptconstruction by expert radiologists. By automating the process of prompttuning, prompt learning techniques have emerged as an efficient way to adaptVLMs to downstream tasks. Yet, existing CoOp-based strategies fall short inperforming class-specific prompts on unseen categories, limitinggeneralizability in fine-grained scenarios. To overcome these constraints, weintroduce a novel prompt generation approach inspirited by text generation innatural language processing (NLP). Our method, named Pseudo-Prompt Generating(PsPG), capitalizes on the priori knowledge of multi-modal features. Featuringa RNN-based decoder, PsPG autoregressively generates class-tailored embeddingvectors, i.e., pseudo-prompts. Comparative evaluations on various multi-labelchest radiograph datasets affirm the superiority of our approach againstleading medical vision-language and multi-label prompt learning methods. Thesource code is available at https://github.com/fallingnight/PsPG</description><author>Yaoqin Ye, Junjie Zhang, Hongwei Shi</author><pubDate>Fri, 13 Sep 2024 16:44:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06468v3</guid></item><item><title>Safeguarding Decentralized Social Media: LLM Agents for Automating Community Rule Compliance</title><link>http://arxiv.org/abs/2409.08963v1</link><description>Ensuring content compliance with community guidelines is crucial formaintaining healthy online social environments. However, traditionalhuman-based compliance checking struggles with scaling due to the increasingvolume of user-generated content and a limited number of moderators. Recentadvancements in Natural Language Understanding demonstrated by Large LanguageModels unlock new opportunities for automated content compliance verification.This work evaluates six AI-agents built on Open-LLMs for automated rulecompliance checking in Decentralized Social Networks, a challenging environmentdue to heterogeneous community scopes and rules. Analyzing over 50,000 postsfrom hundreds of Mastodon servers, we find that AI-agents effectively detectnon-compliant content, grasp linguistic subtleties, and adapt to diversecommunity contexts. Most agents also show high inter-rater reliability andconsistency in score justification and suggestions for compliance. Human-basedevaluation with domain experts confirmed the agents' reliability andusefulness, rendering them promising tools for semi-automated orhuman-in-the-loop content moderation systems.</description><author>Lucio La Cava, Andrea Tagarelli</author><pubDate>Fri, 13 Sep 2024 16:29:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08963v1</guid></item><item><title>PINNfluence: Influence Functions for Physics-Informed Neural Networks</title><link>http://arxiv.org/abs/2409.08958v1</link><description>Recently, physics-informed neural networks (PINNs) have emerged as a flexibleand promising application of deep learning to partial differential equations inthe physical sciences. While offering strong performance and competitiveinference speeds on forward and inverse problems, their black-box nature limitsinterpretability, particularly regarding alignment with expected physicalbehavior. In the present work, we explore the application of influencefunctions (IFs) to validate and debug PINNs post-hoc. Specifically, we applyvariations of IF-based indicators to gauge the influence of different types ofcollocation points on the prediction of PINNs applied to a 2D Navier-Stokesfluid flow problem. Our results demonstrate how IFs can be adapted to PINNs toreveal the potential for further studies.</description><author>Jonas R. Naujoks, Aleksander Krasowski, Moritz Weckbecker, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek, René P. Klausen</author><pubDate>Fri, 13 Sep 2024 16:23:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08958v1</guid></item><item><title>IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning</title><link>http://arxiv.org/abs/2403.10984v2</link><description>To improve privacy and ensure quality-of-service (QoS), deep learning (DL)models are increasingly deployed on Internet of Things (IoT) devices for dataprocessing, significantly increasing the carbon footprint associated with DL onIoT, covering both operational and embodied aspects. Existing operationalenergy predictors often overlook quantized DL models and emerging neuralprocessing units (NPUs), while embodied carbon footprint modeling tools neglectnon-computing hardware components common in IoT devices, creating a gap inaccurate carbon footprint modeling tools for IoT-enabled DL. This paperintroduces \textit{\carb}, an end-to-end tool for precise carbon footprintestimation in IoT-enabled DL, with deviations as low as 5\% for operational and3.23\% for embodied carbon footprints compared to actual measurements acrossvarious DL models. Additionally, practical applications of \carb~are showcasedthrough multiple user case studies.</description><author>Fan Chen, Shahzeen Attari, Gayle Buck, Lei Jiang</author><pubDate>Fri, 13 Sep 2024 16:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10984v2</guid></item><item><title>A Bayesian Approach to Clustering via the Proper Bayesian Bootstrap: the Bayesian Bagged Clustering (BBC) algorithm</title><link>http://arxiv.org/abs/2409.08954v1</link><description>The paper presents a novel approach for unsupervised techniques in the fieldof clustering. A new method is proposed to enhance existing literature modelsusing the proper Bayesian bootstrap to improve results in terms of robustnessand interpretability. Our approach is organized in two steps: k-meansclustering is used for prior elicitation, then proper Bayesian bootstrap isapplied as resampling method in an ensemble clustering approach. Results areanalyzed introducing measures of uncertainty based on Shannon entropy. Theproposal provides clear indication on the optimal number of clusters, as wellas a better representation of the clustered data. Empirical results areprovided on simulated data showing the methodological and empirical advancesobtained.</description><author>Federico Maria Quetti, Silvia Figini, Elena ballante</author><pubDate>Fri, 13 Sep 2024 16:14:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08954v1</guid></item><item><title>Pushing the boundaries of event subsampling in event-based video classification using CNNs</title><link>http://arxiv.org/abs/2409.08953v1</link><description>Event cameras offer low-power visual sensing capabilities ideal foredge-device applications. However, their high event rate, driven by hightemporal details, can be restrictive in terms of bandwidth and computationalresources. In edge AI applications, determining the minimum amount of eventsfor specific tasks can allow reducing the event rate to improve bandwidth,memory, and processing efficiency. In this paper, we study the effect of eventsubsampling on the accuracy of event data classification using convolutionalneural network (CNN) models. Surprisingly, across various datasets, the numberof events per video can be reduced by an order of magnitude with little drop inaccuracy, revealing the extent to which we can push the boundaries in accuracyvs. event rate trade-off. Additionally, we also find that lower classificationaccuracy in high subsampling rates is not solely attributable to informationloss due to the subsampling of the events, but that the training of CNNs can bechallenging in highly subsampled scenarios, where the sensitivity tohyperparameters increases. We quantify training instability across multipleevent-based classification datasets using a novel metric for evaluating thehyperparameter sensitivity of CNNs in different subsampling settings. Finally,we analyze the weight gradients of the network to gain insight into thisinstability.</description><author>Hesam Araghi, Jan van Gemert, Nergis Tomen</author><pubDate>Fri, 13 Sep 2024 16:14:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08953v1</guid></item><item><title>A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis</title><link>http://arxiv.org/abs/2409.08947v1</link><description>Relighting radiance fields is severely underconstrained for multi-view data,which is most often captured under a single illumination condition; It isespecially hard for full scenes containing multiple objects. We introduce amethod to create relightable radiance fields using such single-illuminationdata by exploiting priors extracted from 2D image diffusion models. We firstfine-tune a 2D diffusion model on a multi-illumination dataset conditioned bylight direction, allowing us to augment a single-illumination capture into arealistic -- but possibly inconsistent -- multi-illumination dataset fromdirectly defined light directions. We use this augmented data to create arelightable radiance field represented by 3D Gaussian splats. To allow directcontrol of light direction for low-frequency lighting, we represent appearancewith a multi-layer perceptron parameterized on light direction. To enforcemulti-view consistency and overcome inaccuracies we optimize a per-imageauxiliary feature vector. We show results on synthetic and real multi-view dataunder single illumination, demonstrating that our method successfully exploits2D diffusion model priors to allow realistic 3D relighting for complete scenes.Project sitehttps://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/</description><author>Yohan Poirier-Ginter, Alban Gauthier, Julien Phillip, Jean-Francois Lalonde, George Drettakis</author><pubDate>Fri, 13 Sep 2024 16:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08947v1</guid></item><item><title>DELTA: Dual Consistency Delving with Topological Uncertainty for Active Graph Domain Adaptation</title><link>http://arxiv.org/abs/2409.08946v1</link><description>Graph domain adaptation has recently enabled knowledge transfer acrossdifferent graphs. However, without the semantic information on target graphs,the performance on target graphs is still far from satisfactory. To address theissue, we study the problem of active graph domain adaptation, which selects asmall quantitative of informative nodes on the target graph for extraannotation. This problem is highly challenging due to the complicatedtopological relationships and the distribution discrepancy across graphs. Inthis paper, we propose a novel approach named Dual Consistency Delving withTopological Uncertainty (DELTA) for active graph domain adaptation. Our DELTAconsists of an edge-oriented graph subnetwork and a path-oriented graphsubnetwork, which can explore topological semantics from complementaryperspectives. In particular, our edge-oriented graph subnetwork utilizes themessage passing mechanism to learn neighborhood information, while ourpath-oriented graph subnetwork explores high-order relationships fromsubstructures. To jointly learn from two subnetworks, we roughly selectinformative candidate nodes with the consideration of consistency across twosubnetworks. Then, we aggregate local semantics from its K-hop subgraph basedon node degrees for topological uncertainty estimation. To overcome potentialdistribution shifts, we compare target nodes and their corresponding sourcenodes for discrepancy scores as an additional component for fine selection.Extensive experiments on benchmark datasets demonstrate that DELTA outperformsvarious state-of-the-art approaches.</description><author>Pengyun Wang, Yadi Cao, Chris Russell, Siyu Heng, Junyu Luo, Yanxin Shen, Xiao Luo</author><pubDate>Fri, 13 Sep 2024 16:06:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08946v1</guid></item><item><title>Pushing Joint Image Denoising and Classification to the Edge</title><link>http://arxiv.org/abs/2409.08943v1</link><description>In this paper, we jointly combine image classification and image denoising,aiming to enhance human perception of noisy images captured by edge devices,like low-light security cameras. In such settings, it is important to retainthe ability of humans to verify the automatic classification decision and thusjointly denoise the image to enhance human perception. Since edge devices havelittle computational power, we explicitly optimize for efficiency by proposinga novel architecture that integrates the two tasks. Additionally, we alter aNeural Architecture Search (NAS) method, which searches for classifiers tosearch for the integrated model while optimizing for a target latency,classification accuracy, and denoising performance. The NAS architecturesoutperform our manually designed alternatives in both denoising andclassification, offering a significant improvement to human perception. Ourapproach empowers users to construct architectures tailored to domains likemedical imaging, surveillance systems, and industrial inspections.</description><author>Thomas C Markhorst, Jan C van Gemert, Osman S Kayhan</author><pubDate>Fri, 13 Sep 2024 16:01:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08943v1</guid></item><item><title>Average-Reward Maximum Entropy Reinforcement Learning for Underactuated Double Pendulum Tasks</title><link>http://arxiv.org/abs/2409.08938v1</link><description>This report presents a solution for the swing-up and stabilisation tasks ofthe acrobot and the pendubot, developed for the AI Olympics competition at IROS2024. Our approach employs the Average-Reward Entropy Advantage PolicyOptimization (AR-EAPO), a model-free reinforcement learning (RL) algorithm thatcombines average-reward RL and maximum entropy RL. Results demonstrate that ourcontroller achieves improved performance and robustness scores compared toestablished baseline methods in both the acrobot and pendubot scenarios,without the need for a heavily engineered reward function or system model. Thecurrent results are applicable exclusively to the simulation stage setup.</description><author>Jean Seong Bjorn Choe, Bumkyu Choi, Jong-kook Kim</author><pubDate>Fri, 13 Sep 2024 15:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08938v1</guid></item><item><title>SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical Records</title><link>http://arxiv.org/abs/2409.08936v1</link><description>We present the SynSUM benchmark, a synthetic dataset linking unstructuredclinical notes to structured background variables. The dataset consists of10,000 artificial patient records containing tabular variables (like symptoms,diagnoses and underlying conditions) and related notes describing the fictionalpatient encounter in the domain of respiratory diseases. The tabular portion ofthe data is generated through a Bayesian network, where both the causalstructure between the variables and the conditional probabilities are proposedby an expert based on domain knowledge. We then prompt a large language model(GPT-4o) to generate a clinical note related to this patient encounter,describing the patient symptoms and additional context. The SynSUM dataset isprimarily designed to facilitate research on clinical information extraction inthe presence of tabular background variables, which can be linked throughdomain knowledge to concepts of interest to be extracted from the text - thesymptoms, in the case of SynSUM. Secondary uses include research on theautomation of clinical reasoning over both tabular data and text, causal effectestimation in the presence of tabular and/or textual confounders, andmulti-modal synthetic data generation. The dataset can be downloaded fromhttps://github.com/prabaey/SynSUM.</description><author>Paloma Rabaey, Henri Arno, Stefan Heytens, Thomas Demeester</author><pubDate>Fri, 13 Sep 2024 15:55:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08936v1</guid></item><item><title>Optimization and Generalization Guarantees for Weight Normalization</title><link>http://arxiv.org/abs/2409.08935v1</link><description>Weight normalization (WeightNorm) is widely used in practice for the trainingof deep neural networks and modern deep learning libraries have built-inimplementations of it. In this paper, we provide the first theoreticalcharacterizations of both optimization and generalization of deep WeightNormmodels with smooth activation functions. For optimization, from the form of theHessian of the loss, we note that a small Hessian of the predictor leads to atractable analysis. Thus, we bound the spectral norm of the Hessian ofWeightNorm networks and show its dependence on the network width and weightnormalization terms--the latter being unique to networks without WeightNorm.Then, we use this bound to establish training convergence guarantees undersuitable assumptions for gradient decent. For generalization, we use WeightNormto get a uniform convergence based generalization bound, which is independentfrom the width and depends sublinearly on the depth. Finally, we presentexperimental results which illustrate how the normalization terms and otherquantities of theoretical interest relate to the training of WeightNormnetworks.</description><author>Pedro Cisneros-Velarde, Zhijie Chen, Sanmi Koyejo, Arindam Banerjee</author><pubDate>Fri, 13 Sep 2024 15:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08935v1</guid></item><item><title>Yes, Prime Minister, question order does matter -- and it's certainly not classical! But is it quantum?</title><link>http://arxiv.org/abs/2409.08930v1</link><description>Response to a poll can be manipulated by means of a series of leadingquestions. We show that such phenomena cannot be explained by use of classicalprobability theory, whereas quantum probability theory admits a possibility ofoffering an explanation. Admissible transformation rules in quantumprobability, however, do impose some constraints on the modelling of cognitivebehaviour, which are highlighted here. Focusing on a recent poll conducted byIpsos on a set of questions posed by Sir Humphrey Appleby in an episode of theBritish political satire \textit{Yes, Prime Minister}, we show that theresulting data cannot be explained quite so simply using quantum rules,although it seems not impossible.</description><author>Dorje C. Brody</author><pubDate>Fri, 13 Sep 2024 15:46:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08930v1</guid></item><item><title>CoverUp: Coverage-Guided LLM-Based Test Generation</title><link>http://arxiv.org/abs/2403.16218v2</link><description>Testing is an essential part of software development. Test generation toolsattempt to automate the otherwise labor-intensive task of test creation, butgenerating high-coverage tests remains a challenge. This paper proposesCoverUp, a novel approach to driving the generation of high-coverage Pythonregression tests. CoverUp iteratively improves test coverage, interleavingcoverage analysis with dialogs with the LLM that steer it to refine tests sothat they increase coverage of lines and branches. We evaluate our prototypeCoverUp implementation across a benchmark of challenging code derived fromopen-source Python projects, and show that CoverUp substantially improves onthe state of the art. Compared to CodaMosa, a hybrid search/LLM-based testgenerator, CoverUp achieves a per-module median line+branch coverage of 80%(vs. 47%). Compared to MuTAP, a mutation/LLM-based test generator, CoverUpachieves an overall line+branch coverage of 90% (vs. 77%). We show thatCoverUp's iterative, coverage-guided approach is crucial to its effectiveness,contributing to nearly 40% of its successes.</description><author>Juan Altmayer Pizzorno, Emery D. Berger</author><pubDate>Fri, 13 Sep 2024 15:45:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16218v2</guid></item><item><title>ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation</title><link>http://arxiv.org/abs/2409.08926v1</link><description>Transparent object depth perception poses a challenge in everyday life andlogistics, primarily due to the inability of standard 3D sensors to accuratelycapture depth on transparent or reflective surfaces. This limitationsignificantly affects depth map and point cloud-reliant applications,especially in robotic manipulation. We developed a vision transformer-basedalgorithm for stereo depth recovery of transparent objects. This approach iscomplemented by an innovative feature post-fusion module, which enhances theaccuracy of depth recovery by structural features in images. To address thehigh costs associated with dataset collection for stereo camera-basedperception of transparent objects, our method incorporates a parameter-aligned,domain-adaptive, and physically realistic Sim2Real simulation for efficientdata generation, accelerated by AI algorithm. Our experimental resultsdemonstrate the model's exceptional Sim2Real generalizability in real-worldscenarios, enabling precise depth mapping of transparent objects to assist inrobotic manipulation. Project details are available athttps://sites.google.com/view/cleardepth/ .</description><author>Kaixin Bai, Huajian Zeng, Lei Zhang, Yiwen Liu, Hongli Xu, Zhaopeng Chen, Jianwei Zhang</author><pubDate>Fri, 13 Sep 2024 15:44:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08926v1</guid></item><item><title>Multi forests: Variable importance for multi-class outcomes</title><link>http://arxiv.org/abs/2409.08925v1</link><description>In prediction tasks with multi-class outcomes, identifying covariatesspecifically associated with one or more outcome classes can be important.Conventional variable importance measures (VIMs) from random forests (RFs),like permutation and Gini importance, focus on overall predictive performanceor node purity, without differentiating between the classes. Therefore, theycan be expected to fail to distinguish class-associated covariates fromcovariates that only distinguish between groups of classes. We introduce a VIMcalled multi-class VIM, tailored for identifying exclusively class-associatedcovariates, via a novel RF variant called multi forests (MuFs). The trees inMuFs use both multi-way and binary splitting. The multi-way splits generatechild nodes for each class, using a split criterion that evaluates how wellthese nodes represent their respective classes. This setup forms the basis ofthe multi-class VIM, which measures the discriminatory ability of the splitsperformed in the respective covariates with regard to this split criterion.Alongside the multi-class VIM, we introduce a second VIM, the discriminatoryVIM. This measure, based on the binary splits, assesses the strength of thegeneral influence of the covariates, irrespective of theirclass-associatedness. Simulation studies demonstrate that the multi-class VIMspecifically ranks class-associated covariates highly, unlike conventional VIMswhich also rank other types of covariates highly. Analyses of 121 datasetsreveal that MuFs often have slightly lower predictive performance compared toconventional RFs. This is, however, not a limiting factor given the algorithm'sprimary purpose of calculating the multi-class VIM.</description><author>Roman Hornung, Alexander Hapfelmeier</author><pubDate>Fri, 13 Sep 2024 15:40:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08925v1</guid></item><item><title>Does a Neural Network Really Encode Symbolic Concepts?</title><link>http://arxiv.org/abs/2302.13080v3</link><description>Recently, a series of studies have tried to extract interactions betweeninput variables modeled by a DNN and define such interactions as conceptsencoded by the DNN. However, strictly speaking, there still lacks a solidguarantee whether such interactions indeed represent meaningful concepts.Therefore, in this paper, we examine the trustworthiness of interactionconcepts from four perspectives. Extensive empirical studies have verified thata well-trained DNN usually encodes sparse, transferable, and discriminativeconcepts, which is partially aligned with human intuition.</description><author>Mingjie Li, Quanshi Zhang</author><pubDate>Fri, 13 Sep 2024 15:38:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13080v3</guid></item><item><title>XSub: Explanation-Driven Adversarial Attack against Blackbox Classifiers via Feature Substitution</title><link>http://arxiv.org/abs/2409.08919v1</link><description>Despite its significant benefits in enhancing the transparency andtrustworthiness of artificial intelligence (AI) systems, explainable AI (XAI)has yet to reach its full potential in real-world applications. One keychallenge is that XAI can unintentionally provide adversaries with insightsinto black-box models, inevitably increasing their vulnerability to variousattacks. In this paper, we develop a novel explanation-driven adversarialattack against black-box classifiers based on feature substitution, calledXSub. The key idea of XSub is to strategically replace important features(identified via XAI) in the original sample with corresponding importantfeatures from a "golden sample" of a different label, thereby increasing thelikelihood of the model misclassifying the perturbed sample. The degree offeature substitution is adjustable, allowing us to control how much of theoriginal samples information is replaced. This flexibility effectively balancesa trade-off between the attacks effectiveness and its stealthiness. XSub isalso highly cost-effective in that the number of required queries to theprediction model and the explanation model in conducting the attack is in O(1).In addition, XSub can be easily extended to launch backdoor attacks in case theattacker has access to the models training data. Our evaluation demonstratesthat XSub is not only effective and stealthy but also cost-effective, enablingits application across a wide range of AI models.</description><author>Kiana Vu, Phung Lai, Truc Nguyen</author><pubDate>Fri, 13 Sep 2024 15:33:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08919v1</guid></item><item><title>Latent Space Score-based Diffusion Model for Probabilistic Multivariate Time Series Imputation</title><link>http://arxiv.org/abs/2409.08917v1</link><description>Accurate imputation is essential for the reliability and success ofdownstream tasks. Recently, diffusion models have attracted great attention inthis field. However, these models neglect the latent distribution in alower-dimensional space derived from the observed data, which limits thegenerative capacity of the diffusion model. Additionally, dealing with theoriginal missing data without labels becomes particularly problematic. Toaddress these issues, we propose the Latent Space Score-Based Diffusion Model(LSSDM) for probabilistic multivariate time series imputation. Observed valuesare projected onto low-dimensional latent space and coarse values of themissing data are reconstructed without knowing their ground truth values bythis unsupervised learning approach. Finally, the reconstructed values are fedinto a conditional diffusion model to obtain the precise imputed values of thetime series. In this way, LSSDM not only possesses the power to identify thelatent distribution but also seamlessly integrates the diffusion model toobtain the high-fidelity imputed values and assess the uncertainty of thedataset. Experimental results demonstrate that LSSDM achieves superiorimputation performance while also providing a better explanation anduncertainty analysis of the imputation mechanism. The website of the code is\textit{https://github.com/gorgen2020/LSSDM\_imputation}.</description><author>Guojun Liang, Najmeh Abiri, Atiye Sadat Hashemi, Jens Lundström, Stefan Byttner, Prayag Tiwari</author><pubDate>Fri, 13 Sep 2024 15:32:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08917v1</guid></item><item><title>Farmer.Chat: Scaling AI-Powered Agricultural Services for Smallholder Farmers</title><link>http://arxiv.org/abs/2409.08916v1</link><description>Small and medium-sized agricultural holders face challenges like limitedaccess to localized, timely information, impacting productivity andsustainability. Traditional extension services, which rely on in-person agents,struggle with scalability and timely delivery, especially in remote areas. Weintroduce Farmer.Chat, a generative AI-powered chatbot designed to addressthese issues. Leveraging Generative AI, Farmer.Chat offers personalized,reliable, and contextually relevant advice, overcoming limitations of previouschatbots in deterministic dialogue flows, language support, and unstructureddata processing. Deployed in four countries, Farmer.Chat has engaged over15,000 farmers and answered over 300,000 queries. This paper highlights howFarmer.Chat's innovative use of GenAI enhances agricultural service scalabilityand effectiveness. Our evaluation, combining quantitative analysis andqualitative insights, highlights Farmer.Chat's effectiveness in improvingfarming practices, enhancing trust, response quality, and user engagement.</description><author>Namita Singh, Jacqueline Wang'ombe, Nereah Okanga, Tetyana Zelenska, Jona Repishti, Jayasankar G K, Sanjeev Mishra, Rajsekar Manokaran, Vineet Singh, Mohammed Irfan Rafiq, Rikin Gandhi, Akshay Nambi</author><pubDate>Fri, 13 Sep 2024 15:31:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08916v1</guid></item><item><title>HLTCOE JHU Submission to the Voice Privacy Challenge 2024</title><link>http://arxiv.org/abs/2409.08913v1</link><description>We present a number of systems for the Voice Privacy Challenge, includingvoice conversion based systems such as the kNN-VC method and the WavLM voiceConversion method, and text-to-speech (TTS) based systems includingWhisper-VITS. We found that while voice conversion systems better preserveemotional content, they struggle to conceal speaker identity in semi-white-boxattack scenarios; conversely, TTS methods perform better at anonymization andworse at emotion preservation. Finally, we propose a random admixture systemwhich seeks to balance out the strengths and weaknesses of the two category ofsystems, achieving a strong EER of over 40% while maintaining UAR at arespectable 47%.</description><author>Henry Li Xinyuan, Zexin Cai, Ashi Garg, Kevin Duh, Leibny Paola García-Perera, Sanjeev Khudanpur, Nicholas Andrews, Matthew Wiesner</author><pubDate>Fri, 13 Sep 2024 15:29:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08913v1</guid></item><item><title>Affective Computing Has Changed: The Foundation Model Disruption</title><link>http://arxiv.org/abs/2409.08907v1</link><description>The dawn of Foundation Models has on the one hand revolutionised a wide rangeof research problems, and, on the other hand, democratised the access and useof AI-based tools by the general public. We even observe an incursion of thesemodels into disciplines related to human psychology, such as the AffectiveComputing domain, suggesting their affective, emerging capabilities. In thiswork, we aim to raise awareness of the power of Foundation Models in the fieldof Affective Computing by synthetically generating and analysing multimodalaffective data, focusing on vision, linguistics, and speech (acoustics). Wealso discuss some fundamental problems, such as ethical issues and regulatoryaspects, related to the use of Foundation Models in this research area.</description><author>Björn Schuller, Adria Mallol-Ragolta, Alejandro Peña Almansa, Iosif Tsangko, Mostafa M. Amin, Anastasia Semertzidou, Lukas Christ, Shahin Amiriparian</author><pubDate>Fri, 13 Sep 2024 15:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08907v1</guid></item><item><title>Gaussian is All You Need: A Unified Framework for Solving Inverse Problems via Diffusion Posterior Sampling</title><link>http://arxiv.org/abs/2409.08906v1</link><description>Diffusion models can generate a variety of high-quality images by modelingcomplex data distributions. Trained diffusion models can also be very effectiveimage priors for solving inverse problems. Most of the existing diffusion-basedmethods integrate data consistency steps within the diffusion reverse samplingprocess. The data consistency steps rely on an approximate likelihood function.In this paper, we show that the existing approximations are either insufficientor computationally inefficient. To address these issues, we propose a unifiedlikelihood approximation method that incorporates a covariance correction termto enhance the performance and avoids propagating gradients through thediffusion model. The correction term, when integrated into the reversediffusion sampling process, achieves better convergence towards the true dataposterior for selected distributions and improves performance on real-worldnatural image datasets. Furthermore, we present an efficient way to factorizeand invert the covariance matrix of the likelihood function for several inverseproblems. We present comprehensive experiments to demonstrate the effectivenessof our method over several existing approaches.</description><author>Nebiyou Yismaw, Ulugbek S. Kamilov, M. Salman Asif</author><pubDate>Fri, 13 Sep 2024 15:20:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08906v1</guid></item><item><title>AlphaDou: High-Performance End-to-End Doudizhu AI Integrating Bidding</title><link>http://arxiv.org/abs/2407.10279v2</link><description>Artificial intelligence for card games has long been a popular topic in AIresearch. In recent years, complex card games like Mahjong and Texas Hold'emhave been solved, with corresponding AI programs reaching the level of humanexperts. However, the game of Doudizhu presents significant challenges due toits vast state/action space and unique characteristics involving reasoningabout competition and cooperation, making the game extremely difficult tosolve.The RL model Douzero, trained using the Deep Monte Carlo algorithmframework, has shown excellent performance in Doudizhu. However, there aredifferences between its simplified game environment and the actual Doudizhuenvironment, and its performance is still a considerable distance from that ofhuman experts. This paper modifies the Deep Monte Carlo algorithm framework byusing reinforcement learning to obtain a neural network that simultaneouslyestimates win rates and expectations. The action space is pruned usingexpectations, and strategies are generated based on win rates. The modifiedalgorithm enables the AI to perform the full range of tasks in the Doudizhugame, including bidding and cardplay. The model was trained in a actualDoudizhu environment and achieved state-of-the-art performance among publiclyavailable models. We hope that this new framework will provide valuableinsights for AI development in other bidding-based games.</description><author>Chang Lei, Huan Lei</author><pubDate>Fri, 13 Sep 2024 15:17:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10279v2</guid></item><item><title>D2-MLP: Dynamic Decomposed MLP Mixer for Medical Image Segmentation</title><link>http://arxiv.org/abs/2409.08905v1</link><description>Convolutional neural networks are widely used in various segmentation tasksin medical images. However, they are challenged to learn global featuresadaptively due to the inherent locality of convolutional operations. Incontrast, MLP Mixers are proposed as a backbone to learn global informationacross channels with low complexity. However, they cannot capture spatialfeatures efficiently. Additionally, they lack effective mechanisms to fuse andmix features adaptively. To tackle these limitations, we propose a novelDynamic Decomposed Mixer module. It is designed to employ novel Mixers toextract features and aggregate information across different spatial locationsand channels. Additionally, it employs novel dynamic mixing mechanisms to modelinter-dependencies between channel and spatial feature representations and tofuse them adaptively. Subsequently, we incorporate it into a U-shapedTransformer-based architecture to generate a novel network, termed the DynamicDecomposed MLP Mixer. We evaluated it for medical image segmentation on twodatasets, and it achieved superior segmentation performance than otherstate-of-the-art methods.</description><author>Jin Yang, Xiaobing Yu, Peijie Qiu</author><pubDate>Fri, 13 Sep 2024 15:16:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08905v1</guid></item><item><title>AnyBipe: An End-to-End Framework for Training and Deploying Bipedal Robots Guided by Large Language Models</title><link>http://arxiv.org/abs/2409.08904v1</link><description>Training and deploying reinforcement learning (RL) policies for robots,especially in accomplishing specific tasks, presents substantial challenges.Recent advancements have explored diverse reward function designs, trainingtechniques, simulation-to-reality (sim-to-real) transfers, and performanceanalysis methodologies, yet these still require significant human intervention.This paper introduces an end-to-end framework for training and deploying RLpolicies, guided by Large Language Models (LLMs), and evaluates itseffectiveness on bipedal robots. The framework consists of three interconnectedmodules: an LLM-guided reward function design module, an RL training moduleleveraging prior work, and a sim-to-real homomorphic evaluation module. Thisdesign significantly reduces the need for human input by utilizing onlyessential simulation and deployment platforms, with the option to incorporatehuman-engineered strategies and historical data. We detail the construction ofthese modules, their advantages over traditional approaches, and demonstratethe framework's capability to autonomously develop and refine controllingstrategies for bipedal robot locomotion, showcasing its potential to operateindependently of human intervention.</description><author>Yifei Yao, Wentao He, Chenyu Gu, Jiaheng Du, Fuwei Tan, Zhen Zhu, Junguo Lu</author><pubDate>Fri, 13 Sep 2024 15:15:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08904v1</guid></item><item><title>Pavlok-Nudge: A Feedback Mechanism for Atomic Behaviour Modification with Snoring Usecase</title><link>http://arxiv.org/abs/2305.06110v3</link><description>This paper proposes a feedback mechanism to change behavioural patterns usingthe Pavlok device. Pavlok utilises beeps, vibration and shocks as a mode ofaversion technique to help individuals with behaviour modification. While thedevice can be useful in certain periodic daily life situations, like alarms andexercise notifications, the device relies on manual operations that limit itsusage. To automate behaviour modification, we propose a framework that firstdetects targeted behaviours through a lightweight deep learning model andsubsequently nudges the user through Pavlok. Our proposed solution isimplemented and verified in the context of snoring, which captures audio fromthe environment following a prediction of whether the audio content is a snoreor not using a 1D convolutional neural network. Based on the prediction, we usePavlok to nudge users for preventive measures, such as a change in sleepingposture. We believe that this simple solution can help people to change theiratomic habits, which may lead to long-term health benefits. Our proposedreal-time, lightweight model (99.8% less parameters over SOTA; 1,278,049 --&gt;1337) achieves SOTA performance (test accuracy of 0.99) on a public domainbenchmark. The code and model are publicly available athttps://github.com/hasan-rakibul/pavlok-nudge-snore.</description><author>Md Rakibul Hasan, Shreya Ghosh, Pradyumna Agrawal, Zhixi Cai, Abhinav Dhall, Tom Gedeon</author><pubDate>Fri, 13 Sep 2024 15:09:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06110v3</guid></item><item><title>Synthetic Human Memories: AI-Edited Images and Videos Can Implant False Memories and Distort Recollection</title><link>http://arxiv.org/abs/2409.08895v1</link><description>AI is increasingly used to enhance images and videos, both intentionally andunintentionally. As AI editing tools become more integrated into smartphones,users can modify or animate photos into realistic videos. This study examinesthe impact of AI-altered visuals on false memories--recollections of eventsthat didn't occur or deviate from reality. In a pre-registered study, 200participants were divided into four conditions of 50 each. Participants viewedoriginal images, completed a filler task, then saw stimuli corresponding totheir assigned condition: unedited images, AI-edited images, AI-generatedvideos, or AI-generated videos of AI-edited images. AI-edited visualssignificantly increased false recollections, with AI-generated videos ofAI-edited images having the strongest effect (2.05x compared to control).Confidence in false memories was also highest for this condition (1.19xcompared to control). We discuss potential applications in HCI, such astherapeutic memory reframing, and challenges in ethical, legal, political, andsocietal domains.</description><author>Pat Pataranutaporn, Chayapatr Archiwaranguprok, Samantha W. T. Chan, Elizabeth Loftus, Pattie Maes</author><pubDate>Fri, 13 Sep 2024 15:08:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08895v1</guid></item><item><title>Exploring Action-Centric Representations Through the Lens of Rate-Distortion Theory</title><link>http://arxiv.org/abs/2409.08892v1</link><description>Organisms have to keep track of the information in the environment that isrelevant for adaptive behaviour. Transmitting information in an economical andefficient way becomes crucial for limited-resourced agents living inhigh-dimensional environments. The efficient coding hypothesis claims thatorganisms seek to maximize the information about the sensory input in anefficient manner. Under Bayesian inference, this means that the role of thebrain is to efficiently allocate resources in order to make predictions aboutthe hidden states that cause sensory data. However, neither of those frameworksaccounts for how that information is exploited downstream, leaving aside theaction-oriented role of the perceptual system. Rate-distortion theory, whichdefines optimal lossy compression under constraints, has gained attention as aformal framework to explore goal-oriented efficient coding. In this work, weexplore action-centric representations in the context of rate-distortiontheory. We also provide a mathematical definition of abstractions and we arguethat, as a summary of the relevant details, they can be used to fix the contentof action-centric representations. We model action-centric representationsusing VAEs and we find that such representations i) are efficient lossycompressions of the data; ii) capture the task-dependent invariances necessaryto achieve successful behaviour; and iii) are not in service of reconstructingthe data. Thus, we conclude that full reconstruction of the data is rarelyneeded to achieve optimal behaviour, consistent with a teleological approach toperception.</description><author>Miguel de Llanza Varona, Christopher L. Buckley, Beren Millidge</author><pubDate>Fri, 13 Sep 2024 15:07:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08892v1</guid></item><item><title>MLP, XGBoost, KAN, TDNN, and LSTM-GRU Hybrid RNN with Attention for SPX and NDX European Call Option Pricing</title><link>http://arxiv.org/abs/2409.06724v2</link><description>We explore the performance of various artificial neural networkarchitectures, including a multilayer perceptron (MLP), Kolmogorov-Arnoldnetwork (KAN), LSTM-GRU hybrid recursive neural network (RNN) models, and atime-delay neural network (TDNN) for pricing European call options. In thisstudy, we attempt to leverage the ability of supervised learning methods, suchas ANNs, KANs, and gradient-boosted decision trees, to approximate complexmultivariate functions in order to calibrate option prices based on past marketdata. The motivation for using ANNs and KANs is the Universal ApproximationTheorem and Kolmogorov-Arnold Representation Theorem, respectively.Specifically, we use S\&amp;P 500 (SPX) and NASDAQ 100 (NDX) index options tradedduring 2015-2023 with times to maturity ranging from 15 days to over 4 years(OptionMetrics IvyDB US dataset). Black \&amp; Scholes's (BS) PDE \cite{Black1973}model's performance in pricing the same options compared to real data is usedas a benchmark. This model relies on strong assumptions, and it has beenobserved and discussed in the literature that real data does not match itspredictions. Supervised learning methods are widely used as an alternative forcalibrating option prices due to some of the limitations of this model. In ourexperiments, the BS model underperforms compared to all of the others. Also,the best TDNN model outperforms the best MLP model on all error metrics. Weimplement a simple self-attention mechanism to enhance the RNN models,significantly improving their performance. The best-performing model overall isthe LSTM-GRU hybrid RNN model with attention. Also, the KAN model outperformsthe TDNN and MLP models. We analyze the performance of all models by ticker,moneyness category, and over/under/correctly-priced percentage.</description><author>Boris Ter-Avanesov, Homayoon Beigi</author><pubDate>Fri, 13 Sep 2024 15:06:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06724v2</guid></item><item><title>IBoxCLA: Towards Robust Box-supervised Segmentation of Polyp via Improved Box-dice and Contrastive Latent-anchors</title><link>http://arxiv.org/abs/2310.07248v4</link><description>Box-supervised polyp segmentation attracts increasing attention for itscost-effective potential. Existing solutions often rely on learning-freemethods or pretrained models to laboriously generate pseudo masks, triggeringDice constraint subsequently. In this paper, we found that a model guided bythe simplest box-filled masks can accurately predict polyp locations/sizes, butsuffers from shape collapsing. In response, we propose two innovative learningfashions, Improved Box-dice (IBox) and Contrastive Latent-Anchors (CLA), andcombine them to train a robust box-supervised model IBoxCLA. The core ideabehind IBoxCLA is to decouple the learning of location/size and shape, allowingfor focused constraints on each of them. Specifically, IBox transforms thesegmentation map into a proxy map using shape decoupling and confusion-regionswapping sequentially. Within the proxy map, shapes are disentangled, whilelocations/sizes are encoded as box-like responses. By constraining the proxymap instead of the raw prediction, the box-filled mask can well superviseIBoxCLA without misleading its shape learning. Furthermore, CLA contributes toshape learning by generating two types of latent anchors, which are learned andupdated using momentum and segmented polyps to steadily represent polyp andbackground features. The latent anchors facilitate IBoxCLA to capturediscriminative features within and outside boxes in a contrastive manner,yielding clearer boundaries. We benchmark IBoxCLA on five public polypdatasets. The experimental results demonstrate the competitive performance ofIBoxCLA compared to recent fully-supervised polyp segmentation methods, and itssuperiority over other box-supervised state-of-the-arts with a relativeincrease of overall mDice and mIoU by at least 6.5% and 7.5%, respectively.</description><author>Zhiwei Wang, Qiang Hu, Hongkuan Shi, Li He, Man He, Wenxuan Dai, Yinjiao Tian, Xin Yang, Mei Liu, Qiang Li</author><pubDate>Fri, 13 Sep 2024 15:06:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07248v4</guid></item><item><title>Natural Language Processing with Commonsense Knowledge: A Survey</title><link>http://arxiv.org/abs/2108.04674v2</link><description>Commonsense knowledge is essential for advancing natural language processing(NLP) by enabling models to engage in human-like reasoning, which requires adeeper understanding of context and often involves making inferences based onimplicit external knowledge. This paper explores the integration of commonsenseknowledge into various NLP tasks. We begin by reviewing prominent commonsenseknowledge bases and then discuss the benchmarks used to evaluate thecommonsense reasoning capabilities of NLP models, particularly language models.Furthermore, we highlight key methodologies for incorporating commonsenseknowledge and their applications across different NLP tasks. The paper alsoexamines the challenges and emerging trends in enhancing NLP systems withcommonsense reasoning. All literature referenced in this survey can be accessedvia our GitHub repository: https://github.com/yuboxie/awesome-commonsense.</description><author>Yubo Xie, Zonghui Liu, Zongyang Ma, Fanyuan Meng, Yan Xiao, Fahui Miao, Pearl Pu</author><pubDate>Fri, 13 Sep 2024 15:00:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.04674v2</guid></item><item><title>Understanding How CodeLLMs (Mis)Predict Types with Activation Steering</title><link>http://arxiv.org/abs/2404.01903v2</link><description>CodeLLMs are transforming software development as we know it. This isespecially true for tasks where rule-based approaches fall short, like typeprediction. The type prediction task consists in adding a new type annotationto a partially typed program, such that the resulting program is closer tobeing fully typed. The intractability of rule-based approaches and high cost ofmanual annotation make CodeLLMs an attractive solution to the problem. However,CodeLLMs are still far from being deployed on the large-scale due to doubtssurrounding their reliability. To shed some light on how CodeLLMs approach type prediction, we investigatewhat happens when a model mispredicts a type. We show that by applyingsemantics-preserving edits to code, CodeLLMs are eventually misled intomispredicting type annotations. However, by leveraging activation steering weare able to "steer" the model back to the correct prediction, making modelsmore robust against semantically irrelevant prompt features. We show thatsteering achieves comparable performance to fine-tuning directly on the typeprediction task. Furthermore, we find that steering vectors computed fromPython code are effective at correcting TypeScript mispredictions, and viceversa. To our knowledge, this is the first evidence of its kind to suggest thatCodeLLMs learn task representations that transfer across languages.</description><author>Francesca Lucchetti, Arjun Guha</author><pubDate>Fri, 13 Sep 2024 14:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01903v2</guid></item><item><title>Visual Language Tracking with Multi-modal Interaction: A Robust Benchmark</title><link>http://arxiv.org/abs/2409.08887v1</link><description>Visual Language Tracking (VLT) enhances tracking by mitigating thelimitations of relying solely on the visual modality, utilizing high-levelsemantic information through language. This integration of the language enablesmore advanced human-machine interaction. The essence of interaction iscognitive alignment, which typically requires multiple information exchanges,especially in the sequential decision-making process of VLT. However, currentVLT benchmarks do not account for multi-round interactions during tracking.They provide only an initial text and bounding box (bbox) in the first frame,with no further interaction as tracking progresses, deviating from the originalmotivation of the VLT task. To address these limitations, we propose a noveland robust benchmark, VLT-MI (Visual Language Tracking with Multi-modalInteraction), which introduces multi-round interaction into the VLT task forthe first time. (1) We generate diverse, multi-granularity texts formulti-round, multi-modal interaction based on existing mainstream VLTbenchmarks using DTLLM-VLT, leveraging the world knowledge of LLMs. (2) Wepropose a new VLT interaction paradigm that achieves multi-round interactionthrough text updates and object recovery. When multiple tracking failuresoccur, we provide the tracker with more aligned texts and corrected bboxesthrough interaction, thereby expanding the scope of VLT downstream tasks. (3)We conduct comparative experiments on both traditional VLT benchmarks andVLT-MI, evaluating and analyzing the accuracy and robustness of trackers underthe interactive paradigm. This work offers new insights and paradigms for theVLT task, enabling a fine-grained evaluation of multi-modal trackers. Webelieve this approach can be extended to additional datasets in the future,supporting broader evaluations and comparisons of video-language modelcapabilities.</description><author>Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang</author><pubDate>Fri, 13 Sep 2024 14:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08887v1</guid></item><item><title>Interactive Masked Image Modeling for Multimodal Object Detection in Remote Sensing</title><link>http://arxiv.org/abs/2409.08885v1</link><description>Object detection in remote sensing imagery plays a vital role in variousEarth observation applications. However, unlike object detection in naturalscene images, this task is particularly challenging due to the abundance ofsmall, often barely visible objects across diverse terrains. To address thesechallenges, multimodal learning can be used to integrate features fromdifferent data modalities, thereby improving detection accuracy. Nonetheless,the performance of multimodal learning is often constrained by the limited sizeof labeled datasets. In this paper, we propose to use Masked Image Modeling(MIM) as a pre-training technique, leveraging self-supervised learning onunlabeled data to enhance detection performance. However, conventional MIM suchas MAE which uses masked tokens without any contextual information, strugglesto capture the fine-grained details due to a lack of interactions with otherparts of image. To address this, we propose a new interactive MIM method thatcan establish interactions between different tokens, which is particularlybeneficial for object detection in remote sensing. The extensive ablationstudies and evluation demonstrate the effectiveness of our approach.</description><author>Minh-Duc Vu, Zuheng Ming, Fangchen Feng, Bissmella Bahaduri, Anissa Mokraoui</author><pubDate>Fri, 13 Sep 2024 14:50:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08885v1</guid></item><item><title>Detect Fake with Fake: Leveraging Synthetic Data-driven Representation for Synthetic Image Detection</title><link>http://arxiv.org/abs/2409.08884v1</link><description>Are general-purpose visual representations acquired solely from syntheticdata useful for detecting fake images? In this work, we show the effectivenessof synthetic data-driven representations for synthetic image detection. Uponanalysis, we find that vision transformers trained by the latest visualrepresentation learners with synthetic data can effectively distinguish fakefrom real images without seeing any real images during pre-training. Notably,using SynCLR as the backbone in a state-of-the-art detection methoddemonstrates a performance improvement of +10.32 mAP and +4.73% accuracy overthe widely used CLIP, when tested on previously unseen GAN models. Code isavailable at https://github.com/cvpaperchallenge/detect-fake-with-fake.</description><author>Hina Otake, Yoshihiro Fukuhara, Yoshiki Kubotani, Shigeo Morishima</author><pubDate>Fri, 13 Sep 2024 14:50:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08884v1</guid></item><item><title>DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large Language Models</title><link>http://arxiv.org/abs/2404.03275v2</link><description>Recent advancements in Large Language Models (LLMs) have sparked a revolutionacross many research fields. In robotics, the integration of common-senseknowledge from LLMs into task and motion planning has drastically advanced thefield by unlocking unprecedented levels of context awareness. Despite theirvast collection of knowledge, large language models may generate infeasibleplans due to hallucinations or missing domain information. To address thesechallenges and improve plan feasibility and computational efficiency, weintroduce DELTA, a novel LLM-informed task planning approach. By using scenegraphs as environment representations within LLMs, DELTA achieves rapidgeneration of precise planning problem descriptions. To enhance planningperformance, DELTA decomposes long-term task goals with LLMs into anautoregressive sequence of sub-goals, enabling automated task planners toefficiently solve complex problems. In our extensive evaluation, we show thatDELTA enables an efficient and fully automatic task planning pipeline,achieving higher planning success rates and significantly shorter planningtimes compared to the state of the art.</description><author>Yuchen Liu, Luigi Palmieri, Sebastian Koch, Ilche Georgievski, Marco Aiello</author><pubDate>Fri, 13 Sep 2024 14:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03275v2</guid></item><item><title>An Automatic Quality Metric for Evaluating Simultaneous Interpretation</title><link>http://arxiv.org/abs/2407.06650v2</link><description>Simultaneous interpretation (SI), the translation of one language to anotherin real time, starts translation before the original speech has finished. Itsevaluation needs to consider both latency and quality. This trade-off ischallenging especially for distant word order language pairs such as Englishand Japanese. To handle this word order gap, interpreters maintain the wordorder of the source language as much as possible to keep up with originallanguage to minimize its latency while maintaining its quality, whereas intranslation reordering happens to keep fluency in the target language. Thismeans outputs synchronized with the source language are desirable based on thereal SI situation, and it's a key for further progress in computational SI andsimultaneous machine translation (SiMT). In this work, we propose an automaticevaluation metric for SI and SiMT focusing on word order synchronization. Ourevaluation metric is based on rank correlation coefficients, leveragingcross-lingual pre-trained language models. Our experimental results onNAIST-SIC-Aligned and JNPC showed our metrics' effectiveness to measure wordorder synchronization between source and target language.</description><author>Mana Makinae, Katsuhito Sudoh, Mararu Yamada, Satoshi Nakamura</author><pubDate>Fri, 13 Sep 2024 14:39:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06650v2</guid></item><item><title>Exploring the Impact of Data Quantity on ASR in Extremely Low-resource Languages</title><link>http://arxiv.org/abs/2409.08872v1</link><description>This study investigates the efficacy of data augmentation techniques forlow-resource automatic speech recognition (ASR), focusing on two endangeredAustronesian languages, Amis and Seediq. Recognizing the potential ofself-supervised learning (SSL) in low-resource settings, we explore the impactof data volume on the continued pre-training of SSL models. We propose a noveldata-selection scheme leveraging a multilingual corpus to augment the limitedtarget language data. This scheme utilizes a language classifier to extractutterance embeddings and employs one-class classifiers to identify utterancesphonetically and phonologically proximate to the target languages. Utterancesare ranked and selected based on their decision scores, ensuring the inclusionof highly relevant data in the SSL-ASR pipeline. Our experimental resultsdemonstrate the effectiveness of this approach, yielding substantialimprovements in ASR performance for both Amis and Seediq. These findingsunderscore the feasibility and promise of data augmentation throughcross-lingual transfer learning for low-resource language ASR.</description><author>Yao-Fei Cheng, Li-Wei Chen, Hung-Shin Lee, Hsin-Min Wang</author><pubDate>Fri, 13 Sep 2024 14:35:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08872v1</guid></item><item><title>Linear Attention is Enough in Spatial-Temporal Forecasting</title><link>http://arxiv.org/abs/2408.09158v2</link><description>As the most representative scenario of spatial-temporal forecasting tasks,the traffic forecasting task attracted numerous attention from machine learningcommunity due to its intricate correlation both in space and time dimension.Existing methods often treat road networks over time as spatial-temporalgraphs, addressing spatial and temporal representations independently. However,these approaches struggle to capture the dynamic topology of road networks,encounter issues with message passing mechanisms and over-smoothing, and facechallenges in learning spatial and temporal relationships separately. Toaddress these limitations, we propose treating nodes in road networks atdifferent time steps as independent spatial-temporal tokens and feeding theminto a vanilla Transformer to learn complex spatial-temporal patterns, design\textbf{STformer} achieving SOTA. Given its quadratic complexity, we introducea variant \textbf{NSTformer} based on Nystr$\ddot{o}$m method to approximateself-attention with linear complexity but even slightly better than former in afew cases astonishingly. Extensive experimental results on traffic datasetsdemonstrate that the proposed method achieves state-of-the-art performance atan affordable computational cost. Our code is available at\href{https://github.com/XinyuNing/STformer-and-NSTformer}{https://github.com/XinyuNing/STformer-and-NSTformer}.</description><author>Xinyu Ning</author><pubDate>Fri, 13 Sep 2024 14:34:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.09158v2</guid></item><item><title>The Role of Explainable AI in Revolutionizing Human Health Monitoring</title><link>http://arxiv.org/abs/2409.07347v2</link><description>The complex nature of disease mechanisms and the variability of patientsymptoms present significant obstacles in developing effective diagnostictools. Although machine learning has made considerable advances in medicaldiagnosis, its decision-making processes frequently lack transparency, whichcan jeopardize patient outcomes. This underscores the critical need forExplainable AI (XAI), which not only offers greater clarity but also has thepotential to significantly improve patient care. In this literature review, weconduct a detailed analysis of analyzing XAI methods identified throughsearches across various databases, focusing on chronic conditions such asParkinson's, stroke, depression, cancer, heart disease, and Alzheimer'sdisease. The literature search revealed the application of 9 trending XAIalgorithms in the field of healthcare and highlighted the pros and cons of eachof them. Thus, the article is concluded with a critical appraisal of thechallenges and future research opportunities for XAI in human healthmonitoring.</description><author>Abdullah Alharthi, Ahmed Alqurashi, Turki Alharbi, Mohammed Alammar, Nasser Aldosari, Houssem Bouchekara, Yusuf Shaaban, Mohammad Shoaib Shahriar, Abdulrahman Al Ayidh</author><pubDate>Fri, 13 Sep 2024 14:32:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07347v2</guid></item><item><title>Exploring Graph Structure Comprehension Ability of Multimodal Large Language Models: Case Studies</title><link>http://arxiv.org/abs/2409.08864v1</link><description>Large Language Models (LLMs) have shown remarkable capabilities in processingvarious data structures, including graphs. While previous research has focusedon developing textual encoding methods for graph representation, the emergenceof multimodal LLMs presents a new frontier for graph comprehension. Theseadvanced models, capable of processing both text and images, offer potentialimprovements in graph understanding by incorporating visual representationsalongside traditional textual data. This study investigates the impact of graphvisualisations on LLM performance across a range of benchmark tasks at node,edge, and graph levels. Our experiments compare the effectiveness of multimodalapproaches against purely textual graph representations. The results providevaluable insights into both the potential and limitations of leveraging visualgraph modalities to enhance LLMs' graph structure comprehension abilities.</description><author>Zhiqiang Zhong, Davide Mottin</author><pubDate>Fri, 13 Sep 2024 14:26:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08864v1</guid></item><item><title>Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control</title><link>http://arxiv.org/abs/2409.08861v1</link><description>Dynamical generative models that produce samples through an iterativeprocess, such as Flow Matching and denoising diffusion models, have seenwidespread use, but there has not been many theoretically-sound methods forimproving these models with reward fine-tuning. In this work, we cast rewardfine-tuning as stochastic optimal control (SOC). Critically, we prove that avery specific memoryless noise schedule must be enforced during fine-tuning, inorder to account for the dependency between the noise variable and thegenerated samples. We also propose a new algorithm named Adjoint Matching whichoutperforms existing SOC algorithms, by casting SOC problems as a regressionproblem. We find that our approach significantly improves over existing methodsfor reward fine-tuning, achieving better consistency, realism, andgeneralization to unseen human preference reward models, while retaining samplediversity.</description><author>Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, Ricky T. Q. Chen</author><pubDate>Fri, 13 Sep 2024 14:22:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08861v1</guid></item><item><title>InstantDrag: Improving Interactivity in Drag-based Image Editing</title><link>http://arxiv.org/abs/2409.08857v1</link><description>Drag-based image editing has recently gained popularity for its interactivityand precision. However, despite the ability of text-to-image models to generatesamples within a second, drag editing still lags behind due to the challenge ofaccurately reflecting user interaction while maintaining image content. Someexisting approaches rely on computationally intensive per-image optimization orintricate guidance-based methods, requiring additional inputs such as masks formovable regions and text prompts, thereby compromising the interactivity of theediting process. We introduce InstantDrag, an optimization-free pipeline thatenhances interactivity and speed, requiring only an image and a draginstruction as input. InstantDrag consists of two carefully designed networks:a drag-conditioned optical flow generator (FlowGen) and an opticalflow-conditioned diffusion model (FlowDiffusion). InstantDrag learns motiondynamics for drag-based image editing in real-world video datasets bydecomposing the task into motion generation and motion-conditioned imagegeneration. We demonstrate InstantDrag's capability to perform fast,photo-realistic edits without masks or text prompts through experiments onfacial video datasets and general scenes. These results highlight theefficiency of our approach in handling drag-based image editing, making it apromising solution for interactive, real-time applications.</description><author>Joonghyuk Shin, Daehyeon Choi, Jaesik Park</author><pubDate>Fri, 13 Sep 2024 14:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08857v1</guid></item><item><title>ODYSSEE: Oyster Detection Yielded by Sensor Systems on Edge Electronics</title><link>http://arxiv.org/abs/2409.07003v2</link><description>Oysters are a vital keystone species in coastal ecosystems, providingsignificant economic, environmental, and cultural benefits. As the importanceof oysters grows, so does the relevance of autonomous systems for theirdetection and monitoring. However, current monitoring strategies often rely ondestructive methods. While manual identification of oysters from video footageis non-destructive, it is time-consuming, requires expert input, and is furthercomplicated by the challenges of the underwater environment. To address these challenges, we propose a novel pipeline using stablediffusion to augment a collected real dataset with realistic synthetic data.This method enhances the dataset used to train a YOLOv10-based vision model.The model is then deployed and tested on an edge platform in underwaterrobotics, achieving a state-of-the-art 0.657 mAP@50 for oyster detection on theAqua2 platform.</description><author>Xiaomin Lin, Vivek Mange, Arjun Suresh, Bernhard Neuberger, Aadi Palnitkar, Brendan Campbell, Alan Williams, Kleio Baxevani, Jeremy Mallette, Alhim Vera, Markus Vincze, Ioannis Rekleitis, Herbert G. Tanner, Yiannis Aloimonos</author><pubDate>Fri, 13 Sep 2024 14:17:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07003v2</guid></item><item><title>Using The Concept Hierarchy for Household Action Recognition</title><link>http://arxiv.org/abs/2409.08853v1</link><description>We propose a method to systematically represent both the static and thedynamic components of environments, i.e. objects and agents, as well as thechanges that are happening in the environment, i.e. the actions and skillsperformed by agents. Our approach, the Concept Hierarchy, provides thenecessary information for autonomous systems to represent environment states,perform action modeling and recognition, and plan the execution of tasks.Additionally, the hierarchical structure supports generalization and knowledgetransfer to environments. We rigorously define tasks, actions, skills, andaffordances that enable human-understandable action and skill recognition.</description><author>Andrei Costinescu, Luis Figueredo, Darius Burschka</author><pubDate>Fri, 13 Sep 2024 14:16:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08853v1</guid></item><item><title>Combining Data Generation and Active Learning for Low-Resource Question Answering</title><link>http://arxiv.org/abs/2211.14880v2</link><description>Neural approaches have become very popular in Question Answering (QA),however, they require a large amount of annotated data. In this work, wepropose a novel approach that combines data augmentation via question-answergeneration with Active Learning to improve performance in low-resourcesettings, where the target domains are diverse in terms of difficulty andsimilarity to the source domain. We also investigate Active Learning forquestion answering in different stages, overall reducing the annotation effortof humans. For this purpose, we consider target domains in realistic settings,with an extremely low amount of annotated samples but with many unlabeleddocuments, which we assume can be obtained with little effort. Additionally, weassume a sufficient amount of labeled data from the source domain beingavailable. We perform extensive experiments to find the best setup forincorporating domain experts. Our findings show that our novel approach, wherehumans are incorporated in a data generation approach, boosts performance inthe low-resource, domain-specific setting, allowing for low-labeling-effortquestion answering systems in new, specialized domains. They furtherdemonstrate how human annotation affects the performance of QA depending on thestage it is performed.</description><author>Maximilian Kimmich, Andrea Bartezzaghi, Jasmina Bogojeska, Cristiano Malossi, Ngoc Thang Vu</author><pubDate>Fri, 13 Sep 2024 14:06:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14880v2</guid></item><item><title>DX2CT: Diffusion Model for 3D CT Reconstruction from Bi or Mono-planar 2D X-ray(s)</title><link>http://arxiv.org/abs/2409.08850v1</link><description>Computational tomography (CT) provides high-resolution medical imaging, butit can expose patients to high radiation. X-ray scanners have low radiationexposure, but their resolutions are low. This paper proposes a new conditionaldiffusion model, DX2CT, that reconstructs three-dimensional (3D) CT volumesfrom bi or mono-planar X-ray image(s). Proposed DX2CT consists of two keycomponents: 1) modulating feature maps extracted from two-dimensional (2D)X-ray(s) with 3D positions of CT volume using a new transformer and 2)effectively using the modulated 3D position-aware feature maps as conditions ofDX2CT. In particular, the proposed transformer can provide conditions with richinformation of a target CT slice to the conditional diffusion model, enablinghigh-quality CT reconstruction. Our experiments with the bi or mono-planarX-ray(s) benchmark datasets show that proposed DX2CT outperforms severalstate-of-the-art methods. Our codes and model will be available at:https://www.github.com/intyeger/DX2CT.</description><author>Yun Su Jeong, Hye Bin Yoo, Il Yong Chun</author><pubDate>Fri, 13 Sep 2024 14:06:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08850v1</guid></item><item><title>Kinect Calibration and Data Optimization For Anthropometric Parameters</title><link>http://arxiv.org/abs/2409.08847v1</link><description>Recently, through development of several 3d vision systems, widely used invarious applications, medical and biometric fields. Microsoft kinect sensorhave been most of used camera among 3d vision systems. Microsoft kinect sensorcan obtain depth images of a scene and 3d coordinates of human joints. Thus,anthropometric features can extractable easily. Anthropometric feature and 3djoint coordinate raw datas which captured from kinect sensor is unstable. Thestrongest reason for this, datas vary by distance between joints of individualand location of kinect sensor. Consequently, usage of this datas without kinectcalibration and data optimization does not result in sufficient and healthy. Inthis study, proposed a novel method to calibrating kinect sensor and optimizingskeleton features. Results indicate that the proposed method is quite effectiveand worthy of further study in more general scenarios.</description><author>M. S. Gokmen, M. Akbaba, O. Findik</author><pubDate>Fri, 13 Sep 2024 14:05:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08847v1</guid></item><item><title>FP-VEC: Fingerprinting Large Language Models via Efficient Vector Addition</title><link>http://arxiv.org/abs/2409.08846v1</link><description>Training Large Language Models (LLMs) requires immense computational powerand vast amounts of data. As a result, protecting the intellectual property ofthese models through fingerprinting is essential for ownership authentication.While adding fingerprints to LLMs through fine-tuning has been attempted, itremains costly and unscalable. In this paper, we introduce FP-VEC, a pilotstudy on using fingerprint vectors as an efficient fingerprinting method forLLMs. Our approach generates a fingerprint vector that represents aconfidential signature embedded in the model, allowing the same fingerprint tobe seamlessly incorporated into an unlimited number of LLMs via vectoraddition. Results on several LLMs show that FP-VEC is lightweight by running onCPU-only devices for fingerprinting, scalable with a single training andunlimited fingerprinting process, and preserves the model's normal behavior.The project page is available at https://fingerprintvector.github.io .</description><author>Zhenhua Xu, Wenpeng Xing, Zhebo Wang, Chang Hu, Chen Jie, Meng Han</author><pubDate>Fri, 13 Sep 2024 14:04:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08846v1</guid></item><item><title>AIPO: Improving Training Objective for Iterative Preference Optimization</title><link>http://arxiv.org/abs/2409.08845v1</link><description>Preference Optimization (PO), is gaining popularity as an alternative choiceof Proximal Policy Optimization (PPO) for aligning Large Language Models(LLMs). Recent research on aligning LLMs iteratively with synthetic orpartially synthetic data shows promising results in scaling up PO training forboth academic settings and proprietary trained models such as Llama3. Despiteits success, our study shows that the length exploitation issue present in POis even more severe in Iterative Preference Optimization (IPO) due to theiterative nature of the process. In this work, we study iterative preferenceoptimization with synthetic data. We share the findings and analysis along theway of building the iterative preference optimization pipeline. Morespecifically, we discuss the length exploitation issue during iterativepreference optimization and propose our training objective for iterativepreference optimization, namely Agreement-aware Iterative PreferenceOptimization (AIPO). To demonstrate the effectiveness of our method, we conductcomprehensive experiments and achieve state-of-the-art performance on MT-Bench,AlpacaEval 2.0, and Arena-Hard. Our implementation and model checkpoints willbe made available at https://github.com/bytedance/AIPO.</description><author>Yaojie Shen, Xinyao Wang, Yulei Niu, Ying Zhou, Lexin Tang, Libo Zhang, Fan Chen, Longyin Wen</author><pubDate>Fri, 13 Sep 2024 14:03:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08845v1</guid></item><item><title>Predicting Sentence-Level Factuality of News and Bias of Media Outlets</title><link>http://arxiv.org/abs/2301.11850v4</link><description>Automated news credibility and fact-checking at scale require accuratelypredicting news factuality and media bias. This paper introduces a largesentence-level dataset, titled "FactNews", composed of 6,191 sentences expertlyannotated according to factuality and media bias definitions proposed byAllSides. We use FactNews to assess the overall reliability of news sources, byformulating two text classification problems for predicting sentence-levelfactuality of news reporting and bias of media outlets. Our experimentsdemonstrate that biased sentences present a higher number of words compared tofactual sentences, besides having a predominance of emotions. Hence, thefine-grained analysis of subjectivity and impartiality of news articlesprovided promising results for predicting the reliability of media outlets.Finally, due to the severity of fake news and political polarization in Brazil,and the lack of research for Portuguese, both dataset and baseline wereproposed for Brazilian Portuguese.</description><author>Francielle Vargas, Kokil Jaidka, Thiago A. S. Pardo, Fabrício Benevenuto</author><pubDate>Fri, 13 Sep 2024 13:53:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11850v4</guid></item><item><title>Direct-CP: Directed Collaborative Perception for Connected and Autonomous Vehicles via Proactive Attention</title><link>http://arxiv.org/abs/2409.08840v1</link><description>Collaborative perception (CP) leverages visual data from connected andautonomous vehicles (CAV) to enhance an ego vehicle's field of view (FoV).Despite recent progress, current CP methods expand the ego vehicle's 360-degreeperceptual range almost equally, which faces two key challenges. Firstly, inareas with uneven traffic distribution, focusing on directions with littletraffic offers limited benefits. Secondly, under limited communication budgets,allocating excessive bandwidth to less critical directions lowers theperception accuracy in more vital areas. To address these issues, we proposeDirect-CP, a proactive and direction-aware CP system aiming at improving CP inspecific directions. Our key idea is to enable an ego vehicle to proactivelysignal its interested directions and readjust its attention to enhance localdirectional CP performance. To achieve this, we first propose an RSU-aideddirection masking mechanism that assists an ego vehicle in identifying vitaldirections. Additionally, we design a direction-aware selective attentionmodule to wisely aggregate pertinent features based on ego vehicle'sdirectional priorities, communication budget, and the positional data of CAVs.Moreover, we introduce a direction-weighted detection loss (DWLoss) to capturethe divergence between directional CP outcomes and the ground truth,facilitating effective model training. Extensive experiments on the V2X-Sim 2.0dataset demonstrate that our approach achieves 19.8\% higher local perceptionaccuracy in interested directions and 2.5\% higher overall perception accuracythan the state-of-the-art methods in collaborative 3D object detection tasks.</description><author>Yihang Tao, Senkang Hu, Zhengru Fang, Yuguang Fang</author><pubDate>Fri, 13 Sep 2024 13:53:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08840v1</guid></item><item><title>RF Challenge: The Data-Driven Radio Frequency Signal Separation Challenge</title><link>http://arxiv.org/abs/2409.08839v1</link><description>This paper addresses the critical problem of interference rejection inradio-frequency (RF) signals using a novel, data-driven approach that leveragesstate-of-the-art AI models. Traditionally, interference rejection algorithmsare manually tailored to specific types of interference. This work introduces amore scalable data-driven solution and contains the following contributions.First, we present an insightful signal model that serves as a foundation fordeveloping and analyzing interference rejection algorithms. Second, weintroduce the RF Challenge, a publicly available dataset featuring diverse RFsignals along with code templates, which facilitates data-driven analysis of RFsignal problems. Third, we propose novel AI-based rejection algorithms,specifically architectures like UNet and WaveNet, and evaluate theirperformance across eight different signal mixture types. These modelsdemonstrate superior performance exceeding traditional methods like matchedfiltering and linear minimum mean square error estimation by up to two ordersof magnitude in bit-error rate. Fourth, we summarize the results from an opencompetition hosted at 2024 IEEE International Conference on Acoustics, Speech,and Signal Processing (ICASSP 2024) based on the RF Challenge, highlighting thesignificant potential for continued advancements in this area. Our findingsunderscore the promise of deep learning algorithms in mitigating interference,offering a strong foundation for future research.</description><author>Alejandro Lancho, Amir Weiss, Gary C. F. Lee, Tejas Jayashankar, Binoy Kurien, Yury Polyanskiy, Gregory W. Wornell</author><pubDate>Fri, 13 Sep 2024 13:53:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08839v1</guid></item><item><title>Can Kans (re)discover predictive models for Direct-Drive Laser Fusion?</title><link>http://arxiv.org/abs/2409.08832v1</link><description>The domain of laser fusion presents a unique and challenging predictivemodeling application landscape for machine learning methods due to high problemcomplexity and limited training data. Data-driven approaches utilizingprescribed functional forms, inductive biases and physics-informed learning(PIL) schemes have been successful in the past for achieving desiredgeneralization ability and model interpretation that aligns with physicsexpectations. In complex multi-physics application domains, however, it is notalways obvious how architectural biases or discriminative penalties can beformulated. In this work, focusing on nuclear fusion energy using high poweredlasers, we present the use of Kolmogorov-Arnold Networks (KANs) as analternative to PIL for developing a new type of data-driven predictive modelwhich is able to achieve high prediction accuracy and physics interpretability.A KAN based model, a MLP with PIL, and a baseline MLP model are compared ingeneralization ability and interpretation with a domain expert-derived symbolicregression model. Through empirical studies in this high physics complexitydomain, we show that KANs can potentially provide benefits when developingpredictive models for data-starved physics applications.</description><author>Rahman Ejaz, Varchas Gopalaswamy, Riccardo Betti, Aarne Lees, Christopher Kanan</author><pubDate>Fri, 13 Sep 2024 13:48:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08832v1</guid></item><item><title>Breaking reCAPTCHAv2</title><link>http://arxiv.org/abs/2409.08831v1</link><description>Our work examines the efficacy of employing advanced machine learning methodsto solve captchas from Google's reCAPTCHAv2 system. We evaluate theeffectiveness of automated systems in solving captchas by utilizing advancedYOLO models for image segmentation and classification. Our main result is thatwe can solve 100% of the captchas, while previous work only solved 68-71%.Furthermore, our findings suggest that there is no significant difference inthe number of challenges humans and bots must solve to pass the captchas inreCAPTCHAv2. This implies that current AI technologies can exploit advancedimage-based captchas. We also look under the hood of reCAPTCHAv2, and findevidence that reCAPTCHAv2 is heavily based on cookie and browser history datawhen evaluating whether a user is human or not. The code is provided alongsidethis paper.</description><author>Andreas Plesner, Tobias Vontobel, Roger Wattenhofer</author><pubDate>Fri, 13 Sep 2024 13:47:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08831v1</guid></item><item><title>MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth Estimation of Endoscopic Images</title><link>http://arxiv.org/abs/2404.16571v3</link><description>Photometric constraint is indispensable for self-supervised monocular depthestimation. It involves warping a source image onto a target view usingestimated depth&amp;pose, and then minimizing the difference between the warped andtarget images. However, the endoscopic built-in light causes significantbrightness fluctuations, and thus makes the photometric constraint unreliable.Previous efforts only mitigate this relying on extra models to calibrate imagebrightness. In this paper, we propose MonoPCC to address the brightnessinconsistency radically by reshaping the photometric constraint into a cycleform. Instead of only warping the source image, MonoPCC constructs a closedloop consisting of two opposite forward-backward warping paths: from target tosource and then back to target. Thus, the target image finally receives animage cycle-warped from itself, which naturally makes the constraint invariantto brightness changes. Moreover, MonoPCC transplants the source image'sphase-frequency into the intermediate warped image to avoid structure lost, andalso stabilizes the training via an exponential moving average (EMA) strategyto avoid frequent changes in the forward warping. The comprehensive andextensive experimental results on four endoscopic datasets demonstrate that ourproposed MonoPCC shows a great robustness to the brightness inconsistency, andexceeds other state-of-the-arts by reducing the absolute relative error by atleast 7.27%, 9.38%, 9.90% and 3.17%, respectively.</description><author>Zhiwei Wang, Ying Zhou, Shiquan He, Ting Li, Fan Huang, Qiang Ding, Xinxia Feng, Mei Liu, Qiang Li</author><pubDate>Fri, 13 Sep 2024 13:40:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16571v3</guid></item><item><title>UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with Ultrasound Reflection Direction Parameterization</title><link>http://arxiv.org/abs/2408.00860v3</link><description>Three-dimensional ultrasound imaging is a critical technology widely used inmedical diagnostics. However, traditional 3D ultrasound imaging methods havelimitations such as fixed resolution, low storage efficiency, and insufficientcontextual connectivity, leading to poor performance in handling complexartifacts and reflection characteristics. Recently, techniques based on NeRF(Neural Radiance Fields) have made significant progress in view synthesis and3D reconstruction, but there remains a research gap in high-quality ultrasoundimaging. To address these issues, we propose a new model, UlRe-NeRF, whichcombines implicit neural networks and explicit ultrasound volume rendering intoan ultrasound neural rendering architecture. This model incorporates reflectiondirection parameterization and harmonic encoding, using a directional MLPmodule to generate view-dependent high-frequency reflection intensityestimates, and a spatial MLP module to produce the medium's physical propertyparameters. These parameters are used in the volume rendering process toaccurately reproduce the propagation and reflection behavior of ultrasoundwaves in the medium. Experimental results demonstrate that the UlRe-NeRF modelsignificantly enhances the realism and accuracy of high-fidelity ultrasoundimage reconstruction, especially in handling complex medium structures.</description><author>Ziwen Guo, Zi Fang, Zhuang Fu</author><pubDate>Fri, 13 Sep 2024 13:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00860v3</guid></item><item><title>Four Facets of Forecast Felicity: Calibration, Predictiveness, Randomness and Regret</title><link>http://arxiv.org/abs/2401.14483v2</link><description>Machine learning is about forecasting. Forecasts, however, obtain theirusefulness only through their evaluation. Machine learning has traditionallyfocused on types of losses and their corresponding regret. Currently, themachine learning community regained interest in calibration. In this work, weshow the conceptual equivalence of calibration and regret in evaluatingforecasts. We frame the evaluation problem as a game between a forecaster, agambler and nature. Putting intuitive restrictions on gambler and forecaster,calibration and regret naturally fall out of the framework. In addition, thisgame links evaluation of forecasts to randomness of outcomes. Random outcomeswith respect to forecasts are equivalent to good forecasts with respect tooutcomes. We call those dual aspects, calibration and regret, predictivenessand randomness, the four facets of forecast felicity.</description><author>Rabanus Derr, Robert C. Williamson</author><pubDate>Fri, 13 Sep 2024 13:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14483v2</guid></item><item><title>A Bayesian framework for active object recognition, pose estimation and shape transfer learning through touch</title><link>http://arxiv.org/abs/2409.06912v2</link><description>As humans can explore and understand the world through the sense of touch,tactile sensing is also an important aspect of robotic perception. Inunstructured environments, robots can encounter both known and novel objects,this calls for a method to address both known and novel objects. In this study,we combine a particle filter (PF) and Gaussian process implicit surface (GPIS)in a unified Bayesian framework. The framework can differentiate between knownand novel objects, perform object recognition, estimate pose for known objects,and reconstruct shapes for unknown objects, in an active learning fashion. Bygrounding the selection of the GPIS prior with themaximum-likelihood-estimation (MLE) shape from the PF, the knowledge aboutknown objects' shapes can be transferred to learn novel shapes. An explorationprocedure with global shape estimation is proposed to guide active dataacquisition and conclude the exploration when sufficient information isobtained. The performance of the proposed Bayesian framework is evaluatedthrough simulations on known and novel objects, initialized with random poses.The results show that the proposed exploration procedure, utilizing globalshape estimation, achieves faster exploration than a local explorationprocedure based on rapidly explore random tree (RRT). Overall, our resultsindicate that the proposed framework is effective and efficient in objectrecognition, pose estimation and shape reconstruction. Moreover, we show that alearned shape can be included as a new prior and used effectively for futureobject recognition and pose estimation.</description><author>Haodong Zheng, Andrei Jalba, Raymond H. Cuijpers, Wijnand IJsselsteijn, Sanne Schoenmakers</author><pubDate>Fri, 13 Sep 2024 13:38:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06912v2</guid></item><item><title>Pathfinder for Low-altitude Aircraft with Binary Neural Network</title><link>http://arxiv.org/abs/2409.08824v1</link><description>A prior global topological map (e.g., the OpenStreetMap, OSM) can boost theperformance of autonomous mapping by a ground mobile robot. However, the priormap is usually incomplete due to lacking labeling in partial paths. To solvethis problem, this paper proposes an OSM maker using airborne sensors carriedby low-altitude aircraft, where the core of the OSM maker is a novel efficientpathfinder approach based on LiDAR and camera data, i.e., a binary dual-streamroad segmentation model. Specifically, a multi-scale feature extraction basedon the UNet architecture is implemented for images and point clouds. To reducethe effect caused by the sparsity of point cloud, an attention-guided gatedblock is designed to integrate image and point-cloud features. For enhancingthe efficiency of the model, we propose a binarization streamline to each modelcomponent, including a variant of vision transformer (ViT) architecture as theencoder of the image branch, and new focal and perception losses to optimizethe model training. The experimental results on two datasets demonstrate thatour pathfinder method achieves SOTA accuracy with high efficiency in findingpaths from the low-level airborne sensors, and we can create complete OSM priormaps based on the segmented road skeletons. Code and data are availableat:https://github.com/IMRL/Pathfinder}{https://github.com/IMRL/Pathfinder.</description><author>Kaijie Yin, Tian Gao, Hui Kong</author><pubDate>Fri, 13 Sep 2024 13:37:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08824v1</guid></item><item><title>AutoIRT: Calibrating Item Response Theory Models with Automated Machine Learning</title><link>http://arxiv.org/abs/2409.08823v1</link><description>Item response theory (IRT) is a class of interpretable factor models that arewidely used in computerized adaptive tests (CATs), such as language proficiencytests. Traditionally, these are fit using parametric mixed effects models onthe probability of a test taker getting the correct answer to a test item(i.e., question). Neural net extensions of these models, such as BertIRT,require specialized architectures and parameter tuning. We propose a multistagefitting procedure that is compatible with out-of-the-box Automated MachineLearning (AutoML) tools. It is based on a Monte Carlo EM (MCEM) outer loop witha two stage inner loop, which trains a non-parametric AutoML grade model usingitem features followed by an item specific parametric model. This greatlyaccelerates the modeling workflow for scoring tests. We demonstrate itseffectiveness by applying it to the Duolingo English Test, a high stakes,online English proficiency test. We show that the resulting model is typicallymore well calibrated, gets better predictive performance, and more accuratescores than existing methods (non-explanatory IRT models and explanatory IRTmodels like BERT-IRT). Along the way, we provide a brief survey of machinelearning methods for calibration of item parameters for CATs.</description><author>James Sharpnack, Phoebe Mulcaire, Klinton Bicknell, Geoff LaFlair, Kevin Yancey</author><pubDate>Fri, 13 Sep 2024 13:36:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08823v1</guid></item><item><title>A call for embodied AI</title><link>http://arxiv.org/abs/2402.03824v4</link><description>We propose Embodied AI as the next fundamental step in the pursuit ofArtificial General Intelligence, juxtaposing it against current AIadvancements, particularly Large Language Models. We traverse the evolution ofthe embodiment concept across diverse fields - philosophy, psychology,neuroscience, and robotics - to highlight how EAI distinguishes itself from theclassical paradigm of static learning. By broadening the scope of Embodied AI,we introduce a theoretical framework based on cognitive architectures,emphasizing perception, action, memory, and learning as essential components ofan embodied agent. This framework is aligned with Friston's active inferenceprinciple, offering a comprehensive approach to EAI development. Despite theprogress made in the field of AI, substantial challenges, such as theformulation of a novel AI learning theory and the innovation of advancedhardware, persist. Our discussion lays down a foundational guideline for futureEmbodied AI research. Highlighting the importance of creating Embodied AIagents capable of seamless communication, collaboration, and coexistence withhumans and other intelligent entities within real-world environments, we aim tosteer the AI community towards addressing the multifaceted challenges andseizing the opportunities that lie ahead in the quest for AGI.</description><author>Giuseppe Paolo, Jonas Gonzalez-Billandon, Balázs Kégl</author><pubDate>Fri, 13 Sep 2024 13:36:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03824v4</guid></item><item><title>A RAG Approach for Generating Competency Questions in Ontology Engineering</title><link>http://arxiv.org/abs/2409.08820v1</link><description>Competency question (CQ) formulation is central to several ontologydevelopment and evaluation methodologies. Traditionally, the task of craftingthese competency questions heavily relies on the effort of domain experts andknowledge engineers which is often time-consuming and labor-intensive. With theemergence of Large Language Models (LLMs), there arises the possibility toautomate and enhance this process. Unlike other similar works which useexisting ontologies or knowledge graphs as input to LLMs, we present aretrieval-augmented generation (RAG) approach that uses LLMs for the automaticgeneration of CQs given a set of scientific papers considered to be a domainknowledge base. We investigate its performance and specifically, we study theimpact of different number of papers to the RAG and different temperaturesetting of the LLM. We conduct experiments using GPT-4 on two domain ontologyengineering tasks and compare results against ground-truth CQs constructed bydomain experts. Empirical assessments on the results, utilizing evaluationmetrics (precision and consistency), reveal that compared to zero-shotprompting, adding relevant domain knowledge to the RAG improves the performanceof LLMs on generating CQs for concrete ontology engineering tasks.</description><author>Xueli Pan, Jacco van Ossenbruggen, Victor de Boer, Zhisheng Huang</author><pubDate>Fri, 13 Sep 2024 13:34:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08820v1</guid></item><item><title>Deep reinforcement learning for tracking a moving target in jellyfish-like swimming</title><link>http://arxiv.org/abs/2409.08815v1</link><description>We develop a deep reinforcement learning method for training a jellyfish-likeswimmer to effectively track a moving target in a two-dimensional flow. Thisswimmer is a flexible object equipped with a muscle model based on torsionalsprings. We employ a deep Q-network (DQN) that takes the swimmer's geometry anddynamic parameters as inputs, and outputs actions which are the forces appliedto the swimmer. In particular, we introduce an action regulation to mitigatethe interference from complex fluid-structure interactions. The goal of theseactions is to navigate the swimmer to a target point in the shortest possibletime. In the DQN training, the data on the swimmer's motions are obtained fromsimulations conducted using the immersed boundary method. During tracking amoving target, there is an inherent delay between the application of forces andthe corresponding response of the swimmer's body due to hydrodynamicinteractions between the shedding vortices and the swimmer's own locomotion.Our tests demonstrate that the swimmer, with the DQN agent and actionregulation, is able to dynamically adjust its course based on its instantaneousstate. This work extends the application scope of machine learning incontrolling flexible objects within fluid environments.</description><author>Yihao Chen, Yue Yang</author><pubDate>Fri, 13 Sep 2024 13:29:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08815v1</guid></item><item><title>Event Detection in Time Series: Universal Deep Learning Approach</title><link>http://arxiv.org/abs/2311.15654v3</link><description>Event detection in time series is a challenging task due to the prevalence ofimbalanced datasets, rare events, and time interval-defined events. Traditionalsupervised deep learning methods primarily employ binary classification, whereeach time step is assigned a binary label indicating the presence or absence ofan event. However, these methods struggle to handle these specific scenarioseffectively. To address these limitations, we propose a novel supervisedregression-based deep learning approach that offers several advantages overclassification-based methods. Our approach, with a limited number ofparameters, can effectively handle various types of events within a unifiedframework, including rare events and imbalanced datasets. We providetheoretical justifications for its universality and precision and demonstrateits superior performance across diverse domains, particularly for rare eventsand imbalanced datasets.</description><author>Menouar Azib, Benjamin Renard, Philippe Garnier, Vincent Génot, Nicolas André</author><pubDate>Fri, 13 Sep 2024 13:28:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15654v3</guid></item><item><title>Revisiting Convergence of AdaGrad with Relaxed Assumptions</title><link>http://arxiv.org/abs/2402.13794v2</link><description>In this study, we revisit the convergence of AdaGrad with momentum (coveringAdaGrad as a special case) on non-convex smooth optimization problems. Weconsider a general noise model where the noise magnitude is controlled by thefunction value gap together with the gradient magnitude. This model encompassesa broad range of noises including bounded noise, sub-Gaussian noise, affinevariance noise and the expected smoothness, and it has been shown to be morerealistic in many practical applications. Our analysis yields a probabilisticconvergence rate which, under the general noise, could reach at(\tilde{\mathcal{O}}(1/\sqrt{T})). This rate does not rely on prior knowledgeof problem-parameters and could accelerate to (\tilde{\mathcal{O}}(1/T)) where(T) denotes the total number iterations, when the noise parameters related tothe function value gap and noise level are sufficiently small. The convergencerate thus matches the lower rate for stochastic first-order methods overnon-convex smooth landscape up to logarithm terms [Arjevani et al., 2023]. Wefurther derive a convergence bound for AdaGrad with mometum, considering thegeneralized smoothness where the local smoothness is controlled by afirst-order function of the gradient norm.</description><author>Yusu Hong, Junhong Lin</author><pubDate>Fri, 13 Sep 2024 13:28:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13794v2</guid></item><item><title>Algorithmic Decision-Making under Agents with Persistent Improvement</title><link>http://arxiv.org/abs/2405.01807v3</link><description>This paper studies algorithmic decision-making under human's strategicbehavior, where a decision maker uses an algorithm to make decisions abouthuman agents, and the latter with information about the algorithm may exerteffort strategically and improve to receive favorable decisions. Unlike priorworks that assume agents benefit from their efforts immediately, we considerrealistic scenarios where the impacts of these efforts are persistent andagents benefit from efforts by making improvements gradually. We first developa dynamic model to characterize persistent improvements and based on thisconstruct a Stackelberg game to model the interplay between agents and thedecision-maker. We analytically characterize the equilibrium strategies andidentify conditions under which agents have incentives to improve. With thedynamics, we then study how the decision-maker can design an optimal policy toincentivize the largest improvements inside the agent population. We alsoextend the model to settings where 1) agents may be dishonest and game thealgorithm into making favorable but erroneous decisions; 2) honest efforts areforgettable and not sufficient to guarantee persistent improvements. With theextended models, we further examine conditions under which agents prefer honestefforts over dishonest behavior and the impacts of forgettable efforts.</description><author>Tian Xie, Xuwei Tan, Xueru Zhang</author><pubDate>Fri, 13 Sep 2024 13:25:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01807v3</guid></item><item><title>Your Weak LLM is Secretly a Strong Teacher for Alignment</title><link>http://arxiv.org/abs/2409.08813v1</link><description>The burgeoning capabilities of large language models (LLMs) have underscoredthe need for alignment to ensure these models act in accordance with humanvalues and intentions. Existing alignment frameworks present constraints eitherin the form of expensive human effort or high computational costs. This paperexplores a promising middle ground, where we employ a weak LLM that issignificantly less resource-intensive than top-tier models, yet offers moreautomation than purely human feedback. We present a systematic study toevaluate and understand weak LLM's ability to generate feedback for alignment.Our empirical findings demonstrate that weak LLMs can provide feedback thatrivals or even exceeds that of fully human-annotated data. Our study indicatesa minimized impact of model size on feedback efficacy, shedding light on ascalable and sustainable alignment strategy. To deepen our understanding ofalignment under weak LLM feedback, we conduct a series of qualitative andquantitative analyses, offering novel insights into the quality discrepanciesbetween human feedback vs. weak LLM feedback.</description><author>Leitian Tao, Yixuan Li</author><pubDate>Fri, 13 Sep 2024 13:24:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08813v1</guid></item><item><title>Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task</title><link>http://arxiv.org/abs/2409.08811v1</link><description>Theory of Mind (ToM) significantly impacts human collaboration andcommunication as a crucial capability to understand others. When AI agents withToM capability collaborate with humans, Mutual Theory of Mind (MToM) arises insuch human-AI teams (HATs). The MToM process, which involves interactivecommunication and ToM-based strategy adjustment, affects the team's performanceand collaboration process. To explore the MToM process, we conducted amixed-design experiment using a large language model-driven AI agent with ToMand communication modules in a real-time shared-workspace task. We find thatthe agent's ToM capability does not significantly impact team performance butenhances human understanding of the agent and the feeling of being understood.Most participants in our study believe verbal communication increases humanburden, and the results show that bidirectional communication leads to lowerHAT performance. We discuss the results' implications for designing AI agentsthat collaborate with humans in real-time shared workspace tasks.</description><author>Shao Zhang, Xihuai Wang, Wenhao Zhang, Yongshan Chen, Landi Gao, Dakuo Wang, Weinan Zhang, Xinbing Wang, Ying Wen</author><pubDate>Fri, 13 Sep 2024 13:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08811v1</guid></item><item><title>TabKANet: Tabular Data Modelling with Kolmogorov-Arnold Network and Transformer</title><link>http://arxiv.org/abs/2409.08806v1</link><description>Tabular data is the most common type of data in real-life scenarios. In thisstudy, we propose a method based on the TabKANet architecture, which utilizesthe Kolmogorov-Arnold network to encode numerical features and merge them withcategorical features, enabling unified modeling of tabular data on theTransformer architecture. This model demonstrates outstanding performance insix widely used binary classification tasks, suggesting that TabKANet has thepotential to become a standard approach for tabular modeling, surpassingtraditional neural networks. Furthermore, this research reveals the significantadvantages of the Kolmogorov-Arnold network in encoding numerical features. Thecode of our work is available at https://github.com/tsinghuamedgao20/TabKANet.</description><author>Weihao Gao, Zheng Gong, Zhuo Deng, Fuju Rong, Chucheng Chen, Lan Ma</author><pubDate>Fri, 13 Sep 2024 13:14:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08806v1</guid></item><item><title>Exploring SSL Discrete Tokens for Multilingual ASR</title><link>http://arxiv.org/abs/2409.08805v1</link><description>With the advancement of Self-supervised Learning (SSL) in speech-relatedtasks, there has been growing interest in utilizing discrete tokens generatedby SSL for automatic speech recognition (ASR), as they offer faster processingtechniques. However, previous studies primarily focused on multilingual ASRwith Fbank features or English ASR with discrete tokens, leaving a gap inadapting discrete tokens for multilingual ASR scenarios. This study presents acomprehensive comparison of discrete tokens generated by various leading SSLmodels across multiple language domains. We aim to explore the performance andefficiency of speech discrete tokens across multiple language domains for bothmonolingual and multilingual ASR scenarios. Experimental results demonstratethat discrete tokens achieve comparable results against systems trained onFbank features in ASR tasks across seven language domains with an average worderror rate (WER) reduction of 0.31% and 1.76% absolute (2.80% and 15.70%relative) on dev and test sets respectively, with particularly WER reduction of6.82% absolute (41.48% relative) on the Polish test set.</description><author>Mingyu Cui, Daxin Tan, Yifan Yang, Dingdong Wang, Huimeng Wang, Xiao Chen, Xie Chen, Xunying Liu</author><pubDate>Fri, 13 Sep 2024 13:13:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08805v1</guid></item><item><title>Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks</title><link>http://arxiv.org/abs/2309.12927v2</link><description>Recurrent neural networks (RNNs) in the brain and in silico excel at solvingtasks with intricate temporal dependencies. Long timescales required forsolving such tasks can arise from properties of individual neurons(single-neuron timescale, $\tau$, e.g., membrane time constant in biologicalneurons) or recurrent interactions among them (network-mediated timescale).However, the contribution of each mechanism for optimally solvingmemory-dependent tasks remains poorly understood. Here, we train RNNs to solve$N$-parity and $N$-delayed match-to-sample tasks with increasing memoryrequirements controlled by $N$ by simultaneously optimizing recurrent weightsand $\tau$s. We find that for both tasks RNNs develop longer timescales withincreasing $N$, but depending on the learning objective, they use differentmechanisms. Two distinct curricula define learning objectives: sequentiallearning of a single-$N$ (single-head) or simultaneous learning of multiple$N$s (multi-head). Single-head networks increase their $\tau$ with $N$ and areable to solve tasks for large $N$, but they suffer from catastrophicforgetting. However, multi-head networks, which are explicitly required to holdmultiple concurrent memories, keep $\tau$ constant and develop longertimescales through recurrent connectivity. Moreover, we show that themulti-head curriculum increases training speed and network stability toablations and perturbations, and allows RNNs to generalize better to tasksbeyond their training regime. This curriculum also significantly improvestraining GRUs and LSTMs for large-$N$ tasks. Our results suggest that adaptingtimescales to task requirements via recurrent interactions allows learning morecomplex objectives and improves the RNN's performance.</description><author>Sina Khajehabdollahi, Roxana Zeraati, Emmanouil Giannakakis, Tim Jakob Schäfer, Georg Martius, Anna Levina</author><pubDate>Fri, 13 Sep 2024 13:12:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12927v2</guid></item><item><title>Finite Sample Analysis of Distribution-Free Confidence Ellipsoids for Linear Regression</title><link>http://arxiv.org/abs/2409.08801v1</link><description>The least squares (LS) estimate is the archetypical solution of linearregression problems. The asymptotic Gaussianity of the scaled LS error is oftenused to construct approximate confidence ellipsoids around the LS estimate,however, for finite samples these ellipsoids do not come with strictguarantees, unless some strong assumptions are made on the noise distributions.The paper studies the distribution-free Sign-Perturbed Sums (SPS) ellipsoidalouter approximation (EOA) algorithm which can construct non-asymptoticallyguaranteed confidence ellipsoids under mild assumptions, such as independentand symmetric noise terms. These ellipsoids have the same center andorientation as the classical asymptotic ellipsoids, only their radii aredifferent, which radii can be computed by convex optimization. Here, weestablish high probability non-asymptotic upper bounds for the sizes of SPSouter ellipsoids for linear regression problems and show that the volumes ofthese ellipsoids decrease at the optimal rate. Finally, the difference betweenour theoretical bounds and the empirical sizes of the regions are investigatedexperimentally.</description><author>Szabolcs Szentpéteri, Balázs Csanád Csáji</author><pubDate>Fri, 13 Sep 2024 13:10:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08801v1</guid></item><item><title>Operationalizing Contextual Integrity in Privacy-Conscious Assistants</title><link>http://arxiv.org/abs/2408.02373v2</link><description>Advanced AI assistants combine frontier LLMs and tool access to autonomouslyperform complex tasks on behalf of users. While the helpfulness of suchassistants can increase dramatically with access to user information includingemails and documents, this raises privacy concerns about assistants sharinginappropriate information with third parties without user supervision. To steerinformation-sharing assistants to behave in accordance with privacyexpectations, we propose to operationalize contextual integrity (CI), aframework that equates privacy with the appropriate flow of information in agiven context. In particular, we design and evaluate a number of strategies tosteer assistants' information-sharing actions to be CI compliant. Ourevaluation is based on a novel form filling benchmark composed of humanannotations of common webform applications, and it reveals that promptingfrontier LLMs to perform CI-based reasoning yields strong results.</description><author>Sahra Ghalebikesabi, Eugene Bagdasaryan, Ren Yi, Itay Yona, Ilia Shumailov, Aneesh Pappu, Chongyang Shi, Laura Weidinger, Robert Stanforth, Leonard Berrada, Pushmeet Kohli, Po-Sen Huang, Borja Balle</author><pubDate>Fri, 13 Sep 2024 13:09:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02373v2</guid></item><item><title>Task-Specific Data Preparation for Deep Learning to Reconstruct Structures of Interest from Severely Truncated CBCT Data</title><link>http://arxiv.org/abs/2409.08800v1</link><description>Cone-beam computed tomography (CBCT) is widely used in interventionalsurgeries and radiation oncology. Due to the limited size of flat-paneldetectors, anatomical structures might be missing outside the limitedfield-of-view (FOV), which restricts the clinical applications of CBCT systems.Recently, deep learning methods have been proposed to extend the FOV formulti-slice CT systems. However, in mobile CBCT system with a smaller FOV size,projection data is severely truncated and it is challenging for a network torestore all missing structures outside the FOV. In some applications, onlycertain structures outside the FOV are of interest, e.g., ribs in needle pathplanning for liver/lung cancer diagnosis. Therefore, a task-specific datapreparation method is proposed in this work, which automatically let thenetwork focus on structures of interest instead of all the structures. Ourpreliminary experiment shows that Pix2pixGAN with a conventional training hasthe risk to reconstruct false positive and false negative rib structures fromseverely truncated CBCT data, whereas Pix2pixGAN with the proposedtask-specific training can reconstruct all the ribs reliably. The proposedmethod is promising to empower CBCT with more clinical applications.</description><author>Yixing Huang, Fuxin Fan, Ahmed Gomaa, Andreas Maier, Rainer Fietkau, Christoph Bert, Florian Putz</author><pubDate>Fri, 13 Sep 2024 13:08:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08800v1</guid></item><item><title>Reading ability detection using eye-tracking data with LSTM-based few-shot learning</title><link>http://arxiv.org/abs/2409.08798v1</link><description>Reading ability detection is important in modern educational field. In thispaper, a method of predicting scores of reading ability is proposed, using theeye-tracking data of a few subjects (e.g., 68 subjects). The proposed methodbuilt a regression model for the score prediction by combining Long Short TimeMemory (LSTM) and light-weighted neural networks. Experiments show that withfew-shot learning strategy, the proposed method achieved higher accuracy thanprevious methods of score prediction in reading ability detection. The code canlater be downloaded athttps://github.com/pumpkinLNX/LSTM-eye-tracking-pytorch.git</description><author>Nanxi Li, Hongjiang Wang, Zehui Zhan</author><pubDate>Fri, 13 Sep 2024 13:06:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08798v1</guid></item><item><title>Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR</title><link>http://arxiv.org/abs/2409.08797v1</link><description>Self-supervised learning (SSL) based discrete speech representations arehighly compact and domain adaptable. In this paper, SSL discrete speechfeatures extracted from WavLM models are used as additional cross-utteranceacoustic context features in Zipformer-Transducer ASR systems. The efficacy ofreplacing Fbank features with discrete token features for modelling eithercross-utterance contexts (from preceding and future segments), or currentutterance's internal contexts alone, or both at the same time, are demonstratedthoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducersystem using discrete tokens based cross-utterance context features outperformsthe baseline using utterance internal context only with statisticallysignificant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78%to 3.54% relative) on the dev and test data. The lowest published WER of 11.15%and 11.14% were obtained on the dev and test sets. Our work is open-source andpublicly available athttps://github.com/open-creator/icefall/tree/master/egs/gigaspeech/Context\_ASR.</description><author>Mingyu Cui, Yifan Yang, Jiajun Deng, Jiawen Kang, Shujie Hu, Tianzi Wang, Zhaoqing Li, Shiliang Zhang, Xie Chen, Xunying Liu</author><pubDate>Fri, 13 Sep 2024 13:01:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08797v1</guid></item><item><title>Optimizing Ingredient Substitution Using Large Language Models to Enhance Phytochemical Content in Recipes</title><link>http://arxiv.org/abs/2409.08792v1</link><description>In the emerging field of computational gastronomy, aligning culinarypractices with scientifically supported nutritional goals is increasinglyimportant. This study explores how large language models (LLMs) can be appliedto optimize ingredient substitutions in recipes, specifically to enhance thephytochemical content of meals. Phytochemicals are bioactive compounds found inplants, which, based on preclinical studies, may offer potential healthbenefits. We fine-tuned models, including OpenAI's GPT-3.5, DaVinci, and Meta'sTinyLlama, using an ingredient substitution dataset. These models were used topredict substitutions that enhance phytochemical content and create acorresponding enriched recipe dataset. Our approach improved Hit@1 accuracy oningredient substitution tasks, from the baseline 34.53 plus-minus 0.10% to38.03 plus-minus 0.28% on the original GISMo dataset, and from 40.24 plus-minus0.36% to 54.46 plus-minus 0.29% on a refined version of the same dataset. Thesesubstitutions led to the creation of 1,951 phytochemically enriched ingredientpairings and 1,639 unique recipes. While this approach demonstrates potentialin optimizing ingredient substitutions, caution must be taken when drawingconclusions about health benefits, as the claims are based on preclinicalevidence. Future work should include clinical validation and broader datasetsto further evaluate the nutritional impact of these substitutions. Thisresearch represents a step forward in using AI to promote healthier eatingpractices, providing potential pathways for integrating computational methodswith nutritional science.</description><author>Luis Rita, Josh Southern, Ivan Laponogov, Kyle Higgins, Kirill Veselkov</author><pubDate>Fri, 13 Sep 2024 12:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08792v1</guid></item><item><title>Electrocardiogram Report Generation and Question Answering via Retrieval-Augmented Self-Supervised Modeling</title><link>http://arxiv.org/abs/2409.08788v1</link><description>Interpreting electrocardiograms (ECGs) and generating comprehensive reportsremain challenging tasks in cardiology, often requiring specialized expertiseand significant time investment. To address these critical issues, we proposeECG-ReGen, a retrieval-based approach for ECG-to-text report generation andquestion answering. Our method leverages a self-supervised learning for the ECGencoder, enabling efficient similarity searches and report retrieval. Bycombining pre-training with dynamic retrieval and Large Language Model(LLM)-based refinement, ECG-ReGen effectively analyzes ECG data and answersrelated queries, with the potential of improving patient care. Experimentsconducted on the PTB-XL and MIMIC-IV-ECG datasets demonstrate superiorperformance in both in-domain and cross-domain scenarios for report generation.Furthermore, our approach exhibits competitive performance on ECG-QA datasetcompared to fully supervised methods when utilizing off-the-shelf LLMs forzero-shot question answering. This approach, effectively combiningself-supervised encoder and LLMs, offers a scalable and efficient solution foraccurate ECG interpretation, holding significant potential to enhance clinicaldecision-making.</description><author>Jialu Tang, Tong Xia, Yuan Lu, Cecilia Mascolo, Aaqib Saeed</author><pubDate>Fri, 13 Sep 2024 12:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08788v1</guid></item><item><title>Deep Learning-based Codes for Wiretap Fading Channels</title><link>http://arxiv.org/abs/2409.08786v1</link><description>The wiretap channel is a well-studied problem in the physical layer security(PLS) literature. Although it is proven that the decoding error probability andinformation leakage can be made arbitrarily small in the asymptotic regime,further research on finite-blocklength codes is required on the path towardspractical, secure communications systems. This work provides the firstexperimental characterization of a deep learning-based, finite-blocklength codeconstruction for multi-tap fading wiretap channels without channel stateinformation (CSI). In addition to the evaluation of the average probability oferror and information leakage, we illustrate the influence of (i) the number offading taps, (ii) differing variances of the fading coefficients and (iii) theseed selection for the hash function-based security layer.</description><author>Daniel Seifert, Onur Günlü, Rafael F. Schaefer</author><pubDate>Fri, 13 Sep 2024 12:45:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08786v1</guid></item></channel></rss>