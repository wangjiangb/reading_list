<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 06 Dec 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Yo'LLaVA: Your Personalized Language and Vision Assistant</title><link>http://arxiv.org/abs/2406.09400v2</link><description>Large Multimodal Models (LMMs) have shown remarkable capabilities across avariety of tasks (e.g., image captioning, visual question answering). Whilebroad, their knowledge remains generic (e.g., recognizing a dog), and they areunable to handle personalized subjects (e.g., recognizing a user's pet dog).Human reasoning, in contrast, typically operates within the context of specificsubjects in our surroundings. For example, one might ask, "What should I buyfor my dog's birthday?"; as opposed to a generic inquiry about "What should Ibuy for a dog's birthday?". Similarly, when looking at a friend's image, theinterest lies in seeing their activities (e.g., "my friend is holding a cat"),rather than merely observing generic human actions (e.g., "a man is holding acat"). In this paper, we introduce the novel task of personalizing LMMs, sothat they can have conversations about a specific subject. We propose Yo'LLaVA,which learns to embed a personalized subject into a set of latent tokens givena handful of example images of the subject. Our qualitative and quantitativeanalyses reveal that Yo'LLaVA can learn the concept more efficiently usingfewer tokens and more effectively encode the visual attributes compared tostrong prompting baselines (e.g., LLaVA).</description><author>Thao Nguyen, Haotian Liu, Yuheng Li, Mu Cai, Utkarsh Ojha, Yong Jae Lee</author><pubDate>Wed, 04 Dec 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09400v2</guid></item><item><title>Navigation World Models</title><link>http://arxiv.org/abs/2412.03572v1</link><description>Navigation is a fundamental skill of agents with visual-motor capabilities.We introduce a Navigation World Model (NWM), a controllable video generationmodel that predicts future visual observations based on past observations andnavigation actions. To capture complex environment dynamics, NWM employs aConditional Diffusion Transformer (CDiT), trained on a diverse collection ofegocentric videos of both human and robotic agents, and scaled up to 1 billionparameters. In familiar environments, NWM can plan navigation trajectories bysimulating them and evaluating whether they achieve the desired goal. Unlikesupervised navigation policies with fixed behavior, NWM can dynamicallyincorporate constraints during planning. Experiments demonstrate itseffectiveness in planning trajectories from scratch or by ranking trajectoriessampled from an external policy. Furthermore, NWM leverages its learned visualpriors to imagine trajectories in unfamiliar environments from a single inputimage, making it a flexible and powerful tool for next-generation navigationsystems.</description><author>Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, Yann LeCun</author><pubDate>Wed, 04 Dec 2024 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03572v1</guid></item><item><title>Style3D: Attention-guided Multi-view Style Transfer for 3D Object Generation</title><link>http://arxiv.org/abs/2412.03571v1</link><description>We present Style3D, a novel approach for generating stylized 3D objects froma content image and a style image. Unlike most previous methods that requirecase- or style-specific training, Style3D supports instant 3D objectstylization. Our key insight is that 3D object stylization can be decomposedinto two interconnected processes: multi-view dual-feature alignment andsparse-view spatial reconstruction. We introduce MultiFusion Attention, anattention-guided technique to achieve multi-view stylization from thecontent-style pair. Specifically, the query features from the content imagepreserve geometric consistency across multiple views, while the key and valuefeatures from the style image are used to guide the stylistic transfer. Thisdual-feature alignment ensures that spatial coherence and stylistic fidelityare maintained across multi-view images. Finally, a large 3D reconstructionmodel is introduced to generate coherent stylized 3D objects. By establishingan interplay between structural and stylistic features across multiple views,our approach enables a holistic 3D stylization process. Extensive experimentsdemonstrate that Style3D offers a more flexible and scalable solution forgenerating style-consistent 3D assets, surpassing existing methods in bothcomputational efficiency and visual quality.</description><author>Bingjie Song, Xin Huang, Ruting Xie, Xue Wang, Qing Wang</author><pubDate>Wed, 04 Dec 2024 18:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03571v1</guid></item><item><title>Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis</title><link>http://arxiv.org/abs/2412.03570v1</link><description>Inferring the 3D structure underlying a set of multi-view images typicallyrequires solving two co-dependent tasks -- accurate 3D reconstruction requiresprecise camera poses, and predicting camera poses relies on (implicitly orexplicitly) modeling the underlying 3D. The classical framework of analysis bysynthesis casts this inference as a joint optimization seeking to explain theobserved pixels, and recent instantiations learn expressive 3D representations(e.g., Neural Fields) with gradient-descent-based pose refinement of initialpose estimates. However, given a sparse set of observed views, the observationsmay not provide sufficient direct evidence to obtain complete and accurate 3D.Moreover, large errors in pose estimation may not be easily corrected and canfurther degrade the inferred 3D. To allow robust 3D reconstruction and poseestimation in this challenging setup, we propose SparseAGS, a method thatadapts this analysis-by-synthesis approach by: a) includingnovel-view-synthesis-based generative priors in conjunction with photometricobjectives to improve the quality of the inferred 3D, and b) explicitlyreasoning about outliers and using a discrete search with a continuousoptimization-based strategy to correct them. We validate our framework acrossreal-world and synthetic datasets in combination with several off-the-shelfpose estimation systems as initialization. We find that it significantlyimproves the base systems' pose accuracy while yielding high-quality 3Dreconstructions that outperform the results from current multi-viewreconstruction baselines.</description><author>Qitao Zhao, Shubham Tulsiani</author><pubDate>Wed, 04 Dec 2024 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03570v1</guid></item><item><title>The Matrix: Infinite-Horizon World Generation with Real-Time Moving Control</title><link>http://arxiv.org/abs/2412.03568v1</link><description>We present The Matrix, the first foundational realistic world simulatorcapable of generating continuous 720p high-fidelity real-scene video streamswith real-time, responsive control in both first- and third-personperspectives, enabling immersive exploration of richly dynamic environments.Trained on limited supervised data from AAA games like Forza Horizon 5 andCyberpunk 2077, complemented by large-scale unsupervised footage fromreal-world settings like Tokyo streets, The Matrix allows users to traversediverse terrains -- deserts, grasslands, water bodies, and urban landscapes --in continuous, uncut hour-long sequences. Operating at 16 FPS, the systemsupports real-time interactivity and demonstrates zero-shot generalization,translating virtual game environments to real-world contexts where collectingcontinuous movement data is often infeasible. For example, The Matrix cansimulate a BMW X3 driving through an office setting--an environment present inneither gaming data nor real-world sources. This approach showcases thepotential of AAA game data to advance robust world models, bridging the gapbetween simulations and real-world applications in scenarios with limited data.</description><author>Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, Hongyang Zhang</author><pubDate>Wed, 04 Dec 2024 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03568v1</guid></item><item><title>Streaming Detection of Queried Event Start</title><link>http://arxiv.org/abs/2412.03567v1</link><description>Robotics, autonomous driving, augmented reality, and many embodied computervision applications must quickly react to user-defined events unfolding in realtime. We address this setting by proposing a novel task for multimodal videounderstanding-Streaming Detection of Queried Event Start (SDQES). The goal ofSDQES is to identify the beginning of a complex event as described by a naturallanguage query, with high accuracy and low latency. We introduce a newbenchmark based on the Ego4D dataset, as well as new task-specific metrics tostudy streaming multimodal detection of diverse events in an egocentric videosetting. Inspired by parameter-efficient fine-tuning methods in NLP and forvideo tasks, we propose adapter-based baselines that enable image-to-videotransfer learning, allowing for efficient online video modeling. We evaluatethree vision-language backbones and three adapter architectures on bothshort-clip and untrimmed video settings.</description><author>Cristobal Eyzaguirre, Eric Tang, Shyamal Buch, Adrien Gaidon, Jiajun Wu, Juan Carlos Niebles</author><pubDate>Wed, 04 Dec 2024 18:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03567v1</guid></item><item><title>FreeSim: Toward Free-viewpoint Camera Simulation in Driving Scenes</title><link>http://arxiv.org/abs/2412.03566v1</link><description>We propose FreeSim, a camera simulation method for autonomous driving.FreeSim emphasizes high-quality rendering from viewpoints beyond the recordedego trajectories. In such viewpoints, previous methods have unacceptabledegradation because the training data of these viewpoints is unavailable. Toaddress such data scarcity, we first propose a generative enhancement modelwith a matched data construction strategy. The resulting model can generatehigh-quality images in a viewpoint slightly deviated from the recordedtrajectories, conditioned on the degraded rendering of this viewpoint. We thenpropose a progressive reconstruction strategy, which progressively addsgenerated images of unrecorded views into the reconstruction process, startingfrom slightly off-trajectory viewpoints and moving progressively farther away.With this progressive generation-reconstruction pipeline, FreeSim supportshigh-quality off-trajectory view synthesis under large deviations of more than3 meters.</description><author>Lue Fan, Hao Zhang, Qitai Wang, Hongsheng Li, Zhaoxiang Zhang</author><pubDate>Wed, 04 Dec 2024 18:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03566v1</guid></item><item><title>Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning</title><link>http://arxiv.org/abs/2412.03565v1</link><description>Large Multimodal Models (LMMs) have made significant breakthroughs with theadvancement of instruction tuning. However, while existing models canunderstand images and videos at a holistic level, they still struggle withinstance-level understanding that requires a more nuanced comprehension andalignment. Instance-level understanding is crucial, as it focuses on thespecific elements that we are most interested in. Excitingly, existing worksfind that the state-of-the-art LMMs exhibit strong instance understandingcapabilities when provided with explicit visual cues. Motivated by this, weintroduce an automated annotation pipeline assisted by GPT-4o to extractinstance-level information from images and videos through explicit visualprompting for instance guidance. Building upon this pipeline, we proposedInst-IT, a solution to enhance LMMs in Instance understanding via explicitvisual prompt Instruction Tuning. Inst-IT consists of a benchmark to diagnosemultimodal instance-level understanding, a large-scale instruction-tuningdataset, and a continuous instruction-tuning training paradigm to effectivelyenhance spatial-temporal instance understanding capabilities of existing LMMs.Experimental results show that, with the boost of Inst-IT, our models not onlyachieve outstanding performance on Inst-IT Bench but also demonstratesignificant improvements across various generic image and video understandingbenchmarks. This highlights that our dataset not only boosts instance-levelunderstanding but also strengthens the overall capabilities of generic imageand video comprehension.</description><author>Wujian Peng, Lingchen Meng, Yitong Chen, Yiweng Xie, Yang Liu, Tao Gui, Hang Xu, Xipeng Qiu, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Wed, 04 Dec 2024 18:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03565v1</guid></item><item><title>From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents</title><link>http://arxiv.org/abs/2412.03563v1</link><description>Traditional sociological research often relies on human participation, which,though effective, is expensive, challenging to scale, and with ethicalconcerns. Recent advancements in large language models (LLMs) highlight theirpotential to simulate human behavior, enabling the replication of individualresponses and facilitating studies on many interdisciplinary studies. In thispaper, we conduct a comprehensive survey of this field, illustrating the recentprogress in simulation driven by LLM-empowered agents. We categorize thesimulations into three types: (1) Individual Simulation, which mimics specificindividuals or demographic groups; (2) Scenario Simulation, where multipleagents collaborate to achieve goals within specific contexts; and (3) SocietySimulation, which models interactions within agent societies to reflect thecomplexity and variety of real-world dynamics. These simulations follow aprogression, ranging from detailed individual modeling to large-scale societalphenomena. We provide a detailed discussion of each simulation type, includingthe architecture or key components of the simulation, the classification ofobjectives or scenarios and the evaluation method. Afterward, we summarizecommonly used datasets and benchmarks. Finally, we discuss the trends acrossthese three types of simulation. A repository for the related sources is at{\url{https://github.com/FudanDISC/SocialAgent}}.</description><author>Xinyi Mou, Xuanwen Ding, Qi He, Liang Wang, Jingcong Liang, Xinnong Zhang, Libo Sun, Jiayu Lin, Jie Zhou, Xuanjing Huang, Zhongyu Wei</author><pubDate>Wed, 04 Dec 2024 18:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03563v1</guid></item><item><title>FLAIR: VLM with Fine-grained Language-informed Image Representations</title><link>http://arxiv.org/abs/2412.03561v1</link><description>CLIP has shown impressive results in aligning images and texts at scale.However, its ability to capture detailed visual features remains limitedbecause CLIP matches images and texts at a global level. To address this issue,we propose FLAIR, Fine-grained Language-informed Image Representations, anapproach that utilizes long and detailed image descriptions to learn localizedimage embeddings. By sampling diverse sub-captions that describe fine-graineddetails about an image, we train our vision-language model to produce not onlyglobal embeddings but also text-specific image representations. Our modelintroduces text-conditioned attention pooling on top of local image tokens toproduce fine-grained image representations that excel at retrieving detailedimage content. We achieve state-of-the-art performance on both, existingmultimodal retrieval benchmarks, as well as, our newly introduced fine-grainedretrieval task which evaluates vision-language models' ability to retrievepartial image content. Furthermore, our experiments demonstrate theeffectiveness of FLAIR trained on 30M image-text pairs in capturingfine-grained visual information, including zero-shot semantic segmentation,outperforming models trained on billions of pairs. Code is available athttps://github.com/ExplainableML/flair .</description><author>Rui Xiao, Sanghwan Kim, Mariana-Iuliana Georgescu, Zeynep Akata, Stephan Alaniz</author><pubDate>Wed, 04 Dec 2024 18:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03561v1</guid></item><item><title>MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation</title><link>http://arxiv.org/abs/2412.03558v1</link><description>This paper introduces MIDI, a novel paradigm for compositional 3D scenegeneration from a single image. Unlike existing methods that rely onreconstruction or retrieval techniques or recent approaches that employmulti-stage object-by-object generation, MIDI extends pre-trained image-to-3Dobject generation models to multi-instance diffusion models, enabling thesimultaneous generation of multiple 3D instances with accurate spatialrelationships and high generalizability. At its core, MIDI incorporates a novelmulti-instance attention mechanism, that effectively captures inter-objectinteractions and spatial coherence directly within the generation process,without the need for complex multi-step processes. The method utilizes partialobject images and global scene context as inputs, directly modeling objectcompletion during 3D generation. During training, we effectively supervise theinteractions between 3D instances using a limited amount of scene-level data,while incorporating single-object data for regularization, thereby maintainingthe pre-trained generalization ability. MIDI demonstrates state-of-the-artperformance in image-to-scene generation, validated through evaluations onsynthetic data, real-world scene data, and stylized scene images generated bytext-to-image diffusion models.</description><author>Zehuan Huang, Yuan-Chen Guo, Xingqiao An, Yunhan Yang, Yangguang Li, Zi-Xin Zou, Ding Liang, Xihui Liu, Yan-Pei Cao, Lu Sheng</author><pubDate>Wed, 04 Dec 2024 18:52:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03558v1</guid></item><item><title>Best-of-N Jailbreaking</title><link>http://arxiv.org/abs/2412.03556v1</link><description>We introduce Best-of-N (BoN) Jailbreaking, a simple black-box algorithm thatjailbreaks frontier AI systems across modalities. BoN Jailbreaking works byrepeatedly sampling variations of a prompt with a combination of augmentations- such as random shuffling or capitalization for textual prompts - until aharmful response is elicited. We find that BoN Jailbreaking achieves highattack success rates (ASRs) on closed-source language models, such as 89% onGPT-4o and 78% on Claude 3.5 Sonnet when sampling 10,000 augmented prompts.Further, it is similarly effective at circumventing state-of-the-artopen-source defenses like circuit breakers. BoN also seamlessly extends toother modalities: it jailbreaks vision language models (VLMs) such as GPT-4oand audio language models (ALMs) like Gemini 1.5 Pro, using modality-specificaugmentations. BoN reliably improves when we sample more augmented prompts.Across all modalities, ASR, as a function of the number of samples (N),empirically follows power-law-like behavior for many orders of magnitude. BoNJailbreaking can also be composed with other black-box algorithms for even moreeffective attacks - combining BoN with an optimized prefix attack achieves upto a 35% increase in ASR. Overall, our work indicates that, despite theircapability, language models are sensitive to seemingly innocuous changes toinputs, which attackers can exploit across modalities.</description><author>John Hughes, Sara Price, Aengus Lynch, Rylan Schaeffer, Fazl Barez, Sanmi Koyejo, Henry Sleight, Erik Jones, Ethan Perez, Mrinank Sharma</author><pubDate>Wed, 04 Dec 2024 18:51:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03556v1</guid></item><item><title>PaliGemma 2: A Family of Versatile VLMs for Transfer</title><link>http://arxiv.org/abs/2412.03555v1</link><description>PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM)based on the Gemma 2 family of language models. We combine the SigLIP-So400mvision encoder that was also used by PaliGemma with the whole range of Gemma 2models, from the 2B one all the way up to the 27B model. We train these modelsat three resolutions (224px, 448px, and 896px) in multiple stages to equip themwith broad knowledge for transfer via fine-tuning. The resulting family of basemodels covering different model sizes and resolutions allows us to investigatefactors impacting transfer performance (such as learning rate) and to analyzethe interplay between the type of task, model size, and resolution. We furtherincrease the number and breadth of transfer tasks beyond the scope of PaliGemmaincluding different OCR-related tasks such as table structure recognition,molecular structure recognition, music score recognition, as well as longfine-grained captioning and radiography report generation, on which PaliGemma 2obtains state-of-the-art results.</description><author>Andreas Steiner, André Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, Siyang Qin, Reeve Ingle, Emanuele Bugliarello, Sahar Kazemzadeh, Thomas Mesnard, Ibrahim Alabdulmohsin, Lucas Beyer, Xiaohua Zhai</author><pubDate>Wed, 04 Dec 2024 18:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03555v1</guid></item><item><title>Imagine360: Immersive 360 Video Generation from Perspective Anchor</title><link>http://arxiv.org/abs/2412.03552v1</link><description>$360^\circ$ videos offer a hyper-immersive experience that allows the viewersto explore a dynamic scene from full 360 degrees. To achieve more user-friendlyand personalized content creation in $360^\circ$ video format, we seek to liftstandard perspective videos into $360^\circ$ equirectangular videos. To thisend, we introduce Imagine360, the first perspective-to-$360^\circ$ videogeneration framework that creates high-quality $360^\circ$ videos with rich anddiverse motion patterns from video anchors. Imagine360 learns fine-grainedspherical visual and motion patterns from limited $360^\circ$ video data withseveral key designs. 1) Firstly we adopt the dual-branch design, including aperspective and a panorama video denoising branch to provide local and globalconstraints for $360^\circ$ video generation, with motion module and spatialLoRA layers fine-tuned on extended web $360^\circ$ videos. 2) Additionally, anantipodal mask is devised to capture long-range motion dependencies, enhancingthe reversed camera motion between antipodal pixels across hemispheres. 3) Tohandle diverse perspective video inputs, we propose elevation-aware designsthat adapt to varying video masking due to changing elevations across frames.Extensive experiments show Imagine360 achieves superior graphics quality andmotion coherence among state-of-the-art $360^\circ$ video generation methods.We believe Imagine360 holds promise for advancing personalized, immersive$360^\circ$ video creation.</description><author>Jing Tan, Shuai Yang, Tong Wu, Jingwen He, Yuwei Guo, Ziwei Liu, Dahua Lin</author><pubDate>Wed, 04 Dec 2024 18:50:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03552v1</guid></item><item><title>DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning</title><link>http://arxiv.org/abs/2402.15957v2</link><description>We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach toapproximate inference in environments where the latent state evolves at varyingrates. We model episode sessions - parts of the episode where the latent stateis fixed - and propose three key modifications to existing meta-RL methods:consistency of latent information within sessions, session masking, and priorlatent conditioning. We demonstrate the importance of these modifications invarious domains, ranging from discrete Gridworld environments tocontinuous-control and simulated robot assistive tasks, demonstrating thatDynaMITE-RL significantly outperforms state-of-the-art baselines in sampleefficiency and inference returns.</description><author>Anthony Liang, Guy Tennenholtz, Chih-wei Hsu, Yinlam Chow, Erdem Bıyık, Craig Boutilier</author><pubDate>Wed, 04 Dec 2024 18:48:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15957v2</guid></item><item><title>Fast and reliable uncertainty quantification with neural network ensembles for industrial image classification</title><link>http://arxiv.org/abs/2403.10182v3</link><description>Image classification with neural networks (NNs) is widely used in industrialprocesses, situations where the model likely encounters unknown objects duringdeployment, i.e., out-of-distribution (OOD) data. Worryingly, NNs tend to makeconfident yet incorrect predictions when confronted with OOD data. To increasethe models' reliability, they should quantify the uncertainty in their ownpredictions, communicating when the output should (not) be trusted. Deepensembles, composed of multiple independent NNs, have been shown to performstrongly but are computationally expensive. Recent research has proposed moreefficient NN ensembles, namely the snapshot, batch, and multi-inputmulti-output ensemble. This study investigates the predictive and uncertaintyperformance of efficient NN ensembles in the context of image classificationfor industrial processes. It is the first to provide a comprehensive comparisonand it proposes a novel Diversity Quality metric to quantify the ensembles'performance on the in-distribution and OOD sets in one single metric. Theresults highlight the batch ensemble as a cost-effective and competitivealternative to the deep ensemble. It matches the deep ensemble in bothuncertainty and accuracy while exhibiting considerable savings in trainingtime, test time, and memory storage.</description><author>Arthur Thuy, Dries F. Benoit</author><pubDate>Wed, 04 Dec 2024 18:48:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10182v3</guid></item><item><title>Perception Tokens Enhance Visual Reasoning in Multimodal Language Models</title><link>http://arxiv.org/abs/2412.03548v1</link><description>Multimodal language models (MLMs) still face challenges in fundamental visualperception tasks where specialized models excel. Tasks requiring reasoningabout 3D structures benefit from depth estimation, and reasoning about 2Dobject instances benefits from object detection. Yet, MLMs can not produceintermediate depth or boxes to reason over. Finetuning MLMs on relevant datadoesn't generalize well and outsourcing computation to specialized vision toolsis too compute-intensive and memory-inefficient. To address this, we introducePerception Tokens, intrinsic image representations designed to assist reasoningtasks where language is insufficient. Perception tokens act as auxiliaryreasoning tokens, akin to chain-of-thought prompts in language models. Forexample, in a depth-related task, an MLM augmented with perception tokens canreason by generating a depth map as tokens, enabling it to solve the problemeffectively. We propose AURORA, a training method that augments MLMs withperception tokens for improved reasoning over visual inputs. AURORA leverages aVQVAE to transform intermediate image representations, such as depth maps intoa tokenized format and bounding box tokens, which is then used in a multi-tasktraining framework. AURORA achieves notable improvements across countingbenchmarks: +10.8% on BLINK, +11.3% on CVBench, and +8.3% on SEED-Bench,outperforming finetuning approaches in generalization across datasets. It alsoimproves on relative depth: over +6% on BLINK. With perception tokens, AURORAexpands the scope of MLMs beyond language-based reasoning, paving the way formore effective visual reasoning capabilities.</description><author>Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda G. Shapiro, Ranjay Krishna</author><pubDate>Wed, 04 Dec 2024 18:45:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03548v1</guid></item><item><title>Marconi: Prefix Caching for the Era of Hybrid LLMs</title><link>http://arxiv.org/abs/2411.19379v2</link><description>Hybrid models that combine the language modeling capabilities of Attentionlayers with the efficiency of Recurrent layers (e.g., State Space Models) havegained traction in practically supporting long contexts in Large Language Modelserving. Yet, the unique properties of these models complicate the usage ofcomplementary efficiency optimizations such as prefix caching that skipredundant computations across requests. Most notably, their use of in-placestate updates for recurrent layers precludes rolling back cache entries forpartial sequence overlaps, and instead mandates only exact-match cache hits;the effect is a deluge of (large) cache entries per sequence, most of whichyield minimal reuse opportunities. We present Marconi, the first system thatsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are itsnovel admission and eviction policies that more judiciously assess potentialcache entries based not only on recency, but also on (1) forecasts of theirreuse likelihood across a taxonomy of different hit scenarios, and (2) thecompute savings that hits deliver relative to memory footprints. Across diverseworkloads and Hybrid models, Marconi achieves up to 34.4$\times$ higher tokenhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefixcaching systems.</description><author>Rui Pan, Zhuang Wang, Zhen Jia, Can Karakus, Luca Zancato, Tri Dao, Yida Wang, Ravi Netravali</author><pubDate>Wed, 04 Dec 2024 18:40:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19379v2</guid></item><item><title>NODE-AdvGAN: Improving the transferability and perceptual similarity of adversarial examples by dynamic-system-driven adversarial generative model</title><link>http://arxiv.org/abs/2412.03539v1</link><description>Understanding adversarial examples is crucial for improving the model'srobustness, as they introduce imperceptible perturbations that deceive models.Effective adversarial examples, therefore, offer the potential to train morerobust models by removing their singularities. We propose NODE-AdvGAN, a novelapproach that treats adversarial generation as a continuous process and employsa Neural Ordinary Differential Equation (NODE) for simulating the dynamics ofthe generator. By mimicking the iterative nature of traditional gradient-basedmethods, NODE-AdvGAN generates smoother and more precise perturbations thatpreserve high perceptual similarity when added to benign images. We alsopropose a new training strategy, NODE-AdvGAN-T, which enhances transferabilityin black-box attacks by effectively tuning noise parameters during training.Experiments demonstrate that NODE-AdvGAN and NODE-AdvGAN-T generate moreeffective adversarial examples that achieve higher attack success rates whilepreserving better perceptual quality than traditional GAN-based methods.</description><author>Xinheng Xie, Yue Wu, Cuiyu He</author><pubDate>Wed, 04 Dec 2024 18:36:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03539v1</guid></item><item><title>VoxNeRF: Bridging Voxel Representation and Neural Radiance Fields for Enhanced Indoor View Synthesis</title><link>http://arxiv.org/abs/2311.05289v2</link><description>The generation of high-fidelity view synthesis is essential for roboticnavigation and interaction but remains challenging, particularly in indoorenvironments and real-time scenarios. Existing techniques often requiresignificant computational resources for both training and rendering, and theyfrequently result in suboptimal 3D representations due to insufficientgeometric structuring. To address these limitations, we introduce VoxNeRF, anovel approach that utilizes easy-to-obtain geometry priors to enhance both thequality and efficiency of neural indoor reconstruction and novel viewsynthesis. We propose an efficient voxel-guided sampling technique thatallocates computational resources selectively to the most relevant segments ofrays based on a voxel-encoded geometry prior, significantly reducing trainingand rendering time. Additionally, we incorporate a robust depth loss to improvereconstruction and rendering quality in sparse view settings. Our approach isvalidated with extensive experiments on ScanNet and ScanNet++ where VoxNeRFoutperforms existing state-of-the-art methods and establishes a new benchmarkfor indoor immersive interpolation and extrapolation settings.</description><author>Sen Wang, Qing Cheng, Stefano Gasperini, Wei Zhang, Shun-Cheng Wu, Niclas Zeller, Daniel Cremers, Nassir Navab</author><pubDate>Wed, 04 Dec 2024 18:32:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05289v2</guid></item><item><title>Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted Language Models</title><link>http://arxiv.org/abs/2412.03537v1</link><description>Large language models (LLMs) are increasingly being adapted to achievetask-specificity for deployment in real-world decision systems. Severalprevious works have investigated the bias transfer hypothesis (BTH) by studyingthe effect of the fine-tuning adaptation strategy on model fairness to findthat fairness in pre-trained masked language models have limited effect on thefairness of models when adapted using fine-tuning. In this work, we expand thestudy of BTH to causal models under prompt adaptations, as prompting is anaccessible, and compute-efficient way to deploy models in real-world systems.In contrast to previous works, we establish that intrinsic biases inpre-trained Mistral, Falcon and Llama models are strongly correlated (rho &gt;=0.94) with biases when the same models are zero- and few-shot prompted, using apronoun co-reference resolution task. Further, we find that bias transferremains strongly correlated even when LLMs are specifically prompted to exhibitfair or biased behavior (rho &gt;= 0.92), and few-shot length and stereotypicalcomposition are varied (rho &gt;= 0.97). Our findings highlight the importance ofensuring fairness in pre-trained LLMs, especially when they are later used toperform downstream tasks via prompt adaptation.</description><author>Natalie Mackraz, Nivedha Sivakumar, Samira Khorshidi, Krishna Patel, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff</author><pubDate>Wed, 04 Dec 2024 18:32:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03537v1</guid></item><item><title>StarVector: Generating Scalable Vector Graphics Code from Images and Text</title><link>http://arxiv.org/abs/2312.11556v2</link><description>Scalable Vector Graphics (SVGs) are vital for modern image rendering due totheir scalability and versatility. Previous SVG generation methods have focusedon curve-based vectorization, lacking semantic understanding, often producingartifacts, and struggling with SVG primitives beyond path curves. To addressthese issues, we introduce StarVector, a multimodal large language model forSVG generation. It performs image vectorization by understanding imagesemantics and using SVG primitives for compact, precise outputs. Unliketraditional methods, StarVector works directly in the SVG code space,leveraging visual understanding to apply accurate SVG primitives. To trainStarVector, we create SVG-Stack, a diverse dataset of 2M samples that enablesgeneralization across vectorization tasks and precise use of primitives likeellipses, polygons, and text. We address challenges in SVG evaluation, showingthat pixel-based metrics like MSE fail to capture the unique qualities ofvector graphics. We introduce SVG-Bench, a benchmark across 10 datasets, and 3tasks: Image-to-SVG, Text-to-SVG generation, and diagram generation. Using thissetup, StarVector achieves state-of-the-art performance, producing more compactand semantically rich SVGs.</description><author>Juan A. Rodriguez, Abhay Puri, Shubham Agarwal, Issam H. Laradji, Pau Rodriguez, Sai Rajeswar, David Vazquez, Christopher Pal, Marco Pedersoli</author><pubDate>Wed, 04 Dec 2024 18:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11556v2</guid></item><item><title>A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences</title><link>http://arxiv.org/abs/2412.03531v1</link><description>The rapid advancement of large language models (LLMs) has opened newboundaries in the extraction and synthesis of medical knowledge, particularlywithin evidence synthesis. This paper reviews the state-of-the-art applicationsof LLMs in the biomedical domain, exploring their effectiveness in automatingcomplex tasks such as evidence synthesis and data extraction from a biomedicalcorpus of documents. While LLMs demonstrate remarkable potential, significantchallenges remain, including issues related to hallucinations, contextualunderstanding, and the ability to generalize across diverse medical tasks. Wehighlight critical gaps in the current research literature, particularly theneed for unified benchmarks to standardize evaluations and ensure reliabilityin real-world applications. In addition, we propose directions for futureresearch, emphasizing the integration of state-of-the-art techniques such asretrieval-augmented generation (RAG) to enhance LLM performance in evidencesynthesis. By addressing these challenges and utilizing the strengths of LLMs,we aim to improve access to medical literature and facilitate meaningfuldiscoveries in healthcare.</description><author>Gabriel Lino Garcia, João Renato Ribeiro Manesco, Pedro Henrique Paiola, Lucas Miranda, Maria Paola de Salvo, João Paulo Papa</author><pubDate>Wed, 04 Dec 2024 18:26:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03531v1</guid></item><item><title>Instance-Warp: Saliency Guided Image Warping for Unsupervised Domain Adaptation</title><link>http://arxiv.org/abs/2403.12712v3</link><description>Driving is challenging in conditions like night, rain, and snow. Lack of goodlabeled datasets has hampered progress in scene understanding under suchconditions. Unsupervised Domain Adaptation (UDA) using large labeled clear-daydatasets is a promising research direction in such cases. However, many UDAmethods are trained with dominant scene backgrounds (e.g., roads, sky,sidewalks) that appear dramatically different across domains. As a result, theystruggle to learn effective features of smaller and often sparse foregroundobjects (e.g., people, vehicles, signs). In this work, we improve UDA training by applying in-place image warping tofocus on salient objects. We design instance-level saliency guidance toadaptively oversample object regions and undersample background areas, whichreduces adverse effects from background context and enhances backbone featurelearning. Our approach improves adaptation across geographies, lighting, andweather conditions, and is agnostic to the task (segmentation, detection),domain adaptation algorithm, saliency guidance, and underlying modelarchitecture. Result highlights include +6.1 mAP50 for BDD100K Clear$\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\rightarrow$ Night, +3.0mAP50 for BDD100K Clear $\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes$\rightarrow$ ACDC. Besides, Our method adds minimal training memory and noadditional inference latency. Code is available athttps://github.com/ShenZheng2000/Instance-Warp</description><author>Shen Zheng, Anurag Ghosh, Srinivasa G. Narasimhan</author><pubDate>Wed, 04 Dec 2024 18:18:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12712v3</guid></item><item><title>FANAL -- Financial Activity News Alerting Language Modeling Framework</title><link>http://arxiv.org/abs/2412.03527v1</link><description>In the rapidly evolving financial sector, the accurate and timelyinterpretation of market news is essential for stakeholders needing to navigateunpredictable events. This paper introduces FANAL (Financial Activity NewsAlerting Language Modeling Framework), a specialized BERT-based frameworkengineered for real-time financial event detection and analysis, categorizingnews into twelve distinct financial categories. FANAL leverages silver-labeleddata processed through XGBoost and employs advanced fine-tuning techniques,alongside ORBERT (Odds Ratio BERT), a novel variant of BERT fine-tuned withORPO (Odds Ratio Preference Optimization) for superior class-wise probabilitycalibration and alignment with financial event relevance. We evaluate FANAL'sperformance against leading large language models, including GPT-4o, Llama-3.18B, and Phi-3, demonstrating its superior accuracy and cost efficiency. Thisframework sets a new standard for financial intelligence and responsiveness,significantly outstripping existing models in both performance andaffordability.</description><author>Urjitkumar Patel, Fang-Chun Yeh, Chinmay Gondhalekar, Hari Nalluri</author><pubDate>Wed, 04 Dec 2024 18:15:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03527v1</guid></item><item><title>Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos</title><link>http://arxiv.org/abs/2412.03526v1</link><description>Recent advancements in static feed-forward scene reconstruction havedemonstrated significant progress in high-quality novel view synthesis.However, these models often struggle with generalizability across diverseenvironments and fail to effectively handle dynamic content. We present BTimer(short for BulletTimer), the first motion-aware feed-forward model forreal-time reconstruction and novel view synthesis of dynamic scenes. Ourapproach reconstructs the full scene in a 3D Gaussian Splatting representationat a given target ('bullet') timestamp by aggregating information from all thecontext frames. Such a formulation allows BTimer to gain scalability andgeneralization by leveraging both static and dynamic scene datasets. Given acasual monocular dynamic video, BTimer reconstructs a bullet-time scene within150ms while reaching state-of-the-art performance on both static and dynamicscene datasets, even compared with optimization-based approaches.</description><author>Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang</author><pubDate>Wed, 04 Dec 2024 18:15:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03526v1</guid></item><item><title>Seeing Beyond Views: Multi-View Driving Scene Video Generation with Holistic Attention</title><link>http://arxiv.org/abs/2412.03520v1</link><description>Generating multi-view videos for autonomous driving training has recentlygained much attention, with the challenge of addressing both cross-view andcross-frame consistency. Existing methods typically apply decoupled attentionmechanisms for spatial, temporal, and view dimensions. However, theseapproaches often struggle to maintain consistency across dimensions,particularly when handling fast-moving objects that appear at different timesand viewpoints. In this paper, we present CogDriving, a novel network designedfor synthesizing high-quality multi-view driving videos. CogDriving leverages aDiffusion Transformer architecture with holistic-4D attention modules, enablingsimultaneous associations across the spatial, temporal, and viewpointdimensions. We also propose a lightweight controller tailored for CogDriving,i.e., Micro-Controller, which uses only 1.1% of the parameters of the standardControlNet, enabling precise control over Bird's-Eye-View layouts. To enhancethe generation of object instances crucial for autonomous driving, we propose are-weighted learning objective, dynamically adjusting the learning weights forobject instances during training. CogDriving demonstrates strong performance onthe nuScenes validation set, achieving an FVD score of 37.8, highlighting itsability to generate realistic driving videos. The project can be found athttps://luhannan.github.io/CogDrivingPage/.</description><author>Hannan Lu, Xiaohe Wu, Shudong Wang, Xiameng Qin, Xinyu Zhang, Junyu Han, Wangmeng Zuo, Ji Tao</author><pubDate>Wed, 04 Dec 2024 18:02:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03520v1</guid></item><item><title>Dense Scene Reconstruction from Light-Field Images Affected by Rolling Shutter</title><link>http://arxiv.org/abs/2412.03518v1</link><description>This paper presents a dense depth estimation approach from light-field (LF)images that is able to compensate for strong rolling shutter (RS) effects. Ourmethod estimates RS compensated views and dense RS compensated disparity maps.We present a two-stage method based on a 2D Gaussians Splatting that allows fora ``render and compare" strategy with a point cloud formulation. In the firststage, a subset of sub-aperture images is used to estimate an RS agnostic 3Dshape that is related to the scene target shape ``up to a motion". In thesecond stage, the deformation of the 3D shape is computed by estimating anadmissible camera motion. We demonstrate the effectiveness and advantages ofthis approach through several experiments conducted for different scenes andtypes of motions. Due to lack of suitable datasets for evaluation, we alsopresent a new carefully designed synthetic dataset of RS LF images. The sourcecode, trained models and dataset will be made publicly available at:https://github.com/ICB-Vision-AI/DenseRSLF</description><author>Hermes McGriff, Renato Martins, Nicolas Andreff, Cedric Demonceaux</author><pubDate>Wed, 04 Dec 2024 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03518v1</guid></item><item><title>Boosting Latent Diffusion with Flow Matching</title><link>http://arxiv.org/abs/2312.07360v3</link><description>Visual synthesis has recently seen significant leaps in performance, largelydue to breakthroughs in generative models. Diffusion models have been a keyenabler, as they excel in image diversity. However, this comes at the cost ofslow training and synthesis, which is only partially alleviated by latentdiffusion. To this end, flow matching is an appealing approach due to itscomplementary characteristics of faster training and inference but less diversesynthesis. We demonstrate that introducing flow matching between a frozendiffusion model and a convolutional decoder enables high-resolution imagesynthesis at reduced computational cost and model size. A small diffusion modelcan then effectively provide the necessary visual diversity, while flowmatching efficiently enhances resolution and detail by mapping the small to ahigh-dimensional latent space. These latents are then projected tohigh-resolution images by the subsequent convolutional decoder of the latentdiffusion approach. Combining the diversity of diffusion models, the efficiencyof flow matching, and the effectiveness of convolutional decoders,state-of-the-art high-resolution image synthesis is achieved at $1024^2$ pixelswith minimal computational cost. Further scaling up our method we can reachresolutions up to $2048^2$ pixels. Importantly, our approach is orthogonal torecent approximation and speed-up strategies for the underlying model, makingit easily integrable into the various diffusion model frameworks.</description><author>Johannes Schusterbauer, Ming Gui, Pingchuan Ma, Nick Stracke, Stefan A. Baumann, Vincent Tao Hu, Björn Ommer</author><pubDate>Wed, 04 Dec 2024 17:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07360v3</guid></item><item><title>NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images</title><link>http://arxiv.org/abs/2412.03517v1</link><description>Recent advancements in generative models have significantly improved novelview synthesis (NVS) from multi-view data. However, existing methods depend onexternal multi-view alignment processes, such as explicit pose estimation orpre-reconstruction, which limits their flexibility and accessibility,especially when alignment is unstable due to insufficient overlap or occlusionsbetween views. In this paper, we propose NVComposer, a novel approach thateliminates the need for explicit external alignment. NVComposer enables thegenerative model to implicitly infer spatial and geometric relationshipsbetween multiple conditional views by introducing two key components: 1) animage-pose dual-stream diffusion model that simultaneously generates targetnovel views and condition camera poses, and 2) a geometry-aware featurealignment module that distills geometric priors from dense stereo models duringtraining. Extensive experiments demonstrate that NVComposer achievesstate-of-the-art performance in generative multi-view NVS tasks, removing thereliance on external alignment and thus improving model accessibility. Ourapproach shows substantial improvements in synthesis quality as the number ofunposed input views increases, highlighting its potential for more flexible andaccessible generative NVS systems.</description><author>Lingen Li, Zhaoyang Zhang, Yaowei Li, Jiale Xu, Xiaoyu Li, Wenbo Hu, Weihao Cheng, Jinwei Gu, Tianfan Xue, Ying Shan</author><pubDate>Wed, 04 Dec 2024 17:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03517v1</guid></item><item><title>You're (Not) My Type -- Can LLMs Generate Feedback of Specific Types for Introductory Programming Tasks?</title><link>http://arxiv.org/abs/2412.03516v1</link><description>Background: Feedback as one of the most influential factors for learning hasbeen subject to a great body of research. It plays a key role in thedevelopment of educational technology systems and is traditionally rooted indeterministic feedback defined by experts and their experience. However, withthe rise of generative AI and especially Large Language Models (LLMs), weexpect feedback as part of learning systems to transform, especially for thecontext of programming. In the past, it was challenging to automate feedbackfor learners of programming. LLMs may create new possibilities to providericher, and more individual feedback than ever before. Objectives: This paper aims to generate specific types of feedback forintroductory programming tasks using LLMs. We revisit existing feedbacktaxonomies to capture the specifics of the generated feedback, such asrandomness, uncertainty, and degrees of variation. Methods: We iteratively designed prompts for the generation of specificfeedback types (as part of existing feedback taxonomies) in response toauthentic student programs. We then evaluated the generated output anddetermined to what extent it reflected certain feedback types. Results and Conclusion: The present work provides a better understanding ofdifferent feedback dimensions and characteristics. The results haveimplications for future feedback research with regard to, for example, feedbackeffects and learners' informational needs. It further provides a basis for thedevelopment of new tools and learning systems for novice programmers includingfeedback generated by AI.</description><author>Dominic Lohr, Hieke Keuning, Natalie Kiesler</author><pubDate>Wed, 04 Dec 2024 17:57:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03516v1</guid></item><item><title>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</title><link>http://arxiv.org/abs/2406.16860v2</link><description>We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with avision-centric approach. While stronger language models can enhance multimodalcapabilities, the design choices for vision components are often insufficientlyexplored and disconnected from visual representation learning research. Thisgap hinders accurate sensory grounding in real-world scenarios. Our study usesLLMs and visual instruction tuning as an interface to evaluate various visualrepresentations, offering new insights into different models and architectures-- self-supervised, strongly supervised, or combinations thereof -- based onexperiments with over 20 vision encoders. We critically examine existing MLLMbenchmarks, address the difficulties involved in consolidating and interpretingresults from various tasks, and introduce a new vision-centric benchmark,CV-Bench. To further improve visual grounding, we propose the Spatial VisionAggregator (SVA), a dynamic and spatially-aware connector that integrateshigh-resolution vision features with LLMs while reducing the number of tokens.Additionally, we discuss the curation of high-quality visual instruction-tuningdata from publicly available sources, emphasizing the importance of data sourcebalancing and distribution ratio. Collectively, Cambrian-1 not only achievesstate-of-the-art performance but also serves as a comprehensive, open cookbookfor instruction-tuned MLLMs. We provide model weights, code, supporting tools,datasets, and detailed instruction-tuning and evaluation recipes. We hope ourrelease will inspire and accelerate advancements in multimodal systems andvisual representation learning.</description><author>Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Ziteng Wang, Rob Fergus, Yann LeCun, Saining Xie</author><pubDate>Wed, 04 Dec 2024 17:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16860v2</guid></item><item><title>Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion</title><link>http://arxiv.org/abs/2412.03515v1</link><description>Diffusion models have been applied to 3D LiDAR scene completion due to theirstrong training stability and high completion quality. However, the slowsampling speed limits the practical application of diffusion-based scenecompletion models since autonomous vehicles require an efficient perception ofsurrounding environments. This paper proposes a novel distillation methodtailored for 3D LiDAR scene completion models, dubbed $\textbf{ScoreLiDAR}$,which achieves efficient yet high-quality scene completion. ScoreLiDAR enablesthe distilled model to sample in significantly fewer steps after distillation.To improve completion quality, we also introduce a novel $\textbf{StructuralLoss}$, which encourages the distilled model to capture the geometric structureof the 3D LiDAR scene. The loss contains a scene-wise term constraining theholistic structure and a point-wise term constraining the key landmark pointsand their relative configuration. Extensive experiments demonstrate thatScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37seconds per frame ($&gt;$5$\times$) on SemanticKITTI and achieves superiorperformance compared to state-of-the-art 3D LiDAR scene completion models. Ourcode is publicly available at https://github.com/happyw1nd/ScoreLiDAR.</description><author>Shengyuan Zhang, An Zhao, Ling Yang, Zejian Li, Chenye Meng, Haoran Xu, Tianrun Chen, AnYang Wei, Perry Pengyun GU, Lingyun Sun</author><pubDate>Wed, 04 Dec 2024 17:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03515v1</guid></item><item><title>Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models (Extended Version)</title><link>http://arxiv.org/abs/2407.08152v2</link><description>Deduplication is a vital preprocessing step that enhances machine learningmodel performance and saves training time and energy. However, enhancingfederated learning through deduplication poses challenges, especially regardingscalability and potential privacy violations if deduplication involves sharingall clients' data. In this paper, we address the problem of deduplication in afederated setup by introducing a pioneering protocol, EfficientPrivacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removesduplicates from multiple clients' datasets without compromising data privacy.EP-MPD is constructed in a modular fashion, utilizing two novel variants of thePrivate Set Intersection protocol. Our extensive experiments demonstrate thesignificant benefits of deduplication in federated learning of large languagemodels. For instance, we observe up to 19.62\% improvement in perplexity and upto 27.95\% reduction in running time while varying the duplication levelbetween 10\% and 30\%. EP-MPD effectively balances privacy and performance infederated learning, making it a valuable solution for large-scale applications.</description><author>Aydin Abadi, Vishnu Asutosh Dasu, Sumanta Sarkar</author><pubDate>Wed, 04 Dec 2024 17:56:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08152v2</guid></item><item><title>KKLIP: Knowledge Distillation Exploiting K-means Clustering for Language-Image Pre-Training</title><link>http://arxiv.org/abs/2412.03513v1</link><description>Recently, CLIP has emerged as a valuable model for aligning image and textinformation in multi-modal scenarios. However, researchers have observedlimitations in the ability of CLIP's text and image encoders to extractdetailed knowledge from caption-image pairs. In response, this paper introducesKKLIP, a novel approach designed to enhance the quality of CLIP byincorporating a new knowledge distillation (KD) method derived from Llama 2.Our method comprises three objectives: Text Embedding Distillation, ConceptLearning, and Contrastive Learning. Firstly, Text Embedding Distillationinvolves training the KKLIP text encoder to emulate the teacher model, Llama 2.Secondly, Concept Learning assigns a soft concept label to each caption-imagepair through offline k-means clustering of text information from Llama 2,allowing KKLIP to learn from these soft concept labels. Finally, ContrastiveLearning harmonizes text and image embeddings. Our experimental resultsdemonstrate that KKLIP enhances the quality of both text and image encoders.</description><author>Kuei-Chun Kao</author><pubDate>Wed, 04 Dec 2024 17:56:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03513v1</guid></item><item><title>Distillation of Diffusion Features for Semantic Correspondence</title><link>http://arxiv.org/abs/2412.03512v1</link><description>Semantic correspondence, the task of determining relationships betweendifferent parts of images, underpins various applications including 3Dreconstruction, image-to-image translation, object tracking, and visual placerecognition. Recent studies have begun to explore representations learned inlarge generative image models for semantic correspondence, demonstratingpromising results. Building on this progress, current state-of-the-art methodsrely on combining multiple large models, resulting in high computationaldemands and reduced efficiency. In this work, we address this challenge byproposing a more computationally efficient approach. We propose a novelknowledge distillation technique to overcome the problem of reduced efficiency.We show how to use two large vision foundation models and distill thecapabilities of these complementary models into one smaller model thatmaintains high accuracy at reduced computational cost. Furthermore, wedemonstrate that by incorporating 3D data, we are able to further improveperformance, without the need for human-annotated correspondences. Overall, ourempirical results demonstrate that our distilled model with 3D dataaugmentation achieves performance superior to current state-of-the-art methodswhile significantly reducing computational load and enhancing practicality forreal-world applications, such as semantic video correspondence. Our code andweights are publicly available on our project page.</description><author>Frank Fundel, Johannes Schusterbauer, Vincent Tao Hu, Björn Ommer</author><pubDate>Wed, 04 Dec 2024 17:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03512v1</guid></item><item><title>Self-test loss functions for learning weak-form operators and gradient flows</title><link>http://arxiv.org/abs/2412.03506v1</link><description>The construction of loss functions presents a major challenge in data-drivenmodeling involving weak-form operators in PDEs and gradient flows, particularlydue to the need to select test functions appropriately. We address thischallenge by introducing self-test loss functions, which employ test functionsthat depend on the unknown parameters, specifically for cases where theoperator depends linearly on the unknowns. The proposed self-test loss functionconserves energy for gradient flows and coincides with the expectedlog-likelihood ratio for stochastic differential equations. Importantly, it isquadratic, facilitating theoretical analysis of identifiability andwell-posedness of the inverse problem, while also leading to efficientparametric or nonparametric regression algorithms. It is computationallysimple, requiring only low-order derivatives or even being entirelyderivative-free, and numerical experiments demonstrate its robustness againstnoisy and discrete data.</description><author>Yuan Gao, Quanjun Lang, Fei Lu</author><pubDate>Wed, 04 Dec 2024 17:48:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03506v1</guid></item><item><title>Towards Time Series Reasoning with LLMs</title><link>http://arxiv.org/abs/2409.11376v2</link><description>Multi-modal large language models (MLLMs) have enabled numerous advances inunderstanding and reasoning in domains like vision, but we have not yet seenthis broad success for time-series. Although prior works on time-series MLLMshave shown promising performance in time-series forecasting, very few worksshow how an LLM could be used for time-series reasoning in natural language. Wepropose a novel multi-modal time-series LLM approach that learns generalizableinformation across various domains with powerful zero-shot performance. First,we train a lightweight time-series encoder on top of an LLM to directly extracttime-series information. Then, we fine-tune our model with chain-of-thoughtaugmented time-series tasks to encourage the model to generate reasoning paths.We show that our model learns a latent representation that reflects specifictime-series features (e.g. slope, frequency), as well as outperforming GPT-4oon a set of zero-shot reasoning tasks on a variety of domains.</description><author>Winnie Chow, Lauren Gardiner, Haraldur T. Hallgrímsson, Maxwell A. Xu, Shirley You Ren</author><pubDate>Wed, 04 Dec 2024 17:45:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11376v2</guid></item><item><title>Winners with Confidence: Discrete Argmin Inference with an Application to Model Selection</title><link>http://arxiv.org/abs/2408.02060v2</link><description>We study the problem of finding the index of the minimum value of a vectorfrom noisy observations. This problem is relevant in population/policycomparison, discrete maximum likelihood, and model selection. We develop anasymptotically normal test statistic, even in high-dimensional settings andwith potentially many ties in the population mean vector, by integratingconcepts and tools from cross-validation and differential privacy. The keytechnical ingredient is a central limit theorem for globally dependent data. Wealso propose practical ways to select the tuning parameter that adapts to thesignal landscape. Numerical experiments and data examples demonstrate theability of the proposed method to achieve a favorable bias-variance trade-offin practical scenarios.</description><author>Tianyu Zhang, Hao Lee, Jing Lei</author><pubDate>Wed, 04 Dec 2024 17:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02060v2</guid></item><item><title>A Bidirectional Siamese Recurrent Neural Network for Accurate Gait Recognition Using Body Landmarks</title><link>http://arxiv.org/abs/2412.03498v1</link><description>Gait recognition is a significant biometric technique for personidentification, particularly in scenarios where other physiological biometricsare impractical or ineffective. In this paper, we address the challengesassociated with gait recognition and present a novel approach to improve itsaccuracy and reliability. The proposed method leverages advanced techniques,including sequential gait landmarks obtained through the Mediapipe poseestimation model, Procrustes analysis for alignment, and a SiamesebiGRU-dualStack Neural Network architecture for capturing temporaldependencies. Extensive experiments were conducted on large-scale cross-viewdatasets to demonstrate the effectiveness of the approach, achieving highrecognition accuracy compared to other models. The model demonstratedaccuracies of 95.7%, 94.44%, 87.71%, and 86.6% on CASIA-B, SZU RGB-D, OU-MVLP,and Gait3D datasets respectively. The results highlight the potentialapplications of the proposed method in various practical domains, indicatingits significant contribution to the field of gait recognition.</description><author>Proma Hossain Progga, Md. Jobayer Rahman, Swapnil Biswas, Md. Shakil Ahmed, Arif Reza Anwary, Swakkhar Shatabda</author><pubDate>Wed, 04 Dec 2024 17:39:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03498v1</guid></item><item><title>Soft Checksums to Flag Untrustworthy Machine Learning Surrogate Predictions and Application to Atomic Physics Simulations</title><link>http://arxiv.org/abs/2412.03497v1</link><description>Trained neural networks (NN) are attractive as surrogate models to replacecostly calculations in physical simulations, but are often unknowingly appliedto states not adequately represented in the training dataset. We present thenovel technique of soft checksums for scientific machine learning, ageneral-purpose method to differentiate between trustworthy predictions withsmall errors on in-distribution (ID) data points, and untrustworthy predictionswith large errors on out-of-distribution (OOD) data points. By adding a checknode to the existing output layer, we train the model to learn the chosenchecksum function encoded within the NN predictions and show that violations ofthis function correlate with high prediction errors. As the checksum functiondepends only on the NN predictions, we can calculate the checksum error for anyprediction with a single forward pass, incurring negligible time and memorycosts. Additionally, we find that incorporating the checksum function into theloss function and exposing the NN to OOD data points during the trainingprocess improves separation between ID and OOD predictions. By applying softchecksums to a physically complex and high-dimensional non-local thermodynamicequilibrium atomic physics dataset, we show that a well-chosen thresholdchecksum error can effectively separate ID and OOD predictions.</description><author>Casey Lauer, Robert C. Blake, Jonathan B. Freund</author><pubDate>Wed, 04 Dec 2024 17:39:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03497v1</guid></item><item><title>Towards Size-Independent Generalization Bounds for Deep Operator Nets</title><link>http://arxiv.org/abs/2205.11359v3</link><description>In recent times machine learning methods have made significant advances inbecoming a useful tool for analyzing physical systems. A particularly activearea in this theme has been "physics-informed machine learning" which focuseson using neural nets for numerically solving differential equations. In thiswork, we aim to advance the theory of measuring out-of-sample error whiletraining DeepONets - which is among the most versatile ways to solve P.D.Esystems in one-shot. Firstly, for a class of DeepONets, we prove a bound ontheir Rademacher complexity which does not explicitly scale with the width ofthe nets involved. Secondly, we use this to show how the Huber loss can bechosen so that for these DeepONet classes generalization error bounds can beobtained that have no explicit dependence on the size of the nets. Theeffective capacity measure for DeepONets that we thus derive is also shown tocorrelate with the behavior of generalization error in experiments.</description><author>Pulkit Gopalani, Sayar Karmakar, Dibyakanti Kumar, Anirbit Mukherjee</author><pubDate>Wed, 04 Dec 2024 17:37:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.11359v3</guid></item><item><title>TRENDy: Temporal Regression of Effective Non-linear Dynamics</title><link>http://arxiv.org/abs/2412.03496v1</link><description>Spatiotemporal dynamics pervade the natural sciences, from the morphogendynamics underlying patterning in animal pigmentation to the protein wavescontrolling cell division. A central challenge lies in understanding howcontrollable parameters induce qualitative changes in system behavior calledbifurcations. This endeavor is made particularly difficult in realisticsettings where governing partial differential equations (PDEs) are unknown anddata is limited and noisy. To address this challenge, we propose TRENDy(Temporal Regression of Effective Nonlinear Dynamics), an equation-freeapproach to learning low-dimensional, predictive models of spatiotemporaldynamics. Following classical work in spatial coarse-graining, TRENDy firstmaps input data to a low-dimensional space of effective dynamics via a cascadeof multiscale filtering operations. Our key insight is the recognition thatthese effective dynamics can be fit by a neural ordinary differential equation(NODE) having the same parameter space as the input PDE. The precedingfiltering operations strongly regularize the phase space of the NODE, makingTRENDy significantly more robust to noise compared to existing methods. Wetrain TRENDy to predict the effective dynamics of synthetic and real datarepresenting dynamics from across the physical and life sciences. We thendemonstrate how our framework can automatically locate both Turing and Hopfbifurcations in unseen regions of parameter space. We finally apply our methodto the analysis of spatial patterning of the ocellated lizard throughdevelopment. We found that TRENDy's effective state not only accuratelypredicts spatial changes over time but also identifies distinct patternfeatures unique to different anatomical regions, highlighting the potentialinfluence of surface geometry on reaction-diffusion mechanisms and their rolein driving spatially varying pattern dynamics.</description><author>Matthew Ricci, Guy Pelc, Zoe Piran, Noa Moriel, Mor Nitzan</author><pubDate>Wed, 04 Dec 2024 17:36:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03496v1</guid></item><item><title>FCL-ViT: Task-Aware Attention Tuning for Continual Learning</title><link>http://arxiv.org/abs/2412.02509v2</link><description>Continual Learning (CL) involves adapting the prior Deep Neural Network (DNN)knowledge to new tasks, without forgetting the old ones. However, modern CLtechniques focus on provisioning memory capabilities to existing DNN modelsrather than designing new ones that are able to adapt according to the task athand. This paper presents the novel Feedback Continual Learning VisionTransformer (FCL-ViT) that uses a feedback mechanism to generate real-timedynamic attention features tailored to the current task. The FCL-ViT operatesin two Phases. In phase 1, the generic image features are produced anddetermine where the Transformer should attend on the current image. In phase 2,task-specific image features are generated that leverage dynamic attention. Tothis end, Tunable self-Attention Blocks (TABs) and Task Specific Blocks (TSBs)are introduced that operate in both phases and are responsible for tuning theTABs attention, respectively. The FCL-ViT surpasses state-of-the-artperformance on Continual Learning compared to benchmark methods, whileretaining a small number of trainable DNN parameters.</description><author>Anestis Kaimakamidis, Ioannis Pitas</author><pubDate>Wed, 04 Dec 2024 17:35:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02509v2</guid></item><item><title>SR+Codec: a Benchmark of Super-Resolution for Video Compression Bitrate Reduction</title><link>http://arxiv.org/abs/2305.04844v3</link><description>In recent years, there has been significant interest in Super-Resolution(SR), which focuses on generating a high-resolution image from a low-resolutioninput. Deep learning-based methods for super-resolution have been particularlypopular and have shown impressive results on various benchmarks. However,research indicates that these methods may not perform as well on stronglycompressed videos. We developed a super-resolution benchmark to analyze SR'scapacity to upscale compressed videos. Our dataset employed video codecs basedon five widely-used compression standards: H.264, H.265, H.266, AV1, and AVS3.We assessed 19 popular SR models using our benchmark and evaluated theirability to restore details and their susceptibility to compression artifacts.To get an accurate perceptual ranking of SR models, we conducted acrowd-sourced side-by-side comparison of their outputs. We found that some SRmodels, combined with compression, allow us to reduce the video bitrate withoutsignificant loss of quality. We also compared a range of image and videoquality metrics with subjective scores to evaluate their accuracy onsuper-resolved compressed videos. The benchmark is publicly available athttps://videoprocessing.ai/benchmarks/super-resolution-for-video-compression.html</description><author>Evgeney Bogatyrev, Ivan Molodetskikh, Dmitriy Vatolin</author><pubDate>Wed, 04 Dec 2024 17:34:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04844v3</guid></item><item><title>Beyond algorithm hyperparameters: on preprocessing hyperparameters and associated pitfalls in machine learning applications</title><link>http://arxiv.org/abs/2412.03491v1</link><description>Adequately generating and evaluating prediction models based on supervisedmachine learning (ML) is often challenging, especially for less experiencedusers in applied research areas. Special attention is required in settingswhere the model generation process involves hyperparameter tuning, i.e.data-driven optimization of different types of hyperparameters to improve thepredictive performance of the resulting model. Discussions about tuningtypically focus on the hyperparameters of the ML algorithm (e.g., the minimumnumber of observations in each terminal node for a tree-based algorithm). Inthis context, it is often neglected that hyperparameters also exist for thepreprocessing steps that are applied to the data before it is provided to thealgorithm (e.g., how to handle missing feature values in the data). As aconsequence, users experimenting with different preprocessing options toimprove model performance may be unaware that this constitutes a form ofhyperparameter tuning - albeit informal and unsystematic - and thus may fail toreport or account for this optimization. To illuminate this issue, this paperreviews and empirically illustrates different procedures for generating andevaluating prediction models, explicitly addressing the different waysalgorithm and preprocessing hyperparameters are typically handled by applied MLusers. By highlighting potential pitfalls, especially those that may lead toexaggerated performance claims, this review aims to further improve the qualityof predictive modeling in ML applications.</description><author>Christina Sauer, Anne-Laure Boulesteix, Luzia Hanßum, Farina Hodiamont, Claudia Bausewein, Theresa Ullmann</author><pubDate>Wed, 04 Dec 2024 17:29:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03491v1</guid></item><item><title>Data Fusion of Semantic and Depth Information in the Context of Object Detection</title><link>http://arxiv.org/abs/2412.03490v1</link><description>Considerable study has already been conducted regarding autonomous driving inmodern era. An autonomous driving system must be extremely good at detectingobjects surrounding the car to ensure safety. In this paper, classification,and estimation of an object's (pedestrian) position (concerning an ego 3Dcoordinate system) are studied and the distance between the ego vehicle and theobject in the context of autonomous driving is measured. To classify theobject, faster Region-based Convolution Neural Network (R-CNN) with inceptionv2 is utilized. First, a network is trained with customized dataset to estimatethe reference position of objects as well as the distance from the vehicle.From camera calibration to computing the distance, cutting-edge technologies ofcomputer vision algorithms in a series of processes are applied to generate a3D reference point of the region of interest. The foremost step in this processis generating a disparity map using the concept of stereo vision.</description><author>Md Abu Yusuf, Md Rezaul Karim Khan, Partha Pratim Saha, Mohammed Mahbubur Rahaman</author><pubDate>Wed, 04 Dec 2024 17:26:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03490v1</guid></item><item><title>Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective</title><link>http://arxiv.org/abs/2412.03487v1</link><description>The design space of discrete-space diffusion or flow generative models aresignificantly less well-understood than their continuous-space counterparts,with many works focusing only on a simple masked construction. In this work, weaim to take a holistic approach to the construction of discrete generativemodels based on continuous-time Markov chains, and for the first time, allowthe use of arbitrary discrete probability paths, or colloquially, corruptionprocesses. Through the lens of optimizing the symmetric kinetic energy, wepropose velocity formulas that can be applied to any given probability path,completely decoupling the probability and velocity, and giving the user thefreedom to specify any desirable probability path based on expert knowledgespecific to the data domain. Furthermore, we find that a special constructionof mixture probability paths optimizes the symmetric kinetic energy for thediscrete case. We empirically validate the usefulness of this new design spaceacross multiple modalities: text generation, inorganic material generation, andimage generation. We find that we can outperform the mask construction even intext with kinetic-optimal mixture paths, while we can make use ofdomain-specific constructions of the probability path over the visual domain.</description><author>Neta Shaul, Itai Gat, Marton Havasi, Daniel Severo, Anuroop Sriram, Peter Holderrieth, Brian Karrer, Yaron Lipman, Ricky T. Q. Chen</author><pubDate>Wed, 04 Dec 2024 17:24:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03487v1</guid></item><item><title>Tight PAC-Bayesian Risk Certificates for Contrastive Learning</title><link>http://arxiv.org/abs/2412.03486v1</link><description>Contrastive representation learning is a modern paradigm for learningrepresentations of unlabeled data via augmentations -- precisely, contrastivemodels learn to embed semantically similar pairs of samples (positive pairs)closer than independently drawn samples (negative samples). In spite of itsempirical success and widespread use in foundation models, statistical theoryfor contrastive learning remains less explored. Recent works have developedgeneralization error bounds for contrastive losses, but the resulting riskcertificates are either vacuous (certificates based on Rademacher complexity or$f$-divergence) or require strong assumptions about samples that areunreasonable in practice. The present paper develops non-vacuous PAC-Bayesianrisk certificates for contrastive representation learning, considering thepractical considerations of the popular SimCLR framework. Notably, we take intoaccount that SimCLR reuses positive pairs of augmented data as negative samplesfor other data, thereby inducing strong dependence and making classical PAC orPAC-Bayesian bounds inapplicable. We further refine existing bounds on thedownstream classification loss by incorporating SimCLR-specific factors,including data augmentation and temperature scaling, and derive riskcertificates for the contrastive zero-one risk. The resulting bounds forcontrastive loss and downstream prediction are much tighter than those ofprevious risk certificates, as demonstrated by experiments on CIFAR-10.</description><author>Anna van Elst, Debarghya Ghoshdastidar</author><pubDate>Wed, 04 Dec 2024 17:23:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03486v1</guid></item><item><title>Convolutional Neural Networks and Mixture of Experts for Intrusion Detection in 5G Networks and beyond</title><link>http://arxiv.org/abs/2412.03483v1</link><description>The advent of 6G/NextG networks comes along with a series of benefits,including extreme capacity, reliability, and efficiency. However, thesenetworks may become vulnerable to new security threats. Therefore, 6G/NextGnetworks must be equipped with advanced Artificial Intelligence algorithms, inorder to evade these attacks. Existing studies on the intrusion detection taskrely on the train of shallow machine learning classifiers, including LogisticRegression, Decision Trees, and so on, yielding suboptimal performance. Othersare based on deep neural networks consisting of static components, which arenot conditional on the input. This limits their representation power andefficiency. To resolve these issues, we present the first study integratingMixture of Experts (MoE) for identifying malicious traffic. Specifically, weuse network traffic data and convert the 1D array of features into a 2D matrix.Next, we pass this matrix through convolutional neural network (CNN) layersfollowed by batch normalization and max pooling layers. After obtaining therepresentation vector via the CNN layers, a sparsely gated MoE layer is used.This layer consists of a set of experts (dense layers) and a router, where therouter assigns weights to the output of each expert. Sparsity is achieved bychoosing the most relevant experts of the total ones. Finally, we perform aseries of ablation experiments to prove the effectiveness of our proposedmodel. Experiments are conducted on the 5G-NIDD dataset, a network intrusiondetection dataset generated from a real 5G test network. Results show that ourintroduced approach reaches weighted F1-score up to 99.95% achieving comparableperformance to existing approaches. Findings also show that our proposed modelachieves multiple advantages over state-of-the-art approaches.</description><author>Loukas Ilias, George Doukas, Vangelis Lamprou, Christos Ntanos, Dimitris Askounis</author><pubDate>Wed, 04 Dec 2024 17:20:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03483v1</guid></item><item><title>Fast Computation of Leave-One-Out Cross-Validation for $k$-NN Regression</title><link>http://arxiv.org/abs/2405.04919v2</link><description>We describe a fast computation method for leave-one-out cross-validation(LOOCV) for $k$-nearest neighbours ($k$-NN) regression. We show that, under atie-breaking condition for nearest neighbours, the LOOCV estimate of the meansquare error for $k$-NN regression is identical to the mean square error of$(k+1)$-NN regression evaluated on the training data, multiplied by the scalingfactor $(k+1)^2/k^2$. Therefore, to compute the LOOCV score, one only needs tofit $(k+1)$-NN regression only once, and does not need to repeattraining-validation of $k$-NN regression for the number of training data.Numerical experiments confirm the validity of the fast computation method.</description><author>Motonobu Kanagawa</author><pubDate>Wed, 04 Dec 2024 17:18:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04919v2</guid></item><item><title>Coverage-Constrained Human-AI Cooperation with Multiple Experts</title><link>http://arxiv.org/abs/2411.11976v2</link><description>Human-AI cooperative classification (HAI-CC) approaches aim to develop hybridintelligent systems that enhance decision-making in various high-stakesreal-world scenarios by leveraging both human expertise and AI capabilities.Current HAI-CC methods primarily focus on learning-to-defer (L2D), wheredecisions are deferred to human experts, and learning-to-complement (L2C),where AI and human experts make predictions cooperatively. However, a notableresearch gap remains in effectively exploring both L2D and L2C under diverseexpert knowledge to improve decision-making, particularly when constrained bythe cooperation cost required to achieve a target probability for AI-onlyselection (i.e., coverage). In this paper, we address this research gap byproposing the Coverage-constrained Learning to Defer and Complement withSpecific Experts (CL2DC) method. CL2DC makes final decisions through either AIprediction alone or by deferring to or complementing a specific expert,depending on the input data. Furthermore, we propose a coverage-constrainedoptimisation to control the cooperation cost, ensuring it approximates a targetprobability for AI-only selection. This approach enables an effectiveassessment of system performance within a specified budget. Also, CL2DC isdesigned to address scenarios where training sets contain multiple noisy-labelannotations without any clean-label references. Comprehensive evaluations onboth synthetic and real-world datasets demonstrate that CL2DC achieves superiorperformance compared to state-of-the-art HAI-CC methods.</description><author>Zheng Zhang, Cuong Nguyen, Kevin Wells, Thanh-Toan Do, David Rosewarne, Gustavo Carneiro</author><pubDate>Wed, 04 Dec 2024 17:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11976v2</guid></item><item><title>Cross-View-Prediction: Exploring Contrastive Feature for Hyperspectral Image Classification</title><link>http://arxiv.org/abs/2203.07000v2</link><description>This paper presents a self-supervised feature learning method forhyperspectral image classification. Our method tries to construct two differentviews of the raw hyperspectral image through a cross-representation learningmethod. And then to learn semantically consistent representation over thecreated views by contrastive learning method. Specifically, fourcross-channel-prediction based augmentation methods are naturally designed toutilize the high dimension characteristic of hyperspectral data for the viewconstruction. And the better representative features are learned by maximizingmutual information and minimizing conditional entropy across different viewsfrom our contrastive network. This 'Cross-View-Predicton' style isstraightforward and gets the state-of-the-art performance of unsupervisedclassification with a simple SVM classifier.</description><author>Anyu Zhang, Haotian Wu, Zeyu Cao</author><pubDate>Wed, 04 Dec 2024 17:11:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.07000v2</guid></item><item><title>Distributionally robust self-supervised learning for tabular data</title><link>http://arxiv.org/abs/2410.08511v5</link><description>Machine learning (ML) models trained using Empirical Risk Minimization (ERM)often exhibit systematic errors on specific subpopulations of tabular data,known as error slices. Learning robust representation in presence of errorslices is challenging, especially in self-supervised settings during thefeature reconstruction phase, due to high cardinality features and thecomplexity of constructing error sets. Traditional robust representationlearning methods are largely focused on improving worst group performance insupervised setting in computer vision, leaving a gap in approaches tailored fortabular data. We address this gap by developing a framework to learn robustrepresentation in tabular data during self-supervised pre-training. Ourapproach utilizes an encoder-decoder model trained with Masked LanguageModeling (MLM) loss to learn robust latent representations. This paper appliesthe Just Train Twice (JTT) and Deep Feature Reweighting (DFR) methods duringthe pre-training phase for tabular data. These methods fine-tune the ERMpre-trained model by up-weighting error-prone samples or creating balanceddatasets for specific categorical features. This results in specialized modelsfor each feature, which are then used in an ensemble approach to enhancedownstream classification performance. This methodology improves robustnessacross slices, thus enhancing overall generalization performance. Extensiveexperiments across various datasets demonstrate the efficacy of our approach.The code is available:\url{https://github.com/amazon-science/distributionally-robust-self-supervised-learning-for-tabular-data}.</description><author>Shantanu Ghosh, Tiankang Xie, Mikhail Kuznetsov</author><pubDate>Wed, 04 Dec 2024 17:10:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08511v5</guid></item><item><title>Enhancing Biomedical Knowledge Discovery for Diseases: An Open-Source Framework Applied on Rett Syndrome and Alzheimer's Disease</title><link>http://arxiv.org/abs/2407.13492v3</link><description>The ever-growing volume of biomedical publications creates a critical needfor efficient knowledge discovery. In this context, we introduce an open-sourceend-to-end framework designed to construct knowledge around specific diseasesdirectly from raw text. To facilitate research in disease-related knowledgediscovery, we create two annotated datasets focused on Rett syndrome andAlzheimer's disease, enabling the identification of semantic relations betweenbiomedical entities. Extensive benchmarking explores various ways to representrelations and entity representations, offering insights into optimal modelingstrategies for semantic relation detection and highlighting language models'competence in knowledge discovery. We also conduct probing experiments usingdifferent layer representations and attention scores to explore transformers'ability to capture semantic relations.</description><author>Christos Theodoropoulos, Andrei Catalin Coman, James Henderson, Marie-Francine Moens</author><pubDate>Wed, 04 Dec 2024 17:05:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13492v3</guid></item><item><title>Automatically Interpreting Millions of Features in Large Language Models</title><link>http://arxiv.org/abs/2410.13928v2</link><description>While the activations of neurons in deep neural networks usually do not havea simple human-understandable interpretation, sparse autoencoders (SAEs) can beused to transform these activations into a higher-dimensional latent spacewhich may be more easily interpretable. However, these SAEs can have millionsof distinct latent features, making it infeasible for humans to manuallyinterpret each one. In this work, we build an open-source automated pipeline togenerate and evaluate natural language explanations for SAE features usingLLMs. We test our framework on SAEs of varying sizes, activation functions, andlosses, trained on two different open-weight LLMs. We introduce five newtechniques to score the quality of explanations that are cheaper to run thanthe previous state of the art. One of these techniques, intervention scoring,evaluates the interpretability of the effects of intervening on a feature,which we find explains features that are not recalled by existing methods. Wepropose guidelines for generating better explanations that remain valid for abroader set of activating contexts, and discuss pitfalls with existing scoringtechniques. We use our explanations to measure the semantic similarity ofindependently trained SAEs, and find that SAEs trained on nearby layers of theresidual stream are highly similar. Our large-scale analysis confirms that SAElatents are indeed much more interpretable than neurons, even when neurons aresparsified using top-$k$ postprocessing. Our code is available athttps://github.com/EleutherAI/sae-auto-interp, and our explanations areavailable athttps://huggingface.co/datasets/EleutherAI/auto_interp_explanations.</description><author>Gonçalo Paulo, Alex Mallen, Caden Juang, Nora Belrose</author><pubDate>Wed, 04 Dec 2024 17:03:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13928v2</guid></item><item><title>Urban4D: Semantic-Guided 4D Gaussian Splatting for Urban Scene Reconstruction</title><link>http://arxiv.org/abs/2412.03473v1</link><description>Reconstructing dynamic urban scenes presents significant challenges due totheir intrinsic geometric structures and spatiotemporal dynamics. Existingmethods that attempt to model dynamic urban scenes without leveraging priors onpotentially moving regions often produce suboptimal results. Meanwhile,approaches based on manual 3D annotations yield improved reconstruction qualitybut are impractical due to labor-intensive labeling. In this paper, we revisitthe potential of 2D semantic maps for classifying dynamic and static Gaussiansand integrating spatial and temporal dimensions for urban scene representation.We introduce Urban4D, a novel framework that employs a semantic-guideddecomposition strategy inspired by advances in deep 2D semantic map generation.Our approach distinguishes potentially dynamic objects through reliablesemantic Gaussians. To explicitly model dynamic objects, we propose anintuitive and effective 4D Gaussian splatting (4DGS) representation thataggregates temporal information through learnable time embeddings for eachGaussian, predicting their deformations at desired timestamps using amultilayer perceptron (MLP). For more accurate static reconstruction, we alsodesign a k-nearest neighbor (KNN)-based consistency regularization to handlethe ground surface due to its low-texture characteristic. Extensive experimentson real-world datasets demonstrate that Urban4D not only achieves comparable orbetter quality than previous state-of-the-art methods but also effectivelycaptures dynamic objects while maintaining high visual fidelity for staticelements.</description><author>Ziwen Li, Jiaxin Huang, Runnan Chen, Yunlong Che, Yandong Guo, Tongliang Liu, Fakhri Karray, Mingming Gong</author><pubDate>Wed, 04 Dec 2024 16:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03473v1</guid></item><item><title>Measure Anything: Real-time, Multi-stage Vision-based Dimensional Measurement using Segment Anything</title><link>http://arxiv.org/abs/2412.03472v1</link><description>We present Measure Anything, a comprehensive vision-based framework fordimensional measurement of objects with circular cross-sections, leveraging theSegment Anything Model (SAM). Our approach estimates key geometric features --including diameter, length, and volume -- for rod-like geometries with varyingcurvature and general objects with constant skeleton slope. The frameworkintegrates segmentation, mask processing, skeleton construction, and 2D-3Dtransformation, packaged in a user-friendly interface. We validate ourframework by estimating the diameters of Canola stems -- collected fromagricultural fields in North Dakota -- which are thin and non-uniform, posingchallenges for existing methods. Measuring its diameters is critical, as it isa phenotypic traits that correlates with the health and yield of Canola crops.This application also exemplifies the potential of Measure Anything, whereintegrating intelligent models -- such as keypoint detection -- extends itsscalability to fully automate the measurement process for high-throughputapplications. Furthermore, we showcase its versatility in robotic grasping,leveraging extracted geometric features to identify optimal grasp points.</description><author>Yongkyu Lee, Shivam Kumar Panda, Wei Wang, Mohammad Khalid Jawed</author><pubDate>Wed, 04 Dec 2024 16:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03472v1</guid></item><item><title>Generalization Bounds and Model Complexity for Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2410.08026v2</link><description>Kolmogorov-Arnold Network (KAN) is a network structure recently proposed byLiu et al. (2024) that offers improved interpretability and a more parsimoniousdesign in many science-oriented tasks compared to multi-layer perceptrons. Thiswork provides a rigorous theoretical analysis of KAN by establishinggeneralization bounds for KAN equipped with activation functions that areeither represented by linear combinations of basis functions or lying in alow-rank Reproducing Kernel Hilbert Space (RKHS). In the first case, thegeneralization bound accommodates various choices of basis functions in formingthe activation functions in each layer of KAN and is adapted to differentoperator norms at each layer. For a particular choice of operator norms, thebound scales with the $l_1$ norm of the coefficient matrices and the Lipschitzconstants for the activation functions, and it has no dependence oncombinatorial parameters (e.g., number of nodes) outside of logarithmicfactors. Moreover, our result does not require the boundedness assumption onthe loss function and, hence, is applicable to a general class ofregression-type loss functions. In the low-rank case, the generalization boundscales polynomially with the underlying ranks as well as the Lipschitzconstants of the activation functions in each layer. These bounds areempirically investigated for KANs trained with stochastic gradient descent onsimulated and real data sets. The numerical results demonstrate the practicalrelevance of these bounds.</description><author>Xianyang Zhang, Huijuan Zhou</author><pubDate>Wed, 04 Dec 2024 16:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08026v2</guid></item><item><title>Cluster Specific Representation Learning</title><link>http://arxiv.org/abs/2412.03471v1</link><description>Representation learning aims to extract meaningful lower-dimensionalembeddings from data, known as representations. Despite its widespreadapplication, there is no established definition of a ``good'' representation.Typically, the representation quality is evaluated based on its performance indownstream tasks such as clustering, de-noising, etc. However, thistask-specific approach has a limitation where a representation that performswell for one task may not necessarily be effective for another. This highlightsthe need for a more agnostic formulation, which is the focus of our work. Wepropose a downstream-agnostic formulation: when inherent clusters exist in thedata, the representations should be specific to each cluster. Under this idea,we develop a meta-algorithm that jointly learns cluster-specificrepresentations and cluster assignments. As our approach is easy to integratewith any representation learning framework, we demonstrate its effectiveness invarious setups, including Autoencoders, Variational Autoencoders, Contrastivelearning models, and Restricted Boltzmann Machines. We qualitatively compareour cluster-specific embeddings to standard embeddings and downstream taskssuch as de-noising and clustering. While our method slightly increases runtimeand parameters compared to the standard model, the experiments clearly showthat it extracts the inherent cluster structures in the data, resulting inimproved performance in relevant applications.</description><author>Mahalakshmi Sabanayagam, Omar Al-Dabooni, Pascal Esser</author><pubDate>Wed, 04 Dec 2024 16:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03471v1</guid></item><item><title>Training-Free Mitigation of Language Reasoning Degradation After Multimodal Instruction Tuning</title><link>http://arxiv.org/abs/2412.03467v1</link><description>Multimodal models typically combine a powerful large language model (LLM)with a vision encoder and are then trained on multimodal data via instructiontuning. While this process adapts LLMs to multimodal settings, it remainsunclear whether this adaptation compromises their original language reasoningcapabilities. In this work, we explore the effects of multimodal instructiontuning on language reasoning performance. We focus on LLaVA, a leadingmultimodal framework that integrates LLMs such as Vicuna or Mistral with theCLIP vision encoder. We compare the performance of the original LLMs with theirmultimodal-adapted counterparts across eight language reasoning tasks. Ourexperiments yield several key insights. First, the impact of multimodallearning varies between Vicuna and Mistral: we observe a degradation inlanguage reasoning for Mistral but improvements for Vicuna across most tasks.Second, while multimodal instruction learning consistently degrades performanceon mathematical reasoning tasks (e.g., GSM8K), it enhances performance oncommonsense reasoning tasks (e.g., CommonsenseQA). Finally, we demonstrate thata training-free model merging technique can effectively mitigate the languagereasoning degradation observed in multimodal-adapted Mistral and even improveperformance on visual tasks.</description><author>Neale Ratzlaff, Man Luo, Xin Su, Vasudev Lal, Phillip Howard</author><pubDate>Wed, 04 Dec 2024 16:56:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03467v1</guid></item><item><title>Challenges in Guardrailing Large Language Models for Science</title><link>http://arxiv.org/abs/2411.08181v2</link><description>The rapid development in large language models (LLMs) has transformed thelandscape of natural language processing and understanding (NLP/NLU), offeringsignificant benefits across various domains. However, when applied toscientific research, these powerful models exhibit critical failure modesrelated to scientific integrity and trustworthiness. Existing general-purposeLLM guardrails are insufficient to address these unique challenges in thescientific domain. We provide comprehensive guidelines for deploying LLMguardrails in the scientific domain. We identify specific challenges --including time sensitivity, knowledge contextualization, conflict resolution,and intellectual property concerns -- and propose a guideline framework for theguardrails that can align with scientific needs. These guardrail dimensionsinclude trustworthiness, ethics &amp; bias, safety, and legal aspects. We alsooutline in detail the implementation strategies that employ white-box,black-box, and gray-box methodologies that can be enforced within scientificcontexts.</description><author>Nishan Pantha, Muthukumaran Ramasubramanian, Iksha Gurung, Manil Maskey, Rahul Ramachandran</author><pubDate>Wed, 04 Dec 2024 16:55:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08181v2</guid></item><item><title>YT-30M: A multi-lingual multi-category dataset of YouTube comments</title><link>http://arxiv.org/abs/2412.03465v1</link><description>This paper introduces two large-scale multilingual comment datasets, YT-30M(and YT-100K) from YouTube. The analysis in this paper is performed on asmaller sample (YT-100K) of YT-30M. Both the datasets: YT-30M (full) andYT-100K (randomly selected 100K sample from YT-30M) are publicly released forfurther research. YT-30M (YT-100K) contains 32236173 (108694) comments postedby YouTube channel that belong to YouTube categories. Each comment isassociated with a video ID, comment ID, commentor name, commentor channel ID,comment text, upvotes, original channel ID and category of the YouTube channel(e.g., 'News &amp; Politics', 'Science &amp; Technology', etc.).</description><author>Hridoy Sankar Dutta</author><pubDate>Wed, 04 Dec 2024 16:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03465v1</guid></item><item><title>Data quality dimensions for fair AI</title><link>http://arxiv.org/abs/2305.06967v2</link><description>Artificial Intelligence (AI) systems are not intrinsically neutral and biasestrickle in any type of technological tool. In particular when dealing withpeople, the impact of AI algorithms' technical errors originating withmislabeled data is undeniable. As they feed wrong and discriminatoryclassifications, these systems are not systematically guarded against bias. Inthis article we consider the problem of bias in AI systems from the point ofview of data quality dimensions. We highlight the limited model construction ofbias mitigation tools based on accuracy strategy, illustrating potentialimprovements of a specific tool in gender classification errors occurring intwo typically difficult contexts: the classification of non-binary individuals,for which the label set becomes incomplete with respect to the dataset; and theclassification of transgender individuals, for which the dataset becomesinconsistent with respect to the label set. Using formal methods for reasoningabout the behavior of the classification system in presence of a changingworld, we propose to reconsider the fairness of the classification task interms of completeness, consistency, timeliness and reliability, and offer sometheoretical results.</description><author>Camilla Quaresmini, Giuseppe Primiero</author><pubDate>Wed, 04 Dec 2024 16:54:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06967v2</guid></item><item><title>Validity and efficiency of the conformal CUSUM procedure</title><link>http://arxiv.org/abs/2412.03464v1</link><description>In this paper we study the validity and efficiency of a conformal version ofthe CUSUM procedure for change detection both experimentally and theoretically.</description><author>Vladimir Vovk, Ilia Nouretdinov, Alex Gammerman</author><pubDate>Wed, 04 Dec 2024 16:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03464v1</guid></item><item><title>Gesture Classification in Artworks Using Contextual Image Features</title><link>http://arxiv.org/abs/2412.03456v1</link><description>Recognizing gestures in artworks can add a valuable dimension to artunderstanding and help to acknowledge the role of the sense of smell incultural heritage. We propose a method to recognize smell gestures inhistorical artworks. We show that combining local features with global imagecontext improves classification performance notably on different backbones.</description><author>Azhar Hussian, Mathias Zinnen, Thi My Hang Tran, Andreas Maier, Vincent Christlein</author><pubDate>Wed, 04 Dec 2024 16:45:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03456v1</guid></item><item><title>GaussianBeV: 3D Gaussian Representation meets Perception Models for BeV Segmentation</title><link>http://arxiv.org/abs/2407.14108v2</link><description>The Bird's-eye View (BeV) representation is widely used for 3D perceptionfrom multi-view camera images. It allows to merge features from differentcameras into a common space, providing a unified representation of the 3Dscene. The key component is the view transformer, which transforms image viewsinto the BeV. However, actual view transformer methods based on geometry orcross-attention do not provide a sufficiently detailed representation of thescene, as they use a sub-sampling of the 3D space that is non-optimal formodeling the fine structures of the environment. In this paper, we proposeGaussianBeV, a novel method for transforming image features to BeV by finelyrepresenting the scene using a set of 3D gaussians located and oriented in 3Dspace. This representation is then splattered to produce the BeV feature map byadapting recent advances in 3D representation rendering based on gaussiansplatting. GaussianBeV is the first approach to use this 3D gaussian modelingand 3D scene rendering process online, i.e. without optimizing it on a specificscene and directly integrated into a single stage model for BeV sceneunderstanding. Experiments show that the proposed representation is highlyeffective and place GaussianBeV as the new state-of-the-art on the BeV semanticsegmentation task on the nuScenes dataset.</description><author>Florian Chabot, Nicolas Granger, Guillaume Lapouge</author><pubDate>Wed, 04 Dec 2024 16:43:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14108v2</guid></item><item><title>Pre-trained Multiple Latent Variable Generative Models are good defenders against Adversarial Attacks</title><link>http://arxiv.org/abs/2412.03453v1</link><description>Attackers can deliberately perturb classifiers' input with subtle noise,altering final predictions. Among proposed countermeasures, adversarialpurification employs generative networks to preprocess input images, filteringout adversarial noise. In this study, we propose specific generators, definedMultiple Latent Variable Generative Models (MLVGMs), for adversarialpurification. These models possess multiple latent variables that naturallydisentangle coarse from fine features. Taking advantage of these properties, weautoencode images to maintain class-relevant information, while discarding andre-sampling any detail, including adversarial noise. The procedure iscompletely training-free, exploring the generalization abilities of pre-trainedMLVGMs on the adversarial purification downstream task. Despite the lack oflarge models, trained on billions of samples, we show that smaller MLVGMs arealready competitive with traditional methods, and can be used as foundationmodels. Official code released at https://github.com/SerezD/gen_adversarial.</description><author>Dario Serez, Marco Cristani, Alessio Del Bue, Vittorio Murino, Pietro Morerio</author><pubDate>Wed, 04 Dec 2024 16:40:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03453v1</guid></item><item><title>Affordance-based Robot Manipulation with Flow Matching</title><link>http://arxiv.org/abs/2409.01083v3</link><description>We present a framework for assistive robot manipulation, which focuses on twofundamental challenges: first, efficiently adapting large-scale models todownstream scene affordance understanding tasks, especially in daily livingscenarios where gathering multi-task data involving humans requires strenuouseffort; second, effectively learning robot trajectories by grounding the visualaffordance model. We tackle the first challenge by employing aparameter-efficient prompt tuning method that prepends learnable text promptsto the frozen vision model to predict manipulation affordances in multi-taskscenarios. Then we propose to learn robot trajectories guided by affordances ina supervised Flow Matching method. Flow matching represents a robot visuomotorpolicy as a conditional process of flowing random waypoints to desired robottrajectories. Finally, we introduce a real-world dataset with 10 tasks acrossActivities of Daily Living to test our framework. Our extensive evaluationhighlights that the proposed prompt tuning method for learning manipulationaffordance with language prompter achieves competitive performance and evenoutperforms other finetuning protocols across data scales, while satisfyingparameter efficiency. Learning multi-task robot trajectories with flow matchingpolicy also leads to consistently better results than alternative behaviorcloning methods, including marginally better generalization performance andprominently faster inference than diffusion policy with DDPM. Our frameworkseamlessly unifies affordance model learning and trajectory generation withflow matching for robot manipulation.</description><author>Fan Zhang, Michael Gienger</author><pubDate>Wed, 04 Dec 2024 16:39:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.01083v3</guid></item><item><title>Number Cookbook: Number Understanding of Language Models and How to Improve It</title><link>http://arxiv.org/abs/2411.03766v2</link><description>Large language models (LLMs) can solve an increasing number of complexreasoning tasks while making surprising mistakes in basic numericalunderstanding and processing (such as 9.11 &gt; 9.9). The latter ability isessential for tackling complex arithmetic and mathematical problems and servesas a foundation for most reasoning tasks, but previous work paid littleattention to it or only discussed several restricted tasks (like integeraddition). In this paper, we comprehensively investigate the numericalunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce abenchmark covering four common numerical representations and 17 distinctnumerical tasks in four major categories, resulting in 41 meaningfulcombinations in total. These tasks are derived from primary and secondaryeducation curricula, encompassing nearly all everyday numerical understandingand processing scenarios, and the rules of these tasks are very simple andclear. Through the benchmark, we find that current LLMs fail frequently in manyof the tasks. To study the problem, we train small models with existing andpotential techniques for enhancing NUPA (such as tokenizers, PEs, and numberformats), comprehensively evaluating their effectiveness using our testbed. Wealso finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)naive finetuning can improve NUPA a lot on many but not all tasks, and 2)surprisingly, techniques designed to enhance NUPA prove ineffective forfinetuning pretrained models. We further explore the impact of chain-of-thoughttechniques on NUPA. Our work provides a more detailed and comprehensiveunderstanding of NUPA in LLMs. Our benchmark and code are released athttps://github.com/GraphPKU/number_cookbook.</description><author>Haotong Yang, Yi Hu, Shijia Kang, Zhouchen Lin, Muhan Zhang</author><pubDate>Wed, 04 Dec 2024 16:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03766v2</guid></item><item><title>PlanarSplatting: Accurate Planar Surface Reconstruction in 3 Minutes</title><link>http://arxiv.org/abs/2412.03451v1</link><description>This paper presents PlanarSplatting, an ultra-fast and accurate surfacereconstruction approach for multiview indoor images. We take the 3D planes asthe main objective due to their compactness and structural expressiveness inindoor scenes, and develop an explicit optimization framework that learns tofit the expected surface of indoor scenes by splatting the 3D planes into 2.5Ddepth and normal maps. As our PlanarSplatting operates directly on the 3D planeprimitives, it eliminates the dependencies on 2D/3D plane detection and planematching and tracking for planar surface reconstruction. Furthermore, theessential merits of plane-based representation plus CUDA-based implementationof planar splatting functions, PlanarSplatting reconstructs an indoor scene in3 minutes while having significantly better geometric accuracy. Thanks to ourultra-fast reconstruction speed, the largest quantitative evaluation on theScanNet and ScanNet++ datasets over hundreds of scenes clearly demonstrated theadvantages of our method. We believe that our accurate and ultrafast planarsurface reconstruction method will be applied in the structured data curationfor surface reconstruction in the future. The code of our CUDA implementationwill be publicly available. Project page:https://icetttb.github.io/PlanarSplatting/</description><author>Bin Tan, Rui Yu, Yujun Shen, Nan Xue</author><pubDate>Wed, 04 Dec 2024 16:38:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03451v1</guid></item><item><title>GenMix: Effective Data Augmentation with Generative Diffusion Model Image Editing</title><link>http://arxiv.org/abs/2412.02366v2</link><description>Data augmentation is widely used to enhance generalization in visualclassification tasks. However, traditional methods struggle when source andtarget domains differ, as in domain adaptation, due to their inability toaddress domain gaps. This paper introduces GenMix, a generalizableprompt-guided generative data augmentation approach that enhances bothin-domain and cross-domain image classification. Our technique leverages imageediting to generate augmented images based on custom conditional prompts,designed specifically for each problem type. By blending portions of the inputimage with its edited generative counterpart and incorporating fractalpatterns, our approach mitigates unrealistic images and label ambiguity,improving the performance and adversarial robustness of the resulting models.Efficacy of our method is established with extensive experiments on eightpublic datasets for general and fine-grained classification, in both in-domainand cross-domain settings. Additionally, we demonstrate performanceimprovements for self-supervised learning, learning with data scarcity, andadversarial robustness. As compared to the existing state-of-the-art methods,our technique achieves stronger performance across the board.</description><author>Khawar Islam, Muhammad Zaigham Zaheer, Arif Mahmood, Karthik Nandakumar, Naveed Akhtar</author><pubDate>Wed, 04 Dec 2024 16:38:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02366v2</guid></item><item><title>From Words to Workflows: Automating Business Processes</title><link>http://arxiv.org/abs/2412.03446v1</link><description>As businesses increasingly rely on automation to streamline operations, thelimitations of Robotic Process Automation (RPA) have become apparent,particularly its dependence on expert knowledge and inability to handle complexdecision-making tasks. Recent advancements in Artificial Intelligence (AI),particularly Generative AI (GenAI) and Large Language Models (LLMs), have pavedthe way for Intelligent Automation (IA), which integrates cognitivecapabilities to overcome the shortcomings of RPA. This paper introducesText2Workflow, a novel method that automatically generates workflows fromnatural language user requests. Unlike traditional automation approaches,Text2Workflow offers a generalized solution for automating any businessprocess, translating user inputs into a sequence of executable stepsrepresented in JavaScript Object Notation (JSON) format. Leveraging thedecision-making and instruction-following capabilities of LLMs, this methodprovides a scalable, adaptable framework that enables users to visualize andexecute workflows with minimal manual intervention. This research outlines theText2Workflow methodology and its broader implications for automating complexbusiness processes.</description><author>Laura Minkova, Jessica López Espejel, Taki Eddine Toufik Djaidja, Walid Dahhane, El Hassane Ettifouri</author><pubDate>Wed, 04 Dec 2024 16:34:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03446v1</guid></item><item><title>State Frequency Estimation for Anomaly Detection</title><link>http://arxiv.org/abs/2412.03442v1</link><description>Many works have studied the efficacy of state machines for detectinganomalies within NetFlows. These works typically learn a model from unlabeleddata and compute anomaly scores for arbitrary traces based on their likelihoodof occurrence or how well they fit within the model. However, these methods donot dynamically adapt their scores based on the traces seen at test time. Thisbecomes a problem when an adversary produces seemingly common traces in theirattack, causing the model to miss the detection by assigning low anomalyscores. We propose SEQUENT, a new approach that uses the state visit frequencyto adapt its scoring for anomaly detection dynamically. SEQUENT subsequentlyuses the scores to generate root causes for anomalies. These allow the groupingof alarms and simplify the analysis of anomalies. Our evaluation of SEQUENT onthree NetFlow datasets indicates that our approach outperforms existingmethods, demonstrating its effectiveness in detecting anomalies.</description><author>Clinton Cao, Agathe Blaise, Annibale Panichella, Sicco Verwer</author><pubDate>Wed, 04 Dec 2024 16:30:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03442v1</guid></item><item><title>PBP: Post-training Backdoor Purification for Malware Classifiers</title><link>http://arxiv.org/abs/2412.03441v1</link><description>In recent years, the rise of machine learning (ML) in cybersecurity hasbrought new challenges, including the increasing threat of backdoor poisoningattacks on ML malware classifiers. For instance, adversaries could injectmalicious samples into public malware repositories, contaminating the trainingdata and potentially misclassifying malware by the ML model. Currentcountermeasures predominantly focus on detecting poisoned samples by leveragingdisagreements within the outputs of a diverse set of ensemble models ontraining data points. However, these methods are not suitable for scenarioswhere Machine Learning-as-a-Service (MLaaS) is used or when users aim to removebackdoors from a model after it has been trained. Addressing this scenario, weintroduce PBP, a post-training defense for malware classifiers that mitigatesvarious types of backdoor embeddings without assuming any specific backdoorembedding mechanism. Our method exploits the influence of backdoor attacks onthe activation distribution of neural networks, independent of thetrigger-embedding method. In the presence of a backdoor attack, the activationdistribution of each layer is distorted into a mixture of distributions. Byregulating the statistics of the batch normalization layers, we can guide abackdoored model to perform similarly to a clean one. Our method demonstratessubstantial advantages over several state-of-the-art methods, as evidenced byexperiments on two datasets, two types of backdoor methods, and various attackconfigurations. Notably, our approach requires only a small portion of thetraining data -- only 1\% -- to purify the backdoor and reduce the attacksuccess rate from 100\% to almost 0\%, a 100-fold improvement over the baselinemethods. Our code is available at\url{https://github.com/judydnguyen/pbp-backdoor-purification-official}.</description><author>Dung Thuy Nguyen, Ngoc N. Tran, Taylor T. Johnson, Kevin Leach</author><pubDate>Wed, 04 Dec 2024 16:30:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03441v1</guid></item><item><title>CleanDIFT: Diffusion Features without Noise</title><link>http://arxiv.org/abs/2412.03439v1</link><description>Internal features from large-scale pre-trained diffusion models have recentlybeen established as powerful semantic descriptors for a wide range ofdownstream tasks. Works that use these features generally need to add noise toimages before passing them through the model to obtain the semantic features,as the models do not offer the most useful features when given images withlittle to no noise. We show that this noise has a critical impact on theusefulness of these features that cannot be remedied by ensembling withdifferent random noises. We address this issue by introducing a lightweight,unsupervised fine-tuning method that enables diffusion backbones to providehigh-quality, noise-free semantic features. We show that these features readilyoutperform previous diffusion features by a wide margin in a wide variety ofextraction setups and downstream tasks, offering better performance than evenensemble-based methods at a fraction of the cost.</description><author>Nick Stracke, Stefan Andreas Baumann, Kolja Bauer, Frank Fundel, Björn Ommer</author><pubDate>Wed, 04 Dec 2024 16:29:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03439v1</guid></item><item><title>BIMCaP: BIM-based AI-supported LiDAR-Camera Pose Refinement</title><link>http://arxiv.org/abs/2412.03434v1</link><description>This paper introduces BIMCaP, a novel method to integrate mobile 3D sparseLiDAR data and camera measurements with pre-existing building informationmodels (BIMs), enhancing fast and accurate indoor mapping with affordablesensors. BIMCaP refines sensor poses by leveraging a 3D BIM and employing abundle adjustment technique to align real-world measurements with the model.Experiments using real-world open-access data show that BIMCaP achievessuperior accuracy, reducing translational error by over 4 cm compared tocurrent state-of-the-art methods. This advancement enhances the accuracy andcost-effectiveness of 3D mapping methodologies like SLAM. BIMCaP's improvementsbenefit various fields, including construction site management and emergencyresponse, by providing up-to-date, aligned digital maps for betterdecision-making and productivity. Link to the repository:https://github.com/MigVega/BIMCaP</description><author>Miguel Arturo Vega Torres, Anna Ribic, Borja García de Soto, André Borrmann</author><pubDate>Wed, 04 Dec 2024 16:26:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03434v1</guid></item><item><title>Genetic Algorithm Based System for Path Planning with Unmanned Aerial Vehicles Swarms in Cell-Grid Environments</title><link>http://arxiv.org/abs/2412.03433v1</link><description>Path Planning methods for autonomously controlling swarms of unmanned aerialvehicles (UAVs) are gaining momentum due to their operational advantages. Anincreasing number of scenarios now require autonomous control of multiple UAVs,as autonomous operation can significantly reduce labor costs. Additionally,obtaining optimal flight paths can lower energy consumption, thereby extendingbattery life for other critical operations. Many of these scenarios, however,involve obstacles such as power lines and trees, which complicate PathPlanning. This paper presents an evolutionary computation-based systememploying genetic algorithms to address this problem in environments withobstacles. The proposed approach aims to ensure complete coverage of areas withfixed obstacles, such as in field exploration tasks, while minimizing flighttime regardless of map size or the number of UAVs in the swarm. No specificgoal points or prior information beyond the provided map is required. Theexperiments conducted in this study used five maps of varying sizes andobstacle densities, as well as a control map without obstacles, with differentnumbers of UAVs. The results demonstrate that this method can determine optimalpaths for all UAVs during full map traversal, thus minimizing resourceconsumption. A comparative analysis with other state-of-the-art approach ispresented to highlight the advantages and potential limitations of the proposedmethod.</description><author>Alejandro Puente-Castro, Enrique Fernandez-Blanco, Daniel Rivero</author><pubDate>Wed, 04 Dec 2024 16:24:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03433v1</guid></item><item><title>SINGER: Vivid Audio-driven Singing Video Generation with Multi-scale Spectral Diffusion Model</title><link>http://arxiv.org/abs/2412.03430v1</link><description>Recent advancements in generative models have significantly enhanced talkingface video generation, yet singing video generation remains underexplored. Thedifferences between human talking and singing limit the performance of existingtalking face video generation models when applied to singing. The fundamentaldifferences between talking and singing-specifically in audio characteristicsand behavioral expressions-limit the effectiveness of existing models. Weobserve that the differences between singing and talking audios manifest interms of frequency and amplitude. To address this, we have designed amulti-scale spectral module to help the model learn singing patterns in thespectral domain. Additionally, we develop a spectral-filtering module that aidsthe model in learning the human behaviors associated with singing audio. Thesetwo modules are integrated into the diffusion model to enhance singing videogeneration performance, resulting in our proposed model, SINGER. Furthermore,the lack of high-quality real-world singing face videos has hindered thedevelopment of the singing video generation community. To address this gap, wehave collected an in-the-wild audio-visual singing dataset to facilitateresearch in this area. Our experiments demonstrate that SINGER is capable ofgenerating vivid singing videos and outperforms state-of-the-art methods inboth objective and subjective evaluations.</description><author>Yan Li, Ziya Zhou, Zhiqiang Wang, Wei Xue, Wenhan Luo, Yike Guo</author><pubDate>Wed, 04 Dec 2024 16:19:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03430v1</guid></item><item><title>2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction</title><link>http://arxiv.org/abs/2412.03428v1</link><description>The reconstruction of indoor scenes remains challenging due to the inherentcomplexity of spatial structures and the prevalence of textureless regions.Recent advancements in 3D Gaussian Splatting have improved novel view synthesiswith accelerated processing but have yet to deliver comparable performance insurface reconstruction. In this paper, we introduce 2DGS-Room, a novel methodleveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction.Specifically, we employ a seed-guided mechanism to control the distribution of2D Gaussians, with the density of seed points dynamically optimized throughadaptive growth and pruning mechanisms. To further improve geometric accuracy,we incorporate monocular depth and normal priors to provide constraints fordetails and textureless regions respectively. Additionally, multi-viewconsistency constraints are employed to mitigate artifacts and further enhancereconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasetsdemonstrate that our method achieves state-of-the-art performance in indoorscene reconstruction.</description><author>Wanting Zhang, Haodong Xiang, Zhichao Liao, Xiansong Lai, Xinghui Li, Long Zeng</author><pubDate>Wed, 04 Dec 2024 16:17:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03428v1</guid></item><item><title>Assessing Foundation Models' Transferability to Physiological Signals in Precision Medicine</title><link>http://arxiv.org/abs/2412.03427v1</link><description>The success of precision medicine requires computational models that caneffectively process and interpret diverse physiological signals acrossheterogeneous patient populations. While foundation models have demonstratedremarkable transfer capabilities across various domains, their effectiveness inhandling individual-specific physiological signals - crucial for precisionmedicine - remains largely unexplored. This work introduces a systematicpipeline for rapidly and efficiently evaluating foundation models' transfercapabilities in medical contexts. Our pipeline employs a three-stage approach.First, it leverages physiological simulation software to generate diverse,clinically relevant scenarios, particularly focusing on data-scarce medicalconditions. This simulation-based approach enables both targeted capabilityassessment and subsequent model fine-tuning. Second, the pipeline projectsthese simulated signals through the foundation model to obtain embeddings,which are then evaluated using linear methods. This evaluation quantifies themodel's ability to capture three critical aspects: physiological featureindependence, temporal dynamics preservation, and medical scenariodifferentiation. Finally, the pipeline validates these representations throughspecific downstream medical tasks. Initial testing of our pipeline on theMoirai time series foundation model revealed significant limitations inphysiological signal processing, including feature entanglement, temporaldynamics distortion, and reduced scenario discrimination. These findingssuggest that current foundation models may require substantial architecturalmodifications or targeted fine-tuning before deployment in clinical settings.</description><author>Matthias Christenson, Cove Geary, Brian Locke, Pranav Koirala, Warren Woodrich Pettine</author><pubDate>Wed, 04 Dec 2024 16:17:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03427v1</guid></item><item><title>Tango*: Constrained synthesis planning using chemically informed value functions</title><link>http://arxiv.org/abs/2412.03424v1</link><description>Computer-aided synthesis planning (CASP) has made significant strides ingenerating retrosynthetic pathways for simple molecules in a non-constrainedfashion. Recent work introduces a specialised bidirectional search algorithmwith forward and retro expansion to address the starting material-constrainedsynthesis problem, allowing CASP systems to provide synthesis pathways fromspecified starting materials, such as waste products or renewable feed-stocks.In this work, we introduce a simple guided search which allows solving thestarting material-constrained synthesis planning problem using an existing,uni-directional search algorithm, Retro*. We show that by optimising a singlehyperparameter, Tango* outperforms existing methods in terms of efficiency andsolve rate. We find the Tango* cost function catalyses strong improvements forthe bidirectional DESP methods. Our method also achieves lower wall clock timeswhile proposing synthetic routes of similar length, a common metric for routequality. Finally, we highlight potential reasons for the strong performance ofTango over neural guided search methods</description><author>Daniel Armstrong, Zlatko Joncev, Jeff Guo, Philippe Schwaller</author><pubDate>Wed, 04 Dec 2024 16:14:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03424v1</guid></item><item><title>DataLab: A Unified Platform for LLM-Powered Business Intelligence</title><link>http://arxiv.org/abs/2412.02205v2</link><description>Business intelligence (BI) transforms large volumes of data within modernorganizations into actionable insights for informed decision-making. Recently,large language model (LLM)-based agents have streamlined the BI workflow byautomatically performing task planning, reasoning, and actions in executableenvironments based on natural language (NL) queries. However, existingapproaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS.The fragmentation of tasks across different data roles and tools lead toinefficiencies and potential errors due to the iterative and collaborativenature of BI. In this paper, we introduce DataLab, a unified BI platform thatintegrates a one-stop LLM-based agent framework with an augmented computationalnotebook interface. DataLab supports a wide range of BI tasks for differentdata roles by seamlessly combining LLM assistance with user customizationwithin a single environment. To achieve this unification, we design a domainknowledge incorporation module tailored for enterprise-specific BI tasks, aninter-agent communication mechanism to facilitate information sharing acrossthe BI workflow, and a cell-based context management strategy to enhancecontext utilization efficiency in BI notebooks. Extensive experimentsdemonstrate that DataLab achieves state-of-the-art performance on various BItasks across popular research benchmarks. Moreover, DataLab maintains higheffectiveness and efficiency on real-world datasets from Tencent, achieving upto a 58.58% increase in accuracy and a 61.65% reduction in token cost onenterprise-specific BI tasks.</description><author>Luoxuan Weng, Yinghao Tang, Yingchaojie Feng, Zhuo Chang, Peng Chen, Ruiqin Chen, Haozhe Feng, Chen Hou, Danqing Huang, Yang Li, Huaming Rao, Haonan Wang, Canshi Wei, Xiaofeng Yang, Yuhui Zhang, Yifeng Zheng, Xiuqi Huang, Minfeng Zhu, Yuxin Ma, Bin Cui, Wei Chen</author><pubDate>Wed, 04 Dec 2024 16:12:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02205v2</guid></item><item><title>Controlling Counterfactual Harm in Decision Support Systems Based on Prediction Sets</title><link>http://arxiv.org/abs/2406.06671v2</link><description>Decision support systems based on prediction sets help humans solvemulticlass classification tasks by narrowing down the set of potential labelvalues to a subset of them, namely a prediction set, and asking them to alwayspredict label values from the prediction sets. While this type of systems havebeen proven to be effective at improving the average accuracy of thepredictions made by humans, by restricting human agency, they may causeharm$\unicode{x2014}$a human who has succeeded at predicting the ground-truthlabel of an instance on their own may have failed had they used these systems.In this paper, our goal is to control how frequently a decision support systembased on prediction sets may cause harm, by design. To this end, we start bycharacterizing the above notion of harm using the theoretical framework ofstructural causal models. Then, we show that, under a natural, albeitunverifiable, monotonicity assumption, we can estimate how frequently a systemmay cause harm using only predictions made by humans on their own. Further, wealso show that, under a weaker monotonicity assumption, which can be verifiedexperimentally, we can bound how frequently a system may cause harm again usingonly predictions made by humans on their own. Building upon these assumptions,we introduce a computational framework to design decision support systems basedon prediction sets that are guaranteed to cause harm less frequently than auser-specified value using conformal risk control. We validate our frameworkusing real human predictions from two different human subject studies and showthat, in decision support systems based on prediction sets, there is atrade-off between accuracy and counterfactual harm.</description><author>Eleni Straitouri, Suhas Thejaswi, Manuel Gomez Rodriguez</author><pubDate>Wed, 04 Dec 2024 16:04:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06671v2</guid></item><item><title>Prediction-Powered Ranking of Large Language Models</title><link>http://arxiv.org/abs/2402.17826v3</link><description>Large language models are often ranked according to their level of alignmentwith human preferences -- a model is better than other models if its outputsare more frequently preferred by humans. One of the popular ways to elicithuman preferences utilizes pairwise comparisons between the outputs provided bydifferent models to the same inputs. However, since gathering pairwisecomparisons by humans is costly and time-consuming, it has become a commonpractice to gather pairwise comparisons by a strong large language model -- amodel strongly aligned with human preferences. Surprisingly, practitionerscannot currently measure the uncertainty that any mismatch between human andmodel preferences may introduce in the constructed rankings. In this work, wedevelop a statistical framework to bridge this gap. Given a (small) set ofpairwise comparisons by humans and a large set of pairwise comparisons by amodel, our framework provides a rank-set -- a set of possible ranking positions-- for each of the models under comparison. Moreover, it guarantees that, witha probability greater than or equal to a user-specified value, the rank-setscover the true ranking consistent with the distribution of human pairwisepreferences asymptotically. Using pairwise comparisons made by humans in theLMSYS Chatbot Arena platform and pairwise comparisons made by three stronglarge language models, we empirically demonstrate the effectivity of ourframework and show that the rank-sets constructed using only pairwisecomparisons by the strong large language models are often inconsistent with(the distribution of) human pairwise preferences.</description><author>Ivi Chatzi, Eleni Straitouri, Suhas Thejaswi, Manuel Gomez Rodriguez</author><pubDate>Wed, 04 Dec 2024 16:03:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17826v3</guid></item><item><title>Automated Test-Case Generation for REST APIs Using Model Inference Search Heuristic</title><link>http://arxiv.org/abs/2412.03420v1</link><description>The rising popularity of the microservice architectural style has led to agrowing demand for automated testing approaches tailored to these systems.EvoMaster is a state-of-the-art tool that uses Evolutionary Algorithms (EAs) toautomatically generate test cases for microservices' REST APIs. One limitationof these EAs is the use of unit-level search heuristics, such as branchdistances, which focus on fine-grained code coverage and may not effectivelycapture the complex, interconnected behaviors characteristic of system-leveltesting. To address this limitation, we propose a new search heuristic (MISH)that uses real-time automaton learning to guide the test case generationprocess. We capture the sequential call patterns exhibited by a test case bylearning an automaton from the stream of log events outputted by differentmicroservices within the same system. Therefore, MISH learns a representationof the systemwide behavior, allowing us to define the fitness of a test casebased on the path it traverses within the inferred automaton. We empiricallyevaluate MISH's effectiveness on six real-world benchmark microserviceapplications and compare it against a state-of-the-art technique, MOSA, fortesting REST APIs. Our evaluation shows promising results for using MISH toguide the automated test case generation within EvoMaster.</description><author>Clinton Cao, Annibale Panichella, Sicco Verwer</author><pubDate>Wed, 04 Dec 2024 16:00:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03420v1</guid></item><item><title>Simplified derivations for high-dimensional convex learning problems</title><link>http://arxiv.org/abs/2412.01110v2</link><description>Statistical physics provides tools for analyzing high-dimensional problems inmachine learning and theoretical neuroscience. These calculations, particularlythose using the replica method, often involve lengthy derivations that canobscure physical interpretation. We give concise, non-replica derivations ofseveral key results and highlight their underlying similarities. Specifically,we introduce a cavity approach to analyzing high-dimensional learning problemsand apply it to three cases: perceptron classification of points, perceptronclassification of manifolds, and kernel ridge regression. These problems sharea common structure -- a bipartite system of interacting feature and datumvariables -- enabling a unified analysis. For perceptron-capacity problems, weidentify a symmetry that allows derivation of correct capacities through ana\"ive method. These results match those obtained through the replica method.</description><author>David G. Clark, Haim Sompolinsky</author><pubDate>Wed, 04 Dec 2024 15:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.01110v2</guid></item><item><title>Dialectal Coverage And Generalization in Arabic Speech Recognition</title><link>http://arxiv.org/abs/2411.05872v2</link><description>Developing robust automatic speech recognition (ASR) systems for Arabic, alanguage characterized by its rich dialectal diversity and often considered alow-resource language in speech technology, demands effective strategies tomanage its complexity. This study explores three critical factors influencingASR performance: the role of dialectal coverage in pre-training, theeffectiveness of dialect-specific fine-tuning compared to a multi-dialectalapproach, and the ability to generalize to unseen dialects. Through extensiveexperiments across different dialect combinations, our findings offer keyinsights towards advancing the development of ASR systems for pluricentriclanguages like Arabic.</description><author>Amirbek Djanibekov, Hawau Olamide Toyin, Raghad Alshalan, Abdullah Alitr, Hanan Aldarmaki</author><pubDate>Wed, 04 Dec 2024 15:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05872v2</guid></item><item><title>Learning Semantic Association Rules from Internet of Things Data</title><link>http://arxiv.org/abs/2412.03417v1</link><description>Association Rule Mining (ARM) is the task of discovering commonalities indata in the form of logical implications. ARM is used in the Internet of Things(IoT) for different tasks including monitoring and decision-making. However,existing methods give limited consideration to IoT-specific requirements suchas heterogeneity and volume. Furthermore, they do not utilize important staticdomain-specific description data about IoT systems, which is increasinglyrepresented as knowledge graphs. In this paper, we propose a novel ARM pipelinefor IoT data that utilizes both dynamic sensor data and static IoT systemmetadata. Furthermore, we propose an Autoencoder-based Neurosymbolic ARM method(Aerial) as part of the pipeline to address the high volume of IoT data andreduce the total number of rules that are resource-intensive to process. Aeriallearns a neural representation of a given data and extracts association rulesfrom this representation by exploiting the reconstruction (decoding) mechanismof an autoencoder. Extensive evaluations on 3 IoT datasets from 2 domains showthat ARM on both static and dynamic IoT data results in more genericallyapplicable rules while Aerial can learn a more concise set of high-qualityassociation rules than the state-of-the-art with full coverage over thedatasets.</description><author>Erkan Karabulut, Paul Groth, Victoria Degeler</author><pubDate>Wed, 04 Dec 2024 15:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03417v1</guid></item><item><title>Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization</title><link>http://arxiv.org/abs/2411.03752v2</link><description>Recent studies have shown that deep learning models are very vulnerable topoisoning attacks. Many defense methods have been proposed to address thisissue. However, traditional poisoning attacks are not as threatening ascommonly believed. This is because they often cause differences in how themodel performs on the training set compared to the validation set. Suchinconsistency can alert defenders that their data has been poisoned, allowingthem to take the necessary defensive actions. In this paper, we introduce amore threatening type of poisoning attack called the Deferred Poisoning Attack.This new attack allows the model to function normally during the training andvalidation phases but makes it very sensitive to evasion attacks or evennatural noise. We achieve this by ensuring the poisoned model's loss functionhas a similar value as a normally trained model at each input sample but with alarge local curvature. A similar model loss ensures that there is no obviousinconsistency between the training and validation accuracy, demonstrating highstealthiness. On the other hand, the large curvature implies that a smallperturbation may cause a significant increase in model loss, leading tosubstantial performance degradation, which reflects a worse robustness. Wefulfill this purpose by making the model have singular Hessian information atthe optimal point via our proposed Singularization Regularization term. We haveconducted both theoretical and empirical analyses of the proposed method andvalidated its effectiveness through experiments on image classification tasks.Furthermore, we have confirmed the hazards of this form of poisoning attackunder more general scenarios using natural noise, offering a new perspectivefor research in the field of security.</description><author>Yuhao He, Jinyu Tian, Xianwei Zheng, Li Dong, Yuanman Li, Jiantao Zhou</author><pubDate>Wed, 04 Dec 2024 15:53:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03752v2</guid></item><item><title>Bounds in Wasserstein distance for locally stationary processes</title><link>http://arxiv.org/abs/2412.03414v1</link><description>Locally stationary processes (LSPs) provide a robust framework for modelingtime-varying phenomena, allowing for smooth variations in statisticalproperties such as mean and variance over time. In this paper, we address theestimation of the conditional probability distribution of LSPs usingNadaraya-Watson (NW) type estimators. The NW estimator approximates theconditional distribution of a target variable given covariates through kernelsmoothing techniques. We establish the convergence rate of the NW conditionalprobability estimator for LSPs in the univariate setting under the Wassersteindistance and extend this analysis to the multivariate case using the slicedWasserstein distance. Theoretical results are supported by numericalexperiments on both synthetic and real-world datasets, demonstrating thepractical usefulness of the proposed estimators.</description><author>Jan Nino G. Tinio, Mokhtar Z. Alaya, Salim Bouzebda</author><pubDate>Wed, 04 Dec 2024 15:51:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03414v1</guid></item><item><title>Deep Learning for Sea Surface Temperature Reconstruction under Cloud Occlusion</title><link>http://arxiv.org/abs/2412.03413v1</link><description>Sea Surface Temperature (SST) is crucial for understanding Earth's oceans andclimate, significantly influencing weather patterns, ocean currents, marineecosystem health, and the global energy balance. Large-scale SST monitoringrelies on satellite infrared radiation detection, but cloud cover presents amajor challenge, creating extensive observational gaps and hampering ourability to fully capture large-scale ocean temperature patterns. Efforts toaddress these gaps in existing L4 datasets have been made, but they oftenexhibit notable local and seasonal biases, compromising data reliability andaccuracy. To tackle this challenge, we employed deep neural networks toreconstruct cloud-covered portions of satellite imagery while preserving theintegrity of observed values in cloud-free areas, using MODIS satellite derivedobservations of SST. Our best-performing architecture showed significant skillimprovements over established methodologies, achieving substantial reductionsin error metrics when benchmarked against widely used approaches and datasets.These results underscore the potential of advanced AI techniques to enhance thecompleteness of satellite observations in Earth-science remote sensing,providing more accurate and reliable datasets for environmental assessments,data-driven model training, climate research, and seamless integration intomodel data assimilation workflows.</description><author>Andrea Asperti, Ali Aydogdu, Emanuela Clementi, Angelo Greco, Lorenzo Mentaschi, Fabio Merizzi, Pietro Miraglio, Paolo Oddo, Nadia Pinardi, Alessandro Testa</author><pubDate>Wed, 04 Dec 2024 15:49:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03413v1</guid></item><item><title>PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following Models Need for Efficient Generation</title><link>http://arxiv.org/abs/2412.03409v1</link><description>Recently, large vision-language models (LVLMs) have rapidly gained popularityfor their strong generation and reasoning capabilities given diverse multimodalinputs. However, these models incur significant computational and memoryoverhead during inference, which greatly hinders the efficient deployment inpractical scenarios. The extensive key-value (KV) cache, necessitated by thelengthy input and output sequences, notably contributes to the high inferencecost. Based on this, recent works have investigated ways to reduce the KV cachesize for higher efficiency. Although effective, they generally overlook thedistinct importance distributions of KV vectors across layers and maintain thesame cache size for each layer during the next token prediction. This resultsin the significant contextual information loss for certain layers, leading tonotable performance decline. To address this, we present PrefixKV. It reframesthe challenge of determining KV cache sizes for all layers into the task ofsearching for the optimal global prefix configuration. With an adaptivelayer-wise KV retention recipe based on binary search, the maximum contextualinformation can thus be preserved in each layer, facilitating the generation.Extensive experiments demonstrate that our method achieves the state-of-the-artperformance compared with others. It exhibits superior inference efficiency andgeneration quality trade-offs, showing promising potential for practicalapplications. Code is available at \url{https://github.com/THU-MIG/PrefixKV}.</description><author>Ao Wang, Hui Chen, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Zijia Lin, Jungong Han, Guiguang Ding</author><pubDate>Wed, 04 Dec 2024 15:48:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03409v1</guid></item><item><title>Skel3D: Skeleton Guided Novel View Synthesis</title><link>http://arxiv.org/abs/2412.03407v1</link><description>In this paper, we present an approach for monocular open-set novel viewsynthesis (NVS) that leverages object skeletons to guide the underlyingdiffusion model. Building upon a baseline that utilizes a pre-trained 2D imagegenerator, our method takes advantage of the Objaverse dataset, which includesanimated objects with bone structures. By introducing a skeleton guide layerfollowing the existing ray conditioning normalization (RCN) layer, our approachenhances pose accuracy and multi-view consistency. The skeleton guide layerprovides detailed structural information for the generative model, improvingthe quality of synthesized views. Experimental results demonstrate that ourskeleton-guided method significantly enhances consistency and accuracy acrossdiverse object categories within the Objaverse dataset. Our method outperformsexisting state-of-the-art NVS techniques both quantitatively and qualitatively,without relying on explicit 3D representations.</description><author>Aron Fóthi, Bence Fazekas, Natabara Máté Gyöngyössy, Kristian Fenech</author><pubDate>Wed, 04 Dec 2024 15:45:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03407v1</guid></item><item><title>Deep Operator BSDE: a Numerical Scheme to Approximate the Solution Operators</title><link>http://arxiv.org/abs/2412.03405v1</link><description>Motivated by dynamic risk measures and conditional $g$-expectations, in thiswork we propose a numerical method to approximate the solution operator givenby a Backward Stochastic Differential Equation (BSDE). The main ingredients forthis are the Wiener chaos decomposition and the classical Euler scheme forBSDEs. We show convergence of this scheme under very mild assumptions, andprovide a rate of convergence in more restrictive cases. We then implement itusing neural networks, and we present several numerical examples where we cancheck the accuracy of the method.</description><author>Giulia Di Nunno, Pere Díaz Lozano</author><pubDate>Wed, 04 Dec 2024 15:36:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03405v1</guid></item><item><title>Can In-context Learning Really Generalize to Out-of-distribution Tasks?</title><link>http://arxiv.org/abs/2410.09695v3</link><description>In this work, we explore the mechanism of in-context learning (ICL) onout-of-distribution (OOD) tasks that were not encountered during training. Toachieve this, we conduct synthetic experiments where the objective is to learnOOD mathematical functions through ICL using a GPT-2 model. We reveal thatTransformers may struggle to learn OOD task functions through ICL.Specifically, ICL performance resembles implementing a function within thepretraining hypothesis space and optimizing it with gradient descent based onthe in-context examples. Additionally, we investigate ICL's well-documentedability to learn unseen abstract labels in context. We demonstrate that suchability only manifests in the scenarios without distributional shifts and,therefore, may not serve as evidence of new-task-learning ability. Furthermore,we assess ICL's performance on OOD tasks when the model is pretrained onmultiple tasks. Both empirical and theoretical analyses demonstrate theexistence of the \textbf{low-test-error preference} of ICL, where it tends toimplement the pretraining function that yields low test error in the testingcontext. We validate this through numerical experiments. This new theoreticalresult, combined with our empirical findings, elucidates the mechanism of ICLin addressing OOD tasks.</description><author>Qixun Wang, Yifei Wang, Yisen Wang, Xianghua Ying</author><pubDate>Wed, 04 Dec 2024 15:35:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09695v3</guid></item><item><title>Benchmarking Pretrained Attention-based Models for Real-Time Recognition in Robot-Assisted Esophagectomy</title><link>http://arxiv.org/abs/2412.03401v1</link><description>Esophageal cancer is among the most common types of cancer worldwide. It istraditionally treated using open esophagectomy, but in recent years,robot-assisted minimally invasive esophagectomy (RAMIE) has emerged as apromising alternative. However, robot-assisted surgery can be challenging fornovice surgeons, as they often suffer from a loss of spatial orientation.Computer-aided anatomy recognition holds promise for improving surgicalnavigation, but research in this area remains limited. In this study, wedeveloped a comprehensive dataset for semantic segmentation in RAMIE, featuringthe largest collection of vital anatomical structures and surgical instrumentsto date. Handling this diverse set of classes presents challenges, includingclass imbalance and the recognition of complex structures such as nerves. Thisstudy aims to understand the challenges and limitations of currentstate-of-the-art algorithms on this novel dataset and problem. Therefore, webenchmarked eight real-time deep learning models using two pretrainingdatasets. We assessed both traditional and attention-based networks,hypothesizing that attention-based networks better capture global patterns andaddress challenges such as occlusion caused by blood or other tissues. Thebenchmark includes our RAMIE dataset and the publicly available CholecSeg8kdataset, enabling a thorough assessment of surgical segmentation tasks. Ourfindings indicate that pretraining on ADE20k, a dataset for semanticsegmentation, is more effective than pretraining on ImageNet. Furthermore,attention-based models outperform traditional convolutional neural networks,with SegNeXt and Mask2Former achieving higher Dice scores, and Mask2Formeradditionally excelling in average symmetric surface distance.</description><author>Ronald L. P. D. de Jong, Yasmina al Khalil, Tim J. M. Jaspers, Romy C. van Jaarsveld, Gino M. Kuiper, Yiping Li, Richard van Hillegersberg, Jelle P. Ruurda, Marcel Breeuwer, Fons van der Sommen</author><pubDate>Wed, 04 Dec 2024 15:32:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03401v1</guid></item><item><title>Minimal Learning Machine for Multi-Label Learning</title><link>http://arxiv.org/abs/2305.05518v2</link><description>Distance-based supervised method, the minimal learning machine, constructs apredictive model from data by learning a mapping between input and outputdistance matrices. In this paper, we propose new methods and evaluate how theircore component, the distance mapping, can be adapted to multi-label learning.The proposed approach is based on combining the distance mapping with aninverse distance weighting. Although the proposal is one of the simplestmethods in the multi-label learning literature, it achieves state-of-the-artperformance for small to moderate-sized multi-label learning problems. Inaddition to its simplicity, the proposed method is fully deterministic: Itshyper-parameter can be selected via ranking loss-based statistic which has aclosed form, thus avoiding conventional cross-validation-based hyper-parametertuning. In addition, due to its simple linear distance mapping-basedconstruction, we demonstrate that the proposed method can assess theuncertainty of the predictions for multi-label classification, which is avaluable capability for data-centric machine learning pipelines.</description><author>Joonas Hämäläinen, Antoine Hubermont, Amauri Souza, César L. C. Mattos, João P. P. Gomes, Tommi Kärkkäinen</author><pubDate>Wed, 04 Dec 2024 15:32:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05518v2</guid></item><item><title>Implicit Priors Editing in Stable Diffusion via Targeted Token Adjustment</title><link>http://arxiv.org/abs/2412.03400v1</link><description>Implicit assumptions and priors are often necessary in text-to-imagegeneration tasks, especially when textual prompts lack sufficient context.However, these assumptions can sometimes reflect outdated concepts,inaccuracies, or societal bias embedded in the training data. We presentEmbedding-only Editing (Embedit), a method designed to efficiently adjustimplict assumptions and priors in the model without affecting itsinterpretation of unrelated objects or overall performance. Given a "source"prompt (e.g., "rose") that elicits an implicit assumption (e.g., rose is red)and a "destination" prompt that specifies the desired attribute (e.g., "bluerose"), Embedit fine-tunes only the word token embedding (WTE) of the targetobject ("rose") to optimize the last hidden state of text encoder in StableDiffusion, a SOTA text-to-image model. This targeted adjustment preventsunintended effects on other objects in the model's knowledge base, as the WTEsfor unrelated objects and the model weights remain unchanged. Consequently,when a prompt does not contain the edited object, all representations, and themodel outputs are identical to those of the original, unedited model. Ourmethod is highly efficient, modifying only 768 parameters for Stable Diffusion1.4 and 2048 for XL in a single edit, matching the WTE dimension of eachrespective model. This minimal scope, combined with rapid execution, makesEmbedit highly practical for real-world applications. Additionally, changes areeasily reversible by restoring the original WTE layers. Our experimentalresults demonstrate that Embedit consistently outperforms previous methodsacross various models, tasks, and editing scenarios (both single and sequentialmultiple edits), achieving at least a 6.01% improvement (from 87.17% to93.18%).</description><author>Feng He, Chao Zhang, Zhixue Zhao</author><pubDate>Wed, 04 Dec 2024 15:31:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03400v1</guid></item><item><title>Defending Against Repetitive Backdoor Attacks on Semi-supervised Learning through Lens of Rate-Distortion-Perception Trade-off</title><link>http://arxiv.org/abs/2407.10180v2</link><description>Semi-supervised learning (SSL) has achieved remarkable performance with asmall fraction of labeled data by leveraging vast amounts of unlabeled datafrom the Internet. However, this large pool of untrusted data is extremelyvulnerable to data poisoning, leading to potential backdoor attacks. Currentbackdoor defenses are not yet effective against such a vulnerability in SSL. Inthis study, we propose a novel method, Unlabeled Data Purification (UPure), todisrupt the association between trigger patterns and target classes byintroducing perturbations in the frequency domain. By leveraging theRate-Distortion-Perception (RDP) trade-off, we further identify the frequencyband, where the perturbations are added, and justify this selection. Notably,UPure purifies poisoned unlabeled data without the need of extra clean labeleddata. Extensive experiments on four benchmark datasets and five SSL algorithmsdemonstrate that UPure effectively reduces the attack success rate from 99.78%to 0% while maintaining model accuracy. Code is available here:\url{https://github.com/chengyi-chris/UPure}.</description><author>Cheng-Yi Lee, Ching-Chia Kao, Cheng-Han Yeh, Chun-Shien Lu, Chia-Mu Yu, Chu-Song Chen</author><pubDate>Wed, 04 Dec 2024 15:28:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10180v2</guid></item></channel></rss>