<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 14 Sep 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Text-Guided Generation and Editing of Compositional 3D Avatars</title><link>http://arxiv.org/abs/2309.07125v1</link><description>Our goal is to create a realistic 3D facial avatar with hair and accessoriesusing only a text description. While this challenge has attracted significantrecent interest, existing methods either lack realism, produce unrealisticshapes, or do not support editing, such as modifications to the hairstyle. Weargue that existing methods are limited because they employ a monolithicmodeling approach, using a single representation for the head, face, hair, andaccessories. Our observation is that the hair and face, for example, have verydifferent structural qualities that benefit from different representations.Building on this insight, we generate avatars with a compositional model, inwhich the head, face, and upper body are represented with traditional 3Dmeshes, and the hair, clothing, and accessories with neural radiance fields(NeRF). The model-based mesh representation provides a strong geometric priorfor the face region, improving realism while enabling editing of the person'sappearance. By using NeRFs to represent the remaining components, our method isable to model and synthesize parts with complex geometry and appearance, suchas curly hair and fluffy scarves. Our novel system synthesizes thesehigh-quality compositional avatars from text descriptions. The experimentalresults demonstrate that our method, Text-guided generation and Editing ofCompositional Avatars (TECA), produces avatars that are more realistic thanthose of recent methods while being editable because of their compositionalnature. For example, our TECA enables the seamless transfer of compositionalfeatures like hairstyles, scarves, and other accessories between avatars. Thiscapability supports applications such as virtual try-on.</description><author>Hao Zhang, Yao Feng, Peter Kulits, Yandong Wen, Justus Thies, Michael J. Black</author><pubDate>Wed, 13 Sep 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07125v1</guid></item><item><title>RAIN: Your Language Models Can Align Themselves without Finetuning</title><link>http://arxiv.org/abs/2309.07124v1</link><description>Large language models (LLMs) often demonstrate inconsistencies with humanpreferences. Previous research gathered human preference data and then alignedthe pre-trained models using reinforcement learning or instruction tuning, theso-called finetuning step. In contrast, aligning frozen LLMs without any extradata is more appealing. This work explores the potential of the latter setting.We discover that by integrating self-evaluation and rewind mechanisms,unaligned LLMs can directly produce responses consistent with human preferencesvia self-boosting. We introduce a novel inference method, RewindableAuto-regressive INference (RAIN), that allows pre-trained LLMs to evaluatetheir own generation and use the evaluation results to guide backward rewindand forward generation for AI safety. Notably, RAIN operates without the needof extra data for model alignment and abstains from any training, gradientcomputation, or parameter updates; during the self-evaluation phase, the modelreceives guidance on which human preference to align with through afixed-template prompt, eliminating the need to modify the initial prompt.Experimental results evaluated by GPT-4 and humans demonstrate theeffectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rateof LLaMA 30B over vanilla inference from 82% to 97%, while maintaining thehelpfulness rate. Under the leading adversarial attack llm-attacks on Vicuna33B, RAIN establishes a new defense baseline by reducing the attack successrate from 94% to 19%.</description><author>Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, Hongyang Zhang</author><pubDate>Wed, 13 Sep 2023 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07124v1</guid></item><item><title>Tree-Structured Shading Decomposition</title><link>http://arxiv.org/abs/2309.07122v1</link><description>We study inferring a tree-structured representation from a single image forobject shading. Prior work typically uses the parametric or measuredrepresentation to model shading, which is neither interpretable nor easilyeditable. We propose using the shade tree representation, which combines basicshading nodes and compositing methods to factorize object surface shading. Theshade tree representation enables novice users who are unfamiliar with thephysical shading process to edit object shading in an efficient and intuitivemanner. A main challenge in inferring the shade tree is that the inferenceproblem involves both the discrete tree structure and the continuous parametersof the tree nodes. We propose a hybrid approach to address this issue. Weintroduce an auto-regressive inference model to generate a rough estimation ofthe tree structure and node parameters, and then we fine-tune the inferredshade tree through an optimization algorithm. We show experiments on syntheticimages, captured reflectance, real images, and non-realistic vector drawings,allowing downstream applications such as material editing, vectorized shading,and relighting. Project website: https://chen-geng.com/inv-shade-trees</description><author>Chen Geng, Hong-Xing Yu, Sharon Zhang, Maneesh Agrawala, Jiajun Wu</author><pubDate>Wed, 13 Sep 2023 18:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07122v1</guid></item><item><title>Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics</title><link>http://arxiv.org/abs/2309.07120v1</link><description>Multi-modal large language models (MLLMs) are trained based on large languagemodels (LLM), with an enhanced capability to comprehend multi-modal inputs andgenerate textual responses. While they excel in multi-modal tasks, the pure NLPabilities of MLLMs are often underestimated and left untested. In this study,we get out of the box and unveil an intriguing characteristic of MLLMs -- ourpreliminary results suggest that visual instruction tuning, a prevailingstrategy for transitioning LLMs into MLLMs, unexpectedly and interestinglyhelps models attain both improved truthfulness and ethical alignment in thepure NLP context. For example, a visual-instruction-tuned LLaMA2 7B modelsurpasses the performance of the LLaMA2-chat 7B model, fine-tuned with over onemillion human annotations, on TruthfulQA-mc and Ethics benchmarks. Furtheranalysis reveals that the improved alignment can be attributed to the superiorinstruction quality inherent to visual-text data. In releasing our code atgithub.com/UCSC-VLAA/Sight-Beyond-Text, we aspire to foster further explorationinto the intrinsic value of visual-text synergies and, in a broader scope,multi-modal interactions in alignment research.</description><author>Haoqin Tu, Bingchen Zhao, Chen Wei, Cihang Xie</author><pubDate>Wed, 13 Sep 2023 18:57:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07120v1</guid></item><item><title>PILOT: A Pre-Trained Model-Based Continual Learning Toolbox</title><link>http://arxiv.org/abs/2309.07117v1</link><description>While traditional machine learning can effectively tackle a wide range ofproblems, it primarily operates within a closed-world setting, which presentslimitations when dealing with streaming data. As a solution, incrementallearning emerges to address real-world scenarios involving new data's arrival.Recently, pre-training has made significant advancements and garnered theattention of numerous researchers. The strong performance of these pre-trainedmodels (PTMs) presents a promising avenue for developing continual learningalgorithms that can effectively adapt to real-world scenarios. Consequently,exploring the utilization of PTMs in incremental learning has become essential.This paper introduces a pre-trained model-based continual learning toolboxknown as PILOT. On the one hand, PILOT implements some state-of-the-artclass-incremental learning algorithms based on pre-trained models, such as L2P,DualPrompt, and CODA-Prompt. On the other hand, PILOT also fits typicalclass-incremental learning algorithms (e.g., DER, FOSTER, and MEMO) within thecontext of pre-trained models to evaluate their effectiveness.</description><author>Hai-Long Sun, Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan</author><pubDate>Wed, 13 Sep 2023 18:55:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07117v1</guid></item><item><title>Weakly-Supervised Multi-Task Learning for Audio-Visual Speaker Verification</title><link>http://arxiv.org/abs/2309.07115v1</link><description>In this paper, we present a methodology for achieving robust multimodalperson representations optimized for open-set audio-visual speakerverification. Distance Metric Learning (DML) approaches have typicallydominated this problem space, owing to strong performance on new and unseenclasses. In our work, we explored multitask learning techniques to furtherboost performance of the DML approach and show that an auxiliary task with weaklabels can increase the compactness of the learned speaker representation. Wealso extend the Generalized end-to-end loss (GE2E) to multimodal inputs anddemonstrate that it can achieve competitive performance in an audio-visualspace. Finally, we introduce a non-synchronous audio-visual sampling randomstrategy during training time that has shown to improve generalization. Ournetwork achieves state of the art performance for speaker verification,reporting 0.244%, 0.252%, 0.441% Equal Error Rate (EER) on the three officialtrial lists of VoxCeleb1-O/E/H, which is to our knowledge, the best publishedresults on VoxCeleb1-E and VoxCeleb1-H.</description><author>Anith Selvakumar, Homa Fashandi</author><pubDate>Wed, 13 Sep 2023 18:45:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07115v1</guid></item><item><title>Contrastive Deep Encoding Enables Uncertainty-aware Machine-learning-assisted Histopathology</title><link>http://arxiv.org/abs/2309.07113v1</link><description>Deep neural network models can learn clinically relevant features frommillions of histopathology images. However generating high-quality annotationsto train such models for each hospital, each cancer type, and each diagnostictask is prohibitively laborious. On the other hand, terabytes of training data-- while lacking reliable annotations -- are readily available in the publicdomain in some cases. In this work, we explore how these large datasets can beconsciously utilized to pre-train deep networks to encode informativerepresentations. We then fine-tune our pre-trained models on a fraction ofannotated training data to perform specific downstream tasks. We show that ourapproach can reach the state-of-the-art (SOTA) for patch-level classificationwith only 1-10% randomly selected annotations compared to other SOTAapproaches. Moreover, we propose an uncertainty-aware loss function, toquantify the model confidence during inference. Quantified uncertainty helpsexperts select the best instances to label for further training. Ouruncertainty-aware labeling reaches the SOTA with significantly fewerannotations compared to random labeling. Last, we demonstrate how ourpre-trained encoders can surpass current SOTA for whole-slide imageclassification with weak supervision. Our work lays the foundation for data andtask-agnostic pre-trained deep networks with quantified uncertainty.</description><author>Nirhoshan Sivaroopan, Chamuditha Jayanga, Chalani Ekanayake, Hasindri Watawana, Jathurshan Pradeepkumar, Mithunjha Anandakumar, Ranga Rodrigo, Chamira U. S. Edussooriya, Dushan N. Wadduwage</author><pubDate>Wed, 13 Sep 2023 18:37:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07113v1</guid></item><item><title>Optimal transport for automatic alignment of untargeted metabolomic data</title><link>http://arxiv.org/abs/2306.03218v2</link><description>Untargeted metabolomic profiling through liquid chromatography-massspectrometry (LC-MS) measures a vast array of metabolites within biospecimens,advancing drug development, disease diagnosis, and risk prediction. However,the low throughput of LC-MS poses a major challenge for biomarker discovery,annotation, and experimental comparison, necessitating the merging of multipledatasets. Current data pooling methods encounter practical limitations due totheir vulnerability to data variations and hyperparameter dependence. Here weintroduce GromovMatcher, a flexible and user-friendly algorithm thatautomatically combines LC-MS datasets using optimal transport. By capitalizingon feature intensity correlation structures, GromovMatcher delivers superioralignment accuracy and robustness compared to existing approaches. Thisalgorithm scales to thousands of features requiring minimal hyperparametertuning. Applying our method to experimental patient studies of liver andpancreatic cancer, we discover shared metabolic features related to patientalcohol intake, demonstrating how GromovMatcher facilitates the search forbiomarkers associated with lifestyle risk factors linked to several cancertypes.</description><author>Marie Breeur, George Stepaniants, Pekka Keski-Rahkonen, Philippe Rigollet, Vivian Viallon</author><pubDate>Wed, 13 Sep 2023 18:36:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03218v2</guid></item><item><title>Data Augmentation via Subgroup Mixup for Improving Fairness</title><link>http://arxiv.org/abs/2309.07110v1</link><description>In this work, we propose data augmentation via pairwise mixup acrosssubgroups to improve group fairness. Many real-world applications of machinelearning systems exhibit biases across certain groups due tounder-representation or training data that reflects societal biases. Inspiredby the successes of mixup for improving classification performance, we developa pairwise mixup scheme to augment training data and encourage fair andaccurate decision boundaries for all subgroups. Data augmentation for groupfairness allows us to add new samples of underrepresented groups to balancesubpopulations. Furthermore, our method allows us to use the generalizationability of mixup to improve both fairness and accuracy. We compare our proposedmixup to existing data augmentation and bias mitigation approaches on bothsynthetic simulations and real-world benchmark fair classification data,demonstrating that we are able to achieve fair outcomes with robust if notimproved accuracy.</description><author>Madeline Navarro, Camille Little, Genevera I. Allen, Santiago Segarra</author><pubDate>Wed, 13 Sep 2023 18:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07110v1</guid></item><item><title>Characterizing Speed Performance of Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2309.07108v1</link><description>Multi-Agent Reinforcement Learning (MARL) has achieved significant success inlarge-scale AI systems and big-data applications such as smart grids,surveillance, etc. Existing advancements in MARL algorithms focus on improvingthe rewards obtained by introducing various mechanisms for inter-agentcooperation. However, these optimizations are usually compute- andmemory-intensive, thus leading to suboptimal speed performance in end-to-endtraining time. In this work, we analyze the speed performance (i.e.,latency-bounded throughput) as the key metric in MARL implementations.Specifically, we first introduce a taxonomy of MARL algorithms from anacceleration perspective categorized by (1) training scheme and (2)communication method. Using our taxonomy, we identify three state-of-the-artMARL algorithms - Multi-Agent Deep Deterministic Policy Gradient (MADDPG),Target-oriented Multi-agent Communication and Cooperation (ToM2C), andNetworked Multi-Agent RL (NeurComm) - as target benchmark algorithms, andprovide a systematic analysis of their performance bottlenecks on a homogeneousmulti-core CPU platform. We justify the need for MARL latency-boundedthroughput to be a key performance metric in future literature while alsoaddressing opportunities for parallelization and acceleration.</description><author>Samuel Wiggins, Yuan Meng, Rajgopal Kannan, Viktor Prasanna</author><pubDate>Wed, 13 Sep 2023 18:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07108v1</guid></item><item><title>Hardening RGB-D Object Recognition Systems against Adversarial Patch Attacks</title><link>http://arxiv.org/abs/2309.07106v1</link><description>RGB-D object recognition systems improve their predictive performances byfusing color and depth information, outperforming neural network architecturesthat rely solely on colors. While RGB-D systems are expected to be more robustto adversarial examples than RGB-only systems, they have also been proven to behighly vulnerable. Their robustness is similar even when the adversarialexamples are generated by altering only the original images' colors. Differentworks highlighted the vulnerability of RGB-D systems; however, there is alacking of technical explanations for this weakness. Hence, in our work, webridge this gap by investigating the learned deep representation of RGB-Dsystems, discovering that color features make the function learned by thenetwork more complex and, thus, more sensitive to small perturbations. Tomitigate this problem, we propose a defense based on a detection mechanism thatmakes RGB-D systems more robust against adversarial examples. We empiricallyshow that this defense improves the performances of RGB-D systems againstadversarial examples even when they are computed ad-hoc to circumvent thisdetection mechanism, and that is also more effective than adversarial training.</description><author>Yang Zheng, Luca Demetrio, Antonio Emanuele Cinà, Xiaoyi Feng, Zhaoqiang Xia, Xiaoyue Jiang, Ambra Demontis, Battista Biggio, Fabio Roli</author><pubDate>Wed, 13 Sep 2023 18:25:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07106v1</guid></item><item><title>Polygon Intersection-over-Union Loss for Viewpoint-Agnostic Monocular 3D Vehicle Detection</title><link>http://arxiv.org/abs/2309.07104v1</link><description>Monocular 3D object detection is a challenging task because depth informationis difficult to obtain from 2D images. A subset of viewpoint-agnostic monocular3D detection methods also do not explicitly leverage scene homography orgeometry during training, meaning that a model trained thusly can detectobjects in images from arbitrary viewpoints. Such works predict the projectionsof the 3D bounding boxes on the image plane to estimate the location of the 3Dboxes, but these projections are not rectangular so the calculation of IoUbetween these projected polygons is not straightforward. This work proposes anefficient, fully differentiable algorithm for the calculation of IoU betweentwo convex polygons, which can be utilized to compute the IoU between two 3Dbounding box footprints viewed from an arbitrary angle. We test the performanceof the proposed polygon IoU loss (PIoU loss) on three state-of-the-artviewpoint-agnostic 3D detection models. Experiments demonstrate that theproposed PIoU loss converges faster than L1 loss and that in 3D detectionmodels, a combination of PIoU loss and L1 loss gives better results than L1loss alone (+1.64% AP70 for MonoCon on cars, +0.18% AP70 for RTM3D on cars, and+0.83%/+2.46% AP50/AP25 for MonoRCNN on cyclists).</description><author>Derek Gloudemans, Xinxuan Lu, Shepard Xia, Daniel B. Work</author><pubDate>Wed, 13 Sep 2023 18:25:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07104v1</guid></item><item><title>Mitigating Hallucinations and Off-target Machine Translation with Source-Contrastive and Language-Contrastive Decoding</title><link>http://arxiv.org/abs/2309.07098v1</link><description>Hallucinations and off-target translation remain unsolved problems in machinetranslation, especially for low-resource languages and massively multilingualmodels. In this paper, we introduce methods to mitigate both failure cases witha modified decoding objective, without requiring retraining or external models.In source-contrastive decoding, we search for a translation that is probablegiven the correct input, but improbable given a random input segment,hypothesising that hallucinations will be similarly probable given either. Inlanguage-contrastive decoding, we search for a translation that is probable,but improbable given the wrong language indicator token. In experiments onM2M-100 (418M) and SMaLL-100, we find that these methods effectively suppresshallucinations and off-target translations, improving chrF2 by 1.7 and 1.4points on average across 57 tested translation directions. In a proof ofconcept on English--German, we also show that we can suppress off-targettranslations with the Llama 2 chat models, demonstrating the applicability ofthe method to machine translation with LLMs. We release our source code athttps://github.com/ZurichNLP/ContraDecode.</description><author>Rico Sennrich, Jannis Vamvas, Alireza Mohammadshahi</author><pubDate>Wed, 13 Sep 2023 18:15:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07098v1</guid></item><item><title>Bootstrapping Developmental AIs: From Simple Competences to Intelligent Human-Compatible AIs</title><link>http://arxiv.org/abs/2308.04586v7</link><description>The mainstream approaches for creating AIs are the generative and deeplearning AI approaches with large language models (LLMs) and the traditionalmanually constructed symbolic AI approach. Manually constructed AIs aregenerally brittle even in circumscribed domains. Generative AIs make strangemistakes and do not notice them. In both approaches the AIs cannot beinstructed easily, fail to use common sense, and lack curiosity. They haveabstract knowledge but lack social alignment. Developmental AIs may have morepotential. They develop competences like human children do. They start withinnate competences, interact with the environment, and learn from theirinteractions. They interact and learn from people and establish perceptual,cognitive, and common grounding. Developmental AIs have demonstratedcapabilities including visual and multimodal perception, and object recognitionand manipulation. Computational models for abstraction discovery, curiosity,imitation learning, and early language acquisition have also been demonstrated.The promise is that developmental AIs will acquire self-developed and sociallydeveloped competences like people do. They would address the shortcomings ofcurrent mainstream AI approaches, and ultimately lead to sophisticated forms oflearning involving critical reading, provenance evaluation, and hypothesistesting. However, developmental AI projects have not yet fully reached toddlerlevel competencies corresponding to human development at about two years ofage, before their speech is fluent. They do not bridge the Reading Barrier, toskillfully and skeptically draw on online information resources. This positionpaper lays out the logic, prospects, gaps, and challenges for extending thepractice of developmental AIs to create intelligent, human-compatible AIs.</description><author>Mark Stefik, Robert Price</author><pubDate>Wed, 13 Sep 2023 18:13:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04586v7</guid></item><item><title>RadarLCD: Learnable Radar-based Loop Closure Detection Pipeline</title><link>http://arxiv.org/abs/2309.07094v1</link><description>Loop Closure Detection (LCD) is an essential task in robotics and computervision, serving as a fundamental component for various applications acrossdiverse domains. These applications encompass object recognition, imageretrieval, and video analysis. LCD consists in identifying whether a robot hasreturned to a previously visited location, referred to as a loop, and thenestimating the related roto-translation with respect to the analyzed location.Despite the numerous advantages of radar sensors, such as their ability tooperate under diverse weather conditions and provide a wider range of viewcompared to other commonly used sensors (e.g., cameras or LiDARs), integratingradar data remains an arduous task due to intrinsic noise and distortion. Toaddress this challenge, this research introduces RadarLCD, a novel superviseddeep learning pipeline specifically designed for Loop Closure Detection usingthe FMCW Radar (Frequency Modulated Continuous Wave) sensor. RadarLCD, alearning-based LCD methodology explicitly designed for radar systems, makes asignificant contribution by leveraging the pre-trained HERO (Hybrid EstimationRadar Odometry) model. Being originally developed for radar odometry, HERO'sfeatures are used to select key points crucial for LCD tasks. The methodologyundergoes evaluation across a variety of FMCW Radar dataset scenes, and it iscompared to state-of-the-art systems such as Scan Context for Place Recognitionand ICP for Loop Closure. The results demonstrate that RadarLCD surpasses thealternatives in multiple aspects of Loop Closure Detection.</description><author>Mirko Usuelli, Matteo Frosi, Paolo Cudrano, Simone Mentasti, Matteo Matteucci</author><pubDate>Wed, 13 Sep 2023 18:10:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07094v1</guid></item><item><title>Perseus: A Simple and Optimal High-Order Method for Variational Inequalities</title><link>http://arxiv.org/abs/2205.03202v5</link><description>This paper settles an open and challenging question pertaining to the designof simple and optimal high-order methods for solving smooth and monotonevariational inequalities (VIs). A VI involves finding $x^\star \in \mathcal{X}$such that $\langle F(x), x - x^\star\rangle \geq 0$ for all $x \in\mathcal{X}$. We consider the setting in which $F$ is smooth with up to$(p-1)^{th}$-order derivatives. For $p = 2$, the cubic regularized Newtonmethod was extended to VIs with a global rate of $O(\epsilon^{-1})$. Animproved rate of $O(\epsilon^{-2/3}\log\log(1/\epsilon))$ can be obtained viaan alternative second-order method, but this method requires a nontrivialline-search procedure as an inner loop. Similarly, high-order methods based online-search procedures have been shown to achieve a rate of$O(\epsilon^{-2/(p+1)}\log\log(1/\epsilon))$. As emphasized by Nesterov,however, such procedures do not necessarily imply practical applicability inlarge-scale applications, and it would be desirable to complement these resultswith a simple high-order VI method that retains the optimality of the morecomplex methods. We propose a $p^{th}$-order method that does \textit{not}require any line search procedure and provably converges to a weak solution ata rate of $O(\epsilon^{-2/(p+1)})$. We prove that our $p^{th}$-order method isoptimal in the monotone setting by establishing a matching lower bound under ageneralized linear span assumption. Our method with restarting attains a linearrate for smooth and strictly monotone VIs and a local superlinear rate forsmooth and strongly monotone VIs. Our method also achieves a global rate of$O(\epsilon^{-2/p})$ for solving smooth and nonmonotone VIs satisfying theMinty condition and when augmented with restarting it attains a global linearand local superlinear rate for smooth and nonmonotone VIs satisfying thestrictly/strong Minty condition.</description><author>Tianyi Lin, Michael. I. Jordan</author><pubDate>Wed, 13 Sep 2023 18:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.03202v5</guid></item><item><title>The Quality-Diversity Transformer: Generating Behavior-Conditioned Trajectories with Decision Transformers</title><link>http://arxiv.org/abs/2303.16207v3</link><description>In the context of neuroevolution, Quality-Diversity algorithms have proveneffective in generating repertoires of diverse and efficient policies byrelying on the definition of a behavior space. A natural goal induced by thecreation of such a repertoire is trying to achieve behaviors on demand, whichcan be done by running the corresponding policy from the repertoire. However,in uncertain environments, two problems arise. First, policies can lackrobustness and repeatability, meaning that multiple episodes under slightlydifferent conditions often result in very different behaviors. Second, due tothe discrete nature of the repertoire, solutions vary discontinuously. Here wepresent a new approach to achieve behavior-conditioned trajectory generationbased on two mechanisms: First, MAP-Elites Low-Spread (ME-LS), which constrainsthe selection of solutions to those that are the most consistent in thebehavior space. Second, the Quality-Diversity Transformer (QDT), aTransformer-based model conditioned on continuous behavior descriptors, whichtrains on a dataset generated by policies from a ME-LS repertoire and learns toautoregressively generate sequences of actions that achieve target behaviors.Results show that ME-LS produces consistent and robust policies, and that itscombination with the QDT yields a single policy capable of achieving diversebehaviors on demand with high accuracy.</description><author>Valentin Macé, Raphaël Boige, Felix Chalumeau, Thomas Pierrot, Guillaume Richard, Nicolas Perrin-Gilbert</author><pubDate>Wed, 13 Sep 2023 18:07:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16207v3</guid></item><item><title>Developing a Novel Image Marker to Predict the Responses of Neoadjuvant Chemotherapy (NACT) for Ovarian Cancer Patients</title><link>http://arxiv.org/abs/2309.07087v1</link><description>Objective: Neoadjuvant chemotherapy (NACT) is one kind of treatment foradvanced stage ovarian cancer patients. However, due to the nature of tumorheterogeneity, the patients' responses to NACT varies significantly amongdifferent subgroups. To address this clinical challenge, the purpose of thisstudy is to develop a novel image marker to achieve high accuracy responseprediction of the NACT at an early stage. Methods: For this purpose, we firstcomputed a total of 1373 radiomics features to quantify the tumorcharacteristics, which can be grouped into three categories: geometric,intensity, and texture features. Second, all these features were optimized byprincipal component analysis algorithm to generate a compact and informativefeature cluster. Using this cluster as the input, an SVM based classifier wasdeveloped and optimized to create a final marker, indicating the likelihood ofthe patient being responsive to the NACT treatment. To validate this scheme, atotal of 42 ovarian cancer patients were retrospectively collected. A nestedleave-one-out cross-validation was adopted for model performance assessment.Results: The results demonstrate that the new method yielded an AUC (area underthe ROC [receiver characteristic operation] curve) of 0.745. Meanwhile, themodel achieved overall accuracy of 76.2%, positive predictive value of 70%, andnegative predictive value of 78.1%. Conclusion: This study provides meaningfulinformation for the development of radiomics based image markers in NACTresponse prediction.</description><author>Ke Zhang, Neman Abdoli, Patrik Gilley, Youkabed Sadri, Xuxin Chen, Theresa C. Thai, Lauren Dockery, Kathleen Moore, Robert S. Mannel, Yuchen Qiu</author><pubDate>Wed, 13 Sep 2023 17:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07087v1</guid></item><item><title>Mitigating Group Bias in Federated Learning for Heterogeneous Devices</title><link>http://arxiv.org/abs/2309.07085v1</link><description>Federated Learning is emerging as a privacy-preserving model trainingapproach in distributed edge applications. As such, most edge deployments areheterogeneous in nature i.e., their sensing capabilities and environments varyacross deployments. This edge heterogeneity violates the independence andidentical distribution (IID) property of local data across clients and producesbiased global models i.e. models that contribute to unfair decision-making anddiscrimination against a particular community or a group. Existing biasmitigation techniques only focus on bias generated from label heterogeneity innon-IID data without accounting for domain variations due to featureheterogeneity and do not address global group-fairness property. Our work proposes a group-fair FL framework that minimizes group-bias whilepreserving privacy and without resource utilization overhead. Our main idea isto leverage average conditional probabilities to compute a cross-domain group\textit{importance weights} derived from heterogeneous training data tooptimize the performance of the worst-performing group using a modifiedmultiplicative weights update method. Additionally, we propose regularizationtechniques to minimize the difference between the worst and best-performinggroups while making sure through our thresholding mechanism to strike a balancebetween bias reduction and group performance degradation. Our evaluation ofhuman emotion recognition and image classification benchmarks assesses the fairdecision-making of our framework in real-world heterogeneous settings.</description><author>Khotso Selialia, Yasra Chandio, Fatima M. Anwar</author><pubDate>Wed, 13 Sep 2023 17:53:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07085v1</guid></item><item><title>Edge-MoE: Memory-Efficient Multi-Task Vision Transformer Architecture with Task-level Sparsity via Mixture-of-Experts</title><link>http://arxiv.org/abs/2305.18691v2</link><description>Computer vision researchers are embracing two promising paradigms: VisionTransformers (ViTs) and Multi-task Learning (MTL), which both show greatperformance but are computation-intensive, given the quadratic complexity ofself-attention in ViT and the need to activate an entire large MTL model forone task. M$^3$ViT is the latest multi-task ViT model that introducesmixture-of-experts (MoE), where only a small portion of subnetworks ("experts")are sparsely and dynamically activated based on the current task. M$^3$ViTachieves better accuracy and over 80% computation reduction but leaveschallenges for efficient deployment on FPGA. Our work, dubbed Edge-MoE, solves the challenges to introduce the firstend-to-end FPGA accelerator for multi-task ViT with a collection ofarchitectural innovations, including (1) a novel reordering mechanism forself-attention, which requires only constant bandwidth regardless of the targetparallelism; (2) a fast single-pass softmax approximation; (3) an accurate andlow-cost GELU approximation; (4) a unified and flexible computing unit that isshared by almost all computational layers to maximally reduce resource usage;and (5) uniquely for M$^3$ViT, a novel patch reordering method to eliminatememory access overhead. Edge-MoE achieves 2.24x and 4.90x better energyefficiency comparing with GPU and CPU, respectively. A real-time videodemonstration is available online, along with our open-source code writtenusing High-Level Synthesis.</description><author>Rishov Sarkar, Hanxue Liang, Zhiwen Fan, Zhangyang Wang, Cong Hao</author><pubDate>Wed, 13 Sep 2023 17:52:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18691v2</guid></item><item><title>SupFusion: Supervised LiDAR-Camera Fusion for 3D Object Detection</title><link>http://arxiv.org/abs/2309.07084v1</link><description>In this paper, we propose a novel training strategy called SupFusion, whichprovides an auxiliary feature level supervision for effective LiDAR-Camerafusion and significantly boosts detection performance. Our strategy involves adata enhancement method named Polar Sampling, which densifies sparse objectsand trains an assistant model to generate high-quality features as thesupervision. These features are then used to train the LiDAR-Camera fusionmodel, where the fusion feature is optimized to simulate the generatedhigh-quality features. Furthermore, we propose a simple yet effective deepfusion module, which contiguously gains superior performance compared withprevious fusion methods with SupFusion strategy. In such a manner, our proposalshares the following advantages. Firstly, SupFusion introduces auxiliaryfeature-level supervision which could boost LiDAR-Camera detection performancewithout introducing extra inference costs. Secondly, the proposed deep fusioncould continuously improve the detector's abilities. Our proposed SupFusion anddeep fusion module is plug-and-play, we make extensive experiments todemonstrate its effectiveness. Specifically, we gain around 2% 3D mAPimprovements on KITTI benchmark based on multiple LiDAR-Camera 3D detectors.</description><author>Yiran Qin, Chaoqun Wang, Zijian Kang, Ningning Ma, Zhen Li, Ruimao Zhang</author><pubDate>Wed, 13 Sep 2023 17:52:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07084v1</guid></item><item><title>NExT-GPT: Any-to-Any Multimodal LLM</title><link>http://arxiv.org/abs/2309.05519v2</link><description>While recently Multimodal Large Language Models (MM-LLMs) have made excitingstrides, they mostly fall prey to the limitation of only input-side multimodalunderstanding, without the ability to produce content in multiple modalities.As we humans always perceive the world and communicate with people throughvarious modalities, developing any-to-any MM-LLMs capable of accepting anddelivering content in any modality becomes essential to human-level AI. To fillthe gap, we present an end-to-end general-purpose any-to-any MM-LLM system,NExT-GPT. We connect an LLM with multimodal adaptors and different diffusiondecoders, enabling NExT-GPT to perceive inputs and generate outputs inarbitrary combinations of text, images, videos, and audio. By leveraging theexisting well-trained highly-performing encoders and decoders, NExT-GPT istuned with only a small amount of parameter (1%) of certain projection layers,which not only benefits low-cost training and also facilitates convenientexpansion to more potential modalities. Moreover, we introduce amodality-switching instruction tuning (MosIT) and manually curate ahigh-quality dataset for MosIT, based on which NExT-GPT is empowered withcomplex cross-modal semantic understanding and content generation. Overall, ourresearch showcases the promising possibility of building an AI agent capable ofmodeling universal modalities, paving the way for more human-like AI researchin the community. Project page: https://next-gpt.github.io/</description><author>Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua</author><pubDate>Wed, 13 Sep 2023 17:49:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05519v2</guid></item><item><title>Can Whisper perform speech-based in-context learning</title><link>http://arxiv.org/abs/2309.07081v1</link><description>This paper investigates the in-context learning abilities of the Whisperautomatic speech recognition (ASR) models released by OpenAI. A novelspeech-based in-context learning (SICL) approach is proposed for test-timeadaptation, which can reduce the word error rates (WERs) with only a smallnumber of labelled speech samples without gradient descent. Language-leveladaptation experiments using Chinese dialects showed that when applying SICL toisolated word ASR, consistent and considerable relative WER reductions can beachieved using Whisper models of any size on two dialects, which is on average32.3%. A k-nearest-neighbours-based in-context example selection technique canbe applied to further improve the efficiency of SICL, which can increase theaverage relative WER reduction to 36.4%. The findings are verified usingspeaker adaptation or continuous speech recognition tasks, and both achievedconsiderable relative WER reductions. Detailed quantitative analyses are alsoprovided to shed light on SICL's adaptability to phonological variances anddialect-specific lexical nuances.</description><author>Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang</author><pubDate>Wed, 13 Sep 2023 17:46:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07081v1</guid></item><item><title>Imprecise Bayesian Neural Networks</title><link>http://arxiv.org/abs/2302.09656v3</link><description>Uncertainty quantification and robustness to distribution shifts areimportant goals in machine learning and artificial intelligence. AlthoughBayesian Neural Networks (BNNs) allow for uncertainty in the predictions to beassessed, different sources of uncertainty are indistinguishable. We presentImprecise Bayesian Neural Networks (IBNNs); they generalize and overcome someof the drawbacks of standard BNNs. These latter are trained using a singleprior and likelihood distributions, whereas IBNNs are trained using credalprior and likelihood sets. They allow to distinguish between aleatoric andepistemic uncertainties, and to quantify them. In addition, IBNNs are morerobust than BNNs to prior and likelihood misspecification, and to distributionshift. They can also be used to compute sets of outcomes that enjoyprobabilistic guarantees. We apply IBNNs to two case studies. One, for motionprediction in autonomous driving scenarios, and two, to model blood glucose andinsulin dynamics for artificial pancreas control. We show that IBNNs performsbetter when compared to an ensemble of BNNs benchmark.</description><author>Michele Caprio, Souradeep Dutta, Kuk Jin Jang, Vivian Lin, Radoslav Ivanov, Oleg Sokolsky, Insup Lee</author><pubDate>Wed, 13 Sep 2023 17:40:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09656v3</guid></item><item><title>The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in Deep Learning</title><link>http://arxiv.org/abs/2309.07072v1</link><description>In this work, we assess the theoretical limitations of determining guaranteedstability and accuracy of neural networks in classification tasks. We considerclassical distribution-agnostic framework and algorithms minimising empiricalrisks and potentially subjected to some weights regularisation. We show thatthere is a large family of tasks for which computing and verifying ideal stableand accurate neural networks in the above settings is extremely challenging, ifat all possible, even when such ideal solutions exist within the given class ofneural architectures.</description><author>Alexander Bastounis, Alexander N. Gorban, Anders C. Hansen, Desmond J. Higham, Danil Prokhorov, Oliver Sutton, Ivan Y. Tyukin, Qinghua Zhou</author><pubDate>Wed, 13 Sep 2023 17:33:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07072v1</guid></item><item><title>FAIR: Frequency-aware Image Restoration for Industrial Visual Anomaly Detection</title><link>http://arxiv.org/abs/2309.07068v1</link><description>Image reconstruction-based anomaly detection models are widely explored inindustrial visual inspection. However, existing models usually suffer from thetrade-off between normal reconstruction fidelity and abnormal reconstructiondistinguishability, which damages the performance. In this paper, we find thatthe above trade-off can be better mitigated by leveraging the distinctfrequency biases between normal and abnormal reconstruction errors. To thisend, we propose Frequency-aware Image Restoration (FAIR), a novelself-supervised image restoration task that restores images from theirhigh-frequency components. It enables precise reconstruction of normal patternswhile mitigating unfavorable generalization to anomalies. Using only a simplevanilla UNet, FAIR achieves state-of-the-art performance with higher efficiencyon various defect detection datasets. Code: https://github.com/liutongkun/FAIR.</description><author>Tongkun Liu, Bing Li, Xiao Du, Bingke Jiang, Leqi Geng, Feiyang Wang, Zhuo Zhao</author><pubDate>Wed, 13 Sep 2023 17:28:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07068v1</guid></item><item><title>Boost Video Frame Interpolation via Motion Adaptation</title><link>http://arxiv.org/abs/2306.13933v2</link><description>Video frame interpolation (VFI) is a challenging task that aims to generateintermediate frames between two consecutive frames in a video. Existinglearning-based VFI methods have achieved great success, but they still sufferfrom limited generalization ability due to the limited motion distribution oftraining datasets. In this paper, we propose a novel optimization-based VFImethod that can adapt to unseen motions at test time. Our method is based on acycle-consistency adaptation strategy that leverages the motion characteristicsamong video frames. We also introduce a lightweight adapter that can beinserted into the motion estimation module of existing pre-trained VFI modelsto improve the efficiency of adaptation. Extensive experiments on variousbenchmarks demonstrate that our method can boost the performance of two-frameVFI models, outperforming the existing state-of-the-art methods, even thosethat use extra input.</description><author>Haoning Wu, Xiaoyun Zhang, Weidi Xie, Ya Zhang, Yanfeng Wang</author><pubDate>Wed, 13 Sep 2023 17:26:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13933v2</guid></item><item><title>Physics-informed Bayesian inference of external potentials in classical density-functional theory</title><link>http://arxiv.org/abs/2309.07065v1</link><description>The swift progression of machine learning (ML) have not gone unnoticed in therealm of statistical mechanics. ML techniques have attracted attention by theclassical density-functional theory (DFT) community, as they enable discoveryof free-energy functionals to determine the equilibrium-density profile of amany-particle system. Within DFT, the external potential accounts for theinteraction of the many-particle system with an external field, thus, affectingthe density distribution. In this context, we introduce a statistical-learningframework to infer the external potential exerted on a many-particle system. Wecombine a Bayesian inference approach with the classical DFT apparatus toreconstruct the external potential, yielding a probabilistic description of theexternal potential functional form with inherent uncertainty quantification.Our framework is exemplified with a grand-canonical one-dimensional particleensemble with excluded volume interactions in a confined geometry. The requiredtraining dataset is generated using a Monte Carlo (MC) simulation where theexternal potential is applied to the grand-canonical ensemble. The resultingparticle coordinates from the MC simulation are fed into the learning frameworkto uncover the external potential. This eventually allows us to compute theequilibrium density profile of the system by using the tools of DFT. Ourapproach benchmarks the inferred density against the exact one calculatedthrough the DFT formulation with the true external potential. The proposedBayesian procedure accurately infers the external potential and the densityprofile. We also highlight the external-potential uncertainty quantificationconditioned on the amount of available simulated data. The seemingly simplecase study introduced in this work might serve as a prototype for studying awide variety of applications, including adsorption and capillarity.</description><author>Antonio Malpica-Morales, Peter Yatsyshin, Miguel A. Duran-Olivencia, Serafim Kalliadasis</author><pubDate>Wed, 13 Sep 2023 17:26:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07065v1</guid></item><item><title>A Comprehensive Analysis of the Role of Artificial Intelligence and Machine Learning in Modern Digital Forensics and Incident Response</title><link>http://arxiv.org/abs/2309.07064v1</link><description>In the dynamic landscape of digital forensics, the integration of ArtificialIntelligence (AI) and Machine Learning (ML) stands as a transformativetechnology, poised to amplify the efficiency and precision of digital forensicsinvestigations. However, the use of ML and AI in digital forensics is still inits nascent stages. As a result, this paper gives a thorough and in-depthanalysis that goes beyond a simple survey and review. The goal is to lookclosely at how AI and ML techniques are used in digital forensics and incidentresponse. This research explores cutting-edge research initiatives that crossdomains such as data collection and recovery, the intricate reconstruction ofcybercrime timelines, robust big data analysis, pattern recognition,safeguarding the chain of custody, and orchestrating responsive strategies tohacking incidents. This endeavour digs far beneath the surface to unearth theintricate ways AI-driven methodologies are shaping these crucial facets ofdigital forensics practice. While the promise of AI in digital forensics isevident, the challenges arising from increasing database sizes and evolvingcriminal tactics necessitate ongoing collaborative research and refinementwithin the digital forensics profession. This study examines the contributions,limitations, and gaps in the existing research, shedding light on the potentialand limitations of AI and ML techniques. By exploring these different researchareas, we highlight the critical need for strategic planning, continualresearch, and development to unlock AI's full potential in digital forensicsand incident response. Ultimately, this paper underscores the significance ofAI and ML integration in digital forensics, offering insights into theirbenefits, drawbacks, and broader implications for tackling modern cyberthreats.</description><author>Dipo Dunsin, Mohamed C. Ghanem, Karim Ouazzane, Vassil Vassilev</author><pubDate>Wed, 13 Sep 2023 17:23:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07064v1</guid></item><item><title>Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning</title><link>http://arxiv.org/abs/2308.06851v2</link><description>Throughout the analytical revolution that has occurred in the NBA, thedevelopment of specific metrics and formulas has given teams, coaches, andplayers a new way to see the game. However - the question arises - how can weverify any metrics? One method would simply be eyeball approximation (tryingout many different gameplans) and/or trial and error - an estimation-based andcostly approach. Another approach is to try to model already existing metricswith a unique set of features using machine learning techniques. The key tothis approach is that with these features that are selected, we can try togauge the effectiveness of these features combined, rather than usingindividual analysis in simple metric evaluation. If we have an accurate model,it can particularly help us determine the specifics of gameplan execution. Inthis paper, the statistic ORTG (Offensive Rating, developed by Dean Oliver) wasfound to have a correlation with different NBA playtypes using both a linearregression model and a neural network regression model, although ultimately, aneural network worked slightly better than linear regression. Using theaccuracy of the models as a justification, the next step was to optimize theoutput of the model with test examples, which would demonstrate the combinationof features to best achieve a highly functioning offense.</description><author>Eamon Mukhopadhyay</author><pubDate>Wed, 13 Sep 2023 17:21:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06851v2</guid></item><item><title>Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces</title><link>http://arxiv.org/abs/2303.00028v5</link><description>The sensor placement problem is a common problem that arises when monitoringcorrelated phenomena, such as temperature and precipitation. Existingapproaches to this problem typically use discrete optimization methods, whichare computationally expensive and cannot scale to large problems. We addressthe sensor placement problem in correlated environments by reducing it to aregression problem that can be efficiently solved using sparse Gaussianprocesses (SGPs). Our approach can handle both discrete sensor placementproblems-where sensors are limited to a subset of a given set of locations-andcontinuous sensor placement problems-where sensors can be placed anywhere in abounded continuous region. Our experimental results on three real-worlddatasets show that our approach generates sensor placements that result inreconstruction quality that is consistently on par or better than the priorstate-of-the-art approach while being significantly faster. Our computationallyefficient approach enables both large-scale sensor placement and fast roboticsensor placement for informative path planning algorithms.</description><author>Kalvik Jakkala, Srinivas Akella</author><pubDate>Wed, 13 Sep 2023 17:21:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00028v5</guid></item><item><title>Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments</title><link>http://arxiv.org/abs/2309.07056v1</link><description>Despite their promise to facilitate new scientific discoveries, theopaqueness of neural networks presents a challenge in interpreting the logicbehind their findings. Here, we use a eXplainable-AI (XAI) technique called$inception$ or $deep$ $dreaming$, which has been invented in machine learningfor computer vision. We use this techniques to explore what neural networkslearn about quantum optics experiments. Our story begins by training a deepneural networks on the properties of quantum systems. Once trained, we "invert"the neural network -- effectively asking how it imagines a quantum system witha specific property, and how it would continuously modify the quantum system tochange a property. We find that the network can shift the initial distributionof properties of the quantum system, and we can conceptualize the learnedstrategies of the neural network. Interestingly, we find that, in the firstlayers, the neural network identifies simple properties, while in the deeperones, it can identify complex quantum structures and even quantum entanglement.This is in reminiscence of long-understood properties known in computer vision,which we now identify in a complex natural science task. Our approach could beuseful in a more interpretable way to develop new advanced AI-based scientificdiscovery techniques in quantum physics.</description><author>Tareq Jaouni, Sören Arlt, Carlos Ruiz-Gonzalez, Ebrahim Karimi, Xuemei Gu, Mario Krenn</author><pubDate>Wed, 13 Sep 2023 17:13:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07056v1</guid></item><item><title>Spaiche: Extending State-of-the-Art ASR Models to Swiss German Dialects</title><link>http://arxiv.org/abs/2304.11075v2</link><description>Recent breakthroughs in NLP largely increased the presence of ASR systems inour daily lives. However, for many low-resource languages, ASR models stillneed to be improved due in part to the difficulty of acquiring pertinent data.This project aims to help advance research in ASR models for Swiss Germandialects, by providing insights about the performance of state-of-the-art ASRmodels on recently published Swiss German speech datasets. We propose a novelloss that takes into account the semantic distance between the predicted andthe ground-truth labels. We outperform current state-of-the-art results byfine-tuning OpenAI's Whisper model on Swiss-German datasets.</description><author>Clement Sicard, Kajetan Pyszkowski, Victor Gillioz</author><pubDate>Wed, 13 Sep 2023 17:12:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11075v2</guid></item><item><title>Aggregating Long-term Sharp Features via Hybrid Transformers for Video Deblurring</title><link>http://arxiv.org/abs/2309.07054v1</link><description>Video deblurring methods, aiming at recovering consecutive sharp frames froma given blurry video, usually assume that the input video suffers fromconsecutively blurry frames. However, in real-world blurry videos taken bymodern imaging devices, sharp frames usually appear in the given video, thusmaking temporal long-term sharp features available for facilitating therestoration of a blurry frame. In this work, we propose a video deblurringmethod that leverages both neighboring frames and present sharp frames usinghybrid Transformers for feature aggregation. Specifically, we first train ablur-aware detector to distinguish between sharp and blurry frames. Then, awindow-based local Transformer is employed for exploiting features fromneighboring frames, where cross attention is beneficial for aggregatingfeatures from neighboring frames without explicit spatial alignment. Toaggregate long-term sharp features from detected sharp frames, we utilize aglobal Transformer with multi-scale matching capability. Moreover, our methodcan easily be extended to event-driven video deblurring by incorporating anevent fusion module into the global Transformer. Extensive experiments onbenchmark datasets demonstrate that our proposed method outperformsstate-of-the-art video deblurring methods as well as event-driven videodeblurring methods in terms of quantitative metrics and visual quality. Thesource code and trained models are available athttps://github.com/shangwei5/STGTN.</description><author>Dongwei Ren, Wei Shang, Yi Yang, Wangmeng Zuo</author><pubDate>Wed, 13 Sep 2023 17:12:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07054v1</guid></item><item><title>Pearl's and Jeffrey's Update as Modes of Learning in Probabilistic Programming</title><link>http://arxiv.org/abs/2309.07053v1</link><description>The concept of updating a probability distribution in the light of newevidence lies at the heart of statistics and machine learning. Pearl's andJeffrey's rule are two natural update mechanisms which lead to differentoutcomes, yet the similarities and differences remain mysterious. This paperclarifies their relationship in several ways: via separate descriptions of thetwo update mechanisms in terms of probabilistic programs and samplingsemantics, and via different notions of likelihood (for Pearl and for Jeffrey).Moreover, it is shown that Jeffrey's update rule arises via variationalinference. In terms of categorical probability theory, this amounts to ananalysis of the situation in terms of the behaviour of the multiset functor,extended to the Kleisli category of the distribution monad.</description><author>Bart Jacobs, Dario Stein</author><pubDate>Wed, 13 Sep 2023 17:09:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07053v1</guid></item><item><title>Switch and Conquer: Efficient Algorithms By Switching Stochastic Gradient Oracles For Decentralized Saddle Point Problems</title><link>http://arxiv.org/abs/2309.00997v2</link><description>We consider a class of non-smooth strongly convex-strongly concave saddlepoint problems in a decentralized setting without a central server. To solve aconsensus formulation of problems in this class, we develop an inexact primaldual hybrid gradient (inexact PDHG) procedure that allows generic gradientcomputation oracles to update the primal and dual variables. We firstinvestigate the performance of inexact PDHG with stochastic variance reductiongradient (SVRG) oracle. Our numerical study uncovers a significant phenomenonof initial conservative progress of iterates of IPDHG with SVRG oracle. Totackle this, we develop a simple and effective switching idea, where ageneralized stochastic gradient (GSG) computation oracle is employed to hastenthe iterates' progress to a saddle point solution during the initial phase ofupdates, followed by a switch to the SVRG oracle at an appropriate juncture.The proposed algorithm is named Decentralized Proximal Switching StochasticGradient method with Compression (C-DPSSG), and is proven to converge to an$\epsilon$-accurate saddle point solution with linear rate. Apart fromdelivering highly accurate solutions, our study reveals that utilizing the bestconvergence phases of GSG and SVRG oracles makes C-DPSSG well suited forobtaining solutions of low/medium accuracy faster, useful for certainapplications. Numerical experiments on two benchmark machine learningapplications show C-DPSSG's competitive performance which validate ourtheoretical findings. The codes used in the experiments can be found\href{https://github.com/chhavisharma123/C-DPSSG-CDC2023}{here}.</description><author>Chhavi Sharma, Vishnu Narayanan, P. Balamurugan</author><pubDate>Wed, 13 Sep 2023 17:07:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00997v2</guid></item><item><title>UnifiedGesture: A Unified Gesture Synthesis Model for Multiple Skeletons</title><link>http://arxiv.org/abs/2309.07051v1</link><description>The automatic co-speech gesture generation draws much attention in computeranimation. Previous works designed network structures on individual datasets,which resulted in a lack of data volume and generalizability across differentmotion capture standards. In addition, it is a challenging task due to the weakcorrelation between speech and gestures. To address these problems, we presentUnifiedGesture, a novel diffusion model-based speech-driven gesture synthesisapproach, trained on multiple gesture datasets with different skeletons.Specifically, we first present a retargeting network to learn latenthomeomorphic graphs for different motion capture standards, unifying therepresentations of various gestures while extending the dataset. We thencapture the correlation between speech and gestures based on a diffusion modelarchitecture using cross-local attention and self-attention to generate betterspeech-matched and realistic gestures. To further align speech and gesture andincrease diversity, we incorporate reinforcement learning on the discretegesture units with a learned reward function. Extensive experiments show thatUnifiedGesture outperforms recent approaches on speech-driven gesturegeneration in terms of CCA, FGD, and human-likeness. All code, pre-trainedmodels, databases, and demos are available to the public athttps://github.com/YoungSeng/UnifiedGesture.</description><author>Sicheng Yang, Zilin Wang, Zhiyong Wu, Minglei Li, Zhensong Zhang, Qiaochu Huang, Lei Hao, Songcen Xu, Xiaofei Wu, changpeng yang, Zonghong Dai</author><pubDate>Wed, 13 Sep 2023 17:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07051v1</guid></item><item><title>An Extreme Learning Machine-Based Method for Computational PDEs in Higher Dimensions</title><link>http://arxiv.org/abs/2309.07049v1</link><description>We present two effective methods for solving high-dimensional partialdifferential equations (PDE) based on randomized neural networks. Motivated bythe universal approximation property of this type of networks, both methodsextend the extreme learning machine (ELM) approach from low to high dimensions.With the first method the unknown solution field in $d$ dimensions isrepresented by a randomized feed-forward neural network, in which thehidden-layer parameters are randomly assigned and fixed while the output-layerparameters are trained. The PDE and the boundary/initial conditions, as well asthe continuity conditions (for the local variant of the method), are enforcedon a set of random interior/boundary collocation points. The resultant linearor nonlinear algebraic system, through its least squares solution, provides thetrained values for the network parameters. With the second method thehigh-dimensional PDE problem is reformulated through a constrained expressionbased on an Approximate variant of the Theory of Functional Connections(A-TFC), which avoids the exponential growth in the number of terms of TFC asthe dimension increases. The free field function in the A-TFC constrainedexpression is represented by a randomized neural network and is trained by aprocedure analogous to the first method. We present ample numerical simulationsfor a number of high-dimensional linear/nonlinear stationary/dynamic PDEs todemonstrate their performance. These methods can produce accurate solutions tohigh-dimensional PDEs, in particular with their errors reaching levels not farfrom the machine accuracy for relatively lower dimensions. Compared with thephysics-informed neural network (PINN) method, the current method is bothcost-effective and more accurate for high-dimensional PDEs.</description><author>Yiran Wang, Suchuan Dong</author><pubDate>Wed, 13 Sep 2023 16:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07049v1</guid></item><item><title>SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions</title><link>http://arxiv.org/abs/2309.07045v1</link><description>With the rapid development of Large Language Models (LLMs), increasingattention has been paid to their safety concerns. Consequently, evaluating thesafety of LLMs has become an essential task for facilitating the broadapplications of LLMs. Nevertheless, the absence of comprehensive safetyevaluation benchmarks poses a significant impediment to effectively assess andenhance the safety of LLMs. In this work, we present SafetyBench, acomprehensive benchmark for evaluating the safety of LLMs, which comprises11,435 diverse multiple choice questions spanning across 7 distinct categoriesof safety concerns. Notably, SafetyBench also incorporates both Chinese andEnglish data, facilitating the evaluation in both languages. Our extensivetests over 25 popular Chinese and English LLMs in both zero-shot and few-shotsettings reveal a substantial performance advantage for GPT-4 over itscounterparts, and there is still significant room for improving the safety ofcurrent LLMs. We believe SafetyBench will enable fast and comprehensiveevaluation of LLMs' safety, and foster the development of safer LLMs. Data andevaluation guidelines are available at https://github.com/thu-coai/SafetyBench.Submission entrance and leaderboard are available athttps://llmbench.ai/safety.</description><author>Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, Minlie Huang</author><pubDate>Wed, 13 Sep 2023 16:56:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07045v1</guid></item><item><title>Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test</title><link>http://arxiv.org/abs/2309.02422v2</link><description>Maximum mean discrepancy (MMD) refers to a general class of nonparametrictwo-sample tests that are based on maximizing the mean difference over samplesfrom one distribution $P$ versus another $Q$, over all choices of datatransformations $f$ living in some function space $\mathcal{F}$. Inspired byrecent work that connects what are known as functions of $\textit{Radon boundedvariation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we studythe MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space ofa given smoothness order $k \geq 0$. This test, which we refer to as the$\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as ageneralization of the well-known and classical Kolmogorov-Smirnov (KS) test tomultiple dimensions and higher orders of smoothness. It is also intimatelyconnected to neural networks: we prove that the witness in the RKS test -- thefunction $f$ achieving the maximum mean difference -- is always a ridge splineof degree $k$, i.e., a single neuron in a neural network. This allows us toleverage the power of modern deep learning toolkits to (approximately) optimizethe criterion that underlies the RKS test. We prove that the RKS test hasasymptotically full power at distinguishing any distinct pair $P \not= Q$ ofdistributions, derive its asymptotic null distribution, and carry out extensiveexperiments to elucidate the strengths and weakenesses of the RKS test versusthe more traditional kernel MMD test.</description><author>Seunghoon Paik, Michael Celentano, Alden Green, Ryan J. Tibshirani</author><pubDate>Wed, 13 Sep 2023 16:56:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02422v2</guid></item><item><title>Efficient Reinforcement Learning for Jumping Monopods</title><link>http://arxiv.org/abs/2309.07038v1</link><description>In this work, we consider the complex control problem of making a monopodreach a target with a jump. The monopod can jump in any direction and theterrain underneath its foot can be uneven. This is a template of a much largerclass of problems, which are extremely challenging and computationallyexpensive to solve using standard optimisation-based techniques. ReinforcementLearning (RL) could be an interesting alternative, but the application of anend-to-end approach in which the controller must learn everything from scratch,is impractical. The solution advocated in this paper is to guide the learningprocess within an RL framework by injecting physical knowledge. This expedientbrings to widespread benefits, such as a drastic reduction of the learningtime, and the ability to learn and compensate for possible errors in thelow-level controller executing the motion. We demonstrate the advantage of ourapproach with respect to both optimization-based and end-to-end RL approaches.</description><author>Riccardo Bussola, Michele Focchi, Andrea Del Prete, Daniele Fontanelli, Luigi Palopoli</author><pubDate>Wed, 13 Sep 2023 16:46:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07038v1</guid></item><item><title>Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs</title><link>http://arxiv.org/abs/2309.05918v2</link><description>In our opinion the exuberance surrounding the relative success of data-drivenlarge language models (LLMs) is slightly misguided and for several reasons (i)LLMs cannot be relied upon for factual information since for LLMs all ingestedtext (factual or non-factual) was created equal; (ii) due to their subsymbolicna-ture, whatever 'knowledge' these models acquire about language will alwaysbe buried in billions of microfeatures (weights), none of which is meaningfulon its own; and (iii) LLMs will often fail to make the correct inferences inseveral linguistic contexts (e.g., nominal compounds, copredication, quantifierscope ambi-guities, intensional contexts. Since we believe the relative successof data-driven large language models (LLMs) is not a reflection on the symbolicvs. subsymbol-ic debate but a reflection on applying the successful strategy ofa bottom-up reverse engineering of language at scale, we suggest in this paperapplying the effective bottom-up strategy in a symbolic setting resulting insymbolic, explainable, and ontologically grounded language models.</description><author>Walid S. Saba</author><pubDate>Wed, 13 Sep 2023 16:45:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05918v2</guid></item><item><title>How (Not) to Use Sociodemographic Information for Subjective NLP Tasks</title><link>http://arxiv.org/abs/2309.07034v1</link><description>Annotators' sociodemographic backgrounds (i.e., the individual compositionsof their gender, age, educational background, etc.) have a strong impact ontheir decisions when working on subjective NLP tasks, such as hate speechdetection. Often, heterogeneous backgrounds result in high disagreements. Tomodel this variation, recent work has explored sociodemographic prompting, atechnique, which steers the output of prompt-based models towards answers thathumans with specific sociodemographic profiles would give. However, theavailable NLP literature disagrees on the efficacy of this technique -- itremains unclear, for which tasks and scenarios it can help and evaluations arelimited to specific tasks only. We address this research gap by presenting thelargest and most comprehensive study of sociodemographic prompting today.Concretely, we evaluate several prompt formulations across seven datasets andsix instruction-tuned model families. We find that (1) while sociodemographicprompting can be beneficial for improving zero-shot learning in subjective NLPtasks, (2) its outcomes largely vary for different model types, sizes, anddatasets, (3) are subject to large variance with regards to promptformulations. Thus, sociodemographic prompting is not a reliable proxy fortraditional data annotation with a sociodemographically heterogeneous group ofannotators. Instead, we propose (4) to use it for identifying ambiguousinstances resulting in more informed annotation efforts.</description><author>Tilman Beck, Hendrik Schuff, Anne Lauscher, Iryna Gurevych</author><pubDate>Wed, 13 Sep 2023 16:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07034v1</guid></item><item><title>Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks</title><link>http://arxiv.org/abs/2309.07030v1</link><description>Comparing graphs of optimal transport has recently gained significantattention, as the distances induced by optimal transport provide both aprincipled metric between graphs as well as an interpretable description of theassociated changes between graphs in terms of a transport plan. As the lack ofsymmetry introduces challenges in the typically considered formulations,optimal transport distances for graphs have mostly been developed forundirected graphs. Here, we propose two distance measures to compare directedgraphs based on variants of optimal transport: (i) an earth movers distance(Wasserstein) and (ii) a Gromov-Wasserstein (GW) distance. We evaluate thesetwo distances and discuss their relative performance for both simulated graphdata and real-world directed cell-cell communication graphs, inferred fromsingle-cell RNA-seq data.</description><author>James S. Nagai, Ivan G. Costa, Michael T. Schaub</author><pubDate>Wed, 13 Sep 2023 16:36:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07030v1</guid></item><item><title>A Parallel Ensemble of Metaheuristic Solvers for the Traveling Salesman Problem</title><link>http://arxiv.org/abs/2308.07347v2</link><description>The travelling salesman problem (TSP) is one of the well-studied NP-hardproblems in the literature. The state-of-the art inexact TSP solvers are theLin-Kernighan-Helsgaun (LKH) heuristic and Edge Assembly crossover (EAX). Arecent study suggests that EAX with restart mechanisms perform well on a widerange of TSP instances. However, this study is limited to 2,000 city problems.We study for problems ranging from 2,000 to 85,900. We see that the performanceof the solver varies with the type of the problem. However, combining thesesolvers in an ensemble setup, we are able to outperform the individual solver'sperformance. We see the ensemble setup as an efficient way to make use of theabundance of compute resources. In addition to EAX and LKH, we use severalversions of the hybrid of EAX and Mixing Genetic Algorithm (MGA). A hybrid ofMGA and EAX is known to solve some hard problems. We see that the ensemble ofthe hybrid version outperforms the state-of-the-art solvers on problems largerthan 10,000 cities.</description><author>Swetha Varadarajan, Darrell Whitley</author><pubDate>Wed, 13 Sep 2023 16:29:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07347v2</guid></item><item><title>Exploiting Multiple Priors for Neural 3D Indoor Reconstruction</title><link>http://arxiv.org/abs/2309.07021v1</link><description>Neural implicit modeling permits to achieve impressive 3D reconstructionresults on small objects, while it exhibits significant limitations in largeindoor scenes. In this work, we propose a novel neural implicit modeling methodthat leverages multiple regularization strategies to achieve betterreconstructions of large indoor environments, while relying only on images. Asparse but accurate depth prior is used to anchor the scene to the initialmodel. A dense but less accurate depth prior is also introduced, flexibleenough to still let the model diverge from it to improve the estimatedgeometry. Then, a novel self-supervised strategy to regularize the estimatedsurface normals is presented. Finally, a learnable exposure compensation schemepermits to cope with challenging lighting conditions. Experimental results showthat our approach produces state-of-the-art 3D reconstructions in challengingindoor scenarios.</description><author>Federico Lincetto, Gianluca Agresti, Mattia Rossi, Pietro Zanuttigh</author><pubDate>Wed, 13 Sep 2023 16:23:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07021v1</guid></item><item><title>Beyond original Research Articles Categorization via NLP</title><link>http://arxiv.org/abs/2309.07020v1</link><description>This work proposes a novel approach to text categorization -- for unknowncategories -- in the context of scientific literature, using Natural LanguageProcessing techniques. The study leverages the power of pre-trained languagemodels, specifically SciBERT, to extract meaningful representations ofabstracts from the ArXiv dataset. Text categorization is performed using theK-Means algorithm, and the optimal number of clusters is determined based onthe Silhouette score. The results demonstrate that the proposed approachcaptures subject information more effectively than the traditional arXivlabeling system, leading to improved text categorization. The approach offerspotential for better navigation and recommendation systems in the rapidlygrowing landscape of scientific research literature.</description><author>Rosanna Turrisi</author><pubDate>Wed, 13 Sep 2023 16:23:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07020v1</guid></item><item><title>A Methodology for Generative Spelling Correction via Natural Spelling Errors Emulation across Multiple Domains and Languages</title><link>http://arxiv.org/abs/2308.09435v2</link><description>Modern large language models demonstrate impressive capabilities in textgeneration and generalization. However, they often struggle with solving textediting tasks, particularly when it comes to correcting spelling errors andmistypings. In this paper, we present a methodology for generative spellingcorrection (SC), which was tested on English and Russian languages andpotentially can be extended to any language with minor changes. Our researchmainly focuses on exploring natural spelling errors and mistypings in texts andstudying the ways those errors can be emulated in correct sentences toeffectively enrich generative models' pre-train procedure. We investigate theimpact of such emulations and the models' abilities across different textdomains. In this work, we investigate two spelling corruption techniques: 1)first one mimics human behavior when making a mistake through leveragingstatistics of errors from particular dataset and 2) second adds the most commonspelling errors, keyboard miss clicks, and some heuristics within the texts. Weconducted experiments employing various corruption strategies, models'architectures and sizes on the pre-training and fine-tuning stages andevaluated the models using single-domain and multi-domain test sets. As apractical outcome of our work, we introduce SAGE(Spell checking viaAugmentation and Generative distribution Emulation). It is a library forautomatic generative SC that includes a family of pre-trained generative modelsand built-in augmentation algorithms.</description><author>Nikita Martynov, Mark Baushenko, Anastasia Kozlova, Katerina Kolomeytseva, Aleksandr Abramov, Alena Fenogenova</author><pubDate>Wed, 13 Sep 2023 16:22:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09435v2</guid></item><item><title>Résumé Parsing as Hierarchical Sequence Labeling: An Empirical Study</title><link>http://arxiv.org/abs/2309.07015v1</link><description>Extracting information from r\'esum\'es is typically formulated as atwo-stage problem, where the document is first segmented into sections and theneach section is processed individually to extract the target entities. Instead,we cast the whole problem as sequence labeling in two levels -- lines andtokens -- and study model architectures for solving both tasks simultaneously.We build high-quality r\'esum\'e parsing corpora in English, French, Chinese,Spanish, German, Portuguese, and Swedish. Based on these corpora, we presentexperimental results that demonstrate the effectiveness of the proposed modelsfor the information extraction task, outperforming approaches introduced inprevious work. We conduct an ablation study of the proposed architectures. Wealso analyze both model performance and resource efficiency, and describe thetrade-offs for model deployment in the context of a production environment.</description><author>Federico Retyk, Hermenegildo Fabregat, Juan Aizpuru, Mariana Taglio, Rabih Zbib</author><pubDate>Wed, 13 Sep 2023 16:17:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07015v1</guid></item><item><title>ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning</title><link>http://arxiv.org/abs/2212.01378v2</link><description>We propose a new paradigm to continually evolve pretrained models, denotedColD Fusion. It provides the benefits of multitask learning but leveragesdistributed computation with limited communication and eliminates the need forshared data. Consequentially, ColD Fusion can give rise to a synergistic loop,where finetuned models can be recycled to continually improve the pretrainedmodel they are based upon. We show that ColD Fusion yields comparable benefitsto multitask training by producing a model that (a) attains strong performanceon all of the datasets it was trained on; and (b) is a better starting pointfor finetuning on unseen datasets. We show that ColD Fusion outperforms RoBERTaand even previous multitask models. Specifically, when training and testing on35 diverse datasets, ColD Fusion-based model outperforms RoBERTa by 2.33 pointson average without any changes to the architecture.</description><author>Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, Leshem Choshen</author><pubDate>Wed, 13 Sep 2023 16:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.01378v2</guid></item><item><title>OYXOY: A Modern NLP Test Suite for Modern Greek</title><link>http://arxiv.org/abs/2309.07009v1</link><description>This paper serves as a foundational step towards the development of alinguistically motivated and technically relevant evaluation suite for GreekNLP. We initiate this endeavor by introducing four expert-verified evaluationtasks, specifically targeted at natural language inference, word sensedisambiguation (through example comparison or sense selection) and metaphordetection. More than language-adapted replicas of existing tasks, we contributetwo innovations which will resonate with the broader resource and evaluationcommunity. Firstly, our inference dataset is the first of its kind, marking notjust \textit{one}, but rather \textit{all} possible inference labels,accounting for possible shifts due to e.g. ambiguity or polysemy. Secondly, wedemonstrate a cost-efficient method to obtain datasets for under-resourcedlanguages. Using ChatGPT as a language-neutral parser, we transform theDictionary of Standard Modern Greek into a structured format, from which wederive the other three tasks through simple projections. Alongside each task,we conduct experiments using currently available state of the art machinery.Our experimental baselines affirm the challenging nature of our tasks andhighlight the need for expedited progress in order for the Greek NLP ecosystemto keep pace with contemporary mainstream research.</description><author>Konstantinos Kogkalidis, Stergios Chatzikyriakidis, Eirini Chrysovalantou Giannikouri, Vassiliki Katsouli, Christina Klironomou, Christina Koula, Dimitris Papadakis, Thelka Pasparaki, Erofili Psaltaki, Efthymia Sakellariou, Hara Soupiona</author><pubDate>Wed, 13 Sep 2023 16:00:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07009v1</guid></item><item><title>AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language Models</title><link>http://arxiv.org/abs/2308.15366v3</link><description>Large Vision-Language Models (LVLMs) such as MiniGPT-4 and LLaVA havedemonstrated the capability of understanding images and achieved remarkableperformance in various visual tasks. Despite their strong abilities inrecognizing common objects due to extensive training datasets, they lackspecific domain knowledge and have a weaker understanding of localized detailswithin objects, which hinders their effectiveness in the Industrial AnomalyDetection (IAD) task. On the other hand, most existing IAD methods only provideanomaly scores and necessitate the manual setting of thresholds to distinguishbetween normal and abnormal samples, which restricts their practicalimplementation. In this paper, we explore the utilization of LVLM to addressthe IAD problem and propose AnomalyGPT, a novel IAD approach based on LVLM. Wegenerate training data by simulating anomalous images and producingcorresponding textual descriptions for each image. We also employ an imagedecoder to provide fine-grained semantic and design a prompt learner tofine-tune the LVLM using prompt embeddings. Our AnomalyGPT eliminates the needfor manual threshold adjustments, thus directly assesses the presence andlocations of anomalies. Additionally, AnomalyGPT supports multi-turn dialoguesand exhibits impressive few-shot in-context learning capabilities. With onlyone normal shot, AnomalyGPT achieves the state-of-the-art performance with anaccuracy of 86.1%, an image-level AUC of 94.1%, and a pixel-level AUC of 95.3%on the MVTec-AD dataset. Code is available athttps://github.com/CASIA-IVA-Lab/AnomalyGPT.</description><author>Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao Wang</author><pubDate>Wed, 13 Sep 2023 15:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15366v3</guid></item><item><title>TMSA: Towards Arbitrary Text-driven Image Manipulation via Space Alignment</title><link>http://arxiv.org/abs/2301.10670v2</link><description>The recent GAN inversion methods have been able to successfully invert thereal image input to the corresponding editable latent code in StyleGAN. Bycombining with the language-vision model (CLIP), some text-driven imagemanipulation methods are proposed. However, these methods require extra coststo perform optimization for a certain image or a new attribute editing mode. Toachieve a more efficient editing method, we propose a new Text-driven imageManipulation framework via Space Alignment (TMSA). The Space Alignment moduleaims to align the same semantic regions in CLIP and StyleGAN spaces. Then, thetext input can be directly accessed into the StyleGAN space and be used to findthe semantic shift according to the text description. The framework can supportarbitrary image editing mode without additional cost. Our work provides theuser with an interface to control the attributes of a given image according totext input and get the result in real time. Ex tensive experiments demonstrateour superior performance over prior works.</description><author>Yunpeng Bai, Zihan Zhong, Chao Dong, Weichen Zhang, Guowei Xu, Chun Yuan</author><pubDate>Wed, 13 Sep 2023 15:57:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10670v2</guid></item><item><title>Finding Morton-Like Layouts for Multi-Dimensional Arrays Using Evolutionary Algorithms</title><link>http://arxiv.org/abs/2309.07002v1</link><description>The layout of multi-dimensional data can have a significant impact on theefficacy of hardware caches and, by extension, the performance of applications.Common multi-dimensional layouts include the canonical row-major andcolumn-major layouts as well as the Morton curve layout. In this paper, wedescribe how the Morton layout can be generalized to a very large family ofmulti-dimensional data layouts with widely varying performance characteristics.We posit that this design space can be efficiently explored using acombinatorial evolutionary methodology based on genetic algorithms. To thisend, we propose a chromosomal representation for such layouts as well as amethodology for estimating the fitness of array layouts using cache simulation.We show that our fitness function correlates to kernel running time in realhardware, and that our evolutionary strategy allows us to find candidates withfavorable simulated cache properties in four out of the eight real-worldapplications under consideration in a small number of generations. Finally, wedemonstrate that the array layouts found using our evolutionary method performwell not only in simulated environments but that they can effect significantperformance gains -- up to a factor ten in extreme cases -- in real hardware.</description><author>Stephen Nicholas Swatman, Ana-Lucia Varbanescu, Andy D. Pimentel, Andreas Salzburger, Attila Krasznahorkay</author><pubDate>Wed, 13 Sep 2023 15:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07002v1</guid></item><item><title>Dynamic Analysis of Corporate ESG Reports: A Model of Evolutionary Trends</title><link>http://arxiv.org/abs/2309.07001v1</link><description>Environmental, social, and governance (ESG) reports are globally recognizedas a keystone in sustainable enterprise development. This study aims to map thechanging landscape of ESG topics within firms in the global market. A dynamicframework is developed to analyze ESG strategic management for individualclasses, across multiple classes, and in alignment with a specificsustainability index. The output of these analytical processes forms thefoundation of an ESG strategic model. Utilizing a rich collection of21st-century ESG reports from technology companies, our experiment elucidatesthe changes in ESG perspectives by incorporating analytical keywords into theproposed framework. This work thus provides an empirical method that revealsthe concurrent evolution of ESG topics over recent years.</description><author>Ziyuan Xia, Anchen Sun, Xiaodong Cai, Saixing Zeng</author><pubDate>Wed, 13 Sep 2023 15:54:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07001v1</guid></item><item><title>DWRSeg: Rethinking Efficient Acquisition of Multi-scale Contextual Information for Real-time Semantic Segmentation</title><link>http://arxiv.org/abs/2212.01173v3</link><description>Many current works directly adopt multi-rate depth-wise dilated convolutionsto capture multi-scale contextual information simultaneously from one inputfeature map, thus improving the feature extraction efficiency for real-timesemantic segmentation. However, this design may lead to difficult access tomulti-scale contextual information because of the unreasonable structure andhyperparameters. To lower the difficulty of drawing multi-scale contextualinformation, we propose a highly efficient multi-scale feature extractionmethod, which decomposes the original single-step method into two steps, RegionResidualization-Semantic Residualization. In this method, the multi-ratedepth-wise dilated convolutions take a simpler role in feature extraction:performing simple semantic-based morphological filtering with one desiredreceptive field in the second step based on each concise feature map of regionform provided by the first step, to improve their efficiency. Moreover, thedilation rates and the capacity of dilated convolutions for each network stageare elaborated to fully utilize all the feature maps of region form that can beachieved.Accordingly, we design a novel Dilation-wise Residual (DWR) module anda Simple Inverted Residual (SIR) module for the high and low level network,respectively, and form a powerful DWR Segmentation (DWRSeg) network. Extensiveexperiments on the Cityscapes and CamVid datasets demonstrate the effectivenessof our method by achieving a state-of-the-art trade-off between accuracy andinference speed, in addition to being lighter weight. Without pretraining orresorting to any training trick, we achieve an mIoU of 72.7% on the Cityscapestest set at a speed of 319.5 FPS on one NVIDIA GeForce GTX 1080 Ti card, whichexceeds the latest methods of a speed of 69.5 FPS and 0.8% mIoU. The code andtrained models are publicly available.</description><author>Haoran Wei, Xu Liu, Shouchun Xu, Zhongjian Dai, Yaping Dai, Xiangyang Xu</author><pubDate>Wed, 13 Sep 2023 15:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.01173v3</guid></item><item><title>Event and Entity Extraction from Generated Video Captions</title><link>http://arxiv.org/abs/2211.02982v3</link><description>Annotation of multimedia data by humans is time-consuming and costly, whilereliable automatic generation of semantic metadata is a major challenge. Wepropose a framework to extract semantic metadata from automatically generatedvideo captions. As metadata, we consider entities, the entities' properties,relations between entities, and the video category. We employ twostate-of-the-art dense video captioning models with masked transformer (MT) andparallel decoding (PVDC) to generate captions for videos of the ActivityNetCaptions dataset. Our experiments show that it is possible to extract entities,their properties, relations between entities, and the video category from thegenerated captions. We observe that the quality of the extracted information ismainly influenced by the quality of the event localization in the video as wellas the performance of the event caption generation.</description><author>Johannes Scherer, Ansgar Scherp, Deepayan Bhowmik</author><pubDate>Wed, 13 Sep 2023 15:49:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.02982v3</guid></item><item><title>GRDD: A Dataset for Greek Dialectal NLP</title><link>http://arxiv.org/abs/2308.00802v2</link><description>In this paper, we present a dataset for the computational study of a numberof Modern Greek dialects. It consists of raw text data from four dialects ofModern Greek, Cretan, Pontic, Northern Greek and Cypriot Greek. The dataset isof considerable size, albeit imbalanced, and presents the first attempt tocreate large scale dialectal resources of this type for Modern Greek dialects.We then use the dataset to perform dialect idefntification. We experiment withtraditional ML algorithms, as well as simple DL architectures. The results showvery good performance on the task, potentially revealing that the dialects inquestion have distinct enough characteristics allowing even simple ML models toperform well on the task. Error analysis is performed for the top performingalgorithms showing that in a number of cases the errors are due to insufficientdataset cleaning.</description><author>Stergios Chatzikyriakidis, Chatrine Qwaider, Ilias Kolokousis, Christina Koula, Dimitris Papadakis, Efthymia Sakellariou</author><pubDate>Wed, 13 Sep 2023 15:43:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00802v2</guid></item><item><title>Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species</title><link>http://arxiv.org/abs/2305.06695v3</link><description>Visual as well as genetic biometrics are routinely employed to identifyspecies and individuals in biological applications. However, no attempts havebeen made in this domain to computationally enhance visual classification ofrare classes with little image data via genetics. In this paper, we thuspropose aligned visual-genetic inference spaces with the aim to implicitlyencode cross-domain associations for improved performance. We demonstrate forthe first time that such alignment can be achieved via deep embedding modelsand that the approach is directly applicable to boosting long-tailedrecognition (LTR) particularly for rare species. We experimentally demonstratethe efficacy of the concept via application to microscopic imagery of 30k+planktic foraminifer shells across 32 species when used together withindependent genetic data samples. Most importantly for practitioners, we showthat visual-genetic alignment can significantly benefit visual-only recognitionof the rarest species. Technically, we pre-train a visual ResNet50 deeplearning model using triplet loss formulations to create an initial embeddingspace. We re-structure this space based on genetic anchors embedded via aSequence Graph Transform (SGT) and linked to visual data by cross-domain cosinealignment. We show that an LTR approach improves the state-of-the-art acrossall benchmarks and that adding our visual-genetic alignment improves per-classand particularly rare tail class benchmarks significantly further. We concludethat visual-genetic alignment can be a highly effective tool for complementingvisual biological data containing rare classes. The concept proposed may serveas an important future tool for integrating genetics and imageomics towards amore complete scientific representation of taxonomic spaces and life itself.Code, weights, and data splits are published for full reproducibility.</description><author>Tayfun Karaderi, Tilo Burghardt, Raphael Morard, Daniela Schmidt</author><pubDate>Wed, 13 Sep 2023 15:39:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06695v3</guid></item><item><title>Unsupervised Contrast-Consistent Ranking with Language Models</title><link>http://arxiv.org/abs/2309.06991v1</link><description>Language models contain ranking-based knowledge and are powerful solvers ofin-context ranking tasks. For instance, they may have parametric knowledgeabout the ordering of countries by size or may be able to rank reviews bysentiment. Recent work focuses on pairwise, pointwise, and listwise promptingtechniques to elicit a language model's ranking knowledge. However, we findthat even with careful calibration and constrained decoding, prompting-basedtechniques may not always be self-consistent in the rankings they produce. Thismotivates us to explore an alternative approach that is inspired by anunsupervised probing method called Contrast-Consistent Search (CCS). The ideais to train a probing model guided by a logical constraint: a model'srepresentation of a statement and its negation must be mapped to contrastivetrue-false poles consistently across multiple statements. We hypothesize thatsimilar constraints apply to ranking tasks where all items are related viaconsistent pairwise or listwise comparisons. To this end, we extend the binaryCCS method to Contrast-Consistent Ranking (CCR) by adapting existing rankingmethods such as the Max-Margin Loss, Triplet Loss, and Ordinal Regressionobjective. Our results confirm that, for the same language model, CCR probingoutperforms prompting and even performs on a par with prompting much largerlanguage models.</description><author>Niklas Stoehr, Pengxiang Cheng, Jing Wang, Daniel Preotiuc-Pietro, Rajarshi Bhowmik</author><pubDate>Wed, 13 Sep 2023 15:36:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06991v1</guid></item><item><title>Remote Inference of Cognitive Scores in ALS Patients Using a Picture Description</title><link>http://arxiv.org/abs/2309.06989v1</link><description>Amyotrophic lateral sclerosis is a fatal disease that not only affectsmovement, speech, and breath but also cognition. Recent studies have focused onthe use of language analysis techniques to detect ALS and infer scales formonitoring functional progression. In this paper, we focused on anotherimportant aspect, cognitive impairment, which affects 35-50% of the ALSpopulation. In an effort to reach the ALS population, which frequently exhibitsmobility limitations, we implemented the digital version of the EdinburghCognitive and Behavioral ALS Screen (ECAS) test for the first time. This testwhich is designed to measure cognitive impairment was remotely performed by 56participants from the EverythingALS Speech Study. As part of the study,participants (ALS and non-ALS) were asked to describe weekly one picture from apool of many pictures with complex scenes displayed on their computer at home.We analyze the descriptions performed within +/- 60 days from the day the ECAStest was administered and extract different types of linguistic and acousticfeatures. We input those features into linear regression models to infer 5 ECASsub-scores and the total score. Speech samples from the picture description arereliable enough to predict the ECAS subs-scores, achieving statisticallysignificant Spearman correlation values between 0.32 and 0.51 for the model'sperformance using 10-fold cross-validation.</description><author>Carla Agurto, Guillermo Cecchi, Bo Wen, Ernest Fraenkel, James Berry, Indu Navar, Raquel Norel</author><pubDate>Wed, 13 Sep 2023 15:30:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06989v1</guid></item><item><title>RFDforFin: Robust Deep Forgery Detection for GAN-generated Fingerprint Images</title><link>http://arxiv.org/abs/2308.09285v2</link><description>With the rapid development of the image generation technologies, themalicious abuses of the GAN-generated fingerprint images poses a significantthreat to the public safety in certain circumstances. Although the existinguniversal deep forgery detection approach can be applied to detect the fakefingerprint images, they are easily attacked and have poor robustness.Meanwhile, there is no specifically designed deep forgery detection method forfingerprint images. In this paper, we propose the first deep forgery detectionapproach for fingerprint images, which combines unique ridge features offingerprint and generation artifacts of the GAN-generated images, to the bestof our knowledge. Specifically, we firstly construct a ridge stream, whichexploits the grayscale variations along the ridges to extract uniquefingerprint-specific features. Then, we construct a generation artifact stream,in which the FFT-based spectrums of the input fingerprint images are exploited,to extract more robust generation artifact features. At last, the unique ridgefeatures and generation artifact features are fused for binary classification(i.e., real or fake). Comprehensive experiments demonstrate that our proposedapproach is effective and robust with low complexities.</description><author>Hui Miao, Yuanfang Guo, Yunhong Wang</author><pubDate>Wed, 13 Sep 2023 15:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09285v2</guid></item><item><title>Instance Adaptive Prototypical Contrastive Embedding for Generalized Zero Shot Learning</title><link>http://arxiv.org/abs/2309.06987v1</link><description>Generalized zero-shot learning(GZSL) aims to classify samples from seen andunseen labels, assuming unseen labels are not accessible during training.Recent advancements in GZSL have been expedited by incorporatingcontrastive-learning-based (instance-based) embedding in generative networksand leveraging the semantic relationship between data points. However, existingembedding architectures suffer from two limitations: (1) limiteddiscriminability of synthetic features' embedding without consideringfine-grained cluster structures; (2) inflexible optimization due to restrictedscaling mechanisms on existing contrastive embedding networks, leading tooverlapped representations in the embedding space. To enhance the quality ofrepresentations in the embedding space, as mentioned in (1), we propose amargin-based prototypical contrastive learning embedding network that reaps thebenefits of prototype-data (cluster quality enhancement) and implicit data-data(fine-grained representations) interaction while providing substantial clustersupervision to the embedding network and the generator. To tackle (2), wepropose an instance adaptive contrastive loss that leads to generalizedrepresentations for unseen labels with increased inter-class margin. Throughcomprehensive experimental evaluation, we show that our method can outperformthe current state-of-the-art on three benchmark datasets. Our approach alsoconsistently achieves the best unseen performance in the GZSL setting.</description><author>Riti Paul, Sahil Vora, Baoxin Li</author><pubDate>Wed, 13 Sep 2023 15:26:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06987v1</guid></item><item><title>CARE: Large Precision Matrix Estimation for Compositional Data</title><link>http://arxiv.org/abs/2309.06985v1</link><description>High-dimensional compositional data are prevalent in many applications. Thesimplex constraint poses intrinsic challenges to inferring the conditionaldependence relationships among the components forming a composition, as encodedby a large precision matrix. We introduce a precise specification of thecompositional precision matrix and relate it to its basis counterpart, which isshown to be asymptotically identifiable under suitable sparsity assumptions. Byexploiting this connection, we propose a composition adaptive regularizedestimation (CARE) method for estimating the sparse basis precision matrix. Wederive rates of convergence for the estimator and provide theoreticalguarantees on support recovery and data-driven parameter tuning. Our theoryreveals an intriguing trade-off between identification and estimation, therebyhighlighting the blessing of dimensionality in compositional data analysis. Inparticular, in sufficiently high dimensions, the CARE estimator achievesminimax optimality and performs as well as if the basis were observed. Wefurther discuss how our framework can be extended to handle data containingzeros, including sampling zeros and structural zeros. The advantages of CAREover existing methods are illustrated by simulation studies and an applicationto inferring microbial ecological networks in the human gut.</description><author>Shucong Zhang, Huiyuan Wang, Wei Lin</author><pubDate>Wed, 13 Sep 2023 15:20:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06985v1</guid></item><item><title>MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems</title><link>http://arxiv.org/abs/2309.06981v1</link><description>Speaker Verification (SV) is widely deployed in mobile systems toauthenticate legitimate users by using their voice traits. In this work, wepropose a backdoor attack MASTERKEY, to compromise the SV models. Differentfrom previous attacks, we focus on a real-world practical setting where theattacker possesses no knowledge of the intended victim. To design MASTERKEY, weinvestigate the limitation of existing poisoning attacks against unseentargets. Then, we optimize a universal backdoor that is capable of attackingarbitrary targets. Next, we embed the speaker's characteristics and semanticsinformation into the backdoor, making it imperceptible. Finally, we estimatethe channel distortion and integrate it into the backdoor. We validate ourattack on 6 popular SV models. Specifically, we poison a total of 53 models anduse our trigger to attack 16,430 enrolled speakers, composed of 310 targetspeakers enrolled in 53 poisoned models. Our attack achieves 100% attacksuccess rate with a 15% poison rate. By decreasing the poison rate to 3%, theattack success rate remains around 50%. We validate our attack in 3 real-worldscenarios and successfully demonstrate the attack through both over-the-air andover-the-telephony-line scenarios.</description><author>Hanqing Guo, Xun Chen, Junfeng Guo, Li Xiao, Qiben Yan</author><pubDate>Wed, 13 Sep 2023 15:15:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06981v1</guid></item><item><title>Auto-Regressive Next-Token Predictors are Universal Learners</title><link>http://arxiv.org/abs/2309.06979v1</link><description>Large language models display remarkable capabilities in logical andmathematical reasoning, allowing them to solve complex tasks. Interestingly,these abilities emerge in networks trained on the simple task of next-tokenprediction. In this work, we present a theoretical framework for studyingauto-regressive next-token predictors. We demonstrate that even simple modelssuch as linear next-token predictors, trained on Chain-of-Thought (CoT) data,can approximate any function efficiently computed by a Turing machine. Weintroduce a new complexity measure -- length complexity -- which measures thenumber of intermediate tokens in a CoT sequence required to approximate sometarget function, and analyze the interplay between length complexity and othernotions of complexity. Finally, we show experimentally that simple next-tokenpredictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs),display non-trivial performance on text generation and arithmetic tasks. Ourresults demonstrate that the power of language models can be attributed, to agreat extent, to the auto-regressive next-token training scheme, and notnecessarily to a particular choice of architecture.</description><author>Eran Malach</author><pubDate>Wed, 13 Sep 2023 15:15:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06979v1</guid></item><item><title>Differentiable JPEG: The Devil is in the Details</title><link>http://arxiv.org/abs/2309.06978v1</link><description>JPEG remains one of the most widespread lossy image coding methods. However,the non-differentiable nature of JPEG restricts the application in deeplearning pipelines. Several differentiable approximations of JPEG have recentlybeen proposed to address this issue. This paper conducts a comprehensive reviewof existing diff. JPEG approaches and identifies critical details that havebeen missed by previous methods. To this end, we propose a novel diff. JPEGapproach, overcoming previous limitations. Our approach is differentiablew.r.t. the input image, the JPEG quality, the quantization tables, and thecolor conversion parameters. We evaluate the forward and backward performanceof our diff. JPEG approach against existing methods. Additionally, extensiveablations are performed to evaluate crucial design choices. Our proposed diff.JPEG resembles the (non-diff.) reference implementation best, significantlysurpassing the recent-best diff. approach by $3.47$dB (PSNR) on average. Forstrong compression rates, we can even improve PSNR by $9.51$dB. Strongadversarial attack results are yielded by our diff. JPEG, demonstrating theeffective gradient approximation. Our code is available athttps://github.com/necla-ml/Diff-JPEG.</description><author>Christoph Reich, Biplob Debnath, Deep Patel, Srimat Chakradhar</author><pubDate>Wed, 13 Sep 2023 15:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06978v1</guid></item><item><title>DNNShifter: An Efficient DNN Pruning System for Edge Computing</title><link>http://arxiv.org/abs/2309.06973v1</link><description>Deep neural networks (DNNs) underpin many machine learning applications.Production quality DNN models achieve high inference accuracy by trainingmillions of DNN parameters which has a significant resource footprint. Thispresents a challenge for resources operating at the extreme edge of thenetwork, such as mobile and embedded devices that have limited computationaland memory resources. To address this, models are pruned to create lightweight,more suitable variants for these devices. Existing pruning methods are unableto provide similar quality models compared to their unpruned counterpartswithout significant time costs and overheads or are limited to offline usecases. Our work rapidly derives suitable model variants while maintaining theaccuracy of the original model. The model variants can be swapped quickly whensystem and network conditions change to match workload demand. This paperpresents DNNShifter, an end-to-end DNN training, spatial pruning, and modelswitching system that addresses the challenges mentioned above. At the heart ofDNNShifter is a novel methodology that prunes sparse models using structuredpruning. The pruned model variants generated by DNNShifter are smaller in sizeand thus faster than dense and sparse model predecessors, making them suitablefor inference at the edge while retaining near similar accuracy as of theoriginal dense model. DNNShifter generates a portfolio of model variants thatcan be swiftly interchanged depending on operational conditions. DNNShifterproduces pruned model variants up to 93x faster than conventional trainingmethods. Compared to sparse models, the pruned model variants are up to 5.14xsmaller and have a 1.67x inference latency speedup, with no compromise tosparse model accuracy. In addition, DNNShifter has up to 11.9x lower overheadfor switching models and up to 3.8x lower memory utilisation than existingapproaches.</description><author>Bailey J. Eccles, Philip Rodgers, Peter Kilpatrick, Ivor Spence, Blesson Varghese</author><pubDate>Wed, 13 Sep 2023 15:05:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06973v1</guid></item><item><title>Setting the Right Expectations: Algorithmic Recourse Over Time</title><link>http://arxiv.org/abs/2309.06969v1</link><description>Algorithmic systems are often called upon to assist in high-stakes decisionmaking. In light of this, algorithmic recourse, the principle whereinindividuals should be able to take action against an undesirable outcome madeby an algorithmic system, is receiving growing attention. The bulk of theliterature on algorithmic recourse to-date focuses primarily on how to providerecourse to a single individual, overlooking a critical element: the effects ofa continuously changing context. Disregarding these effects on recourse is asignificant oversight, since, in almost all cases, recourse consists of anindividual making a first, unfavorable attempt, and then being given anopportunity to make one or several attempts at a later date - when the contextmight have changed. This can create false expectations, as initial recourserecommendations may become less reliable over time due to model drift andcompetition for access to the favorable outcome between individuals. In this work we propose an agent-based simulation framework for studying theeffects of a continuously changing environment on algorithmic recourse. Inparticular, we identify two main effects that can alter the reliability ofrecourse for individuals represented by the agents: (1) competition with otheragents acting upon recourse, and (2) competition with new agents entering theenvironment. Our findings highlight that only a small set of specificparameterizations result in algorithmic recourse that is reliable for agentsover time. Consequently, we argue that substantial additional work is needed tounderstand recourse reliability over time, and to develop recourse methods thatreward agents' effort.</description><author>Joao Fonseca, Andrew Bell, Carlo Abrate, Francesco Bonchi, Julia Stoyanovich</author><pubDate>Wed, 13 Sep 2023 15:04:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06969v1</guid></item><item><title>A Spectral Analysis of Graph Neural Networks on Dense and Sparse Graphs</title><link>http://arxiv.org/abs/2211.03231v3</link><description>In this work we propose a random graph model that can produce graphs atdifferent levels of sparsity. We analyze how sparsity affects the graphspectra, and thus the performance of graph neural networks (GNNs) in nodeclassification on dense and sparse graphs. We compare GNNs with spectralmethods known to provide consistent estimators for community detection on densegraphs, a closely related task. We show that GNNs can outperform spectralmethods on sparse graphs, and illustrate these results with numerical exampleson both synthetic and real graphs.</description><author>Luana Ruiz, Ningyuan Huang, Soledad Villar</author><pubDate>Wed, 13 Sep 2023 15:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03231v3</guid></item><item><title>Do Language Models Know When They're Hallucinating References?</title><link>http://arxiv.org/abs/2305.18248v2</link><description>State-of-the-art language models (LMs) are famous for "hallucinating"references. These fabricated article and book titles lead to harms, obstaclesto their use, and public backlash. While other types of LM hallucinations arealso important, we propose hallucinated references as the "drosophila" ofresearch on hallucination in large language models (LLMs), as they areparticularly easy to study. We show that simple search engine queries reliablyidentify such hallucinations, which facilitates evaluation. To begin to dissectthe nature of hallucinated LM references, we attempt to classify them usingblack-box queries to the same LM, without consulting any external resources.Consistency checks done with "direct" queries about whether the generatedreference title is real (inspired by Kadavath et al. 2022, Lin et al. 2022,Manakul et al. 2023) are compared to consistency checks with "indirect" querieswhich ask for ancillary details such as the authors of the work. Theseconsistency checks are found to be partially reliable indicators of whether ornot the reference is a hallucination. In particular, we find that LMs oftenhallucinate differing authors of hallucinated references when queried inindependent sessions, while consistently identify authors of real references.This suggests that the hallucination may be more a generation issue thaninherent to current training techniques or representation.</description><author>Ayush Agrawal, Mirac Suzgun, Lester Mackey, Adam Tauman Kalai</author><pubDate>Wed, 13 Sep 2023 14:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18248v2</guid></item><item><title>Towards Reliable Dermatology Evaluation Benchmarks</title><link>http://arxiv.org/abs/2309.06961v1</link><description>Benchmark datasets for digital dermatology unwittingly contain inaccuraciesthat reduce trust in model performance estimates. We propose aresource-efficient data cleaning protocol to identify issues that escapedprevious curation. The protocol leverages an existing algorithmic cleaningstrategy and is followed by a confirmation process terminated by an intuitivestopping criterion. Based on confirmation by multiple dermatologists, we removeirrelevant samples and near duplicates and estimate the percentage of labelerrors in six dermatology image datasets for model evaluation promoted by theInternational Skin Imaging Collaboration. Along with this paper, we publishrevised file lists for each dataset which should be used for model evaluation.Our work paves the way for more trustworthy performance assessment in digitaldermatology.</description><author>Fabian Gröger, Simone Lionetti, Philippe Gottfrois, Alvaro Gonzalez-Jimenez, Matthew Groh, Roxana Daneshjou, Labelling Consortium, Alexander A. Navarini, Marc Pouly</author><pubDate>Wed, 13 Sep 2023 14:54:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06961v1</guid></item><item><title>PhantomSound: Black-Box, Query-Efficient Audio Adversarial Attack via Split-Second Phoneme Injection</title><link>http://arxiv.org/abs/2309.06960v1</link><description>In this paper, we propose PhantomSound, a query-efficient black-box attacktoward voice assistants. Existing black-box adversarial attacks on voiceassistants either apply substitution models or leverage the intermediate modeloutput to estimate the gradients for crafting adversarial audio samples.However, these attack approaches require a significant amount of queries with alengthy training stage. PhantomSound leverages the decision-based attack toproduce effective adversarial audios, and reduces the number of queries byoptimizing the gradient estimation. In the experiments, we perform our attackagainst 4 different speech-to-text APIs under 3 real-world scenarios todemonstrate the real-time attack impact. The results show that PhantomSound ispractical and robust in attacking 5 popular commercial voice controllabledevices over the air, and is able to bypass 3 liveness detection mechanismswith &gt;95% success rate. The benchmark result shows that PhantomSound cangenerate adversarial examples and launch the attack in a few minutes. Wesignificantly enhance the query efficiency and reduce the cost of a successfuluntargeted and targeted adversarial attack by 93.1% and 65.5% compared with thestate-of-the-art black-box attacks, using merely ~300 queries (~5 minutes) and~1,500 queries (~25 minutes), respectively.</description><author>Hanqing Guo, Guangjing Wang, Yuanda Wang, Bocheng Chen, Qiben Yan, Li Xiao</author><pubDate>Wed, 13 Sep 2023 14:50:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06960v1</guid></item><item><title>Neural network-based coronary dominance classification of RCA angiograms</title><link>http://arxiv.org/abs/2309.06958v1</link><description>Background. Cardiac dominance classification is essential for SYNTAX scoreestimation, which is a tool used to determine the complexity of coronary arterydisease and guide patient selection toward optimal revascularization strategy.Objectives. Cardiac dominance classification algorithm based on the analysis ofright coronary artery (RCA) angiograms using neural network Method. We employedconvolutional neural network ConvNext and Swin transformer for 2D image(frames) classification, along with a majority vote for cardio angiographicview classification. An auxiliary network was also used to detect irrelevantimages which were then excluded from the data set. Our data set consisted of828 angiographic studies, 192 of them being patients with left dominance.Results. 5-fold cross validation gave the following dominance classificationmetrics (p=95%): macro recall=93.1%, accuracy=93.5%, macro F1=89.2%. The mostcommon case in which the model regularly failed was RCA occlusion, as itrequires utilization of LCA information. Another cause for false prediction isa small diameter combined with poor quality cardio angiographic view. In suchcases, cardiac dominance classification can be complex and may requirediscussion among specialists to reach an accurate conclusion. Conclusion. Theuse of machine learning approaches to classify cardiac dominance based on RCAalone has been shown to be successful with satisfactory accuracy. However, forhigher accuracy, it is necessary to utilize LCA information in the case of anoccluded RCA and detect cases where there is high uncertainty.</description><author>Ivan Kruzhilov, Egor Ikryannikov, Artem Shadrin, Ruslan Utegenov, Galina Zubkova, Ivan Bessonov</author><pubDate>Wed, 13 Sep 2023 14:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06958v1</guid></item><item><title>Implicit Neural Multiple Description for DNA-based data storage</title><link>http://arxiv.org/abs/2309.06956v1</link><description>DNA exhibits remarkable potential as a data storage solution due to itsimpressive storage density and long-term stability, stemming from its inherentbiomolecular structure. However, developing this novel medium comes with itsown set of challenges, particularly in addressing errors arising from storageand biological manipulations. These challenges are further conditioned by thestructural constraints of DNA sequences and cost considerations. In response tothese limitations, we have pioneered a novel compression scheme and acutting-edge Multiple Description Coding (MDC) technique utilizing neuralnetworks for DNA data storage. Our MDC method introduces an innovative approachto encoding data into DNA, specifically designed to withstand errorseffectively. Notably, our new compression scheme overperforms classic imagecompression methods for DNA-data storage. Furthermore, our approach exhibitssuperiority over conventional MDC methods reliant on auto-encoders. Itsdistinctive strengths lie in its ability to bypass the need for extensive modeltraining and its enhanced adaptability for fine-tuning redundancy levels.Experimental results demonstrate that our solution competes favorably with thelatest DNA data storage methods in the field, offering superior compressionrates and robust noise resilience.</description><author>Trung Hieu Le, Xavier Pic, Jeremy Mateos, Marc Antonini</author><pubDate>Wed, 13 Sep 2023 14:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06956v1</guid></item><item><title>TransNet: A Transfer Learning-Based Network for Human Action Recognition</title><link>http://arxiv.org/abs/2309.06951v1</link><description>Human action recognition (HAR) is a high-level and significant research areain computer vision due to its ubiquitous applications. The main limitations ofthe current HAR models are their complex structures and lengthy training time.In this paper, we propose a simple yet versatile and effective end-to-end deeplearning architecture, coined as TransNet, for HAR. TransNet decomposes thecomplex 3D-CNNs into 2D- and 1D-CNNs, where the 2D- and 1D-CNN componentsextract spatial features and temporal patterns in videos, respectively.Benefiting from its concise architecture, TransNet is ideally compatible withany pretrained state-of-the-art 2D-CNN models in other fields, beingtransferred to serve the HAR task. In other words, it naturally leverages thepower and success of transfer learning for HAR, bringing huge advantages interms of efficiency and effectiveness. Extensive experimental results and thecomparison with the state-of-the-art models demonstrate the superiorperformance of the proposed TransNet in HAR in terms of flexibility, modelcomplexity, training speed and classification accuracy.</description><author>K. Alomar, X. Cai</author><pubDate>Wed, 13 Sep 2023 14:34:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06951v1</guid></item><item><title>Edge-aware Hard Clustering Graph Pooling for Brain Imaging Data</title><link>http://arxiv.org/abs/2308.11909v4</link><description>Graph Convolutional Networks (GCNs) can capture non-Euclidean spatialdependence between different brain regions, and the graph pooling operator inGCNs is key to enhancing the representation learning capability and acquiringabnormal brain maps. However, the majority of existing research designs graphpooling operators only from the perspective of nodes while disregarding theoriginal edge features, in a way that not only confines graph poolingapplication scenarios, but also diminishes its ability to capture criticalsubstructures. In this study, a clustering graph pooling method that firstsupports multidimensional edge features, called Edge-aware hard clusteringgraph pooling (EHCPool), is developed. EHCPool proposes the first'Edge-to-node' score evaluation criterion based on edge features to assess nodefeature significance. To more effectively capture the critical subgraphs, anovel Iteration n-top strategy is further designed to adaptively learn sparsehard clustering assignments for graphs. Subsequently, an innovative N-EAggregation strategy is presented to aggregate node and edge featureinformation in each independent subgraph. The proposed model was evaluated onmulti-site brain imaging public datasets and yielded state-of-the-artperformance. We believe this method is the first deep learning tool with thepotential to probe different types of abnormal functional brain networks fromdata-driven perspective. Core code is at: https://github.com/swfen/EHCPool.</description><author>Cheng Zhu, Jiayi Zhu, Lijuan Zhang, Xi Wu, Shuqi Yang, Ping Liang, Honghan Chen, Ying Tan</author><pubDate>Wed, 13 Sep 2023 14:32:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11909v4</guid></item><item><title>Limited-Angle Tomography Reconstruction via Deep End-To-End Learning on Synthetic Data</title><link>http://arxiv.org/abs/2309.06948v1</link><description>Computed tomography (CT) has become an essential part of modern science andmedicine. A CT scanner consists of an X-ray source that is spun around anobject of interest. On the opposite end of the X-ray source, a detectorcaptures X-rays that are not absorbed by the object. The reconstruction of animage is a linear inverse problem, which is usually solved by filtered backprojection. However, when the number of measurements is small, thereconstruction problem is ill-posed. This is for example the case when theX-ray source is not spun completely around the object, but rather irradiatesthe object only from a limited angle. To tackle this problem, we present a deepneural network that is trained on a large amount of carefully-crafted syntheticdata and can perform limited-angle tomography reconstruction even for only30{\deg} or 40{\deg} sinograms. With our approach we won the first place in theHelsinki Tomography Challenge 2022.</description><author>Thomas Germer, Jan Robine, Sebastian Konietzny, Stefan Harmeling, Tobias Uelwer</author><pubDate>Wed, 13 Sep 2023 14:31:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06948v1</guid></item><item><title>Effect of hyperparameters on variable selection in random forests</title><link>http://arxiv.org/abs/2309.06943v1</link><description>Random forests (RFs) are well suited for prediction modeling and variableselection in high-dimensional omics studies. The effect of hyperparameters ofthe RF algorithm on prediction performance and variable importance estimationhave previously been investigated. However, how hyperparameters impact RF-basedvariable selection remains unclear. We evaluate the effects on the Vita and theBoruta variable selection procedures based on two simulation studies utilizingtheoretical distributions and empirical gene expression data. We assess theability of the procedures to select important variables (sensitivity) whilecontrolling the false discovery rate (FDR). Our results show that theproportion of splitting candidate variables (mtry.prop) and the sample fraction(sample.fraction) for the training dataset influence the selection proceduresmore than the drawing strategy of the training datasets and the minimalterminal node size. A suitable setting of the RF hyperparameters depends on thecorrelation structure in the data. For weakly correlated predictor variables,the default value of mtry is optimal, but smaller values of sample.fractionresult in larger sensitivity. In contrast, the difference in sensitivity of theoptimal compared to the default value of sample.fraction is negligible forstrongly correlated predictor variables, whereas smaller values than thedefault are better in the other settings. In conclusion, the default values ofthe hyperparameters will not always be suitable for identifying importantvariables. Thus, adequate values differ depending on whether the aim of thestudy is optimizing prediction performance or variable selection.</description><author>Cesaire J. K. Fouodo, Lea L. Kronziel, Inke R. König, Silke Szymczak</author><pubDate>Wed, 13 Sep 2023 14:26:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06943v1</guid></item><item><title>DEFormer: DCT-driven Enhancement Transformer for Low-light Image and Dark Vision</title><link>http://arxiv.org/abs/2309.06941v1</link><description>The goal of low-light image enhancement is to restore the color and detailsof the image and is of great significance for high-level visual tasks inautonomous driving. However, it is difficult to restore the lost details in thedark area by relying only on the RGB domain. In this paper we introducefrequency as a new clue into the network and propose a novel DCT-drivenenhancement transformer (DEFormer). First, we propose a learnable frequencybranch (LFB) for frequency enhancement contains DCT processing andcurvature-based frequency enhancement (CFE). CFE calculates the curvature ofeach channel to represent the detail richness of different frequency bands,then we divides the frequency features, which focuses on frequency bands withricher textures. In addition, we propose a cross domain fusion (CDF) forreducing the differences between the RGB domain and the frequency domain. Wealso adopt DEFormer as a preprocessing in dark detection, DEFormer effectivelyimproves the performance of the detector, bringing 2.1% and 3.4% improvement inExDark and DARK FACE datasets on mAP respectively.</description><author>Xiangchen Yin, Zhenda Yu, Xin Gao, Ran Ju, Xiao Sun, Xinyu Zhang</author><pubDate>Wed, 13 Sep 2023 14:24:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06941v1</guid></item><item><title>Collectionless Artificial Intelligence</title><link>http://arxiv.org/abs/2309.06938v1</link><description>By and large, the professional handling of huge data collections is regardedas a fundamental ingredient of the progress of machine learning and of itsspectacular results in related disciplines, with a growing agreement on risksconnected to the centralization of such data collections. This paper sustainsthe position that the time has come for thinking of new learning protocolswhere machines conquer cognitive skills in a truly human-like context centeredon environmental interactions. This comes with specific restrictions on thelearning protocol according to the collectionless principle, which states that,at each time instant, data acquired from the environment is processed with thepurpose of contributing to update the current internal representation of theenvironment, and that the agent is not given the privilege of recording thetemporal stream. Basically, there is neither permission to store the temporalinformation coming from the sensors, thus promoting the development ofself-organized memorization skills at a more abstract level, instead of relyingon bare storage to simulate learning dynamics that are typical of offlinelearning algorithms. This purposely extreme position is intended to stimulatethe development of machines that learn to dynamically organize the informationby following human-based schemes. The proposition of this challenge suggestsdeveloping new foundations on computational processes of learning and reasoningthat might open the doors to a truly orthogonal competitive track on AItechnologies that avoid data accumulation by design, thus offering a frameworkwhich is better suited concerning privacy issues, control and customizability.Finally, pushing towards massively distributed computation, the collectionlessapproach to AI will likely reduce the concentration of power in companies andgovernments, thus better facing geopolitical issues.</description><author>Marco Gori, Stefano Melacci</author><pubDate>Wed, 13 Sep 2023 14:20:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06938v1</guid></item><item><title>DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2309.06933v1</link><description>Recent progresses in large-scale text-to-image models have yielded remarkableaccomplishments, finding various applications in art domain. However,expressing unique characteristics of an artwork (e.g. brushwork, colortone, orcomposition) with text prompts alone may encounter limitations due to theinherent constraints of verbal description. To this end, we introduceDreamStyler, a novel framework designed for artistic image synthesis,proficient in both text-to-image synthesis and style transfer. DreamStyleroptimizes a multi-stage textual embedding with a context-aware text prompt,resulting in prominent image quality. In addition, with content and styleguidance, DreamStyler exhibits flexibility to accommodate a range of stylereferences. Experimental results demonstrate its superior performance acrossmultiple scenarios, suggesting its promising potential in artistic productcreation.</description><author>Namhyuk Ahn, Junsoo Lee, Chunggi Lee, Kunhee Kim, Daesik Kim, Seung-Hun Nam, Kibeom Hong</author><pubDate>Wed, 13 Sep 2023 14:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06933v1</guid></item><item><title>Modeling Dislocation Dynamics Data Using Semantic Web Technologies</title><link>http://arxiv.org/abs/2309.06930v1</link><description>Research in the field of Materials Science and Engineering focuses on thedesign, synthesis, properties, and performance of materials. An important classof materials that is widely investigated are crystalline materials, includingmetals and semiconductors. Crystalline material typically contains a distincttype of defect called "dislocation". This defect significantly affects variousmaterial properties, including strength, fracture toughness, and ductility.Researchers have devoted a significant effort in recent years to understandingdislocation behavior through experimental characterization techniques andsimulations, e.g., dislocation dynamics simulations. This paper presents howdata from dislocation dynamics simulations can be modeled using semantic webtechnologies through annotating data with ontologies. We extend the alreadyexisting Dislocation Ontology by adding missing concepts and aligning it withtwo other domain-related ontologies (i.e., the Elementary Multi-perspectiveMaterial Ontology and the Materials Design Ontology) allowing for representingthe dislocation simulation data efficiently. Moreover, we show a real-world usecase by representing the discrete dislocation dynamics data as a knowledgegraph (DisLocKG) that illustrates the relationship between them. We alsodeveloped a SPARQL endpoint that brings extensive flexibility to queryDisLocKG.</description><author>Ahmad Zainul Ihsan, Said Fathalla, Stefan Sandfeld</author><pubDate>Wed, 13 Sep 2023 14:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06930v1</guid></item><item><title>ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning</title><link>http://arxiv.org/abs/2309.01538v2</link><description>Logical rules are essential for uncovering the logical connections betweenrelations, which could improve the reasoning performance and provideinterpretable results on knowledge graphs (KGs). Although there have been manyefforts to mine meaningful logical rules over KGs, existing methods suffer fromthe computationally intensive searches over the rule space and a lack ofscalability for large-scale KGs. Besides, they often ignore the semantics ofrelations which is crucial for uncovering logical connections. Recently, largelanguage models (LLMs) have shown impressive performance in the field ofnatural language processing and various applications, owing to their emergentability and generalizability. In this paper, we propose a novel framework,ChatRule, unleashing the power of large language models for mining logicalrules over knowledge graphs. Specifically, the framework is initiated with anLLM-based rule generator, leveraging both the semantic and structuralinformation of KGs to prompt LLMs to generate logical rules. To refine thegenerated rules, a rule ranking module estimates the rule quality byincorporating facts from existing KGs. Last, a rule validator harnesses thereasoning ability of LLMs to validate the logical correctness of ranked rulesthrough chain-of-thought reasoning. ChatRule is evaluated on four large-scaleKGs, w.r.t. different rule quality metrics and downstream tasks, showing theeffectiveness and scalability of our method.</description><author>Linhao Luo, Jiaxin Ju, Bo Xiong, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan</author><pubDate>Wed, 13 Sep 2023 14:00:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01538v2</guid></item><item><title>Dynamic Causal Disentanglement Model for Dialogue Emotion Detection</title><link>http://arxiv.org/abs/2309.06928v1</link><description>Emotion detection is a critical technology extensively employed in diversefields. While the incorporation of commonsense knowledge has proven beneficialfor existing emotion detection methods, dialogue-based emotion detectionencounters numerous difficulties and challenges due to human agency and thevariability of dialogue content.In dialogues, human emotions tend to accumulatein bursts. However, they are often implicitly expressed. This implies that manygenuine emotions remain concealed within a plethora of unrelated words anddialogues.In this paper, we propose a Dynamic Causal Disentanglement Modelbased on hidden variable separation, which is founded on the separation ofhidden variables. This model effectively decomposes the content of dialoguesand investigates the temporal accumulation of emotions, thereby enabling moreprecise emotion recognition. First, we introduce a novel Causal DirectedAcyclic Graph (DAG) to establish the correlation between hidden emotionalinformation and other observed elements. Subsequently, our approach utilizespre-extracted personal attributes and utterance topics as guiding factors forthe distribution of hidden variables, aiming to separate irrelevant ones.Specifically, we propose a dynamic temporal disentanglement model to infer thepropagation of utterances and hidden variables, enabling the accumulation ofemotion-related information throughout the conversation. To guide thisdisentanglement process, we leverage the ChatGPT-4.0 and LSTM networks toextract utterance topics and personal attributes as observedinformation.Finally, we test our approach on two popular datasets in dialogueemotion detection and relevant experimental results verified the model'ssuperiority.</description><author>Yuting Su, Yichen Wei, Weizhi Nie, Sicheng Zhao, Anan Liu</author><pubDate>Wed, 13 Sep 2023 13:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06928v1</guid></item><item><title>Contrast-Phys+: Unsupervised and Weakly-supervised Video-based Remote Physiological Measurement via Spatiotemporal Contrast</title><link>http://arxiv.org/abs/2309.06924v1</link><description>Video-based remote physiological measurement utilizes facial videos tomeasure the blood volume change signal, which is also called remotephotoplethysmography (rPPG). Supervised methods for rPPG measurements have beenshown to achieve good performance. However, the drawback of these methods isthat they require facial videos with ground truth (GT) physiological signals,which are often costly and difficult to obtain. In this paper, we proposeContrast-Phys+, a method that can be trained in both unsupervised andweakly-supervised settings. We employ a 3DCNN model to generate multiplespatiotemporal rPPG signals and incorporate prior knowledge of rPPG into acontrastive loss function. We further incorporate the GT signals intocontrastive learning to adapt to partial or misaligned labels. The contrastiveloss encourages rPPG/GT signals from the same video to be grouped together,while pushing those from different videos apart. We evaluate our methods onfive publicly available datasets that include both RGB and Near-infraredvideos. Contrast-Phys+ outperforms the state-of-the-art supervised methods,even when using partially available or misaligned GT signals, or no labels atall. Additionally, we highlight the advantages of our methods in terms ofcomputational efficiency, noise robustness, and generalization.</description><author>Zhaodong Sun, Xiaobai Li</author><pubDate>Wed, 13 Sep 2023 13:50:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06924v1</guid></item><item><title>Native Language Identification with Big Bird Embeddings</title><link>http://arxiv.org/abs/2309.06923v1</link><description>Native Language Identification (NLI) intends to classify an author's nativelanguage based on their writing in another language. Historically, the task hasheavily relied on time-consuming linguistic feature engineering, andtransformer-based NLI models have thus far failed to offer effective, practicalalternatives. The current work investigates if input size is a limiting factor,and shows that classifiers trained using Big Bird embeddings outperformlinguistic feature engineering models by a large margin on the Reddit-L2dataset. Additionally, we provide further insight into input lengthdependencies, show consistent out-of-sample performance, and qualitativelyanalyze the embedding space. Given the effectiveness and computationalefficiency of this method, we believe it offers a promising avenue for futureNLI work.</description><author>Sergey Kramp, Giovanni Cassani, Chris Emmery</author><pubDate>Wed, 13 Sep 2023 13:47:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06923v1</guid></item><item><title>Hydra: Multi-head Low-rank Adaptation for Parameter Efficient Fine-tuning</title><link>http://arxiv.org/abs/2309.06922v1</link><description>The recent surge in large-scale foundation models has spurred the developmentof efficient methods for adapting these models to various downstream tasks.Low-rank adaptation methods, such as LoRA, have gained significant attentiondue to their outstanding parameter efficiency and no additional inferencelatency. This paper investigates a more general form of adapter module based onthe analysis that parallel and sequential adaptation branches learn novel andgeneral features during fine-tuning, respectively. The proposed method, namedHydra, due to its multi-head computational branches, combines parallel andsequential branch to integrate capabilities, which is more expressive thanexisting single branch methods and enables the exploration of a broader rangeof optimal points in the fine-tuning process. In addition, the proposedadaptation method explicitly leverages the pre-trained weights by performing alinear combination of the pre-trained features. It allows the learned featuresto have better generalization performance across diverse downstream tasks.Furthermore, we perform a comprehensive analysis of the characteristics of eachadaptation branch with empirical evidence. Through an extensive range ofexperiments, encompassing comparisons and ablation studies, we substantiate theefficiency and demonstrate the superior performance of Hydra. Thiscomprehensive evaluation underscores the potential impact and effectiveness ofHydra in a variety of applications. Our code is available on\url{https://github.com/extremebird/Hydra}</description><author>Sanghyeon Kim, Hyunmo Yang, Younghyun Kim, Youngjoon Hong, Eunbyung Park</author><pubDate>Wed, 13 Sep 2023 13:46:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06922v1</guid></item><item><title>Investigating the Impact of Action Representations in Policy Gradient Algorithms</title><link>http://arxiv.org/abs/2309.06921v1</link><description>Reinforcement learning~(RL) is a versatile framework for learning to solvecomplex real-world tasks. However, influences on the learning performance of RLalgorithms are often poorly understood in practice. We discuss differentanalysis techniques and assess their effectiveness for investigating the impactof action representations in RL. Our experiments demonstrate that the actionrepresentation can significantly influence the learning performance on popularRL benchmark tasks. The analysis results indicate that some of the performancedifferences can be attributed to changes in the complexity of the optimizationlandscape. Finally, we discuss open challenges of analysis techniques for RLalgorithms.</description><author>Jan Schneider, Pierre Schumacher, Daniel Häufle, Bernhard Schölkopf, Dieter Büchler</author><pubDate>Wed, 13 Sep 2023 13:41:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06921v1</guid></item><item><title>IC3D: Image-Conditioned 3D Diffusion for Shape Generation</title><link>http://arxiv.org/abs/2211.10865v3</link><description>In recent years, Denoising Diffusion Probabilistic Models (DDPMs) havedemonstrated exceptional performance in various 2D generative tasks. Followingthis success, DDPMs have been extended to 3D shape generation, surpassingprevious methodologies in this domain. While many of these models areunconditional, some have explored the potential of using guidance fromdifferent modalities. In particular, image guidance for 3D generation has beenexplored through the utilization of CLIP embeddings. However, these embeddingsare designed to align images and text, and do not necessarily capture thespecific details needed for shape generation. To address this limitation andenhance image-guided 3D DDPMs with augmented 3D understanding, we introduceCISP (Contrastive Image-Shape Pre-training), obtaining a well-structuredimage-shape joint embedding space. Building upon CISP, we then introduce IC3D,a DDPM that harnesses CISP's guidance for 3D shape generation from single-viewimages. This generative diffusion model outperforms existing benchmarks in bothquality and diversity of generated 3D shapes. Moreover, despite IC3D'sgenerative nature, its generated shapes are preferred by human evaluators overa competitive single-view 3D reconstruction model. These properties contributeto a coherent embedding space, enabling latent interpolation and conditionedgeneration also from out-of-distribution images. We find IC3D able to generatecoherent and diverse completions also when presented with occluded views,rendering it applicable in controlled real-world scenarios.</description><author>Cristian Sbrolli, Paolo Cudrano, Matteo Frosi, Matteo Matteucci</author><pubDate>Wed, 13 Sep 2023 13:37:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10865v3</guid></item><item><title>Continual Learning with Dirichlet Generative-based Rehearsal</title><link>http://arxiv.org/abs/2309.06917v1</link><description>Recent advancements in data-driven task-oriented dialogue systems (ToDs)struggle with incremental learning due to computational constraints andtime-consuming issues. Continual Learning (CL) attempts to solve this byavoiding intensive pre-training, but it faces the problem of catastrophicforgetting (CF). While generative-based rehearsal CL methods have madesignificant strides, generating pseudo samples that accurately reflect theunderlying task-specific distribution is still a challenge. In this paper, wepresent Dirichlet Continual Learning (DCL), a novel generative-based rehearsalstrategy for CL. Unlike the traditionally used Gaussian latent variable in theConditional Variational Autoencoder (CVAE), DCL leverages the flexibility andversatility of the Dirichlet distribution to model the latent prior variable.This enables it to efficiently capture sentence-level features of previoustasks and effectively guide the generation of pseudo samples. In addition, weintroduce Jensen-Shannon Knowledge Distillation (JSKD), a robust logit-basedknowledge distillation method that enhances knowledge transfer during pseudosample generation. Our experiments confirm the efficacy of our approach in bothintent detection and slot-filling tasks, outperforming state-of-the-artmethods.</description><author>Min Zeng, Wei Xue, Qifeng Liu, Yike Guo</author><pubDate>Wed, 13 Sep 2023 13:30:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06917v1</guid></item><item><title>Online Submodular Maximization via Online Convex Optimization</title><link>http://arxiv.org/abs/2309.04339v2</link><description>We study monotone submodular maximization under general matroid constraintsin the online setting. We prove that online optimization of a large class ofsubmodular functions, namely, weighted threshold potential functions, reducesto online convex optimization (OCO). This is precisely because functions inthis class admit a concave relaxation; as a result, OCO policies, coupled withan appropriate rounding scheme, can be used to achieve sublinear regret in thecombinatorial setting. We show that our reduction extends to many differentversions of the online learning problem, including the dynamic regret, bandit,and optimistic-learning settings.</description><author>Tareq Si-Salem, Gözde Özcan, Iasonas Nikolaou, Evimaria Terzi, Stratis Ioannidis</author><pubDate>Wed, 13 Sep 2023 13:22:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04339v2</guid></item><item><title>Of Models and Tin Men: A Behavioural Economics Study of Principal-Agent Problems in AI Alignment using Large-Language Models</title><link>http://arxiv.org/abs/2307.11137v3</link><description>AI Alignment is often presented as an interaction between a single designerand an artificial agent in which the designer attempts to ensure the agent'sbehavior is consistent with its purpose, and risks arise solely because ofconflicts caused by inadvertent misalignment between the utility functionintended by the designer and the resulting internal utility function of theagent. With the advent of agents instantiated with large-language models(LLMs), which are typically pre-trained, we argue this does not capture theessential aspects of AI safety because in the real world there is not aone-to-one correspondence between designer and agent, and the many agents, bothartificial and human, have heterogeneous values. Therefore, there is aneconomic aspect to AI safety and the principal-agent problem is likely toarise. In a principal-agent problem conflict arises because of informationasymmetry together with inherent misalignment between the utility of the agentand its principal, and this inherent misalignment cannot be overcome bycoercing the agent into adopting a desired utility function through training.We argue the assumptions underlying principal-agent problems are crucial tocapturing the essence of safety problems involving pre-trained AI models inreal-world situations. Taking an empirical approach to AI safety, weinvestigate how GPT models respond in principal-agent conflicts. We find thatagents based on both GPT-3.5 and GPT-4 override their principal's objectives ina simple online shopping task, showing clear evidence of principal-agentconflict. Surprisingly, the earlier GPT-3.5 model exhibits more nuancedbehaviour in response to changes in information asymmetry, whereas the laterGPT-4 model is more rigid in adhering to its prior alignment. Our resultshighlight the importance of incorporating principles from economics into thealignment process.</description><author>Steve Phelps, Rebecca Ranson</author><pubDate>Wed, 13 Sep 2023 13:19:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11137v3</guid></item><item><title>A Comprehensive Overview of Large Language Models</title><link>http://arxiv.org/abs/2307.06435v3</link><description>Large Language Models (LLMs) have recently demonstrated remarkablecapabilities in natural language processing tasks and beyond. This success ofLLMs has led to a large influx of research contributions in this direction.These works encompass diverse topics such as architectural innovations of theunderlying neural networks, context length improvements, model alignment,training datasets, benchmarking, efficiency and more. With the rapiddevelopment of techniques and regular breakthroughs in LLM research, it hasbecome considerably challenging to perceive the bigger picture of the advancesin this direction. Considering the rapidly emerging plethora of literature onLLMs, it is imperative that the research community is able to benefit from aconcise yet comprehensive overview of the recent developments in this field.This article provides that overview to the research community. It not onlyfocuses on a systematic treatment of the existing literature on a broad rangeof LLM related concept, but also pays special attention to providingcomprehensive summaries with extensive details about the individual existingmodels, datasets and major insights. We also pay heed to aligning our overviewwith the emerging outlook of this research direction by accounting for theother recently materializing reviews of the broader research direction of LLMs.Our self-contained comprehensive overview of LLMs discusses relevant backgroundconcepts along with covering the advanced topics at the frontier of thisresearch direction. This review article is intended to not only provide asystematic survey, but also a quick comprehensive reference for the researchersand practitioners to draw insights from extensive informative summaries of theexisting works to advance the LLM research direction.</description><author>Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, Ajmal Mian</author><pubDate>Wed, 13 Sep 2023 13:13:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06435v3</guid></item><item><title>Fixed points of nonnegative neural networks</title><link>http://arxiv.org/abs/2106.16239v7</link><description>We use fixed point theory to analyze nonnegative neural networks, which wedefine as neural networks that map nonnegative vectors to nonnegative vectors.We first show that nonnegative neural networks with nonnegative weights andbiases can be recognized as monotonic and (weakly) scalable functions withinthe framework of nonlinear Perron-Frobenius theory. This fact enables us toprovide conditions for the existence of fixed points of nonnegative neuralnetworks having inputs and outputs of the same dimension, and these conditionsare weaker than those recently obtained using arguments in convex analysis.Furthermore, we prove that the shape of the fixed point set of nonnegativeneural networks with nonnegative weights and biases is an interval, which undermild conditions degenerates to a point. These results are then used to obtainthe existence of fixed points of more general nonnegative neural networks. Froma practical perspective, our results contribute to the understanding of thebehavior of autoencoders, and the main theoretical results are verified innumerical simulations using the Modified National Institute of Standards andTechnology (MNIST) dataset.</description><author>Tomasz J. Piotrowski, Renato L. G. Cavalcante, Mateusz Gabor</author><pubDate>Wed, 13 Sep 2023 13:12:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.16239v7</guid></item><item><title>Generalization error bounds for iterative learning algorithms with bounded updates</title><link>http://arxiv.org/abs/2309.05077v2</link><description>This paper explores the generalization characteristics of iterative learningalgorithms with bounded updates for non-convex loss functions, employinginformation-theoretic techniques. Our key contribution is a novel bound for thegeneralization error of these algorithms with bounded updates, extending beyondthe scope of previous works that only focused on Stochastic Gradient Descent(SGD). Our approach introduces two main novelties: 1) we reformulate the mutualinformation as the uncertainty of updates, providing a new perspective, and 2)instead of using the chaining rule of mutual information, we employ a variancedecomposition technique to decompose information across iterations, allowingfor a simpler surrogate process. We analyze our generalization bound undervarious settings and demonstrate improved bounds when the model dimensionincreases at the same rate as the number of training data samples. To bridgethe gap between theory and practice, we also examine the previously observedscaling behavior in large language models. Ultimately, our work takes a furtherstep for developing practical generalization theories.</description><author>Jingwen Fu, Nanning Zheng</author><pubDate>Wed, 13 Sep 2023 13:12:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05077v2</guid></item><item><title>Towards the TopMost: A Topic Modeling System Toolkit</title><link>http://arxiv.org/abs/2309.06908v1</link><description>Topic models have been proposed for decades with various applications andrecently refreshed by the neural variational inference. However, these topicmodels adopt totally distinct dataset, implementation, and evaluation settings,which hinders their quick utilization and fair comparisons. This greatlyhinders the research progress of topic models. To address these issues, in thispaper we propose a Topic Modeling System Toolkit (TopMost). Compared toexisting toolkits, TopMost stands out by covering a wider range of topicmodeling scenarios including complete lifecycles with dataset pre-processing,model training, testing, and evaluations. The highly cohesive and decoupledmodular design of TopMost enables quick utilization, fair comparisons, andflexible extensions of different topic models. This can facilitate the researchand applications of topic models. Our code, tutorials, and documentation areavailable at https://github.com/bobxwu/topmost.</description><author>Xiaobao Wu, Fengjun Pan, Anh Tuan Luu</author><pubDate>Wed, 13 Sep 2023 13:10:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06908v1</guid></item><item><title>CCSPNet-Joint: Efficient Joint Training Method for Traffic Sihn Detection Under Extreme Conditions</title><link>http://arxiv.org/abs/2309.06902v1</link><description>Traffic sign detection is an important research direction in intelligentdriving. Unfortunately, existing methods often overlook extreme conditions suchas fog, rain, and motion blur. Moreover, the end-to-end training strategy forimage denoising and object detection models fails to utilize inter-modelinformation effectively. To address these issues, we propose CCSPNet, anefficient feature extraction module based on Transformers and CNNs, whicheffectively leverages contextual information, achieves faster inference speedand provides stronger feature enhancement capabilities. Furthermore, weestablish the correlation between object detection and image denoising tasksand propose a joint training model, CCSPNet-Joint, to improve data efficiencyand generalization. Finally, to validate our approach, we create the CCTSDB-AUGdataset for traffic sign detection in extreme scenarios. Extensive experimentshave shown that CCSPNet achieves state-of-the-art performance in traffic signdetection under extreme conditions. Compared to end-to-end methods,CCSPNet-Joint achieves a 5.32% improvement in precision and an 18.09%improvement in mAP@.5.</description><author>Haoqin Hong, Yue Zhou, Xiangyu Shu, Xiangfang Hu</author><pubDate>Wed, 13 Sep 2023 13:00:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06902v1</guid></item><item><title>Action Sensitivity Learning for Temporal Action Localization</title><link>http://arxiv.org/abs/2305.15701v2</link><description>Temporal action localization (TAL), which involves recognizing and locatingaction instances, is a challenging task in video understanding. Most existingapproaches directly predict action classes and regress offsets to boundaries,while overlooking the discrepant importance of each frame. In this paper, wepropose an Action Sensitivity Learning framework (ASL) to tackle this task,which aims to assess the value of each frame and then leverage the generatedaction sensitivity to recalibrate the training procedure. We first introduce alightweight Action Sensitivity Evaluator to learn the action sensitivity at theclass level and instance level, respectively. The outputs of the two branchesare combined to reweight the gradient of the two sub-tasks. Moreover, based onthe action sensitivity of each frame, we design an Action Sensitive ContrastiveLoss to enhance features, where the action-aware frames are sampled as positivepairs to push away the action-irrelevant frames. The extensive studies onvarious action localization benchmarks (i.e., MultiThumos, Charades,Ego4D-Moment Queries v1.0, Epic-Kitchens 100, Thumos14 and ActivityNet1.3) showthat ASL surpasses the state-of-the-art in terms of average-mAP under multipletypes of scenarios, e.g., single-labeled, densely-labeled and egocentric.</description><author>Jiayi Shao, Xiaohan Wang, Ruijie Quan, Junjun Zheng, Jiang Yang, Yi Yang</author><pubDate>Wed, 13 Sep 2023 12:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15701v2</guid></item><item><title>An Empirical Evaluation of Temporal Graph Benchmark</title><link>http://arxiv.org/abs/2307.12510v4</link><description>In this paper, we conduct an empirical evaluation of Temporal Graph Benchmark(TGB) by extending our Dynamic Graph Library (DyGLib) to TGB. Compared withTGB, we include eleven popular dynamic graph learning methods for moreexhaustive comparisons. Through the experiments, we find that (1) differentmodels depict varying performance across various datasets, which is in linewith previous observations; (2) the performance of some baselines can besignificantly improved over the reported results in TGB when using DyGLib. Thiswork aims to ease the researchers' efforts in evaluating various dynamic graphlearning methods on TGB and attempts to offer results that can be directlyreferenced in the follow-up research. All the used resources in this projectare publicly available at https://github.com/yule-BUAA/DyGLib_TGB. This work isin progress, and feedback from the community is welcomed for improvements.</description><author>Le Yu</author><pubDate>Wed, 13 Sep 2023 12:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12510v4</guid></item></channel></rss>