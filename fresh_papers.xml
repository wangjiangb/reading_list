<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 15 Nov 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Instant3D: Instant Text-to-3D Generation</title><link>http://arxiv.org/abs/2311.08403v1</link><description>Text-to-3D generation, which aims to synthesize vivid 3D objects from textprompts, has attracted much attention from the computer vision community. Whileseveral existing works have achieved impressive results for this task, theymainly rely on a time-consuming optimization paradigm. Specifically, thesemethods optimize a neural field from scratch for each text prompt, takingapproximately one hour or more to generate one object. This heavy andrepetitive training cost impedes their practical deployment. In this paper, wepropose a novel framework for fast text-to-3D generation, dubbed Instant3D.Once trained, Instant3D is able to create a 3D object for an unseen text promptin less than one second with a single run of a feedforward network. We achievethis remarkable speed by devising a new network that directly constructs a 3Dtriplane from a text prompt. The core innovation of our Instant3D lies in ourexploration of strategies to effectively inject text conditions into thenetwork. Furthermore, we propose a simple yet effective activation function,the scaled-sigmoid, to replace the original sigmoid function, which speeds upthe training convergence by more than ten times. Finally, to address the Janus(multi-head) problem in 3D generation, we propose an adaptive Perp-Negalgorithm that can dynamically adjust its concept negation scales according tothe severity of the Janus problem during training, effectively reducing themulti-head effect. Extensive experiments on a wide variety of benchmarkdatasets demonstrate that the proposed algorithm performs favorably against thestate-of-the-art methods both qualitatively and quantitatively, while achievingsignificantly better efficiency. The project page is athttps://ming1993li.github.io/Instant3DProj.</description><author>Ming Li, Pan Zhou, Jia-Wei Liu, Jussi Keppo, Min Lin, Shuicheng Yan, Xiangyu Xu</author><pubDate>Tue, 14 Nov 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08403v1</guid></item><item><title>Retrieve and Copy: Scaling ASR Personalization to Large Catalogs</title><link>http://arxiv.org/abs/2311.08402v1</link><description>Personalization of automatic speech recognition (ASR) models is a widelystudied topic because of its many practical applications. Most recently,attention-based contextual biasing techniques are used to improve therecognition of rare words and domain specific entities. However, due toperformance constraints, the biasing is often limited to a few thousandentities, restricting real-world usability. To address this, we first propose a"Retrieve and Copy" mechanism to improve latency while retaining the accuracyeven when scaled to a large catalog. We also propose a training strategy toovercome the degradation in recall at such scale due to an increased number ofconfusing entities. Overall, our approach achieves up to 6% more Word ErrorRate reduction (WERR) and 3.6% absolute improvement in F1 when compared to astrong baseline. Our method also allows for large catalog sizes of up to 20Kwithout significantly affecting WER and F1-scores, while achieving at least 20%inference speedup per acoustic frame.</description><author>Sai Muralidhar Jayanthi, Devang Kulshreshtha, Saket Dingliwal, Srikanth Ronanki, Sravan Bodapati</author><pubDate>Tue, 14 Nov 2023 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08402v1</guid></item><item><title>Fine-tuning Language Models for Factuality</title><link>http://arxiv.org/abs/2311.08401v1</link><description>The fluency and creativity of large pre-trained language models (LLMs) haveled to their widespread use, sometimes even as a replacement for traditionalsearch engines. Yet language models are prone to making convincing butfactually inaccurate claims, often referred to as 'hallucinations.' Theseerrors can inadvertently spread misinformation or harmfully perpetuatemisconceptions. Further, manual fact-checking of model responses is atime-consuming process, making human factuality labels expensive to acquire. Inthis work, we fine-tune language models to be more factual, without humanlabeling and targeting more open-ended generation settings than past work. Weleverage two key recent innovations in NLP to do so. First, several recentworks have proposed methods for judging the factuality of open-ended text bymeasuring consistency with an external knowledge base or simply a large model'sconfidence scores. Second, the direct preference optimization algorithm enablesstraightforward fine-tuning of language models on objectives other thansupervised imitation, using a preference ranking over possible model responses.We show that learning from automatically generated factuality preferencerankings, generated either through existing retrieval systems or our novelretrieval-free approach, significantly improves the factuality (percent ofgenerated claims that are correct) of Llama-2 on held-out topics compared withRLHF or decoding strategies targeted at factuality. At 7B scale, compared toLlama-2-chat, we observe 58% and 40% reduction in factual error rate whengenerating biographies and answering medical questions, respectively.</description><author>Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D. Manning, Chelsea Finn</author><pubDate>Tue, 14 Nov 2023 18:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08401v1</guid></item><item><title>Towards Open-Ended Visual Recognition with Large Language Model</title><link>http://arxiv.org/abs/2311.08400v1</link><description>Localizing and recognizing objects in the open-ended physical world poses along-standing challenge within the domain of machine perception. Recent methodshave endeavored to address the issue by employing a class-agnostic mask (orbox) proposal model, complemented by an open-vocabulary classifier (e.g., CLIP)using pre-extracted text embeddings. However, it is worth noting that theseopen-vocabulary recognition models still exhibit limitations in practicalapplications. On one hand, they rely on the provision of class names duringtesting, where the recognition performance heavily depends on this predefinedset of semantic classes by users. On the other hand, when training withmultiple datasets, human intervention is required to alleviate the labeldefinition conflict between them. In this paper, we introduce the OmniScientModel (OSM), a novel Large Language Model (LLM) based mask classifier, as astraightforward and effective solution to the aforementioned challenges.Specifically, OSM predicts class labels in a generative manner, thus removingthe supply of class names during both training and testing. It also enablescross-dataset training without any human interference, exhibiting robustgeneralization capabilities due to the world knowledge acquired from the LLM.By combining OSM with an off-the-shelf mask proposal model, we presentpromising results on various benchmarks, and demonstrate its effectiveness inhandling novel concepts. Code/model are available athttps://github.com/bytedance/OmniScient-Model.</description><author>Qihang Yu, Xiaohui Shen, Liang-Chieh Chen</author><pubDate>Tue, 14 Nov 2023 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08400v1</guid></item><item><title>Are Large Language Models Temporally Grounded?</title><link>http://arxiv.org/abs/2311.08398v1</link><description>Are Large language models (LLMs) temporally grounded? Since LLMs cannotperceive and interact with the environment, it is impossible to answer thisquestion directly. Instead, we provide LLMs with textual narratives and probethem with respect to their common-sense knowledge of the structure and durationof events, their ability to order events along a timeline, and self-consistencywithin their temporal model (e.g., temporal relations such as after and beforeare mutually exclusive for any pair of events). We evaluate state-of-the-artLLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities.Generally, we find that LLMs lag significantly behind both human performance aswell as small-scale, specialised LMs. In-context learning, instruction tuning,and chain-of-thought prompting reduce this gap only to a limited degree.Crucially, LLMs struggle the most with self-consistency, displaying incoherentbehaviour in at least 27.23% of their predictions. Contrary to expectations, wealso find that scaling the model size does not guarantee positive gains inperformance. To explain these results, we study the sources from which LLMs maygather temporal information: we find that sentence ordering in unlabelledtexts, available during pre-training, is only weakly correlated with eventordering. Moreover, public instruction tuning mixtures contain few temporaltasks. Hence, we conclude that current LLMs lack a consistent temporal model oftextual narratives. Code, datasets, and LLM outputs are available athttps://github.com/yfqiu-nlp/temporal-llms.</description><author>Yifu Qiu, Zheng Zhao, Yftah Ziser, Anna Korhonen, Edoardo M. Ponti, Shay B. Cohen</author><pubDate>Tue, 14 Nov 2023 18:57:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08398v1</guid></item><item><title>Zero-shot audio captioning with audio-language model guidance and audio context keywords</title><link>http://arxiv.org/abs/2311.08396v1</link><description>Zero-shot audio captioning aims at automatically generating descriptivetextual captions for audio content without prior training for this task.Different from speech recognition which translates audio content that containsspoken language into text, audio captioning is commonly concerned with ambientsounds, or sounds produced by a human performing an action. Inspired byzero-shot image captioning methods, we propose ZerAuCap, a novel framework forsummarising such general audio signals in a text caption without requiringtask-specific training. In particular, our framework exploits a pre-trainedlarge language model (LLM) for generating the text which is guided by apre-trained audio-language model to produce captions that describe the audiocontent. Additionally, we use audio context keywords that prompt the languagemodel to generate text that is broadly relevant to sounds. Our proposedframework achieves state-of-the-art results in zero-shot audio captioning onthe AudioCaps and Clotho datasets. Our code is available athttps://github.com/ExplainableML/ZerAuCap.</description><author>Leonard Salewski, Stefan Fauth, A. Sophia Koepke, Zeynep Akata</author><pubDate>Tue, 14 Nov 2023 18:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08396v1</guid></item><item><title>MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable Trajectory Generation</title><link>http://arxiv.org/abs/2311.08393v1</link><description>The learn-from-observation (LfO) paradigm is a human-inspired mode for arobot to learn to perform a task simply by watching it being performed. LfO canfacilitate robot integration on factory floors by minimizing disruption andreducing tedious programming. A key component of the LfO pipeline is atransformation of the depth camera frames to the corresponding task state andaction pairs, which are then relayed to learning techniques such as imitationor inverse reinforcement learning for understanding the task parameters. Whileseveral existing computer vision models analyze videos for activityrecognition, SA-Net specifically targets robotic LfO from RGB-D data. However,SA-Net and many other models analyze frame data captured from a singleviewpoint. Their analysis is therefore highly sensitive to occlusions of theobserved task, which are frequent in deployments. An obvious way of reducingocclusions is to simultaneously observe the task from multiple viewpoints andsynchronously fuse the multiple streams in the model. Toward this, we presentmulti-view SA-Net, which generalizes the SA-Net model to allow the perceptionof multiple viewpoints of the task activity, integrate them, and betterrecognize the state and action in each frame. Performance evaluations on twodistinct domains establish that MVSA-Net recognizes the state-action pairsunder occlusion more accurately compared to single-view MVSA-Net and otherbaselines. Our ablation studies further evaluate its performance underdifferent ambient conditions and establish the contribution of the architecturecomponents. As such, MVSA-Net offers a significantly more robust and deployablestate-action trajectory generation compared to previous methods.</description><author>Ehsan Asali, Prashant Doshi, Jin Sun</author><pubDate>Tue, 14 Nov 2023 18:53:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08393v1</guid></item><item><title>A Material Lens on Coloniality in NLP</title><link>http://arxiv.org/abs/2311.08391v1</link><description>Coloniality, the continuation of colonial harms beyond "official"colonization, has pervasive effects across society and scientific fields.Natural Language Processing (NLP) is no exception to this broad phenomenon. Inthis work, we argue that coloniality is implicitly embedded in and amplified byNLP data, algorithms, and software. We formalize this analysis usingActor-Network Theory (ANT): an approach to understanding social phenomenathrough the network of relationships between human stakeholders and technology.We use our Actor-Network to guide a quantitative survey of the geography ofdifferent phases of NLP research, providing evidence that inequality alongcolonial boundaries increases as NLP builds on itself. Based on this, we arguethat combating coloniality in NLP requires not only changing current values butalso active work to remove the accumulation of colonial ideals in ourfoundational data and algorithms.</description><author>William Held, Camille Harris, Michael Best, Diyi Yang</author><pubDate>Tue, 14 Nov 2023 18:52:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08391v1</guid></item><item><title>On What Basis? Predicting Text Preference Via Structured Comparative Reasoning</title><link>http://arxiv.org/abs/2311.08390v1</link><description>Comparative reasoning plays a crucial role in text preference prediction;however, large language models (LLMs) often demonstrate inconsistencies intheir reasoning. While approaches like Chain-of-Thought improve accuracy inmany other settings, they struggle to consistently distinguish the similaritiesand differences of complex texts. We introduce SC, a prompting approach thatpredicts text preferences by generating structured intermediate comparisons. SCbegins by proposing aspects of comparison, followed by generating textualcomparisons under each aspect. We select consistent comparisons with a pairwiseconsistency comparator that ensures each aspect's comparisons clearlydistinguish differences between texts, significantly reducing hallucination andimproving consistency. Our comprehensive evaluations across various NLP tasks,including summarization, retrieval, and automatic rating, demonstrate that SCequips LLMs to achieve state-of-the-art performance in text preferenceprediction.</description><author>Jing Nathan Yan, Tianqi Liu, Justin T Chiu, Jiaming Shen, Zhen Qin, Yue Yu, Yao Zhao, Charu Lakshmanan, Yair Kurzion, Alexander M. Rush, Jialu Liu, Michael Bendersky</author><pubDate>Tue, 14 Nov 2023 18:51:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08390v1</guid></item><item><title>TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer</title><link>http://arxiv.org/abs/2311.08389v1</link><description>Text style is highly abstract, as it encompasses various aspects of aspeaker's characteristics, habits, logical thinking, and the content theyexpress. However, previous text-style transfer tasks have primarily focused ondata-driven approaches, lacking in-depth analysis and research from theperspectives of linguistics and cognitive science. In this paper, we introducea novel task called Text Speech-Style Transfer (TSST). The main objective is tofurther explore topics related to human cognition, such as personality andemotion, based on the capabilities of existing LLMs. Considering the objectiveof our task and the distinctive characteristics of oral speech in real-lifescenarios, we trained multi-dimension (i.e. filler words, vividness,interactivity, emotionality) evaluation models for the TSST and validated theircorrelation with human assessments. We thoroughly analyze the performance ofseveral large language models (LLMs) and identify areas where furtherimprovement is needed. Moreover, driven by our evaluation models, we havereleased a new corpus that improves the capabilities of LLMs in generating textwith speech-style characteristics. In summary, we present the TSST task, a newbenchmark for style transfer and emphasizing human-oriented evaluation,exploring and advancing the performance of current LLMs.</description><author>Huashan Sun, Yixiao Wu, Yinghao Li, Jiawei Li, Yizhe Yang, Yang Gao</author><pubDate>Tue, 14 Nov 2023 18:50:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08389v1</guid></item><item><title>ChOiRe: Characterizing and Predicting Human Opinions with Chain of Opinion Reasoning</title><link>http://arxiv.org/abs/2311.08385v1</link><description>Aligning language models (LMs) with human opinion is challenging yet vital toenhance their grasp of human values, preferences, and beliefs. We presentChOiRe, a four-step solution framework to predict human opinion thatdifferentiates between the user explicit personae (i.e. demographic orideological attributes) that are manually declared and implicit personaeinferred from user historical opinions. Specifically, it consists of (i) an LManalyzing the user explicit personae to filter out irrelevant attributes; (ii)the LM ranking the implicit persona opinions into a preferential list; (iii)Chain-of-Opinion (CoO) reasoning, where the LM sequentially analyzes theexplicit personae and the most relevant implicit personae to perform opinionprediction; (iv) and where ChOiRe executes Step (iii) CoO multiple times withincreasingly larger lists of implicit personae to overcome insufficientpersonae information to infer a final result. ChOiRe achieves newstate-of-the-art effectiveness with limited inference calls, improving previousLLM-based techniques significantly by 3.22%.</description><author>Xuan Long Do, Kenji Kawaguchi, Min Yen Kan, Nancy F. Chen</author><pubDate>Tue, 14 Nov 2023 18:48:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08385v1</guid></item><item><title>Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees</title><link>http://arxiv.org/abs/2311.08384v1</link><description>Hybrid RL is the setting where an RL agent has access to both offline dataand online data by interacting with the real-world environment. In this work,we propose a new hybrid RL algorithm that combines an on-policy actor-criticmethod with offline data. On-policy methods such as policy gradient and naturalpolicy gradient (NPG) have shown to be more robust to model misspecification,though sometimes it may not be as sample efficient as methods that rely onoff-policy learning. On the other hand, offline methods that depend onoff-policy training often require strong assumptions in theory and are lessstable to train in practice. Our new approach integrates a procedure ofoff-policy training on the offline data into an on-policy NPG framework. Weshow that our approach, in theory, can obtain a best-of-both-worlds type ofresult -- it achieves the state-of-art theoretical guarantees of offline RLwhen offline RL-specific assumptions hold, while at the same time maintainingthe theoretical guarantees of on-policy NPG regardless of the offline RLassumptions' validity. Experimentally, in challenging rich-observationenvironments, we show that our approach outperforms a state-of-the-art hybridRL baseline which only relies on off-policy policy optimization, demonstratingthe empirical benefit of combining on-policy and off-policy learning. Our codeis publicly available at https://github.com/YifeiZhou02/HNPG.</description><author>Yifei Zhou, Ayush Sekhari, Yuda Song, Wen Sun</author><pubDate>Tue, 14 Nov 2023 18:45:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08384v1</guid></item><item><title>Direct Preference Optimization for Neural Machine Translation with Minimum Bayes Risk Decoding</title><link>http://arxiv.org/abs/2311.08380v1</link><description>Minimum Bayes Risk (MBR) decoding can significantly improve translationperformance of Multilingual Large Language Models (MLLMs). However, MBRdecoding is computationally expensive and in this paper, we show how recentlydeveloped Reinforcement Learning (RL) technique, Direct Preference Optimization(DPO) can be used to fine-tune MLLMs so that we get the gains from MBR withoutthe additional computation in inference. Our fine-tuned models havesignificantly improved performance on multiple NMT test sets compared to baseMLLMs without preference optimization. Our method boosts the translationperformance of MLLMs using relatively small monolingual fine-tuning sets.</description><author>Guangyu Yang, Jinghong Chen, Weizhe Lin, Bill Byrne</author><pubDate>Tue, 14 Nov 2023 18:43:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08380v1</guid></item><item><title>Scheming AIs: Will AIs fake alignment during training in order to get power?</title><link>http://arxiv.org/abs/2311.08379v1</link><description>This report examines whether advanced AIs that perform well in training willbe doing so in order to gain power later -- a behavior I call "scheming" (alsosometimes called "deceptive alignment"). I conclude that scheming is adisturbingly plausible outcome of using baseline machine learning methods totrain goal-directed AIs sophisticated enough to scheme (my subjectiveprobability on such an outcome, given these conditions, is roughly 25%). Inparticular: if performing well in training is a good strategy for gaining power(as I think it might well be), then a very wide variety of goals would motivatescheming -- and hence, good training performance. This makes it plausible thattraining might either land on such a goal naturally and then reinforce it, oractively push a model's motivations towards such a goal as an easy way ofimproving performance. What's more, because schemers pretend to be aligned ontests designed to reveal their motivations, it may be quite difficult to tellwhether this has occurred. However, I also think there are reasons for comfort.In particular: scheming may not actually be such a good strategy for gainingpower; various selection pressures in training might work against schemer-likegoals (for example, relative to non-schemers, schemers need to engage in extrainstrumental reasoning, which might harm their training performance); and wemay be able to increase such pressures intentionally. The report discussesthese and a wide variety of other considerations in detail, and it suggests anarray of empirical research directions for probing the topic further.</description><author>Joe Carlsmith</author><pubDate>Tue, 14 Nov 2023 18:42:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08379v1</guid></item><item><title>Learning to Filter Context for Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2311.08377v1</link><description>On-the-fly retrieval of relevant knowledge has proven an essential element ofreliable systems for tasks such as open-domain question answering and factverification. However, because retrieval systems are not perfect, generationmodels are required to generate outputs given partially or entirely irrelevantpassages. This can cause over- or under-reliance on context, and result inproblems in the generated output such as hallucinations. To alleviate theseproblems, we propose FILCO, a method that improves the quality of the contextprovided to the generator by (1) identifying useful context based on lexicaland information-theoretic approaches, and (2) training context filtering modelsthat can filter retrieved contexts at test time. We experiment on sixknowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that ourmethod outperforms existing approaches on extractive question answering (QA),complex multi-hop and long-form QA, fact verification, and dialog generationtasks. FILCO effectively improves the quality of context, whether or not itsupports the canonical output.</description><author>Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, Graham Neubig</author><pubDate>Tue, 14 Nov 2023 18:41:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08377v1</guid></item><item><title>Ensemble sampling for linear bandits: small ensembles suffice</title><link>http://arxiv.org/abs/2311.08376v1</link><description>We provide the first useful, rigorous analysis of ensemble sampling for thestochastic linear bandit setting. In particular, we show that, under standardassumptions, for a $d$-dimensional stochastic linear bandit with an interactionhorizon $T$, ensemble sampling with an ensemble of size $m$ on the order of $d\log T$ incurs regret bounded by order $(d \log T)^{5/2} \sqrt{T}$. Ours is thefirst result in any structured setting not to require the size of the ensembleto scale linearly with $T$ -- which defeats the purpose of ensemble sampling --while obtaining near $\sqrt{T}$ order regret. Ours is also the first resultthat allows infinite action sets.</description><author>David Janz, Alexander E. Litvak, Csaba Szepesvári</author><pubDate>Tue, 14 Nov 2023 18:41:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08376v1</guid></item><item><title>A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts</title><link>http://arxiv.org/abs/2311.08374v1</link><description>In the realm of text manipulation and linguistic transformation, the questionof authorship has always been a subject of fascination and philosophicalinquiry. Much like the \textbf{Ship of Theseus paradox}, which ponders whethera ship remains the same when each of its original planks is replaced, ourresearch delves into an intriguing question: \textit{Does a text retain itsoriginal authorship when it undergoes numerous paraphrasing iterations?}Specifically, since Large Language Models (LLMs) have demonstrated remarkableproficiency in the generation of both original content and the modification ofhuman-authored texts, a pivotal question emerges concerning the determinationof authorship in instances where LLMs or similar paraphrasing tools areemployed to rephrase the text. This inquiry revolves around \textit{whetherauthorship should be attributed to the original human author or the AI-poweredtool, given the tool's independent capacity to produce text that closelyresembles human-generated content.} Therefore, we embark on a philosophicalvoyage through the seas of language and authorship to unravel this intricatepuzzle.</description><author>Nafis Irtiza Tripto, Saranya Venkatraman, Dominik Macko, Robert Moro, Ivan Srba, Adaku Uchendu, Thai Le, Dongwon Lee</author><pubDate>Tue, 14 Nov 2023 18:40:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08374v1</guid></item><item><title>USLR: an open-source tool for unbiased and smooth longitudinal registration of brain MR</title><link>http://arxiv.org/abs/2311.08371v1</link><description>We present USLR, a computational framework for longitudinal registration ofbrain MRI scans to estimate nonlinear image trajectories that are smooth acrosstime, unbiased to any timepoint, and robust to imaging artefacts. It operateson the Lie algebra parameterisation of spatial transforms (which is compatiblewith rigid transforms and stationary velocity fields for nonlinear deformation)and takes advantage of log-domain properties to solve the problem usingBayesian inference. USRL estimates rigid and nonlinear registrations that: (i)bring all timepoints to an unbiased subject-specific space; and (i) compute asmooth trajectory across the imaging time-series. We capitalise onlearning-based registration algorithms and closed-form expressions for fastinference. A use-case Alzheimer's disease study is used to showcase thebenefits of the pipeline in multiple fronts, such as time-consistent imagesegmentation to reduce intra-subject variability, subject-specific predictionor population analysis using tensor-based morphometry. We demonstrate that suchapproach improves upon cross-sectional methods in identifying groupdifferences, which can be helpful in detecting more subtle atrophy levels or inreducing sample sizes in clinical trials. The code is publicly available inhttps://github.com/acasamitjana/uslr</description><author>Adrià Casamitjana, Roser Sala-Llonch, Karim Lekadir, Juan Eugenio Iglesias</author><pubDate>Tue, 14 Nov 2023 18:34:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08371v1</guid></item><item><title>SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models</title><link>http://arxiv.org/abs/2311.08370v1</link><description>The past year has seen rapid acceleration in the development of largelanguage models (LLMs). For many tasks, there is now a wide range ofopen-source and open-access LLMs that are viable alternatives to proprietarymodels like ChatGPT. Without proper steering and safeguards, however, LLMs willreadily follow malicious instructions, provide unsafe advice, and generatetoxic content. This is a critical safety risk for businesses and developers. Weintroduce SimpleSafetyTests as a new test suite for rapidly and systematicallyidentifying such critical safety risks. The test suite comprises 100 testprompts across five harm areas that LLMs, for the vast majority ofapplications, should refuse to comply with. We test 11 popular open LLMs andfind critical safety weaknesses in several of them. While some LLMs do not givea single unsafe response, most models we test respond unsafely on more than 20%of cases, with over 50% unsafe responses in the extreme. Prepending asafety-emphasising system prompt substantially reduces the occurrence of unsaferesponses, but does not completely stop them from happening. We recommend thatdevelopers use such system prompts as a first line of defence against criticalsafety risks.</description><author>Bertie Vidgen, Hannah Rose Kirk, Rebecca Qian, Nino Scherrer, Anand Kannappan, Scott A. Hale, Paul Röttger</author><pubDate>Tue, 14 Nov 2023 18:33:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08370v1</guid></item><item><title>How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection</title><link>http://arxiv.org/abs/2311.08369v1</link><description>Against the misuse (e.g., plagiarism or spreading misinformation) of LargeLanguage Models (LLMs), many recent works have presented LLM-generated-textdetectors with promising detection performance. Spotlighting a situation whereusers instruct LLMs to generate texts (e.g., essay writing), there are variousways to write the instruction (e.g., what task-oriented constraint to include).In this paper, we discover that even a task-oriented constraint in instructioncan cause the inconsistent performance of current detectors to the generatedtexts. Specifically, we focus on student essay writing as a realistic domainand manually create the task-oriented constraint for each factor on essayquality by Ke and Ng (2019). Our experiment shows that the detectionperformance variance of the current detector on texts generated by instructionwith each task-oriented constraint is up to 20 times larger than the variancecaused by generating texts multiple times and paraphrasing the instruction. Ourfinding calls for further research on developing robust detectors that candetect such distributional shifts caused by a task-oriented constraint in theinstruction.</description><author>Ryuto Koike, Masahiro Kaneko, Naoaki Okazaki</author><pubDate>Tue, 14 Nov 2023 18:32:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08369v1</guid></item><item><title>Exploration of TPUs for AI Applications</title><link>http://arxiv.org/abs/2309.08918v2</link><description>Tensor Processing Units (TPUs) are specialized hardware accelerators for deeplearning developed by Google. This paper aims to explore TPUs in cloud and edgecomputing focusing on its applications in AI. We provide an overview of TPUs,their general architecture, specifically their design in relation to neuralnetworks, compilation techniques and supporting frameworks. Furthermore, weprovide a comparative analysis of Cloud and Edge TPU performance against othercounterpart chip architectures. Our results show that TPUs can providesignificant performance improvements in both cloud and edge computing.Additionally, this paper underscores the imperative need for further researchin optimization techniques for efficient deployment of AI architectures on theEdge TPU and benchmarking standards for a more robust comparative analysis inedge computing scenarios. The primary motivation behind this push for researchis that efficient AI acceleration, facilitated by TPUs, can lead to substantialsavings in terms of time, money, and environmental resources.</description><author>Diego Sanmartín Carrión, Vera Prohaska</author><pubDate>Tue, 14 Nov 2023 18:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08918v2</guid></item><item><title>Using Natural Language Explanations to Rescale Human Judgments</title><link>http://arxiv.org/abs/2305.14770v2</link><description>The rise of large language models (LLMs) has brought a critical need forhigh-quality human-labeled data, particularly for processes like human feedbackand evaluation. A common practice is to label data via consensus annotationover crowdworker judgments. However, annotators' judgments for subjective taskscan differ in many ways: they may have different qualitative judgments about anexample, and they may map those to a labeling scheme in different ways. We showthat these nuances can be captured by natural language explanations, andpropose a method to rescale ordinal annotations and explanations using LLMs.Specifically, we feed annotators' Likert ratings and corresponding explanationsinto an LLM and prompt it to produce a numeric score anchored in a scoringrubric. These scores should reflect the annotators' underlying assessments ofthe example. The rubric can be designed or modified after annotation, andinclude distinctions that may not have been known when the original errortaxonomy was devised. We explore our technique in the context of rating systemoutputs for a document-grounded question answering task, where LLMs achievenear-human performance. Our method rescales the raw judgments without impactingagreement and brings the scores closer to human judgments grounded in the samescoring rubric.</description><author>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</author><pubDate>Tue, 14 Nov 2023 18:30:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14770v2</guid></item><item><title>Plum: Prompt Learning using Metaheuristic</title><link>http://arxiv.org/abs/2311.08364v1</link><description>Since the emergence of large language models, prompt learning has become apopular method for optimizing and customizing these models. Special prompts,such as Chain-of-Thought, have even revealed previously unknown reasoningcapabilities within these models. However, the progress of discoveringeffective prompts has been slow, driving a desire for general promptoptimization methods. Unfortunately, few existing prompt learning methodssatisfy the criteria of being truly "general", i.e., automatic, discrete,black-box, gradient-free, and interpretable all at once. In this paper, weintroduce metaheuristics, a branch of discrete non-convex optimization methodswith over 100 options, as a promising approach to prompt learning. Within ourparadigm, we test six typical methods: hill climbing, simulated annealing,genetic algorithms with/without crossover, tabu search, and harmony search,demonstrating their effectiveness in black-box prompt learning andChain-of-Thought prompt tuning. Furthermore, we show that these methods can beused to discover more human-understandable prompts that were previouslyunknown, opening the door to a cornucopia of possibilities in promptoptimization. We release all the codes in\url{https://github.com/research4pan/Plum}.</description><author>Rui Pan, Shuo Xing, Shizhe Diao, Xiang Liu, Kashun Shum, Jipeng Zhang, Tong Zhang</author><pubDate>Tue, 14 Nov 2023 18:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08364v1</guid></item><item><title>On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers</title><link>http://arxiv.org/abs/2310.14421v3</link><description>Simply-verifiable mathematical conditions for existence, uniqueness andexplicit analytical computation of minimal adversarial paths (MAP) and minimaladversarial distances (MAD) for (locally) uniquely-invertible classifiers, forgeneralized linear models (GLM), and for entropic AI (EAI) are formulated andproven. Practical computation of MAP and MAD, their comparison andinterpretations for various classes of AI tools (for neuronal networks, boostedrandom forests, GLM and EAI) are demonstrated on the common syntheticbenchmarks: on a double Swiss roll spiral and its extensions, as well as on thetwo biomedical data problems (for the health insurance claim predictions, andfor the heart attack lethality classification). On biomedical applications itis demonstrated how MAP provides unique minimal patient-specificrisk-mitigating interventions in the predefined subsets of accessible controlvariables.</description><author>Illia Horenko</author><pubDate>Tue, 14 Nov 2023 18:13:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14421v3</guid></item><item><title>Transformers can optimally learn regression mixture models</title><link>http://arxiv.org/abs/2311.08362v1</link><description>Mixture models arise in many regression problems, but most methods have seenlimited adoption partly due to these algorithms' highly-tailored andmodel-specific nature. On the other hand, transformers are flexible, neuralsequence models that present the intriguing possibility of providinggeneral-purpose prediction methods, even in this mixture setting. In this work,we investigate the hypothesis that transformers can learn an optimal predictorfor mixtures of regressions. We construct a generative process for a mixture oflinear regressions for which the decision-theoretic optimal procedure is givenby data-driven exponential weights on a finite set of parameters. We observethat transformers achieve low mean-squared error on data generated via thisprocess. By probing the transformer's output at inference time, we also showthat transformers typically make predictions that are close to the optimalpredictor. Our experiments also demonstrate that transformers can learnmixtures of regressions in a sample-efficient fashion and are somewhat robustto distribution shifts. We complement our experimental observations by provingconstructively that the decision-theoretic optimal procedure is indeedimplementable by a transformer.</description><author>Reese Pathak, Rajat Sen, Weihao Kong, Abhimanyu Das</author><pubDate>Tue, 14 Nov 2023 18:09:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08362v1</guid></item><item><title>MOPRD: A multidisciplinary open peer review dataset</title><link>http://arxiv.org/abs/2212.04972v2</link><description>Open peer review is a growing trend in academic publications. Public accessto peer review data can benefit both the academic and publishing communities.It also serves as a great support to studies on review comment generation andfurther to the realization of automated scholarly paper review. However, mostof the existing peer review datasets do not provide data that cover the wholepeer review process. Apart from this, their data are not diversified enough asthe data are mainly collected from the field of computer science. These twodrawbacks of the currently available peer review datasets need to be addressedto unlock more opportunities for related studies. In response, we constructMOPRD, a multidisciplinary open peer review dataset. This dataset consists ofpaper metadata, multiple version manuscripts, review comments, meta-reviews,author's rebuttal letters, and editorial decisions. Moreover, we propose amodular guided review comment generation method based on MOPRD. Experimentsshow that our method delivers better performance as indicated by both automaticmetrics and human evaluation. We also explore other potential applications ofMOPRD, including meta-review generation, editorial decision prediction, authorrebuttal generation, and scientometric analysis. MOPRD is a strong endorsementfor further studies in peer review-related research and other applications.</description><author>Jialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong Chen, Xiaodong Shi</author><pubDate>Tue, 14 Nov 2023 18:06:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04972v2</guid></item><item><title>The Transient Nature of Emergent In-Context Learning in Transformers</title><link>http://arxiv.org/abs/2311.08360v1</link><description>Transformer neural networks can exhibit a surprising capacity for in-contextlearning (ICL) despite not being explicitly trained for it. Prior work hasprovided a deeper understanding of how ICL emerges in transformers, e.g.through the lens of mechanistic interpretability, Bayesian inference, or byexamining the distributional properties of training data. However, in each ofthese cases, ICL is treated largely as a persistent phenomenon; namely, onceICL emerges, it is assumed to persist asymptotically. Here, we show that theemergence of ICL during transformer training is, in fact, often transient. Wetrain transformers on synthetic data designed so that both ICL and in-weightslearning (IWL) strategies can lead to correct predictions. We find that ICLfirst emerges, then disappears and gives way to IWL, all while the trainingloss decreases, indicating an asymptotic preference for IWL. The transientnature of ICL is observed in transformers across a range of model sizes anddatasets, raising the question of how much to "overtrain" transformers whenseeking compact, cheaper-to-run models. We find that L2 regularization mayoffer a path to more persistent ICL that removes the need for early stoppingbased on ICL-style validation tasks. Finally, we present initial evidence thatICL transience may be caused by competition between ICL and IWL circuits.</description><author>Aaditya K. Singh, Stephanie C. Y. Chan, Ted Moskovitz, Erin Grant, Andrew M. Saxe, Felix Hill</author><pubDate>Tue, 14 Nov 2023 18:03:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08360v1</guid></item><item><title>Rotation-Agnostic Image Representation Learning for Digital Pathology</title><link>http://arxiv.org/abs/2311.08359v1</link><description>This paper addresses complex challenges in histopathological image analysisthrough three key contributions. Firstly, it introduces a fast patch selectionmethod, FPS, for whole-slide image (WSI) analysis, significantly reducingcomputational cost while maintaining accuracy. Secondly, it presents PathDino,a lightweight histopathology feature extractor with a minimal configuration offive Transformer blocks and only 9 million parameters, markedly fewer thanalternatives. Thirdly, it introduces a rotation-agnostic representationlearning paradigm using self-supervised learning, effectively mitigatingoverfitting. We also show that our compact model outperforms existingstate-of-the-art histopathology-specific vision transformers on 12 diversedatasets, including both internal datasets spanning four sites (breast, liver,skin, and colorectal) and seven public datasets (PANDA, CAMELYON16, BRACS,DigestPath, Kather, PanNuke, and WSSS4LUAD). Notably, even with a trainingdataset of 6 million histopathology patches from The Cancer Genome Atlas(TCGA), our approach demonstrates an average 8.5% improvement in patch-levelmajority vote performance. These contributions provide a robust framework forenhancing image analysis in digital pathology, rigorously validated throughextensive evaluation. Project Page: https://rhazeslab.github.io/PathDino-Page/</description><author>Saghir Alfasly, Abubakr Shafique, Peyman Nejat, Jibran Khan, Areej Alsaafin, Ghazal Alabtah, H. R. Tizhoosh</author><pubDate>Tue, 14 Nov 2023 18:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08359v1</guid></item><item><title>Sparsity-Preserving Differentially Private Training of Large Embedding Models</title><link>http://arxiv.org/abs/2311.08357v1</link><description>As the use of large embedding models in recommendation systems and languageapplications increases, concerns over user data privacy have also risen.DP-SGD, a training algorithm that combines differential privacy with stochasticgradient descent, has been the workhorse in protecting user privacy withoutcompromising model accuracy by much. However, applying DP-SGD naively toembedding models can destroy gradient sparsity, leading to reduced trainingefficiency. To address this issue, we present two new algorithms, DP-FEST andDP-AdaFEST, that preserve gradient sparsity during private training of largeembedding models. Our algorithms achieve substantial reductions ($10^6 \times$)in gradient size, while maintaining comparable levels of accuracy, on benchmarkreal-world datasets.</description><author>Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang</author><pubDate>Tue, 14 Nov 2023 17:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08357v1</guid></item><item><title>Artificial Text Boundary Detection with Topological Data Analysis and Sliding Window Techniques</title><link>http://arxiv.org/abs/2311.08349v1</link><description>Due to the rapid development of text generation models, people increasinglyoften encounter texts that may start out as written by a human but thencontinue as machine-generated results of large language models. Detecting theboundary between human-written and machine-generated parts of such texts is avery challenging problem that has not received much attention in literature. Inthis work, we consider and compare a number of different approaches for thisartificial text boundary detection problem, comparing several predictors overfeatures of different nature. We show that supervised fine-tuning of theRoBERTa model works well for this task in general but fails to generalize inimportant cross-domain and cross-generator settings, demonstrating a tendencyto overfit to spurious properties of the data. Then, we propose novelapproaches based on features extracted from a frozen language model'sembeddings that are able to outperform both the human accuracy level andpreviously considered baselines on the Real or Fake Text benchmark. Moreover,we adapt perplexity-based approaches for the boundary detection task andanalyze their behaviour. We analyze the robustness of all proposed classifiersin cross-domain and cross-model settings, discovering important properties ofthe data that can negatively influence the performance of artificial textboundary detection algorithms.</description><author>Laida Kushnareva, Tatiana Gaintseva, German Magai, Serguei Barannikov, Dmitry Abulkhanov, Kristian Kuznetsov, Irina Piontkovskaya, Sergey Nikolenko</author><pubDate>Tue, 14 Nov 2023 17:48:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08349v1</guid></item><item><title>MC^2: A Multilingual Corpus of Minority Languages in China</title><link>http://arxiv.org/abs/2311.08348v1</link><description>Large-scale corpora play a vital role in the construction of large languagemodels (LLMs). However, existing LLMs exhibit limited abilities inunderstanding low-resource languages, including the minority languages inChina, due to a lack of training data. To improve the accessibility of theselanguages, we present MC^2, a Multilingual Corpus of Minority Languages inChina, which is the largest open-source corpus so far. It encompasses fourunderrepresented languages, i.e., Tibetan, Uyghur, Kazakh in the Kazakh Arabicscript, and Mongolian in the traditional Mongolian script. Notably, two writingsystems in MC^2 are long neglected in previous corpora. As we identify seriouscontamination in the low-resource language split in the existing multilingualcorpora, we propose a quality-centric solution for collecting MC^2,prioritizing quality and accuracy while enhancing representativeness anddiversity. By in-depth analysis, we demonstrate the new research challengesMC^2 brings, such as long-text modeling and multiplicity of writing systems. Wehope MC^2 can help enhance the equity of the underrepresented languages inChina and provide a reliable data foundation for further research onlow-resource languages.</description><author>Chen Zhang, Mingxu Tao, Quzhe Huang, Jiuheng Lin, Zhibin Chen, Yansong Feng</author><pubDate>Tue, 14 Nov 2023 17:45:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08348v1</guid></item><item><title>Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification</title><link>http://arxiv.org/abs/2301.09702v4</link><description>Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims tolearn identity information from labeled images in source domains and apply itto unlabeled images in a target domain. One major issue with many unsupervisedre-identification methods is that they do not perform well relative to largedomain variations such as illumination, viewpoint, and occlusions. In thispaper, we propose a Synthesis Model Bank (SMB) to deal with illuminationvariation in unsupervised person re-ID. The proposed SMB consists of severalconvolutional neural networks (CNN) for feature extraction and Mahalanobismatrices for distance metrics. They are trained using synthetic data withdifferent illumination conditions such that their synergistic effect makes theSMB robust against illumination variation. To better quantify the illuminationintensity and improve the quality of synthetic images, we introduce a new 3Dvirtual-human dataset for GAN-based image synthesis. From our experiments, theproposed SMB outperforms other synthesis methods on several re-ID benchmarks.</description><author>Jiaqi Guo, Amy R. Reibman, Edward J. Delp</author><pubDate>Tue, 14 Nov 2023 17:44:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.09702v4</guid></item><item><title>LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud</title><link>http://arxiv.org/abs/2309.17157v3</link><description>In the current user-server interaction paradigm of prompted generation withlarge language models (LLM) on cloud, the server fully controls the generationprocess, which leaves zero options for users who want to keep the generatedtext to themselves. We propose LatticeGen, a cooperative framework in which theserver still handles most of the computation while the user controls thesampling operation. The key idea is that the true generated sequence is mixedwith noise tokens by the user and hidden in a noised lattice. Consideringpotential attacks from a hypothetically malicious server and how the user candefend against it, we propose the repeated beam-search attack and the mixingnoise scheme. In our experiments we apply LatticeGen to protect both prompt andgeneration. It is shown that while the noised lattice degrades generationquality, LatticeGen successfully protects the true generation to a remarkabledegree under strong attacks (more than 50% of the semantic remains hidden asmeasured by BERTScore).</description><author>Mengke Zhang, Tianxing He, Tianle Wang, Lu Mi, Fatemehsadat Mireshghallah, Binyi Chen, Hao Wang, Yulia Tsvetkov</author><pubDate>Tue, 14 Nov 2023 17:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17157v3</guid></item><item><title>Fast swap regret minimization and applications to approximate correlated equilibria</title><link>http://arxiv.org/abs/2310.19647v2</link><description>We give a simple and computationally efficient algorithm that, for anyconstant $\varepsilon&gt;0$, obtains $\varepsilon T$-swap regret within only $T =\mathsf{polylog}(n)$ rounds; this is an exponential improvement compared to thesuper-linear number of rounds required by the state-of-the-art algorithm, andresolves the main open problem of [Blum and Mansour 2007]. Our algorithm has anexponential dependence on $\varepsilon$, but we prove a new, matching lowerbound. Our algorithm for swap regret implies faster convergence to$\varepsilon$-Correlated Equilibrium ($\varepsilon$-CE) in several regimes: Fornormal form two-player games with $n$ actions, it implies the first uncoupleddynamics that converges to the set of $\varepsilon$-CE in polylogarithmicrounds; a $\mathsf{polylog}(n)$-bit communication protocol for $\varepsilon$-CEin two-player games (resolving an open problem mentioned by[Babichenko-Rubinstein'2017, Goos-Rubinstein'2018, Ganor-CS'2018]); and an$\tilde{O}(n)$-query algorithm for $\varepsilon$-CE (resolving an open problemof [Babichenko'2020] and obtaining the first separation between$\varepsilon$-CE and $\varepsilon$-Nash equilibrium in the query complexitymodel). For extensive-form games, our algorithm implies a PTAS for $\mathit{normal}$$\mathit{form}$ $\mathit{correlated}$ $\mathit{equilibria}$, a solution conceptoften conjectured to be computationally intractable (e.g. [Stengel-Forges'08,Fujii'23]).</description><author>Binghui Peng, Aviad Rubinstein</author><pubDate>Tue, 14 Nov 2023 17:39:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19647v2</guid></item><item><title>Enabling Human-Centered AI: A Methodological Perspective</title><link>http://arxiv.org/abs/2311.06703v2</link><description>Human-centered AI (HCAI) is a design philosophy that advocates prioritizinghumans in designing, developing, and deploying intelligent systems, aiming tomaximize the benefits of AI to humans and avoid potential adverse impacts.While HCAI continues to influence, the lack of guidance on methodology inpractice makes its adoption challenging. This paper proposes a comprehensiveHCAI framework based on our previous work with integrated components, includingdesign goals, design principles, implementation approaches, interdisciplinaryteams, HCAI methods, and HCAI processes. This paper also presents a"three-layer" approach to facilitate the implementation of the framework. Webelieve this systematic and executable framework can overcome the weaknesses incurrent HCAI frameworks and the challenges currently faced in practice, puttingit into action to enable HCAI further.</description><author>Wei Xu, Zaifeng Gao</author><pubDate>Tue, 14 Nov 2023 17:32:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06703v2</guid></item><item><title>Causal Message Passing: A Method for Experiments with Unknown and General Network Interference</title><link>http://arxiv.org/abs/2311.08340v1</link><description>Randomized experiments are a powerful methodology for data-driven evaluationof decisions or interventions. Yet, their validity may be undermined by networkinterference. This occurs when the treatment of one unit impacts not only itsoutcome but also that of connected units, biasing traditional treatment effectestimations. Our study introduces a new framework to accommodate complex andunknown network interference, moving beyond specialized models in the existingliterature. Our framework, which we term causal message-passing, is grounded ina high-dimensional approximate message passing methodology and is specificallytailored to experimental design settings with prevalent network interference.Utilizing causal message-passing, we present a practical algorithm forestimating the total treatment effect and demonstrate its efficacy in fournumerical scenarios, each with its unique interference structure.</description><author>Sadegh Shirani, Mohsen Bayati</author><pubDate>Tue, 14 Nov 2023 17:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08340v1</guid></item><item><title>Exploring Variational Auto-Encoder Architectures, Configurations, and Datasets for Generative Music Explainable AI</title><link>http://arxiv.org/abs/2311.08336v1</link><description>Generative AI models for music and the arts in general are increasinglycomplex and hard to understand. The field of eXplainable AI (XAI) seeks to makecomplex and opaque AI models such as neural networks more understandable topeople. One approach to making generative AI models more understandable is toimpose a small number of semantically meaningful attributes on generative AImodels. This paper contributes a systematic examination of the impact thatdifferent combinations of Variational Auto-Encoder models (MeasureVAE andAdversarialVAE), configurations of latent space in the AI model (from 4 to 256latent dimensions), and training datasets (Irish folk, Turkish folk, Classical,and pop) have on music generation performance when 2 or 4 meaningful musicalattributes are imposed on the generative model. To date there have been nosystematic comparisons of such models at this level of combinatorial detail.Our findings show that MeasureVAE has better reconstruction performance thanAdversarialVAE which has better musical attribute independence. Resultsdemonstrate that MeasureVAE was able to generate music across music genres withinterpretable musical dimensions of control, and performs best with lowcomplexity music such a pop and rock. We recommend that a 32 or 64 latentdimensional space is optimal for 4 regularised dimensions when using MeasureVAEto generate music across genres. Our results are the first detailed comparisonsof configurations of state-of-the-art generative AI models for music and can beused to help select and configure AI models, musical features, and datasets formore understandable generation of music.</description><author>Nick Bryan-Kinns, Bingyuan Zhang, Songyan Zhao, Berker Banar</author><pubDate>Tue, 14 Nov 2023 17:27:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08336v1</guid></item><item><title>Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning</title><link>http://arxiv.org/abs/2303.16445v3</link><description>Language model probing is often used to test specific capabilities of models.However, conclusions from such studies may be limited when the probingbenchmarks are small and lack statistical power. In this work, we introducenew, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500)inspired by psycholinguistic studies. We dramatically extend existing NEG-136and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44sentence pairs to 750 each. We also create another version of extended negationdataset (NEG-1500-SIMP-TEMP), created using template-based generation. Itconsists of 770 sentence pairs. We evaluate 22 models on the extended datasets,seeing model performance dip 20-57% compared to the original smallerbenchmarks. We observe high levels of negation sensitivity in models like BERTand ALBERT demonstrating that previous findings might have been skewed due tosmaller test sets. Finally, we observe that while GPT3 has generated all theexamples in ROLE-1500 is only able to solve 24.6% of them during probing. Thedatasets and code are available on$\href{https://github.com/text-machine-lab/extending_psycholinguistic_dataset}{Github}$.</description><author>Namrata Shivagunde, Vladislav Lialin, Anna Rumshisky</author><pubDate>Tue, 14 Nov 2023 17:24:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16445v3</guid></item><item><title>KTRL+F: Knowledge-Augmented In-Document Search</title><link>http://arxiv.org/abs/2311.08329v1</link><description>We introduce a new problem KTRL+F, a knowledge-augmented in-document searchtask that necessitates real-time identification of all semantic targets withina document with the awareness of external sources through a single naturalquery. This task addresses following unique challenges for in-document search:1) utilizing knowledge outside the document for extended use of additionalinformation about targets to bridge the semantic gap between the query and thetargets, and 2) balancing between real-time applicability with the performance.We analyze various baselines in KTRL+F and find there are limitations ofexisting models, such as hallucinations, low latency, or difficulties inleveraging external knowledge. Therefore we propose a Knowledge-AugmentedPhrase Retrieval model that shows a promising balance between speed andperformance by simply augmenting external knowledge embedding in phraseembedding. Additionally, we conduct a user study to verify whether solvingKTRL+F can enhance search experience of users. It demonstrates that even withour simple model users can reduce the time for searching with less queries andreduced extra visits to other sources for collecting evidence. We encourage theresearch community to work on KTRL+F to enhance more efficient in-documentinformation access.</description><author>Hanseok Oh, Haebin Shin, Miyoung Ko, Hyunji Lee, Minjoon Seo</author><pubDate>Tue, 14 Nov 2023 17:18:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08329v1</guid></item><item><title>Improved Beam Search for Hallucination Mitigation in Abstractive Summarization</title><link>http://arxiv.org/abs/2212.02712v2</link><description>Advancement in large pretrained language models has significantly improvedtheir performance for conditional language generation tasks includingsummarization albeit with hallucinations. To reduce hallucinations,conventional methods proposed improving beam search or using a fact checker asa postprocessing step. In this paper, we investigate the use of the NaturalLanguage Inference (NLI) entailment metric to detect and prevent hallucinationsin summary generation. We propose an NLI-assisted beam re-ranking mechanism bycomputing entailment probability scores between the input context andsummarization model-generated beams during saliency-enhanced greedy decoding.Moreover, a diversity metric is introduced to compare its effectiveness againstvanilla beam search. Our proposed algorithm significantly outperforms vanillabeam decoding on XSum and CNN/DM datasets.</description><author>Arvind Krishna Sridhar, Erik Visser</author><pubDate>Tue, 14 Nov 2023 17:12:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02712v2</guid></item><item><title>Anti-LM Decoding for Zero-shot In-context Machine Translation</title><link>http://arxiv.org/abs/2311.08324v1</link><description>Zero-shot In-context learning is the phenomenon where models can perform thetask simply given the instructions. However, pre-trained large language modelsare known to be poorly calibrated for this task. One of the most effectiveapproaches to handling this bias is to adopt a contrastive decoding objective,which accounts for the prior probability of generating the next token byconditioning on some context. This work introduces an Anti-Language Modelobjective with a decay factor designed to address the weaknesses of In-contextMachine Translation. We conduct our experiments across 3 model types and sizes,3 language directions, and for both greedy decoding and beam search ($B=5$).The proposed method outperforms other state-of-art decoding objectives, with upto $20$ BLEU point improvement from the default objective observed in somesettings.</description><author>Suzanna Sia, Alexandra DeLucia, Kevin Duh</author><pubDate>Tue, 14 Nov 2023 17:09:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08324v1</guid></item><item><title>Open-vocabulary keyword spotting in any language through multilingual contrastive speech-phoneme pretraining</title><link>http://arxiv.org/abs/2311.08323v1</link><description>In this paper, we introduce a massively multilingual speech corpora withfine-grained phonemic transcriptions, encompassing more than 115 languages fromdiverse language families. Based on this multilingual dataset, we proposeCLAP-IPA, a multilingual phoneme-speech contrastive embedding model capable ofopen-vocabulary matching between speech signals and phonemically transcribedkeywords or arbitrary phrases. The proposed model has been tested on twofieldwork speech corpora in 97 unseen languages, exhibiting stronggeneralizability across languages. Comparison with a text-based model showsthat using phonemes as modeling units enables much better crosslinguisticgeneralization than orthographic texts.</description><author>Jian Zhu, Farhan Samir, Changbing Yang, Jahurul Islam</author><pubDate>Tue, 14 Nov 2023 17:09:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08323v1</guid></item><item><title>Diffused Redundancy in Pre-trained Representations</title><link>http://arxiv.org/abs/2306.00183v3</link><description>Representations learned by pre-training a neural network on a large datasetare increasingly used successfully to perform a variety of downstream tasks. Inthis work, we take a closer look at how features are encoded in suchpre-trained representations. We find that learned representations in a givenlayer exhibit a degree of diffuse redundancy, ie, any randomly chosen subset ofneurons in the layer that is larger than a threshold size shares a large degreeof similarity with the full layer and is able to perform similarly as the wholelayer on a variety of downstream tasks. For example, a linear probe trained on$20\%$ of randomly picked neurons from the penultimate layer of a ResNet50pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probetrained on the full layer of neurons for downstream CIFAR10 classification. Weconduct experiments on different neural architectures (including CNNs andTransformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate avariety of downstream tasks taken from the VTAB benchmark. We find that theloss and dataset used during pre-training largely govern the degree of diffuseredundancy and the "critical mass" of neurons needed often depends on thedownstream task, suggesting that there is a task-inherentredundancy-performance Pareto frontier. Our findings shed light on the natureof representations learned by pre-trained deep neural networks and suggest thatentire layers might not be necessary to perform many downstream tasks. Weinvestigate the potential for exploiting this redundancy to achieve efficientgeneralization for downstream tasks and also draw caution to certain possibleunintended consequences. Our code is available at\url{https://github.com/nvedant07/diffused-redundancy}.</description><author>Vedant Nanda, Till Speicher, John P. Dickerson, Soheil Feizi, Krishna P. Gummadi, Adrian Weller</author><pubDate>Tue, 14 Nov 2023 17:00:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00183v3</guid></item><item><title>Convolutional Neural Networks Exploiting Attributes of Biological Neurons</title><link>http://arxiv.org/abs/2311.08314v1</link><description>In this era of artificial intelligence, deep neural networks likeConvolutional Neural Networks (CNNs) have emerged as front-runners, oftensurpassing human capabilities. These deep networks are often perceived as thepanacea for all challenges. Unfortunately, a common downside of these networksis their ''black-box'' character, which does not necessarily mirror theoperation of biological neural systems. Some even have millions/billions oflearnable (tunable) parameters, and their training demands extensive data andtime. Here, we integrate the principles of biological neurons in certain layer(s)of CNNs. Specifically, we explore the use of neuro-science-inspiredcomputational models of the Lateral Geniculate Nucleus (LGN) and simple cellsof the primary visual cortex. By leveraging such models, we aim to extractimage features to use as input to CNNs, hoping to enhance training efficiencyand achieve better accuracy. We aspire to enable shallow networks with aPush-Pull Combination of Receptive Fields (PP-CORF) model of simple cells asthe foundation layer of CNNs to enhance their learning process and performance.To achieve this, we propose a two-tower CNN, one shallow tower and the other asResNet 18. Rather than extracting the features blindly, it seeks to mimic howthe brain perceives and extracts features. The proposed system exhibits anoticeable improvement in the performance (on an average of $5\%-10\%$) onCIFAR-10, CIFAR-100, and ImageNet-100 datasets compared to ResNet-18. We alsocheck the efficiency of only the Push-Pull tower of the network.</description><author>Neeraj Kumar Singh, Nikhil R. Pal</author><pubDate>Tue, 14 Nov 2023 16:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08314v1</guid></item><item><title>Introducing an Improved Information-Theoretic Measure of Predictive Uncertainty</title><link>http://arxiv.org/abs/2311.08309v1</link><description>Applying a machine learning model for decision-making in the real worldrequires to distinguish what the model knows from what it does not. A criticalfactor in assessing the knowledge of a model is to quantify its predictiveuncertainty. Predictive uncertainty is commonly measured by the entropy of theBayesian model average (BMA) predictive distribution. Yet, the properness ofthis current measure of predictive uncertainty was recently questioned. Weprovide new insights regarding those limitations. Our analyses show that thecurrent measure erroneously assumes that the BMA predictive distribution isequivalent to the predictive distribution of the true model that generated thedataset. Consequently, we introduce a theoretically grounded measure toovercome these limitations. We experimentally verify the benefits of ourintroduced measure of predictive uncertainty. We find that our introducedmeasure behaves more reasonably in controlled synthetic tasks. Moreover, ourevaluations on ImageNet demonstrate that our introduced measure is advantageousin real-world applications utilizing predictive uncertainty.</description><author>Kajetan Schweighofer, Lukas Aichberger, Mykyta Ielanskyi, Sepp Hochreiter</author><pubDate>Tue, 14 Nov 2023 16:55:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08309v1</guid></item><item><title>The Heat is On: Thermal Facial Landmark Tracking</title><link>http://arxiv.org/abs/2311.08308v1</link><description>Facial landmark tracking for thermal images requires tracking certainimportant regions of subjects' faces, using images from thermal images, whichomit lighting and shading, but show the temperatures of their subjects. Thefluctuations of heat in particular places reflect physiological changes likebloodflow and perspiration, which can be used to remotely gauge things likeanxiety and excitement. Past work in this domain has been limited to only avery limited set of architectures and techniques. This work goes further bytrying a comprehensive suit of various models with different components, suchas residual connections, channel and feature-wise attention, as well as thepractice of ensembling components of the network to work in parallel. The bestmodel integrated convolutional and residual layers followed by a channel-wiseself-attention layer, requiring less than 100K parameters.</description><author>James Baker</author><pubDate>Tue, 14 Nov 2023 16:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08308v1</guid></item><item><title>Simplifying and Understanding State Space Models with Diagonal Linear RNNs</title><link>http://arxiv.org/abs/2212.00768v3</link><description>Sequence models based on linear state spaces (SSMs) have recently emerged asa promising choice of architecture for modeling long range dependencies acrossvarious modalities. However, they invariably rely on discretization of acontinuous state space, which complicates their presentation and understanding.In this work, we dispose of the discretization step, and propose a model basedon vanilla Diagonal Linear RNNs ($\mathrm{DLR}$). We empirically show that,despite being conceptually much simpler, $\mathrm{DLR}$ is as performant aspreviously-proposed SSMs on a variety of tasks and benchmarks including LongRange Arena and raw speech classification. Moreover, we characterize theexpressivity of SSMs (including $\mathrm{DLR}$) and attention-based models viaa suite of $13$ synthetic sequence-to-sequence tasks involving interactionsover tens of thousands of tokens, ranging from simple operations, such asshifting an input sequence, to detecting co-dependent visual features over longspatial ranges in flattened images. We find that while SSMs report near-perfectperformance on tasks that can be modeled via $\textit{few}$ convolutionalkernels, they struggle on tasks requiring $\textit{many}$ such kernels andespecially when the desired sequence manipulation is$\textit{context-dependent}$. Despite these limitations, $\mathrm{DLR}$ reacheshigh performance on two higher-order reasoning tasks $\mathrm{ListOpsSubTrees}$and $\mathrm{PathfinderSegmentation}\text{-}\mathrm{256}$ with input lengths$8K$ and $65K$ respectively, and gives encouraging performance on$\mathrm{PathfinderSegmentation}\text{-}\mathrm{512}$ with input length $262K$for which attention is not a viable choice.</description><author>Ankit Gupta, Harsh Mehta, Jonathan Berant</author><pubDate>Tue, 14 Nov 2023 16:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00768v3</guid></item><item><title>On-the-Fly Fusion of Large Language Models and Machine Translation</title><link>http://arxiv.org/abs/2311.08306v1</link><description>We propose the on-the-fly ensembling of a machine translation model with anLLM, prompted on the same task and input. We perform experiments on 4 languagepairs (both directions) with varying data amounts. We find that a slightlyweaker-at-translation LLM can improve translations of a NMT model, andensembling with an LLM can produce better translations than ensembling twostronger MT models. We combine our method with various techniques from LLMprompting, such as in context learning and translation context.</description><author>Hieu Hoang, Huda Khayrallah, Marcin Junczys-Dowmunt</author><pubDate>Tue, 14 Nov 2023 16:49:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08306v1</guid></item><item><title>Extrinsically-Focused Evaluation of Omissions in Medical Summarization</title><link>http://arxiv.org/abs/2311.08303v1</link><description>The goal of automated summarization techniques (Paice, 1990; Kupiec et al,1995) is to condense text by focusing on the most critical information.Generative large language models (LLMs) have shown to be robust summarizers,yet traditional metrics struggle to capture resulting performance (Goyal et al,2022) in more powerful LLMs. In safety-critical domains such as medicine, morerigorous evaluation is required, especially given the potential for LLMs toomit important information in the resulting summary. We propose MED-OMIT, a newomission benchmark for medical summarization. Given a doctor-patientconversation and a generated summary, MED-OMIT categorizes the chat into a setof facts and identifies which are omitted from the summary. We further proposeto determine fact importance by simulating the impact of each fact on adownstream clinical task: differential diagnosis (DDx) generation. MED-OMITleverages LLM prompt-based approaches which categorize the importance of factsand cluster them as supporting or negating evidence to the diagnosis. Weevaluate MED-OMIT on a publicly-released dataset of patient-doctorconversations and find that MED-OMIT captures omissions better than alternativemetrics.</description><author>Elliot Schumacher, Daniel Rosenthal, Varun Nair, Luladay Price, Geoffrey Tso, Anitha Kannan</author><pubDate>Tue, 14 Nov 2023 16:46:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08303v1</guid></item><item><title>Single-Model Attribution of Generative Models Through Final-Layer Inversion</title><link>http://arxiv.org/abs/2306.06210v3</link><description>Recent breakthroughs in generative modeling have sparked interest inpractical single-model attribution. Such methods predict whether a sample wasgenerated by a specific generator or not, for instance, to prove intellectualproperty theft. However, previous works are either limited to the closed-worldsetting or require undesirable changes to the generative model. We addressthese shortcomings by, first, viewing single-model attribution through the lensof anomaly detection. Arising from this change of perspective, we proposeFLIPAD, a new approach for single-model attribution in the open-world settingbased on final-layer inversion and anomaly detection. We show that the utilizedfinal-layer inversion can be reduced to a convex lasso optimization problem,making our approach theoretically sound and computationally efficient. Thetheoretical findings are accompanied by an experimental study demonstrating theeffectiveness of our approach and its flexibility to various domains.</description><author>Mike Laszkiewicz, Jonas Ricker, Johannes Lederer, Asja Fischer</author><pubDate>Tue, 14 Nov 2023 16:46:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06210v3</guid></item><item><title>Workflow-Guided Response Generation for Task-Oriented Dialogue</title><link>http://arxiv.org/abs/2311.08300v1</link><description>Task-oriented dialogue (TOD) systems aim to achieve specific goals throughinteractive dialogue. Such tasks usually involve following specific workflows,i.e. executing a sequence of actions in a particular order. While prior workhas focused on supervised learning methods to condition on past actions, theydo not explicitly optimize for compliance to a desired workflow. In this paper,we propose a novel framework based on reinforcement learning (RL) to generatedialogue responses that are aligned with a given workflow. Our frameworkconsists of ComplianceScorer, a metric designed to evaluate how well agenerated response executes the specified action, combined with an RLopimization process that utilizes an interactive sampling technique. Weevaluate our approach on two TOD datasets, Action-Based Conversations Dataset(ABCD) (Chen et al., 2021a) and MultiWOZ 2.2 (Zang et al., 2020) on a range ofautomated and human evaluation metrics. Our findings indicate that our RL-basedframework outperforms baselines and is effective at enerating responses thatboth comply with the intended workflows while being expressed in a natural andfluent manner.</description><author>Do June Min, Paloma Sodhi, Ramya Ramakrishnan</author><pubDate>Tue, 14 Nov 2023 16:44:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08300v1</guid></item><item><title>VERVE: Template-based ReflectiVE Rewriting for MotiVational IntErviewing</title><link>http://arxiv.org/abs/2311.08299v1</link><description>Reflective listening is a fundamental skill that counselors must acquire toachieve proficiency in motivational interviewing (MI). It involves respondingin a manner that acknowledges and explores the meaning of what the client hasexpressed in the conversation. In this work, we introduce the task ofcounseling response rewriting, which transforms non-reflective statements intoreflective responses. We introduce VERVE, a template-based rewriting systemwith paraphrase-augmented training and adaptive template updating. VERVE firstcreates a template by identifying and filtering out tokens that are notrelevant to reflections and constructs a reflective response using thetemplate. Paraphrase-augmented training allows the model to learn less-strictfillings of masked spans, and adaptive template updating helps discovereffective templates for rewriting without significantly removing the originalcontent. Using both automatic and human evaluations, we compare our methodagainst text rewriting baselines and show that our framework is effective inturning non-reflective statements into more reflective responses whileachieving a good content preservation-reflection style trade-off.</description><author>Do June Min, Verónica Pérez-Rosas, Kenneth Resnicow, Rada Mihalcea</author><pubDate>Tue, 14 Nov 2023 16:44:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08299v1</guid></item><item><title>A Survey of Language Model Confidence Estimation and Calibration</title><link>http://arxiv.org/abs/2311.08298v1</link><description>Language models (LMs) have demonstrated remarkable capabilities across a widerange of tasks in various domains. Despite their impressive performance, thereliability of their output is concerning and questionable regarding the demandfor AI safety. Assessing the confidence of LM predictions and calibrating themacross different tasks with the aim to align LM confidence with accuracy canhelp mitigate risks and enable LMs to make better decisions. There have beenvarious works in this respect, but there has been no comprehensive overview ofthis important research area. The present survey aims to bridge this gap. Inparticular, we discuss methods and techniques for LM confidence estimation andcalibration, encompassing different LMs and various tasks. We further outlinethe challenges of estimating the confidence for large language models and wesuggest some promising directions for future work.</description><author>Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, Iryna Gurevych</author><pubDate>Tue, 14 Nov 2023 16:43:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08298v1</guid></item><item><title>On-Policy Policy Gradient Reinforcement Learning Without On-Policy Sampling</title><link>http://arxiv.org/abs/2311.08290v1</link><description>On-policy reinforcement learning (RL) algorithms perform policy updates usingi.i.d. trajectories collected by the current policy. However, after observingonly a finite number of trajectories, on-policy sampling may produce data thatfails to match the expected on-policy data distribution. This sampling errorleads to noisy updates and data inefficient on-policy learning. Recent work inthe policy evaluation setting has shown that non-i.i.d., off-policy samplingcan produce data with lower sampling error than on-policy sampling can produce.Motivated by this observation, we introduce an adaptive, off-policy samplingmethod to improve the data efficiency of on-policy policy gradient algorithms.Our method, Proximal Robust On-Policy Sampling (PROPS), reduces sampling errorby collecting data with a behavior policy that increases the probability ofsampling actions that are under-sampled with respect to the current policy.Rather than discarding data from old policies -- as is commonly done inon-policy algorithms -- PROPS uses data collection to adjust the distributionof previously collected data to be approximately on-policy. We empiricallyevaluate PROPS on both continuous-action MuJoCo benchmark tasks as well asdiscrete-action tasks and demonstrate that (1) PROPS decreases sampling errorthroughout training and (2) improves the data efficiency of on-policy policygradient algorithms. Our work improves the RL community's understanding of anuance in the on-policy vs off-policy dichotomy: on-policy learning requireson-policy data, not on-policy sampling.</description><author>Nicholas E. Corrado, Josiah P. Hanna</author><pubDate>Tue, 14 Nov 2023 16:37:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08290v1</guid></item><item><title>FinGPT: Democratizing Internet-scale Data for Financial Large Language Models</title><link>http://arxiv.org/abs/2307.10485v2</link><description>Large language models (LLMs) have demonstrated remarkable proficiency inunderstanding and generating human-like texts, which may potentiallyrevolutionize the finance industry. However, existing LLMs often fall short inthe financial field, which is mainly attributed to the disparities betweengeneral text data and financial text data. Unfortunately, there is only alimited number of financial text datasets available, and BloombergGPT, thefirst financial LLM (FinLLM), is close-sourced (only the training logs werereleased). In light of this, we aim to democratize Internet-scale financialdata for LLMs, which is an open challenge due to diverse data sources, lowsignal-to-noise ratio, and high time-validity. To address the challenges, weintroduce an open-sourced and data-centric framework, Financial GenerativePre-trained Transformer (FinGPT), that automates the collection and curation ofreal-time financial data from 34 diverse sources on the Internet, providingresearchers and practitioners with accessible and transparent resources todevelop their FinLLMs. Additionally, we propose a simple yet effective strategyfor fine-tuning FinLLM using the inherent feedback from the market, dubbedReinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rankAdaptation (LoRA, QLoRA) method that enables users to customize their ownFinLLMs from general-purpose LLMs at a low cost. Finally, we showcase severalFinGPT applications, including robo-advisor, sentiment analysis for algorithmictrading, and low-code development. FinGPT aims to democratize FinLLMs,stimulate innovation, and unlock new opportunities in open finance. The codeshave been open-sourced.</description><author>Xiao-Yang Liu, Guoxuan Wang, Hongyang Yang, Daochen Zha</author><pubDate>Tue, 14 Nov 2023 16:34:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10485v2</guid></item><item><title>Scale-MIA: A Scalable Model Inversion Attack against Secure Federated Learning via Latent Space Reconstruction</title><link>http://arxiv.org/abs/2311.05808v2</link><description>Federated learning is known for its capability to safeguard participants'data privacy. However, recently emerged model inversion attacks (MIAs) haveshown that a malicious parameter server can reconstruct individual users' localdata samples through model updates. The state-of-the-art attacks either rely oncomputation-intensive search-based optimization processes to recover each inputbatch, making scaling difficult, or they involve the malicious parameter serveradding extra modules before the global model architecture, rendering theattacks too conspicuous and easily detectable. To overcome these limitations, we propose Scale-MIA, a novel MIA capable ofefficiently and accurately recovering training samples of clients from theaggregated updates, even when the system is under the protection of a robustsecure aggregation protocol. Unlike existing approaches treating models asblack boxes, Scale-MIA recognizes the importance of the intricate architectureand inner workings of machine learning models. It identifies the latent spaceas the critical layer for breaching privacy and decomposes the complex recoverytask into an innovative two-step process to reduce computation complexity. Thefirst step involves reconstructing the latent space representations (LSRs) fromthe aggregated model updates using a closed-form inversion mechanism,leveraging specially crafted adversarial linear layers. In the second step, thewhole input batches are recovered from the LSRs by feeding them into afine-tuned generative decoder. We implemented Scale-MIA on multiple commonly used machine learning modelsand conducted comprehensive experiments across various settings. The resultsdemonstrate that Scale-MIA achieves excellent recovery performance on differentdatasets, exhibiting high reconstruction rates, accuracy, and attack efficiencyon a larger scale compared to state-of-the-art MIAs.</description><author>Shanghao Shi, Ning Wang, Yang Xiao, Chaoyu Zhang, Yi Shi, Y. Thomas Hou, Wenjing Lou</author><pubDate>Tue, 14 Nov 2023 16:33:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05808v2</guid></item><item><title>How Well Do Large Language Models Understand Syntax? An Evaluation by Asking Natural Language Questions</title><link>http://arxiv.org/abs/2311.08287v1</link><description>While recent advancements in large language models (LLMs) bring us closer toachieving artificial general intelligence, the question persists: Do LLMs trulyunderstand language, or do they merely mimic comprehension through patternrecognition? This study seeks to explore this question through the lens ofsyntax, a crucial component of sentence comprehension. Adopting a naturallanguage question-answering (Q&amp;A) scheme, we craft questions targeting ninesyntactic knowledge points that are most closely related to sentencecomprehension. Experiments conducted on 24 LLMs suggest that most have alimited grasp of syntactic knowledge, exhibiting notable discrepancies acrossdifferent syntactic knowledge points. In particular, questions involvingprepositional phrase attachment pose the greatest challenge, whereas thoseconcerning adjectival modifier and indirect object are relatively easier forLLMs to handle. Furthermore, a case study on the training dynamics of the LLMsreveals that the majority of syntactic knowledge is learned during the initialstages of training, hinting that simply increasing the number of trainingtokens may not be the `silver bullet' for improving the comprehension abilityof LLMs.</description><author>Houquan Zhou, Yang Hou, Zhenghua Li, Xuebin Wang, Zhefeng Wang, Xinyu Duan, Min Zhang</author><pubDate>Tue, 14 Nov 2023 16:30:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08287v1</guid></item><item><title>Level Set KSVD</title><link>http://arxiv.org/abs/2311.08284v1</link><description>We present a new algorithm for image segmentation - Level-set KSVD. Level-setKSVD merges the methods of sparse dictionary learning for feature extractionand variational level-set method for image segmentation. Specifically, we use ageneralization of the Chan-Vese functional with features learned by KSVD. Themotivation for this model is agriculture based. Aerial images are taken inorder to detect the spread of fungi in various crops. Our model is tested onsuch images of cotton fields. The results are compared to other methods.</description><author>Omer Sapir, Iftach Klapp, Nir Sochen</author><pubDate>Tue, 14 Nov 2023 16:27:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08284v1</guid></item><item><title>Inferring Causal Effects Under Heterogeneous Peer Influence</title><link>http://arxiv.org/abs/2305.17479v2</link><description>Causal inference in networks should account for interference, which occurswhen a unit's outcome is influenced by treatments or outcomes of peers.Heterogeneous peer influence (HPI) occurs when a unit's outcome is influenceddifferently by different peers based on their attributes and relationships, orwhen each unit has a different susceptibility to peer influence. Existingsolutions to estimating direct causal effects under interference considereither homogeneous influence from peers or specific heterogeneous influencemechanisms (e.g., based on local neighborhood structure). This paper presents amethodology for estimating individual direct causal effects in the presence ofHPI where the mechanism of influence is not known a priori. We propose astructural causal model for networks that can capture different possibleassumptions about network structure, interference conditions, and causaldependence and enables reasoning about identifiability in the presence of HPI.We find potential heterogeneous contexts using the causal model and propose anovel graph neural network-based estimator to estimate individual direct causaleffects. We show that state-of-the-art methods for individual direct effectestimation produce biased results in the presence of HPI, and that our proposedestimator is robust.</description><author>Shishir Adhikari, Elena Zheleva</author><pubDate>Tue, 14 Nov 2023 16:20:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17479v2</guid></item><item><title>ARTEMIS: Using GANs with Multiple Discriminators to Generate Art</title><link>http://arxiv.org/abs/2311.08278v1</link><description>We propose a novel method for generating abstract art. First an autoencoderis trained to encode and decode the style representations of images, which areextracted from source images with a pretrained VGG network. Then, the decodercomponent of the autoencoder is extracted and used as a generator in a GAN. Thegenerator works with an ensemble of discriminators. Each discriminator takesdifferent style representations of the same images, and the generator istrained to create images that create convincing style representations in orderto deceive all of the generators. The generator is also trained to maximize adiversity term. The resulting images had a surreal, geometric quality. We callour approach ARTEMIS (ARTistic Encoder- Multi- Discriminators IncludingSelf-Attention), as it uses the self-attention layers and an encoder-decoderarchitecture.</description><author>James Baker</author><pubDate>Tue, 14 Nov 2023 16:19:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08278v1</guid></item><item><title>Examining Modularity in Multilingual LMs via Language-Specialized Subnetworks</title><link>http://arxiv.org/abs/2311.08273v1</link><description>Recent work has proposed explicitly inducing language-wise modularity inmultilingual LMs via sparse fine-tuning (SFT) on per-language subnetworks as ameans of better guiding cross-lingual sharing. In this work, we investigate (1)the degree to which language-wise modularity naturally arises within modelswith no special modularity interventions, and (2) how cross-lingual sharing andinterference differ between such models and those with explicit SFT-guidedsubnetwork modularity. To quantify language specialization and cross-lingualinteraction, we use a Training Data Attribution method that estimates thedegree to which a model's predictions are influenced by in-language orcross-language training examples. Our results show that language-specializedsubnetworks do naturally arise, and that SFT, rather than always increasingmodularity, can decrease language specialization of subnetworks in favor ofmore cross-lingual sharing.</description><author>Rochelle Choenni, Ekaterina Shutova, Dan Garrette</author><pubDate>Tue, 14 Nov 2023 16:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08273v1</guid></item><item><title>Mixed Attention Network for Cross-domain Sequential Recommendation</title><link>http://arxiv.org/abs/2311.08272v1</link><description>In modern recommender systems, sequential recommendation leverageschronological user behaviors to make effective next-item suggestions, whichsuffers from data sparsity issues, especially for new users. One promising lineof work is the cross-domain recommendation, which trains models with dataacross multiple domains to improve the performance in data-scarce domains.Recent proposed cross-domain sequential recommendation models such as PiNet andDASL have a common drawback relying heavily on overlapped users in differentdomains, which limits their usage in practical recommender systems. In thispaper, we propose a Mixed Attention Network (MAN) with local and globalattention modules to extract the domain-specific and cross-domain information.Firstly, we propose a local/global encoding layer to capture thedomain-specific/cross-domain sequential pattern. Then we propose a mixedattention layer with item similarity attention, sequence-fusion attention, andgroup-prototype attention to capture the local/global item similarity, fuse thelocal/global item sequence, and extract the user groups across differentdomains, respectively. Finally, we propose a local/global prediction layer tofurther evolve and combine the domain-specific and cross-domain interests.Experimental results on two real-world datasets (each with two domains)demonstrate the superiority of our proposed model. Further study alsoillustrates that our proposed method and components are model-agnostic andeffective, respectively. The code and data are available athttps://github.com/Guanyu-Lin/MAN.</description><author>Guanyu Lin, Chen Gao, Yu Zheng, Jianxin Chang, Yanan Niu, Yang Song, Kun Gai, Zhiheng Li, Depeng Jin, Yong Li, Meng Wang</author><pubDate>Tue, 14 Nov 2023 16:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08272v1</guid></item><item><title>Investigating and Improving Latent Density Segmentation Models for Aleatoric Uncertainty Quantification in Medical Imaging</title><link>http://arxiv.org/abs/2307.16694v3</link><description>Data uncertainties, such as sensor noise or occlusions, can introduceirreducible ambiguities in images, which result in varying, yet plausible,semantic hypotheses. In Machine Learning, this ambiguity is commonly referredto as aleatoric uncertainty. Latent density models can be utilized to addressthis problem in image segmentation. The most popular approach is theProbabilistic U-Net (PU-Net), which uses latent Normal densities to optimizethe conditional data log-likelihood Evidence Lower Bound. In this work, wedemonstrate that the PU- Net latent space is severely inhomogenous. As aresult, the effectiveness of gradient descent is inhibited and the modelbecomes extremely sensitive to the localization of the latent space samples,resulting in defective predictions. To address this, we present the SinkhornPU-Net (SPU-Net), which uses the Sinkhorn Divergence to promote homogeneityacross all latent dimensions, effectively improving gradient-descent updatesand model robustness. Our results show that by applying this on public datasetsof various clinical segmentation problems, the SPU-Net receives up to 11%performance gains compared against preceding latent variable models forprobabilistic segmentation on the Hungarian-Matched metric. The resultsindicate that by encouraging a homogeneous latent space, one can significantlyimprove latent density modeling for medical image segmentation.</description><author>M. M. Amaan Valiuddin, Christiaan G. A. Viviers, Ruud J. G. van Sloun, Peter H. N. de With, Fons van der Sommen</author><pubDate>Tue, 14 Nov 2023 16:06:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16694v3</guid></item><item><title>An efficient semi-supervised quality control system trained using physics-based MRI-artefact generators and adversarial training</title><link>http://arxiv.org/abs/2206.03359v2</link><description>Large medical imaging data sets are becoming increasingly available, butensuring sample quality without significant artefacts is challenging. Existingmethods for identifying imperfections in medical imaging rely on data-intensiveapproaches, compounded by a scarcity of artefact-rich scans for trainingmachine learning models in clinical research. To tackle this problem, wepropose a framework with four main components: 1) artefact generators inspiredby magnetic resonance physics to corrupt brain MRI scans and augment a trainingdataset, 2) abstract and engineered features to represent images compactly, 3)a feature selection process depending on the artefact class to improveclassification, and 4) SVM classifiers to identify artefacts. Our contributionsare threefold: first, physics-based artefact generators produce synthetic brainMRI scans with controlled artefacts for data augmentation. This will avoid thelabour-intensive collection and labelling process of scans with rare artefacts.Second, we propose a pool of abstract and engineered image features to identify9 different artefacts for structural MRI. Finally, we use an artefact-basedfeature selection block that, for each class of artefacts, finds the set offeatures providing the best classification performance. We performed validationexperiments on a large data set of scans with artificially-generated artefacts,and in a multiple sclerosis clinical trial where real artefacts were identifiedby experts, showing that the proposed pipeline outperforms traditional methods.In particular, our data augmentation increases performance by up to 12.5percentage points on accuracy, precision, and recall. The computationalefficiency of our pipeline enables potential real-time deployment, promisinghigh-throughput clinical applications through automated image-processingpipelines driven by quality control systems.</description><author>Daniele Ravi, Frederik Barkhof, Daniel C. Alexander, Lemuel Puglisi, Geoffrey JM Parker, Arman Eshaghi</author><pubDate>Tue, 14 Nov 2023 16:06:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.03359v2</guid></item><item><title>Mobility-Induced Graph Learning for WiFi Positioning</title><link>http://arxiv.org/abs/2311.08271v1</link><description>A smartphone-based user mobility tracking could be effective in findinghis/her location, while the unpredictable error therein due to lowspecification of built-in inertial measurement units (IMUs) rejects itsstandalone usage but demands the integration to another positioning techniquelike WiFi positioning. This paper aims to propose a novel integration techniqueusing a graph neural network called Mobility-INduced Graph LEarning (MINGLE),which is designed based on two types of graphs made by capturing different usermobility features. Specifically, considering sequential measurement points(MPs) as nodes, a user's regular mobility pattern allows us to connect neighborMPs as edges, called time-driven mobility graph (TMG). Second, a user'srelatively straight transition at a constant pace when moving from one positionto another can be captured by connecting the nodes on each path, called adirection-driven mobility graph (DMG). Then, we can design graph convolutionnetwork (GCN)-based cross-graph learning, where two different GCN models forTMG and DMG are jointly trained by feeding different input features created byWiFi RTTs yet sharing their weights. Besides, the loss function includes amobility regularization term such that the differences between adjacentlocation estimates should be less variant due to the user's stable moving pace.Noting that the regularization term does not require ground-truth location,MINGLE can be designed under semi- and self-supervised learning frameworks. Theproposed MINGLE's effectiveness is extensively verified through fieldexperiments, showing a better positioning accuracy than benchmarks, say rootmean square errors (RMSEs) being 1.398 (m) and 1.073 (m) for self- andsemi-supervised learning cases, respectively.</description><author>Kyuwon Han, Seung Min Yu, Seong-Lyun Kim, Seung-Woo Ko</author><pubDate>Tue, 14 Nov 2023 16:06:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08271v1</guid></item><item><title>Defining the boundaries: challenges and advances in identifying cells in microscopy images</title><link>http://arxiv.org/abs/2311.08269v1</link><description>Segmentation, or the outlining of objects within images, is a critical stepin the measurement and analysis of cells within microscopy images. Whileimprovements continue to be made in tools that rely on classical methods forsegmentation, deep learning-based tools increasingly dominate advances in thetechnology. Specialist models such as Cellpose continue to improve in accuracyand user-friendliness, and segmentation challenges such as the Multi-ModalityCell Segmentation Challenge continue to push innovation in accuracy acrosswidely-varying test data as well as efficiency and usability. Increasedattention on documentation, sharing, and evaluation standards are leading toincreased user-friendliness and acceleration towards the goal of a trulyuniversal method.</description><author>Nodar Gogoberidze, Beth A. Cimini</author><pubDate>Tue, 14 Nov 2023 16:02:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08269v1</guid></item><item><title>A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily</title><link>http://arxiv.org/abs/2311.08268v1</link><description>Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed toprovide useful and safe responses. However, adversarial prompts known as'jailbreaks' can circumvent safeguards, leading LLMs to generate harmfulcontent. Exploring jailbreak prompts can help to better reveal the weaknessesof LLMs and further steer us to secure them. Unfortunately, existing jailbreakmethods either suffer from intricate manual design or require optimization onanother white-box model, compromising generalization or jailbreak efficiency.In this paper, we generalize jailbreak prompt attacks into two aspects: (1)Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM,an automatic framework that leverages LLMs themselves to generate effectivejailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantlyimproves the attack success rate while greatly reducing the time cost comparedto existing baselines. Our study also reveals the inadequacy of current defensemethods in safeguarding LLMs. Finally, we offer detailed analysis anddiscussion from the perspective of prompt execution priority on the failure ofLLMs' defense. We hope that our research can catalyze both the academiccommunity and LLMs vendors towards the provision of safer and more regulatedLarge Language Models.</description><author>Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, Shujian Huang</author><pubDate>Tue, 14 Nov 2023 16:02:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08268v1</guid></item><item><title>On The Relationship Between Universal Adversarial Attacks And Sparse Representations</title><link>http://arxiv.org/abs/2311.08265v1</link><description>The prominent success of neural networks, mainly in computer vision tasks, isincreasingly shadowed by their sensitivity to small, barely perceivableadversarial perturbations in image input. In this work, we aim at explaining this vulnerability through the frameworkof sparsity. We show the connection between adversarial attacks and sparserepresentations, with a focus on explaining the universality andtransferability of adversarial examples in neural networks. To this end, we show that sparse coding algorithms, and the neuralnetwork-based learned iterative shrinkage thresholding algorithm (LISTA) amongthem, suffer from this sensitivity, and that common attacks on neural networkscan be expressed as attacks on the sparse representation of the input image.The phenomenon that we observe holds true also when the network is agnostic tothe sparse representation and dictionary, and thus can provide a possibleexplanation for the universality and transferability of adversarial attacks. The code is available athttps://github.com/danawr/adversarial_attacks_and_sparse_representations.</description><author>Dana Weitzner, Raja Giryes</author><pubDate>Tue, 14 Nov 2023 16:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08265v1</guid></item><item><title>Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers Faster</title><link>http://arxiv.org/abs/2311.08263v1</link><description>In this work, we propose FastCoT, a model-agnostic framework based onparallel decoding without any further training of an auxiliary model ormodification to the LLM itself. FastCoT uses a size-varying context windowwhose size changes with position to conduct parallel decoding andauto-regressive decoding simultaneously, thus fully utilizing GPU computationresources. In FastCoT, the parallel decoding part provides the LLM with a quickglance of the future composed of approximate tokens, which could lead to fasteranswers compared to regular autoregressive decoding used by causaltransformers. We also provide an implementation of parallel decoding withinLLM, which supports KV-cache generation and batch processing. Through extensiveexperiments, we demonstrate that FastCoT saves inference time by nearly 20%with only a negligible performance drop compared to the regular approach.Additionally, we show that the context window size exhibits considerablerobustness for different tasks.</description><author>Hongxuan Zhang, Zhining Liu, Jiaqi Zheng, Chenyi Zhuang, Jinjie Gu, Guihai Chen</author><pubDate>Tue, 14 Nov 2023 15:56:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08263v1</guid></item><item><title>GenTKG: Generative Forecasting on Temporal Knowledge Graph</title><link>http://arxiv.org/abs/2310.07793v2</link><description>The rapid advancements in large language models (LLMs) have ignited interestin the temporal knowledge graph (tKG) domain, where conventional carefullydesigned embedding-based and rule-based models dominate. The question remainsopen of whether pre-trained LLMs can understand structured temporal relationaldata and replace them as the foundation model for temporal relationalforecasting. Therefore, we bring temporal knowledge forecasting into thegenerative setting. However, challenges occur in the huge chasms betweencomplex temporal graph data structure and sequential natural expressions LLMscan handle, and between the enormous data sizes of tKGs and heavy computationcosts of finetuning LLMs. To address these challenges, we propose a novelretrieval augmented generation framework that performs generative forecastingon tKGs named GenTKG, which combines a temporal logical rule-based retrievalstrategy and lightweight parameter-efficient instruction tuning. Extensiveexperiments have shown that GenTKG outperforms conventional methods of temporalrelational forecasting under low computation resources. GenTKG also highlightsremarkable transferability with exceeding performance on unseen datasetswithout re-training. Our work reveals the huge potential of LLMs in the tKGdomain and opens a new frontier for generative forecasting on tKGs.</description><author>Ruotong Liao, Xu Jia, Yunpu Ma, Volker Tresp</author><pubDate>Tue, 14 Nov 2023 15:51:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07793v2</guid></item><item><title>On the Lipschitz Constant of Deep Networks and Double Descent</title><link>http://arxiv.org/abs/2301.12309v4</link><description>Existing bounds on the generalization error of deep networks assume some formof smooth or bounded dependence on the input variable, falling short ofinvestigating the mechanisms controlling such factors in practice. In thiswork, we present an extensive experimental study of the empirical Lipschitzconstant of deep networks undergoing double descent, and highlightnon-monotonic trends strongly correlating with the test error. Building aconnection between parameter-space and input-space gradients for SGD around acritical point, we isolate two important factors -- namely loss landscapecurvature and distance of parameters from initialization -- respectivelycontrolling optimization dynamics around a critical point and bounding modelfunction complexity, even beyond the training data. Our study presents novelsinsights on implicit regularization via overparameterization, and effectivemodel complexity for networks trained in practice.</description><author>Matteo Gamba, Hossein Azizpour, Mårten Björkman</author><pubDate>Tue, 14 Nov 2023 15:48:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12309v4</guid></item><item><title>REST: Retrieval-Based Speculative Decoding</title><link>http://arxiv.org/abs/2311.08252v1</link><description>We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithmdesigned to speed up language model generation. The key insight driving thedevelopment of REST is the observation that the process of text generationoften includes certain common phases and patterns. Unlike previous methods thatrely on a draft language model for speculative decoding, REST harnesses thepower of retrieval to generate draft tokens. This method draws from thereservoir of existing knowledge, retrieving and employing relevant tokens basedon the current context. Its plug-and-play nature allows for seamlessintegration and acceleration of any language models, all without necessitatingadditional training. When benchmarked on 7B and 13B language models in asingle-batch setting, REST achieves a significant speedup of 1.62X to 2.36X oncode or text generation. The code of REST is available athttps://github.com/FasterDecoding/REST.</description><author>Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, Di He</author><pubDate>Tue, 14 Nov 2023 15:43:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08252v1</guid></item><item><title>Ruffle&amp;Riley: Towards the Automated Induction of Conversational Tutoring Systems</title><link>http://arxiv.org/abs/2310.01420v2</link><description>Conversational tutoring systems (CTSs) offer learning experiences driven bynatural language interaction. They are known to promote high levels ofcognitive engagement and benefit learning outcomes, particularly in reasoningtasks. Nonetheless, the time and cost required to author CTS content is a majorobstacle to widespread adoption. In this paper, we introduce a novel type ofCTS that leverages the recent advances in large language models (LLMs) in twoways: First, the system induces a tutoring script automatically from a lessontext. Second, the system automates the script orchestration via two LLM-basedagents (Ruffle&amp;Riley) with the roles of a student and a professor in alearning-by-teaching format. The system allows a free-form conversation thatfollows the ITS-typical inner and outer loop structure. In an initialbetween-subject online user study (N = 100) comparing Ruffle&amp;Riley to simplerQA chatbots and reading activity, we found no significant differences inpost-test scores. Nonetheless, in the learning experience survey, Ruffle&amp;Rileyusers expressed higher ratings of understanding and remembering and furtherperceived the offered support as more helpful and the conversation as coherent.Our study provides insights for a new generation of scalable CTS technologies.</description><author>Robin Schmucker, Meng Xia, Amos Azaria, Tom Mitchell</author><pubDate>Tue, 14 Nov 2023 15:41:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01420v2</guid></item><item><title>On Using Distribution-Based Compositionality Assessment to Evaluate Compositional Generalisation in Machine Translation</title><link>http://arxiv.org/abs/2311.08249v1</link><description>Compositional generalisation (CG), in NLP and in machine learning moregenerally, has been assessed mostly using artificial datasets. It is importantto develop benchmarks to assess CG also in real-world natural language tasks inorder to understand the abilities and limitations of systems deployed in thewild. To this end, our GenBench Collaborative Benchmarking Task submissionutilises the distribution-based compositionality assessment (DBCA) framework tosplit the Europarl translation corpus into a training and a test set in such away that the test set requires compositional generalisation capacity.Specifically, the training and test sets have divergent distributions ofdependency relations, testing NMT systems' capability of translatingdependencies that they have not been trained on. This is a fully-automatedprocedure to create natural language compositionality benchmarks, making itsimple and inexpensive to apply it further to other datasets and languages. Thecode and data for the experiments is available athttps://github.com/aalto-speech/dbca.</description><author>Anssi Moisio, Mathias Creutz, Mikko Kurimo</author><pubDate>Tue, 14 Nov 2023 15:37:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08249v1</guid></item><item><title>TENT: Connect Language Models with IoT Sensors for Zero-Shot Activity Recognition</title><link>http://arxiv.org/abs/2311.08245v1</link><description>Recent achievements in language models have showcased their extraordinarycapabilities in bridging visual information with semantic languageunderstanding. This leads us to a novel question: can language models connecttextual semantics with IoT sensory signals to perform recognition tasks, e.g.,Human Activity Recognition (HAR)? If so, an intelligent HAR system withhuman-like cognition can be built, capable of adapting to new environments andunseen categories. This paper explores its feasibility with an innovativeapproach, IoT-sEnsors-language alignmEnt pre-Training (TENT), which jointlyaligns textual embeddings with IoT sensor signals, including camera video,LiDAR, and mmWave. Through the IoT-language contrastive learning, we derive aunified semantic feature space that aligns multi-modal features with languageembeddings, so that the IoT data corresponds to specific words that describethe IoT data. To enhance the connection between textual categories and theirIoT data, we propose supplementary descriptions and learnable prompts thatbring more semantic information into the joint feature space. TENT can not onlyrecognize actions that have been seen but also ``guess'' the unseen action bythe closest textual words from the feature space. We demonstrate TENT achievesstate-of-the-art performance on zero-shot HAR tasks using different modalities,improving the best vision-language models by over 12%.</description><author>Yunjiao Zhou, Jianfei Yang, Han Zou, Lihua Xie</author><pubDate>Tue, 14 Nov 2023 15:30:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08245v1</guid></item><item><title>Investigating the Encoding of Words in BERT's Neurons using Feature Textualization</title><link>http://arxiv.org/abs/2311.08240v1</link><description>Pretrained language models (PLMs) form the basis of most state-of-the-art NLPtechnologies. Nevertheless, they are essentially black boxes: Humans do nothave a clear understanding of what knowledge is encoded in different parts ofthe models, especially in individual neurons. The situation is different incomputer vision, where feature visualization provides a decompositionalinterpretability technique for neurons of vision models. Activationmaximization is used to synthesize inherently interpretable visualrepresentations of the information encoded in individual neurons. Our work isinspired by this but presents a cautionary tale on the interpretability ofsingle neurons, based on the first large-scale attempt to adapt activationmaximization to NLP, and, more specifically, large PLMs. We propose featuretextualization, a technique to produce dense representations of neurons in thePLM word embedding space. We apply feature textualization to the BERT model(Devlin et al., 2019) to investigate whether the knowledge encoded inindividual neurons can be interpreted and symbolized. We find that the producedrepresentations can provide insights about the knowledge encoded in individualneurons, but that individual neurons do not represent clearcut symbolic unitsof language such as words. Additionally, we use feature textualization toinvestigate how many neurons are needed to encode words in BERT.</description><author>Tanja Baeumel, Soniya Vijayakumar, Josef van Genabith, Guenter Neumann, Simon Ostermann</author><pubDate>Tue, 14 Nov 2023 15:21:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08240v1</guid></item><item><title>Learning Physics-Inspired Regularization for Medical Image Registration with Hypernetworks</title><link>http://arxiv.org/abs/2311.08239v1</link><description>Medical image registration aims at identifying the spatial deformationbetween images of the same anatomical region and is fundamental to image-baseddiagnostics and therapy. To date, the majority of the deep learning-basedregistration methods employ regularizers that enforce global spatialsmoothness, e.g., the diffusion regularizer. However, such regularizers are nottailored to the data and might not be capable of reflecting the complexunderlying deformation. In contrast, physics-inspired regularizers promotephysically plausible deformations. One such regularizer is the linear elasticregularizer which models the deformation of elastic material. Theseregularizers are driven by parameters that define the material's physicalproperties. For biological tissue, a wide range of estimations of suchparameters can be found in the literature and it remains an open challenge toidentify suitable parameter values for successful registration. To overcomethis problem and to incorporate physical properties into learning-basedregistration, we propose to use a hypernetwork that learns the effect of thephysical parameters of a physics-inspired regularizer on the resulting spatialdeformation field. In particular, we adapt the HyperMorph framework to learnthe effect of the two elasticity parameters of the linear elastic regularizer.Our approach enables the efficient discovery of suitable, data-specificphysical parameters at test time.</description><author>Anna Reithmeir, Julia A. Schnabel, Veronika A. Zimmer</author><pubDate>Tue, 14 Nov 2023 15:20:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08239v1</guid></item><item><title>Towards ethical multimodal systems</title><link>http://arxiv.org/abs/2304.13765v2</link><description>Generative AI systems (ChatGPT, DALL-E, etc) are expanding into multipleareas of our lives, from art Rombach et al. [2021] to mental health Rob Morrisand Kareem Kouddous [2022]; their rapidly growing societal impact opens newopportunities, but also raises ethical concerns. The emerging field of AIalignment aims to make AI systems reflect human values. This paper focuses onevaluating the ethics of multimodal AI systems involving both text and images -a relatively under-explored area, as most alignment work is currently focusedon language models. We first create a multimodal ethical database from humanfeedback on ethicality. Then, using this database, we develop algorithms,including a RoBERTa-large classifier and a multilayer perceptron, toautomatically assess the ethicality of system responses.</description><author>Alexis Roger, Esma Aïmeur, Irina Rish</author><pubDate>Tue, 14 Nov 2023 15:19:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13765v2</guid></item><item><title>MeLo: Low-rank Adaptation is Better than Fine-tuning for Medical Image Diagnosis</title><link>http://arxiv.org/abs/2311.08236v1</link><description>The common practice in developing computer-aided diagnosis (CAD) models basedon transformer architectures usually involves fine-tuning from ImageNetpre-trained weights. However, with recent advances in large-scale pre-trainingand the practice of scaling laws, Vision Transformers (ViT) have become muchlarger and less accessible to medical imaging communities. Additionally, inreal-world scenarios, the deployments of multiple CAD models can be troublesomedue to problems such as limited storage space and time-consuming modelswitching. To address these challenges, we propose a new method MeLo (Medicalimage Low-rank adaptation), which enables the development of a single CAD modelfor multiple clinical tasks in a lightweight manner. It adopts low-rankadaptation instead of resource-demanding fine-tuning. By fixing the weight ofViT models and only adding small low-rank plug-ins, we achieve competitiveresults on various diagnosis tasks across different imaging modalities usingonly a few trainable parameters. Specifically, our proposed method achievescomparable performance to fully fine-tuned ViT models on four distinct medicalimaging datasets using about 0.17% trainable parameters. Moreover, MeLo addsonly about 0.5MB of storage space and allows for extremely fast model switchingin deployment and inference. Our source code and pre-trained weights areavailable on our website (https://absterzhu.github.io/melo.github.io/).</description><author>Yitao Zhu, Zhenrong Shen, Zihao Zhao, Sheng Wang, Xin Wang, Xiangyu Zhao, Dinggang Shen, Qian Wang</author><pubDate>Tue, 14 Nov 2023 15:18:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08236v1</guid></item><item><title>The convergence of the Stochastic Gradient Descent (SGD) : a self-contained proof</title><link>http://arxiv.org/abs/2103.14350v2</link><description>We give here a proof of the convergence of the Stochastic Gradient Descent(SGD) in a self-contained manner.</description><author>Gabrel Turinici</author><pubDate>Tue, 14 Nov 2023 15:18:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.14350v2</guid></item><item><title>From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL</title><link>http://arxiv.org/abs/2311.06595v2</link><description>The remarkable ability of Large Language Models (LLMs) to understand andfollow instructions has sometimes been limited by their in-context learning(ICL) performance in low-resource languages. To address this, we introduce anovel approach that leverages cross-lingual retrieval-augmented in-contextlearning (CREA-ICL). By extracting semantically similar prompts fromhigh-resource languages, we aim to improve the zero-shot performance ofmultilingual pre-trained language models (MPLMs) across diverse tasks. Thoughour approach yields steady improvements in classification tasks, it faceschallenges in generation tasks. Our evaluation offers insights into theperformance dynamics of retrieval-augmented in-context learning across bothclassification and generation domains.</description><author>Xiaoqian Li, Ercong Nie, Sheng Liang</author><pubDate>Tue, 14 Nov 2023 15:14:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06595v2</guid></item><item><title>Counterfactual Explanation for Regression via Disentanglement in Latent Space</title><link>http://arxiv.org/abs/2311.08228v1</link><description>Counterfactual Explanations (CEs) help address the question: How can thefactors that influence the prediction of a predictive model be changed toachieve a more favorable outcome from a user's perspective? Thus, they bear thepotential to guide the user's interaction with AI systems since they representeasy-to-understand explanations. To be applicable, CEs need to be realistic andactionable. In the literature, various methods have been proposed to generateCEs. However, the majority of research on CEs focuses on classificationproblems where questions like ``What should I do to get my rejected loanapproved?" are raised. In practice, answering questions like ``What should I doto increase my salary?" are of a more regressive nature. In this paper, weintroduce a novel method to generate CEs for a pre-trained regressor by firstdisentangling the label-relevant from the label-irrelevant dimensions in thelatent space. CEs are then generated by combining the label-irrelevantdimensions and the predefined output. The intuition behind this approach isthat the ideal counterfactual search should focus on the label-irrelevantcharacteristics of the input and suggest changes toward target-relevantcharacteristics. Searching in the latent space could help achieve this goal. Weshow that our method maintains the characteristics of the query sample duringthe counterfactual search. In various experiments, we demonstrate that theproposed method is competitive based on different quality measures on image andtabular datasets in regression problem settings. It efficiently returns resultscloser to the original data manifold compared to three state-of-the-artmethods, which is essential for realistic high-dimensional machine learningapplications. Our code will be made available as an open-source package uponthe publication of this work.</description><author>Xuan Zhao, Klaus Broelemann, Gjergji Kasneci</author><pubDate>Tue, 14 Nov 2023 15:08:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08228v1</guid></item><item><title>Uni-COAL: A Unified Framework for Cross-Modality Synthesis and Super-Resolution of MR Images</title><link>http://arxiv.org/abs/2311.08225v1</link><description>Cross-modality synthesis (CMS), super-resolution (SR), and their combination(CMSR) have been extensively studied for magnetic resonance imaging (MRI).Their primary goals are to enhance the imaging quality by synthesizing thedesired modality and reducing the slice thickness. Despite the promisingsynthetic results, these techniques are often tailored to specific tasks,thereby limiting their adaptability to complex clinical scenarios. Therefore,it is crucial to build a unified network that can handle various imagesynthesis tasks with arbitrary requirements of modality and resolutionsettings, so that the resources for training and deploying the models can begreatly reduced. However, none of the previous works is capable of performingCMS, SR, and CMSR using a unified network. Moreover, these MRI reconstructionmethods often treat alias frequencies improperly, resulting in suboptimaldetail restoration. In this paper, we propose a Unified Co-Modulated Alias-freeframework (Uni-COAL) to accomplish the aforementioned tasks with a singlenetwork. The co-modulation design of the image-conditioned and stochasticattribute representations ensures the consistency between CMS and SR, whilesimultaneously accommodating arbitrary combinations of input/output modalitiesand thickness. The generator of Uni-COAL is also designed to be alias-freebased on the Shannon-Nyquist signal processing framework, ensuring effectivesuppression of alias frequencies. Additionally, we leverage the semantic priorof Segment Anything Model (SAM) to guide Uni-COAL, ensuring a more authenticpreservation of anatomical structures during synthesis. Experiments on threedatasets demonstrate that Uni-COAL outperforms the alternatives in CMS, SR, andCMSR tasks for MR images, which highlights its generalizability to wide-rangeapplications.</description><author>Zhiyun Song, Zengxin Qi, Xin Wang, Xiangyu Zhao, Zhenrong Shen, Sheng Wang, Manman Fei, Zhe Wang, Di Zang, Dongdong Chen, Linlin Yao, Qian Wang, Xuehai Wu, Lichi Zhang</author><pubDate>Tue, 14 Nov 2023 15:05:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08225v1</guid></item><item><title>Improving Image Captioning via Predicting Structured Concepts</title><link>http://arxiv.org/abs/2311.08223v1</link><description>Having the difficulty of solving the semantic gap between images and textsfor the image captioning task, conventional studies in this area paid someattention to treating semantic concepts as a bridge between the two modalitiesand improved captioning performance accordingly. Although promising results onconcept prediction were obtained, the aforementioned studies normally ignorethe relationship among concepts, which relies on not only objects in the image,but also word dependencies in the text, so that offers a considerable potentialfor improving the process of generating good descriptions. In this paper, wepropose a structured concept predictor (SCP) to predict concepts and theirstructures, then we integrate them into captioning, so as to enhance thecontribution of visual signals in this task via concepts and further use theirrelations to distinguish cross-modal semantics for better descriptiongeneration. Particularly, we design weighted graph convolutional networks(W-GCN) to depict concept relations driven by word dependencies, and thenlearns differentiated contributions from these concepts for following decodingprocess. Therefore, our approach captures potential relations among conceptsand discriminatively learns different concepts, so that effectively facilitatesimage captioning with inherited information across modalities. Extensiveexperiments and their results demonstrate the effectiveness of our approach aswell as each proposed module in this work.</description><author>Ting Wang, Weidong Chen, Yuanhe Tian, Yan Song, Zhendong Mao</author><pubDate>Tue, 14 Nov 2023 15:01:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08223v1</guid></item><item><title>Eval-GCSC: A New Metric for Evaluating ChatGPT's Performance in Chinese Spelling Correction</title><link>http://arxiv.org/abs/2311.08219v1</link><description>ChatGPT has demonstrated impressive performance in various downstream tasks.However, in the Chinese Spelling Correction (CSC) task, we observe adiscrepancy: while ChatGPT performs well under human evaluation, it scorespoorly according to traditional metrics. We believe this inconsistency arisesbecause the traditional metrics are not well-suited for evaluating generativemodels. Their overly strict length and phonics constraints may lead tounderestimating ChatGPT's correction capabilities. To better evaluategenerative models in the CSC task, this paper proposes a new evaluation metric:Eval-GCSC. By incorporating word-level and semantic similarity judgments, itrelaxes the stringent length and phonics constraints. Experimental results showthat Eval-GCSC closely aligns with human evaluations. Under this metric,ChatGPT's performance is comparable to traditional token-level classificationmodels (TCM), demonstrating its potential as a CSC tool. The source code andscripts can be accessed at https://github.com/ktlKTL/Eval-GCSC.</description><author>Kunting Li, Yong Hu, Shaolei Wang, Hanhan Ma, Liang He, Fandong Meng, Jie Zhou</author><pubDate>Tue, 14 Nov 2023 14:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08219v1</guid></item><item><title>Peer is Your Pillar: A Data-unbalanced Conditional GANs for Few-shot Image Generation</title><link>http://arxiv.org/abs/2311.08217v1</link><description>Few-shot image generation aims to train generative models using a smallnumber of training images. When there are few images available for training(e.g. 10 images), Learning From Scratch (LFS) methods often generate imagesthat closely resemble the training data while Transfer Learning (TL) methodstry to improve performance by leveraging prior knowledge from GANs pre-trainedon large-scale datasets. However, current TL methods may not allow forsufficient control over the degree of knowledge preservation from the sourcemodel, making them unsuitable for setups where the source and target domainsare not closely related. To address this, we propose a novel pipeline calledPeer is your Pillar (PIP), which combines a target few-shot dataset with a peerdataset to create a data-unbalanced conditional generation. Our approachincludes a class embedding method that separates the class space from thelatent space, and we use a direction loss based on pre-trained CLIP to improveimage diversity. Experiments on various few-shot datasets demonstrate theadvancement of the proposed PIP, especially reduces the training requirementsof few-shot image generation.</description><author>Ziqiang Li, Chaoyue Wang, Xue Rui, Chao Xue, Jiaxu Leng, Bin Li</author><pubDate>Tue, 14 Nov 2023 14:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08217v1</guid></item><item><title>Frequentist Guarantees of Distributed (Non)-Bayesian Inference</title><link>http://arxiv.org/abs/2311.08214v1</link><description>Motivated by the need to analyze large, decentralized datasets, distributedBayesian inference has become a critical research area across multiple fields,including statistics, electrical engineering, and economics. This paperestablishes Frequentist properties, such as posterior consistency, asymptoticnormality, and posterior contraction rates, for the distributed (non-)BayesInference problem among agents connected via a communication network. Ourresults show that, under appropriate assumptions on the communication graph,distributed Bayesian inference retains parametric efficiency while enhancingrobustness in uncertainty quantification. We also explore the trade-off betweenstatistical efficiency and communication efficiency by examining how the designand size of the communication graph impact the posterior contraction rate.Furthermore, We extend our analysis to time-varying graphs and apply ourresults to exponential family models, distributed logistic regression, anddecentralized detection models.</description><author>Bohan Wu, César A. Uribe</author><pubDate>Tue, 14 Nov 2023 14:50:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08214v1</guid></item><item><title>Unlock the Power: Competitive Distillation for Multi-Modal Large Language Models</title><link>http://arxiv.org/abs/2311.08213v1</link><description>Recently, multi-modal content generation has attracted lots of attention fromresearchers by investigating the utilization of visual instruction tuning basedon large language models (LLMs). To enhance the performance and generalizationability of such LLMs, the practice of distilling knowledge from pretrainedmulti-modal models (a.k.a. teachers) to more compact multi-modal LLMs(students) has gained considerable interest. However, the prevailing paradigmof instructiontuning in multi-modal LLMs knowledge distillation isresource-intensive and unidirectional, neglecting the potential for mutualfeedback between the student and teacher models. Thus, we propose an innovativeCompetitive Multi-modal Distillation framework (CoMD), which capturesbidirectional feedback between teacher and student models and continuallyupdates the multi-modal capabilities that the student model has learned. Itcomprises two stages: multi-modal pre-training and multi-modal competitivedistillation. The first stage pre-trains the student model on a large number offiltered multi-modal datasets. The second stage facilitates a bidirectionalknowledge transfer between the student and teacher models. Our experimentalanalysis of diverse datasets shows that our knowledge transfer methodconsistently improves the capabilities of the student model. Finally, the7B-sized student model after four distillations surpassed the currentstate-of-the-art model LLaVA-13B on the ScienceQA and LLaVA Test dataset, alsooutperforms other strong baselines in the zero-shot setting.</description><author>Xinwei Li, Li Lin, Shuai Wang, Chen Qian</author><pubDate>Tue, 14 Nov 2023 14:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08213v1</guid></item><item><title>Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving</title><link>http://arxiv.org/abs/2310.02251v2</link><description>Talk2BEV is a large vision-language model (LVLM) interface for bird's-eyeview (BEV) maps in autonomous driving contexts. While existing perceptionsystems for autonomous driving scenarios have largely focused on a pre-defined(closed) set of object categories and driving scenarios, Talk2BEV blends recentadvances in general-purpose language and vision models with BEV-structured maprepresentations, eliminating the need for task-specific models. This enables asingle system to cater to a variety of autonomous driving tasks encompassingvisual and spatial reasoning, predicting the intents of traffic actors, anddecision-making based on visual cues. We extensively evaluate Talk2BEV on alarge number of scene understanding tasks that rely on both the ability tointerpret free-form natural language queries, and in grounding these queries tothe visual context embedded into the language-enhanced BEV map. To enablefurther research in LVLMs for autonomous driving scenarios, we develop andrelease Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEVscenarios, with more than 20,000 questions and ground-truth responses from theNuScenes dataset.</description><author>Tushar Choudhary, Vikrant Dewangan, Shivam Chandhok, Shubham Priyadarshan, Anushka Jain, Arun K. Singh, Siddharth Srivastava, Krishna Murthy Jatavallabhula, K. Madhava Krishna</author><pubDate>Tue, 14 Nov 2023 14:46:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02251v2</guid></item><item><title>Human-Centric Autonomous Systems With LLMs for User Command Reasoning</title><link>http://arxiv.org/abs/2311.08206v1</link><description>The evolution of autonomous driving has made remarkable advancements inrecent years, evolving into a tangible reality. However, a human-centriclarge-scale adoption hinges on meeting a variety of multifaceted requirements.To ensure that the autonomous system meets the user's intent, it is essentialto accurately discern and interpret user commands, especially in complex oremergency situations. To this end, we propose to leverage the reasoningcapabilities of Large Language Models (LLMs) to infer system requirements fromin-cabin users' commands. Through a series of experiments that includedifferent LLM models and prompt designs, we explore the few-shot multivariatebinary classification accuracy of system requirements from natural languagetextual commands. We confirm the general ability of LLMs to understand andreason about prompts but underline that their effectiveness is conditioned onthe quality of both the LLM model and the design of appropriate sequentialprompts. Code and models are public with the link\url{https://github.com/KTH-RPL/DriveCmd_LLM}.</description><author>Yi Yang, Qingwen Zhang, Ci Li, Daniel Simões Marta, Nazre Batool, John Folkesson</author><pubDate>Tue, 14 Nov 2023 14:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08206v1</guid></item><item><title>Missing Value Imputation for Multi-attribute Sensor Data Streams via Message Propagation (Extended Version)</title><link>http://arxiv.org/abs/2311.07344v2</link><description>Sensor data streams occur widely in various real-time applications in thecontext of the Internet of Things (IoT). However, sensor data streams featuremissing values due to factors such as sensor failures, communication errors, ordepleted batteries. Missing values can compromise the quality of real-timeanalytics tasks and downstream applications. Existing imputation methods eithermake strong assumptions about streams or have low efficiency. In this study, weaim to accurately and efficiently impute missing values in data streams thatsatisfy only general characteristics in order to benefit real-time applicationsmore widely. First, we propose a message propagation imputation network (MPIN)that is able to recover the missing values of data instances in a time window.We give a theoretical analysis of why MPIN is effective. Second, we present acontinuous imputation framework that consists of data update and model updatemechanisms to enable MPIN to perform continuous imputation both effectively andefficiently. Extensive experiments on multiple real datasets show that MPIN canoutperform the existing data imputers by wide margins and that the continuousimputation framework is efficient and accurate.</description><author>Xiao Li, Huan Li, Hua Lu, Christian S. Jensen, Varun Pandey, Volker Markl</author><pubDate>Tue, 14 Nov 2023 14:39:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07344v2</guid></item><item><title>Federated Skewed Label Learning with Logits Fusion</title><link>http://arxiv.org/abs/2311.08202v1</link><description>Federated learning (FL) aims to collaboratively train a shared model acrossmultiple clients without transmitting their local data. Data heterogeneity is acritical challenge in realistic FL settings, as it causes significantperformance deterioration due to discrepancies in optimization among localmodels. In this work, we focus on label distribution skew, a common scenario indata heterogeneity, where the data label categories are imbalanced on eachclient. To address this issue, we propose FedBalance, which corrects theoptimization bias among local models by calibrating their logits. Specifically,we introduce an extra private weak learner on the client side, which forms anensemble model with the local model. By fusing the logits of the two models,the private weak learner can capture the variance of different data, regardlessof their category. Therefore, the optimization direction of local models can beimproved by increasing the penalty for misclassifying minority classes andreducing the attention to majority classes, resulting in a better global model.Extensive experiments show that our method can gain 13\% higher averageaccuracy compared with state-of-the-art methods.</description><author>Yuwei Wang, Runhan Li, Hao Tan, Xuefeng Jiang, Sheng Sun, Min Liu, Bo Gao, Zhiyuan Wu</author><pubDate>Tue, 14 Nov 2023 14:37:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08202v1</guid></item><item><title>Diffusion-based generation of Histopathological Whole Slide Images at a Gigapixel scale</title><link>http://arxiv.org/abs/2311.08199v1</link><description>We present a novel diffusion-based approach to generate synthetichistopathological Whole Slide Images (WSIs) at an unprecedented gigapixelscale. Synthetic WSIs have many potential applications: They can augmenttraining datasets to enhance the performance of many computational pathologyapplications. They allow the creation of synthesized copies of datasets thatcan be shared without violating privacy regulations. Or they can facilitatelearning representations of WSIs without requiring data annotations. Despitethis variety of applications, no existing deep-learning-based method generatesWSIs at their typically high resolutions. Mainly due to the high computationalcomplexity. Therefore, we propose a novel coarse-to-fine sampling scheme totackle image generation of high-resolution WSIs. In this scheme, we increasethe resolution of an initial low-resolution image to a high-resolution WSI.Particularly, a diffusion model sequentially adds fine details to images andincreases their resolution. In our experiments, we train our method with WSIsfrom the TCGA-BRCA dataset. Additionally to quantitative evaluations, we alsoperformed a user study with pathologists. The study results suggest that ourgenerated WSIs resemble the structure of real WSIs.</description><author>Robert Harb, Thomas Pock, Heimo Müller</author><pubDate>Tue, 14 Nov 2023 14:33:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08199v1</guid></item><item><title>Automated Fact-Checking in Dialogue: Are Specialized Models Needed?</title><link>http://arxiv.org/abs/2311.08195v1</link><description>Prior research has shown that typical fact-checking models for stand-aloneclaims struggle with claims made in dialogues. As a solution, fine-tuning thesemodels on labelled dialogue data has been proposed. However, creating separatemodels for each use case is impractical, and we show that fine-tuning modelsfor dialogue results in poor performance on typical fact-checking. To overcomethis challenge, we present techniques that allow us to use the same models forboth dialogue and typical fact-checking. These mainly focus on retrievaladaptation and transforming conversational inputs so that they can beaccurately predicted by models trained on stand-alone claims. We demonstratethat a typical fact-checking model incorporating these techniques iscompetitive with state-of-the-art models fine-tuned for dialogue, whilemaintaining its accuracy on stand-alone claims.</description><author>Eric Chamoun, Marzieh Saeidi, Andreas Vlachos</author><pubDate>Tue, 14 Nov 2023 14:29:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08195v1</guid></item><item><title>GEC-DePenD: Non-Autoregressive Grammatical Error Correction with Decoupled Permutation and Decoding</title><link>http://arxiv.org/abs/2311.08191v1</link><description>Grammatical error correction (GEC) is an important NLP task that is currentlyusually solved with autoregressive sequence-to-sequence models. However,approaches of this class are inherently slow due to one-by-one tokengeneration, so non-autoregressive alternatives are needed. In this work, wepropose a novel non-autoregressive approach to GEC that decouples thearchitecture into a permutation network that outputs a self-attention weightmatrix that can be used in beam search to find the best permutation of inputtokens (with auxiliary {ins} tokens) and a decoder network based on astep-unrolled denoising autoencoder that fills in specific tokens. This allowsus to find the token permutation after only one forward pass of the permutationnetwork, avoiding autoregressive constructions. We show that the resultingnetwork improves over previously known non-autoregressive methods for GEC andreaches the level of autoregressive methods that do not use language-specificsynthetic data generation methods. Our results are supported by a comprehensiveexperimental validation on the ConLL-2014 and Write&amp;Improve+LOCNESS datasetsand an extensive ablation study that supports our architectural and algorithmicchoices.</description><author>Konstantin Yakovlev, Alexander Podolskiy, Andrey Bout, Sergey Nikolenko, Irina Piontkovskaya</author><pubDate>Tue, 14 Nov 2023 14:24:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08191v1</guid></item><item><title>SAMIHS: Adaptation of Segment Anything Model for Intracranial Hemorrhage Segmentation</title><link>http://arxiv.org/abs/2311.08190v1</link><description>Segment Anything Model (SAM), a vision foundation model trained onlarge-scale annotations, has recently continued raising awareness withinmedical image segmentation. Despite the impressive capabilities of SAM onnatural scenes, it struggles with performance decline when confronted withmedical images, especially those involving blurry boundaries and highlyirregular regions of low contrast. In this paper, a SAM-basedparameter-efficient fine-tuning method, called SAMIHS, is proposed forintracranial hemorrhage segmentation, which is a crucial and challenging stepin stroke diagnosis and surgical planning. Distinguished from previous SAM andSAM-based methods, SAMIHS incorporates parameter-refactoring adapters intoSAM's image encoder and considers the efficient and flexible utilization ofadapters' parameters. Additionally, we employ a combo loss that combines binarycross-entropy loss and boundary-sensitive loss to enhance SAMIHS's ability torecognize the boundary regions. Our experimental results on two public datasetsdemonstrate the effectiveness of our proposed method. Code is available athttps://github.com/mileswyn/SAMIHS .</description><author>Yinuo Wang, Kai Chen, Weimin Yuan, Cai Meng, XiangZhi Bai</author><pubDate>Tue, 14 Nov 2023 14:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08190v1</guid></item><item><title>Unlocking Science: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction</title><link>http://arxiv.org/abs/2311.08189v1</link><description>Extracting key information from scientific papers has the potential to helpresearchers work more efficiently and accelerate the pace of scientificprogress. Over the last few years, research on Scientific InformationExtraction (SciIE) witnessed the release of several new systems and benchmarks.However, existing paper-focused datasets mostly focus only on specific parts ofa manuscript (e.g., abstracts) and are single-modality (i.e., text- ortable-only), due to complex processing and expensive annotations. Moreover,core information can be present in either text or tables or across both. Toclose this gap in data availability and enable cross-modality IE, whilealleviating labeling costs, we propose a semi-supervised pipeline forannotating entities in text, as well as entities and relations in tables, in aniterative procedure. Based on this pipeline, we release novel resources for thescientific community, including a high-quality benchmark, a large-scale corpus,and a semi-supervised annotation pipeline. We further report the performance ofstate-of-the-art IE models on the proposed benchmark dataset, as a baseline.Lastly, we explore the potential capability of large language models such asChatGPT for the current task. Our new dataset, results, and analysis validatethe effectiveness and efficiency of our semi-supervised pipeline, and wediscuss its remaining limitations.</description><author>Yuhan Li, Jian Wu, Zhiwei Yu, Börje F. Karlsso, Wei Shen, Manabu Okumura, Chin-Yew Lin</author><pubDate>Tue, 14 Nov 2023 14:22:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08189v1</guid></item><item><title>Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning</title><link>http://arxiv.org/abs/2311.08182v1</link><description>Enhancing the instruction-following ability of Large Language Models (LLMs)primarily demands substantial instruction-tuning datasets. However, the sheervolume of these imposes a considerable computational burden and annotationcost. To investigate a label-efficient instruction tuning method that allowsthe model itself to actively sample subsets that are equally or even moreeffective, we introduce a self-evolving mechanism DiverseEvol. In this process,a model iteratively augments its training subset to refine its own performance,without requiring any intervention from humans or more advanced LLMs. The keyto our data sampling technique lies in the enhancement of diversity in thechosen subsets, as the model selects new data points most distinct from anyexisting ones according to its current embedding space. Extensive experimentsacross three datasets and benchmarks demonstrate the effectiveness ofDiverseEvol. Our models, trained on less than 8% of the original dataset,maintain or improve performance compared with finetuning on full data. We alsoprovide empirical evidence to analyze the importance of diversity ininstruction data and the iterative scheme as opposed to one-time sampling. Ourcode is publicly available at https://github.com/OFA-Sys/DiverseEvol.git.</description><author>Shengguang Wu, Keming Lu, Benfeng Xu, Junyang Lin, Qi Su, Chang Zhou</author><pubDate>Tue, 14 Nov 2023 14:10:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08182v1</guid></item><item><title>Semi-Supervised Learning via Swapped Prediction for Communication Signal Recognition</title><link>http://arxiv.org/abs/2311.08179v1</link><description>Deep neural networks have been widely used in communication signalrecognition and achieved remarkable performance, but this superiority typicallydepends on using massive examples for supervised learning, whereas training adeep neural network on small datasets with few labels generally falls intooverfitting, resulting in degenerated performance. To this end, we develop asemi-supervised learning (SSL) method that effectively utilizes a largecollection of more readily available unlabeled signal data to improvegeneralization. The proposed method relies largely on a novel implementation ofconsistency-based regularization, termed Swapped Prediction, which leveragesstrong data augmentation to perturb an unlabeled sample and then encourage itscorresponding model prediction to be close to its original, optimized with ascaled cross-entropy loss with swapped symmetry. Extensive experiments indicatethat our proposed method can achieve a promising result for deep SSL ofcommunication signal recognition.</description><author>Weidong Wang, Hongshu Liao, Lu Gan</author><pubDate>Tue, 14 Nov 2023 14:08:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08179v1</guid></item><item><title>Generalized partitioned local depth</title><link>http://arxiv.org/abs/2303.10167v4</link><description>In this paper we provide a generalization of the concept of cohesion asintroduced recently by Berenhaut, Moore and Melvin [Proceedings of the NationalAcademy of Sciences, 119 (4) (2022)]. The formulation presented builds on thetechnique of partitioned local depth by distilling two key probabilisticconcepts: local relevance and support division. Earlier results are extendedwithin the new context, and examples of applications to revealing communitiesin data with uncertainty are included. The work sheds light on the foundationsof partitioned local depth, and extends the original ideas to enableprobabilistic consideration of uncertain, variable and potentially conflictinginformation.</description><author>Kenneth S. Berenhaut, John D. Foley, Liangdongsheng Lyu</author><pubDate>Tue, 14 Nov 2023 14:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10167v4</guid></item></channel></rss>