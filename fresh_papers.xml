<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 14 Dec 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>SAM-guided Graph Cut for 3D Instance Segmentation</title><link>http://arxiv.org/abs/2312.08372v1</link><description>This paper addresses the challenge of 3D instance segmentation bysimultaneously leveraging 3D geometric and multi-view image information. Manyprevious works have applied deep learning techniques to 3D point clouds forinstance segmentation. However, these methods often failed to generalize tovarious types of scenes due to the scarcity and low-diversity of labeled 3Dpoint cloud data. Some recent works have attempted to lift 2D instancesegmentations to 3D within a bottom-up framework. The inconsistency in 2Dinstance segmentations among views can substantially degrade the performance of3D segmentation. In this work, we introduce a novel 3D-to-2D query framework toeffectively exploit 2D segmentation models for 3D instance segmentation.Specifically, we pre-segment the scene into several superpoints in 3D,formulating the task into a graph cut problem. The superpoint graph isconstructed based on 2D segmentation models, where node features are obtainedfrom multi-view image features and edge weights are computed based onmulti-view segmentation results, enabling the better generalization ability. Toprocess the graph, we train a graph neural network using pseudo 3D labels from2D segmentation models. Experimental results on the ScanNet, ScanNet++ andKITTI-360 datasets demonstrate that our method achieves robust segmentationperformance and can generalize across different types of scenes. Our projectpage is available at https://zju3dv.github.io/sam_graph.</description><author>Haoyu Guo, He Zhu, Sida Peng, Yuang Wang, Yujun Shen, Ruizhen Hu, Xiaowei Zhou</author><pubDate>Wed, 13 Dec 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08372v1</guid></item><item><title>PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object Detection</title><link>http://arxiv.org/abs/2312.08371v1</link><description>Recent temporal LiDAR-based 3D object detectors achieve promising performancebased on the two-stage proposal-based approach. They generate 3D box candidatesfrom the first-stage dense detector, followed by different temporal aggregationmethods. However, these approaches require per-frame objects or whole pointclouds, posing challenges related to memory bank utilization. Moreover, pointclouds and trajectory features are combined solely based on concatenation,which may neglect effective interactions between them. In this paper, wepropose a point-trajectory transformer with long short-term memory forefficient temporal 3D object detection. To this end, we only utilize pointclouds of current-frame objects and their historical trajectories as input tominimize the memory bank storage requirement. Furthermore, we introduce modulesto encode trajectory features, focusing on long short-term and future-awareperspectives, and then effectively aggregate them with point cloud features. Weconduct extensive experiments on the large-scale Waymo dataset to demonstratethat our approach performs well against state-of-the-art methods. Code andmodels will be made publicly available at https://github.com/kuanchihhuang/PTT.</description><author>Kuan-Chih Huang, Weijie Lyu, Ming-Hsuan Yang, Yi-Hsuan Tsai</author><pubDate>Wed, 13 Dec 2023 18:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08371v1</guid></item><item><title>The Effective Horizon Explains Deep RL Performance in Stochastic Environments</title><link>http://arxiv.org/abs/2312.08369v1</link><description>Reinforcement learning (RL) theory has largely focused on proving minimaxsample complexity bounds. These require strategic exploration algorithms thatuse relatively limited function classes for representing the policy or valuefunction. Our goal is to explain why deep RL algorithms often perform well inpractice, despite using random exploration and much more expressive functionclasses like neural networks. Our work arrives at an explanation by showingthat many stochastic MDPs can be solved by performing only a few steps of valueiteration on the random policy's Q function and then acting greedily. When thisis true, we find that it is possible to separate the exploration and learningcomponents of RL, making it much easier to analyze. We introduce a new RLalgorithm, SQIRL, that iteratively learns a near-optimal policy by exploringrandomly to collect rollouts and then performing a limited number of steps offitted-Q iteration over those rollouts. Any regression algorithm that satisfiesbasic in-distribution generalization properties can be used in SQIRL toefficiently solve common MDPs. This can explain why deep RL works neuralnetworks, since it is empirically established that neural networks generalizewell in-distribution. Furthermore, SQIRL explains why random exploration workswell in practice, since we show many environments can be solved by estimatingthe random policy's Q-function and then applying zero or a few steps of valueiteration. We leverage SQIRL to derive instance-dependent sample complexitybounds for RL that are exponential only in an "effective horizon" of lookaheadand on the complexity of the class used for function approximation.Empirically, we also find that SQIRL performance strongly correlates with PPOand DQN performance in a variety of stochastic environments, supporting thatour theoretical analysis is predictive of practical performance.</description><author>Cassidy Laidlaw, Banghua Zhu, Stuart Russell, Anca Dragan</author><pubDate>Wed, 13 Dec 2023 18:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08369v1</guid></item><item><title>VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering</title><link>http://arxiv.org/abs/2312.08367v1</link><description>In this work, we propose an efficient Video-Language Alignment viaFrame-Prompting and Distilling (VLAP) network. Our VLAP model addresses bothefficient frame sampling and effective cross-modal alignment in a unified way.In our VLAP network, we design a new learnable question-aware Frame-Promptertogether with a new cross-modal distillation (QFormer-Distiller) module.Pre-trained large image-language models have shown promising results onproblems such as visual question answering. However, how to efficiently andeffectively sample image frames when adapting pre-trained large image-languagemodel to video-language alignment is still the major challenge. Compared withprior work, our VLAP model demonstrates the capability of selecting key frameswith critical contents, thus improving the video-language alignment accuracywhile reducing the inference latency (+3.3% on NExT-QA Temporal with 3.0X speedup). Overall, our VLAP network outperforms (e.g. +4.6% on STAR Interaction and+2.2% on STAR average with 3.0X speed up, ours 2-frames out-perform SeViLA4-frames on VLEP with 4.2X speed up) the state-of-the-art methods on the videoquestion-answering benchmarks.</description><author>Xijun Wang, Junbang Liang, Chun-Kai Wang, Kenan Deng, Yu Lou, Ming Lin, Shan Yang</author><pubDate>Wed, 13 Dec 2023 18:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08367v1</guid></item><item><title>See, Say, and Segment: Teaching LMMs to Overcome False Premises</title><link>http://arxiv.org/abs/2312.08366v1</link><description>Current open-source Large Multimodal Models (LMMs) excel at tasks such asopen-vocabulary language grounding and segmentation but can suffer under falsepremises when queries imply the existence of something that is not actuallypresent in the image. We observe that existing methods that fine-tune an LMM tosegment images significantly degrade their ability to reliably determine("see") if an object is present and to interact naturally with humans ("say"),a form of catastrophic forgetting. In this work, we propose a cascading andjoint training approach for LMMs to solve this task, avoiding catastrophicforgetting of previous skills. Our resulting model can "see" by detectingwhether objects are present in an image, "say" by telling the user if they arenot, proposing alternative queries or correcting semantic errors in the query,and finally "segment" by outputting the mask of the desired objects if theyexist. Additionally, we introduce a novel False Premise Correction benchmarkdataset, an extension of existing RefCOCO(+/g) referring segmentation datasets(which we call FP-RefCOCO(+/g)). The results show that our method not onlydetects false premises up to 55% better than existing approaches, but underfalse premise conditions produces relative cIOU improvements of more than 31%over baselines, and produces natural language feedback judged helpful up to 67%of the time.</description><author>Tsung-Han Wu, Giscard Biamby, David Chan, Lisa Dunlap, Ritwik Gupta, Xudong Wang, Joseph E. Gonzalez, Trevor Darrell</author><pubDate>Wed, 13 Dec 2023 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08366v1</guid></item><item><title>An Invitation to Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2312.08365v1</link><description>Training a deep neural network to maximize a target objective has become thestandard recipe for successful machine learning over the last decade. Thesenetworks can be optimized with supervised learning, if the target objective isdifferentiable. For many interesting problems, this is however not the case.Common objectives like intersection over union (IoU), bilingual evaluationunderstudy (BLEU) score or rewards cannot be optimized with supervisedlearning. A common workaround is to define differentiable surrogate losses,leading to suboptimal solutions with respect to the actual objective.Reinforcement learning (RL) has emerged as a promising alternative foroptimizing deep neural networks to maximize non-differentiable objectives inrecent years. Examples include aligning large language models via humanfeedback, code generation, object detection or control problems. This makes RLtechniques relevant to the larger machine learning audience. The subject is,however, time intensive to approach due to the large range of methods, as wellas the often very theoretical presentation. In this introduction, we take analternative approach, different from classic reinforcement learning textbooks.Rather than focusing on tabular problems, we introduce reinforcement learningas a generalization of supervised learning, which we first apply tonon-differentiable objectives and later to temporal problems. Assuming onlybasic knowledge of supervised learning, the reader will be able to understandstate-of-the-art deep RL algorithms like proximal policy optimization (PPO)after reading this tutorial.</description><author>Bernhard Jaeger, Andreas Geiger</author><pubDate>Wed, 13 Dec 2023 18:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08365v1</guid></item><item><title>View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data</title><link>http://arxiv.org/abs/2312.08364v1</link><description>Procedural synthetic data generation has received increasing attention incomputer vision. Procedural signed distance functions (SDFs) are a powerfultool for modeling large-scale detailed scenes, but existing mesh extractionmethods have artifacts or performance profiles that limit their use forsynthetic data. We propose OcMesher, a mesh extraction algorithm thatefficiently handles high-detail unbounded scenes with perfect view-consistency,with easy export to downstream real-time engines. The main novelty of oursolution is an algorithm to construct an octree based on a given SDF andmultiple camera views. We performed extensive experiments, and show oursolution produces better synthetic data for training and evaluation of computervision models.</description><author>Zeyu Ma, Alexander Raistrick, Lahav Lipson, Jia Deng</author><pubDate>Wed, 13 Dec 2023 18:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08364v1</guid></item><item><title>Distributed Inference and Fine-tuning of Large Language Models Over The Internet</title><link>http://arxiv.org/abs/2312.08361v1</link><description>Large language models (LLMs) are useful in many NLP tasks and become morecapable with size, with the best open-source models having over 50 billionparameters. However, using these 50B+ models requires high-end hardware, makingthem inaccessible to most researchers. In this work, we investigate methods forcost-efficient inference and fine-tuning of LLMs, comparing local anddistributed strategies. We observe that a large enough model (50B+) can runefficiently even on geodistributed devices in a consumer-grade network. Thiscould allow running LLM efficiently by pooling together idle compute resourcesof multiple research groups and volunteers. We address two open problems: (1)how to perform inference and fine-tuning reliably if any device can disconnectabruptly and (2) how to partition LLMs between devices with uneven hardware,joining and leaving at will. In order to do that, we develop specialfault-tolerant inference algorithms and load-balancing protocols thatautomatically assign devices to maximize the total system throughput. Weshowcase these algorithms in Petals - a decentralized system that runs Llama 2(70B) and BLOOM (176B) over the Internet up to 10x faster than offloading forinteractive generation. We evaluate the performance of our system in simulatedconditions and a real-world setup spanning two continents.</description><author>Alexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, Colin Raffel</author><pubDate>Wed, 13 Dec 2023 18:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08361v1</guid></item><item><title>Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF</title><link>http://arxiv.org/abs/2312.08358v1</link><description>In practice, preference learning from human feedback depends on incompletedata with hidden context. Hidden context refers to data that affects thefeedback received, but which is not represented in the data used to train apreference model. This captures common issues of data collection, such ashaving human annotators with varied preferences, cognitive processes thatresult in seemingly irrational behavior, and combining data labeled accordingto different criteria. We prove that standard applications of preferencelearning, including reinforcement learning from human feedback (RLHF),implicitly aggregate over hidden contexts according to a well-known voting rulecalled Borda count. We show this can produce counter-intuitive results that arevery different from other methods which implicitly aggregate via expectedutility. Furthermore, our analysis formalizes the way that preference learningfrom users with diverse values tacitly implements a social choice function. Akey implication of this result is that annotators have an incentive tomisreport their preferences in order to influence the learned model, leading tovulnerabilities in the deployment of RLHF. As a step towards mitigating theseproblems, we introduce a class of methods called distributional preferencelearning (DPL). DPL methods estimate a distribution of possible score valuesfor each alternative in order to better account for hidden context.Experimental results indicate that applying DPL to RLHF for LLM chatbotsidentifies hidden context in the data and significantly reduces subsequentjailbreak vulnerability. Our code and data are available athttps://github.com/cassidylaidlaw/hidden-context</description><author>Anand Siththaranjan, Cassidy Laidlaw, Dylan Hadfield-Menell</author><pubDate>Wed, 13 Dec 2023 18:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08358v1</guid></item><item><title>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</title><link>http://arxiv.org/abs/2305.18290v2</link><description>While large-scale unsupervised language models (LMs) learn broad worldknowledge and some reasoning skills, achieving precise control of theirbehavior is difficult due to the completely unsupervised nature of theirtraining. Existing methods for gaining such steerability collect human labelsof the relative quality of model generations and fine-tune the unsupervised LMto align with these preferences, often with reinforcement learning from humanfeedback (RLHF). However, RLHF is a complex and often unstable procedure, firstfitting a reward model that reflects the human preferences, and thenfine-tuning the large unsupervised LM using reinforcement learning to maximizethis estimated reward without drifting too far from the original model. In thispaper we introduce a new parameterization of the reward model in RLHF thatenables extraction of the corresponding optimal policy in closed form, allowingus to solve the standard RLHF problem with only a simple classification loss.The resulting algorithm, which we call Direct Preference Optimization (DPO), isstable, performant, and computationally lightweight, eliminating the need forsampling from the LM during fine-tuning or performing significanthyperparameter tuning. Our experiments show that DPO can fine-tune LMs to alignwith human preferences as well as or better than existing methods. Notably,fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment ofgenerations, and matches or improves response quality in summarization andsingle-turn dialogue while being substantially simpler to implement and train.</description><author>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn</author><pubDate>Wed, 13 Dec 2023 18:48:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18290v2</guid></item><item><title>Saturn: An Optimized Data System for Large Model Deep Learning Workloads</title><link>http://arxiv.org/abs/2309.01226v2</link><description>Large language models such as GPT-3 &amp; ChatGPT have transformed deep learning(DL), powering applications that have captured the public's imagination. Thesemodels are rapidly being adopted across domains for analytics on variousmodalities, often by finetuning pre-trained base models. Such models needmultiple GPUs due to both their size and computational load, driving thedevelopment of a bevy of "model parallelism" techniques &amp; tools. Navigatingsuch parallelism choices, however, is a new burden for end users of DL such asdata scientists, domain scientists, etc. who may lack the necessary systemsknowhow. The need for model selection, which leads to many models to train dueto hyper-parameter tuning or layer-wise finetuning, compounds the situationwith two more burdens: resource apportioning and scheduling. In this work, wetackle these three burdens for DL users in a unified manner by formalizing themas a joint problem that we call SPASE: Select a Parallelism, Allocateresources, and SchedulE. We propose a new information system architecture totackle the SPASE problem holistically, representing a key step toward enablingwider adoption of large DL models. We devise an extensible template forexisting parallelism schemes and combine it with an automated empiricalprofiler for runtime estimation. We then formulate SPASE as an MILP. We find that direct use of an MILP-solver is significantly more effectivethan several baseline heuristics. We optimize the system runtime further withan introspective scheduling approach. We implement all these techniques into anew data system we call Saturn. Experiments with benchmark DL workloads showthat Saturn achieves 39-49% lower model selection runtimes than typical currentDL practice.</description><author>Kabir Nagrecha, Arun Kumar</author><pubDate>Wed, 13 Dec 2023 18:42:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01226v2</guid></item><item><title>Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes</title><link>http://arxiv.org/abs/2309.01922v2</link><description>In this paper, we consider an infinite horizon average reward Markov DecisionProcess (MDP). Distinguishing itself from existing works within this context,our approach harnesses the power of the general policy gradient-basedalgorithm, liberating it from the constraints of assuming a linear MDPstructure. We propose a policy gradient-based algorithm and show its globalconvergence property. We then prove that the proposed algorithm has$\tilde{\mathcal{O}}({T}^{3/4})$ regret. Remarkably, this paper marks apioneering effort by presenting the first exploration into regret-boundcomputation for the general parameterized policy gradient algorithm in thecontext of average reward scenarios.</description><author>Qinbo Bai, Washim Uddin Mondal, Vaneet Aggarwal</author><pubDate>Wed, 13 Dec 2023 18:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01922v2</guid></item><item><title>FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects</title><link>http://arxiv.org/abs/2312.08344v1</link><description>We present FoundationPose, a unified foundation model for 6D object poseestimation and tracking, supporting both model-based and model-free setups. Ourapproach can be instantly applied at test-time to a novel object withoutfine-tuning, as long as its CAD model is given, or a small number of referenceimages are captured. We bridge the gap between these two setups with a neuralimplicit representation that allows for effective novel view synthesis, keepingthe downstream pose estimation modules invariant under the same unifiedframework. Strong generalizability is achieved via large-scale synthetictraining, aided by a large language model (LLM), a novel transformer-basedarchitecture, and contrastive learning formulation. Extensive evaluation onmultiple public datasets involving challenging scenarios and objects indicateour unified approach outperforms existing methods specialized for each task bya large margin. In addition, it even achieves comparable results toinstance-level methods despite the reduced assumptions. Project page:https://nvlabs.github.io/FoundationPose/</description><author>Bowen Wen, Wei Yang, Jan Kautz, Stan Birchfield</author><pubDate>Wed, 13 Dec 2023 18:28:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08344v1</guid></item><item><title>Ehancing CT Image synthesis from multi-modal MRI data based on a multi-task neural network framework</title><link>http://arxiv.org/abs/2312.08343v1</link><description>Image segmentation, real-value prediction, and cross-modal translation arecritical challenges in medical imaging. In this study, we propose a versatilemulti-task neural network framework, based on an enhanced Transformer U-Netarchitecture, capable of simultaneously, selectively, and adaptively addressingthese medical image tasks. Validation is performed on a public repository ofhuman brain MR and CT images. We decompose the traditional problem ofsynthesizing CT images into distinct subtasks, which include skullsegmentation, Hounsfield unit (HU) value prediction, and image sequentialreconstruction. To enhance the framework's versatility in handling multi-modaldata, we expand the model with multiple image channels. Comparisons betweensynthesized CT images derived from T1-weighted and T2-Flair images wereconducted, evaluating the model's capability to integrate multi-modalinformation from both morphological and pixel value perspectives.</description><author>Zhuoyao Xin, Christopher Wu, Dong Liu, Chunming Gu, Jia Guo, Jun Hua</author><pubDate>Wed, 13 Dec 2023 18:22:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08343v1</guid></item><item><title>Structured Voronoi Sampling</title><link>http://arxiv.org/abs/2306.03061v2</link><description>Gradient-based sampling algorithms have demonstrated their effectiveness intext generation, especially in the context of controlled text generation.However, there exists a lack of theoretically grounded and principledapproaches for this task. In this paper, we take an important step towardbuilding a principled approach for sampling from language models withgradient-based methods. We use discrete distributions given by language modelsto define densities and develop an algorithm based on Hamiltonian Monte Carloto sample from them. We name our gradient-based technique Structured VoronoiSampling (SVS). In an experimental setup where the reference distribution isknown, we show that the empirical distribution of SVS samples is closer to thereference distribution compared to alternative sampling schemes. Furthermore,in a controlled generation task, SVS is able to generate fluent and diversesamples while following the control targets significantly better than othermethods.</description><author>Afra Amini, Li Du, Ryan Cotterell</author><pubDate>Wed, 13 Dec 2023 18:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03061v2</guid></item><item><title>Global Latent Neural Rendering</title><link>http://arxiv.org/abs/2312.08338v1</link><description>A recent trend among generalizable novel view synthesis methods is to learn arendering operator acting over single camera rays. This approach is promisingbecause it removes the need for explicit volumetric rendering, but iteffectively treats target images as collections of independent pixels. Here, wepropose to learn a global rendering operator acting over all camera raysjointly. We show that the right representation to enable such rendering is the5-dimensional plane sweep volume, consisting of the projection of the inputimages on a set of planes facing the target camera. Based on thisunderstanding, we introduce our Convolutional Global Latent Renderer (ConvGLR),an efficient convolutional architecture that performs the rendering operationglobally in a low-resolution latent space. Experiments on various datasetsunder sparse and generalizable setups show that our approach consistentlyoutperforms existing methods by significant margins.</description><author>Thomas Tanay, Matteo Maggioni</author><pubDate>Wed, 13 Dec 2023 18:14:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08338v1</guid></item><item><title>LD-SDM: Language-Driven Hierarchical Species Distribution Modeling</title><link>http://arxiv.org/abs/2312.08334v1</link><description>We focus on the problem of species distribution modeling using global-scalepresence-only data. Most previous studies have mapped the range of a givenspecies using geographical and environmental features alone. To capture astronger implicit relationship between species, we encode the taxonomichierarchy of species using a large language model. This enables range mappingfor any taxonomic rank and unseen species without additional supervision.Further, we propose a novel proximity-aware evaluation metric that enablesevaluating species distribution models using any pixel-level representation ofground-truth species range map. The proposed metric penalizes the predictionsof a model based on its proximity to the ground truth. We describe theeffectiveness of our model by systematically evaluating on the task of speciesrange prediction, zero-shot prediction and geo-feature regression against thestate-of-the-art. Results show our model outperforms the strong baselines whentrained with a variety of multi-label learning losses.</description><author>Srikumar Sastry, Xin Xing, Aayush Dhakal, Subash Khanal, Adeel Ahmad, Nathan Jacobs</author><pubDate>Wed, 13 Dec 2023 18:11:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08334v1</guid></item><item><title>Discretization-Induced Dirichlet Posterior for Robust Uncertainty Quantification on Regression</title><link>http://arxiv.org/abs/2308.09065v2</link><description>Uncertainty quantification is critical for deploying deep neural networks(DNNs) in real-world applications. An Auxiliary Uncertainty Estimator (AuxUE)is one of the most effective means to estimate the uncertainty of the main taskprediction without modifying the main task model. To be considered robust, anAuxUE must be capable of maintaining its performance and triggering higheruncertainties while encountering Out-of-Distribution (OOD) inputs, i.e., toprovide robust aleatoric and epistemic uncertainty. However, for visionregression tasks, current AuxUE designs are mainly adopted for aleatoricuncertainty estimates, and AuxUE robustness has not been explored. In thiswork, we propose a generalized AuxUE scheme for more robust uncertaintyquantification on regression tasks. Concretely, to achieve a more robustaleatoric uncertainty estimation, different distribution assumptions areconsidered for heteroscedastic noise, and Laplace distribution is finallychosen to approximate the prediction error. For epistemic uncertainty, wepropose a novel solution named Discretization-Induced Dirichlet pOsterior(DIDO), which models the Dirichlet posterior on the discretized predictionerror. Extensive experiments on age estimation, monocular depth estimation, andsuper-resolution tasks show that our proposed method can provide robustuncertainty estimates in the face of noisy inputs and that it can be scalableto both image-level and pixel-wise tasks. Code is available athttps://github.com/ENSTA-U2IS/DIDO .</description><author>Xuanlong Yu, Gianni Franchi, Jindong Gu, Emanuel Aldea</author><pubDate>Wed, 13 Dec 2023 18:01:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09065v2</guid></item><item><title>FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts</title><link>http://arxiv.org/abs/2311.05608v2</link><description>Ensuring the safety of artificial intelligence-generated content (AIGC) is alongstanding topic in the artificial intelligence (AI) community, and thesafety concerns associated with Large Language Models (LLMs) have been widelyinvestigated. Recently, large vision-language models (VLMs) represent anunprecedented revolution, as they are built upon LLMs but can incorporateadditional modalities (e.g., images). However, the safety of VLMs lackssystematic evaluation, and there may be an overconfidence in the safetyguarantees provided by their underlying LLMs. In this paper, to demonstratethat introducing additional modality modules leads to unforeseen AI safetyissues, we propose FigStep, a straightforward yet effective jailbreakingalgorithm against VLMs. Instead of feeding textual harmful instructionsdirectly, FigStep converts the harmful content into images through typographyto bypass the safety alignment within the textual module of the VLMs, inducingVLMs to output unsafe responses that violate common AI safety policies. In ourevaluation, we manually review 46,500 model responses generated by 3 familiesof the promising open-source VLMs, i.e., LLaVA, MiniGPT4, and CogVLM (a totalof 6 VLMs). The experimental results show that FigStep can achieve an averageattack success rate of 82.50% on 500 harmful queries in 10 topics. Moreover, wedemonstrate that the methodology of FigStep can even jailbreak GPT-4V, whichalready leverages an OCR detector to filter harmful queries. Above all, ourwork reveals that VLMs are vulnerable to jailbreaking attacks, which highlightsthe necessity of novel safety alignments between visual and textual modalities.</description><author>Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, Xiaoyun Wang</author><pubDate>Wed, 13 Dec 2023 17:54:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05608v2</guid></item><item><title>ADCNet: Learning from Raw Radar Data via Distillation</title><link>http://arxiv.org/abs/2303.11420v3</link><description>As autonomous vehicles and advanced driving assistance systems have enteredwider deployment, there is an increased interest in building robust perceptionsystems using radars. Radar-based systems are lower cost and more robust toadverse weather conditions than their LiDAR-based counterparts; however thepoint clouds produced are typically noisy and sparse by comparison. In order tocombat these challenges, recent research has focused on consuming the raw radardata, instead of the final radar point cloud. We build on this line of work anddemonstrate that by bringing elements of the signal processing pipeline intoour network and then pre-training on the signal processing task, we are able toachieve state of the art detection performance on the RADIal dataset. Ourmethod uses expensive offline signal processing algorithms to pseudo-label dataand trains a network to distill this information into a fast convolutionalbackbone, which can then be finetuned for perception tasks. Extensiveexperiment results corroborate the effectiveness of the proposed techniques.</description><author>Bo Yang, Ishan Khatri, Michael Happold, Chulong Chen</author><pubDate>Wed, 13 Dec 2023 17:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11420v3</guid></item><item><title>PnPNet: Pull-and-Push Networks for Volumetric Segmentation with Boundary Confusion</title><link>http://arxiv.org/abs/2312.08323v1</link><description>Precise boundary segmentation of volumetric images is a critical task forimage-guided diagnosis and computer-assisted intervention, especially forboundary confusion in clinical practice. However, U-shape networks cannoteffectively resolve this challenge due to the lack of boundary shapeconstraints. Besides, existing methods of refining boundaries overemphasize theslender structure, which results in the overfitting phenomenon due to networks'limited abilities to model tiny objects. In this paper, we reconceptualize themechanism of boundary generation by encompassing the interaction dynamics withadjacent regions. Moreover, we propose a unified network termed PnPNet to modelshape characteristics of the confused boundary region. Core ingredients ofPnPNet contain the pushing and pulling branches. Specifically, based ondiffusion theory, we devise the semantic difference module (SDM) from thepushing branch to squeeze the boundary region. Explicit and implicitdifferential information inside SDM significantly boost representationabilities for inter-class boundaries. Additionally, motivated by the K-meansalgorithm, the class clustering module (CCM) from the pulling branch isintroduced to stretch the intersected boundary region. Thus, pushing andpulling branches will shrink and enlarge the boundary uncertainty respectively.They furnish two adversarial forces to promote models to output a more precisedelineation of boundaries. We carry out experiments on three challenging publicdatasets and one in-house dataset, containing three types of boundary confusionin model predictions. Experimental results demonstrate the superiority ofPnPNet over other segmentation networks, especially on evaluation metrics of HDand ASSD. Besides, pushing and pulling branches can serve as plug-and-playmodules to enhance classic U-shape baseline models. Codes are available.</description><author>Xin You, Ming Ding, Minghui Zhang, Hanxiao Zhang, Yi Yu, Jie Yang, Yun Gu</author><pubDate>Wed, 13 Dec 2023 17:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08323v1</guid></item><item><title>Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving</title><link>http://arxiv.org/abs/2304.14365v3</link><description>Robotic perception requires the modeling of both 3D geometry and semantics.Existing methods typically focus on estimating 3D bounding boxes, neglectingfiner geometric details and struggling to handle general, out-of-vocabularyobjects. 3D occupancy prediction, which estimates the detailed occupancy statesand semantics of a scene, is an emerging task to overcome these limitations. Tosupport 3D occupancy prediction, we develop a label generation pipeline thatproduces dense, visibility-aware labels for any given scene. This pipelinecomprises three stages: voxel densification, occlusion reasoning, andimage-guided voxel refinement. We establish two benchmarks, derived from theWaymo Open Dataset and the nuScenes Dataset, namely Occ3D-Waymo andOcc3D-nuScenes benchmarks. Furthermore, we provide an extensive analysis of theproposed dataset with various baseline models. Lastly, we propose a new model,dubbed Coarse-to-Fine Occupancy (CTF-Occ) network, which demonstrates superiorperformance on the Occ3D benchmarks. The code, data, and benchmarks arereleased at https://tsinghua-mars-lab.github.io/Occ3D/.</description><author>Xiaoyu Tian, Tao Jiang, Longfei Yun, Yucheng Mao, Huitong Yang, Yue Wang, Yilun Wang, Hang Zhao</author><pubDate>Wed, 13 Dec 2023 17:41:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14365v3</guid></item><item><title>Prompt Engineering-assisted Malware Dynamic Analysis Using GPT-4</title><link>http://arxiv.org/abs/2312.08317v1</link><description>Dynamic analysis methods effectively identify shelled, wrapped, or obfuscatedmalware, thereby preventing them from invading computers. As a significantrepresentation of dynamic malware behavior, the API (Application ProgrammingInterface) sequence, comprised of consecutive API calls, has progressivelybecome the dominant feature of dynamic analysis methods. Though there have beennumerous deep learning models for malware detection based on API sequences, thequality of API call representations produced by those models is limited. Thesemodels cannot generate representations for unknown API calls, which weakensboth the detection performance and the generalization. Further, the conceptdrift phenomenon of API calls is prominent. To tackle these issues, weintroduce a prompt engineering-assisted malware dynamic analysis using GPT-4.In this method, GPT-4 is employed to create explanatory text for each API callwithin the API sequence. Afterward, the pre-trained language model BERT is usedto obtain the representation of the text, from which we derive therepresentation of the API sequence. Theoretically, this proposed method iscapable of generating representations for all API calls, excluding thenecessity for dataset training during the generation process. Utilizing therepresentation, a CNN-based detection model is designed to extract the feature.We adopt five benchmark datasets to validate the performance of the proposedmodel. The experimental results reveal that the proposed detection algorithmperforms better than the state-of-the-art method (TextCNN). Specifically, incross-database experiments and few-shot learning experiments, the proposedmodel achieves excellent detection performance and almost a 100% recall ratefor malware, verifying its superior generalization performance. The code isavailable at: github.com/yan-scnu/Prompted_Dynamic_Detection.</description><author>Pei Yan, Shunquan Tan, Miaohui Wang, Jiwu Huang</author><pubDate>Wed, 13 Dec 2023 17:39:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08317v1</guid></item><item><title>Motion2Language, unsupervised learning of synchronized semantic motion segmentation</title><link>http://arxiv.org/abs/2310.10594v2</link><description>In this paper, we investigate building a sequence to sequence architecturefor motion to language translation and synchronization. The aim is to translatemotion capture inputs into English natural-language descriptions, such that thedescriptions are generated synchronously with the actions performed, enablingsemantic segmentation as a byproduct, but without requiring synchronizedtraining data. We propose a new recurrent formulation of local attention thatis suited for synchronous/live text generation, as well as an improved motionencoder architecture better suited to smaller data and for synchronousgeneration. We evaluate both contributions in individual experiments, using thestandard BLEU4 metric, as well as a simple semantic equivalence measure, on theKIT motion language dataset. In a follow-up experiment, we assess the qualityof the synchronization of generated text in our proposed approaches throughmultiple evaluation metrics. We find that both contributions to the attentionmechanism and the encoder architecture additively improve the quality ofgenerated text (BLEU and semantic equivalence), but also of synchronization.Our code is available athttps://github.com/rd20karim/M2T-Segmentation/tree/main</description><author>Karim Radouane, Andon Tchechmedjiev, Julien Lagarde, Sylvie Ranwez</author><pubDate>Wed, 13 Dec 2023 17:29:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10594v2</guid></item><item><title>EquiReact: An equivariant neural network for chemical reactions</title><link>http://arxiv.org/abs/2312.08307v1</link><description>Equivariant neural networks have considerably improved the accuracy anddata-efficiency of predictions of molecular properties. Building on thissuccess, we introduce EquiReact, an equivariant neural network to inferproperties of chemical reactions, built from three-dimensional structures ofreactants and products. We illustrate its competitive performance on theprediction of activation barriers on the GDB7-22-TS, Cyclo-23-TS andProparg-21-TS datasets with different regimes according to the inclusion ofatom-mapping information. We show that, compared to state-of-the-art models forreaction property prediction, EquiReact offers: (i) a flexible model withreduced sensitivity between atom-mapping regimes, (ii) better extrapolationcapabilities to unseen chemistries, (iii) impressive prediction errors fordatasets exhibiting subtle variations in three-dimensional geometries ofreactants/products, (iv) reduced sensitivity to geometry quality and (iv)excellent data efficiency.</description><author>Puck van Gerwen, Ksenia R. Briling, Charlotte Bunne, Vignesh Ram Somnath, Ruben Laplaza, Andreas Krause, Clemence Corminboeuf</author><pubDate>Wed, 13 Dec 2023 17:26:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08307v1</guid></item><item><title>PG-Video-LLaVA: Pixel Grounding Large Video-Language Models</title><link>http://arxiv.org/abs/2311.13435v2</link><description>Extending image-based Large Multimodal Models (LMMs) to videos is challengingdue to the inherent complexity of video data. The recent approaches extendingimage-based LMMs to videos either lack the grounding capabilities (e.g.,VideoChat, Video-ChatGPT, Video-LLaMA) or do not utilize the audio-signals forbetter video understanding (e.g., Video-ChatGPT). Addressing these gaps, wepropose PG-Video-LLaVA, the first LMM with pixel-level grounding capability,integrating audio cues by transcribing them into text to enrich video-contextunderstanding. Our framework uses an off-the-shelf tracker and a novelgrounding module, enabling it to spatially localize objects in videos followinguser instructions. We evaluate PG-Video-LLaVA using video-based generative andquestion-answering benchmarks and introduce new benchmarks specificallydesigned to measure prompt-based object grounding performance in videos.Further, we propose the use of Vicuna over GPT-3.5, as utilized inVideo-ChatGPT, for video-based conversation benchmarking, ensuringreproducibility of results which is a concern with the proprietary nature ofGPT-3.5. Our framework builds on SoTA image-based LLaVA model and extends itsadvantages to the video domain, delivering promising gains on video-basedconversation and grounding tasks. Project Page:https://github.com/mbzuai-oryx/Video-LLaVA</description><author>Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Mubarak Shah, Fahad Khan</author><pubDate>Wed, 13 Dec 2023 17:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13435v2</guid></item><item><title>Conformal Prediction Regions for Time Series using Linear Complementarity Programming</title><link>http://arxiv.org/abs/2304.01075v4</link><description>Conformal prediction is a statistical tool for producing prediction regionsof machine learning models that are valid with high probability. However,applying conformal prediction to time series data leads to conservativeprediction regions. In fact, to obtain prediction regions over $T$ time stepswith confidence $1-\delta$, {previous works require that each individualprediction region is valid} with confidence $1-\delta/T$. We propose anoptimization-based method for reducing this conservatism to enable long horizonplanning and verification when using learning-enabled time series predictors.Instead of considering prediction errors individually at each time step, weconsider a parameterized prediction error over multiple time steps. Byoptimizing the parameters over an additional dataset, we find predictionregions that are not conservative. We show that this problem can be cast as amixed integer linear complementarity program (MILCP), which we then relax intoa linear complementarity program (LCP). Additionally, we prove that the relaxedLP has the same optimal cost as the original MILCP. Finally, we demonstrate theefficacy of our method on case studies using pedestrian trajectory predictorsand F16 fighter jet altitude predictors.</description><author>Matthew Cleaveland, Insup Lee, George J. Pappas, Lars Lindemann</author><pubDate>Wed, 13 Dec 2023 17:23:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01075v4</guid></item><item><title>Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models</title><link>http://arxiv.org/abs/2312.08303v1</link><description>Toxic content detection is crucial for online services to removeinappropriate content that violates community standards. To automate thedetection process, prior works have proposed varieties of machine learning (ML)approaches to train Language Models (LMs) for toxic content detection. However,both their accuracy and transferability across datasets are limited. Recently,Large Language Models (LLMs) have shown promise in toxic content detection dueto their superior zero-shot and few-shot in-context learning ability as well asbroad transferability on ML tasks. However, efficiently designing prompts forLLMs remains challenging. Moreover, the high run-time cost of LLMs may hindertheir deployments in production. To address these challenges, in this work, wepropose BD-LLM, a novel and efficient approach to Bootstrapping and DistillingLLMs for toxic content detection. Specifically, we design a novel promptingmethod named Decision-Tree-of-Thought (DToT) to bootstrap LLMs' detectionperformance and extract high-quality rationales. DToT can automatically selectmore fine-grained context to re-prompt LLMs when their responses lackconfidence. Additionally, we use the rationales extracted via DToT to fine-tunestudent LMs. Our experimental results on various datasets demonstrate that DToTcan improve the accuracy of LLMs by up to 4.6%. Furthermore, student LMsfine-tuned with rationales extracted via DToT outperform baselines on alldatasets with up to 16.9\% accuracy improvement, while being more than 60xsmaller than conventional LLMs. Finally, we observe that student LMs fine-tunedwith rationales exhibit better cross-dataset transferability.</description><author>Jiang Zhang, Qiong Wu, Yiming Xu, Cheng Cao, Zheng Du, Konstantinos Psounis</author><pubDate>Wed, 13 Dec 2023 17:22:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08303v1</guid></item><item><title>Conceptualizing Suicidal Behavior: Utilizing Explanations of Predicted Outcomes to Analyze Longitudinal Social Media Data</title><link>http://arxiv.org/abs/2312.08299v1</link><description>The COVID-19 pandemic has escalated mental health crises worldwide, withsocial isolation and economic instability contributing to a rise in suicidalbehavior. Suicide can result from social factors such as shame, abuse,abandonment, and mental health conditions like depression, Post-TraumaticStress Disorder (PTSD), Attention-Deficit/Hyperactivity Disorder (ADHD),anxiety disorders, and bipolar disorders. As these conditions develop, signs ofsuicidal ideation may manifest in social media interactions. Analyzing socialmedia data using artificial intelligence (AI) techniques can help identifypatterns of suicidal behavior, providing invaluable insights for suicideprevention agencies, professionals, and broader community awarenessinitiatives. Machine learning algorithms for this purpose require large volumesof accurately labeled data. Previous research has not fully explored thepotential of incorporating explanations in analyzing and labeling longitudinalsocial media data. In this study, we employed a model explanation method, LayerIntegrated Gradients, on top of a fine-tuned state-of-the-art language model,to assign each token from Reddit users' posts an attribution score forpredicting suicidal ideation. By extracting and analyzing attributions oftokens from the data, we propose a methodology for preliminary screening ofsocial media posts for suicidal ideation without using large language modelsduring inference.</description><author>Van Minh Nguyen, Nasheen Nur, William Stern, Thomas Mercer, Chiradeep Sen, Siddhartha Bhattacharyya, Victor Tumbiolo, Seng Jhing Goh</author><pubDate>Wed, 13 Dec 2023 17:15:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08299v1</guid></item><item><title>Venn: Resource Management Across Federated Learning Jobs</title><link>http://arxiv.org/abs/2312.08298v1</link><description>In recent years, federated learning (FL) has emerged as a promising approachfor machine learning (ML) and data science across distributed edge devices.With the increasing popularity of FL, resource contention between multiple FLjobs training on the same device population is increasing as well. Schedulingedge resources among multiple FL jobs is different from GPU scheduling forcloud ML because of the ephemeral nature and planetary scale of participatingdevices as well as the overlapping resource requirements of diverse FL jobs.Existing resource managers for FL jobs opt for random assignment of devices toFL jobs for simplicity and scalability, which leads to poor performance. Inthis paper, we present Venn, an FL resource manager, that efficiently schedulesephemeral, heterogeneous devices among many FL jobs, with the goal of reducingtheir average job completion time (JCT). Venn formulates the IntersectionResource Scheduling (IRS) problem to identify complex resource contention amongmultiple FL jobs. Then, Venn proposes a contention-aware scheduling heuristicto minimize the average scheduling delay. Furthermore, it proposes aresource-aware device-to-job matching heuristic that focuses on optimizingresponse collection time by mitigating stragglers. Our evaluation shows that,compared to the state-of-the-art FL resource managers, Venn improves theaverage JCT by up to 1.88X.</description><author>Jiachen Liu, Fan Lai, Ding Ding, Yiwen Zhang, Mosharaf Chowdhury</author><pubDate>Wed, 13 Dec 2023 17:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08298v1</guid></item><item><title>Inferring Atmospheric Properties of Exoplanets with Flow Matching and Neural Importance Sampling</title><link>http://arxiv.org/abs/2312.08295v1</link><description>Atmospheric retrievals (AR) characterize exoplanets by estimating atmosphericparameters from observed light spectra, typically by framing the task as aBayesian inference problem. However, traditional approaches such as nestedsampling are computationally expensive, thus sparking an interest in solutionsbased on machine learning (ML). In this ongoing work, we first explore flowmatching posterior estimation (FMPE) as a new ML-based method for AR and findthat, in our case, it is more accurate than neural posterior estimation (NPE),but less accurate than nested sampling. We then combine both FMPE and NPE withimportance sampling, in which case both methods outperform nested sampling interms of accuracy and simulation efficiency. Going forward, our analysissuggests that simulation-based inference with likelihood-based importancesampling provides a framework for accurate and efficient AR that may become avaluable tool not only for the analysis of observational data from existingtelescopes, but also for the development of new missions and instruments.</description><author>Timothy D. Gebhard, Jonas Wildberger, Maximilian Dax, Daniel Angerhausen, Sascha P. Quanz, Bernhard Schölkopf</author><pubDate>Wed, 13 Dec 2023 17:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08295v1</guid></item><item><title>VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space</title><link>http://arxiv.org/abs/2312.08291v1</link><description>Human Pose and Shape Estimation (HPSE) from RGB images can be broadlycategorized into two main groups: parametric and non-parametric approaches.Parametric techniques leverage a low-dimensional statistical body model forrealistic results, whereas recent non-parametric methods achieve higherprecision by directly regressing the 3D coordinates of the human body. Despitetheir strengths, both approaches face limitations: the parameters ofstatistical body models pose challenges as regression targets, and predicting3D coordinates introduces computational complexities and issues related tosmoothness. In this work, we take a novel approach to address the HPSE problem.We introduce a unique method involving a low-dimensional discrete latentrepresentation of the human mesh, framing HPSE as a classification task.Instead of predicting body model parameters or 3D vertex coordinates, our focusis on forecasting the proposed discrete latent representation, which can bedecoded into a registered human mesh. This innovative paradigm offers two keyadvantages: firstly, predicting a low-dimensional discrete representationconfines our predictions to the space of anthropomorphic poses and shapes;secondly, by framing the problem as a classification task, we can harness thediscriminative power inherent in neural networks. Our proposed model, VQ-HPS, atransformer-based architecture, forecasts the discrete latent representation ofthe mesh, trained through minimizing a cross-entropy loss. Our resultsdemonstrate that VQ-HPS outperforms the current state-of-the-art non-parametricapproaches while yielding results as realistic as those produced by parametricmethods. This highlights the significant potential of the classificationapproach for HPSE.</description><author>Guénolé Fiche, Simon Leglaive, Xavier Alameda-Pineda, Antonio Agudo, Francesc Moreno-Noguer</author><pubDate>Wed, 13 Dec 2023 17:08:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08291v1</guid></item><item><title>PhenDiff: Revealing Invisible Phenotypes with Conditional Diffusion Models</title><link>http://arxiv.org/abs/2312.08290v1</link><description>Over the last five years, deep generative models have gradually been adoptedfor various tasks in biological research. Notably, image-to-image translationmethods showed to be effective in revealing subtle phenotypic cell variationsotherwise invisible to the human eye. Current methods to achieve this goalmainly rely on Generative Adversarial Networks (GANs). However, these modelsare known to suffer from some shortcomings such as training instability andmode collapse. Furthermore, the lack of robustness to invert a real image intothe latent of a trained GAN prevents flexible editing of real images. In thiswork, we propose PhenDiff, an image-to-image translation method based onconditional diffusion models to identify subtle phenotypes in microscopyimages. We evaluate this approach on biological datasets against previous worksuch as CycleGAN. We show that PhenDiff outperforms this baseline in terms ofquality and diversity of the generated images. We then apply this method todisplay invisible phenotypic changes triggered by a rare neurodevelopmentaldisorder on microscopy images of organoids. Altogether, we demonstrate thatPhenDiff is able to perform high quality biological image-to-image translationallowing to spot subtle phenotype variations on a real image.</description><author>Anis Bourou, Thomas Boyer, Kévin Daupin, Véronique Dubreuil, Aurélie De Thonel, Valérie Mezger, Auguste Genovesio</author><pubDate>Wed, 13 Dec 2023 17:06:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08290v1</guid></item><item><title>Hybrid Sample Synthesis-based Debiasing of Classifier in Limited Data Setting</title><link>http://arxiv.org/abs/2312.08288v1</link><description>Deep learning models are known to suffer from the problem of bias, andresearchers have been exploring methods to address this issue. However, most ofthese methods require prior knowledge of the bias and are not always practical.In this paper, we focus on a more practical setting with no prior informationabout the bias. Generally, in this setting, there are a large number ofbias-aligned samples that cause the model to produce biased predictions and afew bias-conflicting samples that do not conform to the bias. If the trainingdata is limited, the influence of the bias-aligned samples may become evenstronger on the model predictions, and we experimentally demonstrate thatexisting debiasing techniques suffer severely in such cases. In this paper, weexamine the effects of unknown bias in small dataset regimes and present anovel approach to mitigate this issue. The proposed approach directly addressesthe issue of the extremely low occurrence of bias-conflicting samples inlimited data settings through the synthesis of hybrid samples that can be usedto reduce the effect of bias. We perform extensive experiments on severalbenchmark datasets and experimentally demonstrate the effectiveness of ourproposed approach in addressing any unknown bias in the presence of limiteddata. Specifically, our approach outperforms the vanilla, LfF, LDD, and DebiANdebiasing methods by absolute margins of 10.39%, 9.08%, 8.07%, and 9.67% whenonly 10% of the Corrupted CIFAR-10 Type 1 dataset is available with abias-conflicting sample ratio of 0.05.</description><author>Piyush Arora, Pratik Mazumder</author><pubDate>Wed, 13 Dec 2023 17:04:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08288v1</guid></item><item><title>On the verification of Embeddings using Hybrid Markov Logic</title><link>http://arxiv.org/abs/2312.08287v1</link><description>The standard approach to verify representations learned by Deep NeuralNetworks is to use them in specific tasks such as classification or regression,and measure their performance based on accuracy in such tasks. However, in manycases, we would want to verify more complex properties of a learnedrepresentation. To do this, we propose a framework based on a probabilisticfirst-order language, namely, Hybrid Markov Logic Networks (HMLNs) where wespecify properties over embeddings mixed with symbolic domain knowledge. Wepresent an approach to learn parameters for the properties within thisframework. Further, we develop a verification method to test embeddings in thisframework by encoding this task as a Mixed Integer Linear Program for which wecan leverage existing state-of-the-art solvers. We illustrate verification inGraph Neural Networks, Deep Knowledge Tracing and Intelligent Tutoring Systemsto demonstrate the generality of our approach.</description><author>Anup Shakya, Abisha Thapa Magar, Somdeb Sarkhel, Deepak Venugopal</author><pubDate>Wed, 13 Dec 2023 17:04:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08287v1</guid></item><item><title>Prompting LLMs with content plans to enhance the summarization of scientific articles</title><link>http://arxiv.org/abs/2312.08282v1</link><description>This paper presents novel prompting techniques to improve the performance ofautomatic summarization systems for scientific articles. Scientific articlesummarization is highly challenging due to the length and complexity of thesedocuments. We conceive, implement, and evaluate prompting techniques thatprovide additional contextual information to guide summarization systems.Specifically, we feed summarizers with lists of key terms extracted fromarticles, such as author keywords or automatically generated keywords. Ourtechniques are tested with various summarization models and input texts.Results show performance gains, especially for smaller models summarizingsections separately. This evidences that prompting is a promising approach toovercoming the limitations of less powerful systems. Our findings introduce anew research direction of using prompts to aid smaller models.</description><author>Aldan Creo, Manuel Lama, Juan C. Vidal</author><pubDate>Wed, 13 Dec 2023 16:57:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08282v1</guid></item><item><title>GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values</title><link>http://arxiv.org/abs/2311.03426v2</link><description>Massive transformer-based models face several challenges, including slow andcomputationally intensive pre-training and over-parametrization. This paperaddresses these challenges by proposing a versatile method called GQKVA, whichgeneralizes query, key, and value grouping techniques. GQKVA is designed tospeed up transformer pre-training while reducing the model size. Ourexperiments with various GQKVA variants highlight a clear trade-off betweenperformance and model size, allowing for customized choices based on resourceand time limitations. Our findings also indicate that the conventionalmulti-head attention approach is not always the best choice, as there arelighter and faster alternatives available. We tested our method on ViT, whichachieved an approximate 0.3% increase in accuracy while reducing the model sizeby about 4% in the task of image classification. Additionally, our mostaggressive model reduction experiment resulted in a reduction of approximately15% in model size, with only around a 1% drop in accuracy.</description><author>Farnoosh Javadi, Walid Ahmed, Habib Hajimolahoseini, Foozhan Ataiefard, Mohammad Hassanpour, Saina Asani, Austin Wen, Omar Mohamed Awad, Kangling Liu, Yang Liu</author><pubDate>Wed, 13 Dec 2023 16:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03426v2</guid></item><item><title>High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models</title><link>http://arxiv.org/abs/2312.08274v1</link><description>Objective: To develop a high-throughput biomedical relation extraction systemthat takes advantage of the large language models' (LLMs) reading comprehensionability and biomedical world knowledge in a scalable and evidential manner.Methods: We formulate the relation extraction task as a simple binaryclassification problem for large language models such as ChatGPT. Specifically,LLMs make the decision based on the external corpus and its world knowledge,giving the reason for the judgment to factual verification. This method istailored for semi-structured web articles, wherein we designate the main titleas the tail entity and explicitly incorporate it into the context, and thepotential head entities are matched based on a biomedical thesaurus. Moreover,lengthy contents are sliced into text chunks, embedded, and retrieved withadditional embedding models, ensuring compatibility with the context windowsize constraints of available open-source LLMs. Results: Using an open-sourceLLM, we extracted 304315 relation triplets of three distinct relation typesfrom four reputable biomedical websites. To assess the efficacy of the basicpipeline employed for biomedical relation extraction, we curated a benchmarkdataset annotated by a medical expert. Evaluation results indicate that thepipeline exhibits performance comparable to that of GPT-4. Case studies furtherilluminate challenges faced by contemporary LLMs in the context of biomedicalrelation extraction for semi-structured web articles. Conclusion: The proposedmethod has demonstrated its effectiveness in leveraging the strengths of LLMsfor high-throughput biomedical relation extraction. Its adaptability isevident, as it can be seamlessly extended to diverse semi-structured biomedicalwebsites, facilitating the extraction of various types of biomedical relationswith ease.</description><author>Songchi Zhou, Sheng Yu</author><pubDate>Wed, 13 Dec 2023 16:43:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08274v1</guid></item><item><title>Mixed moving average field guided learning for spatio-temporal data</title><link>http://arxiv.org/abs/2301.00736v3</link><description>Influenced mixed moving average fields are a versatile modeling class forspatio-temporal data. However, their predictive distribution is not generallyknown. Under this modeling assumption, we define a novel spatio-temporalembedding and a theory-guided machine learning approach that employs ageneralized Bayesian algorithm to make ensemble forecasts. We employ Lipschitzpredictors and determine fixed-time and any-time PAC Bayesian bounds in thebatch learning setting. Performing causal forecast is a highlight of ourmethodology as its potential application to data with spatial and temporalshort and long-range dependence. We then test the performance of our learningmethodology by using linear predictors and data sets simulated from aspatio-temporal Ornstein-Uhlenbeck process.</description><author>Imma Valentina Curato, Orkun Furat, Lorenzo Proietti, Bennet Stroeh</author><pubDate>Wed, 13 Dec 2023 16:33:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00736v3</guid></item><item><title>Fit Like You Sample: Sample-Efficient Generalized Score Matching from Fast Mixing Diffusions</title><link>http://arxiv.org/abs/2306.09332v3</link><description>Score matching is an approach to learning probability distributionsparametrized up to a constant of proportionality (e.g. Energy-Based Models).The idea is to fit the score of the distribution, rather than the likelihood,thus avoiding the need to evaluate the constant of proportionality. Whilethere's a clear algorithmic benefit, the statistical "cost'' can be steep:recent work by Koehler et al. 2022 showed that for distributions that have poorisoperimetric properties (a large Poincar\'e or log-Sobolev constant), scorematching is substantially statistically less efficient than maximum likelihood.However, many natural realistic distributions, e.g. multimodal distributions assimple as a mixture of two Gaussians in one dimension -- have a poor Poincar\'econstant. In this paper, we show a close connection between the mixing time of a broadclass of Markov processes with generator $\mathcal{L}$ and an appropriatelychosen generalized score matching loss that tries to fit $\frac{\mathcal{O}p}{p}$. This allows us to adapt techniques to speed up Markov chains toconstruct better score-matching losses. In particular, ``preconditioning'' thediffusion can be translated to an appropriate ``preconditioning'' of the scoreloss. Lifting the chain by adding a temperature like in simulated tempering canbe shown to result in a Gaussian-convolution annealed score matching loss,similar to Song and Ermon, 2019. Moreover, we show that if the distributionbeing learned is a finite mixture of Gaussians in $d$ dimensions with a sharedcovariance, the sample complexity of annealed score matching is polynomial inthe ambient dimension, the diameter of the means, and the smallest and largesteigenvalues of the covariance -- obviating the Poincar\'e constant-based lowerbounds of the basic score matching loss shown in Koehler et al. 2022.</description><author>Yilong Qin, Andrej Risteski</author><pubDate>Wed, 13 Dec 2023 16:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09332v3</guid></item><item><title>Efficient Multi-Object Pose Estimation using Multi-Resolution Deformable Attention and Query Aggregation</title><link>http://arxiv.org/abs/2312.08268v1</link><description>Object pose estimation is a long-standing problem in computer vision.Recently, attention-based vision transformer models have achievedstate-of-the-art results in many computer vision applications. Exploiting thepermutation-invariant nature of the attention mechanism, a family of visiontransformer models formulate multi-object pose estimation as a set predictionproblem. However, existing vision transformer models for multi-object poseestimation rely exclusively on the attention mechanism. Convolutional neuralnetworks, on the other hand, hard-wire various inductive biases into theirarchitecture. In this paper, we investigate incorporating inductive biases invision transformer models for multi-object pose estimation, which facilitateslearning long-range dependencies while circumventing the costly globalattention. In particular, we use multi-resolution deformable attention, wherethe attention operation is performed only between a few deformed referencepoints. Furthermore, we propose a query aggregation mechanism that enablesincreasing the number of object queries without increasing the computationalcomplexity. We evaluate the proposed model on the challenging YCB-Video datasetand report state-of-the-art results.</description><author>Arul Selvam Periyasamy, Vladimir Tsaturyan, Sven Behnke</author><pubDate>Wed, 13 Dec 2023 16:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08268v1</guid></item><item><title>TABSurfer: a Hybrid Deep Learning Architecture for Subcortical Segmentation</title><link>http://arxiv.org/abs/2312.08267v1</link><description>Subcortical segmentation remains challenging despite its importantapplications in quantitative structural analysis of brain MRI scans. The mostaccurate method, manual segmentation, is highly labor intensive, so automatedtools like FreeSurfer have been adopted to handle this task. However, thesetraditional pipelines are slow and inefficient for processing large datasets.In this study, we propose TABSurfer, a novel 3D patch-based CNN-Transformerhybrid deep learning model designed for superior subcortical segmentationcompared to existing state-of-the-art tools. To evaluate, we first demonstrateTABSurfer's consistent performance across various T1w MRI datasets withsignificantly shorter processing times compared to FreeSurfer. Then, wevalidate against manual segmentations, where TABSurfer outperforms FreeSurferbased on the manual ground truth. In each test, we also establish TABSurfer'sadvantage over a leading deep learning benchmark, FastSurferVINN. Together,these studies highlight TABSurfer's utility as a powerful tool for fullyautomated subcortical segmentation with high fidelity.</description><author>Aaron Cao, Vishwanatha M. Rao, Kejia Liu, Xinru Liu, Andrew F. Laine, Jia Guo</author><pubDate>Wed, 13 Dec 2023 16:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08267v1</guid></item><item><title>Rethinking Label Smoothing on Multi-hop Question Answering</title><link>http://arxiv.org/abs/2212.09512v3</link><description>Multi-Hop Question Answering (MHQA) is a significant area in questionanswering, requiring multiple reasoning components, including documentretrieval, supporting sentence prediction, and answer span extraction. In thiswork, we analyze the primary factors limiting the performance of multi-hopreasoning and introduce label smoothing into the MHQA task. This is aimed atenhancing the generalization capabilities of MHQA systems and mitigatingoverfitting of answer spans and reasoning paths in training set. We propose anovel label smoothing technique, F1 Smoothing, which incorporates uncertaintyinto the learning process and is specifically tailored for Machine ReadingComprehension (MRC) tasks. Inspired by the principles of curriculum learning,we introduce the Linear Decay Label Smoothing Algorithm (LDLA), whichprogressively reduces uncertainty throughout the training process. Experimenton the HotpotQA dataset demonstrates the effectiveness of our methods inenhancing performance and generalizability in multi-hop reasoning, achievingnew state-of-the-art results on the leaderboard.</description><author>Zhangyue Yin, Yuxin Wang, Xiannian Hu, Yiguang Wu, Hang Yan, Xinyu Zhang, Zhao Cao, Xuanjing Huang, Xipeng Qiu</author><pubDate>Wed, 13 Dec 2023 16:27:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09512v3</guid></item><item><title>Crystal-GFN: sampling crystals with desirable properties and constraints</title><link>http://arxiv.org/abs/2310.04925v2</link><description>Accelerating material discovery holds the potential to greatly help mitigatethe climate crisis. Discovering new solid-state materials such aselectrocatalysts, super-ionic conductors or photovoltaic materials can have acrucial impact, for instance, in improving the efficiency of renewable energyproduction and storage. In this paper, we introduce Crystal-GFN, a generativemodel of crystal structures that sequentially samples structural properties ofcrystalline materials, namely the space group, composition and latticeparameters. This domain-inspired approach enables the flexible incorporation ofphysical and structural hard constraints, as well as the use of any availablepredictive model of a desired physicochemical property as an objectivefunction. To design stable materials, one must target the candidates with thelowest formation energy. Here, we use as objective the formation energy peratom of a crystal structure predicted by a new proxy machine learning modeltrained on MatBench. The results demonstrate that Crystal-GFN is able to samplehighly diverse crystals with low (median -3.1 eV/atom) predicted formationenergy.</description><author>Mila AI4Science, Alex Hernandez-Garcia, Alexandre Duval, Alexandra Volokhova, Yoshua Bengio, Divya Sharma, Pierre Luc Carrier, Yasmine Benabed, Michał Koziarski, Victor Schmidt</author><pubDate>Wed, 13 Dec 2023 16:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04925v2</guid></item><item><title>Towards Optimal Sobolev Norm Rates for the Vector-Valued Regularized Least-Squares Algorithm</title><link>http://arxiv.org/abs/2312.07186v2</link><description>We present the first optimal rates for infinite-dimensional vector-valuedridge regression on a continuous scale of norms that interpolate between $L_2$and the hypothesis space, which we consider as a vector-valued reproducingkernel Hilbert space. These rates allow to treat the misspecified case in whichthe true regression function is not contained in the hypothesis space. Wecombine standard assumptions on the capacity of the hypothesis space with anovel tensor product construction of vector-valued interpolation spaces inorder to characterize the smoothness of the regression function. Our upperbound not only attains the same rate as real-valued kernel ridge regression,but also removes the assumption that the target regression function is bounded.For the lower bound, we reduce the problem to the scalar setting using aprojection argument. We show that these rates are optimal in most cases andindependent of the dimension of the output space. We illustrate our results forthe special case of vector-valued Sobolev spaces.</description><author>Zhu Li, Dimitri Meunier, Mattes Mollenhauer, Arthur Gretton</author><pubDate>Wed, 13 Dec 2023 16:23:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07186v2</guid></item><item><title>HappyFeat -- An interactive and efficient BCI framework for clinical applications</title><link>http://arxiv.org/abs/2310.02948v2</link><description>Brain-Computer Interface (BCI) systems allow users to perform actions bytranslating their brain activity into commands. Such systems usually need atraining phase, consisting in training a classification algorithm todiscriminate between mental states using specific features from the recordedsignals. This phase of feature selection and training is crucial for BCIperformance and presents specific constraints to be met in a clinical context,such as post-stroke rehabilitation. In this paper, we present HappyFeat, a software making Motor Imagery (MI)based BCI experiments easier, by gathering all necessary manipulations andanalysis in a single convenient GUI and via automation of experiment oranalysis parameters. The resulting workflow allows for effortlessly selectingthe best features, helping to achieve good BCI performance in time-constrainedenvironments. Alternative features based on Functional Connectivity can be usedand compared or combined with Power Spectral Density, allowing anetwork-oriented approach. We then give details of HappyFeat's main mechanisms, and a review of itsperformances in typical use cases. We also show that it can be used as anefficient tool for comparing different metrics extracted from the signals, totrain the classification algorithm. To this end, we show a comparison betweenthe commonly-used Power Spectral Density and network metrics based onFunctional Connectivity. HappyFeat is available as an open-source project which can be freelydownloaded on GitHub.</description><author>Arthur Desbois, Tristan Venot, Fabrizio De Vico Fallani, Marie-Constance Corsi</author><pubDate>Wed, 13 Dec 2023 16:21:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02948v2</guid></item><item><title>\emph{Lifted} RDT based capacity analysis of the 1-hidden layer treelike \emph{sign} perceptrons neural networks</title><link>http://arxiv.org/abs/2312.08257v1</link><description>We consider the memorization capabilities of multilayered \emph{sign}perceptrons neural networks (SPNNs). A recent rigorous upper-bounding capacitycharacterization, obtained in \cite{Stojnictcmspnncaprdt23} utilizing theRandom Duality Theory (RDT), demonstrated that adding neurons in a networkconfiguration may indeed be very beneficial. Moreover, for particular\emph{treelike committee machines} (TCM) architectures with $d\leq 5$ neuronsin the hidden layer, \cite{Stojnictcmspnncaprdt23} made a very firstmathematically rigorous progress in over 30 years by lowering the previouslybest known capacity bounds of \cite{MitchDurb89}. Here, we first establish thatthe RDT bounds from \cite{Stojnictcmspnncaprdt23} scale as $\sim \sqrt{d}$ andcan not on their own \emph{universally} (over the entire range of $d$) beat thebest known $\sim \log(d)$ scaling of the bounds from \cite{MitchDurb89}. Afterrecognizing that the progress from \cite{Stojnictcmspnncaprdt23} is thereforepromising, but yet without a complete concretization, we then proceed byconsidering the recently developed fully lifted RDT (fl RDT) as an alternative.While the fl RDT is indeed a powerful juggernaut, it typically relies on heavynumerical evaluations. To avoid such heavy numerics, we here focus on asimplified, \emph{partially lifted}, variant and show that it allows for veryneat, closed form, analytical capacity characterizations. Moreover, we obtainthe concrete capacity bounds that \emph{universally} improve for \emph{any} $d$over the best known ones of \cite{MitchDurb89}.</description><author>Mihailo Stojnic</author><pubDate>Wed, 13 Dec 2023 16:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08257v1</guid></item><item><title>A Compact and Semantic Latent Space for Disentangled and Controllable Image Editing</title><link>http://arxiv.org/abs/2312.08256v1</link><description>Recent advances in the field of generative models and in particulargenerative adversarial networks (GANs) have lead to substantial progress forcontrolled image editing, especially compared with the pre-deep learning era.Despite their powerful ability to apply realistic modifications to an image,these methods often lack properties like disentanglement (the capacity to editattributes independently). In this paper, we propose an auto-encoder whichre-organizes the latent space of StyleGAN, so that each attribute which we wishto edit corresponds to an axis of the new latent space, and furthermore thatthe latent axes are decorrelated, encouraging disentanglement. We work in acompressed version of the latent space, using Principal Component Analysis,meaning that the parameter complexity of our autoencoder is reduced, leading toshort training times ($\sim$ 45 mins). Qualitative and quantitative resultsdemonstrate the editing capabilities of our approach, with greaterdisentanglement than competing methods, while maintaining fidelity to theoriginal image with respect to identity. Our autoencoder architecture simpleand straightforward, facilitating implementation.</description><author>Gwilherm Lesné, Yann Gousseau, Saïd Ladjal, Alasdair Newson</author><pubDate>Wed, 13 Dec 2023 16:18:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08256v1</guid></item><item><title>OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods</title><link>http://arxiv.org/abs/2312.08255v1</link><description>Optical coherence tomography (OCT) is a non-invasive imaging technique withextensive clinical applications in ophthalmology. OCT enables the visualizationof the retinal layers, playing a vital role in the early detection andmonitoring of retinal diseases. OCT uses the principle of light waveinterference to create detailed images of the retinal microstructures, makingit a valuable tool for diagnosing ocular conditions. This work presents anopen-access OCT dataset (OCTDL) comprising over 1600 high-resolution OCT imageslabeled according to disease group and retinal pathology. The dataset consistsof OCT records of patients with Age-related Macular Degeneration (AMD),Diabetic Macular Edema (DME), Epiretinal Membrane (ERM), Retinal ArteryOcclusion (RAO), Retinal Vein Occlusion (RVO), and Vitreomacular InterfaceDisease (VID). The images were acquired with an Optovue Avanti RTVue XR usingraster scanning protocols with dynamic scan length and image resolution. Eachretinal b-scan was acquired by centering on the fovea and interpreted andcataloged by an experienced retinal specialist. In this work, we applied DeepLearning classification techniques to this new open-access dataset.</description><author>Mikhail Kulyabin, Aleksei Zhdanov, Anastasia Nikiforova, Andrey Stepichev, Anna Kuznetsova, Mikhail Ronkin, Vasilii Borisov, Alexander Bogachev, Sergey Korotkich, Paul A Constable, Andreas Maier</author><pubDate>Wed, 13 Dec 2023 16:18:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08255v1</guid></item><item><title>A Survey of Generative AI for Intelligent Transportation Systems</title><link>http://arxiv.org/abs/2312.08248v1</link><description>Intelligent transportation systems play a crucial role in modern trafficmanagement and optimization, greatly improving traffic efficiency and safety.With the rapid development of generative artificial intelligence (GenerativeAI) technologies in the fields of image generation and natural languageprocessing, generative AI has also played a crucial role in addressing keyissues in intelligent transportation systems, such as data sparsity, difficultyin observing abnormal scenarios, and in modeling data uncertainty. In thisreview, we systematically investigate the relevant literature on generative AItechniques in addressing key issues in different types of tasks in intelligenttransportation systems. First, we introduce the principles of differentgenerative AI techniques, and their potential applications. Then, we classifytasks in intelligent transportation systems into four types: trafficperception, traffic prediction, traffic simulation, and trafficdecision-making. We systematically illustrate how generative AI techniquesaddresses key issues in these four different types of tasks. Finally, wesummarize the challenges faced in applying generative AI to intelligenttransportation systems, and discuss future research directions based ondifferent application scenarios.</description><author>Huan Yan, Yong Li</author><pubDate>Wed, 13 Dec 2023 16:13:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08248v1</guid></item><item><title>Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening</title><link>http://arxiv.org/abs/2308.07707v2</link><description>Machine unlearning, the ability for a machine learning model to forget, isbecoming increasingly important to comply with data privacy regulations, aswell as to remove harmful, manipulated, or outdated information. The keychallenge lies in forgetting specific information while protecting modelperformance on the remaining data. While current state-of-the-art methodsperform well, they typically require some level of retraining over the retaineddata, in order to protect or restore model performance. This adds computationaloverhead and mandates that the training data remain available and accessible,which may not be feasible. In contrast, other methods employ a retrain-freeparadigm, however, these approaches are prohibitively computationally expensiveand do not perform on par with their retrain-based counterparts. We presentSelective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-freeapproach to machine unlearning which is fast, performant, and does not requirelong-term storage of the training data. First, SSD uses the Fisher informationmatrix of the training and forgetting data to select parameters that aredisproportionately important to the forget set. Second, SSD induces forgettingby dampening these parameters proportional to their relative importance to theforget set with respect to the wider training data. We evaluate our methodagainst several existing unlearning methods in a range of experiments usingResNet18 and Vision Transformer. Results show that the performance of SSD iscompetitive with retrain-based post hoc methods, demonstrating the viability ofretrain-free post hoc unlearning approaches.</description><author>Jack Foster, Stefan Schoepf, Alexandra Brintrup</author><pubDate>Wed, 13 Dec 2023 16:11:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07707v2</guid></item><item><title>Capacity of the treelike sign perceptrons neural networks with one hidden layer -- RDT based upper bounds</title><link>http://arxiv.org/abs/2312.08244v1</link><description>We study the capacity of \emph{sign} perceptrons neural networks (SPNN) andparticularly focus on 1-hidden layer \emph{treelike committee machine} (TCM)architectures. Similarly to what happens in the case of a single perceptronneuron, it turns out that, in a statistical sense, the capacity of acorresponding multilayered network architecture consisting of multiple\emph{sign} perceptrons also undergoes the so-called phase transition (PT)phenomenon. This means: (i) for certain range of system parameters (size ofdata, number of neurons), the network can be properly trained to accuratelymemorize \emph{all} elements of the input dataset; and (ii) outside the regionsuch a training does not exist. Clearly, determining the corresponding phasetransition curve that separates these regions is an extraordinary task andamong the most fundamental questions related to the performance of any network.Utilizing powerful mathematical engine called Random Duality Theory (RDT), weestablish a generic framework for determining the upper bounds on the 1-hiddenlayer TCM SPNN capacity. Moreover, we do so for \emph{any} given (odd) numberof neurons. We further show that the obtained results \emph{exactly} match thereplica symmetry predictions of \cite{EKTVZ92,BHS92}, thereby proving that thestatistical physics based results are not only nice estimates but alsomathematically rigorous bounds as well. Moreover, for $d\leq 5$, we obtain thecapacity values that improve on the best known rigorous ones of\cite{MitchDurb89}, thereby establishing a first, mathematically rigorous,progress in well over 30 years.</description><author>Mihailo Stojnic</author><pubDate>Wed, 13 Dec 2023 16:06:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08244v1</guid></item><item><title>CenterGrasp: Object-Aware Implicit Representation Learning for Simultaneous Shape Reconstruction and 6-DoF Grasp Estimation</title><link>http://arxiv.org/abs/2312.08240v1</link><description>Reliable object grasping is a crucial capability for autonomous robots.However, many existing grasping approaches focus on general clutter removalwithout explicitly modeling objects and thus only relying on the visible localgeometry. We introduce CenterGrasp, a novel framework that combines objectawareness and holistic grasping. CenterGrasp learns a general object prior byencoding shapes and valid grasps in a continuous latent space. It consists ofan RGB-D image encoder that leverages recent advances to detect objects andinfer their pose and latent code, and a decoder to predict shape and grasps foreach object in the scene. We perform extensive experiments on simulated as wellas real-world cluttered scenes and demonstrate strong scene reconstruction and6-DoF grasp-pose estimation performance. Compared to the state of the art,CenterGrasp achieves an improvement of 38.5 mm in shape reconstruction and 33percentage points on average in grasp success. We make the code and trainedmodels publicly available at http://centergrasp.cs.uni-freiburg.de.</description><author>Eugenio Chisari, Nick Heppert, Tim Welschehold, Wolfram Burgard, Abhinav Valada</author><pubDate>Wed, 13 Dec 2023 16:01:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08240v1</guid></item><item><title>Beyond the Label Itself: Latent Labels Enhance Semi-supervised Point Cloud Panoptic Segmentation</title><link>http://arxiv.org/abs/2312.08234v1</link><description>As the exorbitant expense of labeling autopilot datasets and the growingtrend of utilizing unlabeled data, semi-supervised segmentation on point cloudsbecomes increasingly imperative. Intuitively, finding out more ``unspokenwords'' (i.e., latent instance information) beyond the label itself should behelpful to improve performance. In this paper, we discover two types of latentlabels behind the displayed label embedded in LiDAR and image data. First, inthe LiDAR Branch, we propose a novel augmentation, Cylinder-Mix, which is ableto augment more yet reliable samples for training. Second, in the Image Branch,we propose the Instance Position-scale Learning (IPSL) Module to learn and fusethe information of instance position and scale, which is from a 2D pre-traineddetector and a type of latent label obtained from 3D to 2D projection. Finally,the two latent labels are embedded into the multi-modal panoptic segmentationnetwork. The ablation of the IPSL module demonstrates its robust adaptability,and the experiments evaluated on SemanticKITTI and nuScenes demonstrate thatour model outperforms the state-of-the-art method, LaserMix.</description><author>Yujun Chen, Xin Tan, Zhizhong Zhang, Yanyun Qu, Yuan Xie</author><pubDate>Wed, 13 Dec 2023 15:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08234v1</guid></item><item><title>Partial Symmetry Detection for 3D Geometry using Contrastive Learning with Geodesic Point Cloud Patches</title><link>http://arxiv.org/abs/2312.08230v1</link><description>Symmetry detection, especially partial and extrinsic symmetry, is essentialfor various downstream tasks, like 3D geometry completion, segmentation,compression and structure-aware shape encoding or generation. In order todetect partial extrinsic symmetries, we propose to learn rotation, reflection,translation and scale invariant local shape features for geodesic point cloudpatches via contrastive learning, which are robust across multiple classes andgeneralize over different datasets. We show that our approach is able toextract multiple valid solutions for this ambiguous problem. Furthermore, weintroduce a novel benchmark test for partial extrinsic symmetry detection toevaluate our method. Lastly, we incorporate the detected symmetries togetherwith a region growing algorithm to demonstrate a downstream task with the goalof computing symmetry-aware partitions of 3D shapes. To our knowledge, we arethe first to propose a self-supervised data-driven method for partial extrinsicsymmetry detection.</description><author>Gregor Kobsik, Isaak Lim, Leif Kobbelt</author><pubDate>Wed, 13 Dec 2023 15:48:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08230v1</guid></item><item><title>BIRB: A Generalization Benchmark for Information Retrieval in Bioacoustics</title><link>http://arxiv.org/abs/2312.07439v2</link><description>The ability for a machine learning model to cope with differences in trainingand deployment conditions--e.g. in the presence of distribution shift or thegeneralization to new classes altogether--is crucial for real-world use cases.However, most empirical work in this area has focused on the image domain withartificial benchmarks constructed to measure individual aspects ofgeneralization. We present BIRB, a complex benchmark centered on the retrievalof bird vocalizations from passively-recorded datasets given focal recordingsfrom a large citizen science corpus available for training. We propose abaseline system for this collection of tasks using representation learning anda nearest-centroid search. Our thorough empirical evaluation and analysissurfaces open research directions, suggesting that BIRB fills the need for amore realistic and complex benchmark to drive progress on robustness todistribution shifts and generalization of ML models.</description><author>Jenny Hamer, Eleni Triantafillou, Bart van Merriënboer, Stefan Kahl, Holger Klinck, Tom Denton, Vincent Dumoulin</author><pubDate>Wed, 13 Dec 2023 15:48:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07439v2</guid></item><item><title>Differentially Private Gradient Flow based on the Sliced Wasserstein Distance for Non-Parametric Generative Modeling</title><link>http://arxiv.org/abs/2312.08227v1</link><description>Safeguarding privacy in sensitive training data is paramount, particularly inthe context of generative modeling. This is done through either differentiallyprivate stochastic gradient descent, or with a differentially private metricfor training models or generators. In this paper, we introduce a noveldifferentially private generative modeling approach based on parameter-freegradient flows in the space of probability measures. The proposed algorithm isa new discretized flow which operates through a particle scheme, utilizingdrift derived from the sliced Wasserstein distance and computed in a privatemanner. Our experiments show that compared to a generator-based model, ourproposed model can generate higher-fidelity data at a low privacy budget,offering a viable alternative to generator-based approaches.</description><author>Ilana Sebag, Muni Sreenivas PYDI, Jean-Yves Franceschi, Alain Rakotomamonjy, Mike Gartrell, Jamal Atif, Alexandre Allauzen</author><pubDate>Wed, 13 Dec 2023 15:47:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08227v1</guid></item><item><title>GLOP: Learning Global Partition and Local Construction for Solving Large-scale Routing Problems in Real-time</title><link>http://arxiv.org/abs/2312.08224v1</link><description>The recent end-to-end neural solvers have shown promise for small-scalerouting problems but suffered from limited real-time scaling-up performance.This paper proposes GLOP (Global and Local Optimization Policies), a unifiedhierarchical framework that efficiently scales toward large-scale routingproblems. GLOP partitions large routing problems into Travelling SalesmanProblems (TSPs) and TSPs into Shortest Hamiltonian Path Problems. For the firsttime, we hybridize non-autoregressive neural heuristics for coarse-grainedproblem partitions and autoregressive neural heuristics for fine-grained routeconstructions, leveraging the scalability of the former and the meticulousnessof the latter. Experimental results show that GLOP achieves competitive andstate-of-the-art real-time performance on large-scale routing problems,including TSP, ATSP, CVRP, and PCTSP.</description><author>Haoran Ye, Jiarui Wang, Helan Liang, Zhiguang Cao, Yong Li, Fanzhang Li</author><pubDate>Wed, 13 Dec 2023 15:46:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08224v1</guid></item><item><title>Patch-wise Graph Contrastive Learning for Image Translation</title><link>http://arxiv.org/abs/2312.08223v1</link><description>Recently, patch-wise contrastive learning is drawing attention for the imagetranslation by exploring the semantic correspondence between the input andoutput images. To further explore the patch-wise topology for high-levelsemantic understanding, here we exploit the graph neural network to capture thetopology-aware features. Specifically, we construct the graph based on thepatch-wise similarity from a pretrained encoder, whose adjacency matrix isshared to enhance the consistency of patch-wise relation between the input andthe output. Then, we obtain the node feature from the graph neural network, andenhance the correspondence between the nodes by increasing mutual informationusing the contrastive loss. In order to capture the hierarchical semanticstructure, we further propose the graph pooling. Experimental resultsdemonstrate the state-of-art results for the image translation thanks to thesemantic encoding by the constructed graphs.</description><author>Chanyong Jung, Gihyun Kwon, Jong Chul Ye</author><pubDate>Wed, 13 Dec 2023 15:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08223v1</guid></item><item><title>Curriculum-Enhanced Residual Soft An-Isotropic Normalization for Over-smoothness in Deep GNNs</title><link>http://arxiv.org/abs/2312.08221v1</link><description>Despite Graph neural networks' significant performance gain over many classictechniques in various graph-related downstream tasks, their successes arerestricted in shallow models due to over-smoothness and the difficulties ofoptimizations among many other issues. In this paper, to alleviate theover-smoothing issue, we propose a soft graph normalization method to preservethe diversities of node embeddings and prevent indiscrimination due to possibleover-closeness. Combined with residual connections, we analyze the reason whythe method can effectively capture the knowledge in both input graph structuresand node features even with deep networks. Additionally, inspired by CurriculumLearning that learns easy examples before the hard ones, we propose a novellabel-smoothing-based learning framework to enhance the optimization of deepGNNs, which iteratively smooths labels in an auxiliary graph and constructsmany gradual non-smooth tasks for extracting increasingly complex knowledge andgradually discriminating nodes from coarse to fine. The method arguably reducesthe risk of overfitting and generalizes better results. Finally, extensiveexperiments are carried out to demonstrate the effectiveness and potential ofthe proposed model and learning framework through comparison with twelveexisting baselines including the state-of-the-art methods on twelve real-worldnode classification benchmarks.</description><author>Jin Li, Qirong Zhang, Shuling Xu, Xinlong Chen, Longkun Guo, Yang-Geng Fu</author><pubDate>Wed, 13 Dec 2023 15:42:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08221v1</guid></item><item><title>EventAid: Benchmarking Event-aided Image/Video Enhancement Algorithms with Real-captured Hybrid Dataset</title><link>http://arxiv.org/abs/2312.08220v1</link><description>Event cameras are emerging imaging technology that offers advantages overconventional frame-based imaging sensors in dynamic range and sensing speed.Complementing the rich texture and color perception of traditional imageframes, the hybrid camera system of event and frame-based cameras enableshigh-performance imaging. With the assistance of event cameras, high-qualityimage/video enhancement methods make it possible to break the limits oftraditional frame-based cameras, especially exposure time, resolution, dynamicrange, and frame rate limits. This paper focuses on five event-aided image andvideo enhancement tasks (i.e., event-based video reconstruction, event-aidedhigh frame rate video reconstruction, image deblurring, image super-resolution,and high dynamic range image reconstruction), provides an analysis of theeffects of different event properties, a real-captured and ground truth labeledbenchmark dataset, a unified benchmarking of state-of-the-art methods, and anevaluation for two mainstream event simulators. In detail, this paper collectsa real-captured evaluation dataset EventAid for five event-aided image/videoenhancement tasks, by using "Event-RGB" multi-camera hybrid system, taking intoaccount scene diversity and spatiotemporal synchronization. We further performquantitative and visual comparisons for state-of-the-art algorithms, provide acontrolled experiment to analyze the performance limit of event-aided imagedeblurring methods, and discuss open problems to inspire future research.</description><author>Peiqi Duan, Boyu Li, Yixin Yang, Hanyue Lou, Minggui Teng, Yi Ma, Boxin Shi</author><pubDate>Wed, 13 Dec 2023 15:42:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08220v1</guid></item><item><title>Loci-Segmented: Improving Scene Segmentation Learning</title><link>http://arxiv.org/abs/2310.10410v2</link><description>Slot-oriented approaches for compositional scene segmentation from images andvideos still depend on provided background information or slot assignments. Wepresent Loci-Segmented (Loci-s) building on the slot-based location andidentity tracking architecture Loci (Traub et al., ICLR 2023). Loci-s enablesdynamic (i) background processing by means of a foreground identifying moduleand a background re-generator; (ii) top-down modified object-focused bottom-upprocessing; and (iii) depth estimate generation. We also improve automatic slotassignment via a slot-location-entity regularization mechanism and a priorsegmentation network. The results reveal superior video decompositionperformance in the MOVi datasets and in another established dataset collectiontargeting scene segmentation. Loci-s outperforms the state-of-the-art withrespect to the intersection over union (IoU) score in the multi-object videodataset MOVi-E by a large margin and even without supervised slot assignmentsand without the provision of background information. We furthermore show thatLoci-s generates well-interpretable latent representations. Theserepresentations may serve as a foundation-model-like interpretable basis forsolving downstream tasks, such as grounding language, forming compositionalrules, or solving one-shot reinforcement learning tasks.</description><author>Manuel Traub, Frederic Becker, Adrian Sauter, Sebastian Otte, Martin V. Butz</author><pubDate>Wed, 13 Dec 2023 15:30:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10410v2</guid></item><item><title>Accelerated Event-Based Feature Detection and Compression for Surveillance Video Systems</title><link>http://arxiv.org/abs/2312.08213v1</link><description>The strong temporal consistency of surveillance video enables compellingcompression performance with traditional methods, but downstream visionapplications operate on decoded image frames with a high data rate. Since it isnot straightforward for applications to extract information on temporalredundancy from the compressed video representations, we propose a novel systemwhich conveys temporal redundancy within a sparse decompressed representation.We leverage a video representation framework called ADDER to transcode framedvideos to sparse, asynchronous intensity samples. We introduce mechanisms forcontent adaptation, lossy compression, and asynchronous forms of classicalvision algorithms. We evaluate our system on the VIRAT surveillance videodataset, and we show a median 43.7% speed improvement in FAST feature detectioncompared to OpenCV. We run the same algorithm as OpenCV, but only processpixels that receive new asynchronous events, rather than process every pixel inan image frame. Our work paves the way for upcoming neuromorphic sensors and isamenable to future applications with spiking neural networks.</description><author>Andrew C. Freeman, Ketan Mayer-Patel, Montek Singh</author><pubDate>Wed, 13 Dec 2023 15:30:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08213v1</guid></item><item><title>LAMM: Label Alignment for Multi-Modal Prompt Learning</title><link>http://arxiv.org/abs/2312.08212v1</link><description>With the success of pre-trained visual-language (VL) models such as CLIP invisual representation tasks, transferring pre-trained models to downstreamtasks has become a crucial paradigm. Recently, the prompt tuning paradigm,which draws inspiration from natural language processing (NLP), has madesignificant progress in VL field. However, preceding methods mainly focus onconstructing prompt templates for text and visual inputs, neglecting the gap inclass label representations between the VL models and downstream tasks. Toaddress this challenge, we introduce an innovative label alignment method named\textbf{LAMM}, which can dynamically adjust the category embeddings ofdownstream datasets through end-to-end training. Moreover, to achieve a moreappropriate label distribution, we propose a hierarchical loss, encompassingthe alignment of the parameter space, feature space, and logits space. Weconduct experiments on 11 downstream vision datasets and demonstrate that ourmethod significantly improves the performance of existing multi-modal promptlearning models in few-shot scenarios, exhibiting an average accuracyimprovement of 2.31(\%) compared to the state-of-the-art methods on 16 shots.Moreover, our methodology exhibits the preeminence in continual learningcompared to other prompt tuning methods. Importantly, our method is synergisticwith existing prompt tuning methods and can boost the performance on top ofthem. Our code and dataset will be publicly available athttps://github.com/gaojingsheng/LAMM.</description><author>Jingsheng Gao, Jiacheng Ruan, Suncheng Xiang, Zefang Yu, Ke Ji, Mingye Xie, Ting Liu, Yuzhuo Fu</author><pubDate>Wed, 13 Dec 2023 15:29:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08212v1</guid></item><item><title>Big Data -- Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques</title><link>http://arxiv.org/abs/2307.12971v2</link><description>This article intends to systematically identify and comparatively analyzestate-of-the-art supply chain (SC) forecasting strategies and technologies. Anovel framework has been proposed incorporating Big Data Analytics in SCManagement (problem identification, data sources, exploratory data analysis,machine-learning model training, hyperparameter tuning, performance evaluation,and optimization), forecasting effects on human-workforce, inventory, andoverall SC. Initially, the need to collect data according to SC strategy andhow to collect them has been discussed. The article discusses the need fordifferent types of forecasting according to the period or SC objective. The SCKPIs and the error-measurement systems have been recommended to optimize thetop-performing model. The adverse effects of phantom inventory on forecastingand the dependence of managerial decisions on the SC KPIs for determining modelperformance parameters and improving operations management, transparency, andplanning efficiency have been illustrated. The cyclic connection within theframework introduces preprocessing optimization based on the post-process KPIs,optimizing the overall control process (inventory management, workforcedetermination, cost, production and capacity planning). The contribution ofthis research lies in the standard SC process framework proposal, recommendedforecasting data analysis, forecasting effects on SC performance, machinelearning algorithms optimization followed, and in shedding light on futureresearch.</description><author>Md Abrar Jahin, Md Sakib Hossain Shovon, Jungpil Shin, Istiyaque Ahmed Ridoy, Yoichi Tomioka, M. F. Mridha</author><pubDate>Wed, 13 Dec 2023 15:10:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12971v2</guid></item><item><title>SPD-DDPM: Denoising Diffusion Probabilistic Models in the Symmetric Positive Definite Space</title><link>http://arxiv.org/abs/2312.08200v1</link><description>Symmetric positive definite~(SPD) matrices have shown important value andapplications in statistics and machine learning, such as FMRI analysis andtraffic prediction. Previous works on SPD matrices mostly focus ondiscriminative models, where predictions are made directly on $E(X|y)$, where$y$ is a vector and $X$ is an SPD matrix. However, these methods arechallenging to handle for large-scale data, as they need to access and processthe whole data. In this paper, inspired by denoising diffusion probabilisticmodel~(DDPM), we propose a novel generative model, termed SPD-DDPM, byintroducing Gaussian distribution in the SPD space to estimate $E(X|y)$.Moreover, our model is able to estimate $p(X)$ unconditionally and flexiblywithout giving $y$. On the one hand, the model conditionally learns $p(X|y)$and utilizes the mean of samples to obtain $E(X|y)$ as a prediction. On theother hand, the model unconditionally learns the probability distribution ofthe data $p(X)$ and generates samples that conform to this distribution.Furthermore, we propose a new SPD net which is much deeper than the previousnetworks and allows for the inclusion of conditional factors. Experimentresults on toy data and real taxi data demonstrate that our models effectivelyfit the data distribution both unconditionally and unconditionally and provideaccurate predictions.</description><author>Yunchen Li, Zhou Yu, Gaoqi He, Yunhang Shen, Ke Li, Xing Sun, Shaohui Lin</author><pubDate>Wed, 13 Dec 2023 15:08:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08200v1</guid></item><item><title>New Online Communities: Graph Deep Learning on Anonymous Voting Networks to Identify Sybils in Polycentric Governance</title><link>http://arxiv.org/abs/2311.17929v2</link><description>This research examines the polycentric governance of digital assets inblockchain-based Decentralized Autonomous Organizations (DAOs). It offers atheoretical framework and addresses a critical challenge facing decentralizedgovernance by developing a method to identify sybils, or spurious identities.The method uses graph deep learning techniques to identify sybil activity in aDAO governance dataset (snapshot.org). Specifically, a Graph ConvolutionalNeural Network (GCNN) learned voting behaviours and a fast k-means vectorclustering algorithm (FAISS) used the high dimensional embeddings to identifysimilar nodes in a graph. The results reveal that deep learning can effectivelyidentify sybils, reducing the voting graph by 2-5%. This research underscoresthe importance of sybil resistance in DAOs and offers a novel perspective ondecentralized governance, informing future policy, regulation, and governancepractices.</description><author>Quinn DuPont</author><pubDate>Wed, 13 Dec 2023 15:06:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17929v2</guid></item><item><title>Towards Model-Based Data Acquisition for Subjective Multi-Task NLP Problems</title><link>http://arxiv.org/abs/2312.08198v1</link><description>Data annotated by humans is a source of knowledge by describing thepeculiarities of the problem and therefore fueling the decision process of thetrained model. Unfortunately, the annotation process for subjective naturallanguage processing (NLP) problems like offensiveness or emotion detection isoften very expensive and time-consuming. One of the inevitable risks is tospend some of the funds and annotator effort on annotations that do not provideany additional knowledge about the specific task. To minimize these costs, wepropose a new model-based approach that allows the selection of tasks annotatedindividually for each text in a multi-task scenario. The experiments carriedout on three datasets, dozens of NLP tasks, and thousands of annotations showthat our method allows up to 40% reduction in the number of annotations withnegligible loss of knowledge. The results also emphasize the need to collect adiverse amount of data required to efficiently train a model, depending on thesubjectivity of the annotation task. We also focused on measuring the relationbetween subjective tasks by evaluating the model in single-task and multi-taskscenarios. Moreover, for some datasets, training only on the labels predictedby our model improved the efficiency of task selection as a self-supervisedlearning regularization technique.</description><author>Kamil Kanclerz, Julita Bielaniewicz, Marcin Gruza, Jan Kocon, Stanisław Woźniak, Przemysław Kazienko</author><pubDate>Wed, 13 Dec 2023 15:03:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08198v1</guid></item><item><title>Towards a Perceptual Evaluation Framework for Lighting Estimation</title><link>http://arxiv.org/abs/2312.04334v2</link><description>Progress in lighting estimation is tracked by computing existing imagequality assessment (IQA) metrics on images from standard datasets. While thismay appear to be a reasonable approach, we demonstrate that doing so does notcorrelate to human preference when the estimated lighting is used to relight avirtual scene into a real photograph. To study this, we design a controlledpsychophysical experiment where human observers must choose their preferenceamongst rendered scenes lit using a set of lighting estimation algorithmsselected from the recent literature, and use it to analyse how these algorithmsperform according to human perception. Then, we demonstrate that none of themost popular IQA metrics from the literature, taken individually, correctlyrepresent human perception. Finally, we show that by learning a combination ofexisting IQA metrics, we can more accurately represent human preference. Thisprovides a new perceptual framework to help evaluate future lighting estimationalgorithms.</description><author>Justine Giroux, Mohammad Reza Karimi Dastjerdi, Yannick Hold-Geoffroy, Javier Vazquez-Corral, Jean-François Lalonde</author><pubDate>Wed, 13 Dec 2023 15:00:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04334v2</guid></item><item><title>Exploiting Machine Unlearning for Backdoor Attacks in Deep Learning System</title><link>http://arxiv.org/abs/2310.10659v2</link><description>In recent years, the security issues of artificial intelligence have becomeincreasingly prominent due to the rapid development of deep learning researchand applications. Backdoor attack is an attack targeting the vulnerability ofdeep learning models, where hidden backdoors are activated by triggers embeddedby the attacker, thereby outputting malicious predictions that may not alignwith the intended output for a given input. In this work, we propose a novelblack-box backdoor attack based on machine unlearning. The attacker firstaugments the training set with carefully designed samples, including poison andmitigation data, to train a `benign' model. Then, the attacker posts unlearningrequests for the mitigation samples to remove the impact of relevant data onthe model, gradually activating the hidden backdoor. Since backdoors areimplanted during the iterative unlearning process, it significantly increasesthe computational overhead of existing defense methods for backdoor detectionor mitigation. To address this new security threat, we proposes two methods fordetecting or mitigating such malicious unlearning requests. We conduct theexperiment in both exact unlearning and approximate unlearning (i.e., SISA)settings. Experimental results indicate that: 1) our attack approach cansuccessfully implant backdoor into the model, and sharding increases thedifficult of attack; 2) our detection algorithms are effective in identifyingthe mitigation samples, while sharding reduces the effectiveness of ourdetection algorithms.</description><author>Peixin Zhang, Jun Sun, Mingtian Tan, Xinyu Wang</author><pubDate>Wed, 13 Dec 2023 15:00:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10659v2</guid></item><item><title>Concept-centric Personalization with Large-scale Diffusion Priors</title><link>http://arxiv.org/abs/2312.08195v1</link><description>Despite large-scale diffusion models being highly capable of generatingdiverse open-world content, they still struggle to match the photorealism andfidelity of concept-specific generators. In this work, we present the task ofcustomizing large-scale diffusion priors for specific concepts asconcept-centric personalization. Our goal is to generate high-qualityconcept-centric images while maintaining the versatile controllability inherentto open-world models, enabling applications in diverse tasks such asconcept-centric stylization and image translation. To tackle these challenges,we identify catastrophic forgetting of guidance prediction from diffusionpriors as the fundamental issue. Consequently, we develop a guidance-decoupledpersonalization framework specifically designed to address this task. Wepropose Generalized Classifier-free Guidance (GCFG) as the foundational theoryfor our framework. This approach extends Classifier-free Guidance (CFG) toaccommodate an arbitrary number of guidances, sourced from a variety ofconditions and models. Employing GCFG enables us to separate conditionalguidance into two distinct components: concept guidance for fidelity andcontrol guidance for controllability. This division makes it feasible to traina specialized model for concept guidance, while ensuring both control andunconditional guidance remain intact. We then present a null-textConcept-centric Diffusion Model as a concept-specific generator to learnconcept guidance without the need for text annotations. Code will be availableat https://github.com/PRIV-Creation/Concept-centric-Personalization.</description><author>Pu Cao, Lu Yang, Feng Zhou, Tianrui Huang, Qing Song</author><pubDate>Wed, 13 Dec 2023 14:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08195v1</guid></item><item><title>SVInvNet: A Densely Connected Encoder-Decoder Architecture for Seismic Velocity Inversion</title><link>http://arxiv.org/abs/2312.08194v1</link><description>This study presents a deep learning-based approach to seismic velocityinversion problem, focusing on both noisy and noiseless training datasets ofvarying sizes. Our Seismic Velocity Inversion Network (SVInvNet) introduces anovel architecture that contains a multi-connection encoder-decoder structureenhanced with dense blocks. This design is specifically tuned to effectivelyprocess complex information, crucial for addressing the challenges ofnon-linear seismic velocity inversion. For training and testing, we createddiverse seismic velocity models, including multi-layered, faulty, and salt domecategories. We also investigated how different kinds of ambient noise, bothcoherent and stochastic, and the size of the training dataset affect learningoutcomes. SVInvNet is trained on datasets ranging from 750 to 6,000 samples andis tested using a large benchmark dataset of 12,000 samples. Despite its fewerparameters compared to the baseline, SVInvNet achieves superior performancewith this dataset. The outcomes of the SVInvNet are additionally compared tothose of the Full Waveform Inversion (FWI) method. The comparative analysisclearly reveals the effectiveness of the proposed model.</description><author>Mojtaba Najafi Khatounabad, Hacer Yalim Keles, Selma Kadioglu</author><pubDate>Wed, 13 Dec 2023 14:58:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08194v1</guid></item><item><title>Universal Adversarial Framework to Improve Adversarial Robustness for Diabetic Retinopathy Detection</title><link>http://arxiv.org/abs/2312.08193v1</link><description>Diabetic Retinopathy (DR) is a prevalent illness associated with Diabeteswhich, if left untreated, can result in irreversible blindness. Deep Learningbased systems are gradually being introduced as automated support for clinicaldiagnosis. Since healthcare has always been an extremely important domaindemanding error-free performance, any adversaries could pose a big threat tothe applicability of such systems. In this work, we use Universal AdversarialPerturbations (UAPs) to quantify the vulnerability of Medical Deep NeuralNetworks (DNNs) for detecting DR. To the best of our knowledge, this is thevery first attempt that works on attacking complete fine-grained classificationof DR images using various UAPs. Also, as a part of this work, we use UAPs tofine-tune the trained models to defend against adversarial samples. Weexperiment on several models and observe that the performance of such modelstowards unseen adversarial attacks gets boosted on average by $3.41$Cohen-kappa value and maximum by $31.92$ Cohen-kappa value. The performancedegradation on normal data upon ensembling the fine-tuned models was found tobe statistically insignificant using t-test, highlighting the benefits ofUAP-based adversarial fine-tuning.</description><author>Samrat Mukherjee, Dibyanayan Bandyopadhyay, Baban Gain, Asif Ekbal</author><pubDate>Wed, 13 Dec 2023 14:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08193v1</guid></item><item><title>PAD: Self-Supervised Pre-Training with Patchwise-Scale Adapter for Infrared Images</title><link>http://arxiv.org/abs/2312.08192v1</link><description>Self-supervised learning (SSL) for RGB images has achieved significantsuccess, yet there is still limited research on SSL for infrared images,primarily due to three prominent challenges: 1) the lack of a suitablelarge-scale infrared pre-training dataset, 2) the distinctiveness of non-iconicinfrared images rendering common pre-training tasks like masked image modeling(MIM) less effective, and 3) the scarcity of fine-grained textures making itparticularly challenging to learn general image features. To address theseissues, we construct a Multi-Scene Infrared Pre-training (MSIP) datasetcomprising 178,756 images, and introduce object-sensitive random RoI cropping,an image preprocessing method, to tackle the challenge posed by non-iconicimages. To alleviate the impact of weak textures on feature learning, wepropose a pre-training paradigm called Pre-training with ADapter (PAD), whichuses adapters to learn domain-specific features while freezing parameterspre-trained on ImageNet to retain the general feature extraction capability.This new paradigm is applicable to any transformer-based SSL method.Furthermore, to achieve more flexible coordination between pre-trained andnewly-learned features in different layers and patches, a patchwise-scaleadapter with dynamically learnable scale factors is introduced. Extensiveexperiments on three downstream tasks show that PAD, with only 1.23Mpre-trainable parameters, outperforms other baseline paradigms includingcontinual full pre-training on MSIP. Our code and dataset are available athttps://github.com/casiatao/PAD.</description><author>Tao Zhang, Kun Ding, Jinyong Wen, Yu Xiong, Zeyu Zhang, Shiming Xiang, Chunhong Pan</author><pubDate>Wed, 13 Dec 2023 14:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08192v1</guid></item><item><title>StarCoder: may the source be with you!</title><link>http://arxiv.org/abs/2305.06161v2</link><description>The BigCode community, an open-scientific collaboration working on theresponsible development of Large Language Models for Code (Code LLMs),introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K contextlength, infilling capabilities and fast large-batch inference enabled bymulti-query attention. StarCoderBase is trained on 1 trillion tokens sourcedfrom The Stack, a large collection of permissively licensed GitHub repositorieswith inspection tools and an opt-out process. We fine-tuned StarCoderBase on35B Python tokens, resulting in the creation of StarCoder. We perform the mostcomprehensive evaluation of Code LLMs to date and show that StarCoderBaseoutperforms every open Code LLM that supports multiple programming languagesand matches or outperforms the OpenAI code-cushman-001 model. Furthermore,StarCoder outperforms every model that is fine-tuned on Python, can be promptedto achieve 40\% pass@1 on HumanEval, and still retains its performance on otherprogramming languages. We take several important steps towards a safeopen-access model release, including an improved PII redaction pipeline and anovel attribution tracing tool, and make the StarCoder models publiclyavailable under a more commercially viable version of the Open Responsible AIModel license.</description><author>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex </author><pubDate>Wed, 13 Dec 2023 14:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06161v2</guid></item><item><title>LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning</title><link>http://arxiv.org/abs/2307.02345v4</link><description>Modern reinforcement learning (RL) can be categorized into online and offlinevariants. As a pivotal aspect of both online and offline RL, current researchon the Bellman equation revolves primarily around optimization techniques andperformance enhancement rather than exploring the inherent structuralproperties of the Bellman error, such as its distribution characteristics. Thisstudy investigates the distribution of the Bellman approximation error throughiterative exploration of the Bellman equation with the observation that theBellman error approximately follows the Logistic distribution. Based on this,we proposed the utilization of the Logistic maximum likelihood function (LLoss)as an alternative to the commonly used mean squared error (MSELoss) thatassumes a Normal distribution for Bellman errors. We validated the hypothesesthrough extensive numerical experiments across diverse online and offlineenvironments. In particular, we applied the Logistic correction to lossfunctions in various RL baseline methods and observed that the results withLLoss consistently outperformed the MSE counterparts. We also conducted theKolmogorov-Smirnov tests to confirm the reliability of the Logisticdistribution. Moreover, our theory connects the Bellman error to theproportional reward scaling phenomenon by providing a distribution-basedanalysis. Furthermore, we applied the bias-variance decomposition for samplingfrom the Logistic distribution. The theoretical and empirical insights of thisstudy lay a valuable foundation for future investigations and enhancementscentered on the distribution of Bellman error.</description><author>Outongyi Lv, Bingxin Zhou</author><pubDate>Wed, 13 Dec 2023 14:43:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02345v4</guid></item><item><title>On the convex formulations of robust Markov decision processes</title><link>http://arxiv.org/abs/2209.10187v2</link><description>Robust Markov decision processes (MDPs) are used for applications of dynamicoptimization in uncertain environments and have been studied extensively. Manyof the main properties and algorithms of MDPs, such as value iteration andpolicy iteration, extend directly to RMDPs. Surprisingly, there is no knownanalog of the MDP convex optimization formulation for solving RMDPs. This workdescribes the first convex optimization formulation of RMDPs under theclassical sa-rectangularity and s-rectangularity assumptions. By using entropicregularization and exponential change of variables, we derive a convexformulation with a number of variables and constraints polynomial in the numberof states and actions, but with large coefficients in the constraints. Wefurther simplify the formulation for RMDPs with polyhedral, ellipsoidal, orentropy-based uncertainty sets, showing that, in these cases, RMDPs can bereformulated as conic programs based on exponential cones, quadratic cones, andnon-negative orthants. Our work opens a new research direction for RMDPs andcan serve as a first step toward obtaining a tractable convex formulation ofRMDPs.</description><author>Julien Grand-Clément, Marek Petrik</author><pubDate>Wed, 13 Dec 2023 14:40:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.10187v2</guid></item><item><title>Cem Mil Podcasts: A Spoken Portuguese Document Corpus For Multi-modal, Multi-lingual and Multi-Dialect Information Access Research</title><link>http://arxiv.org/abs/2209.11871v2</link><description>In this paper we describe the Portuguese-language podcast dataset we havereleased for academic research purposes. We give an overview of how the datawas sampled, descriptive statistics over the collection, as well as informationabout the distribution over Brazilian and Portuguese dialects. We give resultsfrom experiments on multi-lingual summarization, showing that summarizingpodcast transcripts can be performed well by a system supporting both Englishand Portuguese. We also show experiments on Portuguese podcast genreclassification using text metadata. Combining this collection with previouslyreleased English-language collection opens up the potential for multi-modal,multi-lingual and multi-dialect podcast information access research.</description><author>Ekaterina Garmash, Edgar Tanaka, Ann Clifton, Joana Correia, Sharmistha Jat, Winstead Zhu, Rosie Jones, Jussi Karlgren</author><pubDate>Wed, 13 Dec 2023 14:39:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.11871v2</guid></item><item><title>Advanced Image Segmentation Techniques for Neural Activity Detection via C-fos Immediate Early Gene Expression</title><link>http://arxiv.org/abs/2312.08177v1</link><description>This paper investigates the application of advanced image segmentationtechniques to analyze C-fos immediate early gene expression, a crucial markerfor neural activity. Due to the complexity and high variability of neuralcircuits, accurate segmentation of C-fos images is paramount for thedevelopment of new insights into neural function. Amidst this backdrop, thisresearch aims to improve accuracy and minimize manual intervention in C-fosimage segmentation by leveraging the capabilities of CNNs and the Unet model.We describe the development of a novel workflow for the segmentation processinvolving Convolutional Neural Networks (CNNs) and the Unet model,demonstrating their efficiency in various image segmentation tasks. Ourworkflow incorporates pre-processing steps such as cropping, image featureextraction, and clustering for the training dataset selection. We used anAutoEncoder model to extract features and implement constrained clustering toidentify similarities and differences in image types. Additionally, we utilizedmanual and automatic labeling approaches to enhance the performance of ourmodel. We demonstrated the effectiveness of our method in distinguishing areaswith significant C-fos expression from normal tissue areas. Lastly, weimplemented a modified Unet network for the detection of C-fos expressions.This research contributes to the development of more efficient and automatedimage segmentation methods, advancing the understanding of neural function inneuroscience research.</description><author>Peilin Cai</author><pubDate>Wed, 13 Dec 2023 14:36:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08177v1</guid></item><item><title>ASC: Adaptive Scale Feature Map Compression for Deep Neural Network</title><link>http://arxiv.org/abs/2312.08176v1</link><description>Deep-learning accelerators are increasingly in demand; however, theirperformance is constrained by the size of the feature map, leading to highbandwidth requirements and large buffer sizes. We propose an adaptive scalefeature map compression technique leveraging the unique properties of thefeature map. This technique adopts independent channel indexing given the weakchannel correlation and utilizes a cubical-like block shape to benefit fromstrong local correlations. The method further optimizes compression using aswitchable endpoint mode and adaptive scale interpolation to handle unimodaldata distributions, both with and without outliers. This results in 4$\times$and up to 7.69$\times$ compression rates for 16-bit data in constant andvariable bitrates, respectively. Our hardware design minimizes area cost byadjusting interpolation scales, which facilitates hardware sharing amonginterpolation points. Additionally, we introduce a threshold concept forstraightforward interpolation, preventing the need for intricate hardware. TheTSMC 28nm implementation showcases an equivalent gate count of 6135 for the8-bit version. Furthermore, the hardware architecture scales effectively, withonly a sublinear increase in area cost. Achieving a 32$\times$ throughputincrease meets the theoretical bandwidth of DDR5-6400 at just 7.65$\times$ thehardware cost.</description><author>Yuan Yao, Tian-Sheuan Chang</author><pubDate>Wed, 13 Dec 2023 14:36:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08176v1</guid></item><item><title>Double Machine Learning for Static Panel Models with Fixed Effects</title><link>http://arxiv.org/abs/2312.08174v1</link><description>Machine Learning (ML) algorithms are powerful data-driven tools forapproximating high-dimensional or non-linear nuisance functions which areuseful in practice because the true functional form of the predictors isex-ante unknown. In this paper, we develop estimators of policy interventionsfrom panel data which allow for non-linear effects of the confoundingregressors, and investigate the performance of these estimators using threewell-known ML algorithms, specifically, LASSO, classification and regressiontrees, and random forests. We use Double Machine Learning (DML) (Chernozhukovet al., 2018) for the estimation of causal effects of homogeneous treatmentswith unobserved individual heterogeneity (fixed effects) and no unobservedconfounding by extending Robinson (1988)'s partially linear regression model.We develop three alternative approaches for handling unobserved individualheterogeneity based on extending the within-group estimator, first-differenceestimator, and correlated random effect estimator (Mundlak, 1978) fornon-linear models. Using Monte Carlo simulations, we find that conventionalleast squares estimators can perform well even if the data generating processis non-linear, but there are substantial performance gains in terms of biasreduction under a process where the true effect of the regressors is non-linearand discontinuous. However, for the same scenarios, we also find -- despiteextensive hyperparameter tuning -- inference to be problematic for bothtree-based learners because these lead to highly non-normal estimatordistributions and the estimator variance being severely under-estimated. Thiscontradicts the performance of trees in other circumstances and requiresfurther investigation. Finally, we provide an illustrative example of DML forobservational panel data showing the impact of the introduction of the nationalminimum wage in the UK.</description><author>Paul Clarke, Annalivia Polselli</author><pubDate>Wed, 13 Dec 2023 14:34:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08174v1</guid></item><item><title>Chat-3D v2: Bridging 3D Scene and Large Language Models with Object Identifiers</title><link>http://arxiv.org/abs/2312.08168v1</link><description>Recent research has evidenced the significant potentials of Large LanguageModels (LLMs) in handling challenging tasks within 3D scenes. However, currentmodels are constrained to addressing object-centric tasks, where eachquestion-answer pair focuses solely on an individual object. In real-worldapplications, users may pose queries involving multiple objects or expect foranswers that precisely reference various objects. We introduce the use ofobject identifiers to freely reference objects during a conversation. Whilethis solution appears straightforward, it presents two main challenges: 1) Howto establish a reliable one-to-one correspondence between each object and itsidentifier? 2) How to incorporate complex spatial relationships among dozens ofobjects into the embedding space of the LLM? To address these challenges, wepropose a two-stage alignment method, which involves learning anattribute-aware token and a relation-aware token for each object. These tokenscapture the object's attributes and spatial relationships with surroundingobjects in the 3D scene. Once the alignment is established, we can fine-tuneour model on various downstream tasks using instruction tuning. Experimentsconducted on traditional datasets like ScanQA, ScanRefer, and Nr3D/Sr3Dshowcase the effectiveness of our proposed method. Additionally, we create a 3Dscene captioning dataset annotated with rich object identifiers, with theassistant of GPT-4. This dataset aims to further explore the capability ofobject identifiers in effective object referencing and precise sceneunderstanding.</description><author>Haifeng Huang, Zehan Wang, Rongjie Huang, Luping Liu, Xize Cheng, Yang Zhao, Tao Jin, Zhou Zhao</author><pubDate>Wed, 13 Dec 2023 14:27:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08168v1</guid></item><item><title>On Dynamic Programming Decompositions of Static Risk Measures in Markov Decision Processes</title><link>http://arxiv.org/abs/2304.12477v3</link><description>Optimizing static risk-averse objectives in Markov decision processes isdifficult because they do not admit standard dynamic programming equationscommon in Reinforcement Learning (RL) algorithms. Dynamic programmingdecompositions that augment the state space with discrete risk levels haverecently gained popularity in the RL community. Prior work has shown that thesedecompositions are optimal when the risk level is discretized sufficiently.However, we show that these popular decompositions forConditional-Value-at-Risk (CVaR) and Entropic-Value-at-Risk (EVaR) areinherently suboptimal regardless of the discretization level. In particular, weshow that a saddle point property assumed to hold in prior literature may beviolated. However, a decomposition does hold for Value-at-Risk and our proofdemonstrates how this risk measure differs from CVaR and EVaR. Our findings aresignificant because risk-averse algorithms are used in high-stake environments,making their correctness much more critical.</description><author>Jia Lin Hau, Erick Delage, Mohammad Ghavamzadeh, Marek Petrik</author><pubDate>Wed, 13 Dec 2023 14:19:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12477v3</guid></item><item><title>CIDR: A Cooperative Integrated Dynamic Refining Method for Minimal Feature Removal Problem</title><link>http://arxiv.org/abs/2312.08157v1</link><description>The minimal feature removal problem in the post-hoc explanation area aims toidentify the minimal feature set (MFS). Prior studies using the greedyalgorithm to calculate the minimal feature set lack the exploration of featureinteractions under a monotonic assumption which cannot be satisfied in generalscenarios. In order to address the above limitations, we propose a CooperativeIntegrated Dynamic Refining method (CIDR) to efficiently discover minimalfeature sets. Specifically, we design Cooperative Integrated Gradients (CIG) todetect interactions between features. By incorporating CIG and characteristicsof the minimal feature set, we transform the minimal feature removal probleminto a knapsack problem. Additionally, we devise an auxiliary Minimal FeatureRefinement algorithm to determine the minimal feature set from numerouscandidate sets. To the best of our knowledge, our work is the first to addressthe minimal feature removal problem in the field of natural languageprocessing. Extensive experiments demonstrate that CIDR is capable of tracingrepresentative minimal feature sets with improved interpretability acrossvarious models and datasets.</description><author>Qian Chen, Taolin Zhang, Dongyang Li, Xiaofeng He</author><pubDate>Wed, 13 Dec 2023 14:10:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08157v1</guid></item><item><title>ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by Learning to Scale</title><link>http://arxiv.org/abs/2310.01217v2</link><description>Multi-task learning (MTL) has shown considerable practical benefits,particularly when using pre-trained language models (PLMs). While this iscommonly achieved by simultaneously learning $n$ tasks under a jointoptimization procedure, recent methods such as AdapterFusion structure theproblem into two distinct stages: (i) task learning, where knowledge specificto a task is encapsulated within sets of parameters (e.g., adapters), and (ii)transfer, where this already learned knowledge is leveraged for a target task.This separation of concerns provides numerous benefits, such as promotingreusability, and addressing cases involving data privacy and societal concerns;on the flip side, current two-stage MTL methods come with the cost ofintroducing a substantial number of additional parameters. In this work, weaddress this issue by leveraging the usefulness of linearly scaling the outputrepresentations of source adapters for transfer learning. We introduceScaLearn, a simple and highly parameter-efficient two-stage MTL method thatcapitalizes on the knowledge of the source tasks by learning a minimal set ofscaling parameters that enable effective knowledge transfer to a target task.Our experiments on three benchmarks (GLUE, SuperGLUE, and HumSet) show that ourScaLearn, in addition to facilitating the benefits of two-stage MTL,consistently outperforms strong baselines with only a small number of transferparameters - roughly 0.35% of those of AdapterFusion. Remarkably, we observethat ScaLearn maintains its strong abilities even when further reducingparameters through uniform scaling and layer-sharing, achieving similarlycompetitive results with only $8$ transfer parameters for each target task. Ourproposed approach thus demonstrates the power of simple scaling as a promisefor more efficient task transfer.</description><author>Markus Frohmann, Carolin Holtermann, Shahed Masoudian, Anne Lauscher, Navid Rekabsaz</author><pubDate>Wed, 13 Dec 2023 14:09:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01217v2</guid></item><item><title>$ρ$-Diffusion: A diffusion-based density estimation framework for computational physics</title><link>http://arxiv.org/abs/2312.08153v1</link><description>In physics, density $\rho(\cdot)$ is a fundamentally important scalarfunction to model, since it describes a scalar field or a probability densityfunction that governs a physical process. Modeling $\rho(\cdot)$ typicallyscales poorly with parameter space, however, and quickly becomes prohibitivelydifficult and computationally expensive. One promising avenue to bypass this isto leverage the capabilities of denoising diffusion models often used inhigh-fidelity image generation to parameterize $\rho(\cdot)$ from existingscientific data, from which new samples can be trivially sampled from. In thispaper, we propose $\rho$-Diffusion, an implementation of denoising diffusionprobabilistic models for multidimensional density estimation in physics, whichis currently in active development and, from our results, performs well onphysically motivated 2D and 3D density functions. Moreover, we propose a novelhashing technique that allows $\rho$-Diffusion to be conditioned by arbitraryamounts of physical parameters of interest.</description><author>Maxwell X. Cai, Kin Long Kelvin Lee</author><pubDate>Wed, 13 Dec 2023 14:05:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08153v1</guid></item><item><title>Active learning with biased non-response to label requests</title><link>http://arxiv.org/abs/2312.08150v1</link><description>Active learning can improve the efficiency of training prediction models byidentifying the most informative new labels to acquire. However, non-responseto label requests can impact active learning's effectiveness in real-worldcontexts. We conceptualise this degradation by considering the type ofnon-response present in the data, demonstrating that biased non-response isparticularly detrimental to model performance. We argue that this sort ofnon-response is particularly likely in contexts where the labelling process, bynature, relies on user interactions. To mitigate the impact of biasednon-response, we propose a cost-based correction to the sampling strategy--theUpper Confidence Bound of the Expected Utility (UCB-EU)--that can, plausibly,be applied to any active learning algorithm. Through experiments, wedemonstrate that our method successfully reduces the harm from labellingnon-response in many settings. However, we also characterise settings where thenon-response bias in the annotations remains detrimental under UCB-EU forparticular sampling methods and data generating processes. Finally, we evaluateour method on a real-world dataset from e-commerce platform Taobao. We showthat UCB-EU yields substantial performance improvements to conversion modelsthat are trained on clicked impressions. Most generally, this research servesto both better conceptualise the interplay between types of non-response andmodel improvements via active learning, and to provide a practical, easy toimplement correction that helps mitigate model degradation.</description><author>Thomas Robinson, Niek Tax, Richard Mudd, Ido Guy</author><pubDate>Wed, 13 Dec 2023 14:01:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08150v1</guid></item><item><title>High-accuracy Vision-Based Attitude Estimation System for Air-Bearing Spacecraft Simulators</title><link>http://arxiv.org/abs/2312.08146v1</link><description>Air-bearing platforms for simulating the rotational dynamics of satellitesrequire highly precise ground truth systems. Unfortunately, commercial motioncapture systems used for this scope are complex and expensive. This paper showsa novel and versatile method to compute the attitude of rotational air-bearingplatforms using a monocular camera and sets of fiducial markers. The workproposes a geometry-based iterative algorithm that is significantly moreaccurate than other literature methods that involve the solution of thePerspective-n-Point problem. Additionally, auto-calibration procedures toperform a preliminary estimation of the system parameters are shown. Thedeveloped methodology is deployed onto a Raspberry Pi 4 micro-computer andtested with a set of LED markers. Data obtained with this setup are comparedagainst computer simulations of the same system to understand and validate theattitude estimation performances. Simulation results show expected 1-sigmaaccuracies in the order of $\sim$ 12 arcsec and $\sim$ 37 arcsec for about- andcross-boresight rotations of the platform, and average latency times of 6 ms.</description><author>Fabio Ornati, Gianfranco Di Domenico, Paolo Panicucci, Francesco Topputo</author><pubDate>Wed, 13 Dec 2023 13:55:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08146v1</guid></item><item><title>Enabling Fast 2-bit LLM on GPUs: Memory Alignment and Asynchronous Dequantization</title><link>http://arxiv.org/abs/2311.16442v2</link><description>Large language models (LLMs) have demonstrated impressive abilities invarious domains while the inference cost is expensive. The state-of-the-artmethods use 2-bit quantization for mainstream LLMs. However, challenges stillexist: (1) Nonnegligible accuracy loss for 2-bit quantization. Weights arequantized by groups, while the ranges of weights are large in some groups,resulting in large quantization errors and nonnegligible accuracy loss (e.g.&gt;3% for Llama2-7b with 2-bit quantization in GPTQ and Greenbit). (2) Limitedaccuracy improvement by adding 4-bit weights. Increasing 10% extra average bitmore 4-bit weights only leads to &lt;0.5% accuracy improvement on a quantizedLlama2-7b. (3) Time-consuming dequantization operations on GPUs. Thedequantization operations lead to &gt;50% execution time, hindering the potentialof reducing LLM inference cost. To tackle these challenges, we propose thefollowing techniques: (1) We only quantize a small fraction of groups with thelarger range using 4-bit with memory alignment consideration on GPUs.(2) Wedesign the asynchronous dequantization on GPUs, leading to up to 3.92X speedup.We conduct extensive experiments on different model sizes. We achieve 2.85-bitfor each weight and the end-to-end speedup for Llama2-7b is 1.74X over theoriginal model, and we reduce both runtime cost and hardware cost by up to2.70X and 2.81X with less GPU requirements.</description><author>Jinhao Li, Shiyao Li, Jiaming Xu, Shan Huang, Yaoxiu Lian, Jun Liu, Yu Wang, Guohao Dai</author><pubDate>Wed, 13 Dec 2023 13:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16442v2</guid></item><item><title>Efficient Representation of the Activation Space in Deep Neural Networks</title><link>http://arxiv.org/abs/2312.08143v1</link><description>The representations of the activation space of deep neural networks (DNNs)are widely utilized for tasks like natural language processing, anomalydetection and speech recognition. Due to the diverse nature of these tasks andthe large size of DNNs, an efficient and task-independent representation ofactivations becomes crucial. Empirical p-values have been used to quantify therelative strength of an observed node activation compared to activationscreated by already-known inputs. Nonetheless, keeping raw data for thesecalculations increases memory resource consumption and raises privacy concerns.To this end, we propose a model-agnostic framework for creating representationsof activations in DNNs using node-specific histograms to compute p-values ofobserved activations without retaining already-known inputs. Our proposedapproach demonstrates promising potential when validated with multiple networkarchitectures across various downstream tasks and compared with the kerneldensity estimates and brute-force empirical baselines. In addition, theframework reduces memory usage by 30% with up to 4 times faster p-valuecomputing time while maintaining state of-the-art detection power in downstreamtasks such as the detection of adversarial attacks and synthesized content.Moreover, as we do not persist raw data at inference time, we could potentiallyreduce susceptibility to attacks and privacy issues.</description><author>Tanya Akumu, Celia Cintas, Girmaw Abebe Tadesse, Adebayo Oshingbesan, Skyler Speakman, Edward McFowland III</author><pubDate>Wed, 13 Dec 2023 13:46:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08143v1</guid></item><item><title>Generating Novel Scene Compositions from Single Images and Videos</title><link>http://arxiv.org/abs/2103.13389v5</link><description>Given a large dataset for training, generative adversarial networks (GANs)can achieve remarkable performance for the image synthesis task. However,training GANs in extremely low data regimes remains a challenge, as overfittingoften occurs, leading to memorization or training divergence. In this work, weintroduce SIV-GAN, an unconditional generative model that can generate newscene compositions from a single training image or a single video clip. Wepropose a two-branch discriminator architecture, with content and layoutbranches designed to judge internal content and scene layout realism separatelyfrom each other. This discriminator design enables synthesis of visuallyplausible, novel compositions of a scene, with varying content and layout,while preserving the context of the original sample. Compared to previoussingle image GANs, our model generates more diverse, higher quality images,while not being restricted to a single image setting. We further introduce anew challenging task of learning from a few frames of a single video. In thistraining setup the training images are highly similar to each other, whichmakes it difficult for prior GAN models to achieve a synthesis of both highquality and diversity.</description><author>Vadim Sushko, Dan Zhang, Juergen Gall, Anna Khoreva</author><pubDate>Wed, 13 Dec 2023 13:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.13389v5</guid></item><item><title>PECANN: Parallel Efficient Clustering with Graph-Based Approximate Nearest Neighbor Search</title><link>http://arxiv.org/abs/2312.03940v2</link><description>This paper studies density-based clustering of point sets. These methods usedense regions of points to detect clusters of arbitrary shapes. In particular,we study variants of density peaks clustering, a popular type of algorithm thathas been shown to work well in practice. Our goal is to cluster largehigh-dimensional datasets, which are prevalent in practice. Prior solutions areeither sequential, and cannot scale to large data, or are specialized forlow-dimensional data. This paper unifies the different variants of density peaks clustering into asingle framework, PECANN, by abstracting out several key steps common to thisclass of algorithms. One such key step is to find nearest neighbors thatsatisfy a predicate function, and one of the main contributions of this paperis an efficient way to do this predicate search using graph-based approximatenearest neighbor search (ANNS). To provide ample parallelism, we propose adoubling search technique that enables points to find an approximate nearestneighbor satisfying the predicate in a small number of rounds. Our techniquecan be applied to many existing graph-based ANNS algorithms, which can all beplugged into PECANN. We implement five clustering algorithms with PECANN and evaluate them onsynthetic and real-world datasets with up to 1.28 million points and up to 1024dimensions on a 30-core machine with two-way hyper-threading. Compared to thestate-of-the-art FASTDP algorithm for high-dimensional density peaksclustering, which is sequential, our best algorithm is 45x-734x faster whileachieving competitive ARI scores. Compared to the state-of-the-art parallelDPC-based algorithm, which is optimized for low dimensions, we show that PECANNis two orders of magnitude faster. As far as we know, our work is the first toevaluate DPC variants on large high-dimensional real-world image and textembedding datasets.</description><author>Shangdi Yu, Joshua Engels, Yihao Huang, Julian Shun</author><pubDate>Wed, 13 Dec 2023 13:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03940v2</guid></item><item><title>ProNeRF: Learning Efficient Projection-Aware Ray Sampling for Fine-Grained Implicit Neural Radiance Fields</title><link>http://arxiv.org/abs/2312.08136v1</link><description>Recent advances in neural rendering have shown that, albeit slow, implicitcompact models can learn a scene's geometries and view-dependent appearancesfrom multiple views. To maintain such a small memory footprint but achievefaster inference times, recent works have adopted `sampler' networks thatadaptively sample a small subset of points along each ray in the implicitneural radiance fields. Although these methods achieve up to a 10$\times$reduction in rendering time, they still suffer from considerable qualitydegradation compared to the vanilla NeRF. In contrast, we propose ProNeRF,which provides an optimal trade-off between memory footprint (similar to NeRF),speed (faster than HyperReel), and quality (better than K-Planes). ProNeRF isequipped with a novel projection-aware sampling (PAS) network together with anew training strategy for ray exploration and exploitation, allowing forefficient fine-grained particle sampling. Our ProNeRF yields state-of-the-artmetrics, being 15-23x faster with 0.65dB higher PSNR than NeRF and yielding0.95dB higher PSNR than the best published sampler-based method, HyperReel. Ourexploration and exploitation training strategy allows ProNeRF to learn the fullscenes' color and density distributions while also learning efficient raysampling focused on the highest-density regions. We provide extensiveexperimental results that support the effectiveness of our method on the widelyadopted forward-facing and 360 datasets, LLFF and Blender, respectively.</description><author>Juan Luis Gonzalez Bello, Minh-Quan Viet Bui, Munchurl Kim</author><pubDate>Wed, 13 Dec 2023 13:37:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08136v1</guid></item><item><title>A New Perspective On Denoising Based On Optimal Transport</title><link>http://arxiv.org/abs/2312.08135v1</link><description>In the standard formulation of the denoising problem, one is given aprobabilistic model relating a latent variable $\Theta \in \Omega \subset\mathbb{R}^m \; (m\ge 1)$ and an observation $Z \in \mathbb{R}^d$ according to:$Z \mid \Theta \sim p(\cdot\mid \Theta)$ and $\Theta \sim G^*$, and the goal isto construct a map to recover the latent variable from the observation. Theposterior mean, a natural candidate for estimating $\Theta$ from $Z$, attainsthe minimum Bayes risk (under the squared error loss) but at the expense ofover-shrinking the $Z$, and in general may fail to capture the geometricfeatures of the prior distribution $G^*$ (e.g., low dimensionality,discreteness, sparsity, etc.). To rectify these drawbacks, in this paper wetake a new perspective on this denoising problem that is inspired by optimaltransport (OT) theory and use it to propose a new OT-based denoiser at thepopulation level setting. We rigorously prove that, under general assumptionson the model, our OT-based denoiser is well-defined and unique, and is closelyconnected to solutions to a Monge OT problem. We then prove that, underappropriate identifiability assumptions on the model, our OT-based denoiser canbe recovered solely from information of the marginal distribution of $Z$ andthe posterior mean of the model, after solving a linear relaxation problem overa suitable space of couplings that is reminiscent of a standard multimarginalOT (MOT) problem. In particular, thanks to Tweedie's formula, when thelikelihood model $\{ p(\cdot \mid \theta) \}_{\theta \in \Omega}$ is anexponential family of distributions, the OT-based denoiser can be recoveredsolely from the marginal distribution of $Z$. In general, our family of OT-likerelaxations is of interest in its own right and for the denoising problemsuggests alternative numerical methods inspired by the rich literature oncomputational OT.</description><author>Nicolas Garcia Trillos, Bodhisattva Sen</author><pubDate>Wed, 13 Dec 2023 13:36:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08135v1</guid></item><item><title>MToP: A MATLAB Optimization Platform for Evolutionary Multitasking</title><link>http://arxiv.org/abs/2312.08134v1</link><description>Evolutionary multitasking (EMT) has been attracting much attention over thepast years. It aims to handle multiple optimization tasks simultaneously withinlimited computing resources assisted by inter-task knowledge transfertechniques. Numerous multitask evolutionary algorithms (MTEAs) for solvingmultitask optimization (MTO) problems have been proposed in the EMT field, butthere lacks a comprehensive software platform to help researchers evaluate MTEAperformance on benchmark MTO problems as well as explore real-worldapplications. To address this issue, we introduce the first open-sourceoptimization platform, named MTO-Platform (MToP), for EMT. It incorporates morethan 30 MTEAs, more than 150 MTO problem cases with real-world applications,and more than 10 performance metrics. Moreover, for comparing MTEAs withtraditional evolutionary algorithms, we modified more than 30 popularsingle-task evolutionary algorithms to be able to solve MTO problems in MToP.MToP is a user-friendly tool with a graphical user interface that makes it easyto analyze results, export data, and plot schematics. More importantly, MToP isextensible, allowing users to develop new algorithms and define new problems.The source code of MToP is available at https://github.com/intLyc/MTO-Platform.</description><author>Yanchi Li, Wenyin Gong, Fei Ming, Tingyu Zhang, Shuijia Li, Qiong Gu</author><pubDate>Wed, 13 Dec 2023 13:36:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08134v1</guid></item><item><title>Ultra Low Complexity Deep Learning Based Noise Suppression</title><link>http://arxiv.org/abs/2312.08132v1</link><description>This paper introduces an innovative method for reducing the computationalcomplexity of deep neural networks in real-time speech enhancement onresource-constrained devices. The proposed approach utilizes a two-stageprocessing framework, employing channelwise feature reorientation to reduce thecomputational load of convolutional operations. By combining this with amodified power law compression technique for enhanced perceptual quality, thisapproach achieves noise suppression performance comparable to state-of-the-artmethods with significantly less computational requirements. Notably, ouralgorithm exhibits 3 to 4 times less computational complexity and memory usagethan prior state-of-the-art approaches.</description><author>Shrishti Saha Shetu, Soumitro Chakrabarty, Oliver Thiergart, Edwin Mabande</author><pubDate>Wed, 13 Dec 2023 13:34:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08132v1</guid></item><item><title>Clockwork Diffusion: Efficient Generation With Model-Step Distillation</title><link>http://arxiv.org/abs/2312.08128v1</link><description>This work aims to improve the efficiency of text-to-image diffusion models.While diffusion models use computationally expensive UNet-based denoisingoperations in every generation step, we identify that not all operations areequally relevant for the final output quality. In particular, we observe thatUNet layers operating on high-res feature maps are relatively sensitive tosmall perturbations. In contrast, low-res feature maps influence the semanticlayout of the final image and can often be perturbed with no noticeable changein the output. Based on this observation, we propose Clockwork Diffusion, amethod that periodically reuses computation from preceding denoising steps toapproximate low-res feature maps at one or more subsequent steps. For multiplebaselines, and for both text-to-image generation and image editing, wedemonstrate that Clockwork leads to comparable or improved perceptual scoreswith drastically reduced computational complexity. As an example, for StableDiffusion v1.5 with 8 DPM++ steps we save 32% of FLOPs with negligible FID andCLIP change.</description><author>Amirhossein Habibian, Amir Ghodrati, Noor Fathima, Guillaume Sautiere, Risheek Garrepalli, Fatih Porikli, Jens Petersen</author><pubDate>Wed, 13 Dec 2023 13:30:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08128v1</guid></item><item><title>GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting</title><link>http://arxiv.org/abs/2311.14521v3</link><description>3D editing plays a crucial role in many areas such as gaming and virtualreality. Traditional 3D editing methods, which rely on representations likemeshes and point clouds, often fall short in realistically depicting complexscenes. On the other hand, methods based on implicit 3D representations, likeNeural Radiance Field (NeRF), render complex scenes effectively but suffer fromslow processing speeds and limited control over specific scene areas. Inresponse to these challenges, our paper presents GaussianEditor, an innovativeand efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3Drepresentation. GaussianEditor enhances precision and control in editingthrough our proposed Gaussian semantic tracing, which traces the editing targetthroughout the training process. Additionally, we propose Hierarchical Gaussiansplatting (HGS) to achieve stabilized and fine results under stochasticgenerative guidance from 2D diffusion models. We also develop editingstrategies for efficient object removal and integration, a challenging task forexisting methods. Our comprehensive experiments demonstrate GaussianEditor'ssuperior control, efficacy, and rapid performance, marking a significantadvancement in 3D editing. Project Page:https://buaacyw.github.io/gaussian-editor/</description><author>Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, Guosheng Lin</author><pubDate>Wed, 13 Dec 2023 13:29:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14521v3</guid></item><item><title>Norm Tweaking: High-performance Low-bit Quantization of Large Language Models</title><link>http://arxiv.org/abs/2309.02784v2</link><description>As the size of large language models (LLMs) continues to grow, modelcompression without sacrificing accuracy has become a crucial challenge fordeployment. While some quantization methods, such as GPTQ, have made progressin achieving acceptable 4-bit weight-only quantization, attempts at lower-bitquantization often result in severe performance degradation. In this paper, weintroduce a technique called norm tweaking, which can be used as a plugin incurrent PTQ methods to achieve high precision while being cost-efficient. Ourapproach is inspired by the observation that rectifying the quantizedactivation distribution to match its float counterpart can readily restoreaccuracy for LLMs. To achieve this, we carefully design a tweaking strategythat includes calibration data generation and channel-wise distance constraintto update the weights of normalization layers for better generalization. Weconduct extensive experiments on various datasets using several open-sourcedLLMs. Our method demonstrates significant improvements in both weight-onlyquantization and joint quantization of weights and activations, surpassingexisting PTQ methods. On GLM-130B and OPT-66B, our method even achieves thesame level of accuracy at 2-bit quantization as their float ones. Our simpleand effective approach makes it more practical for real-world applications.</description><author>Liang Li, Qingyuan Li, Bo Zhang, Xiangxiang Chu</author><pubDate>Wed, 13 Dec 2023 13:29:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02784v2</guid></item><item><title>Ensemble Reinforcement Learning: A Survey</title><link>http://arxiv.org/abs/2303.02618v3</link><description>Reinforcement Learning (RL) has emerged as a highly effective technique foraddressing various scientific and applied problems. Despite its success,certain complex tasks remain challenging to be addressed solely with a singlemodel and algorithm. In response, ensemble reinforcement learning (ERL), apromising approach that combines the benefits of both RL and ensemble learning(EL), has gained widespread popularity. ERL leverages multiple models ortraining algorithms to comprehensively explore the problem space and possessesstrong generalization capabilities. In this study, we present a comprehensivesurvey on ERL to provide readers with an overview of recent advances andchallenges in the field. Firstly, we provide an introduction to the backgroundand motivation for ERL. Secondly, we conduct a detailed analysis of strategiessuch as model selection and combination that have been successfully implementedin ERL. Subsequently, we explore the application of ERL, summarize thedatasets, and analyze the algorithms employed. Finally, we outline several openquestions and discuss future research directions of ERL. By offering guidancefor future scientific research and engineering applications, this surveysignificantly contributes to the advancement of ERL.</description><author>Yanjie Song, P. N. Suganthan, Witold Pedrycz, Junwei Ou, Yongming He, Yingwu Chen, Yutong Wu</author><pubDate>Wed, 13 Dec 2023 13:27:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02618v3</guid></item></channel></rss>