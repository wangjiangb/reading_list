<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 30 Sep 2024 18:50:29 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation</title><link>http://arxiv.org/abs/2409.18964v1</link><description>We present PhysGen, a novel image-to-video generation method that converts asingle image and an input condition (e.g., force and torque applied to anobject in the image) to produce a realistic, physically plausible, andtemporally consistent video. Our key insight is to integrate model-basedphysical simulation with a data-driven video generation process, enablingplausible image-space dynamics. At the heart of our system are three corecomponents: (i) an image understanding module that effectively captures thegeometry, materials, and physical parameters of the image; (ii) an image-spacedynamics simulation model that utilizes rigid-body physics and inferredparameters to simulate realistic behaviors; and (iii) an image-based renderingand refinement module that leverages generative video diffusion to producerealistic video footage featuring the simulated motion. The resulting videosare realistic in both physics and appearance and are even preciselycontrollable, showcasing superior results over existing data-drivenimage-to-video generation works through quantitative comparison andcomprehensive user study. PhysGen's resulting videos can be used for variousdownstream applications, such as turning an image into a realistic animation orallowing users to interact with the image and create various dynamics. Projectpage: https://stevenlsw.github.io/physgen/</description><author>Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, Shenlong Wang</author><pubDate>Fri, 27 Sep 2024 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18964v1</guid></item><item><title>Exploring Token Pruning in Vision State Space Models</title><link>http://arxiv.org/abs/2409.18962v1</link><description>State Space Models (SSMs) have the advantage of keeping linear computationalcomplexity compared to attention modules in transformers, and have been appliedto vision tasks as a new type of powerful vision foundation model. Inspired bythe observations that the final prediction in vision transformers (ViTs) isonly based on a subset of most informative tokens, we take the novel step ofenhancing the efficiency of SSM-based vision models through token-basedpruning. However, direct applications of existing token pruning techniquesdesigned for ViTs fail to deliver good performance, even with extensivefine-tuning. To address this issue, we revisit the unique computationalcharacteristics of SSMs and discover that naive application disrupts thesequential token positions. This insight motivates us to design a novel andgeneral token pruning method specifically for SSM-based vision models. We firstintroduce a pruning-aware hidden state alignment method to stabilize theneighborhood of remaining tokens for performance enhancement. Besides, based onour detailed analysis, we propose a token importance evaluation method adaptedfor SSM models, to guide the token pruning. With efficient implementation andpractical acceleration methods, our method brings actual speedup. Extensiveexperiments demonstrate that our approach can achieve significant computationreduction with minimal impact on performance across different tasks. Notably,we achieve 81.7\% accuracy on ImageNet with a 41.6\% reduction in the FLOPs forpruned PlainMamba-L3. Furthermore, our work provides deeper insights intounderstanding the behavior of SSM-based vision models for future research.</description><author>Zheng Zhan, Zhenglun Kong, Yifan Gong, Yushu Wu, Zichong Meng, Hangyu Zheng, Xuan Shen, Stratis Ioannidis, Wei Niu, Pu Zhao, Yanzhi Wang</author><pubDate>Fri, 27 Sep 2024 17:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18962v1</guid></item><item><title>ProMerge: Prompt and Merge for Unsupervised Instance Segmentation</title><link>http://arxiv.org/abs/2409.18961v1</link><description>Unsupervised instance segmentation aims to segment distinct object instancesin an image without relying on human-labeled data. This field has recently seensignificant advancements, partly due to the strong local correspondencesafforded by rich visual feature representations from self-supervised models(e.g., DINO). Recent state-of-the-art approaches use self-supervised featuresto represent images as graphs and solve a generalized eigenvalue system (i.e.,normalized-cut) to generate foreground masks. While effective, this strategy islimited by its attendant computational demands, leading to slow inferencespeeds. In this paper, we propose Prompt and Merge (ProMerge), which leveragesself-supervised visual features to obtain initial groupings of patches andapplies a strategic merging to these segments, aided by a sophisticatedbackground-based mask pruning technique. ProMerge not only yields competitiveresults but also offers a significant reduction in inference time compared tostate-of-the-art normalized-cut-based approaches. Furthermore, when training anobject detector using our mask predictions as pseudo-labels, the resultingdetector surpasses the current leading unsupervised model on variouschallenging instance segmentation benchmarks.</description><author>Dylan Li, Gyungin Shin</author><pubDate>Fri, 27 Sep 2024 17:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18961v1</guid></item><item><title>$O(d/T)$ Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions</title><link>http://arxiv.org/abs/2409.18959v1</link><description>Score-based diffusion models, which generate new data by learning to reversea diffusion process that perturbs data from the target distribution into noise,have achieved remarkable success across various generative tasks. Despite theirsuperior empirical performance, existing theoretical guarantees are oftenconstrained by stringent assumptions or suboptimal convergence rates. In thispaper, we establish a fast convergence theory for a popular SDE-based samplerunder minimal assumptions. Our analysis shows that, provided$\ell_{2}$-accurate estimates of the score functions, the total variationdistance between the target and generated distributions is upper bounded by$O(d/T)$ (ignoring logarithmic factors), where $d$ is the data dimensionalityand $T$ is the number of steps. This result holds for any target distributionwith finite first-order moment. To our knowledge, this improves upon existingconvergence theory for both the SDE-based sampler and another ODE-basedsampler, while imposing minimal assumptions on the target data distribution andscore estimates. This is achieved through a novel set of analytical tools thatprovides a fine-grained characterization of how the error propagates at eachstep of the reverse process.</description><author>Gen Li, Yuling Yan</author><pubDate>Fri, 27 Sep 2024 17:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18959v1</guid></item><item><title>LML: Language Model Learning a Dataset for Data-Augmented Prediction</title><link>http://arxiv.org/abs/2409.18957v1</link><description>This paper introduces a new approach to using Large Language Models (LLMs)for classification tasks, which are typically handled using Machine Learning(ML) models. Unlike ML models that rely heavily on data cleaning and featureengineering, this method streamlines the process using LLMs. This paperproposes a new concept called "Language Model Learning (LML)" powered by a newmethod called "Data-Augmented Prediction (DAP)". The classification isperformed by LLMs using a method similar to humans manually exploring andunderstanding the data and deciding classifications using data as a reference.Training data is summarized and evaluated to determine the features that leadto the classification of each label the most. In the process of DAP, the systemuses the data summary to automatically create a query, which is used toretrieve relevant rows from the dataset. A classification is generated by theLLM using data summary and relevant rows, ensuring satisfactory accuracy evenwith complex data. Usage of data summary and similar data in DAP ensurescontext-aware decision-making. The proposed method uses the words "Act as anExplainable Machine Learning Model" in the prompt to enhance theinterpretability of the predictions by allowing users to review the logicbehind each prediction. In some test cases, the system scored an accuracy above90%, proving the effectiveness of the system and its potential to outperformconventional ML models in various scenarios. The code is available athttps://github.com/Pro-GenAI/LML-DAP</description><author>Praneeth Vadlapati</author><pubDate>Fri, 27 Sep 2024 17:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18957v1</guid></item><item><title>UniCal: Unified Neural Sensor Calibration</title><link>http://arxiv.org/abs/2409.18953v1</link><description>Self-driving vehicles (SDVs) require accurate calibration of LiDARs andcameras to fuse sensor data accurately for autonomy. Traditional calibrationmethods typically leverage fiducials captured in a controlled and structuredscene and compute correspondences to optimize over. These approaches are costlyand require substantial infrastructure and operations, making it challenging toscale for vehicle fleets. In this work, we propose UniCal, a unified frameworkfor effortlessly calibrating SDVs equipped with multiple LiDARs and cameras.Our approach is built upon a differentiable scene representation capable ofrendering multi-view geometrically and photometrically consistent sensorobservations. We jointly learn the sensor calibration and the underlying scenerepresentation through differentiable volume rendering, utilizing outdoorsensor data without the need for specific calibration fiducials. This"drive-and-calibrate" approach significantly reduces costs and operationaloverhead compared to existing calibration systems, enabling efficientcalibration for large SDV fleets at scale. To ensure geometric consistencyacross observations from different sensors, we introduce a novel surfacealignment loss that combines feature-based registration with neural rendering.Comprehensive evaluations on multiple datasets demonstrate that UniCaloutperforms or matches the accuracy of existing calibration approaches whilebeing more efficient, demonstrating the value of UniCal for scalablecalibration.</description><author>Ze Yang, George Chen, Haowei Zhang, Kevin Ta, Ioan Andrei Bârsan, Daniel Murphy, Sivabalan Manivasagam, Raquel Urtasun</author><pubDate>Fri, 27 Sep 2024 17:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18953v1</guid></item><item><title>RepairBench: Leaderboard of Frontier Models for Program Repair</title><link>http://arxiv.org/abs/2409.18952v1</link><description>AI-driven program repair uses AI models to repair buggy software by producingpatches. Rapid advancements in AI surely impact state-of-the-art performance ofprogram repair. Yet, grasping this progress requires frequent and standardizedevaluations. We propose RepairBench, a novel leaderboard for AI-driven programrepair. The key characteristics of RepairBench are: 1) it is execution-based:all patches are compiled and executed against a test suite, 2) it assessesfrontier models in a frequent and standardized way. RepairBench leverages twohigh-quality benchmarks, Defects4J and GitBug-Java, to evaluate frontier modelsagainst real-world program repair tasks. We publicly release the evaluationframework of RepairBench. We will update the leaderboard as new frontier modelsare released.</description><author>André Silva, Martin Monperrus</author><pubDate>Fri, 27 Sep 2024 17:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18952v1</guid></item><item><title>Spectral Wavelet Dropout: Regularization in the Wavelet Domain</title><link>http://arxiv.org/abs/2409.18951v1</link><description>Regularization techniques help prevent overfitting and therefore improve theability of convolutional neural networks (CNNs) to generalize. One reason foroverfitting is the complex co-adaptations among different parts of the network,which make the CNN dependent on their joint response rather than encouragingeach part to learn a useful feature representation independently. Frequencydomain manipulation is a powerful strategy for modifying data that has temporaland spatial coherence by utilizing frequency decomposition. This workintroduces Spectral Wavelet Dropout (SWD), a novel regularization method thatincludes two variants: 1D-SWD and 2D-SWD. These variants improve CNNgeneralization by randomly dropping detailed frequency bands in the discretewavelet decomposition of feature maps. Our approach distinguishes itself fromthe pre-existing Spectral "Fourier" Dropout (2D-SFD), which eliminatescoefficients in the Fourier domain. Notably, SWD requires only a singlehyperparameter, unlike the two required by SFD. We also extend the literatureby implementing a one-dimensional version of Spectral "Fourier" Dropout(1D-SFD), setting the stage for a comprehensive comparison. Our evaluationshows that both 1D and 2D SWD variants have competitive performance onCIFAR-10/100 benchmarks relative to both 1D-SFD and 2D-SFD. Specifically,1D-SWD has a significantly lower computational complexity compared to1D/2D-SFD. In the Pascal VOC Object Detection benchmark, SWD variants surpass1D-SFD and 2D-SFD in performance and demonstrate lower computational complexityduring training.</description><author>Rinor Cakaj, Jens Mehnert, Bin Yang</author><pubDate>Fri, 27 Sep 2024 17:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18951v1</guid></item><item><title>Unconditional stability of a recurrent neural circuit implementing divisive normalization</title><link>http://arxiv.org/abs/2409.18946v1</link><description>Stability in recurrent neural models poses a significant challenge,particularly in developing biologically plausible neurodynamical models thatcan be seamlessly trained. Traditional cortical circuit models are notoriouslydifficult to train due to expansive nonlinearities in the dynamical system,leading to an optimization problem with nonlinear stability constraints thatare difficult to impose. Conversely, recurrent neural networks (RNNs) excel intasks involving sequential data but lack biological plausibility andinterpretability. In this work, we address these challenges by linking dynamicdivisive normalization (DN) to the stability of ORGaNICs, a biologicallyplausible recurrent cortical circuit model that dynamically achieves DN and hasbeen shown to simulate a wide range of neurophysiological phenomena. By usingthe indirect method of Lyapunov, we prove the remarkable property ofunconditional local stability for an arbitrary-dimensional ORGaNICs circuitwhen the recurrent weight matrix is the identity. We thus connect ORGaNICs to asystem of coupled damped harmonic oscillators, which enables us to derive thecircuit's energy function, providing a normative principle of what the circuit,and individual neurons, aim to accomplish. Further, for a generic recurrentweight matrix, we prove the stability of the 2D model and demonstrateempirically that stability holds in higher dimensions. Finally, we show thatORGaNICs can be trained by backpropagation through time without gradientclipping/scaling, thanks to its intrinsic stability property and adaptive timeconstants, which address the problems of exploding, vanishing, and oscillatinggradients. By evaluating the model's performance on RNN benchmarks, we findthat ORGaNICs outperform alternative neurodynamical models on static imageclassification tasks and perform comparably to LSTMs on sequential tasks.</description><author>Shivang Rawat, David J. Heeger, Stefano Martiniani</author><pubDate>Fri, 27 Sep 2024 17:46:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18946v1</guid></item><item><title>Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models</title><link>http://arxiv.org/abs/2409.18943v1</link><description>The instruction-following ability of large language models enables humans tointeract with AI agents in a natural way. However, when required to generateresponses of a specific length, large language models often struggle to meetusers' needs due to their inherent difficulty in accurately perceivingnumerical constraints. To explore the ability of large language models tocontrol the length of generated responses, we propose the Target LengthGeneration Task (TLG) and design two metrics, Precise Match (PM) and FlexibleMatch (FM) to evaluate the model's performance in adhering to specifiedresponse lengths. Furthermore, we introduce a novel, model-agnostic approachcalled Ruler, which employs Meta Length Tokens (MLTs) to enhance theinstruction-following ability of large language models under length-constrainedinstructions. Specifically, Ruler equips LLMs with the ability to generateresponses of a specified length based on length constraints within theinstructions. Moreover, Ruler can automatically generate appropriate MLT whenlength constraints are not explicitly provided, demonstrating excellentversatility and generalization. Comprehensive experiments show theeffectiveness of Ruler across different LLMs on Target Length Generation Task,e.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. Inaddition, we conduct extensive ablation experiments to further substantiate theefficacy and generalization of Ruler. Our code and data is available athttps://github.com/Geaming2002/Ruler.</description><author>Jiaming Li, Lei Zhang, Yunshui Li, Ziqiang Liu, yuelin bai, Run Luo, Longze Chen, Min Yang</author><pubDate>Fri, 27 Sep 2024 17:44:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18943v1</guid></item><item><title>Building Trust Through Voice: How Vocal Tone Impacts User Perception of Attractiveness of Voice Assistants</title><link>http://arxiv.org/abs/2409.18941v1</link><description>Voice Assistants (VAs) are popular for simple tasks, but users are oftenhesitant to use them for complex activities like online shopping. We exploredwhether the vocal characteristics like the VA's vocal tone, can make VAsperceived as more attractive and trustworthy to users for complex tasks. Ourfindings show that the tone of the VA voice significantly impacts its perceivedattractiveness and trustworthiness. Participants in our experiment were morelikely to be attracted to VAs with positive or neutral tones and ultimatelytrusted the VAs they found more attractive. We conclude that VA's perceivedtrustworthiness can be enhanced through thoughtful voice design, incorporatinga variety of vocal tones.</description><author>Sabid Bin Habib Pias, Alicia Freel, Ran Huang, Donald Williamson, Minjeong Kim, Apu Kapadia</author><pubDate>Fri, 27 Sep 2024 17:41:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18941v1</guid></item><item><title>From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding</title><link>http://arxiv.org/abs/2409.18938v1</link><description>The integration of Large Language Models (LLMs) with visual encoders hasrecently shown promising performance in visual understanding tasks, leveragingtheir inherent capability to comprehend and generate human-like text for visualreasoning. Given the diverse nature of visual data, MultiModal Large LanguageModels (MM-LLMs) exhibit variations in model designing and training forunderstanding images, short videos, and long videos. Our paper focuses on thesubstantial differences and unique challenges posed by long video understandingcompared to static image and short video understanding. Unlike static images,short videos encompass sequential frames with both spatial and within-eventtemporal information, while long videos consist of multiple events withbetween-event and long-term temporal information. In this survey, we aim totrace and summarize the advancements of MM-LLMs from image understanding tolong video understanding. We review the differences among various visualunderstanding tasks and highlight the challenges in long video understanding,including more fine-grained spatiotemporal details, dynamic events, andlong-term dependencies. We then provide a detailed summary of the advancementsin MM-LLMs in terms of model design and training methodologies forunderstanding long videos. Finally, we compare the performance of existingMM-LLMs on video understanding benchmarks of various lengths and discusspotential future directions for MM-LLMs in long video understanding.</description><author>Heqing Zou, Tianze Luo, Guiyang Xie, Victor, Zhang, Fengmao Lv, Guangcong Wang, Juanyang Chen, Zhuochen Wang, Hansheng Zhang, Huaijian Zhang</author><pubDate>Fri, 27 Sep 2024 17:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18938v1</guid></item><item><title>SpaRED benchmark: Enhancing Gene Expression Prediction from Histology Images with Spatial Transcriptomics Completion</title><link>http://arxiv.org/abs/2407.13027v2</link><description>Spatial Transcriptomics is a novel technology that aligns histology imageswith spatially resolved gene expression profiles. Although groundbreaking, itstruggles with gene capture yielding high corruption in acquired data. Givenpotential applications, recent efforts have focused on predictingtranscriptomic profiles solely from histology images. However, differences indatabases, preprocessing techniques, and training hyperparameters hinder a faircomparison between methods. To address these challenges, we present asystematically curated and processed database collected from 26 public sources,representing an 8.6-fold increase compared to previous works. Additionally, wepropose a state-of-the-art transformer based completion technique for inferringmissing gene expression, which significantly boosts the performance oftranscriptomic profile predictions across all datasets. Altogether, ourcontributions constitute the most comprehensive benchmark of gene expressionprediction from histology images to date and a stepping stone for futureresearch on spatial transcriptomics.</description><author>Gabriel Mejia, Daniela Ruiz, Paula Cárdenas, Leonardo Manrique, Daniela Vega, Pablo Arbeláez</author><pubDate>Fri, 27 Sep 2024 17:29:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13027v2</guid></item><item><title>On Rademacher Complexity-based Generalization Bounds for Deep Learning</title><link>http://arxiv.org/abs/2208.04284v3</link><description>We show that the Rademacher complexity-based approach can generatenon-vacuous generalisation bounds on Convolutional Neural Networks (CNNs) forclassifying a small number of classes of images. The development of newTalagrand's contraction lemmas for high-dimensional mappings between functionspaces and CNNs for general Lipschitz activation functions is a key technicalcontribution. Our results show that the Rademacher complexity does not dependon the network length for CNNs with some special types of activation functionssuch as ReLU, Leaky ReLU, Parametric Rectifier Linear Unit, Sigmoid, and Tanh.</description><author>Lan V. Truong</author><pubDate>Fri, 27 Sep 2024 17:29:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.04284v3</guid></item><item><title>ReviveDiff: A Universal Diffusion Model for Restoring Images in Adverse Weather Conditions</title><link>http://arxiv.org/abs/2409.18932v1</link><description>Images captured in challenging environments--such as nighttime, foggy, rainyweather, and underwater--often suffer from significant degradation, resultingin a substantial loss of visual quality. Effective restoration of thesedegraded images is critical for the subsequent vision tasks. While manyexisting approaches have successfully incorporated specific priors forindividual tasks, these tailored solutions limit their applicability to otherdegradations. In this work, we propose a universal network architecture, dubbed"ReviveDiff", which can address a wide range of degradations and bring imagesback to life by enhancing and restoring their quality. Our approach is inspiredby the observation that, unlike degradation caused by movement or electronicissues, quality degradation under adverse conditions primarily stems fromnatural media (such as fog, water, and low luminance), which generallypreserves the original structures of objects. To restore the quality of suchimages, we leveraged the latest advancements in diffusion models and developedReviveDiff to restore image quality from both macro and micro levels acrosssome key factors determining image quality, such as sharpness, distortion,noise level, dynamic range, and color accuracy. We rigorously evaluatedReviveDiff on seven benchmark datasets covering five types of degradingconditions: Rainy, Underwater, Low-light, Smoke, and Nighttime Hazy. Ourexperimental results demonstrate that ReviveDiff outperforms thestate-of-the-art methods both quantitatively and visually.</description><author>Wenfeng Huang, Guoan Xu, Wenjing Jia, Stuart Perry, Guangwei Gao</author><pubDate>Fri, 27 Sep 2024 17:29:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18932v1</guid></item><item><title>AIPatient: Simulating Patients with EHRs and LLM Powered Agentic Workflow</title><link>http://arxiv.org/abs/2409.18924v1</link><description>Simulated patient systems play a crucial role in modern medical education andresearch, providing safe, integrative learning environments and enablingclinical decision-making simulations. Large Language Models (LLM) could advancesimulated patient systems by replicating medical conditions and patient-doctorinteractions with high fidelity and low cost. However, ensuring theeffectiveness and trustworthiness of these systems remains a challenge, as theyrequire a large, diverse, and precise patient knowledgebase, along with arobust and stable knowledge diffusion to users. Here, we developed AIPatient,an advanced simulated patient system with AIPatient Knowledge Graph (AIPatientKG) as the input and the Reasoning Retrieval-Augmented Generation (ReasoningRAG) agentic workflow as the generation backbone. AIPatient KG samples datafrom Electronic Health Records (EHRs) in the Medical Information Mart forIntensive Care (MIMIC)-III database, producing a clinically diverse andrelevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).Reasoning RAG leverages six LLM powered agents spanning tasks includingretrieval, KG query generation, abstraction, checker, rewrite, andsummarization. This agentic framework reaches an overall accuracy of 94.15% inEHR-based medical Question Answering (QA), outperforming benchmarks that useeither no agent or only partial agent integration. Our system also presentshigh readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade5.6), robustness (ANOVA F-value 0.6126, p&lt;0.1), and stability (ANOVA F-value0.782, p&lt;0.1). The promising performance of the AIPatient system highlights itspotential to support a wide range of applications, including medical education,model evaluation, and system integration.</description><author>Huizi Yu, Jiayan Zhou, Lingyao Li, Shan Chen, Jack Gallifant, Anye Shi, Xiang Li, Wenyue Hua, Mingyu Jin, Guang Chen, Yang Zhou, Zhao Li, Trisha Gupte, Ming-Li Chen, Zahra Azizi, Yongfeng Zhang, Themistocles L. Assimes, Xin Ma, Danielle S. Bitterman, Lin Lu, Lizhou Fan</author><pubDate>Fri, 27 Sep 2024 17:17:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18924v1</guid></item><item><title>Proprioception Is All You Need: Terrain Classification for Boreal Forests</title><link>http://arxiv.org/abs/2403.16877v2</link><description>Recent works in field robotics highlighted the importance of resiliencyagainst different types of terrains. Boreal forests, in particular, are home tomany mobility-impeding terrains that should be considered for off-roadautonomous navigation. Also, being one of the largest land biomes on Earth,boreal forests are an area where autonomous vehicles are expected to becomeincreasingly common. In this paper, we address this issue by introducingBorealTC, a publicly available dataset for proprioceptive-based terrainclassification (TC). Recorded with a Husky A200, our dataset contains 116 minof Inertial Measurement Unit (IMU), motor current, and wheel odometry data,focusing on typical boreal forest terrains, notably snow, ice, and silty loam.Combining our dataset with another dataset from the state-of-the-art, weevaluate both a Convolutional Neural Network (CNN) and the novel state spacemodel (SSM)-based Mamba architecture on a TC task. Interestingly, we show thatwhile CNN outperforms Mamba on each separate dataset, Mamba achieves greateraccuracy when trained on a combination of both. In addition, we demonstratethat Mamba's learning capacity is greater than a CNN for increasing amounts ofdata. We show that the combination of two TC datasets yields a latent spacethat can be interpreted with the properties of the terrains. We also discussthe implications of merging datasets on classification. Our source code anddataset are publicly available online:https://github.com/norlab-ulaval/BorealTC.</description><author>Damien LaRocque, William Guimont-Martin, David-Alexandre Duclos, Philippe Giguère, François Pomerleau</author><pubDate>Fri, 27 Sep 2024 17:14:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16877v2</guid></item><item><title>SurfaceAI: Automated creation of cohesive road surface quality datasets based on open street-level imagery</title><link>http://arxiv.org/abs/2409.18922v1</link><description>This paper introduces SurfaceAI, a pipeline designed to generatecomprehensive georeferenced datasets on road surface type and quality fromopenly available street-level imagery. The motivation stems from thesignificant impact of road unevenness on the safety and comfort of trafficparticipants, especially vulnerable road users, emphasizing the need fordetailed road surface data in infrastructure modeling and analysis. SurfaceAIaddresses this gap by leveraging crowdsourced Mapillary data to train modelsthat predict the type and quality of road surfaces visible in street-levelimages, which are then aggregated to provide cohesive information on entireroad segment conditions.</description><author>Alexandra Kapp, Edith Hoffmann, Esther Weigmann, Helena Mihaljević</author><pubDate>Fri, 27 Sep 2024 17:13:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18922v1</guid></item><item><title>A-FedPD: Aligning Dual-Drift is All Federated Primal-Dual Learning Needs</title><link>http://arxiv.org/abs/2409.18915v1</link><description>As a popular paradigm for juggling data privacy and collaborative training,federated learning (FL) is flourishing to distributively process the largescale of heterogeneous datasets on edged clients. Due to bandwidth limitationsand security considerations, it ingeniously splits the original problem intomultiple subproblems to be solved in parallel, which empowers primal dualsolutions to great application values in FL. In this paper, we review therecent development of classical federated primal dual methods and point out aserious common defect of such methods in non-convex scenarios, which we say isa "dual drift" caused by dual hysteresis of those longstanding inactive clientsunder partial participation training. To further address this problem, wepropose a novel Aligned Federated Primal Dual (A-FedPD) method, whichconstructs virtual dual updates to align global consensus and local dualvariables for those protracted unparticipated local clients. Meanwhile, weprovide a comprehensive analysis of the optimization and generalizationefficiency for the A-FedPD method on smooth non-convex objectives, whichconfirms its high efficiency and practicality. Extensive experiments areconducted on several classical FL setups to validate the effectiveness of ourproposed method.</description><author>Yan Sun, Li Shen, Dacheng Tao</author><pubDate>Fri, 27 Sep 2024 17:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18915v1</guid></item><item><title>Soft Measures for Extracting Causal Collective Intelligence</title><link>http://arxiv.org/abs/2409.18911v1</link><description>Understanding and modeling collective intelligence is essential foraddressing complex social systems. Directed graphs called fuzzy cognitive maps(FCMs) offer a powerful tool for encoding causal mental models, but extractinghigh-integrity FCMs from text is challenging. This study presents an approachusing large language models (LLMs) to automate FCM extraction. We introducenovel graph-based similarity measures and evaluate them by correlating theiroutputs with human judgments through the Elo rating system. Results showpositive correlations with human evaluations, but even the best-performingmeasure exhibits limitations in capturing FCM nuances. Fine-tuning LLMsimproves performance, but existing measures still fall short. This studyhighlights the need for soft similarity measures tailored to FCM extraction,advancing collective intelligence modeling with NLP.</description><author>Maryam Berijanian, Spencer Dork, Kuldeep Singh, Michael Riley Millikan, Ashlin Riggs, Aadarsh Swaminathan, Sarah L. Gibbs, Scott E. Friedman, Nathan Brugnone</author><pubDate>Fri, 27 Sep 2024 16:54:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18911v1</guid></item><item><title>Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code</title><link>http://arxiv.org/abs/2402.09299v2</link><description>Code auditing ensures that the developed code adheres to standards,regulations, and copyright protection by verifying that it does not containcode from protected sources. The recent advent of Large Language Models (LLMs)as coding assistants in the software development process poses new challengesfor code auditing. The dataset for training these models is mainly collectedfrom publicly available sources. This raises the issue of intellectual propertyinfringement as developers' codes are already included in the dataset.Therefore, auditing code developed using LLMs is challenging, as it isdifficult to reliably assert if an LLM used during development has been trainedon specific copyrighted codes, given that we do not have access to the trainingdatasets of these models. Given the non-disclosure of the training datasets,traditional approaches such as code clone detection are insufficient forasserting copyright infringement. To address this challenge, we propose a newapproach, TraWiC; a model-agnostic and interpretable method based on membershipinference for detecting code inclusion in an LLM's training dataset. We extractsyntactic and semantic identifiers unique to each program to train a classifierfor detecting code inclusion. In our experiments, we observe that TraWiC iscapable of detecting 83.87% of codes that were used to train an LLM. Incomparison, the prevalent clone detection tool NiCad is only capable ofdetecting 47.64%. In addition to its remarkable performance, TraWiC has lowresource overhead in contrast to pair-wise clone detection that is conductedduring the auditing process of tools like CodeWhisperer reference tracker,across thousands of code snippets.</description><author>Vahid Majdinasab, Amin Nikanjam, Foutse Khomh</author><pubDate>Fri, 27 Sep 2024 16:54:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09299v2</guid></item><item><title>Best Arm Identification with Minimal Regret</title><link>http://arxiv.org/abs/2409.18909v1</link><description>Motivated by real-world applications that necessitate responsibleexperimentation, we introduce the problem of best arm identification (BAI) withminimal regret. This innovative variant of the multi-armed bandit problemelegantly amalgamates two of its most ubiquitous objectives: regretminimization and BAI. More precisely, the agent's goal is to identify the bestarm with a prescribed confidence level $\delta$, while minimizing thecumulative regret up to the stopping time. Focusing on single-parameterexponential families of distributions, we leverage information-theoretictechniques to establish an instance-dependent lower bound on the expectedcumulative regret. Moreover, we present an intriguing impossibility result thatunderscores the tension between cumulative regret and sample complexity infixed-confidence BAI. Complementarily, we design and analyze the Double KL-UCBalgorithm, which achieves asymptotic optimality as the confidence level tendsto zero. Notably, this algorithm employs two distinct confidence bounds toguide arm selection in a randomized manner. Our findings elucidate a freshperspective on the inherent connections between regret minimization and BAI.</description><author>Junwen Yang, Vincent Y. F. Tan, Tianyuan Jin</author><pubDate>Fri, 27 Sep 2024 16:46:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18909v1</guid></item><item><title>In-depth Analysis of Privacy Threats in Federated Learning for Medical Data</title><link>http://arxiv.org/abs/2409.18907v1</link><description>Federated learning is emerging as a promising machine learning technique inthe medical field for analyzing medical images, as it is considered aneffective method to safeguard sensitive patient data and comply with privacyregulations. However, recent studies have revealed that the default settings offederated learning may inadvertently expose private training data to privacyattacks. Thus, the intensity of such privacy risks and potential mitigationstrategies in the medical domain remain unclear. In this paper, we make threeoriginal contributions to privacy risk analysis and mitigation in federatedlearning for medical data. First, we propose a holistic framework, MedPFL, foranalyzing privacy risks in processing medical data in the federated learningenvironment and developing effective mitigation strategies for protectingprivacy. Second, through our empirical analysis, we demonstrate the severeprivacy risks in federated learning to process medical images, whereadversaries can accurately reconstruct private medical images by performingprivacy attacks. Third, we illustrate that the prevalent defense mechanism ofadding random noises may not always be effective in protecting medical imagesagainst privacy attacks in federated learning, which poses unique and pressingchallenges related to protecting the privacy of medical data. Furthermore, thepaper discusses several unique research questions related to the privacyprotection of medical data in the federated learning environment. We conductextensive experiments on several benchmark medical image datasets to analyzeand mitigate the privacy risks associated with federated learning for medicaldata.</description><author>Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu</author><pubDate>Fri, 27 Sep 2024 16:45:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18907v1</guid></item><item><title>Probabilistic Analysis of Least Squares, Orthogonal Projection, and QR Factorization Algorithms Subject to Gaussian Noise</title><link>http://arxiv.org/abs/2409.18905v1</link><description>In this paper, we extend the work of Liesen et al. (2002), which analyzes howthe condition number of an orthonormal matrix Q changes when a column is added([Q, c]), particularly focusing on the perpendicularity of c to the span of Q.Their result, presented in Theorem 2.3 of Liesen et al. (2002), assumes exactarithmetic and orthonormality of Q, which is a strong assumption when applyingthese results to numerical methods such as QR factorization algorithms. In ourwork, we address this gap by deriving bounds on the condition number increasefor a matrix B without assuming perfect orthonormality, even when a column isnot perfectly orthogonal to the span of B. This framework allows us to analyzeQR factorization methods where orthogonalization is imperfect and subject toGaussian noise. We also provide results on the performance of orthogonalprojection and least squares under Gaussian noise, further supporting thedevelopment of this theory.</description><author>Ali Lotfi, Julien Langou, Mohammad Meysami</author><pubDate>Fri, 27 Sep 2024 16:44:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18905v1</guid></item><item><title>Improving Visual Object Tracking through Visual Prompting</title><link>http://arxiv.org/abs/2409.18901v1</link><description>Learning a discriminative model to distinguish a target from its surroundingdistractors is essential to generic visual object tracking. Dynamic targetrepresentation adaptation against distractors is challenging due to the limiteddiscriminative capabilities of prevailing trackers. We present a new visualPrompting mechanism for generic Visual Object Tracking (PiVOT) to address thisissue. PiVOT proposes a prompt generation network with the pre-trainedfoundation model CLIP to automatically generate and refine visual prompts,enabling the transfer of foundation model knowledge for tracking. While CLIPoffers broad category-level knowledge, the tracker, trained oninstance-specific data, excels at recognizing unique object instances. Thus,PiVOT first compiles a visual prompt highlighting potential target locations.To transfer the knowledge of CLIP to the tracker, PiVOT leverages CLIP torefine the visual prompt based on the similarities between candidate objectsand the reference templates across potential targets. Once the visual prompt isrefined, it can better highlight potential target locations, thereby reducingirrelevant prompt information. With the proposed prompting mechanism, thetracker can generate improved instance-aware feature maps through the guidanceof the visual prompt, thus effectively reducing distractors. The proposedmethod does not involve CLIP during training, thereby keeping the same trainingcomplexity and preserving the generalization capability of the pretrainedfoundation model. Extensive experiments across multiple benchmarks indicatethat PiVOT, using the proposed prompting method can suppress distractingobjects and enhance the tracker.</description><author>Shih-Fang Chen, Jun-Cheng Chen, I-Hong Jhuo, Yen-Yu Lin</author><pubDate>Fri, 27 Sep 2024 16:39:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18901v1</guid></item><item><title>Unsupervised Low-light Image Enhancement with Lookup Tables and Diffusion Priors</title><link>http://arxiv.org/abs/2409.18899v1</link><description>Low-light image enhancement (LIE) aims at precisely and efficientlyrecovering an image degraded in poor illumination environments. Recent advancedLIE techniques are using deep neural networks, which require lots of low-normallight image pairs, network parameters, and computational resources. As aresult, their practicality is limited. In this work, we devise a novelunsupervised LIE framework based on diffusion priors and lookup tables (DPLUT)to achieve efficient low-light image recovery. The proposed approach comprisestwo critical components: a light adjustment lookup table (LLUT) and a noisesuppression lookup table (NLUT). LLUT is optimized with a set of unsupervisedlosses. It aims at predicting pixel-wise curve parameters for the dynamic rangeadjustment of a specific image. NLUT is designed to remove the amplified noiseafter the light brightens. As diffusion models are sensitive to noise,diffusion priors are introduced to achieve high-performance noise suppression.Extensive experiments demonstrate that our approach outperformsstate-of-the-art methods in terms of visual quality and efficiency.</description><author>Yunlong Lin, Zhenqi Fu, Kairun Wen, Tian Ye, Sixiang Chen, Ge Meng, Yingying Wang, Yue Huang, Xiaotong Tu, Xinghao Ding</author><pubDate>Fri, 27 Sep 2024 16:37:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18899v1</guid></item><item><title>Detecting Dataset Abuse in Fine-Tuning Stable Diffusion Models for Text-to-Image Synthesis</title><link>http://arxiv.org/abs/2409.18897v1</link><description>Text-to-image synthesis has become highly popular for generating realisticand stylized images, often requiring fine-tuning generative models withdomain-specific datasets for specialized tasks. However, these valuabledatasets face risks of unauthorized usage and unapproved sharing, compromisingthe rights of the owners. In this paper, we address the issue of dataset abuseduring the fine-tuning of Stable Diffusion models for text-to-image synthesis.We present a dataset watermarking framework designed to detect unauthorizedusage and trace data leaks. The framework employs two key strategies acrossmultiple watermarking schemes and is effective for large-scale datasetauthorization. Extensive experiments demonstrate the framework's effectiveness,minimal impact on the dataset (only 2% of the data required to be modified forhigh detection accuracy), and ability to trace data leaks. Our results alsohighlight the robustness and transferability of the framework, proving itspractical applicability in detecting dataset abuse.</description><author>Songrui Wang, Yubo Zhu, Wei Tong, Sheng Zhong</author><pubDate>Fri, 27 Sep 2024 16:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18897v1</guid></item><item><title>S2O: Static to Openable Enhancement for Articulated 3D Objects</title><link>http://arxiv.org/abs/2409.18896v1</link><description>Despite much progress in large 3D datasets there are currently fewinteractive 3D object datasets, and their scale is limited due to the manualeffort required in their construction. We introduce the static to openable(S2O) task which creates interactive articulated 3D objects from staticcounterparts through openable part detection, motion prediction, and interiorgeometry completion. We formulate a unified framework to tackle this task, andcurate a challenging dataset of openable 3D objects that serves as a test bedfor systematic evaluation. Our experiments benchmark methods from prior workand simple yet effective heuristics for the S2O task. We find that turningstatic 3D objects into interactively openable counterparts is possible but thatall methods struggle to generalize to realistic settings of the task, and wehighlight promising future work directions.</description><author>Denys Iliash, Hanxiao Jiang, Yiming Zhang, Manolis Savva, Angel X. Chang</author><pubDate>Fri, 27 Sep 2024 16:34:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18896v1</guid></item><item><title>Multi-Source Hard and Soft Information Fusion Approach for Accurate Cryptocurrency Price Movement Prediction</title><link>http://arxiv.org/abs/2409.18895v1</link><description>One of the most important challenges in the financial and cryptocurrencyfield is accurately predicting cryptocurrency price trends. Leveragingartificial intelligence (AI) is beneficial in addressing this challenge.Cryptocurrency markets, marked by substantial growth and volatility, attractinvestors and scholars keen on deciphering and forecasting cryptocurrency pricemovements. The vast and diverse array of data available for such predictionsincreases the complexity of the task. In our study, we introduce a novelapproach termed hard and soft information fusion (HSIF) to enhance the accuracyof cryptocurrency price movement forecasts. The hard information component ofour approach encompasses historical price records alongside technicalindicators. Complementing this, the soft data component extracts from X(formerly Twitter), encompassing news headlines and tweets about thecryptocurrency. To use this data, we use the Bidirectional EncoderRepresentations from Transformers (BERT)-based sentiment analysis method,financial BERT (FinBERT), which performs best. Finally, our model feeds on theinformation set including processed hard and soft data. We employ thebidirectional long short-term memory (BiLSTM) model because processinginformation in both forward and backward directions can capture long-termdependencies in sequential information. Our empirical findings emphasize thesuperiority of the HSIF approach over models dependent on single-source data bytesting on Bitcoin-related data. By fusing hard and soft information on Bitcoindataset, our model has about 96.8\% accuracy in predicting price movement.Incorporating information enables our model to grasp the influence of socialsentiment on price fluctuations, thereby supplementing the technicalanalysis-based predictions derived from hard information.</description><author>Saeed Mohammadi Dashtaki, Mehdi Hosseini Chagahi, Behzad Moshiri, Md. Jalil Piran</author><pubDate>Fri, 27 Sep 2024 16:32:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18895v1</guid></item><item><title>HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models</title><link>http://arxiv.org/abs/2409.18893v1</link><description>Model merging is a technique that combines multiple large pretrained modelsinto a single model with enhanced performance and broader task adaptability. Ithas gained popularity in large pretrained model development due to its abilityto bypass the need for original training data and further training processes.However, most existing model merging approaches focus solely on exploring theparameter space, merging models with identical architectures. Merging withinthe architecture space, despite its potential, remains in its early stages dueto the vast search space and the challenges of layer compatibility. This papermarks a significant advance toward more flexible and comprehensive modelmerging techniques by modeling the architecture-space merging process as areinforcement learning task. We train policy and value networks using offlinesampling of weight vectors, which are then employed for the online optimizationof merging strategies. Moreover, a multi-objective optimization paradigm isintroduced to accommodate users' diverse task preferences, learning the Paretofront of optimal models to offer customized merging suggestions. Experimentalresults across multiple tasks, including text translation, mathematicalreasoning, and code generation, validate the effectiveness and superiority ofthe proposed framework in model merging. The code will be made publiclyavailable after the review process.</description><author>Yu Zhou, Xingyu Wu, Jibin Wu, Liang Feng, Kay Chen Tan</author><pubDate>Fri, 27 Sep 2024 16:31:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18893v1</guid></item><item><title>IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation</title><link>http://arxiv.org/abs/2409.18892v1</link><description>As Large Language Models (LLMs) grow increasingly adept at managing complextasks, the evaluation set must keep pace with these advancements to ensure itremains sufficiently discriminative. Item Discrimination (ID) theory, which iswidely used in educational assessment, measures the ability of individual testitems to differentiate between high and low performers. Inspired by thistheory, we propose an ID-induced prompt synthesis framework for evaluating LLMsto ensure the evaluation set can continually update and refine according tomodel abilities. Our data synthesis framework prioritizes both breadth andspecificity. It can generate prompts that comprehensively evaluate thecapabilities of LLMs while revealing meaningful performance differences betweenmodels, allowing for effective discrimination of their relative strengths andweaknesses across various tasks and domains. To produce high-quality data, weincorporate a self-correct mechanism into our generalization framework, anddevelop two models to predict prompt discrimination and difficulty score tofacilitate our data synthesis framework, contributing valuable tools toevaluation data synthesis research. We apply our generated data to evaluatefive SOTA models. Our data achieves an average score of 51.92, accompanied by avariance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT andWizardLM) obtain an average score exceeding 67, with a variance below 3.2. Theresults demonstrate that the data generated by our framework is morechallenging and discriminative compared to previous works. We will release adataset of over 3,000 carefully crafted prompts to facilitate evaluationresearch of LLMs.</description><author>Fan Lin, Shuyi Xie, Yong Dai, Wenlin Yao, Tianjiao Lang, Zishan Xu, Zhichao Hu, Xiao Xiao, Yuhong Liu, Yu Zhang</author><pubDate>Fri, 27 Sep 2024 16:29:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18892v1</guid></item><item><title>ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction</title><link>http://arxiv.org/abs/2402.00712v4</link><description>Accurate prediction of climate in the subseasonal-to-seasonal scale iscrucial for disaster preparedness and robust decision making amidst climatechange. Yet, forecasting beyond the weather timescale is challenging because itdeals with problems other than initial condition, including boundaryinteraction, butterfly effect, and our inherent lack of physical understanding.At present, existing benchmarks tend to have shorter forecasting range of up-to15 days, do not include a wide range of operational baselines, and lackphysics-based constraints for explainability. Thus, we propose ChaosBench, achallenging benchmark to extend the predictability range of data-driven weatheremulators to S2S timescale. First, ChaosBench is comprised of variables beyondthe typical surface-atmospheric ERA5 to also include ocean, ice, and landreanalysis products that span over 45 years to allow for full Earth systememulation that respects boundary conditions. We also propose physics-based, inaddition to deterministic and probabilistic metrics, to ensure aphysically-consistent ensemble that accounts for butterfly effect. Furthermore,we evaluate on a diverse set of physics-based forecasts from four nationalweather agencies as baselines to our data-driven counterpart such asViT/ClimaX, PanguWeather, GraphCast, and FourCastNetV2. Overall, we findmethods originally developed for weather-scale applications fail on S2S task:their performance simply collapse to an unskilled climatology. Nonetheless, weoutline and demonstrate several strategies that can extend the predictabilityrange of existing weather emulators, including the use of ensembles, robustcontrol of error propagation, and the use of physics-informed models. Ourbenchmark, datasets, and instructions are available athttps://leap-stc.github.io/ChaosBench.</description><author>Juan Nathaniel, Yongquan Qu, Tung Nguyen, Sungduk Yu, Julius Busecke, Aditya Grover, Pierre Gentine</author><pubDate>Fri, 27 Sep 2024 16:27:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00712v4</guid></item><item><title>M$^2$PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning</title><link>http://arxiv.org/abs/2409.15657v3</link><description>Multimodal Large Language Models (MLLMs) demonstrate remarkable performanceacross a wide range of domains, with increasing emphasis on enhancing theirzero-shot generalization capabilities for unseen tasks across variousmodalities. Instruction tuning has emerged as an effective strategy forachieving zero-shot generalization by finetuning pretrained models on diversemultimodal tasks. As the scale of MLLMs continues to grow, parameter-efficientfinetuning becomes increasingly critical. However, most existingparameter-efficient approaches focus only on single modalities and oftenoverlook the multimodal characteristics during finetuning. In this work, weintroduce a novel Multimodal Prompt Tuning (M$^2$PT) approach for efficientinstruction tuning of MLLMs. M$^2$PT effectively integrates visual and textualprompts into the vision encoder and language processor respectively duringfinetuning, facilitating the extraction and alignment of features acrossmodalities. Empirical results on various multimodal evaluation datasetsdemonstrate the superior performance of our approach compared to severalstate-of-the-art baselines. A comprehensive set of ablation studies validatesthe effectiveness of our prompt design and the efficiency of our approach.</description><author>Taowen Wang, Yiyang Liu, James Chenhao Liang, junhan zhao, Yiming Cui, Yuning Mao, Shaoliang Nie, Jiahao Liu, Fuli Feng, Zenglin Xu, Cheng Han, Lifu Huang, Qifan Wang, Dongfang Liu</author><pubDate>Fri, 27 Sep 2024 16:24:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15657v3</guid></item><item><title>HR-Extreme: A High-Resolution Dataset for Extreme Weather Forecasting</title><link>http://arxiv.org/abs/2409.18885v1</link><description>The application of large deep learning models in weather forecasting has ledto significant advancements in the field, including higher-resolutionforecasting and extended prediction periods exemplified by models such as Panguand Fuxi. Despite these successes, previous research has largely beencharacterized by the neglect of extreme weather events, and the availability ofdatasets specifically curated for such events remains limited. Given thecritical importance of accurately forecasting extreme weather, this studyintroduces a comprehensive dataset that incorporates high-resolution extremeweather cases derived from the High-Resolution Rapid Refresh (HRRR) data, a3-km real-time dataset provided by NOAA. We also evaluate the currentstate-of-the-art deep learning models and Numerical Weather Prediction (NWP)systems on HR-Extreme, and provide a improved baseline deep learning modelcalled HR-Heim which has superior performance on both general loss andHR-Extreme compared to others. Our results reveal that the errors of extremeweather cases are significantly larger than overall forecast error,highlighting them as an crucial source of loss in weather prediction. Thesefindings underscore the necessity for future research to focus on improving theaccuracy of extreme weather forecasts to enhance their practical utility.</description><author>Nian Ran, Peng Xiao, Yue Wang, Wesley Shi, Jianxin Lin, Qi Meng, Richard Allmendinger</author><pubDate>Fri, 27 Sep 2024 16:20:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18885v1</guid></item><item><title>Explainable Artifacts for Synthetic Western Blot Source Attribution</title><link>http://arxiv.org/abs/2409.18881v1</link><description>Recent advancements in artificial intelligence have enabled generative modelsto produce synthetic scientific images that are indistinguishable from pristineones, posing a challenge even for expert scientists habituated to working withsuch content. When exploited by organizations known as paper mills, whichsystematically generate fraudulent articles, these technologies cansignificantly contribute to the spread of misinformation about ungroundedscience, potentially undermining trust in scientific research. While previousstudies have explored black-box solutions, such as Convolutional NeuralNetworks, for identifying synthetic content, only some have addressed thechallenge of generalizing across different models and providing insight intothe artifacts in synthetic images that inform the detection process. This studyaims to identify explainable artifacts generated by state-of-the-art generativemodels (e.g., Generative Adversarial Networks and Diffusion Models) andleverage them for open-set identification and source attribution (i.e.,pointing to the model that created the image).</description><author>João Phillipe Cardenuto, Sara Mandelli, Daniel Moreira, Paolo Bestagini, Edward Delp, Anderson Rocha</author><pubDate>Fri, 27 Sep 2024 16:18:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18881v1</guid></item><item><title>Suicide Phenotyping from Clinical Notes in Safety-Net Psychiatric Hospital Using Multi-Label Classification with Pre-Trained Language Models</title><link>http://arxiv.org/abs/2409.18878v1</link><description>Accurate identification and categorization of suicidal events can yieldbetter suicide precautions, reducing operational burden, and improving carequality in high-acuity psychiatric settings. Pre-trained language models offerpromise for identifying suicidality from unstructured clinical narratives. Weevaluated the performance of four BERT-based models using two fine-tuningstrategies (multiple single-label and single multi-label) for detectingcoexisting suicidal events from 500 annotated psychiatric evaluation notes. Thenotes were labeled for suicidal ideation (SI), suicide attempts (SA), exposureto suicide (ES), and non-suicidal self-injury (NSSI). RoBERTa outperformedother models using binary relevance (acc=0.86, F1=0.78). MentalBERT (F1=0.74)also exceeded BioClinicalBERT (F1=0.72). RoBERTa fine-tuned with a singlemulti-label classifier further improved performance (acc=0.88, F1=0.81),highlighting that models pre-trained on domain-relevant data and the singlemulti-label classification strategy enhance efficiency and performance. Keywords: EHR-based Phynotyping; Natural Language Processing; Secondary Useof EHR Data; Suicide Classification; BERT-based Model; Psychiatry; MentalHealth</description><author>Zehan Li, Yan Hu, Scott Lane, Salih Selek, Lokesh Shahani, Rodrigo Machado-Vieira, Jair Soares, Hua Xu, Hongfang Liu, Ming Huang</author><pubDate>Fri, 27 Sep 2024 16:13:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18878v1</guid></item><item><title>UniEmoX: Cross-modal Semantic-Guided Large-Scale Pretraining for Universal Scene Emotion Perception</title><link>http://arxiv.org/abs/2409.18877v1</link><description>Visual emotion analysis holds significant research value in both computervision and psychology. However, existing methods for visual emotion analysissuffer from limited generalizability due to the ambiguity of emotion perceptionand the diversity of data scenarios. To tackle this issue, we introduceUniEmoX, a cross-modal semantic-guided large-scale pretraining framework.Inspired by psychological research emphasizing the inseparability of theemotional exploration process from the interaction between individuals andtheir environment, UniEmoX integrates scene-centric and person-centriclow-level image spatial structural information, aiming to derive more nuancedand discriminative emotional representations. By exploiting the similaritybetween paired and unpaired image-text samples, UniEmoX distills rich semanticknowledge from the CLIP model to enhance emotional embedding representationsmore effectively. To the best of our knowledge, this is the first large-scalepretraining framework that integrates psychological theories with contemporarycontrastive learning and masked image modeling techniques for emotion analysisacross diverse scenarios. Additionally, we develop a visual emotional datasettitled Emo8. Emo8 samples cover a range of domains, including cartoon, natural,realistic, science fiction and advertising cover styles, covering nearly allcommon emotional scenes. Comprehensive experiments conducted on six benchmarkdatasets across two downstream tasks validate the effectiveness of UniEmoX. Thesource code is available at https://github.com/chincharles/u-emo.</description><author>Chuang Chen, Xiao Sun, Zhi Liu</author><pubDate>Fri, 27 Sep 2024 16:12:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18877v1</guid></item><item><title>CemiFace: Center-based Semi-hard Synthetic Face Generation for Face Recognition</title><link>http://arxiv.org/abs/2409.18876v1</link><description>Privacy issue is a main concern in developing face recognition techniques.Although synthetic face images can partially mitigate potential legal riskswhile maintaining effective face recognition (FR) performance, FR modelstrained by face images synthesized by existing generative approaches frequentlysuffer from performance degradation problems due to the insufficientdiscriminative quality of these synthesized samples. In this paper, wesystematically investigate what contributes to solid face recognition modeltraining, and reveal that face images with certain degree of similarities totheir identity centers show great effectiveness in the performance of trainedFR models. Inspired by this, we propose a novel diffusion-based approach(namely Center-based Semi-hard Synthetic Face Generation (CemiFace)) whichproduces facial samples with various levels of similarity to the subjectcenter, thus allowing to generate face datasets containing effectivediscriminative samples for training face recognition. Experimental results showthat with a modest degree of similarity, training on the generated dataset canproduce competitive performance compared to previous generation methods.</description><author>Zhonglin Sun, Siyang Song, Ioannis Patras, Georgios Tzimiropoulos</author><pubDate>Fri, 27 Sep 2024 16:11:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18876v1</guid></item><item><title>CESNET-TimeSeries24: Time Series Dataset for Network Traffic Anomaly Detection and Forecasting</title><link>http://arxiv.org/abs/2409.18874v1</link><description>Anomaly detection in network traffic is crucial for maintaining the securityof computer networks and identifying malicious activities. One of the primaryapproaches to anomaly detection are methods based on forecasting. Nevertheless,extensive real-world network datasets for forecasting and anomaly detectiontechniques are missing, potentially causing performance overestimation ofanomaly detection algorithms. This manuscript addresses this gap by introducinga dataset comprising time series data of network entities' behavior, collectedfrom the CESNET3 network. The dataset was created from 40 weeks of networktraffic of 275 thousand active IP addresses. The ISP origin of the presenteddata ensures a high level of variability among network entities, which forms aunique and authentic challenge for forecasting and anomaly detection models. Itprovides valuable insights into the practical deployment of forecast-basedanomaly detection approaches.</description><author>Josef Koumar, Karel Hynek, Tomáš Čejka, Pavel Šiška</author><pubDate>Fri, 27 Sep 2024 16:10:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18874v1</guid></item><item><title>AnySkin: Plug-and-play Skin Sensing for Robotic Touch</title><link>http://arxiv.org/abs/2409.08276v3</link><description>While tactile sensing is widely accepted as an important and useful sensingmodality, its use pales in comparison to other sensory modalities like visionand proprioception. AnySkin addresses the critical challenges that impede theuse of tactile sensing -- versatility, replaceability, and data reusability.Building on the simplistic design of ReSkin, and decoupling the sensingelectronics from the sensing interface, AnySkin simplifies integration makingit as straightforward as putting on a phone case and connecting a charger.Furthermore, AnySkin is the first uncalibrated tactile-sensor withcross-instance generalizability of learned manipulation policies. To summarize,this work makes three key contributions: first, we introduce a streamlinedfabrication process and a design tool for creating an adhesive-free, durableand easily replaceable magnetic tactile sensor; second, we characterize slipdetection and policy learning with the AnySkin sensor; and third, wedemonstrate zero-shot generalization of models trained on one instance ofAnySkin to new instances, and compare it with popular existing tactilesolutions like DIGIT and ReSkin. Videos of experiments, fabrication details anddesign files can be found on https://any-skin.github.io/</description><author>Raunaq Bhirangi, Venkatesh Pattabiraman, Enes Erciyes, Yifeng Cao, Tess Hellebrekers, Lerrel Pinto</author><pubDate>Fri, 27 Sep 2024 16:09:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08276v3</guid></item><item><title>Simulating Dynamic Tumor Contrast Enhancement in Breast MRI using Conditional Generative Adversarial Networks</title><link>http://arxiv.org/abs/2409.18872v1</link><description>This paper presents a method for virtual contrast enhancement in breast MRI,offering a promising non-invasive alternative to traditional contrastagent-based DCE-MRI acquisition. Using a conditional generative adversarialnetwork, we predict DCE-MRI images, including jointly-generated sequences ofmultiple corresponding DCE-MRI timepoints, from non-contrast-enhanced MRIs,enabling tumor localization and characterization without the associated healthrisks. Furthermore, we qualitatively and quantitatively evaluate the syntheticDCE-MRI images, proposing a multi-metric Scaled Aggregate Measure (SAMe),assessing their utility in a tumor segmentation downstream task, and concludewith an analysis of the temporal patterns in multi-sequence DCE-MRI generation.Our approach demonstrates promising results in generating realistic and usefulDCE-MRI sequences, highlighting the potential of virtual contrast enhancementfor improving breast cancer diagnosis and treatment, particularly for patientswhere contrast agent administration is contraindicated.</description><author>Richard Osuala, Smriti Joshi, Apostolia Tsirikoglou, Lidia Garrucho, Walter H. L. Pinaya, Daniel M. Lang, Julia A. Schnabel, Oliver Diaz, Karim Lekadir</author><pubDate>Fri, 27 Sep 2024 16:08:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18872v1</guid></item><item><title>BeanCounter: A low-toxicity, large-scale, and open dataset of business-oriented text</title><link>http://arxiv.org/abs/2409.17827v2</link><description>Many of the recent breakthroughs in language modeling have resulted fromscaling effectively the same model architecture to larger datasets. In thisvein, recent work has highlighted performance gains from increasing trainingdataset size and quality, suggesting a need for novel sources of large-scaledatasets. In this work, we introduce BeanCounter, a public dataset consistingof more than 159B tokens extracted from businesses' disclosures. We show thatthis data is indeed novel: less than 0.1% of BeanCounter appears in CommonCrawl-based datasets and it is an order of magnitude larger than datasetsrelying on similar sources. Given the data's provenance, we hypothesize thatBeanCounter is comparatively more factual and less toxic than web-baseddatasets. Exploring this hypothesis, we find that many demographic identitiesoccur with similar prevalence in BeanCounter but with significantly less toxiccontext relative to other datasets. To demonstrate the utility of BeanCounter,we evaluate and compare two LLMs continually pre-trained on BeanCounter withtheir base models. We find an 18-33% reduction in toxic generation and improvedperformance within the finance domain for the continually pretrained models.Collectively, our work suggests that BeanCounter is a novel source oflow-toxicity and high-quality domain-specific data with sufficient scale totrain multi-billion parameter LLMs.</description><author>Siyan Wang, Bradford Levy</author><pubDate>Fri, 27 Sep 2024 16:07:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17827v2</guid></item><item><title>Emu3: Next-Token Prediction is All You Need</title><link>http://arxiv.org/abs/2409.18869v1</link><description>While next-token prediction is considered a promising path towards artificialgeneral intelligence, it has struggled to excel in multimodal tasks, which arestill dominated by diffusion models (e.g., Stable Diffusion) and compositionalapproaches (e.g., CLIP combined with LLMs). In this paper, we introduce Emu3, anew suite of state-of-the-art multimodal models trained solely with next-tokenprediction. By tokenizing images, text, and videos into a discrete space, wetrain a single transformer from scratch on a mixture of multimodal sequences.Emu3 outperforms several well-established task-specific models in bothgeneration and perception tasks, surpassing flagship models such as SDXL andLLaVA-1.6, while eliminating the need for diffusion or compositionalarchitectures. Emu3 is also capable of generating high-fidelity video viapredicting the next token in a video sequence. We simplify complex multimodalmodel designs by converging on a singular focus: tokens, unlocking greatpotential for scaling both during training and inference. Our resultsdemonstrate that next-token prediction is a promising path towards buildinggeneral multimodal intelligence beyond language. We open-source key techniquesand models to support further research in this direction.</description><author>Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, Zhongyuan Wang</author><pubDate>Fri, 27 Sep 2024 16:06:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18869v1</guid></item><item><title>LLM Detectors Still Fall Short of Real World: Case of LLM-Generated Short News-Like Posts</title><link>http://arxiv.org/abs/2409.03291v2</link><description>With the emergence of widely available powerful LLMs, disinformationgenerated by large Language Models (LLMs) has become a major concern.Historically, LLM detectors have been touted as a solution, but theireffectiveness in the real world is still to be proven. In this paper, we focuson an important setting in information operations -- short news-like postsgenerated by moderately sophisticated attackers. We demonstrate that existing LLM detectors, whether zero-shot orpurpose-trained, are not ready for real-world use in that setting. All testedzero-shot detectors perform inconsistently with prior benchmarks and are highlyvulnerable to sampling temperature increase, a trivial attack absent fromrecent benchmarks. A purpose-trained detector generalizing across LLMs andunseen attacks can be developed, but it fails to generalize to newhuman-written texts. We argue that the former indicates domain-specific benchmarking is needed,while the latter suggests a trade-off between the adversarial evasionresilience and overfitting to the reference human text, with both needingevaluation in benchmarks and currently absent. We believe this suggests are-consideration of current LLM detector benchmarking approaches and provides adynamically extensible benchmark to allow it(https://github.com/Reliable-Information-Lab-HEVS/benchmark_llm_texts_detection).</description><author>Henrique Da Silva Gameiro, Andrei Kucharavy, Ljiljana Dolamic</author><pubDate>Fri, 27 Sep 2024 16:04:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03291v2</guid></item><item><title>Individuation in Neural Models with and without Visual Grounding</title><link>http://arxiv.org/abs/2409.18868v1</link><description>We show differences between a language-and-vision model CLIP and twotext-only models - FastText and SBERT - when it comes to the encoding ofindividuation information. We study latent representations that CLIP providesfor substrates, granular aggregates, and various numbers of objects. Wedemonstrate that CLIP embeddings capture quantitative differences inindividuation better than models trained on text-only data. Moreover, theindividuation hierarchy we deduce from the CLIP embeddings agrees with thehierarchies proposed in linguistics and cognitive science.</description><author>Alexey Tikhonov, Lisa Bylinina, Ivan P. Yamshchikov</author><pubDate>Fri, 27 Sep 2024 16:04:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18868v1</guid></item><item><title>MCUBench: A Benchmark of Tiny Object Detectors on MCUs</title><link>http://arxiv.org/abs/2409.18866v1</link><description>We introduce MCUBench, a benchmark featuring over 100 YOLO-based objectdetection models evaluated on the VOC dataset across seven different MCUs. Thisbenchmark provides detailed data on average precision, latency, RAM, and Flashusage for various input resolutions and YOLO-based one-stage detectors. Byconducting a controlled comparison with a fixed training pipeline, we collectcomprehensive performance metrics. Our Pareto-optimal analysis shows thatintegrating modern detection heads and training techniques allows various YOLOarchitectures, including legacy models like YOLOv3, to achieve a highlyefficient tradeoff between mean Average Precision (mAP) and latency. MCUBenchserves as a valuable tool for benchmarking the MCU performance of contemporaryobject detectors and aids in model selection based on specific constraints.</description><author>Sudhakar Sah, Darshan C. Ganji, Matteo Grimaldi, Ravish Kumar, Alexander Hoffman, Honnesh Rohmetra, Ehsan Saboori</author><pubDate>Fri, 27 Sep 2024 16:02:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18866v1</guid></item><item><title>Positional Encoder Graph Quantile Neural Networks for Geographic Data</title><link>http://arxiv.org/abs/2409.18865v1</link><description>Positional Encoder Graph Neural Networks (PE-GNNs) are a leading approach formodeling continuous spatial data. However, they often fail to producecalibrated predictive distributions, limiting their effectiveness foruncertainty quantification. We introduce the Positional Encoder Graph QuantileNeural Network (PE-GQNN), a novel method that integrates PE-GNNs, QuantileNeural Networks, and recalibration techniques in a fully nonparametricframework, requiring minimal assumptions about the predictive distributions. Wepropose a new network architecture that, when combined with a quantile-basedloss function, yields accurate and reliable probabilistic models withoutincreasing computational complexity. Our approach provides a flexible, robustframework for conditional density estimation, applicable beyond spatial datacontexts. We further introduce a structured method for incorporating a KNNpredictor into the model while avoiding data leakage through the GNN layeroperation. Experiments on benchmark datasets demonstrate that PE-GQNNsignificantly outperforms existing state-of-the-art methods in both predictiveaccuracy and uncertainty quantification.</description><author>William E. R. de Amorim, Scott A. Sisson, T. Rodrigues, David J. Nott, Guilherme S. Rodrigues</author><pubDate>Fri, 27 Sep 2024 16:02:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18865v1</guid></item><item><title>A New Dataset for Monocular Depth Estimation Under Viewpoint Shifts</title><link>http://arxiv.org/abs/2409.17851v2</link><description>Monocular depth estimation is a critical task for autonomous driving and manyother computer vision applications. While significant progress has been made inthis field, the effects of viewpoint shifts on depth estimation models remainlargely underexplored. This paper introduces a novel dataset and evaluationmethodology to quantify the impact of different camera positions andorientations on monocular depth estimation performance. We propose a groundtruth strategy based on homography estimation and object detection, eliminatingthe need for expensive lidar sensors. We collect a diverse dataset of roadscenes from multiple viewpoints and use it to assess the robustness of a moderndepth estimation model to geometric shifts. After assessing the validity of ourstrategy on a public dataset, we provide valuable insights into the limitationsof current models and highlight the importance of considering viewpointvariations in real-world applications.</description><author>Aurel Pjetri, Stefano Caprasecca, Leonardo Taccari, Matteo Simoncini, Henrique Piñeiro Monteagudo, Walter Wallace, Douglas Coimbra de Andrade, Francesco Sambo, Andrew David Bagdanov</author><pubDate>Fri, 27 Sep 2024 15:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17851v2</guid></item><item><title>LW2G: Learning Whether to Grow for Prompt-based Continual Learning</title><link>http://arxiv.org/abs/2409.18860v1</link><description>Continual Learning (CL) aims to learn in non-stationary scenarios,progressively acquiring and maintaining knowledge from sequential tasks. RecentPrompt-based Continual Learning (PCL) has achieved remarkable performance withPre-Trained Models (PTMs). These approaches grow a prompt sets pool by adding anew set of prompts when learning each new task (\emph{prompt learning}) andadopt a matching mechanism to select the correct set for each testing sample(\emph{prompt retrieval}). Previous studies focus on the latter stage byimproving the matching mechanism to enhance Prompt Retrieval Accuracy (PRA). Topromote cross-task knowledge facilitation and form an effective and efficientprompt sets pool, we propose a plug-in module in the former stage to\textbf{Learn Whether to Grow (LW2G)} based on the disparities between tasks.Specifically, a shared set of prompts is utilized when several tasks sharecertain commonalities, and a new set is added when there are significantdifferences between the new task and previous tasks. Inspired by GradientProjection Continual Learning, our LW2G develops a metric called Hinder ForwardCapability (HFC) to measure the hindrance imposed on learning new tasks bysurgically modifying the original gradient onto the orthogonal complement ofthe old feature space. With HFC, an automated scheme Dynamic Growing Approachadaptively learns whether to grow with a dynamic threshold. Furthermore, wedesign a gradient-based constraint to ensure the consistency between theupdating prompts and pre-trained knowledge, and a prompts weights reusingstrategy to enhance forward transfer. Extensive experiments show theeffectiveness of our method. The source codes are available at\url{https://github.com/RAIAN08/LW2G}.</description><author>Qian Feng, Dawei Zhou, Hanbin Zhao, Chao Zhang, Hui Qian</author><pubDate>Fri, 27 Sep 2024 15:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18860v1</guid></item><item><title>Challenges of Generating Structurally Diverse Graphs</title><link>http://arxiv.org/abs/2409.18859v1</link><description>For many graph-related problems, it can be essential to have a set ofstructurally diverse graphs. For instance, such graphs can be used for testinggraph algorithms or their neural approximations. However, to the best of ourknowledge, the problem of generating structurally diverse graphs has not beenexplored in the literature. In this paper, we fill this gap. First, we discusshow to define diversity for a set of graphs, why this task is non-trivial, andhow one can choose a proper diversity measure. Then, for a given diversitymeasure, we propose and compare several algorithms optimizing it: we considerapproaches based on standard random graph models, local graph optimization,genetic algorithms, and neural generative models. We show that it is possibleto significantly improve diversity over basic random graph generators.Additionally, our analysis of generated graphs allows us to better understandthe properties of graph distances: depending on which diversity measure is usedfor optimization, the obtained graphs may possess very different structuralproperties which gives insights about the sensitivity of the graph distanceunderlying the diversity measure.</description><author>Fedor Velikonivtsev, Mikhail Mironov, Liudmila Prokhorenkova</author><pubDate>Fri, 27 Sep 2024 15:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18859v1</guid></item><item><title>Mitigating Selection Bias with Node Pruning and Auxiliary Options</title><link>http://arxiv.org/abs/2409.18857v1</link><description>Large language models (LLMs) often show unwarranted preference for certainchoice options when responding to multiple-choice questions, posing significantreliability concerns in LLM-automated systems. To mitigate this selection biasproblem, previous solutions utilized debiasing methods to adjust the model'sinput and/or output. Our work, in contrast, investigates the model's internalrepresentation of the selection bias. Specifically, we introduce a noveldebiasing approach, Bias Node Pruning (BNP), which eliminates the linear layerparameters that contribute to the bias. Furthermore, we present AuxiliaryOption Injection (AOI), a simple yet effective input modification technique fordebiasing, which is compatible even with black-box LLMs. To provide a moresystematic evaluation of selection bias, we review existing metrics andintroduce Choice Kullback-Leibler Divergence (CKLD), which addresses theinsensitivity of the commonly used metrics to label imbalance. Experiments showthat our methods are robust and adaptable across various datasets when appliedto three LLMs.</description><author>Hyeong Kyu Choi, Weijie Xu, Chi Xue, Stephanie Eckman, Chandan K. Reddy</author><pubDate>Fri, 27 Sep 2024 15:53:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18857v1</guid></item><item><title>Space-time 2D Gaussian Splatting for Accurate Surface Reconstruction under Complex Dynamic Scenes</title><link>http://arxiv.org/abs/2409.18852v1</link><description>Previous surface reconstruction methods either suffer from low geometricaccuracy or lengthy training times when dealing with real-world complex dynamicscenes involving multi-person activities, and human-object interactions. Totackle the dynamic contents and the occlusions in complex scenes, we present aspace-time 2D Gaussian Splatting approach. Specifically, to improve geometricquality in dynamic scenes, we learn canonical 2D Gaussian splats and deformthese 2D Gaussian splats while enforcing the disks of the Gaussian located onthe surface of the objects by introducing depth and normal regularizers.Further, to tackle the occlusion issues in complex scenes, we introduce acompositional opacity deformation strategy, which further reduces the surfacerecovery of those occluded areas. Experiments on real-world sparse-view videodatasets and monocular dynamic datasets demonstrate that our reconstructionsoutperform state-of-the-art methods, especially for the surface of the details.The project page and more visualizations can be found at:https://tb2-sy.github.io/st-2dgs/.</description><author>Shuo Wang, Binbin Huang, Ruoyu Wang, Shenghua Gao</author><pubDate>Fri, 27 Sep 2024 15:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18852v1</guid></item><item><title>Two Sparse Matrices are Better than One: Sparsifying Neural Networks with Double Sparse Factorization</title><link>http://arxiv.org/abs/2409.18850v1</link><description>Neural networks are often challenging to work with due to their large sizeand complexity. To address this, various methods aim to reduce model size bysparsifying or decomposing weight matrices, such as magnitude pruning andlow-rank or block-diagonal factorization. In this work, we present DoubleSparse Factorization (DSF), where we factorize each weight matrix into twosparse matrices. Although solving this problem exactly is computationallyinfeasible, we propose an efficient heuristic based on alternating minimizationvia ADMM that achieves state-of-the-art results, enabling unprecedentedsparsification of neural networks. For instance, in a one-shot pruning setting,our method can reduce the size of the LLaMA2-13B model by 50% while maintainingbetter performance than the dense LLaMA2-7B model. We also compare favorablywith Optimal Brain Compression, the state-of-the-art layer-wise pruningapproach for convolutional neural networks. Furthermore, accuracy improvementsof our method persist even after further model fine-tuning. Code available at: https://github.com/usamec/double_sparse.</description><author>Vladimír Boža, Vladimír Macko</author><pubDate>Fri, 27 Sep 2024 15:48:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18850v1</guid></item><item><title>A preliminary study on continual learning in computer vision using Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2409.13550v2</link><description>Deep learning has long been dominated by multi-layer perceptrons (MLPs),which have demonstrated superiority over other optimizable models in variousdomains. Recently, a new alternative to MLPs has emerged - Kolmogorov-ArnoldNetworks (KAN)- which are based on a fundamentally different mathematicalframework. According to their authors, KANs address several major issues inMLPs, such as catastrophic forgetting in continual learning scenarios. However,this claim has only been supported by results from a regression task on a toy1D dataset. In this paper, we extend the investigation by evaluating theperformance of KANs in continual learning tasks within computer vision,specifically using the MNIST datasets. To this end, we conduct a structuredanalysis of the behavior of MLPs and two KAN-based models in aclass-incremental learning scenario, ensuring that the architectures involvedhave the same number of trainable parameters. Our results demonstrate that anefficient version of KAN outperforms both traditional MLPs and the original KANimplementation. We further analyze the influence of hyperparameters in MLPs andKANs, as well as the impact of certain trainable parameters in KANs, such asbias and scale weights. Additionally, we provide a preliminary investigation ofrecent KAN-based convolutional networks and compare their performance with thatof traditional convolutional neural networks. Our codes can be found athttps://github.com/MrPio/KAN-Continual_Learning_tests.</description><author>Alessandro Cacciatore, Valerio Morelli, Federica Paganica, Emanuele Frontoni, Lucia Migliorelli, Daniele Berardini</author><pubDate>Fri, 27 Sep 2024 15:41:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13550v2</guid></item><item><title>Classical Statistical (In-Sample) Intuitions Don't Generalize Well: A Note on Bias-Variance Tradeoffs, Overfitting and Moving from Fixed to Random Designs</title><link>http://arxiv.org/abs/2409.18842v1</link><description>The sudden appearance of modern machine learning (ML) phenomena like doubledescent and benign overfitting may leave many classically trained statisticiansfeeling uneasy -- these phenomena appear to go against the very core ofstatistical intuitions conveyed in any introductory class on learning fromdata. The historical lack of earlier observation of such phenomena is usuallyattributed to today's reliance on more complex ML methods,overparameterization, interpolation and/or higher data dimensionality. In thisnote, we show that there is another reason why we observe behaviors today thatappear at odds with intuitions taught in classical statistics textbooks, whichis much simpler to understand yet rarely discussed explicitly. In particular,many intuitions originate in fixed design settings, in which in-sampleprediction error (under resampling of noisy outcomes) is of interest, whilemodern ML evaluates its predictions in terms of generalization error, i.e.out-of-sample prediction error in random designs. Here, we highlight that thissimple move from fixed to random designs has (perhaps surprisingly)far-reaching consequences on textbook intuitions relating to the bias-variancetradeoff, and comment on the resulting (im)possibility of observing doubledescent and benign overfitting in fixed versus random designs.</description><author>Alicia Curth</author><pubDate>Fri, 27 Sep 2024 15:36:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18842v1</guid></item><item><title>RNC: Efficient RRAM-aware NAS and Compilation for DNNs on Resource-Constrained Edge Devices</title><link>http://arxiv.org/abs/2409.18841v1</link><description>Computing-in-memory (CIM) is an emerging computing paradigm, offeringnoteworthy potential for accelerating neural networks with high parallelism,low latency, and energy efficiency compared to conventional von Neumannarchitectures. However, existing research has primarily focused on hardwarearchitecture and network co-design for large-scale neural networks, withoutconsidering resource constraints. In this study, we aim to developedge-friendly deep neural networks (DNNs) for accelerators based on resistiverandom-access memory (RRAM). To achieve this, we propose an edge compilationand resource-constrained RRAM-aware neural architecture search (NAS) frameworkto search for optimized neural networks meeting specific hardware constraints.Our compilation approach integrates layer partitioning, duplication, andnetwork packing to maximize the utilization of computation units. The resultingnetwork architecture can be optimized for either high accuracy or low latencyusing a one-shot neural network approach with Pareto optimality achievedthrough the Non-dominated Sorted Genetic Algorithm II (NSGA-II). Thecompilation of mobile-friendly networks, like Squeezenet and MobilenetV3 smallcan achieve over 80% of utilization and over 6x speedup compared to ISAAC-likeframework with different crossbar resources. The resulting model from NASoptimized for speed achieved 5x-30x speedup. The code for this paper isavailable at https://github.com/ArChiiii/rram_nas_comp_pack.</description><author>Kam Chi Loong, Shihao Han, Sishuo Liu, Ning Lin, Zhongrui Wang</author><pubDate>Fri, 27 Sep 2024 15:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18841v1</guid></item><item><title>MinerU: An Open-Source Solution for Precise Document Content Extraction</title><link>http://arxiv.org/abs/2409.18839v1</link><description>Document content analysis has been a crucial research area in computervision. Despite significant advancements in methods such as OCR, layoutdetection, and formula recognition, existing open-source solutions struggle toconsistently deliver high-quality content extraction due to the diversity indocument types and content. To address these challenges, we present MinerU, anopen-source solution for high-precision document content extraction. MinerUleverages the sophisticated PDF-Extract-Kit models to extract content fromdiverse documents effectively and employs finely-tuned preprocessing andpostprocessing rules to ensure the accuracy of the final results. Experimentalresults demonstrate that MinerU consistently achieves high performance acrossvarious document types, significantly enhancing the quality and consistency ofcontent extraction. The MinerU open-source project is available athttps://github.com/opendatalab/MinerU.</description><author>Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, Bo Zhang, Liqun Wei, Zhihao Sui, Wei Li, Botian Shi, Yu Qiao, Dahua Lin, Conghui He</author><pubDate>Fri, 27 Sep 2024 15:35:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18839v1</guid></item><item><title>Towards Physically Consistent Deep Learning For Climate Model Parameterizations</title><link>http://arxiv.org/abs/2406.03920v3</link><description>Climate models play a critical role in understanding and projecting climatechange. Due to their complexity, their horizontal resolution of about 40-100 kmremains too coarse to resolve processes such as clouds and convection, whichneed to be approximated via parameterizations. These parameterizations are amajor source of systematic errors and large uncertainties in climateprojections. Deep learning (DL)-based parameterizations, trained on data fromcomputationally expensive short, high-resolution simulations, have shown greatpromise for improving climate models in that regard. However, their lack ofinterpretability and tendency to learn spurious non-physical correlationsresult in reduced trust in the climate simulation. We propose an efficientsupervised learning framework for DL-based parameterizations that leads tophysically consistent models with improved interpretability and negligiblecomputational overhead compared to standard supervised training. First, keyfeatures determining the target physical processes are uncovered. Subsequently,the neural network is fine-tuned using only those relevant features. We showempirically that our method robustly identifies a small subset of the inputs asactual physical drivers, therefore removing spurious non-physicalrelationships. This results in by design physically consistent andinterpretable neural networks while maintaining the predictive performance ofunconstrained black-box DL-based parameterizations.</description><author>Birgit Kühbacher, Fernando Iglesias-Suarez, Niki Kilbertus, Veronika Eyring</author><pubDate>Fri, 27 Sep 2024 15:31:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03920v3</guid></item><item><title>Lens: A Foundation Model for Network Traffic</title><link>http://arxiv.org/abs/2402.03646v4</link><description>Network traffic refers to the amount of data being sent and received over theinternet or any system that connects computers. Analyzing and understandingnetwork traffic is vital for improving network security and management.However, the analysis of network traffic is challenging due to the diversenature of data packets, which often feature heterogeneous headers and encryptedpayloads lacking semantics. To capture the latent semantics of traffic, a fewstudies have adopted pre-training techniques based on the Transformer encoderor decoder to learn the representations from massive traffic data. However,these methods typically excel in traffic understanding (classification) ortraffic generation tasks. To address this issue, we develop Lens, a foundationmodel for network traffic that leverages the T5 architecture to learn thepre-trained representations from large-scale unlabeled data. Harnessing thestrength of the encoder-decoder framework, which captures the globalinformation while preserving the generative ability, our model can better learnthe representations from raw data. To further enhance pre-trainingeffectiveness, we design a novel loss that combines three distinct tasks:Masked Span Prediction (MSP), Packet Order Prediction (POP), and HomologousTraffic Prediction (HTP). Evaluation results across various benchmark datasetsdemonstrate that the proposed Lens outperforms the baselines in most downstreamtasks related to both traffic understanding and generation. Notably, it alsorequires much less labeled data for fine-tuning compared to current methods.</description><author>Qineng Wang, Chen Qian, Xiaochang Li, Ziyu Yao, Gang Zhou, Huajie Shao</author><pubDate>Fri, 27 Sep 2024 15:30:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03646v4</guid></item><item><title>Constructing Confidence Intervals for 'the' Generalization Error -- a Comprehensive Benchmark Study</title><link>http://arxiv.org/abs/2409.18836v1</link><description>When assessing the quality of prediction models in machine learning,confidence intervals (CIs) for the generalization error, which measurespredictive performance, are a crucial tool. Luckily, there exist many methodsfor computing such CIs and new promising approaches are continuously beingproposed. Typically, these methods combine various resampling procedures, mostpopular among them cross-validation and bootstrapping, with different varianceestimation techniques. Unfortunately, however, there is currently no consensuson when any of these combinations may be most reliably employed and how theygenerally compare. In this work, we conduct the first large-scale studycomparing CIs for the generalization error - empirically evaluating 13different methods on a total of 18 tabular regression and classificationproblems, using four different inducers and a total of eight loss functions. Wegive an overview of the methodological foundations and inherent challenges ofconstructing CIs for the generalization error and provide a concise review ofall 13 methods in a unified framework. Finally, the CI methods are evaluated interms of their relative coverage frequency, width, and runtime. Based on thesefindings, we are able to identify a subset of methods that we would recommend.We also publish the datasets as a benchmarking suite on OpenML and our code onGitHub to serve as a basis for further studies.</description><author>Hannah Schulz-Kümpel, Sebastian Fischer, Thomas Nagler, Anne-Laure Boulesteix, Bernd Bischl, Roman Hornung</author><pubDate>Fri, 27 Sep 2024 15:29:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18836v1</guid></item><item><title>Bridging the Social &amp; Technical Divide in Augmentative and Alternative Communication (AAC) Applications for Autistic Adults</title><link>http://arxiv.org/abs/2404.17730v2</link><description>Natural Language Processing (NLP) techniques are being used more frequentlyto improve high-tech Augmentative and Alternative Communication (AAC), but manyof these techniques are integrated without the inclusion of the users'perspectives. Autistic adults are particularly neglected in the design of AACtools. We conducted in-depth interviews with 12 autistic adults to find thepain points of current AAC and determine what technological advances they mightfind helpful. We found that in addition to technological issues, there are manysocietal issues as well. We found 9 different categories of themes from ourinterviews: input flexibility, output flexibility, selecting or adapting AACfor a good fit, when to start or swap AAC, benefits, access as an adult,stumbling blocks for continued use, social concerns, and control ofcommunication. In this paper, we go through these categories in depth and thensuggest possible guidelines for developers, NLP researchers, and policy makers.</description><author>Lara J. Martin, Malathy Nagalakshmi</author><pubDate>Fri, 27 Sep 2024 15:27:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17730v2</guid></item><item><title>Classification and regression of trajectories rendered as images via 2D Convolutional Neural Networks</title><link>http://arxiv.org/abs/2409.18832v1</link><description>Trajectories can be regarded as time-series of coordinates, typically arisingfrom motile objects. Methods for trajectory classification are particularlyimportant to detect different movement patterns, while methods for regressionto compute motility metrics and forecasting. Recent advances in computer visionhave facilitated the processing of trajectories rendered as images viaartificial neural networks with 2d convolutional layers (CNNs). This approachleverages the capability of CNNs to learn spatial hierarchies of features fromimages, necessary to recognize complex shapes. Moreover, it overcomes thelimitation of other machine learning methods that require input trajectorieswith a fixed number of points. However, rendering trajectories as images canintroduce poorly investigated artifacts such as information loss due to theplotting of coordinates on a discrete grid, and spectral changes due to linethickness and aliasing. In this study, we investigate the effectiveness of CNNsfor solving classification and regression problems from synthetic trajectoriesthat have been rendered as images using different modalities. The parametersconsidered in this study include line thickness, image resolution, usage ofmotion history (color-coding of the temporal component) and anti-aliasing.Results highlight the importance of choosing an appropriate image resolutionaccording to model depth and motion history in applications where movementdirection is critical.</description><author>Mariaclaudia Nicolai, Raffaella Fiamma Cabini, Diego Ulisse Pizzagalli</author><pubDate>Fri, 27 Sep 2024 15:27:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18832v1</guid></item><item><title>Cluster Exploration using Informative Manifold Projections</title><link>http://arxiv.org/abs/2309.14857v3</link><description>Dimensionality reduction (DR) is one of the key tools for the visualexploration of high-dimensional data and uncovering its cluster structure intwo- or three-dimensional spaces. The vast majority of DR methods in theliterature do not take into account any prior knowledge a practitioner may haveregarding the dataset under consideration. We propose a novel method togenerate informative embeddings which not only factor out the structureassociated with different kinds of prior knowledge but also aim to reveal anyremaining underlying structure. To achieve this, we employ a linear combinationof two objectives: firstly, contrastive PCA that discounts the structureassociated with the prior information, and secondly, kurtosis projectionpursuit which ensures meaningful data separation in the obtained embeddings. Weformulate this task as a manifold optimization problem and validate itempirically across a variety of datasets considering three distinct types ofprior knowledge. Lastly, we provide an automated framework to perform iterativevisual exploration of high-dimensional data.</description><author>Stavros Gerolymatos, Xenophon Evangelopoulos, Vladimir Gusev, John Y. Goulermas</author><pubDate>Fri, 27 Sep 2024 15:26:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14857v3</guid></item><item><title>A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning</title><link>http://arxiv.org/abs/2307.13127v2</link><description>It is common practice to use data containing personal information to buildpredictive models in the framework of empirical risk minimization (ERM). Whilethese models can be highly accurate in prediction, sharing the results fromthese models trained on sensitive data may be susceptible to privacy attacks.Differential privacy (DP) is an appealing framework for addressing such dataprivacy issues by providing mathematically provable bounds on the privacy lossincurred when releasing information from sensitive data. Previous work hasprimarily concentrated on applying DP to unweighted ERM. We consider weightedERM (wERM), an important generalization, where each individual's contributionto the objective function can be assigned varying weights. We propose the firstdifferentially private algorithm for general wERM, with theoretical DPguarantees. Extending the existing DP-ERM procedures to wERM creates a pathwayfor deriving privacy-preserving learning methods for individualized treatmentrules, including the popular outcome weighted learning (OWL). We evaluate theperformance of the DP-wERM framework applied to OWL in both simulation studiesand in a real clinical trial. All empirical results demonstrate the feasibilityof training OWL models via wERM with DP guarantees while maintainingsufficiently robust model performance, providing strong evidence for thepracticality of implementing the proposed privacy-preserving OWL procedure inreal-world scenarios involving sensitive data.</description><author>Spencer Giddens, Yiwang Zhou, Kevin R. Krull, Tara M. Brinkman, Peter X. K. Song, Fang Liu</author><pubDate>Fri, 27 Sep 2024 15:24:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13127v2</guid></item><item><title>MECG-E: Mamba-based ECG Enhancer for Baseline Wander Removal</title><link>http://arxiv.org/abs/2409.18828v1</link><description>Electrocardiogram (ECG) is an important non-invasive method for diagnosingcardiovascular disease. However, ECG signals are susceptible to noisecontamination, such as electrical interference or signal wandering, whichreduces diagnostic accuracy. Various ECG denoising methods have been proposed,but most existing methods yield suboptimal performance under very noisyconditions or require several steps during inference, leading to latency duringonline processing. In this paper, we propose a novel ECG denoising model,namely Mamba-based ECG Enhancer (MECG-E), which leverages the Mambaarchitecture known for its fast inference and outstanding nonlinear mappingcapabilities. Experimental results indicate that MECG-E surpasses severalwell-known existing models across multiple metrics under different noiseconditions. Additionally, MECG-E requires less inference time thanstate-of-the-art diffusion-based ECG denoisers, demonstrating the model'sfunctionality and efficiency.</description><author>Kuo-Hsuan Hung, Kuan-Chen Wang, Kai-Chun Liu, Wei-Lun Chen, Xugang Lu, Yu Tsao, Chii-Wann Lin</author><pubDate>Fri, 27 Sep 2024 15:22:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18828v1</guid></item><item><title>ARLBench: Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning</title><link>http://arxiv.org/abs/2409.18827v1</link><description>Hyperparameters are a critical factor in reliably training well-performingreinforcement learning (RL) agents. Unfortunately, developing and evaluatingautomated approaches for tuning such hyperparameters is both costly andtime-consuming. As a result, such approaches are often only evaluated on asingle domain or algorithm, making comparisons difficult and limiting insightsinto their generalizability. We propose ARLBench, a benchmark forhyperparameter optimization (HPO) in RL that allows comparisons of diverse HPOapproaches while being highly efficient in evaluation. To enable research intoHPO in RL, even in settings with low compute resources, we select arepresentative subset of HPO tasks spanning a variety of algorithm andenvironment combinations. This selection allows for generating a performanceprofile of an automated RL (AutoRL) method using only a fraction of the computepreviously necessary, enabling a broader range of researchers to work on HPO inRL. With the extensive and large-scale dataset on hyperparameter landscapesthat our selection is based on, ARLBench is an efficient, flexible, andfuture-oriented foundation for research on AutoRL. Both the benchmark and thedataset are available at https://github.com/automl/arlbench.</description><author>Jannis Becktepe, Julian Dierkes, Carolin Benjamins, Aditya Mohan, David Salinas, Raghu Rajan, Frank Hutter, Holger Hoos, Marius Lindauer, Theresa Eimer</author><pubDate>Fri, 27 Sep 2024 15:22:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18827v1</guid></item><item><title>YOLOv8-ResCBAM: YOLOv8 Based on An Effective Attention Module for Pediatric Wrist Fracture Detection</title><link>http://arxiv.org/abs/2409.18826v1</link><description>Wrist trauma and even fractures occur frequently in daily life, particularlyamong children who account for a significant proportion of fracture cases.Before performing surgery, surgeons often request patients to undergo X-rayimaging first, and prepare for the surgery based on the analysis of the X-rayimages. With the development of neural networks, You Only Look Once (YOLO)series models have been widely used in fracture detection for Computer-AssistedDiagnosis, where the YOLOv8 model has obtained the satisfactory results.Applying the attention modules to neural networks is one of the effectivemethods to improve the model performance. This paper proposes YOLOv8-ResCBAM,which incorporates Convolutional Block Attention Module integrated withresblock (ResCBAM) into the original YOLOv8 network architecture. Theexperimental results on the GRAZPEDWRI-DX dataset demonstrate that the meanAverage Precision calculated at Intersection over Union threshold of 0.5 (mAP50) of the proposed model increased from 63.6% of the original YOLOv8 model to65.8%, which achieves the state-of-the-art performance. The implementation codeis available athttps://github.com/RuiyangJu/Fracture_Detection_Improved_YOLOv8.</description><author>Rui-Yang Ju, Chun-Tse Chien, Jen-Shiun Chiang</author><pubDate>Fri, 27 Sep 2024 15:19:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18826v1</guid></item><item><title>RAMBO: Enhancing RAG-based Repository-Level Method Body Completion</title><link>http://arxiv.org/abs/2409.15204v2</link><description>Code completion is essential in software development, helping developers bypredicting code snippets based on context. Among completion tasks, Method BodyCompletion (MBC) is particularly challenging as it involves generating completemethod bodies based on their signatures and context. This task becomessignificantly harder in large repositories, where method bodies must integraterepositoryspecific elements such as custom APIs, inter-module dependencies, andproject-specific conventions. In this paper, we introduce RAMBO, a novelRAG-based approach for repository-level MBC. Instead of retrieving similarmethod bodies, RAMBO identifies essential repository-specific elements, such asclasses, methods, and variables/fields, and their relevant usages. Byincorporating these elements and their relevant usages into the code generationprocess, RAMBO ensures more accurate and contextually relevant method bodies.Our experimental results with leading code LLMs across 40 Java projects showthat RAMBO significantly outperformed the state-of-the-art repository-level MBCapproaches, with the improvements of up to 46% in BLEU, 57% in CodeBLEU, 36% inCompilation Rate, and up to 3X in Exact Match. Notably, RAMBO surpassedRepoCoder Oracle method by up to 12% in Exact Match, setting a new benchmarkfor repository-level MBC.</description><author>Tuan-Dung Bui, Duc-Thieu Luu-Van, Thanh-Phat Nguyen, Thu-Trang Nguyen, Son Nguyen, Hieu Dinh Vo</author><pubDate>Fri, 27 Sep 2024 15:19:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15204v2</guid></item><item><title>Paraphrase Types Elicit Prompt Engineering Capabilities</title><link>http://arxiv.org/abs/2406.19898v2</link><description>Much of the success of modern language models depends on finding a suitableprompt to instruct the model. Until now, it has been largely unknown howvariations in the linguistic expression of prompts affect these models. Thisstudy systematically and empirically evaluates which linguistic featuresinfluence models through paraphrase types, i.e., different linguistic changesat particular positions. We measure behavioral changes for five models across120 tasks and six families of paraphrases (i.e., morphology, syntax, lexicon,lexico-syntax, discourse, and others). We also control for other promptengineering factors (e.g., prompt length, lexical diversity, and proximity totraining data). Our results show a potential for language models to improvetasks when their prompts are adapted in specific paraphrase types (e.g., 6.7%median gain in Mixtral 8x7B; 5.5% in LLaMA 3 8B). In particular, changes inmorphology and lexicon, i.e., the vocabulary used, showed promise in improvingprompts. These findings contribute to developing more robust language modelscapable of handling variability in linguistic expression.</description><author>Jan Philip Wahle, Terry Ruas, Yang Xu, Bela Gipp</author><pubDate>Fri, 27 Sep 2024 15:17:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19898v2</guid></item><item><title>Local Transcription Models in Home Care Nursing in Switzerland: an Interdisciplinary Case Study</title><link>http://arxiv.org/abs/2409.18819v1</link><description>Latest advances in the field of natural language processing (NLP) enable newuse cases for different domains, including the medical sector. In particular,transcription can be used to support automation in the nursing documentationprocess and give nurses more time to interact with the patients. However,different challenges including (a) data privacy, (b) local languages anddialects, and (c) domain-specific vocabulary need to be addressed. In this casestudy, we investigate the case of home care nursing documentation inSwitzerland. We assessed different transcription tools and models, andconducted several experiments with OpenAI Whisper, involving differentvariations of German (i.e., dialects, foreign accent) and manually curatedexample texts by a domain expert of home care nursing. Our results indicatethat even the used out-of-the-box model performs sufficiently well to be a goodstarting point for future research in the field.</description><author>Jeremy Kramer, Tetiana Kravchenko, Beatrice Kaufmann, Friederike J. S. Thilo, Mascha Kurpicz-Briki</author><pubDate>Fri, 27 Sep 2024 15:15:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18819v1</guid></item><item><title>Optical ISAC: Fundamental Performance Limits and Transceiver Design</title><link>http://arxiv.org/abs/2408.11792v4</link><description>This paper characterizes the optimal Capacity-Distortion (C-D) tradeoff in anoptical point-to-point system with Single-Input Single-Output (SISO) forcommunication and Single-Input Multiple-Output (SIMO) for sensing within anIntegrated Sensing and Communication (ISAC) framework. We consider the optimalRate-Distortion (R-D) region and explore several Inner (IB) and Outer Bounds(OB). We introduce practical, asymptotically optimal Maximum A Posteriori (MAP)and Maximum Likelihood Estimators (MLE) for target distance, addressingnonlinear measurement-to-state relationships and non-conjugate priors. As thenumber of sensing antennas increases, these estimators converge to the BayesianCram\'er-Rao Bound (BCRB). We also establish that the achievableRate-Cram\'er-Rao Bound (R-CRB) serves as an OB for the optimal C-D region,valid for both unbiased estimators and asymptotically large numbers of receiveantennas. To clarify that the input distribution determines the tradeoff acrossthe Pareto boundary of the C-D region, we propose two algorithms: i) aniterative Blahut-Arimoto Algorithm (BAA)-type method, and ii) amemory-efficient Closed-Form (CF) approach. The CF approach includes a CFoptimal distribution for high Optical Signal-to-Noise Ratio (O-SNR) conditions.Additionally, we adapt and refine the Deterministic-Random Tradeoff (DRT) tothis optical ISAC context.</description><author>Alireza Ghazavi Khorasgani, Mahtab Mirmohseni, Ahmed Elzanaty</author><pubDate>Fri, 27 Sep 2024 15:10:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11792v4</guid></item><item><title>On fundamental aspects of quantum extreme learning machines</title><link>http://arxiv.org/abs/2312.15124v2</link><description>Quantum Extreme Learning Machines (QELMs) have emerged as a promisingframework for quantum machine learning. Their appeal lies in the rich featuremap induced by the dynamics of a quantum substrate - the quantum reservoir -and the efficient post-measurement training via linear regression. Here westudy the expressivity of QELMs by decomposing the prediction of QELMs into aFourier series. We show that the achievable Fourier frequencies are determinedby the data encoding scheme, while Fourier coefficients depend on both thereservoir and the measurement. Notably, the expressivity of QELMs isfundamentally limited by the number of Fourier frequencies and the number ofobservables, while the complexity of the prediction hinges on the reservoir. Asa cautionary note on scalability, we identify four sources that can lead to theexponential concentration of the observables as the system size grows(randomness, hardware noise, entanglement, and global measurements) and showhow this can turn QELMs into useless input-agnostic oracles. In particular, ourresult on the reservoir-induced concentration strongly indicates that quantumreservoirs drawn from a highly random ensemble make QELM models unscalable. Ouranalysis elucidates the potential and fundamental limitations of QELMs, andlays the groundwork for systematically exploring quantum reservoir systems forother machine learning tasks.</description><author>Weijie Xiong, Giorgio Facelli, Mehrad Sahebi, Owen Agnel, Thiparat Chotibut, Supanut Thanasilp, Zoë Holmes</author><pubDate>Fri, 27 Sep 2024 15:08:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15124v2</guid></item><item><title>Early diagnosis of Alzheimer's disease from MRI images with deep learning model</title><link>http://arxiv.org/abs/2409.18814v1</link><description>It is acknowledged that the most common cause of dementia worldwide isAlzheimer's disease (AD). This condition progresses in severity from mild tosevere and interferes with people's everyday routines. Early diagnosis plays acritical role in patient care and clinical trials. Convolutional neuralnetworks (CNN) are used to create a framework for identifying specific diseasefeatures from MRI scans Classification of dementia involves approaches such asmedical history review, neuropsychological tests, and magnetic resonanceimaging (MRI). However, the image dataset obtained from Kaggle faces asignificant issue of class imbalance, which requires equal distribution ofsamples from each class to address. In this article, to address this imbalance,the Synthetic Minority Oversampling Technique (SMOTE) is utilized. Furthermore,a pre-trained convolutional neural network has been applied to the DEMNETdementia network to extract key features from AD images. The proposed modelachieved an impressive accuracy of 98.67%.</description><author>Sajjad Aghasi Javid, Mahmood Mohassel Feghhi</author><pubDate>Fri, 27 Sep 2024 15:07:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18814v1</guid></item><item><title>EyeTrAES: Fine-grained, Low-Latency Eye Tracking via Adaptive Event Slicing</title><link>http://arxiv.org/abs/2409.18813v1</link><description>Eye-tracking technology has gained significant attention in recent years dueto its wide range of applications in human-computer interaction, virtual andaugmented reality, and wearable health. Traditional RGB camera-basedeye-tracking systems often struggle with poor temporal resolution andcomputational constraints, limiting their effectiveness in capturing rapid eyemovements. To address these limitations, we propose EyeTrAES, a novel approachusing neuromorphic event cameras for high-fidelity tracking of naturalpupillary movement that shows significant kinematic variance. One of EyeTrAES'shighlights is the use of a novel adaptive windowing/slicing algorithm thatensures just the right amount of descriptive asynchronous event dataaccumulation within an event frame, across a wide range of eye movementpatterns. EyeTrAES then applies lightweight image processing functions overaccumulated event frames from just a single eye to perform pupil segmentationand tracking. We show that these methods boost pupil tracking fidelity by 6+%,achieving IoU~=92%, while incurring at least 3x lower latency than competingpure event-based eye tracking alternatives [38]. We additionally demonstratethat the microscopic pupillary motion captured by EyeTrAES exhibits distinctivevariations across individuals and can thus serve as a biometric fingerprint.For robust user authentication, we train a lightweight per-user Random Forestclassifier using a novel feature vector of short-term pupillary kinematics,comprising a sliding window of pupil (location, velocity, acceleration)triples. Experimental studies with two different datasets demonstrate that theEyeTrAES-based authentication technique can simultaneously achieve highauthentication accuracy (~=0.82) and low processing latency (~=12ms), andsignificantly outperform multiple state-of-the-art competitive baselines.</description><author>Argha Sen, Nuwan Bandara, Ila Gokarn, Thivya Kandappu, Archan Misra</author><pubDate>Fri, 27 Sep 2024 15:06:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18813v1</guid></item><item><title>LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis</title><link>http://arxiv.org/abs/2409.18812v1</link><description>In response to the growing complexity and volume of scientific literature,this paper introduces the LLMs4Synthesis framework, designed to enhance thecapabilities of Large Language Models (LLMs) in generating high-qualityscientific syntheses. This framework addresses the need for rapid, coherent,and contextually rich integration of scientific insights, leveraging bothopen-source and proprietary LLMs. It also examines the effectiveness of LLMs inevaluating the integrity and reliability of these syntheses, alleviatinginadequacies in current quantitative metrics. Our study contributes to thisfield by developing a novel methodology for processing scientific papers,defining new synthesis types, and establishing nine detailed quality criteriafor evaluating syntheses. The integration of LLMs with reinforcement learningand AI feedback is proposed to optimize synthesis quality, ensuring alignmentwith established criteria. The LLMs4Synthesis framework and its components aremade available, promising to enhance both the generation and evaluationprocesses in scientific research synthesis.</description><author>Hamed Babaei Giglou, Jennifer D'Souza, Sören Auer</author><pubDate>Fri, 27 Sep 2024 15:04:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18812v1</guid></item><item><title>Interpretation of Intracardiac Electrograms Through Textual Representations</title><link>http://arxiv.org/abs/2402.01115v5</link><description>Understanding the irregular electrical activity of atrial fibrillation (AFib)has been a key challenge in electrocardiography. For serious cases of AFib,catheter ablations are performed to collect intracardiac electrograms (EGMs).EGMs offer intricately detailed and localized electrical activity of the heartand are an ideal modality for interpretable cardiac studies. Recentadvancements in artificial intelligence (AI) has allowed some works to utilizedeep learning frameworks to interpret EGMs during AFib. Additionally, languagemodels (LMs) have shown exceptional performance in being able to generalize tounseen domains, especially in healthcare. In this study, we are the first toleverage pretrained LMs for finetuning of EGM interpolation and AFibclassification via masked language modeling. We formulate the EGM as a textualsequence and present competitive performances on AFib classification comparedagainst other representations. Lastly, we provide a comprehensiveinterpretability study to provide a multi-perspective intuition of the model'sbehavior, which could greatly benefit the clinical use.</description><author>William Jongwon Han, Diana Gomez, Avi Alok, Chaojing Duan, Michael A. Rosenberg, Douglas Weber, Emerson Liu, Ding Zhao</author><pubDate>Fri, 27 Sep 2024 15:04:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01115v5</guid></item><item><title>A Novel Framework for the Automated Characterization of Gram-Stained Blood Culture Slides Using a Large-Scale Vision Transformer</title><link>http://arxiv.org/abs/2409.15546v2</link><description>This study introduces a new framework for the artificialintelligence-assisted characterization of Gram-stained whole-slide images(WSIs). As a test for the diagnosis of bloodstream infections, Gram stainsprovide critical early data to inform patient treatment. Rapid and reliableanalysis of Gram stains has been shown to be positively associated with betterclinical outcomes, underscoring the need for improved tools to automate Gramstain analysis. In this work, we developed a novel transformer-based model forGram-stained WSI classification, which is more scalable to large datasets thanprevious convolutional neural network (CNN) -based methods as it does notrequire patch-level manual annotations. We also introduce a large Gram staindataset from Dartmouth-Hitchcock Medical Center (Lebanon, New Hampshire, USA)to evaluate our model, exploring the classification of five major categories ofGram-stained WSIs: Gram-positive cocci in clusters, Gram-positive cocci inpairs/chains, Gram-positive rods, Gram-negative rods, and slides with nobacteria. Our model achieves a classification accuracy of 0.858 (95% CI: 0.805,0.905) and an AUC of 0.952 (95% CI: 0.922, 0.976) using five-fold nestedcross-validation on our 475-slide dataset, demonstrating the potential oflarge-scale transformer models for Gram stain classification. We furtherdemonstrate the generalizability of our trained model, which achieves strongperformance on external datasets without additional fine-tuning.</description><author>Jack McMahon, Naofumi Tomita, Elizabeth S. Tatishev, Adrienne A. Workman, Cristina R Costales, Niaz Banaei, Isabella W. Martin, Saeed Hassanpour</author><pubDate>Fri, 27 Sep 2024 15:02:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15546v2</guid></item><item><title>Physics-informed neural networks for parameter learning of wildfire spreading</title><link>http://arxiv.org/abs/2406.14591v2</link><description>Wildland fires pose a terrifying natural hazard, underscoring the urgent needto develop data-driven and physics-informed digital twins for wildfireprevention, monitoring, intervention, and response. In this direction ofresearch, this work introduces a physics-informed neural network (PiNN)designed to learn the unknown parameters of an interpretable wildfire spreadingmodel. The considered modeling approach integrates fundamental physical lawsarticulated by key model parameters essential for capturing the complexbehavior of wildfires. The proposed machine learning framework leverages thetheory of artificial neural networks with the physical constraints governingwildfire dynamics, including the first principles of mass and energyconservation. Training of the PiNN for physics-informed parameteridentification is realized using synthetic data on the spatiotemporal evolutionof one- and two-dimensional firefronts, derived from a high-fidelity simulator,as well as empirical data (ground surface thermal images) from the Troy Firethat occurred on June 19, 2002, in California. The parameter learning resultsdemonstrate the predictive ability of the proposed PiNN in uncovering theunknown coefficients of the wildfire model in one- and two-dimensional firespreading scenarios as well as the Troy Fire. Additionally, this methodologyexhibits robustness by identifying the same parameters even in the presence ofnoisy data. By integrating this PiNN approach into a comprehensive framework,the envisioned physics-informed digital twin will enhance intelligent wildfiremanagement and risk assessment, providing a powerful tool for proactive andreactive strategies.</description><author>Konstantinos Vogiatzoglou, Costas Papadimitriou, Vasilis Bontozoglou, Konstantinos Ampountolas</author><pubDate>Fri, 27 Sep 2024 15:01:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14591v2</guid></item><item><title>Convergence of Diffusion Models Under the Manifold Hypothesis in High-Dimensions</title><link>http://arxiv.org/abs/2409.18804v1</link><description>Denoising Diffusion Probabilistic Models (DDPM) are powerful state-of-the-artmethods used to generate synthetic data from high-dimensional datadistributions and are widely used for image, audio and video generation as wellas many more applications in science and beyond. The manifold hypothesis statesthat high-dimensional data often lie on lower-dimensional manifolds within theambient space, and is widely believed to hold in provided examples. Whilerecent results has provided invaluable insight into how diffusion models adaptto the manifold hypothesis, they do not capture the great empirical success ofthese models, making this a very fruitful research direction. In this work, we study DDPMs under the manifold hypothesis and prove thatthey achieve rates independent of the ambient dimension in terms of learningthe score. In terms of sampling, we obtain rates independent of the ambientdimension w.r.t. the Kullback-Leibler divergence, and $O(\sqrt{D})$ w.r.t. theWasserstein distance. We do this by developing a new framework connectingdiffusion models to the well-studied theory of extrema of Gaussian Processes.</description><author>Iskander Azangulov, George Deligiannidis, Judith Rousseau</author><pubDate>Fri, 27 Sep 2024 14:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18804v1</guid></item><item><title>MiniVLN: Efficient Vision-and-Language Navigation by Progressive Knowledge Distillation</title><link>http://arxiv.org/abs/2409.18800v1</link><description>In recent years, Embodied Artificial Intelligence (Embodied AI) has advancedrapidly, yet the increasing size of models conflicts with the limitedcomputational capabilities of Embodied AI platforms. To address this challenge,we aim to achieve both high model performance and practical deployability.Specifically, we focus on Vision-and-Language Navigation (VLN), a core task inEmbodied AI. This paper introduces a two-stage knowledge distillationframework, producing a student model, MiniVLN, and showcasing the significantpotential of distillation techniques in developing lightweight models. Theproposed method aims to capture fine-grained knowledge during the pretrainingphase and navigation-specific knowledge during the fine-tuning phase. Ourfindings indicate that the two-stage distillation approach is more effective innarrowing the performance gap between the teacher model and the student modelcompared to single-stage distillation. On the public R2R and REVERIEbenchmarks, MiniVLN achieves performance on par with the teacher model whilehaving only about 12% of the teacher model's parameter count.</description><author>Junyou Zhu, Yanyuan Qiao, Siqi Zhang, Xingjian He, Qi Wu, Jing Liu</author><pubDate>Fri, 27 Sep 2024 14:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18800v1</guid></item><item><title>Esports Debut as a Medal Event at 2023 Asian Games: Exploring Public Perceptions with BERTopic and GPT-4 Topic Fine-Tuning</title><link>http://arxiv.org/abs/2409.18798v1</link><description>This study examined the public opinions of esports at the 2023 Asian Gamesand value co-creation during the event using an LLM-enhanced BERTopic modelinganalysis. We identified five major themes representing public perceptions, aswell as how major stakeholders co-created value within and beyond the esportsecosystem. Key findings highlighted the strategic use of social media marketingto influence public opinion and promote esports events and brands, emphasizingthe importance of event logistics and infrastructure. Additionally, the studyrevealed the co-creation value contributed by stakeholders outside thetraditional esports ecosystem, particularly in promoting nationalrepresentation and performance. Our findings supported the ongoing efforts tolegitimize esports as a sport, noting that mainstream recognition remains achallenge. The inclusion of esports as a medal event showcased broaderacceptance and helped mitigate negative public perceptions. Moreover,contributions from non-traditional stakeholders underscored the value ofcross-subcultural collaborations in esports.</description><author>Tyreal Yizhou Qian, Bo Yu, Weizhe Li, Chenglong Xu</author><pubDate>Fri, 27 Sep 2024 14:53:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18798v1</guid></item><item><title>Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning</title><link>http://arxiv.org/abs/2404.16807v2</link><description>Generative Commonsense Reasoning (GCR) requires a model to reason about asituation using commonsense knowledge, while generating coherent sentences.Although the quality of the generated sentences is crucial, the diversity ofthe generation is equally important because it reflects the model's ability touse a range of commonsense knowledge facts. Large Language Models (LLMs) haveshown proficiency in enhancing the generation quality across various tasksthrough in-context learning (ICL) using given examples without the need for anyfine-tuning. However, the diversity aspect in LLM outputs has not beensystematically studied before. To address this, we propose a simple method thatdiversifies the LLM generations, while preserving their quality. Experimentalresults on three benchmark GCR datasets show that our method achieves an idealbalance between the quality and diversity. Moreover, the sentences generated byour proposed method can be used as training data to improve diversity inexisting commonsense generators.</description><author>Tianhui Zhang, Bei Peng, Danushka Bollegala</author><pubDate>Fri, 27 Sep 2024 14:50:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16807v2</guid></item><item><title>Hierarchical Federated ADMM</title><link>http://arxiv.org/abs/2409.18796v1</link><description>In this paper, we depart from the widely-used gradient descent-basedhierarchical federated learning (FL) algorithms to develop a novel hierarchicalFL framework based on the alternating direction method of multipliers (ADMM).Within this framework, we propose two novel FL algorithms, which both use ADMMin the top layer: one that employs ADMM in the lower layer and another thatuses the conventional gradient descent-based approach. The proposed frameworkenhances privacy, and experiments demonstrate the superiority of the proposedalgorithms compared to the conventional algorithms in terms of learningconvergence and accuracy. Additionally, gradient descent on the lower layerperforms well even if the number of local steps is very limited, while ADMM onboth layers lead to better performance otherwise.</description><author>Seyed Mohammad Azimi-Abarghouyi, Nicola Bastianello, Karl H. Johansson, Viktoria Fodor</author><pubDate>Fri, 27 Sep 2024 14:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18796v1</guid></item><item><title>The Role of Masking for Efficient Supervised Knowledge Distillation of Vision Transformers</title><link>http://arxiv.org/abs/2302.10494v4</link><description>Knowledge distillation is an effective method for training lightweight visionmodels. However, acquiring teacher supervision for training samples is oftencostly, especially from large-scale models like vision transformers (ViTs). Inthis paper, we develop a simple framework to reduce the supervision cost of ViTdistillation: masking out a fraction of input tokens given to the teacher. Bymasking input tokens, one can skip the computations associated with the maskedtokens without requiring any change to teacher parameters or architecture. Wefind that masking patches with the lowest student attention scores is highlyeffective, saving up to 50% of teacher FLOPs without any drop in studentaccuracy, while other masking criterion leads to suboptimal efficiency gains.Through in-depth analyses, we reveal that the student-guided masking provides agood curriculum to the student, making teacher supervision easier to followduring the early stage and challenging in the later stage.</description><author>Seungwoo Son, Jegwang Ryu, Namhoon Lee, Jaeho Lee</author><pubDate>Fri, 27 Sep 2024 14:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10494v4</guid></item><item><title>Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in Continuous Environment with Open-Source LLMs</title><link>http://arxiv.org/abs/2409.18794v1</link><description>Vision-and-Language Navigation (VLN) tasks require an agent to follow textualinstructions to navigate through 3D environments. Traditional approaches usesupervised learning methods, relying heavily on domain-specific datasets totrain VLN models. Recent methods try to utilize closed-source large languagemodels (LLMs) like GPT-4 to solve VLN tasks in zero-shot manners, but facechallenges related to expensive token costs and potential data breaches inreal-world applications. In this work, we introduce Open-Nav, a novel studythat explores open-source LLMs for zero-shot VLN in the continuous environment.Open-Nav employs a spatial-temporal chain-of-thought (CoT) reasoning approachto break down tasks into instruction comprehension, progress estimation, anddecision-making. It enhances scene perceptions with fine-grained object andspatial knowledge to improve LLM's reasoning in navigation. Our extensiveexperiments in both simulated and real-world environments demonstrate thatOpen-Nav achieves competitive performance compared to using closed-source LLMs.</description><author>Yanyuan Qiao, Wenqi Lyu, Hui Wang, Zixu Wang, Zerui Li, Yuan Zhang, Mingkui Tan, Qi Wu</author><pubDate>Fri, 27 Sep 2024 14:47:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18794v1</guid></item><item><title>A Learning-based Declarative Privacy-Preserving Framework for Federated Data Management</title><link>http://arxiv.org/abs/2401.12393v2</link><description>It is challenging to select the right privacy-preserving mechanism forfederated query processing over multiple private data silos. There existnumerous privacy-preserving mechanisms, such as secure multi-party computing(SMC), approximate query processing with differential privacy (DP), combinedSMC and DP, DP-based data obfuscation, and federated learning. These mechanismsmake different trade-offs among accuracy, privacy, execution efficiency, andstorage efficiency. In this work, we first introduce a new privacy-preservingtechnique that uses a deep learning model trained using theDifferentially-Private Stochastic Gradient Descent (DP-SGD) algorithm toreplace portions of actual data to answer a query. We then demonstrate a noveldeclarative privacy-preserving workflow that allows users to specify "whatprivate information to protect" rather than "how to protect". Under the hood,the system relies on a cost model to automatically choose privacy-preservingmechanisms as well as hyper-parameters. At the same time, the proposed workflowalso allows human experts to review and tune the selected privacy-preservingmechanism for audit/compliance, and optimization purposes.</description><author>Hong Guan, Summer Gautier, Rajan Hari Ambrish, Yancheng Wang, Chaowei Xiao, Yingzhen Yang, Jia Zou</author><pubDate>Fri, 27 Sep 2024 14:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12393v2</guid></item><item><title>Excavating in the Wild: The GOOSE-Ex Dataset for Semantic Segmentation</title><link>http://arxiv.org/abs/2409.18788v1</link><description>The successful deployment of deep learning-based techniques for autonomoussystems is highly dependent on the data availability for the respective systemin its deployment environment. Especially for unstructured outdoorenvironments, very few datasets exist for even fewer robotic platforms andscenarios. In an earlier work, we presented the German Outdoor and OffroadDataset (GOOSE) framework along with 10000 multimodal frames from an offroadvehicle to enhance the perception capabilities in unstructured environments. Inthis work, we address the generalizability of the GOOSE framework. Toaccomplish this, we open-source the GOOSE-Ex dataset, which contains additional5000 labeled multimodal frames from various completely different environments,recorded on a robotic excavator and a quadruped platform. We perform acomprehensive analysis of the semantic segmentation performance on differentplatforms and sensor modalities in unseen environments. In addition, wedemonstrate how the combined datasets can be utilized for different downstreamapplications or competitions such as offroad navigation, object manipulation orscene completion. The dataset, its platform documentation and pre-trainedstate-of-the-art models for offroad perception will be made available onhttps://goose-dataset.de/. \</description><author>Raphael Hagmanns, Peter Mortimer, Miguel Granero, Thorsten Luettel, Janko Petereit</author><pubDate>Fri, 27 Sep 2024 14:36:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18788v1</guid></item><item><title>A Survey on the Honesty of Large Language Models</title><link>http://arxiv.org/abs/2409.18786v1</link><description>Honesty is a fundamental principle for aligning large language models (LLMs)with human values, requiring these models to recognize what they know and don'tknow and be able to faithfully express their knowledge. Despite promising,current LLMs still exhibit significant dishonest behaviors, such as confidentlypresenting wrong answers or failing to express what they know. In addition,research on the honesty of LLMs also faces challenges, including varyingdefinitions of honesty, difficulties in distinguishing between known andunknown knowledge, and a lack of comprehensive understanding of relatedresearch. To address these issues, we provide a survey on the honesty of LLMs,covering its clarification, evaluation approaches, and strategies forimprovement. Moreover, we offer insights for future research, aiming to inspirefurther exploration in this important area.</description><author>Siheng Li, Cheng Yang, Taiqiang Wu, Chufan Shi, Yuji Zhang, Xinyu Zhu, Zesen Cheng, Deng Cai, Mo Yu, Lemao Liu, Jie Zhou, Yujiu Yang, Ngai Wong, Xixin Wu, Wai Lam</author><pubDate>Fri, 27 Sep 2024 14:34:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18786v1</guid></item><item><title>Student-Oriented Teacher Knowledge Refinement for Knowledge Distillation</title><link>http://arxiv.org/abs/2409.18785v1</link><description>Knowledge distillation has become widely recognized for its ability totransfer knowledge from a large teacher network to a compact and morestreamlined student network. Traditional knowledge distillation methodsprimarily follow a teacher-oriented paradigm that imposes the task of learningthe teacher's complex knowledge onto the student network. However, significantdisparities in model capacity and architectural design hinder the student'scomprehension of the complex knowledge imparted by the teacher, resulting insub-optimal performance. This paper introduces a novel perspective emphasizingstudent-oriented and refining the teacher's knowledge to better align with thestudent's needs, thereby improving knowledge transfer effectiveness.Specifically, we present the Student-Oriented Knowledge Distillation (SoKD),which incorporates a learnable feature augmentation strategy during training torefine the teacher's knowledge of the student dynamically. Furthermore, wedeploy the Distinctive Area Detection Module (DAM) to identify areas of mutualinterest between the teacher and student, concentrating knowledge transferwithin these critical areas to avoid transferring irrelevant information. Thiscustomized module ensures a more focused and effective knowledge distillationprocess. Our approach, functioning as a plug-in, could be integrated withvarious knowledge distillation methods. Extensive experimental resultsdemonstrate the efficacy and generalizability of our method.</description><author>Chaomin Shen, Yaomin Huang, Haokun Zhu, Jinsong Fan, Guixu Zhang</author><pubDate>Fri, 27 Sep 2024 14:34:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18785v1</guid></item><item><title>PIM-Opt: Demystifying Distributed Optimization Algorithms on a Real-World Processing-In-Memory System</title><link>http://arxiv.org/abs/2404.07164v2</link><description>Modern Machine Learning (ML) training on large-scale datasets is a verytime-consuming workload. It relies on the optimization algorithm StochasticGradient Descent (SGD) due to its effectiveness, simplicity, and generalizationperformance. Processor-centric architectures (e.g., CPUs, GPUs) commonly usedfor modern ML training workloads based on SGD are bottlenecked by data movementbetween the processor and memory units due to the poor data locality inaccessing large datasets. As a result, processor-centric architectures sufferfrom low performance and high energy consumption while executing ML trainingworkloads. Processing-In-Memory (PIM) is a promising solution to alleviate thedata movement bottleneck by placing the computation mechanisms inside or nearmemory. Our goal is to understand the capabilities of popular distributed SGDalgorithms on real-world PIM systems to accelerate data-intensive ML trainingworkloads. To this end, we 1) implement several representative centralizedparallel SGD algorithms on the real-world UPMEM PIM system, 2) rigorouslyevaluate these algorithms for ML training on large-scale datasets in terms ofperformance, accuracy, and scalability, 3) compare to conventional CPU and GPUbaselines, and 4) discuss implications for future PIM hardware and highlightthe need for a shift to an algorithm-hardware codesign. Our results demonstrate three major findings: 1) The UPMEM PIM system can bea viable alternative to state-of-the-art CPUs and GPUs for many memory-bound MLtraining workloads, especially when operations and datatypes are nativelysupported by PIM hardware, 2) it is important to carefully choose theoptimization algorithms that best fit PIM, and 3) the UPMEM PIM system does notscale approximately linearly with the number of nodes for many data-intensiveML training workloads. We open source all our code to facilitate futureresearch.</description><author>Steve Rhyner, Haocong Luo, Juan Gómez-Luna, Mohammad Sadrosadati, Jiawei Jiang, Ataberk Olgun, Harshita Gupta, Ce Zhang, Onur Mutlu</author><pubDate>Fri, 27 Sep 2024 14:32:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07164v2</guid></item><item><title>DualDn: Dual-domain Denoising via Differentiable ISP</title><link>http://arxiv.org/abs/2409.18783v1</link><description>Image denoising is a critical component in a camera's Image Signal Processing(ISP) pipeline. There are two typical ways to inject a denoiser into the ISPpipeline: applying a denoiser directly to captured raw frames (raw domain) orto the ISP's output sRGB images (sRGB domain). However, both approaches havetheir limitations. Residual noise from raw-domain denoising can be amplified bythe subsequent ISP processing, and the sRGB domain struggles to handlespatially varying noise since it only sees noise distorted by the ISP.Consequently, most raw or sRGB domain denoising works only for specific noisedistributions and ISP configurations. To address these challenges, we proposeDualDn, a novel learning-based dual-domain denoising. Unlike previoussingle-domain denoising, DualDn consists of two denoising networks: one in theraw domain and one in the sRGB domain. The raw domain denoising adapts tosensor-specific noise as well as spatially varying noise levels, while the sRGBdomain denoising adapts to ISP variations and removes residual noise amplifiedby the ISP. Both denoising networks are connected with a differentiable ISP,which is trained end-to-end and discarded during the inference stage. With thisdesign, DualDn achieves greater generalizability compared to mostlearning-based denoising methods, as it can adapt to different unseen noises,ISP parameters, and even novel ISP pipelines. Experiments show that DualDnachieves state-of-the-art performance and can adapt to different denoisingarchitectures. Moreover, DualDn can be used as a plug-and-play denoising modulewith real cameras without retraining, and still demonstrate better performancethan commercial on-camera denoising. The project website is available at:https://openimaginglab.github.io/DualDn/</description><author>Ruikang Li, Yujin Wang, Shiqi Chen, Fan Zhang, Jinwei Gu, Tianfan Xue</author><pubDate>Fri, 27 Sep 2024 14:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18783v1</guid></item><item><title>Non-parametric efficient estimation of marginal structural models with multi-valued time-varying treatments</title><link>http://arxiv.org/abs/2409.18782v1</link><description>Marginal structural models are a popular method for estimating causal effectsin the presence of time-varying exposures. In spite of their popularity, noscalable non-parametric estimator exist for marginal structural models withmulti-valued and time-varying treatments. In this paper, we use machinelearning together with recent developments in semiparametric efficiency theoryfor longitudinal studies to propose such an estimator. The proposed estimatoris based on a study of the non-parametric identifying functional, includingfirst order von-Mises expansions as well as the efficient influence functionand the efficiency bound. We show conditions under which the proposed estimatoris efficient, asymptotically normal, and sequentially doubly robust in thesense that it is consistent if, for each time point, either the outcome or thetreatment mechanism is consistently estimated. We perform a simulation study toillustrate the properties of the estimators, and present the results of ourmotivating study on a COVID-19 dataset studying the impact of mobility on thecumulative number of observed cases.</description><author>Axel Martin, Michele Santacatterina, Iván Díaz</author><pubDate>Fri, 27 Sep 2024 14:29:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18782v1</guid></item><item><title>Analyzing Probabilistic Methods for Evaluating Agent Capabilities</title><link>http://arxiv.org/abs/2409.16125v2</link><description>To mitigate risks from AI systems, we need to assess their capabilitiesaccurately. This is especially difficult in cases where capabilities are onlyrarely displayed. Phuong et al. propose two methods that aim to obtain betterestimates of the probability of an AI agent successfully completing a giventask. The milestone method decomposes tasks into subtasks, aiming to improveoverall success rate estimation, while the expert best-of-N method leverageshuman guidance as a proxy for the model's independent performance. Our analysis of these methods as Monte Carlo estimators reveals that whileboth effectively reduce variance compared to naive Monte Carlo sampling, theyalso introduce bias. Experimental results demonstrate that the milestone methodunderestimates true solve rates for many real-world tasks due to itsconstraining assumptions. The expert best-of-N method exhibits even more severeunderestimation across all tasks, attributed to an inherently flawedre-weighting factor. To enhance the accuracy of capability estimates of AIagents on difficult tasks, we suggest future work should leverage the richliterature on Monte Carlo Estimators.</description><author>Axel Højmark, Govind Pimpale, Arjun Panickssery, Marius Hobbhahn, Jérémy Scheurer</author><pubDate>Fri, 27 Sep 2024 14:24:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16125v2</guid></item><item><title>HardCore Generation: Generating Hard UNSAT Problems for Data Augmentation</title><link>http://arxiv.org/abs/2409.18778v1</link><description>Efficiently determining the satisfiability of a boolean equation -- known asthe SAT problem for brevity -- is crucial in various industrial problems.Recently, the advent of deep learning methods has introduced significantpotential for enhancing SAT solving. However, a major barrier to theadvancement of this field has been the scarcity of large, realistic datasets.The majority of current public datasets are either randomly generated orextremely limited, containing only a few examples from unrelated problemfamilies. These datasets are inadequate for meaningful training of deeplearning methods. In light of this, researchers have started exploringgenerative techniques to create data that more accurately reflect SAT problemsencountered in practical situations. These methods have so far suffered fromeither the inability to produce challenging SAT problems or time-scalabilityobstacles. In this paper we address both by identifying and manipulating thekey contributors to a problem's ``hardness'', known as cores. Although someprevious work has addressed cores, the time costs are unacceptably high due tothe expense of traditional heuristic core detection techniques. We introduce afast core detection procedure that uses a graph neural network. Our empiricalresults demonstrate that we can efficiently generate problems that remain hardto solve and retain key attributes of the original example problems. We showvia experiment that the generated synthetic SAT problems can be used in a dataaugmentation setting to provide improved prediction of solver runtimes.</description><author>Joseph Cotnareanu, Zhanguang Zhang, Hui-Ling Zhen, Yingxue Zhang, Mark Coates</author><pubDate>Fri, 27 Sep 2024 14:24:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18778v1</guid></item><item><title>Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping</title><link>http://arxiv.org/abs/2403.11876v2</link><description>High-speed off-road navigation requires long-range, high-resolution maps toenable robots to safely navigate over different surfaces while avoidingdangerous obstacles. However, due to limited computational power and sensingnoise, most approaches to off-road mapping focus on producing coarse (20-40cm)maps of the environment. In this paper, we propose Future Fusion, a frameworkcapable of generating dense, high-resolution maps from sparse sensing data (30mforward at 2cm). This is accomplished by - (1) the efficient realization of thewell-known Bayes filtering within the standard deep learning models thatexplicitly accounts for the sparsity pattern in stereo and LiDAR depth data,and (2) leveraging perceptual losses common in generative image completion. Theproposed methodology outperforms the conventional baselines. Moreover, thelearned features and the completed dense maps lead to improvements in thedownstream navigation task.</description><author>Shubhra Aich, Wenshan Wang, Parv Maheshwari, Matthew Sivaprakasam, Samuel Triest, Cherie Ho, Jason M. Gregory, John G. Rogers III, Sebastian Scherer</author><pubDate>Fri, 27 Sep 2024 14:18:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11876v2</guid></item><item><title>A method of using RSVD in residual calculation of LowBit GEMM</title><link>http://arxiv.org/abs/2409.18772v1</link><description>The advancements of hardware technology in recent years has brought manypossibilities for low-precision applications. However, the use of low precisioncan introduce significant computational errors, posing a considerable challengeto maintaining the computational accuracy. We propose low-rank residuals quantized matrix multiplication(LRQMM) methodwhich introduces low-rank approximation in residual compensation for dense lowprecision quantization matrix multiplication. It can bring several timesaccuracy improvement with only BLAS-2 level extra time overhead. Moreover,LRQMM is a completely data-free quantization method that does not requireadditional data for pre-training. And it only works with low precision GEMMoperator, which is easy to couple with other methods. Through experimentation, LRQMM can reduce the error of direct quantizedmatrix multiplication by 1~2 orders of magnitude, when dealing with largermatrix sizes, the computational speed is only reduced by approximately 20\%. Indeep learning networks, LRQMM-4bit achieves 61.8% ImageNet Top-1 accuracy inResnet-50, while the Direct Quant accuracy is only 8.3%.</description><author>Hongyaoxing Gu</author><pubDate>Fri, 27 Sep 2024 14:16:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18772v1</guid></item><item><title>Optimal Differentially Private PCA and Estimation for Spiked Covariance Matrices</title><link>http://arxiv.org/abs/2401.03820v2</link><description>Estimating a covariance matrix and its associated principal components is afundamental problem in contemporary statistics. While optimal estimationprocedures have been developed with well-understood properties, the increasingdemand for privacy preservation introduces new complexities to this classicalproblem. In this paper, we study optimal differentially private PrincipalComponent Analysis (PCA) and covariance estimation within the spiked covariancemodel. We precisely characterize the sensitivity of eigenvalues andeigenvectors under this model and establish the minimax rates of convergencefor estimating both the principal components and covariance matrix. These rateshold up to logarithmic factors and encompass general Schatten norms, includingspectral norm, Frobenius norm, and nuclear norm as special cases. We proposecomputationally efficient differentially private estimators and prove theirminimax optimality for sub-Gaussian distributions, up to logarithmic factors.Additionally, matching minimax lower bounds are established. Notably, comparedto the existing literature, our results accommodate a diverging rank, a broaderrange of signal strengths, and remain valid even when the sample size is muchsmaller than the dimension, provided the signal strength is sufficientlystrong. Both simulation studies and real data experiments demonstrate themerits of our method.</description><author>T. Tony Cai, Dong Xia, Mengyue Zha</author><pubDate>Fri, 27 Sep 2024 14:15:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.03820v2</guid></item><item><title>Relighting from a Single Image: Datasets and Deep Intrinsic-based Architecture</title><link>http://arxiv.org/abs/2409.18770v1</link><description>Single image scene relighting aims to generate a realistic new version of aninput image so that it appears to be illuminated by a new target lightcondition. Although existing works have explored this problem from variousperspectives, generating relit images under arbitrary light conditions remainshighly challenging, and related datasets are scarce. Our work addresses thisproblem from both the dataset and methodological perspectives. We propose twonew datasets: a synthetic dataset with the ground truth of intrinsic componentsand a real dataset collected under laboratory conditions. These datasetsalleviate the scarcity of existing datasets. To incorporate physicalconsistency in the relighting pipeline, we establish a two-stage network basedon intrinsic decomposition, giving outputs at intermediate steps, therebyintroducing physical constraints. When the training set lacks ground truth forintrinsic decomposition, we introduce an unsupervised module to ensure that theintrinsic outputs are satisfactory. Our method outperforms the state-of-the-artmethods in performance, as tested on both existing datasets and our newlydeveloped datasets. Furthermore, pretraining our method or other prior methodsusing our synthetic dataset can enhance their performance on other datasets.Since our method can accommodate any light conditions, it is capable ofproducing animated results. The dataset, method, and videos are publiclyavailable.</description><author>Yixiong Yang, Hassan Ahmed Sial, Ramon Baldrich, Maria Vanrell</author><pubDate>Fri, 27 Sep 2024 14:15:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18770v1</guid></item><item><title>State-of-the-Art Periorbital Distance Prediction and Disease Classification Using Periorbital Features</title><link>http://arxiv.org/abs/2409.18769v1</link><description>Periorbital distances and features around the eyes and lids hold valuableinformation for disease quantification and monitoring of surgical and medicalintervention. These distances are commonly measured manually, a process that isboth subjective and highly time-consuming. Here, we set out to developed threedeep-learning methods for segmentation and periorbital distance prediction, andalso evaluate the utility of periorbital distances for disease classification.The MAE of our deep learning predicted distances was less than or very close tothe error observed between trained human annotators. We compared our models tothe current state-of-the-art (SOTA) method for periorbital distance predictionand found that our methods outperformed SOTA on all of our datasets on all butone periorbital measurement. We also show that robust segmentation can beachieved on diseased eyes using models trained on open-source, healthy eyes,and that periorbital distances have can be used as high-quality features indownstream classification models. Leveraging segmentation networks asintermediary steps in classification has broad implications for increasing thegeneralizability of classification models in ophthalmic plastic andcraniofacial surgery by avoiding the out-of-distribution problem observed intraditional convolutional neural networks.</description><author>George R. Nahass, Ghasem Yazdanpanah, Madison Cheung, Alex Palacios, Jeffery Peterson, Kevin Heinze, Sasha Hubschman, Chad A. Purnell, Pete Setabutr, Ann Q. Tran, Darvin Yi</author><pubDate>Fri, 27 Sep 2024 14:14:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18769v1</guid></item><item><title>Learning from Demonstration with Implicit Nonlinear Dynamics Models</title><link>http://arxiv.org/abs/2409.18768v1</link><description>Learning from Demonstration (LfD) is a useful paradigm for training policiesthat solve tasks involving complex motions. In practice, the successfulapplication of LfD requires overcoming error accumulation during policyexecution, i.e. the problem of drift due to errors compounding over time andthe consequent out-of-distribution behaviours. Existing works seek to addressthis problem through scaling data collection, correcting policy errors with ahuman-in-the-loop, temporally ensembling policy predictions or through learningthe parameters of a dynamical system model. In this work, we propose andvalidate an alternative approach to overcoming this issue. Inspired byreservoir computing, we develop a novel neural network layer that includes afixed nonlinear dynamical system with tunable dynamical properties. We validatethe efficacy of our neural network layer on the task of reproducing humanhandwriting motions using the LASA Human Handwriting Dataset. Through empiricalexperiments we demonstrate that incorporating our layer into existing neuralnetwork architectures addresses the issue of compounding errors in LfD.Furthermore, we perform a comparative evaluation against existing approachesincluding a temporal ensemble of policy predictions and an Echo State Networks(ESNs) implementation. We find that our approach yields greater policyprecision and robustness on the handwriting task while also generalising tomultiple dynamics regimes and maintaining competitive latency scores.</description><author>Peter David Fagan, Subramanian Ramamoorthy</author><pubDate>Fri, 27 Sep 2024 14:12:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18768v1</guid></item></channel></rss>