<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 11 Dec 2023 06:00:14 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Disentangling CO Chemistry in a Protoplanetary Disk Using Explanatory Machine Learning Techniques</title><link>http://arxiv.org/abs/2312.05254v1</link><description>Molecular abundances in protoplanetary disks are highly sensitive to thelocal physical conditions, including gas temperature, gas density, radiationfield, and dust properties. Often multiple factors are intertwined, impactingthe abundances of both simple and complex species. We present a new approach tounderstanding these chemical and physical interdependencies using machinelearning. Specifically we explore the case of CO modeled under the conditionsof a generic disk and build an explanatory regression model to study thedependence of CO spatial density on the gas density, gas temperature, cosmicray ionization rate, X-ray ionization rate, and UV flux. Our findings indicatethat combinations of parameters play a surprisingly powerful role in regulatingCO compared to any singular physical parameter. Moreover, in general, we findthe conditions in the disk are destructive toward CO. CO depletion is furtherenhanced in an increased cosmic ray environment and in disks with higherinitial C/O ratios. These dependencies uncovered by our new approach areconsistent with previous studies, which are more modeling intensive andcomputationally expensive. Our work thus shows that machine learning can be apowerful tool not only for creating efficient predictive models, but also forenabling a deeper understanding of complex chemical processes.</description><author>Amina Diop, Ilse Cleeves, Dana Anderson, Jamila Pegues, Adele Plunkett</author><pubDate>Fri, 08 Dec 2023 18:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05254v1</guid></item><item><title>KBFormer: A Diffusion Model for Structured Entity Completion</title><link>http://arxiv.org/abs/2312.05253v1</link><description>We develop a generative attention-based approach to modeling structuredentities comprising different property types, such as numerical, categorical,string, and composite. This approach handles such heterogeneous data through amixed continuous-discrete diffusion process over the properties. Our flexibleframework can model entities with arbitrary hierarchical properties, enablingapplications to structured Knowledge Base (KB) entities and tabular data. Ourapproach obtains state-of-the-art performance on a majority of cases across 15datasets. In addition, experiments with a device KB and a nuclear physicsdataset demonstrate the model's ability to learn representations useful forentity completion in diverse settings. This has many downstream use cases,including modeling numerical properties with high accuracy - critical forscience applications, which also benefit from the model's inherentprobabilistic nature.</description><author>Ouail Kitouni, Niklas Nolte, James Hensman, Bhaskar Mitra</author><pubDate>Fri, 08 Dec 2023 18:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05253v1</guid></item><item><title>Reconstructing Hands in 3D with Transformers</title><link>http://arxiv.org/abs/2312.05251v1</link><description>We present an approach that can reconstruct hands in 3D from monocular input.Our approach for Hand Mesh Recovery, HaMeR, follows a fully transformer-basedarchitecture and can analyze hands with significantly increased accuracy androbustness compared to previous work. The key to HaMeR's success lies inscaling up both the data used for training and the capacity of the deep networkfor hand reconstruction. For training data, we combine multiple datasets thatcontain 2D or 3D hand annotations. For the deep model, we use a large scaleVision Transformer architecture. Our final model consistently outperforms theprevious baselines on popular 3D hand pose benchmarks. To further evaluate theeffect of our design in non-controlled settings, we annotate existingin-the-wild datasets with 2D hand keypoint annotations. On this newly collecteddataset of annotations, HInt, we demonstrate significant improvements overexisting baselines. We make our code, data and models available on the projectwebsite: https://geopavlakos.github.io/hamer/.</description><author>Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, Jitendra Malik</author><pubDate>Fri, 08 Dec 2023 18:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05251v1</guid></item><item><title>TaskMet: Task-Driven Metric Learning for Model Learning</title><link>http://arxiv.org/abs/2312.05250v1</link><description>Deep learning models are often deployed in downstream tasks that the trainingprocedure may not be aware of. For example, models solely trained to achieveaccurate predictions may struggle to perform well on downstream tasks becauseseemingly small prediction errors may incur drastic task errors. The standardend-to-end learning approach is to make the task loss differentiable or tointroduce a differentiable surrogate that the model can be trained on. In thesesettings, the task loss needs to be carefully balanced with the prediction lossbecause they may have conflicting objectives. We propose take the task losssignal one level deeper than the parameters of the model and use it to learnthe parameters of the loss function the model is trained on, which can be doneby learning a metric in the prediction space. This approach does not alter theoptimal prediction model itself, but rather changes the model learning toemphasize the information important for the downstream task. This enables us toachieve the best of both worlds: a prediction model trained in the originalprediction space while also being valuable for the desired downstream task. Wevalidate our approach through experiments conducted in two main settings: 1)decision-focused model learning scenarios involving portfolio optimization andbudget allocation, and 2) reinforcement learning in noisy environments withdistracting states. The source code to reproduce our experiments is availableat https://github.com/facebookresearch/taskmet</description><author>Dishank Bansal, Ricky T. Q. Chen, Mustafa Mukadam, Brandon Amos</author><pubDate>Fri, 08 Dec 2023 18:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05250v1</guid></item><item><title>Max-Margin Token Selection in Attention Mechanism</title><link>http://arxiv.org/abs/2306.13596v4</link><description>Attention mechanism is a central component of the transformer architecturewhich led to the phenomenal success of large language models. However, thetheoretical principles underlying the attention mechanism are poorlyunderstood, especially its nonconvex optimization dynamics. In this work, weexplore the seminal softmax-attention model $f(\boldsymbol{X})=\langle\boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where$\boldsymbol{X}$ is the token sequence and$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are trainable parameters. Weprove that running gradient descent on $\boldsymbol{p}$, or equivalently$\boldsymbol{W}$, converges in direction to a max-margin solution thatseparates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearlyformalizes attention as an optimal token selection mechanism. Remarkably, ourresults are applicable to general data and precisely characterize$\textit{optimality}$ of tokens in terms of the value embeddings$\boldsymbol{Xv}$ and problem geometry. We also provide a broaderregularization path analysis that establishes the margin maximizing nature ofattention even for nonlinear prediction heads. When optimizing $\boldsymbol{v}$and $\boldsymbol{p}$ simultaneously with logistic loss, we identify conditionsunder which the regularization paths directionally converge to their respectivehard-margin SVM solutions where $\boldsymbol{v}$ separates the input featuresbased on their labels. Interestingly, the SVM formulation of $\boldsymbol{p}$is influenced by the support vector geometry of $\boldsymbol{v}$. Finally, weverify our theoretical findings via numerical experiments and provide insights.</description><author>Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, Samet Oymak</author><pubDate>Fri, 08 Dec 2023 18:56:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13596v4</guid></item><item><title>Topology-Based Reconstruction Prevention for Decentralised Learning</title><link>http://arxiv.org/abs/2312.05248v1</link><description>Decentralised learning has recently gained traction as an alternative tofederated learning in which both data and coordination are distributed over itsusers. To preserve the confidentiality of users' data, decentralised learningrelies on differential privacy, multi-party computation, or a combinationthereof. However, running multiple privacy-preserving summations in sequencemay allow adversaries to perform reconstruction attacks. Unfortunately, currentreconstruction countermeasures either cannot trivially be adapted to thedistributed setting, or add excessive amounts of noise. In this work, we first show that passive honest-but-curious adversaries canreconstruct other users' private data after several privacy-preservingsummations. For example, in subgraphs with 18 users, we show that only threepassive honest-but-curious adversaries succeed at reconstructing private data11.0% of the time, requiring an average of 8.8 summations per adversary. Thesuccess rate is independent of the size of the full network. We consider weakadversaries, who do not control the graph topology and can exploit neither theworkings of the summation protocol nor the specifics of users' data. We develop a mathematical understanding of how reconstruction relates totopology and propose the first topology-based decentralised defence againstreconstruction attacks. Specifically, we show that reconstruction requires anumber of adversaries linear in the length of the network's shortest cycle.Consequently, reconstructing private data from privacy-preserving summations isimpossible in acyclic networks. Our work is a stepping stone for a formal theory of decentralisedreconstruction defences based on topology. Such a theory would generalise ourcountermeasure beyond summation, define confidentiality in terms of entropy,and describe the effects of (topology-aware) differential privacy.</description><author>Florine W. Dekker, Zekeriya Erkin, Mauro Conti</author><pubDate>Fri, 08 Dec 2023 18:55:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05248v1</guid></item><item><title>Dynamic LiDAR Re-simulation using Compositional Neural Fields</title><link>http://arxiv.org/abs/2312.05247v1</link><description>We introduce DyNFL, a novel neural field-based approach for high-fidelityre-simulation of LiDAR scans in dynamic driving scenes. DyNFL processes LiDARmeasurements from dynamic environments, accompanied by bounding boxes of movingobjects, to construct an editable neural field. This field, comprisingseparately reconstructed static backgrounds and dynamic objects, allows usersto modify viewpoints, adjust object positions, and seamlessly add or removeobjects in the re-simulated scene. A key innovation of our method is the neuralfield composition technique, which effectively integrates reconstructed neuralassets from various scenes through a ray drop test, accounting for occlusionsand transparent surfaces. Our evaluation with both synthetic and real-worldenvironments demonstrates that \ShortName substantial improves dynamic scenesimulation based on LiDAR scans, offering a combination of physical fidelityand flexible editing capabilities.</description><author>Hanfeng Wu, Xingxing Zuo, Stefan Leutenegger, Or Litany, Konrad Schindler, Shengyu Huang</author><pubDate>Fri, 08 Dec 2023 18:55:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05247v1</guid></item><item><title>Gradient-Based Spectral Embeddings of Random Dot Product Graphs</title><link>http://arxiv.org/abs/2307.13818v2</link><description>The Random Dot Product Graph (RDPG) is a generative model for relationaldata, where nodes are represented via latent vectors in low-dimensionalEuclidean space. RDPGs crucially postulate that edge formation probabilitiesare given by the dot product of the corresponding latent positions.Accordingly, the embedding task of estimating these vectors from an observedgraph is typically posed as a low-rank matrix factorization problem. Theworkhorse Adjacency Spectral Embedding (ASE) enjoys solid statisticalproperties, but it is formally solving a surrogate problem and can becomputationally intensive. In this paper, we bring to bear recent advances innon-convex optimization and demonstrate their impact to RDPG inference. Weadvocate first-order gradient descent methods to better solve the embeddingproblem, and to organically accommodate broader network embedding applicationsof practical relevance. Notably, we argue that RDPG embeddings of directedgraphs loose interpretability unless the factor matrices are constrained tohave orthogonal columns. We thus develop a novel feasible optimization methodin the resulting manifold. The effectiveness of the graph representationlearning framework is demonstrated on reproducible experiments with bothsynthetic and real network data. Our open-source algorithm implementations arescalable, and unlike the ASE they are robust to missing edge data and can trackslowly-varying latent positions from streaming graphs.</description><author>Marcelo Fiori, Bernardo Marenco, Federico Larroca, Paola Bermolen, Gonzalo Mateos</author><pubDate>Fri, 08 Dec 2023 18:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13818v2</guid></item><item><title>Predictive auxiliary objectives in deep RL mimic learning in the brain</title><link>http://arxiv.org/abs/2310.06089v2</link><description>The ability to predict upcoming events has been hypothesized to comprise akey aspect of natural and machine cognition. This is supported by trends indeep reinforcement learning (RL), where self-supervised auxiliary objectivessuch as prediction are widely used to support representation learning andimprove task performance. Here, we study the effects predictive auxiliaryobjectives have on representation learning across different modules of an RLsystem and how these mimic representational changes observed in the brain. Wefind that predictive objectives improve and stabilize learning particularly inresource-limited architectures, and we identify settings where longerpredictive horizons better support representational transfer. Furthermore, wefind that representational changes in this RL system bear a strikingresemblance to changes in neural activity observed in the brain across variousexperiments. Specifically, we draw a connection between the auxiliarypredictive model of the RL system and hippocampus, an area thought to learn apredictive model to support memory-guided behavior. We also connect the encodernetwork and the value learning network of the RL system to visual cortex andstriatum in the brain, respectively. This work demonstrates how representationlearning in deep RL systems can provide an interpretable framework for modelingmulti-region interactions in the brain. The deep RL perspective taken here alsosuggests an additional role of the hippocampus in the brain -- that of anauxiliary learning system that benefits representation learning in otherregions.</description><author>Ching Fang, Kimberly L Stachenfeld</author><pubDate>Fri, 08 Dec 2023 18:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06089v2</guid></item><item><title>SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation</title><link>http://arxiv.org/abs/2312.05239v1</link><description>Despite their ability to generate high-resolution and diverse images fromtext prompts, text-to-image diffusion models often suffer from slow iterativesampling processes. Model distillation is one of the most effective directionsto accelerate these models. However, previous distillation methods fail toretain the generation quality while requiring a significant amount of imagesfor training, either from real data or synthetically generated by the teachermodel. In response to this limitation, we present a novel image-freedistillation scheme named $\textbf{SwiftBrush}$. Drawing inspiration fromtext-to-3D synthesis, in which a 3D neural radiance field that aligns with theinput prompt can be obtained from a 2D text-to-image diffusion prior via aspecialized loss without the use of any 3D data ground-truth, our approachre-purposes that same loss for distilling a pretrained multi-step text-to-imagemodel to a student network that can generate high-fidelity images with just asingle inference step. In spite of its simplicity, our model stands as one ofthe first one-step text-to-image generators that can produce images ofcomparable quality to Stable Diffusion without reliance on any training imagedata. Remarkably, SwiftBrush achieves an FID score of $\textbf{16.67}$ and aCLIP score of $\textbf{0.29}$ on the COCO-30K benchmark, achieving competitiveresults or even substantially surpassing existing state-of-the-art distillationtechniques.</description><author>Thuan Hoang Nguyen, Anh Tran</author><pubDate>Fri, 08 Dec 2023 18:44:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05239v1</guid></item><item><title>Seeing ChatGPT Through Universities' Policies, Resources and Guidelines</title><link>http://arxiv.org/abs/2312.05235v1</link><description>The advancements in Artificial Intelligence (AI) technologies such as ChatGPThave gained popularity in recent days. The integration of ChatGPT ineducational contexts has already created attractions due to a wide range ofapplications. However, the automatic generation of human-like texts also posespotential risks to academic integrity, especially when faced withwriting-intensive language courses. Considering the ongoing debates, this studyaims to investigate the academic policies and guidelines established by USuniversities regarding the use of ChatGPT in teaching and learning. The datasources include academic policies, statements, guidelines as well as relevantresources that were provided by the top 50 universities in the United States,according to U.S. News. Thematic analysis and qualitative analysis wereemployed in the analysis and showed that most top 50 universities were open butcautious towards the integration of generative AI in teaching and learning andalso expressed their concerns on ethical usage, accuracy, and data privacy.Most universities also provided a variety of resources and guidelines,including syllabus templates/samples, workshops and discussions, sharedarticles, and one-on-one consultations, with focuses on general technicalintroduction, ethical concerns, pedagogical applications, preventivestrategies, data privacy, limitations, and detective tools. The findings willinform future policy-making regarding the integration of ChatGPT incollege-level education and influence the provision of supportive resources byuniversities for the appropriate application of ChatGPT in education.</description><author>Hui Wang, Anh Dang, Zihao Wu, Son Mac</author><pubDate>Fri, 08 Dec 2023 18:33:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05235v1</guid></item><item><title>The impact of heteroskedasticity on uplift modeling</title><link>http://arxiv.org/abs/2312.05234v1</link><description>There are various applications, where companies need to decide to whichindividuals they should best allocate treatment. To support such decisions,uplift models are applied to predict treatment effects on an individual level.Based on the predicted treatment effects, individuals can be ranked andtreatment allocation can be prioritized according to this ranking. An implicitassumption, which has not been doubted in the previous uplift modelingliterature, is that this treatment prioritization approach tends to bringindividuals with high treatment effects to the top and individuals with lowtreatment effects to the bottom of the ranking. In our research, we show thatheteroskedastictity in the training data can cause a bias of the uplift modelranking: individuals with the highest treatment effects can get accumulated inlarge numbers at the bottom of the ranking. We explain theoretically howheteroskedasticity can bias the ranking of uplift models and show this processin a simulation and on real-world data. We argue that this problem of rankingbias due to heteroskedasticity might occur in many real-world applications andrequires modification of the treatment prioritization to achieve an efficienttreatment allocation.</description><author>Björn Bokelmann, Stefan Lessmann</author><pubDate>Fri, 08 Dec 2023 18:32:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05234v1</guid></item><item><title>Investigating how ReLU-networks encode symmetries</title><link>http://arxiv.org/abs/2305.17017v2</link><description>Many data symmetries can be described in terms of group equivariance and themost common way of encoding group equivariances in neural networks is bybuilding linear layers that are group equivariant. In this work we investigatewhether equivariance of a network implies that all layers are equivariant. Onthe theoretical side we find cases where equivariance implies layerwiseequivariance, but also demonstrate that this is not the case generally.Nevertheless, we conjecture that CNNs that are trained to be equivariant willexhibit layerwise equivariance and explain how this conjecture is a weakerversion of the recent permutation conjecture by Entezari et al. [2022]. Weperform quantitative experiments with VGG-nets on CIFAR10 and qualitativeexperiments with ResNets on ImageNet to illustrate and support our theoreticalfindings. These experiments are not only of interest for understanding howgroup equivariance is encoded in ReLU-networks, but they also give a newperspective on Entezari et al.'s permutation conjecture as we find that it istypically easier to merge a network with a group-transformed version of itselfthan merging two different networks.</description><author>Georg Bökman, Fredrik Kahl</author><pubDate>Fri, 08 Dec 2023 18:27:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17017v2</guid></item><item><title>Modeling Risk in Reinforcement Learning: A Literature Mapping</title><link>http://arxiv.org/abs/2312.05231v1</link><description>Safe reinforcement learning deals with mitigating or avoiding unsafesituations by reinforcement learning (RL) agents. Safe RL approaches are basedon specific risk representations for particular problems or domains. In orderto analyze agent behaviors, compare safe RL approaches, and effectivelytransfer techniques between application domains, it is necessary to understandthe types of risk specific to safe RL problems. We performed a systematicliterature mapping with the objective to characterize risk in safe RL. Based onthe obtained results, we present definitions, characteristics, and types ofrisk that hold on multiple application domains. Our literature mapping coversliterature from the last 5 years (2017-2022), from a variety of knowledge areas(AI, finance, engineering, medicine) where RL approaches emphasize riskrepresentation and management. Our mapping covers 72 papers filteredsystematically from over thousands of papers on the topic. Our proposed notionof risk covers a variety of representations, disciplinary differences, commontraining exercises, and types of techniques. We encourage researchers toinclude explicit and detailed accounts of risk in future safe RL researchreports, using this mapping as a starting point. With this information,researchers and practitioners could draw stronger conclusions on theeffectiveness of techniques on different problems.</description><author>Leonardo Villalobos-Arias, Derek Martin, Abhijeet Krishnan, Madeleine Gagné, Colin M. Potts, Arnav Jhala</author><pubDate>Fri, 08 Dec 2023 18:26:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05231v1</guid></item><item><title>Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning</title><link>http://arxiv.org/abs/2312.05230v1</link><description>Despite their tremendous success in many applications, large language modelsoften fall short of consistent reasoning and planning in various (language,embodied, and social) scenarios, due to inherent limitations in theirinference, learning, and modeling capabilities. In this position paper, wepresent a new perspective of machine reasoning, LAW, that connects the conceptsof Language models, Agent models, and World models, for more robust andversatile reasoning capabilities. In particular, we propose that world andagent models are a better abstraction of reasoning, that introduces the crucialelements of deliberate human-like reasoning, including beliefs about the worldand other agents, anticipation of consequences, goals/rewards, and strategicplanning. Crucially, language models in LAW serve as a backend to implement thesystem or its elements and hence provide the computational power andadaptability. We review the recent studies that have made relevant progress anddiscuss future research directions towards operationalizing the LAW framework.</description><author>Zhiting Hu, Tianmin Shu</author><pubDate>Fri, 08 Dec 2023 18:25:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05230v1</guid></item><item><title>Few-Shot Class-Incremental Learning via Training-Free Prototype Calibration</title><link>http://arxiv.org/abs/2312.05229v1</link><description>Real-world scenarios are usually accompanied by continuously appearingclasses with scare labeled samples, which require the machine learning model toincrementally learn new classes and maintain the knowledge of base classes. Inthis Few-Shot Class-Incremental Learning (FSCIL) scenario, existing methodseither introduce extra learnable components or rely on a frozen featureextractor to mitigate catastrophic forgetting and overfitting problems.However, we find a tendency for existing methods to misclassify the samples ofnew classes into base classes, which leads to the poor performance of newclasses. In other words, the strong discriminability of base classes distractsthe classification of new classes. To figure out this intriguing phenomenon, weobserve that although the feature extractor is only trained on base classes, itcan surprisingly represent the semantic similarity between the base and unseennew classes. Building upon these analyses, we propose a simple yet effectiveTraining-frEE calibratioN (TEEN) strategy to enhance the discriminability ofnew classes by fusing the new prototypes (i.e., mean features of a class) withweighted base prototypes. In addition to standard benchmarks in FSCIL, TEENdemonstrates remarkable performance and consistent improvements over baselinemethods in the few-shot learning scenario. Code is available at:https://github.com/wangkiw/TEEN</description><author>Qi-Wei Wang, Da-Wei Zhou, Yi-Kai Zhang, De-Chuan Zhan, Han-Jia Ye</author><pubDate>Fri, 08 Dec 2023 18:24:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05229v1</guid></item><item><title>Neural Spectral Methods: Self-supervised learning in the spectral domain</title><link>http://arxiv.org/abs/2312.05225v1</link><description>We present Neural Spectral Methods, a technique to solve parametric PartialDifferential Equations (PDEs), grounded in classical spectral methods. Ourmethod uses orthogonal bases to learn PDE solutions as mappings betweenspectral coefficients. In contrast to current machine learning approaches whichenforce PDE constraints by minimizing the numerical quadrature of the residualsin the spatiotemporal domain, we leverage Parseval's identity and introduce anew training strategy through a \textit{spectral loss}. Our spectral lossenables more efficient differentiation through the neural network, andsubstantially reduces training complexity. At inference time, the computationalcost of our method remains constant, regardless of the spatiotemporalresolution of the domain. Our experimental results demonstrate that our methodsignificantly outperforms previous machine learning approaches in terms ofspeed and accuracy by one to two orders of magnitude on multiple differentproblems. When compared to numerical solvers of the same accuracy, our methoddemonstrates a $10\times$ increase in performance speed.</description><author>Yiheng Du, Nithin Chalapathi, Aditi Krishnapriyan</author><pubDate>Fri, 08 Dec 2023 18:20:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05225v1</guid></item><item><title>Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit</title><link>http://arxiv.org/abs/2309.16620v2</link><description>The cost of hyperparameter tuning in deep learning has been rising with modelsizes, prompting practitioners to find new tuning methods using a proxy ofsmaller networks. One such proposal uses $\mu$P parameterized networks, wherethe optimal hyperparameters for small width networks transfer to networks witharbitrarily large width. However, in this scheme, hyperparameters do nottransfer across depths. As a remedy, we study residual networks with a residualbranch scale of $1/\sqrt{\text{depth}}$ in combination with the $\mu$Pparameterization. We provide experiments demonstrating that residualarchitectures including convolutional ResNets and Vision Transformers trainedwith this parameterization exhibit transfer of optimal hyperparameters acrosswidth and depth on CIFAR-10 and ImageNet. Furthermore, our empirical findingsare supported and motivated by theory. Using recent developments in thedynamical mean field theory (DMFT) description of neural network learningdynamics, we show that this parameterization of ResNets admits a well-definedfeature learning joint infinite-width and infinite-depth limit and showconvergence of finite-size network dynamics towards this limit.</description><author>Blake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, Cengiz Pehlevan</author><pubDate>Fri, 08 Dec 2023 18:19:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16620v2</guid></item><item><title>Shape Matters: Detecting Vertebral Fractures Using Differentiable Point-Based Shape Decoding</title><link>http://arxiv.org/abs/2312.05220v1</link><description>Degenerative spinal pathologies are highly prevalent among the elderlypopulation. Timely diagnosis of osteoporotic fractures and other degenerativedeformities facilitates proactive measures to mitigate the risk of severe backpain and disability. In this study, we specifically explore the use of shapeauto-encoders for vertebrae, taking advantage of advancements in automatedmulti-label segmentation and the availability of large datasets forunsupervised learning. Our shape auto-encoders are trained on a large set ofvertebrae surface patches, leveraging the vast amount of available data forvertebra segmentation. This addresses the label scarcity problem faced whenlearning shape information of vertebrae from image intensities. Based on thelearned shape features we train an MLP to detect vertebral body fractures.Using segmentation masks that were automatically generated using theTotalSegmentator, our proposed method achieves an AUC of 0.901 on the VerSe19testset. This outperforms image-based and surface-based end-to-end trainedmodels. Additionally, our results demonstrate that pre-training the models inan unsupervised manner enhances geometric methods like PointNet and DGCNN. Ourfindings emphasise the advantages of explicitly learning shape features fordiagnosing osteoporotic vertebrae fractures. This approach improves thereliability of classification results and reduces the need for annotatedlabels. This study provides novel insights into the effectiveness of variousencoder-decoder models for shape analysis of vertebrae and proposes a newdecoder architecture: the point-based shape decoder.</description><author>Hellena Hempe, Alexander Bigalke, Mattias P. Heinrich</author><pubDate>Fri, 08 Dec 2023 18:11:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05220v1</guid></item><item><title>Enhancing Facial Classification and Recognition using 3D Facial Models and Deep Learning</title><link>http://arxiv.org/abs/2312.05219v1</link><description>Accurate analysis and classification of facial attributes are essential invarious applications, from human-computer interaction to security systems. Inthis work, a novel approach to enhance facial classification and recognitiontasks through the integration of 3D facial models with deep learning methodswas proposed. We extract the most useful information for various tasks usingthe 3D Facial Model, leading to improved classification accuracy. Combining 3Dfacial insights with ResNet architecture, our approach achieves notableresults: 100% individual classification, 95.4% gender classification, and 83.5%expression classification accuracy. This method holds promise for advancingfacial analysis and recognition research.</description><author>Houting Li, Mengxuan Dong, Lok Ming Lui</author><pubDate>Fri, 08 Dec 2023 18:09:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05219v1</guid></item><item><title>DeltaZip: Multi-Tenant Language Model Serving via Delta Compression</title><link>http://arxiv.org/abs/2312.05215v1</link><description>Fine-tuning large language models (LLMs) for downstream tasks can greatlyimprove model quality, however serving many different fine-tuned LLMsconcurrently for users in multi-tenant environments is challenging. DedicatingGPU memory for each model is prohibitively expensive and naively swapping largemodel weights in and out of GPU memory is slow. Our key insight is thatfine-tuned models can be quickly swapped in and out of GPU memory by extractingand compressing the delta between each model and its pre-trained base model. Wepropose DeltaZip, an LLM serving system that efficiently serves multiplefull-parameter fine-tuned models concurrently by aggressively compressing modeldeltas by a factor of $6\times$ to $8\times$ while maintaining high modelquality. DeltaZip increases serving throughput by $1.5\times$ to $3\times$ andimproves SLO attainment compared to a vanilla HuggingFace serving system.</description><author>Xiaozhe Yao, Ana Klimovic</author><pubDate>Fri, 08 Dec 2023 18:07:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05215v1</guid></item><item><title>IntrinsicAvatar: Physically Based Inverse Rendering of Dynamic Humans from Monocular Videos via Explicit Ray Tracing</title><link>http://arxiv.org/abs/2312.05210v1</link><description>We present IntrinsicAvatar, a novel approach to recovering the intrinsicproperties of clothed human avatars including geometry, albedo, material, andenvironment lighting from only monocular videos. Recent advancements inhuman-based neural rendering have enabled high-quality geometry and appearancereconstruction of clothed humans from just monocular videos. However, thesemethods bake intrinsic properties such as albedo, material, and environmentlighting into a single entangled neural representation. On the other hand, onlya handful of works tackle the problem of estimating geometry and disentangledappearance properties of clothed humans from monocular videos. They usuallyachieve limited quality and disentanglement due to approximations of secondaryshading effects via learned MLPs. In this work, we propose to model secondaryshading effects explicitly via Monte-Carlo ray tracing. We model the renderingprocess of clothed humans as a volumetric scattering process, and combine raytracing with body articulation. Our approach can recover high-quality geometry,albedo, material, and lighting properties of clothed humans from a singlemonocular video, without requiring supervised pre-training using ground truthmaterials. Furthermore, since we explicitly model the volumetric scatteringprocess and ray tracing, our model naturally generalizes to novel poses,enabling animation of the reconstructed avatar in novel lighting conditions.</description><author>Shaofei Wang, Božidar Antić, Andreas Geiger, Siyu Tang</author><pubDate>Fri, 08 Dec 2023 17:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05210v1</guid></item><item><title>HALO: An Ontology for Representing Hallucinations in Generative Models</title><link>http://arxiv.org/abs/2312.05209v1</link><description>Recent progress in generative AI, including large language models (LLMs) likeChatGPT, has opened up significant opportunities in fields ranging from naturallanguage processing to knowledge discovery and data mining. However, there isalso a growing awareness that the models can be prone to problems such asmaking information up or `hallucinations', and faulty reasoning on seeminglysimple problems. Because of the popularity of models like ChatGPT, bothacademic scholars and citizen scientists have documented hallucinations ofseveral different types and severity. Despite this body of work, a formal modelfor describing and representing these hallucinations (with relevant meta-data)at a fine-grained level, is still lacking. In this paper, we address this gapby presenting the Hallucination Ontology or HALO, a formal, extensible ontologywritten in OWL that currently offers support for six different types ofhallucinations known to arise in LLMs, along with support for provenance andexperimental metadata. We also collect and publish a dataset containinghallucinations that we inductively gathered across multiple independent Websources, and show that HALO can be successfully used to model this dataset andanswer competency questions.</description><author>Navapat Nananukul, Mayank Kejriwal</author><pubDate>Fri, 08 Dec 2023 17:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05209v1</guid></item><item><title>ControlRoom3D: Room Generation using Semantic Proxy Rooms</title><link>http://arxiv.org/abs/2312.05208v1</link><description>Manually creating 3D environments for AR/VR applications is a complex processrequiring expert knowledge in 3D modeling software. Pioneering works facilitatethis process by generating room meshes conditioned on textual styledescriptions. Yet, many of these automatically generated 3D meshes do notadhere to typical room layouts, compromising their plausibility, e.g., byplacing several beds in one bedroom. To address these challenges, we presentControlRoom3D, a novel method to generate high-quality room meshes. Central toour approach is a user-defined 3D semantic proxy room that outlines a roughroom layout based on semantic bounding boxes and a textual description of theoverall room style. Our key insight is that when rendered to 2D, this 3Drepresentation provides valuable geometric and semantic information to controlpowerful 2D models to generate 3D consistent textures and geometry that alignswell with the proxy room. Backed up by an extensive study includingquantitative metrics and qualitative user evaluations, our method generatesdiverse and globally plausible 3D room meshes, thus empowering users to design3D rooms effortlessly without specialized knowledge.</description><author>Jonas Schult, Sam Tsai, Lukas Höllein, Bichen Wu, Jialiang Wang, Chih-Yao Ma, Kunpeng Li, Xiaofang Wang, Felix Wimbauer, Zijian He, Peizhao Zhang, Bastian Leibe, Peter Vajda, Ji Hou</author><pubDate>Fri, 08 Dec 2023 17:55:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05208v1</guid></item><item><title>DelucionQA: Detecting Hallucinations in Domain-specific Question Answering</title><link>http://arxiv.org/abs/2312.05200v1</link><description>Hallucination is a well-known phenomenon in text generated by large languagemodels (LLMs). The existence of hallucinatory responses is found in almost allapplication scenarios e.g., summarization, question-answering (QA) etc. Forapplications requiring high reliability (e.g., customer-facing assistants), thepotential existence of hallucination in LLM-generated text is a criticalproblem. The amount of hallucination can be reduced by leveraging informationretrieval to provide relevant background information to the LLM. However, LLMscan still generate hallucinatory content for various reasons (e.g.,prioritizing its parametric knowledge over the context, failure to capture therelevant information from the context, etc.). Detecting hallucinations throughautomated methods is thus paramount. To facilitate research in this direction,we introduce a sophisticated dataset, DelucionQA, that captures hallucinationsmade by retrieval-augmented LLMs for a domain-specific QA task. Furthermore, wepropose a set of hallucination detection methods to serve as baselines forfuture works from the research community. Analysis and case study are alsoprovided to share valuable insights on hallucination phenomena in the targetscenario.</description><author>Mobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun Araki, Arsalan Gundroo, Bingqing Wang, Rakesh R Menon, Md Rizwan Parvez, Zhe Feng</author><pubDate>Fri, 08 Dec 2023 17:41:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05200v1</guid></item><item><title>Conformal Prediction in Multi-User Settings: An Evaluation</title><link>http://arxiv.org/abs/2312.05195v1</link><description>Typically, machine learning models are trained and evaluated without makingany distinction between users (e.g, using traditional hold-out andcross-validation). However, this produces inaccurate performance metricsestimates in multi-user settings. That is, situations where the data werecollected by multiple users with different characteristics (e.g., age, gender,height, etc.) which is very common in user computer interaction and medicalapplications. For these types of scenarios model evaluation strategies thatprovide better performance estimates have been proposed such as mixed,user-independent, user-dependent, and user-adaptive models. Although thosestrategies are better suited for multi-user systems, they are typicallyassessed with respect to performance metrics that capture the overall behaviorof the models and do not provide any performance guarantees for individualpredictions nor they provide any feedback about the predictions' uncertainty.In order to overcome those limitations, in this work we evaluated the conformalprediction framework in several multi-user settings. Conformal prediction is amodel agnostic method that provides confidence guarantees on the predictions,thus, increasing the trustworthiness and robustness of the models. We conductedextensive experiments using different evaluation strategies and foundsignificant differences in terms of conformal performance measures. We alsoproposed several visualizations based on matrices, graphs, and charts thatcapture different aspects of the resulting prediction sets.</description><author>Enrique Garcia-Ceja, Luciano Garcia-Banuelos, Nicolas Jourdan</author><pubDate>Fri, 08 Dec 2023 17:33:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05195v1</guid></item><item><title>Zero-Shot Point Cloud Registration</title><link>http://arxiv.org/abs/2312.03032v2</link><description>Learning-based point cloud registration approaches have significantlyoutperformed their traditional counterparts. However, they typically requireextensive training on specific datasets. In this paper, we propose , the firstzero-shot point cloud registration approach that eliminates the need fortraining on point cloud datasets. The cornerstone of ZeroReg is the noveltransfer of image features from keypoints to the point cloud, enriched byaggregating information from 3D geometric neighborhoods. Specifically, weextract keypoints and features from 2D image pairs using a frozen pretrained 2Dbackbone. These features are then projected in 3D, and patches are constructedby searching for neighboring points. We integrate the geometric and visualfeatures of each point using our novel parameter-free geometric decoder.Subsequently, the task of determining correspondences between point clouds isformulated as an optimal transport problem. Extensive evaluations of ZeroRegdemonstrate its competitive performance against both traditional andlearning-based methods. On benchmarks such as 3DMatch, 3DLoMatch, and ScanNet,ZeroReg achieves impressive Recall Ratios (RR) of over 84%, 46%, and 75%,respectively.</description><author>Weijie Wang, Guofeng Mei, Bin Ren, Xiaoshui Huang, Fabio Poiesi, Luc Van Gool, Nicu Sebe, Bruno Lepri</author><pubDate>Fri, 08 Dec 2023 17:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03032v2</guid></item><item><title>SymNoise: Advancing Language Model Fine-tuning with Symmetric Noise</title><link>http://arxiv.org/abs/2312.01523v2</link><description>In this paper, we introduce a novel fine-tuning technique for languagemodels, which involves incorporating symmetric noise into the embeddingprocess. This method aims to enhance the model's function by more stringentlyregulating its local curvature, demonstrating superior performance over thecurrent method, NEFTune. When fine-tuning the LLaMA-2-7B model using Alpaca,standard techniques yield a 29.79% score on AlpacaEval. However, our approach,SymNoise, increases this score significantly to 69.04%, using symmetric noisyembeddings. This is a 6.7% improvement over the state-of-the-art method,NEFTune~(64.69%). Furthermore, when tested on various models and strongerbaseline instruction datasets, such as Evol-Instruct, ShareGPT, OpenPlatypus,SymNoise consistently outperforms NEFTune. The current literature, includingNEFTune, has underscored the importance of more in-depth research into theapplication of noise-based strategies in the fine-tuning of language models.Our approach, SymNoise, is another significant step towards this direction,showing notable improvement over the existing state-of-the-art method.</description><author>Abhay Kumar Yadav, Arjun Singh</author><pubDate>Fri, 08 Dec 2023 17:28:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01523v2</guid></item><item><title>Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI)</title><link>http://arxiv.org/abs/2308.11471v4</link><description>This work targets what we consider to be the foundational step for urbanairborne robots, a safe landing. Our attention is directed toward what we deemthe most crucial aspect of the safe landing perception stack: segmentation. Wepresent a streamlined reactive UAV system that employs visual servoing byharnessing the capabilities of open vocabulary image segmentation. Thisapproach can adapt to various scenarios with minimal adjustments, bypassing thenecessity for extensive data accumulation for refining internal models, thanksto its open vocabulary methodology. Given the limitations imposed by localauthorities, our primary focus centers on operations originating from altitudesof 100 meters. This choice is deliberate, as numerous preceding works havedealt with altitudes up to 30 meters, aligning with the capabilities of smallstereo cameras. Consequently, we leave the remaining 20m to be navigated usingconventional 3D path planning methods. Utilizing monocular cameras and imagesegmentation, our findings demonstrate the system's capability to successfullyexecute landing maneuvers at altitudes as low as 20 meters. However, thisapproach is vulnerable to intermittent and occasionally abrupt fluctuations inthe segmentation between frames in a video stream. To address this challenge,we enhance the image segmentation output by introducing what we call a dynamicfocus: a masking mechanism that self adjusts according to the current landingstage. This dynamic focus guides the control system to avoid regions beyond thedrone's safety radius projected onto the ground, thus mitigating the problemswith fluctuations. Through the implementation of this supplementary layer, ourexperiments have reached improvements in the landing success rate of almosttenfold when compared to global segmentation. All the source code is opensource and available online (github.com/MISTLab/DOVESEI).</description><author>Haechan Mark Bong, Rongge Zhang, Ricardo de Azambuja, Giovanni Beltrame</author><pubDate>Fri, 08 Dec 2023 17:22:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11471v4</guid></item><item><title>Fine Dense Alignment of Image Bursts through Camera Pose and Depth Estimation</title><link>http://arxiv.org/abs/2312.05190v1</link><description>This paper introduces a novel approach to the fine alignment of images in aburst captured by a handheld camera. In contrast to traditional techniques thatestimate two-dimensional transformations between frame pairs or rely ondiscrete correspondences, the proposed algorithm establishes densecorrespondences by optimizing both the camera motion and surface depth andorientation at every pixel. This approach improves alignment, particularly inscenarios with parallax challenges. Extensive experiments with synthetic burstsfeaturing small and even tiny baselines demonstrate that it outperforms thebest optical flow methods available today in this setting, without requiringany training. Beyond enhanced alignment, our method opens avenues for tasksbeyond simple image restoration, such as depth estimation and 3Dreconstruction, as supported by promising preliminary results. This positionsour approach as a versatile tool for various burst image processingapplications.</description><author>Bruno Lecouat, Yann Dubois de Mont-Marin, Théo Bodrito, Julien Mairal, Jean Ponce</author><pubDate>Fri, 08 Dec 2023 17:22:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05190v1</guid></item><item><title>Seamless: Multilingual Expressive and Streaming Speech Translation</title><link>http://arxiv.org/abs/2312.05187v1</link><description>Large-scale automatic speech translation systems today lack key features thathelp machine-mediated communication feel seamless when compared tohuman-to-human dialogue. In this work, we introduce a family of models thatenable end-to-end expressive and multilingual translations in a streamingfashion. First, we contribute an improved version of the massively multilingualand multimodal SeamlessM4T model-SeamlessM4T v2. This newer model,incorporating an updated UnitY2 framework, was trained on more low-resourcelanguage data. SeamlessM4T v2 provides the foundation on which our next twomodels are initiated. SeamlessExpressive enables translation that preservesvocal styles and prosody. Compared to previous efforts in expressive speechresearch, our work addresses certain underexplored aspects of prosody, such asspeech rate and pauses, while also preserving the style of one's voice. As forSeamlessStreaming, our model leverages the Efficient Monotonic MultiheadAttention mechanism to generate low-latency target translations without waitingfor complete source utterances. As the first of its kind, SeamlessStreamingenables simultaneous speech-to-speech/text translation for multiple source andtarget languages. To ensure that our models can be used safely and responsibly,we implemented the first known red-teaming effort for multimodal machinetranslation, a system for the detection and mitigation of added toxicity, asystematic evaluation of gender bias, and an inaudible localized watermarkingmechanism designed to dampen the impact of deepfakes. Consequently, we bringmajor components from SeamlessExpressive and SeamlessStreaming together to formSeamless, the first publicly available system that unlocks expressivecross-lingual communication in real-time. The contributions to this work arepublicly released and accessible athttps://github.com/facebookresearch/seamless_communication</description><author>Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma, Christopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan Mavlyutov, Alice Rakotoarison, Kaushik Ram Sadagopan, Abinesh Ramakrishnan, Tuan Tran, Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernandez, Cynthia Gao, Prangthip Hansanti, Elahe Kalbassi, Amanda Kallet, Artyom Kozhevnikov, Gabriel Mejia Gonzalez, Robin San Roman, Christophe Touret, Corinne Wong, Carleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen Chen, Marta R. Costa-jussà, Maha Elbayad, Hongyu Gong, Francisco Guzmán, Kevin Heffernan, Somya Jain, Ju</author><pubDate>Fri, 08 Dec 2023 17:18:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05187v1</guid></item><item><title>AI Competitions and Benchmarks: Competition platforms</title><link>http://arxiv.org/abs/2312.05185v1</link><description>The ecosystem of artificial intelligence competitions is a diverse andmultifaceted landscape, encompassing a variety of platforms that each hostnumerous competitions annually, alongside a plethora of specialized websitesdedicated to singular contests. These platforms adeptly manage the overarchingadministrative responsibilities inherent in orchestrating competitions, thusaffording organizers the liberty to allocate greater attention to other facetsof their contests. Notably, these platforms exhibit considerable diversity intheir operational functionalities, economic models, and community dynamics.This chapter conducts an extensive review of the foremost services in thisrealm and elucidates several alternative methodologies that facilitate theindependent hosting of such challenges. Keywords: competition platform,challenge hosting services, comparison.</description><author>Andrey Ustyuzhanin, Harald Carlens</author><pubDate>Fri, 08 Dec 2023 17:16:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05185v1</guid></item><item><title>Provably Bounding Neural Network Preimages</title><link>http://arxiv.org/abs/2302.01404v3</link><description>Most work on the formal verification of neural networks has focused onbounding the set of outputs that correspond to a given set of inputs (forexample, bounded perturbations of a nominal input). However, many use cases ofneural network verification require solving the inverse problem, orover-approximating the set of inputs that lead to certain outputs. We presentthe INVPROP algorithm for verifying properties over the preimage of a linearlyconstrained output set, which can be combined with branch-and-bound to increaseprecision. Contrary to other approaches, our efficient algorithm isGPU-accelerated and does not require a linear programming solver. Wedemonstrate our algorithm for identifying safe control regions for a dynamicalsystem via backward reachability analysis, verifying adversarial robustness,and detecting out-of-distribution inputs to a neural network. Our results showthat in certain settings, we find over-approximations over 2500x tighter thanprior work while being 2.5x faster. By strengthening robustness verificationwith output constraints, we consistently verify more properties than theprevious state-of-the-art on multiple benchmarks, including a large model with167k neurons in VNN-COMP 2023. Our algorithm has been incorporated into the$\alpha,\!\beta$-CROWN verifier, available at https://abcrown.org.</description><author>Suhas Kotha, Christopher Brix, Zico Kolter, Krishnamurthy Dvijotham, Huan Zhang</author><pubDate>Fri, 08 Dec 2023 17:14:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01404v3</guid></item><item><title>TENPLEX: Changing Resources of Deep Learning Jobs using Parallelizable Tensor Collections</title><link>http://arxiv.org/abs/2312.05181v1</link><description>Deep learning (DL) jobs use multi-dimensional parallelism, i.e they combinedata, model, and pipeline parallelism, to use large GPU clusters efficiently.This couples jobs tightly to a set of GPU devices, but jobs may experiencechanges to the device allocation: (i) resource elasticity during training addsor removes devices; (ii) hardware maintenance may require redeployment ondifferent devices; and (iii) device failures force jobs to run with fewerdevices. Current DL frameworks lack support for these scenarios, as they cannotchange the multi-dimensional parallelism of an already-running job in anefficient and model-independent way. We describe Tenplex, a state management library for DL frameworks thatenables jobs to change the GPU allocation and job parallelism at runtime.Tenplex achieves this by externalizing the DL job state during training as aparallelizable tensor collection (PTC). When the GPU allocation for the DL jobchanges, Tenplex uses the PTC to transform the DL job state: for the datasetstate, Tenplex repartitions it under data parallelism and exposes it to workersthrough a virtual file system; for the model state, Tenplex obtains it aspartitioned checkpoints and transforms them to reflect the new parallelizationconfiguration. For efficiency, these PTC transformations are executed inparallel with a minimum amount of data movement between devices and workers.Our experiments show that Tenplex enables DL jobs to support dynamicparallelization with low overhead.</description><author>Marcel Wagenländer, Guo Li, Bo Zhao, Luo Mai, Peter Pietzuch</author><pubDate>Fri, 08 Dec 2023 17:08:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05181v1</guid></item><item><title>PathFinder: Guided Search over Multi-Step Reasoning Paths</title><link>http://arxiv.org/abs/2312.05180v1</link><description>With recent advancements in large language models, methods likechain-of-thought prompting to elicit reasoning chains have been shown toimprove results on reasoning tasks. However, tasks that require multiple stepsof reasoning still pose significant challenges to state-of-the-art models.Drawing inspiration from the beam search algorithm, we propose PathFinder, atree-search-based reasoning path generation approach. It enhances diversebranching and multi-hop reasoning through the integration of dynamic decoding,enabled by varying sampling methods and parameters. Using constrainedreasoning, PathFinder integrates novel quality constraints, pruning, andexploration methods to enhance the efficiency and the quality of generation.Moreover, it includes scoring and ranking features to improve candidateselection. Our approach outperforms competitive baselines on three complexarithmetic and commonsense reasoning tasks by 6% on average. Our modelgeneralizes well to longer, unseen reasoning chains, reflecting similarcomplexities to beam search with large branching factors.</description><author>Olga Golovneva, Sean O'Brien, Ramakanth Pasunuru, Tianlu Wang, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz</author><pubDate>Fri, 08 Dec 2023 17:05:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05180v1</guid></item><item><title>Video-Based Rendering Techniques: A Survey</title><link>http://arxiv.org/abs/2312.05179v1</link><description>Three-dimensional reconstruction of events recorded on images has been acommon challenge between computer vision and computer graphics for a long time.Estimating the real position of objects and surfaces using vision as an inputis no trivial task and has been approached in several different ways. Althoughhuge progress has been made so far, there are several open issues to which ananswer is needed. The use of videos as an input for a rendering process(video-based rendering, VBR) is something that recently has been started to belooked upon and has added many other challenges and also solutions to theclassical image-based rendering issue (IBR). This article presents the state ofart on video-based rendering and image-based techniques that can be applied onthis scenario, evaluating the open issues yet to be solved, indicating wherefuture work should be focused.</description><author>Rafael Kuffner dos Anjos, João Madeiras Pereira, José Antonio Gaspar</author><pubDate>Fri, 08 Dec 2023 17:03:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05179v1</guid></item><item><title>MRI Scan Synthesis Methods based on Clustering and Pix2Pix</title><link>http://arxiv.org/abs/2312.05176v1</link><description>We consider a missing data problem in the context of automatic segmentationmethods for Magnetic Resonance Imaging (MRI) brain scans. Usually, automatedMRI scan segmentation is based on multiple scans (e.g., T1-weighted,T2-weighted, T1CE, FLAIR). However, quite often a scan is blurry, missing orotherwise unusable. We investigate the question whether a missing scan can besynthesized. We exemplify that this is in principle possible by synthesizing aT2-weighted scan from a given T1-weighted scan. Our first aim is to compute apicture that resembles the missing scan closely, measured by average meansquared error (MSE). We develop/use several methods for this, including arandom baseline approach, a clustering-based method and pixel-to-pixeltranslation method by (Pix2Pix) which is based on conditional GANs. The lowestMSE is achieved by our clustering-based method. Our second aim is to comparethe methods with respect to the affect that using the synthesized scan has onthe segmentation process. For this, we use a DeepMedic model trained with thefour input scan modalities named above. We replace the T2-weighted scan by thesynthesized picture and evaluate the segmentations with respect to the tumoridentification, using Dice scores as numerical evaluation. The evaluation showsthat the segmentation works well with synthesized scans (in particular, withPix2Pix methods) in many cases.</description><author>Giulia Baldini, Melanie Schmidt, Charlotte Zäske, Liliana L. Caldeira</author><pubDate>Fri, 08 Dec 2023 16:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05176v1</guid></item><item><title>From Lengthy to Lucid: A Systematic Literature Review on NLP Techniques for Taming Long Sentences</title><link>http://arxiv.org/abs/2312.05172v1</link><description>Long sentences have been a persistent issue in written communication for manyyears since they make it challenging for readers to grasp the main points orfollow the initial intention of the writer. This survey, conducted using thePRISMA guidelines, systematically reviews two main strategies for addressingthe issue of long sentences: a) sentence compression and b) sentence splitting.An increased trend of interest in this area has been observed since 2005, withsignificant growth after 2017. Current research is dominated by supervisedapproaches for both sentence compression and splitting. Yet, there is aconsiderable gap in weakly and self-supervised techniques, suggesting anopportunity for further research, especially in domains with limited data. Inthis survey, we categorize and group the most representative methods into acomprehensive taxonomy. We also conduct a comparative evaluation analysis ofthese methods on common sentence compression and splitting datasets. Finally,we discuss the challenges and limitations of current methods, providingvaluable insights for future research directions. This survey is meant to serveas a comprehensive resource for addressing the complexities of long sentences.We aim to enable researchers to make further advancements in the field untillong sentences are no longer a barrier to effective communication.</description><author>Tatiana Passali, Efstathios Chatzikyriakidis, Stelios Andreadis, Thanos G. Stavropoulos, Anastasia Matonaki, Anestis Fachantidis, Grigorios Tsoumakas</author><pubDate>Fri, 08 Dec 2023 16:51:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05172v1</guid></item><item><title>DARLEI: Deep Accelerated Reinforcement Learning with Evolutionary Intelligence</title><link>http://arxiv.org/abs/2312.05171v1</link><description>We present DARLEI, a framework that combines evolutionary algorithms withparallelized reinforcement learning for efficiently training and evolvingpopulations of UNIMAL agents. Our approach utilizes Proximal PolicyOptimization (PPO) for individual agent learning and pairs it with a tournamentselection-based generational learning mechanism to foster morphologicalevolution. By building on Nvidia's Isaac Gym, DARLEI leverages GPU acceleratedsimulation to achieve over 20x speedup using just a single workstation,compared to previous work which required large distributed CPU clusters. Wesystematically characterize DARLEI's performance under various conditions,revealing factors impacting diversity of evolved morphologies. For example, byenabling inter-agent collisions within the simulator, we find that we cansimulate some multi-agent interactions between the same morphology, and see howit influences individual agent capabilities and long-term evolutionaryadaptation. While current results demonstrate limited diversity acrossgenerations, we hope to extend DARLEI in future work to include interactionsbetween diverse morphologies in richer environments, and create a platform thatallows for coevolving populations and investigating emergent behaviours inthem. Our source code is also made publicly athttps://saeejithnair.github.io/darlei.</description><author>Saeejith Nair, Mohammad Javad Shafiee, Alexander Wong</author><pubDate>Fri, 08 Dec 2023 16:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05171v1</guid></item><item><title>Onflow: an online portfolio allocation algorithm</title><link>http://arxiv.org/abs/2312.05169v1</link><description>We introduce Onflow, a reinforcement learning technique that enables onlineoptimization of portfolio allocation policies based on gradient flows. Wedevise dynamic allocations of an investment portfolio to maximize its expectedlog return while taking into account transaction fees. The portfolio allocationis parameterized through a softmax function, and at each time step, thegradient flow method leads to an ordinary differential equation whose solutionscorrespond to the updated allocations. This algorithm belongs to the largeclass of stochastic optimization procedures; we measure its efficiency bycomparing our results to the mathematical theoretical values in a log-normalframework and to standard benchmarks from the 'old NYSE' dataset. Forlog-normal assets, the strategy learned by Onflow, with transaction costs atzero, mimics Markowitz's optimal portfolio and thus the best possible assetallocation strategy. Numerical experiments from the 'old NYSE' dataset showthat Onflow leads to dynamic asset allocation strategies whose performancesare: a) comparable to benchmark strategies such as Cover's Universal Portfolioor Helmbold et al. "multiplicative updates" approach when transaction costs arezero, and b) better than previous procedures when transaction costs are high.Onflow can even remain efficient in regimes where other dynamical allocationtechniques do not work anymore. Therefore, as far as tested, Onflow appears tobe a promising dynamic portfolio management strategy based on observed pricesonly and without any assumption on the laws of distributions of the underlyingassets' returns. In particular it could avoid model risk when building atrading strategy.</description><author>Gabriel Turinici, Pierre Brugiere</author><pubDate>Fri, 08 Dec 2023 16:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05169v1</guid></item><item><title>LM-Cocktail: Resilient Tuning of Language Models via Model Merging</title><link>http://arxiv.org/abs/2311.13534v4</link><description>The pre-trained language models are continually fine-tuned to better supportdownstream applications. However, this operation may result in significantperformance degeneration on general tasks beyond the targeted domain. Toovercome this problem, we propose LM-Cocktail which enables the fine-tunedmodel to stay resilient in general perspectives. Our method is conducted in theform of model merging, where the fine-tuned language model is merged with thepre-trained base model or the peer models from other domains through weightedaverage. Despite simplicity, LM-Cocktail is surprisingly effective: theresulted model is able to achieve a strong empirical performance in the wholescope of general tasks while preserving a superior capacity in its targeteddomain. We conduct comprehensive experiments with LLama and BGE model onpopular benchmarks, including FLAN, MMLU, MTEB, whose results validate theefficacy of our proposed method. The code and checkpoints are available athttps://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail.</description><author>Shitao Xiao, Zheng Liu, Peitian Zhang, Xingrun Xing</author><pubDate>Fri, 08 Dec 2023 16:45:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13534v4</guid></item><item><title>Z-BERT-A: a zero-shot Pipeline for Unknown Intent detection</title><link>http://arxiv.org/abs/2208.07084v3</link><description>Intent discovery is a crucial task in natural language processing, and it isincreasingly relevant for various of industrial applications. Identifyingnovel, unseen intents from user inputs remains one of the biggest challenges inthis field. Herein, we propose Zero-Shot-BERT-Adapters, a two-stage method formultilingual intent discovery relying on a Transformer architecture, fine-tunedwith Adapters. We train the model for Natural Language Inference (NLI) andlater perform unknown intent classification in a zero-shot setting for multiplelanguages. In our evaluation, we first analyze the quality of the model afteradaptive fine-tuning on known classes. Secondly, we evaluate its performance incasting intent classification as an NLI task. Lastly, we test the zero-shotperformance of the model on unseen classes, showing how Zero-Shot-BERT-Adapterscan effectively perform intent discovery by generating semantically similarintents, if not equal, to the ground-truth ones. Our experiments show howZero-Shot-BERT-Adapters outperforms various baselines in two zero-shotsettings: known intent classification and unseen intent discovery. The proposedpipeline holds the potential for broad application in customer care. It enablesautomated dynamic triage using a lightweight model that can be easily deployedand scaled in various business scenarios, unlike large language models.Zero-Shot-BERT-Adapters represents an innovative multi-language approach forintent discovery, enabling the online generation of novel intents. A Pythonpackage implementing the pipeline and the new datasets we compiled areavailable at the following link:https://github.com/GT4SD/zero-shot-bert-adapters.</description><author>Daniele Comi, Dimitrios Christofidellis, Pier Francesco Piazza, Matteo Manica</author><pubDate>Fri, 08 Dec 2023 16:42:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.07084v3</guid></item><item><title>A Review of Cooperation in Multi-agent Learning</title><link>http://arxiv.org/abs/2312.05162v1</link><description>Cooperation in multi-agent learning (MAL) is a topic at the intersection ofnumerous disciplines, including game theory, economics, social sciences, andevolutionary biology. Research in this area aims to understand both how agentscan coordinate effectively when goals are aligned and how they may cooperate insettings where gains from working together are possible but possibilities forconflict abound. In this paper we provide an overview of the fundamentalconcepts, problem settings and algorithms of multi-agent learning. Thisencompasses reinforcement learning, multi-agent sequential decision-making,challenges associated with multi-agent cooperation, and a comprehensive reviewof recent progress, along with an evaluation of relevant metrics. Finally wediscuss open challenges in the field with the aim of inspiring new avenues forresearch.</description><author>Yali Du, Joel Z. Leibo, Usman Islam, Richard Willis, Peter Sunehag</author><pubDate>Fri, 08 Dec 2023 16:42:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05162v1</guid></item><item><title>TriHuman : A Real-time and Controllable Tri-plane Representation for Detailed Human Geometry and Appearance Synthesis</title><link>http://arxiv.org/abs/2312.05161v1</link><description>Creating controllable, photorealistic, and geometrically detailed digitaldoubles of real humans solely from video data is a key challenge in ComputerGraphics and Vision, especially when real-time performance is required. Recentmethods attach a neural radiance field (NeRF) to an articulated structure,e.g., a body model or a skeleton, to map points into a pose canonical spacewhile conditioning the NeRF on the skeletal pose. These approaches typicallyparameterize the neural field with a multi-layer perceptron (MLP) leading to aslow runtime. To address this drawback, we propose TriHuman a novelhuman-tailored, deformable, and efficient tri-plane representation, whichachieves real-time performance, state-of-the-art pose-controllable geometrysynthesis as well as photorealistic rendering quality. At the core, wenon-rigidly warp global ray samples into our undeformed tri-plane texturespace, which effectively addresses the problem of global points being mapped tothe same tri-plane locations. We then show how such a tri-plane featurerepresentation can be conditioned on the skeletal motion to account for dynamicappearance and geometry changes. Our results demonstrate a clear step towardshigher quality in terms of geometry and appearance modeling of humans as wellas runtime performance.</description><author>Heming Zhu, Fangneng Zhan, Christian Theobalt, Marc Habermann</author><pubDate>Fri, 08 Dec 2023 16:40:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05161v1</guid></item><item><title>Self-supervised OCT Image Denoising with Slice-to-Slice Registration and Reconstruction</title><link>http://arxiv.org/abs/2311.15167v2</link><description>Strong speckle noise is inherent to optical coherence tomography (OCT)imaging and represents a significant obstacle for accurate quantitativeanalysis of retinal structures which is key for advances in clinical diagnosisand monitoring of disease. Learning-based self-supervised methods forstructure-preserving noise reduction have demonstrated superior performanceover traditional methods but face unique challenges in OCT imaging. The highcorrelation of voxels generated by coherent A-scan beams undermines theefficacy of self-supervised learning methods as it violates the assumption ofindependent pixel noise. We conduct experiments demonstrating limitations ofexisting models due to this independence assumption. We then introduce a newend-to-end self-supervised learning framework specifically tailored for OCTimage denoising, integrating slice-by-slice training and registration modulesinto one network. An extensive ablation study is conducted for the proposedapproach. Comparison to previously published self-supervised denoising modelsdemonstrates improved performance of the proposed framework, potentiallyserving as a preprocessing step towards superior segmentation performance andquantitative analysis.</description><author>Shijie Li, Palaiologos Alexopoulos, Anse Vellappally, Ronald Zambrano, Wollstein Gadi, Guido Gerig</author><pubDate>Fri, 08 Dec 2023 16:40:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15167v2</guid></item><item><title>Detecting Atomic Scale Surface Defects in STM of TMDs with Ensemble Deep Learning</title><link>http://arxiv.org/abs/2312.05160v1</link><description>Atomic-scale defect detection is shown in scanning tunneling microscopyimages of single crystal WSe2 using an ensemble of U-Net-like convolutionalneural networks. Standard deep learning test metrics indicated good detectionperformance with an average F1 score of 0.66 and demonstrated ensemblegeneralization to C-AFM images of WSe2 and STM images of MoSe2. Defectcoordinates were automatically extracted from defect detections maps showingthat STM image analysis enhanced by machine learning can be used todramatically increase sample characterization throughput.</description><author>Darian Smalley, Stephanie D. Lough, Luke Holtzman, Kaikui Xu, Madisen Holbrook, Matthew R. Rosenberger, J. C. Hone, Katayun Barmak, Masahiro Ishigami</author><pubDate>Fri, 08 Dec 2023 16:38:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05160v1</guid></item><item><title>Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis</title><link>http://arxiv.org/abs/2308.09835v3</link><description>Current deep learning-based approaches for the segmentation of microscopyimages heavily rely on large amount of training data with dense annotation,which is highly costly and laborious in practice. Compared to full annotationwhere the complete contour of objects is depicted, point annotations,specifically object centroids, are much easier to acquire and still providecrucial information about the objects for subsequent segmentation. In thispaper, we assume access to point annotations only during training and develop aunified pipeline for microscopy image segmentation using syntheticallygenerated training data. Our framework includes three stages: (1) it takespoint annotations and samples a pseudo dense segmentation mask constrained withshape priors; (2) with an image generative model trained in an unpaired manner,it translates the mask to a realistic microscopy image regularized by objectlevel consistency; (3) the pseudo masks along with the synthetic images thenconstitute a pairwise dataset for training an ad-hoc segmentation model. On thepublic MoNuSeg dataset, our synthesis pipeline produces more diverse andrealistic images than baseline models while maintaining high coherence betweeninput masks and generated images. When using the identical segmentationbackbones, the models trained on our synthetic dataset significantly outperformthose trained with pseudo-labels or baseline-generated images. Moreover, ourframework achieves comparable results to models trained on authentic microscopyimages with dense labels, demonstrating its potential as a reliable and highlyefficient alternative to labor-intensive manual pixel-wise annotations inmicroscopy image segmentation. The code is available.</description><author>Shijie Li, Mengwei Ren, Thomas Ach, Guido Gerig</author><pubDate>Fri, 08 Dec 2023 16:38:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09835v3</guid></item><item><title>Deep Learning-Based Pilotless Spatial Multiplexing</title><link>http://arxiv.org/abs/2312.05158v1</link><description>This paper investigates the feasibility of machine learning (ML)-basedpilotless spatial multiplexing in multiple-input and multiple-output (MIMO)communication systems. Especially, it is shown that by training the transmitterand receiver jointly, the transmitter can learn such constellation shapes forthe spatial streams which facilitate completely blind separation and detectionby the simultaneously learned receiver. To the best of our knowledge, this isthe first time ML-based spatial multiplexing without channel estimation pilotsis demonstrated. The results show that the learned pilotless scheme canoutperform a conventional pilot-based system by as much as 15-20% in terms ofspectral efficiency, depending on the modulation order and signal-to-noiseratio.</description><author>Dani Korpi, Mikko Honkala, Janne M. J. Huttunen</author><pubDate>Fri, 08 Dec 2023 16:38:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05158v1</guid></item><item><title>Uncertainty Quantification and Propagation in Surrogate-based Bayesian Inference</title><link>http://arxiv.org/abs/2312.05153v1</link><description>Surrogate models are statistical or conceptual approximations for morecomplex simulation models. In this context, it is crucial to propagate theuncertainty induced by limited simulation budget and surrogate approximationerror to predictions, inference, and subsequent decision-relevant quantities.However, quantifying and then propagating the uncertainty of surrogates isusually limited to special analytic cases or is otherwise computationally veryexpensive. In this paper, we propose a framework enabling a scalable, Bayesianapproach to surrogate modeling with thorough uncertainty quantification,propagation, and validation. Specifically, we present three methods forBayesian inference with surrogate models given measurement data. This is a taskwhere the propagation of surrogate uncertainty is especially relevant, becausefailing to account for it may lead to biased and/or overconfident estimates ofthe parameters of interest. We showcase our approach in two detailed casestudies for both linear and nonlinear modeling scenarios. Uncertaintypropagation in surrogate models enables more reliable and safe approximation ofexpensive simulators and will therefore be useful in various fields ofapplications.</description><author>Philipp Reiser, Javier Enrique Aguilar, Anneli Guthke, Paul-Christian Bürkner</author><pubDate>Fri, 08 Dec 2023 16:31:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05153v1</guid></item><item><title>Shape-aware Segmentation of the Placenta in BOLD Fetal MRI Time Series</title><link>http://arxiv.org/abs/2312.05148v1</link><description>Blood oxygen level dependent (BOLD) MRI time series with maternal hyperoxiacan assess placental oxygenation and function. Measuring precise BOLD changesin the placenta requires accurate temporal placental segmentation and isconfounded by fetal and maternal motion, contractions, and hyperoxia-inducedintensity changes. Current BOLD placenta segmentation methods warp a manuallyannotated subject-specific template to the entire time series. However, as theplacenta is a thin, elongated, and highly non-rigid organ subject to largedeformations and obfuscated edges, existing work cannot accurately segment theplacental shape, especially near boundaries. In this work, we propose a machinelearning segmentation framework for placental BOLD MRI and apply it tosegmenting each volume in a time series. We use a placental-boundary weightedloss formulation and perform a comprehensive evaluation across several popularsegmentation objectives. Our model is trained and tested on a cohort of 91subjects containing healthy fetuses, fetuses with fetal growth restriction, andmothers with high BMI. Biomedically, our model performs reliably in segmentingvolumes in both normoxic and hyperoxic points in the BOLD time series. Wefurther find that boundary-weighting increases placental segmentationperformance by 8.3% and 6.0% Dice coefficient for the cross-entropy and signeddistance transform objectives, respectively. Our code and trained model isavailable at https://github.com/mabulnaga/automatic-placenta-segmentation.</description><author>S. Mazdak Abulnaga, Neel Dey, Sean I. Young, Eileen Pan, Katherine I. Hobgood, Clinton J. Wang, P. Ellen Grant, Esra Abaci Turk, Polina Golland</author><pubDate>Fri, 08 Dec 2023 16:29:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05148v1</guid></item><item><title>Kraken: enabling joint trajectory prediction by utilizing Mode Transformer and Greedy Mode Processing</title><link>http://arxiv.org/abs/2312.05144v1</link><description>Accurate and reliable motion prediction is essential for safe urban autonomy.The most prominent motion prediction approaches are based on modeling thedistribution of possible future trajectories of each actor in autonomoussystem's vicinity. These "independent" marginal predictions might be accurateenough to properly describe casual driving situations where the predictiontarget is not likely to interact with other actors. They are, however,inadequate for modeling interactive situations where the actors' futuretrajectories are likely to intersect. To mitigate this issue we propose Kraken-- a real-time trajectory prediction model capable of approximating pairwiseinteractions between the actors as well as producing accurate marginalpredictions. Kraken relies on a simple Greedy Mode Processing techniqueallowing it to convert a factorized prediction for a pair of agents into aphysically-plausible joint prediction. It also utilizes the Mode Transformermodule to increase the diversity of predicted trajectories and make the jointprediction more informative. We evaluate Kraken on Waymo Motion Predictionchallenge where it held the first place in the Interaction leaderboard and thesecond place in the Motion leaderboard in October 2021.</description><author>Daniil S. Antonenko, Stepan Konev, Yuriy Biktairov, Boris Yangel</author><pubDate>Fri, 08 Dec 2023 16:24:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05144v1</guid></item><item><title>A Recycling Training Strategy for Medical Image Segmentation with Diffusion Denoising Models</title><link>http://arxiv.org/abs/2308.16355v3</link><description>Denoising diffusion models have found applications in image segmentation bygenerating segmented masks conditioned on images. Existing studiespredominantly focus on adjusting model architecture or improving inference,such as test-time sampling strategies. In this work, we focus on improving thetraining strategy and propose a novel recycling method. During each trainingstep, a segmentation mask is first predicted given an image and a random noise.This predicted mask, which replaces the conventional ground truth mask, is usedfor denoising task during training. This approach can be interpreted asaligning the training strategy with inference by eliminating the dependence onground truth masks for generating noisy samples. Our proposed methodsignificantly outperforms standard diffusion training, self-conditioning, andexisting recycling strategies across multiple medical imaging data sets: muscleultrasound, abdominal CT, prostate MR, and brain MR. This holds for two widelyadopted sampling strategies: denoising diffusion probabilistic model anddenoising diffusion implicit model. Importantly, existing diffusion modelsoften display a declining or unstable performance during inference, whereas ournovel recycling consistently enhances or maintains performance. We show that,under a fair comparison with the same network architectures and computingbudget, the proposed recycling-based diffusion models achieved on-parperformance with non-diffusion-based supervised training. By ensembling theproposed diffusion and the non-diffusion models, significant improvements tothe non-diffusion models have been observed across all applications,demonstrating the value of this novel training method. This paper summarizesthese quantitative results and discusses their values, with a fullyreproducible JAX-based implementation, released athttps://github.com/mathpluscode/ImgX-DiffSeg.</description><author>Yunguan Fu, Yiwen Li, Shaheer U Saeed, Matthew J Clarkson, Yipeng Hu</author><pubDate>Fri, 08 Dec 2023 16:22:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16355v3</guid></item><item><title>Open Domain Generalization with a Single Network by Regularization Exploiting Pre-trained Features</title><link>http://arxiv.org/abs/2312.05141v1</link><description>Open Domain Generalization (ODG) is a challenging task as it not only dealswith distribution shifts but also category shifts between the source and targetdatasets. To handle this task, the model has to learn a generalizablerepresentation that can be applied to unseen domains while also identifyunknown classes that were not present during training. Previous work has usedmultiple source-specific networks, which involve a high computation cost.Therefore, this paper proposes a method that can handle ODG using only a singlenetwork. The proposed method utilizes a head that is pre-trained bylinear-probing and employs two regularization terms, each targeting theregularization of feature extractor and the classification head, respectively.The two regularization terms fully utilize the pre-trained features andcollaborate to modify the head of the model without excessively altering thefeature extractor. This ensures a smoother softmax output and prevents themodel from being biased towards the source domains. The proposed method showsimproved adaptability to unseen domains and increased capability to detectunseen classes as well. Extensive experiments show that our method achievescompetitive performance in several benchmarks. We also justify our method withcareful analysis of the effect on the logits, features, and the head.</description><author>Inseop Chung, KiYoon Yoo, Nojun Kwak</author><pubDate>Fri, 08 Dec 2023 16:22:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05141v1</guid></item><item><title>Membership Inference Attacks on Diffusion Models via Quantile Regression</title><link>http://arxiv.org/abs/2312.05140v1</link><description>Recently, diffusion models have become popular tools for image synthesisbecause of their high-quality outputs. However, like other large-scale models,they may leak private information about their training data. Here, wedemonstrate a privacy vulnerability of diffusion models through a\emph{membership inference (MI) attack}, which aims to identify whether atarget example belongs to the training set when given the trained diffusionmodel. Our proposed MI attack learns quantile regression models that predict (aquantile of) the distribution of reconstruction loss on examples not used intraining. This allows us to define a granular hypothesis test for determiningthe membership of a point in the training set, based on thresholding thereconstruction loss of that point using a custom threshold tailored to theexample. We also provide a simple bootstrap technique that takes a majoritymembership prediction over ``a bag of weak attackers'' which improves theaccuracy over individual quantile regression models. We show that our attackoutperforms the prior state-of-the-art attack while being substantially lesscomputationally expensive -- prior attacks required training multiple ``shadowmodels'' with the same architecture as the model under attack, whereas ourattack requires training only much smaller models.</description><author>Shuai Tang, Zhiwei Steven Wu, Sergul Aydore, Michael Kearns, Aaron Roth</author><pubDate>Fri, 08 Dec 2023 16:21:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05140v1</guid></item><item><title>Variational Classification</title><link>http://arxiv.org/abs/2305.10406v4</link><description>We present a latent variable model for classification that provides a novelprobabilistic interpretation of neural network softmax classifiers. We derive avariational objective to train the model, analogous to the evidence lower bound(ELBO) used to train variational auto-encoders, that generalises thecross-entropy loss used to train classification models. Treating inputs to thesoftmax layer as samples of a latent variable, our abstracted perspectivereveals a potential inconsistency between their anticipated distribution,required for accurate label predictions, and the empirical distribution theyfollow in practice. We then devise a variational objective to mitigate suchinconsistency and encourage a specified latent distribution, instead of theimplicit assumption in off-the-shelf softmax classifiers. Overall, we providenew theoretical insight into the inner workings of widely-used softmaxclassification; and empirical evaluation on image and text classificationdatasets demonstrates that our proposed remedy, variational classification,maintains classification accuracy while the reshaped latent space improvesother desirable classifier properties, such as calibration, adversarialrobustness, robustness to distribution shift and sample efficiency useful inlow data settings.</description><author>Shehzaad Dhuliawala, Mrinmaya Sachan, Carl Allen</author><pubDate>Fri, 08 Dec 2023 16:12:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10406v4</guid></item><item><title>Optimal Multi-Distribution Learning</title><link>http://arxiv.org/abs/2312.05134v1</link><description>Multi-distribution learning (MDL), which seeks to learn a shared model thatminimizes the worst-case risk across $k$ distinct data distributions, hasemerged as a unified framework in response to the evolving demand forrobustness, fairness, multi-group collaboration, etc. Achieving data-efficientMDL necessitates adaptive sampling, also called on-demand sampling, throughoutthe learning process. However, there exist substantial gaps between thestate-of-the-art upper and lower bounds on the optimal sample complexity.Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$, wepropose a novel algorithm that yields an $varepsilon$-optimal randomizedhypothesis with a sample complexity on the order of $(d+k)/\varepsilon^2$(modulo some logarithmic factor), matching the best-known lower bound. Ouralgorithmic ideas and theory have been further extended to accommodateRademacher classes. The proposed algorithms are oracle-efficient, which accessthe hypothesis class solely through an empirical risk minimization oracle.Additionally, we establish the necessity of randomization, unveiling a largesample size barrier when only deterministic hypotheses are permitted. Thesefindings successfully resolve three open problems presented in COLT 2023 (i.e.,Awasthi et al., (2023, Problem 1, 3 and 4)).</description><author>Zihan Zhang, Wenhao Zhan, Yuxin Chen, Simon S. Du, Jason D. Lee</author><pubDate>Fri, 08 Dec 2023 16:06:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05134v1</guid></item><item><title>GIR: 3D Gaussian Inverse Rendering for Relightable Scene Factorization</title><link>http://arxiv.org/abs/2312.05133v1</link><description>This paper presents GIR, a 3D Gaussian Inverse Rendering method forrelightable scene factorization. Compared to existing methods leveragingdiscrete meshes or neural implicit fields for inverse rendering, our methodutilizes 3D Gaussians to estimate the material properties, illumination, andgeometry of an object from multi-view images. Our study is motivated by theevidence showing that 3D Gaussian is a more promising backbone than neuralfields in terms of performance, versatility, and efficiency. In this paper, weaim to answer the question: ``How can 3D Gaussian be applied to improve theperformance of inverse rendering?'' To address the complexity of estimatingnormals based on discrete and often in-homogeneous distributed 3D Gaussianrepresentations, we proposed an efficient self-regularization method thatfacilitates the modeling of surface normals without the need for additionalsupervision. To reconstruct indirect illumination, we propose an approach thatsimulates ray tracing. Extensive experiments demonstrate our proposed GIR'ssuperior performance over existing methods across multiple tasks on a varietyof widely used datasets in inverse rendering. This substantiates its efficacyand broad applicability, highlighting its potential as an influential tool inrelighting and reconstruction. Project page: https://3dgir.github.io</description><author>Yahao Shi, Yanmin Wu, Chenming Wu, Xing Liu, Chen Zhao, Haocheng Feng, Jingtuo Liu, Liangjun Zhang, Jian Zhang, Bin Zhou, Errui Ding, Jingdong Wang</author><pubDate>Fri, 08 Dec 2023 16:05:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05133v1</guid></item><item><title>Bayesian data fusion with shared priors</title><link>http://arxiv.org/abs/2212.07311v2</link><description>The integration of data and knowledge from several sources is known as datafusion. When data is only available in a distributed fashion or when differentsensors are used to infer a quantity of interest, data fusion becomesessential. In Bayesian settings, a priori information of the unknown quantitiesis available and, possibly, present among the different distributed estimators.When the local estimates are fused, the prior knowledge used to constructseveral local posteriors might be overused unless the fusion node accounts forthis and corrects it. In this paper, we analyze the effects of shared priors inBayesian data fusion contexts. Depending on different common fusion rules, ouranalysis helps to understand the performance behavior as a function of thenumber of collaborative agents and as a consequence of different types ofpriors. The analysis is performed by using two divergences which are common inBayesian inference, and the generality of the results allows to analyze verygeneric distributions. These theoretical results are corroborated throughexperiments in a variety of estimation and classification problems, includinglinear and nonlinear models, and federated learning schemes.</description><author>Peng Wu, Tales Imbiriba, Victor Elvira, Pau Closas</author><pubDate>Fri, 08 Dec 2023 16:04:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07311v2</guid></item><item><title>BayesDAG: Gradient-Based Posterior Inference for Causal Discovery</title><link>http://arxiv.org/abs/2307.13917v2</link><description>Bayesian causal discovery aims to infer the posterior distribution overcausal models from observed data, quantifying epistemic uncertainty andbenefiting downstream tasks. However, computational challenges arise due tojoint inference over combinatorial space of Directed Acyclic Graphs (DAGs) andnonlinear functions. Despite recent progress towards efficient posteriorinference over DAGs, existing methods are either limited to variationalinference on node permutation matrices for linear causal models, leading tocompromised inference accuracy, or continuous relaxation of adjacency matricesconstrained by a DAG regularizer, which cannot ensure resulting graphs areDAGs. In this work, we introduce a scalable Bayesian causal discovery frameworkbased on a combination of stochastic gradient Markov Chain Monte Carlo(SG-MCMC) and Variational Inference (VI) that overcomes these limitations. Ourapproach directly samples DAGs from the posterior without requiring any DAGregularization, simultaneously draws function parameter samples and isapplicable to both linear and nonlinear causal models. To enable our approach,we derive a novel equivalence to the permutation-based DAG learning, whichopens up possibilities of using any relaxed gradient estimator defined overpermutations. To our knowledge, this is the first framework applyinggradient-based MCMC sampling for causal discovery. Empirical evaluation onsynthetic and real-world datasets demonstrate our approach's effectivenesscompared to state-of-the-art baselines.</description><author>Yashas Annadani, Nick Pawlowski, Joel Jennings, Stefan Bauer, Cheng Zhang, Wenbo Gong</author><pubDate>Fri, 08 Dec 2023 15:49:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13917v2</guid></item><item><title>Quantifying white matter hyperintensity and brain volumes in heterogeneous clinical and low-field portable MRI</title><link>http://arxiv.org/abs/2312.05119v1</link><description>Brain atrophy and white matter hyperintensity (WMH) are critical neuroimagingfeatures for ascertaining brain injury in cerebrovascular disease and multiplesclerosis. Automated segmentation and quantification is desirable but existingmethods require high-resolution MRI with good signal-to-noise ratio (SNR). Thisprecludes application to clinical and low-field portable MRI (pMRI) scans, thushampering large-scale tracking of atrophy and WMH progression, especially inunderserved areas where pMRI has huge potential. Here we present a method thatsegments white matter hyperintensity and 36 brain regions from scans of anyresolution and contrast (including pMRI) without retraining. We show results onsix public datasets and on a private dataset with paired high- and low-fieldscans (3T and 64mT), where we attain strong correlation between the WMH($\rho$=.85) and hippocampal volumes (r=.89) estimated at both fields. Ourmethod is publicly available as part of FreeSurfer, at:http://surfer.nmr.mgh.harvard.edu/fswiki/WMH-SynthSeg.</description><author>Pablo Laso, Stefano Cerri, Annabel Sorby-Adams, Jennifer Guo, Farrah Mateen, Philipp Goebl, Jiaming Wu, Peirong Liu, Hongwei Li, Sean I. Young, Benjamin Billot, Oula Puonti, Gordon Sze, Sam Payabavash, Adam DeHavenon, Kevin N. Sheth, Matthew S. Rosen, John Kirsch, Nicola Strisciuglio, Jelmer M. Wolterink, Arman Eshaghi, Frederik Barkhof, W. Taylor Kimberly, Juan Eugenio Iglesias</author><pubDate>Fri, 08 Dec 2023 15:47:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05119v1</guid></item><item><title>AFN: Adaptive Fusion Normalization via Encoder-Decoder Framework</title><link>http://arxiv.org/abs/2308.03321v2</link><description>The success of deep learning is inseparable from normalization layers.Researchers have proposed various normalization functions, and each of them hasboth advantages and disadvantages. In response, efforts have been made todesign a unified normalization function that combines all normalizationprocedures and mitigates their weaknesses. We also proposed a new normalizationfunction called Adaptive Fusion Normalization. Through experiments, wedemonstrate AFN outperforms the previous normalization techniques in domaingeneralization and image classification tasks.</description><author>Zikai Zhou, Huanran Chen</author><pubDate>Fri, 08 Dec 2023 15:42:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03321v2</guid></item><item><title>On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction Attacks against "Truly Anonymous Synthetic Data''</title><link>http://arxiv.org/abs/2312.05114v1</link><description>Training generative models to produce synthetic data is meant to provide aprivacy-friendly approach to data release. However, we get robust guaranteesonly when models are trained to satisfy Differential Privacy (DP). Alas, thisis not the standard in industry as many companies use ad-hoc strategies toempirically evaluate privacy based on the statistical similarity betweensynthetic and real data. In this paper, we review the privacy metrics offeredby leading companies in this space and shed light on a few critical flaws inreasoning about privacy entirely via empirical evaluations. We analyze theundesirable properties of the most popular metrics and filters and demonstratetheir unreliability and inconsistency through counter-examples. We then presenta reconstruction attack, ReconSyn, which successfully recovers (i.e., leaks allattributes of) at least 78% of the low-density train records (or outliers) withonly black-box access to a single fitted generative model and the privacymetrics. Finally, we show that applying DP only to the model or usinglow-utility generators does not mitigate ReconSyn as the privacy leakagepredominantly comes from the metrics. Overall, our work serves as a warning topractitioners not to deviate from established privacy-preserving mechanisms.</description><author>Georgi Ganev, Emiliano De Cristofaro</author><pubDate>Fri, 08 Dec 2023 15:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05114v1</guid></item><item><title>On the Robustness of Large Multimodal Models Against Image Adversarial Attacks</title><link>http://arxiv.org/abs/2312.03777v2</link><description>Recent advances in instruction tuning have led to the development ofState-of-the-Art Large Multimodal Models (LMMs). Given the novelty of thesemodels, the impact of visual adversarial attacks on LMMs has not beenthoroughly examined. We conduct a comprehensive study of the robustness ofvarious LMMs against different adversarial attacks, evaluated across tasksincluding image classification, image captioning, and Visual Question Answer(VQA). We find that in general LMMs are not robust to visual adversarialinputs. However, our findings suggest that context provided to the model viaprompts, such as questions in a QA pair helps to mitigate the effects of visualadversarial inputs. Notably, the LMMs evaluated demonstrated remarkableresilience to such attacks on the ScienceQA task with only an 8.10% drop inperformance compared to their visual counterparts which dropped 99.73%. We alsopropose a new approach to real-world image classification which we term querydecomposition. By incorporating existence queries into our input prompt weobserve diminished attack effectiveness and improvements in imageclassification accuracy. This research highlights a previously under-exploredfacet of LMM robustness and sets the stage for future work aimed atstrengthening the resilience of multimodal systems in adversarial environments.</description><author>Xuanming Cui, Alejandro Aparcedo, Young Kyun Jang, Ser-Nam Lim</author><pubDate>Fri, 08 Dec 2023 15:41:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03777v2</guid></item><item><title>VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing</title><link>http://arxiv.org/abs/2306.08707v2</link><description>Recently, diffusion-based generative models have achieved remarkable successfor image generation and edition. However, their use for video editing stillfaces important limitations. This paper introduces VidEdit, a novel method forzero-shot text-based video editing ensuring strong temporal and spatialconsistency. Firstly, we propose to combine atlas-based and pre-trainedtext-to-image diffusion models to provide a training-free and efficient editingmethod, which by design fulfills temporal smoothness. Secondly, we leverageoff-the-shelf panoptic segmenters along with edge detectors and adapt their usefor conditioned diffusion-based atlas editing. This ensures a fine spatialcontrol on targeted regions while strictly preserving the structure of theoriginal video. Quantitative and qualitative experiments show that VidEditoutperforms state-of-the-art methods on DAVIS dataset, regarding semanticfaithfulness, image preservation, and temporal consistency metrics. With thisframework, processing a single video only takes approximately one minute, andit can generate multiple compatible edits based on a unique text prompt.Project web-page at https://videdit.github.io</description><author>Paul Couairon, Clément Rambour, Jean-Emmanuel Haugeard, Nicolas Thome</author><pubDate>Fri, 08 Dec 2023 15:37:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08707v2</guid></item><item><title>DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models</title><link>http://arxiv.org/abs/2312.05107v1</link><description>In this paper, we present DreaMoving, a diffusion-based controllable videogeneration framework to produce high-quality customized human dance videos.Specifically, given target identity and posture sequences, DreaMoving cangenerate a video of the target identity dancing anywhere driven by the posturesequences. To this end, we propose a Video ControlNet for motion-controllingand a Content Guider for identity preserving. The proposed model is easy to useand can be adapted to most stylized diffusion models to generate diverseresults. The project page is available athttps://dreamoving.github.io/dreamoving.</description><author>Mengyang Feng, Jinlin Liu, Kai Yu, Yuan Yao, Zheng Hui, Xiefan Guo, Xianhui Lin, Haolan Xue, Chen Shi, Xiaowen Li, Aojie Li, Miaomiao Cui, Peiran Ren, Xuansong Xie</author><pubDate>Fri, 08 Dec 2023 15:37:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05107v1</guid></item><item><title>Multi-Frequency Joint Community Detection and Phase Synchronization</title><link>http://arxiv.org/abs/2206.12276v3</link><description>This paper studies the joint community detection and phase synchronizationproblem on the \textit{stochastic block model with relative phase}, where eachnode is associated with an unknown phase angle. This problem, with a variety ofreal-world applications, aims to recover the cluster structure and associatedphase angles simultaneously. We show this problem exhibits a\textit{``multi-frequency''} structure by closely examining its maximumlikelihood estimation (MLE) formulation, whereas existing methods are notoriginated from this perspective. To this end, two simple yet efficientalgorithms that leverage the MLE formulation and benefit from the informationacross multiple frequencies are proposed. The former is a spectral method basedon the novel multi-frequency column-pivoted QR factorization. The factorizationapplied to the top eigenvectors of the observation matrix provides keyinformation about the cluster structure and associated phase angles. The secondapproach is an iterative multi-frequency generalized power method, where eachiteration updates the estimation in a matrix-multiplication-then-projectionmanner. Numerical experiments show that our proposed algorithms significantlyimprove the ability of exactly recovering the cluster structure and theaccuracy of the estimated phase angles, compared to state-of-the-artalgorithms.</description><author>Lingda Wang, Zhizhen Zhao</author><pubDate>Fri, 08 Dec 2023 15:32:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.12276v3</guid></item><item><title>TMID: A Comprehensive Real-world Dataset for Trademark Infringement Detection in E-Commerce</title><link>http://arxiv.org/abs/2312.05103v1</link><description>Annually, e-commerce platforms incur substantial financial losses due totrademark infringements, making it crucial to identify and mitigate potentiallegal risks tied to merchant information registered to the platforms. However,the absence of high-quality datasets hampers research in this area. To addressthis gap, our study introduces TMID, a novel dataset to detect trademarkinfringement in merchant registrations. This is a real-world dataset sourceddirectly from Alipay, one of the world's largest e-commerce and digital paymentplatforms. As infringement detection is a legal reasoning task requiring anunderstanding of the contexts and legal rules, we offer a thorough collectionof legal rules and merchant and trademark-related contextual information withannotations from legal experts. We ensure the data quality by performing anextensive statistical analysis. Furthermore, we conduct an empirical study onthis dataset to highlight its value and the key challenges. Through this study,we aim to contribute valuable resources to advance research into legalcompliance related to trademark infringement within the e-commerce sphere. Thedataset is available at https://github.com/emnlpTMID/emnlpTMID.github.io .</description><author>Tongxin Hu, Zhuang Li, Xin Jin, Lizhen Qu, Xin Zhang</author><pubDate>Fri, 08 Dec 2023 15:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05103v1</guid></item><item><title>Continual learning for surface defect segmentation by subnetwork creation and selection</title><link>http://arxiv.org/abs/2312.05100v1</link><description>We introduce a new continual (or lifelong) learning algorithm called LDA-CP&amp;Sthat performs segmentation tasks without undergoing catastrophic forgetting.The method is applied to two different surface defect segmentation problemsthat are learned incrementally, i.e. providing data about one type of defect ata time, while still being capable of predicting every defect that was seenpreviously. Our method creates a defect-related subnetwork for each defect typevia iterative pruning and trains a classifier based on linear discriminantanalysis (LDA). At the inference stage, we first predict the defect type withLDA and then predict the surface defects using the selected subnetwork. Wecompare our method with other continual learning methods showing a significantimprovement -- mean Intersection over Union better by a factor of two whencompared to existing methods on both datasets. Importantly, our approach showscomparable results with joint training when all the training data (all defects)are seen simultaneously</description><author>Aleksandr Dekhovich, Miguel A. Bessa</author><pubDate>Fri, 08 Dec 2023 15:28:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05100v1</guid></item><item><title>INSPECT: Intrinsic and Systematic Probing Evaluation for Code Transformers</title><link>http://arxiv.org/abs/2312.05092v1</link><description>Pre-trained models of source code have recently been successfully applied toa wide variety of Software Engineering tasks; they have also seen somepractical adoption in practice, e.g. for code completion. Yet, we still knowvery little about what these pre-trained models learn about source code. Inthis article, we use probing--simple diagnostic tasks that do not further trainthe models--to discover to what extent pre-trained models learn about specificaspects of source code. We use an extensible framework to define 15 probingtasks that exercise surface, syntactic, structural and semantic characteristicsof source code. We probe 8 pre-trained source code models, as well as a naturallanguage model (BERT) as our baseline. We find that models that incorporatesome structural information (such as GraphCodeBERT) have a betterrepresentation of source code characteristics. Surprisingly, we find that forsome probing tasks, BERT is competitive with the source code models, indicatingthat there are ample opportunities to improve source-code specific pre-trainingon the respective code characteristics. We encourage other researchers toevaluate their models with our probing task suite, so that they may peer intothe hidden layers of the models and identify what intrinsic codecharacteristics are encoded.</description><author>Anjan Karmakar, Romain Robbes</author><pubDate>Fri, 08 Dec 2023 15:21:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05092v1</guid></item><item><title>UniTSA: A Universal Reinforcement Learning Framework for V2X Traffic Signal Control</title><link>http://arxiv.org/abs/2312.05090v1</link><description>Traffic congestion is a persistent problem in urban areas, which calls forthe development of effective traffic signal control (TSC) systems. Whileexisting Reinforcement Learning (RL)-based methods have shown promisingperformance in optimizing TSC, it is challenging to generalize these methodsacross intersections of different structures. In this work, a universalRL-based TSC framework is proposed for Vehicle-to-Everything (V2X)environments. The proposed framework introduces a novel agent design thatincorporates a junction matrix to characterize intersection states, making theproposed model applicable to diverse intersections. To equip the proposedRL-based framework with enhanced capability of handling various intersectionstructures, novel traffic state augmentation methods are tailor-made for signallight control systems. Finally, extensive experimental results derived frommultiple intersection configurations confirm the effectiveness of the proposedframework. The source code in this work is available athttps://github.com/wmn7/Universal_Light</description><author>Maonan Wang, Xi Xiong, Yuheng Kan, Chengcheng Xu, Man-On Pun</author><pubDate>Fri, 08 Dec 2023 15:18:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05090v1</guid></item><item><title>I Can't Believe It's Not Better: In-air Movement For Alzheimer Handwriting Synthetic Generation</title><link>http://arxiv.org/abs/2312.05086v1</link><description>During recent years, there here has been a boom in terms of deep learning usefor handwriting analysis and recognition. One main application for handwritinganalysis is early detection and diagnosis in the health field. Unfortunately,most real case problems still suffer a scarcity of data, which makes difficultthe use of deep learning-based models. To alleviate this problem, some worksresort to synthetic data generation. Lately, more works are directed towardsguided data synthetic generation, a generation that uses the domain and dataknowledge to generate realistic data that can be useful to train deep learningmodels. In this work, we combine the domain knowledge about the Alzheimer'sdisease for handwriting and use it for a more guided data generation.Concretely, we have explored the use of in-air movements for synthetic datageneration.</description><author>Asma Bensalah, Antonio Parziale, Giuseppe De Gregorio, Angelo Marcelli, Alicia Fornés, Lladós</author><pubDate>Fri, 08 Dec 2023 15:14:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05086v1</guid></item><item><title>Precision estimation and second-order errors in cortical circuits</title><link>http://arxiv.org/abs/2309.16046v2</link><description>Minimization of cortical prediction errors has been considered a keycomputational goal of the cerebral cortex underlying perception, action andlearning. However, it is still unclear how the cortex should form and useinformation about uncertainty in this process of prediction error minimization.Here we derive neural dynamics that minimize prediction errors under theassumption that cortical areas must not only predict the activity in otherareas and sensory streams, but also jointly estimate the precision of theirpredictions. This results in a dynamic modulatory balancing of cortical streamsbased on context-dependent precision estimates. Moreover, the theory predictsthe existence of cortical second-order errors, comparing estimated and actualprecision, propagated through the cortical hierarchy alongside classicalprediction errors. These second-order errors are used to learn weights ofsynapses responsible for precision estimation through an error-correctingsynaptic learning rule. Finally, we propose a detailed mapping of the theory tocortical circuitry.</description><author>Arno Granier, Mihai A. Petrovici, Walter Senn, Katharina A. Wilmes</author><pubDate>Fri, 08 Dec 2023 15:06:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16046v2</guid></item><item><title>On the Impact of Multi-dimensional Local Differential Privacy on Fairness</title><link>http://arxiv.org/abs/2312.04404v2</link><description>Automated decision systems are increasingly used to make consequentialdecisions in people's lives. Due to the sensitivity of the manipulated data aswell as the resulting decisions, several ethical concerns need to be addressedfor the appropriate use of such technologies, in particular, fairness andprivacy. Unlike previous work, which focused on centralized differentialprivacy (DP) or local DP (LDP) for a single sensitive attribute, in this paper,we examine the impact of LDP in the presence of several sensitive attributes(i.e., multi-dimensional data) on fairness. Detailed empirical analysis onsynthetic and benchmark datasets revealed very relevant observations. Inparticular, (1) multi-dimensional LDP is an efficient approach to reducedisparity, (2) the multi-dimensional approach of LDP (independent vs. combined)matters only at low privacy guarantees, and (3) the outcome Y distribution hasan important effect on which group is more sensitive to the obfuscation. Last,we summarize our findings in the form of recommendations to guide practitionersin adopting effective privacy-preserving practices while maintaining fairnessand utility in ML applications.</description><author>Karima Makhlouf, Heber H. Arcolezi, Sami Zhioua, Ghassen Ben Brahim, Catuscia Palamidessi</author><pubDate>Fri, 08 Dec 2023 15:00:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04404v2</guid></item><item><title>Skilful Precipitation Nowcasting Using NowcastNet</title><link>http://arxiv.org/abs/2311.17961v2</link><description>Designing early warning system for precipitation requires accurate short-termforecasting system. Climate change has led to an increase in frequency ofextreme weather events, and hence such systems can prevent disasters and lossof life. Managing such events remain a challenge for both public and privateinstitutions. Precipitation nowcasting can help relevant institutions to betterprepare for such events as they impact agriculture, transport, public healthand safety, etc. Physics-based numerical weather prediction (NWP) is unable toperform well for nowcasting because of large computational turn-around time.Deep-learning based models on the other hand are able to give predictionswithin seconds. We use recently proposed NowcastNet, a physics-conditioned deepgenerative network, to forecast precipitation for different regions of Europeusing satellite images. Both spatial and temporal transfer learning is done byforecasting for the unseen regions and year. Model makes realistic predictionsand is able to outperform baseline for such a prediction task.</description><author>Ajitabh Kumar</author><pubDate>Fri, 08 Dec 2023 14:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17961v2</guid></item><item><title>A Distributed ADMM-based Deep Learning Approach for Thermal Control in Multi-Zone Buildings</title><link>http://arxiv.org/abs/2312.05073v1</link><description>The surge in electricity use, coupled with the dependency on intermittentrenewable energy sources, poses significant hurdles to effectively managingpower grids, particularly during times of peak demand. Demand Response programsand energy conservation measures are essential to operate energy grids whileensuring a responsible use of our resources This research combines distributedoptimization using ADMM with Deep Learning models to plan indoor temperaturesetpoints effectively. A two-layer hierarchical structure is used, with acentral building coordinator at the upper layer and local controllers at thethermal zone layer. The coordinator must limit the building's maximum power bytranslating the building's total power to local power targets for each zone.Local controllers can modify the temperature setpoints to meet the local powertargets. The resulting control algorithm, called Distributed Planning Networks,is designed to be both adaptable and scalable to many types of buildings,tackling two of the main challenges in the development of such systems. Theproposed approach is tested on an 18-zone building modeled in EnergyPlus. Thealgorithm successfully manages Demand Response peak events.</description><author>Vincent Taboga, Hanane Dagdougui</author><pubDate>Fri, 08 Dec 2023 14:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05073v1</guid></item><item><title>FreestyleRet: Retrieving Images from Style-Diversified Queries</title><link>http://arxiv.org/abs/2312.02428v2</link><description>Image Retrieval aims to retrieve corresponding images based on a given query.In application scenarios, users intend to express their retrieval intentthrough various query styles. However, current retrieval tasks predominantlyfocus on text-query retrieval exploration, leading to limited retrieval queryoptions and potential ambiguity or bias in user intention. In this paper, wepropose the Style-Diversified Query-Based Image Retrieval task, which enablesretrieval based on various query styles. To facilitate the novel setting, wepropose the first Diverse-Style Retrieval dataset, encompassing diverse querystyles including text, sketch, low-resolution, and art. We also propose alight-weighted style-diversified retrieval framework. For various query styleinputs, we apply the Gram Matrix to extract the query's textural features andcluster them into a style space with style-specific bases. Then we employ thestyle-init prompt tuning module to enable the visual encoder to comprehend thetexture and style information of the query. Experiments demonstrate that ourmodel, employing the style-init prompt tuning strategy, outperforms existingretrieval models on the style-diversified retrieval task. Moreover,style-diversified queries~(sketch+text, art+text, etc) can be simultaneouslyretrieved in our model. The auxiliary information from other queries enhancesthe retrieval performance within the respective query.</description><author>Hao Li, Curise Jia, Peng Jin, Zesen Cheng, Kehan Li, Jialu Sui, Chang Liu, Li Yuan</author><pubDate>Fri, 08 Dec 2023 14:30:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02428v2</guid></item><item><title>LaCour!: Enabling Research on Argumentation in Hearings of the European Court of Human Rights</title><link>http://arxiv.org/abs/2312.05061v1</link><description>Why does an argument end up in the final court decision? Was it deliberatedor questioned during the oral hearings? Was there something in the hearingsthat triggered a particular judge to write a dissenting opinion? Despite theavailability of the final judgments of the European Court of Human Rights(ECHR), none of these legal research questions can currently be answered as theECHR's multilingual oral hearings are not transcribed, structured, orspeaker-attributed. We address this fundamental gap by presenting LaCour!, thefirst corpus of textual oral arguments of the ECHR, consisting of 154 fullhearings (2.1 million tokens from over 267 hours of video footage) in English,French, and other court languages, each linked to the corresponding finaljudgment documents. In addition to the transcribed and partially manuallycorrected text from the video, we provide sentence-level timestamps andmanually annotated role and language labels. We also showcase LaCour! in a setof preliminary experiments that explore the interplay between questions anddissenting opinions. Apart from the use cases in legal NLP, we hope that lawstudents or other interested parties will also use LaCour! as a learningresource, as it is freely available in various formats athttps://huggingface.co/datasets/TrustHLT/LaCour.</description><author>Lena Held, Ivan Habernal</author><pubDate>Fri, 08 Dec 2023 14:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05061v1</guid></item><item><title>Causal normalizing flows: from theory to practice</title><link>http://arxiv.org/abs/2306.05415v2</link><description>In this work, we deepen on the use of normalizing flows for causal reasoning.Specifically, we first leverage recent results on non-linear ICA to show thatcausal models are identifiable from observational data given a causal ordering,and thus can be recovered using autoregressive normalizing flows (NFs). Second,we analyze different design and learning choices for causal normalizing flowsto capture the underlying causal data-generating process. Third, we describehow to implement the do-operator in causal NFs, and thus, how to answerinterventional and counterfactual questions. Finally, in our experiments, wevalidate our design and training choices through a comprehensive ablationstudy; compare causal NFs to other approaches for approximating causal models;and empirically demonstrate that causal NFs can be used to address real-worldproblems, where the presence of mixed discrete-continuous data and partialknowledge on the causal graph is the norm. The code for this work can be foundat https://github.com/psanch21/causal-flows.</description><author>Adrián Javaloy, Pablo Sánchez-Martín, Isabel Valera</author><pubDate>Fri, 08 Dec 2023 14:29:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05415v2</guid></item><item><title>Measuring Pointwise $\mathcal{V}$-Usable Information In-Context-ly</title><link>http://arxiv.org/abs/2310.12300v2</link><description>In-context learning (ICL) is a new learning paradigm that has gainedpopularity along with the development of large language models. In this work,we adapt a recently proposed hardness metric, pointwise $\mathcal{V}$-usableinformation (PVI), to an in-context version (in-context PVI). Compared to theoriginal PVI, in-context PVI is more efficient in that it requires only a fewexemplars and does not require fine-tuning. We conducted a comprehensiveempirical analysis to evaluate the reliability of in-context PVI. Our findingsindicate that in-context PVI estimates exhibit similar characteristics to theoriginal PVI. Specific to the in-context setting, we show that in-context PVIestimates remain consistent across different exemplar selections and numbers ofshots. The variance of in-context PVI estimates across different exemplarselections is insignificant, which suggests that in-context PVI are stable.Furthermore, we demonstrate how in-context PVI can be employed to identifychallenging instances. Our work highlights the potential of in-context PVI andprovides new insights into the capabilities of ICL.</description><author>Sheng Lu, Shan Chen, Yingya Li, Danielle Bitterman, Guergana Savova, Iryna Gurevych</author><pubDate>Fri, 08 Dec 2023 14:20:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12300v2</guid></item><item><title>Wireless Transmission of Images With The Assistance of Multi-level Semantic Information</title><link>http://arxiv.org/abs/2202.04754v2</link><description>Semantic-oriented communication has been considered as a promising to boostthe bandwidth efficiency by only transmitting the semantics of the data. Inthis paper, we propose a multi-level semantic aware communication system forwireless image transmission, named MLSC-image, which is based on the deeplearning techniques and trained in an end to end manner. In particular, theproposed model includes a multilevel semantic feature extractor, that extractsboth the highlevel semantic information, such as the text semantics and thesegmentation semantics, and the low-level semantic information, such as localspatial details of the images. We employ a pretrained image caption to capturethe text semantics and a pretrained image segmentation model to obtain thesegmentation semantics. These high-level and low-level semantic features arethen combined and encoded by a joint semantic and channel encoder into symbolsto transmit over the physical channel. The numerical results validate theeffectiveness and efficiency of the proposed semantic communication system,especially under the limited bandwidth condition, which indicates theadvantages of the high-level semantics in the compression of images.</description><author>Zhenguo Zhang, Qianqian Yang, Shibo He, Mingyang Sun, Jiming Chen</author><pubDate>Fri, 08 Dec 2023 14:16:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.04754v2</guid></item><item><title>Soft Frequency Capping for Improved Ad Click Prediction in Yahoo Gemini Native</title><link>http://arxiv.org/abs/2312.05052v1</link><description>Yahoo's native advertising (also known as Gemini native) serves billions ofad impressions daily, reaching a yearly run-rate of many hundred of millionsUSD. Driving the Gemini native models that are used to predict both clickprobability (pCTR) and conversion probability (pCONV) is OFFSET - a featureenhanced collaborative-filtering (CF) based event prediction algorithm. \offsetis a one-pass algorithm that updates its model for every new batch of loggeddata using a stochastic gradient descent (SGD) based approach. Since OFFSETrepresents its users by their features (i.e., user-less model) due to sparsityissues, rule based hard frequency capping (HFC) is used to control the numberof times a certain user views a certain ad. Moreover, related statistics revealthat user ad fatigue results in a dramatic drop in click through rate (CTR).Therefore, to improve click prediction accuracy, we propose a soft frequencycapping (SFC) approach, where the frequency feature is incorporated into theOFFSET model as a user-ad feature and its weight vector is learned via logisticregression as part of OFFSET training. Online evaluation of the soft frequencycapping algorithm via bucket testing showed a significant 7.3% revenue lift.Since then, the frequency feature enhanced model has been pushed to productionserving all traffic, and is generating a hefty revenue lift for Yahoo Gemininative. We also report related statistics that reveal, among other things, thatwhile users' gender does not affect ad fatigue, the latter seems to increasewith users' age.</description><author>Michal Aharon, Yohay Kaplan, Rina Levy, Oren Somekh, Ayelet Blanc, Neetai Eshel, Avi Shahar, Assaf Singer, Alex Zlotnik</author><pubDate>Fri, 08 Dec 2023 14:12:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05052v1</guid></item><item><title>Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars</title><link>http://arxiv.org/abs/2211.01842v3</link><description>The discovery of neural architectures from simple building blocks is along-standing goal of Neural Architecture Search (NAS). Hierarchical searchspaces are a promising step towards this goal but lack a unifying search spacedesign framework and typically only search over some limited aspect ofarchitectures. In this work, we introduce a unifying search space designframework based on context-free grammars that can naturally and compactlygenerate expressive hierarchical search spaces that are 100s of orders ofmagnitude larger than common spaces from the literature. By enhancing and usingtheir properties, we effectively enable search over the complete architectureand can foster regularity. Further, we propose an efficient hierarchical kerneldesign for a Bayesian Optimization search strategy to efficiently search oversuch huge spaces. We demonstrate the versatility of our search space designframework and show that our search strategy can be superior to existing NASapproaches. Code is available athttps://github.com/automl/hierarchical_nas_construction.</description><author>Simon Schrodi, Danny Stoll, Binxin Ru, Rhea Sukthanker, Thomas Brox, Frank Hutter</author><pubDate>Fri, 08 Dec 2023 14:09:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01842v3</guid></item><item><title>Novel Fundus Image Preprocessing for Retcam Images to Improve Deep Learning Classification of Retinopathy of Prematurity</title><link>http://arxiv.org/abs/2302.02524v4</link><description>Retinopathy of Prematurity (ROP) is a potentially blinding eye disorderbecause of damage to the eye's retina which can affect babies born prematurely.Screening of ROP is essential for early detection and treatment. This is alaborious and manual process which requires trained physician performingdilated ophthalmological examination which can be subjective resulting in lowerdiagnosis success for clinically significant disease. Automated diagnosticmethods can assist ophthalmologists increase diagnosis accuracy using deeplearning. Several research groups have highlighted various approaches. CapturedROP Retcam images suffer from poor quality. This paper proposes the use ofimproved novel fundus preprocessing methods using pretrained transfer learningframeworks to create hybrid models to give higher diagnosis accuracy. Oncetrained and validated, the evaluations showed that these novel methods incomparison to traditional imaging processing contribute to better and in manyaspects higher accuracy in classifying Plus disease, Stages of ROP and Zones incomparison to peer papers.</description><author>Sajid Rahim, Kourosh Sabri, Anna Ells, Alan Wassyng, Mark Lawford, Linyang Chu, Wenbo He</author><pubDate>Fri, 08 Dec 2023 14:03:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02524v4</guid></item><item><title>Converting Epics/Stories into Pseudocode using Transformers</title><link>http://arxiv.org/abs/2312.05047v1</link><description>The conversion of user epics or stories into their appropriate representationin pseudocode or code is a time-consuming task, which can take up a largeportion of the time in an industrial project. With this research paper, we aimto present a methodology to generate pseudocode from a given agile user storyof small functionalities so as to reduce the overall time spent on theindustrial project. Pseudocode is a programming language agnosticrepresentation of the steps involved in a computer program, which can be easilyconverted into any programming language. Leveraging the potential of NaturalLanguage Processing, we want to simplify the development process inorganizations that use the Agile Model of Software Development. We present amethodology to convert a problem described in the English language intopseudocode. This methodology divides the Text to Pseudocode conversion taskinto two stages or subtasks, each of which is treated like an individualmachine translation task. Stage 1 is Text to Code Conversion and Stage 2 isCode to Pseudocode Conversion. We find that the CodeT5 model gives the bestresults in terms of BLEU score when trained separately on the two subtasksmentioned above. BLEU score is a metric that is used to measure the similaritybetween a machine-translated text and a set of reference translations.</description><author>Gaurav Kolhatkar, Akshit Madan, Nidhi Kowtal, Satyajit Roy, Sheetal Sonawane</author><pubDate>Fri, 08 Dec 2023 14:01:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05047v1</guid></item><item><title>MuVieCAST: Multi-View Consistent Artistic Style Transfer</title><link>http://arxiv.org/abs/2312.05046v1</link><description>We introduce MuVieCAST, a modular multi-view consistent style transfernetwork architecture that enables consistent style transfer between multipleviewpoints of the same scene. This network architecture supports both sparseand dense views, making it versatile enough to handle a wide range ofmulti-view image datasets. The approach consists of three modules that performspecific tasks related to style transfer, namely content preservation, imagetransformation, and multi-view consistency enforcement. We extensively evaluateour approach across multiple application domains including depth-map-basedpoint cloud fusion, mesh reconstruction, and novel-view synthesis. Ourexperiments reveal that the proposed framework achieves an exceptionalgeneration of stylized images, exhibiting consistent outcomes acrossperspectives. A user study focusing on novel-view synthesis further confirmsthese results, with approximately 68\% of cases participants expressing apreference for our generated outputs compared to the recent state-of-the-artmethod. Our modular framework is extensible and can easily be integrated withvarious backbone architectures, making it a flexible solution for multi-viewstyle transfer. More results are demonstrated on our project page:muviecast.github.io</description><author>Nail Ibrahimli, Julian F. P. Kooij, Liangliang Nan</author><pubDate>Fri, 08 Dec 2023 14:01:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05046v1</guid></item><item><title>DisCO: Portrait Distortion Correction with Perspective-Aware 3D GANs</title><link>http://arxiv.org/abs/2302.12253v3</link><description>Close-up facial images captured at short distances often suffer fromperspective distortion, resulting in exaggerated facial features andunnatural/unattractive appearances. We propose a simple yet effective methodfor correcting perspective distortions in a single close-up face. We firstperform GAN inversion using a perspective-distorted input facial image byjointly optimizing the camera intrinsic/extrinsic parameters and face latentcode. To address the ambiguity of joint optimization, we develop starting froma short distance, optimization scheduling, reparametrizations, and geometricregularization. Re-rendering the portrait at a proper focal length and cameradistance effectively corrects perspective distortions and produces morenatural-looking results. Our experiments show that our method comparesfavorably against previous approaches qualitatively and quantitatively. Weshowcase numerous examples validating the applicability of our method onin-the-wild portrait photos. We will release our code and the evaluationprotocol to facilitate future work.</description><author>Zhixiang Wang, Yu-Lun Liu, Jia-Bin Huang, Shin'ichi Satoh, Sizhuo Ma, Gurunandan Krishnan, Jian Wang</author><pubDate>Fri, 08 Dec 2023 14:00:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12253v3</guid></item><item><title>Temporal graph models fail to capture global temporal dynamics</title><link>http://arxiv.org/abs/2309.15730v3</link><description>A recently released Temporal Graph Benchmark is analyzed in the context ofDynamic Link Property Prediction. We outline our observations and propose atrivial optimization-free baseline of "recently popular nodes" outperformingother methods on medium and large-size datasets in the Temporal GraphBenchmark. We propose two measures based on Wasserstein distance which canquantify the strength of short-term and long-term global dynamics of datasets.By analyzing our unexpectedly strong baseline, we show how standard negativesampling evaluation can be unsuitable for datasets with strong temporaldynamics. We also show how simple negative-sampling can lead to modeldegeneration during training, resulting in impossible to rank, fully saturatedpredictions of temporal graph networks. We propose improved negative samplingschemes for both training and evaluation and prove their usefulness. We conducta comparison with a model trained non-contrastively without negative sampling.Our results provide a challenging baseline and indicate that temporal graphnetwork architectures need deep rethinking for usage in problems withsignificant global dynamics, such as social media, cryptocurrency markets ore-commerce. We open-source the code for baselines, measures and proposednegative sampling schemes.</description><author>Michał Daniluk, Jacek Dąbrowski</author><pubDate>Fri, 08 Dec 2023 13:54:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15730v3</guid></item><item><title>Backward Learning for Goal-Conditioned Policies</title><link>http://arxiv.org/abs/2312.05044v1</link><description>Can we learn policies in reinforcement learning without rewards? Can we learna policy just by trying to reach a goal state? We answer these questionspositively by proposing a multi-step procedure that first learns a world modelthat goes backward in time, secondly generates goal-reaching backwardtrajectories, thirdly improves those sequences using shortest path findingalgorithms, and finally trains a neural network policy by imitation learning.We evaluate our method on a deterministic maze environment where theobservations are $64\times 64$ pixel bird's eye images and can show that itconsistently reaches several goals.</description><author>Marc Höftmann, Jan Robine, Stefan Harmeling</author><pubDate>Fri, 08 Dec 2023 13:52:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05044v1</guid></item><item><title>Behavioral Intention Prediction in Driving Scenes: A Survey</title><link>http://arxiv.org/abs/2211.00385v3</link><description>In the driving scene, the road agents usually conduct frequent interactionsand intention understanding of the surroundings. Ego-agent (each road agentitself) predicts what behavior will be engaged by other road users all the timeand expects a shared and consistent understanding for safe movement. BehavioralIntention Prediction (BIP) simulates such a human consideration process andfulfills the early prediction of specific behaviors. Similar to otherprediction tasks, such as trajectory prediction, data-driven deep learningmethods have taken the primary pipeline in research. The rapid development ofBIP inevitably leads to new issues and challenges. To catalyze future research,this work provides a comprehensive review of BIP from the available datasets,key factors and challenges, pedestrian-centric and vehicle-centric BIPapproaches, and BIP-aware applications. Based on the investigation, data-drivendeep learning approaches have become the primary pipelines. The behavioralintention types are still monotonous in most current datasets and methods(e.g., Crossing (C) and Not Crossing (NC) for pedestrians and Lane Changing(LC) for vehicles) in this field. In addition, for the safe-critical scenarios(e.g., near-crashing situations), current research is limited. Through thisinvestigation, we identify open issues in behavioral intention prediction andsuggest possible insights for future research.</description><author>Jianwu Fang, Fan Wang, Jianru Xue, Tat-seng Chua</author><pubDate>Fri, 08 Dec 2023 13:51:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.00385v3</guid></item><item><title>Physical-Layer Semantic-Aware Network for Zero-Shot Wireless Sensing</title><link>http://arxiv.org/abs/2312.05043v1</link><description>Device-free wireless sensing has recently attracted significant interest dueto its potential to support a wide range of immersive human-machine interactiveapplications. However, data heterogeneity in wireless signals and data privacyregulation of distributed sensing have been considered as the major challengesthat hinder the wide applications of wireless sensing in large area networkingsystems. Motivated by the observation that signals recorded by wirelessreceivers are closely related to a set of physical-layer semantic features, inthis paper we propose a novel zero-shot wireless sensing solution that allowsmodels constructed in one or a limited number of locations to be directlytransferred to other locations without any labeled data. We develop a novelphysical-layer semantic-aware network (pSAN) framework to characterize thecorrelation between physical-layer semantic features and the sensing datadistributions across different receivers. We then propose a pSAN-basedzero-shot learning solution in which each receiver can obtain alocation-specific gesture recognition model by directly aggregating the alreadyconstructed models of other receivers. We theoretically prove that modelsobtained by our proposed solution can approach the optimal model withoutrequiring any local model training. Experimental results once again verify thatthe accuracy of models derived by our proposed solution matches that of themodels trained by the real labeled data based on supervised learning approach.</description><author>Huixiang Zhu, Yong Xiao, Yingyu Li, Guangming Shi, Walid Saad</author><pubDate>Fri, 08 Dec 2023 13:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05043v1</guid></item><item><title>A Study of Forward-Forward Algorithm for Self-Supervised Learning</title><link>http://arxiv.org/abs/2309.11955v2</link><description>Self-supervised representation learning has seen remarkable progress in thelast few years, with some of the recent methods being able to learn usefulimage representations without labels. These methods are trained usingbackpropagation, the de facto standard. Recently, Geoffrey Hinton proposed theforward-forward algorithm as an alternative training method. It utilizes twoforward passes and a separate loss function for each layer to train the networkwithout backpropagation. In this study, for the first time, we study the performance offorward-forward vs. backpropagation for self-supervised representation learningand provide insights into the learned representation spaces. Our benchmarkemploys four standard datasets, namely MNIST, F-MNIST, SVHN and CIFAR-10, andthree commonly used self-supervised representation learning techniques, namelyrotation, flip and jigsaw. Our main finding is that while the forward-forward algorithm performscomparably to backpropagation during (self-)supervised training, the transferperformance is significantly lagging behind in all the studied settings. Thismay be caused by a combination of factors, including having a loss function foreach layer and the way the supervised training is realized in theforward-forward paradigm. In comparison to backpropagation, the forward-forwardalgorithm focuses more on the boundaries and drops part of the informationunnecessary for making decisions which harms the representation learning goal.Further investigation and research are necessary to stabilize theforward-forward strategy for self-supervised learning, to work beyond thedatasets and configurations demonstrated by Geoffrey Hinton.</description><author>Jonas Brenig, Radu Timofte</author><pubDate>Fri, 08 Dec 2023 13:46:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11955v2</guid></item><item><title>Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities</title><link>http://arxiv.org/abs/2306.04829v2</link><description>Unsupervised video-based object-centric learning is a promising avenue tolearn structured representations from large, unlabeled video collections, butprevious approaches have only managed to scale to real-world datasets inrestricted domains. Recently, it was shown that the reconstruction ofpre-trained self-supervised features leads to object-centric representations onunconstrained real-world image datasets. Building on this approach, we proposea novel way to use such pre-trained features in the form of a temporal featuresimilarity loss. This loss encodes semantic and temporal correlations betweenimage patches and is a natural way to introduce a motion bias for objectdiscovery. We demonstrate that this loss leads to state-of-the-art performanceon the challenging synthetic MOVi datasets. When used in combination with thefeature reconstruction loss, our model is the first object-centric video modelthat scales to unconstrained video datasets such as YouTube-VIS.</description><author>Andrii Zadaianchuk, Maximilian Seitzer, Georg Martius</author><pubDate>Fri, 08 Dec 2023 13:40:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04829v2</guid></item><item><title>Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization</title><link>http://arxiv.org/abs/2310.03234v3</link><description>This paper investigates new families of compositional optimization problems,called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bfw}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bfc}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWCFCCO). There has been a growing interest in FCCO due to its wide-rangingapplications in machine learning and AI, as well as its ability to address theshortcomings of stochastic algorithms based on empirical risk minimization.However, current research on FCCO presumes that both the inner and outerfunctions are smooth, limiting their potential to tackle a more diverse set ofproblems. Our research expands on this area by examining non-smoothweakly-convex FCCO, where the outer function is weakly convex andnon-decreasing, and the inner function is weakly-convex. We analyze asingle-loop algorithm and establish its complexity for finding an$\epsilon$-stationary point of the Moreau envelop of the objective function.Additionally, we also extend the algorithm to solving novel non-smoothweakly-convex tri-level finite-sum coupled compositional optimization problems,which feature a nested arrangement of three functions. Lastly, we explore theapplications of our algorithms in deep learning for two-way partial AUCmaximization and multi-instance two-way partial AUC maximization, usingempirical studies to showcase the effectiveness of the proposed algorithms.</description><author>Quanqi Hu, Dixian Zhu, Tianbao Yang</author><pubDate>Fri, 08 Dec 2023 13:39:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03234v3</guid></item><item><title>Graph Neural Networks with polynomial activations have limited expressivity</title><link>http://arxiv.org/abs/2310.13139v3</link><description>The expressivity of Graph Neural Networks (GNNs) can be entirelycharacterized by appropriate fragments of the first-order logic. Namely, anyquery of the two variable fragment of graded modal logic (GC2) interpreted overlabeled graphs can be expressed using a GNN whose size depends only on thedepth of the query. As pointed out by [Barcelo &amp; Al., 2020, Grohe, 2021], thisdescription holds for a family of activation functions, leaving the possibilityfor a hierarchy of logics expressible by GNNs depending on the chosenactivation function. In this article, we show that such hierarchy indeed existsby proving that GC2 queries cannot be expressed by GNNs with polynomialactivation functions. This implies a separation between polynomial and popularnon-polynomial activations (such as Rectified Linear Units) and answers an openquestion formulated by [Grohe, 2021].</description><author>Sammy Khalife</author><pubDate>Fri, 08 Dec 2023 13:38:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13139v3</guid></item><item><title>SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control</title><link>http://arxiv.org/abs/2312.05039v1</link><description>The field of generative image inpainting and object insertion has madesignificant progress with the recent advent of latent diffusion models.Utilizing a precise object mask can greatly enhance these applications.However, due to the challenges users encounter in creating high-fidelity masks,there is a tendency for these methods to rely on more coarse masks (e.g.,bounding box) for these applications. This results in limited control andcompromised background content preservation. To overcome these limitations, weintroduce SmartMask, which allows any novice user to create detailed masks forprecise object insertion. Combined with a ControlNet-Inpaint model, ourexperiments demonstrate that SmartMask achieves superior object insertionquality, preserving the background content more effectively than previousmethods. Notably, unlike prior works the proposed approach can also be usedeven without user-mask guidance, which allows it to perform mask-free objectinsertion at diverse positions and scales. Furthermore, we find that when usediteratively with a novel instruction-tuning based planning model, SmartMask canbe used to design detailed layouts from scratch. As compared with user-scribblebased layout design, we observe that SmartMask allows for better qualityoutputs with layout-to-image generation methods. Project page is available athttps://smartmask-gen.github.io</description><author>Jaskirat Singh, Jianming Zhang, Qing Liu, Cameron Smith, Zhe Lin, Liang Zheng</author><pubDate>Fri, 08 Dec 2023 13:38:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05039v1</guid></item><item><title>Prompt-In-Prompt Learning for Universal Image Restoration</title><link>http://arxiv.org/abs/2312.05038v1</link><description>Image restoration, which aims to retrieve and enhance degraded images, isfundamental across a wide range of applications. While conventional deeplearning approaches have notably improved the image quality across varioustasks, they still suffer from (i) the high storage cost needed for varioustask-specific models and (ii) the lack of interactivity and flexibility,hindering their wider application. Drawing inspiration from the pronouncedsuccess of prompts in both linguistic and visual domains, we propose novelPrompt-In-Prompt learning for universal image restoration, named PIP. First, wepresent two novel prompts, a degradation-aware prompt to encode high-leveldegradation knowledge and a basic restoration prompt to provide essentiallow-level information. Second, we devise a novel prompt-to-prompt interactionmodule to fuse these two prompts into a universal restoration prompt. Third, weintroduce a selective prompt-to-feature interaction module to modulate thedegradation-related feature. By doing so, the resultant PIP works as aplug-and-play module to enhance existing restoration models for universal imagerestoration. Extensive experimental results demonstrate the superiorperformance of PIP on multiple restoration tasks, including image denoising,deraining, dehazing, deblurring, and low-light enhancement. Remarkably, PIP isinterpretable, flexible, efficient, and easy-to-use, showing promisingpotential for real-world applications. The code is available athttps://github.com/longzilicart/pip_universal.</description><author>Zilong Li, Yiming Lei, Chenglong Ma, Junping Zhang, Hongming Shan</author><pubDate>Fri, 08 Dec 2023 13:36:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05038v1</guid></item><item><title>SAfER: Layer-Level Sensitivity Assessment for Efficient and Robust Neural Network Inference</title><link>http://arxiv.org/abs/2308.04753v2</link><description>Deep neural networks (DNNs) demonstrate outstanding performance across mostcomputer vision tasks. Some critical applications, such as autonomous drivingor medical imaging, also require investigation into their behavior and thereasons behind the decisions they make. In this vein, DNN attribution consistsin studying the relationship between the predictions of a DNN and its inputs.Attribution methods have been adapted to highlight the most relevant weights orneurons in a DNN, allowing to more efficiently select which weights or neuronscan be pruned. However, a limitation of these approaches is that weights aretypically compared within each layer separately, while some layers might appearas more critical than others. In this work, we propose to investigate DNN layerimportance, i.e. to estimate the sensitivity of the accuracy w.r.t.perturbations applied at the layer level. To do so, we propose a novel datasetto evaluate our method as well as future works. We benchmark a number ofcriteria and draw conclusions regarding how to assess DNN layer importance and,consequently, how to budgetize layers for increased DNN efficiency (withapplications for DNN pruning and quantization), as well as robustness tohardware failure (e.g. bit swaps).</description><author>Edouard Yvinec, Arnaud Dapogny, Kevin Bailly, Xavier Fischer</author><pubDate>Fri, 08 Dec 2023 13:35:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04753v2</guid></item><item><title>Grasp Force Optimization as a Bilinear Matrix Inequality Problem: A Deep Learning Approach</title><link>http://arxiv.org/abs/2312.05034v1</link><description>Grasp force synthesis is a non-convex optimization problem involvingconstraints that are bilinear. Traditional approaches to this problem involvegeneral-purpose gradient-based nonlinear optimization and semi-definiteprogramming. With a view towards dealing with postural synergies and non-smoothbut convex positive semidefinite constraints, we look beyond gradient-basedoptimization. The focus of this paper is to undertake a grasp analysis ofbiomimetic grasping in multi-fingered robotic hands as a bilinear matrixinequality (BMI) problem. Our analysis is to solve it using a deep learningapproach to make the algorithm efficiently generate force closure grasps withoptimal grasp quality on untrained/unseen objects.</description><author>Hirakjyoti Basumatary, Daksh Adhar, Riddhiman Shaw, Shyamanta M. Hazarika</author><pubDate>Fri, 08 Dec 2023 13:28:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05034v1</guid></item><item><title>Synthesizing Traffic Datasets using Graph Neural Networks</title><link>http://arxiv.org/abs/2312.05031v1</link><description>Traffic congestion in urban areas presents significant challenges, andIntelligent Transportation Systems (ITS) have sought to address these viaautomated and adaptive controls. However, these systems often struggle totransfer simulated experiences to real-world scenarios. This paper introduces anovel methodology for bridging this `sim-real' gap by creating photorealisticimages from 2D traffic simulations and recorded junction footage. We propose anovel image generation approach, integrating a Conditional GenerativeAdversarial Network with a Graph Neural Network (GNN) to facilitate thecreation of realistic urban traffic images. We harness GNNs' ability to processinformation at different levels of abstraction alongside segmented images forpreserving locality data. The presented architecture leverages the power ofSPADE and Graph ATtention (GAT) network models to create images based onsimulated traffic scenarios. These images are conditioned by factors such asentity positions, colors, and time of day. The uniqueness of our approach liesin its ability to effectively translate structured and human-readableconditions, encoded as graphs, into realistic images. This advancementcontributes to applications requiring rich traffic image datasets, from dataaugmentation to urban traffic solutions. We further provide an application totest the model's capabilities, including generating images with manuallydefined positions for various entities.</description><author>Daniel Rodriguez-Criado, Maria Chli, Luis J. Manso, George Vogiatzis</author><pubDate>Fri, 08 Dec 2023 13:24:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05031v1</guid></item><item><title>Cluster images with AntClust: a clustering algorithm based on the chemical recognition system of ants</title><link>http://arxiv.org/abs/2312.05028v1</link><description>We implement AntClust, a clustering algorithm based on the chemicalrecognition system of ants and use it to cluster images of cars. We will give ashort recap summary of the main working principles of the algorithm as devisedby the original paper [1]. Further, we will describe how to define a similarityfunction for images and how the implementation is used to cluster images ofcars from the vehicle re-identification data set. We then test the clusteringperformance of AntClust against DBSCAN, HDBSCAN and OPTICS. Finally one of thecore parts in AntClust, the rule set can be easily redefined with ourimplementation, enabling a way for other bio-inspired algorithms to find rulesin an automated process. The implementation can be found on GitLab [9].</description><author>Winfried Gero Oed, Parisa Memarmoshrefi</author><pubDate>Fri, 08 Dec 2023 13:14:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05028v1</guid></item></channel></rss>